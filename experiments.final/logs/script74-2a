+ RUN=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ LAYERS=300_300_300_1
+ case $RUN in
+ PSI=2
+ OPTIONS='			 --optimizer adam 				 --n_pairs 50000 				 --batch_size 5000 				 --max_epochs 30 				 --learning_rate 0.001 				 --decay_rate 0.8 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output74
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output75
+ for fn in f1
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output74/f1_psi2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output75/f1_psi2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output74/f1_psi2_phi0 /home/mrdouglas/Manifold/experiments.final/output75/f1_psi2_phi0
+ date
Sat Oct 31 16:28:42 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output74/f1_psi2_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --function f1 --psi 2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output74/f1_psi2_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 30 --learning_rate 0.001 --decay_rate 0.8 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d9062c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d9062c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d905b8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d904bbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90605268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90605d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90551f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d905467b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d905468c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d9044ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90546bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d9029b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90403620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90210400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d9029be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d9029b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d9017d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90266598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d900e1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d900e1ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d901021e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90096840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d903ac8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d903c6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90057730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d903528c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d903368c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90352400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90116488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90120a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90116f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d9021fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d9021f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90044b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d9035b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3d90392d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.3568527
test_loss: 0.35916302
train_loss: 0.36127922
test_loss: 0.3587995
train_loss: 0.3587767
test_loss: 0.3586277
train_loss: 0.3693859
test_loss: 0.35855785
train_loss: 0.35855657
test_loss: 0.35851
train_loss: 0.35658962
test_loss: 0.3585333
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output75/f1_psi2_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output74/f1_psi2_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output75/f1_psi2_phi0/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec676aa6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec67676488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec676ae840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec676cc158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec675e61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec675e66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec675e6158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec67614268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec675e6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec676167b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec675261e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec67555b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec675558c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5d171488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec67555bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5d0ffea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec6747c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec6748dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5d0abe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5d049048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5d049268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5d0ddae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5d01d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5d020f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5d0062f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5cfa07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5cfa0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5cfa0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5cfa01e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5cf1e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5cf65f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5cfe0a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5cfe0e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5ce669d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5ce668c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec5cdc6158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.224626765
Iter: 2 loss: 0.078667596
Iter: 3 loss: 0.0649956912
Iter: 4 loss: 0.0279183388
Iter: 5 loss: 0.0251646806
Iter: 6 loss: 0.0202013217
Iter: 7 loss: 0.02029147
Iter: 8 loss: 0.016316399
Iter: 9 loss: 0.0140033783
Iter: 10 loss: 0.0136971939
Iter: 11 loss: 0.012745929
Iter: 12 loss: 0.0122135784
Iter: 13 loss: 0.0118308021
Iter: 14 loss: 0.010942705
Iter: 15 loss: 0.0116394749
Iter: 16 loss: 0.010438526
Iter: 17 loss: 0.00984963
Iter: 18 loss: 0.0150704198
Iter: 19 loss: 0.00982519425
Iter: 20 loss: 0.0094419606
Iter: 21 loss: 0.00913576595
Iter: 22 loss: 0.0090341568
Iter: 23 loss: 0.00857923552
Iter: 24 loss: 0.00854757614
Iter: 25 loss: 0.00822465215
Iter: 26 loss: 0.00788792
Iter: 27 loss: 0.00781477615
Iter: 28 loss: 0.0076124547
Iter: 29 loss: 0.00729966955
Iter: 30 loss: 0.00969567057
Iter: 31 loss: 0.00728336349
Iter: 32 loss: 0.0070966538
Iter: 33 loss: 0.00722081028
Iter: 34 loss: 0.00698732445
Iter: 35 loss: 0.00684645027
Iter: 36 loss: 0.00677213818
Iter: 37 loss: 0.00671543041
Iter: 38 loss: 0.00661994051
Iter: 39 loss: 0.00661361776
Iter: 40 loss: 0.00654471759
Iter: 41 loss: 0.00646667276
Iter: 42 loss: 0.00643277
Iter: 43 loss: 0.00639634207
Iter: 44 loss: 0.0063335
Iter: 45 loss: 0.00674523972
Iter: 46 loss: 0.00632836297
Iter: 47 loss: 0.00628410187
Iter: 48 loss: 0.00632264838
Iter: 49 loss: 0.00625925884
Iter: 50 loss: 0.00622302759
Iter: 51 loss: 0.00618598657
Iter: 52 loss: 0.00617973041
Iter: 53 loss: 0.00612760615
Iter: 54 loss: 0.00614976883
Iter: 55 loss: 0.00609429646
Iter: 56 loss: 0.00601002
Iter: 57 loss: 0.00614314573
Iter: 58 loss: 0.00597665319
Iter: 59 loss: 0.00590509642
Iter: 60 loss: 0.00582943065
Iter: 61 loss: 0.00581833394
Iter: 62 loss: 0.00571492966
Iter: 63 loss: 0.00572290411
Iter: 64 loss: 0.00563752372
Iter: 65 loss: 0.00549259782
Iter: 66 loss: 0.00561147463
Iter: 67 loss: 0.00540390285
Iter: 68 loss: 0.00524736149
Iter: 69 loss: 0.00524607953
Iter: 70 loss: 0.00513308728
Iter: 71 loss: 0.00511809
Iter: 72 loss: 0.00505583454
Iter: 73 loss: 0.0049377745
Iter: 74 loss: 0.00501812343
Iter: 75 loss: 0.00485707
Iter: 76 loss: 0.00473351637
Iter: 77 loss: 0.00479190797
Iter: 78 loss: 0.00466785673
Iter: 79 loss: 0.00451891869
Iter: 80 loss: 0.00450140703
Iter: 81 loss: 0.00442294776
Iter: 82 loss: 0.00469288696
Iter: 83 loss: 0.00440818816
Iter: 84 loss: 0.00434853928
Iter: 85 loss: 0.0049764053
Iter: 86 loss: 0.00434654113
Iter: 87 loss: 0.00428431341
Iter: 88 loss: 0.00440905709
Iter: 89 loss: 0.00426084362
Iter: 90 loss: 0.00422056485
Iter: 91 loss: 0.00435134117
Iter: 92 loss: 0.00421040412
Iter: 93 loss: 0.00418103393
Iter: 94 loss: 0.00426166598
Iter: 95 loss: 0.0041715
Iter: 96 loss: 0.00413627923
Iter: 97 loss: 0.00408436218
Iter: 98 loss: 0.00408363808
Iter: 99 loss: 0.00404065382
Iter: 100 loss: 0.00416681264
Iter: 101 loss: 0.00402829144
Iter: 102 loss: 0.00399367837
Iter: 103 loss: 0.00441965275
Iter: 104 loss: 0.0039919652
Iter: 105 loss: 0.00395143125
Iter: 106 loss: 0.00415419694
Iter: 107 loss: 0.0039442433
Iter: 108 loss: 0.0038877239
Iter: 109 loss: 0.00400392618
Iter: 110 loss: 0.00386576238
Iter: 111 loss: 0.00382679305
Iter: 112 loss: 0.00385099044
Iter: 113 loss: 0.00380082568
Iter: 114 loss: 0.00373500446
Iter: 115 loss: 0.00385900587
Iter: 116 loss: 0.0037045849
Iter: 117 loss: 0.00363543374
Iter: 118 loss: 0.00363500696
Iter: 119 loss: 0.00356203085
Iter: 120 loss: 0.00354560185
Iter: 121 loss: 0.00354564609
Iter: 122 loss: 0.00349638727
Iter: 123 loss: 0.00339498
Iter: 124 loss: 0.0104859602
Iter: 125 loss: 0.00339454692
Iter: 126 loss: 0.00328690535
Iter: 127 loss: 0.00426501082
Iter: 128 loss: 0.0032780217
Iter: 129 loss: 0.00321838958
Iter: 130 loss: 0.00596652366
Iter: 131 loss: 0.0032179635
Iter: 132 loss: 0.00315269036
Iter: 133 loss: 0.00352228712
Iter: 134 loss: 0.0031457406
Iter: 135 loss: 0.00308041694
Iter: 136 loss: 0.00309522077
Iter: 137 loss: 0.00302315224
Iter: 138 loss: 0.00306901732
Iter: 139 loss: 0.00298325811
Iter: 140 loss: 0.00296353432
Iter: 141 loss: 0.00336310943
Iter: 142 loss: 0.00296333805
Iter: 143 loss: 0.00294785877
Iter: 144 loss: 0.0029559359
Iter: 145 loss: 0.0029360312
Iter: 146 loss: 0.00288294302
Iter: 147 loss: 0.002930969
Iter: 148 loss: 0.0028541619
Iter: 149 loss: 0.00278486032
Iter: 150 loss: 0.00370013225
Iter: 151 loss: 0.00277978228
Iter: 152 loss: 0.00275237905
Iter: 153 loss: 0.0027523376
Iter: 154 loss: 0.00273679802
Iter: 155 loss: 0.0027807178
Iter: 156 loss: 0.00273032486
Iter: 157 loss: 0.00270848675
Iter: 158 loss: 0.0027012818
Iter: 159 loss: 0.00268911803
Iter: 160 loss: 0.00269273249
Iter: 161 loss: 0.00267100171
Iter: 162 loss: 0.00264607882
Iter: 163 loss: 0.00276556378
Iter: 164 loss: 0.00264205434
Iter: 165 loss: 0.0026300007
Iter: 166 loss: 0.00286756223
Iter: 167 loss: 0.00262965099
Iter: 168 loss: 0.00261269975
Iter: 169 loss: 0.0026111817
Iter: 170 loss: 0.00259847404
Iter: 171 loss: 0.00257577468
Iter: 172 loss: 0.00275293877
Iter: 173 loss: 0.00257408479
Iter: 174 loss: 0.00256220391
Iter: 175 loss: 0.0026130951
Iter: 176 loss: 0.00255973358
Iter: 177 loss: 0.00254747621
Iter: 178 loss: 0.00257460703
Iter: 179 loss: 0.00254212529
Iter: 180 loss: 0.00253512128
Iter: 181 loss: 0.00253459206
Iter: 182 loss: 0.0025288309
Iter: 183 loss: 0.00252468046
Iter: 184 loss: 0.00252264645
Iter: 185 loss: 0.00250900257
Iter: 186 loss: 0.00248544104
Iter: 187 loss: 0.00248542195
Iter: 188 loss: 0.00247041602
Iter: 189 loss: 0.00258080428
Iter: 190 loss: 0.00246892544
Iter: 191 loss: 0.00245789438
Iter: 192 loss: 0.0024307156
Iter: 193 loss: 0.00266786152
Iter: 194 loss: 0.00242698984
Iter: 195 loss: 0.00240719109
Iter: 196 loss: 0.00293808803
Iter: 197 loss: 0.00240718573
Iter: 198 loss: 0.00239969604
Iter: 199 loss: 0.00239871978
Iter: 200 loss: 0.00239464315
Iter: 201 loss: 0.00239205686
Iter: 202 loss: 0.00239044055
Iter: 203 loss: 0.00238507194
Iter: 204 loss: 0.00237568887
Iter: 205 loss: 0.00237568328
Iter: 206 loss: 0.00235922355
Iter: 207 loss: 0.00253024092
Iter: 208 loss: 0.00235881144
Iter: 209 loss: 0.00235323375
Iter: 210 loss: 0.00237083901
Iter: 211 loss: 0.00235168054
Iter: 212 loss: 0.00234240945
Iter: 213 loss: 0.00232217181
Iter: 214 loss: 0.00276146689
Iter: 215 loss: 0.00232159812
Iter: 216 loss: 0.00231025601
Iter: 217 loss: 0.00234869798
Iter: 218 loss: 0.002307249
Iter: 219 loss: 0.00230270345
Iter: 220 loss: 0.00229607103
Iter: 221 loss: 0.00229576649
Iter: 222 loss: 0.00227793958
Iter: 223 loss: 0.00235120719
Iter: 224 loss: 0.00227392884
Iter: 225 loss: 0.0022471936
Iter: 226 loss: 0.00238824356
Iter: 227 loss: 0.00224229135
Iter: 228 loss: 0.00224049343
Iter: 229 loss: 0.00223578396
Iter: 230 loss: 0.00223266985
Iter: 231 loss: 0.00224407692
Iter: 232 loss: 0.00223196344
Iter: 233 loss: 0.00222740369
Iter: 234 loss: 0.00222725305
Iter: 235 loss: 0.00222021458
Iter: 236 loss: 0.00220902078
Iter: 237 loss: 0.00220891
Iter: 238 loss: 0.00219624769
Iter: 239 loss: 0.00228958437
Iter: 240 loss: 0.00219522347
Iter: 241 loss: 0.00218728208
Iter: 242 loss: 0.00222100527
Iter: 243 loss: 0.00218551909
Iter: 244 loss: 0.00218183827
Iter: 245 loss: 0.00222664024
Iter: 246 loss: 0.00218173582
Iter: 247 loss: 0.00217919238
Iter: 248 loss: 0.00218249205
Iter: 249 loss: 0.0021779642
Iter: 250 loss: 0.00217530481
Iter: 251 loss: 0.00217776443
Iter: 252 loss: 0.00217376184
Iter: 253 loss: 0.00217127986
Iter: 254 loss: 0.00218452141
Iter: 255 loss: 0.00217091944
Iter: 256 loss: 0.00216691499
Iter: 257 loss: 0.00217754883
Iter: 258 loss: 0.00216560718
Iter: 259 loss: 0.00216032146
Iter: 260 loss: 0.00217986386
Iter: 261 loss: 0.00215898245
Iter: 262 loss: 0.00215093605
Iter: 263 loss: 0.00216084253
Iter: 264 loss: 0.00214661332
Iter: 265 loss: 0.00214813557
Iter: 266 loss: 0.0021448466
Iter: 267 loss: 0.00214246567
Iter: 268 loss: 0.00215544528
Iter: 269 loss: 0.00214212737
Iter: 270 loss: 0.00214090059
Iter: 271 loss: 0.00213842979
Iter: 272 loss: 0.00218886719
Iter: 273 loss: 0.00213839789
Iter: 274 loss: 0.00213568541
Iter: 275 loss: 0.00213440927
Iter: 276 loss: 0.00213310542
Iter: 277 loss: 0.00213063601
Iter: 278 loss: 0.00213237526
Iter: 279 loss: 0.00212903554
Iter: 280 loss: 0.00212818524
Iter: 281 loss: 0.002130426
Iter: 282 loss: 0.00212790165
Iter: 283 loss: 0.00212634262
Iter: 284 loss: 0.00214701262
Iter: 285 loss: 0.002126334
Iter: 286 loss: 0.00212491211
Iter: 287 loss: 0.00213017687
Iter: 288 loss: 0.00212455774
Iter: 289 loss: 0.00212337542
Iter: 290 loss: 0.00212393934
Iter: 291 loss: 0.00212258333
Iter: 292 loss: 0.00212113
Iter: 293 loss: 0.00211858936
Iter: 294 loss: 0.00211858843
Iter: 295 loss: 0.00211613509
Iter: 296 loss: 0.00212543039
Iter: 297 loss: 0.00211553
Iter: 298 loss: 0.00211299956
Iter: 299 loss: 0.00211671088
Iter: 300 loss: 0.0021117921
Iter: 301 loss: 0.00210967544
Iter: 302 loss: 0.00210963748
Iter: 303 loss: 0.00210821722
Iter: 304 loss: 0.00210958463
Iter: 305 loss: 0.00210741581
Iter: 306 loss: 0.00210657716
Iter: 307 loss: 0.00210800627
Iter: 308 loss: 0.00210617902
Iter: 309 loss: 0.00210530963
Iter: 310 loss: 0.00210461207
Iter: 311 loss: 0.00210435269
Iter: 312 loss: 0.00210165023
Iter: 313 loss: 0.00210862141
Iter: 314 loss: 0.0021007224
Iter: 315 loss: 0.00209836964
Iter: 316 loss: 0.0021038549
Iter: 317 loss: 0.00209755264
Iter: 318 loss: 0.00209448067
Iter: 319 loss: 0.00209857943
Iter: 320 loss: 0.00209290278
Iter: 321 loss: 0.00208841357
Iter: 322 loss: 0.00209612213
Iter: 323 loss: 0.00208629761
Iter: 324 loss: 0.00207794
Iter: 325 loss: 0.00208586222
Iter: 326 loss: 0.00207287609
Iter: 327 loss: 0.00207073637
Iter: 328 loss: 0.00209544343
Iter: 329 loss: 0.00207070913
Iter: 330 loss: 0.00206989422
Iter: 331 loss: 0.00207864819
Iter: 332 loss: 0.00206985651
Iter: 333 loss: 0.0020693636
Iter: 334 loss: 0.00206817663
Iter: 335 loss: 0.00208107312
Iter: 336 loss: 0.00206805486
Iter: 337 loss: 0.0020645177
Iter: 338 loss: 0.00206448254
Iter: 339 loss: 0.00205764035
Iter: 340 loss: 0.00208591577
Iter: 341 loss: 0.0020560876
Iter: 342 loss: 0.0020496042
Iter: 343 loss: 0.00206147181
Iter: 344 loss: 0.00204680488
Iter: 345 loss: 0.0020428542
Iter: 346 loss: 0.00204339018
Iter: 347 loss: 0.0020396621
Iter: 348 loss: 0.00203707395
Iter: 349 loss: 0.00205261074
Iter: 350 loss: 0.00203679455
Iter: 351 loss: 0.00203468604
Iter: 352 loss: 0.00204544375
Iter: 353 loss: 0.00203433726
Iter: 354 loss: 0.00203251
Iter: 355 loss: 0.00202944758
Iter: 356 loss: 0.00202943897
Iter: 357 loss: 0.00202377024
Iter: 358 loss: 0.00202041096
Iter: 359 loss: 0.00201803679
Iter: 360 loss: 0.0020135832
Iter: 361 loss: 0.00203525
Iter: 362 loss: 0.00201279321
Iter: 363 loss: 0.00202015555
Iter: 364 loss: 0.00201219274
Iter: 365 loss: 0.00201093778
Iter: 366 loss: 0.00201817974
Iter: 367 loss: 0.00201077573
Iter: 368 loss: 0.00200886512
Iter: 369 loss: 0.0020088274
Iter: 370 loss: 0.00200698501
Iter: 371 loss: 0.0020104975
Iter: 372 loss: 0.0020062027
Iter: 373 loss: 0.00200510956
Iter: 374 loss: 0.00201625307
Iter: 375 loss: 0.00200507836
Iter: 376 loss: 0.00200464763
Iter: 377 loss: 0.00200398127
Iter: 378 loss: 0.00200397335
Iter: 379 loss: 0.00200327486
Iter: 380 loss: 0.00200167065
Iter: 381 loss: 0.00202203519
Iter: 382 loss: 0.00200155471
Iter: 383 loss: 0.00199923618
Iter: 384 loss: 0.0020058509
Iter: 385 loss: 0.00199848483
Iter: 386 loss: 0.00199784432
Iter: 387 loss: 0.00199698494
Iter: 388 loss: 0.0019967705
Iter: 389 loss: 0.00199792138
Iter: 390 loss: 0.00199673697
Iter: 391 loss: 0.0019965726
Iter: 392 loss: 0.00199623592
Iter: 393 loss: 0.00200269464
Iter: 394 loss: 0.00199623196
Iter: 395 loss: 0.00203110185
Iter: 396 loss: 0.00199577282
Iter: 397 loss: 0.00199519587
Iter: 398 loss: 0.00199648459
Iter: 399 loss: 0.00199498306
Iter: 400 loss: 0.00199451251
Iter: 401 loss: 0.00199373858
Iter: 402 loss: 0.00199373392
Iter: 403 loss: 0.00199302193
Iter: 404 loss: 0.00199140189
Iter: 405 loss: 0.00201220205
Iter: 406 loss: 0.00199129502
Iter: 407 loss: 0.00199057534
Iter: 408 loss: 0.0019937614
Iter: 409 loss: 0.00199041981
Iter: 410 loss: 0.00199019047
Iter: 411 loss: 0.00198973832
Iter: 412 loss: 0.00199841661
Iter: 413 loss: 0.00198973436
Iter: 414 loss: 0.00198835507
Iter: 415 loss: 0.00198740326
Iter: 416 loss: 0.0019869064
Iter: 417 loss: 0.0019861632
Iter: 418 loss: 0.00198570965
Iter: 419 loss: 0.00198513828
Iter: 420 loss: 0.00198396761
Iter: 421 loss: 0.00200435077
Iter: 422 loss: 0.00198394363
Iter: 423 loss: 0.00198155548
Iter: 424 loss: 0.00197631
Iter: 425 loss: 0.00205258839
Iter: 426 loss: 0.00197607465
Iter: 427 loss: 0.00197541155
Iter: 428 loss: 0.0019752658
Iter: 429 loss: 0.00197453238
Iter: 430 loss: 0.00197934313
Iter: 431 loss: 0.00197445136
Iter: 432 loss: 0.00197502715
Iter: 433 loss: 0.00197385298
Iter: 434 loss: 0.00197340967
Iter: 435 loss: 0.00197321363
Iter: 436 loss: 0.00197298522
Iter: 437 loss: 0.00197211816
Iter: 438 loss: 0.00197127741
Iter: 439 loss: 0.00197108928
Iter: 440 loss: 0.00196912
Iter: 441 loss: 0.00197083433
Iter: 442 loss: 0.00196788437
Iter: 443 loss: 0.00196719728
Iter: 444 loss: 0.0019715298
Iter: 445 loss: 0.00196710881
Iter: 446 loss: 0.00196626689
Iter: 447 loss: 0.00196441216
Iter: 448 loss: 0.00199052365
Iter: 449 loss: 0.00196432206
Iter: 450 loss: 0.00196321215
Iter: 451 loss: 0.00196754048
Iter: 452 loss: 0.00196289667
Iter: 453 loss: 0.00196205825
Iter: 454 loss: 0.0019614012
Iter: 455 loss: 0.00196114671
Iter: 456 loss: 0.00198022695
Iter: 457 loss: 0.00195978885
Iter: 458 loss: 0.00195716275
Iter: 459 loss: 0.0019773664
Iter: 460 loss: 0.00195696764
Iter: 461 loss: 0.00195646798
Iter: 462 loss: 0.00195637462
Iter: 463 loss: 0.00195602048
Iter: 464 loss: 0.00195793761
Iter: 465 loss: 0.0019559667
Iter: 466 loss: 0.0019555348
Iter: 467 loss: 0.00195820676
Iter: 468 loss: 0.00195548404
Iter: 469 loss: 0.00195492851
Iter: 470 loss: 0.00195473433
Iter: 471 loss: 0.00195442303
Iter: 472 loss: 0.00195986312
Iter: 473 loss: 0.00195356342
Iter: 474 loss: 0.00195327168
Iter: 475 loss: 0.00195504865
Iter: 476 loss: 0.00195324607
Iter: 477 loss: 0.00195315573
Iter: 478 loss: 0.00195285992
Iter: 479 loss: 0.00195309124
Iter: 480 loss: 0.00195259589
Iter: 481 loss: 0.0019525073
Iter: 482 loss: 0.00195210264
Iter: 483 loss: 0.00195146701
Iter: 484 loss: 0.00195265457
Iter: 485 loss: 0.00195119972
Iter: 486 loss: 0.0019508017
Iter: 487 loss: 0.00195073208
Iter: 488 loss: 0.00195046281
Iter: 489 loss: 0.00195054105
Iter: 490 loss: 0.0019500585
Iter: 491 loss: 0.00194992987
Iter: 492 loss: 0.00195017713
Iter: 493 loss: 0.00194987282
Iter: 494 loss: 0.00194974872
Iter: 495 loss: 0.00194933382
Iter: 496 loss: 0.0019493734
Iter: 497 loss: 0.00194891228
Iter: 498 loss: 0.00194897701
Iter: 499 loss: 0.0019486167
Iter: 500 loss: 0.00194849842
Iter: 501 loss: 0.00194943324
Iter: 502 loss: 0.00194849307
Iter: 503 loss: 0.00194847
Iter: 504 loss: 0.00194843614
Iter: 505 loss: 0.00194843695
Iter: 506 loss: 0.00194836548
Iter: 507 loss: 0.00194837409
Iter: 508 loss: 0.00194831402
Iter: 509 loss: 0.00194819365
Iter: 510 loss: 0.0019479508
Iter: 511 loss: 0.00195231452
Iter: 512 loss: 0.00194794638
Iter: 513 loss: 0.00194726977
Iter: 514 loss: 0.00194620225
Iter: 515 loss: 0.00194619014
Iter: 516 loss: 0.00194784405
Iter: 517 loss: 0.00194582541
Iter: 518 loss: 0.00194561051
Iter: 519 loss: 0.00194563542
Iter: 520 loss: 0.00194544892
Iter: 521 loss: 0.00194520527
Iter: 522 loss: 0.00194801029
Iter: 523 loss: 0.00194520515
Iter: 524 loss: 0.00194555486
Iter: 525 loss: 0.00194513367
Iter: 526 loss: 0.00194508582
Iter: 527 loss: 0.00194504822
Iter: 528 loss: 0.00194503739
Iter: 529 loss: 0.00194499118
Iter: 530 loss: 0.00194497639
Iter: 531 loss: 0.00194495346
Iter: 532 loss: 0.0019449282
Iter: 533 loss: 0.00194483297
Iter: 534 loss: 0.00194458163
Iter: 535 loss: 0.00194912252
Iter: 536 loss: 0.00194457697
Iter: 537 loss: 0.00194397639
Iter: 538 loss: 0.00194397697
Iter: 539 loss: 0.00194370409
Iter: 540 loss: 0.00194459572
Iter: 541 loss: 0.00194363133
Iter: 542 loss: 0.00194352097
Iter: 543 loss: 0.00194366113
Iter: 544 loss: 0.0019434694
Iter: 545 loss: 0.00194331142
Iter: 546 loss: 0.00194276473
Iter: 547 loss: 0.00194165029
Iter: 548 loss: 0.00194164203
Iter: 549 loss: 0.00194107823
Iter: 550 loss: 0.00194277172
Iter: 551 loss: 0.00194091361
Iter: 552 loss: 0.00194060279
Iter: 553 loss: 0.00194092793
Iter: 554 loss: 0.00194043305
Iter: 555 loss: 0.00193970744
Iter: 556 loss: 0.00194039103
Iter: 557 loss: 0.00193929067
Iter: 558 loss: 0.00194025482
Iter: 559 loss: 0.00193908368
Iter: 560 loss: 0.00193897
Iter: 561 loss: 0.00193967368
Iter: 562 loss: 0.00193895551
Iter: 563 loss: 0.00193888182
Iter: 564 loss: 0.00193945621
Iter: 565 loss: 0.00193887611
Iter: 566 loss: 0.00193880952
Iter: 567 loss: 0.00193857553
Iter: 568 loss: 0.00193822477
Iter: 569 loss: 0.00193818379
Iter: 570 loss: 0.00193719578
Iter: 571 loss: 0.00193822477
Iter: 572 loss: 0.00193661312
Iter: 573 loss: 0.00193599658
Iter: 574 loss: 0.0019389363
Iter: 575 loss: 0.00193589972
Iter: 576 loss: 0.001935657
Iter: 577 loss: 0.00193585688
Iter: 578 loss: 0.0019355095
Iter: 579 loss: 0.00193527678
Iter: 580 loss: 0.0019367988
Iter: 581 loss: 0.00193524756
Iter: 582 loss: 0.00193511555
Iter: 583 loss: 0.00193480204
Iter: 584 loss: 0.0019384108
Iter: 585 loss: 0.00193477259
Iter: 586 loss: 0.00193415862
Iter: 587 loss: 0.00193390436
Iter: 588 loss: 0.00193358201
Iter: 589 loss: 0.00193329668
Iter: 590 loss: 0.00193327409
Iter: 591 loss: 0.00193317875
Iter: 592 loss: 0.00193295465
Iter: 593 loss: 0.00193550088
Iter: 594 loss: 0.00193293428
Iter: 595 loss: 0.00193261215
Iter: 596 loss: 0.00193268107
Iter: 597 loss: 0.00193237199
Iter: 598 loss: 0.00193227082
Iter: 599 loss: 0.0019322678
Iter: 600 loss: 0.00193216407
Iter: 601 loss: 0.00193195138
Iter: 602 loss: 0.001935862
Iter: 603 loss: 0.00193194277
Iter: 604 loss: 0.00193158421
Iter: 605 loss: 0.0019347897
Iter: 606 loss: 0.00193156954
Iter: 607 loss: 0.00193137256
Iter: 608 loss: 0.00193134556
Iter: 609 loss: 0.00193127221
Iter: 610 loss: 0.0019309998
Iter: 611 loss: 0.0019303048
Iter: 612 loss: 0.00194351445
Iter: 613 loss: 0.00193029502
Iter: 614 loss: 0.00192963087
Iter: 615 loss: 0.00193967484
Iter: 616 loss: 0.00192963087
Iter: 617 loss: 0.00192930363
Iter: 618 loss: 0.00193118432
Iter: 619 loss: 0.00192926254
Iter: 620 loss: 0.00192917523
Iter: 621 loss: 0.00192921702
Iter: 622 loss: 0.00192911469
Iter: 623 loss: 0.00192908454
Iter: 624 loss: 0.00192902086
Iter: 625 loss: 0.00193019013
Iter: 626 loss: 0.00192902226
Iter: 627 loss: 0.0019289942
Iter: 628 loss: 0.00192896242
Iter: 629 loss: 0.00192888675
Iter: 630 loss: 0.00192887872
Iter: 631 loss: 0.00192880817
Iter: 632 loss: 0.00192952203
Iter: 633 loss: 0.00192880887
Iter: 634 loss: 0.00192860945
Iter: 635 loss: 0.00192847359
Iter: 636 loss: 0.00192840153
Iter: 637 loss: 0.00192809012
Iter: 638 loss: 0.00192740245
Iter: 639 loss: 0.0019375172
Iter: 640 loss: 0.00192737195
Iter: 641 loss: 0.00192692783
Iter: 642 loss: 0.00192688243
Iter: 643 loss: 0.00192655833
Iter: 644 loss: 0.00192643143
Iter: 645 loss: 0.00192607916
Iter: 646 loss: 0.00192787894
Iter: 647 loss: 0.00192596
Iter: 648 loss: 0.00192631152
Iter: 649 loss: 0.00192568055
Iter: 650 loss: 0.00192564726
Iter: 651 loss: 0.00192549499
Iter: 652 loss: 0.00192536553
Iter: 653 loss: 0.00192536821
Iter: 654 loss: 0.00192530884
Iter: 655 loss: 0.00192517915
Iter: 656 loss: 0.00192676752
Iter: 657 loss: 0.0019251639
Iter: 658 loss: 0.00192508567
Iter: 659 loss: 0.00192550535
Iter: 660 loss: 0.0019250745
Iter: 661 loss: 0.00192499417
Iter: 662 loss: 0.00192518916
Iter: 663 loss: 0.00192496693
Iter: 664 loss: 0.00192484353
Iter: 665 loss: 0.00192594167
Iter: 666 loss: 0.0019248398
Iter: 667 loss: 0.00192462583
Iter: 668 loss: 0.00192449742
Iter: 669 loss: 0.0019244086
Iter: 670 loss: 0.00192417728
Iter: 671 loss: 0.00192656077
Iter: 672 loss: 0.00192417344
Iter: 673 loss: 0.00192410429
Iter: 674 loss: 0.00192390126
Iter: 675 loss: 0.00192470674
Iter: 676 loss: 0.00192381791
Iter: 677 loss: 0.0019235902
Iter: 678 loss: 0.00192373316
Iter: 679 loss: 0.00192344445
Iter: 680 loss: 0.00192330778
Iter: 681 loss: 0.00192339823
Iter: 682 loss: 0.00192322396
Iter: 683 loss: 0.00192312489
Iter: 684 loss: 0.00192312524
Iter: 685 loss: 0.00192304235
Iter: 686 loss: 0.00192291266
Iter: 687 loss: 0.0019229122
Iter: 688 loss: 0.00192252337
Iter: 689 loss: 0.00192223792
Iter: 690 loss: 0.00192211452
Iter: 691 loss: 0.00192200951
Iter: 692 loss: 0.00192184804
Iter: 693 loss: 0.00192184583
Iter: 694 loss: 0.00192168436
Iter: 695 loss: 0.00192214944
Iter: 696 loss: 0.00192163
Iter: 697 loss: 0.00192157645
Iter: 698 loss: 0.00192157459
Iter: 699 loss: 0.00192152872
Iter: 700 loss: 0.00192137517
Iter: 701 loss: 0.00192308193
Iter: 702 loss: 0.00192137458
Iter: 703 loss: 0.00192128029
Iter: 704 loss: 0.00192121416
Iter: 705 loss: 0.00192118017
Iter: 706 loss: 0.00192104571
Iter: 707 loss: 0.00192094734
Iter: 708 loss: 0.00192090147
Iter: 709 loss: 0.00192069856
Iter: 710 loss: 0.00192056503
Iter: 711 loss: 0.00192048587
Iter: 712 loss: 0.00191998447
Iter: 713 loss: 0.00191985583
Iter: 714 loss: 0.0019193599
Iter: 715 loss: 0.00191883976
Iter: 716 loss: 0.00191875477
Iter: 717 loss: 0.00191830308
Iter: 718 loss: 0.00192316587
Iter: 719 loss: 0.00191828771
Iter: 720 loss: 0.00191821682
Iter: 721 loss: 0.00191837
Iter: 722 loss: 0.00191818888
Iter: 723 loss: 0.00191809679
Iter: 724 loss: 0.00191825745
Iter: 725 loss: 0.00191805908
Iter: 726 loss: 0.00191788469
Iter: 727 loss: 0.001917382
Iter: 728 loss: 0.00191924046
Iter: 729 loss: 0.00191716338
Iter: 730 loss: 0.00191651424
Iter: 731 loss: 0.00191626535
Iter: 732 loss: 0.00191618805
Iter: 733 loss: 0.00191607978
Iter: 734 loss: 0.00191604905
Iter: 735 loss: 0.00191604416
Iter: 736 loss: 0.00191600993
Iter: 737 loss: 0.00191623298
Iter: 738 loss: 0.00191600551
Iter: 739 loss: 0.00191597792
Iter: 740 loss: 0.00191589165
Iter: 741 loss: 0.00191631238
Iter: 742 loss: 0.00191586162
Iter: 743 loss: 0.00191576965
Iter: 744 loss: 0.00191617059
Iter: 745 loss: 0.00191575161
Iter: 746 loss: 0.00191557826
Iter: 747 loss: 0.00191547466
Iter: 748 loss: 0.0019154062
Iter: 749 loss: 0.00191498967
Iter: 750 loss: 0.00191719108
Iter: 751 loss: 0.00191492494
Iter: 752 loss: 0.00191483647
Iter: 753 loss: 0.001914597
Iter: 754 loss: 0.00191598572
Iter: 755 loss: 0.00191452703
Iter: 756 loss: 0.00191423774
Iter: 757 loss: 0.00191433076
Iter: 758 loss: 0.00191403367
Iter: 759 loss: 0.00191395544
Iter: 760 loss: 0.00191408757
Iter: 761 loss: 0.00191392412
Iter: 762 loss: 0.0019138779
Iter: 763 loss: 0.00191379478
Iter: 764 loss: 0.00191558502
Iter: 765 loss: 0.00191379362
Iter: 766 loss: 0.00191373902
Iter: 767 loss: 0.00191357557
Iter: 768 loss: 0.00191431621
Iter: 769 loss: 0.00191351492
Iter: 770 loss: 0.00191353378
Iter: 771 loss: 0.00191337825
Iter: 772 loss: 0.00191324879
Iter: 773 loss: 0.00191375602
Iter: 774 loss: 0.00191321922
Iter: 775 loss: 0.00191314262
Iter: 776 loss: 0.00191345089
Iter: 777 loss: 0.00191312563
Iter: 778 loss: 0.00191306416
Iter: 779 loss: 0.00191327615
Iter: 780 loss: 0.00191305159
Iter: 781 loss: 0.00191295228
Iter: 782 loss: 0.00191282341
Iter: 783 loss: 0.00191281305
Iter: 784 loss: 0.00191383203
Iter: 785 loss: 0.00191275519
Iter: 786 loss: 0.00191273424
Iter: 787 loss: 0.00191269233
Iter: 788 loss: 0.00191269349
Iter: 789 loss: 0.00191260246
Iter: 790 loss: 0.00191235449
Iter: 791 loss: 0.00191393879
Iter: 792 loss: 0.00191228476
Iter: 793 loss: 0.00191319548
Iter: 794 loss: 0.00191206
Iter: 795 loss: 0.00191185786
Iter: 796 loss: 0.00191282923
Iter: 797 loss: 0.00191182317
Iter: 798 loss: 0.00191175938
Iter: 799 loss: 0.00191156939
Iter: 800 loss: 0.00191186892
Iter: 801 loss: 0.00191143551
Iter: 802 loss: 0.00191183225
Iter: 803 loss: 0.00191134668
Iter: 804 loss: 0.00191119383
Iter: 805 loss: 0.00191287627
Iter: 806 loss: 0.00191119092
Iter: 807 loss: 0.00191116333
Iter: 808 loss: 0.00191119546
Iter: 809 loss: 0.00191114785
Iter: 810 loss: 0.00191109697
Iter: 811 loss: 0.00191100023
Iter: 812 loss: 0.0019127822
Iter: 813 loss: 0.0019109986
Iter: 814 loss: 0.00191078463
Iter: 815 loss: 0.00191050256
Iter: 816 loss: 0.00191048626
Iter: 817 loss: 0.00191028393
Iter: 818 loss: 0.00191111374
Iter: 819 loss: 0.00191024353
Iter: 820 loss: 0.00190994551
Iter: 821 loss: 0.00191194122
Iter: 822 loss: 0.00190991652
Iter: 823 loss: 0.00190970802
Iter: 824 loss: 0.00191007089
Iter: 825 loss: 0.00190961559
Iter: 826 loss: 0.00191015122
Iter: 827 loss: 0.00190959009
Iter: 828 loss: 0.00190956902
Iter: 829 loss: 0.00190963922
Iter: 830 loss: 0.001909566
Iter: 831 loss: 0.00190951116
Iter: 832 loss: 0.0019096192
Iter: 833 loss: 0.00190948648
Iter: 834 loss: 0.00190941873
Iter: 835 loss: 0.00190968346
Iter: 836 loss: 0.0019094008
Iter: 837 loss: 0.00190929801
Iter: 838 loss: 0.00190900988
Iter: 839 loss: 0.00191088
Iter: 840 loss: 0.00190893689
Iter: 841 loss: 0.00190881197
Iter: 842 loss: 0.00190942409
Iter: 843 loss: 0.00190878776
Iter: 844 loss: 0.00190857926
Iter: 845 loss: 0.00190959813
Iter: 846 loss: 0.00190854573
Iter: 847 loss: 0.00190848717
Iter: 848 loss: 0.00190844538
Iter: 849 loss: 0.00190842687
Iter: 850 loss: 0.00190829369
Iter: 851 loss: 0.00190807681
Iter: 852 loss: 0.00190807739
Iter: 853 loss: 0.00190807844
Iter: 854 loss: 0.00190764596
Iter: 855 loss: 0.00190729776
Iter: 856 loss: 0.00190824852
Iter: 857 loss: 0.00190718891
Iter: 858 loss: 0.00190707669
Iter: 859 loss: 0.00190721126
Iter: 860 loss: 0.00190701766
Iter: 861 loss: 0.00190695224
Iter: 862 loss: 0.00190674991
Iter: 863 loss: 0.00190733769
Iter: 864 loss: 0.00190664071
Iter: 865 loss: 0.00190669578
Iter: 866 loss: 0.00190655631
Iter: 867 loss: 0.00190638017
Iter: 868 loss: 0.00190655526
Iter: 869 loss: 0.00190628041
Iter: 870 loss: 0.00190621242
Iter: 871 loss: 0.00190639636
Iter: 872 loss: 0.00190619566
Iter: 873 loss: 0.00190611556
Iter: 874 loss: 0.00190605526
Iter: 875 loss: 0.00190602848
Iter: 876 loss: 0.00190596469
Iter: 877 loss: 0.00190575351
Iter: 878 loss: 0.00190553372
Iter: 879 loss: 0.00190545211
Iter: 880 loss: 0.00190533139
Iter: 881 loss: 0.00190531067
Iter: 882 loss: 0.00190499343
Iter: 883 loss: 0.00190628448
Iter: 884 loss: 0.00190492731
Iter: 885 loss: 0.00190472766
Iter: 886 loss: 0.00190470868
Iter: 887 loss: 0.00190456701
Iter: 888 loss: 0.00190421008
Iter: 889 loss: 0.00190446281
Iter: 890 loss: 0.0019039925
Iter: 891 loss: 0.0019036656
Iter: 892 loss: 0.00190365023
Iter: 893 loss: 0.00190360937
Iter: 894 loss: 0.0019035961
Iter: 895 loss: 0.00190357305
Iter: 896 loss: 0.00190357969
Iter: 897 loss: 0.00190355664
Iter: 898 loss: 0.0019035039
Iter: 899 loss: 0.00190360192
Iter: 900 loss: 0.00190347701
Iter: 901 loss: 0.00190341845
Iter: 902 loss: 0.00190332043
Iter: 903 loss: 0.00190331927
Iter: 904 loss: 0.00190316688
Iter: 905 loss: 0.00190279167
Iter: 906 loss: 0.00190653163
Iter: 907 loss: 0.0019027479
Iter: 908 loss: 0.00190243474
Iter: 909 loss: 0.00190713489
Iter: 910 loss: 0.00190243509
Iter: 911 loss: 0.00190217607
Iter: 912 loss: 0.00190294744
Iter: 913 loss: 0.00190210133
Iter: 914 loss: 0.00190192705
Iter: 915 loss: 0.00190177979
Iter: 916 loss: 0.00190173299
Iter: 917 loss: 0.00190175453
Iter: 918 loss: 0.00190158549
Iter: 919 loss: 0.00190153299
Iter: 920 loss: 0.00190150132
Iter: 921 loss: 0.0019014834
Iter: 922 loss: 0.00190135813
Iter: 923 loss: 0.00190254743
Iter: 924 loss: 0.00190135161
Iter: 925 loss: 0.00190128188
Iter: 926 loss: 0.00190126442
Iter: 927 loss: 0.00190124544
Iter: 928 loss: 0.00190123834
Iter: 929 loss: 0.00190121017
Iter: 930 loss: 0.00190114102
Iter: 931 loss: 0.00190153066
Iter: 932 loss: 0.00190112286
Iter: 933 loss: 0.00190103636
Iter: 934 loss: 0.00190139771
Iter: 935 loss: 0.00190101762
Iter: 936 loss: 0.00190099049
Iter: 937 loss: 0.00190087978
Iter: 938 loss: 0.00190060982
Iter: 939 loss: 0.00190594024
Iter: 940 loss: 0.00190060865
Iter: 941 loss: 0.00190023752
Iter: 942 loss: 0.00190318108
Iter: 943 loss: 0.00190021144
Iter: 944 loss: 0.00190008385
Iter: 945 loss: 0.00190008292
Iter: 946 loss: 0.0018999764
Iter: 947 loss: 0.00189977244
Iter: 948 loss: 0.00190367
Iter: 949 loss: 0.00189977034
Iter: 950 loss: 0.00189949537
Iter: 951 loss: 0.00189926242
Iter: 952 loss: 0.00189918559
Iter: 953 loss: 0.0018990793
Iter: 954 loss: 0.00189919095
Iter: 955 loss: 0.0018990197
Iter: 956 loss: 0.00189897604
Iter: 957 loss: 0.00189892517
Iter: 958 loss: 0.0018989197
Iter: 959 loss: 0.00189888221
Iter: 960 loss: 0.00189896405
Iter: 961 loss: 0.0018988736
Iter: 962 loss: 0.00189895101
Iter: 963 loss: 0.00189885695
Iter: 964 loss: 0.00189884449
Iter: 965 loss: 0.00189883588
Iter: 966 loss: 0.00189883099
Iter: 967 loss: 0.0018987034
Iter: 968 loss: 0.0018986084
Iter: 969 loss: 0.00189856405
Iter: 970 loss: 0.0018983169
Iter: 971 loss: 0.00189876521
Iter: 972 loss: 0.00189820642
Iter: 973 loss: 0.00189800235
Iter: 974 loss: 0.00189865695
Iter: 975 loss: 0.00189794146
Iter: 976 loss: 0.00189774169
Iter: 977 loss: 0.00189767405
Iter: 978 loss: 0.00189755519
Iter: 979 loss: 0.00189760327
Iter: 980 loss: 0.00189746323
Iter: 981 loss: 0.0018974198
Iter: 982 loss: 0.00189738744
Iter: 983 loss: 0.00189737126
Iter: 984 loss: 0.00189730246
Iter: 985 loss: 0.0018972971
Iter: 986 loss: 0.00189724565
Iter: 987 loss: 0.00189736742
Iter: 988 loss: 0.00189722516
Iter: 989 loss: 0.00189715775
Iter: 990 loss: 0.00189693901
Iter: 991 loss: 0.00189710676
Iter: 992 loss: 0.00189674797
Iter: 993 loss: 0.00189706264
Iter: 994 loss: 0.00189663086
Iter: 995 loss: 0.0018965326
Iter: 996 loss: 0.00189646101
Iter: 997 loss: 0.00189642515
Iter: 998 loss: 0.0018962553
Iter: 999 loss: 0.00189624296
Iter: 1000 loss: 0.00189619721
Iter: 1001 loss: 0.00189612783
Iter: 1002 loss: 0.00189612794
Iter: 1003 loss: 0.00189597346
Iter: 1004 loss: 0.00189563725
Iter: 1005 loss: 0.00190076442
Iter: 1006 loss: 0.00189562305
Iter: 1007 loss: 0.0018954355
Iter: 1008 loss: 0.00189788162
Iter: 1009 loss: 0.00189543341
Iter: 1010 loss: 0.00189531851
Iter: 1011 loss: 0.00189574459
Iter: 1012 loss: 0.00189528964
Iter: 1013 loss: 0.0018954135
Iter: 1014 loss: 0.00189517066
Iter: 1015 loss: 0.0018950433
Iter: 1016 loss: 0.00189582945
Iter: 1017 loss: 0.00189502712
Iter: 1018 loss: 0.00189500512
Iter: 1019 loss: 0.00189496856
Iter: 1020 loss: 0.00189496833
Iter: 1021 loss: 0.00189491967
Iter: 1022 loss: 0.00189524074
Iter: 1023 loss: 0.00189491617
Iter: 1024 loss: 0.00189489964
Iter: 1025 loss: 0.00189490127
Iter: 1026 loss: 0.00189489126
Iter: 1027 loss: 0.00189486006
Iter: 1028 loss: 0.0018950121
Iter: 1029 loss: 0.00189484865
Iter: 1030 loss: 0.00189466646
Iter: 1031 loss: 0.0018950135
Iter: 1032 loss: 0.00189459289
Iter: 1033 loss: 0.00189422513
Iter: 1034 loss: 0.00189594761
Iter: 1035 loss: 0.00189415552
Iter: 1036 loss: 0.00189411314
Iter: 1037 loss: 0.00189402746
Iter: 1038 loss: 0.00189559651
Iter: 1039 loss: 0.00189402618
Iter: 1040 loss: 0.00189395121
Iter: 1041 loss: 0.00189395039
Iter: 1042 loss: 0.00189389172
Iter: 1043 loss: 0.00189386227
Iter: 1044 loss: 0.00189386087
Iter: 1045 loss: 0.00189384026
Iter: 1046 loss: 0.00189382804
Iter: 1047 loss: 0.00189380045
Iter: 1048 loss: 0.00189436658
Iter: 1049 loss: 0.00189380185
Iter: 1050 loss: 0.00189376646
Iter: 1051 loss: 0.00189370406
Iter: 1052 loss: 0.00189517462
Iter: 1053 loss: 0.00189370243
Iter: 1054 loss: 0.0018937327
Iter: 1055 loss: 0.00189367007
Iter: 1056 loss: 0.00189361686
Iter: 1057 loss: 0.00189379067
Iter: 1058 loss: 0.00189360604
Iter: 1059 loss: 0.00189357903
Iter: 1060 loss: 0.00189357949
Iter: 1061 loss: 0.00189355249
Iter: 1062 loss: 0.00189353095
Iter: 1063 loss: 0.00189352164
Iter: 1064 loss: 0.00189349987
Iter: 1065 loss: 0.00189345377
Iter: 1066 loss: 0.00189427752
Iter: 1067 loss: 0.00189345307
Iter: 1068 loss: 0.00189334119
Iter: 1069 loss: 0.00189325167
Iter: 1070 loss: 0.00189321674
Iter: 1071 loss: 0.00189313758
Iter: 1072 loss: 0.00189312745
Iter: 1073 loss: 0.00189306086
Iter: 1074 loss: 0.00189304445
Iter: 1075 loss: 0.0018930044
Iter: 1076 loss: 0.00189289916
Iter: 1077 loss: 0.00189392117
Iter: 1078 loss: 0.00189289707
Iter: 1079 loss: 0.00189287914
Iter: 1080 loss: 0.0018928647
Iter: 1081 loss: 0.00189286214
Iter: 1082 loss: 0.00189279113
Iter: 1083 loss: 0.00189257239
Iter: 1084 loss: 0.00189302664
Iter: 1085 loss: 0.00189243723
Iter: 1086 loss: 0.00189207366
Iter: 1087 loss: 0.00189207273
Iter: 1088 loss: 0.00189200474
Iter: 1089 loss: 0.00189200067
Iter: 1090 loss: 0.00189198076
Iter: 1091 loss: 0.00189196644
Iter: 1092 loss: 0.00189194269
Iter: 1093 loss: 0.00189189147
Iter: 1094 loss: 0.00189260952
Iter: 1095 loss: 0.00189188879
Iter: 1096 loss: 0.0018918456
Iter: 1097 loss: 0.00189180125
Iter: 1098 loss: 0.0018917924
Iter: 1099 loss: 0.00189171883
Iter: 1100 loss: 0.00189152465
Iter: 1101 loss: 0.00189293711
Iter: 1102 loss: 0.0018914846
Iter: 1103 loss: 0.00189132802
Iter: 1104 loss: 0.00189114804
Iter: 1105 loss: 0.00189112918
Iter: 1106 loss: 0.00189099601
Iter: 1107 loss: 0.00189102627
Iter: 1108 loss: 0.001890896
Iter: 1109 loss: 0.00189087342
Iter: 1110 loss: 0.00189080404
Iter: 1111 loss: 0.00189150218
Iter: 1112 loss: 0.00189079531
Iter: 1113 loss: 0.00189061137
Iter: 1114 loss: 0.0018903428
Iter: 1115 loss: 0.00189033663
Iter: 1116 loss: 0.0018901038
Iter: 1117 loss: 0.00189066236
Iter: 1118 loss: 0.00189002312
Iter: 1119 loss: 0.00188988564
Iter: 1120 loss: 0.0018901421
Iter: 1121 loss: 0.00188982766
Iter: 1122 loss: 0.00188974896
Iter: 1123 loss: 0.00188957143
Iter: 1124 loss: 0.00189201511
Iter: 1125 loss: 0.00188955944
Iter: 1126 loss: 0.00188946864
Iter: 1127 loss: 0.00188958063
Iter: 1128 loss: 0.00188942
Iter: 1129 loss: 0.00188939529
Iter: 1130 loss: 0.00188939518
Iter: 1131 loss: 0.00188936247
Iter: 1132 loss: 0.00188929273
Iter: 1133 loss: 0.00189054408
Iter: 1134 loss: 0.00188929401
Iter: 1135 loss: 0.00188931567
Iter: 1136 loss: 0.00188923825
Iter: 1137 loss: 0.0018891748
Iter: 1138 loss: 0.00188928307
Iter: 1139 loss: 0.00188914745
Iter: 1140 loss: 0.00188908167
Iter: 1141 loss: 0.00188935653
Iter: 1142 loss: 0.00188906584
Iter: 1143 loss: 0.00188900274
Iter: 1144 loss: 0.00188897748
Iter: 1145 loss: 0.00188894325
Iter: 1146 loss: 0.00188888295
Iter: 1147 loss: 0.00188872009
Iter: 1148 loss: 0.00188975804
Iter: 1149 loss: 0.0018886769
Iter: 1150 loss: 0.00188851857
Iter: 1151 loss: 0.00188851182
Iter: 1152 loss: 0.0018884025
Iter: 1153 loss: 0.00188838854
Iter: 1154 loss: 0.00188829436
Iter: 1155 loss: 0.00188951206
Iter: 1156 loss: 0.0018882947
Iter: 1157 loss: 0.00188823929
Iter: 1158 loss: 0.00188828155
Iter: 1159 loss: 0.001888206
Iter: 1160 loss: 0.00188816804
Iter: 1161 loss: 0.00188806769
Iter: 1162 loss: 0.00188864442
Iter: 1163 loss: 0.00188803766
Iter: 1164 loss: 0.00188782869
Iter: 1165 loss: 0.00188745908
Iter: 1166 loss: 0.00188745861
Iter: 1167 loss: 0.00188736687
Iter: 1168 loss: 0.00188748399
Iter: 1169 loss: 0.00188731507
Iter: 1170 loss: 0.00188720087
Iter: 1171 loss: 0.00188750122
Iter: 1172 loss: 0.00188716059
Iter: 1173 loss: 0.00188712415
Iter: 1174 loss: 0.00188709178
Iter: 1175 loss: 0.00188708468
Iter: 1176 loss: 0.00188727374
Iter: 1177 loss: 0.00188705546
Iter: 1178 loss: 0.00188703125
Iter: 1179 loss: 0.00188709586
Iter: 1180 loss: 0.00188702287
Iter: 1181 loss: 0.00188700121
Iter: 1182 loss: 0.00188691937
Iter: 1183 loss: 0.00188685395
Iter: 1184 loss: 0.00188681285
Iter: 1185 loss: 0.00188663381
Iter: 1186 loss: 0.0018869522
Iter: 1187 loss: 0.00188655499
Iter: 1188 loss: 0.00188649434
Iter: 1189 loss: 0.0018864763
Iter: 1190 loss: 0.00188642577
Iter: 1191 loss: 0.00188671402
Iter: 1192 loss: 0.00188641914
Iter: 1193 loss: 0.00188639725
Iter: 1194 loss: 0.00188633415
Iter: 1195 loss: 0.00188627688
Iter: 1196 loss: 0.00188624673
Iter: 1197 loss: 0.00188615429
Iter: 1198 loss: 0.00188606139
Iter: 1199 loss: 0.00188604183
Iter: 1200 loss: 0.00188590935
Iter: 1201 loss: 0.00188568525
Iter: 1202 loss: 0.00188568537
Iter: 1203 loss: 0.00188560726
Iter: 1204 loss: 0.00188553403
Iter: 1205 loss: 0.00188551482
Iter: 1206 loss: 0.00188540155
Iter: 1207 loss: 0.00188641343
Iter: 1208 loss: 0.0018853941
Iter: 1209 loss: 0.00188536057
Iter: 1210 loss: 0.00188535475
Iter: 1211 loss: 0.0018853338
Iter: 1212 loss: 0.00188532157
Iter: 1213 loss: 0.00188530982
Iter: 1214 loss: 0.0018852544
Iter: 1215 loss: 0.00188513671
Iter: 1216 loss: 0.00188694499
Iter: 1217 loss: 0.00188512821
Iter: 1218 loss: 0.00188502669
Iter: 1219 loss: 0.00188524
Iter: 1220 loss: 0.00188498641
Iter: 1221 loss: 0.00188507349
Iter: 1222 loss: 0.00188493822
Iter: 1223 loss: 0.00188489177
Iter: 1224 loss: 0.00188504579
Iter: 1225 loss: 0.00188487885
Iter: 1226 loss: 0.00188483763
Iter: 1227 loss: 0.00188487
Iter: 1228 loss: 0.00188481284
Iter: 1229 loss: 0.00188475405
Iter: 1230 loss: 0.00188501494
Iter: 1231 loss: 0.00188474287
Iter: 1232 loss: 0.00188467675
Iter: 1233 loss: 0.00188469572
Iter: 1234 loss: 0.00188463368
Iter: 1235 loss: 0.00188458269
Iter: 1236 loss: 0.00188463612
Iter: 1237 loss: 0.00188455521
Iter: 1238 loss: 0.00188446394
Iter: 1239 loss: 0.00188427442
Iter: 1240 loss: 0.00188757339
Iter: 1241 loss: 0.00188427162
Iter: 1242 loss: 0.00188416406
Iter: 1243 loss: 0.00188412878
Iter: 1244 loss: 0.00188407989
Iter: 1245 loss: 0.00188403344
Iter: 1246 loss: 0.00188402226
Iter: 1247 loss: 0.00188410352
Iter: 1248 loss: 0.00188393891
Iter: 1249 loss: 0.0018838828
Iter: 1250 loss: 0.00188414229
Iter: 1251 loss: 0.00188387767
Iter: 1252 loss: 0.00188386044
Iter: 1253 loss: 0.00188380794
Iter: 1254 loss: 0.0018839261
Iter: 1255 loss: 0.00188377837
Iter: 1256 loss: 0.0018836481
Iter: 1257 loss: 0.00188364438
Iter: 1258 loss: 0.001883385
Iter: 1259 loss: 0.00188546663
Iter: 1260 loss: 0.0018833715
Iter: 1261 loss: 0.00188319676
Iter: 1262 loss: 0.00188426161
Iter: 1263 loss: 0.00188317569
Iter: 1264 loss: 0.00188305497
Iter: 1265 loss: 0.0018834736
Iter: 1266 loss: 0.00188302284
Iter: 1267 loss: 0.00188294589
Iter: 1268 loss: 0.00188276975
Iter: 1269 loss: 0.00188506418
Iter: 1270 loss: 0.00188275904
Iter: 1271 loss: 0.00188253925
Iter: 1272 loss: 0.00188246602
Iter: 1273 loss: 0.00188229908
Iter: 1274 loss: 0.00188410748
Iter: 1275 loss: 0.00188229722
Iter: 1276 loss: 0.00188223796
Iter: 1277 loss: 0.00188211841
Iter: 1278 loss: 0.0018843665
Iter: 1279 loss: 0.00188211654
Iter: 1280 loss: 0.0018819758
Iter: 1281 loss: 0.00188224169
Iter: 1282 loss: 0.0018819148
Iter: 1283 loss: 0.00188188627
Iter: 1284 loss: 0.00188194658
Iter: 1285 loss: 0.00188187312
Iter: 1286 loss: 0.00188182713
Iter: 1287 loss: 0.00188178557
Iter: 1288 loss: 0.00188171957
Iter: 1289 loss: 0.00188172143
Iter: 1290 loss: 0.00188166671
Iter: 1291 loss: 0.00188149838
Iter: 1292 loss: 0.00188159919
Iter: 1293 loss: 0.00188134238
Iter: 1294 loss: 0.00188090978
Iter: 1295 loss: 0.00188180176
Iter: 1296 loss: 0.00188073597
Iter: 1297 loss: 0.00188047951
Iter: 1298 loss: 0.00188227324
Iter: 1299 loss: 0.00188045716
Iter: 1300 loss: 0.00188032188
Iter: 1301 loss: 0.0018805177
Iter: 1302 loss: 0.0018802546
Iter: 1303 loss: 0.00188020791
Iter: 1304 loss: 0.00188017031
Iter: 1305 loss: 0.00188015238
Iter: 1306 loss: 0.00188020477
Iter: 1307 loss: 0.00188008579
Iter: 1308 loss: 0.00188003865
Iter: 1309 loss: 0.00188007986
Iter: 1310 loss: 0.0018800057
Iter: 1311 loss: 0.00187993713
Iter: 1312 loss: 0.0018798915
Iter: 1313 loss: 0.00187986391
Iter: 1314 loss: 0.00187969347
Iter: 1315 loss: 0.00187973119
Iter: 1316 loss: 0.00187956705
Iter: 1317 loss: 0.00187934423
Iter: 1318 loss: 0.0018814113
Iter: 1319 loss: 0.00187933375
Iter: 1320 loss: 0.00187920057
Iter: 1321 loss: 0.00187919964
Iter: 1322 loss: 0.00187908905
Iter: 1323 loss: 0.00188077113
Iter: 1324 loss: 0.00187908928
Iter: 1325 loss: 0.00187898544
Iter: 1326 loss: 0.001879049
Iter: 1327 loss: 0.00187892187
Iter: 1328 loss: 0.00187880686
Iter: 1329 loss: 0.00187877915
Iter: 1330 loss: 0.00187870779
Iter: 1331 loss: 0.00187856203
Iter: 1332 loss: 0.00188009744
Iter: 1333 loss: 0.00187855877
Iter: 1334 loss: 0.00187849661
Iter: 1335 loss: 0.00187867519
Iter: 1336 loss: 0.00187847926
Iter: 1337 loss: 0.00187845167
Iter: 1338 loss: 0.00187843456
Iter: 1339 loss: 0.00187842292
Iter: 1340 loss: 0.0018783561
Iter: 1341 loss: 0.00187828066
Iter: 1342 loss: 0.00187827274
Iter: 1343 loss: 0.00187818496
Iter: 1344 loss: 0.00187890104
Iter: 1345 loss: 0.00187817973
Iter: 1346 loss: 0.00187812396
Iter: 1347 loss: 0.00187801546
Iter: 1348 loss: 0.00188028323
Iter: 1349 loss: 0.0018780157
Iter: 1350 loss: 0.00187788322
Iter: 1351 loss: 0.00187756238
Iter: 1352 loss: 0.00188139721
Iter: 1353 loss: 0.00187753607
Iter: 1354 loss: 0.00187739905
Iter: 1355 loss: 0.00187811512
Iter: 1356 loss: 0.00187737891
Iter: 1357 loss: 0.001877337
Iter: 1358 loss: 0.00187731604
Iter: 1359 loss: 0.00187725562
Iter: 1360 loss: 0.00187710638
Iter: 1361 loss: 0.00187862781
Iter: 1362 loss: 0.00187708857
Iter: 1363 loss: 0.00187675096
Iter: 1364 loss: 0.00187809975
Iter: 1365 loss: 0.00187667611
Iter: 1366 loss: 0.00187652919
Iter: 1367 loss: 0.00187765644
Iter: 1368 loss: 0.00187651697
Iter: 1369 loss: 0.00187645259
Iter: 1370 loss: 0.0018765314
Iter: 1371 loss: 0.0018764179
Iter: 1372 loss: 0.00187640241
Iter: 1373 loss: 0.00187640171
Iter: 1374 loss: 0.00187638332
Iter: 1375 loss: 0.00187661825
Iter: 1376 loss: 0.00187638006
Iter: 1377 loss: 0.0018763605
Iter: 1378 loss: 0.00187632791
Iter: 1379 loss: 0.00187632674
Iter: 1380 loss: 0.00187625922
Iter: 1381 loss: 0.00187640369
Iter: 1382 loss: 0.00187623315
Iter: 1383 loss: 0.00187617063
Iter: 1384 loss: 0.00187626225
Iter: 1385 loss: 0.00187614153
Iter: 1386 loss: 0.00187600846
Iter: 1387 loss: 0.00187566807
Iter: 1388 loss: 0.00187886809
Iter: 1389 loss: 0.00187561801
Iter: 1390 loss: 0.00187573431
Iter: 1391 loss: 0.00187551067
Iter: 1392 loss: 0.00187539868
Iter: 1393 loss: 0.00187523104
Iter: 1394 loss: 0.00187522895
Iter: 1395 loss: 0.0018751428
Iter: 1396 loss: 0.00187509053
Iter: 1397 loss: 0.00187506666
Iter: 1398 loss: 0.00187506189
Iter: 1399 loss: 0.00187504257
Iter: 1400 loss: 0.0018750194
Iter: 1401 loss: 0.00187501789
Iter: 1402 loss: 0.00187496806
Iter: 1403 loss: 0.00187494303
Iter: 1404 loss: 0.0018749201
Iter: 1405 loss: 0.00187481777
Iter: 1406 loss: 0.00187543244
Iter: 1407 loss: 0.00187480426
Iter: 1408 loss: 0.00187468913
Iter: 1409 loss: 0.00187492371
Iter: 1410 loss: 0.00187464606
Iter: 1411 loss: 0.0018745556
Iter: 1412 loss: 0.00187455525
Iter: 1413 loss: 0.00187452172
Iter: 1414 loss: 0.00187456782
Iter: 1415 loss: 0.00187450624
Iter: 1416 loss: 0.00187445898
Iter: 1417 loss: 0.00187433278
Iter: 1418 loss: 0.00187518029
Iter: 1419 loss: 0.0018743024
Iter: 1420 loss: 0.00187416817
Iter: 1421 loss: 0.00187551812
Iter: 1422 loss: 0.00187416258
Iter: 1423 loss: 0.00187410333
Iter: 1424 loss: 0.00187410053
Iter: 1425 loss: 0.00187405944
Iter: 1426 loss: 0.00187392591
Iter: 1427 loss: 0.00187378144
Iter: 1428 loss: 0.00187373231
Iter: 1429 loss: 0.00187679823
Iter: 1430 loss: 0.00187370565
Iter: 1431 loss: 0.00187366863
Iter: 1432 loss: 0.00187379424
Iter: 1433 loss: 0.00187365781
Iter: 1434 loss: 0.00187364733
Iter: 1435 loss: 0.00187366153
Iter: 1436 loss: 0.00187364069
Iter: 1437 loss: 0.00187362148
Iter: 1438 loss: 0.00187357387
Iter: 1439 loss: 0.00187436049
Iter: 1440 loss: 0.00187357422
Iter: 1441 loss: 0.00187345513
Iter: 1442 loss: 0.001873628
Iter: 1443 loss: 0.0018733982
Iter: 1444 loss: 0.00187362276
Iter: 1445 loss: 0.0018732592
Iter: 1446 loss: 0.00187311904
Iter: 1447 loss: 0.00187334488
Iter: 1448 loss: 0.00187305338
Iter: 1449 loss: 0.00187299028
Iter: 1450 loss: 0.00187296944
Iter: 1451 loss: 0.00187288807
Iter: 1452 loss: 0.00187322102
Iter: 1453 loss: 0.00187286886
Iter: 1454 loss: 0.00187277119
Iter: 1455 loss: 0.00187258224
Iter: 1456 loss: 0.00187688274
Iter: 1457 loss: 0.00187258096
Iter: 1458 loss: 0.00187244941
Iter: 1459 loss: 0.00187310111
Iter: 1460 loss: 0.00187242602
Iter: 1461 loss: 0.00187231507
Iter: 1462 loss: 0.00187192089
Iter: 1463 loss: 0.00187138608
Iter: 1464 loss: 0.00187129271
Iter: 1465 loss: 0.00187543
Iter: 1466 loss: 0.00187126664
Iter: 1467 loss: 0.00187121518
Iter: 1468 loss: 0.00187154696
Iter: 1469 loss: 0.00187121099
Iter: 1470 loss: 0.00187115127
Iter: 1471 loss: 0.00187117653
Iter: 1472 loss: 0.00187111087
Iter: 1473 loss: 0.00187090435
Iter: 1474 loss: 0.00187027524
Iter: 1475 loss: 0.0018723004
Iter: 1476 loss: 0.00186995149
Iter: 1477 loss: 0.00186957477
Iter: 1478 loss: 0.0018743067
Iter: 1479 loss: 0.00186957128
Iter: 1480 loss: 0.00186951295
Iter: 1481 loss: 0.00186952786
Iter: 1482 loss: 0.0018694693
Iter: 1483 loss: 0.00186940399
Iter: 1484 loss: 0.00186933752
Iter: 1485 loss: 0.00186932739
Iter: 1486 loss: 0.00186922308
Iter: 1487 loss: 0.00186909828
Iter: 1488 loss: 0.00186908513
Iter: 1489 loss: 0.00187005079
Iter: 1490 loss: 0.00186889933
Iter: 1491 loss: 0.00186847325
Iter: 1492 loss: 0.00186918303
Iter: 1493 loss: 0.00186827895
Iter: 1494 loss: 0.00186804519
Iter: 1495 loss: 0.00186851795
Iter: 1496 loss: 0.00186795788
Iter: 1497 loss: 0.00186780014
Iter: 1498 loss: 0.00187035813
Iter: 1499 loss: 0.00186779955
Iter: 1500 loss: 0.00186752761
Iter: 1501 loss: 0.00186895032
Iter: 1502 loss: 0.0018674864
Iter: 1503 loss: 0.00186740269
Iter: 1504 loss: 0.001867285
Iter: 1505 loss: 0.00186728069
Iter: 1506 loss: 0.00186709058
Iter: 1507 loss: 0.00186703517
Iter: 1508 loss: 0.00186691957
Iter: 1509 loss: 0.00186682539
Iter: 1510 loss: 0.00186737056
Iter: 1511 loss: 0.00186681561
Iter: 1512 loss: 0.00186674274
Iter: 1513 loss: 0.00186670595
Iter: 1514 loss: 0.00186667335
Iter: 1515 loss: 0.00186651689
Iter: 1516 loss: 0.00186675438
Iter: 1517 loss: 0.00186643947
Iter: 1518 loss: 0.00186619628
Iter: 1519 loss: 0.00186835218
Iter: 1520 loss: 0.00186618906
Iter: 1521 loss: 0.00186610804
Iter: 1522 loss: 0.00186590781
Iter: 1523 loss: 0.00186828594
Iter: 1524 loss: 0.00186589372
Iter: 1525 loss: 0.00186580734
Iter: 1526 loss: 0.001866073
Iter: 1527 loss: 0.00186578277
Iter: 1528 loss: 0.00186570408
Iter: 1529 loss: 0.00186562166
Iter: 1530 loss: 0.00186560652
Iter: 1531 loss: 0.00186539139
Iter: 1532 loss: 0.00186516251
Iter: 1533 loss: 0.00186512584
Iter: 1534 loss: 0.00186946662
Iter: 1535 loss: 0.00186510419
Iter: 1536 loss: 0.00186508405
Iter: 1537 loss: 0.00186504843
Iter: 1538 loss: 0.00186505029
Iter: 1539 loss: 0.00186494808
Iter: 1540 loss: 0.00186521513
Iter: 1541 loss: 0.00186491222
Iter: 1542 loss: 0.00186486822
Iter: 1543 loss: 0.00186484028
Iter: 1544 loss: 0.00186478
Iter: 1545 loss: 0.00186477718
Iter: 1546 loss: 0.00186472945
Iter: 1547 loss: 0.00186469243
Iter: 1548 loss: 0.00186462793
Iter: 1549 loss: 0.00186462607
Iter: 1550 loss: 0.00186454249
Iter: 1551 loss: 0.00186430686
Iter: 1552 loss: 0.00186562585
Iter: 1553 loss: 0.00186423678
Iter: 1554 loss: 0.00186408707
Iter: 1555 loss: 0.00186608464
Iter: 1556 loss: 0.00186408695
Iter: 1557 loss: 0.00186402991
Iter: 1558 loss: 0.00186405261
Iter: 1559 loss: 0.00186399044
Iter: 1560 loss: 0.00186392991
Iter: 1561 loss: 0.00186389964
Iter: 1562 loss: 0.0018638738
Iter: 1563 loss: 0.00186383678
Iter: 1564 loss: 0.00186372525
Iter: 1565 loss: 0.00186417624
Iter: 1566 loss: 0.00186367892
Iter: 1567 loss: 0.00186361629
Iter: 1568 loss: 0.00186456635
Iter: 1569 loss: 0.00186361535
Iter: 1570 loss: 0.00186357298
Iter: 1571 loss: 0.00186354946
Iter: 1572 loss: 0.00186345982
Iter: 1573 loss: 0.00186327088
Iter: 1574 loss: 0.00186616299
Iter: 1575 loss: 0.00186326331
Iter: 1576 loss: 0.00186315179
Iter: 1577 loss: 0.00186313782
Iter: 1578 loss: 0.00186307018
Iter: 1579 loss: 0.00186300604
Iter: 1580 loss: 0.00186298683
Iter: 1581 loss: 0.00186291791
Iter: 1582 loss: 0.00186401349
Iter: 1583 loss: 0.0018629164
Iter: 1584 loss: 0.00186279719
Iter: 1585 loss: 0.00186325819
Iter: 1586 loss: 0.00186277053
Iter: 1587 loss: 0.00186267542
Iter: 1588 loss: 0.00186267623
Iter: 1589 loss: 0.00186260184
Iter: 1590 loss: 0.00186252128
Iter: 1591 loss: 0.00186301291
Iter: 1592 loss: 0.00186251104
Iter: 1593 loss: 0.00186248403
Iter: 1594 loss: 0.0018628235
Iter: 1595 loss: 0.00186248496
Iter: 1596 loss: 0.00186246308
Iter: 1597 loss: 0.0018624342
Iter: 1598 loss: 0.00186243327
Iter: 1599 loss: 0.00186241185
Iter: 1600 loss: 0.0018623611
Iter: 1601 loss: 0.00186287006
Iter: 1602 loss: 0.00186235644
Iter: 1603 loss: 0.00186225388
Iter: 1604 loss: 0.0018619739
Iter: 1605 loss: 0.00186396972
Iter: 1606 loss: 0.00186190591
Iter: 1607 loss: 0.00186179299
Iter: 1608 loss: 0.00186178694
Iter: 1609 loss: 0.0018616973
Iter: 1610 loss: 0.00186229567
Iter: 1611 loss: 0.00186168938
Iter: 1612 loss: 0.0018616264
Iter: 1613 loss: 0.00186156551
Iter: 1614 loss: 0.00186155248
Iter: 1615 loss: 0.001861508
Iter: 1616 loss: 0.00186136388
Iter: 1617 loss: 0.00186156598
Iter: 1618 loss: 0.00186125794
Iter: 1619 loss: 0.00186109752
Iter: 1620 loss: 0.00186156482
Iter: 1621 loss: 0.00186104607
Iter: 1622 loss: 0.00186086656
Iter: 1623 loss: 0.0018619867
Iter: 1624 loss: 0.00186084502
Iter: 1625 loss: 0.00186072709
Iter: 1626 loss: 0.00186068425
Iter: 1627 loss: 0.00186061836
Iter: 1628 loss: 0.00186061964
Iter: 1629 loss: 0.00186058145
Iter: 1630 loss: 0.00186056411
Iter: 1631 loss: 0.00186053955
Iter: 1632 loss: 0.0018605506
Iter: 1633 loss: 0.0018605229
Iter: 1634 loss: 0.00186048483
Iter: 1635 loss: 0.00186036038
Iter: 1636 loss: 0.00186022976
Iter: 1637 loss: 0.00186017901
Iter: 1638 loss: 0.00185996178
Iter: 1639 loss: 0.00186046527
Iter: 1640 loss: 0.00185987714
Iter: 1641 loss: 0.00185976608
Iter: 1642 loss: 0.00185994152
Iter: 1643 loss: 0.00185971544
Iter: 1644 loss: 0.00186825404
Iter: 1645 loss: 0.00185963581
Iter: 1646 loss: 0.00185948214
Iter: 1647 loss: 0.0018592804
Iter: 1648 loss: 0.00185926678
Iter: 1649 loss: 0.00185867283
Iter: 1650 loss: 0.00186027773
Iter: 1651 loss: 0.00185847469
Iter: 1652 loss: 0.00185809145
Iter: 1653 loss: 0.00185808947
Iter: 1654 loss: 0.00185778691
Iter: 1655 loss: 0.00185777771
Iter: 1656 loss: 0.00185757689
Iter: 1657 loss: 0.00185745535
Iter: 1658 loss: 0.00185744907
Iter: 1659 loss: 0.00185733126
Iter: 1660 loss: 0.00185847504
Iter: 1661 loss: 0.00185732776
Iter: 1662 loss: 0.00185737794
Iter: 1663 loss: 0.00185729994
Iter: 1664 loss: 0.0018572869
Iter: 1665 loss: 0.00185725489
Iter: 1666 loss: 0.00185741531
Iter: 1667 loss: 0.00185724441
Iter: 1668 loss: 0.00185720972
Iter: 1669 loss: 0.00185721251
Iter: 1670 loss: 0.00185718609
Iter: 1671 loss: 0.00185715349
Iter: 1672 loss: 0.00185704767
Iter: 1673 loss: 0.00185694627
Iter: 1674 loss: 0.00185689772
Iter: 1675 loss: 0.00185669318
Iter: 1676 loss: 0.00185850123
Iter: 1677 loss: 0.00185668387
Iter: 1678 loss: 0.00185659504
Iter: 1679 loss: 0.00185749354
Iter: 1680 loss: 0.00185659342
Iter: 1681 loss: 0.00185647083
Iter: 1682 loss: 0.00185641204
Iter: 1683 loss: 0.00185635302
Iter: 1684 loss: 0.00185606198
Iter: 1685 loss: 0.00185585755
Iter: 1686 loss: 0.00185574056
Iter: 1687 loss: 0.00185541098
Iter: 1688 loss: 0.00185616594
Iter: 1689 loss: 0.00185528467
Iter: 1690 loss: 0.00185507
Iter: 1691 loss: 0.00185561902
Iter: 1692 loss: 0.00185499794
Iter: 1693 loss: 0.00185501715
Iter: 1694 loss: 0.00185492798
Iter: 1695 loss: 0.00185489166
Iter: 1696 loss: 0.00185492262
Iter: 1697 loss: 0.00185487361
Iter: 1698 loss: 0.00185478455
Iter: 1699 loss: 0.00185496476
Iter: 1700 loss: 0.00185474916
Iter: 1701 loss: 0.00185459829
Iter: 1702 loss: 0.00185467652
Iter: 1703 loss: 0.00185449515
Iter: 1704 loss: 0.00185439654
Iter: 1705 loss: 0.00185563776
Iter: 1706 loss: 0.00185439538
Iter: 1707 loss: 0.0018543601
Iter: 1708 loss: 0.00185424066
Iter: 1709 loss: 0.00185421831
Iter: 1710 loss: 0.0018541062
Iter: 1711 loss: 0.00185394392
Iter: 1712 loss: 0.00185435067
Iter: 1713 loss: 0.00185388443
Iter: 1714 loss: 0.00185381435
Iter: 1715 loss: 0.00185365346
Iter: 1716 loss: 0.00185558666
Iter: 1717 loss: 0.00185364042
Iter: 1718 loss: 0.00185345369
Iter: 1719 loss: 0.00185362832
Iter: 1720 loss: 0.00185334939
Iter: 1721 loss: 0.00185329258
Iter: 1722 loss: 0.00185329083
Iter: 1723 loss: 0.00185324275
Iter: 1724 loss: 0.0018531594
Iter: 1725 loss: 0.00185315928
Iter: 1726 loss: 0.00185312051
Iter: 1727 loss: 0.00185304624
Iter: 1728 loss: 0.00185304577
Iter: 1729 loss: 0.00185288442
Iter: 1730 loss: 0.00185340736
Iter: 1731 loss: 0.00185284112
Iter: 1732 loss: 0.00185294577
Iter: 1733 loss: 0.00185273541
Iter: 1734 loss: 0.00185265031
Iter: 1735 loss: 0.00185401808
Iter: 1736 loss: 0.00185264787
Iter: 1737 loss: 0.00185261061
Iter: 1738 loss: 0.00185274833
Iter: 1739 loss: 0.00185259944
Iter: 1740 loss: 0.00185255171
Iter: 1741 loss: 0.00185240526
Iter: 1742 loss: 0.00185275241
Iter: 1743 loss: 0.00185231946
Iter: 1744 loss: 0.00185220526
Iter: 1745 loss: 0.00185393402
Iter: 1746 loss: 0.00185220502
Iter: 1747 loss: 0.00185206765
Iter: 1748 loss: 0.00185190362
Iter: 1749 loss: 0.00185188651
Iter: 1750 loss: 0.00185169175
Iter: 1751 loss: 0.00185169023
Iter: 1752 loss: 0.00185158546
Iter: 1753 loss: 0.00185260235
Iter: 1754 loss: 0.00185158022
Iter: 1755 loss: 0.00185146322
Iter: 1756 loss: 0.00185264123
Iter: 1757 loss: 0.00185146008
Iter: 1758 loss: 0.0018513184
Iter: 1759 loss: 0.00185104948
Iter: 1760 loss: 0.00185831077
Iter: 1761 loss: 0.00185104879
Iter: 1762 loss: 0.00185052084
Iter: 1763 loss: 0.00185239606
Iter: 1764 loss: 0.00185038964
Iter: 1765 loss: 0.00185012049
Iter: 1766 loss: 0.00185059884
Iter: 1767 loss: 0.00185000233
Iter: 1768 loss: 0.00184991607
Iter: 1769 loss: 0.00184991409
Iter: 1770 loss: 0.0018498681
Iter: 1771 loss: 0.00184993423
Iter: 1772 loss: 0.00184984913
Iter: 1773 loss: 0.00184974913
Iter: 1774 loss: 0.00185006973
Iter: 1775 loss: 0.00184971781
Iter: 1776 loss: 0.00184958102
Iter: 1777 loss: 0.00184934167
Iter: 1778 loss: 0.00184934144
Iter: 1779 loss: 0.00184903434
Iter: 1780 loss: 0.00184896821
Iter: 1781 loss: 0.00184876611
Iter: 1782 loss: 0.00184854213
Iter: 1783 loss: 0.00185216055
Iter: 1784 loss: 0.00184854085
Iter: 1785 loss: 0.00184841466
Iter: 1786 loss: 0.00184835074
Iter: 1787 loss: 0.00184828951
Iter: 1788 loss: 0.00184817892
Iter: 1789 loss: 0.00184797659
Iter: 1790 loss: 0.00185284764
Iter: 1791 loss: 0.00184797414
Iter: 1792 loss: 0.00184780953
Iter: 1793 loss: 0.00184930151
Iter: 1794 loss: 0.00184779707
Iter: 1795 loss: 0.00184766925
Iter: 1796 loss: 0.00184767006
Iter: 1797 loss: 0.0018476313
Iter: 1798 loss: 0.00184756017
Iter: 1799 loss: 0.00184903061
Iter: 1800 loss: 0.00184755621
Iter: 1801 loss: 0.00184751139
Iter: 1802 loss: 0.00184739474
Iter: 1803 loss: 0.00184864528
Iter: 1804 loss: 0.00184738147
Iter: 1805 loss: 0.00184718217
Iter: 1806 loss: 0.00184717146
Iter: 1807 loss: 0.00184692722
Iter: 1808 loss: 0.00184883527
Iter: 1809 loss: 0.00184690917
Iter: 1810 loss: 0.00184677052
Iter: 1811 loss: 0.00184785994
Iter: 1812 loss: 0.00184676156
Iter: 1813 loss: 0.00184669
Iter: 1814 loss: 0.00184699404
Iter: 1815 loss: 0.00184667169
Iter: 1816 loss: 0.00184653699
Iter: 1817 loss: 0.00184659846
Iter: 1818 loss: 0.00184644619
Iter: 1819 loss: 0.00184624479
Iter: 1820 loss: 0.00184574933
Iter: 1821 loss: 0.00185183901
Iter: 1822 loss: 0.00184569461
Iter: 1823 loss: 0.00184537924
Iter: 1824 loss: 0.00184782234
Iter: 1825 loss: 0.0018453541
Iter: 1826 loss: 0.0018452697
Iter: 1827 loss: 0.00184587
Iter: 1828 loss: 0.00184526492
Iter: 1829 loss: 0.00184515386
Iter: 1830 loss: 0.00184508064
Iter: 1831 loss: 0.00184504269
Iter: 1832 loss: 0.00184502464
Iter: 1833 loss: 0.00184494676
Iter: 1834 loss: 0.00184490066
Iter: 1835 loss: 0.00184486399
Iter: 1836 loss: 0.0018448506
Iter: 1837 loss: 0.00184474466
Iter: 1838 loss: 0.00184504641
Iter: 1839 loss: 0.00184470986
Iter: 1840 loss: 0.00184466341
Iter: 1841 loss: 0.0018451307
Iter: 1842 loss: 0.00184466271
Iter: 1843 loss: 0.00184462755
Iter: 1844 loss: 0.0018445095
Iter: 1845 loss: 0.00184447924
Iter: 1846 loss: 0.001844379
Iter: 1847 loss: 0.00184462045
Iter: 1848 loss: 0.0018443052
Iter: 1849 loss: 0.00184427528
Iter: 1850 loss: 0.00184431334
Iter: 1851 loss: 0.00184425595
Iter: 1852 loss: 0.00184423162
Iter: 1853 loss: 0.00184419227
Iter: 1854 loss: 0.00184419274
Iter: 1855 loss: 0.00184412696
Iter: 1856 loss: 0.00184403465
Iter: 1857 loss: 0.0018440315
Iter: 1858 loss: 0.00184391136
Iter: 1859 loss: 0.00184384512
Iter: 1860 loss: 0.00184379297
Iter: 1861 loss: 0.00184373057
Iter: 1862 loss: 0.00184376666
Iter: 1863 loss: 0.00184368948
Iter: 1864 loss: 0.00184368039
Iter: 1865 loss: 0.00184366736
Iter: 1866 loss: 0.00184364524
Iter: 1867 loss: 0.00184357318
Iter: 1868 loss: 0.00184347713
Iter: 1869 loss: 0.00184345548
Iter: 1870 loss: 0.00184327213
Iter: 1871 loss: 0.00184324849
Iter: 1872 loss: 0.00184365548
Iter: 1873 loss: 0.00184315396
Iter: 1874 loss: 0.00184308295
Iter: 1875 loss: 0.00184320961
Iter: 1876 loss: 0.00184305338
Iter: 1877 loss: 0.00184300402
Iter: 1878 loss: 0.00184294721
Iter: 1879 loss: 0.00184293813
Iter: 1880 loss: 0.00184283801
Iter: 1881 loss: 0.00184304442
Iter: 1882 loss: 0.00184279727
Iter: 1883 loss: 0.001842723
Iter: 1884 loss: 0.00184272206
Iter: 1885 loss: 0.00184267794
Iter: 1886 loss: 0.00184261845
Iter: 1887 loss: 0.0018426138
Iter: 1888 loss: 0.00184248667
Iter: 1889 loss: 0.00184251531
Iter: 1890 loss: 0.00184239354
Iter: 1891 loss: 0.00184225128
Iter: 1892 loss: 0.00184352626
Iter: 1893 loss: 0.00184224453
Iter: 1894 loss: 0.00184211601
Iter: 1895 loss: 0.00184218655
Iter: 1896 loss: 0.00184203184
Iter: 1897 loss: 0.00184291555
Iter: 1898 loss: 0.00184200727
Iter: 1899 loss: 0.00184199587
Iter: 1900 loss: 0.00184196467
Iter: 1901 loss: 0.00184210029
Iter: 1902 loss: 0.00184195233
Iter: 1903 loss: 0.00184188108
Iter: 1904 loss: 0.00184185943
Iter: 1905 loss: 0.00184181472
Iter: 1906 loss: 0.0018416841
Iter: 1907 loss: 0.00184278539
Iter: 1908 loss: 0.00184168061
Iter: 1909 loss: 0.00184150354
Iter: 1910 loss: 0.00184121926
Iter: 1911 loss: 0.00184121751
Iter: 1912 loss: 0.00184101285
Iter: 1913 loss: 0.00184116268
Iter: 1914 loss: 0.00184088899
Iter: 1915 loss: 0.00184074836
Iter: 1916 loss: 0.0018407352
Iter: 1917 loss: 0.00184063427
Iter: 1918 loss: 0.00184048363
Iter: 1919 loss: 0.00184048084
Iter: 1920 loss: 0.0018403749
Iter: 1921 loss: 0.00184090179
Iter: 1922 loss: 0.00184035499
Iter: 1923 loss: 0.00184027397
Iter: 1924 loss: 0.00184013904
Iter: 1925 loss: 0.00184013566
Iter: 1926 loss: 0.00183994556
Iter: 1927 loss: 0.00184093299
Iter: 1928 loss: 0.00183991343
Iter: 1929 loss: 0.00183986407
Iter: 1930 loss: 0.00183980563
Iter: 1931 loss: 0.00183960621
Iter: 1932 loss: 0.00183908886
Iter: 1933 loss: 0.00184482615
Iter: 1934 loss: 0.00183901424
Iter: 1935 loss: 0.00183892529
Iter: 1936 loss: 0.00183869083
Iter: 1937 loss: 0.00183861016
Iter: 1938 loss: 0.0018393246
Iter: 1939 loss: 0.00183860702
Iter: 1940 loss: 0.00183852948
Iter: 1941 loss: 0.00183849037
Iter: 1942 loss: 0.00183845684
Iter: 1943 loss: 0.00183823309
Iter: 1944 loss: 0.00183820201
Iter: 1945 loss: 0.00183803891
Iter: 1946 loss: 0.0018378594
Iter: 1947 loss: 0.00183784636
Iter: 1948 loss: 0.00183806405
Iter: 1949 loss: 0.0018376573
Iter: 1950 loss: 0.0018376
Iter: 1951 loss: 0.00183799828
Iter: 1952 loss: 0.00183759583
Iter: 1953 loss: 0.0018375311
Iter: 1954 loss: 0.00183741609
Iter: 1955 loss: 0.00184030272
Iter: 1956 loss: 0.00183741422
Iter: 1957 loss: 0.0018372772
Iter: 1958 loss: 0.0018369949
Iter: 1959 loss: 0.00184254174
Iter: 1960 loss: 0.0018369907
Iter: 1961 loss: 0.00183732156
Iter: 1962 loss: 0.00183691771
Iter: 1963 loss: 0.00183683459
Iter: 1964 loss: 0.00183684332
Iter: 1965 loss: 0.00183677045
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script74
+ '[' -r STOP.script74 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output74/f1_psi2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output75/f1_psi2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output74/f1_psi2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output75/f1_psi2_phi0.4
+ date
Sat Oct 31 16:57:25 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output74/f1_psi2_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --function f1 --psi 2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output74/f1_psi2_phi0.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 30 --learning_rate 0.001 --decay_rate 0.8 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa01e6268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feff0143620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feff011fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feff004b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feff0042d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef987f8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa013fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa01539d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa00c4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa00c49d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef986ee730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef986f4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa003a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0172730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0116ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa01162f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa011a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef98646400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef98661c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef9867c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef986a39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef9876f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef9850c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef986a3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef984ec598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef984ec840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef985998c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef985997b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef985dc510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef985dcb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef9861b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef987257b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef987251e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef9849b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef9849c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef98341d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.2753523
test_loss: 0.27371243
train_loss: 0.21165878
test_loss: 0.21126384
train_loss: 0.18622099
test_loss: 0.18731597
train_loss: 0.17920697
test_loss: 0.17840739
train_loss: 0.17583968
test_loss: 0.17504437
train_loss: 0.17314671
test_loss: 0.17376214
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output75/f1_psi2_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output74/f1_psi2_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output75/f1_psi2_phi0.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dfb664e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dfb652950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dfb6526a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dfb652d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dd288a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dd288a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dd28438c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dd279d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dd2843158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dd2780378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac6a2158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac6ea730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac6ea378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac666c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac666950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dd27d3c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dd27fb620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac5df510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac58c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac56c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac56c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac56cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac62d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac4ca730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac4ca400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac4a7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac49ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac51d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac451620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac51d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac3f7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac39e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac39e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac3bb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac442a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dac37e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.049951259
Iter: 2 loss: 0.0249651186
Iter: 3 loss: 0.024407614
Iter: 4 loss: 0.0210301392
Iter: 5 loss: 0.0198822562
Iter: 6 loss: 0.0176539756
Iter: 7 loss: 0.0157647058
Iter: 8 loss: 0.0152412113
Iter: 9 loss: 0.0135415019
Iter: 10 loss: 0.0223947018
Iter: 11 loss: 0.0133205829
Iter: 12 loss: 0.0118579222
Iter: 13 loss: 0.0177510828
Iter: 14 loss: 0.0115791243
Iter: 15 loss: 0.0110004283
Iter: 16 loss: 0.010552587
Iter: 17 loss: 0.0103984661
Iter: 18 loss: 0.00979571417
Iter: 19 loss: 0.0103157014
Iter: 20 loss: 0.00946715847
Iter: 21 loss: 0.00893313251
Iter: 22 loss: 0.0120147169
Iter: 23 loss: 0.0088790264
Iter: 24 loss: 0.00855754782
Iter: 25 loss: 0.00864666887
Iter: 26 loss: 0.00834111217
Iter: 27 loss: 0.00812241808
Iter: 28 loss: 0.0083423
Iter: 29 loss: 0.00801195
Iter: 30 loss: 0.00785236526
Iter: 31 loss: 0.0087547563
Iter: 32 loss: 0.00783383101
Iter: 33 loss: 0.00773213804
Iter: 34 loss: 0.00772303157
Iter: 35 loss: 0.00765250577
Iter: 36 loss: 0.00757026207
Iter: 37 loss: 0.00756614655
Iter: 38 loss: 0.0075059291
Iter: 39 loss: 0.00744647346
Iter: 40 loss: 0.00743175438
Iter: 41 loss: 0.00739610102
Iter: 42 loss: 0.00733860349
Iter: 43 loss: 0.00764071383
Iter: 44 loss: 0.00733071472
Iter: 45 loss: 0.0072819842
Iter: 46 loss: 0.00733056199
Iter: 47 loss: 0.00725620706
Iter: 48 loss: 0.00721393526
Iter: 49 loss: 0.00716485549
Iter: 50 loss: 0.00716012344
Iter: 51 loss: 0.00708750263
Iter: 52 loss: 0.00713006407
Iter: 53 loss: 0.00704274187
Iter: 54 loss: 0.00694904663
Iter: 55 loss: 0.00705892639
Iter: 56 loss: 0.0069051832
Iter: 57 loss: 0.00682002353
Iter: 58 loss: 0.00679389946
Iter: 59 loss: 0.00674929749
Iter: 60 loss: 0.00664025405
Iter: 61 loss: 0.00666232267
Iter: 62 loss: 0.00655518286
Iter: 63 loss: 0.00638125278
Iter: 64 loss: 0.00731024519
Iter: 65 loss: 0.00636238093
Iter: 66 loss: 0.0061734207
Iter: 67 loss: 0.0066061
Iter: 68 loss: 0.00613349304
Iter: 69 loss: 0.00600875495
Iter: 70 loss: 0.00628282037
Iter: 71 loss: 0.00596062094
Iter: 72 loss: 0.00588548835
Iter: 73 loss: 0.00579647534
Iter: 74 loss: 0.00578957517
Iter: 75 loss: 0.00566527713
Iter: 76 loss: 0.0056373151
Iter: 77 loss: 0.00560696935
Iter: 78 loss: 0.0055785235
Iter: 79 loss: 0.00551212952
Iter: 80 loss: 0.00553105865
Iter: 81 loss: 0.00546697155
Iter: 82 loss: 0.00541627221
Iter: 83 loss: 0.00542317517
Iter: 84 loss: 0.00537759764
Iter: 85 loss: 0.00532897795
Iter: 86 loss: 0.00560433045
Iter: 87 loss: 0.00532289408
Iter: 88 loss: 0.00527225947
Iter: 89 loss: 0.00531165535
Iter: 90 loss: 0.00524363853
Iter: 91 loss: 0.00519337
Iter: 92 loss: 0.00517218746
Iter: 93 loss: 0.00514354091
Iter: 94 loss: 0.00509182783
Iter: 95 loss: 0.00509154284
Iter: 96 loss: 0.00501136854
Iter: 97 loss: 0.00513737183
Iter: 98 loss: 0.00496530347
Iter: 99 loss: 0.00486991787
Iter: 100 loss: 0.00613677921
Iter: 101 loss: 0.00486625964
Iter: 102 loss: 0.00475817081
Iter: 103 loss: 0.00586014148
Iter: 104 loss: 0.00475429
Iter: 105 loss: 0.00468860939
Iter: 106 loss: 0.00468453858
Iter: 107 loss: 0.00460828189
Iter: 108 loss: 0.00489962427
Iter: 109 loss: 0.00458966848
Iter: 110 loss: 0.00454876386
Iter: 111 loss: 0.00454206718
Iter: 112 loss: 0.00451395381
Iter: 113 loss: 0.00450311787
Iter: 114 loss: 0.00448792148
Iter: 115 loss: 0.00442114193
Iter: 116 loss: 0.00638122205
Iter: 117 loss: 0.00442002
Iter: 118 loss: 0.0043378016
Iter: 119 loss: 0.00461298926
Iter: 120 loss: 0.00431091059
Iter: 121 loss: 0.00420061825
Iter: 122 loss: 0.00454877596
Iter: 123 loss: 0.00416495325
Iter: 124 loss: 0.00406912714
Iter: 125 loss: 0.00415923586
Iter: 126 loss: 0.00401469786
Iter: 127 loss: 0.00397394877
Iter: 128 loss: 0.00392938405
Iter: 129 loss: 0.00387978926
Iter: 130 loss: 0.00383182149
Iter: 131 loss: 0.00374767464
Iter: 132 loss: 0.00374696637
Iter: 133 loss: 0.00365409162
Iter: 134 loss: 0.00412903726
Iter: 135 loss: 0.00363867474
Iter: 136 loss: 0.00354639976
Iter: 137 loss: 0.00384232029
Iter: 138 loss: 0.00350727281
Iter: 139 loss: 0.00341400481
Iter: 140 loss: 0.00497996667
Iter: 141 loss: 0.00341390353
Iter: 142 loss: 0.00336671248
Iter: 143 loss: 0.00415550917
Iter: 144 loss: 0.00336662983
Iter: 145 loss: 0.00332979672
Iter: 146 loss: 0.00344819902
Iter: 147 loss: 0.00331800431
Iter: 148 loss: 0.00329593709
Iter: 149 loss: 0.00329574104
Iter: 150 loss: 0.00326307397
Iter: 151 loss: 0.00373881124
Iter: 152 loss: 0.00326174544
Iter: 153 loss: 0.00322617777
Iter: 154 loss: 0.00332986563
Iter: 155 loss: 0.00321318256
Iter: 156 loss: 0.00316289789
Iter: 157 loss: 0.00324281142
Iter: 158 loss: 0.00313924626
Iter: 159 loss: 0.00311603956
Iter: 160 loss: 0.00351946498
Iter: 161 loss: 0.00311573618
Iter: 162 loss: 0.0030949288
Iter: 163 loss: 0.00315106194
Iter: 164 loss: 0.00308821769
Iter: 165 loss: 0.00305765937
Iter: 166 loss: 0.00327136531
Iter: 167 loss: 0.00305203209
Iter: 168 loss: 0.00300740777
Iter: 169 loss: 0.00312981149
Iter: 170 loss: 0.00299146143
Iter: 171 loss: 0.00293258717
Iter: 172 loss: 0.00342461444
Iter: 173 loss: 0.00292854221
Iter: 174 loss: 0.00289929355
Iter: 175 loss: 0.00289818854
Iter: 176 loss: 0.00288031227
Iter: 177 loss: 0.00305640465
Iter: 178 loss: 0.00287955697
Iter: 179 loss: 0.00287088938
Iter: 180 loss: 0.00286006974
Iter: 181 loss: 0.00285871653
Iter: 182 loss: 0.00282350415
Iter: 183 loss: 0.00305397599
Iter: 184 loss: 0.00282006501
Iter: 185 loss: 0.00278673973
Iter: 186 loss: 0.00334953074
Iter: 187 loss: 0.00278559513
Iter: 188 loss: 0.0027733061
Iter: 189 loss: 0.00280708913
Iter: 190 loss: 0.00276934146
Iter: 191 loss: 0.00275124749
Iter: 192 loss: 0.00280638947
Iter: 193 loss: 0.00274496106
Iter: 194 loss: 0.00272882544
Iter: 195 loss: 0.00279171695
Iter: 196 loss: 0.00272505078
Iter: 197 loss: 0.00270182639
Iter: 198 loss: 0.0027606138
Iter: 199 loss: 0.00269137928
Iter: 200 loss: 0.00266246172
Iter: 201 loss: 0.002900908
Iter: 202 loss: 0.00266078534
Iter: 203 loss: 0.0026432306
Iter: 204 loss: 0.00296314945
Iter: 205 loss: 0.00264272769
Iter: 206 loss: 0.00263189012
Iter: 207 loss: 0.00264683156
Iter: 208 loss: 0.00262647774
Iter: 209 loss: 0.00261261081
Iter: 210 loss: 0.00280995644
Iter: 211 loss: 0.00261256
Iter: 212 loss: 0.00259588426
Iter: 213 loss: 0.00268441765
Iter: 214 loss: 0.00259312871
Iter: 215 loss: 0.00258189021
Iter: 216 loss: 0.00257676234
Iter: 217 loss: 0.00257113855
Iter: 218 loss: 0.00255578454
Iter: 219 loss: 0.0026442383
Iter: 220 loss: 0.00255199848
Iter: 221 loss: 0.00253716623
Iter: 222 loss: 0.00253691478
Iter: 223 loss: 0.00252628676
Iter: 224 loss: 0.00256141834
Iter: 225 loss: 0.00252371654
Iter: 226 loss: 0.0025149493
Iter: 227 loss: 0.00255639851
Iter: 228 loss: 0.00251322333
Iter: 229 loss: 0.00250974623
Iter: 230 loss: 0.00251673209
Iter: 231 loss: 0.00250848965
Iter: 232 loss: 0.00249869376
Iter: 233 loss: 0.00251429854
Iter: 234 loss: 0.00249403552
Iter: 235 loss: 0.00248416723
Iter: 236 loss: 0.00248142425
Iter: 237 loss: 0.00247187261
Iter: 238 loss: 0.0024813232
Iter: 239 loss: 0.00246641412
Iter: 240 loss: 0.00245880894
Iter: 241 loss: 0.00245117629
Iter: 242 loss: 0.00244946545
Iter: 243 loss: 0.00244222255
Iter: 244 loss: 0.00249498291
Iter: 245 loss: 0.00244188774
Iter: 246 loss: 0.00243560481
Iter: 247 loss: 0.00243680249
Iter: 248 loss: 0.00243073679
Iter: 249 loss: 0.00242254254
Iter: 250 loss: 0.00243001105
Iter: 251 loss: 0.00241782097
Iter: 252 loss: 0.0024137767
Iter: 253 loss: 0.00241320371
Iter: 254 loss: 0.00241039693
Iter: 255 loss: 0.00240396755
Iter: 256 loss: 0.00245149853
Iter: 257 loss: 0.00240319129
Iter: 258 loss: 0.00239409553
Iter: 259 loss: 0.00240111072
Iter: 260 loss: 0.00238858187
Iter: 261 loss: 0.00237668166
Iter: 262 loss: 0.00237659318
Iter: 263 loss: 0.00236783177
Iter: 264 loss: 0.00237415801
Iter: 265 loss: 0.00236236118
Iter: 266 loss: 0.00235075946
Iter: 267 loss: 0.00243689539
Iter: 268 loss: 0.00234985212
Iter: 269 loss: 0.00234613614
Iter: 270 loss: 0.00236205175
Iter: 271 loss: 0.00234537385
Iter: 272 loss: 0.00234004506
Iter: 273 loss: 0.00235899701
Iter: 274 loss: 0.00233822549
Iter: 275 loss: 0.00233055884
Iter: 276 loss: 0.00234915316
Iter: 277 loss: 0.00232784427
Iter: 278 loss: 0.00232666172
Iter: 279 loss: 0.00232481561
Iter: 280 loss: 0.00232233503
Iter: 281 loss: 0.00233180728
Iter: 282 loss: 0.0023217462
Iter: 283 loss: 0.00231978437
Iter: 284 loss: 0.00231735827
Iter: 285 loss: 0.00231714221
Iter: 286 loss: 0.00231274962
Iter: 287 loss: 0.00230318168
Iter: 288 loss: 0.00244460162
Iter: 289 loss: 0.00230278261
Iter: 290 loss: 0.00229705893
Iter: 291 loss: 0.00236010831
Iter: 292 loss: 0.00229681237
Iter: 293 loss: 0.00229227357
Iter: 294 loss: 0.00229181722
Iter: 295 loss: 0.00228847051
Iter: 296 loss: 0.00228430983
Iter: 297 loss: 0.00228411215
Iter: 298 loss: 0.00228080899
Iter: 299 loss: 0.00229225541
Iter: 300 loss: 0.00228015101
Iter: 301 loss: 0.00227771839
Iter: 302 loss: 0.00227653421
Iter: 303 loss: 0.0022752923
Iter: 304 loss: 0.00226961146
Iter: 305 loss: 0.0022883371
Iter: 306 loss: 0.00226803054
Iter: 307 loss: 0.00226447824
Iter: 308 loss: 0.00228096358
Iter: 309 loss: 0.00226381654
Iter: 310 loss: 0.00226257648
Iter: 311 loss: 0.00226219604
Iter: 312 loss: 0.0022614561
Iter: 313 loss: 0.00225833151
Iter: 314 loss: 0.00225585746
Iter: 315 loss: 0.0022548472
Iter: 316 loss: 0.00224873191
Iter: 317 loss: 0.00224764366
Iter: 318 loss: 0.0022443505
Iter: 319 loss: 0.00224365806
Iter: 320 loss: 0.00224153465
Iter: 321 loss: 0.00223778118
Iter: 322 loss: 0.00226463075
Iter: 323 loss: 0.00223737396
Iter: 324 loss: 0.00223480863
Iter: 325 loss: 0.00224644528
Iter: 326 loss: 0.00223422702
Iter: 327 loss: 0.00223151618
Iter: 328 loss: 0.00223678607
Iter: 329 loss: 0.00223041186
Iter: 330 loss: 0.00222828216
Iter: 331 loss: 0.0022260691
Iter: 332 loss: 0.00222566468
Iter: 333 loss: 0.00222255359
Iter: 334 loss: 0.00222047
Iter: 335 loss: 0.00221932074
Iter: 336 loss: 0.00221601082
Iter: 337 loss: 0.00224174629
Iter: 338 loss: 0.00221573515
Iter: 339 loss: 0.00222224183
Iter: 340 loss: 0.00221486902
Iter: 341 loss: 0.00221388927
Iter: 342 loss: 0.00221398426
Iter: 343 loss: 0.00221313327
Iter: 344 loss: 0.00220965641
Iter: 345 loss: 0.00220312458
Iter: 346 loss: 0.00236094836
Iter: 347 loss: 0.00220311293
Iter: 348 loss: 0.00219906983
Iter: 349 loss: 0.00219819043
Iter: 350 loss: 0.00219682837
Iter: 351 loss: 0.00219829986
Iter: 352 loss: 0.00219608401
Iter: 353 loss: 0.00219449168
Iter: 354 loss: 0.00219106721
Iter: 355 loss: 0.00226396089
Iter: 356 loss: 0.00219098339
Iter: 357 loss: 0.00218847254
Iter: 358 loss: 0.00219980301
Iter: 359 loss: 0.00218802365
Iter: 360 loss: 0.00218732888
Iter: 361 loss: 0.00218841247
Iter: 362 loss: 0.00218699849
Iter: 363 loss: 0.00218645413
Iter: 364 loss: 0.00218595122
Iter: 365 loss: 0.0021858213
Iter: 366 loss: 0.00218443945
Iter: 367 loss: 0.00218439265
Iter: 368 loss: 0.00218296191
Iter: 369 loss: 0.00218290719
Iter: 370 loss: 0.00218179962
Iter: 371 loss: 0.0021802457
Iter: 372 loss: 0.00218456867
Iter: 373 loss: 0.00217975653
Iter: 374 loss: 0.00217798166
Iter: 375 loss: 0.00219885702
Iter: 376 loss: 0.00217795698
Iter: 377 loss: 0.00217687245
Iter: 378 loss: 0.00217492785
Iter: 379 loss: 0.0022255471
Iter: 380 loss: 0.00217492832
Iter: 381 loss: 0.00217193807
Iter: 382 loss: 0.00220602
Iter: 383 loss: 0.00217188126
Iter: 384 loss: 0.00216913363
Iter: 385 loss: 0.00217025913
Iter: 386 loss: 0.00216726237
Iter: 387 loss: 0.00216477341
Iter: 388 loss: 0.00216477294
Iter: 389 loss: 0.00216407888
Iter: 390 loss: 0.0021638
Iter: 391 loss: 0.00216333498
Iter: 392 loss: 0.00216529658
Iter: 393 loss: 0.0021632344
Iter: 394 loss: 0.00216272799
Iter: 395 loss: 0.00216277479
Iter: 396 loss: 0.0021623408
Iter: 397 loss: 0.00216120807
Iter: 398 loss: 0.00215752656
Iter: 399 loss: 0.00215957174
Iter: 400 loss: 0.00215421012
Iter: 401 loss: 0.00216132565
Iter: 402 loss: 0.00215302547
Iter: 403 loss: 0.00215238659
Iter: 404 loss: 0.00215216912
Iter: 405 loss: 0.00215180311
Iter: 406 loss: 0.00215014233
Iter: 407 loss: 0.00214686454
Iter: 408 loss: 0.00220864592
Iter: 409 loss: 0.00214683264
Iter: 410 loss: 0.00214693393
Iter: 411 loss: 0.00214443589
Iter: 412 loss: 0.00214239117
Iter: 413 loss: 0.00214666687
Iter: 414 loss: 0.00214157067
Iter: 415 loss: 0.00214036624
Iter: 416 loss: 0.00214147917
Iter: 417 loss: 0.00213966263
Iter: 418 loss: 0.00213849684
Iter: 419 loss: 0.00213774177
Iter: 420 loss: 0.00213729497
Iter: 421 loss: 0.00213533547
Iter: 422 loss: 0.00213638134
Iter: 423 loss: 0.00213404046
Iter: 424 loss: 0.00213272916
Iter: 425 loss: 0.00215286482
Iter: 426 loss: 0.00213272823
Iter: 427 loss: 0.00213198503
Iter: 428 loss: 0.00213256571
Iter: 429 loss: 0.0021315231
Iter: 430 loss: 0.00213006651
Iter: 431 loss: 0.00213365839
Iter: 432 loss: 0.00212956616
Iter: 433 loss: 0.00212889956
Iter: 434 loss: 0.00212846417
Iter: 435 loss: 0.00212820619
Iter: 436 loss: 0.00212702877
Iter: 437 loss: 0.0021334663
Iter: 438 loss: 0.00212685624
Iter: 439 loss: 0.00212566275
Iter: 440 loss: 0.0021343329
Iter: 441 loss: 0.0021255645
Iter: 442 loss: 0.00212510722
Iter: 443 loss: 0.00212380709
Iter: 444 loss: 0.00212975661
Iter: 445 loss: 0.00212334283
Iter: 446 loss: 0.00212329
Iter: 447 loss: 0.00212271488
Iter: 448 loss: 0.00212226133
Iter: 449 loss: 0.00212221779
Iter: 450 loss: 0.0021218881
Iter: 451 loss: 0.00212136
Iter: 452 loss: 0.00212016283
Iter: 453 loss: 0.00213586539
Iter: 454 loss: 0.00212008762
Iter: 455 loss: 0.00211895211
Iter: 456 loss: 0.00212130509
Iter: 457 loss: 0.00211849529
Iter: 458 loss: 0.00211634254
Iter: 459 loss: 0.0021206236
Iter: 460 loss: 0.00211547967
Iter: 461 loss: 0.00211387803
Iter: 462 loss: 0.00212375517
Iter: 463 loss: 0.00211367616
Iter: 464 loss: 0.00211307174
Iter: 465 loss: 0.00211559609
Iter: 466 loss: 0.0021129509
Iter: 467 loss: 0.00211211294
Iter: 468 loss: 0.00211103074
Iter: 469 loss: 0.00211095763
Iter: 470 loss: 0.00210849289
Iter: 471 loss: 0.00211113645
Iter: 472 loss: 0.00210713199
Iter: 473 loss: 0.00210667984
Iter: 474 loss: 0.00210661325
Iter: 475 loss: 0.00210614642
Iter: 476 loss: 0.00210638763
Iter: 477 loss: 0.00210583769
Iter: 478 loss: 0.00210523792
Iter: 479 loss: 0.00210871408
Iter: 480 loss: 0.00210515922
Iter: 481 loss: 0.00210433709
Iter: 482 loss: 0.00210272567
Iter: 483 loss: 0.00213472918
Iter: 484 loss: 0.0021027117
Iter: 485 loss: 0.0021012947
Iter: 486 loss: 0.00211154227
Iter: 487 loss: 0.00210117223
Iter: 488 loss: 0.00210045185
Iter: 489 loss: 0.0021052137
Iter: 490 loss: 0.00210037781
Iter: 491 loss: 0.00209974381
Iter: 492 loss: 0.00210312475
Iter: 493 loss: 0.00209964719
Iter: 494 loss: 0.00209933496
Iter: 495 loss: 0.00210211426
Iter: 496 loss: 0.00209931913
Iter: 497 loss: 0.00209888606
Iter: 498 loss: 0.00209804438
Iter: 499 loss: 0.00211414462
Iter: 500 loss: 0.00209803763
Iter: 501 loss: 0.00209760945
Iter: 502 loss: 0.00209951331
Iter: 503 loss: 0.00209752563
Iter: 504 loss: 0.00209704926
Iter: 505 loss: 0.0020979275
Iter: 506 loss: 0.00209684554
Iter: 507 loss: 0.00209630933
Iter: 508 loss: 0.00210317853
Iter: 509 loss: 0.00209630793
Iter: 510 loss: 0.00209596939
Iter: 511 loss: 0.00209748489
Iter: 512 loss: 0.0020959028
Iter: 513 loss: 0.00209574448
Iter: 514 loss: 0.002096802
Iter: 515 loss: 0.00209572865
Iter: 516 loss: 0.00209556799
Iter: 517 loss: 0.00209524715
Iter: 518 loss: 0.00210124394
Iter: 519 loss: 0.00209524366
Iter: 520 loss: 0.00209487
Iter: 521 loss: 0.00209414377
Iter: 522 loss: 0.00210872735
Iter: 523 loss: 0.00209413841
Iter: 524 loss: 0.00209329044
Iter: 525 loss: 0.00209904276
Iter: 526 loss: 0.00209320872
Iter: 527 loss: 0.00209246203
Iter: 528 loss: 0.00209417799
Iter: 529 loss: 0.00209218776
Iter: 530 loss: 0.00209163595
Iter: 531 loss: 0.00209229835
Iter: 532 loss: 0.00209134817
Iter: 533 loss: 0.00209050486
Iter: 534 loss: 0.00209103618
Iter: 535 loss: 0.00208997983
Iter: 536 loss: 0.00208841497
Iter: 537 loss: 0.00208838354
Iter: 538 loss: 0.0020876613
Iter: 539 loss: 0.00208765361
Iter: 540 loss: 0.00208729552
Iter: 541 loss: 0.00208845572
Iter: 542 loss: 0.00208719238
Iter: 543 loss: 0.00208687852
Iter: 544 loss: 0.00208726013
Iter: 545 loss: 0.00208671531
Iter: 546 loss: 0.00208647875
Iter: 547 loss: 0.00208647456
Iter: 548 loss: 0.00208629016
Iter: 549 loss: 0.00208606408
Iter: 550 loss: 0.00208604266
Iter: 551 loss: 0.00208552787
Iter: 552 loss: 0.00208460446
Iter: 553 loss: 0.00210670475
Iter: 554 loss: 0.0020846054
Iter: 555 loss: 0.00208406523
Iter: 556 loss: 0.00208427059
Iter: 557 loss: 0.00208368897
Iter: 558 loss: 0.00208304101
Iter: 559 loss: 0.00208666874
Iter: 560 loss: 0.0020829495
Iter: 561 loss: 0.00208220957
Iter: 562 loss: 0.00208754931
Iter: 563 loss: 0.00208214717
Iter: 564 loss: 0.00208169874
Iter: 565 loss: 0.00208508
Iter: 566 loss: 0.00208166288
Iter: 567 loss: 0.00208141259
Iter: 568 loss: 0.0020811914
Iter: 569 loss: 0.00208112411
Iter: 570 loss: 0.00208075112
Iter: 571 loss: 0.00207989849
Iter: 572 loss: 0.0020907775
Iter: 573 loss: 0.00207983935
Iter: 574 loss: 0.00208163913
Iter: 575 loss: 0.00207955251
Iter: 576 loss: 0.00207936717
Iter: 577 loss: 0.00207930221
Iter: 578 loss: 0.00207919395
Iter: 579 loss: 0.00207893387
Iter: 580 loss: 0.00208173739
Iter: 581 loss: 0.00207892712
Iter: 582 loss: 0.00207860256
Iter: 583 loss: 0.00207925332
Iter: 584 loss: 0.00207847077
Iter: 585 loss: 0.00207807682
Iter: 586 loss: 0.00207817927
Iter: 587 loss: 0.00207779091
Iter: 588 loss: 0.0020775469
Iter: 589 loss: 0.00207716459
Iter: 590 loss: 0.00207716087
Iter: 591 loss: 0.002076637
Iter: 592 loss: 0.00207740744
Iter: 593 loss: 0.00207638694
Iter: 594 loss: 0.0020760214
Iter: 595 loss: 0.00207945053
Iter: 596 loss: 0.00207600743
Iter: 597 loss: 0.00207575038
Iter: 598 loss: 0.00207506609
Iter: 599 loss: 0.00207993877
Iter: 600 loss: 0.00207491778
Iter: 601 loss: 0.00207354408
Iter: 602 loss: 0.00208556908
Iter: 603 loss: 0.0020734712
Iter: 604 loss: 0.00207290053
Iter: 605 loss: 0.0020767895
Iter: 606 loss: 0.00207284745
Iter: 607 loss: 0.00207273709
Iter: 608 loss: 0.00207237015
Iter: 609 loss: 0.00207244372
Iter: 610 loss: 0.00207200903
Iter: 611 loss: 0.00207148958
Iter: 612 loss: 0.00207471778
Iter: 613 loss: 0.00207142788
Iter: 614 loss: 0.00207160879
Iter: 615 loss: 0.00207125628
Iter: 616 loss: 0.00207110355
Iter: 617 loss: 0.00207081297
Iter: 618 loss: 0.00207706029
Iter: 619 loss: 0.00207081065
Iter: 620 loss: 0.00207037339
Iter: 621 loss: 0.00207167957
Iter: 622 loss: 0.00207024324
Iter: 623 loss: 0.00207006256
Iter: 624 loss: 0.0020695813
Iter: 625 loss: 0.00207285024
Iter: 626 loss: 0.00206946814
Iter: 627 loss: 0.00206903904
Iter: 628 loss: 0.0020691962
Iter: 629 loss: 0.00206873845
Iter: 630 loss: 0.00206827186
Iter: 631 loss: 0.00206764974
Iter: 632 loss: 0.00206761528
Iter: 633 loss: 0.00207415828
Iter: 634 loss: 0.00206733565
Iter: 635 loss: 0.00206713565
Iter: 636 loss: 0.00206773076
Iter: 637 loss: 0.00206707558
Iter: 638 loss: 0.00206698151
Iter: 639 loss: 0.0020667878
Iter: 640 loss: 0.00207017665
Iter: 641 loss: 0.00206678524
Iter: 642 loss: 0.00206649723
Iter: 643 loss: 0.00206633285
Iter: 644 loss: 0.00206621038
Iter: 645 loss: 0.00206579641
Iter: 646 loss: 0.00206545647
Iter: 647 loss: 0.00206533633
Iter: 648 loss: 0.00206556544
Iter: 649 loss: 0.00206510862
Iter: 650 loss: 0.00206498778
Iter: 651 loss: 0.00206522853
Iter: 652 loss: 0.00206493726
Iter: 653 loss: 0.00206472958
Iter: 654 loss: 0.00206586253
Iter: 655 loss: 0.00206469884
Iter: 656 loss: 0.00206436915
Iter: 657 loss: 0.00206360826
Iter: 658 loss: 0.00207303325
Iter: 659 loss: 0.00206355168
Iter: 660 loss: 0.002062944
Iter: 661 loss: 0.00206828117
Iter: 662 loss: 0.00206291489
Iter: 663 loss: 0.00206235074
Iter: 664 loss: 0.00206690677
Iter: 665 loss: 0.00206231372
Iter: 666 loss: 0.00206187135
Iter: 667 loss: 0.0020664609
Iter: 668 loss: 0.00206185691
Iter: 669 loss: 0.00206156843
Iter: 670 loss: 0.00206587929
Iter: 671 loss: 0.0020615668
Iter: 672 loss: 0.00206135493
Iter: 673 loss: 0.00206136587
Iter: 674 loss: 0.00206118845
Iter: 675 loss: 0.0020609335
Iter: 676 loss: 0.00206113351
Iter: 677 loss: 0.00206078077
Iter: 678 loss: 0.00206039241
Iter: 679 loss: 0.00206025504
Iter: 680 loss: 0.00206003524
Iter: 681 loss: 0.00205974909
Iter: 682 loss: 0.00206340291
Iter: 683 loss: 0.00205974793
Iter: 684 loss: 0.00205943082
Iter: 685 loss: 0.0020599477
Iter: 686 loss: 0.00205928553
Iter: 687 loss: 0.00205907458
Iter: 688 loss: 0.00205875072
Iter: 689 loss: 0.00205874559
Iter: 690 loss: 0.00205820519
Iter: 691 loss: 0.00206413469
Iter: 692 loss: 0.00205819355
Iter: 693 loss: 0.00205802987
Iter: 694 loss: 0.00205775769
Iter: 695 loss: 0.00205775769
Iter: 696 loss: 0.00205744407
Iter: 697 loss: 0.00205743639
Iter: 698 loss: 0.00205719238
Iter: 699 loss: 0.00205693115
Iter: 700 loss: 0.00205820566
Iter: 701 loss: 0.00205688505
Iter: 702 loss: 0.00205675932
Iter: 703 loss: 0.00205645734
Iter: 704 loss: 0.002059825
Iter: 705 loss: 0.0020564287
Iter: 706 loss: 0.00205636816
Iter: 707 loss: 0.00205628574
Iter: 708 loss: 0.00205613882
Iter: 709 loss: 0.00205711555
Iter: 710 loss: 0.00205612439
Iter: 711 loss: 0.00205604685
Iter: 712 loss: 0.00205582287
Iter: 713 loss: 0.00205683149
Iter: 714 loss: 0.00205574255
Iter: 715 loss: 0.00205541798
Iter: 716 loss: 0.00205822825
Iter: 717 loss: 0.00205540145
Iter: 718 loss: 0.00205523148
Iter: 719 loss: 0.00205554068
Iter: 720 loss: 0.00205515744
Iter: 721 loss: 0.00205502613
Iter: 722 loss: 0.00205505057
Iter: 723 loss: 0.00205492647
Iter: 724 loss: 0.00205463404
Iter: 725 loss: 0.00205488317
Iter: 726 loss: 0.00205446081
Iter: 727 loss: 0.00205400912
Iter: 728 loss: 0.00205477932
Iter: 729 loss: 0.00205380539
Iter: 730 loss: 0.002053604
Iter: 731 loss: 0.00205314043
Iter: 732 loss: 0.00205893256
Iter: 733 loss: 0.0020531076
Iter: 734 loss: 0.00205322355
Iter: 735 loss: 0.00205286266
Iter: 736 loss: 0.00205259747
Iter: 737 loss: 0.00205193879
Iter: 738 loss: 0.00205813441
Iter: 739 loss: 0.00205184659
Iter: 740 loss: 0.00205130782
Iter: 741 loss: 0.00205198536
Iter: 742 loss: 0.00205102842
Iter: 743 loss: 0.00205372227
Iter: 744 loss: 0.00205086148
Iter: 745 loss: 0.00205066754
Iter: 746 loss: 0.00205044355
Iter: 747 loss: 0.00205041794
Iter: 748 loss: 0.00205016858
Iter: 749 loss: 0.0020501609
Iter: 750 loss: 0.00204993831
Iter: 751 loss: 0.00205073925
Iter: 752 loss: 0.0020498808
Iter: 753 loss: 0.00204969477
Iter: 754 loss: 0.00204923609
Iter: 755 loss: 0.00205369573
Iter: 756 loss: 0.00204917276
Iter: 757 loss: 0.00204888289
Iter: 758 loss: 0.00204882
Iter: 759 loss: 0.00204853248
Iter: 760 loss: 0.00204873225
Iter: 761 loss: 0.0020483546
Iter: 762 loss: 0.00204819767
Iter: 763 loss: 0.00204787776
Iter: 764 loss: 0.00205350458
Iter: 765 loss: 0.00204787031
Iter: 766 loss: 0.00204753643
Iter: 767 loss: 0.00204756763
Iter: 768 loss: 0.00204727938
Iter: 769 loss: 0.00204674294
Iter: 770 loss: 0.0020467015
Iter: 771 loss: 0.00204656972
Iter: 772 loss: 0.00204721023
Iter: 773 loss: 0.00204654643
Iter: 774 loss: 0.0020464994
Iter: 775 loss: 0.00204632618
Iter: 776 loss: 0.00204580091
Iter: 777 loss: 0.00205174205
Iter: 778 loss: 0.00204574876
Iter: 779 loss: 0.00204500696
Iter: 780 loss: 0.00205118489
Iter: 781 loss: 0.00204496318
Iter: 782 loss: 0.00204440672
Iter: 783 loss: 0.00204839231
Iter: 784 loss: 0.00204435876
Iter: 785 loss: 0.00204435
Iter: 786 loss: 0.00204415852
Iter: 787 loss: 0.00204401952
Iter: 788 loss: 0.00204391591
Iter: 789 loss: 0.00204387074
Iter: 790 loss: 0.00204360578
Iter: 791 loss: 0.00204477087
Iter: 792 loss: 0.00204355456
Iter: 793 loss: 0.00204312475
Iter: 794 loss: 0.00204477482
Iter: 795 loss: 0.00204302371
Iter: 796 loss: 0.00204267
Iter: 797 loss: 0.00204309635
Iter: 798 loss: 0.00204248331
Iter: 799 loss: 0.0020422549
Iter: 800 loss: 0.00204153429
Iter: 801 loss: 0.00204255758
Iter: 802 loss: 0.00204100762
Iter: 803 loss: 0.00204052147
Iter: 804 loss: 0.0020424393
Iter: 805 loss: 0.00204041158
Iter: 806 loss: 0.00204023113
Iter: 807 loss: 0.00204029423
Iter: 808 loss: 0.00204010401
Iter: 809 loss: 0.00203986093
Iter: 810 loss: 0.0020397671
Iter: 811 loss: 0.00203963369
Iter: 812 loss: 0.00203929306
Iter: 813 loss: 0.00203897804
Iter: 814 loss: 0.00203889585
Iter: 815 loss: 0.00203825813
Iter: 816 loss: 0.00204241928
Iter: 817 loss: 0.002038192
Iter: 818 loss: 0.00203791307
Iter: 819 loss: 0.00204195082
Iter: 820 loss: 0.00203791261
Iter: 821 loss: 0.00203767535
Iter: 822 loss: 0.00203899061
Iter: 823 loss: 0.0020376388
Iter: 824 loss: 0.00203744462
Iter: 825 loss: 0.0020370346
Iter: 826 loss: 0.00204371335
Iter: 827 loss: 0.00203701947
Iter: 828 loss: 0.00203658734
Iter: 829 loss: 0.00203658105
Iter: 830 loss: 0.00203637267
Iter: 831 loss: 0.00203636917
Iter: 832 loss: 0.00203618454
Iter: 833 loss: 0.00203623902
Iter: 834 loss: 0.00203605136
Iter: 835 loss: 0.00203591725
Iter: 836 loss: 0.00203700783
Iter: 837 loss: 0.0020359091
Iter: 838 loss: 0.00203579525
Iter: 839 loss: 0.00203690398
Iter: 840 loss: 0.00203579152
Iter: 841 loss: 0.00203571585
Iter: 842 loss: 0.00203547184
Iter: 843 loss: 0.00203581853
Iter: 844 loss: 0.00203529582
Iter: 845 loss: 0.00203480804
Iter: 846 loss: 0.00203561271
Iter: 847 loss: 0.00203458383
Iter: 848 loss: 0.00203404878
Iter: 849 loss: 0.00203404529
Iter: 850 loss: 0.00203369907
Iter: 851 loss: 0.00203464646
Iter: 852 loss: 0.00203358452
Iter: 853 loss: 0.00203339057
Iter: 854 loss: 0.00203331839
Iter: 855 loss: 0.00203321083
Iter: 856 loss: 0.00203289557
Iter: 857 loss: 0.00203538174
Iter: 858 loss: 0.00203287462
Iter: 859 loss: 0.00203252141
Iter: 860 loss: 0.00203263224
Iter: 861 loss: 0.00203226903
Iter: 862 loss: 0.0020320341
Iter: 863 loss: 0.00203265902
Iter: 864 loss: 0.00203195517
Iter: 865 loss: 0.00203176262
Iter: 866 loss: 0.00203410489
Iter: 867 loss: 0.00203176099
Iter: 868 loss: 0.00203167414
Iter: 869 loss: 0.0020316653
Iter: 870 loss: 0.00203162082
Iter: 871 loss: 0.00203180779
Iter: 872 loss: 0.00203161244
Iter: 873 loss: 0.00203156215
Iter: 874 loss: 0.00203141244
Iter: 875 loss: 0.002031927
Iter: 876 loss: 0.00203134608
Iter: 877 loss: 0.00203106878
Iter: 878 loss: 0.00203111302
Iter: 879 loss: 0.00203086063
Iter: 880 loss: 0.00203061849
Iter: 881 loss: 0.00203344459
Iter: 882 loss: 0.00203061383
Iter: 883 loss: 0.00203042594
Iter: 884 loss: 0.00203106855
Iter: 885 loss: 0.00203037704
Iter: 886 loss: 0.00203021104
Iter: 887 loss: 0.00202999962
Iter: 888 loss: 0.00202998426
Iter: 889 loss: 0.00202975236
Iter: 890 loss: 0.00202975073
Iter: 891 loss: 0.00202960079
Iter: 892 loss: 0.00202950696
Iter: 893 loss: 0.00202944712
Iter: 894 loss: 0.00202922942
Iter: 895 loss: 0.00202945154
Iter: 896 loss: 0.00202910649
Iter: 897 loss: 0.00202891976
Iter: 898 loss: 0.00202928414
Iter: 899 loss: 0.00202884199
Iter: 900 loss: 0.00202864315
Iter: 901 loss: 0.00202864106
Iter: 902 loss: 0.00202850439
Iter: 903 loss: 0.00202922057
Iter: 904 loss: 0.0020284825
Iter: 905 loss: 0.00202835328
Iter: 906 loss: 0.00202818541
Iter: 907 loss: 0.0020281754
Iter: 908 loss: 0.00202803593
Iter: 909 loss: 0.00202777656
Iter: 910 loss: 0.00203378941
Iter: 911 loss: 0.00202777563
Iter: 912 loss: 0.00202752557
Iter: 913 loss: 0.00202942849
Iter: 914 loss: 0.00202750601
Iter: 915 loss: 0.00202727504
Iter: 916 loss: 0.00202866155
Iter: 917 loss: 0.0020272457
Iter: 918 loss: 0.00202705525
Iter: 919 loss: 0.00202646828
Iter: 920 loss: 0.00202770554
Iter: 921 loss: 0.00202610577
Iter: 922 loss: 0.00202692486
Iter: 923 loss: 0.00202589971
Iter: 924 loss: 0.00202563219
Iter: 925 loss: 0.00202567922
Iter: 926 loss: 0.00202543242
Iter: 927 loss: 0.00202504732
Iter: 928 loss: 0.00202553766
Iter: 929 loss: 0.00202484941
Iter: 930 loss: 0.00202461518
Iter: 931 loss: 0.00202736328
Iter: 932 loss: 0.00202461146
Iter: 933 loss: 0.00202451274
Iter: 934 loss: 0.00202450948
Iter: 935 loss: 0.00202442333
Iter: 936 loss: 0.00202428456
Iter: 937 loss: 0.00202428363
Iter: 938 loss: 0.0020240657
Iter: 939 loss: 0.00202536234
Iter: 940 loss: 0.00202403776
Iter: 941 loss: 0.00202388037
Iter: 942 loss: 0.00202362868
Iter: 943 loss: 0.00202362705
Iter: 944 loss: 0.00202322751
Iter: 945 loss: 0.00202372228
Iter: 946 loss: 0.00202302099
Iter: 947 loss: 0.00202545221
Iter: 948 loss: 0.00202279352
Iter: 949 loss: 0.00202257349
Iter: 950 loss: 0.00202244334
Iter: 951 loss: 0.00202235067
Iter: 952 loss: 0.00202206546
Iter: 953 loss: 0.00202370295
Iter: 954 loss: 0.0020220275
Iter: 955 loss: 0.00202185614
Iter: 956 loss: 0.00202184729
Iter: 957 loss: 0.00202172901
Iter: 958 loss: 0.00202136138
Iter: 959 loss: 0.00202210667
Iter: 960 loss: 0.00202113087
Iter: 961 loss: 0.00202078326
Iter: 962 loss: 0.00202290388
Iter: 963 loss: 0.00202074088
Iter: 964 loss: 0.00202054856
Iter: 965 loss: 0.00202053459
Iter: 966 loss: 0.00202037673
Iter: 967 loss: 0.00202005962
Iter: 968 loss: 0.00202591345
Iter: 969 loss: 0.00202005636
Iter: 970 loss: 0.00201962888
Iter: 971 loss: 0.00202390878
Iter: 972 loss: 0.00201961305
Iter: 973 loss: 0.0020194049
Iter: 974 loss: 0.00201952271
Iter: 975 loss: 0.00201927079
Iter: 976 loss: 0.0020190184
Iter: 977 loss: 0.00201855553
Iter: 978 loss: 0.00203015609
Iter: 979 loss: 0.00201855507
Iter: 980 loss: 0.00201806892
Iter: 981 loss: 0.00202162378
Iter: 982 loss: 0.0020180298
Iter: 983 loss: 0.00201762514
Iter: 984 loss: 0.00201783655
Iter: 985 loss: 0.00201735692
Iter: 986 loss: 0.00201673363
Iter: 987 loss: 0.0020221388
Iter: 988 loss: 0.00201670174
Iter: 989 loss: 0.0020165653
Iter: 990 loss: 0.00201802328
Iter: 991 loss: 0.0020165618
Iter: 992 loss: 0.0020164554
Iter: 993 loss: 0.00201612711
Iter: 994 loss: 0.00201700232
Iter: 995 loss: 0.00201594783
Iter: 996 loss: 0.00201565772
Iter: 997 loss: 0.00201739091
Iter: 998 loss: 0.00201562163
Iter: 999 loss: 0.00201543234
Iter: 1000 loss: 0.00201547449
Iter: 1001 loss: 0.00201529358
Iter: 1002 loss: 0.00201501371
Iter: 1003 loss: 0.00201882445
Iter: 1004 loss: 0.00201501348
Iter: 1005 loss: 0.00201477669
Iter: 1006 loss: 0.00201558555
Iter: 1007 loss: 0.00201471476
Iter: 1008 loss: 0.00201450195
Iter: 1009 loss: 0.00201524165
Iter: 1010 loss: 0.00201444468
Iter: 1011 loss: 0.00201427634
Iter: 1012 loss: 0.00201384886
Iter: 1013 loss: 0.00201754179
Iter: 1014 loss: 0.00201378111
Iter: 1015 loss: 0.00201313663
Iter: 1016 loss: 0.00201356271
Iter: 1017 loss: 0.00201273197
Iter: 1018 loss: 0.00201241765
Iter: 1019 loss: 0.00201235292
Iter: 1020 loss: 0.00201207958
Iter: 1021 loss: 0.00201244326
Iter: 1022 loss: 0.00201193988
Iter: 1023 loss: 0.00201163767
Iter: 1024 loss: 0.00201407238
Iter: 1025 loss: 0.00201161858
Iter: 1026 loss: 0.0020112521
Iter: 1027 loss: 0.00201042881
Iter: 1028 loss: 0.00202158466
Iter: 1029 loss: 0.00201038271
Iter: 1030 loss: 0.00200973637
Iter: 1031 loss: 0.00200935476
Iter: 1032 loss: 0.00200908096
Iter: 1033 loss: 0.00200869748
Iter: 1034 loss: 0.00200869655
Iter: 1035 loss: 0.00200834032
Iter: 1036 loss: 0.00200823578
Iter: 1037 loss: 0.00200802437
Iter: 1038 loss: 0.00200702925
Iter: 1039 loss: 0.00201386632
Iter: 1040 loss: 0.00200692192
Iter: 1041 loss: 0.00200673309
Iter: 1042 loss: 0.00200664299
Iter: 1043 loss: 0.00200649397
Iter: 1044 loss: 0.00200641644
Iter: 1045 loss: 0.00200634892
Iter: 1046 loss: 0.00200599153
Iter: 1047 loss: 0.00200613285
Iter: 1048 loss: 0.00200574356
Iter: 1049 loss: 0.00200529909
Iter: 1050 loss: 0.0020051375
Iter: 1051 loss: 0.00200488814
Iter: 1052 loss: 0.00200447184
Iter: 1053 loss: 0.00200619665
Iter: 1054 loss: 0.00200438267
Iter: 1055 loss: 0.00200424413
Iter: 1056 loss: 0.00200399
Iter: 1057 loss: 0.00201021018
Iter: 1058 loss: 0.00200398872
Iter: 1059 loss: 0.00200368511
Iter: 1060 loss: 0.00200476381
Iter: 1061 loss: 0.00200360757
Iter: 1062 loss: 0.00200341549
Iter: 1063 loss: 0.00200327346
Iter: 1064 loss: 0.0020032113
Iter: 1065 loss: 0.00200317265
Iter: 1066 loss: 0.00200310489
Iter: 1067 loss: 0.00200299313
Iter: 1068 loss: 0.00200365135
Iter: 1069 loss: 0.00200297823
Iter: 1070 loss: 0.00200270629
Iter: 1071 loss: 0.00200318312
Iter: 1072 loss: 0.00200258754
Iter: 1073 loss: 0.0020022525
Iter: 1074 loss: 0.00200570864
Iter: 1075 loss: 0.00200224109
Iter: 1076 loss: 0.00200206344
Iter: 1077 loss: 0.00200167485
Iter: 1078 loss: 0.00200743298
Iter: 1079 loss: 0.00200165715
Iter: 1080 loss: 0.00200125203
Iter: 1081 loss: 0.00200324412
Iter: 1082 loss: 0.00200118357
Iter: 1083 loss: 0.00200073863
Iter: 1084 loss: 0.0020001377
Iter: 1085 loss: 0.00200010813
Iter: 1086 loss: 0.00199972768
Iter: 1087 loss: 0.00199955888
Iter: 1088 loss: 0.00199932512
Iter: 1089 loss: 0.00200001826
Iter: 1090 loss: 0.00199925108
Iter: 1091 loss: 0.00199900498
Iter: 1092 loss: 0.00200072862
Iter: 1093 loss: 0.00199898309
Iter: 1094 loss: 0.00199864176
Iter: 1095 loss: 0.00199838029
Iter: 1096 loss: 0.00199826946
Iter: 1097 loss: 0.00199775444
Iter: 1098 loss: 0.00199944573
Iter: 1099 loss: 0.00199761335
Iter: 1100 loss: 0.00199713302
Iter: 1101 loss: 0.00199713348
Iter: 1102 loss: 0.00199693348
Iter: 1103 loss: 0.00199658331
Iter: 1104 loss: 0.00199658307
Iter: 1105 loss: 0.00199596863
Iter: 1106 loss: 0.00199522474
Iter: 1107 loss: 0.00199515279
Iter: 1108 loss: 0.00199463498
Iter: 1109 loss: 0.00199635117
Iter: 1110 loss: 0.00199449435
Iter: 1111 loss: 0.00199397909
Iter: 1112 loss: 0.00199345895
Iter: 1113 loss: 0.00199335814
Iter: 1114 loss: 0.00199288805
Iter: 1115 loss: 0.0019928785
Iter: 1116 loss: 0.0019924934
Iter: 1117 loss: 0.00199483288
Iter: 1118 loss: 0.00199243915
Iter: 1119 loss: 0.00199216325
Iter: 1120 loss: 0.00199189014
Iter: 1121 loss: 0.00199183216
Iter: 1122 loss: 0.0019913218
Iter: 1123 loss: 0.00199872721
Iter: 1124 loss: 0.0019913218
Iter: 1125 loss: 0.00199092226
Iter: 1126 loss: 0.0019917679
Iter: 1127 loss: 0.00199076533
Iter: 1128 loss: 0.00199053017
Iter: 1129 loss: 0.0019905318
Iter: 1130 loss: 0.00199034438
Iter: 1131 loss: 0.00199007336
Iter: 1132 loss: 0.00199120631
Iter: 1133 loss: 0.00199001515
Iter: 1134 loss: 0.00198971247
Iter: 1135 loss: 0.00199157419
Iter: 1136 loss: 0.00198967475
Iter: 1137 loss: 0.00198935
Iter: 1138 loss: 0.0019897786
Iter: 1139 loss: 0.00198918465
Iter: 1140 loss: 0.00198891526
Iter: 1141 loss: 0.00199078606
Iter: 1142 loss: 0.00198889105
Iter: 1143 loss: 0.00198867917
Iter: 1144 loss: 0.00198862096
Iter: 1145 loss: 0.00198849151
Iter: 1146 loss: 0.001988112
Iter: 1147 loss: 0.00198868709
Iter: 1148 loss: 0.00198793132
Iter: 1149 loss: 0.00198715739
Iter: 1150 loss: 0.00199000817
Iter: 1151 loss: 0.00198696065
Iter: 1152 loss: 0.00198626891
Iter: 1153 loss: 0.00199493044
Iter: 1154 loss: 0.00198626309
Iter: 1155 loss: 0.0019859774
Iter: 1156 loss: 0.00198757416
Iter: 1157 loss: 0.00198593782
Iter: 1158 loss: 0.00198552036
Iter: 1159 loss: 0.00198520301
Iter: 1160 loss: 0.00198506773
Iter: 1161 loss: 0.00198444445
Iter: 1162 loss: 0.00198993413
Iter: 1163 loss: 0.00198441371
Iter: 1164 loss: 0.00198407029
Iter: 1165 loss: 0.00198567798
Iter: 1166 loss: 0.00198400416
Iter: 1167 loss: 0.00198357692
Iter: 1168 loss: 0.00198338483
Iter: 1169 loss: 0.00198317133
Iter: 1170 loss: 0.00198271498
Iter: 1171 loss: 0.00198575179
Iter: 1172 loss: 0.00198266888
Iter: 1173 loss: 0.00198216783
Iter: 1174 loss: 0.00198199204
Iter: 1175 loss: 0.00198168401
Iter: 1176 loss: 0.00198088959
Iter: 1177 loss: 0.0019855951
Iter: 1178 loss: 0.00198078807
Iter: 1179 loss: 0.00198030425
Iter: 1180 loss: 0.0019794032
Iter: 1181 loss: 0.00200113
Iter: 1182 loss: 0.00197940157
Iter: 1183 loss: 0.00197874894
Iter: 1184 loss: 0.00198510801
Iter: 1185 loss: 0.00197872752
Iter: 1186 loss: 0.00197827793
Iter: 1187 loss: 0.00197994057
Iter: 1188 loss: 0.0019781685
Iter: 1189 loss: 0.00197779806
Iter: 1190 loss: 0.00197749166
Iter: 1191 loss: 0.00197738013
Iter: 1192 loss: 0.00197896292
Iter: 1193 loss: 0.00197724393
Iter: 1194 loss: 0.00197709794
Iter: 1195 loss: 0.00197790097
Iter: 1196 loss: 0.00197707722
Iter: 1197 loss: 0.00197696313
Iter: 1198 loss: 0.00197665486
Iter: 1199 loss: 0.00197874894
Iter: 1200 loss: 0.00197658129
Iter: 1201 loss: 0.00197606487
Iter: 1202 loss: 0.00197799201
Iter: 1203 loss: 0.00197593682
Iter: 1204 loss: 0.00197649631
Iter: 1205 loss: 0.00197574776
Iter: 1206 loss: 0.001975636
Iter: 1207 loss: 0.00197530724
Iter: 1208 loss: 0.00197663
Iter: 1209 loss: 0.00197517453
Iter: 1210 loss: 0.00197472866
Iter: 1211 loss: 0.00197583437
Iter: 1212 loss: 0.00197457103
Iter: 1213 loss: 0.00197433261
Iter: 1214 loss: 0.00197451445
Iter: 1215 loss: 0.00197418919
Iter: 1216 loss: 0.00197380316
Iter: 1217 loss: 0.00197348068
Iter: 1218 loss: 0.00197337265
Iter: 1219 loss: 0.0019729624
Iter: 1220 loss: 0.0019733347
Iter: 1221 loss: 0.00197272515
Iter: 1222 loss: 0.00197227136
Iter: 1223 loss: 0.00197320268
Iter: 1224 loss: 0.00197208743
Iter: 1225 loss: 0.0019718539
Iter: 1226 loss: 0.00197168626
Iter: 1227 loss: 0.0019716057
Iter: 1228 loss: 0.00197093142
Iter: 1229 loss: 0.00197084062
Iter: 1230 loss: 0.00197036
Iter: 1231 loss: 0.00196873816
Iter: 1232 loss: 0.00197642623
Iter: 1233 loss: 0.00196842686
Iter: 1234 loss: 0.00196820404
Iter: 1235 loss: 0.00196872745
Iter: 1236 loss: 0.00196811859
Iter: 1237 loss: 0.00196785526
Iter: 1238 loss: 0.0019682257
Iter: 1239 loss: 0.00196772581
Iter: 1240 loss: 0.00196724059
Iter: 1241 loss: 0.00196797308
Iter: 1242 loss: 0.00196700986
Iter: 1243 loss: 0.00196664361
Iter: 1244 loss: 0.00196720287
Iter: 1245 loss: 0.00196646946
Iter: 1246 loss: 0.00196617492
Iter: 1247 loss: 0.00196547387
Iter: 1248 loss: 0.00197336823
Iter: 1249 loss: 0.00196540775
Iter: 1250 loss: 0.0019649996
Iter: 1251 loss: 0.00196859799
Iter: 1252 loss: 0.00196497398
Iter: 1253 loss: 0.00196475396
Iter: 1254 loss: 0.00196427293
Iter: 1255 loss: 0.00197144458
Iter: 1256 loss: 0.00196425198
Iter: 1257 loss: 0.0019639337
Iter: 1258 loss: 0.00196384871
Iter: 1259 loss: 0.00196345057
Iter: 1260 loss: 0.00196650485
Iter: 1261 loss: 0.00196341937
Iter: 1262 loss: 0.00196325849
Iter: 1263 loss: 0.00196289551
Iter: 1264 loss: 0.00196781196
Iter: 1265 loss: 0.00196287595
Iter: 1266 loss: 0.00196264917
Iter: 1267 loss: 0.00196347153
Iter: 1268 loss: 0.00196259259
Iter: 1269 loss: 0.00196238188
Iter: 1270 loss: 0.00196212088
Iter: 1271 loss: 0.00196209876
Iter: 1272 loss: 0.00196170388
Iter: 1273 loss: 0.00196335558
Iter: 1274 loss: 0.00196161657
Iter: 1275 loss: 0.00196130387
Iter: 1276 loss: 0.00196130387
Iter: 1277 loss: 0.00196103845
Iter: 1278 loss: 0.00196130294
Iter: 1279 loss: 0.00196088897
Iter: 1280 loss: 0.00196057442
Iter: 1281 loss: 0.00196023821
Iter: 1282 loss: 0.00196018443
Iter: 1283 loss: 0.00195982214
Iter: 1284 loss: 0.00195972854
Iter: 1285 loss: 0.00195950549
Iter: 1286 loss: 0.00195859745
Iter: 1287 loss: 0.00195912062
Iter: 1288 loss: 0.00195799675
Iter: 1289 loss: 0.001957078
Iter: 1290 loss: 0.00196467945
Iter: 1291 loss: 0.00195702561
Iter: 1292 loss: 0.00195644656
Iter: 1293 loss: 0.00195703469
Iter: 1294 loss: 0.00195612386
Iter: 1295 loss: 0.00195554364
Iter: 1296 loss: 0.00195611292
Iter: 1297 loss: 0.00195521815
Iter: 1298 loss: 0.00195479323
Iter: 1299 loss: 0.00195479323
Iter: 1300 loss: 0.00195443537
Iter: 1301 loss: 0.00195462629
Iter: 1302 loss: 0.00195420021
Iter: 1303 loss: 0.0019537129
Iter: 1304 loss: 0.00195444631
Iter: 1305 loss: 0.00195346959
Iter: 1306 loss: 0.00195317436
Iter: 1307 loss: 0.00195317
Iter: 1308 loss: 0.00195302139
Iter: 1309 loss: 0.00195361
Iter: 1310 loss: 0.00195298647
Iter: 1311 loss: 0.00195278018
Iter: 1312 loss: 0.00195249449
Iter: 1313 loss: 0.00195248052
Iter: 1314 loss: 0.00195191195
Iter: 1315 loss: 0.00195238926
Iter: 1316 loss: 0.00195157458
Iter: 1317 loss: 0.00195114757
Iter: 1318 loss: 0.00195086794
Iter: 1319 loss: 0.00195070659
Iter: 1320 loss: 0.0019502308
Iter: 1321 loss: 0.00195022766
Iter: 1322 loss: 0.0019499308
Iter: 1323 loss: 0.00195303594
Iter: 1324 loss: 0.00194992172
Iter: 1325 loss: 0.00194959226
Iter: 1326 loss: 0.001949433
Iter: 1327 loss: 0.00194927584
Iter: 1328 loss: 0.00194889714
Iter: 1329 loss: 0.00194889633
Iter: 1330 loss: 0.00194871589
Iter: 1331 loss: 0.00194877363
Iter: 1332 loss: 0.00194858771
Iter: 1333 loss: 0.0019483238
Iter: 1334 loss: 0.00194945605
Iter: 1335 loss: 0.00194827071
Iter: 1336 loss: 0.0019479529
Iter: 1337 loss: 0.00194799667
Iter: 1338 loss: 0.00194771308
Iter: 1339 loss: 0.00194732018
Iter: 1340 loss: 0.00195173989
Iter: 1341 loss: 0.00194731425
Iter: 1342 loss: 0.00194696
Iter: 1343 loss: 0.00194874301
Iter: 1344 loss: 0.00194689957
Iter: 1345 loss: 0.00194666605
Iter: 1346 loss: 0.00194703974
Iter: 1347 loss: 0.00194656011
Iter: 1348 loss: 0.00194621435
Iter: 1349 loss: 0.00194539281
Iter: 1350 loss: 0.00195479719
Iter: 1351 loss: 0.00194531493
Iter: 1352 loss: 0.00194551155
Iter: 1353 loss: 0.00194490131
Iter: 1354 loss: 0.00194462133
Iter: 1355 loss: 0.00194391399
Iter: 1356 loss: 0.00195029
Iter: 1357 loss: 0.0019438006
Iter: 1358 loss: 0.00194375054
Iter: 1359 loss: 0.00194351678
Iter: 1360 loss: 0.00194319268
Iter: 1361 loss: 0.00194379431
Iter: 1362 loss: 0.00194305298
Iter: 1363 loss: 0.00194277056
Iter: 1364 loss: 0.00194270047
Iter: 1365 loss: 0.00194252445
Iter: 1366 loss: 0.00194257428
Iter: 1367 loss: 0.00194239826
Iter: 1368 loss: 0.00194230466
Iter: 1369 loss: 0.00194246823
Iter: 1370 loss: 0.00194226485
Iter: 1371 loss: 0.00194215111
Iter: 1372 loss: 0.00194182666
Iter: 1373 loss: 0.00194333179
Iter: 1374 loss: 0.00194171246
Iter: 1375 loss: 0.00194078218
Iter: 1376 loss: 0.00194828259
Iter: 1377 loss: 0.00194072071
Iter: 1378 loss: 0.00194058812
Iter: 1379 loss: 0.00194043573
Iter: 1380 loss: 0.0019402192
Iter: 1381 loss: 0.00193976308
Iter: 1382 loss: 0.00194746023
Iter: 1383 loss: 0.00193975121
Iter: 1384 loss: 0.0019392285
Iter: 1385 loss: 0.00194373261
Iter: 1386 loss: 0.00193920033
Iter: 1387 loss: 0.00193881337
Iter: 1388 loss: 0.00194025226
Iter: 1389 loss: 0.00193871895
Iter: 1390 loss: 0.00193844829
Iter: 1391 loss: 0.00193823804
Iter: 1392 loss: 0.0019381512
Iter: 1393 loss: 0.00193791441
Iter: 1394 loss: 0.00193787669
Iter: 1395 loss: 0.00193764898
Iter: 1396 loss: 0.00193773827
Iter: 1397 loss: 0.00193749578
Iter: 1398 loss: 0.00193670462
Iter: 1399 loss: 0.00194381131
Iter: 1400 loss: 0.00193665642
Iter: 1401 loss: 0.00193633605
Iter: 1402 loss: 0.00193618832
Iter: 1403 loss: 0.00193603721
Iter: 1404 loss: 0.00193596398
Iter: 1405 loss: 0.00193589239
Iter: 1406 loss: 0.00193562265
Iter: 1407 loss: 0.00193519169
Iter: 1408 loss: 0.00193518703
Iter: 1409 loss: 0.00193475292
Iter: 1410 loss: 0.00193644874
Iter: 1411 loss: 0.00193465222
Iter: 1412 loss: 0.00193438143
Iter: 1413 loss: 0.00193503872
Iter: 1414 loss: 0.00193428504
Iter: 1415 loss: 0.0019339819
Iter: 1416 loss: 0.00193411275
Iter: 1417 loss: 0.00193377247
Iter: 1418 loss: 0.00193337363
Iter: 1419 loss: 0.00193336757
Iter: 1420 loss: 0.00193308131
Iter: 1421 loss: 0.00193285779
Iter: 1422 loss: 0.00193277071
Iter: 1423 loss: 0.00193237211
Iter: 1424 loss: 0.00193253579
Iter: 1425 loss: 0.00193209259
Iter: 1426 loss: 0.00193201774
Iter: 1427 loss: 0.00193181494
Iter: 1428 loss: 0.00193147373
Iter: 1429 loss: 0.00193178304
Iter: 1430 loss: 0.00193127699
Iter: 1431 loss: 0.00193109515
Iter: 1432 loss: 0.00193302031
Iter: 1433 loss: 0.00193109154
Iter: 1434 loss: 0.0019309005
Iter: 1435 loss: 0.00193041144
Iter: 1436 loss: 0.00193480658
Iter: 1437 loss: 0.00193033041
Iter: 1438 loss: 0.00193018583
Iter: 1439 loss: 0.00193000201
Iter: 1440 loss: 0.00192960049
Iter: 1441 loss: 0.00193157443
Iter: 1442 loss: 0.00192952948
Iter: 1443 loss: 0.001929341
Iter: 1444 loss: 0.00192903588
Iter: 1445 loss: 0.00192903331
Iter: 1446 loss: 0.00192868407
Iter: 1447 loss: 0.00193284487
Iter: 1448 loss: 0.00192867895
Iter: 1449 loss: 0.00192843191
Iter: 1450 loss: 0.00193051482
Iter: 1451 loss: 0.00192841992
Iter: 1452 loss: 0.00192818977
Iter: 1453 loss: 0.00192859978
Iter: 1454 loss: 0.00192808663
Iter: 1455 loss: 0.00192778162
Iter: 1456 loss: 0.00192717009
Iter: 1457 loss: 0.00193939463
Iter: 1458 loss: 0.00192716322
Iter: 1459 loss: 0.00192661048
Iter: 1460 loss: 0.00192988315
Iter: 1461 loss: 0.00192654366
Iter: 1462 loss: 0.00192620489
Iter: 1463 loss: 0.00192935683
Iter: 1464 loss: 0.00192619022
Iter: 1465 loss: 0.00192590093
Iter: 1466 loss: 0.00192543713
Iter: 1467 loss: 0.00192543375
Iter: 1468 loss: 0.00192483363
Iter: 1469 loss: 0.00193054148
Iter: 1470 loss: 0.00192480977
Iter: 1471 loss: 0.00192448031
Iter: 1472 loss: 0.00192443212
Iter: 1473 loss: 0.0019242008
Iter: 1474 loss: 0.00192349509
Iter: 1475 loss: 0.00192581047
Iter: 1476 loss: 0.00192329194
Iter: 1477 loss: 0.00192284922
Iter: 1478 loss: 0.00192935858
Iter: 1479 loss: 0.00192284887
Iter: 1480 loss: 0.00192262453
Iter: 1481 loss: 0.00192213757
Iter: 1482 loss: 0.00192930433
Iter: 1483 loss: 0.0019221158
Iter: 1484 loss: 0.00192166655
Iter: 1485 loss: 0.00192282477
Iter: 1486 loss: 0.00192151254
Iter: 1487 loss: 0.00192114
Iter: 1488 loss: 0.0019211286
Iter: 1489 loss: 0.00192082976
Iter: 1490 loss: 0.0019200861
Iter: 1491 loss: 0.00192727847
Iter: 1492 loss: 0.00191998179
Iter: 1493 loss: 0.00191945466
Iter: 1494 loss: 0.00192749233
Iter: 1495 loss: 0.00191945548
Iter: 1496 loss: 0.00191909715
Iter: 1497 loss: 0.00191840134
Iter: 1498 loss: 0.00193306245
Iter: 1499 loss: 0.00191839738
Iter: 1500 loss: 0.00191764452
Iter: 1501 loss: 0.00192180835
Iter: 1502 loss: 0.00191751402
Iter: 1503 loss: 0.00191708328
Iter: 1504 loss: 0.00191707211
Iter: 1505 loss: 0.00191686838
Iter: 1506 loss: 0.00191670435
Iter: 1507 loss: 0.00191664265
Iter: 1508 loss: 0.00191624474
Iter: 1509 loss: 0.00191806618
Iter: 1510 loss: 0.00191616814
Iter: 1511 loss: 0.00191582984
Iter: 1512 loss: 0.00191707746
Iter: 1513 loss: 0.00191574404
Iter: 1514 loss: 0.001915353
Iter: 1515 loss: 0.00191532925
Iter: 1516 loss: 0.00191503321
Iter: 1517 loss: 0.00191476697
Iter: 1518 loss: 0.00191523321
Iter: 1519 loss: 0.00191464822
Iter: 1520 loss: 0.00191431399
Iter: 1521 loss: 0.00191428477
Iter: 1522 loss: 0.0019140389
Iter: 1523 loss: 0.00191348977
Iter: 1524 loss: 0.00191689027
Iter: 1525 loss: 0.00191342877
Iter: 1526 loss: 0.00191308581
Iter: 1527 loss: 0.0019147112
Iter: 1528 loss: 0.001913024
Iter: 1529 loss: 0.00191279757
Iter: 1530 loss: 0.00191524101
Iter: 1531 loss: 0.00191279361
Iter: 1532 loss: 0.00191261922
Iter: 1533 loss: 0.00191237777
Iter: 1534 loss: 0.00191236753
Iter: 1535 loss: 0.00191209256
Iter: 1536 loss: 0.00191151933
Iter: 1537 loss: 0.00192129367
Iter: 1538 loss: 0.00191150466
Iter: 1539 loss: 0.00191115751
Iter: 1540 loss: 0.00191122247
Iter: 1541 loss: 0.00191089907
Iter: 1542 loss: 0.00191041478
Iter: 1543 loss: 0.00190991256
Iter: 1544 loss: 0.00190981408
Iter: 1545 loss: 0.00190945773
Iter: 1546 loss: 0.00190928706
Iter: 1547 loss: 0.00190901919
Iter: 1548 loss: 0.00190993352
Iter: 1549 loss: 0.00190894539
Iter: 1550 loss: 0.00190862152
Iter: 1551 loss: 0.0019078193
Iter: 1552 loss: 0.00191584136
Iter: 1553 loss: 0.00190771557
Iter: 1554 loss: 0.00190732023
Iter: 1555 loss: 0.00190717261
Iter: 1556 loss: 0.00190687564
Iter: 1557 loss: 0.00190684339
Iter: 1558 loss: 0.00190661545
Iter: 1559 loss: 0.00190560496
Iter: 1560 loss: 0.00190847903
Iter: 1561 loss: 0.00190529088
Iter: 1562 loss: 0.00190613093
Iter: 1563 loss: 0.00190475
Iter: 1564 loss: 0.00190438516
Iter: 1565 loss: 0.0019043386
Iter: 1566 loss: 0.00190407806
Iter: 1567 loss: 0.00190349831
Iter: 1568 loss: 0.00190648134
Iter: 1569 loss: 0.0019034039
Iter: 1570 loss: 0.00190303824
Iter: 1571 loss: 0.00190544943
Iter: 1572 loss: 0.00190300099
Iter: 1573 loss: 0.00190271321
Iter: 1574 loss: 0.00190223416
Iter: 1575 loss: 0.00190223311
Iter: 1576 loss: 0.00190181169
Iter: 1577 loss: 0.00190412649
Iter: 1578 loss: 0.00190175674
Iter: 1579 loss: 0.00190167199
Iter: 1580 loss: 0.00190146232
Iter: 1581 loss: 0.00190106942
Iter: 1582 loss: 0.00190020399
Iter: 1583 loss: 0.0019133559
Iter: 1584 loss: 0.00190016662
Iter: 1585 loss: 0.00189961237
Iter: 1586 loss: 0.00190038327
Iter: 1587 loss: 0.00189933868
Iter: 1588 loss: 0.0018987587
Iter: 1589 loss: 0.0019040613
Iter: 1590 loss: 0.00189873343
Iter: 1591 loss: 0.00189841504
Iter: 1592 loss: 0.00189936161
Iter: 1593 loss: 0.00189831795
Iter: 1594 loss: 0.00189809571
Iter: 1595 loss: 0.00189771794
Iter: 1596 loss: 0.00189771759
Iter: 1597 loss: 0.00189728686
Iter: 1598 loss: 0.0019018146
Iter: 1599 loss: 0.0018972758
Iter: 1600 loss: 0.00189703552
Iter: 1601 loss: 0.00189685635
Iter: 1602 loss: 0.00189677707
Iter: 1603 loss: 0.00189679768
Iter: 1604 loss: 0.00189663982
Iter: 1605 loss: 0.00189648115
Iter: 1606 loss: 0.00189654867
Iter: 1607 loss: 0.00189637137
Iter: 1608 loss: 0.00189617905
Iter: 1609 loss: 0.00189803087
Iter: 1610 loss: 0.00189617183
Iter: 1611 loss: 0.00189595378
Iter: 1612 loss: 0.00189529604
Iter: 1613 loss: 0.00189738639
Iter: 1614 loss: 0.00189497531
Iter: 1615 loss: 0.00189455529
Iter: 1616 loss: 0.00189447112
Iter: 1617 loss: 0.00189422874
Iter: 1618 loss: 0.00189410173
Iter: 1619 loss: 0.00189399021
Iter: 1620 loss: 0.00189367868
Iter: 1621 loss: 0.00189429452
Iter: 1622 loss: 0.00189354911
Iter: 1623 loss: 0.00189339765
Iter: 1624 loss: 0.00189462223
Iter: 1625 loss: 0.00189338718
Iter: 1626 loss: 0.00189321232
Iter: 1627 loss: 0.00189288775
Iter: 1628 loss: 0.00190028688
Iter: 1629 loss: 0.00189288729
Iter: 1630 loss: 0.00189264258
Iter: 1631 loss: 0.00189328974
Iter: 1632 loss: 0.00189255981
Iter: 1633 loss: 0.0018924328
Iter: 1634 loss: 0.00189220393
Iter: 1635 loss: 0.00189220358
Iter: 1636 loss: 0.00189179252
Iter: 1637 loss: 0.00189331721
Iter: 1638 loss: 0.00189169508
Iter: 1639 loss: 0.0018913633
Iter: 1640 loss: 0.00189510407
Iter: 1641 loss: 0.00189135596
Iter: 1642 loss: 0.00189112616
Iter: 1643 loss: 0.00189169194
Iter: 1644 loss: 0.00189104443
Iter: 1645 loss: 0.00189085398
Iter: 1646 loss: 0.00189065142
Iter: 1647 loss: 0.0018906187
Iter: 1648 loss: 0.00189043744
Iter: 1649 loss: 0.00189154083
Iter: 1650 loss: 0.00189041579
Iter: 1651 loss: 0.00189025304
Iter: 1652 loss: 0.00189018366
Iter: 1653 loss: 0.00189010019
Iter: 1654 loss: 0.00188993849
Iter: 1655 loss: 0.00188993406
Iter: 1656 loss: 0.00188976165
Iter: 1657 loss: 0.00188931986
Iter: 1658 loss: 0.00189304678
Iter: 1659 loss: 0.00188924489
Iter: 1660 loss: 0.00188858307
Iter: 1661 loss: 0.00189055433
Iter: 1662 loss: 0.00188838
Iter: 1663 loss: 0.00188804185
Iter: 1664 loss: 0.00188789202
Iter: 1665 loss: 0.00188772148
Iter: 1666 loss: 0.00188708934
Iter: 1667 loss: 0.00188777922
Iter: 1668 loss: 0.00188674335
Iter: 1669 loss: 0.00188651
Iter: 1670 loss: 0.00188617117
Iter: 1671 loss: 0.00188616093
Iter: 1672 loss: 0.00188583985
Iter: 1673 loss: 0.00188833731
Iter: 1674 loss: 0.00188581494
Iter: 1675 loss: 0.00188557722
Iter: 1676 loss: 0.00188602367
Iter: 1677 loss: 0.00188547734
Iter: 1678 loss: 0.00188514253
Iter: 1679 loss: 0.0018861487
Iter: 1680 loss: 0.0018850395
Iter: 1681 loss: 0.00188450096
Iter: 1682 loss: 0.00188552355
Iter: 1683 loss: 0.00188427151
Iter: 1684 loss: 0.00188387337
Iter: 1685 loss: 0.00188419921
Iter: 1686 loss: 0.00188363483
Iter: 1687 loss: 0.00188333157
Iter: 1688 loss: 0.0018831403
Iter: 1689 loss: 0.0018830183
Iter: 1690 loss: 0.00188269699
Iter: 1691 loss: 0.00188264344
Iter: 1692 loss: 0.00188244041
Iter: 1693 loss: 0.00188284833
Iter: 1694 loss: 0.00188235776
Iter: 1695 loss: 0.00188216311
Iter: 1696 loss: 0.00188200397
Iter: 1697 loss: 0.00188194681
Iter: 1698 loss: 0.001881551
Iter: 1699 loss: 0.00188270723
Iter: 1700 loss: 0.00188143132
Iter: 1701 loss: 0.00188116659
Iter: 1702 loss: 0.00188270328
Iter: 1703 loss: 0.00188113179
Iter: 1704 loss: 0.00188092375
Iter: 1705 loss: 0.00188065006
Iter: 1706 loss: 0.00188063295
Iter: 1707 loss: 0.001880439
Iter: 1708 loss: 0.00188039057
Iter: 1709 loss: 0.00188019732
Iter: 1710 loss: 0.00188036927
Iter: 1711 loss: 0.00188008277
Iter: 1712 loss: 0.00187978568
Iter: 1713 loss: 0.00188046577
Iter: 1714 loss: 0.00187967
Iter: 1715 loss: 0.00187942095
Iter: 1716 loss: 0.00188062142
Iter: 1717 loss: 0.00187937636
Iter: 1718 loss: 0.00187919475
Iter: 1719 loss: 0.00187955215
Iter: 1720 loss: 0.00187911815
Iter: 1721 loss: 0.00187890837
Iter: 1722 loss: 0.00187886762
Iter: 1723 loss: 0.00187872758
Iter: 1724 loss: 0.0018784767
Iter: 1725 loss: 0.0018784753
Iter: 1726 loss: 0.00187833607
Iter: 1727 loss: 0.00187817274
Iter: 1728 loss: 0.00187815563
Iter: 1729 loss: 0.00187795423
Iter: 1730 loss: 0.00187912537
Iter: 1731 loss: 0.00187792839
Iter: 1732 loss: 0.00187771698
Iter: 1733 loss: 0.00187719241
Iter: 1734 loss: 0.00188231119
Iter: 1735 loss: 0.00187712268
Iter: 1736 loss: 0.00187667226
Iter: 1737 loss: 0.00187652954
Iter: 1738 loss: 0.00187620881
Iter: 1739 loss: 0.00187657238
Iter: 1740 loss: 0.00187603501
Iter: 1741 loss: 0.00187568169
Iter: 1742 loss: 0.00187615037
Iter: 1743 loss: 0.00187550369
Iter: 1744 loss: 0.00187518774
Iter: 1745 loss: 0.00187549787
Iter: 1746 loss: 0.00187500904
Iter: 1747 loss: 0.00187469437
Iter: 1748 loss: 0.00187662139
Iter: 1749 loss: 0.001874657
Iter: 1750 loss: 0.00187435886
Iter: 1751 loss: 0.00187435502
Iter: 1752 loss: 0.00187424244
Iter: 1753 loss: 0.00187406747
Iter: 1754 loss: 0.00187406456
Iter: 1755 loss: 0.00187389087
Iter: 1756 loss: 0.00187388831
Iter: 1757 loss: 0.00187365164
Iter: 1758 loss: 0.00187344884
Iter: 1759 loss: 0.00187338272
Iter: 1760 loss: 0.00187363557
Iter: 1761 loss: 0.0018732358
Iter: 1762 loss: 0.0018731025
Iter: 1763 loss: 0.00187318167
Iter: 1764 loss: 0.00187301647
Iter: 1765 loss: 0.0018728436
Iter: 1766 loss: 0.00187328458
Iter: 1767 loss: 0.00187278376
Iter: 1768 loss: 0.00187262031
Iter: 1769 loss: 0.00187261379
Iter: 1770 loss: 0.00187248772
Iter: 1771 loss: 0.00187219935
Iter: 1772 loss: 0.00187335804
Iter: 1773 loss: 0.00187213346
Iter: 1774 loss: 0.00187195267
Iter: 1775 loss: 0.00187356281
Iter: 1776 loss: 0.00187194464
Iter: 1777 loss: 0.00187173579
Iter: 1778 loss: 0.0018719798
Iter: 1779 loss: 0.00187162426
Iter: 1780 loss: 0.00187138142
Iter: 1781 loss: 0.00187207828
Iter: 1782 loss: 0.00187130447
Iter: 1783 loss: 0.0018710444
Iter: 1784 loss: 0.00187404291
Iter: 1785 loss: 0.00187104056
Iter: 1786 loss: 0.00187079073
Iter: 1787 loss: 0.00187059026
Iter: 1788 loss: 0.00187051552
Iter: 1789 loss: 0.00187014835
Iter: 1790 loss: 0.00187514257
Iter: 1791 loss: 0.00187014625
Iter: 1792 loss: 0.00186999934
Iter: 1793 loss: 0.00186969666
Iter: 1794 loss: 0.00187515141
Iter: 1795 loss: 0.0018696913
Iter: 1796 loss: 0.00186966686
Iter: 1797 loss: 0.00186947861
Iter: 1798 loss: 0.00186929724
Iter: 1799 loss: 0.00186941121
Iter: 1800 loss: 0.00186918187
Iter: 1801 loss: 0.0018688282
Iter: 1802 loss: 0.00186871644
Iter: 1803 loss: 0.00186850829
Iter: 1804 loss: 0.0018682559
Iter: 1805 loss: 0.0018682396
Iter: 1806 loss: 0.00186796091
Iter: 1807 loss: 0.00186765776
Iter: 1808 loss: 0.00186761026
Iter: 1809 loss: 0.00186742865
Iter: 1810 loss: 0.00186739792
Iter: 1811 loss: 0.00186724216
Iter: 1812 loss: 0.0018668836
Iter: 1813 loss: 0.0018714054
Iter: 1814 loss: 0.0018668574
Iter: 1815 loss: 0.0018667354
Iter: 1816 loss: 0.00186661852
Iter: 1817 loss: 0.00186639628
Iter: 1818 loss: 0.00186733739
Iter: 1819 loss: 0.00186634948
Iter: 1820 loss: 0.00186617242
Iter: 1821 loss: 0.00186658511
Iter: 1822 loss: 0.00186610618
Iter: 1823 loss: 0.00186581956
Iter: 1824 loss: 0.0018658425
Iter: 1825 loss: 0.00186559849
Iter: 1826 loss: 0.00186540862
Iter: 1827 loss: 0.00186626404
Iter: 1828 loss: 0.00186537125
Iter: 1829 loss: 0.00186513865
Iter: 1830 loss: 0.00186560664
Iter: 1831 loss: 0.00186504424
Iter: 1832 loss: 0.00186489918
Iter: 1833 loss: 0.00186512549
Iter: 1834 loss: 0.00186483
Iter: 1835 loss: 0.00186456041
Iter: 1836 loss: 0.00186433771
Iter: 1837 loss: 0.00186425867
Iter: 1838 loss: 0.00186420023
Iter: 1839 loss: 0.00186415343
Iter: 1840 loss: 0.0018640568
Iter: 1841 loss: 0.00186405692
Iter: 1842 loss: 0.0018639802
Iter: 1843 loss: 0.00186387403
Iter: 1844 loss: 0.00186490081
Iter: 1845 loss: 0.00186387112
Iter: 1846 loss: 0.00186380511
Iter: 1847 loss: 0.00186362979
Iter: 1848 loss: 0.00186479301
Iter: 1849 loss: 0.00186358753
Iter: 1850 loss: 0.0018633618
Iter: 1851 loss: 0.00186310126
Iter: 1852 loss: 0.00186307007
Iter: 1853 loss: 0.00186310231
Iter: 1854 loss: 0.00186296017
Iter: 1855 loss: 0.00186289707
Iter: 1856 loss: 0.00186276017
Iter: 1857 loss: 0.0018648773
Iter: 1858 loss: 0.00186275574
Iter: 1859 loss: 0.00186258578
Iter: 1860 loss: 0.00186365144
Iter: 1861 loss: 0.0018625668
Iter: 1862 loss: 0.001862459
Iter: 1863 loss: 0.00186243816
Iter: 1864 loss: 0.00186236831
Iter: 1865 loss: 0.00186223909
Iter: 1866 loss: 0.00186320778
Iter: 1867 loss: 0.00186222955
Iter: 1868 loss: 0.00186209485
Iter: 1869 loss: 0.00186199555
Iter: 1870 loss: 0.00186195015
Iter: 1871 loss: 0.00186173979
Iter: 1872 loss: 0.00186209555
Iter: 1873 loss: 0.00186164444
Iter: 1874 loss: 0.00186153466
Iter: 1875 loss: 0.00186153431
Iter: 1876 loss: 0.00186142209
Iter: 1877 loss: 0.00186130032
Iter: 1878 loss: 0.00186128169
Iter: 1879 loss: 0.00186109834
Iter: 1880 loss: 0.00186164514
Iter: 1881 loss: 0.00186104188
Iter: 1882 loss: 0.00186088786
Iter: 1883 loss: 0.00186069962
Iter: 1884 loss: 0.00186068297
Iter: 1885 loss: 0.00186063873
Iter: 1886 loss: 0.00186052592
Iter: 1887 loss: 0.00186038157
Iter: 1888 loss: 0.00186012452
Iter: 1889 loss: 0.00186012615
Iter: 1890 loss: 0.00186000159
Iter: 1891 loss: 0.00186045631
Iter: 1892 loss: 0.00185997051
Iter: 1893 loss: 0.00185984606
Iter: 1894 loss: 0.00186057517
Iter: 1895 loss: 0.0018598293
Iter: 1896 loss: 0.00185973989
Iter: 1897 loss: 0.00185971381
Iter: 1898 loss: 0.00185966073
Iter: 1899 loss: 0.00185948354
Iter: 1900 loss: 0.00185959646
Iter: 1901 loss: 0.00185937132
Iter: 1902 loss: 0.00185908435
Iter: 1903 loss: 0.0018590712
Iter: 1904 loss: 0.00185885129
Iter: 1905 loss: 0.00185844116
Iter: 1906 loss: 0.00185998366
Iter: 1907 loss: 0.00185834209
Iter: 1908 loss: 0.00185800786
Iter: 1909 loss: 0.00186282711
Iter: 1910 loss: 0.00185800612
Iter: 1911 loss: 0.00185785815
Iter: 1912 loss: 0.00185790774
Iter: 1913 loss: 0.00185775186
Iter: 1914 loss: 0.00185755442
Iter: 1915 loss: 0.00185737724
Iter: 1916 loss: 0.00185732706
Iter: 1917 loss: 0.00185733917
Iter: 1918 loss: 0.00185721205
Iter: 1919 loss: 0.00185707014
Iter: 1920 loss: 0.00185700099
Iter: 1921 loss: 0.00185693312
Iter: 1922 loss: 0.00185671239
Iter: 1923 loss: 0.00185623753
Iter: 1924 loss: 0.00186351431
Iter: 1925 loss: 0.00185622019
Iter: 1926 loss: 0.00185618107
Iter: 1927 loss: 0.00185605022
Iter: 1928 loss: 0.00185587292
Iter: 1929 loss: 0.00185679726
Iter: 1930 loss: 0.00185584428
Iter: 1931 loss: 0.00185572356
Iter: 1932 loss: 0.00185570261
Iter: 1933 loss: 0.00185561972
Iter: 1934 loss: 0.00185539795
Iter: 1935 loss: 0.00185528409
Iter: 1936 loss: 0.00185518176
Iter: 1937 loss: 0.00185510446
Iter: 1938 loss: 0.00185500761
Iter: 1939 loss: 0.00185486535
Iter: 1940 loss: 0.00185463484
Iter: 1941 loss: 0.00185463345
Iter: 1942 loss: 0.00185444683
Iter: 1943 loss: 0.00185527396
Iter: 1944 loss: 0.00185441272
Iter: 1945 loss: 0.0018543076
Iter: 1946 loss: 0.00185430539
Iter: 1947 loss: 0.00185425126
Iter: 1948 loss: 0.00185412122
Iter: 1949 loss: 0.00185554475
