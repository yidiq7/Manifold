+ RUN=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ LAYERS=300_300_300_1
+ case $RUN in
+ PSI=0
+ OPTIONS='			 --optimizer adam 				 --n_pairs 50000 				 --batch_size 5000 				 --max_epochs 200 				 --learning_rate 0.001 				 --decay_rate 0.98 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output78
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output79
+ for fn in f1
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL='--load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi0
+ date
Sat Oct 31 17:27:37 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 0 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f539026d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53902a71e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53902c6bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53903406a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5390322158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5390210b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53901a7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53901a00d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53901baae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f539015f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5344760048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f534476ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53900be510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53900c4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53446cc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53446e8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53446f8ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53447ba8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5344639378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5344663d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5390060840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f539007e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53445bfd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53445aa7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f534458b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53445b7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f539003f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53444f91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5344520bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53444cb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53444e9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53444e7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f534471d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53444540d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f534446cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f534447c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0012109627
test_loss: 0.0013207352
train_loss: 0.001022386
test_loss: 0.0011087103
train_loss: 0.000932
test_loss: 0.0010531497
train_loss: 0.0010635486
test_loss: 0.0010331223
train_loss: 0.000953815
test_loss: 0.0009358203
train_loss: 0.0008430301
test_loss: 0.00085881766
train_loss: 0.0008444118
test_loss: 0.00089274294
train_loss: 0.00074097514
test_loss: 0.00086717174
train_loss: 0.0008000602
test_loss: 0.00076578354
train_loss: 0.00070192694
test_loss: 0.0007381217
train_loss: 0.00069650274
test_loss: 0.00075587316
train_loss: 0.0007144218
test_loss: 0.00075588113
train_loss: 0.0006827703
test_loss: 0.00073079683
train_loss: 0.00069545646
test_loss: 0.00071214914
train_loss: 0.0006728821
test_loss: 0.0007173347
train_loss: 0.000648959
test_loss: 0.0007052241
train_loss: 0.00062608975
test_loss: 0.00072807487
train_loss: 0.00063112716
test_loss: 0.0006914363
train_loss: 0.00063349225
test_loss: 0.0006947203
train_loss: 0.0006106711
test_loss: 0.00067826145
train_loss: 0.00060789083
test_loss: 0.0006749251
train_loss: 0.0006100092
test_loss: 0.00067952235
train_loss: 0.000599505
test_loss: 0.0006749557
train_loss: 0.00059851754
test_loss: 0.00066751183
train_loss: 0.00057080516
test_loss: 0.0006606057
train_loss: 0.0005911764
test_loss: 0.0006649025
train_loss: 0.0005725426
test_loss: 0.00066967314
train_loss: 0.00057359645
test_loss: 0.000664252
train_loss: 0.0005798985
test_loss: 0.0006637376
train_loss: 0.00057440327
test_loss: 0.00066028605
train_loss: 0.00056452514
test_loss: 0.00065481773
train_loss: 0.00056454784
test_loss: 0.0006609783
train_loss: 0.00057409605
test_loss: 0.00065824366
train_loss: 0.0005657269
test_loss: 0.0006554563
train_loss: 0.0005597664
test_loss: 0.0006534343
train_loss: 0.0005734533
test_loss: 0.0006542121
train_loss: 0.0005673688
test_loss: 0.000649688
train_loss: 0.0005654747
test_loss: 0.0006498343
train_loss: 0.0005499785
test_loss: 0.0006493902
train_loss: 0.0005644428
test_loss: 0.0006494614
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi0/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43bde9b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43bdd41268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43bdd4bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43bde27730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43bde321e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43bde51bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43bddaa6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43bdd92158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43bdcb1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43bdcbe620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f439500e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4394ffaae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4394fca598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4394fcc048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43bdcfea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4394f3e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4394f21f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4394ef6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4394eed400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4394f07e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43707ab8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4394f62378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4394f6ed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4370718840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f437071e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43706ebd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f437069a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43706b8268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43706a5c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f437076a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f437062e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f437062cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43705f36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4370608158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43705fdb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43705a0620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.35044557e-07
Iter: 2 loss: 6.31133503e-07
Iter: 3 loss: 6.30322575e-07
Iter: 4 loss: 6.27991881e-07
Iter: 5 loss: 6.29093961e-07
Iter: 6 loss: 6.26440055e-07
Iter: 7 loss: 6.25311543e-07
Iter: 8 loss: 6.42598593e-07
Iter: 9 loss: 6.25306541e-07
Iter: 10 loss: 6.24319341e-07
Iter: 11 loss: 6.22846528e-07
Iter: 12 loss: 6.22818106e-07
Iter: 13 loss: 6.21065396e-07
Iter: 14 loss: 6.25443704e-07
Iter: 15 loss: 6.20451488e-07
Iter: 16 loss: 6.19726848e-07
Iter: 17 loss: 6.19717639e-07
Iter: 18 loss: 6.18830313e-07
Iter: 19 loss: 6.16243142e-07
Iter: 20 loss: 6.26199039e-07
Iter: 21 loss: 6.15158797e-07
Iter: 22 loss: 6.13565135e-07
Iter: 23 loss: 6.29137787e-07
Iter: 24 loss: 6.13505449e-07
Iter: 25 loss: 6.13306611e-07
Iter: 26 loss: 6.12989311e-07
Iter: 27 loss: 6.12545e-07
Iter: 28 loss: 6.11389169e-07
Iter: 29 loss: 6.22084201e-07
Iter: 30 loss: 6.11234213e-07
Iter: 31 loss: 6.10054485e-07
Iter: 32 loss: 6.09151698e-07
Iter: 33 loss: 6.08786081e-07
Iter: 34 loss: 6.06910305e-07
Iter: 35 loss: 6.12929341e-07
Iter: 36 loss: 6.06376489e-07
Iter: 37 loss: 6.05035098e-07
Iter: 38 loss: 6.20730589e-07
Iter: 39 loss: 6.05017647e-07
Iter: 40 loss: 6.04762761e-07
Iter: 41 loss: 6.04686477e-07
Iter: 42 loss: 6.04246281e-07
Iter: 43 loss: 6.0368609e-07
Iter: 44 loss: 6.03647322e-07
Iter: 45 loss: 6.03066951e-07
Iter: 46 loss: 6.07526545e-07
Iter: 47 loss: 6.03025285e-07
Iter: 48 loss: 6.02229534e-07
Iter: 49 loss: 6.00326e-07
Iter: 50 loss: 6.209616e-07
Iter: 51 loss: 6.00139515e-07
Iter: 52 loss: 5.98769304e-07
Iter: 53 loss: 6.04697107e-07
Iter: 54 loss: 5.98501913e-07
Iter: 55 loss: 5.97274e-07
Iter: 56 loss: 5.97279154e-07
Iter: 57 loss: 5.96842483e-07
Iter: 58 loss: 5.96952475e-07
Iter: 59 loss: 5.96539167e-07
Iter: 60 loss: 5.96115228e-07
Iter: 61 loss: 5.9684578e-07
Iter: 62 loss: 5.95924064e-07
Iter: 63 loss: 5.95687766e-07
Iter: 64 loss: 5.98274482e-07
Iter: 65 loss: 5.9569004e-07
Iter: 66 loss: 5.95378424e-07
Iter: 67 loss: 5.95069821e-07
Iter: 68 loss: 5.95005758e-07
Iter: 69 loss: 5.94530434e-07
Iter: 70 loss: 5.93656182e-07
Iter: 71 loss: 6.14074168e-07
Iter: 72 loss: 5.93662605e-07
Iter: 73 loss: 5.92763342e-07
Iter: 74 loss: 5.96309178e-07
Iter: 75 loss: 5.92552169e-07
Iter: 76 loss: 5.92564561e-07
Iter: 77 loss: 5.92128288e-07
Iter: 78 loss: 5.91960543e-07
Iter: 79 loss: 5.91659614e-07
Iter: 80 loss: 5.98849056e-07
Iter: 81 loss: 5.91667913e-07
Iter: 82 loss: 5.91303888e-07
Iter: 83 loss: 5.95673384e-07
Iter: 84 loss: 5.91301443e-07
Iter: 85 loss: 5.91130515e-07
Iter: 86 loss: 5.9069157e-07
Iter: 87 loss: 5.93801701e-07
Iter: 88 loss: 5.90593686e-07
Iter: 89 loss: 5.90476077e-07
Iter: 90 loss: 5.9037086e-07
Iter: 91 loss: 5.90129901e-07
Iter: 92 loss: 5.89903038e-07
Iter: 93 loss: 5.89845456e-07
Iter: 94 loss: 5.89694878e-07
Iter: 95 loss: 5.90549917e-07
Iter: 96 loss: 5.89661227e-07
Iter: 97 loss: 5.89534352e-07
Iter: 98 loss: 5.89392073e-07
Iter: 99 loss: 5.89369279e-07
Iter: 100 loss: 5.89229558e-07
Iter: 101 loss: 5.89230694e-07
Iter: 102 loss: 5.89055276e-07
Iter: 103 loss: 5.88794592e-07
Iter: 104 loss: 5.88797434e-07
Iter: 105 loss: 5.88437558e-07
Iter: 106 loss: 5.87964337e-07
Iter: 107 loss: 5.87925058e-07
Iter: 108 loss: 5.87142608e-07
Iter: 109 loss: 5.89907e-07
Iter: 110 loss: 5.86929332e-07
Iter: 111 loss: 5.87614068e-07
Iter: 112 loss: 5.86800184e-07
Iter: 113 loss: 5.86674616e-07
Iter: 114 loss: 5.86451506e-07
Iter: 115 loss: 5.86447811e-07
Iter: 116 loss: 5.86343901e-07
Iter: 117 loss: 5.86332362e-07
Iter: 118 loss: 5.86226349e-07
Iter: 119 loss: 5.85956741e-07
Iter: 120 loss: 5.881692e-07
Iter: 121 loss: 5.85915473e-07
Iter: 122 loss: 5.85610394e-07
Iter: 123 loss: 5.86512215e-07
Iter: 124 loss: 5.8551575e-07
Iter: 125 loss: 5.850261e-07
Iter: 126 loss: 5.86477e-07
Iter: 127 loss: 5.8486512e-07
Iter: 128 loss: 5.84598752e-07
Iter: 129 loss: 5.84231088e-07
Iter: 130 loss: 5.84216252e-07
Iter: 131 loss: 5.83666292e-07
Iter: 132 loss: 5.8716256e-07
Iter: 133 loss: 5.83606777e-07
Iter: 134 loss: 5.83327335e-07
Iter: 135 loss: 5.83314772e-07
Iter: 136 loss: 5.83076712e-07
Iter: 137 loss: 5.82498956e-07
Iter: 138 loss: 5.88917771e-07
Iter: 139 loss: 5.82446205e-07
Iter: 140 loss: 5.81849122e-07
Iter: 141 loss: 5.82253506e-07
Iter: 142 loss: 5.81469521e-07
Iter: 143 loss: 5.81208838e-07
Iter: 144 loss: 5.81206109e-07
Iter: 145 loss: 5.80920073e-07
Iter: 146 loss: 5.82203143e-07
Iter: 147 loss: 5.80882329e-07
Iter: 148 loss: 5.80763867e-07
Iter: 149 loss: 5.81253403e-07
Iter: 150 loss: 5.8075176e-07
Iter: 151 loss: 5.80609935e-07
Iter: 152 loss: 5.80531946e-07
Iter: 153 loss: 5.80472317e-07
Iter: 154 loss: 5.80303833e-07
Iter: 155 loss: 5.79962887e-07
Iter: 156 loss: 5.85386601e-07
Iter: 157 loss: 5.79937819e-07
Iter: 158 loss: 5.79702771e-07
Iter: 159 loss: 5.79648827e-07
Iter: 160 loss: 5.79418611e-07
Iter: 161 loss: 5.78786057e-07
Iter: 162 loss: 5.82386e-07
Iter: 163 loss: 5.78605466e-07
Iter: 164 loss: 5.78120535e-07
Iter: 165 loss: 5.78121615e-07
Iter: 166 loss: 5.77805281e-07
Iter: 167 loss: 5.79846187e-07
Iter: 168 loss: 5.77768105e-07
Iter: 169 loss: 5.77456e-07
Iter: 170 loss: 5.78534241e-07
Iter: 171 loss: 5.77382139e-07
Iter: 172 loss: 5.77223318e-07
Iter: 173 loss: 5.76894706e-07
Iter: 174 loss: 5.83687211e-07
Iter: 175 loss: 5.76895786e-07
Iter: 176 loss: 5.76665968e-07
Iter: 177 loss: 5.80270353e-07
Iter: 178 loss: 5.76665911e-07
Iter: 179 loss: 5.76492198e-07
Iter: 180 loss: 5.76706441e-07
Iter: 181 loss: 5.76402499e-07
Iter: 182 loss: 5.76205e-07
Iter: 183 loss: 5.76604577e-07
Iter: 184 loss: 5.76123625e-07
Iter: 185 loss: 5.7587431e-07
Iter: 186 loss: 5.76590196e-07
Iter: 187 loss: 5.75797287e-07
Iter: 188 loss: 5.75630224e-07
Iter: 189 loss: 5.75355e-07
Iter: 190 loss: 5.75355671e-07
Iter: 191 loss: 5.75293143e-07
Iter: 192 loss: 5.75207e-07
Iter: 193 loss: 5.75065144e-07
Iter: 194 loss: 5.74749151e-07
Iter: 195 loss: 5.79184643e-07
Iter: 196 loss: 5.74728233e-07
Iter: 197 loss: 5.74445721e-07
Iter: 198 loss: 5.74846e-07
Iter: 199 loss: 5.74313844e-07
Iter: 200 loss: 5.73973e-07
Iter: 201 loss: 5.76039383e-07
Iter: 202 loss: 5.73945044e-07
Iter: 203 loss: 5.73721763e-07
Iter: 204 loss: 5.75539445e-07
Iter: 205 loss: 5.73701527e-07
Iter: 206 loss: 5.7355868e-07
Iter: 207 loss: 5.73282705e-07
Iter: 208 loss: 5.78906054e-07
Iter: 209 loss: 5.73277248e-07
Iter: 210 loss: 5.73166119e-07
Iter: 211 loss: 5.73132922e-07
Iter: 212 loss: 5.73052546e-07
Iter: 213 loss: 5.7305806e-07
Iter: 214 loss: 5.72991269e-07
Iter: 215 loss: 5.72880367e-07
Iter: 216 loss: 5.73244961e-07
Iter: 217 loss: 5.72846602e-07
Iter: 218 loss: 5.72729903e-07
Iter: 219 loss: 5.73051807e-07
Iter: 220 loss: 5.72697559e-07
Iter: 221 loss: 5.7258444e-07
Iter: 222 loss: 5.72311478e-07
Iter: 223 loss: 5.75508352e-07
Iter: 224 loss: 5.72282374e-07
Iter: 225 loss: 5.72030274e-07
Iter: 226 loss: 5.7526222e-07
Iter: 227 loss: 5.72035106e-07
Iter: 228 loss: 5.71751229e-07
Iter: 229 loss: 5.72264867e-07
Iter: 230 loss: 5.71618557e-07
Iter: 231 loss: 5.71473777e-07
Iter: 232 loss: 5.7146849e-07
Iter: 233 loss: 5.71358726e-07
Iter: 234 loss: 5.71204964e-07
Iter: 235 loss: 5.71212354e-07
Iter: 236 loss: 5.71130954e-07
Iter: 237 loss: 5.71610258e-07
Iter: 238 loss: 5.7111879e-07
Iter: 239 loss: 5.7104404e-07
Iter: 240 loss: 5.70905229e-07
Iter: 241 loss: 5.74015644e-07
Iter: 242 loss: 5.70905399e-07
Iter: 243 loss: 5.70832526e-07
Iter: 244 loss: 5.7083372e-07
Iter: 245 loss: 5.7075124e-07
Iter: 246 loss: 5.70896873e-07
Iter: 247 loss: 5.70718271e-07
Iter: 248 loss: 5.70649377e-07
Iter: 249 loss: 5.70758402e-07
Iter: 250 loss: 5.70599809e-07
Iter: 251 loss: 5.70501811e-07
Iter: 252 loss: 5.70963834e-07
Iter: 253 loss: 5.7048851e-07
Iter: 254 loss: 5.70383349e-07
Iter: 255 loss: 5.7023675e-07
Iter: 256 loss: 5.70235329e-07
Iter: 257 loss: 5.70098678e-07
Iter: 258 loss: 5.70646534e-07
Iter: 259 loss: 5.7006e-07
Iter: 260 loss: 5.69898134e-07
Iter: 261 loss: 5.71251803e-07
Iter: 262 loss: 5.6989586e-07
Iter: 263 loss: 5.69818326e-07
Iter: 264 loss: 5.69678036e-07
Iter: 265 loss: 5.72173747e-07
Iter: 266 loss: 5.69677923e-07
Iter: 267 loss: 5.69570943e-07
Iter: 268 loss: 5.69571739e-07
Iter: 269 loss: 5.6948295e-07
Iter: 270 loss: 5.69800932e-07
Iter: 271 loss: 5.69466238e-07
Iter: 272 loss: 5.69382e-07
Iter: 273 loss: 5.69316967e-07
Iter: 274 loss: 5.69294116e-07
Iter: 275 loss: 5.6918816e-07
Iter: 276 loss: 5.69395638e-07
Iter: 277 loss: 5.69139956e-07
Iter: 278 loss: 5.69059353e-07
Iter: 279 loss: 5.69058955e-07
Iter: 280 loss: 5.69001429e-07
Iter: 281 loss: 5.68941346e-07
Iter: 282 loss: 5.68937139e-07
Iter: 283 loss: 5.68841813e-07
Iter: 284 loss: 5.69534961e-07
Iter: 285 loss: 5.68835787e-07
Iter: 286 loss: 5.68757571e-07
Iter: 287 loss: 5.68632572e-07
Iter: 288 loss: 5.71812336e-07
Iter: 289 loss: 5.68635187e-07
Iter: 290 loss: 5.68494102e-07
Iter: 291 loss: 5.68819246e-07
Iter: 292 loss: 5.68436121e-07
Iter: 293 loss: 5.68346081e-07
Iter: 294 loss: 5.68342784e-07
Iter: 295 loss: 5.68278779e-07
Iter: 296 loss: 5.68132805e-07
Iter: 297 loss: 5.68969767e-07
Iter: 298 loss: 5.68097676e-07
Iter: 299 loss: 5.67956818e-07
Iter: 300 loss: 5.67959432e-07
Iter: 301 loss: 5.67851089e-07
Iter: 302 loss: 5.68813448e-07
Iter: 303 loss: 5.67853931e-07
Iter: 304 loss: 5.67774634e-07
Iter: 305 loss: 5.67779693e-07
Iter: 306 loss: 5.67711822e-07
Iter: 307 loss: 5.67623033e-07
Iter: 308 loss: 5.67524694e-07
Iter: 309 loss: 5.6752026e-07
Iter: 310 loss: 5.67512416e-07
Iter: 311 loss: 5.67447159e-07
Iter: 312 loss: 5.67389691e-07
Iter: 313 loss: 5.67249742e-07
Iter: 314 loss: 5.68691e-07
Iter: 315 loss: 5.67236043e-07
Iter: 316 loss: 5.67083134e-07
Iter: 317 loss: 5.67083589e-07
Iter: 318 loss: 5.66989797e-07
Iter: 319 loss: 5.66837741e-07
Iter: 320 loss: 5.66836206e-07
Iter: 321 loss: 5.66635549e-07
Iter: 322 loss: 5.67264067e-07
Iter: 323 loss: 5.66577057e-07
Iter: 324 loss: 5.66572794e-07
Iter: 325 loss: 5.66512881e-07
Iter: 326 loss: 5.66452627e-07
Iter: 327 loss: 5.6632507e-07
Iter: 328 loss: 5.67426127e-07
Iter: 329 loss: 5.66306539e-07
Iter: 330 loss: 5.66160566e-07
Iter: 331 loss: 5.66501626e-07
Iter: 332 loss: 5.66112249e-07
Iter: 333 loss: 5.6600993e-07
Iter: 334 loss: 5.66005269e-07
Iter: 335 loss: 5.65933874e-07
Iter: 336 loss: 5.65873847e-07
Iter: 337 loss: 5.65865946e-07
Iter: 338 loss: 5.65715879e-07
Iter: 339 loss: 5.65511527e-07
Iter: 340 loss: 5.6550931e-07
Iter: 341 loss: 5.65504308e-07
Iter: 342 loss: 5.65422738e-07
Iter: 343 loss: 5.65342134e-07
Iter: 344 loss: 5.6527017e-07
Iter: 345 loss: 5.65245728e-07
Iter: 346 loss: 5.65174901e-07
Iter: 347 loss: 5.65171604e-07
Iter: 348 loss: 5.65120217e-07
Iter: 349 loss: 5.65025061e-07
Iter: 350 loss: 5.65028529e-07
Iter: 351 loss: 5.6492587e-07
Iter: 352 loss: 5.64938034e-07
Iter: 353 loss: 5.64850097e-07
Iter: 354 loss: 5.64824e-07
Iter: 355 loss: 5.64798142e-07
Iter: 356 loss: 5.64731749e-07
Iter: 357 loss: 5.64637958e-07
Iter: 358 loss: 5.64640345e-07
Iter: 359 loss: 5.64535469e-07
Iter: 360 loss: 5.64608399e-07
Iter: 361 loss: 5.64477e-07
Iter: 362 loss: 5.64444e-07
Iter: 363 loss: 5.6442326e-07
Iter: 364 loss: 5.64372726e-07
Iter: 365 loss: 5.64373181e-07
Iter: 366 loss: 5.6434277e-07
Iter: 367 loss: 5.64271943e-07
Iter: 368 loss: 5.64243e-07
Iter: 369 loss: 5.64216407e-07
Iter: 370 loss: 5.64159109e-07
Iter: 371 loss: 5.6446072e-07
Iter: 372 loss: 5.64140464e-07
Iter: 373 loss: 5.64055199e-07
Iter: 374 loss: 5.64389666e-07
Iter: 375 loss: 5.64032916e-07
Iter: 376 loss: 5.63979427e-07
Iter: 377 loss: 5.64223342e-07
Iter: 378 loss: 5.63982326e-07
Iter: 379 loss: 5.63925e-07
Iter: 380 loss: 5.63885237e-07
Iter: 381 loss: 5.63858862e-07
Iter: 382 loss: 5.63797414e-07
Iter: 383 loss: 5.63791559e-07
Iter: 384 loss: 5.63732556e-07
Iter: 385 loss: 5.63685376e-07
Iter: 386 loss: 5.63680544e-07
Iter: 387 loss: 5.63628362e-07
Iter: 388 loss: 5.63728292e-07
Iter: 389 loss: 5.6360966e-07
Iter: 390 loss: 5.63571405e-07
Iter: 391 loss: 5.63503818e-07
Iter: 392 loss: 5.6518769e-07
Iter: 393 loss: 5.63497565e-07
Iter: 394 loss: 5.63422475e-07
Iter: 395 loss: 5.64361301e-07
Iter: 396 loss: 5.63415142e-07
Iter: 397 loss: 5.6334494e-07
Iter: 398 loss: 5.63460787e-07
Iter: 399 loss: 5.63312e-07
Iter: 400 loss: 5.63234266e-07
Iter: 401 loss: 5.63188678e-07
Iter: 402 loss: 5.63155368e-07
Iter: 403 loss: 5.63058279e-07
Iter: 404 loss: 5.631764e-07
Iter: 405 loss: 5.63011099e-07
Iter: 406 loss: 5.63052e-07
Iter: 407 loss: 5.62968523e-07
Iter: 408 loss: 5.62942034e-07
Iter: 409 loss: 5.62909e-07
Iter: 410 loss: 5.63635126e-07
Iter: 411 loss: 5.62908e-07
Iter: 412 loss: 5.6284523e-07
Iter: 413 loss: 5.63206072e-07
Iter: 414 loss: 5.62849095e-07
Iter: 415 loss: 5.62813568e-07
Iter: 416 loss: 5.62735067e-07
Iter: 417 loss: 5.63640356e-07
Iter: 418 loss: 5.62725063e-07
Iter: 419 loss: 5.62626724e-07
Iter: 420 loss: 5.62802143e-07
Iter: 421 loss: 5.62583296e-07
Iter: 422 loss: 5.62519176e-07
Iter: 423 loss: 5.62514629e-07
Iter: 424 loss: 5.62463356e-07
Iter: 425 loss: 5.62349271e-07
Iter: 426 loss: 5.63721528e-07
Iter: 427 loss: 5.62337505e-07
Iter: 428 loss: 5.62256787e-07
Iter: 429 loss: 5.62753826e-07
Iter: 430 loss: 5.62243e-07
Iter: 431 loss: 5.62208641e-07
Iter: 432 loss: 5.62199943e-07
Iter: 433 loss: 5.62176467e-07
Iter: 434 loss: 5.62133721e-07
Iter: 435 loss: 5.62135142e-07
Iter: 436 loss: 5.62079833e-07
Iter: 437 loss: 5.62059768e-07
Iter: 438 loss: 5.62023786e-07
Iter: 439 loss: 5.6196518e-07
Iter: 440 loss: 5.62574769e-07
Iter: 441 loss: 5.61968932e-07
Iter: 442 loss: 5.61903903e-07
Iter: 443 loss: 5.62131e-07
Iter: 444 loss: 5.61887077e-07
Iter: 445 loss: 5.6185695e-07
Iter: 446 loss: 5.61922e-07
Iter: 447 loss: 5.61841091e-07
Iter: 448 loss: 5.6178709e-07
Iter: 449 loss: 5.61910724e-07
Iter: 450 loss: 5.61779075e-07
Iter: 451 loss: 5.61764125e-07
Iter: 452 loss: 5.61711147e-07
Iter: 453 loss: 5.62268269e-07
Iter: 454 loss: 5.616983e-07
Iter: 455 loss: 5.61663455e-07
Iter: 456 loss: 5.6166175e-07
Iter: 457 loss: 5.61617753e-07
Iter: 458 loss: 5.61609113e-07
Iter: 459 loss: 5.61578418e-07
Iter: 460 loss: 5.61529646e-07
Iter: 461 loss: 5.61491e-07
Iter: 462 loss: 5.61474963e-07
Iter: 463 loss: 5.61413458e-07
Iter: 464 loss: 5.62189655e-07
Iter: 465 loss: 5.61417608e-07
Iter: 466 loss: 5.61365e-07
Iter: 467 loss: 5.61574097e-07
Iter: 468 loss: 5.61349452e-07
Iter: 469 loss: 5.61315346e-07
Iter: 470 loss: 5.61321599e-07
Iter: 471 loss: 5.61304603e-07
Iter: 472 loss: 5.61264642e-07
Iter: 473 loss: 5.61207344e-07
Iter: 474 loss: 5.61194668e-07
Iter: 475 loss: 5.61212175e-07
Iter: 476 loss: 5.61169713e-07
Iter: 477 loss: 5.6114925e-07
Iter: 478 loss: 5.61068475e-07
Iter: 479 loss: 5.61932723e-07
Iter: 480 loss: 5.61066599e-07
Iter: 481 loss: 5.61021864e-07
Iter: 482 loss: 5.61012598e-07
Iter: 483 loss: 5.60978037e-07
Iter: 484 loss: 5.60894364e-07
Iter: 485 loss: 5.62419757e-07
Iter: 486 loss: 5.60888168e-07
Iter: 487 loss: 5.60836497e-07
Iter: 488 loss: 5.61333252e-07
Iter: 489 loss: 5.60834224e-07
Iter: 490 loss: 5.60782098e-07
Iter: 491 loss: 5.609852e-07
Iter: 492 loss: 5.60776243e-07
Iter: 493 loss: 5.60738727e-07
Iter: 494 loss: 5.60663807e-07
Iter: 495 loss: 5.61623096e-07
Iter: 496 loss: 5.60658407e-07
Iter: 497 loss: 5.60562796e-07
Iter: 498 loss: 5.60903459e-07
Iter: 499 loss: 5.60539775e-07
Iter: 500 loss: 5.60471562e-07
Iter: 501 loss: 5.6047e-07
Iter: 502 loss: 5.60433875e-07
Iter: 503 loss: 5.60368278e-07
Iter: 504 loss: 5.60370381e-07
Iter: 505 loss: 5.60305352e-07
Iter: 506 loss: 5.60375383e-07
Iter: 507 loss: 5.60265846e-07
Iter: 508 loss: 5.6021014e-07
Iter: 509 loss: 5.60254648e-07
Iter: 510 loss: 5.60176943e-07
Iter: 511 loss: 5.60102762e-07
Iter: 512 loss: 5.60111744e-07
Iter: 513 loss: 5.60074454e-07
Iter: 514 loss: 5.60013575e-07
Iter: 515 loss: 5.60760441e-07
Iter: 516 loss: 5.59996181e-07
Iter: 517 loss: 5.5988653e-07
Iter: 518 loss: 5.61016122e-07
Iter: 519 loss: 5.59889486e-07
Iter: 520 loss: 5.59832415e-07
Iter: 521 loss: 5.59751e-07
Iter: 522 loss: 5.61904358e-07
Iter: 523 loss: 5.59749537e-07
Iter: 524 loss: 5.59694e-07
Iter: 525 loss: 5.59689852e-07
Iter: 526 loss: 5.5961965e-07
Iter: 527 loss: 5.59652278e-07
Iter: 528 loss: 5.59591399e-07
Iter: 529 loss: 5.59527336e-07
Iter: 530 loss: 5.59495049e-07
Iter: 531 loss: 5.59483738e-07
Iter: 532 loss: 5.59410296e-07
Iter: 533 loss: 5.60081276e-07
Iter: 534 loss: 5.59405e-07
Iter: 535 loss: 5.5935152e-07
Iter: 536 loss: 5.59350553e-07
Iter: 537 loss: 5.59321791e-07
Iter: 538 loss: 5.59260229e-07
Iter: 539 loss: 5.59562523e-07
Iter: 540 loss: 5.59249372e-07
Iter: 541 loss: 5.59202135e-07
Iter: 542 loss: 5.59512387e-07
Iter: 543 loss: 5.59197304e-07
Iter: 544 loss: 5.59173372e-07
Iter: 545 loss: 5.59171895e-07
Iter: 546 loss: 5.59147e-07
Iter: 547 loss: 5.59078558e-07
Iter: 548 loss: 5.60093895e-07
Iter: 549 loss: 5.59075716e-07
Iter: 550 loss: 5.59055195e-07
Iter: 551 loss: 5.59045759e-07
Iter: 552 loss: 5.59013188e-07
Iter: 553 loss: 5.58933493e-07
Iter: 554 loss: 5.60155968e-07
Iter: 555 loss: 5.58936e-07
Iter: 556 loss: 5.58905356e-07
Iter: 557 loss: 5.59298883e-07
Iter: 558 loss: 5.58911438e-07
Iter: 559 loss: 5.58874433e-07
Iter: 560 loss: 5.59091291e-07
Iter: 561 loss: 5.58865111e-07
Iter: 562 loss: 5.58850388e-07
Iter: 563 loss: 5.58828219e-07
Iter: 564 loss: 5.58830266e-07
Iter: 565 loss: 5.5879957e-07
Iter: 566 loss: 5.58798774e-07
Iter: 567 loss: 5.58783086e-07
Iter: 568 loss: 5.58764214e-07
Iter: 569 loss: 5.58747047e-07
Iter: 570 loss: 5.58708507e-07
Iter: 571 loss: 5.5883072e-07
Iter: 572 loss: 5.58689408e-07
Iter: 573 loss: 5.58647e-07
Iter: 574 loss: 5.58796103e-07
Iter: 575 loss: 5.58644672e-07
Iter: 576 loss: 5.58615454e-07
Iter: 577 loss: 5.58796216e-07
Iter: 578 loss: 5.58613692e-07
Iter: 579 loss: 5.58594479e-07
Iter: 580 loss: 5.58544571e-07
Iter: 581 loss: 5.58543e-07
Iter: 582 loss: 5.58519901e-07
Iter: 583 loss: 5.58780187e-07
Iter: 584 loss: 5.58506031e-07
Iter: 585 loss: 5.58454474e-07
Iter: 586 loss: 5.58588397e-07
Iter: 587 loss: 5.58437705e-07
Iter: 588 loss: 5.5840809e-07
Iter: 589 loss: 5.58357e-07
Iter: 590 loss: 5.59399496e-07
Iter: 591 loss: 5.58352895e-07
Iter: 592 loss: 5.58326349e-07
Iter: 593 loss: 5.58326292e-07
Iter: 594 loss: 5.58282579e-07
Iter: 595 loss: 5.5816821e-07
Iter: 596 loss: 5.58812644e-07
Iter: 597 loss: 5.58131887e-07
Iter: 598 loss: 5.58162242e-07
Iter: 599 loss: 5.5808448e-07
Iter: 600 loss: 5.58049578e-07
Iter: 601 loss: 5.58042416e-07
Iter: 602 loss: 5.57994156e-07
Iter: 603 loss: 5.57942144e-07
Iter: 604 loss: 5.57991712e-07
Iter: 605 loss: 5.57901785e-07
Iter: 606 loss: 5.57840849e-07
Iter: 607 loss: 5.5777889e-07
Iter: 608 loss: 5.57779458e-07
Iter: 609 loss: 5.57736428e-07
Iter: 610 loss: 5.57710507e-07
Iter: 611 loss: 5.57662133e-07
Iter: 612 loss: 5.57591875e-07
Iter: 613 loss: 5.57590283e-07
Iter: 614 loss: 5.57525539e-07
Iter: 615 loss: 5.58028376e-07
Iter: 616 loss: 5.57523265e-07
Iter: 617 loss: 5.5748518e-07
Iter: 618 loss: 5.57555722e-07
Iter: 619 loss: 5.57462613e-07
Iter: 620 loss: 5.57427768e-07
Iter: 621 loss: 5.57390933e-07
Iter: 622 loss: 5.57388887e-07
Iter: 623 loss: 5.57340456e-07
Iter: 624 loss: 5.57440558e-07
Iter: 625 loss: 5.57325e-07
Iter: 626 loss: 5.57265537e-07
Iter: 627 loss: 5.57720853e-07
Iter: 628 loss: 5.57254339e-07
Iter: 629 loss: 5.57228873e-07
Iter: 630 loss: 5.5716589e-07
Iter: 631 loss: 5.57462499e-07
Iter: 632 loss: 5.57132807e-07
Iter: 633 loss: 5.57126157e-07
Iter: 634 loss: 5.57082842e-07
Iter: 635 loss: 5.57062549e-07
Iter: 636 loss: 5.57021053e-07
Iter: 637 loss: 5.57011958e-07
Iter: 638 loss: 5.56985469e-07
Iter: 639 loss: 5.57038163e-07
Iter: 640 loss: 5.56964721e-07
Iter: 641 loss: 5.56943519e-07
Iter: 642 loss: 5.56898158e-07
Iter: 643 loss: 5.57859948e-07
Iter: 644 loss: 5.56900204e-07
Iter: 645 loss: 5.56850807e-07
Iter: 646 loss: 5.57217618e-07
Iter: 647 loss: 5.56840632e-07
Iter: 648 loss: 5.5679061e-07
Iter: 649 loss: 5.56828411e-07
Iter: 650 loss: 5.56780492e-07
Iter: 651 loss: 5.56737746e-07
Iter: 652 loss: 5.57198803e-07
Iter: 653 loss: 5.56736836e-07
Iter: 654 loss: 5.56704549e-07
Iter: 655 loss: 5.56668681e-07
Iter: 656 loss: 5.5667e-07
Iter: 657 loss: 5.5662008e-07
Iter: 658 loss: 5.5677242e-07
Iter: 659 loss: 5.56607915e-07
Iter: 660 loss: 5.56587906e-07
Iter: 661 loss: 5.56593704e-07
Iter: 662 loss: 5.56573241e-07
Iter: 663 loss: 5.56525947e-07
Iter: 664 loss: 5.56685677e-07
Iter: 665 loss: 5.56505597e-07
Iter: 666 loss: 5.56434543e-07
Iter: 667 loss: 5.56699661e-07
Iter: 668 loss: 5.56419593e-07
Iter: 669 loss: 5.56344219e-07
Iter: 670 loss: 5.57109786e-07
Iter: 671 loss: 5.56344958e-07
Iter: 672 loss: 5.56302098e-07
Iter: 673 loss: 5.56368832e-07
Iter: 674 loss: 5.56293799e-07
Iter: 675 loss: 5.56249574e-07
Iter: 676 loss: 5.56276916e-07
Iter: 677 loss: 5.56214729e-07
Iter: 678 loss: 5.5617511e-07
Iter: 679 loss: 5.56154816e-07
Iter: 680 loss: 5.56121108e-07
Iter: 681 loss: 5.56051646e-07
Iter: 682 loss: 5.56466432e-07
Iter: 683 loss: 5.56040732e-07
Iter: 684 loss: 5.55982069e-07
Iter: 685 loss: 5.56392479e-07
Iter: 686 loss: 5.55985821e-07
Iter: 687 loss: 5.55919883e-07
Iter: 688 loss: 5.55944951e-07
Iter: 689 loss: 5.55893905e-07
Iter: 690 loss: 5.55848032e-07
Iter: 691 loss: 5.5594154e-07
Iter: 692 loss: 5.5582251e-07
Iter: 693 loss: 5.55787551e-07
Iter: 694 loss: 5.55789939e-07
Iter: 695 loss: 5.55756458e-07
Iter: 696 loss: 5.55694214e-07
Iter: 697 loss: 5.56773898e-07
Iter: 698 loss: 5.5568546e-07
Iter: 699 loss: 5.55626343e-07
Iter: 700 loss: 5.55597e-07
Iter: 701 loss: 5.5557274e-07
Iter: 702 loss: 5.55561655e-07
Iter: 703 loss: 5.55531528e-07
Iter: 704 loss: 5.55485883e-07
Iter: 705 loss: 5.55381803e-07
Iter: 706 loss: 5.5675082e-07
Iter: 707 loss: 5.55374299e-07
Iter: 708 loss: 5.55334907e-07
Iter: 709 loss: 5.55325812e-07
Iter: 710 loss: 5.55284771e-07
Iter: 711 loss: 5.55192969e-07
Iter: 712 loss: 5.56470809e-07
Iter: 713 loss: 5.55194561e-07
Iter: 714 loss: 5.5511714e-07
Iter: 715 loss: 5.55828706e-07
Iter: 716 loss: 5.55114525e-07
Iter: 717 loss: 5.55080192e-07
Iter: 718 loss: 5.55045233e-07
Iter: 719 loss: 5.55035228e-07
Iter: 720 loss: 5.55007318e-07
Iter: 721 loss: 5.54990436e-07
Iter: 722 loss: 5.54968892e-07
Iter: 723 loss: 5.54979579e-07
Iter: 724 loss: 5.54950645e-07
Iter: 725 loss: 5.54893802e-07
Iter: 726 loss: 5.54785572e-07
Iter: 727 loss: 5.56876444e-07
Iter: 728 loss: 5.54787334e-07
Iter: 729 loss: 5.54658243e-07
Iter: 730 loss: 5.56218708e-07
Iter: 731 loss: 5.54652956e-07
Iter: 732 loss: 5.54617543e-07
Iter: 733 loss: 5.54587814e-07
Iter: 734 loss: 5.54562746e-07
Iter: 735 loss: 5.54472763e-07
Iter: 736 loss: 5.55017323e-07
Iter: 737 loss: 5.54453663e-07
Iter: 738 loss: 5.54373116e-07
Iter: 739 loss: 5.55169208e-07
Iter: 740 loss: 5.54367261e-07
Iter: 741 loss: 5.54241353e-07
Iter: 742 loss: 5.54331166e-07
Iter: 743 loss: 5.54168764e-07
Iter: 744 loss: 5.54106919e-07
Iter: 745 loss: 5.54361748e-07
Iter: 746 loss: 5.54091e-07
Iter: 747 loss: 5.54018e-07
Iter: 748 loss: 5.54366181e-07
Iter: 749 loss: 5.54004e-07
Iter: 750 loss: 5.53963901e-07
Iter: 751 loss: 5.53956966e-07
Iter: 752 loss: 5.53932807e-07
Iter: 753 loss: 5.53891255e-07
Iter: 754 loss: 5.54131248e-07
Iter: 755 loss: 5.53892164e-07
Iter: 756 loss: 5.53859e-07
Iter: 757 loss: 5.54207531e-07
Iter: 758 loss: 5.53853056e-07
Iter: 759 loss: 5.53823043e-07
Iter: 760 loss: 5.5375034e-07
Iter: 761 loss: 5.54194116e-07
Iter: 762 loss: 5.53736697e-07
Iter: 763 loss: 5.53636312e-07
Iter: 764 loss: 5.53598397e-07
Iter: 765 loss: 5.53546499e-07
Iter: 766 loss: 5.53563609e-07
Iter: 767 loss: 5.53484824e-07
Iter: 768 loss: 5.53446284e-07
Iter: 769 loss: 5.53355392e-07
Iter: 770 loss: 5.53352152e-07
Iter: 771 loss: 5.53272457e-07
Iter: 772 loss: 5.5331509e-07
Iter: 773 loss: 5.53225505e-07
Iter: 774 loss: 5.53250402e-07
Iter: 775 loss: 5.53191796e-07
Iter: 776 loss: 5.53180712e-07
Iter: 777 loss: 5.53142513e-07
Iter: 778 loss: 5.53758298e-07
Iter: 779 loss: 5.53134214e-07
Iter: 780 loss: 5.53108862e-07
Iter: 781 loss: 5.53101131e-07
Iter: 782 loss: 5.53074699e-07
Iter: 783 loss: 5.53001655e-07
Iter: 784 loss: 5.5366246e-07
Iter: 785 loss: 5.5299563e-07
Iter: 786 loss: 5.52925542e-07
Iter: 787 loss: 5.52927077e-07
Iter: 788 loss: 5.52884444e-07
Iter: 789 loss: 5.52981078e-07
Iter: 790 loss: 5.52863924e-07
Iter: 791 loss: 5.52809581e-07
Iter: 792 loss: 5.52964e-07
Iter: 793 loss: 5.52805147e-07
Iter: 794 loss: 5.52748588e-07
Iter: 795 loss: 5.52658321e-07
Iter: 796 loss: 5.52651386e-07
Iter: 797 loss: 5.52583117e-07
Iter: 798 loss: 5.52580559e-07
Iter: 799 loss: 5.52515189e-07
Iter: 800 loss: 5.52616427e-07
Iter: 801 loss: 5.52485233e-07
Iter: 802 loss: 5.52431175e-07
Iter: 803 loss: 5.5233761e-07
Iter: 804 loss: 5.52337042e-07
Iter: 805 loss: 5.52218125e-07
Iter: 806 loss: 5.52674237e-07
Iter: 807 loss: 5.5218959e-07
Iter: 808 loss: 5.52119445e-07
Iter: 809 loss: 5.52331358e-07
Iter: 810 loss: 5.52096253e-07
Iter: 811 loss: 5.52057259e-07
Iter: 812 loss: 5.52120866e-07
Iter: 813 loss: 5.52047936e-07
Iter: 814 loss: 5.52010079e-07
Iter: 815 loss: 5.51952326e-07
Iter: 816 loss: 5.53153654e-07
Iter: 817 loss: 5.51939422e-07
Iter: 818 loss: 5.51859102e-07
Iter: 819 loss: 5.52551796e-07
Iter: 820 loss: 5.51863195e-07
Iter: 821 loss: 5.51770768e-07
Iter: 822 loss: 5.52042366e-07
Iter: 823 loss: 5.51777362e-07
Iter: 824 loss: 5.516996e-07
Iter: 825 loss: 5.52026904e-07
Iter: 826 loss: 5.51695393e-07
Iter: 827 loss: 5.51641961e-07
Iter: 828 loss: 5.51661913e-07
Iter: 829 loss: 5.51599612e-07
Iter: 830 loss: 5.51558571e-07
Iter: 831 loss: 5.51795551e-07
Iter: 832 loss: 5.51548737e-07
Iter: 833 loss: 5.51507128e-07
Iter: 834 loss: 5.51773837e-07
Iter: 835 loss: 5.51495532e-07
Iter: 836 loss: 5.51467338e-07
Iter: 837 loss: 5.51513097e-07
Iter: 838 loss: 5.51458754e-07
Iter: 839 loss: 5.51423113e-07
Iter: 840 loss: 5.51478308e-07
Iter: 841 loss: 5.51390769e-07
Iter: 842 loss: 5.51353253e-07
Iter: 843 loss: 5.51381504e-07
Iter: 844 loss: 5.5132432e-07
Iter: 845 loss: 5.51253038e-07
Iter: 846 loss: 5.51560902e-07
Iter: 847 loss: 5.51235701e-07
Iter: 848 loss: 5.51207847e-07
Iter: 849 loss: 5.51142421e-07
Iter: 850 loss: 5.51144581e-07
Iter: 851 loss: 5.51081143e-07
Iter: 852 loss: 5.51336257e-07
Iter: 853 loss: 5.51070116e-07
Iter: 854 loss: 5.51018729e-07
Iter: 855 loss: 5.51391736e-07
Iter: 856 loss: 5.51030837e-07
Iter: 857 loss: 5.50982918e-07
Iter: 858 loss: 5.51092171e-07
Iter: 859 loss: 5.50979848e-07
Iter: 860 loss: 5.50939262e-07
Iter: 861 loss: 5.50950404e-07
Iter: 862 loss: 5.50920561e-07
Iter: 863 loss: 5.50870482e-07
Iter: 864 loss: 5.50899699e-07
Iter: 865 loss: 5.50828e-07
Iter: 866 loss: 5.50745142e-07
Iter: 867 loss: 5.51570167e-07
Iter: 868 loss: 5.50747131e-07
Iter: 869 loss: 5.50699895e-07
Iter: 870 loss: 5.50669654e-07
Iter: 871 loss: 5.50648224e-07
Iter: 872 loss: 5.50568586e-07
Iter: 873 loss: 5.51109792e-07
Iter: 874 loss: 5.50569155e-07
Iter: 875 loss: 5.50505376e-07
Iter: 876 loss: 5.50581717e-07
Iter: 877 loss: 5.50482468e-07
Iter: 878 loss: 5.50413915e-07
Iter: 879 loss: 5.50975e-07
Iter: 880 loss: 5.50409368e-07
Iter: 881 loss: 5.50366735e-07
Iter: 882 loss: 5.50255379e-07
Iter: 883 loss: 5.50972459e-07
Iter: 884 loss: 5.50220193e-07
Iter: 885 loss: 5.50094569e-07
Iter: 886 loss: 5.50802838e-07
Iter: 887 loss: 5.50075356e-07
Iter: 888 loss: 5.49984463e-07
Iter: 889 loss: 5.51236496e-07
Iter: 890 loss: 5.49987135e-07
Iter: 891 loss: 5.49931315e-07
Iter: 892 loss: 5.50016352e-07
Iter: 893 loss: 5.4989323e-07
Iter: 894 loss: 5.49838148e-07
Iter: 895 loss: 5.5007547e-07
Iter: 896 loss: 5.49830077e-07
Iter: 897 loss: 5.49784204e-07
Iter: 898 loss: 5.49773915e-07
Iter: 899 loss: 5.49732817e-07
Iter: 900 loss: 5.49684444e-07
Iter: 901 loss: 5.49683591e-07
Iter: 902 loss: 5.49628453e-07
Iter: 903 loss: 5.49540914e-07
Iter: 904 loss: 5.49534548e-07
Iter: 905 loss: 5.49435867e-07
Iter: 906 loss: 5.50863092e-07
Iter: 907 loss: 5.49428137e-07
Iter: 908 loss: 5.49359584e-07
Iter: 909 loss: 5.49360891e-07
Iter: 910 loss: 5.49316837e-07
Iter: 911 loss: 5.49279093e-07
Iter: 912 loss: 5.49273068e-07
Iter: 913 loss: 5.49239644e-07
Iter: 914 loss: 5.49164326e-07
Iter: 915 loss: 5.50642199e-07
Iter: 916 loss: 5.49165406e-07
Iter: 917 loss: 5.49128174e-07
Iter: 918 loss: 5.49179504e-07
Iter: 919 loss: 5.49085144e-07
Iter: 920 loss: 5.4905928e-07
Iter: 921 loss: 5.49486231e-07
Iter: 922 loss: 5.49064794e-07
Iter: 923 loss: 5.49031313e-07
Iter: 924 loss: 5.49001811e-07
Iter: 925 loss: 5.48996297e-07
Iter: 926 loss: 5.48952755e-07
Iter: 927 loss: 5.49252377e-07
Iter: 928 loss: 5.48944342e-07
Iter: 929 loss: 5.48901312e-07
Iter: 930 loss: 5.48837932e-07
Iter: 931 loss: 5.48825369e-07
Iter: 932 loss: 5.48770117e-07
Iter: 933 loss: 5.48766593e-07
Iter: 934 loss: 5.48725723e-07
Iter: 935 loss: 5.48688718e-07
Iter: 936 loss: 5.48673029e-07
Iter: 937 loss: 5.48620619e-07
Iter: 938 loss: 5.49119193e-07
Iter: 939 loss: 5.48608568e-07
Iter: 940 loss: 5.4858242e-07
Iter: 941 loss: 5.48570881e-07
Iter: 942 loss: 5.48549565e-07
Iter: 943 loss: 5.48531148e-07
Iter: 944 loss: 5.48521086e-07
Iter: 945 loss: 5.48511878e-07
Iter: 946 loss: 5.48489538e-07
Iter: 947 loss: 5.48978619e-07
Iter: 948 loss: 5.48490732e-07
Iter: 949 loss: 5.48452363e-07
Iter: 950 loss: 5.48442927e-07
Iter: 951 loss: 5.48417233e-07
Iter: 952 loss: 5.48384492e-07
Iter: 953 loss: 5.4843872e-07
Iter: 954 loss: 5.48369371e-07
Iter: 955 loss: 5.48320088e-07
Iter: 956 loss: 5.48228343e-07
Iter: 957 loss: 5.50445293e-07
Iter: 958 loss: 5.48229082e-07
Iter: 959 loss: 5.48123126e-07
Iter: 960 loss: 5.48636194e-07
Iter: 961 loss: 5.48104197e-07
Iter: 962 loss: 5.4808288e-07
Iter: 963 loss: 5.48070375e-07
Iter: 964 loss: 5.4804957e-07
Iter: 965 loss: 5.48020125e-07
Iter: 966 loss: 5.48023763e-07
Iter: 967 loss: 5.47978345e-07
Iter: 968 loss: 5.47977834e-07
Iter: 969 loss: 5.4795396e-07
Iter: 970 loss: 5.47941568e-07
Iter: 971 loss: 5.47939862e-07
Iter: 972 loss: 5.47885634e-07
Iter: 973 loss: 5.47890068e-07
Iter: 974 loss: 5.47839704e-07
Iter: 975 loss: 5.47801278e-07
Iter: 976 loss: 5.48251478e-07
Iter: 977 loss: 5.47805257e-07
Iter: 978 loss: 5.47771549e-07
Iter: 979 loss: 5.478243e-07
Iter: 980 loss: 5.47759e-07
Iter: 981 loss: 5.47727836e-07
Iter: 982 loss: 5.47714535e-07
Iter: 983 loss: 5.47704076e-07
Iter: 984 loss: 5.47656668e-07
Iter: 985 loss: 5.47894103e-07
Iter: 986 loss: 5.47646152e-07
Iter: 987 loss: 5.47599029e-07
Iter: 988 loss: 5.47589e-07
Iter: 989 loss: 5.47575155e-07
Iter: 990 loss: 5.47506147e-07
Iter: 991 loss: 5.47413151e-07
Iter: 992 loss: 5.47415539e-07
Iter: 993 loss: 5.47307422e-07
Iter: 994 loss: 5.48156208e-07
Iter: 995 loss: 5.47299578e-07
Iter: 996 loss: 5.4724228e-07
Iter: 997 loss: 5.47610512e-07
Iter: 998 loss: 5.47246202e-07
Iter: 999 loss: 5.47199079e-07
Iter: 1000 loss: 5.47646266e-07
Iter: 1001 loss: 5.47197203e-07
Iter: 1002 loss: 5.47188733e-07
Iter: 1003 loss: 5.47383763e-07
Iter: 1004 loss: 5.4718447e-07
Iter: 1005 loss: 5.47168895e-07
Iter: 1006 loss: 5.47133766e-07
Iter: 1007 loss: 5.47391323e-07
Iter: 1008 loss: 5.4712865e-07
Iter: 1009 loss: 5.47102786e-07
Iter: 1010 loss: 5.47100683e-07
Iter: 1011 loss: 5.4707732e-07
Iter: 1012 loss: 5.47036848e-07
Iter: 1013 loss: 5.47890181e-07
Iter: 1014 loss: 5.47036791e-07
Iter: 1015 loss: 5.46987792e-07
Iter: 1016 loss: 5.47163381e-07
Iter: 1017 loss: 5.46982221e-07
Iter: 1018 loss: 5.46958915e-07
Iter: 1019 loss: 5.46950787e-07
Iter: 1020 loss: 5.46946296e-07
Iter: 1021 loss: 5.4693e-07
Iter: 1022 loss: 5.46919807e-07
Iter: 1023 loss: 5.46906222e-07
Iter: 1024 loss: 5.46910201e-07
Iter: 1025 loss: 5.46895535e-07
Iter: 1026 loss: 5.46869558e-07
Iter: 1027 loss: 5.47229888e-07
Iter: 1028 loss: 5.46866e-07
Iter: 1029 loss: 5.46828744e-07
Iter: 1030 loss: 5.46786282e-07
Iter: 1031 loss: 5.46779859e-07
Iter: 1032 loss: 5.46745639e-07
Iter: 1033 loss: 5.46836645e-07
Iter: 1034 loss: 5.46734839e-07
Iter: 1035 loss: 5.46691e-07
Iter: 1036 loss: 5.46898491e-07
Iter: 1037 loss: 5.46682941e-07
Iter: 1038 loss: 5.46651052e-07
Iter: 1039 loss: 5.46726596e-07
Iter: 1040 loss: 5.46653609e-07
Iter: 1041 loss: 5.46613592e-07
Iter: 1042 loss: 5.46676688e-07
Iter: 1043 loss: 5.46602394e-07
Iter: 1044 loss: 5.46588126e-07
Iter: 1045 loss: 5.46681122e-07
Iter: 1046 loss: 5.46583919e-07
Iter: 1047 loss: 5.46559136e-07
Iter: 1048 loss: 5.46534181e-07
Iter: 1049 loss: 5.46529236e-07
Iter: 1050 loss: 5.46498654e-07
Iter: 1051 loss: 5.46551291e-07
Iter: 1052 loss: 5.46490185e-07
Iter: 1053 loss: 5.46447609e-07
Iter: 1054 loss: 5.46671686e-07
Iter: 1055 loss: 5.46444426e-07
Iter: 1056 loss: 5.46424303e-07
Iter: 1057 loss: 5.46509455e-07
Iter: 1058 loss: 5.46419756e-07
Iter: 1059 loss: 5.46380306e-07
Iter: 1060 loss: 5.46390936e-07
Iter: 1061 loss: 5.46365129e-07
Iter: 1062 loss: 5.46347906e-07
Iter: 1063 loss: 5.46353817e-07
Iter: 1064 loss: 5.46327385e-07
Iter: 1065 loss: 5.4630982e-07
Iter: 1066 loss: 5.46383092e-07
Iter: 1067 loss: 5.4630857e-07
Iter: 1068 loss: 5.46290835e-07
Iter: 1069 loss: 5.46351885e-07
Iter: 1070 loss: 5.4628174e-07
Iter: 1071 loss: 5.46258434e-07
Iter: 1072 loss: 5.46212107e-07
Iter: 1073 loss: 5.47084937e-07
Iter: 1074 loss: 5.46207843e-07
Iter: 1075 loss: 5.4619295e-07
Iter: 1076 loss: 5.46181923e-07
Iter: 1077 loss: 5.4617e-07
Iter: 1078 loss: 5.46160663e-07
Iter: 1079 loss: 5.46153217e-07
Iter: 1080 loss: 5.46126557e-07
Iter: 1081 loss: 5.46320848e-07
Iter: 1082 loss: 5.46126216e-07
Iter: 1083 loss: 5.46114791e-07
Iter: 1084 loss: 5.46103593e-07
Iter: 1085 loss: 5.46106094e-07
Iter: 1086 loss: 5.4608654e-07
Iter: 1087 loss: 5.46320337e-07
Iter: 1088 loss: 5.4608455e-07
Iter: 1089 loss: 5.46068e-07
Iter: 1090 loss: 5.46122351e-07
Iter: 1091 loss: 5.46061756e-07
Iter: 1092 loss: 5.46045953e-07
Iter: 1093 loss: 5.46075512e-07
Iter: 1094 loss: 5.46037654e-07
Iter: 1095 loss: 5.46027536e-07
Iter: 1096 loss: 5.46018327e-07
Iter: 1097 loss: 5.46018157e-07
Iter: 1098 loss: 5.46003662e-07
Iter: 1099 loss: 5.46035665e-07
Iter: 1100 loss: 5.46000365e-07
Iter: 1101 loss: 5.45978537e-07
Iter: 1102 loss: 5.4600514e-07
Iter: 1103 loss: 5.45975126e-07
Iter: 1104 loss: 5.45949092e-07
Iter: 1105 loss: 5.45943749e-07
Iter: 1106 loss: 5.45930789e-07
Iter: 1107 loss: 5.45906e-07
Iter: 1108 loss: 5.45897e-07
Iter: 1109 loss: 5.45878549e-07
Iter: 1110 loss: 5.45864225e-07
Iter: 1111 loss: 5.45854903e-07
Iter: 1112 loss: 5.45836656e-07
Iter: 1113 loss: 5.45832222e-07
Iter: 1114 loss: 5.4582523e-07
Iter: 1115 loss: 5.4581335e-07
Iter: 1116 loss: 5.46209037e-07
Iter: 1117 loss: 5.45810963e-07
Iter: 1118 loss: 5.45803573e-07
Iter: 1119 loss: 5.45800106e-07
Iter: 1120 loss: 5.45790044e-07
Iter: 1121 loss: 5.45765886e-07
Iter: 1122 loss: 5.46151227e-07
Iter: 1123 loss: 5.45763e-07
Iter: 1124 loss: 5.45750936e-07
Iter: 1125 loss: 5.46071135e-07
Iter: 1126 loss: 5.45750424e-07
Iter: 1127 loss: 5.45731154e-07
Iter: 1128 loss: 5.45727232e-07
Iter: 1129 loss: 5.45712339e-07
Iter: 1130 loss: 5.45693467e-07
Iter: 1131 loss: 5.45642877e-07
Iter: 1132 loss: 5.4638781e-07
Iter: 1133 loss: 5.45626733e-07
Iter: 1134 loss: 5.45552894e-07
Iter: 1135 loss: 5.45596663e-07
Iter: 1136 loss: 5.45515547e-07
Iter: 1137 loss: 5.45471948e-07
Iter: 1138 loss: 5.45459613e-07
Iter: 1139 loss: 5.45429657e-07
Iter: 1140 loss: 5.45414082e-07
Iter: 1141 loss: 5.45393789e-07
Iter: 1142 loss: 5.45366049e-07
Iter: 1143 loss: 5.4569449e-07
Iter: 1144 loss: 5.45366504e-07
Iter: 1145 loss: 5.45344278e-07
Iter: 1146 loss: 5.45326486e-07
Iter: 1147 loss: 5.45324951e-07
Iter: 1148 loss: 5.4529653e-07
Iter: 1149 loss: 5.4553027e-07
Iter: 1150 loss: 5.45306648e-07
Iter: 1151 loss: 5.45278965e-07
Iter: 1152 loss: 5.45238549e-07
Iter: 1153 loss: 5.4606096e-07
Iter: 1154 loss: 5.45243097e-07
Iter: 1155 loss: 5.45228829e-07
Iter: 1156 loss: 5.45228e-07
Iter: 1157 loss: 5.45206092e-07
Iter: 1158 loss: 5.45248895e-07
Iter: 1159 loss: 5.45204102e-07
Iter: 1160 loss: 5.45190574e-07
Iter: 1161 loss: 5.45187618e-07
Iter: 1162 loss: 5.45177613e-07
Iter: 1163 loss: 5.45168e-07
Iter: 1164 loss: 5.45256398e-07
Iter: 1165 loss: 5.45163459e-07
Iter: 1166 loss: 5.45138505e-07
Iter: 1167 loss: 5.45131e-07
Iter: 1168 loss: 5.45121793e-07
Iter: 1169 loss: 5.45102125e-07
Iter: 1170 loss: 5.45091325e-07
Iter: 1171 loss: 5.45075864e-07
Iter: 1172 loss: 5.45051307e-07
Iter: 1173 loss: 5.45450348e-07
Iter: 1174 loss: 5.4505e-07
Iter: 1175 loss: 5.45027433e-07
Iter: 1176 loss: 5.45040052e-07
Iter: 1177 loss: 5.45024136e-07
Iter: 1178 loss: 5.45015155e-07
Iter: 1179 loss: 5.45006515e-07
Iter: 1180 loss: 5.45008106e-07
Iter: 1181 loss: 5.45011062e-07
Iter: 1182 loss: 5.45003672e-07
Iter: 1183 loss: 5.45012085e-07
Iter: 1184 loss: 5.45008561e-07
Iter: 1185 loss: 5.45005037e-07
Iter: 1186 loss: 5.45007538e-07
Iter: 1187 loss: 5.45009186e-07
Iter: 1188 loss: 5.45006571e-07
Iter: 1189 loss: 5.4500822e-07
Iter: 1190 loss: 5.4500822e-07
Iter: 1191 loss: 5.4500606e-07
Iter: 1192 loss: 5.45008e-07
Iter: 1193 loss: 5.4500731e-07
Iter: 1194 loss: 5.45007254e-07
Iter: 1195 loss: 5.45006287e-07
Iter: 1196 loss: 5.4500714e-07
Iter: 1197 loss: 5.45006856e-07
Iter: 1198 loss: 5.45006856e-07
Iter: 1199 loss: 5.4500714e-07
Iter: 1200 loss: 5.45007083e-07
Iter: 1201 loss: 5.45007197e-07
Iter: 1202 loss: 5.45007197e-07
Iter: 1203 loss: 5.45007197e-07
Iter: 1204 loss: 5.45007083e-07
Iter: 1205 loss: 5.45007083e-07
Iter: 1206 loss: 5.45007197e-07
Iter: 1207 loss: 5.44994123e-07
Iter: 1208 loss: 5.45173918e-07
Iter: 1209 loss: 5.44989348e-07
Iter: 1210 loss: 5.44970135e-07
Iter: 1211 loss: 5.44922045e-07
Iter: 1212 loss: 5.45312218e-07
Iter: 1213 loss: 5.44929435e-07
Iter: 1214 loss: 5.44921761e-07
Iter: 1215 loss: 5.44907152e-07
Iter: 1216 loss: 5.44886689e-07
Iter: 1217 loss: 5.44868897e-07
Iter: 1218 loss: 5.44871455e-07
Iter: 1219 loss: 5.4483246e-07
Iter: 1220 loss: 5.45038688e-07
Iter: 1221 loss: 5.44836382e-07
Iter: 1222 loss: 5.44828481e-07
Iter: 1223 loss: 5.44880891e-07
Iter: 1224 loss: 5.44821489e-07
Iter: 1225 loss: 5.44812451e-07
Iter: 1226 loss: 5.44803e-07
Iter: 1227 loss: 5.44795057e-07
Iter: 1228 loss: 5.44771922e-07
Iter: 1229 loss: 5.45029252e-07
Iter: 1230 loss: 5.44768795e-07
Iter: 1231 loss: 5.44752652e-07
Iter: 1232 loss: 5.44728152e-07
Iter: 1233 loss: 5.44725083e-07
Iter: 1234 loss: 5.44693421e-07
Iter: 1235 loss: 5.44682393e-07
Iter: 1236 loss: 5.44666591e-07
Iter: 1237 loss: 5.44658633e-07
Iter: 1238 loss: 5.44658405e-07
Iter: 1239 loss: 5.44653403e-07
Iter: 1240 loss: 5.44675117e-07
Iter: 1241 loss: 5.44643228e-07
Iter: 1242 loss: 5.44628392e-07
Iter: 1243 loss: 5.44612135e-07
Iter: 1244 loss: 5.44881686e-07
Iter: 1245 loss: 5.44615261e-07
Iter: 1246 loss: 5.44580416e-07
Iter: 1247 loss: 5.4463419e-07
Iter: 1248 loss: 5.44567e-07
Iter: 1249 loss: 5.44544719e-07
Iter: 1250 loss: 5.44548243e-07
Iter: 1251 loss: 5.44527438e-07
Iter: 1252 loss: 5.44497766e-07
Iter: 1253 loss: 5.44998898e-07
Iter: 1254 loss: 5.44495492e-07
Iter: 1255 loss: 5.44453599e-07
Iter: 1256 loss: 5.44900672e-07
Iter: 1257 loss: 5.44456498e-07
Iter: 1258 loss: 5.44436944e-07
Iter: 1259 loss: 5.44535169e-07
Iter: 1260 loss: 5.4443467e-07
Iter: 1261 loss: 5.44419038e-07
Iter: 1262 loss: 5.44419891e-07
Iter: 1263 loss: 5.44401814e-07
Iter: 1264 loss: 5.44369414e-07
Iter: 1265 loss: 5.44620718e-07
Iter: 1266 loss: 5.443693e-07
Iter: 1267 loss: 5.44354521e-07
Iter: 1268 loss: 5.44325701e-07
Iter: 1269 loss: 5.44784655e-07
Iter: 1270 loss: 5.44317743e-07
Iter: 1271 loss: 5.44275963e-07
Iter: 1272 loss: 5.44539603e-07
Iter: 1273 loss: 5.4427295e-07
Iter: 1274 loss: 5.4425135e-07
Iter: 1275 loss: 5.44350883e-07
Iter: 1276 loss: 5.44258114e-07
Iter: 1277 loss: 5.44225884e-07
Iter: 1278 loss: 5.44519708e-07
Iter: 1279 loss: 5.44228044e-07
Iter: 1280 loss: 5.44212583e-07
Iter: 1281 loss: 5.44186719e-07
Iter: 1282 loss: 5.44741056e-07
Iter: 1283 loss: 5.4418075e-07
Iter: 1284 loss: 5.44142438e-07
Iter: 1285 loss: 5.44453542e-07
Iter: 1286 loss: 5.4414204e-07
Iter: 1287 loss: 5.44101511e-07
Iter: 1288 loss: 5.44112822e-07
Iter: 1289 loss: 5.44070872e-07
Iter: 1290 loss: 5.44030968e-07
Iter: 1291 loss: 5.44120837e-07
Iter: 1292 loss: 5.44033924e-07
Iter: 1293 loss: 5.43991405e-07
Iter: 1294 loss: 5.4436282e-07
Iter: 1295 loss: 5.43991518e-07
Iter: 1296 loss: 5.43966621e-07
Iter: 1297 loss: 5.44023e-07
Iter: 1298 loss: 5.43960255e-07
Iter: 1299 loss: 5.4394684e-07
Iter: 1300 loss: 5.44027e-07
Iter: 1301 loss: 5.43940246e-07
Iter: 1302 loss: 5.43917849e-07
Iter: 1303 loss: 5.43934107e-07
Iter: 1304 loss: 5.43911597e-07
Iter: 1305 loss: 5.43885903e-07
Iter: 1306 loss: 5.43855e-07
Iter: 1307 loss: 5.44268119e-07
Iter: 1308 loss: 5.43848898e-07
Iter: 1309 loss: 5.43812575e-07
Iter: 1310 loss: 5.443585e-07
Iter: 1311 loss: 5.43811552e-07
Iter: 1312 loss: 5.43797626e-07
Iter: 1313 loss: 5.4409611e-07
Iter: 1314 loss: 5.43798819e-07
Iter: 1315 loss: 5.4377324e-07
Iter: 1316 loss: 5.43801093e-07
Iter: 1317 loss: 5.4376244e-07
Iter: 1318 loss: 5.43743283e-07
Iter: 1319 loss: 5.43764259e-07
Iter: 1320 loss: 5.43742942e-07
Iter: 1321 loss: 5.43714691e-07
Iter: 1322 loss: 5.43843043e-07
Iter: 1323 loss: 5.43707642e-07
Iter: 1324 loss: 5.43686156e-07
Iter: 1325 loss: 5.43630279e-07
Iter: 1326 loss: 5.4435418e-07
Iter: 1327 loss: 5.43637611e-07
Iter: 1328 loss: 5.43595206e-07
Iter: 1329 loss: 5.43592762e-07
Iter: 1330 loss: 5.43578381e-07
Iter: 1331 loss: 5.43564e-07
Iter: 1332 loss: 5.43548708e-07
Iter: 1333 loss: 5.43504029e-07
Iter: 1334 loss: 5.4367e-07
Iter: 1335 loss: 5.43501642e-07
Iter: 1336 loss: 5.43457475e-07
Iter: 1337 loss: 5.43599299e-07
Iter: 1338 loss: 5.43454235e-07
Iter: 1339 loss: 5.43418366e-07
Iter: 1340 loss: 5.43407964e-07
Iter: 1341 loss: 5.43406031e-07
Iter: 1342 loss: 5.43373403e-07
Iter: 1343 loss: 5.43347369e-07
Iter: 1344 loss: 5.43338558e-07
Iter: 1345 loss: 5.43283e-07
Iter: 1346 loss: 5.43828037e-07
Iter: 1347 loss: 5.43273075e-07
Iter: 1348 loss: 5.43222029e-07
Iter: 1349 loss: 5.43513181e-07
Iter: 1350 loss: 5.43208102e-07
Iter: 1351 loss: 5.43154442e-07
Iter: 1352 loss: 5.43171268e-07
Iter: 1353 loss: 5.43111184e-07
Iter: 1354 loss: 5.43072872e-07
Iter: 1355 loss: 5.43075714e-07
Iter: 1356 loss: 5.4304877e-07
Iter: 1357 loss: 5.4300267e-07
Iter: 1358 loss: 5.43943543e-07
Iter: 1359 loss: 5.43000965e-07
Iter: 1360 loss: 5.42953558e-07
Iter: 1361 loss: 5.42956229e-07
Iter: 1362 loss: 5.42915132e-07
Iter: 1363 loss: 5.4290058e-07
Iter: 1364 loss: 5.42890803e-07
Iter: 1365 loss: 5.42851524e-07
Iter: 1366 loss: 5.4328666e-07
Iter: 1367 loss: 5.42849136e-07
Iter: 1368 loss: 5.42821113e-07
Iter: 1369 loss: 5.42833448e-07
Iter: 1370 loss: 5.42805424e-07
Iter: 1371 loss: 5.42782516e-07
Iter: 1372 loss: 5.42815599e-07
Iter: 1373 loss: 5.42756425e-07
Iter: 1374 loss: 5.42739372e-07
Iter: 1375 loss: 5.42720159e-07
Iter: 1376 loss: 5.42714133e-07
Iter: 1377 loss: 5.42674968e-07
Iter: 1378 loss: 5.42804059e-07
Iter: 1379 loss: 5.42664338e-07
Iter: 1380 loss: 5.42634211e-07
Iter: 1381 loss: 5.42872897e-07
Iter: 1382 loss: 5.42624662e-07
Iter: 1383 loss: 5.42584473e-07
Iter: 1384 loss: 5.42567079e-07
Iter: 1385 loss: 5.42549458e-07
Iter: 1386 loss: 5.42499663e-07
Iter: 1387 loss: 5.42923488e-07
Iter: 1388 loss: 5.42492785e-07
Iter: 1389 loss: 5.42438727e-07
Iter: 1390 loss: 5.42413659e-07
Iter: 1391 loss: 5.42399277e-07
Iter: 1392 loss: 5.42348857e-07
Iter: 1393 loss: 5.42548548e-07
Iter: 1394 loss: 5.42334192e-07
Iter: 1395 loss: 5.42276553e-07
Iter: 1396 loss: 5.42655698e-07
Iter: 1397 loss: 5.42281782e-07
Iter: 1398 loss: 5.42263138e-07
Iter: 1399 loss: 5.42245971e-07
Iter: 1400 loss: 5.42250291e-07
Iter: 1401 loss: 5.42212945e-07
Iter: 1402 loss: 5.42538942e-07
Iter: 1403 loss: 5.42209591e-07
Iter: 1404 loss: 5.42193732e-07
Iter: 1405 loss: 5.42192083e-07
Iter: 1406 loss: 5.42169857e-07
Iter: 1407 loss: 5.42142459e-07
Iter: 1408 loss: 5.42145472e-07
Iter: 1409 loss: 5.42131716e-07
Iter: 1410 loss: 5.42084649e-07
Iter: 1411 loss: 5.42049349e-07
Iter: 1412 loss: 5.42039857e-07
Iter: 1413 loss: 5.42028147e-07
Iter: 1414 loss: 5.42004841e-07
Iter: 1415 loss: 5.41979205e-07
Iter: 1416 loss: 5.41987674e-07
Iter: 1417 loss: 5.41959e-07
Iter: 1418 loss: 5.41936743e-07
Iter: 1419 loss: 5.42220846e-07
Iter: 1420 loss: 5.41933446e-07
Iter: 1421 loss: 5.41907298e-07
Iter: 1422 loss: 5.41870918e-07
Iter: 1423 loss: 5.41871259e-07
Iter: 1424 loss: 5.41819475e-07
Iter: 1425 loss: 5.41803729e-07
Iter: 1426 loss: 5.41771783e-07
Iter: 1427 loss: 5.41716702e-07
Iter: 1428 loss: 5.41713234e-07
Iter: 1429 loss: 5.41677423e-07
Iter: 1430 loss: 5.4158e-07
Iter: 1431 loss: 5.42875398e-07
Iter: 1432 loss: 5.41578572e-07
Iter: 1433 loss: 5.41606369e-07
Iter: 1434 loss: 5.41547649e-07
Iter: 1435 loss: 5.41532927e-07
Iter: 1436 loss: 5.41525424e-07
Iter: 1437 loss: 5.41526731e-07
Iter: 1438 loss: 5.41499105e-07
Iter: 1439 loss: 5.41620807e-07
Iter: 1440 loss: 5.4149757e-07
Iter: 1441 loss: 5.41481938e-07
Iter: 1442 loss: 5.41471252e-07
Iter: 1443 loss: 5.41470627e-07
Iter: 1444 loss: 5.41435952e-07
Iter: 1445 loss: 5.41477107e-07
Iter: 1446 loss: 5.4140861e-07
Iter: 1447 loss: 5.41370468e-07
Iter: 1448 loss: 5.41793611e-07
Iter: 1449 loss: 5.41365239e-07
Iter: 1450 loss: 5.41323402e-07
Iter: 1451 loss: 5.41332099e-07
Iter: 1452 loss: 5.41277473e-07
Iter: 1453 loss: 5.41239842e-07
Iter: 1454 loss: 5.41237e-07
Iter: 1455 loss: 5.41217844e-07
Iter: 1456 loss: 5.41182885e-07
Iter: 1457 loss: 5.41179702e-07
Iter: 1458 loss: 5.41152531e-07
Iter: 1459 loss: 5.41208919e-07
Iter: 1460 loss: 5.41147188e-07
Iter: 1461 loss: 5.41118311e-07
Iter: 1462 loss: 5.41117515e-07
Iter: 1463 loss: 5.41099439e-07
Iter: 1464 loss: 5.41091651e-07
Iter: 1465 loss: 5.41309191e-07
Iter: 1466 loss: 5.41097506e-07
Iter: 1467 loss: 5.41067436e-07
Iter: 1468 loss: 5.41273153e-07
Iter: 1469 loss: 5.41070335e-07
Iter: 1470 loss: 5.41035547e-07
Iter: 1471 loss: 5.41031397e-07
Iter: 1472 loss: 5.41019745e-07
Iter: 1473 loss: 5.40978476e-07
Iter: 1474 loss: 5.40896281e-07
Iter: 1475 loss: 5.42766941e-07
Iter: 1476 loss: 5.40902079e-07
Iter: 1477 loss: 5.40817837e-07
Iter: 1478 loss: 5.41114275e-07
Iter: 1479 loss: 5.40783844e-07
Iter: 1480 loss: 5.4075889e-07
Iter: 1481 loss: 5.40771339e-07
Iter: 1482 loss: 5.40733595e-07
Iter: 1483 loss: 5.40815449e-07
Iter: 1484 loss: 5.40732287e-07
Iter: 1485 loss: 5.40718702e-07
Iter: 1486 loss: 5.40843928e-07
Iter: 1487 loss: 5.40713415e-07
Iter: 1488 loss: 5.40705514e-07
Iter: 1489 loss: 5.40671863e-07
Iter: 1490 loss: 5.40673398e-07
Iter: 1491 loss: 5.40648671e-07
Iter: 1492 loss: 5.40672318e-07
Iter: 1493 loss: 5.40639917e-07
Iter: 1494 loss: 5.40625706e-07
Iter: 1495 loss: 5.4062e-07
Iter: 1496 loss: 5.40597512e-07
Iter: 1497 loss: 5.40561643e-07
Iter: 1498 loss: 5.41037707e-07
Iter: 1499 loss: 5.40560222e-07
Iter: 1500 loss: 5.40545102e-07
Iter: 1501 loss: 5.40544079e-07
Iter: 1502 loss: 5.40532085e-07
Iter: 1503 loss: 5.4051327e-07
Iter: 1504 loss: 5.40507585e-07
Iter: 1505 loss: 5.40474275e-07
Iter: 1506 loss: 5.40492067e-07
Iter: 1507 loss: 5.40445683e-07
Iter: 1508 loss: 5.40384349e-07
Iter: 1509 loss: 5.40336259e-07
Iter: 1510 loss: 5.40323299e-07
Iter: 1511 loss: 5.40255e-07
Iter: 1512 loss: 5.41105464e-07
Iter: 1513 loss: 5.402498e-07
Iter: 1514 loss: 5.40183123e-07
Iter: 1515 loss: 5.40716201e-07
Iter: 1516 loss: 5.4018642e-07
Iter: 1517 loss: 5.40166923e-07
Iter: 1518 loss: 5.40211147e-07
Iter: 1519 loss: 5.40147767e-07
Iter: 1520 loss: 5.40112069e-07
Iter: 1521 loss: 5.40308633e-07
Iter: 1522 loss: 5.40115252e-07
Iter: 1523 loss: 5.40089957e-07
Iter: 1524 loss: 5.40080805e-07
Iter: 1525 loss: 5.40076485e-07
Iter: 1526 loss: 5.40053179e-07
Iter: 1527 loss: 5.40339784e-07
Iter: 1528 loss: 5.40055453e-07
Iter: 1529 loss: 5.4002885e-07
Iter: 1530 loss: 5.40009069e-07
Iter: 1531 loss: 5.39997473e-07
Iter: 1532 loss: 5.39957796e-07
Iter: 1533 loss: 5.39859e-07
Iter: 1534 loss: 5.41355234e-07
Iter: 1535 loss: 5.39857922e-07
Iter: 1536 loss: 5.39784537e-07
Iter: 1537 loss: 5.39790506e-07
Iter: 1538 loss: 5.39771463e-07
Iter: 1539 loss: 5.39756343e-07
Iter: 1540 loss: 5.39741279e-07
Iter: 1541 loss: 5.39694156e-07
Iter: 1542 loss: 5.40487918e-07
Iter: 1543 loss: 5.39688756e-07
Iter: 1544 loss: 5.39651637e-07
Iter: 1545 loss: 5.4000094e-07
Iter: 1546 loss: 5.39657321e-07
Iter: 1547 loss: 5.39617304e-07
Iter: 1548 loss: 5.39600421e-07
Iter: 1549 loss: 5.39591156e-07
Iter: 1550 loss: 5.39563075e-07
Iter: 1551 loss: 5.39561029e-07
Iter: 1552 loss: 5.39541304e-07
Iter: 1553 loss: 5.39551365e-07
Iter: 1554 loss: 5.39531356e-07
Iter: 1555 loss: 5.39506345e-07
Iter: 1556 loss: 5.39860707e-07
Iter: 1557 loss: 5.39513735e-07
Iter: 1558 loss: 5.39495773e-07
Iter: 1559 loss: 5.39474911e-07
Iter: 1560 loss: 5.39866392e-07
Iter: 1561 loss: 5.39475081e-07
Iter: 1562 loss: 5.39459847e-07
Iter: 1563 loss: 5.39450298e-07
Iter: 1564 loss: 5.39436769e-07
Iter: 1565 loss: 5.39372309e-07
Iter: 1566 loss: 5.39729058e-07
Iter: 1567 loss: 5.393739e-07
Iter: 1568 loss: 5.39296195e-07
Iter: 1569 loss: 5.3929034e-07
Iter: 1570 loss: 5.39243729e-07
Iter: 1571 loss: 5.39166876e-07
Iter: 1572 loss: 5.39651353e-07
Iter: 1573 loss: 5.39155621e-07
Iter: 1574 loss: 5.39111568e-07
Iter: 1575 loss: 5.39112193e-07
Iter: 1576 loss: 5.39088546e-07
Iter: 1577 loss: 5.39056714e-07
Iter: 1578 loss: 5.39731786e-07
Iter: 1579 loss: 5.39052508e-07
Iter: 1580 loss: 5.39022778e-07
Iter: 1581 loss: 5.39097471e-07
Iter: 1582 loss: 5.39012831e-07
Iter: 1583 loss: 5.38965196e-07
Iter: 1584 loss: 5.3892586e-07
Iter: 1585 loss: 5.3891074e-07
Iter: 1586 loss: 5.38871e-07
Iter: 1587 loss: 5.3886879e-07
Iter: 1588 loss: 5.38857876e-07
Iter: 1589 loss: 5.38922677e-07
Iter: 1590 loss: 5.38856511e-07
Iter: 1591 loss: 5.38833319e-07
Iter: 1592 loss: 5.38846734e-07
Iter: 1593 loss: 5.38825873e-07
Iter: 1594 loss: 5.38803533e-07
Iter: 1595 loss: 5.3882053e-07
Iter: 1596 loss: 5.38793074e-07
Iter: 1597 loss: 5.38773293e-07
Iter: 1598 loss: 5.38832523e-07
Iter: 1599 loss: 5.38762038e-07
Iter: 1600 loss: 5.38749134e-07
Iter: 1601 loss: 5.38720315e-07
Iter: 1602 loss: 5.38712072e-07
Iter: 1603 loss: 5.38679956e-07
Iter: 1604 loss: 5.38610493e-07
Iter: 1605 loss: 5.38614586e-07
Iter: 1606 loss: 5.38544e-07
Iter: 1607 loss: 5.38893801e-07
Iter: 1608 loss: 5.38539325e-07
Iter: 1609 loss: 5.38536597e-07
Iter: 1610 loss: 5.38507152e-07
Iter: 1611 loss: 5.38489928e-07
Iter: 1612 loss: 5.38458835e-07
Iter: 1613 loss: 5.38791596e-07
Iter: 1614 loss: 5.38460711e-07
Iter: 1615 loss: 5.38429333e-07
Iter: 1616 loss: 5.38527274e-07
Iter: 1617 loss: 5.38407903e-07
Iter: 1618 loss: 5.38374366e-07
Iter: 1619 loss: 5.38481743e-07
Iter: 1620 loss: 5.38361235e-07
Iter: 1621 loss: 5.38332e-07
Iter: 1622 loss: 5.38660515e-07
Iter: 1623 loss: 5.38339805e-07
Iter: 1624 loss: 5.38311156e-07
Iter: 1625 loss: 5.3830604e-07
Iter: 1626 loss: 5.38285462e-07
Iter: 1627 loss: 5.38278869e-07
Iter: 1628 loss: 5.38274151e-07
Iter: 1629 loss: 5.38266079e-07
Iter: 1630 loss: 5.38235099e-07
Iter: 1631 loss: 5.38634e-07
Iter: 1632 loss: 5.3823203e-07
Iter: 1633 loss: 5.38206223e-07
Iter: 1634 loss: 5.38410859e-07
Iter: 1635 loss: 5.38206223e-07
Iter: 1636 loss: 5.38182405e-07
Iter: 1637 loss: 5.38183372e-07
Iter: 1638 loss: 5.38160634e-07
Iter: 1639 loss: 5.38134714e-07
Iter: 1640 loss: 5.38096856e-07
Iter: 1641 loss: 5.38099414e-07
Iter: 1642 loss: 5.38055417e-07
Iter: 1643 loss: 5.38178938e-07
Iter: 1644 loss: 5.3803933e-07
Iter: 1645 loss: 5.38046265e-07
Iter: 1646 loss: 5.38026882e-07
Iter: 1647 loss: 5.38014092e-07
Iter: 1648 loss: 5.37991866e-07
Iter: 1649 loss: 5.38391589e-07
Iter: 1650 loss: 5.37993628e-07
Iter: 1651 loss: 5.37976e-07
Iter: 1652 loss: 5.37968106e-07
Iter: 1653 loss: 5.37963331e-07
Iter: 1654 loss: 5.37920187e-07
Iter: 1655 loss: 5.38164386e-07
Iter: 1656 loss: 5.37910523e-07
Iter: 1657 loss: 5.37879941e-07
Iter: 1658 loss: 5.38093957e-07
Iter: 1659 loss: 5.37884603e-07
Iter: 1660 loss: 5.37867322e-07
Iter: 1661 loss: 5.37834e-07
Iter: 1662 loss: 5.37836513e-07
Iter: 1663 loss: 5.37816334e-07
Iter: 1664 loss: 5.37801498e-07
Iter: 1665 loss: 5.37796723e-07
Iter: 1666 loss: 5.37775179e-07
Iter: 1667 loss: 5.38155689e-07
Iter: 1668 loss: 5.37775691e-07
Iter: 1669 loss: 5.37747042e-07
Iter: 1670 loss: 5.38027166e-07
Iter: 1671 loss: 5.3774761e-07
Iter: 1672 loss: 5.37732035e-07
Iter: 1673 loss: 5.37711458e-07
Iter: 1674 loss: 5.37701737e-07
Iter: 1675 loss: 5.37680876e-07
Iter: 1676 loss: 5.37677124e-07
Iter: 1677 loss: 5.37664391e-07
Iter: 1678 loss: 5.37642052e-07
Iter: 1679 loss: 5.37639721e-07
Iter: 1680 loss: 5.37608571e-07
Iter: 1681 loss: 5.37576e-07
Iter: 1682 loss: 5.37667404e-07
Iter: 1683 loss: 5.37561448e-07
Iter: 1684 loss: 5.3753979e-07
Iter: 1685 loss: 5.37529786e-07
Iter: 1686 loss: 5.37517963e-07
Iter: 1687 loss: 5.37491474e-07
Iter: 1688 loss: 5.37516826e-07
Iter: 1689 loss: 5.37469873e-07
Iter: 1690 loss: 5.37432129e-07
Iter: 1691 loss: 5.3762858e-07
Iter: 1692 loss: 5.37422409e-07
Iter: 1693 loss: 5.3736585e-07
Iter: 1694 loss: 5.37829408e-07
Iter: 1695 loss: 5.373563e-07
Iter: 1696 loss: 5.37338963e-07
Iter: 1697 loss: 5.3738728e-07
Iter: 1698 loss: 5.3733072e-07
Iter: 1699 loss: 5.37298149e-07
Iter: 1700 loss: 5.37269329e-07
Iter: 1701 loss: 5.37262167e-07
Iter: 1702 loss: 5.37240737e-07
Iter: 1703 loss: 5.37659957e-07
Iter: 1704 loss: 5.37240112e-07
Iter: 1705 loss: 5.3721385e-07
Iter: 1706 loss: 5.37273252e-07
Iter: 1707 loss: 5.37210781e-07
Iter: 1708 loss: 5.37205949e-07
Iter: 1709 loss: 5.37202709e-07
Iter: 1710 loss: 5.37199071e-07
Iter: 1711 loss: 5.37180256e-07
Iter: 1712 loss: 5.37262281e-07
Iter: 1713 loss: 5.371827e-07
Iter: 1714 loss: 5.37178266e-07
Iter: 1715 loss: 5.37139613e-07
Iter: 1716 loss: 5.37463507e-07
Iter: 1717 loss: 5.37132905e-07
Iter: 1718 loss: 5.37082e-07
Iter: 1719 loss: 5.37285246e-07
Iter: 1720 loss: 5.37075039e-07
Iter: 1721 loss: 5.37052529e-07
Iter: 1722 loss: 5.37096184e-07
Iter: 1723 loss: 5.37038432e-07
Iter: 1724 loss: 5.37007452e-07
Iter: 1725 loss: 5.37042524e-07
Iter: 1726 loss: 5.37001597e-07
Iter: 1727 loss: 5.37012454e-07
Iter: 1728 loss: 5.36987216e-07
Iter: 1729 loss: 5.36971129e-07
Iter: 1730 loss: 5.36952825e-07
Iter: 1731 loss: 5.37161327e-07
Iter: 1732 loss: 5.36945151e-07
Iter: 1733 loss: 5.36936057e-07
Iter: 1734 loss: 5.36936852e-07
Iter: 1735 loss: 5.36919345e-07
Iter: 1736 loss: 5.36889331e-07
Iter: 1737 loss: 5.37100277e-07
Iter: 1738 loss: 5.3688791e-07
Iter: 1739 loss: 5.36857726e-07
Iter: 1740 loss: 5.37109599e-07
Iter: 1741 loss: 5.36862501e-07
Iter: 1742 loss: 5.36833056e-07
Iter: 1743 loss: 5.3685e-07
Iter: 1744 loss: 5.36833909e-07
Iter: 1745 loss: 5.36823279e-07
Iter: 1746 loss: 5.36906668e-07
Iter: 1747 loss: 5.3682669e-07
Iter: 1748 loss: 5.36806567e-07
Iter: 1749 loss: 5.36858693e-07
Iter: 1750 loss: 5.3680867e-07
Iter: 1751 loss: 5.36795596e-07
Iter: 1752 loss: 5.36763878e-07
Iter: 1753 loss: 5.37029564e-07
Iter: 1754 loss: 5.36760922e-07
Iter: 1755 loss: 5.36731477e-07
Iter: 1756 loss: 5.36685434e-07
Iter: 1757 loss: 5.3667577e-07
Iter: 1758 loss: 5.36629841e-07
Iter: 1759 loss: 5.36601874e-07
Iter: 1760 loss: 5.36592268e-07
Iter: 1761 loss: 5.36544349e-07
Iter: 1762 loss: 5.36725111e-07
Iter: 1763 loss: 5.36532525e-07
Iter: 1764 loss: 5.36515529e-07
Iter: 1765 loss: 5.36563391e-07
Iter: 1766 loss: 5.36493076e-07
Iter: 1767 loss: 5.36481139e-07
Iter: 1768 loss: 5.36701577e-07
Iter: 1769 loss: 5.36476762e-07
Iter: 1770 loss: 5.36464654e-07
Iter: 1771 loss: 5.36454763e-07
Iter: 1772 loss: 5.36448056e-07
Iter: 1773 loss: 5.36432822e-07
Iter: 1774 loss: 5.36504388e-07
Iter: 1775 loss: 5.36420771e-07
Iter: 1776 loss: 5.364e-07
Iter: 1777 loss: 5.36351592e-07
Iter: 1778 loss: 5.37417634e-07
Iter: 1779 loss: 5.36349262e-07
Iter: 1780 loss: 5.36312825e-07
Iter: 1781 loss: 5.3652775e-07
Iter: 1782 loss: 5.36307255e-07
Iter: 1783 loss: 5.36268089e-07
Iter: 1784 loss: 5.36498305e-07
Iter: 1785 loss: 5.36272182e-07
Iter: 1786 loss: 5.36241942e-07
Iter: 1787 loss: 5.36322091e-07
Iter: 1788 loss: 5.36227276e-07
Iter: 1789 loss: 5.36203288e-07
Iter: 1790 loss: 5.3617913e-07
Iter: 1791 loss: 5.36165771e-07
Iter: 1792 loss: 5.36148718e-07
Iter: 1793 loss: 5.36380298e-07
Iter: 1794 loss: 5.36146899e-07
Iter: 1795 loss: 5.36121775e-07
Iter: 1796 loss: 5.3611052e-07
Iter: 1797 loss: 5.36093751e-07
Iter: 1798 loss: 5.36070957e-07
Iter: 1799 loss: 5.3617805e-07
Iter: 1800 loss: 5.36066523e-07
Iter: 1801 loss: 5.3603992e-07
Iter: 1802 loss: 5.36174753e-07
Iter: 1803 loss: 5.36047082e-07
Iter: 1804 loss: 5.36029063e-07
Iter: 1805 loss: 5.36169466e-07
Iter: 1806 loss: 5.36018888e-07
Iter: 1807 loss: 5.3600985e-07
Iter: 1808 loss: 5.35999e-07
Iter: 1809 loss: 5.35997515e-07
Iter: 1810 loss: 5.35961703e-07
Iter: 1811 loss: 5.36078232e-07
Iter: 1812 loss: 5.35961135e-07
Iter: 1813 loss: 5.35940444e-07
Iter: 1814 loss: 5.3592828e-07
Iter: 1815 loss: 5.3592e-07
Iter: 1816 loss: 5.35886045e-07
Iter: 1817 loss: 5.36211871e-07
Iter: 1818 loss: 5.35887864e-07
Iter: 1819 loss: 5.35860352e-07
Iter: 1820 loss: 5.3593692e-07
Iter: 1821 loss: 5.35857112e-07
Iter: 1822 loss: 5.35817094e-07
Iter: 1823 loss: 5.35809193e-07
Iter: 1824 loss: 5.35791344e-07
Iter: 1825 loss: 5.35772188e-07
Iter: 1826 loss: 5.35931292e-07
Iter: 1827 loss: 5.35767128e-07
Iter: 1828 loss: 5.35739503e-07
Iter: 1829 loss: 5.35789752e-07
Iter: 1830 loss: 5.35723871e-07
Iter: 1831 loss: 5.35700451e-07
Iter: 1832 loss: 5.35708864e-07
Iter: 1833 loss: 5.35689537e-07
Iter: 1834 loss: 5.35656909e-07
Iter: 1835 loss: 5.35636673e-07
Iter: 1836 loss: 5.35624395e-07
Iter: 1837 loss: 5.35626782e-07
Iter: 1838 loss: 5.35597053e-07
Iter: 1839 loss: 5.35585229e-07
Iter: 1840 loss: 5.35546064e-07
Iter: 1841 loss: 5.35964261e-07
Iter: 1842 loss: 5.35543052e-07
Iter: 1843 loss: 5.35514857e-07
Iter: 1844 loss: 5.35519121e-07
Iter: 1845 loss: 5.35500703e-07
Iter: 1846 loss: 5.35489448e-07
Iter: 1847 loss: 5.35490472e-07
Iter: 1848 loss: 5.35467052e-07
Iter: 1849 loss: 5.35651452e-07
Iter: 1850 loss: 5.35467279e-07
Iter: 1851 loss: 5.35456252e-07
Iter: 1852 loss: 5.35530603e-07
Iter: 1853 loss: 5.35448805e-07
Iter: 1854 loss: 5.35437607e-07
Iter: 1855 loss: 5.35405889e-07
Iter: 1856 loss: 5.35937261e-07
Iter: 1857 loss: 5.35407139e-07
Iter: 1858 loss: 5.35391337e-07
Iter: 1859 loss: 5.35374852e-07
Iter: 1860 loss: 5.3536445e-07
Iter: 1861 loss: 5.35332958e-07
Iter: 1862 loss: 5.35337563e-07
Iter: 1863 loss: 5.35313e-07
Iter: 1864 loss: 5.35275092e-07
Iter: 1865 loss: 5.35284e-07
Iter: 1866 loss: 5.35265258e-07
Iter: 1867 loss: 5.35259e-07
Iter: 1868 loss: 5.35246215e-07
Iter: 1869 loss: 5.35220295e-07
Iter: 1870 loss: 5.35337506e-07
Iter: 1871 loss: 5.35214099e-07
Iter: 1872 loss: 5.35194772e-07
Iter: 1873 loss: 5.35161234e-07
Iter: 1874 loss: 5.35160439e-07
Iter: 1875 loss: 5.35108938e-07
Iter: 1876 loss: 5.35109677e-07
Iter: 1877 loss: 5.35077788e-07
Iter: 1878 loss: 5.3506426e-07
Iter: 1879 loss: 5.35039192e-07
Iter: 1880 loss: 5.35020433e-07
Iter: 1881 loss: 5.34981837e-07
Iter: 1882 loss: 5.34980927e-07
Iter: 1883 loss: 5.34930393e-07
Iter: 1884 loss: 5.35349841e-07
Iter: 1885 loss: 5.34940796e-07
Iter: 1886 loss: 5.34904871e-07
Iter: 1887 loss: 5.34916126e-07
Iter: 1888 loss: 5.34894298e-07
Iter: 1889 loss: 5.34860874e-07
Iter: 1890 loss: 5.35364734e-07
Iter: 1891 loss: 5.34851836e-07
Iter: 1892 loss: 5.34835749e-07
Iter: 1893 loss: 5.34823414e-07
Iter: 1894 loss: 5.34803803e-07
Iter: 1895 loss: 5.34788455e-07
Iter: 1896 loss: 5.34792775e-07
Iter: 1897 loss: 5.34783055e-07
Iter: 1898 loss: 5.34743208e-07
Iter: 1899 loss: 5.35204776e-07
Iter: 1900 loss: 5.34752814e-07
Iter: 1901 loss: 5.34737239e-07
Iter: 1902 loss: 5.34725132e-07
Iter: 1903 loss: 5.34703531e-07
Iter: 1904 loss: 5.34737637e-07
Iter: 1905 loss: 5.34689434e-07
Iter: 1906 loss: 5.34672267e-07
Iter: 1907 loss: 5.34645608e-07
Iter: 1908 loss: 5.34646233e-07
Iter: 1909 loss: 5.34629578e-07
Iter: 1910 loss: 5.34815399e-07
Iter: 1911 loss: 5.34628668e-07
Iter: 1912 loss: 5.34608091e-07
Iter: 1913 loss: 5.34562332e-07
Iter: 1914 loss: 5.34567448e-07
Iter: 1915 loss: 5.34507876e-07
Iter: 1916 loss: 5.34696653e-07
Iter: 1917 loss: 5.3450043e-07
Iter: 1918 loss: 5.3445558e-07
Iter: 1919 loss: 5.3449412e-07
Iter: 1920 loss: 5.3442028e-07
Iter: 1921 loss: 5.34383162e-07
Iter: 1922 loss: 5.34721153e-07
Iter: 1923 loss: 5.34388164e-07
Iter: 1924 loss: 5.343731e-07
Iter: 1925 loss: 5.34373726e-07
Iter: 1926 loss: 5.34359856e-07
Iter: 1927 loss: 5.34324727e-07
Iter: 1928 loss: 5.34678918e-07
Iter: 1929 loss: 5.34317849e-07
Iter: 1930 loss: 5.34288233e-07
Iter: 1931 loss: 5.34275387e-07
Iter: 1932 loss: 5.34258902e-07
Iter: 1933 loss: 5.34229912e-07
Iter: 1934 loss: 5.34228889e-07
Iter: 1935 loss: 5.34202286e-07
Iter: 1936 loss: 5.34202911e-07
Iter: 1937 loss: 5.34184551e-07
Iter: 1938 loss: 5.3414368e-07
Iter: 1939 loss: 5.3451663e-07
Iter: 1940 loss: 5.34147489e-07
Iter: 1941 loss: 5.34129867e-07
Iter: 1942 loss: 5.34106846e-07
Iter: 1943 loss: 5.34108153e-07
Iter: 1944 loss: 5.34087917e-07
Iter: 1945 loss: 5.34080073e-07
Iter: 1946 loss: 5.34076662e-07
Iter: 1947 loss: 5.34053356e-07
Iter: 1948 loss: 5.34053299e-07
Iter: 1949 loss: 5.3402249e-07
Iter: 1950 loss: 5.3411469e-07
Iter: 1951 loss: 5.34029937e-07
Iter: 1952 loss: 5.34004243e-07
Iter: 1953 loss: 5.34005096e-07
Iter: 1954 loss: 5.3399134e-07
Iter: 1955 loss: 5.33961725e-07
Iter: 1956 loss: 5.34347635e-07
Iter: 1957 loss: 5.33960247e-07
Iter: 1958 loss: 5.33945695e-07
Iter: 1959 loss: 5.33951209e-07
Iter: 1960 loss: 5.33931825e-07
Iter: 1961 loss: 5.33912669e-07
Iter: 1962 loss: 5.33930404e-07
Iter: 1963 loss: 5.33904881e-07
Iter: 1964 loss: 5.33887828e-07
Iter: 1965 loss: 5.33964339e-07
Iter: 1966 loss: 5.3389374e-07
Iter: 1967 loss: 5.33861737e-07
Iter: 1968 loss: 5.33883963e-07
Iter: 1969 loss: 5.33852813e-07
Iter: 1970 loss: 5.33834395e-07
Iter: 1971 loss: 5.34066032e-07
Iter: 1972 loss: 5.33830132e-07
Iter: 1973 loss: 5.33812226e-07
Iter: 1974 loss: 5.33778575e-07
Iter: 1975 loss: 5.34524474e-07
Iter: 1976 loss: 5.33777325e-07
Iter: 1977 loss: 5.33747198e-07
Iter: 1978 loss: 5.33753905e-07
Iter: 1979 loss: 5.33727416e-07
Iter: 1980 loss: 5.33701609e-07
Iter: 1981 loss: 5.3369962e-07
Iter: 1982 loss: 5.33668072e-07
Iter: 1983 loss: 5.33936429e-07
Iter: 1984 loss: 5.33662615e-07
Iter: 1985 loss: 5.33652951e-07
Iter: 1986 loss: 5.33674438e-07
Iter: 1987 loss: 5.33647324e-07
Iter: 1988 loss: 5.33625894e-07
Iter: 1989 loss: 5.33697346e-07
Iter: 1990 loss: 5.33630498e-07
Iter: 1991 loss: 5.33615832e-07
Iter: 1992 loss: 5.33590423e-07
Iter: 1993 loss: 5.33592129e-07
Iter: 1994 loss: 5.33545062e-07
Iter: 1995 loss: 5.33588036e-07
Iter: 1996 loss: 5.33530169e-07
Iter: 1997 loss: 5.3348208e-07
Iter: 1998 loss: 5.3361606e-07
Iter: 1999 loss: 5.33471393e-07
Iter: 2000 loss: 5.33437174e-07
Iter: 2001 loss: 5.33430693e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0.4 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi0.4
+ date
Sat Oct 31 18:29:58 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 0 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50579bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b5048e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b505ed730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50509598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b505091e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b505ed7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50509048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b503ab6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b503ab510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b502981e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b503bb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b5046f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b503d2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b5044c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50445598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b501da510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b501daa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50238ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50236b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50238268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50224620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b503121e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50224b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b0476cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b04790620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50120c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50120488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b500e11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b500710d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50071a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b5005b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50071e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b50071598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b504048c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b046c0158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b04639f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0027336176
test_loss: 0.0026809385
train_loss: 0.0018758749
test_loss: 0.0018901578
train_loss: 0.0014241933
test_loss: 0.001683133
train_loss: 0.0014409001
test_loss: 0.001420652
train_loss: 0.0012943643
test_loss: 0.0013169306
train_loss: 0.0011446598
test_loss: 0.0012760237
train_loss: 0.001107858
test_loss: 0.0011038511
train_loss: 0.0010553251
test_loss: 0.0010911156
train_loss: 0.0010486296
test_loss: 0.0010393913
train_loss: 0.00096685486
test_loss: 0.0010282682
train_loss: 0.0009888674
test_loss: 0.0009871137
train_loss: 0.000972852
test_loss: 0.0010040613
train_loss: 0.00090359827
test_loss: 0.0009523026
train_loss: 0.000923538
test_loss: 0.00097247073
train_loss: 0.00094936695
test_loss: 0.00095120515
train_loss: 0.000892957
test_loss: 0.0009477498
train_loss: 0.0008962061
test_loss: 0.0009231696
train_loss: 0.00091188605
test_loss: 0.0009258955
train_loss: 0.0008725742
test_loss: 0.0009160211
train_loss: 0.00086105877
test_loss: 0.00090282847
train_loss: 0.00083741074
test_loss: 0.0008919333
train_loss: 0.00085580396
test_loss: 0.0008854834
train_loss: 0.00081133284
test_loss: 0.00087987614
train_loss: 0.00083986204
test_loss: 0.0008793157
train_loss: 0.00084204506
test_loss: 0.0008678153
train_loss: 0.00083372823
test_loss: 0.00085582637
train_loss: 0.00082431216
test_loss: 0.00086763396
train_loss: 0.0008175203
test_loss: 0.00085543963
train_loss: 0.0008143295
test_loss: 0.0008575554
train_loss: 0.00085125596
test_loss: 0.00085991586
train_loss: 0.0008322712
test_loss: 0.000855979
train_loss: 0.00079551863
test_loss: 0.0008468419
train_loss: 0.0008035001
test_loss: 0.00084913726
train_loss: 0.00077868975
test_loss: 0.0008445707
train_loss: 0.00078384747
test_loss: 0.0008446727
train_loss: 0.00077239465
test_loss: 0.0008393369
train_loss: 0.00079649914
test_loss: 0.0008439454
train_loss: 0.0007782178
test_loss: 0.00084023405
train_loss: 0.0008024602
test_loss: 0.0008398097
train_loss: 0.00081257476
test_loss: 0.00083851523
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi0.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa87ba95f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa87ba640d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa87bb3ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa87b9e3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa87b9e3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa87b9e3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa87b9e3378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa87b947840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa87b9470d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa87b96dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa87b952268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872f02e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872f02bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872f88488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872eaaea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872f35ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872f4d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872ed9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa87b96d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872e8f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872dbf510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872dbf8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872dbf488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872d1a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872d1aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872d37d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872d18ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872d14158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872d89488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872c752f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872c68730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872c14f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872b9e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872c751e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872b9ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa872b73bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.21661321e-06
Iter: 2 loss: 1.20091283e-06
Iter: 3 loss: 1.19943627e-06
Iter: 4 loss: 1.19288075e-06
Iter: 5 loss: 1.21229198e-06
Iter: 6 loss: 1.19087713e-06
Iter: 7 loss: 1.18802564e-06
Iter: 8 loss: 1.19002823e-06
Iter: 9 loss: 1.18626349e-06
Iter: 10 loss: 1.18414164e-06
Iter: 11 loss: 1.18411072e-06
Iter: 12 loss: 1.18247817e-06
Iter: 13 loss: 1.17973673e-06
Iter: 14 loss: 1.17973491e-06
Iter: 15 loss: 1.17873378e-06
Iter: 16 loss: 1.17841705e-06
Iter: 17 loss: 1.17730872e-06
Iter: 18 loss: 1.17417221e-06
Iter: 19 loss: 1.19033552e-06
Iter: 20 loss: 1.17316313e-06
Iter: 21 loss: 1.16907108e-06
Iter: 22 loss: 1.18150365e-06
Iter: 23 loss: 1.16783451e-06
Iter: 24 loss: 1.16880938e-06
Iter: 25 loss: 1.16640933e-06
Iter: 26 loss: 1.16580941e-06
Iter: 27 loss: 1.16419142e-06
Iter: 28 loss: 1.1748474e-06
Iter: 29 loss: 1.16377919e-06
Iter: 30 loss: 1.16230433e-06
Iter: 31 loss: 1.16608271e-06
Iter: 32 loss: 1.16180195e-06
Iter: 33 loss: 1.16025569e-06
Iter: 34 loss: 1.16068782e-06
Iter: 35 loss: 1.15913372e-06
Iter: 36 loss: 1.15671651e-06
Iter: 37 loss: 1.16353874e-06
Iter: 38 loss: 1.15594253e-06
Iter: 39 loss: 1.15377065e-06
Iter: 40 loss: 1.16736942e-06
Iter: 41 loss: 1.15351736e-06
Iter: 42 loss: 1.15316629e-06
Iter: 43 loss: 1.15279295e-06
Iter: 44 loss: 1.15195189e-06
Iter: 45 loss: 1.14982026e-06
Iter: 46 loss: 1.16937156e-06
Iter: 47 loss: 1.1495099e-06
Iter: 48 loss: 1.14830891e-06
Iter: 49 loss: 1.14828788e-06
Iter: 50 loss: 1.14686e-06
Iter: 51 loss: 1.1470878e-06
Iter: 52 loss: 1.14582303e-06
Iter: 53 loss: 1.14479644e-06
Iter: 54 loss: 1.15138141e-06
Iter: 55 loss: 1.14468821e-06
Iter: 56 loss: 1.14367492e-06
Iter: 57 loss: 1.1457048e-06
Iter: 58 loss: 1.14327463e-06
Iter: 59 loss: 1.14241539e-06
Iter: 60 loss: 1.14278396e-06
Iter: 61 loss: 1.14183149e-06
Iter: 62 loss: 1.14102136e-06
Iter: 63 loss: 1.14757097e-06
Iter: 64 loss: 1.14096883e-06
Iter: 65 loss: 1.14007889e-06
Iter: 66 loss: 1.14049362e-06
Iter: 67 loss: 1.13946226e-06
Iter: 68 loss: 1.1384775e-06
Iter: 69 loss: 1.13606768e-06
Iter: 70 loss: 1.1610631e-06
Iter: 71 loss: 1.13578517e-06
Iter: 72 loss: 1.13360284e-06
Iter: 73 loss: 1.15654132e-06
Iter: 74 loss: 1.1335344e-06
Iter: 75 loss: 1.13235842e-06
Iter: 76 loss: 1.13462397e-06
Iter: 77 loss: 1.13186047e-06
Iter: 78 loss: 1.13096155e-06
Iter: 79 loss: 1.13848625e-06
Iter: 80 loss: 1.13092801e-06
Iter: 81 loss: 1.13079602e-06
Iter: 82 loss: 1.13058388e-06
Iter: 83 loss: 1.13028091e-06
Iter: 84 loss: 1.12943871e-06
Iter: 85 loss: 1.13256988e-06
Iter: 86 loss: 1.12904854e-06
Iter: 87 loss: 1.12945281e-06
Iter: 88 loss: 1.1286985e-06
Iter: 89 loss: 1.12841417e-06
Iter: 90 loss: 1.12781368e-06
Iter: 91 loss: 1.13903229e-06
Iter: 92 loss: 1.1278089e-06
Iter: 93 loss: 1.1274301e-06
Iter: 94 loss: 1.13247881e-06
Iter: 95 loss: 1.12742214e-06
Iter: 96 loss: 1.12701332e-06
Iter: 97 loss: 1.12665793e-06
Iter: 98 loss: 1.12652913e-06
Iter: 99 loss: 1.12612508e-06
Iter: 100 loss: 1.12790394e-06
Iter: 101 loss: 1.12604755e-06
Iter: 102 loss: 1.12554608e-06
Iter: 103 loss: 1.12719226e-06
Iter: 104 loss: 1.12539135e-06
Iter: 105 loss: 1.12513771e-06
Iter: 106 loss: 1.12506825e-06
Iter: 107 loss: 1.12492103e-06
Iter: 108 loss: 1.12455723e-06
Iter: 109 loss: 1.12574344e-06
Iter: 110 loss: 1.12443945e-06
Iter: 111 loss: 1.12421628e-06
Iter: 112 loss: 1.12433e-06
Iter: 113 loss: 1.12405701e-06
Iter: 114 loss: 1.12372527e-06
Iter: 115 loss: 1.12377813e-06
Iter: 116 loss: 1.12347016e-06
Iter: 117 loss: 1.12311682e-06
Iter: 118 loss: 1.12310067e-06
Iter: 119 loss: 1.12285727e-06
Iter: 120 loss: 1.12228565e-06
Iter: 121 loss: 1.12730322e-06
Iter: 122 loss: 1.1221963e-06
Iter: 123 loss: 1.12213479e-06
Iter: 124 loss: 1.12189286e-06
Iter: 125 loss: 1.12162252e-06
Iter: 126 loss: 1.12102589e-06
Iter: 127 loss: 1.12897567e-06
Iter: 128 loss: 1.12097064e-06
Iter: 129 loss: 1.12090947e-06
Iter: 130 loss: 1.12078408e-06
Iter: 131 loss: 1.12062639e-06
Iter: 132 loss: 1.12043131e-06
Iter: 133 loss: 1.12041312e-06
Iter: 134 loss: 1.12024918e-06
Iter: 135 loss: 1.12099065e-06
Iter: 136 loss: 1.12021212e-06
Iter: 137 loss: 1.11995814e-06
Iter: 138 loss: 1.11993097e-06
Iter: 139 loss: 1.11976317e-06
Iter: 140 loss: 1.11948816e-06
Iter: 141 loss: 1.11892314e-06
Iter: 142 loss: 1.12929661e-06
Iter: 143 loss: 1.118932e-06
Iter: 144 loss: 1.11817076e-06
Iter: 145 loss: 1.11943586e-06
Iter: 146 loss: 1.11784902e-06
Iter: 147 loss: 1.11717918e-06
Iter: 148 loss: 1.12444536e-06
Iter: 149 loss: 1.11716076e-06
Iter: 150 loss: 1.11690451e-06
Iter: 151 loss: 1.11688087e-06
Iter: 152 loss: 1.11667782e-06
Iter: 153 loss: 1.11686336e-06
Iter: 154 loss: 1.116568e-06
Iter: 155 loss: 1.1163404e-06
Iter: 156 loss: 1.11607085e-06
Iter: 157 loss: 1.11606221e-06
Iter: 158 loss: 1.11586019e-06
Iter: 159 loss: 1.11582028e-06
Iter: 160 loss: 1.1156161e-06
Iter: 161 loss: 1.11526754e-06
Iter: 162 loss: 1.12391717e-06
Iter: 163 loss: 1.1152722e-06
Iter: 164 loss: 1.11502e-06
Iter: 165 loss: 1.11717691e-06
Iter: 166 loss: 1.11499935e-06
Iter: 167 loss: 1.11467466e-06
Iter: 168 loss: 1.11458189e-06
Iter: 169 loss: 1.11438158e-06
Iter: 170 loss: 1.11406689e-06
Iter: 171 loss: 1.11420138e-06
Iter: 172 loss: 1.11387317e-06
Iter: 173 loss: 1.11361726e-06
Iter: 174 loss: 1.1135869e-06
Iter: 175 loss: 1.11342081e-06
Iter: 176 loss: 1.11294662e-06
Iter: 177 loss: 1.11503505e-06
Iter: 178 loss: 1.11275983e-06
Iter: 179 loss: 1.1122163e-06
Iter: 180 loss: 1.11371855e-06
Iter: 181 loss: 1.11204145e-06
Iter: 182 loss: 1.11166537e-06
Iter: 183 loss: 1.11368786e-06
Iter: 184 loss: 1.11160239e-06
Iter: 185 loss: 1.11134204e-06
Iter: 186 loss: 1.11134614e-06
Iter: 187 loss: 1.11112945e-06
Iter: 188 loss: 1.11077975e-06
Iter: 189 loss: 1.11182044e-06
Iter: 190 loss: 1.11066663e-06
Iter: 191 loss: 1.1103441e-06
Iter: 192 loss: 1.11033751e-06
Iter: 193 loss: 1.11000486e-06
Iter: 194 loss: 1.10994779e-06
Iter: 195 loss: 1.10970302e-06
Iter: 196 loss: 1.10920337e-06
Iter: 197 loss: 1.10895007e-06
Iter: 198 loss: 1.10871406e-06
Iter: 199 loss: 1.10856899e-06
Iter: 200 loss: 1.10846554e-06
Iter: 201 loss: 1.10823737e-06
Iter: 202 loss: 1.10784811e-06
Iter: 203 loss: 1.10784e-06
Iter: 204 loss: 1.10763733e-06
Iter: 205 loss: 1.11028066e-06
Iter: 206 loss: 1.1076263e-06
Iter: 207 loss: 1.10740302e-06
Iter: 208 loss: 1.10788847e-06
Iter: 209 loss: 1.10731207e-06
Iter: 210 loss: 1.10716633e-06
Iter: 211 loss: 1.10685892e-06
Iter: 212 loss: 1.11149097e-06
Iter: 213 loss: 1.10683459e-06
Iter: 214 loss: 1.10667088e-06
Iter: 215 loss: 1.10664507e-06
Iter: 216 loss: 1.10639689e-06
Iter: 217 loss: 1.10603878e-06
Iter: 218 loss: 1.10601e-06
Iter: 219 loss: 1.10576366e-06
Iter: 220 loss: 1.10567919e-06
Iter: 221 loss: 1.10553981e-06
Iter: 222 loss: 1.10522228e-06
Iter: 223 loss: 1.10531573e-06
Iter: 224 loss: 1.10499184e-06
Iter: 225 loss: 1.10503447e-06
Iter: 226 loss: 1.10483188e-06
Iter: 227 loss: 1.10468795e-06
Iter: 228 loss: 1.1044026e-06
Iter: 229 loss: 1.10842518e-06
Iter: 230 loss: 1.10439123e-06
Iter: 231 loss: 1.10405449e-06
Iter: 232 loss: 1.10530505e-06
Iter: 233 loss: 1.10397104e-06
Iter: 234 loss: 1.10371093e-06
Iter: 235 loss: 1.10723886e-06
Iter: 236 loss: 1.1037182e-06
Iter: 237 loss: 1.1035263e-06
Iter: 238 loss: 1.10357701e-06
Iter: 239 loss: 1.1033859e-06
Iter: 240 loss: 1.10323356e-06
Iter: 241 loss: 1.1032605e-06
Iter: 242 loss: 1.10312317e-06
Iter: 243 loss: 1.1028626e-06
Iter: 244 loss: 1.10547899e-06
Iter: 245 loss: 1.1028585e-06
Iter: 246 loss: 1.10272606e-06
Iter: 247 loss: 1.1023435e-06
Iter: 248 loss: 1.10336077e-06
Iter: 249 loss: 1.10214296e-06
Iter: 250 loss: 1.10188103e-06
Iter: 251 loss: 1.10184169e-06
Iter: 252 loss: 1.10148574e-06
Iter: 253 loss: 1.10217456e-06
Iter: 254 loss: 1.10132009e-06
Iter: 255 loss: 1.10114627e-06
Iter: 256 loss: 1.10104384e-06
Iter: 257 loss: 1.10097415e-06
Iter: 258 loss: 1.10085853e-06
Iter: 259 loss: 1.10212977e-06
Iter: 260 loss: 1.10084238e-06
Iter: 261 loss: 1.10067299e-06
Iter: 262 loss: 1.10127394e-06
Iter: 263 loss: 1.10063729e-06
Iter: 264 loss: 1.10056169e-06
Iter: 265 loss: 1.10041969e-06
Iter: 266 loss: 1.10041674e-06
Iter: 267 loss: 1.10024052e-06
Iter: 268 loss: 1.10197504e-06
Iter: 269 loss: 1.10022802e-06
Iter: 270 loss: 1.10006158e-06
Iter: 271 loss: 1.100193e-06
Iter: 272 loss: 1.099972e-06
Iter: 273 loss: 1.09974121e-06
Iter: 274 loss: 1.09964822e-06
Iter: 275 loss: 1.09954681e-06
Iter: 276 loss: 1.09940231e-06
Iter: 277 loss: 1.09939469e-06
Iter: 278 loss: 1.09925475e-06
Iter: 279 loss: 1.09909217e-06
Iter: 280 loss: 1.09907e-06
Iter: 281 loss: 1.09894324e-06
Iter: 282 loss: 1.09891016e-06
Iter: 283 loss: 1.09882103e-06
Iter: 284 loss: 1.09877953e-06
Iter: 285 loss: 1.09873486e-06
Iter: 286 loss: 1.09862754e-06
Iter: 287 loss: 1.0983747e-06
Iter: 288 loss: 1.10059602e-06
Iter: 289 loss: 1.09833104e-06
Iter: 290 loss: 1.09811526e-06
Iter: 291 loss: 1.0982003e-06
Iter: 292 loss: 1.09796144e-06
Iter: 293 loss: 1.09786356e-06
Iter: 294 loss: 1.09782297e-06
Iter: 295 loss: 1.09766552e-06
Iter: 296 loss: 1.09764778e-06
Iter: 297 loss: 1.09752705e-06
Iter: 298 loss: 1.0973863e-06
Iter: 299 loss: 1.0973547e-06
Iter: 300 loss: 1.09731377e-06
Iter: 301 loss: 1.09712516e-06
Iter: 302 loss: 1.09712346e-06
Iter: 303 loss: 1.09700125e-06
Iter: 304 loss: 1.09687903e-06
Iter: 305 loss: 1.0968422e-06
Iter: 306 loss: 1.0966628e-06
Iter: 307 loss: 1.09760106e-06
Iter: 308 loss: 1.09663642e-06
Iter: 309 loss: 1.09653365e-06
Iter: 310 loss: 1.09831399e-06
Iter: 311 loss: 1.09653763e-06
Iter: 312 loss: 1.09646453e-06
Iter: 313 loss: 1.09632492e-06
Iter: 314 loss: 1.09776079e-06
Iter: 315 loss: 1.09631071e-06
Iter: 316 loss: 1.09617713e-06
Iter: 317 loss: 1.09645975e-06
Iter: 318 loss: 1.09611187e-06
Iter: 319 loss: 1.09610346e-06
Iter: 320 loss: 1.09602388e-06
Iter: 321 loss: 1.09597977e-06
Iter: 322 loss: 1.09583357e-06
Iter: 323 loss: 1.09591497e-06
Iter: 324 loss: 1.09568373e-06
Iter: 325 loss: 1.09538314e-06
Iter: 326 loss: 1.09683765e-06
Iter: 327 loss: 1.09531459e-06
Iter: 328 loss: 1.09532323e-06
Iter: 329 loss: 1.09520249e-06
Iter: 330 loss: 1.09514781e-06
Iter: 331 loss: 1.09497591e-06
Iter: 332 loss: 1.09627126e-06
Iter: 333 loss: 1.09495204e-06
Iter: 334 loss: 1.09485745e-06
Iter: 335 loss: 1.09485904e-06
Iter: 336 loss: 1.09474354e-06
Iter: 337 loss: 1.0947947e-06
Iter: 338 loss: 1.09467328e-06
Iter: 339 loss: 1.09460711e-06
Iter: 340 loss: 1.09490213e-06
Iter: 341 loss: 1.09458199e-06
Iter: 342 loss: 1.09450355e-06
Iter: 343 loss: 1.09465873e-06
Iter: 344 loss: 1.09446364e-06
Iter: 345 loss: 1.09436894e-06
Iter: 346 loss: 1.09443454e-06
Iter: 347 loss: 1.0943088e-06
Iter: 348 loss: 1.09422308e-06
Iter: 349 loss: 1.09417761e-06
Iter: 350 loss: 1.09413622e-06
Iter: 351 loss: 1.09403197e-06
Iter: 352 loss: 1.09468488e-06
Iter: 353 loss: 1.09399605e-06
Iter: 354 loss: 1.09385132e-06
Iter: 355 loss: 1.09431107e-06
Iter: 356 loss: 1.09380881e-06
Iter: 357 loss: 1.09371467e-06
Iter: 358 loss: 1.09352e-06
Iter: 359 loss: 1.09678422e-06
Iter: 360 loss: 1.09352118e-06
Iter: 361 loss: 1.09343989e-06
Iter: 362 loss: 1.09341897e-06
Iter: 363 loss: 1.09330483e-06
Iter: 364 loss: 1.09338453e-06
Iter: 365 loss: 1.09324128e-06
Iter: 366 loss: 1.09317523e-06
Iter: 367 loss: 1.09315533e-06
Iter: 368 loss: 1.093121e-06
Iter: 369 loss: 1.09306313e-06
Iter: 370 loss: 1.09306166e-06
Iter: 371 loss: 1.09303028e-06
Iter: 372 loss: 1.09295297e-06
Iter: 373 loss: 1.09368705e-06
Iter: 374 loss: 1.092915e-06
Iter: 375 loss: 1.09282678e-06
Iter: 376 loss: 1.09422854e-06
Iter: 377 loss: 1.09282064e-06
Iter: 378 loss: 1.09270047e-06
Iter: 379 loss: 1.09274242e-06
Iter: 380 loss: 1.09263874e-06
Iter: 381 loss: 1.09247412e-06
Iter: 382 loss: 1.09220241e-06
Iter: 383 loss: 1.09905193e-06
Iter: 384 loss: 1.09219945e-06
Iter: 385 loss: 1.09190808e-06
Iter: 386 loss: 1.09237726e-06
Iter: 387 loss: 1.09178541e-06
Iter: 388 loss: 1.09166388e-06
Iter: 389 loss: 1.09166137e-06
Iter: 390 loss: 1.09156167e-06
Iter: 391 loss: 1.09156599e-06
Iter: 392 loss: 1.09152734e-06
Iter: 393 loss: 1.0913983e-06
Iter: 394 loss: 1.09193047e-06
Iter: 395 loss: 1.09135385e-06
Iter: 396 loss: 1.09133384e-06
Iter: 397 loss: 1.09128757e-06
Iter: 398 loss: 1.09121322e-06
Iter: 399 loss: 1.0910303e-06
Iter: 400 loss: 1.09383e-06
Iter: 401 loss: 1.09103144e-06
Iter: 402 loss: 1.09083192e-06
Iter: 403 loss: 1.09088342e-06
Iter: 404 loss: 1.09071095e-06
Iter: 405 loss: 1.0904248e-06
Iter: 406 loss: 1.09383063e-06
Iter: 407 loss: 1.0904173e-06
Iter: 408 loss: 1.09030111e-06
Iter: 409 loss: 1.09029247e-06
Iter: 410 loss: 1.09021084e-06
Iter: 411 loss: 1.09007919e-06
Iter: 412 loss: 1.09178927e-06
Iter: 413 loss: 1.09007476e-06
Iter: 414 loss: 1.09000894e-06
Iter: 415 loss: 1.08999188e-06
Iter: 416 loss: 1.08995778e-06
Iter: 417 loss: 1.08982545e-06
Iter: 418 loss: 1.08972858e-06
Iter: 419 loss: 1.08970471e-06
Iter: 420 loss: 1.08956181e-06
Iter: 421 loss: 1.08997165e-06
Iter: 422 loss: 1.08949541e-06
Iter: 423 loss: 1.08942618e-06
Iter: 424 loss: 1.08942697e-06
Iter: 425 loss: 1.08934785e-06
Iter: 426 loss: 1.08941049e-06
Iter: 427 loss: 1.0892926e-06
Iter: 428 loss: 1.08923246e-06
Iter: 429 loss: 1.08921927e-06
Iter: 430 loss: 1.08916606e-06
Iter: 431 loss: 1.08912548e-06
Iter: 432 loss: 1.08911786e-06
Iter: 433 loss: 1.08908193e-06
Iter: 434 loss: 1.08893471e-06
Iter: 435 loss: 1.08916174e-06
Iter: 436 loss: 1.08885592e-06
Iter: 437 loss: 1.08875861e-06
Iter: 438 loss: 1.08875247e-06
Iter: 439 loss: 1.08863367e-06
Iter: 440 loss: 1.08859842e-06
Iter: 441 loss: 1.08852612e-06
Iter: 442 loss: 1.08841402e-06
Iter: 443 loss: 1.08991242e-06
Iter: 444 loss: 1.08841459e-06
Iter: 445 loss: 1.08836161e-06
Iter: 446 loss: 1.0882577e-06
Iter: 447 loss: 1.0882668e-06
Iter: 448 loss: 1.08816562e-06
Iter: 449 loss: 1.08840482e-06
Iter: 450 loss: 1.08813333e-06
Iter: 451 loss: 1.08808069e-06
Iter: 452 loss: 1.08801248e-06
Iter: 453 loss: 1.08801646e-06
Iter: 454 loss: 1.0878947e-06
Iter: 455 loss: 1.08794052e-06
Iter: 456 loss: 1.08783661e-06
Iter: 457 loss: 1.08768552e-06
Iter: 458 loss: 1.08825679e-06
Iter: 459 loss: 1.08765403e-06
Iter: 460 loss: 1.08751124e-06
Iter: 461 loss: 1.08750316e-06
Iter: 462 loss: 1.08740596e-06
Iter: 463 loss: 1.08731945e-06
Iter: 464 loss: 1.08731695e-06
Iter: 465 loss: 1.08726908e-06
Iter: 466 loss: 1.08710537e-06
Iter: 467 loss: 1.08845825e-06
Iter: 468 loss: 1.08707832e-06
Iter: 469 loss: 1.08695e-06
Iter: 470 loss: 1.08724464e-06
Iter: 471 loss: 1.0868979e-06
Iter: 472 loss: 1.08677773e-06
Iter: 473 loss: 1.08690347e-06
Iter: 474 loss: 1.08669428e-06
Iter: 475 loss: 1.08669292e-06
Iter: 476 loss: 1.08663301e-06
Iter: 477 loss: 1.08657468e-06
Iter: 478 loss: 1.08654967e-06
Iter: 479 loss: 1.08651648e-06
Iter: 480 loss: 1.08647714e-06
Iter: 481 loss: 1.08692768e-06
Iter: 482 loss: 1.08647293e-06
Iter: 483 loss: 1.08641746e-06
Iter: 484 loss: 1.08630604e-06
Iter: 485 loss: 1.08861559e-06
Iter: 486 loss: 1.08630957e-06
Iter: 487 loss: 1.08618053e-06
Iter: 488 loss: 1.08621589e-06
Iter: 489 loss: 1.08609697e-06
Iter: 490 loss: 1.08595873e-06
Iter: 491 loss: 1.08596657e-06
Iter: 492 loss: 1.08585891e-06
Iter: 493 loss: 1.08606889e-06
Iter: 494 loss: 1.08581162e-06
Iter: 495 loss: 1.08567747e-06
Iter: 496 loss: 1.08601216e-06
Iter: 497 loss: 1.08564336e-06
Iter: 498 loss: 1.08553979e-06
Iter: 499 loss: 1.08589188e-06
Iter: 500 loss: 1.08551797e-06
Iter: 501 loss: 1.08543281e-06
Iter: 502 loss: 1.08608651e-06
Iter: 503 loss: 1.08542008e-06
Iter: 504 loss: 1.08535914e-06
Iter: 505 loss: 1.08525194e-06
Iter: 506 loss: 1.08649363e-06
Iter: 507 loss: 1.08525182e-06
Iter: 508 loss: 1.08513905e-06
Iter: 509 loss: 1.08543304e-06
Iter: 510 loss: 1.08507493e-06
Iter: 511 loss: 1.08501467e-06
Iter: 512 loss: 1.08500512e-06
Iter: 513 loss: 1.08493714e-06
Iter: 514 loss: 1.08484301e-06
Iter: 515 loss: 1.08484767e-06
Iter: 516 loss: 1.08475e-06
Iter: 517 loss: 1.08608049e-06
Iter: 518 loss: 1.08474717e-06
Iter: 519 loss: 1.08464326e-06
Iter: 520 loss: 1.08456345e-06
Iter: 521 loss: 1.0845456e-06
Iter: 522 loss: 1.08445465e-06
Iter: 523 loss: 1.08443726e-06
Iter: 524 loss: 1.08436825e-06
Iter: 525 loss: 1.08427639e-06
Iter: 526 loss: 1.08428617e-06
Iter: 527 loss: 1.0841967e-06
Iter: 528 loss: 1.08423433e-06
Iter: 529 loss: 1.08414361e-06
Iter: 530 loss: 1.08407085e-06
Iter: 531 loss: 1.08448614e-06
Iter: 532 loss: 1.08406925e-06
Iter: 533 loss: 1.08401321e-06
Iter: 534 loss: 1.08410313e-06
Iter: 535 loss: 1.08398115e-06
Iter: 536 loss: 1.08390623e-06
Iter: 537 loss: 1.08393579e-06
Iter: 538 loss: 1.08384461e-06
Iter: 539 loss: 1.08376946e-06
Iter: 540 loss: 1.08369386e-06
Iter: 541 loss: 1.08367328e-06
Iter: 542 loss: 1.08352594e-06
Iter: 543 loss: 1.08339736e-06
Iter: 544 loss: 1.08339248e-06
Iter: 545 loss: 1.08331415e-06
Iter: 546 loss: 1.08324605e-06
Iter: 547 loss: 1.08311679e-06
Iter: 548 loss: 1.08304789e-06
Iter: 549 loss: 1.08298218e-06
Iter: 550 loss: 1.08289203e-06
Iter: 551 loss: 1.08359302e-06
Iter: 552 loss: 1.08287043e-06
Iter: 553 loss: 1.08277095e-06
Iter: 554 loss: 1.08293239e-06
Iter: 555 loss: 1.08272104e-06
Iter: 556 loss: 1.08263976e-06
Iter: 557 loss: 1.08261008e-06
Iter: 558 loss: 1.08257802e-06
Iter: 559 loss: 1.08252732e-06
Iter: 560 loss: 1.08251731e-06
Iter: 561 loss: 1.082469e-06
Iter: 562 loss: 1.08237487e-06
Iter: 563 loss: 1.08387349e-06
Iter: 564 loss: 1.08237862e-06
Iter: 565 loss: 1.08228733e-06
Iter: 566 loss: 1.08346353e-06
Iter: 567 loss: 1.08227755e-06
Iter: 568 loss: 1.08219433e-06
Iter: 569 loss: 1.08219888e-06
Iter: 570 loss: 1.08212566e-06
Iter: 571 loss: 1.08200675e-06
Iter: 572 loss: 1.08186737e-06
Iter: 573 loss: 1.08184599e-06
Iter: 574 loss: 1.08169615e-06
Iter: 575 loss: 1.08177665e-06
Iter: 576 loss: 1.08159497e-06
Iter: 577 loss: 1.08142729e-06
Iter: 578 loss: 1.08219092e-06
Iter: 579 loss: 1.08138158e-06
Iter: 580 loss: 1.08142319e-06
Iter: 581 loss: 1.08134418e-06
Iter: 582 loss: 1.08129643e-06
Iter: 583 loss: 1.0811973e-06
Iter: 584 loss: 1.08212521e-06
Iter: 585 loss: 1.08116501e-06
Iter: 586 loss: 1.08107758e-06
Iter: 587 loss: 1.08217796e-06
Iter: 588 loss: 1.0810711e-06
Iter: 589 loss: 1.08097947e-06
Iter: 590 loss: 1.08118593e-06
Iter: 591 loss: 1.08093309e-06
Iter: 592 loss: 1.08087204e-06
Iter: 593 loss: 1.08091035e-06
Iter: 594 loss: 1.08082895e-06
Iter: 595 loss: 1.08075017e-06
Iter: 596 loss: 1.08158224e-06
Iter: 597 loss: 1.08075301e-06
Iter: 598 loss: 1.08068161e-06
Iter: 599 loss: 1.08062818e-06
Iter: 600 loss: 1.08060055e-06
Iter: 601 loss: 1.08049744e-06
Iter: 602 loss: 1.08178563e-06
Iter: 603 loss: 1.08049937e-06
Iter: 604 loss: 1.08045481e-06
Iter: 605 loss: 1.08042957e-06
Iter: 606 loss: 1.08038319e-06
Iter: 607 loss: 1.08029917e-06
Iter: 608 loss: 1.08024096e-06
Iter: 609 loss: 1.08019731e-06
Iter: 610 loss: 1.08005747e-06
Iter: 611 loss: 1.0800785e-06
Iter: 612 loss: 1.07999142e-06
Iter: 613 loss: 1.07989126e-06
Iter: 614 loss: 1.07988876e-06
Iter: 615 loss: 1.07980873e-06
Iter: 616 loss: 1.08074607e-06
Iter: 617 loss: 1.07980736e-06
Iter: 618 loss: 1.07977598e-06
Iter: 619 loss: 1.07966991e-06
Iter: 620 loss: 1.08022232e-06
Iter: 621 loss: 1.07965593e-06
Iter: 622 loss: 1.07965138e-06
Iter: 623 loss: 1.07959329e-06
Iter: 624 loss: 1.07954293e-06
Iter: 625 loss: 1.07945527e-06
Iter: 626 loss: 1.08151335e-06
Iter: 627 loss: 1.07945129e-06
Iter: 628 loss: 1.07940195e-06
Iter: 629 loss: 1.07938627e-06
Iter: 630 loss: 1.07932817e-06
Iter: 631 loss: 1.07930737e-06
Iter: 632 loss: 1.07928065e-06
Iter: 633 loss: 1.07921892e-06
Iter: 634 loss: 1.07983146e-06
Iter: 635 loss: 1.07922756e-06
Iter: 636 loss: 1.07916594e-06
Iter: 637 loss: 1.07911922e-06
Iter: 638 loss: 1.07911478e-06
Iter: 639 loss: 1.0790493e-06
Iter: 640 loss: 1.07957692e-06
Iter: 641 loss: 1.07904634e-06
Iter: 642 loss: 1.07897813e-06
Iter: 643 loss: 1.07897426e-06
Iter: 644 loss: 1.07894084e-06
Iter: 645 loss: 1.07888991e-06
Iter: 646 loss: 1.07941753e-06
Iter: 647 loss: 1.07887445e-06
Iter: 648 loss: 1.07883011e-06
Iter: 649 loss: 1.07882738e-06
Iter: 650 loss: 1.07879521e-06
Iter: 651 loss: 1.07873939e-06
Iter: 652 loss: 1.07982692e-06
Iter: 653 loss: 1.07871961e-06
Iter: 654 loss: 1.07864048e-06
Iter: 655 loss: 1.07882579e-06
Iter: 656 loss: 1.07864014e-06
Iter: 657 loss: 1.0786008e-06
Iter: 658 loss: 1.07858375e-06
Iter: 659 loss: 1.07855305e-06
Iter: 660 loss: 1.07847904e-06
Iter: 661 loss: 1.07944379e-06
Iter: 662 loss: 1.07847723e-06
Iter: 663 loss: 1.0784529e-06
Iter: 664 loss: 1.07842425e-06
Iter: 665 loss: 1.07839753e-06
Iter: 666 loss: 1.07832989e-06
Iter: 667 loss: 1.07947562e-06
Iter: 668 loss: 1.07832761e-06
Iter: 669 loss: 1.07828964e-06
Iter: 670 loss: 1.07828009e-06
Iter: 671 loss: 1.07826475e-06
Iter: 672 loss: 1.07820495e-06
Iter: 673 loss: 1.0785252e-06
Iter: 674 loss: 1.07816936e-06
Iter: 675 loss: 1.07812684e-06
Iter: 676 loss: 1.07812093e-06
Iter: 677 loss: 1.0780816e-06
Iter: 678 loss: 1.07818346e-06
Iter: 679 loss: 1.07806818e-06
Iter: 680 loss: 1.07802293e-06
Iter: 681 loss: 1.07822211e-06
Iter: 682 loss: 1.07801429e-06
Iter: 683 loss: 1.07798405e-06
Iter: 684 loss: 1.07794244e-06
Iter: 685 loss: 1.07792357e-06
Iter: 686 loss: 1.07787741e-06
Iter: 687 loss: 1.07794199e-06
Iter: 688 loss: 1.07783194e-06
Iter: 689 loss: 1.07774576e-06
Iter: 690 loss: 1.07766982e-06
Iter: 691 loss: 1.07765572e-06
Iter: 692 loss: 1.07753385e-06
Iter: 693 loss: 1.07836649e-06
Iter: 694 loss: 1.07754101e-06
Iter: 695 loss: 1.07748679e-06
Iter: 696 loss: 1.07749247e-06
Iter: 697 loss: 1.07744961e-06
Iter: 698 loss: 1.07739265e-06
Iter: 699 loss: 1.07738265e-06
Iter: 700 loss: 1.07734138e-06
Iter: 701 loss: 1.07733149e-06
Iter: 702 loss: 1.07731239e-06
Iter: 703 loss: 1.07721587e-06
Iter: 704 loss: 1.07837513e-06
Iter: 705 loss: 1.07720552e-06
Iter: 706 loss: 1.07717256e-06
Iter: 707 loss: 1.07716937e-06
Iter: 708 loss: 1.07712322e-06
Iter: 709 loss: 1.07708297e-06
Iter: 710 loss: 1.07706455e-06
Iter: 711 loss: 1.07700271e-06
Iter: 712 loss: 1.07742937e-06
Iter: 713 loss: 1.07698611e-06
Iter: 714 loss: 1.07694666e-06
Iter: 715 loss: 1.07700237e-06
Iter: 716 loss: 1.07692176e-06
Iter: 717 loss: 1.07685082e-06
Iter: 718 loss: 1.07714652e-06
Iter: 719 loss: 1.07684139e-06
Iter: 720 loss: 1.07677454e-06
Iter: 721 loss: 1.07679773e-06
Iter: 722 loss: 1.0767327e-06
Iter: 723 loss: 1.07665164e-06
Iter: 724 loss: 1.07658263e-06
Iter: 725 loss: 1.07656933e-06
Iter: 726 loss: 1.07648646e-06
Iter: 727 loss: 1.07706205e-06
Iter: 728 loss: 1.07648191e-06
Iter: 729 loss: 1.07642e-06
Iter: 730 loss: 1.07638471e-06
Iter: 731 loss: 1.07632582e-06
Iter: 732 loss: 1.07626261e-06
Iter: 733 loss: 1.07745007e-06
Iter: 734 loss: 1.07625942e-06
Iter: 735 loss: 1.07619849e-06
Iter: 736 loss: 1.07636367e-06
Iter: 737 loss: 1.07617143e-06
Iter: 738 loss: 1.07612391e-06
Iter: 739 loss: 1.07606979e-06
Iter: 740 loss: 1.0760707e-06
Iter: 741 loss: 1.0760366e-06
Iter: 742 loss: 1.07600806e-06
Iter: 743 loss: 1.07597e-06
Iter: 744 loss: 1.07592336e-06
Iter: 745 loss: 1.07689016e-06
Iter: 746 loss: 1.07591541e-06
Iter: 747 loss: 1.07581627e-06
Iter: 748 loss: 1.07620508e-06
Iter: 749 loss: 1.0757974e-06
Iter: 750 loss: 1.07575056e-06
Iter: 751 loss: 1.07585629e-06
Iter: 752 loss: 1.07573555e-06
Iter: 753 loss: 1.07568644e-06
Iter: 754 loss: 1.07607707e-06
Iter: 755 loss: 1.07567553e-06
Iter: 756 loss: 1.07564028e-06
Iter: 757 loss: 1.07568485e-06
Iter: 758 loss: 1.07563017e-06
Iter: 759 loss: 1.07559993e-06
Iter: 760 loss: 1.07566052e-06
Iter: 761 loss: 1.07558503e-06
Iter: 762 loss: 1.07553e-06
Iter: 763 loss: 1.07553592e-06
Iter: 764 loss: 1.07548783e-06
Iter: 765 loss: 1.0754103e-06
Iter: 766 loss: 1.07563085e-06
Iter: 767 loss: 1.07539211e-06
Iter: 768 loss: 1.07530673e-06
Iter: 769 loss: 1.0756803e-06
Iter: 770 loss: 1.07528513e-06
Iter: 771 loss: 1.07522169e-06
Iter: 772 loss: 1.07512346e-06
Iter: 773 loss: 1.07511607e-06
Iter: 774 loss: 1.07510164e-06
Iter: 775 loss: 1.07506298e-06
Iter: 776 loss: 1.0750183e-06
Iter: 777 loss: 1.07496544e-06
Iter: 778 loss: 1.07495123e-06
Iter: 779 loss: 1.07491246e-06
Iter: 780 loss: 1.07549499e-06
Iter: 781 loss: 1.07491235e-06
Iter: 782 loss: 1.07487699e-06
Iter: 783 loss: 1.07488643e-06
Iter: 784 loss: 1.07485664e-06
Iter: 785 loss: 1.07480832e-06
Iter: 786 loss: 1.07504752e-06
Iter: 787 loss: 1.07480287e-06
Iter: 788 loss: 1.07477069e-06
Iter: 789 loss: 1.07480571e-06
Iter: 790 loss: 1.07473784e-06
Iter: 791 loss: 1.07468259e-06
Iter: 792 loss: 1.07478627e-06
Iter: 793 loss: 1.07467031e-06
Iter: 794 loss: 1.07461165e-06
Iter: 795 loss: 1.07473147e-06
Iter: 796 loss: 1.07458e-06
Iter: 797 loss: 1.07453423e-06
Iter: 798 loss: 1.07459141e-06
Iter: 799 loss: 1.07450046e-06
Iter: 800 loss: 1.07442611e-06
Iter: 801 loss: 1.07515189e-06
Iter: 802 loss: 1.07442941e-06
Iter: 803 loss: 1.07438223e-06
Iter: 804 loss: 1.07431083e-06
Iter: 805 loss: 1.07583219e-06
Iter: 806 loss: 1.07430492e-06
Iter: 807 loss: 1.07421761e-06
Iter: 808 loss: 1.07424523e-06
Iter: 809 loss: 1.07416054e-06
Iter: 810 loss: 1.07408357e-06
Iter: 811 loss: 1.07406618e-06
Iter: 812 loss: 1.07402866e-06
Iter: 813 loss: 1.07397534e-06
Iter: 814 loss: 1.07526876e-06
Iter: 815 loss: 1.07395954e-06
Iter: 816 loss: 1.07389315e-06
Iter: 817 loss: 1.07486778e-06
Iter: 818 loss: 1.07389144e-06
Iter: 819 loss: 1.07384392e-06
Iter: 820 loss: 1.07394e-06
Iter: 821 loss: 1.07382414e-06
Iter: 822 loss: 1.07378071e-06
Iter: 823 loss: 1.07399626e-06
Iter: 824 loss: 1.07376832e-06
Iter: 825 loss: 1.0737192e-06
Iter: 826 loss: 1.07375683e-06
Iter: 827 loss: 1.07369249e-06
Iter: 828 loss: 1.07363974e-06
Iter: 829 loss: 1.07374069e-06
Iter: 830 loss: 1.07361643e-06
Iter: 831 loss: 1.07354595e-06
Iter: 832 loss: 1.07357801e-06
Iter: 833 loss: 1.07350024e-06
Iter: 834 loss: 1.07343908e-06
Iter: 835 loss: 1.07441338e-06
Iter: 836 loss: 1.0734359e-06
Iter: 837 loss: 1.07337678e-06
Iter: 838 loss: 1.07331152e-06
Iter: 839 loss: 1.07330084e-06
Iter: 840 loss: 1.07323876e-06
Iter: 841 loss: 1.073421e-06
Iter: 842 loss: 1.07320216e-06
Iter: 843 loss: 1.07318067e-06
Iter: 844 loss: 1.07317396e-06
Iter: 845 loss: 1.07313167e-06
Iter: 846 loss: 1.073053e-06
Iter: 847 loss: 1.07377093e-06
Iter: 848 loss: 1.07303174e-06
Iter: 849 loss: 1.07297456e-06
Iter: 850 loss: 1.07296728e-06
Iter: 851 loss: 1.07291578e-06
Iter: 852 loss: 1.07292715e-06
Iter: 853 loss: 1.07286178e-06
Iter: 854 loss: 1.07282301e-06
Iter: 855 loss: 1.07338724e-06
Iter: 856 loss: 1.07281949e-06
Iter: 857 loss: 1.07279243e-06
Iter: 858 loss: 1.07275923e-06
Iter: 859 loss: 1.07275548e-06
Iter: 860 loss: 1.07269238e-06
Iter: 861 loss: 1.07304618e-06
Iter: 862 loss: 1.07268124e-06
Iter: 863 loss: 1.07263895e-06
Iter: 864 loss: 1.0726327e-06
Iter: 865 loss: 1.07258734e-06
Iter: 866 loss: 1.07251606e-06
Iter: 867 loss: 1.07323217e-06
Iter: 868 loss: 1.07252413e-06
Iter: 869 loss: 1.0724757e-06
Iter: 870 loss: 1.0724109e-06
Iter: 871 loss: 1.07239339e-06
Iter: 872 loss: 1.0723079e-06
Iter: 873 loss: 1.07250935e-06
Iter: 874 loss: 1.07228698e-06
Iter: 875 loss: 1.07224184e-06
Iter: 876 loss: 1.07224162e-06
Iter: 877 loss: 1.07222036e-06
Iter: 878 loss: 1.07221854e-06
Iter: 879 loss: 1.07218966e-06
Iter: 880 loss: 1.07214657e-06
Iter: 881 loss: 1.07219535e-06
Iter: 882 loss: 1.07213054e-06
Iter: 883 loss: 1.07206642e-06
Iter: 884 loss: 1.07227288e-06
Iter: 885 loss: 1.07207e-06
Iter: 886 loss: 1.07203323e-06
Iter: 887 loss: 1.07213782e-06
Iter: 888 loss: 1.0720189e-06
Iter: 889 loss: 1.07200196e-06
Iter: 890 loss: 1.07196445e-06
Iter: 891 loss: 1.0719416e-06
Iter: 892 loss: 1.07188498e-06
Iter: 893 loss: 1.07204096e-06
Iter: 894 loss: 1.07186565e-06
Iter: 895 loss: 1.07176857e-06
Iter: 896 loss: 1.07181006e-06
Iter: 897 loss: 1.07172173e-06
Iter: 898 loss: 1.07167182e-06
Iter: 899 loss: 1.0716675e-06
Iter: 900 loss: 1.07162623e-06
Iter: 901 loss: 1.07160008e-06
Iter: 902 loss: 1.07157712e-06
Iter: 903 loss: 1.07153346e-06
Iter: 904 loss: 1.0715512e-06
Iter: 905 loss: 1.07150584e-06
Iter: 906 loss: 1.07145729e-06
Iter: 907 loss: 1.0714557e-06
Iter: 908 loss: 1.07142762e-06
Iter: 909 loss: 1.071513e-06
Iter: 910 loss: 1.07142569e-06
Iter: 911 loss: 1.07137953e-06
Iter: 912 loss: 1.07135315e-06
Iter: 913 loss: 1.07135111e-06
Iter: 914 loss: 1.07127937e-06
Iter: 915 loss: 1.07129449e-06
Iter: 916 loss: 1.07126482e-06
Iter: 917 loss: 1.07124242e-06
Iter: 918 loss: 1.07124401e-06
Iter: 919 loss: 1.07118501e-06
Iter: 920 loss: 1.07145661e-06
Iter: 921 loss: 1.07116534e-06
Iter: 922 loss: 1.07112476e-06
Iter: 923 loss: 1.0711492e-06
Iter: 924 loss: 1.07109065e-06
Iter: 925 loss: 1.07103358e-06
Iter: 926 loss: 1.07107167e-06
Iter: 927 loss: 1.07100959e-06
Iter: 928 loss: 1.07093319e-06
Iter: 929 loss: 1.07159326e-06
Iter: 930 loss: 1.07093774e-06
Iter: 931 loss: 1.07090455e-06
Iter: 932 loss: 1.07093501e-06
Iter: 933 loss: 1.07087158e-06
Iter: 934 loss: 1.07082042e-06
Iter: 935 loss: 1.07081951e-06
Iter: 936 loss: 1.0707904e-06
Iter: 937 loss: 1.07073186e-06
Iter: 938 loss: 1.07115227e-06
Iter: 939 loss: 1.07073924e-06
Iter: 940 loss: 1.07068786e-06
Iter: 941 loss: 1.07095661e-06
Iter: 942 loss: 1.07068831e-06
Iter: 943 loss: 1.07066353e-06
Iter: 944 loss: 1.07060214e-06
Iter: 945 loss: 1.07060509e-06
Iter: 946 loss: 1.07055007e-06
Iter: 947 loss: 1.07054393e-06
Iter: 948 loss: 1.07050687e-06
Iter: 949 loss: 1.07043411e-06
Iter: 950 loss: 1.07044048e-06
Iter: 951 loss: 1.07039546e-06
Iter: 952 loss: 1.07040034e-06
Iter: 953 loss: 1.07036931e-06
Iter: 954 loss: 1.07035601e-06
Iter: 955 loss: 1.07034714e-06
Iter: 956 loss: 1.07028063e-06
Iter: 957 loss: 1.0703767e-06
Iter: 958 loss: 1.0702571e-06
Iter: 959 loss: 1.07021037e-06
Iter: 960 loss: 1.07047288e-06
Iter: 961 loss: 1.07020298e-06
Iter: 962 loss: 1.07015705e-06
Iter: 963 loss: 1.07023709e-06
Iter: 964 loss: 1.07012784e-06
Iter: 965 loss: 1.07010123e-06
Iter: 966 loss: 1.0700785e-06
Iter: 967 loss: 1.07005826e-06
Iter: 968 loss: 1.07002177e-06
Iter: 969 loss: 1.07022197e-06
Iter: 970 loss: 1.07000722e-06
Iter: 971 loss: 1.0699539e-06
Iter: 972 loss: 1.07009328e-06
Iter: 973 loss: 1.06994526e-06
Iter: 974 loss: 1.06989728e-06
Iter: 975 loss: 1.06985726e-06
Iter: 976 loss: 1.06984555e-06
Iter: 977 loss: 1.06982179e-06
Iter: 978 loss: 1.06982134e-06
Iter: 979 loss: 1.06977404e-06
Iter: 980 loss: 1.06974812e-06
Iter: 981 loss: 1.06973607e-06
Iter: 982 loss: 1.06969173e-06
Iter: 983 loss: 1.07034771e-06
Iter: 984 loss: 1.06969253e-06
Iter: 985 loss: 1.06966024e-06
Iter: 986 loss: 1.06959862e-06
Iter: 987 loss: 1.06959578e-06
Iter: 988 loss: 1.0695378e-06
Iter: 989 loss: 1.06997879e-06
Iter: 990 loss: 1.06953212e-06
Iter: 991 loss: 1.06947618e-06
Iter: 992 loss: 1.06954235e-06
Iter: 993 loss: 1.06945254e-06
Iter: 994 loss: 1.06937568e-06
Iter: 995 loss: 1.06960692e-06
Iter: 996 loss: 1.06933976e-06
Iter: 997 loss: 1.06929087e-06
Iter: 998 loss: 1.06930202e-06
Iter: 999 loss: 1.06925972e-06
Iter: 1000 loss: 1.06919867e-06
Iter: 1001 loss: 1.06961807e-06
Iter: 1002 loss: 1.06920595e-06
Iter: 1003 loss: 1.06915286e-06
Iter: 1004 loss: 1.06928974e-06
Iter: 1005 loss: 1.06914797e-06
Iter: 1006 loss: 1.06910284e-06
Iter: 1007 loss: 1.06902667e-06
Iter: 1008 loss: 1.06903587e-06
Iter: 1009 loss: 1.06896277e-06
Iter: 1010 loss: 1.06962568e-06
Iter: 1011 loss: 1.06896618e-06
Iter: 1012 loss: 1.06888683e-06
Iter: 1013 loss: 1.06895027e-06
Iter: 1014 loss: 1.06885022e-06
Iter: 1015 loss: 1.06880407e-06
Iter: 1016 loss: 1.0693451e-06
Iter: 1017 loss: 1.0688e-06
Iter: 1018 loss: 1.06876166e-06
Iter: 1019 loss: 1.06871653e-06
Iter: 1020 loss: 1.06870118e-06
Iter: 1021 loss: 1.06863308e-06
Iter: 1022 loss: 1.06899017e-06
Iter: 1023 loss: 1.06864229e-06
Iter: 1024 loss: 1.06857044e-06
Iter: 1025 loss: 1.06855225e-06
Iter: 1026 loss: 1.06850575e-06
Iter: 1027 loss: 1.06841162e-06
Iter: 1028 loss: 1.06943139e-06
Iter: 1029 loss: 1.06842117e-06
Iter: 1030 loss: 1.06836319e-06
Iter: 1031 loss: 1.06828713e-06
Iter: 1032 loss: 1.06828429e-06
Iter: 1033 loss: 1.06820744e-06
Iter: 1034 loss: 1.06897619e-06
Iter: 1035 loss: 1.0682237e-06
Iter: 1036 loss: 1.06817652e-06
Iter: 1037 loss: 1.0681805e-06
Iter: 1038 loss: 1.0681274e-06
Iter: 1039 loss: 1.06806726e-06
Iter: 1040 loss: 1.06813911e-06
Iter: 1041 loss: 1.06801042e-06
Iter: 1042 loss: 1.0679521e-06
Iter: 1043 loss: 1.06841082e-06
Iter: 1044 loss: 1.06794062e-06
Iter: 1045 loss: 1.06786069e-06
Iter: 1046 loss: 1.06803793e-06
Iter: 1047 loss: 1.06785319e-06
Iter: 1048 loss: 1.06778236e-06
Iter: 1049 loss: 1.06800803e-06
Iter: 1050 loss: 1.0677702e-06
Iter: 1051 loss: 1.06769949e-06
Iter: 1052 loss: 1.0678184e-06
Iter: 1053 loss: 1.0676647e-06
Iter: 1054 loss: 1.06761092e-06
Iter: 1055 loss: 1.06771063e-06
Iter: 1056 loss: 1.06758966e-06
Iter: 1057 loss: 1.06749712e-06
Iter: 1058 loss: 1.06748803e-06
Iter: 1059 loss: 1.06742391e-06
Iter: 1060 loss: 1.06737411e-06
Iter: 1061 loss: 1.0673773e-06
Iter: 1062 loss: 1.06733023e-06
Iter: 1063 loss: 1.06724792e-06
Iter: 1064 loss: 1.06831294e-06
Iter: 1065 loss: 1.06722507e-06
Iter: 1066 loss: 1.06719335e-06
Iter: 1067 loss: 1.06718539e-06
Iter: 1068 loss: 1.06714879e-06
Iter: 1069 loss: 1.0671306e-06
Iter: 1070 loss: 1.06709922e-06
Iter: 1071 loss: 1.06705295e-06
Iter: 1072 loss: 1.0671564e-06
Iter: 1073 loss: 1.06701282e-06
Iter: 1074 loss: 1.06696098e-06
Iter: 1075 loss: 1.06728658e-06
Iter: 1076 loss: 1.06695074e-06
Iter: 1077 loss: 1.06689254e-06
Iter: 1078 loss: 1.06721427e-06
Iter: 1079 loss: 1.06689549e-06
Iter: 1080 loss: 1.06683058e-06
Iter: 1081 loss: 1.06675202e-06
Iter: 1082 loss: 1.06874654e-06
Iter: 1083 loss: 1.06675679e-06
Iter: 1084 loss: 1.06664538e-06
Iter: 1085 loss: 1.06817697e-06
Iter: 1086 loss: 1.06665743e-06
Iter: 1087 loss: 1.06659036e-06
Iter: 1088 loss: 1.06653829e-06
Iter: 1089 loss: 1.06652078e-06
Iter: 1090 loss: 1.06641e-06
Iter: 1091 loss: 1.06713128e-06
Iter: 1092 loss: 1.06639982e-06
Iter: 1093 loss: 1.06635821e-06
Iter: 1094 loss: 1.06652624e-06
Iter: 1095 loss: 1.06633365e-06
Iter: 1096 loss: 1.06627135e-06
Iter: 1097 loss: 1.0663457e-06
Iter: 1098 loss: 1.06623065e-06
Iter: 1099 loss: 1.06619109e-06
Iter: 1100 loss: 1.06628329e-06
Iter: 1101 loss: 1.06616574e-06
Iter: 1102 loss: 1.06611583e-06
Iter: 1103 loss: 1.0663864e-06
Iter: 1104 loss: 1.06609741e-06
Iter: 1105 loss: 1.06603034e-06
Iter: 1106 loss: 1.06601703e-06
Iter: 1107 loss: 1.06597804e-06
Iter: 1108 loss: 1.06590983e-06
Iter: 1109 loss: 1.06617608e-06
Iter: 1110 loss: 1.06588277e-06
Iter: 1111 loss: 1.06579296e-06
Iter: 1112 loss: 1.0665e-06
Iter: 1113 loss: 1.0657958e-06
Iter: 1114 loss: 1.0657584e-06
Iter: 1115 loss: 1.0657152e-06
Iter: 1116 loss: 1.06570792e-06
Iter: 1117 loss: 1.06566472e-06
Iter: 1118 loss: 1.06565517e-06
Iter: 1119 loss: 1.06563039e-06
Iter: 1120 loss: 1.06556422e-06
Iter: 1121 loss: 1.06678135e-06
Iter: 1122 loss: 1.06556126e-06
Iter: 1123 loss: 1.06550897e-06
Iter: 1124 loss: 1.0662136e-06
Iter: 1125 loss: 1.06551352e-06
Iter: 1126 loss: 1.06546031e-06
Iter: 1127 loss: 1.06546793e-06
Iter: 1128 loss: 1.06543894e-06
Iter: 1129 loss: 1.06536947e-06
Iter: 1130 loss: 1.06600453e-06
Iter: 1131 loss: 1.06538357e-06
Iter: 1132 loss: 1.06535299e-06
Iter: 1133 loss: 1.06526772e-06
Iter: 1134 loss: 1.06641721e-06
Iter: 1135 loss: 1.06526375e-06
Iter: 1136 loss: 1.06521554e-06
Iter: 1137 loss: 1.06519553e-06
Iter: 1138 loss: 1.06515381e-06
Iter: 1139 loss: 1.06514744e-06
Iter: 1140 loss: 1.06511368e-06
Iter: 1141 loss: 1.06506468e-06
Iter: 1142 loss: 1.06505752e-06
Iter: 1143 loss: 1.06500875e-06
Iter: 1144 loss: 1.06494736e-06
Iter: 1145 loss: 1.06561492e-06
Iter: 1146 loss: 1.06495304e-06
Iter: 1147 loss: 1.06488903e-06
Iter: 1148 loss: 1.06483708e-06
Iter: 1149 loss: 1.06483412e-06
Iter: 1150 loss: 1.06474113e-06
Iter: 1151 loss: 1.06486993e-06
Iter: 1152 loss: 1.06468337e-06
Iter: 1153 loss: 1.06455957e-06
Iter: 1154 loss: 1.06518723e-06
Iter: 1155 loss: 1.06450022e-06
Iter: 1156 loss: 1.06441712e-06
Iter: 1157 loss: 1.06447305e-06
Iter: 1158 loss: 1.06437233e-06
Iter: 1159 loss: 1.06429184e-06
Iter: 1160 loss: 1.06480275e-06
Iter: 1161 loss: 1.0642641e-06
Iter: 1162 loss: 1.06421157e-06
Iter: 1163 loss: 1.06476205e-06
Iter: 1164 loss: 1.06420589e-06
Iter: 1165 loss: 1.06414859e-06
Iter: 1166 loss: 1.06405605e-06
Iter: 1167 loss: 1.0659893e-06
Iter: 1168 loss: 1.06404786e-06
Iter: 1169 loss: 1.06399659e-06
Iter: 1170 loss: 1.0640033e-06
Iter: 1171 loss: 1.06393441e-06
Iter: 1172 loss: 1.06403695e-06
Iter: 1173 loss: 1.06390667e-06
Iter: 1174 loss: 1.06386597e-06
Iter: 1175 loss: 1.06383163e-06
Iter: 1176 loss: 1.06380548e-06
Iter: 1177 loss: 1.06373136e-06
Iter: 1178 loss: 1.06437051e-06
Iter: 1179 loss: 1.06373238e-06
Iter: 1180 loss: 1.0636569e-06
Iter: 1181 loss: 1.06376979e-06
Iter: 1182 loss: 1.06363939e-06
Iter: 1183 loss: 1.06355492e-06
Iter: 1184 loss: 1.06348193e-06
Iter: 1185 loss: 1.06347397e-06
Iter: 1186 loss: 1.06349353e-06
Iter: 1187 loss: 1.0634426e-06
Iter: 1188 loss: 1.06341565e-06
Iter: 1189 loss: 1.06337268e-06
Iter: 1190 loss: 1.06386824e-06
Iter: 1191 loss: 1.06336825e-06
Iter: 1192 loss: 1.06333448e-06
Iter: 1193 loss: 1.06385016e-06
Iter: 1194 loss: 1.06333971e-06
Iter: 1195 loss: 1.06329765e-06
Iter: 1196 loss: 1.06330049e-06
Iter: 1197 loss: 1.06326172e-06
Iter: 1198 loss: 1.06322273e-06
Iter: 1199 loss: 1.06340224e-06
Iter: 1200 loss: 1.06318794e-06
Iter: 1201 loss: 1.06315667e-06
Iter: 1202 loss: 1.0632632e-06
Iter: 1203 loss: 1.06314769e-06
Iter: 1204 loss: 1.06309631e-06
Iter: 1205 loss: 1.06323978e-06
Iter: 1206 loss: 1.06308471e-06
Iter: 1207 loss: 1.0630306e-06
Iter: 1208 loss: 1.06297387e-06
Iter: 1209 loss: 1.06296625e-06
Iter: 1210 loss: 1.0628936e-06
Iter: 1211 loss: 1.06300797e-06
Iter: 1212 loss: 1.06285938e-06
Iter: 1213 loss: 1.06278367e-06
Iter: 1214 loss: 1.06279822e-06
Iter: 1215 loss: 1.06275422e-06
Iter: 1216 loss: 1.06268135e-06
Iter: 1217 loss: 1.06406037e-06
Iter: 1218 loss: 1.06268271e-06
Iter: 1219 loss: 1.06262019e-06
Iter: 1220 loss: 1.06330731e-06
Iter: 1221 loss: 1.06262883e-06
Iter: 1222 loss: 1.06257698e-06
Iter: 1223 loss: 1.06265111e-06
Iter: 1224 loss: 1.06254242e-06
Iter: 1225 loss: 1.06249104e-06
Iter: 1226 loss: 1.06249786e-06
Iter: 1227 loss: 1.06247489e-06
Iter: 1228 loss: 1.06240691e-06
Iter: 1229 loss: 1.06283971e-06
Iter: 1230 loss: 1.062396e-06
Iter: 1231 loss: 1.0623562e-06
Iter: 1232 loss: 1.06252674e-06
Iter: 1233 loss: 1.06234427e-06
Iter: 1234 loss: 1.06229459e-06
Iter: 1235 loss: 1.06225593e-06
Iter: 1236 loss: 1.06221842e-06
Iter: 1237 loss: 1.06218693e-06
Iter: 1238 loss: 1.06218351e-06
Iter: 1239 loss: 1.0621502e-06
Iter: 1240 loss: 1.06210541e-06
Iter: 1241 loss: 1.06209814e-06
Iter: 1242 loss: 1.06203174e-06
Iter: 1243 loss: 1.06203e-06
Iter: 1244 loss: 1.0619782e-06
Iter: 1245 loss: 1.06194352e-06
Iter: 1246 loss: 1.06194648e-06
Iter: 1247 loss: 1.06189827e-06
Iter: 1248 loss: 1.0619392e-06
Iter: 1249 loss: 1.06188145e-06
Iter: 1250 loss: 1.06185905e-06
Iter: 1251 loss: 1.06176e-06
Iter: 1252 loss: 1.06282869e-06
Iter: 1253 loss: 1.06177617e-06
Iter: 1254 loss: 1.06168534e-06
Iter: 1255 loss: 1.06224661e-06
Iter: 1256 loss: 1.06168022e-06
Iter: 1257 loss: 1.06161269e-06
Iter: 1258 loss: 1.06161872e-06
Iter: 1259 loss: 1.06159314e-06
Iter: 1260 loss: 1.06150696e-06
Iter: 1261 loss: 1.06294294e-06
Iter: 1262 loss: 1.06151947e-06
Iter: 1263 loss: 1.0614649e-06
Iter: 1264 loss: 1.06145853e-06
Iter: 1265 loss: 1.06142875e-06
Iter: 1266 loss: 1.06141897e-06
Iter: 1267 loss: 1.0613885e-06
Iter: 1268 loss: 1.06131313e-06
Iter: 1269 loss: 1.06152083e-06
Iter: 1270 loss: 1.06128982e-06
Iter: 1271 loss: 1.06125822e-06
Iter: 1272 loss: 1.0613569e-06
Iter: 1273 loss: 1.06125049e-06
Iter: 1274 loss: 1.06119091e-06
Iter: 1275 loss: 1.06125231e-06
Iter: 1276 loss: 1.06115499e-06
Iter: 1277 loss: 1.06111133e-06
Iter: 1278 loss: 1.06107177e-06
Iter: 1279 loss: 1.0610604e-06
Iter: 1280 loss: 1.06105654e-06
Iter: 1281 loss: 1.06103448e-06
Iter: 1282 loss: 1.06101754e-06
Iter: 1283 loss: 1.0609574e-06
Iter: 1284 loss: 1.06145e-06
Iter: 1285 loss: 1.06096047e-06
Iter: 1286 loss: 1.06088885e-06
Iter: 1287 loss: 1.060915e-06
Iter: 1288 loss: 1.06082427e-06
Iter: 1289 loss: 1.06072525e-06
Iter: 1290 loss: 1.06076652e-06
Iter: 1291 loss: 1.06063976e-06
Iter: 1292 loss: 1.06066636e-06
Iter: 1293 loss: 1.060572e-06
Iter: 1294 loss: 1.06053017e-06
Iter: 1295 loss: 1.06042035e-06
Iter: 1296 loss: 1.06257505e-06
Iter: 1297 loss: 1.06042512e-06
Iter: 1298 loss: 1.06035327e-06
Iter: 1299 loss: 1.06058724e-06
Iter: 1300 loss: 1.06036896e-06
Iter: 1301 loss: 1.06034202e-06
Iter: 1302 loss: 1.06034304e-06
Iter: 1303 loss: 1.06031735e-06
Iter: 1304 loss: 1.06034122e-06
Iter: 1305 loss: 1.06027574e-06
Iter: 1306 loss: 1.06024208e-06
Iter: 1307 loss: 1.06033394e-06
Iter: 1308 loss: 1.06024402e-06
Iter: 1309 loss: 1.0602007e-06
Iter: 1310 loss: 1.0601575e-06
Iter: 1311 loss: 1.06017069e-06
Iter: 1312 loss: 1.06015102e-06
Iter: 1313 loss: 1.06012749e-06
Iter: 1314 loss: 1.06010361e-06
Iter: 1315 loss: 1.0600354e-06
Iter: 1316 loss: 1.06059349e-06
Iter: 1317 loss: 1.06001198e-06
Iter: 1318 loss: 1.0599756e-06
Iter: 1319 loss: 1.05997469e-06
Iter: 1320 loss: 1.05991865e-06
Iter: 1321 loss: 1.05997788e-06
Iter: 1322 loss: 1.05989579e-06
Iter: 1323 loss: 1.05985782e-06
Iter: 1324 loss: 1.05983577e-06
Iter: 1325 loss: 1.05982031e-06
Iter: 1326 loss: 1.05977267e-06
Iter: 1327 loss: 1.05976e-06
Iter: 1328 loss: 1.05974584e-06
Iter: 1329 loss: 1.05971458e-06
Iter: 1330 loss: 1.06058508e-06
Iter: 1331 loss: 1.05971071e-06
Iter: 1332 loss: 1.05966456e-06
Iter: 1333 loss: 1.05997992e-06
Iter: 1334 loss: 1.05966728e-06
Iter: 1335 loss: 1.05961749e-06
Iter: 1336 loss: 1.05970571e-06
Iter: 1337 loss: 1.05962e-06
Iter: 1338 loss: 1.05956406e-06
Iter: 1339 loss: 1.05973027e-06
Iter: 1340 loss: 1.05955064e-06
Iter: 1341 loss: 1.05953814e-06
Iter: 1342 loss: 1.05950426e-06
Iter: 1343 loss: 1.05950744e-06
Iter: 1344 loss: 1.0594681e-06
Iter: 1345 loss: 1.05986101e-06
Iter: 1346 loss: 1.05947061e-06
Iter: 1347 loss: 1.05945037e-06
Iter: 1348 loss: 1.05940387e-06
Iter: 1349 loss: 1.06006075e-06
Iter: 1350 loss: 1.0593991e-06
Iter: 1351 loss: 1.05934942e-06
Iter: 1352 loss: 1.06002506e-06
Iter: 1353 loss: 1.05935078e-06
Iter: 1354 loss: 1.05929416e-06
Iter: 1355 loss: 1.0594257e-06
Iter: 1356 loss: 1.05926688e-06
Iter: 1357 loss: 1.059223e-06
Iter: 1358 loss: 1.05927552e-06
Iter: 1359 loss: 1.05920299e-06
Iter: 1360 loss: 1.05918355e-06
Iter: 1361 loss: 1.05918184e-06
Iter: 1362 loss: 1.05916388e-06
Iter: 1363 loss: 1.05913523e-06
Iter: 1364 loss: 1.05912977e-06
Iter: 1365 loss: 1.0590976e-06
Iter: 1366 loss: 1.05916206e-06
Iter: 1367 loss: 1.05908464e-06
Iter: 1368 loss: 1.05903905e-06
Iter: 1369 loss: 1.05917411e-06
Iter: 1370 loss: 1.05902086e-06
Iter: 1371 loss: 1.05899517e-06
Iter: 1372 loss: 1.05941649e-06
Iter: 1373 loss: 1.05899665e-06
Iter: 1374 loss: 1.05897425e-06
Iter: 1375 loss: 1.05891104e-06
Iter: 1376 loss: 1.05928268e-06
Iter: 1377 loss: 1.0588833e-06
Iter: 1378 loss: 1.05882123e-06
Iter: 1379 loss: 1.05934976e-06
Iter: 1380 loss: 1.05882077e-06
Iter: 1381 loss: 1.05875426e-06
Iter: 1382 loss: 1.05947856e-06
Iter: 1383 loss: 1.05875677e-06
Iter: 1384 loss: 1.05873846e-06
Iter: 1385 loss: 1.05867025e-06
Iter: 1386 loss: 1.05910158e-06
Iter: 1387 loss: 1.05864251e-06
Iter: 1388 loss: 1.05857566e-06
Iter: 1389 loss: 1.05908657e-06
Iter: 1390 loss: 1.05857293e-06
Iter: 1391 loss: 1.05854372e-06
Iter: 1392 loss: 1.05854315e-06
Iter: 1393 loss: 1.05849858e-06
Iter: 1394 loss: 1.05846664e-06
Iter: 1395 loss: 1.05904724e-06
Iter: 1396 loss: 1.05846129e-06
Iter: 1397 loss: 1.0584165e-06
Iter: 1398 loss: 1.05854235e-06
Iter: 1399 loss: 1.05840559e-06
Iter: 1400 loss: 1.05835147e-06
Iter: 1401 loss: 1.05835863e-06
Iter: 1402 loss: 1.05832805e-06
Iter: 1403 loss: 1.05827849e-06
Iter: 1404 loss: 1.05870186e-06
Iter: 1405 loss: 1.05825188e-06
Iter: 1406 loss: 1.0582105e-06
Iter: 1407 loss: 1.05820664e-06
Iter: 1408 loss: 1.0581582e-06
Iter: 1409 loss: 1.05816241e-06
Iter: 1410 loss: 1.05810966e-06
Iter: 1411 loss: 1.05805816e-06
Iter: 1412 loss: 1.0581972e-06
Iter: 1413 loss: 1.05804406e-06
Iter: 1414 loss: 1.05799404e-06
Iter: 1415 loss: 1.05802781e-06
Iter: 1416 loss: 1.05795596e-06
Iter: 1417 loss: 1.0579721e-06
Iter: 1418 loss: 1.05794993e-06
Iter: 1419 loss: 1.05792628e-06
Iter: 1420 loss: 1.05788604e-06
Iter: 1421 loss: 1.05865433e-06
Iter: 1422 loss: 1.05789184e-06
Iter: 1423 loss: 1.05785875e-06
Iter: 1424 loss: 1.05786501e-06
Iter: 1425 loss: 1.05783647e-06
Iter: 1426 loss: 1.05779668e-06
Iter: 1427 loss: 1.05802474e-06
Iter: 1428 loss: 1.05779213e-06
Iter: 1429 loss: 1.05772153e-06
Iter: 1430 loss: 1.05771653e-06
Iter: 1431 loss: 1.0576664e-06
Iter: 1432 loss: 1.05764559e-06
Iter: 1433 loss: 1.05764286e-06
Iter: 1434 loss: 1.05758841e-06
Iter: 1435 loss: 1.05763604e-06
Iter: 1436 loss: 1.05756203e-06
Iter: 1437 loss: 1.05754009e-06
Iter: 1438 loss: 1.05748029e-06
Iter: 1439 loss: 1.05869822e-06
Iter: 1440 loss: 1.05747597e-06
Iter: 1441 loss: 1.05742299e-06
Iter: 1442 loss: 1.05767651e-06
Iter: 1443 loss: 1.0574206e-06
Iter: 1444 loss: 1.05738991e-06
Iter: 1445 loss: 1.0573799e-06
Iter: 1446 loss: 1.05736433e-06
Iter: 1447 loss: 1.05731613e-06
Iter: 1448 loss: 1.05753952e-06
Iter: 1449 loss: 1.05729941e-06
Iter: 1450 loss: 1.05727395e-06
Iter: 1451 loss: 1.05726667e-06
Iter: 1452 loss: 1.05724268e-06
Iter: 1453 loss: 1.05737604e-06
Iter: 1454 loss: 1.05724348e-06
Iter: 1455 loss: 1.05721551e-06
Iter: 1456 loss: 1.05718073e-06
Iter: 1457 loss: 1.05717641e-06
Iter: 1458 loss: 1.05715048e-06
Iter: 1459 loss: 1.05765787e-06
Iter: 1460 loss: 1.0571481e-06
Iter: 1461 loss: 1.05710888e-06
Iter: 1462 loss: 1.05711638e-06
Iter: 1463 loss: 1.05708307e-06
Iter: 1464 loss: 1.05704032e-06
Iter: 1465 loss: 1.05694733e-06
Iter: 1466 loss: 1.05864535e-06
Iter: 1467 loss: 1.05693925e-06
Iter: 1468 loss: 1.05698427e-06
Iter: 1469 loss: 1.05691765e-06
Iter: 1470 loss: 1.05688969e-06
Iter: 1471 loss: 1.05683966e-06
Iter: 1472 loss: 1.05776405e-06
Iter: 1473 loss: 1.05683944e-06
Iter: 1474 loss: 1.05678509e-06
Iter: 1475 loss: 1.05686354e-06
Iter: 1476 loss: 1.05675986e-06
Iter: 1477 loss: 1.0567378e-06
Iter: 1478 loss: 1.05673939e-06
Iter: 1479 loss: 1.05671677e-06
Iter: 1480 loss: 1.056668e-06
Iter: 1481 loss: 1.05731328e-06
Iter: 1482 loss: 1.05667073e-06
Iter: 1483 loss: 1.05662571e-06
Iter: 1484 loss: 1.05659e-06
Iter: 1485 loss: 1.05656693e-06
Iter: 1486 loss: 1.05649087e-06
Iter: 1487 loss: 1.05706442e-06
Iter: 1488 loss: 1.05649235e-06
Iter: 1489 loss: 1.0564579e-06
Iter: 1490 loss: 1.0564994e-06
Iter: 1491 loss: 1.05641698e-06
Iter: 1492 loss: 1.05637946e-06
Iter: 1493 loss: 1.05647257e-06
Iter: 1494 loss: 1.05636605e-06
Iter: 1495 loss: 1.05632375e-06
Iter: 1496 loss: 1.05639572e-06
Iter: 1497 loss: 1.05630875e-06
Iter: 1498 loss: 1.05625622e-06
Iter: 1499 loss: 1.05640811e-06
Iter: 1500 loss: 1.05623974e-06
Iter: 1501 loss: 1.05620268e-06
Iter: 1502 loss: 1.05630352e-06
Iter: 1503 loss: 1.05618142e-06
Iter: 1504 loss: 1.05615095e-06
Iter: 1505 loss: 1.05645836e-06
Iter: 1506 loss: 1.05613447e-06
Iter: 1507 loss: 1.05610752e-06
Iter: 1508 loss: 1.05605909e-06
Iter: 1509 loss: 1.05661297e-06
Iter: 1510 loss: 1.05602919e-06
Iter: 1511 loss: 1.05601976e-06
Iter: 1512 loss: 1.05598929e-06
Iter: 1513 loss: 1.05596291e-06
Iter: 1514 loss: 1.05591334e-06
Iter: 1515 loss: 1.05653385e-06
Iter: 1516 loss: 1.05593017e-06
Iter: 1517 loss: 1.05586537e-06
Iter: 1518 loss: 1.05613913e-06
Iter: 1519 loss: 1.05586332e-06
Iter: 1520 loss: 1.05583524e-06
Iter: 1521 loss: 1.05583013e-06
Iter: 1522 loss: 1.0558058e-06
Iter: 1523 loss: 1.05578602e-06
Iter: 1524 loss: 1.05577442e-06
Iter: 1525 loss: 1.05573e-06
Iter: 1526 loss: 1.05585934e-06
Iter: 1527 loss: 1.05572053e-06
Iter: 1528 loss: 1.05570803e-06
Iter: 1529 loss: 1.05574827e-06
Iter: 1530 loss: 1.05569904e-06
Iter: 1531 loss: 1.05567597e-06
Iter: 1532 loss: 1.05576066e-06
Iter: 1533 loss: 1.05566482e-06
Iter: 1534 loss: 1.05563424e-06
Iter: 1535 loss: 1.05562322e-06
Iter: 1536 loss: 1.05560628e-06
Iter: 1537 loss: 1.05557638e-06
Iter: 1538 loss: 1.05557626e-06
Iter: 1539 loss: 1.05554864e-06
Iter: 1540 loss: 1.05548349e-06
Iter: 1541 loss: 1.05616823e-06
Iter: 1542 loss: 1.05548611e-06
Iter: 1543 loss: 1.05544837e-06
Iter: 1544 loss: 1.0554395e-06
Iter: 1545 loss: 1.05540664e-06
Iter: 1546 loss: 1.05534968e-06
Iter: 1547 loss: 1.05536105e-06
Iter: 1548 loss: 1.0553041e-06
Iter: 1549 loss: 1.05530034e-06
Iter: 1550 loss: 1.05525396e-06
Iter: 1551 loss: 1.0552335e-06
Iter: 1552 loss: 1.05523429e-06
Iter: 1553 loss: 1.05517961e-06
Iter: 1554 loss: 1.05519348e-06
Iter: 1555 loss: 1.05514573e-06
Iter: 1556 loss: 1.05512754e-06
Iter: 1557 loss: 1.05547201e-06
Iter: 1558 loss: 1.05512356e-06
Iter: 1559 loss: 1.05509139e-06
Iter: 1560 loss: 1.05511992e-06
Iter: 1561 loss: 1.05507718e-06
Iter: 1562 loss: 1.05505364e-06
Iter: 1563 loss: 1.05512481e-06
Iter: 1564 loss: 1.05504773e-06
Iter: 1565 loss: 1.05500749e-06
Iter: 1566 loss: 1.05499566e-06
Iter: 1567 loss: 1.05496895e-06
Iter: 1568 loss: 1.0549511e-06
Iter: 1569 loss: 1.05493768e-06
Iter: 1570 loss: 1.05491267e-06
Iter: 1571 loss: 1.05486197e-06
Iter: 1572 loss: 1.05601612e-06
Iter: 1573 loss: 1.05485776e-06
Iter: 1574 loss: 1.05481683e-06
Iter: 1575 loss: 1.05524293e-06
Iter: 1576 loss: 1.05481377e-06
Iter: 1577 loss: 1.05477989e-06
Iter: 1578 loss: 1.05487572e-06
Iter: 1579 loss: 1.05475863e-06
Iter: 1580 loss: 1.05472395e-06
Iter: 1581 loss: 1.05469235e-06
Iter: 1582 loss: 1.05468541e-06
Iter: 1583 loss: 1.05464346e-06
Iter: 1584 loss: 1.05482513e-06
Iter: 1585 loss: 1.05464824e-06
Iter: 1586 loss: 1.05461709e-06
Iter: 1587 loss: 1.05462937e-06
Iter: 1588 loss: 1.05458946e-06
Iter: 1589 loss: 1.05455342e-06
Iter: 1590 loss: 1.05472225e-06
Iter: 1591 loss: 1.05453682e-06
Iter: 1592 loss: 1.05451852e-06
Iter: 1593 loss: 1.05453091e-06
Iter: 1594 loss: 1.05449067e-06
Iter: 1595 loss: 1.0544411e-06
Iter: 1596 loss: 1.05464096e-06
Iter: 1597 loss: 1.05444781e-06
Iter: 1598 loss: 1.0544145e-06
Iter: 1599 loss: 1.05442086e-06
Iter: 1600 loss: 1.05439972e-06
Iter: 1601 loss: 1.0543813e-06
Iter: 1602 loss: 1.05432275e-06
Iter: 1603 loss: 1.05432514e-06
Iter: 1604 loss: 1.05433105e-06
Iter: 1605 loss: 1.05430843e-06
Iter: 1606 loss: 1.05428092e-06
Iter: 1607 loss: 1.05423624e-06
Iter: 1608 loss: 1.05476238e-06
Iter: 1609 loss: 1.05422805e-06
Iter: 1610 loss: 1.0542168e-06
Iter: 1611 loss: 1.05420986e-06
Iter: 1612 loss: 1.05418269e-06
Iter: 1613 loss: 1.05413164e-06
Iter: 1614 loss: 1.05413892e-06
Iter: 1615 loss: 1.05410209e-06
Iter: 1616 loss: 1.05411732e-06
Iter: 1617 loss: 1.0540632e-06
Iter: 1618 loss: 1.0540798e-06
Iter: 1619 loss: 1.05406093e-06
Iter: 1620 loss: 1.05405775e-06
Iter: 1621 loss: 1.05405911e-06
Iter: 1622 loss: 1.05404456e-06
Iter: 1623 loss: 1.05402182e-06
Iter: 1624 loss: 1.05401489e-06
Iter: 1625 loss: 1.05401648e-06
Iter: 1626 loss: 1.05398578e-06
Iter: 1627 loss: 1.05406752e-06
Iter: 1628 loss: 1.05396657e-06
Iter: 1629 loss: 1.05395566e-06
Iter: 1630 loss: 1.05393667e-06
Iter: 1631 loss: 1.05393474e-06
Iter: 1632 loss: 1.05386721e-06
Iter: 1633 loss: 1.0539294e-06
Iter: 1634 loss: 1.05384606e-06
Iter: 1635 loss: 1.05381332e-06
Iter: 1636 loss: 1.0542509e-06
Iter: 1637 loss: 1.05380263e-06
Iter: 1638 loss: 1.05378876e-06
Iter: 1639 loss: 1.05403547e-06
Iter: 1640 loss: 1.05378035e-06
Iter: 1641 loss: 1.05378354e-06
Iter: 1642 loss: 1.0537758e-06
Iter: 1643 loss: 1.05377876e-06
Iter: 1644 loss: 1.05377944e-06
Iter: 1645 loss: 1.05378399e-06
Iter: 1646 loss: 1.05377967e-06
Iter: 1647 loss: 1.05378388e-06
Iter: 1648 loss: 1.05378717e-06
Iter: 1649 loss: 1.05378649e-06
Iter: 1650 loss: 1.05377831e-06
Iter: 1651 loss: 1.05378649e-06
Iter: 1652 loss: 1.05377944e-06
Iter: 1653 loss: 1.05378012e-06
Iter: 1654 loss: 1.05378422e-06
Iter: 1655 loss: 1.05378513e-06
Iter: 1656 loss: 1.05378467e-06
Iter: 1657 loss: 1.05377944e-06
Iter: 1658 loss: 1.05377967e-06
Iter: 1659 loss: 1.0537799e-06
Iter: 1660 loss: 1.05378263e-06
Iter: 1661 loss: 1.05378263e-06
Iter: 1662 loss: 1.0537799e-06
Iter: 1663 loss: 1.05378103e-06
Iter: 1664 loss: 1.05378263e-06
Iter: 1665 loss: 1.05378103e-06
Iter: 1666 loss: 1.05378263e-06
Iter: 1667 loss: 1.05375375e-06
Iter: 1668 loss: 1.05410641e-06
Iter: 1669 loss: 1.05374534e-06
Iter: 1670 loss: 1.0537176e-06
Iter: 1671 loss: 1.0538721e-06
Iter: 1672 loss: 1.0537176e-06
Iter: 1673 loss: 1.05367599e-06
Iter: 1674 loss: 1.0538231e-06
Iter: 1675 loss: 1.05367167e-06
Iter: 1676 loss: 1.05364802e-06
Iter: 1677 loss: 1.05371555e-06
Iter: 1678 loss: 1.05362255e-06
Iter: 1679 loss: 1.0535914e-06
Iter: 1680 loss: 1.05359322e-06
Iter: 1681 loss: 1.05356708e-06
Iter: 1682 loss: 1.05353126e-06
Iter: 1683 loss: 1.05360641e-06
Iter: 1684 loss: 1.05352183e-06
Iter: 1685 loss: 1.0534784e-06
Iter: 1686 loss: 1.05362801e-06
Iter: 1687 loss: 1.05346476e-06
Iter: 1688 loss: 1.05344952e-06
Iter: 1689 loss: 1.05342065e-06
Iter: 1690 loss: 1.05341678e-06
Iter: 1691 loss: 1.05339461e-06
Iter: 1692 loss: 1.05356116e-06
Iter: 1693 loss: 1.05338086e-06
Iter: 1694 loss: 1.05336403e-06
Iter: 1695 loss: 1.05358049e-06
Iter: 1696 loss: 1.05336801e-06
Iter: 1697 loss: 1.05335857e-06
Iter: 1698 loss: 1.05334061e-06
Iter: 1699 loss: 1.05374841e-06
Iter: 1700 loss: 1.05334504e-06
Iter: 1701 loss: 1.05330651e-06
Iter: 1702 loss: 1.05335835e-06
Iter: 1703 loss: 1.05327786e-06
Iter: 1704 loss: 1.05323966e-06
Iter: 1705 loss: 1.05331992e-06
Iter: 1706 loss: 1.05322931e-06
Iter: 1707 loss: 1.05317918e-06
Iter: 1708 loss: 1.05317588e-06
Iter: 1709 loss: 1.05315e-06
Iter: 1710 loss: 1.05310869e-06
Iter: 1711 loss: 1.05310869e-06
Iter: 1712 loss: 1.05306026e-06
Iter: 1713 loss: 1.05317417e-06
Iter: 1714 loss: 1.05305628e-06
Iter: 1715 loss: 1.05302797e-06
Iter: 1716 loss: 1.05313666e-06
Iter: 1717 loss: 1.05301137e-06
Iter: 1718 loss: 1.05298238e-06
Iter: 1719 loss: 1.05304343e-06
Iter: 1720 loss: 1.05296601e-06
Iter: 1721 loss: 1.05294396e-06
Iter: 1722 loss: 1.05292747e-06
Iter: 1723 loss: 1.05290133e-06
Iter: 1724 loss: 1.05288814e-06
Iter: 1725 loss: 1.05312643e-06
Iter: 1726 loss: 1.05288245e-06
Iter: 1727 loss: 1.05285176e-06
Iter: 1728 loss: 1.05311528e-06
Iter: 1729 loss: 1.05286267e-06
Iter: 1730 loss: 1.0528197e-06
Iter: 1731 loss: 1.05277559e-06
Iter: 1732 loss: 1.05390473e-06
Iter: 1733 loss: 1.05277763e-06
Iter: 1734 loss: 1.05272227e-06
Iter: 1735 loss: 1.0533164e-06
Iter: 1736 loss: 1.05272488e-06
Iter: 1737 loss: 1.05268487e-06
Iter: 1738 loss: 1.05271909e-06
Iter: 1739 loss: 1.05265849e-06
Iter: 1740 loss: 1.05262416e-06
Iter: 1741 loss: 1.05271647e-06
Iter: 1742 loss: 1.05261961e-06
Iter: 1743 loss: 1.05258687e-06
Iter: 1744 loss: 1.05257789e-06
Iter: 1745 loss: 1.05255344e-06
Iter: 1746 loss: 1.05253139e-06
Iter: 1747 loss: 1.05252116e-06
Iter: 1748 loss: 1.0525132e-06
Iter: 1749 loss: 1.0525672e-06
Iter: 1750 loss: 1.05249137e-06
Iter: 1751 loss: 1.05249455e-06
Iter: 1752 loss: 1.05249353e-06
Iter: 1753 loss: 1.05248751e-06
Iter: 1754 loss: 1.0524958e-06
Iter: 1755 loss: 1.05249501e-06
Iter: 1756 loss: 1.05249035e-06
Iter: 1757 loss: 1.05249126e-06
Iter: 1758 loss: 1.05249228e-06
Iter: 1759 loss: 1.05249501e-06
Iter: 1760 loss: 1.05249592e-06
Iter: 1761 loss: 1.05249069e-06
Iter: 1762 loss: 1.05249228e-06
Iter: 1763 loss: 1.05249785e-06
Iter: 1764 loss: 1.05249217e-06
Iter: 1765 loss: 1.05249615e-06
Iter: 1766 loss: 1.0524941e-06
Iter: 1767 loss: 1.0524916e-06
Iter: 1768 loss: 1.05249273e-06
Iter: 1769 loss: 1.05249228e-06
Iter: 1770 loss: 1.05249251e-06
Iter: 1771 loss: 1.05249262e-06
Iter: 1772 loss: 1.05249251e-06
Iter: 1773 loss: 1.05249228e-06
Iter: 1774 loss: 1.05249228e-06
Iter: 1775 loss: 1.05249262e-06
Iter: 1776 loss: 1.05249262e-06
Iter: 1777 loss: 1.05249228e-06
Iter: 1778 loss: 1.05248012e-06
Iter: 1779 loss: 1.05259141e-06
Iter: 1780 loss: 1.05247022e-06
Iter: 1781 loss: 1.05245499e-06
Iter: 1782 loss: 1.05243396e-06
Iter: 1783 loss: 1.05243544e-06
Iter: 1784 loss: 1.05241952e-06
Iter: 1785 loss: 1.05240827e-06
Iter: 1786 loss: 1.05238882e-06
Iter: 1787 loss: 1.052377e-06
Iter: 1788 loss: 1.0526112e-06
Iter: 1789 loss: 1.05236131e-06
Iter: 1790 loss: 1.05234176e-06
Iter: 1791 loss: 1.05234335e-06
Iter: 1792 loss: 1.05232493e-06
Iter: 1793 loss: 1.05229969e-06
Iter: 1794 loss: 1.05224819e-06
Iter: 1795 loss: 1.0522333e-06
Iter: 1796 loss: 1.05216964e-06
Iter: 1797 loss: 1.05276797e-06
Iter: 1798 loss: 1.05217475e-06
Iter: 1799 loss: 1.05213212e-06
Iter: 1800 loss: 1.05215395e-06
Iter: 1801 loss: 1.05210586e-06
Iter: 1802 loss: 1.05203935e-06
Iter: 1803 loss: 1.05221761e-06
Iter: 1804 loss: 1.05203333e-06
Iter: 1805 loss: 1.05200866e-06
Iter: 1806 loss: 1.05215986e-06
Iter: 1807 loss: 1.05200377e-06
Iter: 1808 loss: 1.05199035e-06
Iter: 1809 loss: 1.05200013e-06
Iter: 1810 loss: 1.05195375e-06
Iter: 1811 loss: 1.05194113e-06
Iter: 1812 loss: 1.05194545e-06
Iter: 1813 loss: 1.05194135e-06
Iter: 1814 loss: 1.05190679e-06
Iter: 1815 loss: 1.05228537e-06
Iter: 1816 loss: 1.05189883e-06
Iter: 1817 loss: 1.05187712e-06
Iter: 1818 loss: 1.05185848e-06
Iter: 1819 loss: 1.05184427e-06
Iter: 1820 loss: 1.05181869e-06
Iter: 1821 loss: 1.05192885e-06
Iter: 1822 loss: 1.05180743e-06
Iter: 1823 loss: 1.05175843e-06
Iter: 1824 loss: 1.05200752e-06
Iter: 1825 loss: 1.05175297e-06
Iter: 1826 loss: 1.0517299e-06
Iter: 1827 loss: 1.05169693e-06
Iter: 1828 loss: 1.05169795e-06
Iter: 1829 loss: 1.05166532e-06
Iter: 1830 loss: 1.05213849e-06
Iter: 1831 loss: 1.05166191e-06
Iter: 1832 loss: 1.05164645e-06
Iter: 1833 loss: 1.05169386e-06
Iter: 1834 loss: 1.05163554e-06
Iter: 1835 loss: 1.05159302e-06
Iter: 1836 loss: 1.05162599e-06
Iter: 1837 loss: 1.05157096e-06
Iter: 1838 loss: 1.05154606e-06
Iter: 1839 loss: 1.05155925e-06
Iter: 1840 loss: 1.05153345e-06
Iter: 1841 loss: 1.05148945e-06
Iter: 1842 loss: 1.05149195e-06
Iter: 1843 loss: 1.05147433e-06
Iter: 1844 loss: 1.05142078e-06
Iter: 1845 loss: 1.05190838e-06
Iter: 1846 loss: 1.05141453e-06
Iter: 1847 loss: 1.05138406e-06
Iter: 1848 loss: 1.05138975e-06
Iter: 1849 loss: 1.05134859e-06
Iter: 1850 loss: 1.05142453e-06
Iter: 1851 loss: 1.05133404e-06
Iter: 1852 loss: 1.05132517e-06
Iter: 1853 loss: 1.05133506e-06
Iter: 1854 loss: 1.05130687e-06
Iter: 1855 loss: 1.05127629e-06
Iter: 1856 loss: 1.05151628e-06
Iter: 1857 loss: 1.05129061e-06
Iter: 1858 loss: 1.05126765e-06
Iter: 1859 loss: 1.05119284e-06
Iter: 1860 loss: 1.05224581e-06
Iter: 1861 loss: 1.05119443e-06
Iter: 1862 loss: 1.05116249e-06
Iter: 1863 loss: 1.05150116e-06
Iter: 1864 loss: 1.05116374e-06
Iter: 1865 loss: 1.05113213e-06
Iter: 1866 loss: 1.05125253e-06
Iter: 1867 loss: 1.0511144e-06
Iter: 1868 loss: 1.05106392e-06
Iter: 1869 loss: 1.05108859e-06
Iter: 1870 loss: 1.05104948e-06
Iter: 1871 loss: 1.05102242e-06
Iter: 1872 loss: 1.05110871e-06
Iter: 1873 loss: 1.05100276e-06
Iter: 1874 loss: 1.05099048e-06
Iter: 1875 loss: 1.05098786e-06
Iter: 1876 loss: 1.0509707e-06
Iter: 1877 loss: 1.05094909e-06
Iter: 1878 loss: 1.05093784e-06
Iter: 1879 loss: 1.05092681e-06
Iter: 1880 loss: 1.05104255e-06
Iter: 1881 loss: 1.05092261e-06
Iter: 1882 loss: 1.05090339e-06
Iter: 1883 loss: 1.05096399e-06
Iter: 1884 loss: 1.05089975e-06
Iter: 1885 loss: 1.05086974e-06
Iter: 1886 loss: 1.0508827e-06
Iter: 1887 loss: 1.05087179e-06
Iter: 1888 loss: 1.0508279e-06
Iter: 1889 loss: 1.05113145e-06
Iter: 1890 loss: 1.05083609e-06
Iter: 1891 loss: 1.05081835e-06
Iter: 1892 loss: 1.05080221e-06
Iter: 1893 loss: 1.05079596e-06
Iter: 1894 loss: 1.05076765e-06
Iter: 1895 loss: 1.0508403e-06
Iter: 1896 loss: 1.05075276e-06
Iter: 1897 loss: 1.05074105e-06
Iter: 1898 loss: 1.05085689e-06
Iter: 1899 loss: 1.0507332e-06
Iter: 1900 loss: 1.05069182e-06
Iter: 1901 loss: 1.0507755e-06
Iter: 1902 loss: 1.05069523e-06
Iter: 1903 loss: 1.05069807e-06
Iter: 1904 loss: 1.0506991e-06
Iter: 1905 loss: 1.05070399e-06
Iter: 1906 loss: 1.05070376e-06
Iter: 1907 loss: 1.05070478e-06
Iter: 1908 loss: 1.05069034e-06
Iter: 1909 loss: 1.05069682e-06
Iter: 1910 loss: 1.05069046e-06
Iter: 1911 loss: 1.05070058e-06
Iter: 1912 loss: 1.05069694e-06
Iter: 1913 loss: 1.05069876e-06
Iter: 1914 loss: 1.05069466e-06
Iter: 1915 loss: 1.05069387e-06
Iter: 1916 loss: 1.05069626e-06
Iter: 1917 loss: 1.050695e-06
Iter: 1918 loss: 1.05069671e-06
Iter: 1919 loss: 1.05069671e-06
Iter: 1920 loss: 1.05069773e-06
Iter: 1921 loss: 1.0506966e-06
Iter: 1922 loss: 1.05069626e-06
Iter: 1923 loss: 1.05069682e-06
Iter: 1924 loss: 1.05069682e-06
Iter: 1925 loss: 1.05069626e-06
Iter: 1926 loss: 1.05069603e-06
Iter: 1927 loss: 1.05069603e-06
Iter: 1928 loss: 1.05069603e-06
Iter: 1929 loss: 1.05069682e-06
Iter: 1930 loss: 1.05069682e-06
Iter: 1931 loss: 1.05069603e-06
Iter: 1932 loss: 1.05069682e-06
Iter: 1933 loss: 1.05066454e-06
Iter: 1934 loss: 1.05113509e-06
Iter: 1935 loss: 1.05064896e-06
Iter: 1936 loss: 1.05062929e-06
Iter: 1937 loss: 1.05081835e-06
Iter: 1938 loss: 1.05063805e-06
Iter: 1939 loss: 1.05058587e-06
Iter: 1940 loss: 1.05063793e-06
Iter: 1941 loss: 1.05057131e-06
Iter: 1942 loss: 1.05056506e-06
Iter: 1943 loss: 1.05069091e-06
Iter: 1944 loss: 1.05055346e-06
Iter: 1945 loss: 1.05054e-06
Iter: 1946 loss: 1.05053277e-06
Iter: 1947 loss: 1.05053334e-06
Iter: 1948 loss: 1.05051492e-06
Iter: 1949 loss: 1.05058e-06
Iter: 1950 loss: 1.05050481e-06
Iter: 1951 loss: 1.05049207e-06
Iter: 1952 loss: 1.05061054e-06
Iter: 1953 loss: 1.05050208e-06
Iter: 1954 loss: 1.05048684e-06
Iter: 1955 loss: 1.05044114e-06
Iter: 1956 loss: 1.05044455e-06
Iter: 1957 loss: 1.05041568e-06
Iter: 1958 loss: 1.05057848e-06
Iter: 1959 loss: 1.05042e-06
Iter: 1960 loss: 1.05040942e-06
Iter: 1961 loss: 1.05040863e-06
Iter: 1962 loss: 1.05038794e-06
Iter: 1963 loss: 1.05037304e-06
Iter: 1964 loss: 1.05031631e-06
Iter: 1965 loss: 1.05129868e-06
Iter: 1966 loss: 1.05032609e-06
Iter: 1967 loss: 1.05027243e-06
Iter: 1968 loss: 1.05059348e-06
Iter: 1969 loss: 1.05026891e-06
Iter: 1970 loss: 1.05022991e-06
Iter: 1971 loss: 1.0503328e-06
Iter: 1972 loss: 1.05019922e-06
Iter: 1973 loss: 1.05017762e-06
Iter: 1974 loss: 1.05018228e-06
Iter: 1975 loss: 1.05015727e-06
Iter: 1976 loss: 1.0501567e-06
Iter: 1977 loss: 1.0501351e-06
Iter: 1978 loss: 1.05012236e-06
Iter: 1979 loss: 1.0502531e-06
Iter: 1980 loss: 1.05010452e-06
Iter: 1981 loss: 1.05008178e-06
Iter: 1982 loss: 1.05009121e-06
Iter: 1983 loss: 1.05006302e-06
Iter: 1984 loss: 1.05003051e-06
Iter: 1985 loss: 1.05033268e-06
Iter: 1986 loss: 1.05003619e-06
Iter: 1987 loss: 1.05002061e-06
Iter: 1988 loss: 1.05005006e-06
Iter: 1989 loss: 1.05001072e-06
Iter: 1990 loss: 1.04998708e-06
Iter: 1991 loss: 1.04996968e-06
Iter: 1992 loss: 1.04995661e-06
Iter: 1993 loss: 1.04992773e-06
Iter: 1994 loss: 1.05018944e-06
Iter: 1995 loss: 1.04991761e-06
Iter: 1996 loss: 1.04989022e-06
Iter: 1997 loss: 1.05000379e-06
Iter: 1998 loss: 1.04987794e-06
Iter: 1999 loss: 1.04985975e-06
Iter: 2000 loss: 1.04982439e-06
Iter: 2001 loss: 1.05070512e-06
Iter: 2002 loss: 1.04981382e-06
Iter: 2003 loss: 1.04978585e-06
Iter: 2004 loss: 1.04988044e-06
Iter: 2005 loss: 1.04976584e-06
Iter: 2006 loss: 1.0497497e-06
Iter: 2007 loss: 1.04994672e-06
Iter: 2008 loss: 1.04974515e-06
Iter: 2009 loss: 1.04972855e-06
Iter: 2010 loss: 1.04972355e-06
Iter: 2011 loss: 1.04971639e-06
Iter: 2012 loss: 1.04968683e-06
Iter: 2013 loss: 1.04986395e-06
Iter: 2014 loss: 1.04966603e-06
Iter: 2015 loss: 1.04962078e-06
Iter: 2016 loss: 1.04962885e-06
Iter: 2017 loss: 1.04959656e-06
Iter: 2018 loss: 1.04962589e-06
Iter: 2019 loss: 1.04957689e-06
Iter: 2020 loss: 1.04956302e-06
Iter: 2021 loss: 1.04985793e-06
Iter: 2022 loss: 1.04956644e-06
Iter: 2023 loss: 1.0495487e-06
Iter: 2024 loss: 1.0495487e-06
Iter: 2025 loss: 1.04953403e-06
Iter: 2026 loss: 1.04951027e-06
Iter: 2027 loss: 1.04947071e-06
Iter: 2028 loss: 1.04947094e-06
Iter: 2029 loss: 1.04945661e-06
Iter: 2030 loss: 1.04944263e-06
Iter: 2031 loss: 1.04943365e-06
Iter: 2032 loss: 1.04940295e-06
Iter: 2033 loss: 1.04940318e-06
Iter: 2034 loss: 1.04937385e-06
Iter: 2035 loss: 1.04941103e-06
Iter: 2036 loss: 1.04936078e-06
Iter: 2037 loss: 1.04931564e-06
Iter: 2038 loss: 1.0493809e-06
Iter: 2039 loss: 1.04930314e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0.8 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi0.8
+ date
Sat Oct 31 19:29:06 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 0 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b001db2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b001da9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b002269d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b0015d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b0015d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b002261e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ab01317b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ab017b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ab0150488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b0004d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b0004d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b0007f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ab01ae840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ab01d81e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ab01c4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ab009b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ab01ae620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa8769ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa875bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa8769048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa87a40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ab0033268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa87a4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa87e5c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa87e5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa868bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa86b6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa868b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa8647158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa857d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa8678400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ab0226158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ab0226510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa8587ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa85041e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6aa84dac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0037138965
test_loss: 0.0039202464
train_loss: 0.002388291
test_loss: 0.0023848498
train_loss: 0.0018952314
test_loss: 0.0021285298
train_loss: 0.0017669192
test_loss: 0.0018112957
train_loss: 0.0014739924
test_loss: 0.0014619578
train_loss: 0.0014698105
test_loss: 0.0014571392
train_loss: 0.0012675473
test_loss: 0.0012911744
train_loss: 0.0012718781
test_loss: 0.0013359295
train_loss: 0.0012135768
test_loss: 0.0012449856
train_loss: 0.0012543509
test_loss: 0.0012032479
train_loss: 0.0011177585
test_loss: 0.0011517522
train_loss: 0.0011827605
test_loss: 0.0011520081
train_loss: 0.0010787228
test_loss: 0.001125012
train_loss: 0.001069358
test_loss: 0.0010692298
train_loss: 0.0010432671
test_loss: 0.0010917402
train_loss: 0.0010133323
test_loss: 0.0010575057
train_loss: 0.0010084397
test_loss: 0.0010369346
train_loss: 0.0009959745
test_loss: 0.0010353007
train_loss: 0.00095864944
test_loss: 0.0010269197
train_loss: 0.00095885387
test_loss: 0.0009959105
train_loss: 0.0009845763
test_loss: 0.0010194276
train_loss: 0.0009955632
test_loss: 0.0010019251
train_loss: 0.00094440003
test_loss: 0.0010039528
train_loss: 0.0009719713
test_loss: 0.0009920785
train_loss: 0.0009133809
test_loss: 0.0009785646
train_loss: 0.0009381764
test_loss: 0.0009729355
train_loss: 0.00092225784
test_loss: 0.0009688837
train_loss: 0.00091769686
test_loss: 0.0009646203
train_loss: 0.00089956983
test_loss: 0.00095618167
train_loss: 0.000915672
test_loss: 0.0009537948
train_loss: 0.0009256635
test_loss: 0.00095213874
train_loss: 0.00089616724
test_loss: 0.00094241917
train_loss: 0.0008874467
test_loss: 0.0009390681
train_loss: 0.00090311404
test_loss: 0.000934842
train_loss: 0.0008881078
test_loss: 0.00093378674
train_loss: 0.0009108645
test_loss: 0.0009338671
train_loss: 0.00091280724
test_loss: 0.00093156536
train_loss: 0.0008814697
test_loss: 0.0009336365
train_loss: 0.000877293
test_loss: 0.0009271975
train_loss: 0.00086617714
test_loss: 0.0009238817
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi0.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi0.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4f3bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4ec2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4fb1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4f0f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4f0f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4f0f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4def950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4df1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4df1378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4e24bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4e631e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4e637b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c0b8ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4dc2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c0b3cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4d84f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4d86488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c0b4fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0d4e24ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c0aeeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c0a49950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c0a49bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c0af7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c09c47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c09c4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c09c4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c0a1ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c0a1e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c09af158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c09442f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c09747b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c089dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c083d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c09441e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c083da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0c0910d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.50154415e-06
Iter: 2 loss: 1.4917091e-06
Iter: 3 loss: 1.49117545e-06
Iter: 4 loss: 1.48598588e-06
Iter: 5 loss: 1.4931868e-06
Iter: 6 loss: 1.48341644e-06
Iter: 7 loss: 1.48112599e-06
Iter: 8 loss: 1.488115e-06
Iter: 9 loss: 1.48043728e-06
Iter: 10 loss: 1.47843662e-06
Iter: 11 loss: 1.49603829e-06
Iter: 12 loss: 1.47834953e-06
Iter: 13 loss: 1.47680123e-06
Iter: 14 loss: 1.47585013e-06
Iter: 15 loss: 1.47523519e-06
Iter: 16 loss: 1.47365813e-06
Iter: 17 loss: 1.4842426e-06
Iter: 18 loss: 1.47349169e-06
Iter: 19 loss: 1.47166827e-06
Iter: 20 loss: 1.47276705e-06
Iter: 21 loss: 1.4704874e-06
Iter: 22 loss: 1.46837829e-06
Iter: 23 loss: 1.46558955e-06
Iter: 24 loss: 1.46542766e-06
Iter: 25 loss: 1.46284401e-06
Iter: 26 loss: 1.46254274e-06
Iter: 27 loss: 1.46147192e-06
Iter: 28 loss: 1.45899389e-06
Iter: 29 loss: 1.48963613e-06
Iter: 30 loss: 1.45880256e-06
Iter: 31 loss: 1.45648551e-06
Iter: 32 loss: 1.45917284e-06
Iter: 33 loss: 1.45523575e-06
Iter: 34 loss: 1.45282502e-06
Iter: 35 loss: 1.46405682e-06
Iter: 36 loss: 1.45235026e-06
Iter: 37 loss: 1.44949752e-06
Iter: 38 loss: 1.44759315e-06
Iter: 39 loss: 1.44647902e-06
Iter: 40 loss: 1.4492083e-06
Iter: 41 loss: 1.4452396e-06
Iter: 42 loss: 1.44402179e-06
Iter: 43 loss: 1.44191392e-06
Iter: 44 loss: 1.44192472e-06
Iter: 45 loss: 1.44053877e-06
Iter: 46 loss: 1.45331921e-06
Iter: 47 loss: 1.44047613e-06
Iter: 48 loss: 1.43911166e-06
Iter: 49 loss: 1.44470425e-06
Iter: 50 loss: 1.43880243e-06
Iter: 51 loss: 1.43816169e-06
Iter: 52 loss: 1.43801117e-06
Iter: 53 loss: 1.43762645e-06
Iter: 54 loss: 1.43645309e-06
Iter: 55 loss: 1.44090518e-06
Iter: 56 loss: 1.43616467e-06
Iter: 57 loss: 1.43493764e-06
Iter: 58 loss: 1.4345153e-06
Iter: 59 loss: 1.43386228e-06
Iter: 60 loss: 1.43216539e-06
Iter: 61 loss: 1.43727527e-06
Iter: 62 loss: 1.43167131e-06
Iter: 63 loss: 1.42987e-06
Iter: 64 loss: 1.44456794e-06
Iter: 65 loss: 1.42976728e-06
Iter: 66 loss: 1.42870726e-06
Iter: 67 loss: 1.42675242e-06
Iter: 68 loss: 1.47089531e-06
Iter: 69 loss: 1.42674253e-06
Iter: 70 loss: 1.42505598e-06
Iter: 71 loss: 1.42572617e-06
Iter: 72 loss: 1.42388149e-06
Iter: 73 loss: 1.42214878e-06
Iter: 74 loss: 1.43407215e-06
Iter: 75 loss: 1.42198394e-06
Iter: 76 loss: 1.42035651e-06
Iter: 77 loss: 1.42381805e-06
Iter: 78 loss: 1.41973987e-06
Iter: 79 loss: 1.42051272e-06
Iter: 80 loss: 1.41917712e-06
Iter: 81 loss: 1.41883595e-06
Iter: 82 loss: 1.41792771e-06
Iter: 83 loss: 1.42478757e-06
Iter: 84 loss: 1.41775354e-06
Iter: 85 loss: 1.41782823e-06
Iter: 86 loss: 1.41745181e-06
Iter: 87 loss: 1.417161e-06
Iter: 88 loss: 1.41654903e-06
Iter: 89 loss: 1.42514682e-06
Iter: 90 loss: 1.41652561e-06
Iter: 91 loss: 1.41604664e-06
Iter: 92 loss: 1.41600344e-06
Iter: 93 loss: 1.41566227e-06
Iter: 94 loss: 1.41486839e-06
Iter: 95 loss: 1.41742339e-06
Iter: 96 loss: 1.41466126e-06
Iter: 97 loss: 1.41396833e-06
Iter: 98 loss: 1.41394162e-06
Iter: 99 loss: 1.4136582e-06
Iter: 100 loss: 1.41304963e-06
Iter: 101 loss: 1.42305248e-06
Iter: 102 loss: 1.41303758e-06
Iter: 103 loss: 1.41260512e-06
Iter: 104 loss: 1.41255077e-06
Iter: 105 loss: 1.41218879e-06
Iter: 106 loss: 1.41191867e-06
Iter: 107 loss: 1.41179578e-06
Iter: 108 loss: 1.41135047e-06
Iter: 109 loss: 1.41258101e-06
Iter: 110 loss: 1.41121529e-06
Iter: 111 loss: 1.41086025e-06
Iter: 112 loss: 1.41045643e-06
Iter: 113 loss: 1.41039391e-06
Iter: 114 loss: 1.41010355e-06
Iter: 115 loss: 1.4099993e-06
Iter: 116 loss: 1.40978602e-06
Iter: 117 loss: 1.40924271e-06
Iter: 118 loss: 1.41437295e-06
Iter: 119 loss: 1.40918894e-06
Iter: 120 loss: 1.40887528e-06
Iter: 121 loss: 1.4088655e-06
Iter: 122 loss: 1.40854763e-06
Iter: 123 loss: 1.4091496e-06
Iter: 124 loss: 1.40840325e-06
Iter: 125 loss: 1.40818258e-06
Iter: 126 loss: 1.40775478e-06
Iter: 127 loss: 1.4166626e-06
Iter: 128 loss: 1.40775455e-06
Iter: 129 loss: 1.40731618e-06
Iter: 130 loss: 1.40816746e-06
Iter: 131 loss: 1.40714451e-06
Iter: 132 loss: 1.40678446e-06
Iter: 133 loss: 1.40675e-06
Iter: 134 loss: 1.40651787e-06
Iter: 135 loss: 1.40591328e-06
Iter: 136 loss: 1.41160035e-06
Iter: 137 loss: 1.40585507e-06
Iter: 138 loss: 1.40582222e-06
Iter: 139 loss: 1.40560724e-06
Iter: 140 loss: 1.40538771e-06
Iter: 141 loss: 1.4049665e-06
Iter: 142 loss: 1.40496877e-06
Iter: 143 loss: 1.40462816e-06
Iter: 144 loss: 1.40622319e-06
Iter: 145 loss: 1.40455506e-06
Iter: 146 loss: 1.40429813e-06
Iter: 147 loss: 1.40524821e-06
Iter: 148 loss: 1.40424e-06
Iter: 149 loss: 1.40400437e-06
Iter: 150 loss: 1.40619477e-06
Iter: 151 loss: 1.40397515e-06
Iter: 152 loss: 1.40377574e-06
Iter: 153 loss: 1.40340717e-06
Iter: 154 loss: 1.40341274e-06
Iter: 155 loss: 1.40304428e-06
Iter: 156 loss: 1.40349744e-06
Iter: 157 loss: 1.40287489e-06
Iter: 158 loss: 1.40290467e-06
Iter: 159 loss: 1.40270254e-06
Iter: 160 loss: 1.40258771e-06
Iter: 161 loss: 1.40227644e-06
Iter: 162 loss: 1.40342013e-06
Iter: 163 loss: 1.40212626e-06
Iter: 164 loss: 1.40168981e-06
Iter: 165 loss: 1.4038535e-06
Iter: 166 loss: 1.40163706e-06
Iter: 167 loss: 1.4014397e-06
Iter: 168 loss: 1.40139412e-06
Iter: 169 loss: 1.40120301e-06
Iter: 170 loss: 1.40067937e-06
Iter: 171 loss: 1.40339637e-06
Iter: 172 loss: 1.4005027e-06
Iter: 173 loss: 1.39995655e-06
Iter: 174 loss: 1.40508268e-06
Iter: 175 loss: 1.39992403e-06
Iter: 176 loss: 1.39938959e-06
Iter: 177 loss: 1.40317684e-06
Iter: 178 loss: 1.39931876e-06
Iter: 179 loss: 1.39909139e-06
Iter: 180 loss: 1.39860936e-06
Iter: 181 loss: 1.40945326e-06
Iter: 182 loss: 1.39861493e-06
Iter: 183 loss: 1.3984e-06
Iter: 184 loss: 1.39832548e-06
Iter: 185 loss: 1.39808e-06
Iter: 186 loss: 1.39836015e-06
Iter: 187 loss: 1.3979809e-06
Iter: 188 loss: 1.39769e-06
Iter: 189 loss: 1.3976121e-06
Iter: 190 loss: 1.39744259e-06
Iter: 191 loss: 1.39706731e-06
Iter: 192 loss: 1.39711233e-06
Iter: 193 loss: 1.39678889e-06
Iter: 194 loss: 1.39663052e-06
Iter: 195 loss: 1.39653218e-06
Iter: 196 loss: 1.39629492e-06
Iter: 197 loss: 1.395715e-06
Iter: 198 loss: 1.40131726e-06
Iter: 199 loss: 1.39564486e-06
Iter: 200 loss: 1.39533552e-06
Iter: 201 loss: 1.39922599e-06
Iter: 202 loss: 1.39532472e-06
Iter: 203 loss: 1.39497251e-06
Iter: 204 loss: 1.39596523e-06
Iter: 205 loss: 1.39487031e-06
Iter: 206 loss: 1.39468125e-06
Iter: 207 loss: 1.39429756e-06
Iter: 208 loss: 1.39429858e-06
Iter: 209 loss: 1.39394797e-06
Iter: 210 loss: 1.39907479e-06
Iter: 211 loss: 1.3939565e-06
Iter: 212 loss: 1.39350504e-06
Iter: 213 loss: 1.39323549e-06
Iter: 214 loss: 1.39306417e-06
Iter: 215 loss: 1.39257531e-06
Iter: 216 loss: 1.39192923e-06
Iter: 217 loss: 1.39187728e-06
Iter: 218 loss: 1.39184522e-06
Iter: 219 loss: 1.39158306e-06
Iter: 220 loss: 1.39124643e-06
Iter: 221 loss: 1.39111148e-06
Iter: 222 loss: 1.39093902e-06
Iter: 223 loss: 1.39058557e-06
Iter: 224 loss: 1.39126064e-06
Iter: 225 loss: 1.39044084e-06
Iter: 226 loss: 1.39011854e-06
Iter: 227 loss: 1.39092151e-06
Iter: 228 loss: 1.39000917e-06
Iter: 229 loss: 1.38969449e-06
Iter: 230 loss: 1.39386907e-06
Iter: 231 loss: 1.38968926e-06
Iter: 232 loss: 1.38946621e-06
Iter: 233 loss: 1.38894325e-06
Iter: 234 loss: 1.39489327e-06
Iter: 235 loss: 1.38890755e-06
Iter: 236 loss: 1.38863106e-06
Iter: 237 loss: 1.38862674e-06
Iter: 238 loss: 1.38828375e-06
Iter: 239 loss: 1.38849873e-06
Iter: 240 loss: 1.38808332e-06
Iter: 241 loss: 1.3877775e-06
Iter: 242 loss: 1.38729388e-06
Iter: 243 loss: 1.38727614e-06
Iter: 244 loss: 1.38729683e-06
Iter: 245 loss: 1.38702569e-06
Iter: 246 loss: 1.38681571e-06
Iter: 247 loss: 1.38618634e-06
Iter: 248 loss: 1.39063286e-06
Iter: 249 loss: 1.38606174e-06
Iter: 250 loss: 1.38551081e-06
Iter: 251 loss: 1.38704286e-06
Iter: 252 loss: 1.38532039e-06
Iter: 253 loss: 1.38524479e-06
Iter: 254 loss: 1.3850663e-06
Iter: 255 loss: 1.38484324e-06
Iter: 256 loss: 1.38460655e-06
Iter: 257 loss: 1.38456119e-06
Iter: 258 loss: 1.3842789e-06
Iter: 259 loss: 1.38461655e-06
Iter: 260 loss: 1.38410587e-06
Iter: 261 loss: 1.38383348e-06
Iter: 262 loss: 1.38794417e-06
Iter: 263 loss: 1.38383439e-06
Iter: 264 loss: 1.38360792e-06
Iter: 265 loss: 1.38418079e-06
Iter: 266 loss: 1.38353857e-06
Iter: 267 loss: 1.38336713e-06
Iter: 268 loss: 1.38304017e-06
Iter: 269 loss: 1.38304108e-06
Iter: 270 loss: 1.38290341e-06
Iter: 271 loss: 1.38285895e-06
Iter: 272 loss: 1.38267751e-06
Iter: 273 loss: 1.38247788e-06
Iter: 274 loss: 1.38244127e-06
Iter: 275 loss: 1.38227688e-06
Iter: 276 loss: 1.38244422e-06
Iter: 277 loss: 1.38217808e-06
Iter: 278 loss: 1.38194764e-06
Iter: 279 loss: 1.38506664e-06
Iter: 280 loss: 1.38194673e-06
Iter: 281 loss: 1.3818385e-06
Iter: 282 loss: 1.38152893e-06
Iter: 283 loss: 1.3837489e-06
Iter: 284 loss: 1.38149244e-06
Iter: 285 loss: 1.38105213e-06
Iter: 286 loss: 1.38250766e-06
Iter: 287 loss: 1.38095493e-06
Iter: 288 loss: 1.3808442e-06
Iter: 289 loss: 1.38074324e-06
Iter: 290 loss: 1.3806341e-06
Iter: 291 loss: 1.38035239e-06
Iter: 292 loss: 1.382439e-06
Iter: 293 loss: 1.38026689e-06
Iter: 294 loss: 1.3800958e-06
Iter: 295 loss: 1.38008647e-06
Iter: 296 loss: 1.37992288e-06
Iter: 297 loss: 1.3805643e-06
Iter: 298 loss: 1.37989809e-06
Iter: 299 loss: 1.37977361e-06
Iter: 300 loss: 1.37978589e-06
Iter: 301 loss: 1.37966913e-06
Iter: 302 loss: 1.37952736e-06
Iter: 303 loss: 1.37952247e-06
Iter: 304 loss: 1.37941504e-06
Iter: 305 loss: 1.37924462e-06
Iter: 306 loss: 1.37924189e-06
Iter: 307 loss: 1.37914458e-06
Iter: 308 loss: 1.37891539e-06
Iter: 309 loss: 1.38200903e-06
Iter: 310 loss: 1.37890288e-06
Iter: 311 loss: 1.37867869e-06
Iter: 312 loss: 1.3803176e-06
Iter: 313 loss: 1.37867698e-06
Iter: 314 loss: 1.3784736e-06
Iter: 315 loss: 1.37971574e-06
Iter: 316 loss: 1.37842119e-06
Iter: 317 loss: 1.37832592e-06
Iter: 318 loss: 1.37804068e-06
Iter: 319 loss: 1.3815702e-06
Iter: 320 loss: 1.37804113e-06
Iter: 321 loss: 1.37791835e-06
Iter: 322 loss: 1.37790084e-06
Iter: 323 loss: 1.37771224e-06
Iter: 324 loss: 1.3774295e-06
Iter: 325 loss: 1.37742e-06
Iter: 326 loss: 1.37716665e-06
Iter: 327 loss: 1.37728784e-06
Iter: 328 loss: 1.37698203e-06
Iter: 329 loss: 1.37684788e-06
Iter: 330 loss: 1.37682537e-06
Iter: 331 loss: 1.37663096e-06
Iter: 332 loss: 1.37672669e-06
Iter: 333 loss: 1.37653342e-06
Iter: 334 loss: 1.37638472e-06
Iter: 335 loss: 1.37653592e-06
Iter: 336 loss: 1.37630309e-06
Iter: 337 loss: 1.37615439e-06
Iter: 338 loss: 1.37676057e-06
Iter: 339 loss: 1.37611028e-06
Iter: 340 loss: 1.37596908e-06
Iter: 341 loss: 1.37716245e-06
Iter: 342 loss: 1.37597624e-06
Iter: 343 loss: 1.37586881e-06
Iter: 344 loss: 1.37564871e-06
Iter: 345 loss: 1.37712868e-06
Iter: 346 loss: 1.3755913e-06
Iter: 347 loss: 1.37549932e-06
Iter: 348 loss: 1.37543429e-06
Iter: 349 loss: 1.37526774e-06
Iter: 350 loss: 1.37501888e-06
Iter: 351 loss: 1.37501752e-06
Iter: 352 loss: 1.37479026e-06
Iter: 353 loss: 1.37486234e-06
Iter: 354 loss: 1.37467089e-06
Iter: 355 loss: 1.37461166e-06
Iter: 356 loss: 1.37452923e-06
Iter: 357 loss: 1.37444783e-06
Iter: 358 loss: 1.37427332e-06
Iter: 359 loss: 1.37830978e-06
Iter: 360 loss: 1.37427355e-06
Iter: 361 loss: 1.37413235e-06
Iter: 362 loss: 1.3753338e-06
Iter: 363 loss: 1.37412121e-06
Iter: 364 loss: 1.37400764e-06
Iter: 365 loss: 1.37383608e-06
Iter: 366 loss: 1.37383097e-06
Iter: 367 loss: 1.37357347e-06
Iter: 368 loss: 1.37414622e-06
Iter: 369 loss: 1.37352038e-06
Iter: 370 loss: 1.37321899e-06
Iter: 371 loss: 1.3740206e-06
Iter: 372 loss: 1.37316124e-06
Iter: 373 loss: 1.3729582e-06
Iter: 374 loss: 1.37624534e-06
Iter: 375 loss: 1.37295012e-06
Iter: 376 loss: 1.37286111e-06
Iter: 377 loss: 1.37268626e-06
Iter: 378 loss: 1.37267739e-06
Iter: 379 loss: 1.37249469e-06
Iter: 380 loss: 1.37282132e-06
Iter: 381 loss: 1.37244467e-06
Iter: 382 loss: 1.37232223e-06
Iter: 383 loss: 1.37230325e-06
Iter: 384 loss: 1.37223583e-06
Iter: 385 loss: 1.37199822e-06
Iter: 386 loss: 1.37312645e-06
Iter: 387 loss: 1.37192865e-06
Iter: 388 loss: 1.37173834e-06
Iter: 389 loss: 1.37172549e-06
Iter: 390 loss: 1.37151824e-06
Iter: 391 loss: 1.37197e-06
Iter: 392 loss: 1.3714357e-06
Iter: 393 loss: 1.37133634e-06
Iter: 394 loss: 1.3713418e-06
Iter: 395 loss: 1.37123584e-06
Iter: 396 loss: 1.37109589e-06
Iter: 397 loss: 1.37280631e-06
Iter: 398 loss: 1.37107668e-06
Iter: 399 loss: 1.37100324e-06
Iter: 400 loss: 1.37087e-06
Iter: 401 loss: 1.37370262e-06
Iter: 402 loss: 1.37086022e-06
Iter: 403 loss: 1.37070731e-06
Iter: 404 loss: 1.37159611e-06
Iter: 405 loss: 1.37067252e-06
Iter: 406 loss: 1.37052234e-06
Iter: 407 loss: 1.37159736e-06
Iter: 408 loss: 1.37052689e-06
Iter: 409 loss: 1.37037887e-06
Iter: 410 loss: 1.37032157e-06
Iter: 411 loss: 1.37025859e-06
Iter: 412 loss: 1.37007692e-06
Iter: 413 loss: 1.36994788e-06
Iter: 414 loss: 1.36987228e-06
Iter: 415 loss: 1.36975405e-06
Iter: 416 loss: 1.37192251e-06
Iter: 417 loss: 1.36975336e-06
Iter: 418 loss: 1.36958306e-06
Iter: 419 loss: 1.37009476e-06
Iter: 420 loss: 1.36954031e-06
Iter: 421 loss: 1.36944459e-06
Iter: 422 loss: 1.36926451e-06
Iter: 423 loss: 1.36927929e-06
Iter: 424 loss: 1.36907488e-06
Iter: 425 loss: 1.36965775e-06
Iter: 426 loss: 1.36900712e-06
Iter: 427 loss: 1.36875224e-06
Iter: 428 loss: 1.37185737e-06
Iter: 429 loss: 1.36874519e-06
Iter: 430 loss: 1.36865924e-06
Iter: 431 loss: 1.3684122e-06
Iter: 432 loss: 1.37087795e-06
Iter: 433 loss: 1.36841209e-06
Iter: 434 loss: 1.36843221e-06
Iter: 435 loss: 1.36831022e-06
Iter: 436 loss: 1.36823701e-06
Iter: 437 loss: 1.3680617e-06
Iter: 438 loss: 1.36992185e-06
Iter: 439 loss: 1.36805374e-06
Iter: 440 loss: 1.36794779e-06
Iter: 441 loss: 1.36922358e-06
Iter: 442 loss: 1.36794438e-06
Iter: 443 loss: 1.3678316e-06
Iter: 444 loss: 1.36814469e-06
Iter: 445 loss: 1.3677942e-06
Iter: 446 loss: 1.36768517e-06
Iter: 447 loss: 1.36771041e-06
Iter: 448 loss: 1.36760673e-06
Iter: 449 loss: 1.3674595e-06
Iter: 450 loss: 1.36748167e-06
Iter: 451 loss: 1.36734184e-06
Iter: 452 loss: 1.36729955e-06
Iter: 453 loss: 1.36726044e-06
Iter: 454 loss: 1.36718904e-06
Iter: 455 loss: 1.36709332e-06
Iter: 456 loss: 1.36709536e-06
Iter: 457 loss: 1.36698122e-06
Iter: 458 loss: 1.366893e-06
Iter: 459 loss: 1.3668664e-06
Iter: 460 loss: 1.3668448e-06
Iter: 461 loss: 1.36679876e-06
Iter: 462 loss: 1.36672736e-06
Iter: 463 loss: 1.36657945e-06
Iter: 464 loss: 1.36865583e-06
Iter: 465 loss: 1.36658241e-06
Iter: 466 loss: 1.36645303e-06
Iter: 467 loss: 1.36765789e-06
Iter: 468 loss: 1.36645303e-06
Iter: 469 loss: 1.3663132e-06
Iter: 470 loss: 1.36640119e-06
Iter: 471 loss: 1.36624328e-06
Iter: 472 loss: 1.3661587e-06
Iter: 473 loss: 1.36606013e-06
Iter: 474 loss: 1.36602921e-06
Iter: 475 loss: 1.3660233e-06
Iter: 476 loss: 1.36596918e-06
Iter: 477 loss: 1.36591348e-06
Iter: 478 loss: 1.36582275e-06
Iter: 479 loss: 1.36581241e-06
Iter: 480 loss: 1.36572089e-06
Iter: 481 loss: 1.36588562e-06
Iter: 482 loss: 1.36566598e-06
Iter: 483 loss: 1.36559311e-06
Iter: 484 loss: 1.36639926e-06
Iter: 485 loss: 1.3655872e-06
Iter: 486 loss: 1.36550068e-06
Iter: 487 loss: 1.36584117e-06
Iter: 488 loss: 1.36548908e-06
Iter: 489 loss: 1.36544168e-06
Iter: 490 loss: 1.36529138e-06
Iter: 491 loss: 1.36733752e-06
Iter: 492 loss: 1.36528661e-06
Iter: 493 loss: 1.3651902e-06
Iter: 494 loss: 1.36519463e-06
Iter: 495 loss: 1.36509288e-06
Iter: 496 loss: 1.36508049e-06
Iter: 497 loss: 1.36500546e-06
Iter: 498 loss: 1.36490962e-06
Iter: 499 loss: 1.36493441e-06
Iter: 500 loss: 1.36485642e-06
Iter: 501 loss: 1.36472e-06
Iter: 502 loss: 1.36472681e-06
Iter: 503 loss: 1.36467952e-06
Iter: 504 loss: 1.36455185e-06
Iter: 505 loss: 1.36591211e-06
Iter: 506 loss: 1.36454491e-06
Iter: 507 loss: 1.36445078e-06
Iter: 508 loss: 1.36583481e-06
Iter: 509 loss: 1.36443873e-06
Iter: 510 loss: 1.36435426e-06
Iter: 511 loss: 1.36437461e-06
Iter: 512 loss: 1.36429082e-06
Iter: 513 loss: 1.3641735e-06
Iter: 514 loss: 1.36419237e-06
Iter: 515 loss: 1.3641112e-06
Iter: 516 loss: 1.36401354e-06
Iter: 517 loss: 1.36390258e-06
Iter: 518 loss: 1.36389599e-06
Iter: 519 loss: 1.36378856e-06
Iter: 520 loss: 1.36507151e-06
Iter: 521 loss: 1.36377457e-06
Iter: 522 loss: 1.36367453e-06
Iter: 523 loss: 1.36395261e-06
Iter: 524 loss: 1.36362246e-06
Iter: 525 loss: 1.36356732e-06
Iter: 526 loss: 1.36356266e-06
Iter: 527 loss: 1.36351514e-06
Iter: 528 loss: 1.36339486e-06
Iter: 529 loss: 1.36446624e-06
Iter: 530 loss: 1.36338724e-06
Iter: 531 loss: 1.36331414e-06
Iter: 532 loss: 1.36317885e-06
Iter: 533 loss: 1.36636322e-06
Iter: 534 loss: 1.36316703e-06
Iter: 535 loss: 1.36305471e-06
Iter: 536 loss: 1.36425479e-06
Iter: 537 loss: 1.36306062e-06
Iter: 538 loss: 1.36291555e-06
Iter: 539 loss: 1.36323456e-06
Iter: 540 loss: 1.36287235e-06
Iter: 541 loss: 1.36276856e-06
Iter: 542 loss: 1.36259973e-06
Iter: 543 loss: 1.36261667e-06
Iter: 544 loss: 1.36254039e-06
Iter: 545 loss: 1.3624981e-06
Iter: 546 loss: 1.36246013e-06
Iter: 547 loss: 1.36236508e-06
Iter: 548 loss: 1.36396466e-06
Iter: 549 loss: 1.36233871e-06
Iter: 550 loss: 1.36224492e-06
Iter: 551 loss: 1.36344283e-06
Iter: 552 loss: 1.36222479e-06
Iter: 553 loss: 1.36213396e-06
Iter: 554 loss: 1.36209383e-06
Iter: 555 loss: 1.36204267e-06
Iter: 556 loss: 1.36193353e-06
Iter: 557 loss: 1.36263611e-06
Iter: 558 loss: 1.36189067e-06
Iter: 559 loss: 1.36176936e-06
Iter: 560 loss: 1.36191693e-06
Iter: 561 loss: 1.3616966e-06
Iter: 562 loss: 1.36158928e-06
Iter: 563 loss: 1.36187771e-06
Iter: 564 loss: 1.3615529e-06
Iter: 565 loss: 1.36140682e-06
Iter: 566 loss: 1.36211349e-06
Iter: 567 loss: 1.36138567e-06
Iter: 568 loss: 1.36129904e-06
Iter: 569 loss: 1.36125686e-06
Iter: 570 loss: 1.36125118e-06
Iter: 571 loss: 1.36116455e-06
Iter: 572 loss: 1.36270273e-06
Iter: 573 loss: 1.36116716e-06
Iter: 574 loss: 1.3610661e-06
Iter: 575 loss: 1.36096628e-06
Iter: 576 loss: 1.36097105e-06
Iter: 577 loss: 1.36085771e-06
Iter: 578 loss: 1.36139204e-06
Iter: 579 loss: 1.36083202e-06
Iter: 580 loss: 1.36072515e-06
Iter: 581 loss: 1.3611027e-06
Iter: 582 loss: 1.36066262e-06
Iter: 583 loss: 1.36057906e-06
Iter: 584 loss: 1.36042149e-06
Iter: 585 loss: 1.36042672e-06
Iter: 586 loss: 1.36024244e-06
Iter: 587 loss: 1.36130382e-06
Iter: 588 loss: 1.36023186e-06
Iter: 589 loss: 1.36011079e-06
Iter: 590 loss: 1.3601051e-06
Iter: 591 loss: 1.36005463e-06
Iter: 592 loss: 1.36005985e-06
Iter: 593 loss: 1.36000165e-06
Iter: 594 loss: 1.35990206e-06
Iter: 595 loss: 1.36000767e-06
Iter: 596 loss: 1.35983646e-06
Iter: 597 loss: 1.3597861e-06
Iter: 598 loss: 1.35977416e-06
Iter: 599 loss: 1.3597346e-06
Iter: 600 loss: 1.35960977e-06
Iter: 601 loss: 1.3607746e-06
Iter: 602 loss: 1.3595959e-06
Iter: 603 loss: 1.35948187e-06
Iter: 604 loss: 1.36007884e-06
Iter: 605 loss: 1.35947744e-06
Iter: 606 loss: 1.35935693e-06
Iter: 607 loss: 1.36014705e-06
Iter: 608 loss: 1.35933681e-06
Iter: 609 loss: 1.35926371e-06
Iter: 610 loss: 1.359154e-06
Iter: 611 loss: 1.36182211e-06
Iter: 612 loss: 1.35914979e-06
Iter: 613 loss: 1.35899927e-06
Iter: 614 loss: 1.35902314e-06
Iter: 615 loss: 1.35890423e-06
Iter: 616 loss: 1.3587221e-06
Iter: 617 loss: 1.36161862e-06
Iter: 618 loss: 1.35871278e-06
Iter: 619 loss: 1.35851133e-06
Iter: 620 loss: 1.35950859e-06
Iter: 621 loss: 1.35848745e-06
Iter: 622 loss: 1.35840355e-06
Iter: 623 loss: 1.35839196e-06
Iter: 624 loss: 1.35832806e-06
Iter: 625 loss: 1.35831237e-06
Iter: 626 loss: 1.35827315e-06
Iter: 627 loss: 1.35817527e-06
Iter: 628 loss: 1.35847699e-06
Iter: 629 loss: 1.35814946e-06
Iter: 630 loss: 1.35806181e-06
Iter: 631 loss: 1.35842765e-06
Iter: 632 loss: 1.35802918e-06
Iter: 633 loss: 1.35791447e-06
Iter: 634 loss: 1.35804407e-06
Iter: 635 loss: 1.35784239e-06
Iter: 636 loss: 1.35772689e-06
Iter: 637 loss: 1.35761661e-06
Iter: 638 loss: 1.35759819e-06
Iter: 639 loss: 1.35753749e-06
Iter: 640 loss: 1.35749531e-06
Iter: 641 loss: 1.35743e-06
Iter: 642 loss: 1.35728487e-06
Iter: 643 loss: 1.3572834e-06
Iter: 644 loss: 1.35714367e-06
Iter: 645 loss: 1.35763855e-06
Iter: 646 loss: 1.35711207e-06
Iter: 647 loss: 1.35696655e-06
Iter: 648 loss: 1.35814764e-06
Iter: 649 loss: 1.35695632e-06
Iter: 650 loss: 1.35691448e-06
Iter: 651 loss: 1.3567535e-06
Iter: 652 loss: 1.35816413e-06
Iter: 653 loss: 1.35673463e-06
Iter: 654 loss: 1.35666733e-06
Iter: 655 loss: 1.35664072e-06
Iter: 656 loss: 1.35655648e-06
Iter: 657 loss: 1.35649429e-06
Iter: 658 loss: 1.35645507e-06
Iter: 659 loss: 1.3563299e-06
Iter: 660 loss: 1.35671769e-06
Iter: 661 loss: 1.35629102e-06
Iter: 662 loss: 1.35618438e-06
Iter: 663 loss: 1.35643313e-06
Iter: 664 loss: 1.35612549e-06
Iter: 665 loss: 1.35598566e-06
Iter: 666 loss: 1.35691471e-06
Iter: 667 loss: 1.3559843e-06
Iter: 668 loss: 1.3558863e-06
Iter: 669 loss: 1.35577488e-06
Iter: 670 loss: 1.35577102e-06
Iter: 671 loss: 1.35561186e-06
Iter: 672 loss: 1.35611481e-06
Iter: 673 loss: 1.3555898e-06
Iter: 674 loss: 1.35542166e-06
Iter: 675 loss: 1.35668711e-06
Iter: 676 loss: 1.35541268e-06
Iter: 677 loss: 1.35531809e-06
Iter: 678 loss: 1.35505684e-06
Iter: 679 loss: 1.35769e-06
Iter: 680 loss: 1.35501818e-06
Iter: 681 loss: 1.35492678e-06
Iter: 682 loss: 1.35486243e-06
Iter: 683 loss: 1.35474818e-06
Iter: 684 loss: 1.35458254e-06
Iter: 685 loss: 1.35459072e-06
Iter: 686 loss: 1.35445111e-06
Iter: 687 loss: 1.3554212e-06
Iter: 688 loss: 1.35444679e-06
Iter: 689 loss: 1.35431037e-06
Iter: 690 loss: 1.35461346e-06
Iter: 691 loss: 1.35426012e-06
Iter: 692 loss: 1.35418031e-06
Iter: 693 loss: 1.3543e-06
Iter: 694 loss: 1.35415451e-06
Iter: 695 loss: 1.35401308e-06
Iter: 696 loss: 1.35460505e-06
Iter: 697 loss: 1.35399682e-06
Iter: 698 loss: 1.35389701e-06
Iter: 699 loss: 1.35424978e-06
Iter: 700 loss: 1.35387495e-06
Iter: 701 loss: 1.3537757e-06
Iter: 702 loss: 1.3536046e-06
Iter: 703 loss: 1.3535855e-06
Iter: 704 loss: 1.35339792e-06
Iter: 705 loss: 1.35352036e-06
Iter: 706 loss: 1.35326081e-06
Iter: 707 loss: 1.35314804e-06
Iter: 708 loss: 1.35311643e-06
Iter: 709 loss: 1.35302253e-06
Iter: 710 loss: 1.3527399e-06
Iter: 711 loss: 1.3557401e-06
Iter: 712 loss: 1.35273604e-06
Iter: 713 loss: 1.35263383e-06
Iter: 714 loss: 1.35260188e-06
Iter: 715 loss: 1.35248365e-06
Iter: 716 loss: 1.3523e-06
Iter: 717 loss: 1.35230357e-06
Iter: 718 loss: 1.35211485e-06
Iter: 719 loss: 1.35284495e-06
Iter: 720 loss: 1.35207858e-06
Iter: 721 loss: 1.35187452e-06
Iter: 722 loss: 1.35299263e-06
Iter: 723 loss: 1.35184246e-06
Iter: 724 loss: 1.35167636e-06
Iter: 725 loss: 1.35152868e-06
Iter: 726 loss: 1.35150276e-06
Iter: 727 loss: 1.35129153e-06
Iter: 728 loss: 1.35129176e-06
Iter: 729 loss: 1.35116954e-06
Iter: 730 loss: 1.35156927e-06
Iter: 731 loss: 1.35113851e-06
Iter: 732 loss: 1.35100288e-06
Iter: 733 loss: 1.35110542e-06
Iter: 734 loss: 1.35093387e-06
Iter: 735 loss: 1.35079915e-06
Iter: 736 loss: 1.35079335e-06
Iter: 737 loss: 1.35070604e-06
Iter: 738 loss: 1.35063124e-06
Iter: 739 loss: 1.35062555e-06
Iter: 740 loss: 1.35054165e-06
Iter: 741 loss: 1.3503369e-06
Iter: 742 loss: 1.35325149e-06
Iter: 743 loss: 1.3503053e-06
Iter: 744 loss: 1.35015534e-06
Iter: 745 loss: 1.35202345e-06
Iter: 746 loss: 1.35016376e-06
Iter: 747 loss: 1.34999902e-06
Iter: 748 loss: 1.34997163e-06
Iter: 749 loss: 1.34985862e-06
Iter: 750 loss: 1.349684e-06
Iter: 751 loss: 1.34999777e-06
Iter: 752 loss: 1.34962534e-06
Iter: 753 loss: 1.34945458e-06
Iter: 754 loss: 1.35093649e-06
Iter: 755 loss: 1.34944617e-06
Iter: 756 loss: 1.34934749e-06
Iter: 757 loss: 1.34925654e-06
Iter: 758 loss: 1.34923243e-06
Iter: 759 loss: 1.34910078e-06
Iter: 760 loss: 1.35093114e-06
Iter: 761 loss: 1.34907793e-06
Iter: 762 loss: 1.34897869e-06
Iter: 763 loss: 1.34932441e-06
Iter: 764 loss: 1.34893617e-06
Iter: 765 loss: 1.34884476e-06
Iter: 766 loss: 1.34890763e-06
Iter: 767 loss: 1.3487479e-06
Iter: 768 loss: 1.34864217e-06
Iter: 769 loss: 1.34860272e-06
Iter: 770 loss: 1.34853735e-06
Iter: 771 loss: 1.34847392e-06
Iter: 772 loss: 1.34846061e-06
Iter: 773 loss: 1.34836841e-06
Iter: 774 loss: 1.34820709e-06
Iter: 775 loss: 1.34820186e-06
Iter: 776 loss: 1.34809511e-06
Iter: 777 loss: 1.34884522e-06
Iter: 778 loss: 1.34807908e-06
Iter: 779 loss: 1.34794732e-06
Iter: 780 loss: 1.34845322e-06
Iter: 781 loss: 1.34791412e-06
Iter: 782 loss: 1.34782192e-06
Iter: 783 loss: 1.3478159e-06
Iter: 784 loss: 1.34776087e-06
Iter: 785 loss: 1.34764923e-06
Iter: 786 loss: 1.34912443e-06
Iter: 787 loss: 1.34763388e-06
Iter: 788 loss: 1.34757101e-06
Iter: 789 loss: 1.34748188e-06
Iter: 790 loss: 1.34746801e-06
Iter: 791 loss: 1.34737456e-06
Iter: 792 loss: 1.34791048e-06
Iter: 793 loss: 1.34735831e-06
Iter: 794 loss: 1.34727122e-06
Iter: 795 loss: 1.34785023e-06
Iter: 796 loss: 1.34724928e-06
Iter: 797 loss: 1.34716345e-06
Iter: 798 loss: 1.3472411e-06
Iter: 799 loss: 1.34712855e-06
Iter: 800 loss: 1.34702555e-06
Iter: 801 loss: 1.3469828e-06
Iter: 802 loss: 1.34696131e-06
Iter: 803 loss: 1.3468574e-06
Iter: 804 loss: 1.34686343e-06
Iter: 805 loss: 1.34677248e-06
Iter: 806 loss: 1.34666777e-06
Iter: 807 loss: 1.34666379e-06
Iter: 808 loss: 1.34653828e-06
Iter: 809 loss: 1.34684137e-06
Iter: 810 loss: 1.3464836e-06
Iter: 811 loss: 1.34631659e-06
Iter: 812 loss: 1.34759614e-06
Iter: 813 loss: 1.34631341e-06
Iter: 814 loss: 1.34622837e-06
Iter: 815 loss: 1.34615743e-06
Iter: 816 loss: 1.34614993e-06
Iter: 817 loss: 1.34604136e-06
Iter: 818 loss: 1.34606114e-06
Iter: 819 loss: 1.3459844e-06
Iter: 820 loss: 1.34598565e-06
Iter: 821 loss: 1.34593665e-06
Iter: 822 loss: 1.34585207e-06
Iter: 823 loss: 1.34602055e-06
Iter: 824 loss: 1.34581126e-06
Iter: 825 loss: 1.34571621e-06
Iter: 826 loss: 1.34656386e-06
Iter: 827 loss: 1.34572667e-06
Iter: 828 loss: 1.34566017e-06
Iter: 829 loss: 1.34566972e-06
Iter: 830 loss: 1.34560435e-06
Iter: 831 loss: 1.34549623e-06
Iter: 832 loss: 1.34542893e-06
Iter: 833 loss: 1.34541438e-06
Iter: 834 loss: 1.34531717e-06
Iter: 835 loss: 1.34531831e-06
Iter: 836 loss: 1.3452393e-06
Iter: 837 loss: 1.345226e-06
Iter: 838 loss: 1.34517074e-06
Iter: 839 loss: 1.34508355e-06
Iter: 840 loss: 1.34519973e-06
Iter: 841 loss: 1.3450433e-06
Iter: 842 loss: 1.3449453e-06
Iter: 843 loss: 1.3459852e-06
Iter: 844 loss: 1.34493325e-06
Iter: 845 loss: 1.34490642e-06
Iter: 846 loss: 1.34481911e-06
Iter: 847 loss: 1.34482309e-06
Iter: 848 loss: 1.34476295e-06
Iter: 849 loss: 1.34476204e-06
Iter: 850 loss: 1.34470304e-06
Iter: 851 loss: 1.344672e-06
Iter: 852 loss: 1.34464631e-06
Iter: 853 loss: 1.34457264e-06
Iter: 854 loss: 1.34462721e-06
Iter: 855 loss: 1.34450659e-06
Iter: 856 loss: 1.34444622e-06
Iter: 857 loss: 1.34443917e-06
Iter: 858 loss: 1.34436755e-06
Iter: 859 loss: 1.34437914e-06
Iter: 860 loss: 1.34433992e-06
Iter: 861 loss: 1.34425022e-06
Iter: 862 loss: 1.34417405e-06
Iter: 863 loss: 1.3441786e-06
Iter: 864 loss: 1.34410573e-06
Iter: 865 loss: 1.34410197e-06
Iter: 866 loss: 1.34404297e-06
Iter: 867 loss: 1.34408094e-06
Iter: 868 loss: 1.34399977e-06
Iter: 869 loss: 1.34392008e-06
Iter: 870 loss: 1.34391075e-06
Iter: 871 loss: 1.34386664e-06
Iter: 872 loss: 1.34379093e-06
Iter: 873 loss: 1.34377274e-06
Iter: 874 loss: 1.34374511e-06
Iter: 875 loss: 1.34367838e-06
Iter: 876 loss: 1.34528523e-06
Iter: 877 loss: 1.34367599e-06
Iter: 878 loss: 1.34361517e-06
Iter: 879 loss: 1.34434526e-06
Iter: 880 loss: 1.34358766e-06
Iter: 881 loss: 1.34352354e-06
Iter: 882 loss: 1.34352763e-06
Iter: 883 loss: 1.34348704e-06
Iter: 884 loss: 1.34339518e-06
Iter: 885 loss: 1.34332663e-06
Iter: 886 loss: 1.34329207e-06
Iter: 887 loss: 1.34319873e-06
Iter: 888 loss: 1.34319953e-06
Iter: 889 loss: 1.34309562e-06
Iter: 890 loss: 1.3430938e-06
Iter: 891 loss: 1.34303093e-06
Iter: 892 loss: 1.34288575e-06
Iter: 893 loss: 1.34286006e-06
Iter: 894 loss: 1.34277639e-06
Iter: 895 loss: 1.34267134e-06
Iter: 896 loss: 1.34268021e-06
Iter: 897 loss: 1.34257675e-06
Iter: 898 loss: 1.34277343e-06
Iter: 899 loss: 1.34253958e-06
Iter: 900 loss: 1.34244897e-06
Iter: 901 loss: 1.34243351e-06
Iter: 902 loss: 1.34239031e-06
Iter: 903 loss: 1.34229617e-06
Iter: 904 loss: 1.34229822e-06
Iter: 905 loss: 1.3422748e-06
Iter: 906 loss: 1.342161e-06
Iter: 907 loss: 1.34390473e-06
Iter: 908 loss: 1.34214565e-06
Iter: 909 loss: 1.3420738e-06
Iter: 910 loss: 1.34205698e-06
Iter: 911 loss: 1.34198842e-06
Iter: 912 loss: 1.34197739e-06
Iter: 913 loss: 1.34192771e-06
Iter: 914 loss: 1.34182278e-06
Iter: 915 loss: 1.34186382e-06
Iter: 916 loss: 1.34174729e-06
Iter: 917 loss: 1.34166191e-06
Iter: 918 loss: 1.34165373e-06
Iter: 919 loss: 1.34160064e-06
Iter: 920 loss: 1.34160257e-06
Iter: 921 loss: 1.3415422e-06
Iter: 922 loss: 1.34146524e-06
Iter: 923 loss: 1.34142169e-06
Iter: 924 loss: 1.34138418e-06
Iter: 925 loss: 1.3412922e-06
Iter: 926 loss: 1.34175878e-06
Iter: 927 loss: 1.34127356e-06
Iter: 928 loss: 1.3411593e-06
Iter: 929 loss: 1.34168363e-06
Iter: 930 loss: 1.34115714e-06
Iter: 931 loss: 1.34108143e-06
Iter: 932 loss: 1.3410156e-06
Iter: 933 loss: 1.34099764e-06
Iter: 934 loss: 1.34091158e-06
Iter: 935 loss: 1.34091283e-06
Iter: 936 loss: 1.34082916e-06
Iter: 937 loss: 1.34073503e-06
Iter: 938 loss: 1.34073707e-06
Iter: 939 loss: 1.34065181e-06
Iter: 940 loss: 1.34064226e-06
Iter: 941 loss: 1.34057609e-06
Iter: 942 loss: 1.34060861e-06
Iter: 943 loss: 1.34053244e-06
Iter: 944 loss: 1.34043944e-06
Iter: 945 loss: 1.34043205e-06
Iter: 946 loss: 1.3403577e-06
Iter: 947 loss: 1.34027687e-06
Iter: 948 loss: 1.34027346e-06
Iter: 949 loss: 1.34020559e-06
Iter: 950 loss: 1.34025856e-06
Iter: 951 loss: 1.34015193e-06
Iter: 952 loss: 1.34006564e-06
Iter: 953 loss: 1.34004938e-06
Iter: 954 loss: 1.33996775e-06
Iter: 955 loss: 1.33987237e-06
Iter: 956 loss: 1.34029631e-06
Iter: 957 loss: 1.33984258e-06
Iter: 958 loss: 1.33974083e-06
Iter: 959 loss: 1.34078778e-06
Iter: 960 loss: 1.33973595e-06
Iter: 961 loss: 1.3396666e-06
Iter: 962 loss: 1.33958656e-06
Iter: 963 loss: 1.33957246e-06
Iter: 964 loss: 1.33949936e-06
Iter: 965 loss: 1.3394872e-06
Iter: 966 loss: 1.33942854e-06
Iter: 967 loss: 1.33933872e-06
Iter: 968 loss: 1.33934611e-06
Iter: 969 loss: 1.33924686e-06
Iter: 970 loss: 1.34004495e-06
Iter: 971 loss: 1.33923731e-06
Iter: 972 loss: 1.33913863e-06
Iter: 973 loss: 1.3391998e-06
Iter: 974 loss: 1.33909543e-06
Iter: 975 loss: 1.33900562e-06
Iter: 976 loss: 1.33904246e-06
Iter: 977 loss: 1.33891729e-06
Iter: 978 loss: 1.33883043e-06
Iter: 979 loss: 1.34002676e-06
Iter: 980 loss: 1.33881429e-06
Iter: 981 loss: 1.33875074e-06
Iter: 982 loss: 1.33889614e-06
Iter: 983 loss: 1.33870753e-06
Iter: 984 loss: 1.33862454e-06
Iter: 985 loss: 1.33862397e-06
Iter: 986 loss: 1.33855895e-06
Iter: 987 loss: 1.3384697e-06
Iter: 988 loss: 1.33864228e-06
Iter: 989 loss: 1.3384124e-06
Iter: 990 loss: 1.33829144e-06
Iter: 991 loss: 1.3397979e-06
Iter: 992 loss: 1.33829394e-06
Iter: 993 loss: 1.33823164e-06
Iter: 994 loss: 1.33811659e-06
Iter: 995 loss: 1.33811568e-06
Iter: 996 loss: 1.33801245e-06
Iter: 997 loss: 1.33802e-06
Iter: 998 loss: 1.3379497e-06
Iter: 999 loss: 1.33787717e-06
Iter: 1000 loss: 1.33784806e-06
Iter: 1001 loss: 1.33777939e-06
Iter: 1002 loss: 1.33840388e-06
Iter: 1003 loss: 1.33777417e-06
Iter: 1004 loss: 1.33767298e-06
Iter: 1005 loss: 1.33793321e-06
Iter: 1006 loss: 1.33763854e-06
Iter: 1007 loss: 1.33757385e-06
Iter: 1008 loss: 1.33751939e-06
Iter: 1009 loss: 1.33749722e-06
Iter: 1010 loss: 1.33740878e-06
Iter: 1011 loss: 1.33831747e-06
Iter: 1012 loss: 1.33740377e-06
Iter: 1013 loss: 1.33731407e-06
Iter: 1014 loss: 1.33757192e-06
Iter: 1015 loss: 1.33728554e-06
Iter: 1016 loss: 1.33721653e-06
Iter: 1017 loss: 1.337198e-06
Iter: 1018 loss: 1.33714821e-06
Iter: 1019 loss: 1.33705089e-06
Iter: 1020 loss: 1.33718981e-06
Iter: 1021 loss: 1.33698882e-06
Iter: 1022 loss: 1.33691776e-06
Iter: 1023 loss: 1.33692811e-06
Iter: 1024 loss: 1.33686149e-06
Iter: 1025 loss: 1.33674359e-06
Iter: 1026 loss: 1.33855031e-06
Iter: 1027 loss: 1.33675155e-06
Iter: 1028 loss: 1.33666344e-06
Iter: 1029 loss: 1.33664867e-06
Iter: 1030 loss: 1.33655976e-06
Iter: 1031 loss: 1.33647404e-06
Iter: 1032 loss: 1.33644937e-06
Iter: 1033 loss: 1.33635126e-06
Iter: 1034 loss: 1.33662274e-06
Iter: 1035 loss: 1.33630806e-06
Iter: 1036 loss: 1.33619642e-06
Iter: 1037 loss: 1.33707204e-06
Iter: 1038 loss: 1.33619449e-06
Iter: 1039 loss: 1.33613514e-06
Iter: 1040 loss: 1.33608387e-06
Iter: 1041 loss: 1.33604237e-06
Iter: 1042 loss: 1.33595893e-06
Iter: 1043 loss: 1.33648e-06
Iter: 1044 loss: 1.33594665e-06
Iter: 1045 loss: 1.33585047e-06
Iter: 1046 loss: 1.33655396e-06
Iter: 1047 loss: 1.33584035e-06
Iter: 1048 loss: 1.3357693e-06
Iter: 1049 loss: 1.33571166e-06
Iter: 1050 loss: 1.33571166e-06
Iter: 1051 loss: 1.33558638e-06
Iter: 1052 loss: 1.33563435e-06
Iter: 1053 loss: 1.33552408e-06
Iter: 1054 loss: 1.33546519e-06
Iter: 1055 loss: 1.33545836e-06
Iter: 1056 loss: 1.33538424e-06
Iter: 1057 loss: 1.33523827e-06
Iter: 1058 loss: 1.33715673e-06
Iter: 1059 loss: 1.33520962e-06
Iter: 1060 loss: 1.33514516e-06
Iter: 1061 loss: 1.33512663e-06
Iter: 1062 loss: 1.33503659e-06
Iter: 1063 loss: 1.3350234e-06
Iter: 1064 loss: 1.33495632e-06
Iter: 1065 loss: 1.33486833e-06
Iter: 1066 loss: 1.3349927e-06
Iter: 1067 loss: 1.33479966e-06
Iter: 1068 loss: 1.33467552e-06
Iter: 1069 loss: 1.33565595e-06
Iter: 1070 loss: 1.33467506e-06
Iter: 1071 loss: 1.33460082e-06
Iter: 1072 loss: 1.33448407e-06
Iter: 1073 loss: 1.33449146e-06
Iter: 1074 loss: 1.33433537e-06
Iter: 1075 loss: 1.33476829e-06
Iter: 1076 loss: 1.33429546e-06
Iter: 1077 loss: 1.33416279e-06
Iter: 1078 loss: 1.33417552e-06
Iter: 1079 loss: 1.3340923e-06
Iter: 1080 loss: 1.33398396e-06
Iter: 1081 loss: 1.33397407e-06
Iter: 1082 loss: 1.33385959e-06
Iter: 1083 loss: 1.3339436e-06
Iter: 1084 loss: 1.33377398e-06
Iter: 1085 loss: 1.33371054e-06
Iter: 1086 loss: 1.33368474e-06
Iter: 1087 loss: 1.33363221e-06
Iter: 1088 loss: 1.33348431e-06
Iter: 1089 loss: 1.33598257e-06
Iter: 1090 loss: 1.33347783e-06
Iter: 1091 loss: 1.33334856e-06
Iter: 1092 loss: 1.33456024e-06
Iter: 1093 loss: 1.33334413e-06
Iter: 1094 loss: 1.33319759e-06
Iter: 1095 loss: 1.33342519e-06
Iter: 1096 loss: 1.33316064e-06
Iter: 1097 loss: 1.33304388e-06
Iter: 1098 loss: 1.33311914e-06
Iter: 1099 loss: 1.33298317e-06
Iter: 1100 loss: 1.33289018e-06
Iter: 1101 loss: 1.33431354e-06
Iter: 1102 loss: 1.33289836e-06
Iter: 1103 loss: 1.33282958e-06
Iter: 1104 loss: 1.33270112e-06
Iter: 1105 loss: 1.33556352e-06
Iter: 1106 loss: 1.33270362e-06
Iter: 1107 loss: 1.33255139e-06
Iter: 1108 loss: 1.33291337e-06
Iter: 1109 loss: 1.33248568e-06
Iter: 1110 loss: 1.33237506e-06
Iter: 1111 loss: 1.33235164e-06
Iter: 1112 loss: 1.33228264e-06
Iter: 1113 loss: 1.33214326e-06
Iter: 1114 loss: 1.33215235e-06
Iter: 1115 loss: 1.33199e-06
Iter: 1116 loss: 1.33232629e-06
Iter: 1117 loss: 1.33194817e-06
Iter: 1118 loss: 1.33185404e-06
Iter: 1119 loss: 1.33184437e-06
Iter: 1120 loss: 1.33174967e-06
Iter: 1121 loss: 1.33161302e-06
Iter: 1122 loss: 1.33160961e-06
Iter: 1123 loss: 1.3314982e-06
Iter: 1124 loss: 1.33230492e-06
Iter: 1125 loss: 1.33146648e-06
Iter: 1126 loss: 1.33136609e-06
Iter: 1127 loss: 1.33195692e-06
Iter: 1128 loss: 1.33133585e-06
Iter: 1129 loss: 1.33124763e-06
Iter: 1130 loss: 1.33113645e-06
Iter: 1131 loss: 1.33114304e-06
Iter: 1132 loss: 1.33101776e-06
Iter: 1133 loss: 1.33102151e-06
Iter: 1134 loss: 1.33090293e-06
Iter: 1135 loss: 1.33076765e-06
Iter: 1136 loss: 1.33076151e-06
Iter: 1137 loss: 1.3305862e-06
Iter: 1138 loss: 1.33085109e-06
Iter: 1139 loss: 1.33047047e-06
Iter: 1140 loss: 1.33037952e-06
Iter: 1141 loss: 1.33035871e-06
Iter: 1142 loss: 1.33026992e-06
Iter: 1143 loss: 1.33018784e-06
Iter: 1144 loss: 1.33015624e-06
Iter: 1145 loss: 1.33002141e-06
Iter: 1146 loss: 1.33025139e-06
Iter: 1147 loss: 1.32996934e-06
Iter: 1148 loss: 1.32983723e-06
Iter: 1149 loss: 1.3307756e-06
Iter: 1150 loss: 1.32981938e-06
Iter: 1151 loss: 1.32967182e-06
Iter: 1152 loss: 1.32970797e-06
Iter: 1153 loss: 1.3295554e-06
Iter: 1154 loss: 1.32943092e-06
Iter: 1155 loss: 1.32960736e-06
Iter: 1156 loss: 1.32936566e-06
Iter: 1157 loss: 1.32924049e-06
Iter: 1158 loss: 1.33069693e-06
Iter: 1159 loss: 1.32922207e-06
Iter: 1160 loss: 1.32916216e-06
Iter: 1161 loss: 1.32904256e-06
Iter: 1162 loss: 1.32902687e-06
Iter: 1163 loss: 1.32891637e-06
Iter: 1164 loss: 1.3289216e-06
Iter: 1165 loss: 1.32883906e-06
Iter: 1166 loss: 1.32869127e-06
Iter: 1167 loss: 1.33275137e-06
Iter: 1168 loss: 1.32870377e-06
Iter: 1169 loss: 1.32849959e-06
Iter: 1170 loss: 1.32884418e-06
Iter: 1171 loss: 1.32843388e-06
Iter: 1172 loss: 1.32835066e-06
Iter: 1173 loss: 1.32831497e-06
Iter: 1174 loss: 1.3282563e-06
Iter: 1175 loss: 1.32814125e-06
Iter: 1176 loss: 1.32812772e-06
Iter: 1177 loss: 1.32798345e-06
Iter: 1178 loss: 1.32813943e-06
Iter: 1179 loss: 1.32791547e-06
Iter: 1180 loss: 1.32778041e-06
Iter: 1181 loss: 1.32867137e-06
Iter: 1182 loss: 1.327767e-06
Iter: 1183 loss: 1.32763535e-06
Iter: 1184 loss: 1.32805735e-06
Iter: 1185 loss: 1.32757032e-06
Iter: 1186 loss: 1.32749324e-06
Iter: 1187 loss: 1.32733771e-06
Iter: 1188 loss: 1.32734851e-06
Iter: 1189 loss: 1.32720106e-06
Iter: 1190 loss: 1.32717855e-06
Iter: 1191 loss: 1.32708828e-06
Iter: 1192 loss: 1.32689195e-06
Iter: 1193 loss: 1.3307548e-06
Iter: 1194 loss: 1.32689468e-06
Iter: 1195 loss: 1.32675427e-06
Iter: 1196 loss: 1.32674359e-06
Iter: 1197 loss: 1.32662694e-06
Iter: 1198 loss: 1.32657306e-06
Iter: 1199 loss: 1.32649438e-06
Iter: 1200 loss: 1.32638013e-06
Iter: 1201 loss: 1.32653736e-06
Iter: 1202 loss: 1.32632613e-06
Iter: 1203 loss: 1.32621e-06
Iter: 1204 loss: 1.32769298e-06
Iter: 1205 loss: 1.32619402e-06
Iter: 1206 loss: 1.32608875e-06
Iter: 1207 loss: 1.32604941e-06
Iter: 1208 loss: 1.32596574e-06
Iter: 1209 loss: 1.32584034e-06
Iter: 1210 loss: 1.32584898e-06
Iter: 1211 loss: 1.32570608e-06
Iter: 1212 loss: 1.32557409e-06
Iter: 1213 loss: 1.32653952e-06
Iter: 1214 loss: 1.32555579e-06
Iter: 1215 loss: 1.32539481e-06
Iter: 1216 loss: 1.32671425e-06
Iter: 1217 loss: 1.32538116e-06
Iter: 1218 loss: 1.32532205e-06
Iter: 1219 loss: 1.32516197e-06
Iter: 1220 loss: 1.3285046e-06
Iter: 1221 loss: 1.32515356e-06
Iter: 1222 loss: 1.32513424e-06
Iter: 1223 loss: 1.32508785e-06
Iter: 1224 loss: 1.3250343e-06
Iter: 1225 loss: 1.32490868e-06
Iter: 1226 loss: 1.32699938e-06
Iter: 1227 loss: 1.32488424e-06
Iter: 1228 loss: 1.3247502e-06
Iter: 1229 loss: 1.32605646e-06
Iter: 1230 loss: 1.32474634e-06
Iter: 1231 loss: 1.32459809e-06
Iter: 1232 loss: 1.32485457e-06
Iter: 1233 loss: 1.32452215e-06
Iter: 1234 loss: 1.32441937e-06
Iter: 1235 loss: 1.32430796e-06
Iter: 1236 loss: 1.3242875e-06
Iter: 1237 loss: 1.32413334e-06
Iter: 1238 loss: 1.32534205e-06
Iter: 1239 loss: 1.32411537e-06
Iter: 1240 loss: 1.32398964e-06
Iter: 1241 loss: 1.32397736e-06
Iter: 1242 loss: 1.32392711e-06
Iter: 1243 loss: 1.32384241e-06
Iter: 1244 loss: 1.32382866e-06
Iter: 1245 loss: 1.32370405e-06
Iter: 1246 loss: 1.32383366e-06
Iter: 1247 loss: 1.32363107e-06
Iter: 1248 loss: 1.3236172e-06
Iter: 1249 loss: 1.32354535e-06
Iter: 1250 loss: 1.32346543e-06
Iter: 1251 loss: 1.32327068e-06
Iter: 1252 loss: 1.32466107e-06
Iter: 1253 loss: 1.32322782e-06
Iter: 1254 loss: 1.32317155e-06
Iter: 1255 loss: 1.3231321e-06
Iter: 1256 loss: 1.32303921e-06
Iter: 1257 loss: 1.32302921e-06
Iter: 1258 loss: 1.32296839e-06
Iter: 1259 loss: 1.32288756e-06
Iter: 1260 loss: 1.32305263e-06
Iter: 1261 loss: 1.32285561e-06
Iter: 1262 loss: 1.32274158e-06
Iter: 1263 loss: 1.32388993e-06
Iter: 1264 loss: 1.32272407e-06
Iter: 1265 loss: 1.3226354e-06
Iter: 1266 loss: 1.32240712e-06
Iter: 1267 loss: 1.32486775e-06
Iter: 1268 loss: 1.32240325e-06
Iter: 1269 loss: 1.32212779e-06
Iter: 1270 loss: 1.32312653e-06
Iter: 1271 loss: 1.32206242e-06
Iter: 1272 loss: 1.32195339e-06
Iter: 1273 loss: 1.32187643e-06
Iter: 1274 loss: 1.32178911e-06
Iter: 1275 loss: 1.3214883e-06
Iter: 1276 loss: 1.32332627e-06
Iter: 1277 loss: 1.32146056e-06
Iter: 1278 loss: 1.32112268e-06
Iter: 1279 loss: 1.32349521e-06
Iter: 1280 loss: 1.32108721e-06
Iter: 1281 loss: 1.32094533e-06
Iter: 1282 loss: 1.3215755e-06
Iter: 1283 loss: 1.32088758e-06
Iter: 1284 loss: 1.32071432e-06
Iter: 1285 loss: 1.32208891e-06
Iter: 1286 loss: 1.3206909e-06
Iter: 1287 loss: 1.32061268e-06
Iter: 1288 loss: 1.3204974e-06
Iter: 1289 loss: 1.32354489e-06
Iter: 1290 loss: 1.32049172e-06
Iter: 1291 loss: 1.32026196e-06
Iter: 1292 loss: 1.32106948e-06
Iter: 1293 loss: 1.3202133e-06
Iter: 1294 loss: 1.32003106e-06
Iter: 1295 loss: 1.32007858e-06
Iter: 1296 loss: 1.31993409e-06
Iter: 1297 loss: 1.31980846e-06
Iter: 1298 loss: 1.31979027e-06
Iter: 1299 loss: 1.31969603e-06
Iter: 1300 loss: 1.31960121e-06
Iter: 1301 loss: 1.3195862e-06
Iter: 1302 loss: 1.31947297e-06
Iter: 1303 loss: 1.31965146e-06
Iter: 1304 loss: 1.31940249e-06
Iter: 1305 loss: 1.31930483e-06
Iter: 1306 loss: 1.32044784e-06
Iter: 1307 loss: 1.31929835e-06
Iter: 1308 loss: 1.31917739e-06
Iter: 1309 loss: 1.31903619e-06
Iter: 1310 loss: 1.31900765e-06
Iter: 1311 loss: 1.31884167e-06
Iter: 1312 loss: 1.31886111e-06
Iter: 1313 loss: 1.31872753e-06
Iter: 1314 loss: 1.31848446e-06
Iter: 1315 loss: 1.31978277e-06
Iter: 1316 loss: 1.31846434e-06
Iter: 1317 loss: 1.3183278e-06
Iter: 1318 loss: 1.31832007e-06
Iter: 1319 loss: 1.31824061e-06
Iter: 1320 loss: 1.318062e-06
Iter: 1321 loss: 1.31805223e-06
Iter: 1322 loss: 1.31799402e-06
Iter: 1323 loss: 1.31796912e-06
Iter: 1324 loss: 1.31790387e-06
Iter: 1325 loss: 1.31779575e-06
Iter: 1326 loss: 1.3205663e-06
Iter: 1327 loss: 1.31779007e-06
Iter: 1328 loss: 1.31772481e-06
Iter: 1329 loss: 1.31772117e-06
Iter: 1330 loss: 1.31766581e-06
Iter: 1331 loss: 1.31757486e-06
Iter: 1332 loss: 1.31755837e-06
Iter: 1333 loss: 1.31743616e-06
Iter: 1334 loss: 1.31748516e-06
Iter: 1335 loss: 1.31733168e-06
Iter: 1336 loss: 1.31718593e-06
Iter: 1337 loss: 1.31746594e-06
Iter: 1338 loss: 1.317135e-06
Iter: 1339 loss: 1.31717343e-06
Iter: 1340 loss: 1.31708612e-06
Iter: 1341 loss: 1.31703905e-06
Iter: 1342 loss: 1.31692082e-06
Iter: 1343 loss: 1.31754086e-06
Iter: 1344 loss: 1.31690285e-06
Iter: 1345 loss: 1.31676825e-06
Iter: 1346 loss: 1.31691399e-06
Iter: 1347 loss: 1.31669151e-06
Iter: 1348 loss: 1.31650961e-06
Iter: 1349 loss: 1.31697993e-06
Iter: 1350 loss: 1.31645243e-06
Iter: 1351 loss: 1.3163849e-06
Iter: 1352 loss: 1.31635375e-06
Iter: 1353 loss: 1.31628826e-06
Iter: 1354 loss: 1.31640297e-06
Iter: 1355 loss: 1.31623108e-06
Iter: 1356 loss: 1.31615377e-06
Iter: 1357 loss: 1.31614752e-06
Iter: 1358 loss: 1.31608385e-06
Iter: 1359 loss: 1.31597335e-06
Iter: 1360 loss: 1.31738909e-06
Iter: 1361 loss: 1.31597653e-06
Iter: 1362 loss: 1.31592378e-06
Iter: 1363 loss: 1.3158035e-06
Iter: 1364 loss: 1.3157952e-06
Iter: 1365 loss: 1.31571664e-06
Iter: 1366 loss: 1.31571255e-06
Iter: 1367 loss: 1.31561171e-06
Iter: 1368 loss: 1.31552156e-06
Iter: 1369 loss: 1.31552156e-06
Iter: 1370 loss: 1.31542129e-06
Iter: 1371 loss: 1.31590173e-06
Iter: 1372 loss: 1.3154031e-06
Iter: 1373 loss: 1.31534955e-06
Iter: 1374 loss: 1.31534091e-06
Iter: 1375 loss: 1.31530476e-06
Iter: 1376 loss: 1.31520073e-06
Iter: 1377 loss: 1.31608238e-06
Iter: 1378 loss: 1.31516879e-06
Iter: 1379 loss: 1.31503134e-06
Iter: 1380 loss: 1.3155402e-06
Iter: 1381 loss: 1.31500235e-06
Iter: 1382 loss: 1.31488127e-06
Iter: 1383 loss: 1.31491265e-06
Iter: 1384 loss: 1.31481261e-06
Iter: 1385 loss: 1.3147104e-06
Iter: 1386 loss: 1.31472052e-06
Iter: 1387 loss: 1.3145991e-06
Iter: 1388 loss: 1.31505749e-06
Iter: 1389 loss: 1.31458887e-06
Iter: 1390 loss: 1.3145318e-06
Iter: 1391 loss: 1.31442198e-06
Iter: 1392 loss: 1.31442766e-06
Iter: 1393 loss: 1.31435399e-06
Iter: 1394 loss: 1.31435218e-06
Iter: 1395 loss: 1.31429283e-06
Iter: 1396 loss: 1.31423371e-06
Iter: 1397 loss: 1.31423826e-06
Iter: 1398 loss: 1.31415777e-06
Iter: 1399 loss: 1.31474837e-06
Iter: 1400 loss: 1.31416073e-06
Iter: 1401 loss: 1.31412071e-06
Iter: 1402 loss: 1.3141613e-06
Iter: 1403 loss: 1.31406773e-06
Iter: 1404 loss: 1.31402635e-06
Iter: 1405 loss: 1.31393358e-06
Iter: 1406 loss: 1.31632282e-06
Iter: 1407 loss: 1.31392392e-06
Iter: 1408 loss: 1.31395893e-06
Iter: 1409 loss: 1.31388515e-06
Iter: 1410 loss: 1.31384809e-06
Iter: 1411 loss: 1.31379488e-06
Iter: 1412 loss: 1.31499712e-06
Iter: 1413 loss: 1.31379238e-06
Iter: 1414 loss: 1.31373156e-06
Iter: 1415 loss: 1.31371235e-06
Iter: 1416 loss: 1.31369347e-06
Iter: 1417 loss: 1.31362287e-06
Iter: 1418 loss: 1.31408581e-06
Iter: 1419 loss: 1.31360196e-06
Iter: 1420 loss: 1.31357376e-06
Iter: 1421 loss: 1.31355648e-06
Iter: 1422 loss: 1.31352988e-06
Iter: 1423 loss: 1.31351521e-06
Iter: 1424 loss: 1.31350259e-06
Iter: 1425 loss: 1.31344564e-06
Iter: 1426 loss: 1.31370098e-06
Iter: 1427 loss: 1.31344177e-06
Iter: 1428 loss: 1.3134038e-06
Iter: 1429 loss: 1.31348e-06
Iter: 1430 loss: 1.31339982e-06
Iter: 1431 loss: 1.31335275e-06
Iter: 1432 loss: 1.31329136e-06
Iter: 1433 loss: 1.31327283e-06
Iter: 1434 loss: 1.31321281e-06
Iter: 1435 loss: 1.31372349e-06
Iter: 1436 loss: 1.31321463e-06
Iter: 1437 loss: 1.31315062e-06
Iter: 1438 loss: 1.31363208e-06
Iter: 1439 loss: 1.31314948e-06
Iter: 1440 loss: 1.31309685e-06
Iter: 1441 loss: 1.3130707e-06
Iter: 1442 loss: 1.31303705e-06
Iter: 1443 loss: 1.31298646e-06
Iter: 1444 loss: 1.31312208e-06
Iter: 1445 loss: 1.31299e-06
Iter: 1446 loss: 1.31295906e-06
Iter: 1447 loss: 1.3129503e-06
Iter: 1448 loss: 1.31294485e-06
Iter: 1449 loss: 1.31288061e-06
Iter: 1450 loss: 1.31337561e-06
Iter: 1451 loss: 1.31285958e-06
Iter: 1452 loss: 1.31280626e-06
Iter: 1453 loss: 1.31333866e-06
Iter: 1454 loss: 1.3128091e-06
Iter: 1455 loss: 1.31275328e-06
Iter: 1456 loss: 1.31315505e-06
Iter: 1457 loss: 1.31274692e-06
Iter: 1458 loss: 1.31272805e-06
Iter: 1459 loss: 1.31270508e-06
Iter: 1460 loss: 1.31270963e-06
Iter: 1461 loss: 1.31266791e-06
Iter: 1462 loss: 1.31285992e-06
Iter: 1463 loss: 1.31266961e-06
Iter: 1464 loss: 1.31264119e-06
Iter: 1465 loss: 1.3125516e-06
Iter: 1466 loss: 1.31384309e-06
Iter: 1467 loss: 1.31256e-06
Iter: 1468 loss: 1.31250749e-06
Iter: 1469 loss: 1.312721e-06
Iter: 1470 loss: 1.31248726e-06
Iter: 1471 loss: 1.31244406e-06
Iter: 1472 loss: 1.31315051e-06
Iter: 1473 loss: 1.31243746e-06
Iter: 1474 loss: 1.31238494e-06
Iter: 1475 loss: 1.31249863e-06
Iter: 1476 loss: 1.31237084e-06
Iter: 1477 loss: 1.31235095e-06
Iter: 1478 loss: 1.31232321e-06
Iter: 1479 loss: 1.31228649e-06
Iter: 1480 loss: 1.31226193e-06
Iter: 1481 loss: 1.31226193e-06
Iter: 1482 loss: 1.31221486e-06
Iter: 1483 loss: 1.31217985e-06
Iter: 1484 loss: 1.31218724e-06
Iter: 1485 loss: 1.31214631e-06
Iter: 1486 loss: 1.31211561e-06
Iter: 1487 loss: 1.31209629e-06
Iter: 1488 loss: 1.31205661e-06
Iter: 1489 loss: 1.31246293e-06
Iter: 1490 loss: 1.3120432e-06
Iter: 1491 loss: 1.31200568e-06
Iter: 1492 loss: 1.31193667e-06
Iter: 1493 loss: 1.31193724e-06
Iter: 1494 loss: 1.31188472e-06
Iter: 1495 loss: 1.31199567e-06
Iter: 1496 loss: 1.31186584e-06
Iter: 1497 loss: 1.31185163e-06
Iter: 1498 loss: 1.31182742e-06
Iter: 1499 loss: 1.31181832e-06
Iter: 1500 loss: 1.31176705e-06
Iter: 1501 loss: 1.31240267e-06
Iter: 1502 loss: 1.31175034e-06
Iter: 1503 loss: 1.31170032e-06
Iter: 1504 loss: 1.31198203e-06
Iter: 1505 loss: 1.31169338e-06
Iter: 1506 loss: 1.31167883e-06
Iter: 1507 loss: 1.31166621e-06
Iter: 1508 loss: 1.31163699e-06
Iter: 1509 loss: 1.31157663e-06
Iter: 1510 loss: 1.31244167e-06
Iter: 1511 loss: 1.31157617e-06
Iter: 1512 loss: 1.31151e-06
Iter: 1513 loss: 1.31228217e-06
Iter: 1514 loss: 1.31150591e-06
Iter: 1515 loss: 1.3114618e-06
Iter: 1516 loss: 1.31172146e-06
Iter: 1517 loss: 1.31147283e-06
Iter: 1518 loss: 1.31144634e-06
Iter: 1519 loss: 1.31142224e-06
Iter: 1520 loss: 1.31141223e-06
Iter: 1521 loss: 1.31138279e-06
Iter: 1522 loss: 1.31151864e-06
Iter: 1523 loss: 1.3113779e-06
Iter: 1524 loss: 1.31132697e-06
Iter: 1525 loss: 1.31154468e-06
Iter: 1526 loss: 1.31131196e-06
Iter: 1527 loss: 1.31130605e-06
Iter: 1528 loss: 1.3112342e-06
Iter: 1529 loss: 1.31174033e-06
Iter: 1530 loss: 1.31125148e-06
Iter: 1531 loss: 1.31121419e-06
Iter: 1532 loss: 1.3112051e-06
Iter: 1533 loss: 1.31116053e-06
Iter: 1534 loss: 1.31116406e-06
Iter: 1535 loss: 1.31114325e-06
Iter: 1536 loss: 1.31109732e-06
Iter: 1537 loss: 1.31108072e-06
Iter: 1538 loss: 1.31106663e-06
Iter: 1539 loss: 1.31103945e-06
Iter: 1540 loss: 1.31104071e-06
Iter: 1541 loss: 1.31098341e-06
Iter: 1542 loss: 1.3109767e-06
Iter: 1543 loss: 1.31094328e-06
Iter: 1544 loss: 1.31091679e-06
Iter: 1545 loss: 1.31092111e-06
Iter: 1546 loss: 1.31088314e-06
Iter: 1547 loss: 1.31083038e-06
Iter: 1548 loss: 1.31082561e-06
Iter: 1549 loss: 1.31080651e-06
Iter: 1550 loss: 1.31078104e-06
Iter: 1551 loss: 1.31077684e-06
Iter: 1552 loss: 1.31072295e-06
Iter: 1553 loss: 1.31079457e-06
Iter: 1554 loss: 1.3107117e-06
Iter: 1555 loss: 1.31067782e-06
Iter: 1556 loss: 1.31069532e-06
Iter: 1557 loss: 1.31064553e-06
Iter: 1558 loss: 1.3106179e-06
Iter: 1559 loss: 1.31155844e-06
Iter: 1560 loss: 1.31061313e-06
Iter: 1561 loss: 1.31056447e-06
Iter: 1562 loss: 1.3106461e-06
Iter: 1563 loss: 1.3105639e-06
Iter: 1564 loss: 1.310524e-06
Iter: 1565 loss: 1.31051081e-06
Iter: 1566 loss: 1.3104974e-06
Iter: 1567 loss: 1.31044476e-06
Iter: 1568 loss: 1.31068441e-06
Iter: 1569 loss: 1.31041054e-06
Iter: 1570 loss: 1.31033983e-06
Iter: 1571 loss: 1.31049342e-06
Iter: 1572 loss: 1.31032277e-06
Iter: 1573 loss: 1.31030208e-06
Iter: 1574 loss: 1.31029856e-06
Iter: 1575 loss: 1.31024785e-06
Iter: 1576 loss: 1.31031413e-06
Iter: 1577 loss: 1.31022625e-06
Iter: 1578 loss: 1.31019738e-06
Iter: 1579 loss: 1.31016873e-06
Iter: 1580 loss: 1.31015872e-06
Iter: 1581 loss: 1.31012848e-06
Iter: 1582 loss: 1.31012484e-06
Iter: 1583 loss: 1.31010779e-06
Iter: 1584 loss: 1.31006095e-06
Iter: 1585 loss: 1.31090326e-06
Iter: 1586 loss: 1.31007494e-06
Iter: 1587 loss: 1.3099866e-06
Iter: 1588 loss: 1.31011848e-06
Iter: 1589 loss: 1.30998637e-06
Iter: 1590 loss: 1.3099359e-06
Iter: 1591 loss: 1.30993e-06
Iter: 1592 loss: 1.3098977e-06
Iter: 1593 loss: 1.30985484e-06
Iter: 1594 loss: 1.30985245e-06
Iter: 1595 loss: 1.30979902e-06
Iter: 1596 loss: 1.31007937e-06
Iter: 1597 loss: 1.30978697e-06
Iter: 1598 loss: 1.30973376e-06
Iter: 1599 loss: 1.30998649e-06
Iter: 1600 loss: 1.30975332e-06
Iter: 1601 loss: 1.30973604e-06
Iter: 1602 loss: 1.30966987e-06
Iter: 1603 loss: 1.3104252e-06
Iter: 1604 loss: 1.30966794e-06
Iter: 1605 loss: 1.30961666e-06
Iter: 1606 loss: 1.30974422e-06
Iter: 1607 loss: 1.30960598e-06
Iter: 1608 loss: 1.30958665e-06
Iter: 1609 loss: 1.3095555e-06
Iter: 1610 loss: 1.30953981e-06
Iter: 1611 loss: 1.30948138e-06
Iter: 1612 loss: 1.31039087e-06
Iter: 1613 loss: 1.3094641e-06
Iter: 1614 loss: 1.30942453e-06
Iter: 1615 loss: 1.30982141e-06
Iter: 1616 loss: 1.30940191e-06
Iter: 1617 loss: 1.30936212e-06
Iter: 1618 loss: 1.30989736e-06
Iter: 1619 loss: 1.30935564e-06
Iter: 1620 loss: 1.30933711e-06
Iter: 1621 loss: 1.30928902e-06
Iter: 1622 loss: 1.30928345e-06
Iter: 1623 loss: 1.30924161e-06
Iter: 1624 loss: 1.30955539e-06
Iter: 1625 loss: 1.30923252e-06
Iter: 1626 loss: 1.30918966e-06
Iter: 1627 loss: 1.30945011e-06
Iter: 1628 loss: 1.30918147e-06
Iter: 1629 loss: 1.30914668e-06
Iter: 1630 loss: 1.30910371e-06
Iter: 1631 loss: 1.30910144e-06
Iter: 1632 loss: 1.30906756e-06
Iter: 1633 loss: 1.30937383e-06
Iter: 1634 loss: 1.30904766e-06
Iter: 1635 loss: 1.3090123e-06
Iter: 1636 loss: 1.30932131e-06
Iter: 1637 loss: 1.30902458e-06
Iter: 1638 loss: 1.30899173e-06
Iter: 1639 loss: 1.3089392e-06
Iter: 1640 loss: 1.30933677e-06
Iter: 1641 loss: 1.30894341e-06
Iter: 1642 loss: 1.30888986e-06
Iter: 1643 loss: 1.30913782e-06
Iter: 1644 loss: 1.3088777e-06
Iter: 1645 loss: 1.30884769e-06
Iter: 1646 loss: 1.30884132e-06
Iter: 1647 loss: 1.3088013e-06
Iter: 1648 loss: 1.30874776e-06
Iter: 1649 loss: 1.30873991e-06
Iter: 1650 loss: 1.30868705e-06
Iter: 1651 loss: 1.30877072e-06
Iter: 1652 loss: 1.30865749e-06
Iter: 1653 loss: 1.30861156e-06
Iter: 1654 loss: 1.30860167e-06
Iter: 1655 loss: 1.30859098e-06
Iter: 1656 loss: 1.30854551e-06
Iter: 1657 loss: 1.30897922e-06
Iter: 1658 loss: 1.30854528e-06
Iter: 1659 loss: 1.30850935e-06
Iter: 1660 loss: 1.30850185e-06
Iter: 1661 loss: 1.3084757e-06
Iter: 1662 loss: 1.30846456e-06
Iter: 1663 loss: 1.30846581e-06
Iter: 1664 loss: 1.30841272e-06
Iter: 1665 loss: 1.3084084e-06
Iter: 1666 loss: 1.30837043e-06
Iter: 1667 loss: 1.30831609e-06
Iter: 1668 loss: 1.30870762e-06
Iter: 1669 loss: 1.30829301e-06
Iter: 1670 loss: 1.30826766e-06
Iter: 1671 loss: 1.3084707e-06
Iter: 1672 loss: 1.30827107e-06
Iter: 1673 loss: 1.30825879e-06
Iter: 1674 loss: 1.30819899e-06
Iter: 1675 loss: 1.30892408e-06
Iter: 1676 loss: 1.30820081e-06
Iter: 1677 loss: 1.30814533e-06
Iter: 1678 loss: 1.30815306e-06
Iter: 1679 loss: 1.30811827e-06
Iter: 1680 loss: 1.30830324e-06
Iter: 1681 loss: 1.30810804e-06
Iter: 1682 loss: 1.30807598e-06
Iter: 1683 loss: 1.30804369e-06
Iter: 1684 loss: 1.30881256e-06
Iter: 1685 loss: 1.30804483e-06
Iter: 1686 loss: 1.30802016e-06
Iter: 1687 loss: 1.30801664e-06
Iter: 1688 loss: 1.30798594e-06
Iter: 1689 loss: 1.30795934e-06
Iter: 1690 loss: 1.30795786e-06
Iter: 1691 loss: 1.30791432e-06
Iter: 1692 loss: 1.30805097e-06
Iter: 1693 loss: 1.30791636e-06
Iter: 1694 loss: 1.30785463e-06
Iter: 1695 loss: 1.3078934e-06
Iter: 1696 loss: 1.30780484e-06
Iter: 1697 loss: 1.30778244e-06
Iter: 1698 loss: 1.3078577e-06
Iter: 1699 loss: 1.30775334e-06
Iter: 1700 loss: 1.30772287e-06
Iter: 1701 loss: 1.30770877e-06
Iter: 1702 loss: 1.30767899e-06
Iter: 1703 loss: 1.30764749e-06
Iter: 1704 loss: 1.30765216e-06
Iter: 1705 loss: 1.30760668e-06
Iter: 1706 loss: 1.3075022e-06
Iter: 1707 loss: 1.30905619e-06
Iter: 1708 loss: 1.307508e-06
Iter: 1709 loss: 1.30747071e-06
Iter: 1710 loss: 1.30765852e-06
Iter: 1711 loss: 1.30744945e-06
Iter: 1712 loss: 1.30742285e-06
Iter: 1713 loss: 1.30742183e-06
Iter: 1714 loss: 1.30740364e-06
Iter: 1715 loss: 1.30736044e-06
Iter: 1716 loss: 1.30735589e-06
Iter: 1717 loss: 1.30731485e-06
Iter: 1718 loss: 1.30779563e-06
Iter: 1719 loss: 1.30732906e-06
Iter: 1720 loss: 1.30729711e-06
Iter: 1721 loss: 1.30729472e-06
Iter: 1722 loss: 1.3072704e-06
Iter: 1723 loss: 1.30722606e-06
Iter: 1724 loss: 1.30715739e-06
Iter: 1725 loss: 1.30716921e-06
Iter: 1726 loss: 1.30711692e-06
Iter: 1727 loss: 1.30711692e-06
Iter: 1728 loss: 1.30704564e-06
Iter: 1729 loss: 1.30702904e-06
Iter: 1730 loss: 1.30700982e-06
Iter: 1731 loss: 1.30695798e-06
Iter: 1732 loss: 1.30707303e-06
Iter: 1733 loss: 1.30691899e-06
Iter: 1734 loss: 1.30685294e-06
Iter: 1735 loss: 1.30693695e-06
Iter: 1736 loss: 1.30682702e-06
Iter: 1737 loss: 1.30679e-06
Iter: 1738 loss: 1.30680337e-06
Iter: 1739 loss: 1.3067696e-06
Iter: 1740 loss: 1.30675892e-06
Iter: 1741 loss: 1.30674334e-06
Iter: 1742 loss: 1.30669991e-06
Iter: 1743 loss: 1.30675608e-06
Iter: 1744 loss: 1.30666172e-06
Iter: 1745 loss: 1.30662636e-06
Iter: 1746 loss: 1.30693957e-06
Iter: 1747 loss: 1.3066234e-06
Iter: 1748 loss: 1.30658566e-06
Iter: 1749 loss: 1.30657725e-06
Iter: 1750 loss: 1.30657054e-06
Iter: 1751 loss: 1.30650369e-06
Iter: 1752 loss: 1.30678086e-06
Iter: 1753 loss: 1.30649607e-06
Iter: 1754 loss: 1.30645799e-06
Iter: 1755 loss: 1.30641183e-06
Iter: 1756 loss: 1.30639762e-06
Iter: 1757 loss: 1.30634555e-06
Iter: 1758 loss: 1.30637829e-06
Iter: 1759 loss: 1.30629905e-06
Iter: 1760 loss: 1.30624858e-06
Iter: 1761 loss: 1.3064647e-06
Iter: 1762 loss: 1.30622129e-06
Iter: 1763 loss: 1.30613671e-06
Iter: 1764 loss: 1.30730973e-06
Iter: 1765 loss: 1.30612887e-06
Iter: 1766 loss: 1.30609396e-06
Iter: 1767 loss: 1.3060029e-06
Iter: 1768 loss: 1.30686033e-06
Iter: 1769 loss: 1.30598391e-06
Iter: 1770 loss: 1.30589342e-06
Iter: 1771 loss: 1.30588683e-06
Iter: 1772 loss: 1.30582589e-06
Iter: 1773 loss: 1.30603144e-06
Iter: 1774 loss: 1.30580315e-06
Iter: 1775 loss: 1.3057537e-06
Iter: 1776 loss: 1.30583453e-06
Iter: 1777 loss: 1.30571425e-06
Iter: 1778 loss: 1.30564047e-06
Iter: 1779 loss: 1.30597277e-06
Iter: 1780 loss: 1.30563387e-06
Iter: 1781 loss: 1.30559306e-06
Iter: 1782 loss: 1.30562171e-06
Iter: 1783 loss: 1.30554724e-06
Iter: 1784 loss: 1.30548892e-06
Iter: 1785 loss: 1.3058218e-06
Iter: 1786 loss: 1.30549938e-06
Iter: 1787 loss: 1.30544754e-06
Iter: 1788 loss: 1.30539036e-06
Iter: 1789 loss: 1.30537558e-06
Iter: 1790 loss: 1.30532749e-06
Iter: 1791 loss: 1.30530327e-06
Iter: 1792 loss: 1.30526223e-06
Iter: 1793 loss: 1.30522744e-06
Iter: 1794 loss: 1.30553224e-06
Iter: 1795 loss: 1.30520402e-06
Iter: 1796 loss: 1.3051432e-06
Iter: 1797 loss: 1.30531407e-06
Iter: 1798 loss: 1.3051465e-06
Iter: 1799 loss: 1.30503633e-06
Iter: 1800 loss: 1.30533761e-06
Iter: 1801 loss: 1.30500916e-06
Iter: 1802 loss: 1.30495835e-06
Iter: 1803 loss: 1.3048832e-06
Iter: 1804 loss: 1.30487354e-06
Iter: 1805 loss: 1.30481374e-06
Iter: 1806 loss: 1.3061956e-06
Iter: 1807 loss: 1.30481965e-06
Iter: 1808 loss: 1.30473791e-06
Iter: 1809 loss: 1.30492845e-06
Iter: 1810 loss: 1.30474359e-06
Iter: 1811 loss: 1.30467606e-06
Iter: 1812 loss: 1.30479043e-06
Iter: 1813 loss: 1.30465901e-06
Iter: 1814 loss: 1.30461581e-06
Iter: 1815 loss: 1.30486717e-06
Iter: 1816 loss: 1.30461297e-06
Iter: 1817 loss: 1.30457101e-06
Iter: 1818 loss: 1.30453e-06
Iter: 1819 loss: 1.30451895e-06
Iter: 1820 loss: 1.30448245e-06
Iter: 1821 loss: 1.30449541e-06
Iter: 1822 loss: 1.30446415e-06
Iter: 1823 loss: 1.30441526e-06
Iter: 1824 loss: 1.30483102e-06
Iter: 1825 loss: 1.30440935e-06
Iter: 1826 loss: 1.30434273e-06
Iter: 1827 loss: 1.30436661e-06
Iter: 1828 loss: 1.30429862e-06
Iter: 1829 loss: 1.30422643e-06
Iter: 1830 loss: 1.30491333e-06
Iter: 1831 loss: 1.30423746e-06
Iter: 1832 loss: 1.30421472e-06
Iter: 1833 loss: 1.30420108e-06
Iter: 1834 loss: 1.30417015e-06
Iter: 1835 loss: 1.30415515e-06
Iter: 1836 loss: 1.30416561e-06
Iter: 1837 loss: 1.30411399e-06
Iter: 1838 loss: 1.3042511e-06
Iter: 1839 loss: 1.30411752e-06
Iter: 1840 loss: 1.30405613e-06
Iter: 1841 loss: 1.30427054e-06
Iter: 1842 loss: 1.30407204e-06
Iter: 1843 loss: 1.3040526e-06
Iter: 1844 loss: 1.30411092e-06
Iter: 1845 loss: 1.30404032e-06
Iter: 1846 loss: 1.3040185e-06
Iter: 1847 loss: 1.30404828e-06
Iter: 1848 loss: 1.30398962e-06
Iter: 1849 loss: 1.30395506e-06
Iter: 1850 loss: 1.30394346e-06
Iter: 1851 loss: 1.30388048e-06
Iter: 1852 loss: 1.30386275e-06
Iter: 1853 loss: 1.30436842e-06
Iter: 1854 loss: 1.30387434e-06
Iter: 1855 loss: 1.30382023e-06
Iter: 1856 loss: 1.3037984e-06
Iter: 1857 loss: 1.30377816e-06
Iter: 1858 loss: 1.30374792e-06
Iter: 1859 loss: 1.30370336e-06
Iter: 1860 loss: 1.30369835e-06
Iter: 1861 loss: 1.30364958e-06
Iter: 1862 loss: 1.30374838e-06
Iter: 1863 loss: 1.30361241e-06
Iter: 1864 loss: 1.30357535e-06
Iter: 1865 loss: 1.30414867e-06
Iter: 1866 loss: 1.30358308e-06
Iter: 1867 loss: 1.30354783e-06
Iter: 1868 loss: 1.3039114e-06
Iter: 1869 loss: 1.30354238e-06
Iter: 1870 loss: 1.30353828e-06
Iter: 1871 loss: 1.30347871e-06
Iter: 1872 loss: 1.30407204e-06
Iter: 1873 loss: 1.30349258e-06
Iter: 1874 loss: 1.30345063e-06
Iter: 1875 loss: 1.30345597e-06
Iter: 1876 loss: 1.30342301e-06
Iter: 1877 loss: 1.3033781e-06
Iter: 1878 loss: 1.30337389e-06
Iter: 1879 loss: 1.30331205e-06
Iter: 1880 loss: 1.30348349e-06
Iter: 1881 loss: 1.30332398e-06
Iter: 1882 loss: 1.30325759e-06
Iter: 1883 loss: 1.30345143e-06
Iter: 1884 loss: 1.30322292e-06
Iter: 1885 loss: 1.30317369e-06
Iter: 1886 loss: 1.3031771e-06
Iter: 1887 loss: 1.30313845e-06
Iter: 1888 loss: 1.30310571e-06
Iter: 1889 loss: 1.30310571e-06
Iter: 1890 loss: 1.3030708e-06
Iter: 1891 loss: 1.30302431e-06
Iter: 1892 loss: 1.3036356e-06
Iter: 1893 loss: 1.30301487e-06
Iter: 1894 loss: 1.30296235e-06
Iter: 1895 loss: 1.30297883e-06
Iter: 1896 loss: 1.30292392e-06
Iter: 1897 loss: 1.30286458e-06
Iter: 1898 loss: 1.30311219e-06
Iter: 1899 loss: 1.30282831e-06
Iter: 1900 loss: 1.30280364e-06
Iter: 1901 loss: 1.30279523e-06
Iter: 1902 loss: 1.30274861e-06
Iter: 1903 loss: 1.30271519e-06
Iter: 1904 loss: 1.30272792e-06
Iter: 1905 loss: 1.30266369e-06
Iter: 1906 loss: 1.30271428e-06
Iter: 1907 loss: 1.30262447e-06
Iter: 1908 loss: 1.30260423e-06
Iter: 1909 loss: 1.30260264e-06
Iter: 1910 loss: 1.3025624e-06
Iter: 1911 loss: 1.30252874e-06
Iter: 1912 loss: 1.30254023e-06
Iter: 1913 loss: 1.30247258e-06
Iter: 1914 loss: 1.30268677e-06
Iter: 1915 loss: 1.30246849e-06
Iter: 1916 loss: 1.3024403e-06
Iter: 1917 loss: 1.30252647e-06
Iter: 1918 loss: 1.30241301e-06
Iter: 1919 loss: 1.30237129e-06
Iter: 1920 loss: 1.30232866e-06
Iter: 1921 loss: 1.30231615e-06
Iter: 1922 loss: 1.30227977e-06
Iter: 1923 loss: 1.30227056e-06
Iter: 1924 loss: 1.30224794e-06
Iter: 1925 loss: 1.30217677e-06
Iter: 1926 loss: 1.30258445e-06
Iter: 1927 loss: 1.30214391e-06
Iter: 1928 loss: 1.30208969e-06
Iter: 1929 loss: 1.30229864e-06
Iter: 1930 loss: 1.3020632e-06
Iter: 1931 loss: 1.30200192e-06
Iter: 1932 loss: 1.30220508e-06
Iter: 1933 loss: 1.30199896e-06
Iter: 1934 loss: 1.30197554e-06
Iter: 1935 loss: 1.30196099e-06
Iter: 1936 loss: 1.30193825e-06
Iter: 1937 loss: 1.30189392e-06
Iter: 1938 loss: 1.30281614e-06
Iter: 1939 loss: 1.30189972e-06
Iter: 1940 loss: 1.3018431e-06
Iter: 1941 loss: 1.30181661e-06
Iter: 1942 loss: 1.30181172e-06
Iter: 1943 loss: 1.30175636e-06
Iter: 1944 loss: 1.30176807e-06
Iter: 1945 loss: 1.30171713e-06
Iter: 1946 loss: 1.30166984e-06
Iter: 1947 loss: 1.3016529e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi1.2 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi1.2
+ date
Sat Oct 31 20:28:54 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 0 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88204dc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88205bda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8820552840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8820624730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8820624488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88205520d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88205522f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8820436488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8820436620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88202f0158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88202f09d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f882048eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88201b6d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8820436378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f882023a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88202328c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88201bb1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f882013dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8820121b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f882013d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88200e8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88201f81e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88200e8b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88202ccbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88202ccc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88202ccae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88200af378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8820063a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f88200540d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8820054a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87d47cc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8820054e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8820054c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f882040e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87d468d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f882037bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.004210397
test_loss: 0.0041392017
train_loss: 0.002835124
test_loss: 0.0029899857
train_loss: 0.0022949
test_loss: 0.0024823977
train_loss: 0.0017731793
test_loss: 0.0020631435
train_loss: 0.0018739245
test_loss: 0.0019668634
train_loss: 0.0016490053
test_loss: 0.0015024254
train_loss: 0.0014030582
test_loss: 0.0014673625
train_loss: 0.0014220483
test_loss: 0.0013761377
train_loss: 0.0012588301
test_loss: 0.0013485414
train_loss: 0.0013013978
test_loss: 0.0013503178
train_loss: 0.0012921265
test_loss: 0.0012492315
train_loss: 0.0012171507
test_loss: 0.0012674044
train_loss: 0.0011401237
test_loss: 0.0011662081
train_loss: 0.0011026954
test_loss: 0.0011938354
train_loss: 0.0011217068
test_loss: 0.0011498973
train_loss: 0.0011315924
test_loss: 0.0011320985
train_loss: 0.0010815342
test_loss: 0.0011062406
train_loss: 0.0010956116
test_loss: 0.0011030631
train_loss: 0.0010091211
test_loss: 0.0010844758
train_loss: 0.001048714
test_loss: 0.0010792235
train_loss: 0.0010125196
test_loss: 0.0010684577
train_loss: 0.0010174783
test_loss: 0.0010511961
train_loss: 0.0009995069
test_loss: 0.0010409374
train_loss: 0.0009831379
test_loss: 0.0010245584
train_loss: 0.0009651828
test_loss: 0.001020048
train_loss: 0.0009754916
test_loss: 0.001030401
train_loss: 0.0009744328
test_loss: 0.0010206931
train_loss: 0.00095710694
test_loss: 0.0010002337
train_loss: 0.00096155284
test_loss: 0.0009992414
train_loss: 0.00094407523
test_loss: 0.0010057528
train_loss: 0.0009632733
test_loss: 0.0009963713
train_loss: 0.0009040561
test_loss: 0.0009915556
train_loss: 0.0009372742
test_loss: 0.0009962771
train_loss: 0.0009307596
test_loss: 0.0009794658
train_loss: 0.0009503785
test_loss: 0.000980272
train_loss: 0.0009224827
test_loss: 0.0009773258
train_loss: 0.00091216824
test_loss: 0.0009724795
train_loss: 0.00090076297
test_loss: 0.00097532186
train_loss: 0.0009121308
test_loss: 0.00096941483
train_loss: 0.00094146095
test_loss: 0.00097308395
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feef249b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feef247cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feef2508bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feef24c7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feef24c7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feef24c7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feef23f1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feec3f04488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feef23f1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feec3f22158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feec3f229d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feef243aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feec3e7cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feec3eb01e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feec3ec7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feec3e48d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feec3e4cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feec3ec4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c39ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feec3e4cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c373950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c373d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c373488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c2ffb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c2ff7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c2ff950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c2c5ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c2c5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c270488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c2692f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c282730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c243f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c15a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c2691e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c15ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee9c0e6bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.64502922e-06
Iter: 2 loss: 1.63797029e-06
Iter: 3 loss: 1.62531433e-06
Iter: 4 loss: 1.61975618e-06
Iter: 5 loss: 1.61535775e-06
Iter: 6 loss: 1.61362937e-06
Iter: 7 loss: 1.60897321e-06
Iter: 8 loss: 1.60886543e-06
Iter: 9 loss: 1.60678155e-06
Iter: 10 loss: 1.60498314e-06
Iter: 11 loss: 1.60441368e-06
Iter: 12 loss: 1.6023962e-06
Iter: 13 loss: 1.60235527e-06
Iter: 14 loss: 1.60105856e-06
Iter: 15 loss: 1.59840715e-06
Iter: 16 loss: 1.6477851e-06
Iter: 17 loss: 1.59835668e-06
Iter: 18 loss: 1.59607657e-06
Iter: 19 loss: 1.60748573e-06
Iter: 20 loss: 1.59569743e-06
Iter: 21 loss: 1.59336173e-06
Iter: 22 loss: 1.60301238e-06
Iter: 23 loss: 1.59286287e-06
Iter: 24 loss: 1.59063381e-06
Iter: 25 loss: 1.60752074e-06
Iter: 26 loss: 1.59049262e-06
Iter: 27 loss: 1.58911894e-06
Iter: 28 loss: 1.58581e-06
Iter: 29 loss: 1.62101458e-06
Iter: 30 loss: 1.58545981e-06
Iter: 31 loss: 1.58236389e-06
Iter: 32 loss: 1.59525121e-06
Iter: 33 loss: 1.58169792e-06
Iter: 34 loss: 1.57910108e-06
Iter: 35 loss: 1.5930359e-06
Iter: 36 loss: 1.57870363e-06
Iter: 37 loss: 1.57675504e-06
Iter: 38 loss: 1.57659724e-06
Iter: 39 loss: 1.57510055e-06
Iter: 40 loss: 1.57438558e-06
Iter: 41 loss: 1.57349609e-06
Iter: 42 loss: 1.572359e-06
Iter: 43 loss: 1.57052489e-06
Iter: 44 loss: 1.57051181e-06
Iter: 45 loss: 1.56951637e-06
Iter: 46 loss: 1.56936539e-06
Iter: 47 loss: 1.56856231e-06
Iter: 48 loss: 1.56666283e-06
Iter: 49 loss: 1.58716807e-06
Iter: 50 loss: 1.56644114e-06
Iter: 51 loss: 1.56633428e-06
Iter: 52 loss: 1.5655645e-06
Iter: 53 loss: 1.56511294e-06
Iter: 54 loss: 1.56392537e-06
Iter: 55 loss: 1.57292561e-06
Iter: 56 loss: 1.5636931e-06
Iter: 57 loss: 1.56206681e-06
Iter: 58 loss: 1.56559622e-06
Iter: 59 loss: 1.56147269e-06
Iter: 60 loss: 1.5600441e-06
Iter: 61 loss: 1.57009299e-06
Iter: 62 loss: 1.5599561e-06
Iter: 63 loss: 1.55831219e-06
Iter: 64 loss: 1.56586691e-06
Iter: 65 loss: 1.55801195e-06
Iter: 66 loss: 1.55694192e-06
Iter: 67 loss: 1.55499288e-06
Iter: 68 loss: 1.60213256e-06
Iter: 69 loss: 1.5549972e-06
Iter: 70 loss: 1.55347641e-06
Iter: 71 loss: 1.57728698e-06
Iter: 72 loss: 1.55348903e-06
Iter: 73 loss: 1.55257749e-06
Iter: 74 loss: 1.55421799e-06
Iter: 75 loss: 1.55216037e-06
Iter: 76 loss: 1.55155635e-06
Iter: 77 loss: 1.55148439e-06
Iter: 78 loss: 1.55117039e-06
Iter: 79 loss: 1.55051077e-06
Iter: 80 loss: 1.56155033e-06
Iter: 81 loss: 1.55048417e-06
Iter: 82 loss: 1.54991733e-06
Iter: 83 loss: 1.54991687e-06
Iter: 84 loss: 1.54941438e-06
Iter: 85 loss: 1.54859868e-06
Iter: 86 loss: 1.5485864e-06
Iter: 87 loss: 1.54794861e-06
Iter: 88 loss: 1.55296652e-06
Iter: 89 loss: 1.54789814e-06
Iter: 90 loss: 1.54718407e-06
Iter: 91 loss: 1.54790814e-06
Iter: 92 loss: 1.5467856e-06
Iter: 93 loss: 1.54626036e-06
Iter: 94 loss: 1.54599275e-06
Iter: 95 loss: 1.54575298e-06
Iter: 96 loss: 1.54548e-06
Iter: 97 loss: 1.54541203e-06
Iter: 98 loss: 1.5450106e-06
Iter: 99 loss: 1.5443112e-06
Iter: 100 loss: 1.56059912e-06
Iter: 101 loss: 1.54431859e-06
Iter: 102 loss: 1.54356735e-06
Iter: 103 loss: 1.54420491e-06
Iter: 104 loss: 1.5431508e-06
Iter: 105 loss: 1.54225904e-06
Iter: 106 loss: 1.54658824e-06
Iter: 107 loss: 1.54211364e-06
Iter: 108 loss: 1.54137751e-06
Iter: 109 loss: 1.54464294e-06
Iter: 110 loss: 1.54117879e-06
Iter: 111 loss: 1.54055738e-06
Iter: 112 loss: 1.54970928e-06
Iter: 113 loss: 1.54056022e-06
Iter: 114 loss: 1.54026179e-06
Iter: 115 loss: 1.53969597e-06
Iter: 116 loss: 1.55200814e-06
Iter: 117 loss: 1.53969177e-06
Iter: 118 loss: 1.53937117e-06
Iter: 119 loss: 1.53933024e-06
Iter: 120 loss: 1.5390184e-06
Iter: 121 loss: 1.53852955e-06
Iter: 122 loss: 1.53855194e-06
Iter: 123 loss: 1.53815563e-06
Iter: 124 loss: 1.54318525e-06
Iter: 125 loss: 1.53818519e-06
Iter: 126 loss: 1.53778046e-06
Iter: 127 loss: 1.53724261e-06
Iter: 128 loss: 1.53720475e-06
Iter: 129 loss: 1.53662677e-06
Iter: 130 loss: 1.53655287e-06
Iter: 131 loss: 1.53615133e-06
Iter: 132 loss: 1.53550036e-06
Iter: 133 loss: 1.54251393e-06
Iter: 134 loss: 1.53551218e-06
Iter: 135 loss: 1.53491294e-06
Iter: 136 loss: 1.53967926e-06
Iter: 137 loss: 1.53486212e-06
Iter: 138 loss: 1.53449241e-06
Iter: 139 loss: 1.53379574e-06
Iter: 140 loss: 1.54866962e-06
Iter: 141 loss: 1.53378687e-06
Iter: 142 loss: 1.53316478e-06
Iter: 143 loss: 1.53394808e-06
Iter: 144 loss: 1.53283679e-06
Iter: 145 loss: 1.53246492e-06
Iter: 146 loss: 1.53245719e-06
Iter: 147 loss: 1.53219287e-06
Iter: 148 loss: 1.53585427e-06
Iter: 149 loss: 1.53218821e-06
Iter: 150 loss: 1.5320079e-06
Iter: 151 loss: 1.53192786e-06
Iter: 152 loss: 1.53182793e-06
Iter: 153 loss: 1.53154758e-06
Iter: 154 loss: 1.53216729e-06
Iter: 155 loss: 1.53145811e-06
Iter: 156 loss: 1.53115411e-06
Iter: 157 loss: 1.53256019e-06
Iter: 158 loss: 1.5310851e-06
Iter: 159 loss: 1.53084613e-06
Iter: 160 loss: 1.5305244e-06
Iter: 161 loss: 1.5305136e-06
Iter: 162 loss: 1.53014889e-06
Iter: 163 loss: 1.53570704e-06
Iter: 164 loss: 1.53014366e-06
Iter: 165 loss: 1.52986308e-06
Iter: 166 loss: 1.52932364e-06
Iter: 167 loss: 1.54068118e-06
Iter: 168 loss: 1.5293308e-06
Iter: 169 loss: 1.5288e-06
Iter: 170 loss: 1.53013809e-06
Iter: 171 loss: 1.52861548e-06
Iter: 172 loss: 1.52851612e-06
Iter: 173 loss: 1.52840641e-06
Iter: 174 loss: 1.52823088e-06
Iter: 175 loss: 1.52782354e-06
Iter: 176 loss: 1.53624012e-06
Iter: 177 loss: 1.52782172e-06
Iter: 178 loss: 1.52746111e-06
Iter: 179 loss: 1.52752091e-06
Iter: 180 loss: 1.52717405e-06
Iter: 181 loss: 1.52671441e-06
Iter: 182 loss: 1.52822736e-06
Iter: 183 loss: 1.52659709e-06
Iter: 184 loss: 1.52606833e-06
Iter: 185 loss: 1.52706139e-06
Iter: 186 loss: 1.52584835e-06
Iter: 187 loss: 1.52604025e-06
Iter: 188 loss: 1.52566281e-06
Iter: 189 loss: 1.52555526e-06
Iter: 190 loss: 1.52528423e-06
Iter: 191 loss: 1.53003612e-06
Iter: 192 loss: 1.52529219e-06
Iter: 193 loss: 1.52498183e-06
Iter: 194 loss: 1.52696794e-06
Iter: 195 loss: 1.52494931e-06
Iter: 196 loss: 1.52467373e-06
Iter: 197 loss: 1.52545238e-06
Iter: 198 loss: 1.52460632e-06
Iter: 199 loss: 1.52435507e-06
Iter: 200 loss: 1.52415021e-06
Iter: 201 loss: 1.52407665e-06
Iter: 202 loss: 1.52378709e-06
Iter: 203 loss: 1.52379266e-06
Iter: 204 loss: 1.52358609e-06
Iter: 205 loss: 1.52312259e-06
Iter: 206 loss: 1.53091014e-06
Iter: 207 loss: 1.52311782e-06
Iter: 208 loss: 1.52258428e-06
Iter: 209 loss: 1.52328732e-06
Iter: 210 loss: 1.52232815e-06
Iter: 211 loss: 1.5220495e-06
Iter: 212 loss: 1.52203552e-06
Iter: 213 loss: 1.52169059e-06
Iter: 214 loss: 1.52130258e-06
Iter: 215 loss: 1.52126381e-06
Iter: 216 loss: 1.52090058e-06
Iter: 217 loss: 1.52066377e-06
Iter: 218 loss: 1.52052291e-06
Iter: 219 loss: 1.51994755e-06
Iter: 220 loss: 1.52266966e-06
Iter: 221 loss: 1.51985296e-06
Iter: 222 loss: 1.51940685e-06
Iter: 223 loss: 1.52057032e-06
Iter: 224 loss: 1.51924314e-06
Iter: 225 loss: 1.51897052e-06
Iter: 226 loss: 1.51890663e-06
Iter: 227 loss: 1.51868903e-06
Iter: 228 loss: 1.51820961e-06
Iter: 229 loss: 1.52627467e-06
Iter: 230 loss: 1.51820427e-06
Iter: 231 loss: 1.5178839e-06
Iter: 232 loss: 1.51788686e-06
Iter: 233 loss: 1.51752181e-06
Iter: 234 loss: 1.51700397e-06
Iter: 235 loss: 1.51702829e-06
Iter: 236 loss: 1.51650056e-06
Iter: 237 loss: 1.51981624e-06
Iter: 238 loss: 1.51644349e-06
Iter: 239 loss: 1.51612016e-06
Iter: 240 loss: 1.51819813e-06
Iter: 241 loss: 1.51606366e-06
Iter: 242 loss: 1.5157e-06
Iter: 243 loss: 1.51554582e-06
Iter: 244 loss: 1.51537074e-06
Iter: 245 loss: 1.51495749e-06
Iter: 246 loss: 1.51487052e-06
Iter: 247 loss: 1.51458744e-06
Iter: 248 loss: 1.51403992e-06
Iter: 249 loss: 1.5159078e-06
Iter: 250 loss: 1.51391077e-06
Iter: 251 loss: 1.51348922e-06
Iter: 252 loss: 1.51689278e-06
Iter: 253 loss: 1.51348024e-06
Iter: 254 loss: 1.51304812e-06
Iter: 255 loss: 1.51635686e-06
Iter: 256 loss: 1.51303334e-06
Iter: 257 loss: 1.51283189e-06
Iter: 258 loss: 1.51234292e-06
Iter: 259 loss: 1.51744189e-06
Iter: 260 loss: 1.51228141e-06
Iter: 261 loss: 1.51173424e-06
Iter: 262 loss: 1.51421614e-06
Iter: 263 loss: 1.5116359e-06
Iter: 264 loss: 1.51138079e-06
Iter: 265 loss: 1.51132349e-06
Iter: 266 loss: 1.51109407e-06
Iter: 267 loss: 1.51067502e-06
Iter: 268 loss: 1.5205427e-06
Iter: 269 loss: 1.51068411e-06
Iter: 270 loss: 1.51047573e-06
Iter: 271 loss: 1.51046493e-06
Iter: 272 loss: 1.51023482e-06
Iter: 273 loss: 1.50982692e-06
Iter: 274 loss: 1.51949166e-06
Iter: 275 loss: 1.50982646e-06
Iter: 276 loss: 1.50943e-06
Iter: 277 loss: 1.51138636e-06
Iter: 278 loss: 1.50937387e-06
Iter: 279 loss: 1.50907431e-06
Iter: 280 loss: 1.51185043e-06
Iter: 281 loss: 1.50907283e-06
Iter: 282 loss: 1.50885808e-06
Iter: 283 loss: 1.50903406e-06
Iter: 284 loss: 1.50871551e-06
Iter: 285 loss: 1.50844221e-06
Iter: 286 loss: 1.50819483e-06
Iter: 287 loss: 1.50814208e-06
Iter: 288 loss: 1.50781989e-06
Iter: 289 loss: 1.51131349e-06
Iter: 290 loss: 1.50780704e-06
Iter: 291 loss: 1.50755636e-06
Iter: 292 loss: 1.50966252e-06
Iter: 293 loss: 1.50751703e-06
Iter: 294 loss: 1.50733808e-06
Iter: 295 loss: 1.50698338e-06
Iter: 296 loss: 1.5112447e-06
Iter: 297 loss: 1.50692904e-06
Iter: 298 loss: 1.50656751e-06
Iter: 299 loss: 1.50887593e-06
Iter: 300 loss: 1.50652033e-06
Iter: 301 loss: 1.50638539e-06
Iter: 302 loss: 1.50633468e-06
Iter: 303 loss: 1.50617916e-06
Iter: 304 loss: 1.50570486e-06
Iter: 305 loss: 1.50762264e-06
Iter: 306 loss: 1.50553274e-06
Iter: 307 loss: 1.50544543e-06
Iter: 308 loss: 1.50531775e-06
Iter: 309 loss: 1.50507617e-06
Iter: 310 loss: 1.50464462e-06
Iter: 311 loss: 1.50465485e-06
Iter: 312 loss: 1.50436892e-06
Iter: 313 loss: 1.50674305e-06
Iter: 314 loss: 1.50433675e-06
Iter: 315 loss: 1.50414269e-06
Iter: 316 loss: 1.505932e-06
Iter: 317 loss: 1.5041179e-06
Iter: 318 loss: 1.50396181e-06
Iter: 319 loss: 1.50388109e-06
Iter: 320 loss: 1.50381857e-06
Iter: 321 loss: 1.50362609e-06
Iter: 322 loss: 1.50428377e-06
Iter: 323 loss: 1.50355527e-06
Iter: 324 loss: 1.50338724e-06
Iter: 325 loss: 1.50413416e-06
Iter: 326 loss: 1.503367e-06
Iter: 327 loss: 1.50311075e-06
Iter: 328 loss: 1.50355095e-06
Iter: 329 loss: 1.5030073e-06
Iter: 330 loss: 1.50278379e-06
Iter: 331 loss: 1.50240089e-06
Iter: 332 loss: 1.51166046e-06
Iter: 333 loss: 1.50239111e-06
Iter: 334 loss: 1.50204551e-06
Iter: 335 loss: 1.50603478e-06
Iter: 336 loss: 1.50204232e-06
Iter: 337 loss: 1.50175231e-06
Iter: 338 loss: 1.50539972e-06
Iter: 339 loss: 1.50175129e-06
Iter: 340 loss: 1.50159485e-06
Iter: 341 loss: 1.50126607e-06
Iter: 342 loss: 1.50657831e-06
Iter: 343 loss: 1.50127778e-06
Iter: 344 loss: 1.5012123e-06
Iter: 345 loss: 1.50112692e-06
Iter: 346 loss: 1.50101391e-06
Iter: 347 loss: 1.50070423e-06
Iter: 348 loss: 1.50362393e-06
Iter: 349 loss: 1.50068126e-06
Iter: 350 loss: 1.50038341e-06
Iter: 351 loss: 1.50273809e-06
Iter: 352 loss: 1.5003568e-06
Iter: 353 loss: 1.50003518e-06
Iter: 354 loss: 1.50077506e-06
Iter: 355 loss: 1.49993639e-06
Iter: 356 loss: 1.49968059e-06
Iter: 357 loss: 1.49979542e-06
Iter: 358 loss: 1.49952541e-06
Iter: 359 loss: 1.49927712e-06
Iter: 360 loss: 1.50090364e-06
Iter: 361 loss: 1.4992595e-06
Iter: 362 loss: 1.49911716e-06
Iter: 363 loss: 1.50073879e-06
Iter: 364 loss: 1.49911625e-06
Iter: 365 loss: 1.49898642e-06
Iter: 366 loss: 1.49894367e-06
Iter: 367 loss: 1.49887933e-06
Iter: 368 loss: 1.49870857e-06
Iter: 369 loss: 1.49854543e-06
Iter: 370 loss: 1.49850109e-06
Iter: 371 loss: 1.49835057e-06
Iter: 372 loss: 1.49834386e-06
Iter: 373 loss: 1.49815628e-06
Iter: 374 loss: 1.49834682e-06
Iter: 375 loss: 1.49803429e-06
Iter: 376 loss: 1.49784614e-06
Iter: 377 loss: 1.49757057e-06
Iter: 378 loss: 1.49753646e-06
Iter: 379 loss: 1.49735911e-06
Iter: 380 loss: 1.49732273e-06
Iter: 381 loss: 1.49720267e-06
Iter: 382 loss: 1.49693722e-06
Iter: 383 loss: 1.50022879e-06
Iter: 384 loss: 1.49691698e-06
Iter: 385 loss: 1.49675475e-06
Iter: 386 loss: 1.4967643e-06
Iter: 387 loss: 1.49659809e-06
Iter: 388 loss: 1.49669711e-06
Iter: 389 loss: 1.49649861e-06
Iter: 390 loss: 1.49636548e-06
Iter: 391 loss: 1.49616585e-06
Iter: 392 loss: 1.49616494e-06
Iter: 393 loss: 1.49596895e-06
Iter: 394 loss: 1.49594962e-06
Iter: 395 loss: 1.49580376e-06
Iter: 396 loss: 1.49609104e-06
Iter: 397 loss: 1.4957227e-06
Iter: 398 loss: 1.4955059e-06
Iter: 399 loss: 1.49559696e-06
Iter: 400 loss: 1.49537186e-06
Iter: 401 loss: 1.49514381e-06
Iter: 402 loss: 1.49526136e-06
Iter: 403 loss: 1.49499124e-06
Iter: 404 loss: 1.4948946e-06
Iter: 405 loss: 1.4948539e-06
Iter: 406 loss: 1.49471191e-06
Iter: 407 loss: 1.49447624e-06
Iter: 408 loss: 1.50003962e-06
Iter: 409 loss: 1.49446623e-06
Iter: 410 loss: 1.49430275e-06
Iter: 411 loss: 1.49638436e-06
Iter: 412 loss: 1.49428547e-06
Iter: 413 loss: 1.4941038e-06
Iter: 414 loss: 1.49400307e-06
Iter: 415 loss: 1.49392736e-06
Iter: 416 loss: 1.49369316e-06
Iter: 417 loss: 1.49371567e-06
Iter: 418 loss: 1.49353752e-06
Iter: 419 loss: 1.49328321e-06
Iter: 420 loss: 1.49328162e-06
Iter: 421 loss: 1.49315838e-06
Iter: 422 loss: 1.49299569e-06
Iter: 423 loss: 1.49296989e-06
Iter: 424 loss: 1.49280083e-06
Iter: 425 loss: 1.49276764e-06
Iter: 426 loss: 1.49264781e-06
Iter: 427 loss: 1.49243419e-06
Iter: 428 loss: 1.49320499e-06
Iter: 429 loss: 1.49236325e-06
Iter: 430 loss: 1.49223115e-06
Iter: 431 loss: 1.49224365e-06
Iter: 432 loss: 1.4920995e-06
Iter: 433 loss: 1.49204402e-06
Iter: 434 loss: 1.49194614e-06
Iter: 435 loss: 1.49171831e-06
Iter: 436 loss: 1.49213861e-06
Iter: 437 loss: 1.49160451e-06
Iter: 438 loss: 1.49144773e-06
Iter: 439 loss: 1.49229129e-06
Iter: 440 loss: 1.4913835e-06
Iter: 441 loss: 1.4911177e-06
Iter: 442 loss: 1.49152083e-06
Iter: 443 loss: 1.49101493e-06
Iter: 444 loss: 1.49081075e-06
Iter: 445 loss: 1.49075856e-06
Iter: 446 loss: 1.49063953e-06
Iter: 447 loss: 1.49050891e-06
Iter: 448 loss: 1.49048162e-06
Iter: 449 loss: 1.49037487e-06
Iter: 450 loss: 1.49015796e-06
Iter: 451 loss: 1.49327207e-06
Iter: 452 loss: 1.49014602e-06
Iter: 453 loss: 1.48998834e-06
Iter: 454 loss: 1.49237724e-06
Iter: 455 loss: 1.4899922e-06
Iter: 456 loss: 1.48980212e-06
Iter: 457 loss: 1.48990352e-06
Iter: 458 loss: 1.4897098e-06
Iter: 459 loss: 1.48953654e-06
Iter: 460 loss: 1.48932952e-06
Iter: 461 loss: 1.48930951e-06
Iter: 462 loss: 1.48909965e-06
Iter: 463 loss: 1.49175685e-06
Iter: 464 loss: 1.48907918e-06
Iter: 465 loss: 1.48885943e-06
Iter: 466 loss: 1.4896541e-06
Iter: 467 loss: 1.48879042e-06
Iter: 468 loss: 1.48862932e-06
Iter: 469 loss: 1.48877155e-06
Iter: 470 loss: 1.48848676e-06
Iter: 471 loss: 1.48831487e-06
Iter: 472 loss: 1.48899699e-06
Iter: 473 loss: 1.48827553e-06
Iter: 474 loss: 1.4881366e-06
Iter: 475 loss: 1.48999538e-06
Iter: 476 loss: 1.48812069e-06
Iter: 477 loss: 1.4880211e-06
Iter: 478 loss: 1.48779316e-06
Iter: 479 loss: 1.48780191e-06
Iter: 480 loss: 1.48763547e-06
Iter: 481 loss: 1.48855156e-06
Iter: 482 loss: 1.48761944e-06
Iter: 483 loss: 1.48737593e-06
Iter: 484 loss: 1.48751724e-06
Iter: 485 loss: 1.48722154e-06
Iter: 486 loss: 1.48702213e-06
Iter: 487 loss: 1.48687172e-06
Iter: 488 loss: 1.48679499e-06
Iter: 489 loss: 1.48677452e-06
Iter: 490 loss: 1.48668278e-06
Iter: 491 loss: 1.48661525e-06
Iter: 492 loss: 1.48648292e-06
Iter: 493 loss: 1.48957076e-06
Iter: 494 loss: 1.48648087e-06
Iter: 495 loss: 1.48631943e-06
Iter: 496 loss: 1.4865542e-06
Iter: 497 loss: 1.48627419e-06
Iter: 498 loss: 1.48615447e-06
Iter: 499 loss: 1.48616925e-06
Iter: 500 loss: 1.48603158e-06
Iter: 501 loss: 1.48592426e-06
Iter: 502 loss: 1.48590948e-06
Iter: 503 loss: 1.48573793e-06
Iter: 504 loss: 1.48611798e-06
Iter: 505 loss: 1.48568495e-06
Iter: 506 loss: 1.48550055e-06
Iter: 507 loss: 1.48662252e-06
Iter: 508 loss: 1.48546087e-06
Iter: 509 loss: 1.48527192e-06
Iter: 510 loss: 1.48553795e-06
Iter: 511 loss: 1.48519462e-06
Iter: 512 loss: 1.48500772e-06
Iter: 513 loss: 1.48496167e-06
Iter: 514 loss: 1.48487061e-06
Iter: 515 loss: 1.48477261e-06
Iter: 516 loss: 1.48473077e-06
Iter: 517 loss: 1.48465779e-06
Iter: 518 loss: 1.48444292e-06
Iter: 519 loss: 1.48593517e-06
Iter: 520 loss: 1.48440233e-06
Iter: 521 loss: 1.48422828e-06
Iter: 522 loss: 1.48692607e-06
Iter: 523 loss: 1.48421805e-06
Iter: 524 loss: 1.48407105e-06
Iter: 525 loss: 1.4847011e-06
Iter: 526 loss: 1.4840291e-06
Iter: 527 loss: 1.48393656e-06
Iter: 528 loss: 1.48374261e-06
Iter: 529 loss: 1.48375329e-06
Iter: 530 loss: 1.48355014e-06
Iter: 531 loss: 1.48507604e-06
Iter: 532 loss: 1.48354366e-06
Iter: 533 loss: 1.48340439e-06
Iter: 534 loss: 1.48338245e-06
Iter: 535 loss: 1.4833181e-06
Iter: 536 loss: 1.48319464e-06
Iter: 537 loss: 1.48554545e-06
Iter: 538 loss: 1.48319236e-06
Iter: 539 loss: 1.48307106e-06
Iter: 540 loss: 1.48460276e-06
Iter: 541 loss: 1.48304957e-06
Iter: 542 loss: 1.48296033e-06
Iter: 543 loss: 1.48353854e-06
Iter: 544 loss: 1.48293952e-06
Iter: 545 loss: 1.4828704e-06
Iter: 546 loss: 1.48268373e-06
Iter: 547 loss: 1.48620643e-06
Iter: 548 loss: 1.48271056e-06
Iter: 549 loss: 1.48251752e-06
Iter: 550 loss: 1.48442473e-06
Iter: 551 loss: 1.48252457e-06
Iter: 552 loss: 1.48238e-06
Iter: 553 loss: 1.48248182e-06
Iter: 554 loss: 1.48227582e-06
Iter: 555 loss: 1.48211461e-06
Iter: 556 loss: 1.4819168e-06
Iter: 557 loss: 1.48191327e-06
Iter: 558 loss: 1.4818163e-06
Iter: 559 loss: 1.481764e-06
Iter: 560 loss: 1.48163781e-06
Iter: 561 loss: 1.4815048e-06
Iter: 562 loss: 1.4815048e-06
Iter: 563 loss: 1.48137178e-06
Iter: 564 loss: 1.48147615e-06
Iter: 565 loss: 1.48126696e-06
Iter: 566 loss: 1.48124877e-06
Iter: 567 loss: 1.48119284e-06
Iter: 568 loss: 1.48111894e-06
Iter: 569 loss: 1.48099014e-06
Iter: 570 loss: 1.4839585e-06
Iter: 571 loss: 1.48100048e-06
Iter: 572 loss: 1.48086338e-06
Iter: 573 loss: 1.48166282e-06
Iter: 574 loss: 1.48083086e-06
Iter: 575 loss: 1.48068375e-06
Iter: 576 loss: 1.48156425e-06
Iter: 577 loss: 1.4806717e-06
Iter: 578 loss: 1.48056597e-06
Iter: 579 loss: 1.48052993e-06
Iter: 580 loss: 1.48044887e-06
Iter: 581 loss: 1.48032564e-06
Iter: 582 loss: 1.4806061e-06
Iter: 583 loss: 1.48026072e-06
Iter: 584 loss: 1.48008849e-06
Iter: 585 loss: 1.48160098e-06
Iter: 586 loss: 1.48008644e-06
Iter: 587 loss: 1.47998935e-06
Iter: 588 loss: 1.47979699e-06
Iter: 589 loss: 1.48386448e-06
Iter: 590 loss: 1.47981291e-06
Iter: 591 loss: 1.47960372e-06
Iter: 592 loss: 1.48171819e-06
Iter: 593 loss: 1.47962294e-06
Iter: 594 loss: 1.47945764e-06
Iter: 595 loss: 1.47958121e-06
Iter: 596 loss: 1.47936066e-06
Iter: 597 loss: 1.47922799e-06
Iter: 598 loss: 1.47922378e-06
Iter: 599 loss: 1.47913761e-06
Iter: 600 loss: 1.47898e-06
Iter: 601 loss: 1.47953642e-06
Iter: 602 loss: 1.47893843e-06
Iter: 603 loss: 1.47881576e-06
Iter: 604 loss: 1.47880269e-06
Iter: 605 loss: 1.47871253e-06
Iter: 606 loss: 1.47854655e-06
Iter: 607 loss: 1.4805222e-06
Iter: 608 loss: 1.47853086e-06
Iter: 609 loss: 1.47847413e-06
Iter: 610 loss: 1.47842684e-06
Iter: 611 loss: 1.47831702e-06
Iter: 612 loss: 1.47821788e-06
Iter: 613 loss: 1.4782097e-06
Iter: 614 loss: 1.47807441e-06
Iter: 615 loss: 1.47832793e-06
Iter: 616 loss: 1.47798619e-06
Iter: 617 loss: 1.47786477e-06
Iter: 618 loss: 1.47924402e-06
Iter: 619 loss: 1.47786125e-06
Iter: 620 loss: 1.47771505e-06
Iter: 621 loss: 1.47750552e-06
Iter: 622 loss: 1.47749586e-06
Iter: 623 loss: 1.4772329e-06
Iter: 624 loss: 1.47771971e-06
Iter: 625 loss: 1.47715059e-06
Iter: 626 loss: 1.47699609e-06
Iter: 627 loss: 1.47699336e-06
Iter: 628 loss: 1.47686842e-06
Iter: 629 loss: 1.47657079e-06
Iter: 630 loss: 1.47998708e-06
Iter: 631 loss: 1.47655726e-06
Iter: 632 loss: 1.47628543e-06
Iter: 633 loss: 1.47727e-06
Iter: 634 loss: 1.47623336e-06
Iter: 635 loss: 1.47603669e-06
Iter: 636 loss: 1.47757396e-06
Iter: 637 loss: 1.47601622e-06
Iter: 638 loss: 1.47577862e-06
Iter: 639 loss: 1.47716241e-06
Iter: 640 loss: 1.47574656e-06
Iter: 641 loss: 1.47563424e-06
Iter: 642 loss: 1.4754587e-06
Iter: 643 loss: 1.47544961e-06
Iter: 644 loss: 1.47533888e-06
Iter: 645 loss: 1.47529613e-06
Iter: 646 loss: 1.47522087e-06
Iter: 647 loss: 1.47504238e-06
Iter: 648 loss: 1.4772653e-06
Iter: 649 loss: 1.47505057e-06
Iter: 650 loss: 1.47480932e-06
Iter: 651 loss: 1.47554715e-06
Iter: 652 loss: 1.47475339e-06
Iter: 653 loss: 1.47454034e-06
Iter: 654 loss: 1.47688456e-06
Iter: 655 loss: 1.47455603e-06
Iter: 656 loss: 1.47440619e-06
Iter: 657 loss: 1.47422725e-06
Iter: 658 loss: 1.47418223e-06
Iter: 659 loss: 1.47396577e-06
Iter: 660 loss: 1.47405399e-06
Iter: 661 loss: 1.47377227e-06
Iter: 662 loss: 1.47358207e-06
Iter: 663 loss: 1.47357866e-06
Iter: 664 loss: 1.47342371e-06
Iter: 665 loss: 1.47318747e-06
Iter: 666 loss: 1.47316928e-06
Iter: 667 loss: 1.472946e-06
Iter: 668 loss: 1.47293247e-06
Iter: 669 loss: 1.47277387e-06
Iter: 670 loss: 1.47272408e-06
Iter: 671 loss: 1.47264871e-06
Iter: 672 loss: 1.47255e-06
Iter: 673 loss: 1.47276569e-06
Iter: 674 loss: 1.47250375e-06
Iter: 675 loss: 1.47240507e-06
Iter: 676 loss: 1.4722898e-06
Iter: 677 loss: 1.47229662e-06
Iter: 678 loss: 1.47212677e-06
Iter: 679 loss: 1.47403807e-06
Iter: 680 loss: 1.47209744e-06
Iter: 681 loss: 1.47198034e-06
Iter: 682 loss: 1.47181265e-06
Iter: 683 loss: 1.47512606e-06
Iter: 684 loss: 1.47179708e-06
Iter: 685 loss: 1.47161677e-06
Iter: 686 loss: 1.47323271e-06
Iter: 687 loss: 1.4716112e-06
Iter: 688 loss: 1.47145704e-06
Iter: 689 loss: 1.47233084e-06
Iter: 690 loss: 1.47139883e-06
Iter: 691 loss: 1.47126184e-06
Iter: 692 loss: 1.47124092e-06
Iter: 693 loss: 1.47113428e-06
Iter: 694 loss: 1.47097853e-06
Iter: 695 loss: 1.47100241e-06
Iter: 696 loss: 1.47086132e-06
Iter: 697 loss: 1.47068567e-06
Iter: 698 loss: 1.4725299e-06
Iter: 699 loss: 1.47068761e-06
Iter: 700 loss: 1.4704998e-06
Iter: 701 loss: 1.47107994e-06
Iter: 702 loss: 1.47050321e-06
Iter: 703 loss: 1.47040339e-06
Iter: 704 loss: 1.47025935e-06
Iter: 705 loss: 1.47411231e-06
Iter: 706 loss: 1.47026367e-06
Iter: 707 loss: 1.47008609e-06
Iter: 708 loss: 1.471068e-06
Iter: 709 loss: 1.47007177e-06
Iter: 710 loss: 1.46994955e-06
Iter: 711 loss: 1.46994694e-06
Iter: 712 loss: 1.46986554e-06
Iter: 713 loss: 1.46973389e-06
Iter: 714 loss: 1.46973866e-06
Iter: 715 loss: 1.4696725e-06
Iter: 716 loss: 1.46966499e-06
Iter: 717 loss: 1.46960724e-06
Iter: 718 loss: 1.4694574e-06
Iter: 719 loss: 1.47058927e-06
Iter: 720 loss: 1.4694308e-06
Iter: 721 loss: 1.46927391e-06
Iter: 722 loss: 1.46951265e-06
Iter: 723 loss: 1.4691866e-06
Iter: 724 loss: 1.46909326e-06
Iter: 725 loss: 1.46908201e-06
Iter: 726 loss: 1.46901357e-06
Iter: 727 loss: 1.46884884e-06
Iter: 728 loss: 1.47078958e-06
Iter: 729 loss: 1.46882223e-06
Iter: 730 loss: 1.46866273e-06
Iter: 731 loss: 1.46961293e-06
Iter: 732 loss: 1.46863533e-06
Iter: 733 loss: 1.4685246e-06
Iter: 734 loss: 1.46921661e-06
Iter: 735 loss: 1.46851016e-06
Iter: 736 loss: 1.46844195e-06
Iter: 737 loss: 1.46890773e-06
Iter: 738 loss: 1.4684415e-06
Iter: 739 loss: 1.46836237e-06
Iter: 740 loss: 1.46848561e-06
Iter: 741 loss: 1.46832099e-06
Iter: 742 loss: 1.46825778e-06
Iter: 743 loss: 1.46818775e-06
Iter: 744 loss: 1.46815773e-06
Iter: 745 loss: 1.46806167e-06
Iter: 746 loss: 1.46803711e-06
Iter: 747 loss: 1.46795821e-06
Iter: 748 loss: 1.46780326e-06
Iter: 749 loss: 1.46917841e-06
Iter: 750 loss: 1.46780894e-06
Iter: 751 loss: 1.46769787e-06
Iter: 752 loss: 1.46930029e-06
Iter: 753 loss: 1.46769651e-06
Iter: 754 loss: 1.46760317e-06
Iter: 755 loss: 1.46748221e-06
Iter: 756 loss: 1.46973673e-06
Iter: 757 loss: 1.46747357e-06
Iter: 758 loss: 1.46745015e-06
Iter: 759 loss: 1.46739387e-06
Iter: 760 loss: 1.46734601e-06
Iter: 761 loss: 1.46724915e-06
Iter: 762 loss: 1.46912657e-06
Iter: 763 loss: 1.46724506e-06
Iter: 764 loss: 1.46715877e-06
Iter: 765 loss: 1.4674298e-06
Iter: 766 loss: 1.46711773e-06
Iter: 767 loss: 1.46701905e-06
Iter: 768 loss: 1.46778621e-06
Iter: 769 loss: 1.46698812e-06
Iter: 770 loss: 1.46693992e-06
Iter: 771 loss: 1.46681259e-06
Iter: 772 loss: 1.4683319e-06
Iter: 773 loss: 1.46679463e-06
Iter: 774 loss: 1.46672414e-06
Iter: 775 loss: 1.46672573e-06
Iter: 776 loss: 1.46664627e-06
Iter: 777 loss: 1.46658067e-06
Iter: 778 loss: 1.46654179e-06
Iter: 779 loss: 1.46642128e-06
Iter: 780 loss: 1.46641457e-06
Iter: 781 loss: 1.46634522e-06
Iter: 782 loss: 1.46620641e-06
Iter: 783 loss: 1.46615014e-06
Iter: 784 loss: 1.46607351e-06
Iter: 785 loss: 1.46597881e-06
Iter: 786 loss: 1.46594994e-06
Iter: 787 loss: 1.46582363e-06
Iter: 788 loss: 1.46582477e-06
Iter: 789 loss: 1.46573257e-06
Iter: 790 loss: 1.46559375e-06
Iter: 791 loss: 1.46607158e-06
Iter: 792 loss: 1.46554089e-06
Iter: 793 loss: 1.46541288e-06
Iter: 794 loss: 1.46606646e-06
Iter: 795 loss: 1.4653931e-06
Iter: 796 loss: 1.46525633e-06
Iter: 797 loss: 1.46568141e-06
Iter: 798 loss: 1.46520449e-06
Iter: 799 loss: 1.46513389e-06
Iter: 800 loss: 1.46496734e-06
Iter: 801 loss: 1.46807929e-06
Iter: 802 loss: 1.46496905e-06
Iter: 803 loss: 1.46483376e-06
Iter: 804 loss: 1.46687569e-06
Iter: 805 loss: 1.46483353e-06
Iter: 806 loss: 1.46470552e-06
Iter: 807 loss: 1.46549621e-06
Iter: 808 loss: 1.46467892e-06
Iter: 809 loss: 1.46463276e-06
Iter: 810 loss: 1.46448542e-06
Iter: 811 loss: 1.46706907e-06
Iter: 812 loss: 1.46449133e-06
Iter: 813 loss: 1.46438947e-06
Iter: 814 loss: 1.46438697e-06
Iter: 815 loss: 1.46429193e-06
Iter: 816 loss: 1.46415027e-06
Iter: 817 loss: 1.46413026e-06
Iter: 818 loss: 1.46399645e-06
Iter: 819 loss: 1.46593402e-06
Iter: 820 loss: 1.46399702e-06
Iter: 821 loss: 1.46389448e-06
Iter: 822 loss: 1.46390903e-06
Iter: 823 loss: 1.46379409e-06
Iter: 824 loss: 1.4636571e-06
Iter: 825 loss: 1.46356388e-06
Iter: 826 loss: 1.46349828e-06
Iter: 827 loss: 1.46333309e-06
Iter: 828 loss: 1.46513958e-06
Iter: 829 loss: 1.46330399e-06
Iter: 830 loss: 1.46319485e-06
Iter: 831 loss: 1.46476839e-06
Iter: 832 loss: 1.46320099e-06
Iter: 833 loss: 1.46311049e-06
Iter: 834 loss: 1.4630948e-06
Iter: 835 loss: 1.46301556e-06
Iter: 836 loss: 1.46292837e-06
Iter: 837 loss: 1.46292007e-06
Iter: 838 loss: 1.46286e-06
Iter: 839 loss: 1.46274522e-06
Iter: 840 loss: 1.46275806e-06
Iter: 841 loss: 1.46266143e-06
Iter: 842 loss: 1.46262028e-06
Iter: 843 loss: 1.46256457e-06
Iter: 844 loss: 1.46247964e-06
Iter: 845 loss: 1.46247726e-06
Iter: 846 loss: 1.46242382e-06
Iter: 847 loss: 1.46231127e-06
Iter: 848 loss: 1.46416119e-06
Iter: 849 loss: 1.46230991e-06
Iter: 850 loss: 1.46222226e-06
Iter: 851 loss: 1.46209675e-06
Iter: 852 loss: 1.46208254e-06
Iter: 853 loss: 1.46198386e-06
Iter: 854 loss: 1.4634893e-06
Iter: 855 loss: 1.46198772e-06
Iter: 856 loss: 1.4618463e-06
Iter: 857 loss: 1.46170203e-06
Iter: 858 loss: 1.46169236e-06
Iter: 859 loss: 1.46149114e-06
Iter: 860 loss: 1.4614775e-06
Iter: 861 loss: 1.46137472e-06
Iter: 862 loss: 1.46127582e-06
Iter: 863 loss: 1.46123489e-06
Iter: 864 loss: 1.46112518e-06
Iter: 865 loss: 1.461068e-06
Iter: 866 loss: 1.46101081e-06
Iter: 867 loss: 1.46086586e-06
Iter: 868 loss: 1.46107573e-06
Iter: 869 loss: 1.46078719e-06
Iter: 870 loss: 1.46066986e-06
Iter: 871 loss: 1.46146704e-06
Iter: 872 loss: 1.46070329e-06
Iter: 873 loss: 1.46058449e-06
Iter: 874 loss: 1.46153639e-06
Iter: 875 loss: 1.46058699e-06
Iter: 876 loss: 1.46051877e-06
Iter: 877 loss: 1.46037314e-06
Iter: 878 loss: 1.4619792e-06
Iter: 879 loss: 1.46036314e-06
Iter: 880 loss: 1.46026605e-06
Iter: 881 loss: 1.46023706e-06
Iter: 882 loss: 1.46016532e-06
Iter: 883 loss: 1.46008574e-06
Iter: 884 loss: 1.46006141e-06
Iter: 885 loss: 1.45994318e-06
Iter: 886 loss: 1.46041464e-06
Iter: 887 loss: 1.45992681e-06
Iter: 888 loss: 1.45980528e-06
Iter: 889 loss: 1.46031073e-06
Iter: 890 loss: 1.45975423e-06
Iter: 891 loss: 1.4596485e-06
Iter: 892 loss: 1.45950139e-06
Iter: 893 loss: 1.45947797e-06
Iter: 894 loss: 1.45932518e-06
Iter: 895 loss: 1.46063508e-06
Iter: 896 loss: 1.45931494e-06
Iter: 897 loss: 1.45916806e-06
Iter: 898 loss: 1.46024649e-06
Iter: 899 loss: 1.45917136e-06
Iter: 900 loss: 1.45905346e-06
Iter: 901 loss: 1.45895171e-06
Iter: 902 loss: 1.45893887e-06
Iter: 903 loss: 1.45876857e-06
Iter: 904 loss: 1.45958461e-06
Iter: 905 loss: 1.45877186e-06
Iter: 906 loss: 1.45866875e-06
Iter: 907 loss: 1.45928288e-06
Iter: 908 loss: 1.45866102e-06
Iter: 909 loss: 1.45856552e-06
Iter: 910 loss: 1.45872968e-06
Iter: 911 loss: 1.45848821e-06
Iter: 912 loss: 1.45841295e-06
Iter: 913 loss: 1.45845081e-06
Iter: 914 loss: 1.45836975e-06
Iter: 915 loss: 1.4582597e-06
Iter: 916 loss: 1.45961474e-06
Iter: 917 loss: 1.45825538e-06
Iter: 918 loss: 1.45820263e-06
Iter: 919 loss: 1.45807212e-06
Iter: 920 loss: 1.45986792e-06
Iter: 921 loss: 1.45807007e-06
Iter: 922 loss: 1.45797037e-06
Iter: 923 loss: 1.45794525e-06
Iter: 924 loss: 1.45786203e-06
Iter: 925 loss: 1.45770207e-06
Iter: 926 loss: 1.46121602e-06
Iter: 927 loss: 1.45770946e-06
Iter: 928 loss: 1.45753347e-06
Iter: 929 loss: 1.45783827e-06
Iter: 930 loss: 1.45747595e-06
Iter: 931 loss: 1.45740432e-06
Iter: 932 loss: 1.4573601e-06
Iter: 933 loss: 1.45729973e-06
Iter: 934 loss: 1.4571649e-06
Iter: 935 loss: 1.45977606e-06
Iter: 936 loss: 1.4571732e-06
Iter: 937 loss: 1.45704598e-06
Iter: 938 loss: 1.45746469e-06
Iter: 939 loss: 1.45702461e-06
Iter: 940 loss: 1.45692729e-06
Iter: 941 loss: 1.45700358e-06
Iter: 942 loss: 1.45681724e-06
Iter: 943 loss: 1.45668332e-06
Iter: 944 loss: 1.4580412e-06
Iter: 945 loss: 1.45669173e-06
Iter: 946 loss: 1.45655918e-06
Iter: 947 loss: 1.45642366e-06
Iter: 948 loss: 1.45642196e-06
Iter: 949 loss: 1.45628871e-06
Iter: 950 loss: 1.45627109e-06
Iter: 951 loss: 1.45617082e-06
Iter: 952 loss: 1.45606748e-06
Iter: 953 loss: 1.45603849e-06
Iter: 954 loss: 1.45588911e-06
Iter: 955 loss: 1.45669492e-06
Iter: 956 loss: 1.45587524e-06
Iter: 957 loss: 1.45574927e-06
Iter: 958 loss: 1.45646402e-06
Iter: 959 loss: 1.45570039e-06
Iter: 960 loss: 1.45560261e-06
Iter: 961 loss: 1.45541276e-06
Iter: 962 loss: 1.45963099e-06
Iter: 963 loss: 1.45540662e-06
Iter: 964 loss: 1.45534159e-06
Iter: 965 loss: 1.45532749e-06
Iter: 966 loss: 1.45519743e-06
Iter: 967 loss: 1.4549621e-06
Iter: 968 loss: 1.45861827e-06
Iter: 969 loss: 1.4549405e-06
Iter: 970 loss: 1.45470904e-06
Iter: 971 loss: 1.45649642e-06
Iter: 972 loss: 1.45470017e-06
Iter: 973 loss: 1.45450258e-06
Iter: 974 loss: 1.45509489e-06
Iter: 975 loss: 1.45444278e-06
Iter: 976 loss: 1.4542245e-06
Iter: 977 loss: 1.45487252e-06
Iter: 978 loss: 1.45420108e-06
Iter: 979 loss: 1.45397803e-06
Iter: 980 loss: 1.45447621e-06
Iter: 981 loss: 1.45392244e-06
Iter: 982 loss: 1.45380159e-06
Iter: 983 loss: 1.45441572e-06
Iter: 984 loss: 1.45379e-06
Iter: 985 loss: 1.45369074e-06
Iter: 986 loss: 1.45369063e-06
Iter: 987 loss: 1.45358661e-06
Iter: 988 loss: 1.45345484e-06
Iter: 989 loss: 1.45339845e-06
Iter: 990 loss: 1.45334479e-06
Iter: 991 loss: 1.45318086e-06
Iter: 992 loss: 1.45317586e-06
Iter: 993 loss: 1.45308377e-06
Iter: 994 loss: 1.45293279e-06
Iter: 995 loss: 1.45293825e-06
Iter: 996 loss: 1.45277158e-06
Iter: 997 loss: 1.45412184e-06
Iter: 998 loss: 1.45275931e-06
Iter: 999 loss: 1.45256786e-06
Iter: 1000 loss: 1.45272747e-06
Iter: 1001 loss: 1.45249146e-06
Iter: 1002 loss: 1.45233525e-06
Iter: 1003 loss: 1.4523664e-06
Iter: 1004 loss: 1.45225522e-06
Iter: 1005 loss: 1.452079e-06
Iter: 1006 loss: 1.45327419e-06
Iter: 1007 loss: 1.45205104e-06
Iter: 1008 loss: 1.4519112e-06
Iter: 1009 loss: 1.45243098e-06
Iter: 1010 loss: 1.45187175e-06
Iter: 1011 loss: 1.45173601e-06
Iter: 1012 loss: 1.45188312e-06
Iter: 1013 loss: 1.45167439e-06
Iter: 1014 loss: 1.45151819e-06
Iter: 1015 loss: 1.45167689e-06
Iter: 1016 loss: 1.45143918e-06
Iter: 1017 loss: 1.45126887e-06
Iter: 1018 loss: 1.45286629e-06
Iter: 1019 loss: 1.45126501e-06
Iter: 1020 loss: 1.45113404e-06
Iter: 1021 loss: 1.45091155e-06
Iter: 1022 loss: 1.4537444e-06
Iter: 1023 loss: 1.45087495e-06
Iter: 1024 loss: 1.45087608e-06
Iter: 1025 loss: 1.45076751e-06
Iter: 1026 loss: 1.45067088e-06
Iter: 1027 loss: 1.45053173e-06
Iter: 1028 loss: 1.45053491e-06
Iter: 1029 loss: 1.45038894e-06
Iter: 1030 loss: 1.45094259e-06
Iter: 1031 loss: 1.45035574e-06
Iter: 1032 loss: 1.45019442e-06
Iter: 1033 loss: 1.45146441e-06
Iter: 1034 loss: 1.45018953e-06
Iter: 1035 loss: 1.45008744e-06
Iter: 1036 loss: 1.44990736e-06
Iter: 1037 loss: 1.4528157e-06
Iter: 1038 loss: 1.44991532e-06
Iter: 1039 loss: 1.44971045e-06
Iter: 1040 loss: 1.45171828e-06
Iter: 1041 loss: 1.44970556e-06
Iter: 1042 loss: 1.44957801e-06
Iter: 1043 loss: 1.44985074e-06
Iter: 1044 loss: 1.44950832e-06
Iter: 1045 loss: 1.44938281e-06
Iter: 1046 loss: 1.44994158e-06
Iter: 1047 loss: 1.44935416e-06
Iter: 1048 loss: 1.44916748e-06
Iter: 1049 loss: 1.44925377e-06
Iter: 1050 loss: 1.44908131e-06
Iter: 1051 loss: 1.44895012e-06
Iter: 1052 loss: 1.4508895e-06
Iter: 1053 loss: 1.44894932e-06
Iter: 1054 loss: 1.44884859e-06
Iter: 1055 loss: 1.4487207e-06
Iter: 1056 loss: 1.44870398e-06
Iter: 1057 loss: 1.44861065e-06
Iter: 1058 loss: 1.44881255e-06
Iter: 1059 loss: 1.44853311e-06
Iter: 1060 loss: 1.44841783e-06
Iter: 1061 loss: 1.44843511e-06
Iter: 1062 loss: 1.4483478e-06
Iter: 1063 loss: 1.44816681e-06
Iter: 1064 loss: 1.44975957e-06
Iter: 1065 loss: 1.44813748e-06
Iter: 1066 loss: 1.44813021e-06
Iter: 1067 loss: 1.44802743e-06
Iter: 1068 loss: 1.44795638e-06
Iter: 1069 loss: 1.44778892e-06
Iter: 1070 loss: 1.45056424e-06
Iter: 1071 loss: 1.44778255e-06
Iter: 1072 loss: 1.44762066e-06
Iter: 1073 loss: 1.44851822e-06
Iter: 1074 loss: 1.44758292e-06
Iter: 1075 loss: 1.44745741e-06
Iter: 1076 loss: 1.44916555e-06
Iter: 1077 loss: 1.4474474e-06
Iter: 1078 loss: 1.44735463e-06
Iter: 1079 loss: 1.44737317e-06
Iter: 1080 loss: 1.44730188e-06
Iter: 1081 loss: 1.44716e-06
Iter: 1082 loss: 1.44737692e-06
Iter: 1083 loss: 1.44710316e-06
Iter: 1084 loss: 1.44696151e-06
Iter: 1085 loss: 1.44784383e-06
Iter: 1086 loss: 1.44696321e-06
Iter: 1087 loss: 1.44679802e-06
Iter: 1088 loss: 1.44682599e-06
Iter: 1089 loss: 1.44670821e-06
Iter: 1090 loss: 1.44659953e-06
Iter: 1091 loss: 1.44654223e-06
Iter: 1092 loss: 1.44648561e-06
Iter: 1093 loss: 1.44643195e-06
Iter: 1094 loss: 1.44638477e-06
Iter: 1095 loss: 1.44630292e-06
Iter: 1096 loss: 1.44624721e-06
Iter: 1097 loss: 1.44620662e-06
Iter: 1098 loss: 1.44610851e-06
Iter: 1099 loss: 1.44641194e-06
Iter: 1100 loss: 1.44605679e-06
Iter: 1101 loss: 1.44592138e-06
Iter: 1102 loss: 1.44672231e-06
Iter: 1103 loss: 1.44591377e-06
Iter: 1104 loss: 1.44581782e-06
Iter: 1105 loss: 1.44563364e-06
Iter: 1106 loss: 1.44708429e-06
Iter: 1107 loss: 1.44559544e-06
Iter: 1108 loss: 1.44550017e-06
Iter: 1109 loss: 1.44546425e-06
Iter: 1110 loss: 1.44536602e-06
Iter: 1111 loss: 1.44530304e-06
Iter: 1112 loss: 1.44525018e-06
Iter: 1113 loss: 1.44512956e-06
Iter: 1114 loss: 1.44586943e-06
Iter: 1115 loss: 1.44510773e-06
Iter: 1116 loss: 1.44500791e-06
Iter: 1117 loss: 1.44494925e-06
Iter: 1118 loss: 1.4448957e-06
Iter: 1119 loss: 1.44476462e-06
Iter: 1120 loss: 1.44685873e-06
Iter: 1121 loss: 1.44475598e-06
Iter: 1122 loss: 1.44469959e-06
Iter: 1123 loss: 1.44455771e-06
Iter: 1124 loss: 1.44458318e-06
Iter: 1125 loss: 1.44437445e-06
Iter: 1126 loss: 1.44447188e-06
Iter: 1127 loss: 1.44428259e-06
Iter: 1128 loss: 1.44415e-06
Iter: 1129 loss: 1.44417459e-06
Iter: 1130 loss: 1.44402918e-06
Iter: 1131 loss: 1.44442265e-06
Iter: 1132 loss: 1.44398098e-06
Iter: 1133 loss: 1.44393152e-06
Iter: 1134 loss: 1.44394312e-06
Iter: 1135 loss: 1.44387491e-06
Iter: 1136 loss: 1.44374758e-06
Iter: 1137 loss: 1.44493424e-06
Iter: 1138 loss: 1.44375531e-06
Iter: 1139 loss: 1.44373234e-06
Iter: 1140 loss: 1.44361991e-06
Iter: 1141 loss: 1.44449564e-06
Iter: 1142 loss: 1.44358364e-06
Iter: 1143 loss: 1.44343994e-06
Iter: 1144 loss: 1.44469891e-06
Iter: 1145 loss: 1.44342107e-06
Iter: 1146 loss: 1.4432494e-06
Iter: 1147 loss: 1.44359365e-06
Iter: 1148 loss: 1.4431821e-06
Iter: 1149 loss: 1.44306125e-06
Iter: 1150 loss: 1.4433e-06
Iter: 1151 loss: 1.44301475e-06
Iter: 1152 loss: 1.44286309e-06
Iter: 1153 loss: 1.44326714e-06
Iter: 1154 loss: 1.44284957e-06
Iter: 1155 loss: 1.44272849e-06
Iter: 1156 loss: 1.44311616e-06
Iter: 1157 loss: 1.44268802e-06
Iter: 1158 loss: 1.44255148e-06
Iter: 1159 loss: 1.4424603e-06
Iter: 1160 loss: 1.44242802e-06
Iter: 1161 loss: 1.44229875e-06
Iter: 1162 loss: 1.44272235e-06
Iter: 1163 loss: 1.44224373e-06
Iter: 1164 loss: 1.44216813e-06
Iter: 1165 loss: 1.44216915e-06
Iter: 1166 loss: 1.44209355e-06
Iter: 1167 loss: 1.44197907e-06
Iter: 1168 loss: 1.44475916e-06
Iter: 1169 loss: 1.44196713e-06
Iter: 1170 loss: 1.44191699e-06
Iter: 1171 loss: 1.44190312e-06
Iter: 1172 loss: 1.44183014e-06
Iter: 1173 loss: 1.44173544e-06
Iter: 1174 loss: 1.44174567e-06
Iter: 1175 loss: 1.44163903e-06
Iter: 1176 loss: 1.44147964e-06
Iter: 1177 loss: 1.44146281e-06
Iter: 1178 loss: 1.44132582e-06
Iter: 1179 loss: 1.44133423e-06
Iter: 1180 loss: 1.44120133e-06
Iter: 1181 loss: 1.44174578e-06
Iter: 1182 loss: 1.44118474e-06
Iter: 1183 loss: 1.44110447e-06
Iter: 1184 loss: 1.44096418e-06
Iter: 1185 loss: 1.44095543e-06
Iter: 1186 loss: 1.44083015e-06
Iter: 1187 loss: 1.44082821e-06
Iter: 1188 loss: 1.44072737e-06
Iter: 1189 loss: 1.44066234e-06
Iter: 1190 loss: 1.44063029e-06
Iter: 1191 loss: 1.44048624e-06
Iter: 1192 loss: 1.44076535e-06
Iter: 1193 loss: 1.44044225e-06
Iter: 1194 loss: 1.44028263e-06
Iter: 1195 loss: 1.44075966e-06
Iter: 1196 loss: 1.44023306e-06
Iter: 1197 loss: 1.44017577e-06
Iter: 1198 loss: 1.44018304e-06
Iter: 1199 loss: 1.44010846e-06
Iter: 1200 loss: 1.43998795e-06
Iter: 1201 loss: 1.44157889e-06
Iter: 1202 loss: 1.43998477e-06
Iter: 1203 loss: 1.4399036e-06
Iter: 1204 loss: 1.43989359e-06
Iter: 1205 loss: 1.43983868e-06
Iter: 1206 loss: 1.43969078e-06
Iter: 1207 loss: 1.44288447e-06
Iter: 1208 loss: 1.43971499e-06
Iter: 1209 loss: 1.43957368e-06
Iter: 1210 loss: 1.4400664e-06
Iter: 1211 loss: 1.43955367e-06
Iter: 1212 loss: 1.43943362e-06
Iter: 1213 loss: 1.43941941e-06
Iter: 1214 loss: 1.43935495e-06
Iter: 1215 loss: 1.43914372e-06
Iter: 1216 loss: 1.43956186e-06
Iter: 1217 loss: 1.43907664e-06
Iter: 1218 loss: 1.43893499e-06
Iter: 1219 loss: 1.43989757e-06
Iter: 1220 loss: 1.43890247e-06
Iter: 1221 loss: 1.43875502e-06
Iter: 1222 loss: 1.43887473e-06
Iter: 1223 loss: 1.438661e-06
Iter: 1224 loss: 1.43852958e-06
Iter: 1225 loss: 1.43874422e-06
Iter: 1226 loss: 1.43847046e-06
Iter: 1227 loss: 1.43829493e-06
Iter: 1228 loss: 1.43842965e-06
Iter: 1229 loss: 1.43817397e-06
Iter: 1230 loss: 1.43804596e-06
Iter: 1231 loss: 1.43805698e-06
Iter: 1232 loss: 1.43793318e-06
Iter: 1233 loss: 1.43804107e-06
Iter: 1234 loss: 1.43784962e-06
Iter: 1235 loss: 1.43772309e-06
Iter: 1236 loss: 1.4379201e-06
Iter: 1237 loss: 1.43770103e-06
Iter: 1238 loss: 1.4375446e-06
Iter: 1239 loss: 1.43806028e-06
Iter: 1240 loss: 1.43751072e-06
Iter: 1241 loss: 1.43739464e-06
Iter: 1242 loss: 1.4373602e-06
Iter: 1243 loss: 1.43730949e-06
Iter: 1244 loss: 1.43720433e-06
Iter: 1245 loss: 1.43862246e-06
Iter: 1246 loss: 1.43718421e-06
Iter: 1247 loss: 1.43709781e-06
Iter: 1248 loss: 1.4369773e-06
Iter: 1249 loss: 1.43695547e-06
Iter: 1250 loss: 1.43680268e-06
Iter: 1251 loss: 1.43740021e-06
Iter: 1252 loss: 1.43675925e-06
Iter: 1253 loss: 1.43660043e-06
Iter: 1254 loss: 1.43837178e-06
Iter: 1255 loss: 1.43660054e-06
Iter: 1256 loss: 1.43651346e-06
Iter: 1257 loss: 1.43636009e-06
Iter: 1258 loss: 1.43955799e-06
Iter: 1259 loss: 1.43634531e-06
Iter: 1260 loss: 1.43616717e-06
Iter: 1261 loss: 1.43748605e-06
Iter: 1262 loss: 1.43614727e-06
Iter: 1263 loss: 1.43600084e-06
Iter: 1264 loss: 1.43672207e-06
Iter: 1265 loss: 1.43595958e-06
Iter: 1266 loss: 1.4358111e-06
Iter: 1267 loss: 1.43612829e-06
Iter: 1268 loss: 1.43575096e-06
Iter: 1269 loss: 1.43557895e-06
Iter: 1270 loss: 1.43557895e-06
Iter: 1271 loss: 1.43550164e-06
Iter: 1272 loss: 1.43535317e-06
Iter: 1273 loss: 1.43533725e-06
Iter: 1274 loss: 1.43525608e-06
Iter: 1275 loss: 1.43510636e-06
Iter: 1276 loss: 1.43811315e-06
Iter: 1277 loss: 1.43506759e-06
Iter: 1278 loss: 1.43497584e-06
Iter: 1279 loss: 1.43496641e-06
Iter: 1280 loss: 1.43487682e-06
Iter: 1281 loss: 1.43469424e-06
Iter: 1282 loss: 1.43821535e-06
Iter: 1283 loss: 1.43469788e-06
Iter: 1284 loss: 1.43452712e-06
Iter: 1285 loss: 1.43545958e-06
Iter: 1286 loss: 1.4344912e-06
Iter: 1287 loss: 1.43436637e-06
Iter: 1288 loss: 1.43532725e-06
Iter: 1289 loss: 1.43432362e-06
Iter: 1290 loss: 1.43420596e-06
Iter: 1291 loss: 1.43412342e-06
Iter: 1292 loss: 1.43408488e-06
Iter: 1293 loss: 1.4338932e-06
Iter: 1294 loss: 1.43415662e-06
Iter: 1295 loss: 1.43379759e-06
Iter: 1296 loss: 1.43357329e-06
Iter: 1297 loss: 1.43497903e-06
Iter: 1298 loss: 1.43351235e-06
Iter: 1299 loss: 1.43334762e-06
Iter: 1300 loss: 1.43464672e-06
Iter: 1301 loss: 1.43333318e-06
Iter: 1302 loss: 1.43320358e-06
Iter: 1303 loss: 1.43310388e-06
Iter: 1304 loss: 1.43306534e-06
Iter: 1305 loss: 1.43295688e-06
Iter: 1306 loss: 1.43295733e-06
Iter: 1307 loss: 1.43284092e-06
Iter: 1308 loss: 1.43264026e-06
Iter: 1309 loss: 1.43510579e-06
Iter: 1310 loss: 1.43259945e-06
Iter: 1311 loss: 1.43249065e-06
Iter: 1312 loss: 1.4324653e-06
Iter: 1313 loss: 1.43236502e-06
Iter: 1314 loss: 1.43226191e-06
Iter: 1315 loss: 1.43222792e-06
Iter: 1316 loss: 1.43205443e-06
Iter: 1317 loss: 1.43237889e-06
Iter: 1318 loss: 1.43204124e-06
Iter: 1319 loss: 1.43189345e-06
Iter: 1320 loss: 1.4337096e-06
Iter: 1321 loss: 1.43190255e-06
Iter: 1322 loss: 1.43177374e-06
Iter: 1323 loss: 1.43172065e-06
Iter: 1324 loss: 1.43166517e-06
Iter: 1325 loss: 1.4315076e-06
Iter: 1326 loss: 1.43153284e-06
Iter: 1327 loss: 1.4313996e-06
Iter: 1328 loss: 1.43117188e-06
Iter: 1329 loss: 1.43233808e-06
Iter: 1330 loss: 1.43112879e-06
Iter: 1331 loss: 1.43093007e-06
Iter: 1332 loss: 1.43320835e-06
Iter: 1333 loss: 1.43095201e-06
Iter: 1334 loss: 1.43078591e-06
Iter: 1335 loss: 1.43075897e-06
Iter: 1336 loss: 1.43066904e-06
Iter: 1337 loss: 1.43052853e-06
Iter: 1338 loss: 1.4318706e-06
Iter: 1339 loss: 1.4305092e-06
Iter: 1340 loss: 1.43035095e-06
Iter: 1341 loss: 1.43041825e-06
Iter: 1342 loss: 1.43024045e-06
Iter: 1343 loss: 1.43011289e-06
Iter: 1344 loss: 1.43009379e-06
Iter: 1345 loss: 1.43000693e-06
Iter: 1346 loss: 1.42974e-06
Iter: 1347 loss: 1.43124623e-06
Iter: 1348 loss: 1.42972135e-06
Iter: 1349 loss: 1.42961028e-06
Iter: 1350 loss: 1.42950648e-06
Iter: 1351 loss: 1.42948386e-06
Iter: 1352 loss: 1.4293604e-06
Iter: 1353 loss: 1.43111515e-06
Iter: 1354 loss: 1.42933698e-06
Iter: 1355 loss: 1.42918952e-06
Iter: 1356 loss: 1.42924318e-06
Iter: 1357 loss: 1.42911017e-06
Iter: 1358 loss: 1.42895522e-06
Iter: 1359 loss: 1.42905378e-06
Iter: 1360 loss: 1.42887734e-06
Iter: 1361 loss: 1.42870272e-06
Iter: 1362 loss: 1.42938006e-06
Iter: 1363 loss: 1.42870363e-06
Iter: 1364 loss: 1.42854628e-06
Iter: 1365 loss: 1.42999556e-06
Iter: 1366 loss: 1.42855617e-06
Iter: 1367 loss: 1.42841191e-06
Iter: 1368 loss: 1.42845215e-06
Iter: 1369 loss: 1.42832778e-06
Iter: 1370 loss: 1.42820022e-06
Iter: 1371 loss: 1.42869408e-06
Iter: 1372 loss: 1.42818476e-06
Iter: 1373 loss: 1.42805686e-06
Iter: 1374 loss: 1.42841884e-06
Iter: 1375 loss: 1.42801127e-06
Iter: 1376 loss: 1.42787962e-06
Iter: 1377 loss: 1.4276194e-06
Iter: 1378 loss: 1.43261093e-06
Iter: 1379 loss: 1.4276435e-06
Iter: 1380 loss: 1.42750491e-06
Iter: 1381 loss: 1.42746762e-06
Iter: 1382 loss: 1.42735746e-06
Iter: 1383 loss: 1.42714498e-06
Iter: 1384 loss: 1.43056411e-06
Iter: 1385 loss: 1.4271377e-06
Iter: 1386 loss: 1.42695399e-06
Iter: 1387 loss: 1.42923227e-06
Iter: 1388 loss: 1.42696331e-06
Iter: 1389 loss: 1.42678732e-06
Iter: 1390 loss: 1.42708177e-06
Iter: 1391 loss: 1.42670569e-06
Iter: 1392 loss: 1.42656177e-06
Iter: 1393 loss: 1.42658473e-06
Iter: 1394 loss: 1.4264607e-06
Iter: 1395 loss: 1.42626345e-06
Iter: 1396 loss: 1.42666204e-06
Iter: 1397 loss: 1.4261874e-06
Iter: 1398 loss: 1.42599606e-06
Iter: 1399 loss: 1.42817839e-06
Iter: 1400 loss: 1.42600857e-06
Iter: 1401 loss: 1.42584486e-06
Iter: 1402 loss: 1.42619933e-06
Iter: 1403 loss: 1.42576096e-06
Iter: 1404 loss: 1.42564841e-06
Iter: 1405 loss: 1.42574731e-06
Iter: 1406 loss: 1.42556564e-06
Iter: 1407 loss: 1.4254299e-06
Iter: 1408 loss: 1.42741715e-06
Iter: 1409 loss: 1.42543081e-06
Iter: 1410 loss: 1.42534191e-06
Iter: 1411 loss: 1.42510396e-06
Iter: 1412 loss: 1.42915974e-06
Iter: 1413 loss: 1.42510055e-06
Iter: 1414 loss: 1.42494605e-06
Iter: 1415 loss: 1.42692579e-06
Iter: 1416 loss: 1.42496037e-06
Iter: 1417 loss: 1.42477666e-06
Iter: 1418 loss: 1.42461533e-06
Iter: 1419 loss: 1.42457907e-06
Iter: 1420 loss: 1.42439762e-06
Iter: 1421 loss: 1.42500448e-06
Iter: 1422 loss: 1.4243725e-06
Iter: 1423 loss: 1.42417446e-06
Iter: 1424 loss: 1.42499312e-06
Iter: 1425 loss: 1.42411591e-06
Iter: 1426 loss: 1.42398494e-06
Iter: 1427 loss: 1.42377667e-06
Iter: 1428 loss: 1.4237653e-06
Iter: 1429 loss: 1.423502e-06
Iter: 1430 loss: 1.42432759e-06
Iter: 1431 loss: 1.42343742e-06
Iter: 1432 loss: 1.42321858e-06
Iter: 1433 loss: 1.42477631e-06
Iter: 1434 loss: 1.42323802e-06
Iter: 1435 loss: 1.42304941e-06
Iter: 1436 loss: 1.42497311e-06
Iter: 1437 loss: 1.42304918e-06
Iter: 1438 loss: 1.42294323e-06
Iter: 1439 loss: 1.42288832e-06
Iter: 1440 loss: 1.42282363e-06
Iter: 1441 loss: 1.42271665e-06
Iter: 1442 loss: 1.42419663e-06
Iter: 1443 loss: 1.42271449e-06
Iter: 1444 loss: 1.42262797e-06
Iter: 1445 loss: 1.42251452e-06
Iter: 1446 loss: 1.42249246e-06
Iter: 1447 loss: 1.42234398e-06
Iter: 1448 loss: 1.42230783e-06
Iter: 1449 loss: 1.42221768e-06
Iter: 1450 loss: 1.42206807e-06
Iter: 1451 loss: 1.42205e-06
Iter: 1452 loss: 1.42194131e-06
Iter: 1453 loss: 1.42165959e-06
Iter: 1454 loss: 1.42527722e-06
Iter: 1455 loss: 1.42166641e-06
Iter: 1456 loss: 1.42146678e-06
Iter: 1457 loss: 1.42146496e-06
Iter: 1458 loss: 1.42125612e-06
Iter: 1459 loss: 1.4212593e-06
Iter: 1460 loss: 1.42109923e-06
Iter: 1461 loss: 1.42090494e-06
Iter: 1462 loss: 1.4207717e-06
Iter: 1463 loss: 1.42069348e-06
Iter: 1464 loss: 1.42047475e-06
Iter: 1465 loss: 1.42284523e-06
Iter: 1466 loss: 1.42044087e-06
Iter: 1467 loss: 1.42027511e-06
Iter: 1468 loss: 1.42151771e-06
Iter: 1469 loss: 1.42024805e-06
Iter: 1470 loss: 1.42006809e-06
Iter: 1471 loss: 1.42091653e-06
Iter: 1472 loss: 1.42003591e-06
Iter: 1473 loss: 1.41994883e-06
Iter: 1474 loss: 1.42012823e-06
Iter: 1475 loss: 1.41989756e-06
Iter: 1476 loss: 1.41977876e-06
Iter: 1477 loss: 1.4201529e-06
Iter: 1478 loss: 1.41973976e-06
Iter: 1479 loss: 1.41963528e-06
Iter: 1480 loss: 1.41952171e-06
Iter: 1481 loss: 1.41949249e-06
Iter: 1482 loss: 1.41933219e-06
Iter: 1483 loss: 1.42021213e-06
Iter: 1484 loss: 1.41928967e-06
Iter: 1485 loss: 1.4191487e-06
Iter: 1486 loss: 1.42004944e-06
Iter: 1487 loss: 1.41912096e-06
Iter: 1488 loss: 1.41903297e-06
Iter: 1489 loss: 1.4188721e-06
Iter: 1490 loss: 1.41886619e-06
Iter: 1491 loss: 1.41866349e-06
Iter: 1492 loss: 1.421344e-06
Iter: 1493 loss: 1.41866326e-06
Iter: 1494 loss: 1.41858982e-06
Iter: 1495 loss: 1.41842622e-06
Iter: 1496 loss: 1.41841633e-06
Iter: 1497 loss: 1.41819146e-06
Iter: 1498 loss: 1.41822181e-06
Iter: 1499 loss: 1.4180539e-06
Iter: 1500 loss: 1.41782664e-06
Iter: 1501 loss: 1.42025578e-06
Iter: 1502 loss: 1.41781652e-06
Iter: 1503 loss: 1.41771284e-06
Iter: 1504 loss: 1.41770579e-06
Iter: 1505 loss: 1.41762109e-06
Iter: 1506 loss: 1.41753969e-06
Iter: 1507 loss: 1.41752275e-06
Iter: 1508 loss: 1.41741702e-06
Iter: 1509 loss: 1.41861028e-06
Iter: 1510 loss: 1.41742521e-06
Iter: 1511 loss: 1.417318e-06
Iter: 1512 loss: 1.41735029e-06
Iter: 1513 loss: 1.4172499e-06
Iter: 1514 loss: 1.4171228e-06
Iter: 1515 loss: 1.4170181e-06
Iter: 1516 loss: 1.41699115e-06
Iter: 1517 loss: 1.41688452e-06
Iter: 1518 loss: 1.41686405e-06
Iter: 1519 loss: 1.41675582e-06
Iter: 1520 loss: 1.41658677e-06
Iter: 1521 loss: 1.41659802e-06
Iter: 1522 loss: 1.416381e-06
Iter: 1523 loss: 1.41722887e-06
Iter: 1524 loss: 1.41636463e-06
Iter: 1525 loss: 1.41618716e-06
Iter: 1526 loss: 1.41722376e-06
Iter: 1527 loss: 1.41614794e-06
Iter: 1528 loss: 1.41603243e-06
Iter: 1529 loss: 1.41585986e-06
Iter: 1530 loss: 1.41584871e-06
Iter: 1531 loss: 1.41561281e-06
Iter: 1532 loss: 1.4156933e-06
Iter: 1533 loss: 1.41549594e-06
Iter: 1534 loss: 1.41536441e-06
Iter: 1535 loss: 1.41534497e-06
Iter: 1536 loss: 1.41524015e-06
Iter: 1537 loss: 1.41626253e-06
Iter: 1538 loss: 1.41522196e-06
Iter: 1539 loss: 1.41512328e-06
Iter: 1540 loss: 1.41509531e-06
Iter: 1541 loss: 1.41503347e-06
Iter: 1542 loss: 1.41492967e-06
Iter: 1543 loss: 1.41629539e-06
Iter: 1544 loss: 1.41492092e-06
Iter: 1545 loss: 1.4148336e-06
Iter: 1546 loss: 1.41469422e-06
Iter: 1547 loss: 1.41470468e-06
Iter: 1548 loss: 1.41456076e-06
Iter: 1549 loss: 1.41495275e-06
Iter: 1550 loss: 1.41448641e-06
Iter: 1551 loss: 1.41434941e-06
Iter: 1552 loss: 1.41609064e-06
Iter: 1553 loss: 1.41434555e-06
Iter: 1554 loss: 1.4142347e-06
Iter: 1555 loss: 1.41407281e-06
Iter: 1556 loss: 1.41405383e-06
Iter: 1557 loss: 1.41394298e-06
Iter: 1558 loss: 1.41394162e-06
Iter: 1559 loss: 1.41383407e-06
Iter: 1560 loss: 1.41376279e-06
Iter: 1561 loss: 1.41373528e-06
Iter: 1562 loss: 1.41358e-06
Iter: 1563 loss: 1.41373971e-06
Iter: 1564 loss: 1.41350438e-06
Iter: 1565 loss: 1.41337114e-06
Iter: 1566 loss: 1.41343128e-06
Iter: 1567 loss: 1.41323767e-06
Iter: 1568 loss: 1.41307169e-06
Iter: 1569 loss: 1.41340263e-06
Iter: 1570 loss: 1.41297323e-06
Iter: 1571 loss: 1.4128716e-06
Iter: 1572 loss: 1.41286819e-06
Iter: 1573 loss: 1.41273881e-06
Iter: 1574 loss: 1.4131324e-06
Iter: 1575 loss: 1.41271221e-06
Iter: 1576 loss: 1.41262967e-06
Iter: 1577 loss: 1.41271585e-06
Iter: 1578 loss: 1.41261592e-06
Iter: 1579 loss: 1.4124696e-06
Iter: 1580 loss: 1.41256646e-06
Iter: 1581 loss: 1.41238911e-06
Iter: 1582 loss: 1.41231067e-06
Iter: 1583 loss: 1.41230021e-06
Iter: 1584 loss: 1.4122204e-06
Iter: 1585 loss: 1.41211274e-06
Iter: 1586 loss: 1.41323426e-06
Iter: 1587 loss: 1.41209057e-06
Iter: 1588 loss: 1.41200292e-06
Iter: 1589 loss: 1.41240344e-06
Iter: 1590 loss: 1.41197756e-06
Iter: 1591 loss: 1.41187331e-06
Iter: 1592 loss: 1.41178202e-06
Iter: 1593 loss: 1.41177429e-06
Iter: 1594 loss: 1.41166333e-06
Iter: 1595 loss: 1.41165822e-06
Iter: 1596 loss: 1.41158466e-06
Iter: 1597 loss: 1.41148064e-06
Iter: 1598 loss: 1.41148712e-06
Iter: 1599 loss: 1.41136707e-06
Iter: 1600 loss: 1.41153032e-06
Iter: 1601 loss: 1.4112934e-06
Iter: 1602 loss: 1.41115947e-06
Iter: 1603 loss: 1.41109808e-06
Iter: 1604 loss: 1.41101987e-06
Iter: 1605 loss: 1.41083751e-06
Iter: 1606 loss: 1.41189082e-06
Iter: 1607 loss: 1.41080272e-06
Iter: 1608 loss: 1.41065061e-06
Iter: 1609 loss: 1.41216333e-06
Iter: 1610 loss: 1.41064811e-06
Iter: 1611 loss: 1.41054193e-06
Iter: 1612 loss: 1.41219732e-06
Iter: 1613 loss: 1.41055193e-06
Iter: 1614 loss: 1.4105018e-06
Iter: 1615 loss: 1.41039527e-06
Iter: 1616 loss: 1.41226326e-06
Iter: 1617 loss: 1.41040823e-06
Iter: 1618 loss: 1.41029398e-06
Iter: 1619 loss: 1.41030227e-06
Iter: 1620 loss: 1.41022178e-06
Iter: 1621 loss: 1.41006547e-06
Iter: 1622 loss: 1.41157227e-06
Iter: 1623 loss: 1.41003682e-06
Iter: 1624 loss: 1.41000476e-06
Iter: 1625 loss: 1.40996713e-06
Iter: 1626 loss: 1.40990778e-06
Iter: 1627 loss: 1.40996849e-06
Iter: 1628 loss: 1.40989255e-06
Iter: 1629 loss: 1.40979148e-06
Iter: 1630 loss: 1.40993052e-06
Iter: 1631 loss: 1.40974112e-06
Iter: 1632 loss: 1.40966404e-06
Iter: 1633 loss: 1.41011799e-06
Iter: 1634 loss: 1.40965062e-06
Iter: 1635 loss: 1.40959799e-06
Iter: 1636 loss: 1.40956513e-06
Iter: 1637 loss: 1.40953114e-06
Iter: 1638 loss: 1.40942325e-06
Iter: 1639 loss: 1.40933696e-06
Iter: 1640 loss: 1.40930331e-06
Iter: 1641 loss: 1.40913608e-06
Iter: 1642 loss: 1.40966813e-06
Iter: 1643 loss: 1.40908469e-06
Iter: 1644 loss: 1.40888119e-06
Iter: 1645 loss: 1.40905229e-06
Iter: 1646 loss: 1.40879831e-06
Iter: 1647 loss: 1.40861243e-06
Iter: 1648 loss: 1.40987e-06
Iter: 1649 loss: 1.40859584e-06
Iter: 1650 loss: 1.40845304e-06
Iter: 1651 loss: 1.40843076e-06
Iter: 1652 loss: 1.40837597e-06
Iter: 1653 loss: 1.40824704e-06
Iter: 1654 loss: 1.40969553e-06
Iter: 1655 loss: 1.40824147e-06
Iter: 1656 loss: 1.40813631e-06
Iter: 1657 loss: 1.40813336e-06
Iter: 1658 loss: 1.40806094e-06
Iter: 1659 loss: 1.40795555e-06
Iter: 1660 loss: 1.40996406e-06
Iter: 1661 loss: 1.40794441e-06
Iter: 1662 loss: 1.40783754e-06
Iter: 1663 loss: 1.4080922e-06
Iter: 1664 loss: 1.40776206e-06
Iter: 1665 loss: 1.40770658e-06
Iter: 1666 loss: 1.40771772e-06
Iter: 1667 loss: 1.40764382e-06
Iter: 1668 loss: 1.40754059e-06
Iter: 1669 loss: 1.407529e-06
Iter: 1670 loss: 1.40743657e-06
Iter: 1671 loss: 1.40843781e-06
Iter: 1672 loss: 1.40742975e-06
Iter: 1673 loss: 1.40735665e-06
Iter: 1674 loss: 1.40759334e-06
Iter: 1675 loss: 1.40731186e-06
Iter: 1676 loss: 1.40722886e-06
Iter: 1677 loss: 1.40709199e-06
Iter: 1678 loss: 1.40708369e-06
Iter: 1679 loss: 1.40692805e-06
Iter: 1680 loss: 1.40706015e-06
Iter: 1681 loss: 1.40685268e-06
Iter: 1682 loss: 1.40665895e-06
Iter: 1683 loss: 1.40693e-06
Iter: 1684 loss: 1.40656209e-06
Iter: 1685 loss: 1.40657437e-06
Iter: 1686 loss: 1.4065032e-06
Iter: 1687 loss: 1.40640839e-06
Iter: 1688 loss: 1.40626116e-06
Iter: 1689 loss: 1.4062573e-06
Iter: 1690 loss: 1.40616567e-06
Iter: 1691 loss: 1.40680186e-06
Iter: 1692 loss: 1.40616089e-06
Iter: 1693 loss: 1.40604675e-06
Iter: 1694 loss: 1.40683039e-06
Iter: 1695 loss: 1.406038e-06
Iter: 1696 loss: 1.40597854e-06
Iter: 1697 loss: 1.40584962e-06
Iter: 1698 loss: 1.40718657e-06
Iter: 1699 loss: 1.40583518e-06
Iter: 1700 loss: 1.4057407e-06
Iter: 1701 loss: 1.40656493e-06
Iter: 1702 loss: 1.40574343e-06
Iter: 1703 loss: 1.4056177e-06
Iter: 1704 loss: 1.40666896e-06
Iter: 1705 loss: 1.4056294e-06
Iter: 1706 loss: 1.40556199e-06
Iter: 1707 loss: 1.40541385e-06
Iter: 1708 loss: 1.40720181e-06
Iter: 1709 loss: 1.40542681e-06
Iter: 1710 loss: 1.40523048e-06
Iter: 1711 loss: 1.4069235e-06
Iter: 1712 loss: 1.40522593e-06
Iter: 1713 loss: 1.40507746e-06
Iter: 1714 loss: 1.40566817e-06
Iter: 1715 loss: 1.40504119e-06
Iter: 1716 loss: 1.40491079e-06
Iter: 1717 loss: 1.40498105e-06
Iter: 1718 loss: 1.40485781e-06
Iter: 1719 loss: 1.40469183e-06
Iter: 1720 loss: 1.40477118e-06
Iter: 1721 loss: 1.404616e-06
Iter: 1722 loss: 1.40454085e-06
Iter: 1723 loss: 1.4045429e-06
Iter: 1724 loss: 1.40444843e-06
Iter: 1725 loss: 1.40440852e-06
Iter: 1726 loss: 1.4043502e-06
Iter: 1727 loss: 1.40429222e-06
Iter: 1728 loss: 1.40427414e-06
Iter: 1729 loss: 1.40423856e-06
Iter: 1730 loss: 1.40411294e-06
Iter: 1731 loss: 1.40544307e-06
Iter: 1732 loss: 1.40410759e-06
Iter: 1733 loss: 1.40406178e-06
Iter: 1734 loss: 1.40391262e-06
Iter: 1735 loss: 1.40536247e-06
Iter: 1736 loss: 1.40390921e-06
Iter: 1737 loss: 1.40377824e-06
Iter: 1738 loss: 1.40515567e-06
Iter: 1739 loss: 1.4037721e-06
Iter: 1740 loss: 1.40367501e-06
Iter: 1741 loss: 1.40459929e-06
Iter: 1742 loss: 1.40368252e-06
Iter: 1743 loss: 1.40358111e-06
Iter: 1744 loss: 1.40342718e-06
Iter: 1745 loss: 1.40626889e-06
Iter: 1746 loss: 1.40344378e-06
Iter: 1747 loss: 1.40338307e-06
Iter: 1748 loss: 1.40338079e-06
Iter: 1749 loss: 1.4032812e-06
Iter: 1750 loss: 1.4032405e-06
Iter: 1751 loss: 1.40321799e-06
Iter: 1752 loss: 1.40310112e-06
Iter: 1753 loss: 1.40315547e-06
Iter: 1754 loss: 1.40300779e-06
Iter: 1755 loss: 1.40285351e-06
Iter: 1756 loss: 1.40348106e-06
Iter: 1757 loss: 1.40283146e-06
Iter: 1758 loss: 1.40270743e-06
Iter: 1759 loss: 1.40393342e-06
Iter: 1760 loss: 1.4027047e-06
Iter: 1761 loss: 1.40258328e-06
Iter: 1762 loss: 1.40261841e-06
Iter: 1763 loss: 1.4024788e-06
Iter: 1764 loss: 1.40240809e-06
Iter: 1765 loss: 1.40250472e-06
Iter: 1766 loss: 1.40236057e-06
Iter: 1767 loss: 1.4022819e-06
Iter: 1768 loss: 1.40327938e-06
Iter: 1769 loss: 1.40229758e-06
Iter: 1770 loss: 1.40222778e-06
Iter: 1771 loss: 1.40213251e-06
Iter: 1772 loss: 1.40213922e-06
Iter: 1773 loss: 1.40206157e-06
Iter: 1774 loss: 1.40285431e-06
Iter: 1775 loss: 1.40205975e-06
Iter: 1776 loss: 1.40196016e-06
Iter: 1777 loss: 1.40199268e-06
Iter: 1778 loss: 1.40188831e-06
Iter: 1779 loss: 1.40181794e-06
Iter: 1780 loss: 1.40174484e-06
Iter: 1781 loss: 1.401708e-06
Iter: 1782 loss: 1.40156271e-06
Iter: 1783 loss: 1.40367092e-06
Iter: 1784 loss: 1.4015493e-06
Iter: 1785 loss: 1.40149041e-06
Iter: 1786 loss: 1.40139286e-06
Iter: 1787 loss: 1.40139355e-06
Iter: 1788 loss: 1.40130362e-06
Iter: 1789 loss: 1.40133136e-06
Iter: 1790 loss: 1.40121097e-06
Iter: 1791 loss: 1.40110819e-06
Iter: 1792 loss: 1.40127145e-06
Iter: 1793 loss: 1.40106636e-06
Iter: 1794 loss: 1.40099371e-06
Iter: 1795 loss: 1.40097518e-06
Iter: 1796 loss: 1.40091959e-06
Iter: 1797 loss: 1.40095221e-06
Iter: 1798 loss: 1.40086649e-06
Iter: 1799 loss: 1.40079248e-06
Iter: 1800 loss: 1.40074314e-06
Iter: 1801 loss: 1.40072848e-06
Iter: 1802 loss: 1.40063526e-06
Iter: 1803 loss: 1.40065026e-06
Iter: 1804 loss: 1.40058478e-06
Iter: 1805 loss: 1.40040584e-06
Iter: 1806 loss: 1.40214e-06
Iter: 1807 loss: 1.40043369e-06
Iter: 1808 loss: 1.40029579e-06
Iter: 1809 loss: 1.40043721e-06
Iter: 1810 loss: 1.40017892e-06
Iter: 1811 loss: 1.40001362e-06
Iter: 1812 loss: 1.40097484e-06
Iter: 1813 loss: 1.40001885e-06
Iter: 1814 loss: 1.39995177e-06
Iter: 1815 loss: 1.39995689e-06
Iter: 1816 loss: 1.39987765e-06
Iter: 1817 loss: 1.39978e-06
Iter: 1818 loss: 1.4022736e-06
Iter: 1819 loss: 1.39976714e-06
Iter: 1820 loss: 1.39967779e-06
Iter: 1821 loss: 1.40085081e-06
Iter: 1822 loss: 1.39966892e-06
Iter: 1823 loss: 1.39960594e-06
Iter: 1824 loss: 1.39956251e-06
Iter: 1825 loss: 1.39953806e-06
Iter: 1826 loss: 1.39945155e-06
Iter: 1827 loss: 1.39973872e-06
Iter: 1828 loss: 1.39943188e-06
Iter: 1829 loss: 1.39935719e-06
Iter: 1830 loss: 1.40036616e-06
Iter: 1831 loss: 1.39935298e-06
Iter: 1832 loss: 1.39930614e-06
Iter: 1833 loss: 1.39923247e-06
Iter: 1834 loss: 1.39926306e-06
Iter: 1835 loss: 1.39918689e-06
Iter: 1836 loss: 1.39927056e-06
Iter: 1837 loss: 1.39915289e-06
Iter: 1838 loss: 1.39908036e-06
Iter: 1839 loss: 1.3997726e-06
Iter: 1840 loss: 1.39908047e-06
Iter: 1841 loss: 1.39898373e-06
Iter: 1842 loss: 1.39929807e-06
Iter: 1843 loss: 1.39897838e-06
Iter: 1844 loss: 1.39893882e-06
Iter: 1845 loss: 1.39878182e-06
Iter: 1846 loss: 1.4004637e-06
Iter: 1847 loss: 1.39880308e-06
Iter: 1848 loss: 1.3986579e-06
Iter: 1849 loss: 1.39905137e-06
Iter: 1850 loss: 1.39861504e-06
Iter: 1851 loss: 1.39850738e-06
Iter: 1852 loss: 1.39850681e-06
Iter: 1853 loss: 1.39843519e-06
Iter: 1854 loss: 1.39865119e-06
Iter: 1855 loss: 1.39839631e-06
Iter: 1856 loss: 1.39835265e-06
Iter: 1857 loss: 1.3982642e-06
Iter: 1858 loss: 1.39827671e-06
Iter: 1859 loss: 1.3981969e-06
Iter: 1860 loss: 1.39817803e-06
Iter: 1861 loss: 1.39814142e-06
Iter: 1862 loss: 1.39805525e-06
Iter: 1863 loss: 1.39980011e-06
Iter: 1864 loss: 1.39803763e-06
Iter: 1865 loss: 1.39799283e-06
Iter: 1866 loss: 1.39798703e-06
Iter: 1867 loss: 1.39792678e-06
Iter: 1868 loss: 1.39784868e-06
Iter: 1869 loss: 1.3978372e-06
Iter: 1870 loss: 1.39778342e-06
Iter: 1871 loss: 1.39783936e-06
Iter: 1872 loss: 1.39773181e-06
Iter: 1873 loss: 1.39769463e-06
Iter: 1874 loss: 1.39769827e-06
Iter: 1875 loss: 1.39763461e-06
Iter: 1876 loss: 1.39757e-06
Iter: 1877 loss: 1.39756798e-06
Iter: 1878 loss: 1.3975116e-06
Iter: 1879 loss: 1.39747794e-06
Iter: 1880 loss: 1.39745327e-06
Iter: 1881 loss: 1.3973571e-06
Iter: 1882 loss: 1.39775148e-06
Iter: 1883 loss: 1.39737858e-06
Iter: 1884 loss: 1.39728024e-06
Iter: 1885 loss: 1.39755537e-06
Iter: 1886 loss: 1.39725501e-06
Iter: 1887 loss: 1.39718668e-06
Iter: 1888 loss: 1.39734379e-06
Iter: 1889 loss: 1.39713256e-06
Iter: 1890 loss: 1.39709402e-06
Iter: 1891 loss: 1.39723352e-06
Iter: 1892 loss: 1.39706731e-06
Iter: 1893 loss: 1.39701e-06
Iter: 1894 loss: 1.39749159e-06
Iter: 1895 loss: 1.39699932e-06
Iter: 1896 loss: 1.39695771e-06
Iter: 1897 loss: 1.39695135e-06
Iter: 1898 loss: 1.39692361e-06
Iter: 1899 loss: 1.39686449e-06
Iter: 1900 loss: 1.39709914e-06
Iter: 1901 loss: 1.39685801e-06
Iter: 1902 loss: 1.39682083e-06
Iter: 1903 loss: 1.39729241e-06
Iter: 1904 loss: 1.39682311e-06
Iter: 1905 loss: 1.39678036e-06
Iter: 1906 loss: 1.39671658e-06
Iter: 1907 loss: 1.39709641e-06
Iter: 1908 loss: 1.39669692e-06
Iter: 1909 loss: 1.39664951e-06
Iter: 1910 loss: 1.39665849e-06
Iter: 1911 loss: 1.39660244e-06
Iter: 1912 loss: 1.39657527e-06
Iter: 1913 loss: 1.39655162e-06
Iter: 1914 loss: 1.39650126e-06
Iter: 1915 loss: 1.39646409e-06
Iter: 1916 loss: 1.39643362e-06
Iter: 1917 loss: 1.39635324e-06
Iter: 1918 loss: 1.39713609e-06
Iter: 1919 loss: 1.39634722e-06
Iter: 1920 loss: 1.39626195e-06
Iter: 1921 loss: 1.39644214e-06
Iter: 1922 loss: 1.39621784e-06
Iter: 1923 loss: 1.3961502e-06
Iter: 1924 loss: 1.39622875e-06
Iter: 1925 loss: 1.39612462e-06
Iter: 1926 loss: 1.39605118e-06
Iter: 1927 loss: 1.39634153e-06
Iter: 1928 loss: 1.39604231e-06
Iter: 1929 loss: 1.39600456e-06
Iter: 1930 loss: 1.39635597e-06
Iter: 1931 loss: 1.39598365e-06
Iter: 1932 loss: 1.39593658e-06
Iter: 1933 loss: 1.39591555e-06
Iter: 1934 loss: 1.3958869e-06
Iter: 1935 loss: 1.3958479e-06
Iter: 1936 loss: 1.39620124e-06
Iter: 1937 loss: 1.39583653e-06
Iter: 1938 loss: 1.39580504e-06
Iter: 1939 loss: 1.39595863e-06
Iter: 1940 loss: 1.39576377e-06
Iter: 1941 loss: 1.39573763e-06
Iter: 1942 loss: 1.39563258e-06
Iter: 1943 loss: 1.39654662e-06
Iter: 1944 loss: 1.39562076e-06
Iter: 1945 loss: 1.39559143e-06
Iter: 1946 loss: 1.3955646e-06
Iter: 1947 loss: 1.39550161e-06
Iter: 1948 loss: 1.39540271e-06
Iter: 1949 loss: 1.39733629e-06
Iter: 1950 loss: 1.39540452e-06
Iter: 1951 loss: 1.39531403e-06
Iter: 1952 loss: 1.39543238e-06
Iter: 1953 loss: 1.39524934e-06
Iter: 1954 loss: 1.39520375e-06
Iter: 1955 loss: 1.39519534e-06
Iter: 1956 loss: 1.39514168e-06
Iter: 1957 loss: 1.39517851e-06
Iter: 1958 loss: 1.39511815e-06
Iter: 1959 loss: 1.39504539e-06
Iter: 1960 loss: 1.39502663e-06
Iter: 1961 loss: 1.39499889e-06
Iter: 1962 loss: 1.39491897e-06
Iter: 1963 loss: 1.39566203e-06
Iter: 1964 loss: 1.39492658e-06
Iter: 1965 loss: 1.3948561e-06
Iter: 1966 loss: 1.39515714e-06
Iter: 1967 loss: 1.39487781e-06
Iter: 1968 loss: 1.3948079e-06
Iter: 1969 loss: 1.39471842e-06
Iter: 1970 loss: 1.3947282e-06
Iter: 1971 loss: 1.39465396e-06
Iter: 1972 loss: 1.39464578e-06
Iter: 1973 loss: 1.39460167e-06
Iter: 1974 loss: 1.39461895e-06
Iter: 1975 loss: 1.39457234e-06
Iter: 1976 loss: 1.39450572e-06
Iter: 1977 loss: 1.39444137e-06
Iter: 1978 loss: 1.39443955e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi1.6 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi1.6
+ date
Sat Oct 31 21:29:21 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 0 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29245c8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29245d89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29246168c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29245270d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29245d8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29245d86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29245d8268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2924427a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2924427620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2924327048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29243278c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f292440af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f292440aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29241ea158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29241e9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29242a3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29242a3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2924166ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2924188d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2924166268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29240dd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29243cb1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29240ddf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2924131f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2924156620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f292437bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f292437b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f291c7d41e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f291c7b80d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f291c7b8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f291c78a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f291c7b8e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f291c7b8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f291c73f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29240ce158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29242dbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0054119593
test_loss: 0.005668353
train_loss: 0.0032056188
test_loss: 0.0031014546
train_loss: 0.002388528
test_loss: 0.0025933087
train_loss: 0.0025266143
test_loss: 0.0021372067
train_loss: 0.0020082227
test_loss: 0.0020094404
train_loss: 0.002049012
test_loss: 0.0017644399
train_loss: 0.0015099059
test_loss: 0.0015669006
train_loss: 0.0013909871
test_loss: 0.0016486204
train_loss: 0.0016023514
test_loss: 0.0015360329
train_loss: 0.0013789773
test_loss: 0.0014161915
train_loss: 0.0014425942
test_loss: 0.0013451268
train_loss: 0.001268127
test_loss: 0.0012895401
train_loss: 0.0012496099
test_loss: 0.0012564233
train_loss: 0.0011944054
test_loss: 0.0012973542
train_loss: 0.0011884919
test_loss: 0.0012312226
train_loss: 0.0011350857
test_loss: 0.0011935126
train_loss: 0.0010850076
test_loss: 0.0011549578
train_loss: 0.0011138027
test_loss: 0.0011529465
train_loss: 0.0011096309
test_loss: 0.001133767
train_loss: 0.0010843782
test_loss: 0.0010839246
train_loss: 0.0010575005
test_loss: 0.001088661
train_loss: 0.0010486465
test_loss: 0.0010581024
train_loss: 0.0010197236
test_loss: 0.001051933
train_loss: 0.0010211118
test_loss: 0.0010579823
train_loss: 0.0010167068
test_loss: 0.0010481064
train_loss: 0.0009995828
test_loss: 0.0010534811
train_loss: 0.0010048641
test_loss: 0.0010350244
train_loss: 0.0009587712
test_loss: 0.0010283635
train_loss: 0.0009844806
test_loss: 0.0010140408
train_loss: 0.00094424235
test_loss: 0.0010100082
train_loss: 0.00092488003
test_loss: 0.0010106098
train_loss: 0.0009724874
test_loss: 0.001003233
train_loss: 0.000956916
test_loss: 0.0010048448
train_loss: 0.0009010535
test_loss: 0.0009940693
train_loss: 0.000959937
test_loss: 0.0010016181
train_loss: 0.0009332891
test_loss: 0.0009869501
train_loss: 0.00095385197
test_loss: 0.0009871982
train_loss: 0.0009203674
test_loss: 0.0009830005
train_loss: 0.0009029668
test_loss: 0.0009819025
train_loss: 0.00094298134
test_loss: 0.0009795477
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d52f1ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d52ee0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d52ee3b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d52e87730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d52e87840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d52e87048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d52e872f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4ccd6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4ccd6158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cc87048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cc962f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cc96488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cb79f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cc57510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cbecf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cbfbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cc2f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cbc5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cc87950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4ca9e048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4ca9e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4ca877b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cb639d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cb32158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cb446a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4cb44f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4ca40d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4ca407b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4c9db158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4c9962f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4c9857b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4c8d8ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4c8816a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4c9961e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4c881a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4c916d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.60676075e-06
Iter: 2 loss: 1.5888703e-06
Iter: 3 loss: 1.85862e-06
Iter: 4 loss: 1.5888462e-06
Iter: 5 loss: 1.5801611e-06
Iter: 6 loss: 1.70425869e-06
Iter: 7 loss: 1.58014177e-06
Iter: 8 loss: 1.57604086e-06
Iter: 9 loss: 1.5834114e-06
Iter: 10 loss: 1.57424381e-06
Iter: 11 loss: 1.57033207e-06
Iter: 12 loss: 1.5798023e-06
Iter: 13 loss: 1.56888882e-06
Iter: 14 loss: 1.56638839e-06
Iter: 15 loss: 1.56446129e-06
Iter: 16 loss: 1.56367662e-06
Iter: 17 loss: 1.56099475e-06
Iter: 18 loss: 1.60138279e-06
Iter: 19 loss: 1.56100714e-06
Iter: 20 loss: 1.55908879e-06
Iter: 21 loss: 1.56593455e-06
Iter: 22 loss: 1.55862199e-06
Iter: 23 loss: 1.55670591e-06
Iter: 24 loss: 1.55318685e-06
Iter: 25 loss: 1.63535492e-06
Iter: 26 loss: 1.55318412e-06
Iter: 27 loss: 1.55065322e-06
Iter: 28 loss: 1.58985722e-06
Iter: 29 loss: 1.55067301e-06
Iter: 30 loss: 1.54844793e-06
Iter: 31 loss: 1.55800035e-06
Iter: 32 loss: 1.54801023e-06
Iter: 33 loss: 1.54646023e-06
Iter: 34 loss: 1.54337158e-06
Iter: 35 loss: 1.6012923e-06
Iter: 36 loss: 1.54335203e-06
Iter: 37 loss: 1.541046e-06
Iter: 38 loss: 1.57269722e-06
Iter: 39 loss: 1.54103748e-06
Iter: 40 loss: 1.53937935e-06
Iter: 41 loss: 1.53985297e-06
Iter: 42 loss: 1.53814472e-06
Iter: 43 loss: 1.5379685e-06
Iter: 44 loss: 1.53735891e-06
Iter: 45 loss: 1.53647693e-06
Iter: 46 loss: 1.5370947e-06
Iter: 47 loss: 1.53594408e-06
Iter: 48 loss: 1.53543704e-06
Iter: 49 loss: 1.53742042e-06
Iter: 50 loss: 1.53533063e-06
Iter: 51 loss: 1.53466806e-06
Iter: 52 loss: 1.53364044e-06
Iter: 53 loss: 1.53363408e-06
Iter: 54 loss: 1.5326741e-06
Iter: 55 loss: 1.53697363e-06
Iter: 56 loss: 1.53250846e-06
Iter: 57 loss: 1.53174472e-06
Iter: 58 loss: 1.53394853e-06
Iter: 59 loss: 1.53153644e-06
Iter: 60 loss: 1.53036774e-06
Iter: 61 loss: 1.53082931e-06
Iter: 62 loss: 1.52953965e-06
Iter: 63 loss: 1.52868972e-06
Iter: 64 loss: 1.52899452e-06
Iter: 65 loss: 1.5280699e-06
Iter: 66 loss: 1.5267899e-06
Iter: 67 loss: 1.53534245e-06
Iter: 68 loss: 1.52662426e-06
Iter: 69 loss: 1.52571556e-06
Iter: 70 loss: 1.53056749e-06
Iter: 71 loss: 1.52559426e-06
Iter: 72 loss: 1.52481539e-06
Iter: 73 loss: 1.52375117e-06
Iter: 74 loss: 1.52369319e-06
Iter: 75 loss: 1.52272673e-06
Iter: 76 loss: 1.53167775e-06
Iter: 77 loss: 1.52266966e-06
Iter: 78 loss: 1.52181894e-06
Iter: 79 loss: 1.52119367e-06
Iter: 80 loss: 1.52091354e-06
Iter: 81 loss: 1.52025109e-06
Iter: 82 loss: 1.52023017e-06
Iter: 83 loss: 1.51934353e-06
Iter: 84 loss: 1.52083783e-06
Iter: 85 loss: 1.51895904e-06
Iter: 86 loss: 1.51855204e-06
Iter: 87 loss: 1.51742483e-06
Iter: 88 loss: 1.52733617e-06
Iter: 89 loss: 1.51725726e-06
Iter: 90 loss: 1.51730023e-06
Iter: 91 loss: 1.51672452e-06
Iter: 92 loss: 1.51631752e-06
Iter: 93 loss: 1.5158148e-06
Iter: 94 loss: 1.51575296e-06
Iter: 95 loss: 1.51502775e-06
Iter: 96 loss: 1.51712288e-06
Iter: 97 loss: 1.51481015e-06
Iter: 98 loss: 1.51421852e-06
Iter: 99 loss: 1.52180519e-06
Iter: 100 loss: 1.51421955e-06
Iter: 101 loss: 1.51381755e-06
Iter: 102 loss: 1.51323093e-06
Iter: 103 loss: 1.5132066e-06
Iter: 104 loss: 1.51283257e-06
Iter: 105 loss: 1.51280244e-06
Iter: 106 loss: 1.51246059e-06
Iter: 107 loss: 1.5118261e-06
Iter: 108 loss: 1.51184713e-06
Iter: 109 loss: 1.51138738e-06
Iter: 110 loss: 1.51476047e-06
Iter: 111 loss: 1.51136578e-06
Iter: 112 loss: 1.5109772e-06
Iter: 113 loss: 1.51110839e-06
Iter: 114 loss: 1.51069821e-06
Iter: 115 loss: 1.51028439e-06
Iter: 116 loss: 1.51471886e-06
Iter: 117 loss: 1.51026939e-06
Iter: 118 loss: 1.50988353e-06
Iter: 119 loss: 1.51296751e-06
Iter: 120 loss: 1.50985477e-06
Iter: 121 loss: 1.50968697e-06
Iter: 122 loss: 1.50923529e-06
Iter: 123 loss: 1.51266136e-06
Iter: 124 loss: 1.50913911e-06
Iter: 125 loss: 1.50854839e-06
Iter: 126 loss: 1.51130735e-06
Iter: 127 loss: 1.50847211e-06
Iter: 128 loss: 1.50799769e-06
Iter: 129 loss: 1.5080077e-06
Iter: 130 loss: 1.50769984e-06
Iter: 131 loss: 1.50689402e-06
Iter: 132 loss: 1.51151619e-06
Iter: 133 loss: 1.50666256e-06
Iter: 134 loss: 1.50620849e-06
Iter: 135 loss: 1.50615426e-06
Iter: 136 loss: 1.50565779e-06
Iter: 137 loss: 1.50699293e-06
Iter: 138 loss: 1.50549567e-06
Iter: 139 loss: 1.50514563e-06
Iter: 140 loss: 1.50520725e-06
Iter: 141 loss: 1.50489404e-06
Iter: 142 loss: 1.50442111e-06
Iter: 143 loss: 1.50405663e-06
Iter: 144 loss: 1.50388632e-06
Iter: 145 loss: 1.50412416e-06
Iter: 146 loss: 1.50367362e-06
Iter: 147 loss: 1.5034816e-06
Iter: 148 loss: 1.50296023e-06
Iter: 149 loss: 1.50570509e-06
Iter: 150 loss: 1.50279732e-06
Iter: 151 loss: 1.50217556e-06
Iter: 152 loss: 1.50747519e-06
Iter: 153 loss: 1.50213373e-06
Iter: 154 loss: 1.50175333e-06
Iter: 155 loss: 1.50634082e-06
Iter: 156 loss: 1.50175504e-06
Iter: 157 loss: 1.50128608e-06
Iter: 158 loss: 1.50171843e-06
Iter: 159 loss: 1.50101039e-06
Iter: 160 loss: 1.50069104e-06
Iter: 161 loss: 1.50042615e-06
Iter: 162 loss: 1.50034487e-06
Iter: 163 loss: 1.49985453e-06
Iter: 164 loss: 1.50406038e-06
Iter: 165 loss: 1.49981975e-06
Iter: 166 loss: 1.49946459e-06
Iter: 167 loss: 1.50090045e-06
Iter: 168 loss: 1.49935704e-06
Iter: 169 loss: 1.49915786e-06
Iter: 170 loss: 1.49880373e-06
Iter: 171 loss: 1.498801e-06
Iter: 172 loss: 1.49839366e-06
Iter: 173 loss: 1.49937739e-06
Iter: 174 loss: 1.49825439e-06
Iter: 175 loss: 1.49772654e-06
Iter: 176 loss: 1.49889684e-06
Iter: 177 loss: 1.49753964e-06
Iter: 178 loss: 1.49694699e-06
Iter: 179 loss: 1.49740322e-06
Iter: 180 loss: 1.49660218e-06
Iter: 181 loss: 1.49652385e-06
Iter: 182 loss: 1.4962759e-06
Iter: 183 loss: 1.4960417e-06
Iter: 184 loss: 1.49556854e-06
Iter: 185 loss: 1.50544656e-06
Iter: 186 loss: 1.49556445e-06
Iter: 187 loss: 1.49506013e-06
Iter: 188 loss: 1.49626385e-06
Iter: 189 loss: 1.4948638e-06
Iter: 190 loss: 1.49453103e-06
Iter: 191 loss: 1.49452035e-06
Iter: 192 loss: 1.4942575e-06
Iter: 193 loss: 1.49418474e-06
Iter: 194 loss: 1.49402308e-06
Iter: 195 loss: 1.49380253e-06
Iter: 196 loss: 1.49378434e-06
Iter: 197 loss: 1.49357084e-06
Iter: 198 loss: 1.49308516e-06
Iter: 199 loss: 1.5002073e-06
Iter: 200 loss: 1.49307289e-06
Iter: 201 loss: 1.49276593e-06
Iter: 202 loss: 1.4922532e-06
Iter: 203 loss: 1.49224411e-06
Iter: 204 loss: 1.49174377e-06
Iter: 205 loss: 1.49175548e-06
Iter: 206 loss: 1.49132666e-06
Iter: 207 loss: 1.49421817e-06
Iter: 208 loss: 1.49129801e-06
Iter: 209 loss: 1.49093444e-06
Iter: 210 loss: 1.49068478e-06
Iter: 211 loss: 1.49055177e-06
Iter: 212 loss: 1.49010202e-06
Iter: 213 loss: 1.49048878e-06
Iter: 214 loss: 1.48985339e-06
Iter: 215 loss: 1.48918866e-06
Iter: 216 loss: 1.49128857e-06
Iter: 217 loss: 1.48898744e-06
Iter: 218 loss: 1.48863046e-06
Iter: 219 loss: 1.48860295e-06
Iter: 220 loss: 1.48839831e-06
Iter: 221 loss: 1.48772233e-06
Iter: 222 loss: 1.49062703e-06
Iter: 223 loss: 1.48748518e-06
Iter: 224 loss: 1.48725724e-06
Iter: 225 loss: 1.48711399e-06
Iter: 226 loss: 1.48674542e-06
Iter: 227 loss: 1.48726986e-06
Iter: 228 loss: 1.48660104e-06
Iter: 229 loss: 1.48640174e-06
Iter: 230 loss: 1.48642084e-06
Iter: 231 loss: 1.48621859e-06
Iter: 232 loss: 1.48594472e-06
Iter: 233 loss: 1.48594313e-06
Iter: 234 loss: 1.48562276e-06
Iter: 235 loss: 1.48541358e-06
Iter: 236 loss: 1.48530557e-06
Iter: 237 loss: 1.48493268e-06
Iter: 238 loss: 1.48573395e-06
Iter: 239 loss: 1.48477272e-06
Iter: 240 loss: 1.48432991e-06
Iter: 241 loss: 1.49008679e-06
Iter: 242 loss: 1.48432662e-06
Iter: 243 loss: 1.48408765e-06
Iter: 244 loss: 1.4837558e-06
Iter: 245 loss: 1.48374124e-06
Iter: 246 loss: 1.48325171e-06
Iter: 247 loss: 1.4850134e-06
Iter: 248 loss: 1.48311256e-06
Iter: 249 loss: 1.48275444e-06
Iter: 250 loss: 1.48582126e-06
Iter: 251 loss: 1.48272932e-06
Iter: 252 loss: 1.48238928e-06
Iter: 253 loss: 1.48320203e-06
Iter: 254 loss: 1.48225695e-06
Iter: 255 loss: 1.48197591e-06
Iter: 256 loss: 1.48248898e-06
Iter: 257 loss: 1.48186859e-06
Iter: 258 loss: 1.48165054e-06
Iter: 259 loss: 1.48154459e-06
Iter: 260 loss: 1.48143022e-06
Iter: 261 loss: 1.48115873e-06
Iter: 262 loss: 1.48116442e-06
Iter: 263 loss: 1.48099275e-06
Iter: 264 loss: 1.48193271e-06
Iter: 265 loss: 1.48098434e-06
Iter: 266 loss: 1.48082836e-06
Iter: 267 loss: 1.48063555e-06
Iter: 268 loss: 1.48061145e-06
Iter: 269 loss: 1.48037066e-06
Iter: 270 loss: 1.48004574e-06
Iter: 271 loss: 1.48003585e-06
Iter: 272 loss: 1.47965579e-06
Iter: 273 loss: 1.48387835e-06
Iter: 274 loss: 1.47963146e-06
Iter: 275 loss: 1.47937294e-06
Iter: 276 loss: 1.48219556e-06
Iter: 277 loss: 1.47938727e-06
Iter: 278 loss: 1.47915148e-06
Iter: 279 loss: 1.47847504e-06
Iter: 280 loss: 1.48112304e-06
Iter: 281 loss: 1.47815786e-06
Iter: 282 loss: 1.47762216e-06
Iter: 283 loss: 1.47762705e-06
Iter: 284 loss: 1.47727485e-06
Iter: 285 loss: 1.48168692e-06
Iter: 286 loss: 1.47729043e-06
Iter: 287 loss: 1.47701689e-06
Iter: 288 loss: 1.47742765e-06
Iter: 289 loss: 1.47690764e-06
Iter: 290 loss: 1.47662774e-06
Iter: 291 loss: 1.4763848e-06
Iter: 292 loss: 1.47631954e-06
Iter: 293 loss: 1.4759986e-06
Iter: 294 loss: 1.479812e-06
Iter: 295 loss: 1.47600247e-06
Iter: 296 loss: 1.47586559e-06
Iter: 297 loss: 1.47584296e-06
Iter: 298 loss: 1.47569017e-06
Iter: 299 loss: 1.47561025e-06
Iter: 300 loss: 1.47554533e-06
Iter: 301 loss: 1.47527498e-06
Iter: 302 loss: 1.47568926e-06
Iter: 303 loss: 1.47514663e-06
Iter: 304 loss: 1.47491824e-06
Iter: 305 loss: 1.47430831e-06
Iter: 306 loss: 1.47987214e-06
Iter: 307 loss: 1.47420155e-06
Iter: 308 loss: 1.47369678e-06
Iter: 309 loss: 1.47369565e-06
Iter: 310 loss: 1.47318747e-06
Iter: 311 loss: 1.47521075e-06
Iter: 312 loss: 1.47306e-06
Iter: 313 loss: 1.47276728e-06
Iter: 314 loss: 1.47295373e-06
Iter: 315 loss: 1.47257379e-06
Iter: 316 loss: 1.47224705e-06
Iter: 317 loss: 1.4720473e-06
Iter: 318 loss: 1.47190701e-06
Iter: 319 loss: 1.47189337e-06
Iter: 320 loss: 1.47172659e-06
Iter: 321 loss: 1.47156038e-06
Iter: 322 loss: 1.47132585e-06
Iter: 323 loss: 1.47133471e-06
Iter: 324 loss: 1.47109563e-06
Iter: 325 loss: 1.47154879e-06
Iter: 326 loss: 1.47096318e-06
Iter: 327 loss: 1.47064e-06
Iter: 328 loss: 1.47189485e-06
Iter: 329 loss: 1.47055471e-06
Iter: 330 loss: 1.47034132e-06
Iter: 331 loss: 1.47034291e-06
Iter: 332 loss: 1.47019932e-06
Iter: 333 loss: 1.47002766e-06
Iter: 334 loss: 1.47003107e-06
Iter: 335 loss: 1.4696725e-06
Iter: 336 loss: 1.4698112e-06
Iter: 337 loss: 1.46944535e-06
Iter: 338 loss: 1.46912544e-06
Iter: 339 loss: 1.46912487e-06
Iter: 340 loss: 1.46882303e-06
Iter: 341 loss: 1.46849663e-06
Iter: 342 loss: 1.47176047e-06
Iter: 343 loss: 1.46849766e-06
Iter: 344 loss: 1.46826335e-06
Iter: 345 loss: 1.46827392e-06
Iter: 346 loss: 1.4681616e-06
Iter: 347 loss: 1.46782304e-06
Iter: 348 loss: 1.46930756e-06
Iter: 349 loss: 1.46767525e-06
Iter: 350 loss: 1.46733055e-06
Iter: 351 loss: 1.47068863e-06
Iter: 352 loss: 1.46731941e-06
Iter: 353 loss: 1.46709795e-06
Iter: 354 loss: 1.46998309e-06
Iter: 355 loss: 1.46707441e-06
Iter: 356 loss: 1.46683374e-06
Iter: 357 loss: 1.46688296e-06
Iter: 358 loss: 1.46664809e-06
Iter: 359 loss: 1.46642196e-06
Iter: 360 loss: 1.46655816e-06
Iter: 361 loss: 1.46628076e-06
Iter: 362 loss: 1.46611387e-06
Iter: 363 loss: 1.46611342e-06
Iter: 364 loss: 1.46591901e-06
Iter: 365 loss: 1.46613809e-06
Iter: 366 loss: 1.46585251e-06
Iter: 367 loss: 1.46564662e-06
Iter: 368 loss: 1.46573632e-06
Iter: 369 loss: 1.46551542e-06
Iter: 370 loss: 1.4653217e-06
Iter: 371 loss: 1.46645903e-06
Iter: 372 loss: 1.46527668e-06
Iter: 373 loss: 1.46511934e-06
Iter: 374 loss: 1.46477237e-06
Iter: 375 loss: 1.46998673e-06
Iter: 376 loss: 1.46474395e-06
Iter: 377 loss: 1.46437014e-06
Iter: 378 loss: 1.46806451e-06
Iter: 379 loss: 1.46436435e-06
Iter: 380 loss: 1.46415e-06
Iter: 381 loss: 1.46415232e-06
Iter: 382 loss: 1.46397952e-06
Iter: 383 loss: 1.46370087e-06
Iter: 384 loss: 1.46369439e-06
Iter: 385 loss: 1.46347088e-06
Iter: 386 loss: 1.46382536e-06
Iter: 387 loss: 1.4633797e-06
Iter: 388 loss: 1.46326602e-06
Iter: 389 loss: 1.46324953e-06
Iter: 390 loss: 1.46310731e-06
Iter: 391 loss: 1.46294713e-06
Iter: 392 loss: 1.46292132e-06
Iter: 393 loss: 1.46272237e-06
Iter: 394 loss: 1.4630524e-06
Iter: 395 loss: 1.46263642e-06
Iter: 396 loss: 1.46250954e-06
Iter: 397 loss: 1.46246384e-06
Iter: 398 loss: 1.46233788e-06
Iter: 399 loss: 1.46213e-06
Iter: 400 loss: 1.46215507e-06
Iter: 401 loss: 1.46193929e-06
Iter: 402 loss: 1.46302182e-06
Iter: 403 loss: 1.4619286e-06
Iter: 404 loss: 1.46172283e-06
Iter: 405 loss: 1.46145953e-06
Iter: 406 loss: 1.46143043e-06
Iter: 407 loss: 1.46109051e-06
Iter: 408 loss: 1.46304865e-06
Iter: 409 loss: 1.46102479e-06
Iter: 410 loss: 1.46078105e-06
Iter: 411 loss: 1.46080015e-06
Iter: 412 loss: 1.46061598e-06
Iter: 413 loss: 1.46051548e-06
Iter: 414 loss: 1.46043408e-06
Iter: 415 loss: 1.46032892e-06
Iter: 416 loss: 1.46019272e-06
Iter: 417 loss: 1.4640774e-06
Iter: 418 loss: 1.46017749e-06
Iter: 419 loss: 1.45997626e-06
Iter: 420 loss: 1.46047557e-06
Iter: 421 loss: 1.45991612e-06
Iter: 422 loss: 1.45970762e-06
Iter: 423 loss: 1.45984689e-06
Iter: 424 loss: 1.45960371e-06
Iter: 425 loss: 1.45948e-06
Iter: 426 loss: 1.45943409e-06
Iter: 427 loss: 1.45936485e-06
Iter: 428 loss: 1.45911531e-06
Iter: 429 loss: 1.46080163e-06
Iter: 430 loss: 1.45903596e-06
Iter: 431 loss: 1.45893364e-06
Iter: 432 loss: 1.45887043e-06
Iter: 433 loss: 1.45871093e-06
Iter: 434 loss: 1.45865556e-06
Iter: 435 loss: 1.4585595e-06
Iter: 436 loss: 1.4583751e-06
Iter: 437 loss: 1.45855029e-06
Iter: 438 loss: 1.45826243e-06
Iter: 439 loss: 1.45802812e-06
Iter: 440 loss: 1.45857621e-06
Iter: 441 loss: 1.45794525e-06
Iter: 442 loss: 1.45773924e-06
Iter: 443 loss: 1.45783429e-06
Iter: 444 loss: 1.45759157e-06
Iter: 445 loss: 1.45736192e-06
Iter: 446 loss: 1.45855893e-06
Iter: 447 loss: 1.45735783e-06
Iter: 448 loss: 1.45713636e-06
Iter: 449 loss: 1.45909621e-06
Iter: 450 loss: 1.45713966e-06
Iter: 451 loss: 1.45695924e-06
Iter: 452 loss: 1.45688728e-06
Iter: 453 loss: 1.45680838e-06
Iter: 454 loss: 1.456576e-06
Iter: 455 loss: 1.45667195e-06
Iter: 456 loss: 1.45643457e-06
Iter: 457 loss: 1.45609454e-06
Iter: 458 loss: 1.45669037e-06
Iter: 459 loss: 1.45599279e-06
Iter: 460 loss: 1.45570914e-06
Iter: 461 loss: 1.45746765e-06
Iter: 462 loss: 1.45567765e-06
Iter: 463 loss: 1.45541833e-06
Iter: 464 loss: 1.45796639e-06
Iter: 465 loss: 1.45538e-06
Iter: 466 loss: 1.4552561e-06
Iter: 467 loss: 1.4549413e-06
Iter: 468 loss: 1.45932222e-06
Iter: 469 loss: 1.45493868e-06
Iter: 470 loss: 1.45482977e-06
Iter: 471 loss: 1.45475587e-06
Iter: 472 loss: 1.45454669e-06
Iter: 473 loss: 1.45419517e-06
Iter: 474 loss: 1.46138268e-06
Iter: 475 loss: 1.45420302e-06
Iter: 476 loss: 1.45395165e-06
Iter: 477 loss: 1.45663171e-06
Iter: 478 loss: 1.4539645e-06
Iter: 479 loss: 1.45378522e-06
Iter: 480 loss: 1.45404351e-06
Iter: 481 loss: 1.45366198e-06
Iter: 482 loss: 1.45351555e-06
Iter: 483 loss: 1.45329579e-06
Iter: 484 loss: 1.45330068e-06
Iter: 485 loss: 1.45322952e-06
Iter: 486 loss: 1.45313402e-06
Iter: 487 loss: 1.45304057e-06
Iter: 488 loss: 1.45285208e-06
Iter: 489 loss: 1.4528473e-06
Iter: 490 loss: 1.45259537e-06
Iter: 491 loss: 1.45295689e-06
Iter: 492 loss: 1.45245906e-06
Iter: 493 loss: 1.45222e-06
Iter: 494 loss: 1.45222634e-06
Iter: 495 loss: 1.4520366e-06
Iter: 496 loss: 1.45184868e-06
Iter: 497 loss: 1.45184197e-06
Iter: 498 loss: 1.45166132e-06
Iter: 499 loss: 1.45162812e-06
Iter: 500 loss: 1.45149795e-06
Iter: 501 loss: 1.45124432e-06
Iter: 502 loss: 1.45112506e-06
Iter: 503 loss: 1.45102842e-06
Iter: 504 loss: 1.45106094e-06
Iter: 505 loss: 1.45092588e-06
Iter: 506 loss: 1.450779e-06
Iter: 507 loss: 1.45051786e-06
Iter: 508 loss: 1.4543798e-06
Iter: 509 loss: 1.45047272e-06
Iter: 510 loss: 1.45026456e-06
Iter: 511 loss: 1.45122112e-06
Iter: 512 loss: 1.45020226e-06
Iter: 513 loss: 1.45000377e-06
Iter: 514 loss: 1.45131833e-06
Iter: 515 loss: 1.44997762e-06
Iter: 516 loss: 1.44982607e-06
Iter: 517 loss: 1.44972159e-06
Iter: 518 loss: 1.44964906e-06
Iter: 519 loss: 1.44949274e-06
Iter: 520 loss: 1.45161607e-06
Iter: 521 loss: 1.44949558e-06
Iter: 522 loss: 1.44934711e-06
Iter: 523 loss: 1.4493512e-06
Iter: 524 loss: 1.44919068e-06
Iter: 525 loss: 1.44906699e-06
Iter: 526 loss: 1.44943692e-06
Iter: 527 loss: 1.44901014e-06
Iter: 528 loss: 1.44881801e-06
Iter: 529 loss: 1.44873047e-06
Iter: 530 loss: 1.44865578e-06
Iter: 531 loss: 1.44847763e-06
Iter: 532 loss: 1.44846922e-06
Iter: 533 loss: 1.44831574e-06
Iter: 534 loss: 1.44813089e-06
Iter: 535 loss: 1.44809701e-06
Iter: 536 loss: 1.44786668e-06
Iter: 537 loss: 1.44823048e-06
Iter: 538 loss: 1.44774356e-06
Iter: 539 loss: 1.44767864e-06
Iter: 540 loss: 1.44764158e-06
Iter: 541 loss: 1.44747355e-06
Iter: 542 loss: 1.44724254e-06
Iter: 543 loss: 1.45141348e-06
Iter: 544 loss: 1.44721537e-06
Iter: 545 loss: 1.44709338e-06
Iter: 546 loss: 1.44741318e-06
Iter: 547 loss: 1.44702631e-06
Iter: 548 loss: 1.44684032e-06
Iter: 549 loss: 1.44840078e-06
Iter: 550 loss: 1.44684964e-06
Iter: 551 loss: 1.44673663e-06
Iter: 552 loss: 1.44659964e-06
Iter: 553 loss: 1.44659577e-06
Iter: 554 loss: 1.44647868e-06
Iter: 555 loss: 1.44834291e-06
Iter: 556 loss: 1.44646538e-06
Iter: 557 loss: 1.44632702e-06
Iter: 558 loss: 1.4461441e-06
Iter: 559 loss: 1.44612898e-06
Iter: 560 loss: 1.4458825e-06
Iter: 561 loss: 1.44620049e-06
Iter: 562 loss: 1.44576313e-06
Iter: 563 loss: 1.44551632e-06
Iter: 564 loss: 1.44616183e-06
Iter: 565 loss: 1.4454165e-06
Iter: 566 loss: 1.44521766e-06
Iter: 567 loss: 1.44522505e-06
Iter: 568 loss: 1.44507771e-06
Iter: 569 loss: 1.44493015e-06
Iter: 570 loss: 1.44494174e-06
Iter: 571 loss: 1.44472335e-06
Iter: 572 loss: 1.44568901e-06
Iter: 573 loss: 1.44464934e-06
Iter: 574 loss: 1.44456715e-06
Iter: 575 loss: 1.44456362e-06
Iter: 576 loss: 1.4444779e-06
Iter: 577 loss: 1.444281e-06
Iter: 578 loss: 1.44862838e-06
Iter: 579 loss: 1.44427781e-06
Iter: 580 loss: 1.44414298e-06
Iter: 581 loss: 1.44416845e-06
Iter: 582 loss: 1.44403441e-06
Iter: 583 loss: 1.44389276e-06
Iter: 584 loss: 1.44389446e-06
Iter: 585 loss: 1.44380658e-06
Iter: 586 loss: 1.44371211e-06
Iter: 587 loss: 1.44367255e-06
Iter: 588 loss: 1.44346859e-06
Iter: 589 loss: 1.44340947e-06
Iter: 590 loss: 1.44331102e-06
Iter: 591 loss: 1.44303522e-06
Iter: 592 loss: 1.44359069e-06
Iter: 593 loss: 1.44293404e-06
Iter: 594 loss: 1.44286469e-06
Iter: 595 loss: 1.44281569e-06
Iter: 596 loss: 1.44270371e-06
Iter: 597 loss: 1.44275714e-06
Iter: 598 loss: 1.44264231e-06
Iter: 599 loss: 1.44253556e-06
Iter: 600 loss: 1.44257194e-06
Iter: 601 loss: 1.44247088e-06
Iter: 602 loss: 1.44232956e-06
Iter: 603 loss: 1.44369324e-06
Iter: 604 loss: 1.44231331e-06
Iter: 605 loss: 1.44221428e-06
Iter: 606 loss: 1.44213436e-06
Iter: 607 loss: 1.44211901e-06
Iter: 608 loss: 1.44207934e-06
Iter: 609 loss: 1.44205524e-06
Iter: 610 loss: 1.44195928e-06
Iter: 611 loss: 1.4417966e-06
Iter: 612 loss: 1.44427258e-06
Iter: 613 loss: 1.44181104e-06
Iter: 614 loss: 1.44165676e-06
Iter: 615 loss: 1.44185901e-06
Iter: 616 loss: 1.44158798e-06
Iter: 617 loss: 1.44140631e-06
Iter: 618 loss: 1.44180581e-06
Iter: 619 loss: 1.44132514e-06
Iter: 620 loss: 1.4411587e-06
Iter: 621 loss: 1.44298269e-06
Iter: 622 loss: 1.4411612e-06
Iter: 623 loss: 1.44105547e-06
Iter: 624 loss: 1.4408256e-06
Iter: 625 loss: 1.44427713e-06
Iter: 626 loss: 1.44083037e-06
Iter: 627 loss: 1.4406628e-06
Iter: 628 loss: 1.44292051e-06
Iter: 629 loss: 1.44065734e-06
Iter: 630 loss: 1.44050614e-06
Iter: 631 loss: 1.44131582e-06
Iter: 632 loss: 1.44049625e-06
Iter: 633 loss: 1.44032549e-06
Iter: 634 loss: 1.44033561e-06
Iter: 635 loss: 1.44021033e-06
Iter: 636 loss: 1.44008675e-06
Iter: 637 loss: 1.44014143e-06
Iter: 638 loss: 1.44001206e-06
Iter: 639 loss: 1.43989473e-06
Iter: 640 loss: 1.43988768e-06
Iter: 641 loss: 1.43983129e-06
Iter: 642 loss: 1.43975626e-06
Iter: 643 loss: 1.43973386e-06
Iter: 644 loss: 1.439622e-06
Iter: 645 loss: 1.44093974e-06
Iter: 646 loss: 1.43961211e-06
Iter: 647 loss: 1.43957323e-06
Iter: 648 loss: 1.43942066e-06
Iter: 649 loss: 1.44228056e-06
Iter: 650 loss: 1.43941361e-06
Iter: 651 loss: 1.43930163e-06
Iter: 652 loss: 1.43934926e-06
Iter: 653 loss: 1.43920192e-06
Iter: 654 loss: 1.43900547e-06
Iter: 655 loss: 1.44039973e-06
Iter: 656 loss: 1.4389916e-06
Iter: 657 loss: 1.43882482e-06
Iter: 658 loss: 1.43971579e-06
Iter: 659 loss: 1.43882312e-06
Iter: 660 loss: 1.43872546e-06
Iter: 661 loss: 1.4385497e-06
Iter: 662 loss: 1.43855391e-06
Iter: 663 loss: 1.43836746e-06
Iter: 664 loss: 1.43993043e-06
Iter: 665 loss: 1.43835155e-06
Iter: 666 loss: 1.43822047e-06
Iter: 667 loss: 1.44028695e-06
Iter: 668 loss: 1.43822331e-06
Iter: 669 loss: 1.43814168e-06
Iter: 670 loss: 1.438033e-06
Iter: 671 loss: 1.43802913e-06
Iter: 672 loss: 1.43792204e-06
Iter: 673 loss: 1.43857847e-06
Iter: 674 loss: 1.43790032e-06
Iter: 675 loss: 1.43779641e-06
Iter: 676 loss: 1.43844909e-06
Iter: 677 loss: 1.43776026e-06
Iter: 678 loss: 1.43769637e-06
Iter: 679 loss: 1.43815498e-06
Iter: 680 loss: 1.43769967e-06
Iter: 681 loss: 1.43763111e-06
Iter: 682 loss: 1.43756188e-06
Iter: 683 loss: 1.43754596e-06
Iter: 684 loss: 1.43745058e-06
Iter: 685 loss: 1.43733382e-06
Iter: 686 loss: 1.43730347e-06
Iter: 687 loss: 1.43717079e-06
Iter: 688 loss: 1.43821239e-06
Iter: 689 loss: 1.43714101e-06
Iter: 690 loss: 1.437008e-06
Iter: 691 loss: 1.43742614e-06
Iter: 692 loss: 1.43700368e-06
Iter: 693 loss: 1.43681655e-06
Iter: 694 loss: 1.43680859e-06
Iter: 695 loss: 1.43665557e-06
Iter: 696 loss: 1.43644752e-06
Iter: 697 loss: 1.43697184e-06
Iter: 698 loss: 1.43637487e-06
Iter: 699 loss: 1.436189e-06
Iter: 700 loss: 1.43636657e-06
Iter: 701 loss: 1.43606485e-06
Iter: 702 loss: 1.43596571e-06
Iter: 703 loss: 1.43591637e-06
Iter: 704 loss: 1.43582918e-06
Iter: 705 loss: 1.43559851e-06
Iter: 706 loss: 1.43769762e-06
Iter: 707 loss: 1.43555735e-06
Iter: 708 loss: 1.43539e-06
Iter: 709 loss: 1.43720695e-06
Iter: 710 loss: 1.435385e-06
Iter: 711 loss: 1.43527393e-06
Iter: 712 loss: 1.43645036e-06
Iter: 713 loss: 1.43526177e-06
Iter: 714 loss: 1.43514899e-06
Iter: 715 loss: 1.43532316e-06
Iter: 716 loss: 1.43510124e-06
Iter: 717 loss: 1.43500188e-06
Iter: 718 loss: 1.4349913e-06
Iter: 719 loss: 1.43494321e-06
Iter: 720 loss: 1.4347753e-06
Iter: 721 loss: 1.43456884e-06
Iter: 722 loss: 1.43454508e-06
Iter: 723 loss: 1.43413934e-06
Iter: 724 loss: 1.43634497e-06
Iter: 725 loss: 1.43409636e-06
Iter: 726 loss: 1.43379077e-06
Iter: 727 loss: 1.43556622e-06
Iter: 728 loss: 1.4337561e-06
Iter: 729 loss: 1.43357477e-06
Iter: 730 loss: 1.43554007e-06
Iter: 731 loss: 1.43355885e-06
Iter: 732 loss: 1.43339662e-06
Iter: 733 loss: 1.43413752e-06
Iter: 734 loss: 1.43335342e-06
Iter: 735 loss: 1.43327441e-06
Iter: 736 loss: 1.4331423e-06
Iter: 737 loss: 1.4330991e-06
Iter: 738 loss: 1.43296552e-06
Iter: 739 loss: 1.4343525e-06
Iter: 740 loss: 1.4329604e-06
Iter: 741 loss: 1.43282432e-06
Iter: 742 loss: 1.43357352e-06
Iter: 743 loss: 1.43280204e-06
Iter: 744 loss: 1.43267039e-06
Iter: 745 loss: 1.43295051e-06
Iter: 746 loss: 1.43260979e-06
Iter: 747 loss: 1.4324894e-06
Iter: 748 loss: 1.43227271e-06
Iter: 749 loss: 1.43663351e-06
Iter: 750 loss: 1.43225645e-06
Iter: 751 loss: 1.43225873e-06
Iter: 752 loss: 1.43215414e-06
Iter: 753 loss: 1.43208194e-06
Iter: 754 loss: 1.43204056e-06
Iter: 755 loss: 1.43199622e-06
Iter: 756 loss: 1.43182865e-06
Iter: 757 loss: 1.43223542e-06
Iter: 758 loss: 1.43179864e-06
Iter: 759 loss: 1.4317111e-06
Iter: 760 loss: 1.43156171e-06
Iter: 761 loss: 1.43157149e-06
Iter: 762 loss: 1.4313448e-06
Iter: 763 loss: 1.43132456e-06
Iter: 764 loss: 1.43117359e-06
Iter: 765 loss: 1.43097827e-06
Iter: 766 loss: 1.43179068e-06
Iter: 767 loss: 1.43094155e-06
Iter: 768 loss: 1.43072964e-06
Iter: 769 loss: 1.43263082e-06
Iter: 770 loss: 1.4307227e-06
Iter: 771 loss: 1.4305491e-06
Iter: 772 loss: 1.43133411e-06
Iter: 773 loss: 1.43051955e-06
Iter: 774 loss: 1.43043098e-06
Iter: 775 loss: 1.43032366e-06
Iter: 776 loss: 1.43027944e-06
Iter: 777 loss: 1.43013358e-06
Iter: 778 loss: 1.4303173e-06
Iter: 779 loss: 1.43002683e-06
Iter: 780 loss: 1.42989734e-06
Iter: 781 loss: 1.42989586e-06
Iter: 782 loss: 1.42982708e-06
Iter: 783 loss: 1.42973579e-06
Iter: 784 loss: 1.42970032e-06
Iter: 785 loss: 1.42956469e-06
Iter: 786 loss: 1.42960766e-06
Iter: 787 loss: 1.42943532e-06
Iter: 788 loss: 1.42941622e-06
Iter: 789 loss: 1.42934095e-06
Iter: 790 loss: 1.4292541e-06
Iter: 791 loss: 1.42914007e-06
Iter: 792 loss: 1.42999647e-06
Iter: 793 loss: 1.42911904e-06
Iter: 794 loss: 1.42897034e-06
Iter: 795 loss: 1.42994247e-06
Iter: 796 loss: 1.42893873e-06
Iter: 797 loss: 1.42879685e-06
Iter: 798 loss: 1.4294377e-06
Iter: 799 loss: 1.42873648e-06
Iter: 800 loss: 1.42866236e-06
Iter: 801 loss: 1.42858232e-06
Iter: 802 loss: 1.42854333e-06
Iter: 803 loss: 1.42843794e-06
Iter: 804 loss: 1.42842237e-06
Iter: 805 loss: 1.42833801e-06
Iter: 806 loss: 1.42818669e-06
Iter: 807 loss: 1.42880754e-06
Iter: 808 loss: 1.42813951e-06
Iter: 809 loss: 1.42798262e-06
Iter: 810 loss: 1.42826423e-06
Iter: 811 loss: 1.427906e-06
Iter: 812 loss: 1.4276909e-06
Iter: 813 loss: 1.42895738e-06
Iter: 814 loss: 1.42766316e-06
Iter: 815 loss: 1.42753333e-06
Iter: 816 loss: 1.42753515e-06
Iter: 817 loss: 1.42742056e-06
Iter: 818 loss: 1.42729198e-06
Iter: 819 loss: 1.42727811e-06
Iter: 820 loss: 1.42720171e-06
Iter: 821 loss: 1.42718932e-06
Iter: 822 loss: 1.42710837e-06
Iter: 823 loss: 1.42702038e-06
Iter: 824 loss: 1.42699696e-06
Iter: 825 loss: 1.42692159e-06
Iter: 826 loss: 1.42691033e-06
Iter: 827 loss: 1.42687315e-06
Iter: 828 loss: 1.4267755e-06
Iter: 829 loss: 1.42896693e-06
Iter: 830 loss: 1.42677845e-06
Iter: 831 loss: 1.42665328e-06
Iter: 832 loss: 1.42677652e-06
Iter: 833 loss: 1.42657461e-06
Iter: 834 loss: 1.42645467e-06
Iter: 835 loss: 1.42812462e-06
Iter: 836 loss: 1.42644535e-06
Iter: 837 loss: 1.42639362e-06
Iter: 838 loss: 1.42630483e-06
Iter: 839 loss: 1.42631336e-06
Iter: 840 loss: 1.42616091e-06
Iter: 841 loss: 1.42603358e-06
Iter: 842 loss: 1.42596798e-06
Iter: 843 loss: 1.42573413e-06
Iter: 844 loss: 1.42600663e-06
Iter: 845 loss: 1.42559668e-06
Iter: 846 loss: 1.42532451e-06
Iter: 847 loss: 1.42700992e-06
Iter: 848 loss: 1.4252995e-06
Iter: 849 loss: 1.42506792e-06
Iter: 850 loss: 1.42640772e-06
Iter: 851 loss: 1.42504041e-06
Iter: 852 loss: 1.42492945e-06
Iter: 853 loss: 1.42492991e-06
Iter: 854 loss: 1.42486408e-06
Iter: 855 loss: 1.42471231e-06
Iter: 856 loss: 1.42472436e-06
Iter: 857 loss: 1.42469287e-06
Iter: 858 loss: 1.42464683e-06
Iter: 859 loss: 1.42459567e-06
Iter: 860 loss: 1.42465569e-06
Iter: 861 loss: 1.42458964e-06
Iter: 862 loss: 1.42447539e-06
Iter: 863 loss: 1.42444958e-06
Iter: 864 loss: 1.42440024e-06
Iter: 865 loss: 1.42434169e-06
Iter: 866 loss: 1.42417753e-06
Iter: 867 loss: 1.42701867e-06
Iter: 868 loss: 1.4241507e-06
Iter: 869 loss: 1.42404065e-06
Iter: 870 loss: 1.42403258e-06
Iter: 871 loss: 1.42389933e-06
Iter: 872 loss: 1.42385898e-06
Iter: 873 loss: 1.42381191e-06
Iter: 874 loss: 1.42366105e-06
Iter: 875 loss: 1.42372119e-06
Iter: 876 loss: 1.42355361e-06
Iter: 877 loss: 1.42330623e-06
Iter: 878 loss: 1.42338104e-06
Iter: 879 loss: 1.42313877e-06
Iter: 880 loss: 1.42287934e-06
Iter: 881 loss: 1.42469855e-06
Iter: 882 loss: 1.42286467e-06
Iter: 883 loss: 1.4226739e-06
Iter: 884 loss: 1.422575e-06
Iter: 885 loss: 1.42247711e-06
Iter: 886 loss: 1.42228635e-06
Iter: 887 loss: 1.42418e-06
Iter: 888 loss: 1.4223167e-06
Iter: 889 loss: 1.42214708e-06
Iter: 890 loss: 1.42342265e-06
Iter: 891 loss: 1.42216345e-06
Iter: 892 loss: 1.42205363e-06
Iter: 893 loss: 1.42189879e-06
Iter: 894 loss: 1.42190743e-06
Iter: 895 loss: 1.42173917e-06
Iter: 896 loss: 1.42174054e-06
Iter: 897 loss: 1.42159411e-06
Iter: 898 loss: 1.42204294e-06
Iter: 899 loss: 1.42156136e-06
Iter: 900 loss: 1.42149474e-06
Iter: 901 loss: 1.42143494e-06
Iter: 902 loss: 1.42140402e-06
Iter: 903 loss: 1.42120086e-06
Iter: 904 loss: 1.42104659e-06
Iter: 905 loss: 1.42098611e-06
Iter: 906 loss: 1.42079011e-06
Iter: 907 loss: 1.42082274e-06
Iter: 908 loss: 1.42067324e-06
Iter: 909 loss: 1.4204054e-06
Iter: 910 loss: 1.42290753e-06
Iter: 911 loss: 1.42036561e-06
Iter: 912 loss: 1.42024101e-06
Iter: 913 loss: 1.42247711e-06
Iter: 914 loss: 1.42024419e-06
Iter: 915 loss: 1.42015e-06
Iter: 916 loss: 1.42001727e-06
Iter: 917 loss: 1.42000761e-06
Iter: 918 loss: 1.41981752e-06
Iter: 919 loss: 1.42013766e-06
Iter: 920 loss: 1.41974238e-06
Iter: 921 loss: 1.41950079e-06
Iter: 922 loss: 1.42009321e-06
Iter: 923 loss: 1.41942655e-06
Iter: 924 loss: 1.41937221e-06
Iter: 925 loss: 1.41934743e-06
Iter: 926 loss: 1.41926716e-06
Iter: 927 loss: 1.41911414e-06
Iter: 928 loss: 1.41910755e-06
Iter: 929 loss: 1.41905366e-06
Iter: 930 loss: 1.41904286e-06
Iter: 931 loss: 1.41894225e-06
Iter: 932 loss: 1.41891474e-06
Iter: 933 loss: 1.41888074e-06
Iter: 934 loss: 1.41880139e-06
Iter: 935 loss: 1.41883572e-06
Iter: 936 loss: 1.41872556e-06
Iter: 937 loss: 1.41858038e-06
Iter: 938 loss: 1.41875501e-06
Iter: 939 loss: 1.41848216e-06
Iter: 940 loss: 1.41833584e-06
Iter: 941 loss: 1.41818373e-06
Iter: 942 loss: 1.41812643e-06
Iter: 943 loss: 1.41796806e-06
Iter: 944 loss: 1.4179725e-06
Iter: 945 loss: 1.41780788e-06
Iter: 946 loss: 1.41832629e-06
Iter: 947 loss: 1.41778241e-06
Iter: 948 loss: 1.41766373e-06
Iter: 949 loss: 1.41766577e-06
Iter: 950 loss: 1.41757255e-06
Iter: 951 loss: 1.41740918e-06
Iter: 952 loss: 1.41775649e-06
Iter: 953 loss: 1.41735927e-06
Iter: 954 loss: 1.41722967e-06
Iter: 955 loss: 1.41806277e-06
Iter: 956 loss: 1.41721978e-06
Iter: 957 loss: 1.41708279e-06
Iter: 958 loss: 1.41766509e-06
Iter: 959 loss: 1.41707051e-06
Iter: 960 loss: 1.41692192e-06
Iter: 961 loss: 1.41686041e-06
Iter: 962 loss: 1.41679175e-06
Iter: 963 loss: 1.41679197e-06
Iter: 964 loss: 1.41672433e-06
Iter: 965 loss: 1.416651e-06
Iter: 966 loss: 1.41648354e-06
Iter: 967 loss: 1.41659893e-06
Iter: 968 loss: 1.41628675e-06
Iter: 969 loss: 1.41610781e-06
Iter: 970 loss: 1.41610599e-06
Iter: 971 loss: 1.41593637e-06
Iter: 972 loss: 1.4161426e-06
Iter: 973 loss: 1.41586793e-06
Iter: 974 loss: 1.41569024e-06
Iter: 975 loss: 1.41577982e-06
Iter: 976 loss: 1.4155894e-06
Iter: 977 loss: 1.41545638e-06
Iter: 978 loss: 1.41645239e-06
Iter: 979 loss: 1.41547025e-06
Iter: 980 loss: 1.41533678e-06
Iter: 981 loss: 1.41531098e-06
Iter: 982 loss: 1.41521878e-06
Iter: 983 loss: 1.41510486e-06
Iter: 984 loss: 1.4154607e-06
Iter: 985 loss: 1.41507894e-06
Iter: 986 loss: 1.41496696e-06
Iter: 987 loss: 1.41500709e-06
Iter: 988 loss: 1.4148734e-06
Iter: 989 loss: 1.4147347e-06
Iter: 990 loss: 1.41649184e-06
Iter: 991 loss: 1.41472481e-06
Iter: 992 loss: 1.4145902e-06
Iter: 993 loss: 1.41477688e-06
Iter: 994 loss: 1.41453711e-06
Iter: 995 loss: 1.41439807e-06
Iter: 996 loss: 1.41485941e-06
Iter: 997 loss: 1.41436522e-06
Iter: 998 loss: 1.41422584e-06
Iter: 999 loss: 1.41501914e-06
Iter: 1000 loss: 1.41420742e-06
Iter: 1001 loss: 1.41410646e-06
Iter: 1002 loss: 1.41391865e-06
Iter: 1003 loss: 1.41689577e-06
Iter: 1004 loss: 1.41390115e-06
Iter: 1005 loss: 1.41380497e-06
Iter: 1006 loss: 1.4156401e-06
Iter: 1007 loss: 1.41380087e-06
Iter: 1008 loss: 1.413635e-06
Iter: 1009 loss: 1.41349619e-06
Iter: 1010 loss: 1.4134431e-06
Iter: 1011 loss: 1.41329883e-06
Iter: 1012 loss: 1.41446867e-06
Iter: 1013 loss: 1.41329519e-06
Iter: 1014 loss: 1.41315286e-06
Iter: 1015 loss: 1.4141151e-06
Iter: 1016 loss: 1.4131399e-06
Iter: 1017 loss: 1.41306407e-06
Iter: 1018 loss: 1.41295277e-06
Iter: 1019 loss: 1.4129148e-06
Iter: 1020 loss: 1.4127786e-06
Iter: 1021 loss: 1.41293594e-06
Iter: 1022 loss: 1.41266196e-06
Iter: 1023 loss: 1.41252235e-06
Iter: 1024 loss: 1.41360988e-06
Iter: 1025 loss: 1.4125078e-06
Iter: 1026 loss: 1.4123566e-06
Iter: 1027 loss: 1.41318128e-06
Iter: 1028 loss: 1.41235921e-06
Iter: 1029 loss: 1.41220426e-06
Iter: 1030 loss: 1.41234784e-06
Iter: 1031 loss: 1.412134e-06
Iter: 1032 loss: 1.41203611e-06
Iter: 1033 loss: 1.41202725e-06
Iter: 1034 loss: 1.41195164e-06
Iter: 1035 loss: 1.41174758e-06
Iter: 1036 loss: 1.41402734e-06
Iter: 1037 loss: 1.4117195e-06
Iter: 1038 loss: 1.41152805e-06
Iter: 1039 loss: 1.41185296e-06
Iter: 1040 loss: 1.41145506e-06
Iter: 1041 loss: 1.41122314e-06
Iter: 1042 loss: 1.41244539e-06
Iter: 1043 loss: 1.41122132e-06
Iter: 1044 loss: 1.4110019e-06
Iter: 1045 loss: 1.41177964e-06
Iter: 1046 loss: 1.41093233e-06
Iter: 1047 loss: 1.41083524e-06
Iter: 1048 loss: 1.41071416e-06
Iter: 1049 loss: 1.41071746e-06
Iter: 1050 loss: 1.41053317e-06
Iter: 1051 loss: 1.41050975e-06
Iter: 1052 loss: 1.41046849e-06
Iter: 1053 loss: 1.41033843e-06
Iter: 1054 loss: 1.41307567e-06
Iter: 1055 loss: 1.4103457e-06
Iter: 1056 loss: 1.41018336e-06
Iter: 1057 loss: 1.41035889e-06
Iter: 1058 loss: 1.41009264e-06
Iter: 1059 loss: 1.40999941e-06
Iter: 1060 loss: 1.40998168e-06
Iter: 1061 loss: 1.40987095e-06
Iter: 1062 loss: 1.41012151e-06
Iter: 1063 loss: 1.40983298e-06
Iter: 1064 loss: 1.40970701e-06
Iter: 1065 loss: 1.40983889e-06
Iter: 1066 loss: 1.40966608e-06
Iter: 1067 loss: 1.40951738e-06
Iter: 1068 loss: 1.41125429e-06
Iter: 1069 loss: 1.40952579e-06
Iter: 1070 loss: 1.40943655e-06
Iter: 1071 loss: 1.4092808e-06
Iter: 1072 loss: 1.41167811e-06
Iter: 1073 loss: 1.4092941e-06
Iter: 1074 loss: 1.40910413e-06
Iter: 1075 loss: 1.4091479e-06
Iter: 1076 loss: 1.40901238e-06
Iter: 1077 loss: 1.40883776e-06
Iter: 1078 loss: 1.40882753e-06
Iter: 1079 loss: 1.40870134e-06
Iter: 1080 loss: 1.40865234e-06
Iter: 1081 loss: 1.40856946e-06
Iter: 1082 loss: 1.40845623e-06
Iter: 1083 loss: 1.40887585e-06
Iter: 1084 loss: 1.40840621e-06
Iter: 1085 loss: 1.4083231e-06
Iter: 1086 loss: 1.40829923e-06
Iter: 1087 loss: 1.40824159e-06
Iter: 1088 loss: 1.40813279e-06
Iter: 1089 loss: 1.40965381e-06
Iter: 1090 loss: 1.40809277e-06
Iter: 1091 loss: 1.40791917e-06
Iter: 1092 loss: 1.40831162e-06
Iter: 1093 loss: 1.40785824e-06
Iter: 1094 loss: 1.40775205e-06
Iter: 1095 loss: 1.40775012e-06
Iter: 1096 loss: 1.40765201e-06
Iter: 1097 loss: 1.40772215e-06
Iter: 1098 loss: 1.40758152e-06
Iter: 1099 loss: 1.40748875e-06
Iter: 1100 loss: 1.40853899e-06
Iter: 1101 loss: 1.40748625e-06
Iter: 1102 loss: 1.40740372e-06
Iter: 1103 loss: 1.40728457e-06
Iter: 1104 loss: 1.40728275e-06
Iter: 1105 loss: 1.40710301e-06
Iter: 1106 loss: 1.40689644e-06
Iter: 1107 loss: 1.40690349e-06
Iter: 1108 loss: 1.40660131e-06
Iter: 1109 loss: 1.40845646e-06
Iter: 1110 loss: 1.40655288e-06
Iter: 1111 loss: 1.40643579e-06
Iter: 1112 loss: 1.40644283e-06
Iter: 1113 loss: 1.40634677e-06
Iter: 1114 loss: 1.40620079e-06
Iter: 1115 loss: 1.40619863e-06
Iter: 1116 loss: 1.40609677e-06
Iter: 1117 loss: 1.40608586e-06
Iter: 1118 loss: 1.40599263e-06
Iter: 1119 loss: 1.40594648e-06
Iter: 1120 loss: 1.40588759e-06
Iter: 1121 loss: 1.40578538e-06
Iter: 1122 loss: 1.40591715e-06
Iter: 1123 loss: 1.40573e-06
Iter: 1124 loss: 1.40556131e-06
Iter: 1125 loss: 1.40549264e-06
Iter: 1126 loss: 1.40541772e-06
Iter: 1127 loss: 1.40540476e-06
Iter: 1128 loss: 1.40531574e-06
Iter: 1129 loss: 1.40522422e-06
Iter: 1130 loss: 1.40517557e-06
Iter: 1131 loss: 1.40514953e-06
Iter: 1132 loss: 1.40493671e-06
Iter: 1133 loss: 1.40537713e-06
Iter: 1134 loss: 1.40485372e-06
Iter: 1135 loss: 1.40472775e-06
Iter: 1136 loss: 1.40448492e-06
Iter: 1137 loss: 1.41028033e-06
Iter: 1138 loss: 1.40449629e-06
Iter: 1139 loss: 1.40426982e-06
Iter: 1140 loss: 1.4052855e-06
Iter: 1141 loss: 1.40418877e-06
Iter: 1142 loss: 1.4040229e-06
Iter: 1143 loss: 1.40500197e-06
Iter: 1144 loss: 1.40401062e-06
Iter: 1145 loss: 1.40386828e-06
Iter: 1146 loss: 1.40486497e-06
Iter: 1147 loss: 1.40385964e-06
Iter: 1148 loss: 1.40376824e-06
Iter: 1149 loss: 1.40373709e-06
Iter: 1150 loss: 1.40368661e-06
Iter: 1151 loss: 1.40351881e-06
Iter: 1152 loss: 1.40349175e-06
Iter: 1153 loss: 1.40339444e-06
Iter: 1154 loss: 1.40322641e-06
Iter: 1155 loss: 1.40576037e-06
Iter: 1156 loss: 1.40322311e-06
Iter: 1157 loss: 1.40306179e-06
Iter: 1158 loss: 1.40336283e-06
Iter: 1159 loss: 1.40299585e-06
Iter: 1160 loss: 1.40289376e-06
Iter: 1161 loss: 1.40265536e-06
Iter: 1162 loss: 1.40707903e-06
Iter: 1163 loss: 1.4026657e-06
Iter: 1164 loss: 1.40241286e-06
Iter: 1165 loss: 1.405989e-06
Iter: 1166 loss: 1.40242105e-06
Iter: 1167 loss: 1.40229361e-06
Iter: 1168 loss: 1.40228394e-06
Iter: 1169 loss: 1.40221368e-06
Iter: 1170 loss: 1.40210102e-06
Iter: 1171 loss: 1.40209602e-06
Iter: 1172 loss: 1.40194504e-06
Iter: 1173 loss: 1.40241741e-06
Iter: 1174 loss: 1.40186773e-06
Iter: 1175 loss: 1.40177269e-06
Iter: 1176 loss: 1.40155339e-06
Iter: 1177 loss: 1.40478869e-06
Iter: 1178 loss: 1.40152088e-06
Iter: 1179 loss: 1.4013192e-06
Iter: 1180 loss: 1.40198279e-06
Iter: 1181 loss: 1.40127531e-06
Iter: 1182 loss: 1.40107329e-06
Iter: 1183 loss: 1.4010709e-06
Iter: 1184 loss: 1.40095392e-06
Iter: 1185 loss: 1.40183681e-06
Iter: 1186 loss: 1.40092266e-06
Iter: 1187 loss: 1.40086013e-06
Iter: 1188 loss: 1.40067743e-06
Iter: 1189 loss: 1.40237137e-06
Iter: 1190 loss: 1.40062764e-06
Iter: 1191 loss: 1.40046029e-06
Iter: 1192 loss: 1.40301211e-06
Iter: 1193 loss: 1.40044131e-06
Iter: 1194 loss: 1.40030738e-06
Iter: 1195 loss: 1.40170391e-06
Iter: 1196 loss: 1.40031455e-06
Iter: 1197 loss: 1.40018392e-06
Iter: 1198 loss: 1.40019813e-06
Iter: 1199 loss: 1.40008365e-06
Iter: 1200 loss: 1.39994847e-06
Iter: 1201 loss: 1.39993926e-06
Iter: 1202 loss: 1.39981785e-06
Iter: 1203 loss: 1.39976567e-06
Iter: 1204 loss: 1.39973474e-06
Iter: 1205 loss: 1.39963208e-06
Iter: 1206 loss: 1.39957388e-06
Iter: 1207 loss: 1.39954614e-06
Iter: 1208 loss: 1.39941721e-06
Iter: 1209 loss: 1.39955739e-06
Iter: 1210 loss: 1.39940039e-06
Iter: 1211 loss: 1.39920598e-06
Iter: 1212 loss: 1.39960991e-06
Iter: 1213 loss: 1.39916233e-06
Iter: 1214 loss: 1.39905478e-06
Iter: 1215 loss: 1.39893609e-06
Iter: 1216 loss: 1.39895542e-06
Iter: 1217 loss: 1.39881786e-06
Iter: 1218 loss: 1.39879921e-06
Iter: 1219 loss: 1.3987011e-06
Iter: 1220 loss: 1.3987144e-06
Iter: 1221 loss: 1.39860947e-06
Iter: 1222 loss: 1.39850158e-06
Iter: 1223 loss: 1.39843132e-06
Iter: 1224 loss: 1.39837027e-06
Iter: 1225 loss: 1.39820827e-06
Iter: 1226 loss: 1.3990234e-06
Iter: 1227 loss: 1.39817132e-06
Iter: 1228 loss: 1.39805343e-06
Iter: 1229 loss: 1.39805445e-06
Iter: 1230 loss: 1.3979402e-06
Iter: 1231 loss: 1.39780241e-06
Iter: 1232 loss: 1.39778535e-06
Iter: 1233 loss: 1.3976869e-06
Iter: 1234 loss: 1.39838801e-06
Iter: 1235 loss: 1.39764552e-06
Iter: 1236 loss: 1.39752069e-06
Iter: 1237 loss: 1.39826784e-06
Iter: 1238 loss: 1.39750966e-06
Iter: 1239 loss: 1.39745066e-06
Iter: 1240 loss: 1.39744725e-06
Iter: 1241 loss: 1.39738336e-06
Iter: 1242 loss: 1.39730571e-06
Iter: 1243 loss: 1.39774613e-06
Iter: 1244 loss: 1.39731469e-06
Iter: 1245 loss: 1.39721976e-06
Iter: 1246 loss: 1.39717713e-06
Iter: 1247 loss: 1.3971345e-06
Iter: 1248 loss: 1.39705e-06
Iter: 1249 loss: 1.39700444e-06
Iter: 1250 loss: 1.39694873e-06
Iter: 1251 loss: 1.39676604e-06
Iter: 1252 loss: 1.39711324e-06
Iter: 1253 loss: 1.39673875e-06
Iter: 1254 loss: 1.39660801e-06
Iter: 1255 loss: 1.39660847e-06
Iter: 1256 loss: 1.396524e-06
Iter: 1257 loss: 1.3962964e-06
Iter: 1258 loss: 1.39778558e-06
Iter: 1259 loss: 1.39622819e-06
Iter: 1260 loss: 1.39597307e-06
Iter: 1261 loss: 1.39753081e-06
Iter: 1262 loss: 1.39594044e-06
Iter: 1263 loss: 1.39577742e-06
Iter: 1264 loss: 1.39576912e-06
Iter: 1265 loss: 1.39565032e-06
Iter: 1266 loss: 1.39554754e-06
Iter: 1267 loss: 1.39549888e-06
Iter: 1268 loss: 1.39533336e-06
Iter: 1269 loss: 1.39580834e-06
Iter: 1270 loss: 1.39527924e-06
Iter: 1271 loss: 1.3951867e-06
Iter: 1272 loss: 1.39516237e-06
Iter: 1273 loss: 1.39512497e-06
Iter: 1274 loss: 1.39504459e-06
Iter: 1275 loss: 1.39514555e-06
Iter: 1276 loss: 1.3949566e-06
Iter: 1277 loss: 1.39482313e-06
Iter: 1278 loss: 1.39482154e-06
Iter: 1279 loss: 1.39474764e-06
Iter: 1280 loss: 1.39470467e-06
Iter: 1281 loss: 1.39469307e-06
Iter: 1282 loss: 1.39456643e-06
Iter: 1283 loss: 1.39456074e-06
Iter: 1284 loss: 1.39448412e-06
Iter: 1285 loss: 1.3942954e-06
Iter: 1286 loss: 1.39530732e-06
Iter: 1287 loss: 1.39429312e-06
Iter: 1288 loss: 1.39415329e-06
Iter: 1289 loss: 1.39534643e-06
Iter: 1290 loss: 1.39415192e-06
Iter: 1291 loss: 1.3940778e-06
Iter: 1292 loss: 1.39408121e-06
Iter: 1293 loss: 1.3940155e-06
Iter: 1294 loss: 1.39386009e-06
Iter: 1295 loss: 1.39388487e-06
Iter: 1296 loss: 1.39375379e-06
Iter: 1297 loss: 1.39360054e-06
Iter: 1298 loss: 1.39452368e-06
Iter: 1299 loss: 1.39361578e-06
Iter: 1300 loss: 1.39349925e-06
Iter: 1301 loss: 1.39505221e-06
Iter: 1302 loss: 1.39348549e-06
Iter: 1303 loss: 1.39338147e-06
Iter: 1304 loss: 1.39342183e-06
Iter: 1305 loss: 1.39334361e-06
Iter: 1306 loss: 1.39328256e-06
Iter: 1307 loss: 1.39326642e-06
Iter: 1308 loss: 1.39321446e-06
Iter: 1309 loss: 1.39305371e-06
Iter: 1310 loss: 1.39398321e-06
Iter: 1311 loss: 1.39302392e-06
Iter: 1312 loss: 1.39289477e-06
Iter: 1313 loss: 1.39313295e-06
Iter: 1314 loss: 1.39282304e-06
Iter: 1315 loss: 1.39270423e-06
Iter: 1316 loss: 1.39270082e-06
Iter: 1317 loss: 1.39258714e-06
Iter: 1318 loss: 1.39240569e-06
Iter: 1319 loss: 1.39593067e-06
Iter: 1320 loss: 1.39240387e-06
Iter: 1321 loss: 1.39216377e-06
Iter: 1322 loss: 1.39276221e-06
Iter: 1323 loss: 1.39208521e-06
Iter: 1324 loss: 1.391875e-06
Iter: 1325 loss: 1.39321151e-06
Iter: 1326 loss: 1.39185113e-06
Iter: 1327 loss: 1.39167162e-06
Iter: 1328 loss: 1.39311555e-06
Iter: 1329 loss: 1.39166582e-06
Iter: 1330 loss: 1.39152507e-06
Iter: 1331 loss: 1.39162682e-06
Iter: 1332 loss: 1.39148096e-06
Iter: 1333 loss: 1.39131589e-06
Iter: 1334 loss: 1.39126973e-06
Iter: 1335 loss: 1.39118811e-06
Iter: 1336 loss: 1.39103247e-06
Iter: 1337 loss: 1.39104827e-06
Iter: 1338 loss: 1.39089377e-06
Iter: 1339 loss: 1.39107931e-06
Iter: 1340 loss: 1.39083681e-06
Iter: 1341 loss: 1.39065844e-06
Iter: 1342 loss: 1.39244344e-06
Iter: 1343 loss: 1.3906747e-06
Iter: 1344 loss: 1.39059318e-06
Iter: 1345 loss: 1.39042152e-06
Iter: 1346 loss: 1.39321537e-06
Iter: 1347 loss: 1.39040071e-06
Iter: 1348 loss: 1.39021108e-06
Iter: 1349 loss: 1.39022529e-06
Iter: 1350 loss: 1.39005851e-06
Iter: 1351 loss: 1.38983512e-06
Iter: 1352 loss: 1.39141434e-06
Iter: 1353 loss: 1.38982205e-06
Iter: 1354 loss: 1.38965584e-06
Iter: 1355 loss: 1.38966402e-06
Iter: 1356 loss: 1.38958012e-06
Iter: 1357 loss: 1.38937298e-06
Iter: 1358 loss: 1.39203189e-06
Iter: 1359 loss: 1.38935491e-06
Iter: 1360 loss: 1.38917926e-06
Iter: 1361 loss: 1.39117094e-06
Iter: 1362 loss: 1.38917176e-06
Iter: 1363 loss: 1.38905375e-06
Iter: 1364 loss: 1.39000349e-06
Iter: 1365 loss: 1.38903306e-06
Iter: 1366 loss: 1.38888959e-06
Iter: 1367 loss: 1.38872269e-06
Iter: 1368 loss: 1.38870223e-06
Iter: 1369 loss: 1.38856342e-06
Iter: 1370 loss: 1.38929101e-06
Iter: 1371 loss: 1.38854557e-06
Iter: 1372 loss: 1.38843757e-06
Iter: 1373 loss: 1.39017357e-06
Iter: 1374 loss: 1.38844757e-06
Iter: 1375 loss: 1.38835208e-06
Iter: 1376 loss: 1.38874134e-06
Iter: 1377 loss: 1.38833502e-06
Iter: 1378 loss: 1.38823577e-06
Iter: 1379 loss: 1.38834866e-06
Iter: 1380 loss: 1.3882086e-06
Iter: 1381 loss: 1.38811174e-06
Iter: 1382 loss: 1.38805308e-06
Iter: 1383 loss: 1.38802977e-06
Iter: 1384 loss: 1.38792575e-06
Iter: 1385 loss: 1.38794735e-06
Iter: 1386 loss: 1.38781877e-06
Iter: 1387 loss: 1.3875931e-06
Iter: 1388 loss: 1.38780047e-06
Iter: 1389 loss: 1.38746168e-06
Iter: 1390 loss: 1.38732128e-06
Iter: 1391 loss: 1.38731923e-06
Iter: 1392 loss: 1.38719633e-06
Iter: 1393 loss: 1.38751079e-06
Iter: 1394 loss: 1.3871138e-06
Iter: 1395 loss: 1.38702319e-06
Iter: 1396 loss: 1.38690257e-06
Iter: 1397 loss: 1.38687471e-06
Iter: 1398 loss: 1.38673738e-06
Iter: 1399 loss: 1.38842131e-06
Iter: 1400 loss: 1.3867309e-06
Iter: 1401 loss: 1.38662472e-06
Iter: 1402 loss: 1.38722351e-06
Iter: 1403 loss: 1.38660812e-06
Iter: 1404 loss: 1.38651239e-06
Iter: 1405 loss: 1.38644987e-06
Iter: 1406 loss: 1.38640678e-06
Iter: 1407 loss: 1.38631867e-06
Iter: 1408 loss: 1.38630935e-06
Iter: 1409 loss: 1.38621158e-06
Iter: 1410 loss: 1.3862624e-06
Iter: 1411 loss: 1.38613859e-06
Iter: 1412 loss: 1.38605355e-06
Iter: 1413 loss: 1.38635755e-06
Iter: 1414 loss: 1.38603343e-06
Iter: 1415 loss: 1.38594271e-06
Iter: 1416 loss: 1.38589974e-06
Iter: 1417 loss: 1.38584505e-06
Iter: 1418 loss: 1.38570488e-06
Iter: 1419 loss: 1.38573705e-06
Iter: 1420 loss: 1.38556356e-06
Iter: 1421 loss: 1.38541736e-06
Iter: 1422 loss: 1.38577536e-06
Iter: 1423 loss: 1.38535529e-06
Iter: 1424 loss: 1.38516066e-06
Iter: 1425 loss: 1.38570533e-06
Iter: 1426 loss: 1.38508096e-06
Iter: 1427 loss: 1.38502844e-06
Iter: 1428 loss: 1.38498785e-06
Iter: 1429 loss: 1.38493419e-06
Iter: 1430 loss: 1.38477924e-06
Iter: 1431 loss: 1.38476992e-06
Iter: 1432 loss: 1.38468022e-06
Iter: 1433 loss: 1.38497592e-06
Iter: 1434 loss: 1.38460655e-06
Iter: 1435 loss: 1.38451696e-06
Iter: 1436 loss: 1.38605105e-06
Iter: 1437 loss: 1.38452378e-06
Iter: 1438 loss: 1.38443443e-06
Iter: 1439 loss: 1.3843553e-06
Iter: 1440 loss: 1.38433347e-06
Iter: 1441 loss: 1.38429311e-06
Iter: 1442 loss: 1.38426572e-06
Iter: 1443 loss: 1.38424025e-06
Iter: 1444 loss: 1.38411701e-06
Iter: 1445 loss: 1.38589212e-06
Iter: 1446 loss: 1.38409928e-06
Iter: 1447 loss: 1.38398707e-06
Iter: 1448 loss: 1.38464657e-06
Iter: 1449 loss: 1.38398104e-06
Iter: 1450 loss: 1.3838237e-06
Iter: 1451 loss: 1.3837564e-06
Iter: 1452 loss: 1.38370137e-06
Iter: 1453 loss: 1.38352891e-06
Iter: 1454 loss: 1.38339601e-06
Iter: 1455 loss: 1.38335577e-06
Iter: 1456 loss: 1.38306405e-06
Iter: 1457 loss: 1.38496307e-06
Iter: 1458 loss: 1.38305677e-06
Iter: 1459 loss: 1.38285623e-06
Iter: 1460 loss: 1.3843545e-06
Iter: 1461 loss: 1.38284872e-06
Iter: 1462 loss: 1.38267308e-06
Iter: 1463 loss: 1.38400048e-06
Iter: 1464 loss: 1.38265705e-06
Iter: 1465 loss: 1.38257974e-06
Iter: 1466 loss: 1.38242183e-06
Iter: 1467 loss: 1.38242115e-06
Iter: 1468 loss: 1.38222697e-06
Iter: 1469 loss: 1.383126e-06
Iter: 1470 loss: 1.38217274e-06
Iter: 1471 loss: 1.38206769e-06
Iter: 1472 loss: 1.38380119e-06
Iter: 1473 loss: 1.38207145e-06
Iter: 1474 loss: 1.38197015e-06
Iter: 1475 loss: 1.38184771e-06
Iter: 1476 loss: 1.38182895e-06
Iter: 1477 loss: 1.38170867e-06
Iter: 1478 loss: 1.38170026e-06
Iter: 1479 loss: 1.38163693e-06
Iter: 1480 loss: 1.38140945e-06
Iter: 1481 loss: 1.38234327e-06
Iter: 1482 loss: 1.38136375e-06
Iter: 1483 loss: 1.38117093e-06
Iter: 1484 loss: 1.38119276e-06
Iter: 1485 loss: 1.38105747e-06
Iter: 1486 loss: 1.38094993e-06
Iter: 1487 loss: 1.3809165e-06
Iter: 1488 loss: 1.38072733e-06
Iter: 1489 loss: 1.380708e-06
Iter: 1490 loss: 1.38059124e-06
Iter: 1491 loss: 1.38038808e-06
Iter: 1492 loss: 1.38136124e-06
Iter: 1493 loss: 1.38031169e-06
Iter: 1494 loss: 1.38023915e-06
Iter: 1495 loss: 1.38020835e-06
Iter: 1496 loss: 1.38009432e-06
Iter: 1497 loss: 1.37999791e-06
Iter: 1498 loss: 1.37998e-06
Iter: 1499 loss: 1.37981613e-06
Iter: 1500 loss: 1.38009341e-06
Iter: 1501 loss: 1.37978645e-06
Iter: 1502 loss: 1.37962513e-06
Iter: 1503 loss: 1.38101018e-06
Iter: 1504 loss: 1.3796398e-06
Iter: 1505 loss: 1.37948496e-06
Iter: 1506 loss: 1.37963332e-06
Iter: 1507 loss: 1.37942038e-06
Iter: 1508 loss: 1.37935695e-06
Iter: 1509 loss: 1.37935467e-06
Iter: 1510 loss: 1.37928532e-06
Iter: 1511 loss: 1.3791323e-06
Iter: 1512 loss: 1.38056021e-06
Iter: 1513 loss: 1.37912434e-06
Iter: 1514 loss: 1.37896222e-06
Iter: 1515 loss: 1.37983227e-06
Iter: 1516 loss: 1.3789429e-06
Iter: 1517 loss: 1.37880511e-06
Iter: 1518 loss: 1.37956908e-06
Iter: 1519 loss: 1.37882228e-06
Iter: 1520 loss: 1.37872291e-06
Iter: 1521 loss: 1.37852408e-06
Iter: 1522 loss: 1.37854499e-06
Iter: 1523 loss: 1.37832728e-06
Iter: 1524 loss: 1.37863481e-06
Iter: 1525 loss: 1.37825452e-06
Iter: 1526 loss: 1.37802726e-06
Iter: 1527 loss: 1.37934717e-06
Iter: 1528 loss: 1.37801806e-06
Iter: 1529 loss: 1.37790107e-06
Iter: 1530 loss: 1.37789129e-06
Iter: 1531 loss: 1.37782513e-06
Iter: 1532 loss: 1.37774396e-06
Iter: 1533 loss: 1.3777003e-06
Iter: 1534 loss: 1.37758821e-06
Iter: 1535 loss: 1.37765267e-06
Iter: 1536 loss: 1.37752522e-06
Iter: 1537 loss: 1.37740869e-06
Iter: 1538 loss: 1.37740483e-06
Iter: 1539 loss: 1.37734571e-06
Iter: 1540 loss: 1.37760594e-06
Iter: 1541 loss: 1.37729126e-06
Iter: 1542 loss: 1.37722282e-06
Iter: 1543 loss: 1.37755819e-06
Iter: 1544 loss: 1.37720406e-06
Iter: 1545 loss: 1.37712573e-06
Iter: 1546 loss: 1.37698703e-06
Iter: 1547 loss: 1.37891038e-06
Iter: 1548 loss: 1.37696748e-06
Iter: 1549 loss: 1.37682741e-06
Iter: 1550 loss: 1.37837412e-06
Iter: 1551 loss: 1.37682423e-06
Iter: 1552 loss: 1.37667894e-06
Iter: 1553 loss: 1.37665347e-06
Iter: 1554 loss: 1.37654592e-06
Iter: 1555 loss: 1.37636482e-06
Iter: 1556 loss: 1.37625727e-06
Iter: 1557 loss: 1.37617519e-06
Iter: 1558 loss: 1.37599761e-06
Iter: 1559 loss: 1.37767404e-06
Iter: 1560 loss: 1.37601819e-06
Iter: 1561 loss: 1.37583925e-06
Iter: 1562 loss: 1.3768904e-06
Iter: 1563 loss: 1.37583834e-06
Iter: 1564 loss: 1.37571578e-06
Iter: 1565 loss: 1.37660663e-06
Iter: 1566 loss: 1.37568122e-06
Iter: 1567 loss: 1.37561312e-06
Iter: 1568 loss: 1.37547397e-06
Iter: 1569 loss: 1.3782892e-06
Iter: 1570 loss: 1.37547772e-06
Iter: 1571 loss: 1.375328e-06
Iter: 1572 loss: 1.37684651e-06
Iter: 1573 loss: 1.37531981e-06
Iter: 1574 loss: 1.37521386e-06
Iter: 1575 loss: 1.3763929e-06
Iter: 1576 loss: 1.37521022e-06
Iter: 1577 loss: 1.37511779e-06
Iter: 1578 loss: 1.37529969e-06
Iter: 1579 loss: 1.37510449e-06
Iter: 1580 loss: 1.37498751e-06
Iter: 1581 loss: 1.37485677e-06
Iter: 1582 loss: 1.37482994e-06
Iter: 1583 loss: 1.37470477e-06
Iter: 1584 loss: 1.37490952e-06
Iter: 1585 loss: 1.37464542e-06
Iter: 1586 loss: 1.37449911e-06
Iter: 1587 loss: 1.37569577e-06
Iter: 1588 loss: 1.37449751e-06
Iter: 1589 loss: 1.37438622e-06
Iter: 1590 loss: 1.37450934e-06
Iter: 1591 loss: 1.37432403e-06
Iter: 1592 loss: 1.37420966e-06
Iter: 1593 loss: 1.37422398e-06
Iter: 1594 loss: 1.37414327e-06
Iter: 1595 loss: 1.37401321e-06
Iter: 1596 loss: 1.3741095e-06
Iter: 1597 loss: 1.37391828e-06
Iter: 1598 loss: 1.37377992e-06
Iter: 1599 loss: 1.37471829e-06
Iter: 1600 loss: 1.37373411e-06
Iter: 1601 loss: 1.373646e-06
Iter: 1602 loss: 1.37482516e-06
Iter: 1603 loss: 1.37364418e-06
Iter: 1604 loss: 1.37353186e-06
Iter: 1605 loss: 1.37367113e-06
Iter: 1606 loss: 1.37350298e-06
Iter: 1607 loss: 1.37338338e-06
Iter: 1608 loss: 1.37334985e-06
Iter: 1609 loss: 1.37330267e-06
Iter: 1610 loss: 1.37324923e-06
Iter: 1611 loss: 1.37320762e-06
Iter: 1612 loss: 1.3731476e-06
Iter: 1613 loss: 1.37307495e-06
Iter: 1614 loss: 1.37306461e-06
Iter: 1615 loss: 1.37295524e-06
Iter: 1616 loss: 1.3733387e-06
Iter: 1617 loss: 1.37292409e-06
Iter: 1618 loss: 1.37281427e-06
Iter: 1619 loss: 1.37286861e-06
Iter: 1620 loss: 1.372724e-06
Iter: 1621 loss: 1.37262691e-06
Iter: 1622 loss: 1.37274105e-06
Iter: 1623 loss: 1.37254506e-06
Iter: 1624 loss: 1.37241477e-06
Iter: 1625 loss: 1.37392567e-06
Iter: 1626 loss: 1.37241886e-06
Iter: 1627 loss: 1.37234895e-06
Iter: 1628 loss: 1.37237294e-06
Iter: 1629 loss: 1.37226596e-06
Iter: 1630 loss: 1.37214124e-06
Iter: 1631 loss: 1.372008e-06
Iter: 1632 loss: 1.37198685e-06
Iter: 1633 loss: 1.37182906e-06
Iter: 1634 loss: 1.37296024e-06
Iter: 1635 loss: 1.37178642e-06
Iter: 1636 loss: 1.37165739e-06
Iter: 1637 loss: 1.37251322e-06
Iter: 1638 loss: 1.37164977e-06
Iter: 1639 loss: 1.3715478e-06
Iter: 1640 loss: 1.37263203e-06
Iter: 1641 loss: 1.37154473e-06
Iter: 1642 loss: 1.37148868e-06
Iter: 1643 loss: 1.37142638e-06
Iter: 1644 loss: 1.37142717e-06
Iter: 1645 loss: 1.37131906e-06
Iter: 1646 loss: 1.37131815e-06
Iter: 1647 loss: 1.37127267e-06
Iter: 1648 loss: 1.37108054e-06
Iter: 1649 loss: 1.37202142e-06
Iter: 1650 loss: 1.37105053e-06
Iter: 1651 loss: 1.37091126e-06
Iter: 1652 loss: 1.37257393e-06
Iter: 1653 loss: 1.37093468e-06
Iter: 1654 loss: 1.37079928e-06
Iter: 1655 loss: 1.37125619e-06
Iter: 1656 loss: 1.37075858e-06
Iter: 1657 loss: 1.37067093e-06
Iter: 1658 loss: 1.3707164e-06
Iter: 1659 loss: 1.37059567e-06
Iter: 1660 loss: 1.3704805e-06
Iter: 1661 loss: 1.3708584e-06
Iter: 1662 loss: 1.37048346e-06
Iter: 1663 loss: 1.37036773e-06
Iter: 1664 loss: 1.37083293e-06
Iter: 1665 loss: 1.37035829e-06
Iter: 1666 loss: 1.37023471e-06
Iter: 1667 loss: 1.37027837e-06
Iter: 1668 loss: 1.37018674e-06
Iter: 1669 loss: 1.37006737e-06
Iter: 1670 loss: 1.37003099e-06
Iter: 1671 loss: 1.3699655e-06
Iter: 1672 loss: 1.36979e-06
Iter: 1673 loss: 1.37015923e-06
Iter: 1674 loss: 1.36973e-06
Iter: 1675 loss: 1.36965e-06
Iter: 1676 loss: 1.36961842e-06
Iter: 1677 loss: 1.36952406e-06
Iter: 1678 loss: 1.3696814e-06
Iter: 1679 loss: 1.36945141e-06
Iter: 1680 loss: 1.36936092e-06
Iter: 1681 loss: 1.37032316e-06
Iter: 1682 loss: 1.36934818e-06
Iter: 1683 loss: 1.36928008e-06
Iter: 1684 loss: 1.36911012e-06
Iter: 1685 loss: 1.37179404e-06
Iter: 1686 loss: 1.36910955e-06
Iter: 1687 loss: 1.36894778e-06
Iter: 1688 loss: 1.36921494e-06
Iter: 1689 loss: 1.36889935e-06
Iter: 1690 loss: 1.3687885e-06
Iter: 1691 loss: 1.37038955e-06
Iter: 1692 loss: 1.36877622e-06
Iter: 1693 loss: 1.36866618e-06
Iter: 1694 loss: 1.36882454e-06
Iter: 1695 loss: 1.36862207e-06
Iter: 1696 loss: 1.36850463e-06
Iter: 1697 loss: 1.36853225e-06
Iter: 1698 loss: 1.36843778e-06
Iter: 1699 loss: 1.36830329e-06
Iter: 1700 loss: 1.36882613e-06
Iter: 1701 loss: 1.36825747e-06
Iter: 1702 loss: 1.36813e-06
Iter: 1703 loss: 1.36913968e-06
Iter: 1704 loss: 1.36814469e-06
Iter: 1705 loss: 1.36805284e-06
Iter: 1706 loss: 1.36800463e-06
Iter: 1707 loss: 1.36796552e-06
Iter: 1708 loss: 1.36783876e-06
Iter: 1709 loss: 1.36796587e-06
Iter: 1710 loss: 1.36778226e-06
Iter: 1711 loss: 1.36766789e-06
Iter: 1712 loss: 1.36873791e-06
Iter: 1713 loss: 1.36764675e-06
Iter: 1714 loss: 1.3675176e-06
Iter: 1715 loss: 1.36795757e-06
Iter: 1716 loss: 1.36749122e-06
Iter: 1717 loss: 1.36740596e-06
Iter: 1718 loss: 1.36785604e-06
Iter: 1719 loss: 1.36739186e-06
Iter: 1720 loss: 1.36732365e-06
Iter: 1721 loss: 1.36721064e-06
Iter: 1722 loss: 1.36721519e-06
Iter: 1723 loss: 1.36707524e-06
Iter: 1724 loss: 1.36704466e-06
Iter: 1725 loss: 1.36698509e-06
Iter: 1726 loss: 1.36682434e-06
Iter: 1727 loss: 1.36755637e-06
Iter: 1728 loss: 1.3668016e-06
Iter: 1729 loss: 1.36669973e-06
Iter: 1730 loss: 1.36724452e-06
Iter: 1731 loss: 1.36668245e-06
Iter: 1732 loss: 1.36659503e-06
Iter: 1733 loss: 1.36760059e-06
Iter: 1734 loss: 1.36660651e-06
Iter: 1735 loss: 1.36652079e-06
Iter: 1736 loss: 1.36647145e-06
Iter: 1737 loss: 1.36644621e-06
Iter: 1738 loss: 1.36631741e-06
Iter: 1739 loss: 1.36636049e-06
Iter: 1740 loss: 1.36626477e-06
Iter: 1741 loss: 1.36612471e-06
Iter: 1742 loss: 1.36742801e-06
Iter: 1743 loss: 1.36610754e-06
Iter: 1744 loss: 1.36599806e-06
Iter: 1745 loss: 1.36630501e-06
Iter: 1746 loss: 1.36595736e-06
Iter: 1747 loss: 1.36586891e-06
Iter: 1748 loss: 1.36584526e-06
Iter: 1749 loss: 1.36578365e-06
Iter: 1750 loss: 1.36574272e-06
Iter: 1751 loss: 1.36571452e-06
Iter: 1752 loss: 1.36565882e-06
Iter: 1753 loss: 1.36554911e-06
Iter: 1754 loss: 1.36634719e-06
Iter: 1755 loss: 1.36551898e-06
Iter: 1756 loss: 1.36544827e-06
Iter: 1757 loss: 1.36542803e-06
Iter: 1758 loss: 1.36535778e-06
Iter: 1759 loss: 1.36544054e-06
Iter: 1760 loss: 1.36532424e-06
Iter: 1761 loss: 1.36527797e-06
Iter: 1762 loss: 1.36523965e-06
Iter: 1763 loss: 1.36519782e-06
Iter: 1764 loss: 1.36510766e-06
Iter: 1765 loss: 1.36526864e-06
Iter: 1766 loss: 1.36507765e-06
Iter: 1767 loss: 1.3649834e-06
Iter: 1768 loss: 1.36626306e-06
Iter: 1769 loss: 1.36497079e-06
Iter: 1770 loss: 1.36493691e-06
Iter: 1771 loss: 1.36481515e-06
Iter: 1772 loss: 1.36483e-06
Iter: 1773 loss: 1.36469396e-06
Iter: 1774 loss: 1.36502365e-06
Iter: 1775 loss: 1.36464371e-06
Iter: 1776 loss: 1.3645307e-06
Iter: 1777 loss: 1.36547374e-06
Iter: 1778 loss: 1.3645207e-06
Iter: 1779 loss: 1.36443373e-06
Iter: 1780 loss: 1.36472511e-06
Iter: 1781 loss: 1.36440178e-06
Iter: 1782 loss: 1.36428662e-06
Iter: 1783 loss: 1.36458198e-06
Iter: 1784 loss: 1.36425706e-06
Iter: 1785 loss: 1.36415201e-06
Iter: 1786 loss: 1.36535562e-06
Iter: 1787 loss: 1.36415e-06
Iter: 1788 loss: 1.3641054e-06
Iter: 1789 loss: 1.36401206e-06
Iter: 1790 loss: 1.36595747e-06
Iter: 1791 loss: 1.36402332e-06
Iter: 1792 loss: 1.36393317e-06
Iter: 1793 loss: 1.36441349e-06
Iter: 1794 loss: 1.36392e-06
Iter: 1795 loss: 1.36381254e-06
Iter: 1796 loss: 1.36409881e-06
Iter: 1797 loss: 1.36378844e-06
Iter: 1798 loss: 1.36368953e-06
Iter: 1799 loss: 1.36355413e-06
Iter: 1800 loss: 1.36672293e-06
Iter: 1801 loss: 1.36354379e-06
Iter: 1802 loss: 1.36338008e-06
Iter: 1803 loss: 1.36470135e-06
Iter: 1804 loss: 1.36337189e-06
Iter: 1805 loss: 1.36326935e-06
Iter: 1806 loss: 1.36450421e-06
Iter: 1807 loss: 1.36325843e-06
Iter: 1808 loss: 1.3631601e-06
Iter: 1809 loss: 1.36312133e-06
Iter: 1810 loss: 1.36304379e-06
Iter: 1811 loss: 1.36296148e-06
Iter: 1812 loss: 1.36308518e-06
Iter: 1813 loss: 1.36290828e-06
Iter: 1814 loss: 1.3628046e-06
Iter: 1815 loss: 1.36305903e-06
Iter: 1816 loss: 1.36274878e-06
Iter: 1817 loss: 1.36265703e-06
Iter: 1818 loss: 1.36342055e-06
Iter: 1819 loss: 1.36264794e-06
Iter: 1820 loss: 1.36254857e-06
Iter: 1821 loss: 1.3628636e-06
Iter: 1822 loss: 1.36251765e-06
Iter: 1823 loss: 1.36245637e-06
Iter: 1824 loss: 1.36285814e-06
Iter: 1825 loss: 1.36245853e-06
Iter: 1826 loss: 1.36235201e-06
Iter: 1827 loss: 1.36258723e-06
Iter: 1828 loss: 1.36231017e-06
Iter: 1829 loss: 1.36227425e-06
Iter: 1830 loss: 1.36218637e-06
Iter: 1831 loss: 1.3621916e-06
Iter: 1832 loss: 1.36211816e-06
Iter: 1833 loss: 1.36311473e-06
Iter: 1834 loss: 1.36211679e-06
Iter: 1835 loss: 1.36206086e-06
Iter: 1836 loss: 1.3620828e-06
Iter: 1837 loss: 1.3620097e-06
Iter: 1838 loss: 1.36194183e-06
Iter: 1839 loss: 1.36185599e-06
Iter: 1840 loss: 1.36186713e-06
Iter: 1841 loss: 1.36174697e-06
Iter: 1842 loss: 1.36222593e-06
Iter: 1843 loss: 1.36170218e-06
Iter: 1844 loss: 1.36159952e-06
Iter: 1845 loss: 1.36168353e-06
Iter: 1846 loss: 1.36151084e-06
Iter: 1847 loss: 1.36143558e-06
Iter: 1848 loss: 1.36142671e-06
Iter: 1849 loss: 1.36131689e-06
Iter: 1850 loss: 1.36139897e-06
Iter: 1851 loss: 1.36125232e-06
Iter: 1852 loss: 1.36116194e-06
Iter: 1853 loss: 1.36119422e-06
Iter: 1854 loss: 1.36106905e-06
Iter: 1855 loss: 1.36097583e-06
Iter: 1856 loss: 1.36123504e-06
Iter: 1857 loss: 1.36091739e-06
Iter: 1858 loss: 1.36087078e-06
Iter: 1859 loss: 1.36085305e-06
Iter: 1860 loss: 1.36081826e-06
Iter: 1861 loss: 1.36073186e-06
Iter: 1862 loss: 1.36073436e-06
Iter: 1863 loss: 1.36065671e-06
Iter: 1864 loss: 1.36097071e-06
Iter: 1865 loss: 1.36063659e-06
Iter: 1866 loss: 1.36055837e-06
Iter: 1867 loss: 1.36053245e-06
Iter: 1868 loss: 1.36049641e-06
Iter: 1869 loss: 1.36039193e-06
Iter: 1870 loss: 1.36086658e-06
Iter: 1871 loss: 1.36035749e-06
Iter: 1872 loss: 1.36023721e-06
Iter: 1873 loss: 1.36068934e-06
Iter: 1874 loss: 1.3602297e-06
Iter: 1875 loss: 1.36011818e-06
Iter: 1876 loss: 1.3600071e-06
Iter: 1877 loss: 1.36000426e-06
Iter: 1878 loss: 1.35986534e-06
Iter: 1879 loss: 1.36059703e-06
Iter: 1880 loss: 1.35984396e-06
Iter: 1881 loss: 1.35972959e-06
Iter: 1882 loss: 1.36017445e-06
Iter: 1883 loss: 1.35972118e-06
Iter: 1884 loss: 1.35964569e-06
Iter: 1885 loss: 1.36074584e-06
Iter: 1886 loss: 1.35962171e-06
Iter: 1887 loss: 1.35960045e-06
Iter: 1888 loss: 1.35946357e-06
Iter: 1889 loss: 1.36139226e-06
Iter: 1890 loss: 1.35947528e-06
Iter: 1891 loss: 1.35940672e-06
Iter: 1892 loss: 1.35939615e-06
Iter: 1893 loss: 1.35934943e-06
Iter: 1894 loss: 1.35960408e-06
Iter: 1895 loss: 1.35932203e-06
Iter: 1896 loss: 1.35929258e-06
Iter: 1897 loss: 1.35920811e-06
Iter: 1898 loss: 1.36044696e-06
Iter: 1899 loss: 1.35922278e-06
Iter: 1900 loss: 1.35912865e-06
Iter: 1901 loss: 1.3602214e-06
Iter: 1902 loss: 1.35912569e-06
Iter: 1903 loss: 1.35907169e-06
Iter: 1904 loss: 1.35909409e-06
Iter: 1905 loss: 1.35900507e-06
Iter: 1906 loss: 1.35894572e-06
Iter: 1907 loss: 1.35938853e-06
Iter: 1908 loss: 1.35893561e-06
Iter: 1909 loss: 1.35886103e-06
Iter: 1910 loss: 1.35900564e-06
Iter: 1911 loss: 1.35882772e-06
Iter: 1912 loss: 1.35876087e-06
Iter: 1913 loss: 1.3586573e-06
Iter: 1914 loss: 1.3586332e-06
Iter: 1915 loss: 1.35853043e-06
Iter: 1916 loss: 1.35881714e-06
Iter: 1917 loss: 1.35844266e-06
Iter: 1918 loss: 1.35835467e-06
Iter: 1919 loss: 1.35932612e-06
Iter: 1920 loss: 1.35832988e-06
Iter: 1921 loss: 1.35826565e-06
Iter: 1922 loss: 1.35915116e-06
Iter: 1923 loss: 1.3582312e-06
Iter: 1924 loss: 1.35821028e-06
Iter: 1925 loss: 1.35813139e-06
Iter: 1926 loss: 1.35812195e-06
Iter: 1927 loss: 1.35806e-06
Iter: 1928 loss: 1.35806602e-06
Iter: 1929 loss: 1.35799644e-06
Iter: 1930 loss: 1.35793039e-06
Iter: 1931 loss: 1.35795119e-06
Iter: 1932 loss: 1.35785399e-06
Iter: 1933 loss: 1.35783534e-06
Iter: 1934 loss: 1.35780419e-06
Iter: 1935 loss: 1.35771177e-06
Iter: 1936 loss: 1.35854054e-06
Iter: 1937 loss: 1.35771609e-06
Iter: 1938 loss: 1.35760581e-06
Iter: 1939 loss: 1.35764162e-06
Iter: 1940 loss: 1.35752521e-06
Iter: 1941 loss: 1.35744563e-06
Iter: 1942 loss: 1.35807807e-06
Iter: 1943 loss: 1.35744904e-06
Iter: 1944 loss: 1.35734138e-06
Iter: 1945 loss: 1.35734865e-06
Iter: 1946 loss: 1.35726646e-06
Iter: 1947 loss: 1.35717096e-06
Iter: 1948 loss: 1.35725259e-06
Iter: 1949 loss: 1.35711753e-06
Iter: 1950 loss: 1.35699702e-06
Iter: 1951 loss: 1.35739788e-06
Iter: 1952 loss: 1.35697428e-06
Iter: 1953 loss: 1.35690368e-06
Iter: 1954 loss: 1.3572112e-06
Iter: 1955 loss: 1.35689027e-06
Iter: 1956 loss: 1.35681955e-06
Iter: 1957 loss: 1.35762457e-06
Iter: 1958 loss: 1.35680966e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi2
+ date
Sat Oct 31 22:28:50 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 0 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d047f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d046a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d03acd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d0437730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d0464510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d04647b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d034c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d02021e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d02027b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d0315048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d02d98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d02ac488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d02ac400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d01c21e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d01df7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d0315ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d026bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d0175f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d0194d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d01752f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d0042048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d0131268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d0042a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d0150ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d037b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d0083ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34c0739510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34d00830d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34c06d30d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34c06dd0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34c06d4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34c060a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34c060a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34c0692ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34c06ff1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34c0706598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.009649253
test_loss: 0.0101376865
train_loss: 0.005607252
test_loss: 0.0066028996
train_loss: 0.003387901
test_loss: 0.003324951
train_loss: 0.0036557908
test_loss: 0.0048424057
train_loss: 0.0026169443
test_loss: 0.002676963
train_loss: 0.002284796
test_loss: 0.0026214586
train_loss: 0.002271133
test_loss: 0.002855581
train_loss: 0.0020190438
test_loss: 0.002172649
train_loss: 0.0023154588
test_loss: 0.0019449596
train_loss: 0.0017578749
test_loss: 0.002162119
train_loss: 0.0018595809
test_loss: 0.0020571772
train_loss: 0.0015535518
test_loss: 0.001832806
train_loss: 0.0015418034
test_loss: 0.0015370809
train_loss: 0.0013396692
test_loss: 0.0013413803
train_loss: 0.0013720911
test_loss: 0.0014611349
train_loss: 0.0013704316
test_loss: 0.0013512977
train_loss: 0.0012493005
test_loss: 0.0012858384
train_loss: 0.0012095307
test_loss: 0.0012759747
train_loss: 0.0011673794
test_loss: 0.001223507
train_loss: 0.0010874491
test_loss: 0.001244672
train_loss: 0.0011517085
test_loss: 0.0012230434
train_loss: 0.001102942
test_loss: 0.0011760066
train_loss: 0.0010712135
test_loss: 0.0011970073
train_loss: 0.001103014
test_loss: 0.0011284392
train_loss: 0.0010713456
test_loss: 0.001146497
train_loss: 0.0010622072
test_loss: 0.0011334696
train_loss: 0.0010407662
test_loss: 0.0011216785
train_loss: 0.0010131251
test_loss: 0.0011007256
train_loss: 0.0010276466
test_loss: 0.0010772013
train_loss: 0.0010548892
test_loss: 0.0011271484
train_loss: 0.0010319854
test_loss: 0.0010756998
train_loss: 0.0009641065
test_loss: 0.0010651929
train_loss: 0.0010006059
test_loss: 0.0010666735
train_loss: 0.0009918178
test_loss: 0.0010456411
train_loss: 0.000947573
test_loss: 0.0010441513
train_loss: 0.00095274014
test_loss: 0.0010455683
train_loss: 0.0009760505
test_loss: 0.0010278625
train_loss: 0.000939845
test_loss: 0.0010368917
train_loss: 0.0009880562
test_loss: 0.0010302322
train_loss: 0.00094845344
test_loss: 0.0010246162
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65f66f1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65f66a1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65f667a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65f65e8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65f65e8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65f65e81e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65f6572950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65f65c8158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65f6572620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea38a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea38a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea320ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea2bad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea2801e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea297f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea337d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea374d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea2b4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea1bed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea297488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea193730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea193d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea193488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea11d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea11e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea104b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea171ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea171d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea0ea488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea08f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea0a0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea0eaa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea0a0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65ea08f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65e9fe47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65e9f43f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.60455261e-06
Iter: 2 loss: 1.63989148e-06
Iter: 3 loss: 1.58506077e-06
Iter: 4 loss: 1.57138834e-06
Iter: 5 loss: 1.61931644e-06
Iter: 6 loss: 1.56783926e-06
Iter: 7 loss: 1.56115914e-06
Iter: 8 loss: 1.55716509e-06
Iter: 9 loss: 1.55439352e-06
Iter: 10 loss: 1.54697295e-06
Iter: 11 loss: 1.58620674e-06
Iter: 12 loss: 1.54585234e-06
Iter: 13 loss: 1.54271584e-06
Iter: 14 loss: 1.55316206e-06
Iter: 15 loss: 1.54177383e-06
Iter: 16 loss: 1.53814665e-06
Iter: 17 loss: 1.53739984e-06
Iter: 18 loss: 1.53495762e-06
Iter: 19 loss: 1.53253495e-06
Iter: 20 loss: 1.53253609e-06
Iter: 21 loss: 1.53109522e-06
Iter: 22 loss: 1.53091378e-06
Iter: 23 loss: 1.52990731e-06
Iter: 24 loss: 1.52786924e-06
Iter: 25 loss: 1.54717702e-06
Iter: 26 loss: 1.52780297e-06
Iter: 27 loss: 1.52662722e-06
Iter: 28 loss: 1.5265515e-06
Iter: 29 loss: 1.52566258e-06
Iter: 30 loss: 1.5239923e-06
Iter: 31 loss: 1.52731718e-06
Iter: 32 loss: 1.52329676e-06
Iter: 33 loss: 1.52159237e-06
Iter: 34 loss: 1.522866e-06
Iter: 35 loss: 1.52054315e-06
Iter: 36 loss: 1.51914605e-06
Iter: 37 loss: 1.53100109e-06
Iter: 38 loss: 1.51905601e-06
Iter: 39 loss: 1.518511e-06
Iter: 40 loss: 1.51838424e-06
Iter: 41 loss: 1.51758024e-06
Iter: 42 loss: 1.51670679e-06
Iter: 43 loss: 1.5165549e-06
Iter: 44 loss: 1.51605536e-06
Iter: 45 loss: 1.51484778e-06
Iter: 46 loss: 1.52981499e-06
Iter: 47 loss: 1.51475922e-06
Iter: 48 loss: 1.51415e-06
Iter: 49 loss: 1.51388701e-06
Iter: 50 loss: 1.51340907e-06
Iter: 51 loss: 1.51289032e-06
Iter: 52 loss: 1.51282029e-06
Iter: 53 loss: 1.51192978e-06
Iter: 54 loss: 1.51401014e-06
Iter: 55 loss: 1.51164886e-06
Iter: 56 loss: 1.51081304e-06
Iter: 57 loss: 1.51542372e-06
Iter: 58 loss: 1.51066638e-06
Iter: 59 loss: 1.50986773e-06
Iter: 60 loss: 1.50962501e-06
Iter: 61 loss: 1.50912751e-06
Iter: 62 loss: 1.50820131e-06
Iter: 63 loss: 1.50925507e-06
Iter: 64 loss: 1.50770984e-06
Iter: 65 loss: 1.50683741e-06
Iter: 66 loss: 1.50681853e-06
Iter: 67 loss: 1.50646599e-06
Iter: 68 loss: 1.50685901e-06
Iter: 69 loss: 1.50627329e-06
Iter: 70 loss: 1.50576989e-06
Iter: 71 loss: 1.5053505e-06
Iter: 72 loss: 1.50521259e-06
Iter: 73 loss: 1.50585686e-06
Iter: 74 loss: 1.50497635e-06
Iter: 75 loss: 1.50477831e-06
Iter: 76 loss: 1.50428923e-06
Iter: 77 loss: 1.50985829e-06
Iter: 78 loss: 1.50421874e-06
Iter: 79 loss: 1.50371102e-06
Iter: 80 loss: 1.50302958e-06
Iter: 81 loss: 1.50299331e-06
Iter: 82 loss: 1.50254436e-06
Iter: 83 loss: 1.50252072e-06
Iter: 84 loss: 1.50210951e-06
Iter: 85 loss: 1.50279936e-06
Iter: 86 loss: 1.50191272e-06
Iter: 87 loss: 1.50151686e-06
Iter: 88 loss: 1.50141591e-06
Iter: 89 loss: 1.50114272e-06
Iter: 90 loss: 1.50057986e-06
Iter: 91 loss: 1.50277776e-06
Iter: 92 loss: 1.50043786e-06
Iter: 93 loss: 1.50002916e-06
Iter: 94 loss: 1.50430708e-06
Iter: 95 loss: 1.50003314e-06
Iter: 96 loss: 1.49969492e-06
Iter: 97 loss: 1.49904599e-06
Iter: 98 loss: 1.51048e-06
Iter: 99 loss: 1.49902758e-06
Iter: 100 loss: 1.49874677e-06
Iter: 101 loss: 1.49869084e-06
Iter: 102 loss: 1.49836319e-06
Iter: 103 loss: 1.49828406e-06
Iter: 104 loss: 1.49810694e-06
Iter: 105 loss: 1.49766129e-06
Iter: 106 loss: 1.50051915e-06
Iter: 107 loss: 1.49762809e-06
Iter: 108 loss: 1.49738912e-06
Iter: 109 loss: 1.49733683e-06
Iter: 110 loss: 1.49725679e-06
Iter: 111 loss: 1.49690482e-06
Iter: 112 loss: 1.49678567e-06
Iter: 113 loss: 1.49649259e-06
Iter: 114 loss: 1.4960342e-06
Iter: 115 loss: 1.49602715e-06
Iter: 116 loss: 1.49575862e-06
Iter: 117 loss: 1.49640312e-06
Iter: 118 loss: 1.49567722e-06
Iter: 119 loss: 1.49528796e-06
Iter: 120 loss: 1.49617142e-06
Iter: 121 loss: 1.49516336e-06
Iter: 122 loss: 1.49479285e-06
Iter: 123 loss: 1.49470225e-06
Iter: 124 loss: 1.49448442e-06
Iter: 125 loss: 1.49402945e-06
Iter: 126 loss: 1.49642142e-06
Iter: 127 loss: 1.49396931e-06
Iter: 128 loss: 1.49360653e-06
Iter: 129 loss: 1.49559924e-06
Iter: 130 loss: 1.49355901e-06
Iter: 131 loss: 1.49314837e-06
Iter: 132 loss: 1.49253026e-06
Iter: 133 loss: 1.49254276e-06
Iter: 134 loss: 1.49219773e-06
Iter: 135 loss: 1.49220386e-06
Iter: 136 loss: 1.49184291e-06
Iter: 137 loss: 1.49181062e-06
Iter: 138 loss: 1.49156597e-06
Iter: 139 loss: 1.49148877e-06
Iter: 140 loss: 1.4913478e-06
Iter: 141 loss: 1.4911368e-06
Iter: 142 loss: 1.49087487e-06
Iter: 143 loss: 1.49084508e-06
Iter: 144 loss: 1.4905637e-06
Iter: 145 loss: 1.49000061e-06
Iter: 146 loss: 1.50036749e-06
Iter: 147 loss: 1.49002733e-06
Iter: 148 loss: 1.48942604e-06
Iter: 149 loss: 1.49424068e-06
Iter: 150 loss: 1.48937693e-06
Iter: 151 loss: 1.48891445e-06
Iter: 152 loss: 1.4934227e-06
Iter: 153 loss: 1.48889694e-06
Iter: 154 loss: 1.48864615e-06
Iter: 155 loss: 1.48907407e-06
Iter: 156 loss: 1.48851086e-06
Iter: 157 loss: 1.4881565e-06
Iter: 158 loss: 1.48739127e-06
Iter: 159 loss: 1.49969446e-06
Iter: 160 loss: 1.48737877e-06
Iter: 161 loss: 1.48684899e-06
Iter: 162 loss: 1.49406901e-06
Iter: 163 loss: 1.48686559e-06
Iter: 164 loss: 1.48634922e-06
Iter: 165 loss: 1.4880552e-06
Iter: 166 loss: 1.48619176e-06
Iter: 167 loss: 1.4857103e-06
Iter: 168 loss: 1.48652134e-06
Iter: 169 loss: 1.4855267e-06
Iter: 170 loss: 1.48500703e-06
Iter: 171 loss: 1.48591323e-06
Iter: 172 loss: 1.48481081e-06
Iter: 173 loss: 1.48437539e-06
Iter: 174 loss: 1.48995048e-06
Iter: 175 loss: 1.4843697e-06
Iter: 176 loss: 1.48404558e-06
Iter: 177 loss: 1.48781169e-06
Iter: 178 loss: 1.48405798e-06
Iter: 179 loss: 1.48374988e-06
Iter: 180 loss: 1.48312222e-06
Iter: 181 loss: 1.49187088e-06
Iter: 182 loss: 1.48311597e-06
Iter: 183 loss: 1.48272716e-06
Iter: 184 loss: 1.48237962e-06
Iter: 185 loss: 1.48227923e-06
Iter: 186 loss: 1.48178344e-06
Iter: 187 loss: 1.48910908e-06
Iter: 188 loss: 1.48178583e-06
Iter: 189 loss: 1.48151935e-06
Iter: 190 loss: 1.48339564e-06
Iter: 191 loss: 1.4815048e-06
Iter: 192 loss: 1.48121137e-06
Iter: 193 loss: 1.48080176e-06
Iter: 194 loss: 1.48077106e-06
Iter: 195 loss: 1.48044307e-06
Iter: 196 loss: 1.48385197e-06
Iter: 197 loss: 1.4804283e-06
Iter: 198 loss: 1.48015488e-06
Iter: 199 loss: 1.47988021e-06
Iter: 200 loss: 1.47982587e-06
Iter: 201 loss: 1.47937453e-06
Iter: 202 loss: 1.48462755e-06
Iter: 203 loss: 1.47936623e-06
Iter: 204 loss: 1.4790545e-06
Iter: 205 loss: 1.47972901e-06
Iter: 206 loss: 1.47892126e-06
Iter: 207 loss: 1.47862943e-06
Iter: 208 loss: 1.47882702e-06
Iter: 209 loss: 1.47844139e-06
Iter: 210 loss: 1.47821811e-06
Iter: 211 loss: 1.47818901e-06
Iter: 212 loss: 1.47790684e-06
Iter: 213 loss: 1.47768969e-06
Iter: 214 loss: 1.47758396e-06
Iter: 215 loss: 1.47730543e-06
Iter: 216 loss: 1.47822038e-06
Iter: 217 loss: 1.47724768e-06
Iter: 218 loss: 1.47706669e-06
Iter: 219 loss: 1.47658511e-06
Iter: 220 loss: 1.47985929e-06
Iter: 221 loss: 1.47648359e-06
Iter: 222 loss: 1.4760451e-06
Iter: 223 loss: 1.47607068e-06
Iter: 224 loss: 1.47572598e-06
Iter: 225 loss: 1.47783942e-06
Iter: 226 loss: 1.47567812e-06
Iter: 227 loss: 1.47534092e-06
Iter: 228 loss: 1.47570893e-06
Iter: 229 loss: 1.47514641e-06
Iter: 230 loss: 1.47484729e-06
Iter: 231 loss: 1.47493358e-06
Iter: 232 loss: 1.47465676e-06
Iter: 233 loss: 1.4743689e-06
Iter: 234 loss: 1.4764471e-06
Iter: 235 loss: 1.47433525e-06
Iter: 236 loss: 1.47403978e-06
Iter: 237 loss: 1.47394553e-06
Iter: 238 loss: 1.47375863e-06
Iter: 239 loss: 1.47343076e-06
Iter: 240 loss: 1.4734369e-06
Iter: 241 loss: 1.47323135e-06
Iter: 242 loss: 1.47299454e-06
Iter: 243 loss: 1.47296839e-06
Iter: 244 loss: 1.47271987e-06
Iter: 245 loss: 1.47265291e-06
Iter: 246 loss: 1.47254718e-06
Iter: 247 loss: 1.4723928e-06
Iter: 248 loss: 1.47240735e-06
Iter: 249 loss: 1.47215064e-06
Iter: 250 loss: 1.47216053e-06
Iter: 251 loss: 1.4719767e-06
Iter: 252 loss: 1.47161109e-06
Iter: 253 loss: 1.47169976e-06
Iter: 254 loss: 1.47133596e-06
Iter: 255 loss: 1.47092464e-06
Iter: 256 loss: 1.47140111e-06
Iter: 257 loss: 1.47072058e-06
Iter: 258 loss: 1.47032642e-06
Iter: 259 loss: 1.4740624e-06
Iter: 260 loss: 1.4702764e-06
Iter: 261 loss: 1.46989692e-06
Iter: 262 loss: 1.4713271e-06
Iter: 263 loss: 1.4697863e-06
Iter: 264 loss: 1.46945763e-06
Iter: 265 loss: 1.46996103e-06
Iter: 266 loss: 1.46929074e-06
Iter: 267 loss: 1.46902789e-06
Iter: 268 loss: 1.46858713e-06
Iter: 269 loss: 1.46858497e-06
Iter: 270 loss: 1.46833986e-06
Iter: 271 loss: 1.46828e-06
Iter: 272 loss: 1.46805701e-06
Iter: 273 loss: 1.46792559e-06
Iter: 274 loss: 1.46782554e-06
Iter: 275 loss: 1.46747743e-06
Iter: 276 loss: 1.47050923e-06
Iter: 277 loss: 1.46745015e-06
Iter: 278 loss: 1.46708817e-06
Iter: 279 loss: 1.46985622e-06
Iter: 280 loss: 1.46705293e-06
Iter: 281 loss: 1.4669863e-06
Iter: 282 loss: 1.46671368e-06
Iter: 283 loss: 1.46892137e-06
Iter: 284 loss: 1.46664343e-06
Iter: 285 loss: 1.46627985e-06
Iter: 286 loss: 1.46772595e-06
Iter: 287 loss: 1.46618686e-06
Iter: 288 loss: 1.46585478e-06
Iter: 289 loss: 1.46639991e-06
Iter: 290 loss: 1.46571949e-06
Iter: 291 loss: 1.46540492e-06
Iter: 292 loss: 1.46630441e-06
Iter: 293 loss: 1.46527043e-06
Iter: 294 loss: 1.4649695e-06
Iter: 295 loss: 1.46458251e-06
Iter: 296 loss: 1.4645567e-06
Iter: 297 loss: 1.46444643e-06
Iter: 298 loss: 1.46425168e-06
Iter: 299 loss: 1.46409639e-06
Iter: 300 loss: 1.46376146e-06
Iter: 301 loss: 1.46893353e-06
Iter: 302 loss: 1.46377238e-06
Iter: 303 loss: 1.46335583e-06
Iter: 304 loss: 1.46442596e-06
Iter: 305 loss: 1.46323384e-06
Iter: 306 loss: 1.462912e-06
Iter: 307 loss: 1.46358445e-06
Iter: 308 loss: 1.46279956e-06
Iter: 309 loss: 1.46234061e-06
Iter: 310 loss: 1.46398781e-06
Iter: 311 loss: 1.4622168e-06
Iter: 312 loss: 1.46220634e-06
Iter: 313 loss: 1.46203843e-06
Iter: 314 loss: 1.46190882e-06
Iter: 315 loss: 1.46155799e-06
Iter: 316 loss: 1.46500952e-06
Iter: 317 loss: 1.46152684e-06
Iter: 318 loss: 1.46119874e-06
Iter: 319 loss: 1.4616362e-06
Iter: 320 loss: 1.46100274e-06
Iter: 321 loss: 1.46072966e-06
Iter: 322 loss: 1.46247658e-06
Iter: 323 loss: 1.46066304e-06
Iter: 324 loss: 1.46038099e-06
Iter: 325 loss: 1.4605705e-06
Iter: 326 loss: 1.46018863e-06
Iter: 327 loss: 1.45992863e-06
Iter: 328 loss: 1.46042635e-06
Iter: 329 loss: 1.45984279e-06
Iter: 330 loss: 1.45941499e-06
Iter: 331 loss: 1.45957983e-06
Iter: 332 loss: 1.4591194e-06
Iter: 333 loss: 1.45897889e-06
Iter: 334 loss: 1.45894228e-06
Iter: 335 loss: 1.45879551e-06
Iter: 336 loss: 1.4584964e-06
Iter: 337 loss: 1.45849515e-06
Iter: 338 loss: 1.45808747e-06
Iter: 339 loss: 1.45870843e-06
Iter: 340 loss: 1.45788567e-06
Iter: 341 loss: 1.45755473e-06
Iter: 342 loss: 1.45768297e-06
Iter: 343 loss: 1.45736294e-06
Iter: 344 loss: 1.45702552e-06
Iter: 345 loss: 1.45705e-06
Iter: 346 loss: 1.45683521e-06
Iter: 347 loss: 1.45687193e-06
Iter: 348 loss: 1.45675199e-06
Iter: 349 loss: 1.45647778e-06
Iter: 350 loss: 1.4594192e-06
Iter: 351 loss: 1.45643912e-06
Iter: 352 loss: 1.45614104e-06
Iter: 353 loss: 1.45633453e-06
Iter: 354 loss: 1.45596618e-06
Iter: 355 loss: 1.45569015e-06
Iter: 356 loss: 1.45802164e-06
Iter: 357 loss: 1.4556781e-06
Iter: 358 loss: 1.45542583e-06
Iter: 359 loss: 1.45544527e-06
Iter: 360 loss: 1.45524814e-06
Iter: 361 loss: 1.45496097e-06
Iter: 362 loss: 1.45540912e-06
Iter: 363 loss: 1.45480044e-06
Iter: 364 loss: 1.45448348e-06
Iter: 365 loss: 1.45670754e-06
Iter: 366 loss: 1.45445745e-06
Iter: 367 loss: 1.454281e-06
Iter: 368 loss: 1.45498848e-06
Iter: 369 loss: 1.45425565e-06
Iter: 370 loss: 1.45395529e-06
Iter: 371 loss: 1.45366425e-06
Iter: 372 loss: 1.45360889e-06
Iter: 373 loss: 1.45329147e-06
Iter: 374 loss: 1.45530726e-06
Iter: 375 loss: 1.45324907e-06
Iter: 376 loss: 1.45301306e-06
Iter: 377 loss: 1.45301101e-06
Iter: 378 loss: 1.45277886e-06
Iter: 379 loss: 1.45275931e-06
Iter: 380 loss: 1.45263311e-06
Iter: 381 loss: 1.452481e-06
Iter: 382 loss: 1.45261504e-06
Iter: 383 loss: 1.45236629e-06
Iter: 384 loss: 1.45218712e-06
Iter: 385 loss: 1.45213255e-06
Iter: 386 loss: 1.45204331e-06
Iter: 387 loss: 1.45184515e-06
Iter: 388 loss: 1.45176318e-06
Iter: 389 loss: 1.45164074e-06
Iter: 390 loss: 1.45132776e-06
Iter: 391 loss: 1.45242916e-06
Iter: 392 loss: 1.45125273e-06
Iter: 393 loss: 1.45095407e-06
Iter: 394 loss: 1.45294234e-06
Iter: 395 loss: 1.45094987e-06
Iter: 396 loss: 1.450745e-06
Iter: 397 loss: 1.45041622e-06
Iter: 398 loss: 1.45040167e-06
Iter: 399 loss: 1.4501486e-06
Iter: 400 loss: 1.45016031e-06
Iter: 401 loss: 1.44999251e-06
Iter: 402 loss: 1.45018532e-06
Iter: 403 loss: 1.44990361e-06
Iter: 404 loss: 1.4496278e-06
Iter: 405 loss: 1.45117883e-06
Iter: 406 loss: 1.44959404e-06
Iter: 407 loss: 1.44945989e-06
Iter: 408 loss: 1.44945989e-06
Iter: 409 loss: 1.44932051e-06
Iter: 410 loss: 1.44907824e-06
Iter: 411 loss: 1.44925139e-06
Iter: 412 loss: 1.44895296e-06
Iter: 413 loss: 1.4488387e-06
Iter: 414 loss: 1.44881e-06
Iter: 415 loss: 1.44863895e-06
Iter: 416 loss: 1.44829301e-06
Iter: 417 loss: 1.44830051e-06
Iter: 418 loss: 1.44810576e-06
Iter: 419 loss: 1.44914873e-06
Iter: 420 loss: 1.44809178e-06
Iter: 421 loss: 1.44785395e-06
Iter: 422 loss: 1.44747378e-06
Iter: 423 loss: 1.4556756e-06
Iter: 424 loss: 1.44745366e-06
Iter: 425 loss: 1.44713704e-06
Iter: 426 loss: 1.45231593e-06
Iter: 427 loss: 1.44713033e-06
Iter: 428 loss: 1.44688295e-06
Iter: 429 loss: 1.44719547e-06
Iter: 430 loss: 1.44678063e-06
Iter: 431 loss: 1.44641467e-06
Iter: 432 loss: 1.44661192e-06
Iter: 433 loss: 1.44617388e-06
Iter: 434 loss: 1.4459406e-06
Iter: 435 loss: 1.44632327e-06
Iter: 436 loss: 1.44585476e-06
Iter: 437 loss: 1.44545095e-06
Iter: 438 loss: 1.44690648e-06
Iter: 439 loss: 1.44540854e-06
Iter: 440 loss: 1.44516548e-06
Iter: 441 loss: 1.44866272e-06
Iter: 442 loss: 1.44515502e-06
Iter: 443 loss: 1.44503679e-06
Iter: 444 loss: 1.44485546e-06
Iter: 445 loss: 1.4448467e-06
Iter: 446 loss: 1.44458909e-06
Iter: 447 loss: 1.44488581e-06
Iter: 448 loss: 1.44442788e-06
Iter: 449 loss: 1.44446892e-06
Iter: 450 loss: 1.44430805e-06
Iter: 451 loss: 1.44418095e-06
Iter: 452 loss: 1.44388764e-06
Iter: 453 loss: 1.44691671e-06
Iter: 454 loss: 1.4438474e-06
Iter: 455 loss: 1.44366811e-06
Iter: 456 loss: 1.44404294e-06
Iter: 457 loss: 1.44356977e-06
Iter: 458 loss: 1.4433009e-06
Iter: 459 loss: 1.4435393e-06
Iter: 460 loss: 1.44310468e-06
Iter: 461 loss: 1.44285013e-06
Iter: 462 loss: 1.4438051e-06
Iter: 463 loss: 1.44277988e-06
Iter: 464 loss: 1.44249975e-06
Iter: 465 loss: 1.44298224e-06
Iter: 466 loss: 1.44238993e-06
Iter: 467 loss: 1.44205751e-06
Iter: 468 loss: 1.44395972e-06
Iter: 469 loss: 1.44200726e-06
Iter: 470 loss: 1.44183821e-06
Iter: 471 loss: 1.44147373e-06
Iter: 472 loss: 1.44850071e-06
Iter: 473 loss: 1.44147441e-06
Iter: 474 loss: 1.44125261e-06
Iter: 475 loss: 1.44120645e-06
Iter: 476 loss: 1.44103944e-06
Iter: 477 loss: 1.44166324e-06
Iter: 478 loss: 1.44101148e-06
Iter: 479 loss: 1.44086187e-06
Iter: 480 loss: 1.44057844e-06
Iter: 481 loss: 1.44057378e-06
Iter: 482 loss: 1.44027649e-06
Iter: 483 loss: 1.44197156e-06
Iter: 484 loss: 1.44021487e-06
Iter: 485 loss: 1.44002684e-06
Iter: 486 loss: 1.43998182e-06
Iter: 487 loss: 1.43988132e-06
Iter: 488 loss: 1.4395896e-06
Iter: 489 loss: 1.44154762e-06
Iter: 490 loss: 1.43949e-06
Iter: 491 loss: 1.43916066e-06
Iter: 492 loss: 1.43975171e-06
Iter: 493 loss: 1.43902901e-06
Iter: 494 loss: 1.43866555e-06
Iter: 495 loss: 1.44153876e-06
Iter: 496 loss: 1.4386286e-06
Iter: 497 loss: 1.43842976e-06
Iter: 498 loss: 1.43840941e-06
Iter: 499 loss: 1.43825173e-06
Iter: 500 loss: 1.4380081e-06
Iter: 501 loss: 1.43931572e-06
Iter: 502 loss: 1.43792943e-06
Iter: 503 loss: 1.43771581e-06
Iter: 504 loss: 1.43863008e-06
Iter: 505 loss: 1.437675e-06
Iter: 506 loss: 1.43749162e-06
Iter: 507 loss: 1.43747263e-06
Iter: 508 loss: 1.43736543e-06
Iter: 509 loss: 1.43711225e-06
Iter: 510 loss: 1.43789123e-06
Iter: 511 loss: 1.43704142e-06
Iter: 512 loss: 1.43676561e-06
Iter: 513 loss: 1.43974808e-06
Iter: 514 loss: 1.4367871e-06
Iter: 515 loss: 1.43663226e-06
Iter: 516 loss: 1.43665557e-06
Iter: 517 loss: 1.43654142e-06
Iter: 518 loss: 1.43637783e-06
Iter: 519 loss: 1.43803823e-06
Iter: 520 loss: 1.43637044e-06
Iter: 521 loss: 1.43615569e-06
Iter: 522 loss: 1.43599459e-06
Iter: 523 loss: 1.4359241e-06
Iter: 524 loss: 1.4357397e-06
Iter: 525 loss: 1.43577631e-06
Iter: 526 loss: 1.43559987e-06
Iter: 527 loss: 1.43538512e-06
Iter: 528 loss: 1.43523243e-06
Iter: 529 loss: 1.43514649e-06
Iter: 530 loss: 1.4348509e-06
Iter: 531 loss: 1.43485863e-06
Iter: 532 loss: 1.43469879e-06
Iter: 533 loss: 1.43447585e-06
Iter: 534 loss: 1.43442958e-06
Iter: 535 loss: 1.43413263e-06
Iter: 536 loss: 1.4377581e-06
Iter: 537 loss: 1.43412865e-06
Iter: 538 loss: 1.43394027e-06
Iter: 539 loss: 1.43438251e-06
Iter: 540 loss: 1.43387e-06
Iter: 541 loss: 1.43369448e-06
Iter: 542 loss: 1.43354055e-06
Iter: 543 loss: 1.43343868e-06
Iter: 544 loss: 1.4331697e-06
Iter: 545 loss: 1.43473721e-06
Iter: 546 loss: 1.43312468e-06
Iter: 547 loss: 1.43283307e-06
Iter: 548 loss: 1.43549778e-06
Iter: 549 loss: 1.43283864e-06
Iter: 550 loss: 1.43266675e-06
Iter: 551 loss: 1.4326256e-06
Iter: 552 loss: 1.43253931e-06
Iter: 553 loss: 1.43242312e-06
Iter: 554 loss: 1.43237264e-06
Iter: 555 loss: 1.43228294e-06
Iter: 556 loss: 1.43191755e-06
Iter: 557 loss: 1.43376792e-06
Iter: 558 loss: 1.43181649e-06
Iter: 559 loss: 1.43153477e-06
Iter: 560 loss: 1.43224895e-06
Iter: 561 loss: 1.43144075e-06
Iter: 562 loss: 1.43109924e-06
Iter: 563 loss: 1.43133616e-06
Iter: 564 loss: 1.43083776e-06
Iter: 565 loss: 1.43051022e-06
Iter: 566 loss: 1.43053319e-06
Iter: 567 loss: 1.43033958e-06
Iter: 568 loss: 1.42998329e-06
Iter: 569 loss: 1.43738816e-06
Iter: 570 loss: 1.42996623e-06
Iter: 571 loss: 1.42961721e-06
Iter: 572 loss: 1.43488205e-06
Iter: 573 loss: 1.42961744e-06
Iter: 574 loss: 1.42937927e-06
Iter: 575 loss: 1.42976887e-06
Iter: 576 loss: 1.42926706e-06
Iter: 577 loss: 1.42906663e-06
Iter: 578 loss: 1.42908732e-06
Iter: 579 loss: 1.42891327e-06
Iter: 580 loss: 1.42858198e-06
Iter: 581 loss: 1.42888791e-06
Iter: 582 loss: 1.4283853e-06
Iter: 583 loss: 1.42810586e-06
Iter: 584 loss: 1.42809586e-06
Iter: 585 loss: 1.42797273e-06
Iter: 586 loss: 1.42782756e-06
Iter: 587 loss: 1.42782619e-06
Iter: 588 loss: 1.42772637e-06
Iter: 589 loss: 1.42766476e-06
Iter: 590 loss: 1.42760632e-06
Iter: 591 loss: 1.42733916e-06
Iter: 592 loss: 1.42952342e-06
Iter: 593 loss: 1.42733052e-06
Iter: 594 loss: 1.42710496e-06
Iter: 595 loss: 1.42717442e-06
Iter: 596 loss: 1.42698718e-06
Iter: 597 loss: 1.42669012e-06
Iter: 598 loss: 1.42749764e-06
Iter: 599 loss: 1.42657746e-06
Iter: 600 loss: 1.42630233e-06
Iter: 601 loss: 1.42831948e-06
Iter: 602 loss: 1.42626971e-06
Iter: 603 loss: 1.4260263e-06
Iter: 604 loss: 1.42609724e-06
Iter: 605 loss: 1.42581484e-06
Iter: 606 loss: 1.42550834e-06
Iter: 607 loss: 1.42596809e-06
Iter: 608 loss: 1.42535953e-06
Iter: 609 loss: 1.42507361e-06
Iter: 610 loss: 1.42712156e-06
Iter: 611 loss: 1.42503313e-06
Iter: 612 loss: 1.42480224e-06
Iter: 613 loss: 1.42559281e-06
Iter: 614 loss: 1.42475e-06
Iter: 615 loss: 1.42451586e-06
Iter: 616 loss: 1.42434328e-06
Iter: 617 loss: 1.42429235e-06
Iter: 618 loss: 1.42413842e-06
Iter: 619 loss: 1.4241125e-06
Iter: 620 loss: 1.42394765e-06
Iter: 621 loss: 1.42399801e-06
Iter: 622 loss: 1.42385841e-06
Iter: 623 loss: 1.42365639e-06
Iter: 624 loss: 1.42607735e-06
Iter: 625 loss: 1.4236482e-06
Iter: 626 loss: 1.42355213e-06
Iter: 627 loss: 1.42326383e-06
Iter: 628 loss: 1.42554256e-06
Iter: 629 loss: 1.42319664e-06
Iter: 630 loss: 1.42294562e-06
Iter: 631 loss: 1.42368719e-06
Iter: 632 loss: 1.42285126e-06
Iter: 633 loss: 1.42260467e-06
Iter: 634 loss: 1.4229646e-06
Iter: 635 loss: 1.42246972e-06
Iter: 636 loss: 1.42209581e-06
Iter: 637 loss: 1.42318447e-06
Iter: 638 loss: 1.42197246e-06
Iter: 639 loss: 1.42175224e-06
Iter: 640 loss: 1.42491058e-06
Iter: 641 loss: 1.42176054e-06
Iter: 642 loss: 1.42154113e-06
Iter: 643 loss: 1.42127544e-06
Iter: 644 loss: 1.42129375e-06
Iter: 645 loss: 1.42104432e-06
Iter: 646 loss: 1.42442013e-06
Iter: 647 loss: 1.42101e-06
Iter: 648 loss: 1.42084536e-06
Iter: 649 loss: 1.42095848e-06
Iter: 650 loss: 1.42075248e-06
Iter: 651 loss: 1.42049294e-06
Iter: 652 loss: 1.42135445e-06
Iter: 653 loss: 1.42039767e-06
Iter: 654 loss: 1.42020201e-06
Iter: 655 loss: 1.4212643e-06
Iter: 656 loss: 1.42015801e-06
Iter: 657 loss: 1.42002114e-06
Iter: 658 loss: 1.42226304e-06
Iter: 659 loss: 1.42001159e-06
Iter: 660 loss: 1.41987141e-06
Iter: 661 loss: 1.42009094e-06
Iter: 662 loss: 1.41981218e-06
Iter: 663 loss: 1.4197459e-06
Iter: 664 loss: 1.41941928e-06
Iter: 665 loss: 1.42166516e-06
Iter: 666 loss: 1.41936869e-06
Iter: 667 loss: 1.41906412e-06
Iter: 668 loss: 1.42106239e-06
Iter: 669 loss: 1.41903161e-06
Iter: 670 loss: 1.4188123e-06
Iter: 671 loss: 1.41884902e-06
Iter: 672 loss: 1.41864382e-06
Iter: 673 loss: 1.41824444e-06
Iter: 674 loss: 1.42040881e-06
Iter: 675 loss: 1.41820169e-06
Iter: 676 loss: 1.41794067e-06
Iter: 677 loss: 1.4194502e-06
Iter: 678 loss: 1.41792407e-06
Iter: 679 loss: 1.41772284e-06
Iter: 680 loss: 1.41780663e-06
Iter: 681 loss: 1.41756209e-06
Iter: 682 loss: 1.41731721e-06
Iter: 683 loss: 1.41770579e-06
Iter: 684 loss: 1.41718374e-06
Iter: 685 loss: 1.41688338e-06
Iter: 686 loss: 1.41858334e-06
Iter: 687 loss: 1.41682949e-06
Iter: 688 loss: 1.41659098e-06
Iter: 689 loss: 1.41688656e-06
Iter: 690 loss: 1.41644455e-06
Iter: 691 loss: 1.4161385e-06
Iter: 692 loss: 1.4176685e-06
Iter: 693 loss: 1.41611213e-06
Iter: 694 loss: 1.41606222e-06
Iter: 695 loss: 1.41599776e-06
Iter: 696 loss: 1.41593318e-06
Iter: 697 loss: 1.41589214e-06
Iter: 698 loss: 1.415871e-06
Iter: 699 loss: 1.4157398e-06
Iter: 700 loss: 1.41545411e-06
Iter: 701 loss: 1.41773364e-06
Iter: 702 loss: 1.41539635e-06
Iter: 703 loss: 1.41507758e-06
Iter: 704 loss: 1.41687167e-06
Iter: 705 loss: 1.41504199e-06
Iter: 706 loss: 1.41477403e-06
Iter: 707 loss: 1.41537112e-06
Iter: 708 loss: 1.41467297e-06
Iter: 709 loss: 1.4143767e-06
Iter: 710 loss: 1.41529677e-06
Iter: 711 loss: 1.41428e-06
Iter: 712 loss: 1.41404519e-06
Iter: 713 loss: 1.41594239e-06
Iter: 714 loss: 1.4139988e-06
Iter: 715 loss: 1.41387216e-06
Iter: 716 loss: 1.41447697e-06
Iter: 717 loss: 1.41379678e-06
Iter: 718 loss: 1.41361636e-06
Iter: 719 loss: 1.41337625e-06
Iter: 720 loss: 1.41334226e-06
Iter: 721 loss: 1.41305213e-06
Iter: 722 loss: 1.41645273e-06
Iter: 723 loss: 1.41302144e-06
Iter: 724 loss: 1.41279452e-06
Iter: 725 loss: 1.41331657e-06
Iter: 726 loss: 1.4127279e-06
Iter: 727 loss: 1.41248393e-06
Iter: 728 loss: 1.41303758e-06
Iter: 729 loss: 1.41241094e-06
Iter: 730 loss: 1.4122827e-06
Iter: 731 loss: 1.41226099e-06
Iter: 732 loss: 1.41214559e-06
Iter: 733 loss: 1.41231249e-06
Iter: 734 loss: 1.41207568e-06
Iter: 735 loss: 1.41192641e-06
Iter: 736 loss: 1.41184034e-06
Iter: 737 loss: 1.41179601e-06
Iter: 738 loss: 1.4116099e-06
Iter: 739 loss: 1.41127043e-06
Iter: 740 loss: 1.41125861e-06
Iter: 741 loss: 1.4108989e-06
Iter: 742 loss: 1.41413079e-06
Iter: 743 loss: 1.41088117e-06
Iter: 744 loss: 1.4106522e-06
Iter: 745 loss: 1.41072724e-06
Iter: 746 loss: 1.4104744e-06
Iter: 747 loss: 1.41018791e-06
Iter: 748 loss: 1.41341616e-06
Iter: 749 loss: 1.41015641e-06
Iter: 750 loss: 1.40997543e-06
Iter: 751 loss: 1.41030978e-06
Iter: 752 loss: 1.40990244e-06
Iter: 753 loss: 1.40961311e-06
Iter: 754 loss: 1.40972793e-06
Iter: 755 loss: 1.40941643e-06
Iter: 756 loss: 1.40911538e-06
Iter: 757 loss: 1.41086e-06
Iter: 758 loss: 1.40911766e-06
Iter: 759 loss: 1.40892507e-06
Iter: 760 loss: 1.40979409e-06
Iter: 761 loss: 1.40883935e-06
Iter: 762 loss: 1.40869849e-06
Iter: 763 loss: 1.40903148e-06
Iter: 764 loss: 1.4086047e-06
Iter: 765 loss: 1.40839597e-06
Iter: 766 loss: 1.40906741e-06
Iter: 767 loss: 1.40835755e-06
Iter: 768 loss: 1.40829775e-06
Iter: 769 loss: 1.40824477e-06
Iter: 770 loss: 1.40821021e-06
Iter: 771 loss: 1.4081171e-06
Iter: 772 loss: 1.40814609e-06
Iter: 773 loss: 1.40802058e-06
Iter: 774 loss: 1.40781026e-06
Iter: 775 loss: 1.41136161e-06
Iter: 776 loss: 1.4077898e-06
Iter: 777 loss: 1.40750944e-06
Iter: 778 loss: 1.4084917e-06
Iter: 779 loss: 1.40742327e-06
Iter: 780 loss: 1.40723967e-06
Iter: 781 loss: 1.4077558e-06
Iter: 782 loss: 1.40717316e-06
Iter: 783 loss: 1.40691134e-06
Iter: 784 loss: 1.40757743e-06
Iter: 785 loss: 1.40683301e-06
Iter: 786 loss: 1.40653515e-06
Iter: 787 loss: 1.40788018e-06
Iter: 788 loss: 1.40646307e-06
Iter: 789 loss: 1.40627935e-06
Iter: 790 loss: 1.40717714e-06
Iter: 791 loss: 1.40624115e-06
Iter: 792 loss: 1.40605459e-06
Iter: 793 loss: 1.40588213e-06
Iter: 794 loss: 1.40585394e-06
Iter: 795 loss: 1.40558882e-06
Iter: 796 loss: 1.4086952e-06
Iter: 797 loss: 1.40562e-06
Iter: 798 loss: 1.40538623e-06
Iter: 799 loss: 1.40560701e-06
Iter: 800 loss: 1.40530267e-06
Iter: 801 loss: 1.40503494e-06
Iter: 802 loss: 1.4058154e-06
Iter: 803 loss: 1.40499048e-06
Iter: 804 loss: 1.40496081e-06
Iter: 805 loss: 1.40488828e-06
Iter: 806 loss: 1.40479551e-06
Iter: 807 loss: 1.40470695e-06
Iter: 808 loss: 1.40470365e-06
Iter: 809 loss: 1.4045595e-06
Iter: 810 loss: 1.40458087e-06
Iter: 811 loss: 1.40444104e-06
Iter: 812 loss: 1.40426664e-06
Iter: 813 loss: 1.40414022e-06
Iter: 814 loss: 1.4040852e-06
Iter: 815 loss: 1.40384e-06
Iter: 816 loss: 1.40528118e-06
Iter: 817 loss: 1.40380416e-06
Iter: 818 loss: 1.40355439e-06
Iter: 819 loss: 1.40351631e-06
Iter: 820 loss: 1.40336056e-06
Iter: 821 loss: 1.40312432e-06
Iter: 822 loss: 1.40312727e-06
Iter: 823 loss: 1.40297243e-06
Iter: 824 loss: 1.40317763e-06
Iter: 825 loss: 1.40288364e-06
Iter: 826 loss: 1.4026873e-06
Iter: 827 loss: 1.4027944e-06
Iter: 828 loss: 1.40252791e-06
Iter: 829 loss: 1.40232225e-06
Iter: 830 loss: 1.40315956e-06
Iter: 831 loss: 1.40224915e-06
Iter: 832 loss: 1.40204418e-06
Iter: 833 loss: 1.40308032e-06
Iter: 834 loss: 1.40201792e-06
Iter: 835 loss: 1.40186e-06
Iter: 836 loss: 1.40214252e-06
Iter: 837 loss: 1.40178281e-06
Iter: 838 loss: 1.40156044e-06
Iter: 839 loss: 1.40236e-06
Iter: 840 loss: 1.40152679e-06
Iter: 841 loss: 1.40136353e-06
Iter: 842 loss: 1.40138911e-06
Iter: 843 loss: 1.40128702e-06
Iter: 844 loss: 1.40120255e-06
Iter: 845 loss: 1.40120449e-06
Iter: 846 loss: 1.40105089e-06
Iter: 847 loss: 1.40082761e-06
Iter: 848 loss: 1.40081897e-06
Iter: 849 loss: 1.40056443e-06
Iter: 850 loss: 1.40190321e-06
Iter: 851 loss: 1.40054328e-06
Iter: 852 loss: 1.40034206e-06
Iter: 853 loss: 1.40052248e-06
Iter: 854 loss: 1.40026168e-06
Iter: 855 loss: 1.40001021e-06
Iter: 856 loss: 1.40073257e-06
Iter: 857 loss: 1.39991721e-06
Iter: 858 loss: 1.39969711e-06
Iter: 859 loss: 1.40166549e-06
Iter: 860 loss: 1.39969461e-06
Iter: 861 loss: 1.39955227e-06
Iter: 862 loss: 1.39985013e-06
Iter: 863 loss: 1.3994719e-06
Iter: 864 loss: 1.39927806e-06
Iter: 865 loss: 1.39922849e-06
Iter: 866 loss: 1.39911413e-06
Iter: 867 loss: 1.39889926e-06
Iter: 868 loss: 1.40083534e-06
Iter: 869 loss: 1.39885583e-06
Iter: 870 loss: 1.39867666e-06
Iter: 871 loss: 1.39863027e-06
Iter: 872 loss: 1.39849681e-06
Iter: 873 loss: 1.39826273e-06
Iter: 874 loss: 1.40079169e-06
Iter: 875 loss: 1.39825806e-06
Iter: 876 loss: 1.39820827e-06
Iter: 877 loss: 1.3981894e-06
Iter: 878 loss: 1.39812801e-06
Iter: 879 loss: 1.39796771e-06
Iter: 880 loss: 1.39991289e-06
Iter: 881 loss: 1.39792201e-06
Iter: 882 loss: 1.39775341e-06
Iter: 883 loss: 1.39858253e-06
Iter: 884 loss: 1.39774261e-06
Iter: 885 loss: 1.39755457e-06
Iter: 886 loss: 1.39749443e-06
Iter: 887 loss: 1.39744043e-06
Iter: 888 loss: 1.39725501e-06
Iter: 889 loss: 1.39771714e-06
Iter: 890 loss: 1.39721055e-06
Iter: 891 loss: 1.39699341e-06
Iter: 892 loss: 1.39685471e-06
Iter: 893 loss: 1.39679184e-06
Iter: 894 loss: 1.39659164e-06
Iter: 895 loss: 1.39659e-06
Iter: 896 loss: 1.39641202e-06
Iter: 897 loss: 1.39653639e-06
Iter: 898 loss: 1.39632016e-06
Iter: 899 loss: 1.39614667e-06
Iter: 900 loss: 1.39662609e-06
Iter: 901 loss: 1.39604936e-06
Iter: 902 loss: 1.39586359e-06
Iter: 903 loss: 1.39621579e-06
Iter: 904 loss: 1.39576468e-06
Iter: 905 loss: 1.39559097e-06
Iter: 906 loss: 1.39652377e-06
Iter: 907 loss: 1.3955671e-06
Iter: 908 loss: 1.3954101e-06
Iter: 909 loss: 1.39543567e-06
Iter: 910 loss: 1.39526992e-06
Iter: 911 loss: 1.39520125e-06
Iter: 912 loss: 1.39515203e-06
Iter: 913 loss: 1.39504436e-06
Iter: 914 loss: 1.39501753e-06
Iter: 915 loss: 1.39496842e-06
Iter: 916 loss: 1.39487156e-06
Iter: 917 loss: 1.39479607e-06
Iter: 918 loss: 1.39477822e-06
Iter: 919 loss: 1.39459712e-06
Iter: 920 loss: 1.39488077e-06
Iter: 921 loss: 1.3945438e-06
Iter: 922 loss: 1.39440044e-06
Iter: 923 loss: 1.39461793e-06
Iter: 924 loss: 1.39433473e-06
Iter: 925 loss: 1.39415499e-06
Iter: 926 loss: 1.39402914e-06
Iter: 927 loss: 1.39395411e-06
Iter: 928 loss: 1.39367489e-06
Iter: 929 loss: 1.39491453e-06
Iter: 930 loss: 1.39364192e-06
Iter: 931 loss: 1.39341387e-06
Iter: 932 loss: 1.39550195e-06
Iter: 933 loss: 1.39341955e-06
Iter: 934 loss: 1.39322356e-06
Iter: 935 loss: 1.39347662e-06
Iter: 936 loss: 1.39314398e-06
Iter: 937 loss: 1.39290751e-06
Iter: 938 loss: 1.39339318e-06
Iter: 939 loss: 1.39282042e-06
Iter: 940 loss: 1.39263136e-06
Iter: 941 loss: 1.39311919e-06
Iter: 942 loss: 1.39260203e-06
Iter: 943 loss: 1.39235192e-06
Iter: 944 loss: 1.39282838e-06
Iter: 945 loss: 1.39223152e-06
Iter: 946 loss: 1.3922313e-06
Iter: 947 loss: 1.39213671e-06
Iter: 948 loss: 1.39205986e-06
Iter: 949 loss: 1.39201757e-06
Iter: 950 loss: 1.39200608e-06
Iter: 951 loss: 1.3918791e-06
Iter: 952 loss: 1.391767e-06
Iter: 953 loss: 1.39177882e-06
Iter: 954 loss: 1.39161637e-06
Iter: 955 loss: 1.39305462e-06
Iter: 956 loss: 1.3915751e-06
Iter: 957 loss: 1.39144709e-06
Iter: 958 loss: 1.39129588e-06
Iter: 959 loss: 1.39130088e-06
Iter: 960 loss: 1.3911075e-06
Iter: 961 loss: 1.39164547e-06
Iter: 962 loss: 1.39102917e-06
Iter: 963 loss: 1.3907819e-06
Iter: 964 loss: 1.39094527e-06
Iter: 965 loss: 1.39061422e-06
Iter: 966 loss: 1.39040742e-06
Iter: 967 loss: 1.39114832e-06
Iter: 968 loss: 1.39033955e-06
Iter: 969 loss: 1.39008557e-06
Iter: 970 loss: 1.39107578e-06
Iter: 971 loss: 1.39002395e-06
Iter: 972 loss: 1.38981784e-06
Iter: 973 loss: 1.39148347e-06
Iter: 974 loss: 1.38980567e-06
Iter: 975 loss: 1.38966118e-06
Iter: 976 loss: 1.3894678e-06
Iter: 977 loss: 1.38945484e-06
Iter: 978 loss: 1.38925509e-06
Iter: 979 loss: 1.39219355e-06
Iter: 980 loss: 1.3892527e-06
Iter: 981 loss: 1.38918858e-06
Iter: 982 loss: 1.38916789e-06
Iter: 983 loss: 1.38907319e-06
Iter: 984 loss: 1.38893176e-06
Iter: 985 loss: 1.38895439e-06
Iter: 986 loss: 1.38881671e-06
Iter: 987 loss: 1.38891619e-06
Iter: 988 loss: 1.38873543e-06
Iter: 989 loss: 1.38856763e-06
Iter: 990 loss: 1.38893654e-06
Iter: 991 loss: 1.38849236e-06
Iter: 992 loss: 1.38831774e-06
Iter: 993 loss: 1.38911037e-06
Iter: 994 loss: 1.38828716e-06
Iter: 995 loss: 1.3881812e-06
Iter: 996 loss: 1.38789915e-06
Iter: 997 loss: 1.39237977e-06
Iter: 998 loss: 1.38789289e-06
Iter: 999 loss: 1.38770201e-06
Iter: 1000 loss: 1.38768951e-06
Iter: 1001 loss: 1.38755559e-06
Iter: 1002 loss: 1.3873555e-06
Iter: 1003 loss: 1.3873879e-06
Iter: 1004 loss: 1.38714199e-06
Iter: 1005 loss: 1.38833707e-06
Iter: 1006 loss: 1.38710925e-06
Iter: 1007 loss: 1.38692474e-06
Iter: 1008 loss: 1.38864425e-06
Iter: 1009 loss: 1.38691848e-06
Iter: 1010 loss: 1.38675955e-06
Iter: 1011 loss: 1.38699795e-06
Iter: 1012 loss: 1.3867068e-06
Iter: 1013 loss: 1.38652138e-06
Iter: 1014 loss: 1.38716e-06
Iter: 1015 loss: 1.38648409e-06
Iter: 1016 loss: 1.38629537e-06
Iter: 1017 loss: 1.38886412e-06
Iter: 1018 loss: 1.38630162e-06
Iter: 1019 loss: 1.38625228e-06
Iter: 1020 loss: 1.3861146e-06
Iter: 1021 loss: 1.38691507e-06
Iter: 1022 loss: 1.38604833e-06
Iter: 1023 loss: 1.38584733e-06
Iter: 1024 loss: 1.38680616e-06
Iter: 1025 loss: 1.38580492e-06
Iter: 1026 loss: 1.3856021e-06
Iter: 1027 loss: 1.38667224e-06
Iter: 1028 loss: 1.38557766e-06
Iter: 1029 loss: 1.38547546e-06
Iter: 1030 loss: 1.38535268e-06
Iter: 1031 loss: 1.3853105e-06
Iter: 1032 loss: 1.38511507e-06
Iter: 1033 loss: 1.38571636e-06
Iter: 1034 loss: 1.38503765e-06
Iter: 1035 loss: 1.38489895e-06
Iter: 1036 loss: 1.38549524e-06
Iter: 1037 loss: 1.38480414e-06
Iter: 1038 loss: 1.38460814e-06
Iter: 1039 loss: 1.38586313e-06
Iter: 1040 loss: 1.3846203e-06
Iter: 1041 loss: 1.3844658e-06
Iter: 1042 loss: 1.38428038e-06
Iter: 1043 loss: 1.38427663e-06
Iter: 1044 loss: 1.38405289e-06
Iter: 1045 loss: 1.38534017e-06
Iter: 1046 loss: 1.38401424e-06
Iter: 1047 loss: 1.38381006e-06
Iter: 1048 loss: 1.3846402e-06
Iter: 1049 loss: 1.38376186e-06
Iter: 1050 loss: 1.38363976e-06
Iter: 1051 loss: 1.38364226e-06
Iter: 1052 loss: 1.383523e-06
Iter: 1053 loss: 1.38344433e-06
Iter: 1054 loss: 1.38339544e-06
Iter: 1055 loss: 1.3832813e-06
Iter: 1056 loss: 1.38302289e-06
Iter: 1057 loss: 1.3873846e-06
Iter: 1058 loss: 1.38301505e-06
Iter: 1059 loss: 1.38283633e-06
Iter: 1060 loss: 1.38283178e-06
Iter: 1061 loss: 1.3826733e-06
Iter: 1062 loss: 1.38266114e-06
Iter: 1063 loss: 1.38254882e-06
Iter: 1064 loss: 1.38236351e-06
Iter: 1065 loss: 1.38270798e-06
Iter: 1066 loss: 1.38228268e-06
Iter: 1067 loss: 1.38208475e-06
Iter: 1068 loss: 1.38202745e-06
Iter: 1069 loss: 1.38190944e-06
Iter: 1070 loss: 1.38175108e-06
Iter: 1071 loss: 1.38175074e-06
Iter: 1072 loss: 1.38163091e-06
Iter: 1073 loss: 1.38143048e-06
Iter: 1074 loss: 1.38143514e-06
Iter: 1075 loss: 1.38114933e-06
Iter: 1076 loss: 1.38213113e-06
Iter: 1077 loss: 1.3810868e-06
Iter: 1078 loss: 1.38081418e-06
Iter: 1079 loss: 1.38160897e-06
Iter: 1080 loss: 1.380752e-06
Iter: 1081 loss: 1.38068299e-06
Iter: 1082 loss: 1.38062683e-06
Iter: 1083 loss: 1.38051678e-06
Iter: 1084 loss: 1.38060739e-06
Iter: 1085 loss: 1.38044265e-06
Iter: 1086 loss: 1.38030623e-06
Iter: 1087 loss: 1.38010705e-06
Iter: 1088 loss: 1.38011558e-06
Iter: 1089 loss: 1.37992879e-06
Iter: 1090 loss: 1.38022267e-06
Iter: 1091 loss: 1.37984193e-06
Iter: 1092 loss: 1.37959296e-06
Iter: 1093 loss: 1.38082885e-06
Iter: 1094 loss: 1.37955135e-06
Iter: 1095 loss: 1.37932125e-06
Iter: 1096 loss: 1.37965174e-06
Iter: 1097 loss: 1.37920608e-06
Iter: 1098 loss: 1.37904613e-06
Iter: 1099 loss: 1.37887469e-06
Iter: 1100 loss: 1.37883762e-06
Iter: 1101 loss: 1.37862139e-06
Iter: 1102 loss: 1.38187897e-06
Iter: 1103 loss: 1.37859979e-06
Iter: 1104 loss: 1.37839038e-06
Iter: 1105 loss: 1.37880784e-06
Iter: 1106 loss: 1.37832285e-06
Iter: 1107 loss: 1.37808729e-06
Iter: 1108 loss: 1.37867642e-06
Iter: 1109 loss: 1.37802454e-06
Iter: 1110 loss: 1.37783854e-06
Iter: 1111 loss: 1.37801624e-06
Iter: 1112 loss: 1.37773077e-06
Iter: 1113 loss: 1.37759594e-06
Iter: 1114 loss: 1.37760105e-06
Iter: 1115 loss: 1.3774943e-06
Iter: 1116 loss: 1.37855113e-06
Iter: 1117 loss: 1.37749612e-06
Iter: 1118 loss: 1.37741722e-06
Iter: 1119 loss: 1.37724237e-06
Iter: 1120 loss: 1.37898382e-06
Iter: 1121 loss: 1.37723737e-06
Iter: 1122 loss: 1.37705547e-06
Iter: 1123 loss: 1.37775282e-06
Iter: 1124 loss: 1.3769951e-06
Iter: 1125 loss: 1.37684265e-06
Iter: 1126 loss: 1.3769336e-06
Iter: 1127 loss: 1.37675011e-06
Iter: 1128 loss: 1.37656821e-06
Iter: 1129 loss: 1.37842994e-06
Iter: 1130 loss: 1.37654752e-06
Iter: 1131 loss: 1.37641337e-06
Iter: 1132 loss: 1.37622237e-06
Iter: 1133 loss: 1.37618395e-06
Iter: 1134 loss: 1.376005e-06
Iter: 1135 loss: 1.37721076e-06
Iter: 1136 loss: 1.37599307e-06
Iter: 1137 loss: 1.37581412e-06
Iter: 1138 loss: 1.37561562e-06
Iter: 1139 loss: 1.37557299e-06
Iter: 1140 loss: 1.37541065e-06
Iter: 1141 loss: 1.37540235e-06
Iter: 1142 loss: 1.37523602e-06
Iter: 1143 loss: 1.37515588e-06
Iter: 1144 loss: 1.37510142e-06
Iter: 1145 loss: 1.37486234e-06
Iter: 1146 loss: 1.37511336e-06
Iter: 1147 loss: 1.37473717e-06
Iter: 1148 loss: 1.3744816e-06
Iter: 1149 loss: 1.37593361e-06
Iter: 1150 loss: 1.37443772e-06
Iter: 1151 loss: 1.37448058e-06
Iter: 1152 loss: 1.3743927e-06
Iter: 1153 loss: 1.37431016e-06
Iter: 1154 loss: 1.37417442e-06
Iter: 1155 loss: 1.37476741e-06
Iter: 1156 loss: 1.37412667e-06
Iter: 1157 loss: 1.37394318e-06
Iter: 1158 loss: 1.37427469e-06
Iter: 1159 loss: 1.37385632e-06
Iter: 1160 loss: 1.37366897e-06
Iter: 1161 loss: 1.3747881e-06
Iter: 1162 loss: 1.37366465e-06
Iter: 1163 loss: 1.37352015e-06
Iter: 1164 loss: 1.3734707e-06
Iter: 1165 loss: 1.37337645e-06
Iter: 1166 loss: 1.37321263e-06
Iter: 1167 loss: 1.37321126e-06
Iter: 1168 loss: 1.37312702e-06
Iter: 1169 loss: 1.37307757e-06
Iter: 1170 loss: 1.37301697e-06
Iter: 1171 loss: 1.37283564e-06
Iter: 1172 loss: 1.37289271e-06
Iter: 1173 loss: 1.37269149e-06
Iter: 1174 loss: 1.3724482e-06
Iter: 1175 loss: 1.37294637e-06
Iter: 1176 loss: 1.37233587e-06
Iter: 1177 loss: 1.37214022e-06
Iter: 1178 loss: 1.37247014e-06
Iter: 1179 loss: 1.37204972e-06
Iter: 1180 loss: 1.37171958e-06
Iter: 1181 loss: 1.37311395e-06
Iter: 1182 loss: 1.37167262e-06
Iter: 1183 loss: 1.37150391e-06
Iter: 1184 loss: 1.37151915e-06
Iter: 1185 loss: 1.37142217e-06
Iter: 1186 loss: 1.37201744e-06
Iter: 1187 loss: 1.37139477e-06
Iter: 1188 loss: 1.37125824e-06
Iter: 1189 loss: 1.3710835e-06
Iter: 1190 loss: 1.37109669e-06
Iter: 1191 loss: 1.37095151e-06
Iter: 1192 loss: 1.37080951e-06
Iter: 1193 loss: 1.37079087e-06
Iter: 1194 loss: 1.37053212e-06
Iter: 1195 loss: 1.37130928e-06
Iter: 1196 loss: 1.37049619e-06
Iter: 1197 loss: 1.37031668e-06
Iter: 1198 loss: 1.37228301e-06
Iter: 1199 loss: 1.37029951e-06
Iter: 1200 loss: 1.37013046e-06
Iter: 1201 loss: 1.37006134e-06
Iter: 1202 loss: 1.36996971e-06
Iter: 1203 loss: 1.36974973e-06
Iter: 1204 loss: 1.37092104e-06
Iter: 1205 loss: 1.36971403e-06
Iter: 1206 loss: 1.36953099e-06
Iter: 1207 loss: 1.36991741e-06
Iter: 1208 loss: 1.36944e-06
Iter: 1209 loss: 1.36927201e-06
Iter: 1210 loss: 1.36959238e-06
Iter: 1211 loss: 1.36921062e-06
Iter: 1212 loss: 1.36901963e-06
Iter: 1213 loss: 1.36894789e-06
Iter: 1214 loss: 1.36884387e-06
Iter: 1215 loss: 1.36858012e-06
Iter: 1216 loss: 1.37009818e-06
Iter: 1217 loss: 1.36856181e-06
Iter: 1218 loss: 1.36832909e-06
Iter: 1219 loss: 1.36914161e-06
Iter: 1220 loss: 1.36827759e-06
Iter: 1221 loss: 1.3682328e-06
Iter: 1222 loss: 1.36818039e-06
Iter: 1223 loss: 1.36811127e-06
Iter: 1224 loss: 1.36805602e-06
Iter: 1225 loss: 1.36798656e-06
Iter: 1226 loss: 1.36787548e-06
Iter: 1227 loss: 1.36777533e-06
Iter: 1228 loss: 1.36772098e-06
Iter: 1229 loss: 1.36758842e-06
Iter: 1230 loss: 1.36733104e-06
Iter: 1231 loss: 1.37374923e-06
Iter: 1232 loss: 1.36733092e-06
Iter: 1233 loss: 1.36710128e-06
Iter: 1234 loss: 1.36712174e-06
Iter: 1235 loss: 1.36697338e-06
Iter: 1236 loss: 1.36742415e-06
Iter: 1237 loss: 1.36692233e-06
Iter: 1238 loss: 1.36675931e-06
Iter: 1239 loss: 1.36674964e-06
Iter: 1240 loss: 1.36662118e-06
Iter: 1241 loss: 1.36653512e-06
Iter: 1242 loss: 1.36653034e-06
Iter: 1243 loss: 1.36644405e-06
Iter: 1244 loss: 1.36630877e-06
Iter: 1245 loss: 1.36965195e-06
Iter: 1246 loss: 1.36631456e-06
Iter: 1247 loss: 1.36614062e-06
Iter: 1248 loss: 1.36635845e-06
Iter: 1249 loss: 1.36603012e-06
Iter: 1250 loss: 1.36578387e-06
Iter: 1251 loss: 1.36661492e-06
Iter: 1252 loss: 1.36576978e-06
Iter: 1253 loss: 1.36558219e-06
Iter: 1254 loss: 1.36624431e-06
Iter: 1255 loss: 1.36552535e-06
Iter: 1256 loss: 1.36532856e-06
Iter: 1257 loss: 1.36586232e-06
Iter: 1258 loss: 1.36528888e-06
Iter: 1259 loss: 1.3651786e-06
Iter: 1260 loss: 1.36513154e-06
Iter: 1261 loss: 1.36506537e-06
Iter: 1262 loss: 1.36493713e-06
Iter: 1263 loss: 1.3673033e-06
Iter: 1264 loss: 1.36494384e-06
Iter: 1265 loss: 1.36473591e-06
Iter: 1266 loss: 1.36458266e-06
Iter: 1267 loss: 1.36453468e-06
Iter: 1268 loss: 1.36429389e-06
Iter: 1269 loss: 1.36504991e-06
Iter: 1270 loss: 1.36423421e-06
Iter: 1271 loss: 1.36405333e-06
Iter: 1272 loss: 1.36557867e-06
Iter: 1273 loss: 1.36402423e-06
Iter: 1274 loss: 1.36384313e-06
Iter: 1275 loss: 1.36425365e-06
Iter: 1276 loss: 1.36377412e-06
Iter: 1277 loss: 1.36359085e-06
Iter: 1278 loss: 1.36390838e-06
Iter: 1279 loss: 1.36349604e-06
Iter: 1280 loss: 1.36333176e-06
Iter: 1281 loss: 1.36528126e-06
Iter: 1282 loss: 1.3633412e-06
Iter: 1283 loss: 1.36320898e-06
Iter: 1284 loss: 1.36291e-06
Iter: 1285 loss: 1.36613346e-06
Iter: 1286 loss: 1.36290066e-06
Iter: 1287 loss: 1.36268636e-06
Iter: 1288 loss: 1.36269182e-06
Iter: 1289 loss: 1.36251776e-06
Iter: 1290 loss: 1.36237441e-06
Iter: 1291 loss: 1.36232893e-06
Iter: 1292 loss: 1.36233291e-06
Iter: 1293 loss: 1.36222764e-06
Iter: 1294 loss: 1.36212327e-06
Iter: 1295 loss: 1.36230574e-06
Iter: 1296 loss: 1.36206165e-06
Iter: 1297 loss: 1.3619607e-06
Iter: 1298 loss: 1.36177027e-06
Iter: 1299 loss: 1.36572453e-06
Iter: 1300 loss: 1.36179392e-06
Iter: 1301 loss: 1.36160656e-06
Iter: 1302 loss: 1.36242943e-06
Iter: 1303 loss: 1.36158019e-06
Iter: 1304 loss: 1.36136214e-06
Iter: 1305 loss: 1.36130711e-06
Iter: 1306 loss: 1.3612007e-06
Iter: 1307 loss: 1.36095628e-06
Iter: 1308 loss: 1.3612912e-06
Iter: 1309 loss: 1.36088408e-06
Iter: 1310 loss: 1.36062272e-06
Iter: 1311 loss: 1.36293681e-06
Iter: 1312 loss: 1.36061772e-06
Iter: 1313 loss: 1.36044173e-06
Iter: 1314 loss: 1.3615944e-06
Iter: 1315 loss: 1.36042468e-06
Iter: 1316 loss: 1.3602596e-06
Iter: 1317 loss: 1.36008362e-06
Iter: 1318 loss: 1.36007952e-06
Iter: 1319 loss: 1.35985078e-06
Iter: 1320 loss: 1.3598418e-06
Iter: 1321 loss: 1.35974278e-06
Iter: 1322 loss: 1.35963819e-06
Iter: 1323 loss: 1.35959567e-06
Iter: 1324 loss: 1.3594248e-06
Iter: 1325 loss: 1.36006338e-06
Iter: 1326 loss: 1.35936273e-06
Iter: 1327 loss: 1.35933954e-06
Iter: 1328 loss: 1.35927769e-06
Iter: 1329 loss: 1.35920686e-06
Iter: 1330 loss: 1.35906646e-06
Iter: 1331 loss: 1.36150015e-06
Iter: 1332 loss: 1.35905543e-06
Iter: 1333 loss: 1.35889729e-06
Iter: 1334 loss: 1.35908795e-06
Iter: 1335 loss: 1.35882181e-06
Iter: 1336 loss: 1.35862911e-06
Iter: 1337 loss: 1.35874325e-06
Iter: 1338 loss: 1.3585003e-06
Iter: 1339 loss: 1.35830055e-06
Iter: 1340 loss: 1.36015865e-06
Iter: 1341 loss: 1.35830555e-06
Iter: 1342 loss: 1.35819914e-06
Iter: 1343 loss: 1.35789173e-06
Iter: 1344 loss: 1.3616143e-06
Iter: 1345 loss: 1.35786013e-06
Iter: 1346 loss: 1.35777339e-06
Iter: 1347 loss: 1.35775292e-06
Iter: 1348 loss: 1.35762343e-06
Iter: 1349 loss: 1.35779874e-06
Iter: 1350 loss: 1.35751384e-06
Iter: 1351 loss: 1.35735945e-06
Iter: 1352 loss: 1.35752134e-06
Iter: 1353 loss: 1.35726054e-06
Iter: 1354 loss: 1.3571281e-06
Iter: 1355 loss: 1.35816776e-06
Iter: 1356 loss: 1.3571032e-06
Iter: 1357 loss: 1.35695268e-06
Iter: 1358 loss: 1.3568249e-06
Iter: 1359 loss: 1.35680477e-06
Iter: 1360 loss: 1.35660889e-06
Iter: 1361 loss: 1.35701771e-06
Iter: 1362 loss: 1.35652829e-06
Iter: 1363 loss: 1.35648156e-06
Iter: 1364 loss: 1.35640789e-06
Iter: 1365 loss: 1.35634116e-06
Iter: 1366 loss: 1.35616119e-06
Iter: 1367 loss: 1.35768278e-06
Iter: 1368 loss: 1.35611765e-06
Iter: 1369 loss: 1.35594e-06
Iter: 1370 loss: 1.35601681e-06
Iter: 1371 loss: 1.35581968e-06
Iter: 1372 loss: 1.35561083e-06
Iter: 1373 loss: 1.35813104e-06
Iter: 1374 loss: 1.35563039e-06
Iter: 1375 loss: 1.35549863e-06
Iter: 1376 loss: 1.35530627e-06
Iter: 1377 loss: 1.35531468e-06
Iter: 1378 loss: 1.35504615e-06
Iter: 1379 loss: 1.35681898e-06
Iter: 1380 loss: 1.35500977e-06
Iter: 1381 loss: 1.35483219e-06
Iter: 1382 loss: 1.35491575e-06
Iter: 1383 loss: 1.35472965e-06
Iter: 1384 loss: 1.35451228e-06
Iter: 1385 loss: 1.35692483e-06
Iter: 1386 loss: 1.35453217e-06
Iter: 1387 loss: 1.35435516e-06
Iter: 1388 loss: 1.35486664e-06
Iter: 1389 loss: 1.35430469e-06
Iter: 1390 loss: 1.35418384e-06
Iter: 1391 loss: 1.35438859e-06
Iter: 1392 loss: 1.35412574e-06
Iter: 1393 loss: 1.35398409e-06
Iter: 1394 loss: 1.35431458e-06
Iter: 1395 loss: 1.35397249e-06
Iter: 1396 loss: 1.35385631e-06
Iter: 1397 loss: 1.35501682e-06
Iter: 1398 loss: 1.35386824e-06
Iter: 1399 loss: 1.35374967e-06
Iter: 1400 loss: 1.35390201e-06
Iter: 1401 loss: 1.35373477e-06
Iter: 1402 loss: 1.35368396e-06
Iter: 1403 loss: 1.35353991e-06
Iter: 1404 loss: 1.35496168e-06
Iter: 1405 loss: 1.35353139e-06
Iter: 1406 loss: 1.35341929e-06
Iter: 1407 loss: 1.35404809e-06
Iter: 1408 loss: 1.35337268e-06
Iter: 1409 loss: 1.35325524e-06
Iter: 1410 loss: 1.3541553e-06
Iter: 1411 loss: 1.35322443e-06
Iter: 1412 loss: 1.35314531e-06
Iter: 1413 loss: 1.35297239e-06
Iter: 1414 loss: 1.35297239e-06
Iter: 1415 loss: 1.35282767e-06
Iter: 1416 loss: 1.35444043e-06
Iter: 1417 loss: 1.35282562e-06
Iter: 1418 loss: 1.35263554e-06
Iter: 1419 loss: 1.35278663e-06
Iter: 1420 loss: 1.35257392e-06
Iter: 1421 loss: 1.35239543e-06
Iter: 1422 loss: 1.35475204e-06
Iter: 1423 loss: 1.35238065e-06
Iter: 1424 loss: 1.35233608e-06
Iter: 1425 loss: 1.3522947e-06
Iter: 1426 loss: 1.35225241e-06
Iter: 1427 loss: 1.35212167e-06
Iter: 1428 loss: 1.35267578e-06
Iter: 1429 loss: 1.3520621e-06
Iter: 1430 loss: 1.35196331e-06
Iter: 1431 loss: 1.35275843e-06
Iter: 1432 loss: 1.35195501e-06
Iter: 1433 loss: 1.35188179e-06
Iter: 1434 loss: 1.3526269e-06
Iter: 1435 loss: 1.35187543e-06
Iter: 1436 loss: 1.35181313e-06
Iter: 1437 loss: 1.35172695e-06
Iter: 1438 loss: 1.35238372e-06
Iter: 1439 loss: 1.35169421e-06
Iter: 1440 loss: 1.35155278e-06
Iter: 1441 loss: 1.35164089e-06
Iter: 1442 loss: 1.35146183e-06
Iter: 1443 loss: 1.35134894e-06
Iter: 1444 loss: 1.35134383e-06
Iter: 1445 loss: 1.35126334e-06
Iter: 1446 loss: 1.3512722e-06
Iter: 1447 loss: 1.35116841e-06
Iter: 1448 loss: 1.35104051e-06
Iter: 1449 loss: 1.35126618e-06
Iter: 1450 loss: 1.35095934e-06
Iter: 1451 loss: 1.35083155e-06
Iter: 1452 loss: 1.35082246e-06
Iter: 1453 loss: 1.35071696e-06
Iter: 1454 loss: 1.35051619e-06
Iter: 1455 loss: 1.35052392e-06
Iter: 1456 loss: 1.35044854e-06
Iter: 1457 loss: 1.3504939e-06
Iter: 1458 loss: 1.35038385e-06
Iter: 1459 loss: 1.35024379e-06
Iter: 1460 loss: 1.35047594e-06
Iter: 1461 loss: 1.35021719e-06
Iter: 1462 loss: 1.35013624e-06
Iter: 1463 loss: 1.35011237e-06
Iter: 1464 loss: 1.35007326e-06
Iter: 1465 loss: 1.35055393e-06
Iter: 1466 loss: 1.35008065e-06
Iter: 1467 loss: 1.35004325e-06
Iter: 1468 loss: 1.34992342e-06
Iter: 1469 loss: 1.35040329e-06
Iter: 1470 loss: 1.34989523e-06
Iter: 1471 loss: 1.34978984e-06
Iter: 1472 loss: 1.34999505e-06
Iter: 1473 loss: 1.34974516e-06
Iter: 1474 loss: 1.34958532e-06
Iter: 1475 loss: 1.34984361e-06
Iter: 1476 loss: 1.34953871e-06
Iter: 1477 loss: 1.34942616e-06
Iter: 1478 loss: 1.35088385e-06
Iter: 1479 loss: 1.34942582e-06
Iter: 1480 loss: 1.34934191e-06
Iter: 1481 loss: 1.34919401e-06
Iter: 1482 loss: 1.34916922e-06
Iter: 1483 loss: 1.34906691e-06
Iter: 1484 loss: 1.34965467e-06
Iter: 1485 loss: 1.34903144e-06
Iter: 1486 loss: 1.34894253e-06
Iter: 1487 loss: 1.34948368e-06
Iter: 1488 loss: 1.34892525e-06
Iter: 1489 loss: 1.34880611e-06
Iter: 1490 loss: 1.34937397e-06
Iter: 1491 loss: 1.34879554e-06
Iter: 1492 loss: 1.34869379e-06
Iter: 1493 loss: 1.34865695e-06
Iter: 1494 loss: 1.34864206e-06
Iter: 1495 loss: 1.34852326e-06
Iter: 1496 loss: 1.34978143e-06
Iter: 1497 loss: 1.34851132e-06
Iter: 1498 loss: 1.34847824e-06
Iter: 1499 loss: 1.34845368e-06
Iter: 1500 loss: 1.34845425e-06
Iter: 1501 loss: 1.34832e-06
Iter: 1502 loss: 1.34928337e-06
Iter: 1503 loss: 1.34829907e-06
Iter: 1504 loss: 1.34815161e-06
Iter: 1505 loss: 1.34843503e-06
Iter: 1506 loss: 1.34810659e-06
Iter: 1507 loss: 1.34799029e-06
Iter: 1508 loss: 1.34801553e-06
Iter: 1509 loss: 1.34789366e-06
Iter: 1510 loss: 1.34777247e-06
Iter: 1511 loss: 1.34909919e-06
Iter: 1512 loss: 1.34780805e-06
Iter: 1513 loss: 1.34765037e-06
Iter: 1514 loss: 1.34777565e-06
Iter: 1515 loss: 1.34759989e-06
Iter: 1516 loss: 1.3474629e-06
Iter: 1517 loss: 1.34787126e-06
Iter: 1518 loss: 1.34742572e-06
Iter: 1519 loss: 1.34730521e-06
Iter: 1520 loss: 1.34720722e-06
Iter: 1521 loss: 1.34715901e-06
Iter: 1522 loss: 1.34709944e-06
Iter: 1523 loss: 1.34706136e-06
Iter: 1524 loss: 1.34697711e-06
Iter: 1525 loss: 1.34688548e-06
Iter: 1526 loss: 1.3468632e-06
Iter: 1527 loss: 1.34673292e-06
Iter: 1528 loss: 1.34687957e-06
Iter: 1529 loss: 1.34663787e-06
Iter: 1530 loss: 1.34668517e-06
Iter: 1531 loss: 1.34656875e-06
Iter: 1532 loss: 1.34653101e-06
Iter: 1533 loss: 1.3465135e-06
Iter: 1534 loss: 1.34647985e-06
Iter: 1535 loss: 1.34641664e-06
Iter: 1536 loss: 1.34631205e-06
Iter: 1537 loss: 1.34629602e-06
Iter: 1538 loss: 1.34618836e-06
Iter: 1539 loss: 1.34620586e-06
Iter: 1540 loss: 1.34608752e-06
Iter: 1541 loss: 1.34592392e-06
Iter: 1542 loss: 1.34713127e-06
Iter: 1543 loss: 1.34594779e-06
Iter: 1544 loss: 1.34581524e-06
Iter: 1545 loss: 1.34604352e-06
Iter: 1546 loss: 1.34576521e-06
Iter: 1547 loss: 1.34561594e-06
Iter: 1548 loss: 1.34613617e-06
Iter: 1549 loss: 1.34556797e-06
Iter: 1550 loss: 1.34545417e-06
Iter: 1551 loss: 1.34558263e-06
Iter: 1552 loss: 1.34538436e-06
Iter: 1553 loss: 1.3452584e-06
Iter: 1554 loss: 1.34561446e-06
Iter: 1555 loss: 1.34523657e-06
Iter: 1556 loss: 1.34511049e-06
Iter: 1557 loss: 1.34512084e-06
Iter: 1558 loss: 1.34506899e-06
Iter: 1559 loss: 1.34490438e-06
Iter: 1560 loss: 1.34693801e-06
Iter: 1561 loss: 1.34490051e-06
Iter: 1562 loss: 1.34494871e-06
Iter: 1563 loss: 1.34484412e-06
Iter: 1564 loss: 1.34479865e-06
Iter: 1565 loss: 1.34485288e-06
Iter: 1566 loss: 1.34473748e-06
Iter: 1567 loss: 1.34468883e-06
Iter: 1568 loss: 1.34457059e-06
Iter: 1569 loss: 1.34637389e-06
Iter: 1570 loss: 1.34455991e-06
Iter: 1571 loss: 1.34442894e-06
Iter: 1572 loss: 1.34501181e-06
Iter: 1573 loss: 1.34439415e-06
Iter: 1574 loss: 1.34428751e-06
Iter: 1575 loss: 1.34427046e-06
Iter: 1576 loss: 1.34420611e-06
Iter: 1577 loss: 1.3440465e-06
Iter: 1578 loss: 1.34505331e-06
Iter: 1579 loss: 1.34401103e-06
Iter: 1580 loss: 1.34388335e-06
Iter: 1581 loss: 1.34426409e-06
Iter: 1582 loss: 1.34385709e-06
Iter: 1583 loss: 1.34374898e-06
Iter: 1584 loss: 1.34439813e-06
Iter: 1585 loss: 1.34374591e-06
Iter: 1586 loss: 1.3436445e-06
Iter: 1587 loss: 1.34350375e-06
Iter: 1588 loss: 1.34350648e-06
Iter: 1589 loss: 1.34341042e-06
Iter: 1590 loss: 1.34339655e-06
Iter: 1591 loss: 1.34330116e-06
Iter: 1592 loss: 1.34333527e-06
Iter: 1593 loss: 1.34322636e-06
Iter: 1594 loss: 1.34310062e-06
Iter: 1595 loss: 1.34378683e-06
Iter: 1596 loss: 1.34309084e-06
Iter: 1597 loss: 1.34298284e-06
Iter: 1598 loss: 1.34460788e-06
Iter: 1599 loss: 1.34298875e-06
Iter: 1600 loss: 1.34293964e-06
Iter: 1601 loss: 1.342877e-06
Iter: 1602 loss: 1.34285222e-06
Iter: 1603 loss: 1.34271727e-06
Iter: 1604 loss: 1.34260154e-06
Iter: 1605 loss: 1.3425838e-06
Iter: 1606 loss: 1.34246932e-06
Iter: 1607 loss: 1.34313416e-06
Iter: 1608 loss: 1.34242862e-06
Iter: 1609 loss: 1.34225763e-06
Iter: 1610 loss: 1.34224751e-06
Iter: 1611 loss: 1.34213292e-06
Iter: 1612 loss: 1.34201377e-06
Iter: 1613 loss: 1.34201332e-06
Iter: 1614 loss: 1.34193033e-06
Iter: 1615 loss: 1.34188372e-06
Iter: 1616 loss: 1.3418246e-06
Iter: 1617 loss: 1.34169773e-06
Iter: 1618 loss: 1.34223978e-06
Iter: 1619 loss: 1.34167431e-06
Iter: 1620 loss: 1.34155289e-06
Iter: 1621 loss: 1.34171978e-06
Iter: 1622 loss: 1.34147558e-06
Iter: 1623 loss: 1.34139623e-06
Iter: 1624 loss: 1.3413927e-06
Iter: 1625 loss: 1.34132756e-06
Iter: 1626 loss: 1.34120057e-06
Iter: 1627 loss: 1.34450966e-06
Iter: 1628 loss: 1.34118716e-06
Iter: 1629 loss: 1.34111247e-06
Iter: 1630 loss: 1.34106972e-06
Iter: 1631 loss: 1.34103288e-06
Iter: 1632 loss: 1.34092738e-06
Iter: 1633 loss: 1.34266122e-06
Iter: 1634 loss: 1.34089555e-06
Iter: 1635 loss: 1.34078482e-06
Iter: 1636 loss: 1.34095319e-06
Iter: 1637 loss: 1.34074821e-06
Iter: 1638 loss: 1.34056768e-06
Iter: 1639 loss: 1.34080153e-06
Iter: 1640 loss: 1.34050356e-06
Iter: 1641 loss: 1.34035736e-06
Iter: 1642 loss: 1.3402132e-06
Iter: 1643 loss: 1.3401999e-06
Iter: 1644 loss: 1.34009383e-06
Iter: 1645 loss: 1.34008087e-06
Iter: 1646 loss: 1.33998537e-06
Iter: 1647 loss: 1.33998822e-06
Iter: 1648 loss: 1.33992876e-06
Iter: 1649 loss: 1.33976789e-06
Iter: 1650 loss: 1.33994251e-06
Iter: 1651 loss: 1.33969161e-06
Iter: 1652 loss: 1.33955473e-06
Iter: 1653 loss: 1.34071786e-06
Iter: 1654 loss: 1.33953267e-06
Iter: 1655 loss: 1.33942444e-06
Iter: 1656 loss: 1.33997855e-06
Iter: 1657 loss: 1.33939125e-06
Iter: 1658 loss: 1.33931667e-06
Iter: 1659 loss: 1.33943126e-06
Iter: 1660 loss: 1.33925596e-06
Iter: 1661 loss: 1.33915034e-06
Iter: 1662 loss: 1.33917968e-06
Iter: 1663 loss: 1.33908463e-06
Iter: 1664 loss: 1.33906804e-06
Iter: 1665 loss: 1.33901813e-06
Iter: 1666 loss: 1.33894571e-06
Iter: 1667 loss: 1.33887943e-06
Iter: 1668 loss: 1.33887545e-06
Iter: 1669 loss: 1.33875733e-06
Iter: 1670 loss: 1.33907793e-06
Iter: 1671 loss: 1.33872368e-06
Iter: 1672 loss: 1.33859862e-06
Iter: 1673 loss: 1.33866206e-06
Iter: 1674 loss: 1.33852814e-06
Iter: 1675 loss: 1.33835169e-06
Iter: 1676 loss: 1.33874391e-06
Iter: 1677 loss: 1.33830326e-06
Iter: 1678 loss: 1.33816604e-06
Iter: 1679 loss: 1.33816798e-06
Iter: 1680 loss: 1.33806611e-06
Iter: 1681 loss: 1.33792037e-06
Iter: 1682 loss: 1.33791673e-06
Iter: 1683 loss: 1.33782589e-06
Iter: 1684 loss: 1.33775234e-06
Iter: 1685 loss: 1.33770061e-06
Iter: 1686 loss: 1.33753713e-06
Iter: 1687 loss: 1.33816752e-06
Iter: 1688 loss: 1.33748176e-06
Iter: 1689 loss: 1.33735466e-06
Iter: 1690 loss: 1.33785727e-06
Iter: 1691 loss: 1.33733329e-06
Iter: 1692 loss: 1.33719459e-06
Iter: 1693 loss: 1.33787375e-06
Iter: 1694 loss: 1.33715025e-06
Iter: 1695 loss: 1.3371224e-06
Iter: 1696 loss: 1.33711444e-06
Iter: 1697 loss: 1.33704316e-06
Iter: 1698 loss: 1.33698927e-06
Iter: 1699 loss: 1.33719391e-06
Iter: 1700 loss: 1.33693709e-06
Iter: 1701 loss: 1.33678827e-06
Iter: 1702 loss: 1.33694925e-06
Iter: 1703 loss: 1.33670028e-06
Iter: 1704 loss: 1.33651156e-06
Iter: 1705 loss: 1.33733965e-06
Iter: 1706 loss: 1.33650701e-06
Iter: 1707 loss: 1.33632432e-06
Iter: 1708 loss: 1.33671847e-06
Iter: 1709 loss: 1.33628623e-06
Iter: 1710 loss: 1.33614412e-06
Iter: 1711 loss: 1.33634592e-06
Iter: 1712 loss: 1.33603658e-06
Iter: 1713 loss: 1.33588048e-06
Iter: 1714 loss: 1.33616197e-06
Iter: 1715 loss: 1.33576782e-06
Iter: 1716 loss: 1.33560627e-06
Iter: 1717 loss: 1.33622109e-06
Iter: 1718 loss: 1.33557273e-06
Iter: 1719 loss: 1.33539993e-06
Iter: 1720 loss: 1.3359944e-06
Iter: 1721 loss: 1.33534195e-06
Iter: 1722 loss: 1.3351962e-06
Iter: 1723 loss: 1.33581011e-06
Iter: 1724 loss: 1.3351696e-06
Iter: 1725 loss: 1.33503477e-06
Iter: 1726 loss: 1.33485992e-06
Iter: 1727 loss: 1.33991034e-06
Iter: 1728 loss: 1.33486617e-06
Iter: 1729 loss: 1.33474657e-06
Iter: 1730 loss: 1.33470405e-06
Iter: 1731 loss: 1.33455273e-06
Iter: 1732 loss: 1.33542085e-06
Iter: 1733 loss: 1.33456865e-06
Iter: 1734 loss: 1.33444223e-06
Iter: 1735 loss: 1.33473509e-06
Iter: 1736 loss: 1.33437311e-06
Iter: 1737 loss: 1.33426863e-06
Iter: 1738 loss: 1.33403398e-06
Iter: 1739 loss: 1.33701053e-06
Iter: 1740 loss: 1.33402727e-06
Iter: 1741 loss: 1.33376511e-06
Iter: 1742 loss: 1.33417393e-06
Iter: 1743 loss: 1.33364028e-06
Iter: 1744 loss: 1.33342553e-06
Iter: 1745 loss: 1.33579283e-06
Iter: 1746 loss: 1.33340154e-06
Iter: 1747 loss: 1.33323056e-06
Iter: 1748 loss: 1.33318667e-06
Iter: 1749 loss: 1.33304775e-06
Iter: 1750 loss: 1.33284436e-06
Iter: 1751 loss: 1.33383651e-06
Iter: 1752 loss: 1.33281401e-06
Iter: 1753 loss: 1.33260801e-06
Iter: 1754 loss: 1.3326196e-06
Iter: 1755 loss: 1.33245317e-06
Iter: 1756 loss: 1.33227786e-06
Iter: 1757 loss: 1.33482615e-06
Iter: 1758 loss: 1.33226058e-06
Iter: 1759 loss: 1.33210176e-06
Iter: 1760 loss: 1.33218919e-06
Iter: 1761 loss: 1.33200751e-06
Iter: 1762 loss: 1.33178014e-06
Iter: 1763 loss: 1.33223352e-06
Iter: 1764 loss: 1.33167532e-06
Iter: 1765 loss: 1.33144522e-06
Iter: 1766 loss: 1.3317192e-06
Iter: 1767 loss: 1.33138656e-06
Iter: 1768 loss: 1.33118692e-06
Iter: 1769 loss: 1.33302251e-06
Iter: 1770 loss: 1.33118624e-06
Iter: 1771 loss: 1.33104e-06
Iter: 1772 loss: 1.33189656e-06
Iter: 1773 loss: 1.33099184e-06
Iter: 1774 loss: 1.33090782e-06
Iter: 1775 loss: 1.3309e-06
Iter: 1776 loss: 1.33080653e-06
Iter: 1777 loss: 1.33065737e-06
Iter: 1778 loss: 1.33416756e-06
Iter: 1779 loss: 1.33065737e-06
Iter: 1780 loss: 1.33047797e-06
Iter: 1781 loss: 1.3303254e-06
Iter: 1782 loss: 1.33498111e-06
Iter: 1783 loss: 1.3302581e-06
Iter: 1784 loss: 1.33011463e-06
Iter: 1785 loss: 1.33009121e-06
Iter: 1786 loss: 1.32993318e-06
Iter: 1787 loss: 1.32981518e-06
Iter: 1788 loss: 1.32977993e-06
Iter: 1789 loss: 1.3295604e-06
Iter: 1790 loss: 1.33059427e-06
Iter: 1791 loss: 1.32952198e-06
Iter: 1792 loss: 1.32936043e-06
Iter: 1793 loss: 1.32962032e-06
Iter: 1794 loss: 1.32927e-06
Iter: 1795 loss: 1.32905416e-06
Iter: 1796 loss: 1.33015715e-06
Iter: 1797 loss: 1.32906666e-06
Iter: 1798 loss: 1.32890443e-06
Iter: 1799 loss: 1.32902267e-06
Iter: 1800 loss: 1.32878313e-06
Iter: 1801 loss: 1.32861464e-06
Iter: 1802 loss: 1.32931712e-06
Iter: 1803 loss: 1.32856405e-06
Iter: 1804 loss: 1.32843104e-06
Iter: 1805 loss: 1.32859191e-06
Iter: 1806 loss: 1.32835657e-06
Iter: 1807 loss: 1.32818889e-06
Iter: 1808 loss: 1.32819946e-06
Iter: 1809 loss: 1.32813989e-06
Iter: 1810 loss: 1.32892637e-06
Iter: 1811 loss: 1.3281475e-06
Iter: 1812 loss: 1.32807941e-06
Iter: 1813 loss: 1.32795185e-06
Iter: 1814 loss: 1.32897105e-06
Iter: 1815 loss: 1.32791524e-06
Iter: 1816 loss: 1.32774767e-06
Iter: 1817 loss: 1.32765467e-06
Iter: 1818 loss: 1.32756918e-06
Iter: 1819 loss: 1.32745913e-06
Iter: 1820 loss: 1.32745174e-06
Iter: 1821 loss: 1.32735795e-06
Iter: 1822 loss: 1.32712103e-06
Iter: 1823 loss: 1.33157539e-06
Iter: 1824 loss: 1.32708794e-06
Iter: 1825 loss: 1.32690388e-06
Iter: 1826 loss: 1.32896798e-06
Iter: 1827 loss: 1.32690661e-06
Iter: 1828 loss: 1.32675518e-06
Iter: 1829 loss: 1.32667924e-06
Iter: 1830 loss: 1.32658681e-06
Iter: 1831 loss: 1.32636433e-06
Iter: 1832 loss: 1.32916534e-06
Iter: 1833 loss: 1.3263799e-06
Iter: 1834 loss: 1.32625962e-06
Iter: 1835 loss: 1.32617799e-06
Iter: 1836 loss: 1.32610785e-06
Iter: 1837 loss: 1.32591254e-06
Iter: 1838 loss: 1.32699392e-06
Iter: 1839 loss: 1.32587513e-06
Iter: 1840 loss: 1.32571108e-06
Iter: 1841 loss: 1.32587888e-06
Iter: 1842 loss: 1.32562e-06
Iter: 1843 loss: 1.32562877e-06
Iter: 1844 loss: 1.32553498e-06
Iter: 1845 loss: 1.32547734e-06
Iter: 1846 loss: 1.32539412e-06
Iter: 1847 loss: 1.32539867e-06
Iter: 1848 loss: 1.32525975e-06
Iter: 1849 loss: 1.32513799e-06
Iter: 1850 loss: 1.32511116e-06
Iter: 1851 loss: 1.32494154e-06
Iter: 1852 loss: 1.32580249e-06
Iter: 1853 loss: 1.32488844e-06
Iter: 1854 loss: 1.32475907e-06
Iter: 1855 loss: 1.32497189e-06
Iter: 1856 loss: 1.32468233e-06
Iter: 1857 loss: 1.32449111e-06
Iter: 1858 loss: 1.32508603e-06
Iter: 1859 loss: 1.32443006e-06
Iter: 1860 loss: 1.32426703e-06
Iter: 1861 loss: 1.32411208e-06
Iter: 1862 loss: 1.32407126e-06
Iter: 1863 loss: 1.32384935e-06
Iter: 1864 loss: 1.32548325e-06
Iter: 1865 loss: 1.32380706e-06
Iter: 1866 loss: 1.323574e-06
Iter: 1867 loss: 1.32466403e-06
Iter: 1868 loss: 1.32354944e-06
Iter: 1869 loss: 1.32338482e-06
Iter: 1870 loss: 1.32427022e-06
Iter: 1871 loss: 1.32336186e-06
Iter: 1872 loss: 1.32325897e-06
Iter: 1873 loss: 1.323079e-06
Iter: 1874 loss: 1.32726e-06
Iter: 1875 loss: 1.32308173e-06
Iter: 1876 loss: 1.32327443e-06
Iter: 1877 loss: 1.32301534e-06
Iter: 1878 loss: 1.32294804e-06
Iter: 1879 loss: 1.32292371e-06
Iter: 1880 loss: 1.3228854e-06
Iter: 1881 loss: 1.32283753e-06
Iter: 1882 loss: 1.32272214e-06
Iter: 1883 loss: 1.32273465e-06
Iter: 1884 loss: 1.3225565e-06
Iter: 1885 loss: 1.32276318e-06
Iter: 1886 loss: 1.32247465e-06
Iter: 1887 loss: 1.32230718e-06
Iter: 1888 loss: 1.32245941e-06
Iter: 1889 loss: 1.32225728e-06
Iter: 1890 loss: 1.32206105e-06
Iter: 1891 loss: 1.32297362e-06
Iter: 1892 loss: 1.32200512e-06
Iter: 1893 loss: 1.32183663e-06
Iter: 1894 loss: 1.32235698e-06
Iter: 1895 loss: 1.32180071e-06
Iter: 1896 loss: 1.32166895e-06
Iter: 1897 loss: 1.32152297e-06
Iter: 1898 loss: 1.32149955e-06
Iter: 1899 loss: 1.32125251e-06
Iter: 1900 loss: 1.32211949e-06
Iter: 1901 loss: 1.3212242e-06
Iter: 1902 loss: 1.32103946e-06
Iter: 1903 loss: 1.32272885e-06
Iter: 1904 loss: 1.3210165e-06
Iter: 1905 loss: 1.32092794e-06
Iter: 1906 loss: 1.32139166e-06
Iter: 1907 loss: 1.32091282e-06
Iter: 1908 loss: 1.32081459e-06
Iter: 1909 loss: 1.32062746e-06
Iter: 1910 loss: 1.32371031e-06
Iter: 1911 loss: 1.32060154e-06
Iter: 1912 loss: 1.32099422e-06
Iter: 1913 loss: 1.32052207e-06
Iter: 1914 loss: 1.32050286e-06
Iter: 1915 loss: 1.32037826e-06
Iter: 1916 loss: 1.32086552e-06
Iter: 1917 loss: 1.32034268e-06
Iter: 1918 loss: 1.32017692e-06
Iter: 1919 loss: 1.32021614e-06
Iter: 1920 loss: 1.32005721e-06
Iter: 1921 loss: 1.31983029e-06
Iter: 1922 loss: 1.32126308e-06
Iter: 1923 loss: 1.31981187e-06
Iter: 1924 loss: 1.31965271e-06
Iter: 1925 loss: 1.31959951e-06
Iter: 1926 loss: 1.31955494e-06
Iter: 1927 loss: 1.31931358e-06
Iter: 1928 loss: 1.32029766e-06
Iter: 1929 loss: 1.31930312e-06
Iter: 1930 loss: 1.31912634e-06
Iter: 1931 loss: 1.31993079e-06
Iter: 1932 loss: 1.31911122e-06
Iter: 1933 loss: 1.31892489e-06
Iter: 1934 loss: 1.31883917e-06
Iter: 1935 loss: 1.31878642e-06
Iter: 1936 loss: 1.31856314e-06
Iter: 1937 loss: 1.31875959e-06
Iter: 1938 loss: 1.31843365e-06
Iter: 1939 loss: 1.31833497e-06
Iter: 1940 loss: 1.31828938e-06
Iter: 1941 loss: 1.31821025e-06
Iter: 1942 loss: 1.31799493e-06
Iter: 1943 loss: 1.32134949e-06
Iter: 1944 loss: 1.31798117e-06
Iter: 1945 loss: 1.31779484e-06
Iter: 1946 loss: 1.3196086e-06
Iter: 1947 loss: 1.31773709e-06
Iter: 1948 loss: 1.31759771e-06
Iter: 1949 loss: 1.3178468e-06
Iter: 1950 loss: 1.31751938e-06
Iter: 1951 loss: 1.31730053e-06
Iter: 1952 loss: 1.32006289e-06
Iter: 1953 loss: 1.31731235e-06
Iter: 1954 loss: 1.3172388e-06
Iter: 1955 loss: 1.31710908e-06
Iter: 1956 loss: 1.31891568e-06
Iter: 1957 loss: 1.31713023e-06
Iter: 1958 loss: 1.31694924e-06
Iter: 1959 loss: 1.31759566e-06
Iter: 1960 loss: 1.31691411e-06
Iter: 1961 loss: 1.31680235e-06
Iter: 1962 loss: 1.31709885e-06
Iter: 1963 loss: 1.31673789e-06
Iter: 1964 loss: 1.31662455e-06
Iter: 1965 loss: 1.31680895e-06
Iter: 1966 loss: 1.31651768e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2.4 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi2.4
+ date
Sat Oct 31 23:29:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 0 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f03d1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f03e9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f02eef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f0323510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f0323158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f0323268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f025c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f027a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f027a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9a00b2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9a00d12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9a00b2b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f003fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f00ab0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f004fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f0391620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f004fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9987caf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9987bdd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9987ca2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f01010d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9987a7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f0101a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd998751ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd998752730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd99865eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9a007e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd99865e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f00ed158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f00ec0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f00c9400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd99859c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd99859c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9986e1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd99861a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f01d9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.1856887
test_loss: 0.19423796
train_loss: 0.17462052
test_loss: 0.1714089
train_loss: 1.9648671
test_loss: 2.44021
train_loss: 1.9605833
test_loss: 1.9403181
train_loss: 1.9792343
test_loss: 1.9468017
train_loss: 1.9733862
test_loss: 1.9620303
train_loss: 1.9499537
test_loss: 1.9433596
train_loss: 1.9708761
test_loss: 1.9516206
train_loss: 1.9215689
test_loss: 1.9576008
train_loss: 1.9728367
test_loss: 1.9613053
train_loss: 1.9561198
test_loss: 1.9578906
train_loss: 1.9920452
test_loss: 1.9644698
train_loss: 1.9595634
test_loss: 1.9493843
train_loss: 1.9545825
test_loss: 1.9524176
train_loss: 1.9932415
test_loss: 1.9583745
train_loss: 1.9265237
test_loss: 1.9569523
train_loss: 1.9498293
test_loss: 1.9580789
train_loss: 1.9428585
test_loss: 1.9546921
train_loss: 1.9555156
test_loss: 1.9627818
train_loss: 1.9925234
test_loss: 1.9613147
train_loss: 1.9284246
test_loss: 1.9696748
train_loss: 1.9792553
test_loss: 1.9624246
train_loss: 1.9854016
test_loss: 1.9676781
train_loss: 1.9650139
test_loss: 1.9623498
train_loss: 1.9040123
test_loss: 2.065327
train_loss: 1.9926857
test_loss: 1.9612792
train_loss: 1.8921331
test_loss: 1.9752306
train_loss: 1.9607321
test_loss: 1.9656751
train_loss: 1.9985647
test_loss: 1.9616714
train_loss: 1.9697518
test_loss: 1.9669706
train_loss: 1.9941117
test_loss: 1.9669054
train_loss: 1.9838771
test_loss: 1.9657681
train_loss: 1.9569321
test_loss: 1.9664801
train_loss: 1.9607171
test_loss: 1.9708095
train_loss: 1.9260674
test_loss: 1.968992
train_loss: 1.9327577
test_loss: 1.9655216
train_loss: 1.9459153
test_loss: 1.9671743
train_loss: 1.9795334
test_loss: 1.9682041
train_loss: 1.8967216
test_loss: 1.9595826
train_loss: 1.966798
test_loss: 1.9708996
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bd0682f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bd0459d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bd0df9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bcff0a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bd026ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bd0df598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bcefb7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bcfa7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bcefb048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bced30d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bced36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bcf6ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589419cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589419c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58941c3d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bcea8ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bce86ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58941d1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bcf4ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58bce86d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c6a3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c6a3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c6c1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c659510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c659bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c6596a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c604ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c5fcd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c59b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c592268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c59b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c59b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c51d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c592158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c584378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f586c58ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 758.3302
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Traceback (most recent call last):
  File "biholoNN_train.py", line 172, in <module>
    results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=train_func, initial_position=init_params, max_iterations=max_epochs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 287, in minimize
    parallel_iterations=parallel_iterations)[0]
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 574, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2499, in while_loop_v2
    return_same_structure=True)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2735, in while_loop
    loop_vars = body(*loop_vars)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 260, in _body
    max_line_search_iterations)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 156, in line_search_step
    max_iterations=max_iterations)  # No search needed for these.
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 259, in hager_zhang
    threshold_use_approximate_wolfe_condition)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 609, in _prepare_args
    val_initial = value_and_gradients_function(initial_step_size)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 251, in _restricted_func
    objective_value, gradient = value_and_gradients_function(pt)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 814, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
	 [[Sum_1/_32]]
  (1) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_f_14711]

Function call stack:
f -> f

++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2.8 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi2.8
+ date
Sun Nov  1 00:12:44 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 0 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15203f7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f152051c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f152043d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1520538950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15205381e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f152043d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f152043d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15202e7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15202e77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f152023c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f152023c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f152016d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15203aa840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15203b01e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f152038a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15202052f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15203aa488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1520112ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1520114d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1520112048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f152007a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15203e4268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f152007aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f14d46f6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f14d46f6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15201acea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f152017ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15201ac2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f14d4799158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15200580d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f14d476d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f14d4601158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f14d4601510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f152006dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15202ad1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f14d456fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.03632998
test_loss: 0.034263566
train_loss: 0.015665516
test_loss: 0.015652474
train_loss: 0.010514795
test_loss: 0.010656543
train_loss: 0.007763221
test_loss: 0.0079947
train_loss: 0.006956153
test_loss: 0.0071781124
train_loss: 0.0052180076
test_loss: 0.005511651
train_loss: 0.0046289265
test_loss: 0.0049825325
train_loss: 0.0043310774
test_loss: 0.0041378587
train_loss: 0.0037634035
test_loss: 0.0037714993
train_loss: 0.0035556788
test_loss: 0.0038162451
train_loss: 0.0030369787
test_loss: 0.0030857485
train_loss: 0.0029460152
test_loss: 0.0030709414
train_loss: 0.0027697468
test_loss: 0.002719743
train_loss: 0.0026206125
test_loss: 0.0025479384
train_loss: 0.0026253986
test_loss: 0.0025935883
train_loss: 0.0022383584
test_loss: 0.0022058655
train_loss: 0.0021453127
test_loss: 0.0022032955
train_loss: 0.0020286525
test_loss: 0.0020532352
train_loss: 0.0019298778
test_loss: 0.0019956476
train_loss: 0.0019217278
test_loss: 0.0019574552
train_loss: 0.0019065326
test_loss: 0.001993403
train_loss: 0.0017985718
test_loss: 0.0020635999
train_loss: 0.0018612416
test_loss: 0.0018732407
train_loss: 0.0015816301
test_loss: 0.0017767587
train_loss: 0.0015305948
test_loss: 0.0016960376
train_loss: 0.0015783093
test_loss: 0.001681475
train_loss: 0.00157229
test_loss: 0.0017721006
train_loss: 0.0015073704
test_loss: 0.0016773128
train_loss: 0.0015069866
test_loss: 0.0016471132
train_loss: 0.0014548542
test_loss: 0.0016384744
train_loss: 0.0013890482
test_loss: 0.0015726336
train_loss: 0.0014339937
test_loss: 0.0015963174
train_loss: 0.0014469918
test_loss: 0.0015582665
train_loss: 0.0013458076
test_loss: 0.0015456246
train_loss: 0.0014478298
test_loss: 0.0015760924
train_loss: 0.0014236728
test_loss: 0.0015184671
train_loss: 0.0014322448
test_loss: 0.0015336188
train_loss: 0.0013607083
test_loss: 0.0015214924
train_loss: 0.0013239102
test_loss: 0.0015049616
train_loss: 0.0013390749
test_loss: 0.001503375
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3b6727a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3b67a99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3b682f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3b67a7d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3b6770d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3b682ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3b668ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3b66cf268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3b66cf400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afbf30d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3b66cf0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afc69d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afbb3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afbb32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afbe0d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afb6fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afb7eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afbec268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afc28bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afb7ed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afa44f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afa44268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afa44158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afac9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afac9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afac9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afa13ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afa0fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afa24378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afa8e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afa247b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afa24730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3af908620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3afa8e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3af985378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3af8d6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.26966119e-06
Iter: 2 loss: 4.46436115e-06
Iter: 3 loss: 3.22609412e-06
Iter: 4 loss: 3.20630306e-06
Iter: 5 loss: 3.17184822e-06
Iter: 6 loss: 3.1717932e-06
Iter: 7 loss: 3.14285717e-06
Iter: 8 loss: 3.19150149e-06
Iter: 9 loss: 3.12974385e-06
Iter: 10 loss: 3.11272379e-06
Iter: 11 loss: 3.21791572e-06
Iter: 12 loss: 3.11069243e-06
Iter: 13 loss: 3.10303221e-06
Iter: 14 loss: 3.11052054e-06
Iter: 15 loss: 3.09872644e-06
Iter: 16 loss: 3.08936e-06
Iter: 17 loss: 3.1380398e-06
Iter: 18 loss: 3.08782523e-06
Iter: 19 loss: 3.08217795e-06
Iter: 20 loss: 3.10586211e-06
Iter: 21 loss: 3.08093217e-06
Iter: 22 loss: 3.07567461e-06
Iter: 23 loss: 3.0903434e-06
Iter: 24 loss: 3.07399546e-06
Iter: 25 loss: 3.06965421e-06
Iter: 26 loss: 3.08751532e-06
Iter: 27 loss: 3.06873267e-06
Iter: 28 loss: 3.0647077e-06
Iter: 29 loss: 3.06676066e-06
Iter: 30 loss: 3.06204311e-06
Iter: 31 loss: 3.05775075e-06
Iter: 32 loss: 3.07508617e-06
Iter: 33 loss: 3.0567162e-06
Iter: 34 loss: 3.05243566e-06
Iter: 35 loss: 3.06864649e-06
Iter: 36 loss: 3.05133744e-06
Iter: 37 loss: 3.04810965e-06
Iter: 38 loss: 3.06041875e-06
Iter: 39 loss: 3.04734795e-06
Iter: 40 loss: 3.04661671e-06
Iter: 41 loss: 3.04562468e-06
Iter: 42 loss: 3.04474543e-06
Iter: 43 loss: 3.04223022e-06
Iter: 44 loss: 3.05667709e-06
Iter: 45 loss: 3.04153309e-06
Iter: 46 loss: 3.03970273e-06
Iter: 47 loss: 3.03731031e-06
Iter: 48 loss: 3.03717707e-06
Iter: 49 loss: 3.03534034e-06
Iter: 50 loss: 3.03499564e-06
Iter: 51 loss: 3.03335537e-06
Iter: 52 loss: 3.03251159e-06
Iter: 53 loss: 3.03169872e-06
Iter: 54 loss: 3.02919079e-06
Iter: 55 loss: 3.03402976e-06
Iter: 56 loss: 3.02818489e-06
Iter: 57 loss: 3.02648778e-06
Iter: 58 loss: 3.03800289e-06
Iter: 59 loss: 3.02633021e-06
Iter: 60 loss: 3.02443368e-06
Iter: 61 loss: 3.02772924e-06
Iter: 62 loss: 3.02359513e-06
Iter: 63 loss: 3.02217063e-06
Iter: 64 loss: 3.02387616e-06
Iter: 65 loss: 3.02144736e-06
Iter: 66 loss: 3.01922773e-06
Iter: 67 loss: 3.02394278e-06
Iter: 68 loss: 3.0183669e-06
Iter: 69 loss: 3.01687714e-06
Iter: 70 loss: 3.02606122e-06
Iter: 71 loss: 3.01662749e-06
Iter: 72 loss: 3.01549198e-06
Iter: 73 loss: 3.02407352e-06
Iter: 74 loss: 3.01542832e-06
Iter: 75 loss: 3.01390855e-06
Iter: 76 loss: 3.0178694e-06
Iter: 77 loss: 3.01346267e-06
Iter: 78 loss: 3.01283762e-06
Iter: 79 loss: 3.01236355e-06
Iter: 80 loss: 3.0121887e-06
Iter: 81 loss: 3.01120872e-06
Iter: 82 loss: 3.00931197e-06
Iter: 83 loss: 3.05241247e-06
Iter: 84 loss: 3.00926854e-06
Iter: 85 loss: 3.00743477e-06
Iter: 86 loss: 3.03396018e-06
Iter: 87 loss: 3.00743341e-06
Iter: 88 loss: 3.00611237e-06
Iter: 89 loss: 3.00533384e-06
Iter: 90 loss: 3.00468673e-06
Iter: 91 loss: 3.00313968e-06
Iter: 92 loss: 3.00313059e-06
Iter: 93 loss: 3.00202305e-06
Iter: 94 loss: 2.99972521e-06
Iter: 95 loss: 3.04245714e-06
Iter: 96 loss: 2.99974499e-06
Iter: 97 loss: 2.99759449e-06
Iter: 98 loss: 3.00855163e-06
Iter: 99 loss: 2.99721114e-06
Iter: 100 loss: 2.9957846e-06
Iter: 101 loss: 3.01777709e-06
Iter: 102 loss: 2.99577096e-06
Iter: 103 loss: 2.99460044e-06
Iter: 104 loss: 2.99493354e-06
Iter: 105 loss: 2.99376597e-06
Iter: 106 loss: 2.9923217e-06
Iter: 107 loss: 2.99498424e-06
Iter: 108 loss: 2.9916082e-06
Iter: 109 loss: 2.99299586e-06
Iter: 110 loss: 2.99128442e-06
Iter: 111 loss: 2.9910052e-06
Iter: 112 loss: 2.98986697e-06
Iter: 113 loss: 2.9893381e-06
Iter: 114 loss: 2.98853229e-06
Iter: 115 loss: 2.98735085e-06
Iter: 116 loss: 2.98733653e-06
Iter: 117 loss: 2.98644477e-06
Iter: 118 loss: 2.98577879e-06
Iter: 119 loss: 2.98548161e-06
Iter: 120 loss: 2.98398868e-06
Iter: 121 loss: 2.99154863e-06
Iter: 122 loss: 2.98373175e-06
Iter: 123 loss: 2.98275745e-06
Iter: 124 loss: 2.98569603e-06
Iter: 125 loss: 2.98244277e-06
Iter: 126 loss: 2.98105806e-06
Iter: 127 loss: 2.98228906e-06
Iter: 128 loss: 2.98028317e-06
Iter: 129 loss: 2.97910879e-06
Iter: 130 loss: 2.9875232e-06
Iter: 131 loss: 2.9790192e-06
Iter: 132 loss: 2.97798715e-06
Iter: 133 loss: 2.97745351e-06
Iter: 134 loss: 2.97701649e-06
Iter: 135 loss: 2.97546831e-06
Iter: 136 loss: 2.98372879e-06
Iter: 137 loss: 2.97524366e-06
Iter: 138 loss: 2.97404586e-06
Iter: 139 loss: 2.97303905e-06
Iter: 140 loss: 2.97262523e-06
Iter: 141 loss: 2.97130782e-06
Iter: 142 loss: 2.97122051e-06
Iter: 143 loss: 2.9699986e-06
Iter: 144 loss: 2.98119357e-06
Iter: 145 loss: 2.96997041e-06
Iter: 146 loss: 2.96941653e-06
Iter: 147 loss: 2.96840199e-06
Iter: 148 loss: 2.99113663e-06
Iter: 149 loss: 2.96843518e-06
Iter: 150 loss: 2.96717167e-06
Iter: 151 loss: 2.96565349e-06
Iter: 152 loss: 2.96545272e-06
Iter: 153 loss: 2.9637954e-06
Iter: 154 loss: 2.97038468e-06
Iter: 155 loss: 2.96330813e-06
Iter: 156 loss: 2.96189251e-06
Iter: 157 loss: 2.96511189e-06
Iter: 158 loss: 2.96131384e-06
Iter: 159 loss: 2.95953441e-06
Iter: 160 loss: 2.97462384e-06
Iter: 161 loss: 2.95946938e-06
Iter: 162 loss: 2.95846621e-06
Iter: 163 loss: 2.96267035e-06
Iter: 164 loss: 2.95827749e-06
Iter: 165 loss: 2.95731661e-06
Iter: 166 loss: 2.95671634e-06
Iter: 167 loss: 2.95637619e-06
Iter: 168 loss: 2.95466725e-06
Iter: 169 loss: 2.96349913e-06
Iter: 170 loss: 2.95435939e-06
Iter: 171 loss: 2.95338941e-06
Iter: 172 loss: 2.953803e-06
Iter: 173 loss: 2.95272798e-06
Iter: 174 loss: 2.9510677e-06
Iter: 175 loss: 2.95378595e-06
Iter: 176 loss: 2.95032805e-06
Iter: 177 loss: 2.95199538e-06
Iter: 178 loss: 2.94986739e-06
Iter: 179 loss: 2.94957954e-06
Iter: 180 loss: 2.9485991e-06
Iter: 181 loss: 2.95571226e-06
Iter: 182 loss: 2.9484363e-06
Iter: 183 loss: 2.94727488e-06
Iter: 184 loss: 2.94915799e-06
Iter: 185 loss: 2.9467451e-06
Iter: 186 loss: 2.94581287e-06
Iter: 187 loss: 2.94669326e-06
Iter: 188 loss: 2.94520532e-06
Iter: 189 loss: 2.94357687e-06
Iter: 190 loss: 2.94599386e-06
Iter: 191 loss: 2.94287611e-06
Iter: 192 loss: 2.94169831e-06
Iter: 193 loss: 2.94865731e-06
Iter: 194 loss: 2.94157871e-06
Iter: 195 loss: 2.94042775e-06
Iter: 196 loss: 2.93865696e-06
Iter: 197 loss: 2.93865583e-06
Iter: 198 loss: 2.93718858e-06
Iter: 199 loss: 2.95794189e-06
Iter: 200 loss: 2.93715766e-06
Iter: 201 loss: 2.93584412e-06
Iter: 202 loss: 2.94218648e-06
Iter: 203 loss: 2.93561493e-06
Iter: 204 loss: 2.93461062e-06
Iter: 205 loss: 2.9350706e-06
Iter: 206 loss: 2.93394828e-06
Iter: 207 loss: 2.93274888e-06
Iter: 208 loss: 2.93443873e-06
Iter: 209 loss: 2.93210314e-06
Iter: 210 loss: 2.93143285e-06
Iter: 211 loss: 2.93140056e-06
Iter: 212 loss: 2.93055837e-06
Iter: 213 loss: 2.93330913e-06
Iter: 214 loss: 2.93027551e-06
Iter: 215 loss: 2.92970799e-06
Iter: 216 loss: 2.92902769e-06
Iter: 217 loss: 2.92896038e-06
Iter: 218 loss: 2.92808591e-06
Iter: 219 loss: 2.92829282e-06
Iter: 220 loss: 2.92749678e-06
Iter: 221 loss: 2.92620325e-06
Iter: 222 loss: 2.93018456e-06
Iter: 223 loss: 2.92584014e-06
Iter: 224 loss: 2.92466393e-06
Iter: 225 loss: 2.92647405e-06
Iter: 226 loss: 2.92408276e-06
Iter: 227 loss: 2.9222756e-06
Iter: 228 loss: 2.92435584e-06
Iter: 229 loss: 2.9213611e-06
Iter: 230 loss: 2.92005598e-06
Iter: 231 loss: 2.92379741e-06
Iter: 232 loss: 2.91964079e-06
Iter: 233 loss: 2.91797778e-06
Iter: 234 loss: 2.9219093e-06
Iter: 235 loss: 2.91731362e-06
Iter: 236 loss: 2.91617562e-06
Iter: 237 loss: 2.9256114e-06
Iter: 238 loss: 2.9160492e-06
Iter: 239 loss: 2.91509264e-06
Iter: 240 loss: 2.91419406e-06
Iter: 241 loss: 2.91391461e-06
Iter: 242 loss: 2.9120115e-06
Iter: 243 loss: 2.92780214e-06
Iter: 244 loss: 2.91195965e-06
Iter: 245 loss: 2.91126025e-06
Iter: 246 loss: 2.9154869e-06
Iter: 247 loss: 2.91105721e-06
Iter: 248 loss: 2.90992284e-06
Iter: 249 loss: 2.90981825e-06
Iter: 250 loss: 2.90900152e-06
Iter: 251 loss: 2.90832509e-06
Iter: 252 loss: 2.90725279e-06
Iter: 253 loss: 2.9072271e-06
Iter: 254 loss: 2.90560047e-06
Iter: 255 loss: 2.90518301e-06
Iter: 256 loss: 2.90417552e-06
Iter: 257 loss: 2.90296884e-06
Iter: 258 loss: 2.90285197e-06
Iter: 259 loss: 2.9018222e-06
Iter: 260 loss: 2.90147682e-06
Iter: 261 loss: 2.90085768e-06
Iter: 262 loss: 2.89955733e-06
Iter: 263 loss: 2.90731941e-06
Iter: 264 loss: 2.89938339e-06
Iter: 265 loss: 2.89854415e-06
Iter: 266 loss: 2.89894979e-06
Iter: 267 loss: 2.89788159e-06
Iter: 268 loss: 2.89661943e-06
Iter: 269 loss: 2.90113826e-06
Iter: 270 loss: 2.89631907e-06
Iter: 271 loss: 2.89500963e-06
Iter: 272 loss: 2.89898162e-06
Iter: 273 loss: 2.89461855e-06
Iter: 274 loss: 2.89372542e-06
Iter: 275 loss: 2.899023e-06
Iter: 276 loss: 2.89360469e-06
Iter: 277 loss: 2.89261698e-06
Iter: 278 loss: 2.89308718e-06
Iter: 279 loss: 2.89194213e-06
Iter: 280 loss: 2.89156242e-06
Iter: 281 loss: 2.89138097e-06
Iter: 282 loss: 2.8906943e-06
Iter: 283 loss: 2.89019954e-06
Iter: 284 loss: 2.89003401e-06
Iter: 285 loss: 2.88957949e-06
Iter: 286 loss: 2.88837873e-06
Iter: 287 loss: 2.89412606e-06
Iter: 288 loss: 2.8879324e-06
Iter: 289 loss: 2.88637398e-06
Iter: 290 loss: 2.90802564e-06
Iter: 291 loss: 2.8863833e-06
Iter: 292 loss: 2.88530327e-06
Iter: 293 loss: 2.88585147e-06
Iter: 294 loss: 2.88466617e-06
Iter: 295 loss: 2.88302181e-06
Iter: 296 loss: 2.88326464e-06
Iter: 297 loss: 2.88174897e-06
Iter: 298 loss: 2.88059e-06
Iter: 299 loss: 2.88619322e-06
Iter: 300 loss: 2.88035585e-06
Iter: 301 loss: 2.87888406e-06
Iter: 302 loss: 2.88462638e-06
Iter: 303 loss: 2.87855755e-06
Iter: 304 loss: 2.87744456e-06
Iter: 305 loss: 2.88280671e-06
Iter: 306 loss: 2.87728699e-06
Iter: 307 loss: 2.87620378e-06
Iter: 308 loss: 2.87479725e-06
Iter: 309 loss: 2.87467765e-06
Iter: 310 loss: 2.87350372e-06
Iter: 311 loss: 2.87350531e-06
Iter: 312 loss: 2.87248986e-06
Iter: 313 loss: 2.87245985e-06
Iter: 314 loss: 2.87169746e-06
Iter: 315 loss: 2.87052944e-06
Iter: 316 loss: 2.88470937e-06
Iter: 317 loss: 2.87050716e-06
Iter: 318 loss: 2.87024932e-06
Iter: 319 loss: 2.87011653e-06
Iter: 320 loss: 2.86982186e-06
Iter: 321 loss: 2.86921863e-06
Iter: 322 loss: 2.87622356e-06
Iter: 323 loss: 2.86905151e-06
Iter: 324 loss: 2.86820296e-06
Iter: 325 loss: 2.86740919e-06
Iter: 326 loss: 2.86713248e-06
Iter: 327 loss: 2.86595082e-06
Iter: 328 loss: 2.86738805e-06
Iter: 329 loss: 2.86532941e-06
Iter: 330 loss: 2.86402746e-06
Iter: 331 loss: 2.8752554e-06
Iter: 332 loss: 2.86395971e-06
Iter: 333 loss: 2.86294153e-06
Iter: 334 loss: 2.86555883e-06
Iter: 335 loss: 2.86257637e-06
Iter: 336 loss: 2.86145064e-06
Iter: 337 loss: 2.86351042e-06
Iter: 338 loss: 2.86097793e-06
Iter: 339 loss: 2.8599452e-06
Iter: 340 loss: 2.86058366e-06
Iter: 341 loss: 2.85921669e-06
Iter: 342 loss: 2.85832789e-06
Iter: 343 loss: 2.87188504e-06
Iter: 344 loss: 2.85832402e-06
Iter: 345 loss: 2.8575173e-06
Iter: 346 loss: 2.85837677e-06
Iter: 347 loss: 2.85701435e-06
Iter: 348 loss: 2.856111e-06
Iter: 349 loss: 2.85836359e-06
Iter: 350 loss: 2.85577926e-06
Iter: 351 loss: 2.85479337e-06
Iter: 352 loss: 2.8547945e-06
Iter: 353 loss: 2.85395117e-06
Iter: 354 loss: 2.85436317e-06
Iter: 355 loss: 2.85331066e-06
Iter: 356 loss: 2.85297756e-06
Iter: 357 loss: 2.85224087e-06
Iter: 358 loss: 2.86409727e-06
Iter: 359 loss: 2.85225178e-06
Iter: 360 loss: 2.85160286e-06
Iter: 361 loss: 2.85268379e-06
Iter: 362 loss: 2.85129317e-06
Iter: 363 loss: 2.85035208e-06
Iter: 364 loss: 2.84962061e-06
Iter: 365 loss: 2.84940734e-06
Iter: 366 loss: 2.84835323e-06
Iter: 367 loss: 2.85351598e-06
Iter: 368 loss: 2.84815223e-06
Iter: 369 loss: 2.8468603e-06
Iter: 370 loss: 2.84763792e-06
Iter: 371 loss: 2.84605358e-06
Iter: 372 loss: 2.84483644e-06
Iter: 373 loss: 2.84722773e-06
Iter: 374 loss: 2.84435237e-06
Iter: 375 loss: 2.84299699e-06
Iter: 376 loss: 2.84806265e-06
Iter: 377 loss: 2.84269777e-06
Iter: 378 loss: 2.84172302e-06
Iter: 379 loss: 2.84164889e-06
Iter: 380 loss: 2.84090311e-06
Iter: 381 loss: 2.83973941e-06
Iter: 382 loss: 2.83975851e-06
Iter: 383 loss: 2.83879899e-06
Iter: 384 loss: 2.83908139e-06
Iter: 385 loss: 2.83815621e-06
Iter: 386 loss: 2.83681766e-06
Iter: 387 loss: 2.8420036e-06
Iter: 388 loss: 2.83651411e-06
Iter: 389 loss: 2.83612894e-06
Iter: 390 loss: 2.83605482e-06
Iter: 391 loss: 2.8355521e-06
Iter: 392 loss: 2.83648933e-06
Iter: 393 loss: 2.83533291e-06
Iter: 394 loss: 2.83489953e-06
Iter: 395 loss: 2.83408872e-06
Iter: 396 loss: 2.84877206e-06
Iter: 397 loss: 2.8339623e-06
Iter: 398 loss: 2.83294139e-06
Iter: 399 loss: 2.83504937e-06
Iter: 400 loss: 2.83254803e-06
Iter: 401 loss: 2.83149757e-06
Iter: 402 loss: 2.83564395e-06
Iter: 403 loss: 2.83121881e-06
Iter: 404 loss: 2.83041163e-06
Iter: 405 loss: 2.83393319e-06
Iter: 406 loss: 2.83018062e-06
Iter: 407 loss: 2.82921064e-06
Iter: 408 loss: 2.82866267e-06
Iter: 409 loss: 2.82821884e-06
Iter: 410 loss: 2.82738802e-06
Iter: 411 loss: 2.83648e-06
Iter: 412 loss: 2.8273289e-06
Iter: 413 loss: 2.82649489e-06
Iter: 414 loss: 2.82558244e-06
Iter: 415 loss: 2.82541419e-06
Iter: 416 loss: 2.82422525e-06
Iter: 417 loss: 2.83394866e-06
Iter: 418 loss: 2.8241252e-06
Iter: 419 loss: 2.82314068e-06
Iter: 420 loss: 2.82224255e-06
Iter: 421 loss: 2.82201609e-06
Iter: 422 loss: 2.82118071e-06
Iter: 423 loss: 2.82111978e-06
Iter: 424 loss: 2.8204845e-06
Iter: 425 loss: 2.82738642e-06
Iter: 426 loss: 2.82047449e-06
Iter: 427 loss: 2.8197112e-06
Iter: 428 loss: 2.81986354e-06
Iter: 429 loss: 2.81923258e-06
Iter: 430 loss: 2.81873326e-06
Iter: 431 loss: 2.81797475e-06
Iter: 432 loss: 2.81797156e-06
Iter: 433 loss: 2.8168356e-06
Iter: 434 loss: 2.81711164e-06
Iter: 435 loss: 2.81589541e-06
Iter: 436 loss: 2.81470511e-06
Iter: 437 loss: 2.82876863e-06
Iter: 438 loss: 2.81464895e-06
Iter: 439 loss: 2.81371285e-06
Iter: 440 loss: 2.81456391e-06
Iter: 441 loss: 2.81311577e-06
Iter: 442 loss: 2.81194025e-06
Iter: 443 loss: 2.81603843e-06
Iter: 444 loss: 2.81164171e-06
Iter: 445 loss: 2.81066423e-06
Iter: 446 loss: 2.81061648e-06
Iter: 447 loss: 2.80986887e-06
Iter: 448 loss: 2.80835047e-06
Iter: 449 loss: 2.82022529e-06
Iter: 450 loss: 2.8082577e-06
Iter: 451 loss: 2.80746599e-06
Iter: 452 loss: 2.81016582e-06
Iter: 453 loss: 2.80731524e-06
Iter: 454 loss: 2.80636596e-06
Iter: 455 loss: 2.80600534e-06
Iter: 456 loss: 2.80553377e-06
Iter: 457 loss: 2.8044e-06
Iter: 458 loss: 2.81239136e-06
Iter: 459 loss: 2.80427957e-06
Iter: 460 loss: 2.80429435e-06
Iter: 461 loss: 2.8039949e-06
Iter: 462 loss: 2.80363474e-06
Iter: 463 loss: 2.80251561e-06
Iter: 464 loss: 2.80457175e-06
Iter: 465 loss: 2.80176437e-06
Iter: 466 loss: 2.8006184e-06
Iter: 467 loss: 2.80787845e-06
Iter: 468 loss: 2.80053246e-06
Iter: 469 loss: 2.79947244e-06
Iter: 470 loss: 2.80133236e-06
Iter: 471 loss: 2.79895448e-06
Iter: 472 loss: 2.79801884e-06
Iter: 473 loss: 2.79944425e-06
Iter: 474 loss: 2.79752749e-06
Iter: 475 loss: 2.79644746e-06
Iter: 476 loss: 2.80049881e-06
Iter: 477 loss: 2.79616484e-06
Iter: 478 loss: 2.79520168e-06
Iter: 479 loss: 2.79851906e-06
Iter: 480 loss: 2.794902e-06
Iter: 481 loss: 2.79360506e-06
Iter: 482 loss: 2.79455685e-06
Iter: 483 loss: 2.79286428e-06
Iter: 484 loss: 2.79160531e-06
Iter: 485 loss: 2.79601454e-06
Iter: 486 loss: 2.791307e-06
Iter: 487 loss: 2.78989955e-06
Iter: 488 loss: 2.7914889e-06
Iter: 489 loss: 2.78915741e-06
Iter: 490 loss: 2.788046e-06
Iter: 491 loss: 2.80452059e-06
Iter: 492 loss: 2.78805692e-06
Iter: 493 loss: 2.78736184e-06
Iter: 494 loss: 2.78716561e-06
Iter: 495 loss: 2.78672269e-06
Iter: 496 loss: 2.78604875e-06
Iter: 497 loss: 2.78602329e-06
Iter: 498 loss: 2.7853107e-06
Iter: 499 loss: 2.78752486e-06
Iter: 500 loss: 2.78514e-06
Iter: 501 loss: 2.78476318e-06
Iter: 502 loss: 2.78381935e-06
Iter: 503 loss: 2.78519542e-06
Iter: 504 loss: 2.78310381e-06
Iter: 505 loss: 2.7815654e-06
Iter: 506 loss: 2.79816959e-06
Iter: 507 loss: 2.78148423e-06
Iter: 508 loss: 2.78061725e-06
Iter: 509 loss: 2.78339121e-06
Iter: 510 loss: 2.7803917e-06
Iter: 511 loss: 2.77912841e-06
Iter: 512 loss: 2.78067955e-06
Iter: 513 loss: 2.7785054e-06
Iter: 514 loss: 2.77763661e-06
Iter: 515 loss: 2.77830759e-06
Iter: 516 loss: 2.77704976e-06
Iter: 517 loss: 2.77579375e-06
Iter: 518 loss: 2.78498e-06
Iter: 519 loss: 2.77572462e-06
Iter: 520 loss: 2.77479535e-06
Iter: 521 loss: 2.77766048e-06
Iter: 522 loss: 2.77452546e-06
Iter: 523 loss: 2.7736055e-06
Iter: 524 loss: 2.77466734e-06
Iter: 525 loss: 2.7731071e-06
Iter: 526 loss: 2.77235222e-06
Iter: 527 loss: 2.77538334e-06
Iter: 528 loss: 2.77219397e-06
Iter: 529 loss: 2.77135541e-06
Iter: 530 loss: 2.77272784e-06
Iter: 531 loss: 2.77103572e-06
Iter: 532 loss: 2.77106619e-06
Iter: 533 loss: 2.77061099e-06
Iter: 534 loss: 2.77027175e-06
Iter: 535 loss: 2.76982e-06
Iter: 536 loss: 2.76975197e-06
Iter: 537 loss: 2.76925266e-06
Iter: 538 loss: 2.76852234e-06
Iter: 539 loss: 2.76848982e-06
Iter: 540 loss: 2.76734409e-06
Iter: 541 loss: 2.77272215e-06
Iter: 542 loss: 2.76713035e-06
Iter: 543 loss: 2.76633273e-06
Iter: 544 loss: 2.76641549e-06
Iter: 545 loss: 2.76578248e-06
Iter: 546 loss: 2.76474793e-06
Iter: 547 loss: 2.76920946e-06
Iter: 548 loss: 2.76458013e-06
Iter: 549 loss: 2.76352944e-06
Iter: 550 loss: 2.76743117e-06
Iter: 551 loss: 2.76333935e-06
Iter: 552 loss: 2.76235232e-06
Iter: 553 loss: 2.76288392e-06
Iter: 554 loss: 2.76176524e-06
Iter: 555 loss: 2.760597e-06
Iter: 556 loss: 2.76302944e-06
Iter: 557 loss: 2.76014066e-06
Iter: 558 loss: 2.75901084e-06
Iter: 559 loss: 2.76993342e-06
Iter: 560 loss: 2.75891807e-06
Iter: 561 loss: 2.75822e-06
Iter: 562 loss: 2.75767184e-06
Iter: 563 loss: 2.75750426e-06
Iter: 564 loss: 2.75606203e-06
Iter: 565 loss: 2.76126593e-06
Iter: 566 loss: 2.75574075e-06
Iter: 567 loss: 2.75612138e-06
Iter: 568 loss: 2.75536922e-06
Iter: 569 loss: 2.75504044e-06
Iter: 570 loss: 2.75426191e-06
Iter: 571 loss: 2.76418905e-06
Iter: 572 loss: 2.75422963e-06
Iter: 573 loss: 2.75322236e-06
Iter: 574 loss: 2.7535757e-06
Iter: 575 loss: 2.75256571e-06
Iter: 576 loss: 2.75165098e-06
Iter: 577 loss: 2.75624484e-06
Iter: 578 loss: 2.75149432e-06
Iter: 579 loss: 2.75058665e-06
Iter: 580 loss: 2.74942749e-06
Iter: 581 loss: 2.74932518e-06
Iter: 582 loss: 2.74781951e-06
Iter: 583 loss: 2.76145374e-06
Iter: 584 loss: 2.7477181e-06
Iter: 585 loss: 2.74681452e-06
Iter: 586 loss: 2.74592e-06
Iter: 587 loss: 2.74577928e-06
Iter: 588 loss: 2.7442486e-06
Iter: 589 loss: 2.7657295e-06
Iter: 590 loss: 2.74423815e-06
Iter: 591 loss: 2.74351032e-06
Iter: 592 loss: 2.74307968e-06
Iter: 593 loss: 2.74273452e-06
Iter: 594 loss: 2.74153808e-06
Iter: 595 loss: 2.74756326e-06
Iter: 596 loss: 2.74129548e-06
Iter: 597 loss: 2.74043805e-06
Iter: 598 loss: 2.74602735e-06
Iter: 599 loss: 2.74028957e-06
Iter: 600 loss: 2.73944761e-06
Iter: 601 loss: 2.73867727e-06
Iter: 602 loss: 2.73851083e-06
Iter: 603 loss: 2.73883234e-06
Iter: 604 loss: 2.73809019e-06
Iter: 605 loss: 2.73766614e-06
Iter: 606 loss: 2.73756837e-06
Iter: 607 loss: 2.73728324e-06
Iter: 608 loss: 2.73683258e-06
Iter: 609 loss: 2.73610272e-06
Iter: 610 loss: 2.73613318e-06
Iter: 611 loss: 2.73524438e-06
Iter: 612 loss: 2.73860269e-06
Iter: 613 loss: 2.73508749e-06
Iter: 614 loss: 2.73415253e-06
Iter: 615 loss: 2.73527394e-06
Iter: 616 loss: 2.73365754e-06
Iter: 617 loss: 2.73267642e-06
Iter: 618 loss: 2.73493561e-06
Iter: 619 loss: 2.73232286e-06
Iter: 620 loss: 2.73111164e-06
Iter: 621 loss: 2.73255364e-06
Iter: 622 loss: 2.73047044e-06
Iter: 623 loss: 2.72946045e-06
Iter: 624 loss: 2.73124579e-06
Iter: 625 loss: 2.72903389e-06
Iter: 626 loss: 2.72763873e-06
Iter: 627 loss: 2.7311321e-06
Iter: 628 loss: 2.7270421e-06
Iter: 629 loss: 2.72602756e-06
Iter: 630 loss: 2.73727119e-06
Iter: 631 loss: 2.72592888e-06
Iter: 632 loss: 2.72519424e-06
Iter: 633 loss: 2.72427815e-06
Iter: 634 loss: 2.72423881e-06
Iter: 635 loss: 2.72333841e-06
Iter: 636 loss: 2.72331545e-06
Iter: 637 loss: 2.722727e-06
Iter: 638 loss: 2.72185321e-06
Iter: 639 loss: 2.72183706e-06
Iter: 640 loss: 2.72085663e-06
Iter: 641 loss: 2.72942862e-06
Iter: 642 loss: 2.72081115e-06
Iter: 643 loss: 2.72100169e-06
Iter: 644 loss: 2.72050875e-06
Iter: 645 loss: 2.72031343e-06
Iter: 646 loss: 2.71958697e-06
Iter: 647 loss: 2.72057969e-06
Iter: 648 loss: 2.71905901e-06
Iter: 649 loss: 2.71811541e-06
Iter: 650 loss: 2.72716875e-06
Iter: 651 loss: 2.71809608e-06
Iter: 652 loss: 2.71755607e-06
Iter: 653 loss: 2.717916e-06
Iter: 654 loss: 2.71726185e-06
Iter: 655 loss: 2.71630711e-06
Iter: 656 loss: 2.71725071e-06
Iter: 657 loss: 2.71580666e-06
Iter: 658 loss: 2.71493764e-06
Iter: 659 loss: 2.71811973e-06
Iter: 660 loss: 2.71477438e-06
Iter: 661 loss: 2.71380509e-06
Iter: 662 loss: 2.71311706e-06
Iter: 663 loss: 2.71268595e-06
Iter: 664 loss: 2.71162912e-06
Iter: 665 loss: 2.72652051e-06
Iter: 666 loss: 2.71166482e-06
Iter: 667 loss: 2.7108631e-06
Iter: 668 loss: 2.71303e-06
Iter: 669 loss: 2.71062981e-06
Iter: 670 loss: 2.70988085e-06
Iter: 671 loss: 2.71086674e-06
Iter: 672 loss: 2.70953228e-06
Iter: 673 loss: 2.70870942e-06
Iter: 674 loss: 2.70994451e-06
Iter: 675 loss: 2.70832879e-06
Iter: 676 loss: 2.70740816e-06
Iter: 677 loss: 2.71520116e-06
Iter: 678 loss: 2.70728606e-06
Iter: 679 loss: 2.70673786e-06
Iter: 680 loss: 2.70623332e-06
Iter: 681 loss: 2.7060571e-06
Iter: 682 loss: 2.7065928e-06
Iter: 683 loss: 2.70575583e-06
Iter: 684 loss: 2.7054391e-06
Iter: 685 loss: 2.70458258e-06
Iter: 686 loss: 2.71172416e-06
Iter: 687 loss: 2.70442797e-06
Iter: 688 loss: 2.70364626e-06
Iter: 689 loss: 2.70427631e-06
Iter: 690 loss: 2.70318515e-06
Iter: 691 loss: 2.70244482e-06
Iter: 692 loss: 2.70888722e-06
Iter: 693 loss: 2.70231294e-06
Iter: 694 loss: 2.70156852e-06
Iter: 695 loss: 2.70087185e-06
Iter: 696 loss: 2.70066607e-06
Iter: 697 loss: 2.69960356e-06
Iter: 698 loss: 2.71347426e-06
Iter: 699 loss: 2.69959787e-06
Iter: 700 loss: 2.69899465e-06
Iter: 701 loss: 2.69804332e-06
Iter: 702 loss: 2.69802331e-06
Iter: 703 loss: 2.69692873e-06
Iter: 704 loss: 2.71162662e-06
Iter: 705 loss: 2.696921e-06
Iter: 706 loss: 2.6962939e-06
Iter: 707 loss: 2.69726024e-06
Iter: 708 loss: 2.69604516e-06
Iter: 709 loss: 2.69518932e-06
Iter: 710 loss: 2.69482098e-06
Iter: 711 loss: 2.69443012e-06
Iter: 712 loss: 2.69339807e-06
Iter: 713 loss: 2.69941029e-06
Iter: 714 loss: 2.69327529e-06
Iter: 715 loss: 2.69237671e-06
Iter: 716 loss: 2.69304223e-06
Iter: 717 loss: 2.69182215e-06
Iter: 718 loss: 2.69243446e-06
Iter: 719 loss: 2.69144221e-06
Iter: 720 loss: 2.69121642e-06
Iter: 721 loss: 2.69073894e-06
Iter: 722 loss: 2.69072962e-06
Iter: 723 loss: 2.6902876e-06
Iter: 724 loss: 2.68918507e-06
Iter: 725 loss: 2.70036639e-06
Iter: 726 loss: 2.68903796e-06
Iter: 727 loss: 2.6880748e-06
Iter: 728 loss: 2.70324131e-06
Iter: 729 loss: 2.6880416e-06
Iter: 730 loss: 2.68718463e-06
Iter: 731 loss: 2.68770191e-06
Iter: 732 loss: 2.68666645e-06
Iter: 733 loss: 2.68547797e-06
Iter: 734 loss: 2.68832173e-06
Iter: 735 loss: 2.68505573e-06
Iter: 736 loss: 2.68408758e-06
Iter: 737 loss: 2.68992471e-06
Iter: 738 loss: 2.68397753e-06
Iter: 739 loss: 2.68337953e-06
Iter: 740 loss: 2.68299164e-06
Iter: 741 loss: 2.68264785e-06
Iter: 742 loss: 2.6815776e-06
Iter: 743 loss: 2.68575104e-06
Iter: 744 loss: 2.68134431e-06
Iter: 745 loss: 2.68037411e-06
Iter: 746 loss: 2.68640906e-06
Iter: 747 loss: 2.68030499e-06
Iter: 748 loss: 2.67956966e-06
Iter: 749 loss: 2.67923156e-06
Iter: 750 loss: 2.67889618e-06
Iter: 751 loss: 2.67850191e-06
Iter: 752 loss: 2.67835958e-06
Iter: 753 loss: 2.67788391e-06
Iter: 754 loss: 2.68229087e-06
Iter: 755 loss: 2.67782389e-06
Iter: 756 loss: 2.67762744e-06
Iter: 757 loss: 2.67698579e-06
Iter: 758 loss: 2.68183135e-06
Iter: 759 loss: 2.67685891e-06
Iter: 760 loss: 2.67582504e-06
Iter: 761 loss: 2.67576274e-06
Iter: 762 loss: 2.67498604e-06
Iter: 763 loss: 2.67401674e-06
Iter: 764 loss: 2.68266581e-06
Iter: 765 loss: 2.67402311e-06
Iter: 766 loss: 2.67304085e-06
Iter: 767 loss: 2.6724631e-06
Iter: 768 loss: 2.67208452e-06
Iter: 769 loss: 2.6711125e-06
Iter: 770 loss: 2.68302074e-06
Iter: 771 loss: 2.67109385e-06
Iter: 772 loss: 2.67026326e-06
Iter: 773 loss: 2.67065434e-06
Iter: 774 loss: 2.6696664e-06
Iter: 775 loss: 2.66879761e-06
Iter: 776 loss: 2.67509e-06
Iter: 777 loss: 2.66876214e-06
Iter: 778 loss: 2.66813117e-06
Iter: 779 loss: 2.66766619e-06
Iter: 780 loss: 2.66737266e-06
Iter: 781 loss: 2.66624374e-06
Iter: 782 loss: 2.6707487e-06
Iter: 783 loss: 2.66594179e-06
Iter: 784 loss: 2.66503503e-06
Iter: 785 loss: 2.66613824e-06
Iter: 786 loss: 2.66462848e-06
Iter: 787 loss: 2.66473103e-06
Iter: 788 loss: 2.66408597e-06
Iter: 789 loss: 2.66379129e-06
Iter: 790 loss: 2.66305597e-06
Iter: 791 loss: 2.67014525e-06
Iter: 792 loss: 2.6628768e-06
Iter: 793 loss: 2.66207576e-06
Iter: 794 loss: 2.66314532e-06
Iter: 795 loss: 2.66159236e-06
Iter: 796 loss: 2.66057987e-06
Iter: 797 loss: 2.6638711e-06
Iter: 798 loss: 2.66024699e-06
Iter: 799 loss: 2.65929157e-06
Iter: 800 loss: 2.66077041e-06
Iter: 801 loss: 2.65886251e-06
Iter: 802 loss: 2.65789186e-06
Iter: 803 loss: 2.66122834e-06
Iter: 804 loss: 2.65764e-06
Iter: 805 loss: 2.65677227e-06
Iter: 806 loss: 2.6592727e-06
Iter: 807 loss: 2.65653762e-06
Iter: 808 loss: 2.6555258e-06
Iter: 809 loss: 2.6589596e-06
Iter: 810 loss: 2.65531867e-06
Iter: 811 loss: 2.6545556e-06
Iter: 812 loss: 2.65582958e-06
Iter: 813 loss: 2.6541652e-06
Iter: 814 loss: 2.65302879e-06
Iter: 815 loss: 2.65363087e-06
Iter: 816 loss: 2.65225435e-06
Iter: 817 loss: 2.65120798e-06
Iter: 818 loss: 2.65624408e-06
Iter: 819 loss: 2.6510229e-06
Iter: 820 loss: 2.65012977e-06
Iter: 821 loss: 2.65752578e-06
Iter: 822 loss: 2.65010567e-06
Iter: 823 loss: 2.64900746e-06
Iter: 824 loss: 2.65356516e-06
Iter: 825 loss: 2.64882146e-06
Iter: 826 loss: 2.64842538e-06
Iter: 827 loss: 2.64766868e-06
Iter: 828 loss: 2.64765322e-06
Iter: 829 loss: 2.64680057e-06
Iter: 830 loss: 2.64582332e-06
Iter: 831 loss: 2.64567416e-06
Iter: 832 loss: 2.64465e-06
Iter: 833 loss: 2.64466325e-06
Iter: 834 loss: 2.64388154e-06
Iter: 835 loss: 2.64403366e-06
Iter: 836 loss: 2.6433122e-06
Iter: 837 loss: 2.64237269e-06
Iter: 838 loss: 2.64978235e-06
Iter: 839 loss: 2.64234473e-06
Iter: 840 loss: 2.64152163e-06
Iter: 841 loss: 2.64262167e-06
Iter: 842 loss: 2.64112759e-06
Iter: 843 loss: 2.64016307e-06
Iter: 844 loss: 2.64192977e-06
Iter: 845 loss: 2.63981246e-06
Iter: 846 loss: 2.63877973e-06
Iter: 847 loss: 2.64182017e-06
Iter: 848 loss: 2.63853303e-06
Iter: 849 loss: 2.63743732e-06
Iter: 850 loss: 2.63988431e-06
Iter: 851 loss: 2.63705806e-06
Iter: 852 loss: 2.6362286e-06
Iter: 853 loss: 2.64155869e-06
Iter: 854 loss: 2.63611014e-06
Iter: 855 loss: 2.63560742e-06
Iter: 856 loss: 2.63554102e-06
Iter: 857 loss: 2.63521542e-06
Iter: 858 loss: 2.63447419e-06
Iter: 859 loss: 2.64818618e-06
Iter: 860 loss: 2.63445918e-06
Iter: 861 loss: 2.63381753e-06
Iter: 862 loss: 2.63334687e-06
Iter: 863 loss: 2.63312586e-06
Iter: 864 loss: 2.63194215e-06
Iter: 865 loss: 2.63725519e-06
Iter: 866 loss: 2.63167522e-06
Iter: 867 loss: 2.63077482e-06
Iter: 868 loss: 2.63119841e-06
Iter: 869 loss: 2.63015227e-06
Iter: 870 loss: 2.62877938e-06
Iter: 871 loss: 2.63225411e-06
Iter: 872 loss: 2.62830804e-06
Iter: 873 loss: 2.6273492e-06
Iter: 874 loss: 2.63361585e-06
Iter: 875 loss: 2.62724802e-06
Iter: 876 loss: 2.62633512e-06
Iter: 877 loss: 2.62610342e-06
Iter: 878 loss: 2.62552112e-06
Iter: 879 loss: 2.62459162e-06
Iter: 880 loss: 2.63763445e-06
Iter: 881 loss: 2.6246139e-06
Iter: 882 loss: 2.62374942e-06
Iter: 883 loss: 2.62363051e-06
Iter: 884 loss: 2.62307412e-06
Iter: 885 loss: 2.6220896e-06
Iter: 886 loss: 2.62934645e-06
Iter: 887 loss: 2.62201866e-06
Iter: 888 loss: 2.62117373e-06
Iter: 889 loss: 2.6217574e-06
Iter: 890 loss: 2.62070012e-06
Iter: 891 loss: 2.62049798e-06
Iter: 892 loss: 2.62025969e-06
Iter: 893 loss: 2.61972536e-06
Iter: 894 loss: 2.6190753e-06
Iter: 895 loss: 2.61911464e-06
Iter: 896 loss: 2.61847799e-06
Iter: 897 loss: 2.61849527e-06
Iter: 898 loss: 2.61801688e-06
Iter: 899 loss: 2.61714604e-06
Iter: 900 loss: 2.61867899e-06
Iter: 901 loss: 2.61683135e-06
Iter: 902 loss: 2.61590867e-06
Iter: 903 loss: 2.61798164e-06
Iter: 904 loss: 2.61551554e-06
Iter: 905 loss: 2.61462355e-06
Iter: 906 loss: 2.61697301e-06
Iter: 907 loss: 2.61424111e-06
Iter: 908 loss: 2.61335299e-06
Iter: 909 loss: 2.61371929e-06
Iter: 910 loss: 2.61272135e-06
Iter: 911 loss: 2.61143578e-06
Iter: 912 loss: 2.61928176e-06
Iter: 913 loss: 2.6113064e-06
Iter: 914 loss: 2.61044147e-06
Iter: 915 loss: 2.61112837e-06
Iter: 916 loss: 2.60982915e-06
Iter: 917 loss: 2.60857723e-06
Iter: 918 loss: 2.61307378e-06
Iter: 919 loss: 2.6082057e-06
Iter: 920 loss: 2.6073144e-06
Iter: 921 loss: 2.61485548e-06
Iter: 922 loss: 2.60731076e-06
Iter: 923 loss: 2.60659522e-06
Iter: 924 loss: 2.60712159e-06
Iter: 925 loss: 2.60611023e-06
Iter: 926 loss: 2.6058367e-06
Iter: 927 loss: 2.60566048e-06
Iter: 928 loss: 2.60531442e-06
Iter: 929 loss: 2.60464867e-06
Iter: 930 loss: 2.61872901e-06
Iter: 931 loss: 2.60468641e-06
Iter: 932 loss: 2.6040484e-06
Iter: 933 loss: 2.60316847e-06
Iter: 934 loss: 2.60318484e-06
Iter: 935 loss: 2.60212914e-06
Iter: 936 loss: 2.61631521e-06
Iter: 937 loss: 2.60218485e-06
Iter: 938 loss: 2.60164688e-06
Iter: 939 loss: 2.60176648e-06
Iter: 940 loss: 2.60128263e-06
Iter: 941 loss: 2.60046909e-06
Iter: 942 loss: 2.60292359e-06
Iter: 943 loss: 2.60019351e-06
Iter: 944 loss: 2.59945273e-06
Iter: 945 loss: 2.60049046e-06
Iter: 946 loss: 2.59905846e-06
Iter: 947 loss: 2.5981758e-06
Iter: 948 loss: 2.59928834e-06
Iter: 949 loss: 2.59768876e-06
Iter: 950 loss: 2.59661169e-06
Iter: 951 loss: 2.60033676e-06
Iter: 952 loss: 2.59628973e-06
Iter: 953 loss: 2.59550325e-06
Iter: 954 loss: 2.60174738e-06
Iter: 955 loss: 2.59546778e-06
Iter: 956 loss: 2.59477633e-06
Iter: 957 loss: 2.5947711e-06
Iter: 958 loss: 2.59425929e-06
Iter: 959 loss: 2.59413309e-06
Iter: 960 loss: 2.59376793e-06
Iter: 961 loss: 2.59334774e-06
Iter: 962 loss: 2.59328294e-06
Iter: 963 loss: 2.59300441e-06
Iter: 964 loss: 2.59259878e-06
Iter: 965 loss: 2.59210356e-06
Iter: 966 loss: 2.59202329e-06
Iter: 967 loss: 2.591255e-06
Iter: 968 loss: 2.59171452e-06
Iter: 969 loss: 2.59070248e-06
Iter: 970 loss: 2.58993123e-06
Iter: 971 loss: 2.59749095e-06
Iter: 972 loss: 2.58990963e-06
Iter: 973 loss: 2.58923683e-06
Iter: 974 loss: 2.58924319e-06
Iter: 975 loss: 2.58874934e-06
Iter: 976 loss: 2.58786281e-06
Iter: 977 loss: 2.59364288e-06
Iter: 978 loss: 2.5877971e-06
Iter: 979 loss: 2.58714772e-06
Iter: 980 loss: 2.58610112e-06
Iter: 981 loss: 2.58604541e-06
Iter: 982 loss: 2.58507e-06
Iter: 983 loss: 2.58505042e-06
Iter: 984 loss: 2.58443288e-06
Iter: 985 loss: 2.58440377e-06
Iter: 986 loss: 2.58397085e-06
Iter: 987 loss: 2.58309092e-06
Iter: 988 loss: 2.58647901e-06
Iter: 989 loss: 2.58288082e-06
Iter: 990 loss: 2.58228351e-06
Iter: 991 loss: 2.58794762e-06
Iter: 992 loss: 2.58230784e-06
Iter: 993 loss: 2.58185287e-06
Iter: 994 loss: 2.58717023e-06
Iter: 995 loss: 2.58183218e-06
Iter: 996 loss: 2.58160662e-06
Iter: 997 loss: 2.58100363e-06
Iter: 998 loss: 2.58548062e-06
Iter: 999 loss: 2.58087857e-06
Iter: 1000 loss: 2.58016348e-06
Iter: 1001 loss: 2.58205728e-06
Iter: 1002 loss: 2.57996862e-06
Iter: 1003 loss: 2.57926104e-06
Iter: 1004 loss: 2.58401451e-06
Iter: 1005 loss: 2.57922511e-06
Iter: 1006 loss: 2.57869465e-06
Iter: 1007 loss: 2.57843067e-06
Iter: 1008 loss: 2.57811689e-06
Iter: 1009 loss: 2.577332e-06
Iter: 1010 loss: 2.58191e-06
Iter: 1011 loss: 2.57724946e-06
Iter: 1012 loss: 2.57662941e-06
Iter: 1013 loss: 2.57734769e-06
Iter: 1014 loss: 2.57630518e-06
Iter: 1015 loss: 2.57556303e-06
Iter: 1016 loss: 2.57820057e-06
Iter: 1017 loss: 2.5753593e-06
Iter: 1018 loss: 2.57465786e-06
Iter: 1019 loss: 2.57746638e-06
Iter: 1020 loss: 2.57449346e-06
Iter: 1021 loss: 2.5737304e-06
Iter: 1022 loss: 2.57275428e-06
Iter: 1023 loss: 2.57269585e-06
Iter: 1024 loss: 2.57207057e-06
Iter: 1025 loss: 2.57198235e-06
Iter: 1026 loss: 2.57176907e-06
Iter: 1027 loss: 2.57167176e-06
Iter: 1028 loss: 2.57140437e-06
Iter: 1029 loss: 2.57058127e-06
Iter: 1030 loss: 2.57581e-06
Iter: 1031 loss: 2.57034617e-06
Iter: 1032 loss: 2.5696886e-06
Iter: 1033 loss: 2.57249167e-06
Iter: 1034 loss: 2.56949488e-06
Iter: 1035 loss: 2.56882868e-06
Iter: 1036 loss: 2.56857857e-06
Iter: 1037 loss: 2.56819067e-06
Iter: 1038 loss: 2.56717522e-06
Iter: 1039 loss: 2.57357487e-06
Iter: 1040 loss: 2.56707949e-06
Iter: 1041 loss: 2.5664192e-06
Iter: 1042 loss: 2.56894828e-06
Iter: 1043 loss: 2.56627732e-06
Iter: 1044 loss: 2.56549038e-06
Iter: 1045 loss: 2.5661127e-06
Iter: 1046 loss: 2.56496742e-06
Iter: 1047 loss: 2.56406975e-06
Iter: 1048 loss: 2.56529574e-06
Iter: 1049 loss: 2.56358589e-06
Iter: 1050 loss: 2.56250814e-06
Iter: 1051 loss: 2.5681079e-06
Iter: 1052 loss: 2.56244221e-06
Iter: 1053 loss: 2.56158887e-06
Iter: 1054 loss: 2.56157728e-06
Iter: 1055 loss: 2.56090834e-06
Iter: 1056 loss: 2.56004978e-06
Iter: 1057 loss: 2.56958128e-06
Iter: 1058 loss: 2.56006319e-06
Iter: 1059 loss: 2.5597592e-06
Iter: 1060 loss: 2.55969417e-06
Iter: 1061 loss: 2.5593115e-06
Iter: 1062 loss: 2.55903865e-06
Iter: 1063 loss: 2.55887608e-06
Iter: 1064 loss: 2.55848886e-06
Iter: 1065 loss: 2.55808209e-06
Iter: 1066 loss: 2.55798977e-06
Iter: 1067 loss: 2.55739064e-06
Iter: 1068 loss: 2.55770942e-06
Iter: 1069 loss: 2.55696159e-06
Iter: 1070 loss: 2.55599434e-06
Iter: 1071 loss: 2.55966688e-06
Iter: 1072 loss: 2.55577925e-06
Iter: 1073 loss: 2.55513305e-06
Iter: 1074 loss: 2.55829218e-06
Iter: 1075 loss: 2.55499208e-06
Iter: 1076 loss: 2.55431041e-06
Iter: 1077 loss: 2.5540669e-06
Iter: 1078 loss: 2.5536674e-06
Iter: 1079 loss: 2.55278974e-06
Iter: 1080 loss: 2.55874602e-06
Iter: 1081 loss: 2.55275813e-06
Iter: 1082 loss: 2.55199757e-06
Iter: 1083 loss: 2.55191981e-06
Iter: 1084 loss: 2.55143868e-06
Iter: 1085 loss: 2.55035252e-06
Iter: 1086 loss: 2.55850318e-06
Iter: 1087 loss: 2.55026816e-06
Iter: 1088 loss: 2.54967358e-06
Iter: 1089 loss: 2.55160103e-06
Iter: 1090 loss: 2.54946349e-06
Iter: 1091 loss: 2.54881115e-06
Iter: 1092 loss: 2.54957695e-06
Iter: 1093 loss: 2.54843394e-06
Iter: 1094 loss: 2.54870793e-06
Iter: 1095 loss: 2.54813585e-06
Iter: 1096 loss: 2.54797487e-06
Iter: 1097 loss: 2.54733959e-06
Iter: 1098 loss: 2.54829183e-06
Iter: 1099 loss: 2.54688939e-06
Iter: 1100 loss: 2.54604e-06
Iter: 1101 loss: 2.54952442e-06
Iter: 1102 loss: 2.54588281e-06
Iter: 1103 loss: 2.54502584e-06
Iter: 1104 loss: 2.54561974e-06
Iter: 1105 loss: 2.5444408e-06
Iter: 1106 loss: 2.54351357e-06
Iter: 1107 loss: 2.549079e-06
Iter: 1108 loss: 2.54341103e-06
Iter: 1109 loss: 2.54246697e-06
Iter: 1110 loss: 2.54281622e-06
Iter: 1111 loss: 2.54186e-06
Iter: 1112 loss: 2.540879e-06
Iter: 1113 loss: 2.54941187e-06
Iter: 1114 loss: 2.5408558e-06
Iter: 1115 loss: 2.54015185e-06
Iter: 1116 loss: 2.54102679e-06
Iter: 1117 loss: 2.53974804e-06
Iter: 1118 loss: 2.53880239e-06
Iter: 1119 loss: 2.54073257e-06
Iter: 1120 loss: 2.53840926e-06
Iter: 1121 loss: 2.53768371e-06
Iter: 1122 loss: 2.5404986e-06
Iter: 1123 loss: 2.53752705e-06
Iter: 1124 loss: 2.53677263e-06
Iter: 1125 loss: 2.53755024e-06
Iter: 1126 loss: 2.53635108e-06
Iter: 1127 loss: 2.53688881e-06
Iter: 1128 loss: 2.53609505e-06
Iter: 1129 loss: 2.53584494e-06
Iter: 1130 loss: 2.53518897e-06
Iter: 1131 loss: 2.53948315e-06
Iter: 1132 loss: 2.53508392e-06
Iter: 1133 loss: 2.53444387e-06
Iter: 1134 loss: 2.53476537e-06
Iter: 1135 loss: 2.53400185e-06
Iter: 1136 loss: 2.5331974e-06
Iter: 1137 loss: 2.53559119e-06
Iter: 1138 loss: 2.53298049e-06
Iter: 1139 loss: 2.53216263e-06
Iter: 1140 loss: 2.53501685e-06
Iter: 1141 loss: 2.53190728e-06
Iter: 1142 loss: 2.531066e-06
Iter: 1143 loss: 2.53270446e-06
Iter: 1144 loss: 2.53070266e-06
Iter: 1145 loss: 2.52977088e-06
Iter: 1146 loss: 2.53298867e-06
Iter: 1147 loss: 2.52951122e-06
Iter: 1148 loss: 2.52879636e-06
Iter: 1149 loss: 2.53066264e-06
Iter: 1150 loss: 2.52847394e-06
Iter: 1151 loss: 2.52751806e-06
Iter: 1152 loss: 2.52871268e-06
Iter: 1153 loss: 2.52706877e-06
Iter: 1154 loss: 2.52635073e-06
Iter: 1155 loss: 2.5295642e-06
Iter: 1156 loss: 2.52616746e-06
Iter: 1157 loss: 2.52530754e-06
Iter: 1158 loss: 2.52616269e-06
Iter: 1159 loss: 2.5248014e-06
Iter: 1160 loss: 2.52409745e-06
Iter: 1161 loss: 2.52414634e-06
Iter: 1162 loss: 2.52384257e-06
Iter: 1163 loss: 2.52380732e-06
Iter: 1164 loss: 2.52361383e-06
Iter: 1165 loss: 2.5229408e-06
Iter: 1166 loss: 2.52457266e-06
Iter: 1167 loss: 2.52259201e-06
Iter: 1168 loss: 2.52159589e-06
Iter: 1169 loss: 2.52423e-06
Iter: 1170 loss: 2.52125847e-06
Iter: 1171 loss: 2.52039172e-06
Iter: 1172 loss: 2.52174777e-06
Iter: 1173 loss: 2.52002951e-06
Iter: 1174 loss: 2.51898837e-06
Iter: 1175 loss: 2.52426344e-06
Iter: 1176 loss: 2.51888923e-06
Iter: 1177 loss: 2.51794745e-06
Iter: 1178 loss: 2.51903248e-06
Iter: 1179 loss: 2.51746701e-06
Iter: 1180 loss: 2.51650317e-06
Iter: 1181 loss: 2.52244445e-06
Iter: 1182 loss: 2.51637766e-06
Iter: 1183 loss: 2.51556366e-06
Iter: 1184 loss: 2.5167576e-06
Iter: 1185 loss: 2.51520987e-06
Iter: 1186 loss: 2.51440065e-06
Iter: 1187 loss: 2.51897654e-06
Iter: 1188 loss: 2.51428582e-06
Iter: 1189 loss: 2.51349047e-06
Iter: 1190 loss: 2.51292931e-06
Iter: 1191 loss: 2.51276515e-06
Iter: 1192 loss: 2.51158326e-06
Iter: 1193 loss: 2.52030145e-06
Iter: 1194 loss: 2.51150232e-06
Iter: 1195 loss: 2.51094639e-06
Iter: 1196 loss: 2.51803885e-06
Iter: 1197 loss: 2.5109166e-06
Iter: 1198 loss: 2.51022084e-06
Iter: 1199 loss: 2.51104166e-06
Iter: 1200 loss: 2.50991434e-06
Iter: 1201 loss: 2.50919447e-06
Iter: 1202 loss: 2.50831135e-06
Iter: 1203 loss: 2.50821813e-06
Iter: 1204 loss: 2.50744552e-06
Iter: 1205 loss: 2.5114723e-06
Iter: 1206 loss: 2.50735707e-06
Iter: 1207 loss: 2.50647031e-06
Iter: 1208 loss: 2.50519679e-06
Iter: 1209 loss: 2.50517e-06
Iter: 1210 loss: 2.50406129e-06
Iter: 1211 loss: 2.5180625e-06
Iter: 1212 loss: 2.50407e-06
Iter: 1213 loss: 2.50317407e-06
Iter: 1214 loss: 2.50421272e-06
Iter: 1215 loss: 2.50263656e-06
Iter: 1216 loss: 2.50164339e-06
Iter: 1217 loss: 2.50616608e-06
Iter: 1218 loss: 2.50143762e-06
Iter: 1219 loss: 2.50045036e-06
Iter: 1220 loss: 2.50216181e-06
Iter: 1221 loss: 2.49998743e-06
Iter: 1222 loss: 2.49903724e-06
Iter: 1223 loss: 2.50378548e-06
Iter: 1224 loss: 2.49879918e-06
Iter: 1225 loss: 2.49808136e-06
Iter: 1226 loss: 2.49968753e-06
Iter: 1227 loss: 2.49772165e-06
Iter: 1228 loss: 2.4968358e-06
Iter: 1229 loss: 2.50046833e-06
Iter: 1230 loss: 2.49669665e-06
Iter: 1231 loss: 2.49606865e-06
Iter: 1232 loss: 2.50370385e-06
Iter: 1233 loss: 2.49601976e-06
Iter: 1234 loss: 2.4953813e-06
Iter: 1235 loss: 2.49752065e-06
Iter: 1236 loss: 2.49520895e-06
Iter: 1237 loss: 2.49489904e-06
Iter: 1238 loss: 2.4945698e-06
Iter: 1239 loss: 2.49450773e-06
Iter: 1240 loss: 2.49391928e-06
Iter: 1241 loss: 2.49327195e-06
Iter: 1242 loss: 2.49310801e-06
Iter: 1243 loss: 2.49232744e-06
Iter: 1244 loss: 2.50567291e-06
Iter: 1245 loss: 2.49228219e-06
Iter: 1246 loss: 2.49183699e-06
Iter: 1247 loss: 2.49129312e-06
Iter: 1248 loss: 2.4912797e-06
Iter: 1249 loss: 2.49021195e-06
Iter: 1250 loss: 2.49396362e-06
Iter: 1251 loss: 2.4899964e-06
Iter: 1252 loss: 2.48936385e-06
Iter: 1253 loss: 2.49160075e-06
Iter: 1254 loss: 2.48922584e-06
Iter: 1255 loss: 2.48836614e-06
Iter: 1256 loss: 2.48918877e-06
Iter: 1257 loss: 2.48794049e-06
Iter: 1258 loss: 2.48701986e-06
Iter: 1259 loss: 2.49110826e-06
Iter: 1260 loss: 2.48690344e-06
Iter: 1261 loss: 2.48609354e-06
Iter: 1262 loss: 2.48786864e-06
Iter: 1263 loss: 2.48572678e-06
Iter: 1264 loss: 2.48506331e-06
Iter: 1265 loss: 2.48986703e-06
Iter: 1266 loss: 2.4850292e-06
Iter: 1267 loss: 2.48469428e-06
Iter: 1268 loss: 2.48467745e-06
Iter: 1269 loss: 2.48438141e-06
Iter: 1270 loss: 2.48368679e-06
Iter: 1271 loss: 2.48851347e-06
Iter: 1272 loss: 2.48358242e-06
Iter: 1273 loss: 2.48286278e-06
Iter: 1274 loss: 2.48515653e-06
Iter: 1275 loss: 2.48261176e-06
Iter: 1276 loss: 2.48199103e-06
Iter: 1277 loss: 2.48296215e-06
Iter: 1278 loss: 2.48171045e-06
Iter: 1279 loss: 2.48093511e-06
Iter: 1280 loss: 2.48251376e-06
Iter: 1281 loss: 2.48067022e-06
Iter: 1282 loss: 2.47974458e-06
Iter: 1283 loss: 2.48125957e-06
Iter: 1284 loss: 2.47932803e-06
Iter: 1285 loss: 2.47867524e-06
Iter: 1286 loss: 2.48745619e-06
Iter: 1287 loss: 2.47868456e-06
Iter: 1288 loss: 2.47808339e-06
Iter: 1289 loss: 2.47696948e-06
Iter: 1290 loss: 2.49518644e-06
Iter: 1291 loss: 2.47695289e-06
Iter: 1292 loss: 2.4757685e-06
Iter: 1293 loss: 2.49242839e-06
Iter: 1294 loss: 2.47577896e-06
Iter: 1295 loss: 2.47511389e-06
Iter: 1296 loss: 2.47524076e-06
Iter: 1297 loss: 2.47461776e-06
Iter: 1298 loss: 2.47359776e-06
Iter: 1299 loss: 2.47757634e-06
Iter: 1300 loss: 2.47334606e-06
Iter: 1301 loss: 2.47346861e-06
Iter: 1302 loss: 2.4730839e-06
Iter: 1303 loss: 2.47276762e-06
Iter: 1304 loss: 2.47244452e-06
Iter: 1305 loss: 2.47240723e-06
Iter: 1306 loss: 2.47196431e-06
Iter: 1307 loss: 2.47124035e-06
Iter: 1308 loss: 2.47122011e-06
Iter: 1309 loss: 2.47037315e-06
Iter: 1310 loss: 2.47143635e-06
Iter: 1311 loss: 2.46996728e-06
Iter: 1312 loss: 2.46883906e-06
Iter: 1313 loss: 2.47354956e-06
Iter: 1314 loss: 2.46859167e-06
Iter: 1315 loss: 2.46777654e-06
Iter: 1316 loss: 2.47143589e-06
Iter: 1317 loss: 2.4676533e-06
Iter: 1318 loss: 2.46692889e-06
Iter: 1319 loss: 2.46901391e-06
Iter: 1320 loss: 2.46670606e-06
Iter: 1321 loss: 2.46590594e-06
Iter: 1322 loss: 2.46639524e-06
Iter: 1323 loss: 2.46531113e-06
Iter: 1324 loss: 2.46445939e-06
Iter: 1325 loss: 2.46745662e-06
Iter: 1326 loss: 2.46420791e-06
Iter: 1327 loss: 2.46347145e-06
Iter: 1328 loss: 2.46563604e-06
Iter: 1329 loss: 2.46318359e-06
Iter: 1330 loss: 2.46235e-06
Iter: 1331 loss: 2.4651913e-06
Iter: 1332 loss: 2.46204718e-06
Iter: 1333 loss: 2.46167815e-06
Iter: 1334 loss: 2.46167565e-06
Iter: 1335 loss: 2.46125705e-06
Iter: 1336 loss: 2.46315108e-06
Iter: 1337 loss: 2.46114041e-06
Iter: 1338 loss: 2.46078866e-06
Iter: 1339 loss: 2.45995057e-06
Iter: 1340 loss: 2.46811464e-06
Iter: 1341 loss: 2.4598321e-06
Iter: 1342 loss: 2.4591086e-06
Iter: 1343 loss: 2.46350419e-06
Iter: 1344 loss: 2.45898809e-06
Iter: 1345 loss: 2.45834326e-06
Iter: 1346 loss: 2.45858496e-06
Iter: 1347 loss: 2.45785941e-06
Iter: 1348 loss: 2.4569988e-06
Iter: 1349 loss: 2.46340846e-06
Iter: 1350 loss: 2.45689193e-06
Iter: 1351 loss: 2.45630099e-06
Iter: 1352 loss: 2.45646015e-06
Iter: 1353 loss: 2.45590445e-06
Iter: 1354 loss: 2.4550377e-06
Iter: 1355 loss: 2.45774208e-06
Iter: 1356 loss: 2.45476099e-06
Iter: 1357 loss: 2.45401247e-06
Iter: 1358 loss: 2.45419733e-06
Iter: 1359 loss: 2.45350748e-06
Iter: 1360 loss: 2.45257888e-06
Iter: 1361 loss: 2.46142622e-06
Iter: 1362 loss: 2.45251613e-06
Iter: 1363 loss: 2.45186652e-06
Iter: 1364 loss: 2.45380215e-06
Iter: 1365 loss: 2.45173783e-06
Iter: 1366 loss: 2.4511412e-06
Iter: 1367 loss: 2.45213732e-06
Iter: 1368 loss: 2.45091496e-06
Iter: 1369 loss: 2.45070692e-06
Iter: 1370 loss: 2.45057345e-06
Iter: 1371 loss: 2.45021783e-06
Iter: 1372 loss: 2.44979628e-06
Iter: 1373 loss: 2.44977491e-06
Iter: 1374 loss: 2.44931539e-06
Iter: 1375 loss: 2.44908279e-06
Iter: 1376 loss: 2.44890725e-06
Iter: 1377 loss: 2.44826197e-06
Iter: 1378 loss: 2.44824423e-06
Iter: 1379 loss: 2.4477381e-06
Iter: 1380 loss: 2.44669786e-06
Iter: 1381 loss: 2.45288106e-06
Iter: 1382 loss: 2.44657463e-06
Iter: 1383 loss: 2.4460096e-06
Iter: 1384 loss: 2.44679609e-06
Iter: 1385 loss: 2.44568764e-06
Iter: 1386 loss: 2.44489411e-06
Iter: 1387 loss: 2.44749435e-06
Iter: 1388 loss: 2.44463399e-06
Iter: 1389 loss: 2.44406692e-06
Iter: 1390 loss: 2.44820194e-06
Iter: 1391 loss: 2.44398461e-06
Iter: 1392 loss: 2.44354737e-06
Iter: 1393 loss: 2.44310149e-06
Iter: 1394 loss: 2.44304897e-06
Iter: 1395 loss: 2.44215789e-06
Iter: 1396 loss: 2.44583634e-06
Iter: 1397 loss: 2.44202e-06
Iter: 1398 loss: 2.44131684e-06
Iter: 1399 loss: 2.44190142e-06
Iter: 1400 loss: 2.44090961e-06
Iter: 1401 loss: 2.44010153e-06
Iter: 1402 loss: 2.45043702e-06
Iter: 1403 loss: 2.44010698e-06
Iter: 1404 loss: 2.43996601e-06
Iter: 1405 loss: 2.43987256e-06
Iter: 1406 loss: 2.43968771e-06
Iter: 1407 loss: 2.4391411e-06
Iter: 1408 loss: 2.44231978e-06
Iter: 1409 loss: 2.43898512e-06
Iter: 1410 loss: 2.4383844e-06
Iter: 1411 loss: 2.43924887e-06
Iter: 1412 loss: 2.43808518e-06
Iter: 1413 loss: 2.43739396e-06
Iter: 1414 loss: 2.43862155e-06
Iter: 1415 loss: 2.43705381e-06
Iter: 1416 loss: 2.43636259e-06
Iter: 1417 loss: 2.43930413e-06
Iter: 1418 loss: 2.43619843e-06
Iter: 1419 loss: 2.435429e-06
Iter: 1420 loss: 2.43532986e-06
Iter: 1421 loss: 2.43473892e-06
Iter: 1422 loss: 2.43395129e-06
Iter: 1423 loss: 2.44265129e-06
Iter: 1424 loss: 2.43397426e-06
Iter: 1425 loss: 2.43325326e-06
Iter: 1426 loss: 2.43370414e-06
Iter: 1427 loss: 2.43285422e-06
Iter: 1428 loss: 2.43185877e-06
Iter: 1429 loss: 2.43455111e-06
Iter: 1430 loss: 2.43155682e-06
Iter: 1431 loss: 2.43075465e-06
Iter: 1432 loss: 2.4335061e-06
Iter: 1433 loss: 2.43057571e-06
Iter: 1434 loss: 2.42964961e-06
Iter: 1435 loss: 2.42875535e-06
Iter: 1436 loss: 2.42860597e-06
Iter: 1437 loss: 2.42768147e-06
Iter: 1438 loss: 2.4276726e-06
Iter: 1439 loss: 2.42731858e-06
Iter: 1440 loss: 2.42724582e-06
Iter: 1441 loss: 2.42685746e-06
Iter: 1442 loss: 2.42610076e-06
Iter: 1443 loss: 2.44291186e-06
Iter: 1444 loss: 2.4260944e-06
Iter: 1445 loss: 2.42541614e-06
Iter: 1446 loss: 2.42657961e-06
Iter: 1447 loss: 2.42514216e-06
Iter: 1448 loss: 2.42450051e-06
Iter: 1449 loss: 2.42449687e-06
Iter: 1450 loss: 2.42400438e-06
Iter: 1451 loss: 2.42309147e-06
Iter: 1452 loss: 2.4275123e-06
Iter: 1453 loss: 2.42289207e-06
Iter: 1454 loss: 2.4222611e-06
Iter: 1455 loss: 2.42384863e-06
Iter: 1456 loss: 2.4220144e-06
Iter: 1457 loss: 2.42120495e-06
Iter: 1458 loss: 2.42284068e-06
Iter: 1459 loss: 2.42086685e-06
Iter: 1460 loss: 2.42008014e-06
Iter: 1461 loss: 2.42195529e-06
Iter: 1462 loss: 2.41976295e-06
Iter: 1463 loss: 2.41905173e-06
Iter: 1464 loss: 2.42438637e-06
Iter: 1465 loss: 2.4189826e-06
Iter: 1466 loss: 2.41834641e-06
Iter: 1467 loss: 2.41830412e-06
Iter: 1468 loss: 2.41782664e-06
Iter: 1469 loss: 2.41679709e-06
Iter: 1470 loss: 2.42109877e-06
Iter: 1471 loss: 2.41660655e-06
Iter: 1472 loss: 2.41597627e-06
Iter: 1473 loss: 2.41704697e-06
Iter: 1474 loss: 2.4156991e-06
Iter: 1475 loss: 2.41537555e-06
Iter: 1476 loss: 2.41526618e-06
Iter: 1477 loss: 2.41501903e-06
Iter: 1478 loss: 2.41431644e-06
Iter: 1479 loss: 2.41948533e-06
Iter: 1480 loss: 2.41416069e-06
Iter: 1481 loss: 2.41355951e-06
Iter: 1482 loss: 2.41466751e-06
Iter: 1483 loss: 2.4133426e-06
Iter: 1484 loss: 2.41248244e-06
Iter: 1485 loss: 2.41388534e-06
Iter: 1486 loss: 2.41218868e-06
Iter: 1487 loss: 2.41147814e-06
Iter: 1488 loss: 2.41531029e-06
Iter: 1489 loss: 2.41139219e-06
Iter: 1490 loss: 2.41081966e-06
Iter: 1491 loss: 2.41069665e-06
Iter: 1492 loss: 2.41024918e-06
Iter: 1493 loss: 2.40957161e-06
Iter: 1494 loss: 2.41579664e-06
Iter: 1495 loss: 2.40955114e-06
Iter: 1496 loss: 2.40895042e-06
Iter: 1497 loss: 2.40861709e-06
Iter: 1498 loss: 2.40839063e-06
Iter: 1499 loss: 2.40743748e-06
Iter: 1500 loss: 2.4112519e-06
Iter: 1501 loss: 2.40716076e-06
Iter: 1502 loss: 2.40640838e-06
Iter: 1503 loss: 2.40912e-06
Iter: 1504 loss: 2.40619283e-06
Iter: 1505 loss: 2.40545023e-06
Iter: 1506 loss: 2.40758732e-06
Iter: 1507 loss: 2.4051692e-06
Iter: 1508 loss: 2.40486293e-06
Iter: 1509 loss: 2.40473378e-06
Iter: 1510 loss: 2.40434247e-06
Iter: 1511 loss: 2.40434429e-06
Iter: 1512 loss: 2.40401619e-06
Iter: 1513 loss: 2.40351756e-06
Iter: 1514 loss: 2.40336522e-06
Iter: 1515 loss: 2.40312056e-06
Iter: 1516 loss: 2.40259669e-06
Iter: 1517 loss: 2.40174882e-06
Iter: 1518 loss: 2.40174359e-06
Iter: 1519 loss: 2.40085319e-06
Iter: 1520 loss: 2.41240741e-06
Iter: 1521 loss: 2.40081499e-06
Iter: 1522 loss: 2.40019585e-06
Iter: 1523 loss: 2.40053851e-06
Iter: 1524 loss: 2.39974656e-06
Iter: 1525 loss: 2.39895553e-06
Iter: 1526 loss: 2.40401187e-06
Iter: 1527 loss: 2.39885458e-06
Iter: 1528 loss: 2.39824703e-06
Iter: 1529 loss: 2.39877454e-06
Iter: 1530 loss: 2.39790165e-06
Iter: 1531 loss: 2.39703468e-06
Iter: 1532 loss: 2.39820974e-06
Iter: 1533 loss: 2.39656742e-06
Iter: 1534 loss: 2.3958371e-06
Iter: 1535 loss: 2.40165082e-06
Iter: 1536 loss: 2.39579776e-06
Iter: 1537 loss: 2.39513952e-06
Iter: 1538 loss: 2.39446285e-06
Iter: 1539 loss: 2.39431347e-06
Iter: 1540 loss: 2.39359315e-06
Iter: 1541 loss: 2.39361771e-06
Iter: 1542 loss: 2.39348356e-06
Iter: 1543 loss: 2.39329097e-06
Iter: 1544 loss: 2.3931259e-06
Iter: 1545 loss: 2.39246856e-06
Iter: 1546 loss: 2.39591782e-06
Iter: 1547 loss: 2.39222481e-06
Iter: 1548 loss: 2.39157e-06
Iter: 1549 loss: 2.39451219e-06
Iter: 1550 loss: 2.39138171e-06
Iter: 1551 loss: 2.39067822e-06
Iter: 1552 loss: 2.39216342e-06
Iter: 1553 loss: 2.39037854e-06
Iter: 1554 loss: 2.389771e-06
Iter: 1555 loss: 2.39050541e-06
Iter: 1556 loss: 2.38942448e-06
Iter: 1557 loss: 2.3886862e-06
Iter: 1558 loss: 2.39087149e-06
Iter: 1559 loss: 2.38846724e-06
Iter: 1560 loss: 2.38777852e-06
Iter: 1561 loss: 2.3913135e-06
Iter: 1562 loss: 2.38769735e-06
Iter: 1563 loss: 2.38705252e-06
Iter: 1564 loss: 2.38723646e-06
Iter: 1565 loss: 2.386608e-06
Iter: 1566 loss: 2.38590837e-06
Iter: 1567 loss: 2.39250494e-06
Iter: 1568 loss: 2.38579514e-06
Iter: 1569 loss: 2.38530697e-06
Iter: 1570 loss: 2.38500229e-06
Iter: 1571 loss: 2.38475559e-06
Iter: 1572 loss: 2.38390339e-06
Iter: 1573 loss: 2.38972325e-06
Iter: 1574 loss: 2.38378061e-06
Iter: 1575 loss: 2.38322764e-06
Iter: 1576 loss: 2.38488656e-06
Iter: 1577 loss: 2.38297457e-06
Iter: 1578 loss: 2.38247799e-06
Iter: 1579 loss: 2.38244957e-06
Iter: 1580 loss: 2.38212806e-06
Iter: 1581 loss: 2.38131e-06
Iter: 1582 loss: 2.38989696e-06
Iter: 1583 loss: 2.38121243e-06
Iter: 1584 loss: 2.38062307e-06
Iter: 1585 loss: 2.38248322e-06
Iter: 1586 loss: 2.38042276e-06
Iter: 1587 loss: 2.37966674e-06
Iter: 1588 loss: 2.37965628e-06
Iter: 1589 loss: 2.37912377e-06
Iter: 1590 loss: 2.37816403e-06
Iter: 1591 loss: 2.38322923e-06
Iter: 1592 loss: 2.37799713e-06
Iter: 1593 loss: 2.37699351e-06
Iter: 1594 loss: 2.38102848e-06
Iter: 1595 loss: 2.37685776e-06
Iter: 1596 loss: 2.37606901e-06
Iter: 1597 loss: 2.37696759e-06
Iter: 1598 loss: 2.37574295e-06
Iter: 1599 loss: 2.37484392e-06
Iter: 1600 loss: 2.37829772e-06
Iter: 1601 loss: 2.3745813e-06
Iter: 1602 loss: 2.37393215e-06
Iter: 1603 loss: 2.37481572e-06
Iter: 1604 loss: 2.37353402e-06
Iter: 1605 loss: 2.37263976e-06
Iter: 1606 loss: 2.37622407e-06
Iter: 1607 loss: 2.37236827e-06
Iter: 1608 loss: 2.37184577e-06
Iter: 1609 loss: 2.3756761e-06
Iter: 1610 loss: 2.37174595e-06
Iter: 1611 loss: 2.37140307e-06
Iter: 1612 loss: 2.37145105e-06
Iter: 1613 loss: 2.37099766e-06
Iter: 1614 loss: 2.37062909e-06
Iter: 1615 loss: 2.37048039e-06
Iter: 1616 loss: 2.36997244e-06
Iter: 1617 loss: 2.3699904e-06
Iter: 1618 loss: 2.36959477e-06
Iter: 1619 loss: 2.36888445e-06
Iter: 1620 loss: 2.3687594e-06
Iter: 1621 loss: 2.36826941e-06
Iter: 1622 loss: 2.36729647e-06
Iter: 1623 loss: 2.37516588e-06
Iter: 1624 loss: 2.36720916e-06
Iter: 1625 loss: 2.36653477e-06
Iter: 1626 loss: 2.36653796e-06
Iter: 1627 loss: 2.36599953e-06
Iter: 1628 loss: 2.36519872e-06
Iter: 1629 loss: 2.37224504e-06
Iter: 1630 loss: 2.36517553e-06
Iter: 1631 loss: 2.36453707e-06
Iter: 1632 loss: 2.36609753e-06
Iter: 1633 loss: 2.36431515e-06
Iter: 1634 loss: 2.36360711e-06
Iter: 1635 loss: 2.36442975e-06
Iter: 1636 loss: 2.36320284e-06
Iter: 1637 loss: 2.36244e-06
Iter: 1638 loss: 2.36422079e-06
Iter: 1639 loss: 2.36211099e-06
Iter: 1640 loss: 2.3613e-06
Iter: 1641 loss: 2.36430219e-06
Iter: 1642 loss: 2.361101e-06
Iter: 1643 loss: 2.36038e-06
Iter: 1644 loss: 2.36164328e-06
Iter: 1645 loss: 2.36009328e-06
Iter: 1646 loss: 2.35989455e-06
Iter: 1647 loss: 2.35969355e-06
Iter: 1648 loss: 2.35933612e-06
Iter: 1649 loss: 2.35922585e-06
Iter: 1650 loss: 2.35901e-06
Iter: 1651 loss: 2.35869152e-06
Iter: 1652 loss: 2.35820698e-06
Iter: 1653 loss: 2.35818015e-06
Iter: 1654 loss: 2.35740754e-06
Iter: 1655 loss: 2.3587445e-06
Iter: 1656 loss: 2.35707421e-06
Iter: 1657 loss: 2.35640482e-06
Iter: 1658 loss: 2.35894913e-06
Iter: 1659 loss: 2.35623156e-06
Iter: 1660 loss: 2.3554353e-06
Iter: 1661 loss: 2.35585094e-06
Iter: 1662 loss: 2.35495691e-06
Iter: 1663 loss: 2.3542641e-06
Iter: 1664 loss: 2.35829043e-06
Iter: 1665 loss: 2.35414018e-06
Iter: 1666 loss: 2.35348898e-06
Iter: 1667 loss: 2.35496736e-06
Iter: 1668 loss: 2.35321886e-06
Iter: 1669 loss: 2.35252264e-06
Iter: 1670 loss: 2.35628841e-06
Iter: 1671 loss: 2.35238417e-06
Iter: 1672 loss: 2.35190828e-06
Iter: 1673 loss: 2.35209745e-06
Iter: 1674 loss: 2.35156722e-06
Iter: 1675 loss: 2.35080279e-06
Iter: 1676 loss: 2.35268953e-06
Iter: 1677 loss: 2.35056359e-06
Iter: 1678 loss: 2.34980394e-06
Iter: 1679 loss: 2.35218249e-06
Iter: 1680 loss: 2.34958497e-06
Iter: 1681 loss: 2.34956656e-06
Iter: 1682 loss: 2.34929621e-06
Iter: 1683 loss: 2.34900654e-06
Iter: 1684 loss: 2.34858953e-06
Iter: 1685 loss: 2.34858135e-06
Iter: 1686 loss: 2.34810386e-06
Iter: 1687 loss: 2.34761956e-06
Iter: 1688 loss: 2.34750428e-06
Iter: 1689 loss: 2.34675144e-06
Iter: 1690 loss: 2.35046332e-06
Iter: 1691 loss: 2.34661366e-06
Iter: 1692 loss: 2.34598019e-06
Iter: 1693 loss: 2.34635172e-06
Iter: 1694 loss: 2.34548702e-06
Iter: 1695 loss: 2.34463437e-06
Iter: 1696 loss: 2.3487114e-06
Iter: 1697 loss: 2.34449067e-06
Iter: 1698 loss: 2.34383401e-06
Iter: 1699 loss: 2.34475715e-06
Iter: 1700 loss: 2.34358208e-06
Iter: 1701 loss: 2.34274057e-06
Iter: 1702 loss: 2.34415734e-06
Iter: 1703 loss: 2.34238109e-06
Iter: 1704 loss: 2.34174968e-06
Iter: 1705 loss: 2.34951358e-06
Iter: 1706 loss: 2.3418072e-06
Iter: 1707 loss: 2.34124218e-06
Iter: 1708 loss: 2.34065055e-06
Iter: 1709 loss: 2.34056165e-06
Iter: 1710 loss: 2.33969877e-06
Iter: 1711 loss: 2.34583831e-06
Iter: 1712 loss: 2.33963465e-06
Iter: 1713 loss: 2.33886635e-06
Iter: 1714 loss: 2.33854075e-06
Iter: 1715 loss: 2.33815445e-06
Iter: 1716 loss: 2.33726769e-06
Iter: 1717 loss: 2.33727405e-06
Iter: 1718 loss: 2.33680043e-06
Iter: 1719 loss: 2.33676201e-06
Iter: 1720 loss: 2.33651645e-06
Iter: 1721 loss: 2.33597302e-06
Iter: 1722 loss: 2.3437542e-06
Iter: 1723 loss: 2.33594892e-06
Iter: 1724 loss: 2.33531955e-06
Iter: 1725 loss: 2.33547462e-06
Iter: 1726 loss: 2.334893e-06
Iter: 1727 loss: 2.33406399e-06
Iter: 1728 loss: 2.3368e-06
Iter: 1729 loss: 2.33386982e-06
Iter: 1730 loss: 2.33313585e-06
Iter: 1731 loss: 2.33596415e-06
Iter: 1732 loss: 2.3329344e-06
Iter: 1733 loss: 2.33226183e-06
Iter: 1734 loss: 2.33227775e-06
Iter: 1735 loss: 2.33172818e-06
Iter: 1736 loss: 2.33077185e-06
Iter: 1737 loss: 2.33664105e-06
Iter: 1738 loss: 2.33068977e-06
Iter: 1739 loss: 2.3300372e-06
Iter: 1740 loss: 2.3302639e-06
Iter: 1741 loss: 2.32951697e-06
Iter: 1742 loss: 2.32862067e-06
Iter: 1743 loss: 2.33647279e-06
Iter: 1744 loss: 2.32852153e-06
Iter: 1745 loss: 2.32801835e-06
Iter: 1746 loss: 2.32802154e-06
Iter: 1747 loss: 2.32755337e-06
Iter: 1748 loss: 2.32658886e-06
Iter: 1749 loss: 2.32886941e-06
Iter: 1750 loss: 2.32625985e-06
Iter: 1751 loss: 2.32592424e-06
Iter: 1752 loss: 2.32584921e-06
Iter: 1753 loss: 2.32536127e-06
Iter: 1754 loss: 2.32614866e-06
Iter: 1755 loss: 2.32521302e-06
Iter: 1756 loss: 2.32480534e-06
Iter: 1757 loss: 2.32395246e-06
Iter: 1758 loss: 2.33272476e-06
Iter: 1759 loss: 2.32377306e-06
Iter: 1760 loss: 2.32293132e-06
Iter: 1761 loss: 2.33226183e-06
Iter: 1762 loss: 2.32290768e-06
Iter: 1763 loss: 2.32224147e-06
Iter: 1764 loss: 2.32222828e-06
Iter: 1765 loss: 2.32173375e-06
Iter: 1766 loss: 2.32078673e-06
Iter: 1767 loss: 2.32648949e-06
Iter: 1768 loss: 2.32065986e-06
Iter: 1769 loss: 2.31997728e-06
Iter: 1770 loss: 2.32159391e-06
Iter: 1771 loss: 2.31965805e-06
Iter: 1772 loss: 2.31906552e-06
Iter: 1773 loss: 2.31986223e-06
Iter: 1774 loss: 2.31877812e-06
Iter: 1775 loss: 2.31787044e-06
Iter: 1776 loss: 2.3196967e-06
Iter: 1777 loss: 2.3175478e-06
Iter: 1778 loss: 2.31687363e-06
Iter: 1779 loss: 2.31953595e-06
Iter: 1780 loss: 2.31674358e-06
Iter: 1781 loss: 2.31594026e-06
Iter: 1782 loss: 2.3174216e-06
Iter: 1783 loss: 2.31566764e-06
Iter: 1784 loss: 2.31483955e-06
Iter: 1785 loss: 2.31882768e-06
Iter: 1786 loss: 2.31473359e-06
Iter: 1787 loss: 2.314114e-06
Iter: 1788 loss: 2.31588251e-06
Iter: 1789 loss: 2.31391641e-06
Iter: 1790 loss: 2.31355534e-06
Iter: 1791 loss: 2.31348804e-06
Iter: 1792 loss: 2.3131156e-06
Iter: 1793 loss: 2.31247441e-06
Iter: 1794 loss: 2.32388334e-06
Iter: 1795 loss: 2.31244803e-06
Iter: 1796 loss: 2.31190666e-06
Iter: 1797 loss: 2.31178274e-06
Iter: 1798 loss: 2.31140393e-06
Iter: 1799 loss: 2.3106295e-06
Iter: 1800 loss: 2.31871354e-06
Iter: 1801 loss: 2.31067452e-06
Iter: 1802 loss: 2.31012245e-06
Iter: 1803 loss: 2.30982459e-06
Iter: 1804 loss: 2.30962701e-06
Iter: 1805 loss: 2.30868113e-06
Iter: 1806 loss: 2.31370859e-06
Iter: 1807 loss: 2.30860223e-06
Iter: 1808 loss: 2.30788646e-06
Iter: 1809 loss: 2.31129684e-06
Iter: 1810 loss: 2.3077921e-06
Iter: 1811 loss: 2.30734986e-06
Iter: 1812 loss: 2.307876e-06
Iter: 1813 loss: 2.30707724e-06
Iter: 1814 loss: 2.30639898e-06
Iter: 1815 loss: 2.30643536e-06
Iter: 1816 loss: 2.305831e-06
Iter: 1817 loss: 2.30510227e-06
Iter: 1818 loss: 2.31176364e-06
Iter: 1819 loss: 2.30507567e-06
Iter: 1820 loss: 2.3046166e-06
Iter: 1821 loss: 2.30523301e-06
Iter: 1822 loss: 2.30427304e-06
Iter: 1823 loss: 2.30410478e-06
Iter: 1824 loss: 2.30398109e-06
Iter: 1825 loss: 2.30371211e-06
Iter: 1826 loss: 2.30316e-06
Iter: 1827 loss: 2.30319165e-06
Iter: 1828 loss: 2.30267506e-06
Iter: 1829 loss: 2.30231922e-06
Iter: 1830 loss: 2.30215096e-06
Iter: 1831 loss: 2.30148089e-06
Iter: 1832 loss: 2.30958085e-06
Iter: 1833 loss: 2.30144815e-06
Iter: 1834 loss: 2.30095657e-06
Iter: 1835 loss: 2.30058686e-06
Iter: 1836 loss: 2.30043838e-06
Iter: 1837 loss: 2.29957436e-06
Iter: 1838 loss: 2.30369642e-06
Iter: 1839 loss: 2.29938905e-06
Iter: 1840 loss: 2.29872649e-06
Iter: 1841 loss: 2.30228579e-06
Iter: 1842 loss: 2.29863872e-06
Iter: 1843 loss: 2.29798729e-06
Iter: 1844 loss: 2.29734042e-06
Iter: 1845 loss: 2.29716147e-06
Iter: 1846 loss: 2.29617376e-06
Iter: 1847 loss: 2.30594287e-06
Iter: 1848 loss: 2.29616512e-06
Iter: 1849 loss: 2.29542593e-06
Iter: 1850 loss: 2.29693114e-06
Iter: 1851 loss: 2.29526381e-06
Iter: 1852 loss: 2.2945153e-06
Iter: 1853 loss: 2.29732359e-06
Iter: 1854 loss: 2.29433704e-06
Iter: 1855 loss: 2.29373723e-06
Iter: 1856 loss: 2.29572743e-06
Iter: 1857 loss: 2.29354532e-06
Iter: 1858 loss: 2.29365401e-06
Iter: 1859 loss: 2.29338275e-06
Iter: 1860 loss: 2.29322e-06
Iter: 1861 loss: 2.29280931e-06
Iter: 1862 loss: 2.29417e-06
Iter: 1863 loss: 2.29258626e-06
Iter: 1864 loss: 2.29193756e-06
Iter: 1865 loss: 2.29326815e-06
Iter: 1866 loss: 2.29165516e-06
Iter: 1867 loss: 2.29106854e-06
Iter: 1868 loss: 2.29373541e-06
Iter: 1869 loss: 2.29093894e-06
Iter: 1870 loss: 2.2903464e-06
Iter: 1871 loss: 2.29129409e-06
Iter: 1872 loss: 2.29006673e-06
Iter: 1873 loss: 2.28940507e-06
Iter: 1874 loss: 2.29133957e-06
Iter: 1875 loss: 2.28912654e-06
Iter: 1876 loss: 2.28853787e-06
Iter: 1877 loss: 2.28875138e-06
Iter: 1878 loss: 2.28807903e-06
Iter: 1879 loss: 2.28724411e-06
Iter: 1880 loss: 2.29355192e-06
Iter: 1881 loss: 2.28717772e-06
Iter: 1882 loss: 2.2866343e-06
Iter: 1883 loss: 2.28654608e-06
Iter: 1884 loss: 2.28618637e-06
Iter: 1885 loss: 2.28526619e-06
Iter: 1886 loss: 2.28959971e-06
Iter: 1887 loss: 2.28506178e-06
Iter: 1888 loss: 2.28442968e-06
Iter: 1889 loss: 2.28699719e-06
Iter: 1890 loss: 2.28428871e-06
Iter: 1891 loss: 2.28369e-06
Iter: 1892 loss: 2.2871252e-06
Iter: 1893 loss: 2.28357567e-06
Iter: 1894 loss: 2.28330464e-06
Iter: 1895 loss: 2.28322642e-06
Iter: 1896 loss: 2.28311865e-06
Iter: 1897 loss: 2.28260069e-06
Iter: 1898 loss: 2.28368276e-06
Iter: 1899 loss: 2.28231852e-06
Iter: 1900 loss: 2.28153067e-06
Iter: 1901 loss: 2.28458384e-06
Iter: 1902 loss: 2.28143972e-06
Iter: 1903 loss: 2.28066733e-06
Iter: 1904 loss: 2.2819163e-06
Iter: 1905 loss: 2.28033537e-06
Iter: 1906 loss: 2.27957139e-06
Iter: 1907 loss: 2.28400586e-06
Iter: 1908 loss: 2.27943769e-06
Iter: 1909 loss: 2.27885357e-06
Iter: 1910 loss: 2.27981013e-06
Iter: 1911 loss: 2.27857936e-06
Iter: 1912 loss: 2.27781493e-06
Iter: 1913 loss: 2.2782242e-06
Iter: 1914 loss: 2.27729333e-06
Iter: 1915 loss: 2.27663531e-06
Iter: 1916 loss: 2.28336262e-06
Iter: 1917 loss: 2.27656392e-06
Iter: 1918 loss: 2.27598389e-06
Iter: 1919 loss: 2.27526425e-06
Iter: 1920 loss: 2.27514488e-06
Iter: 1921 loss: 2.2743161e-06
Iter: 1922 loss: 2.28451336e-06
Iter: 1923 loss: 2.27425926e-06
Iter: 1924 loss: 2.27375494e-06
Iter: 1925 loss: 2.27532746e-06
Iter: 1926 loss: 2.27356645e-06
Iter: 1927 loss: 2.2732122e-06
Iter: 1928 loss: 2.27317241e-06
Iter: 1929 loss: 2.27277042e-06
Iter: 1930 loss: 2.27282044e-06
Iter: 1931 loss: 2.27248665e-06
Iter: 1932 loss: 2.27220471e-06
Iter: 1933 loss: 2.27169903e-06
Iter: 1934 loss: 2.27167857e-06
Iter: 1935 loss: 2.27099736e-06
Iter: 1936 loss: 2.27133569e-06
Iter: 1937 loss: 2.2705317e-06
Iter: 1938 loss: 2.26987345e-06
Iter: 1939 loss: 2.27849205e-06
Iter: 1940 loss: 2.26987117e-06
Iter: 1941 loss: 2.26923589e-06
Iter: 1942 loss: 2.26931411e-06
Iter: 1943 loss: 2.26884e-06
Iter: 1944 loss: 2.26813927e-06
Iter: 1945 loss: 2.273473e-06
Iter: 1946 loss: 2.26812062e-06
Iter: 1947 loss: 2.26763518e-06
Iter: 1948 loss: 2.26756174e-06
Iter: 1949 loss: 2.26722068e-06
Iter: 1950 loss: 2.26656e-06
Iter: 1951 loss: 2.26957172e-06
Iter: 1952 loss: 2.26648945e-06
Iter: 1953 loss: 2.26585371e-06
Iter: 1954 loss: 2.26691577e-06
Iter: 1955 loss: 2.26556426e-06
Iter: 1956 loss: 2.26495581e-06
Iter: 1957 loss: 2.26649536e-06
Iter: 1958 loss: 2.26480074e-06
Iter: 1959 loss: 2.26410293e-06
Iter: 1960 loss: 2.266514e-06
Iter: 1961 loss: 2.26391512e-06
Iter: 1962 loss: 2.26389739e-06
Iter: 1963 loss: 2.26362954e-06
Iter: 1964 loss: 2.26350608e-06
Iter: 1965 loss: 2.26311363e-06
Iter: 1966 loss: 2.26999555e-06
Iter: 1967 loss: 2.26313205e-06
Iter: 1968 loss: 2.26269458e-06
Iter: 1969 loss: 2.26184329e-06
Iter: 1970 loss: 2.27533701e-06
Iter: 1971 loss: 2.26178713e-06
Iter: 1972 loss: 2.26088468e-06
Iter: 1973 loss: 2.27185205e-06
Iter: 1974 loss: 2.26086809e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi3 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi3
+ date
Sun Nov  1 01:12:01 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 0 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi3/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac751400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac769730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac7d09d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac6d1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac6d1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac6d1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac5bc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac5bb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac5bb1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac3ad0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac3f18c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac3deae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac3d58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac6561e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac6717b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac3ada60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac521bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac2bdf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac2cbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac2bd2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac376510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac624268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac376f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac5fcea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac44e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac312ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac1cb510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac312268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac1af0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac1a50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac19a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac5710d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac5716a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac21aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac4341e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84ac3fd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.66987634
test_loss: 0.6878021
train_loss: 0.66706806
test_loss: 0.6874743
train_loss: 0.68731016
test_loss: 0.6871491
train_loss: 0.72080654
test_loss: 0.6866669
train_loss: 0.7171739
test_loss: 0.68613404
train_loss: 0.685295
test_loss: 0.6856159
train_loss: 0.6943958
test_loss: 0.6851276
train_loss: 0.68382967
test_loss: 0.6845889
train_loss: 0.69088006
test_loss: 0.68392694
train_loss: 0.7075521
test_loss: 0.68330127
train_loss: 0.6808018
test_loss: 0.682608
train_loss: 0.6942527
test_loss: 0.68189627
train_loss: 0.66843665
test_loss: 0.68119454
train_loss: 0.69937336
test_loss: 0.68049383
train_loss: 0.67705107
test_loss: 0.67971957
train_loss: 0.66594833
test_loss: 0.67903054
train_loss: 0.6739877
test_loss: 0.67821723
train_loss: 0.6781717
test_loss: 0.6774381
train_loss: 0.6626209
test_loss: 0.6765575
train_loss: 0.67056984
test_loss: 0.6757944
train_loss: 0.6903801
test_loss: 0.6749505
train_loss: 0.6745147
test_loss: 0.6740415
train_loss: 0.68233734
test_loss: 0.6731129
train_loss: 0.6790717
test_loss: 0.6722762
train_loss: 0.6825143
test_loss: 0.6713042
train_loss: 0.6666006
test_loss: 0.6703857
train_loss: 0.682962
test_loss: 0.66937333
train_loss: 0.6814388
test_loss: 0.66837037
train_loss: 0.67439747
test_loss: 0.66739976
train_loss: 0.6575948
test_loss: 0.6663094
train_loss: 0.66637486
test_loss: 0.6652679
train_loss: 0.693287
test_loss: 0.66414875
train_loss: 0.66661483
test_loss: 0.66301036
train_loss: 0.66670394
test_loss: 0.6618895
train_loss: 0.6321695
test_loss: 0.66071916
train_loss: 0.6682643
test_loss: 0.65950125
train_loss: 0.66970515
test_loss: 0.65843517
train_loss: 0.67904806
test_loss: 0.65706646
train_loss: 0.6615508
test_loss: 0.6558151
train_loss: 0.6440792
test_loss: 0.65446895
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi0_phi3/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi0_phi3/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72349f08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72349f46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7234a947b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f723498b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72349b9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72349f4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f720bc2a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f720bc8b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f720bc8b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f720bc6b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f720bba70d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f720bbaed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f720bc129d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71e41cb2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71e418f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f720bba36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71e4186a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71e4186b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71e40a2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71e40beb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71e401aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71e401a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71e401a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71e4034158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71d07cb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71d07e8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71d078dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71d078dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71d075a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71d07791e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71d0777620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71d06a22f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71d068f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71d072fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71d06442f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f71d066fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.887744546
Iter: 2 loss: 0.454628289
Iter: 3 loss: 0.977180243
Iter: 4 loss: 0.608956575
Iter: 5 loss: 1.09663033
Iter: 6 loss: 0.25108847
Iter: 7 loss: 0.17203927
Iter: 8 loss: 0.238662317
Iter: 9 loss: 0.147237062
Iter: 10 loss: 0.0957997367
Iter: 11 loss: 0.158992752
Iter: 12 loss: 0.0612756722
Iter: 13 loss: 0.0358033851
Iter: 14 loss: 0.548262954
Iter: 15 loss: 0.035179235
Iter: 16 loss: 0.0236876197
Iter: 17 loss: 0.0234052874
Iter: 18 loss: 0.0216159821
Iter: 19 loss: 0.0196374841
Iter: 20 loss: 0.0182025693
Iter: 21 loss: 0.0196141209
Iter: 22 loss: 0.0174628217
Iter: 23 loss: 0.0138766477
Iter: 24 loss: 0.0245412271
Iter: 25 loss: 0.0126029039
Iter: 26 loss: 0.0100142676
Iter: 27 loss: 0.0484618917
Iter: 28 loss: 0.0100141279
Iter: 29 loss: 0.00894442573
Iter: 30 loss: 0.0120418444
Iter: 31 loss: 0.0086268913
Iter: 32 loss: 0.00822454877
Iter: 33 loss: 0.0088396538
Iter: 34 loss: 0.00803063437
Iter: 35 loss: 0.00773952249
Iter: 36 loss: 0.00800443441
Iter: 37 loss: 0.00757746305
Iter: 38 loss: 0.00727068959
Iter: 39 loss: 0.00935151149
Iter: 40 loss: 0.0072431704
Iter: 41 loss: 0.00703784637
Iter: 42 loss: 0.0071619954
Iter: 43 loss: 0.00690727308
Iter: 44 loss: 0.00667469529
Iter: 45 loss: 0.00718660746
Iter: 46 loss: 0.00659056474
Iter: 47 loss: 0.00639085192
Iter: 48 loss: 0.00665360037
Iter: 49 loss: 0.00629176293
Iter: 50 loss: 0.00606263429
Iter: 51 loss: 0.00767880538
Iter: 52 loss: 0.00604107
Iter: 53 loss: 0.00582979061
Iter: 54 loss: 0.00708557386
Iter: 55 loss: 0.00580859883
Iter: 56 loss: 0.00570490398
Iter: 57 loss: 0.00563718425
Iter: 58 loss: 0.00559750851
Iter: 59 loss: 0.00542915612
Iter: 60 loss: 0.0053042
Iter: 61 loss: 0.00524776289
Iter: 62 loss: 0.00509109767
Iter: 63 loss: 0.00508613326
Iter: 64 loss: 0.00495044468
Iter: 65 loss: 0.00500769168
Iter: 66 loss: 0.00485952804
Iter: 67 loss: 0.00472361688
Iter: 68 loss: 0.005891209
Iter: 69 loss: 0.00471589109
Iter: 70 loss: 0.00458326843
Iter: 71 loss: 0.00594730722
Iter: 72 loss: 0.00457809865
Iter: 73 loss: 0.00446362374
Iter: 74 loss: 0.00477148499
Iter: 75 loss: 0.00442771427
Iter: 76 loss: 0.00429968536
Iter: 77 loss: 0.00763693871
Iter: 78 loss: 0.00429966
Iter: 79 loss: 0.00419876818
Iter: 80 loss: 0.00419708062
Iter: 81 loss: 0.00411554379
Iter: 82 loss: 0.00413459213
Iter: 83 loss: 0.00405469025
Iter: 84 loss: 0.0039732852
Iter: 85 loss: 0.00491550518
Iter: 86 loss: 0.00397189334
Iter: 87 loss: 0.00389984623
Iter: 88 loss: 0.00413151
Iter: 89 loss: 0.00388081861
Iter: 90 loss: 0.00370754069
Iter: 91 loss: 0.00566399284
Iter: 92 loss: 0.00368810399
Iter: 93 loss: 0.00353872916
Iter: 94 loss: 0.00442510657
Iter: 95 loss: 0.00351916486
Iter: 96 loss: 0.00344041921
Iter: 97 loss: 0.00342715089
Iter: 98 loss: 0.00331329135
Iter: 99 loss: 0.0036539163
Iter: 100 loss: 0.00327859214
Iter: 101 loss: 0.00316622877
Iter: 102 loss: 0.00408627931
Iter: 103 loss: 0.00315065682
Iter: 104 loss: 0.00306288642
Iter: 105 loss: 0.0030628657
Iter: 106 loss: 0.00300110783
Iter: 107 loss: 0.00300035602
Iter: 108 loss: 0.00292607956
Iter: 109 loss: 0.00325265527
Iter: 110 loss: 0.00291137118
Iter: 111 loss: 0.00284515647
Iter: 112 loss: 0.00290288497
Iter: 113 loss: 0.00280515687
Iter: 114 loss: 0.00275109848
Iter: 115 loss: 0.00329340901
Iter: 116 loss: 0.00274948124
Iter: 117 loss: 0.00269650109
Iter: 118 loss: 0.00292291027
Iter: 119 loss: 0.00268525025
Iter: 120 loss: 0.0026449766
Iter: 121 loss: 0.00271166768
Iter: 122 loss: 0.00262693525
Iter: 123 loss: 0.00258163363
Iter: 124 loss: 0.00278594252
Iter: 125 loss: 0.00256994576
Iter: 126 loss: 0.00251158467
Iter: 127 loss: 0.00267293979
Iter: 128 loss: 0.0024937517
Iter: 129 loss: 0.0024273661
Iter: 130 loss: 0.00307370164
Iter: 131 loss: 0.00242371182
Iter: 132 loss: 0.0023699603
Iter: 133 loss: 0.00290304981
Iter: 134 loss: 0.00236780336
Iter: 135 loss: 0.00232748361
Iter: 136 loss: 0.0025055008
Iter: 137 loss: 0.00231969263
Iter: 138 loss: 0.00227590092
Iter: 139 loss: 0.00244975509
Iter: 140 loss: 0.00226700678
Iter: 141 loss: 0.00223392574
Iter: 142 loss: 0.00272038579
Iter: 143 loss: 0.00223312946
Iter: 144 loss: 0.0021973718
Iter: 145 loss: 0.0022963807
Iter: 146 loss: 0.00218652398
Iter: 147 loss: 0.0021338691
Iter: 148 loss: 0.00246668071
Iter: 149 loss: 0.00212720619
Iter: 150 loss: 0.0020982977
Iter: 151 loss: 0.00209765276
Iter: 152 loss: 0.00207159482
Iter: 153 loss: 0.00215679035
Iter: 154 loss: 0.00206443318
Iter: 155 loss: 0.00204007281
Iter: 156 loss: 0.00207181042
Iter: 157 loss: 0.00202655187
Iter: 158 loss: 0.00200181478
Iter: 159 loss: 0.00211945688
Iter: 160 loss: 0.00199756771
Iter: 161 loss: 0.00196678471
Iter: 162 loss: 0.0021712454
Iter: 163 loss: 0.00196252298
Iter: 164 loss: 0.00193093461
Iter: 165 loss: 0.00223364308
Iter: 166 loss: 0.00192988582
Iter: 167 loss: 0.00189889595
Iter: 168 loss: 0.00206323387
Iter: 169 loss: 0.0018943355
Iter: 170 loss: 0.00187183102
Iter: 171 loss: 0.00206869142
Iter: 172 loss: 0.00187073159
Iter: 173 loss: 0.00184993376
Iter: 174 loss: 0.00184516551
Iter: 175 loss: 0.00183160801
Iter: 176 loss: 0.00181137118
Iter: 177 loss: 0.00188818271
Iter: 178 loss: 0.00180644984
Iter: 179 loss: 0.00178462721
Iter: 180 loss: 0.00187297259
Iter: 181 loss: 0.00177992985
Iter: 182 loss: 0.00176080945
Iter: 183 loss: 0.00176458992
Iter: 184 loss: 0.00174655393
Iter: 185 loss: 0.0017286134
Iter: 186 loss: 0.00172793726
Iter: 187 loss: 0.0017124794
Iter: 188 loss: 0.00174047123
Iter: 189 loss: 0.00170551764
Iter: 190 loss: 0.00169296842
Iter: 191 loss: 0.00169239333
Iter: 192 loss: 0.00168200885
Iter: 193 loss: 0.00168700947
Iter: 194 loss: 0.0016749613
Iter: 195 loss: 0.00166045269
Iter: 196 loss: 0.00165840122
Iter: 197 loss: 0.00164827227
Iter: 198 loss: 0.00162522716
Iter: 199 loss: 0.00175381743
Iter: 200 loss: 0.00162179349
Iter: 201 loss: 0.00160210987
Iter: 202 loss: 0.00168211409
Iter: 203 loss: 0.00159771042
Iter: 204 loss: 0.00157921598
Iter: 205 loss: 0.00170681928
Iter: 206 loss: 0.00157741748
Iter: 207 loss: 0.00156042317
Iter: 208 loss: 0.00155648135
Iter: 209 loss: 0.00154552504
Iter: 210 loss: 0.00152406469
Iter: 211 loss: 0.00165327347
Iter: 212 loss: 0.00152157084
Iter: 213 loss: 0.00150499167
Iter: 214 loss: 0.00159768143
Iter: 215 loss: 0.00150228897
Iter: 216 loss: 0.00148776744
Iter: 217 loss: 0.00150001934
Iter: 218 loss: 0.0014793881
Iter: 219 loss: 0.00146075955
Iter: 220 loss: 0.00163118285
Iter: 221 loss: 0.00145975535
Iter: 222 loss: 0.00144245639
Iter: 223 loss: 0.00153398782
Iter: 224 loss: 0.00143977103
Iter: 225 loss: 0.00142781762
Iter: 226 loss: 0.00140917127
Iter: 227 loss: 0.00140895392
Iter: 228 loss: 0.00138684583
Iter: 229 loss: 0.00154290732
Iter: 230 loss: 0.0013846457
Iter: 231 loss: 0.00136096403
Iter: 232 loss: 0.00140861981
Iter: 233 loss: 0.00135069829
Iter: 234 loss: 0.00132596132
Iter: 235 loss: 0.00143856765
Iter: 236 loss: 0.00132150971
Iter: 237 loss: 0.00129786984
Iter: 238 loss: 0.00138221402
Iter: 239 loss: 0.00129163871
Iter: 240 loss: 0.00126722688
Iter: 241 loss: 0.00146119203
Iter: 242 loss: 0.00126528146
Iter: 243 loss: 0.00123900105
Iter: 244 loss: 0.00129119016
Iter: 245 loss: 0.0012285097
Iter: 246 loss: 0.00119611085
Iter: 247 loss: 0.00168536068
Iter: 248 loss: 0.00119596987
Iter: 249 loss: 0.0011694578
Iter: 250 loss: 0.00138529507
Iter: 251 loss: 0.00116769876
Iter: 252 loss: 0.00115049165
Iter: 253 loss: 0.00118755037
Iter: 254 loss: 0.00114362803
Iter: 255 loss: 0.00112398714
Iter: 256 loss: 0.0011235422
Iter: 257 loss: 0.00110708084
Iter: 258 loss: 0.00114364782
Iter: 259 loss: 0.00110033818
Iter: 260 loss: 0.00108109124
Iter: 261 loss: 0.00113832648
Iter: 262 loss: 0.00107533764
Iter: 263 loss: 0.00105312816
Iter: 264 loss: 0.00127196778
Iter: 265 loss: 0.00105205132
Iter: 266 loss: 0.00103558181
Iter: 267 loss: 0.00121534488
Iter: 268 loss: 0.00103532011
Iter: 269 loss: 0.00102278707
Iter: 270 loss: 0.00101668993
Iter: 271 loss: 0.00101060362
Iter: 272 loss: 0.000996051589
Iter: 273 loss: 0.000996370567
Iter: 274 loss: 0.000984542887
Iter: 275 loss: 0.000970295456
Iter: 276 loss: 0.000970216061
Iter: 277 loss: 0.000958139892
Iter: 278 loss: 0.000991450623
Iter: 279 loss: 0.000953884562
Iter: 280 loss: 0.000940896105
Iter: 281 loss: 0.000977512915
Iter: 282 loss: 0.000936818309
Iter: 283 loss: 0.000924546737
Iter: 284 loss: 0.001003435
Iter: 285 loss: 0.00092301087
Iter: 286 loss: 0.000913991
Iter: 287 loss: 0.000925690809
Iter: 288 loss: 0.000909425085
Iter: 289 loss: 0.000897191465
Iter: 290 loss: 0.000977861113
Iter: 291 loss: 0.000895894598
Iter: 292 loss: 0.000888501178
Iter: 293 loss: 0.000895955949
Iter: 294 loss: 0.00088436529
Iter: 295 loss: 0.000872418284
Iter: 296 loss: 0.00088365993
Iter: 297 loss: 0.000865391281
Iter: 298 loss: 0.000848114083
Iter: 299 loss: 0.000986328349
Iter: 300 loss: 0.000847085379
Iter: 301 loss: 0.000837652944
Iter: 302 loss: 0.000834506354
Iter: 303 loss: 0.000825570663
Iter: 304 loss: 0.0008402518
Iter: 305 loss: 0.000821259688
Iter: 306 loss: 0.000808604877
Iter: 307 loss: 0.000810617
Iter: 308 loss: 0.000799081055
Iter: 309 loss: 0.00078310672
Iter: 310 loss: 0.000863799884
Iter: 311 loss: 0.000780273462
Iter: 312 loss: 0.000769339502
Iter: 313 loss: 0.00090899819
Iter: 314 loss: 0.000769245322
Iter: 315 loss: 0.000759917079
Iter: 316 loss: 0.0007495
Iter: 317 loss: 0.000748103834
Iter: 318 loss: 0.00073216774
Iter: 319 loss: 0.000876414939
Iter: 320 loss: 0.000731268548
Iter: 321 loss: 0.000718780328
Iter: 322 loss: 0.000764957629
Iter: 323 loss: 0.000715569884
Iter: 324 loss: 0.000700854522
Iter: 325 loss: 0.000736061658
Iter: 326 loss: 0.000695590046
Iter: 327 loss: 0.000673575269
Iter: 328 loss: 0.000762663083
Iter: 329 loss: 0.000667673536
Iter: 330 loss: 0.000651226554
Iter: 331 loss: 0.000778501504
Iter: 332 loss: 0.000649939233
Iter: 333 loss: 0.000638165278
Iter: 334 loss: 0.000638123893
Iter: 335 loss: 0.000630019698
Iter: 336 loss: 0.000628252339
Iter: 337 loss: 0.000622687628
Iter: 338 loss: 0.000610411575
Iter: 339 loss: 0.000610221119
Iter: 340 loss: 0.000599254854
Iter: 341 loss: 0.000646847184
Iter: 342 loss: 0.000596981728
Iter: 343 loss: 0.000586327
Iter: 344 loss: 0.000620216713
Iter: 345 loss: 0.00058272
Iter: 346 loss: 0.000564636488
Iter: 347 loss: 0.000732815359
Iter: 348 loss: 0.000563883
Iter: 349 loss: 0.000550132
Iter: 350 loss: 0.000597013801
Iter: 351 loss: 0.000546496944
Iter: 352 loss: 0.000530252815
Iter: 353 loss: 0.000653971161
Iter: 354 loss: 0.000527523458
Iter: 355 loss: 0.000517107779
Iter: 356 loss: 0.000515086344
Iter: 357 loss: 0.000504817173
Iter: 358 loss: 0.000608754461
Iter: 359 loss: 0.000504550524
Iter: 360 loss: 0.000496272754
Iter: 361 loss: 0.00050632318
Iter: 362 loss: 0.000491215149
Iter: 363 loss: 0.000476298272
Iter: 364 loss: 0.000624151377
Iter: 365 loss: 0.000475855777
Iter: 366 loss: 0.000461848947
Iter: 367 loss: 0.000530250953
Iter: 368 loss: 0.000459327945
Iter: 369 loss: 0.000450113235
Iter: 370 loss: 0.00049871509
Iter: 371 loss: 0.000448441424
Iter: 372 loss: 0.000439513242
Iter: 373 loss: 0.000496706693
Iter: 374 loss: 0.000438633142
Iter: 375 loss: 0.000429683423
Iter: 376 loss: 0.00056839711
Iter: 377 loss: 0.000429683161
Iter: 378 loss: 0.000424194848
Iter: 379 loss: 0.000473822525
Iter: 380 loss: 0.000423707854
Iter: 381 loss: 0.000417519041
Iter: 382 loss: 0.000419788761
Iter: 383 loss: 0.000413211412
Iter: 384 loss: 0.000401252473
Iter: 385 loss: 0.000476652494
Iter: 386 loss: 0.000399672601
Iter: 387 loss: 0.000391365786
Iter: 388 loss: 0.000438382733
Iter: 389 loss: 0.000390218396
Iter: 390 loss: 0.00038223766
Iter: 391 loss: 0.000386738131
Iter: 392 loss: 0.000377057382
Iter: 393 loss: 0.000369422778
Iter: 394 loss: 0.000381235848
Iter: 395 loss: 0.000365271844
Iter: 396 loss: 0.000357067736
Iter: 397 loss: 0.000388854503
Iter: 398 loss: 0.000355185941
Iter: 399 loss: 0.000345054199
Iter: 400 loss: 0.000381520891
Iter: 401 loss: 0.00034192903
Iter: 402 loss: 0.000336671656
Iter: 403 loss: 0.000335735182
Iter: 404 loss: 0.000329326023
Iter: 405 loss: 0.000395328301
Iter: 406 loss: 0.000329211325
Iter: 407 loss: 0.000322275
Iter: 408 loss: 0.000357724406
Iter: 409 loss: 0.000320773397
Iter: 410 loss: 0.000314618286
Iter: 411 loss: 0.000418179028
Iter: 412 loss: 0.000314618461
Iter: 413 loss: 0.000310350617
Iter: 414 loss: 0.000323848915
Iter: 415 loss: 0.000309148134
Iter: 416 loss: 0.000305393944
Iter: 417 loss: 0.000310204719
Iter: 418 loss: 0.0003033104
Iter: 419 loss: 0.000296914368
Iter: 420 loss: 0.000341362203
Iter: 421 loss: 0.000296295562
Iter: 422 loss: 0.000290441851
Iter: 423 loss: 0.000358034624
Iter: 424 loss: 0.000290367636
Iter: 425 loss: 0.000286844588
Iter: 426 loss: 0.00030157555
Iter: 427 loss: 0.00028598879
Iter: 428 loss: 0.000281369343
Iter: 429 loss: 0.00030427688
Iter: 430 loss: 0.000280583743
Iter: 431 loss: 0.000275768456
Iter: 432 loss: 0.000313673809
Iter: 433 loss: 0.000275427679
Iter: 434 loss: 0.000272037985
Iter: 435 loss: 0.000269580458
Iter: 436 loss: 0.000268404372
Iter: 437 loss: 0.000263148802
Iter: 438 loss: 0.000286614
Iter: 439 loss: 0.00026213337
Iter: 440 loss: 0.00025721945
Iter: 441 loss: 0.000337914214
Iter: 442 loss: 0.000257216685
Iter: 443 loss: 0.000254669751
Iter: 444 loss: 0.000257492939
Iter: 445 loss: 0.000253222359
Iter: 446 loss: 0.000250750396
Iter: 447 loss: 0.000250627316
Iter: 448 loss: 0.000248440425
Iter: 449 loss: 0.000253429171
Iter: 450 loss: 0.000247613061
Iter: 451 loss: 0.000245461386
Iter: 452 loss: 0.000243763963
Iter: 453 loss: 0.000243073169
Iter: 454 loss: 0.000239394634
Iter: 455 loss: 0.000271675934
Iter: 456 loss: 0.000239208588
Iter: 457 loss: 0.000235363143
Iter: 458 loss: 0.000244354538
Iter: 459 loss: 0.000233932238
Iter: 460 loss: 0.000231692306
Iter: 461 loss: 0.000232213468
Iter: 462 loss: 0.000230012942
Iter: 463 loss: 0.000227169818
Iter: 464 loss: 0.00024141834
Iter: 465 loss: 0.000226703269
Iter: 466 loss: 0.000224135205
Iter: 467 loss: 0.00025332917
Iter: 468 loss: 0.000224089599
Iter: 469 loss: 0.000222406379
Iter: 470 loss: 0.000224619973
Iter: 471 loss: 0.000221496593
Iter: 472 loss: 0.000219134294
Iter: 473 loss: 0.000224774674
Iter: 474 loss: 0.000218298112
Iter: 475 loss: 0.000215631982
Iter: 476 loss: 0.000217000153
Iter: 477 loss: 0.000213820444
Iter: 478 loss: 0.000211083272
Iter: 479 loss: 0.000218352739
Iter: 480 loss: 0.00021016071
Iter: 481 loss: 0.000208472047
Iter: 482 loss: 0.000208266283
Iter: 483 loss: 0.00020701802
Iter: 484 loss: 0.000210921367
Iter: 485 loss: 0.000206654193
Iter: 486 loss: 0.000205306293
Iter: 487 loss: 0.000203466625
Iter: 488 loss: 0.000203376607
Iter: 489 loss: 0.000201224757
Iter: 490 loss: 0.000211367529
Iter: 491 loss: 0.000200838433
Iter: 492 loss: 0.000198267822
Iter: 493 loss: 0.000209545571
Iter: 494 loss: 0.000197760863
Iter: 495 loss: 0.000196132198
Iter: 496 loss: 0.000194388296
Iter: 497 loss: 0.000194095643
Iter: 498 loss: 0.000191799103
Iter: 499 loss: 0.000209874706
Iter: 500 loss: 0.000191655432
Iter: 501 loss: 0.000189612547
Iter: 502 loss: 0.000217121022
Iter: 503 loss: 0.000189604732
Iter: 504 loss: 0.000188400969
Iter: 505 loss: 0.000186624
Iter: 506 loss: 0.000186575271
Iter: 507 loss: 0.000183442113
Iter: 508 loss: 0.000199495422
Iter: 509 loss: 0.000182932243
Iter: 510 loss: 0.000181272058
Iter: 511 loss: 0.000202964016
Iter: 512 loss: 0.000181262381
Iter: 513 loss: 0.000179905328
Iter: 514 loss: 0.000180237796
Iter: 515 loss: 0.000178904258
Iter: 516 loss: 0.000176841422
Iter: 517 loss: 0.000192374559
Iter: 518 loss: 0.000176685513
Iter: 519 loss: 0.000175283683
Iter: 520 loss: 0.000179626717
Iter: 521 loss: 0.000174875036
Iter: 522 loss: 0.000173278793
Iter: 523 loss: 0.000171960943
Iter: 524 loss: 0.000171497581
Iter: 525 loss: 0.000169273175
Iter: 526 loss: 0.000171618885
Iter: 527 loss: 0.000168043247
Iter: 528 loss: 0.000166296464
Iter: 529 loss: 0.000166255166
Iter: 530 loss: 0.000164643541
Iter: 531 loss: 0.000164992263
Iter: 532 loss: 0.000163447243
Iter: 533 loss: 0.000161557735
Iter: 534 loss: 0.000168221886
Iter: 535 loss: 0.000161050441
Iter: 536 loss: 0.000159034913
Iter: 537 loss: 0.000166766607
Iter: 538 loss: 0.00015856314
Iter: 539 loss: 0.000156833761
Iter: 540 loss: 0.000169168256
Iter: 541 loss: 0.000156681665
Iter: 542 loss: 0.000155341186
Iter: 543 loss: 0.000157732371
Iter: 544 loss: 0.000154735229
Iter: 545 loss: 0.000153038593
Iter: 546 loss: 0.000154263456
Iter: 547 loss: 0.000151994012
Iter: 548 loss: 0.000152840832
Iter: 549 loss: 0.000151253131
Iter: 550 loss: 0.000150630527
Iter: 551 loss: 0.000150216685
Iter: 552 loss: 0.000149981875
Iter: 553 loss: 0.000148516876
Iter: 554 loss: 0.000149341708
Iter: 555 loss: 0.000147565326
Iter: 556 loss: 0.00014585079
Iter: 557 loss: 0.000153051762
Iter: 558 loss: 0.000145490107
Iter: 559 loss: 0.000144274498
Iter: 560 loss: 0.000148488034
Iter: 561 loss: 0.000143953337
Iter: 562 loss: 0.000142860808
Iter: 563 loss: 0.000146772916
Iter: 564 loss: 0.000142583449
Iter: 565 loss: 0.000141508979
Iter: 566 loss: 0.000140491262
Iter: 567 loss: 0.000140244621
Iter: 568 loss: 0.00013845299
Iter: 569 loss: 0.00015400222
Iter: 570 loss: 0.000138354211
Iter: 571 loss: 0.000137221432
Iter: 572 loss: 0.00014371323
Iter: 573 loss: 0.000137070805
Iter: 574 loss: 0.000135953247
Iter: 575 loss: 0.000137775205
Iter: 576 loss: 0.000135443843
Iter: 577 loss: 0.000134051166
Iter: 578 loss: 0.000137049356
Iter: 579 loss: 0.000133500696
Iter: 580 loss: 0.000132620815
Iter: 581 loss: 0.000132601344
Iter: 582 loss: 0.000131747423
Iter: 583 loss: 0.000131580164
Iter: 584 loss: 0.000131011417
Iter: 585 loss: 0.000130073284
Iter: 586 loss: 0.000131113484
Iter: 587 loss: 0.000129565844
Iter: 588 loss: 0.000128340325
Iter: 589 loss: 0.000130770422
Iter: 590 loss: 0.000127838168
Iter: 591 loss: 0.000126918429
Iter: 592 loss: 0.000132894929
Iter: 593 loss: 0.000126819912
Iter: 594 loss: 0.000125911465
Iter: 595 loss: 0.00012644846
Iter: 596 loss: 0.000125325038
Iter: 597 loss: 0.0001240212
Iter: 598 loss: 0.000126140367
Iter: 599 loss: 0.000123425532
Iter: 600 loss: 0.000122334721
Iter: 601 loss: 0.000126974395
Iter: 602 loss: 0.000122107944
Iter: 603 loss: 0.000121184501
Iter: 604 loss: 0.000126425351
Iter: 605 loss: 0.000121050645
Iter: 606 loss: 0.000120343539
Iter: 607 loss: 0.000122443555
Iter: 608 loss: 0.000120127996
Iter: 609 loss: 0.000119272292
Iter: 610 loss: 0.000120653138
Iter: 611 loss: 0.000118879645
Iter: 612 loss: 0.000118147276
Iter: 613 loss: 0.000126035229
Iter: 614 loss: 0.000118128526
Iter: 615 loss: 0.000117520518
Iter: 616 loss: 0.000119949735
Iter: 617 loss: 0.000117383723
Iter: 618 loss: 0.000116825235
Iter: 619 loss: 0.000115674877
Iter: 620 loss: 0.00013621588
Iter: 621 loss: 0.00011565219
Iter: 622 loss: 0.000114712282
Iter: 623 loss: 0.000128376239
Iter: 624 loss: 0.000114710885
Iter: 625 loss: 0.000113885719
Iter: 626 loss: 0.000114018396
Iter: 627 loss: 0.000113263093
Iter: 628 loss: 0.00011231628
Iter: 629 loss: 0.00012238315
Iter: 630 loss: 0.000112292531
Iter: 631 loss: 0.000111741523
Iter: 632 loss: 0.000111461435
Iter: 633 loss: 0.000111202666
Iter: 634 loss: 0.000110389883
Iter: 635 loss: 0.00011329753
Iter: 636 loss: 0.000110182445
Iter: 637 loss: 0.000109470013
Iter: 638 loss: 0.000117390533
Iter: 639 loss: 0.000109456327
Iter: 640 loss: 0.000108808366
Iter: 641 loss: 0.000109071705
Iter: 642 loss: 0.000108358152
Iter: 643 loss: 0.000107669606
Iter: 644 loss: 0.000111955786
Iter: 645 loss: 0.000107588603
Iter: 646 loss: 0.000106987558
Iter: 647 loss: 0.000108228836
Iter: 648 loss: 0.000106748979
Iter: 649 loss: 0.000106069892
Iter: 650 loss: 0.000110477849
Iter: 651 loss: 0.000105994826
Iter: 652 loss: 0.000105445724
Iter: 653 loss: 0.000105218904
Iter: 654 loss: 0.000104929961
Iter: 655 loss: 0.000104200764
Iter: 656 loss: 0.000103880608
Iter: 657 loss: 0.000103508108
Iter: 658 loss: 0.000102738748
Iter: 659 loss: 0.000102706661
Iter: 660 loss: 0.000102104328
Iter: 661 loss: 0.000103311977
Iter: 662 loss: 0.000101857062
Iter: 663 loss: 0.000101072656
Iter: 664 loss: 0.000100868732
Iter: 665 loss: 0.000100379169
Iter: 666 loss: 9.94570873e-05
Iter: 667 loss: 0.000100100726
Iter: 668 loss: 9.88764223e-05
Iter: 669 loss: 9.81494086e-05
Iter: 670 loss: 0.000108958324
Iter: 671 loss: 9.81487901e-05
Iter: 672 loss: 9.75619769e-05
Iter: 673 loss: 9.98597097e-05
Iter: 674 loss: 9.74279465e-05
Iter: 675 loss: 9.68419699e-05
Iter: 676 loss: 9.72612179e-05
Iter: 677 loss: 9.64732462e-05
Iter: 678 loss: 9.57012526e-05
Iter: 679 loss: 9.940975e-05
Iter: 680 loss: 9.5562209e-05
Iter: 681 loss: 9.50993126e-05
Iter: 682 loss: 0.000101769161
Iter: 683 loss: 9.50986141e-05
Iter: 684 loss: 9.46181244e-05
Iter: 685 loss: 9.46706e-05
Iter: 686 loss: 9.42474653e-05
Iter: 687 loss: 9.36333963e-05
Iter: 688 loss: 9.40275131e-05
Iter: 689 loss: 9.32427211e-05
Iter: 690 loss: 9.25936547e-05
Iter: 691 loss: 9.25719069e-05
Iter: 692 loss: 9.20681268e-05
Iter: 693 loss: 9.12279866e-05
Iter: 694 loss: 9.35589487e-05
Iter: 695 loss: 9.09540177e-05
Iter: 696 loss: 9.05348e-05
Iter: 697 loss: 9.04105618e-05
Iter: 698 loss: 8.99678853e-05
Iter: 699 loss: 9.00673331e-05
Iter: 700 loss: 8.96422862e-05
Iter: 701 loss: 8.90874653e-05
Iter: 702 loss: 8.96952188e-05
Iter: 703 loss: 8.87848e-05
Iter: 704 loss: 8.82509776e-05
Iter: 705 loss: 8.77523926e-05
Iter: 706 loss: 8.76261038e-05
Iter: 707 loss: 8.71795346e-05
Iter: 708 loss: 8.70268486e-05
Iter: 709 loss: 8.67254712e-05
Iter: 710 loss: 8.61470835e-05
Iter: 711 loss: 9.85201259e-05
Iter: 712 loss: 8.61440494e-05
Iter: 713 loss: 8.55774706e-05
Iter: 714 loss: 8.97853315e-05
Iter: 715 loss: 8.55312101e-05
Iter: 716 loss: 8.50085489e-05
Iter: 717 loss: 8.73481549e-05
Iter: 718 loss: 8.49064745e-05
Iter: 719 loss: 8.44034366e-05
Iter: 720 loss: 8.80529624e-05
Iter: 721 loss: 8.43598827e-05
Iter: 722 loss: 8.40936e-05
Iter: 723 loss: 8.35558822e-05
Iter: 724 loss: 9.39101e-05
Iter: 725 loss: 8.35485e-05
Iter: 726 loss: 8.27767508e-05
Iter: 727 loss: 8.33008962e-05
Iter: 728 loss: 8.2292725e-05
Iter: 729 loss: 8.13302904e-05
Iter: 730 loss: 9.0346206e-05
Iter: 731 loss: 8.12875951e-05
Iter: 732 loss: 8.06868484e-05
Iter: 733 loss: 8.06859389e-05
Iter: 734 loss: 8.01456e-05
Iter: 735 loss: 8.05779564e-05
Iter: 736 loss: 7.98209658e-05
Iter: 737 loss: 7.93860818e-05
Iter: 738 loss: 7.90649574e-05
Iter: 739 loss: 7.8918456e-05
Iter: 740 loss: 7.83622454e-05
Iter: 741 loss: 8.6525506e-05
Iter: 742 loss: 7.83613068e-05
Iter: 743 loss: 7.79349066e-05
Iter: 744 loss: 7.90084086e-05
Iter: 745 loss: 7.77869573e-05
Iter: 746 loss: 7.73080101e-05
Iter: 747 loss: 7.96914e-05
Iter: 748 loss: 7.7225428e-05
Iter: 749 loss: 7.68836e-05
Iter: 750 loss: 7.7663688e-05
Iter: 751 loss: 7.67535676e-05
Iter: 752 loss: 7.63042553e-05
Iter: 753 loss: 7.76332745e-05
Iter: 754 loss: 7.61676492e-05
Iter: 755 loss: 7.57289963e-05
Iter: 756 loss: 7.74597793e-05
Iter: 757 loss: 7.56286245e-05
Iter: 758 loss: 7.53184431e-05
Iter: 759 loss: 7.4789561e-05
Iter: 760 loss: 7.47891754e-05
Iter: 761 loss: 7.41920085e-05
Iter: 762 loss: 7.74751679e-05
Iter: 763 loss: 7.4105621e-05
Iter: 764 loss: 7.36862494e-05
Iter: 765 loss: 7.36863367e-05
Iter: 766 loss: 7.32707485e-05
Iter: 767 loss: 7.42244156e-05
Iter: 768 loss: 7.31139589e-05
Iter: 769 loss: 7.27410879e-05
Iter: 770 loss: 7.22939803e-05
Iter: 771 loss: 7.22486e-05
Iter: 772 loss: 7.17820803e-05
Iter: 773 loss: 7.52698761e-05
Iter: 774 loss: 7.17443399e-05
Iter: 775 loss: 7.13010813e-05
Iter: 776 loss: 7.29382373e-05
Iter: 777 loss: 7.11896937e-05
Iter: 778 loss: 7.08546722e-05
Iter: 779 loss: 7.4405616e-05
Iter: 780 loss: 7.08463704e-05
Iter: 781 loss: 7.0576265e-05
Iter: 782 loss: 7.06293213e-05
Iter: 783 loss: 7.03749247e-05
Iter: 784 loss: 6.99666e-05
Iter: 785 loss: 7.18759911e-05
Iter: 786 loss: 6.98896183e-05
Iter: 787 loss: 6.95523e-05
Iter: 788 loss: 7.05806597e-05
Iter: 789 loss: 6.94519113e-05
Iter: 790 loss: 6.90787492e-05
Iter: 791 loss: 6.86070562e-05
Iter: 792 loss: 6.85700797e-05
Iter: 793 loss: 6.79404e-05
Iter: 794 loss: 6.88382715e-05
Iter: 795 loss: 6.76323543e-05
Iter: 796 loss: 6.70969312e-05
Iter: 797 loss: 7.10501408e-05
Iter: 798 loss: 6.70525e-05
Iter: 799 loss: 6.68239372e-05
Iter: 800 loss: 6.67749409e-05
Iter: 801 loss: 6.66023116e-05
Iter: 802 loss: 6.62729144e-05
Iter: 803 loss: 7.3436735e-05
Iter: 804 loss: 6.62714592e-05
Iter: 805 loss: 6.58598e-05
Iter: 806 loss: 6.63716346e-05
Iter: 807 loss: 6.56466582e-05
Iter: 808 loss: 6.52425806e-05
Iter: 809 loss: 7.16485374e-05
Iter: 810 loss: 6.52425369e-05
Iter: 811 loss: 6.48859132e-05
Iter: 812 loss: 6.60703954e-05
Iter: 813 loss: 6.47864872e-05
Iter: 814 loss: 6.44077445e-05
Iter: 815 loss: 6.52375747e-05
Iter: 816 loss: 6.42627128e-05
Iter: 817 loss: 6.40064609e-05
Iter: 818 loss: 6.7047542e-05
Iter: 819 loss: 6.40027138e-05
Iter: 820 loss: 6.37998164e-05
Iter: 821 loss: 6.37175326e-05
Iter: 822 loss: 6.36096811e-05
Iter: 823 loss: 6.32330921e-05
Iter: 824 loss: 6.36278273e-05
Iter: 825 loss: 6.3025378e-05
Iter: 826 loss: 6.26371911e-05
Iter: 827 loss: 6.21699728e-05
Iter: 828 loss: 6.21236832e-05
Iter: 829 loss: 6.15297686e-05
Iter: 830 loss: 6.38589263e-05
Iter: 831 loss: 6.13938391e-05
Iter: 832 loss: 6.10662464e-05
Iter: 833 loss: 6.10551142e-05
Iter: 834 loss: 6.07865877e-05
Iter: 835 loss: 6.24148233e-05
Iter: 836 loss: 6.0754217e-05
Iter: 837 loss: 6.05990717e-05
Iter: 838 loss: 6.02412074e-05
Iter: 839 loss: 6.47663474e-05
Iter: 840 loss: 6.02158834e-05
Iter: 841 loss: 5.98035331e-05
Iter: 842 loss: 6.03508925e-05
Iter: 843 loss: 5.9596e-05
Iter: 844 loss: 5.93115255e-05
Iter: 845 loss: 6.27106783e-05
Iter: 846 loss: 5.93078366e-05
Iter: 847 loss: 5.90596319e-05
Iter: 848 loss: 6.07157926e-05
Iter: 849 loss: 5.90342606e-05
Iter: 850 loss: 5.88525509e-05
Iter: 851 loss: 5.92218239e-05
Iter: 852 loss: 5.87792747e-05
Iter: 853 loss: 5.85705748e-05
Iter: 854 loss: 5.88872426e-05
Iter: 855 loss: 5.84711888e-05
Iter: 856 loss: 5.82635e-05
Iter: 857 loss: 5.96193349e-05
Iter: 858 loss: 5.8241847e-05
Iter: 859 loss: 5.80625929e-05
Iter: 860 loss: 5.84265799e-05
Iter: 861 loss: 5.79899279e-05
Iter: 862 loss: 5.77613173e-05
Iter: 863 loss: 5.80776796e-05
Iter: 864 loss: 5.76475e-05
Iter: 865 loss: 5.74170226e-05
Iter: 866 loss: 5.7016834e-05
Iter: 867 loss: 5.70168377e-05
Iter: 868 loss: 5.71244163e-05
Iter: 869 loss: 5.68385731e-05
Iter: 870 loss: 5.66974413e-05
Iter: 871 loss: 5.63139838e-05
Iter: 872 loss: 5.86885653e-05
Iter: 873 loss: 5.62126443e-05
Iter: 874 loss: 5.58024294e-05
Iter: 875 loss: 5.91039134e-05
Iter: 876 loss: 5.57757448e-05
Iter: 877 loss: 5.5391487e-05
Iter: 878 loss: 5.60165718e-05
Iter: 879 loss: 5.52151178e-05
Iter: 880 loss: 5.47282543e-05
Iter: 881 loss: 5.62047462e-05
Iter: 882 loss: 5.45823423e-05
Iter: 883 loss: 5.39828252e-05
Iter: 884 loss: 6.22631123e-05
Iter: 885 loss: 5.39807e-05
Iter: 886 loss: 5.38231798e-05
Iter: 887 loss: 5.3986405e-05
Iter: 888 loss: 5.37351661e-05
Iter: 889 loss: 5.35151339e-05
Iter: 890 loss: 5.4878481e-05
Iter: 891 loss: 5.34892497e-05
Iter: 892 loss: 5.32824051e-05
Iter: 893 loss: 5.37159503e-05
Iter: 894 loss: 5.32002086e-05
Iter: 895 loss: 5.30318939e-05
Iter: 896 loss: 5.39917601e-05
Iter: 897 loss: 5.30091202e-05
Iter: 898 loss: 5.28310702e-05
Iter: 899 loss: 5.26285658e-05
Iter: 900 loss: 5.26027179e-05
Iter: 901 loss: 5.23193448e-05
Iter: 902 loss: 5.42660637e-05
Iter: 903 loss: 5.22930713e-05
Iter: 904 loss: 5.19071145e-05
Iter: 905 loss: 5.3659689e-05
Iter: 906 loss: 5.18307716e-05
Iter: 907 loss: 5.16481523e-05
Iter: 908 loss: 5.12758415e-05
Iter: 909 loss: 5.78994877e-05
Iter: 910 loss: 5.12687111e-05
Iter: 911 loss: 5.0917457e-05
Iter: 912 loss: 5.21081056e-05
Iter: 913 loss: 5.08219673e-05
Iter: 914 loss: 5.0627481e-05
Iter: 915 loss: 5.16223481e-05
Iter: 916 loss: 5.05959106e-05
Iter: 917 loss: 5.04171257e-05
Iter: 918 loss: 5.0368566e-05
Iter: 919 loss: 5.02586772e-05
Iter: 920 loss: 5.0004448e-05
Iter: 921 loss: 5.26651384e-05
Iter: 922 loss: 4.9997383e-05
Iter: 923 loss: 4.98344489e-05
Iter: 924 loss: 4.98775771e-05
Iter: 925 loss: 4.97149595e-05
Iter: 926 loss: 4.94685228e-05
Iter: 927 loss: 5.06147335e-05
Iter: 928 loss: 4.94234046e-05
Iter: 929 loss: 4.92110703e-05
Iter: 930 loss: 5.00170318e-05
Iter: 931 loss: 4.91604e-05
Iter: 932 loss: 4.89520971e-05
Iter: 933 loss: 4.95376662e-05
Iter: 934 loss: 4.88857768e-05
Iter: 935 loss: 4.87753023e-05
Iter: 936 loss: 4.85757482e-05
Iter: 937 loss: 5.33401981e-05
Iter: 938 loss: 4.85758283e-05
Iter: 939 loss: 4.85019373e-05
Iter: 940 loss: 4.84505217e-05
Iter: 941 loss: 4.83186e-05
Iter: 942 loss: 4.81465177e-05
Iter: 943 loss: 4.8135822e-05
Iter: 944 loss: 4.79498703e-05
Iter: 945 loss: 4.82322721e-05
Iter: 946 loss: 4.78611037e-05
Iter: 947 loss: 4.76668392e-05
Iter: 948 loss: 4.80196541e-05
Iter: 949 loss: 4.75830821e-05
Iter: 950 loss: 4.73813852e-05
Iter: 951 loss: 4.79103473e-05
Iter: 952 loss: 4.7313275e-05
Iter: 953 loss: 4.71157546e-05
Iter: 954 loss: 4.86552162e-05
Iter: 955 loss: 4.71011663e-05
Iter: 956 loss: 4.69676088e-05
Iter: 957 loss: 4.81826355e-05
Iter: 958 loss: 4.69615261e-05
Iter: 959 loss: 4.68723156e-05
Iter: 960 loss: 4.66342826e-05
Iter: 961 loss: 4.83013719e-05
Iter: 962 loss: 4.65801313e-05
Iter: 963 loss: 4.64738769e-05
Iter: 964 loss: 4.64213445e-05
Iter: 965 loss: 4.62803582e-05
Iter: 966 loss: 4.65445919e-05
Iter: 967 loss: 4.62207681e-05
Iter: 968 loss: 4.60767e-05
Iter: 969 loss: 4.6990066e-05
Iter: 970 loss: 4.60604715e-05
Iter: 971 loss: 4.59504299e-05
Iter: 972 loss: 4.57780043e-05
Iter: 973 loss: 4.57759525e-05
Iter: 974 loss: 4.57000569e-05
Iter: 975 loss: 4.56777889e-05
Iter: 976 loss: 4.55761219e-05
Iter: 977 loss: 4.53548346e-05
Iter: 978 loss: 4.86852e-05
Iter: 979 loss: 4.53459725e-05
Iter: 980 loss: 4.51601954e-05
Iter: 981 loss: 4.60061e-05
Iter: 982 loss: 4.5124907e-05
Iter: 983 loss: 4.49587387e-05
Iter: 984 loss: 4.53436296e-05
Iter: 985 loss: 4.4896602e-05
Iter: 986 loss: 4.47308703e-05
Iter: 987 loss: 4.48363426e-05
Iter: 988 loss: 4.46254562e-05
Iter: 989 loss: 4.44725047e-05
Iter: 990 loss: 4.44725229e-05
Iter: 991 loss: 4.43242861e-05
Iter: 992 loss: 4.48103165e-05
Iter: 993 loss: 4.42827877e-05
Iter: 994 loss: 4.41596749e-05
Iter: 995 loss: 4.42249111e-05
Iter: 996 loss: 4.40785916e-05
Iter: 997 loss: 4.39740252e-05
Iter: 998 loss: 4.53806715e-05
Iter: 999 loss: 4.39736432e-05
Iter: 1000 loss: 4.38760508e-05
Iter: 1001 loss: 4.38823881e-05
Iter: 1002 loss: 4.37999734e-05
Iter: 1003 loss: 4.36961709e-05
Iter: 1004 loss: 4.38605821e-05
Iter: 1005 loss: 4.36476766e-05
Iter: 1006 loss: 4.34836365e-05
Iter: 1007 loss: 4.38370625e-05
Iter: 1008 loss: 4.3419248e-05
Iter: 1009 loss: 4.33373316e-05
Iter: 1010 loss: 4.45475671e-05
Iter: 1011 loss: 4.33372e-05
Iter: 1012 loss: 4.32587112e-05
Iter: 1013 loss: 4.3091306e-05
Iter: 1014 loss: 4.57395035e-05
Iter: 1015 loss: 4.30855143e-05
Iter: 1016 loss: 4.29139582e-05
Iter: 1017 loss: 4.36355695e-05
Iter: 1018 loss: 4.28777967e-05
Iter: 1019 loss: 4.26973347e-05
Iter: 1020 loss: 4.3412474e-05
Iter: 1021 loss: 4.26567312e-05
Iter: 1022 loss: 4.24928367e-05
Iter: 1023 loss: 4.24696409e-05
Iter: 1024 loss: 4.23542333e-05
Iter: 1025 loss: 4.22796802e-05
Iter: 1026 loss: 4.22458652e-05
Iter: 1027 loss: 4.21484219e-05
Iter: 1028 loss: 4.20392389e-05
Iter: 1029 loss: 4.2024265e-05
Iter: 1030 loss: 4.18577838e-05
Iter: 1031 loss: 4.19685784e-05
Iter: 1032 loss: 4.17529882e-05
Iter: 1033 loss: 4.16652838e-05
Iter: 1034 loss: 4.1645013e-05
Iter: 1035 loss: 4.15494142e-05
Iter: 1036 loss: 4.14185924e-05
Iter: 1037 loss: 4.14124625e-05
Iter: 1038 loss: 4.12866211e-05
Iter: 1039 loss: 4.22438134e-05
Iter: 1040 loss: 4.12773e-05
Iter: 1041 loss: 4.1165149e-05
Iter: 1042 loss: 4.16398216e-05
Iter: 1043 loss: 4.11413312e-05
Iter: 1044 loss: 4.10682769e-05
Iter: 1045 loss: 4.13352609e-05
Iter: 1046 loss: 4.10500579e-05
Iter: 1047 loss: 4.09474815e-05
Iter: 1048 loss: 4.07836451e-05
Iter: 1049 loss: 4.07822954e-05
Iter: 1050 loss: 4.0605024e-05
Iter: 1051 loss: 4.06256368e-05
Iter: 1052 loss: 4.04695347e-05
Iter: 1053 loss: 4.03118829e-05
Iter: 1054 loss: 4.03068843e-05
Iter: 1055 loss: 4.02115111e-05
Iter: 1056 loss: 4.05849e-05
Iter: 1057 loss: 4.01896468e-05
Iter: 1058 loss: 4.0083527e-05
Iter: 1059 loss: 4.05169121e-05
Iter: 1060 loss: 4.00596109e-05
Iter: 1061 loss: 3.99805795e-05
Iter: 1062 loss: 3.98887423e-05
Iter: 1063 loss: 3.98779084e-05
Iter: 1064 loss: 3.9716484e-05
Iter: 1065 loss: 4.0084582e-05
Iter: 1066 loss: 3.96560499e-05
Iter: 1067 loss: 3.96101932e-05
Iter: 1068 loss: 3.95732641e-05
Iter: 1069 loss: 3.95253592e-05
Iter: 1070 loss: 3.94015806e-05
Iter: 1071 loss: 4.04077218e-05
Iter: 1072 loss: 3.93796e-05
Iter: 1073 loss: 3.93201117e-05
Iter: 1074 loss: 3.930393e-05
Iter: 1075 loss: 3.92344737e-05
Iter: 1076 loss: 3.92733964e-05
Iter: 1077 loss: 3.91893191e-05
Iter: 1078 loss: 3.91052854e-05
Iter: 1079 loss: 3.94896779e-05
Iter: 1080 loss: 3.9089573e-05
Iter: 1081 loss: 3.90028508e-05
Iter: 1082 loss: 3.89334746e-05
Iter: 1083 loss: 3.89072411e-05
Iter: 1084 loss: 3.8785e-05
Iter: 1085 loss: 3.9028404e-05
Iter: 1086 loss: 3.8734921e-05
Iter: 1087 loss: 3.86608262e-05
Iter: 1088 loss: 3.86407955e-05
Iter: 1089 loss: 3.8572427e-05
Iter: 1090 loss: 3.8719063e-05
Iter: 1091 loss: 3.85455751e-05
Iter: 1092 loss: 3.84562445e-05
Iter: 1093 loss: 3.83533588e-05
Iter: 1094 loss: 3.8340695e-05
Iter: 1095 loss: 3.82269936e-05
Iter: 1096 loss: 3.89209526e-05
Iter: 1097 loss: 3.82133512e-05
Iter: 1098 loss: 3.81351856e-05
Iter: 1099 loss: 3.8690574e-05
Iter: 1100 loss: 3.8128037e-05
Iter: 1101 loss: 3.80358106e-05
Iter: 1102 loss: 3.82080652e-05
Iter: 1103 loss: 3.79966077e-05
Iter: 1104 loss: 3.79382e-05
Iter: 1105 loss: 3.78254335e-05
Iter: 1106 loss: 4.0152554e-05
Iter: 1107 loss: 3.78245168e-05
Iter: 1108 loss: 3.77160541e-05
Iter: 1109 loss: 3.77116521e-05
Iter: 1110 loss: 3.76440075e-05
Iter: 1111 loss: 3.76713e-05
Iter: 1112 loss: 3.75974851e-05
Iter: 1113 loss: 3.74944575e-05
Iter: 1114 loss: 3.78917175e-05
Iter: 1115 loss: 3.74703413e-05
Iter: 1116 loss: 3.73841103e-05
Iter: 1117 loss: 3.72747672e-05
Iter: 1118 loss: 3.72666e-05
Iter: 1119 loss: 3.71172e-05
Iter: 1120 loss: 3.7904e-05
Iter: 1121 loss: 3.70937196e-05
Iter: 1122 loss: 3.70337075e-05
Iter: 1123 loss: 3.7009755e-05
Iter: 1124 loss: 3.69587251e-05
Iter: 1125 loss: 3.69246736e-05
Iter: 1126 loss: 3.69058398e-05
Iter: 1127 loss: 3.68009241e-05
Iter: 1128 loss: 3.66396744e-05
Iter: 1129 loss: 3.66370732e-05
Iter: 1130 loss: 3.64924708e-05
Iter: 1131 loss: 3.801395e-05
Iter: 1132 loss: 3.64887201e-05
Iter: 1133 loss: 3.63767285e-05
Iter: 1134 loss: 3.75971867e-05
Iter: 1135 loss: 3.63740619e-05
Iter: 1136 loss: 3.6293739e-05
Iter: 1137 loss: 3.62308565e-05
Iter: 1138 loss: 3.62057835e-05
Iter: 1139 loss: 3.61043931e-05
Iter: 1140 loss: 3.63531872e-05
Iter: 1141 loss: 3.60686608e-05
Iter: 1142 loss: 3.59650439e-05
Iter: 1143 loss: 3.73347284e-05
Iter: 1144 loss: 3.59644764e-05
Iter: 1145 loss: 3.59133192e-05
Iter: 1146 loss: 3.59047554e-05
Iter: 1147 loss: 3.5869798e-05
Iter: 1148 loss: 3.57793e-05
Iter: 1149 loss: 3.61025159e-05
Iter: 1150 loss: 3.57562785e-05
Iter: 1151 loss: 3.57034551e-05
Iter: 1152 loss: 3.56328346e-05
Iter: 1153 loss: 3.56290839e-05
Iter: 1154 loss: 3.55360935e-05
Iter: 1155 loss: 3.68354376e-05
Iter: 1156 loss: 3.55359152e-05
Iter: 1157 loss: 3.54291042e-05
Iter: 1158 loss: 3.57579229e-05
Iter: 1159 loss: 3.53976429e-05
Iter: 1160 loss: 3.53286123e-05
Iter: 1161 loss: 3.54533258e-05
Iter: 1162 loss: 3.5298297e-05
Iter: 1163 loss: 3.52048955e-05
Iter: 1164 loss: 3.52420684e-05
Iter: 1165 loss: 3.51397139e-05
Iter: 1166 loss: 3.51472772e-05
Iter: 1167 loss: 3.50921764e-05
Iter: 1168 loss: 3.5065168e-05
Iter: 1169 loss: 3.49969851e-05
Iter: 1170 loss: 3.56313394e-05
Iter: 1171 loss: 3.49871516e-05
Iter: 1172 loss: 3.49070688e-05
Iter: 1173 loss: 3.4974204e-05
Iter: 1174 loss: 3.48593894e-05
Iter: 1175 loss: 3.47737951e-05
Iter: 1176 loss: 3.53115829e-05
Iter: 1177 loss: 3.47637142e-05
Iter: 1178 loss: 3.46921661e-05
Iter: 1179 loss: 3.57786957e-05
Iter: 1180 loss: 3.4692006e-05
Iter: 1181 loss: 3.46415836e-05
Iter: 1182 loss: 3.46196503e-05
Iter: 1183 loss: 3.45939043e-05
Iter: 1184 loss: 3.45214539e-05
Iter: 1185 loss: 3.49085749e-05
Iter: 1186 loss: 3.45105e-05
Iter: 1187 loss: 3.44389227e-05
Iter: 1188 loss: 3.45058725e-05
Iter: 1189 loss: 3.43975771e-05
Iter: 1190 loss: 3.43186621e-05
Iter: 1191 loss: 3.44547625e-05
Iter: 1192 loss: 3.42836793e-05
Iter: 1193 loss: 3.41966588e-05
Iter: 1194 loss: 3.50485097e-05
Iter: 1195 loss: 3.41937703e-05
Iter: 1196 loss: 3.41358464e-05
Iter: 1197 loss: 3.40636216e-05
Iter: 1198 loss: 3.40578044e-05
Iter: 1199 loss: 3.39723338e-05
Iter: 1200 loss: 3.44167711e-05
Iter: 1201 loss: 3.39582766e-05
Iter: 1202 loss: 3.38885111e-05
Iter: 1203 loss: 3.46734814e-05
Iter: 1204 loss: 3.38873724e-05
Iter: 1205 loss: 3.38329228e-05
Iter: 1206 loss: 3.37685051e-05
Iter: 1207 loss: 3.3761582e-05
Iter: 1208 loss: 3.36803059e-05
Iter: 1209 loss: 3.35569821e-05
Iter: 1210 loss: 3.3554832e-05
Iter: 1211 loss: 3.34414071e-05
Iter: 1212 loss: 3.34414472e-05
Iter: 1213 loss: 3.33795724e-05
Iter: 1214 loss: 3.33764947e-05
Iter: 1215 loss: 3.33403368e-05
Iter: 1216 loss: 3.32483614e-05
Iter: 1217 loss: 3.4041117e-05
Iter: 1218 loss: 3.32327363e-05
Iter: 1219 loss: 3.31011252e-05
Iter: 1220 loss: 3.43766224e-05
Iter: 1221 loss: 3.3096796e-05
Iter: 1222 loss: 3.30273615e-05
Iter: 1223 loss: 3.31274096e-05
Iter: 1224 loss: 3.29936e-05
Iter: 1225 loss: 3.29326795e-05
Iter: 1226 loss: 3.34368124e-05
Iter: 1227 loss: 3.29289142e-05
Iter: 1228 loss: 3.28771566e-05
Iter: 1229 loss: 3.29635368e-05
Iter: 1230 loss: 3.28539463e-05
Iter: 1231 loss: 3.281104e-05
Iter: 1232 loss: 3.27646958e-05
Iter: 1233 loss: 3.27575763e-05
Iter: 1234 loss: 3.27068374e-05
Iter: 1235 loss: 3.27066664e-05
Iter: 1236 loss: 3.26592053e-05
Iter: 1237 loss: 3.26230511e-05
Iter: 1238 loss: 3.26078953e-05
Iter: 1239 loss: 3.25290966e-05
Iter: 1240 loss: 3.25979927e-05
Iter: 1241 loss: 3.24828e-05
Iter: 1242 loss: 3.24080138e-05
Iter: 1243 loss: 3.23640415e-05
Iter: 1244 loss: 3.23326167e-05
Iter: 1245 loss: 3.23051754e-05
Iter: 1246 loss: 3.22808846e-05
Iter: 1247 loss: 3.22182023e-05
Iter: 1248 loss: 3.21118e-05
Iter: 1249 loss: 3.21116604e-05
Iter: 1250 loss: 3.20244e-05
Iter: 1251 loss: 3.24373759e-05
Iter: 1252 loss: 3.20088584e-05
Iter: 1253 loss: 3.19150131e-05
Iter: 1254 loss: 3.25095571e-05
Iter: 1255 loss: 3.19042e-05
Iter: 1256 loss: 3.18480124e-05
Iter: 1257 loss: 3.18405837e-05
Iter: 1258 loss: 3.18012535e-05
Iter: 1259 loss: 3.17043e-05
Iter: 1260 loss: 3.2246131e-05
Iter: 1261 loss: 3.16903097e-05
Iter: 1262 loss: 3.16257028e-05
Iter: 1263 loss: 3.16702826e-05
Iter: 1264 loss: 3.15857687e-05
Iter: 1265 loss: 3.15232173e-05
Iter: 1266 loss: 3.16316109e-05
Iter: 1267 loss: 3.1495787e-05
Iter: 1268 loss: 3.14438221e-05
Iter: 1269 loss: 3.22261112e-05
Iter: 1270 loss: 3.14436111e-05
Iter: 1271 loss: 3.14014396e-05
Iter: 1272 loss: 3.13151322e-05
Iter: 1273 loss: 3.28910319e-05
Iter: 1274 loss: 3.13137716e-05
Iter: 1275 loss: 3.12265292e-05
Iter: 1276 loss: 3.18176608e-05
Iter: 1277 loss: 3.12180637e-05
Iter: 1278 loss: 3.11534095e-05
Iter: 1279 loss: 3.11647673e-05
Iter: 1280 loss: 3.11049589e-05
Iter: 1281 loss: 3.10433788e-05
Iter: 1282 loss: 3.10386677e-05
Iter: 1283 loss: 3.10055548e-05
Iter: 1284 loss: 3.09174538e-05
Iter: 1285 loss: 3.15177786e-05
Iter: 1286 loss: 3.08970775e-05
Iter: 1287 loss: 3.0854033e-05
Iter: 1288 loss: 3.08366762e-05
Iter: 1289 loss: 3.07924201e-05
Iter: 1290 loss: 3.07900264e-05
Iter: 1291 loss: 3.07561277e-05
Iter: 1292 loss: 3.06825459e-05
Iter: 1293 loss: 3.07469236e-05
Iter: 1294 loss: 3.06392e-05
Iter: 1295 loss: 3.05832073e-05
Iter: 1296 loss: 3.04946243e-05
Iter: 1297 loss: 3.04936966e-05
Iter: 1298 loss: 3.04173154e-05
Iter: 1299 loss: 3.11422737e-05
Iter: 1300 loss: 3.04142741e-05
Iter: 1301 loss: 3.03664729e-05
Iter: 1302 loss: 3.02749722e-05
Iter: 1303 loss: 3.22548949e-05
Iter: 1304 loss: 3.02748085e-05
Iter: 1305 loss: 3.01782366e-05
Iter: 1306 loss: 3.02177868e-05
Iter: 1307 loss: 3.01115979e-05
Iter: 1308 loss: 3.0061723e-05
Iter: 1309 loss: 3.00562788e-05
Iter: 1310 loss: 3.00016163e-05
Iter: 1311 loss: 3.00309621e-05
Iter: 1312 loss: 2.99654985e-05
Iter: 1313 loss: 2.99161547e-05
Iter: 1314 loss: 3.01702021e-05
Iter: 1315 loss: 2.99085132e-05
Iter: 1316 loss: 2.9863e-05
Iter: 1317 loss: 2.98694067e-05
Iter: 1318 loss: 2.98286413e-05
Iter: 1319 loss: 2.97864281e-05
Iter: 1320 loss: 3.00428055e-05
Iter: 1321 loss: 2.97813967e-05
Iter: 1322 loss: 2.97276492e-05
Iter: 1323 loss: 2.96761755e-05
Iter: 1324 loss: 2.96647049e-05
Iter: 1325 loss: 2.95950413e-05
Iter: 1326 loss: 2.97086808e-05
Iter: 1327 loss: 2.95632453e-05
Iter: 1328 loss: 2.94962774e-05
Iter: 1329 loss: 2.9921026e-05
Iter: 1330 loss: 2.94887504e-05
Iter: 1331 loss: 2.94096135e-05
Iter: 1332 loss: 2.97790975e-05
Iter: 1333 loss: 2.93944286e-05
Iter: 1334 loss: 2.93554531e-05
Iter: 1335 loss: 2.92755722e-05
Iter: 1336 loss: 3.07131268e-05
Iter: 1337 loss: 2.92743152e-05
Iter: 1338 loss: 2.91691031e-05
Iter: 1339 loss: 3.03657889e-05
Iter: 1340 loss: 2.91672295e-05
Iter: 1341 loss: 2.91242559e-05
Iter: 1342 loss: 2.90981989e-05
Iter: 1343 loss: 2.90805147e-05
Iter: 1344 loss: 2.90690277e-05
Iter: 1345 loss: 2.90285461e-05
Iter: 1346 loss: 2.89614854e-05
Iter: 1347 loss: 2.92904588e-05
Iter: 1348 loss: 2.8949511e-05
Iter: 1349 loss: 2.88966294e-05
Iter: 1350 loss: 2.90317148e-05
Iter: 1351 loss: 2.88780666e-05
Iter: 1352 loss: 2.88188185e-05
Iter: 1353 loss: 2.88298088e-05
Iter: 1354 loss: 2.87747462e-05
Iter: 1355 loss: 2.87104467e-05
Iter: 1356 loss: 2.92588757e-05
Iter: 1357 loss: 2.87066723e-05
Iter: 1358 loss: 2.86575378e-05
Iter: 1359 loss: 2.8856226e-05
Iter: 1360 loss: 2.86467584e-05
Iter: 1361 loss: 2.85950046e-05
Iter: 1362 loss: 2.85317947e-05
Iter: 1363 loss: 2.85257192e-05
Iter: 1364 loss: 2.84331145e-05
Iter: 1365 loss: 2.84211892e-05
Iter: 1366 loss: 2.83554655e-05
Iter: 1367 loss: 2.83741538e-05
Iter: 1368 loss: 2.83127301e-05
Iter: 1369 loss: 2.82776818e-05
Iter: 1370 loss: 2.82270303e-05
Iter: 1371 loss: 2.82254e-05
Iter: 1372 loss: 2.8181139e-05
Iter: 1373 loss: 2.88582123e-05
Iter: 1374 loss: 2.8181048e-05
Iter: 1375 loss: 2.81414577e-05
Iter: 1376 loss: 2.81607608e-05
Iter: 1377 loss: 2.81148896e-05
Iter: 1378 loss: 2.8059927e-05
Iter: 1379 loss: 2.83949521e-05
Iter: 1380 loss: 2.80531713e-05
Iter: 1381 loss: 2.79951382e-05
Iter: 1382 loss: 2.80940476e-05
Iter: 1383 loss: 2.79690612e-05
Iter: 1384 loss: 2.79248143e-05
Iter: 1385 loss: 2.79444466e-05
Iter: 1386 loss: 2.78947955e-05
Iter: 1387 loss: 2.78130847e-05
Iter: 1388 loss: 2.82959736e-05
Iter: 1389 loss: 2.78026255e-05
Iter: 1390 loss: 2.77280506e-05
Iter: 1391 loss: 2.79744909e-05
Iter: 1392 loss: 2.77074432e-05
Iter: 1393 loss: 2.76336541e-05
Iter: 1394 loss: 2.81241955e-05
Iter: 1395 loss: 2.76261599e-05
Iter: 1396 loss: 2.75830389e-05
Iter: 1397 loss: 2.75700731e-05
Iter: 1398 loss: 2.75443745e-05
Iter: 1399 loss: 2.74819213e-05
Iter: 1400 loss: 2.76002e-05
Iter: 1401 loss: 2.7455475e-05
Iter: 1402 loss: 2.74095783e-05
Iter: 1403 loss: 2.74093909e-05
Iter: 1404 loss: 2.73727201e-05
Iter: 1405 loss: 2.73626538e-05
Iter: 1406 loss: 2.73403748e-05
Iter: 1407 loss: 2.72960351e-05
Iter: 1408 loss: 2.73683127e-05
Iter: 1409 loss: 2.72760226e-05
Iter: 1410 loss: 2.72124398e-05
Iter: 1411 loss: 2.75133425e-05
Iter: 1412 loss: 2.72008838e-05
Iter: 1413 loss: 2.71564677e-05
Iter: 1414 loss: 2.71230892e-05
Iter: 1415 loss: 2.71084482e-05
Iter: 1416 loss: 2.71367971e-05
Iter: 1417 loss: 2.70794171e-05
Iter: 1418 loss: 2.70568271e-05
Iter: 1419 loss: 2.70196033e-05
Iter: 1420 loss: 2.70194614e-05
Iter: 1421 loss: 2.69734264e-05
Iter: 1422 loss: 2.69751636e-05
Iter: 1423 loss: 2.69372049e-05
Iter: 1424 loss: 2.68875301e-05
Iter: 1425 loss: 2.69036827e-05
Iter: 1426 loss: 2.68522781e-05
Iter: 1427 loss: 2.67816031e-05
Iter: 1428 loss: 2.72547913e-05
Iter: 1429 loss: 2.67745381e-05
Iter: 1430 loss: 2.67263931e-05
Iter: 1431 loss: 2.66813659e-05
Iter: 1432 loss: 2.66699317e-05
Iter: 1433 loss: 2.6627069e-05
Iter: 1434 loss: 2.66253483e-05
Iter: 1435 loss: 2.65995877e-05
Iter: 1436 loss: 2.66325114e-05
Iter: 1437 loss: 2.65861599e-05
Iter: 1438 loss: 2.65509316e-05
Iter: 1439 loss: 2.64943701e-05
Iter: 1440 loss: 2.64939481e-05
Iter: 1441 loss: 2.64312439e-05
Iter: 1442 loss: 2.65309827e-05
Iter: 1443 loss: 2.64023292e-05
Iter: 1444 loss: 2.63423208e-05
Iter: 1445 loss: 2.72091947e-05
Iter: 1446 loss: 2.63419952e-05
Iter: 1447 loss: 2.6294998e-05
Iter: 1448 loss: 2.63540205e-05
Iter: 1449 loss: 2.62701724e-05
Iter: 1450 loss: 2.62225731e-05
Iter: 1451 loss: 2.69363591e-05
Iter: 1452 loss: 2.62226749e-05
Iter: 1453 loss: 2.61852147e-05
Iter: 1454 loss: 2.61695e-05
Iter: 1455 loss: 2.61499208e-05
Iter: 1456 loss: 2.60967208e-05
Iter: 1457 loss: 2.63274142e-05
Iter: 1458 loss: 2.60862471e-05
Iter: 1459 loss: 2.60433299e-05
Iter: 1460 loss: 2.61483183e-05
Iter: 1461 loss: 2.60278357e-05
Iter: 1462 loss: 2.59820772e-05
Iter: 1463 loss: 2.59844091e-05
Iter: 1464 loss: 2.59462286e-05
Iter: 1465 loss: 2.5890482e-05
Iter: 1466 loss: 2.59195458e-05
Iter: 1467 loss: 2.58532546e-05
Iter: 1468 loss: 2.5828087e-05
Iter: 1469 loss: 2.58149666e-05
Iter: 1470 loss: 2.5778736e-05
Iter: 1471 loss: 2.57128086e-05
Iter: 1472 loss: 2.73088517e-05
Iter: 1473 loss: 2.57128268e-05
Iter: 1474 loss: 2.56584372e-05
Iter: 1475 loss: 2.58745877e-05
Iter: 1476 loss: 2.56462226e-05
Iter: 1477 loss: 2.5585945e-05
Iter: 1478 loss: 2.590636e-05
Iter: 1479 loss: 2.55768136e-05
Iter: 1480 loss: 2.55419327e-05
Iter: 1481 loss: 2.56651292e-05
Iter: 1482 loss: 2.55329069e-05
Iter: 1483 loss: 2.54935039e-05
Iter: 1484 loss: 2.55809664e-05
Iter: 1485 loss: 2.54784791e-05
Iter: 1486 loss: 2.54392035e-05
Iter: 1487 loss: 2.58741093e-05
Iter: 1488 loss: 2.54385141e-05
Iter: 1489 loss: 2.54197512e-05
Iter: 1490 loss: 2.5401896e-05
Iter: 1491 loss: 2.53976323e-05
Iter: 1492 loss: 2.53642665e-05
Iter: 1493 loss: 2.55724535e-05
Iter: 1494 loss: 2.53604085e-05
Iter: 1495 loss: 2.53308681e-05
Iter: 1496 loss: 2.53144717e-05
Iter: 1497 loss: 2.53013932e-05
Iter: 1498 loss: 2.52598438e-05
Iter: 1499 loss: 2.52952777e-05
Iter: 1500 loss: 2.52355094e-05
Iter: 1501 loss: 2.51818619e-05
Iter: 1502 loss: 2.52632562e-05
Iter: 1503 loss: 2.51565471e-05
Iter: 1504 loss: 2.51255115e-05
Iter: 1505 loss: 2.51188285e-05
Iter: 1506 loss: 2.50924768e-05
Iter: 1507 loss: 2.50254889e-05
Iter: 1508 loss: 2.56240492e-05
Iter: 1509 loss: 2.50156e-05
Iter: 1510 loss: 2.49630393e-05
Iter: 1511 loss: 2.5425863e-05
Iter: 1512 loss: 2.49603891e-05
Iter: 1513 loss: 2.49062177e-05
Iter: 1514 loss: 2.50801313e-05
Iter: 1515 loss: 2.48908655e-05
Iter: 1516 loss: 2.48559463e-05
Iter: 1517 loss: 2.49527548e-05
Iter: 1518 loss: 2.48443303e-05
Iter: 1519 loss: 2.48088945e-05
Iter: 1520 loss: 2.50712219e-05
Iter: 1521 loss: 2.48061115e-05
Iter: 1522 loss: 2.47689786e-05
Iter: 1523 loss: 2.47635799e-05
Iter: 1524 loss: 2.47375119e-05
Iter: 1525 loss: 2.47009775e-05
Iter: 1526 loss: 2.47301687e-05
Iter: 1527 loss: 2.46791933e-05
Iter: 1528 loss: 2.46459513e-05
Iter: 1529 loss: 2.46458621e-05
Iter: 1530 loss: 2.46218115e-05
Iter: 1531 loss: 2.45765768e-05
Iter: 1532 loss: 2.55898922e-05
Iter: 1533 loss: 2.4576193e-05
Iter: 1534 loss: 2.45196097e-05
Iter: 1535 loss: 2.4689316e-05
Iter: 1536 loss: 2.45024294e-05
Iter: 1537 loss: 2.4465462e-05
Iter: 1538 loss: 2.47914832e-05
Iter: 1539 loss: 2.44639305e-05
Iter: 1540 loss: 2.44308776e-05
Iter: 1541 loss: 2.46842064e-05
Iter: 1542 loss: 2.44283838e-05
Iter: 1543 loss: 2.44082439e-05
Iter: 1544 loss: 2.43597169e-05
Iter: 1545 loss: 2.49046716e-05
Iter: 1546 loss: 2.43553641e-05
Iter: 1547 loss: 2.43148443e-05
Iter: 1548 loss: 2.48696779e-05
Iter: 1549 loss: 2.43146205e-05
Iter: 1550 loss: 2.42701535e-05
Iter: 1551 loss: 2.4376528e-05
Iter: 1552 loss: 2.42542683e-05
Iter: 1553 loss: 2.42185561e-05
Iter: 1554 loss: 2.42703245e-05
Iter: 1555 loss: 2.42014958e-05
Iter: 1556 loss: 2.41688613e-05
Iter: 1557 loss: 2.41682756e-05
Iter: 1558 loss: 2.41470425e-05
Iter: 1559 loss: 2.41209327e-05
Iter: 1560 loss: 2.41188263e-05
Iter: 1561 loss: 2.4083969e-05
Iter: 1562 loss: 2.40951122e-05
Iter: 1563 loss: 2.4059209e-05
Iter: 1564 loss: 2.40364352e-05
Iter: 1565 loss: 2.40312111e-05
Iter: 1566 loss: 2.40129448e-05
Iter: 1567 loss: 2.39865258e-05
Iter: 1568 loss: 2.39859237e-05
Iter: 1569 loss: 2.39501533e-05
Iter: 1570 loss: 2.40006775e-05
Iter: 1571 loss: 2.39324563e-05
Iter: 1572 loss: 2.38979719e-05
Iter: 1573 loss: 2.44398907e-05
Iter: 1574 loss: 2.38980974e-05
Iter: 1575 loss: 2.38668254e-05
Iter: 1576 loss: 2.3868186e-05
Iter: 1577 loss: 2.38425637e-05
Iter: 1578 loss: 2.38120083e-05
Iter: 1579 loss: 2.37650183e-05
Iter: 1580 loss: 2.37644072e-05
Iter: 1581 loss: 2.37331406e-05
Iter: 1582 loss: 2.3729659e-05
Iter: 1583 loss: 2.36949145e-05
Iter: 1584 loss: 2.3730785e-05
Iter: 1585 loss: 2.36758024e-05
Iter: 1586 loss: 2.36516062e-05
Iter: 1587 loss: 2.37515578e-05
Iter: 1588 loss: 2.36466585e-05
Iter: 1589 loss: 2.36132801e-05
Iter: 1590 loss: 2.37004442e-05
Iter: 1591 loss: 2.36021369e-05
Iter: 1592 loss: 2.35799507e-05
Iter: 1593 loss: 2.35538246e-05
Iter: 1594 loss: 2.35510979e-05
Iter: 1595 loss: 2.35090101e-05
Iter: 1596 loss: 2.34823856e-05
Iter: 1597 loss: 2.34657527e-05
Iter: 1598 loss: 2.34107792e-05
Iter: 1599 loss: 2.37373879e-05
Iter: 1600 loss: 2.34040563e-05
Iter: 1601 loss: 2.33792161e-05
Iter: 1602 loss: 2.33745577e-05
Iter: 1603 loss: 2.33576829e-05
Iter: 1604 loss: 2.33120445e-05
Iter: 1605 loss: 2.36252818e-05
Iter: 1606 loss: 2.33021419e-05
Iter: 1607 loss: 2.32882521e-05
Iter: 1608 loss: 2.327728e-05
Iter: 1609 loss: 2.32557359e-05
Iter: 1610 loss: 2.32706334e-05
Iter: 1611 loss: 2.32420934e-05
Iter: 1612 loss: 2.32190087e-05
Iter: 1613 loss: 2.32145721e-05
Iter: 1614 loss: 2.31988579e-05
Iter: 1615 loss: 2.31712911e-05
Iter: 1616 loss: 2.34027921e-05
Iter: 1617 loss: 2.31697486e-05
Iter: 1618 loss: 2.31427e-05
Iter: 1619 loss: 2.31744416e-05
Iter: 1620 loss: 2.31284284e-05
Iter: 1621 loss: 2.31105259e-05
Iter: 1622 loss: 2.31106242e-05
Iter: 1623 loss: 2.30904443e-05
Iter: 1624 loss: 2.30482601e-05
Iter: 1625 loss: 2.37665772e-05
Iter: 1626 loss: 2.30470796e-05
Iter: 1627 loss: 2.30001406e-05
Iter: 1628 loss: 2.32405146e-05
Iter: 1629 loss: 2.2992559e-05
Iter: 1630 loss: 2.29505786e-05
Iter: 1631 loss: 2.28917452e-05
Iter: 1632 loss: 2.28894496e-05
Iter: 1633 loss: 2.28603021e-05
Iter: 1634 loss: 2.28521512e-05
Iter: 1635 loss: 2.28135705e-05
Iter: 1636 loss: 2.29185698e-05
Iter: 1637 loss: 2.28007739e-05
Iter: 1638 loss: 2.27829551e-05
Iter: 1639 loss: 2.27495657e-05
Iter: 1640 loss: 2.34714535e-05
Iter: 1641 loss: 2.27491801e-05
Iter: 1642 loss: 2.27248929e-05
Iter: 1643 loss: 2.27203473e-05
Iter: 1644 loss: 2.26837401e-05
Iter: 1645 loss: 2.27658584e-05
Iter: 1646 loss: 2.26696975e-05
Iter: 1647 loss: 2.26401316e-05
Iter: 1648 loss: 2.27148485e-05
Iter: 1649 loss: 2.2629798e-05
Iter: 1650 loss: 2.25933709e-05
Iter: 1651 loss: 2.28177141e-05
Iter: 1652 loss: 2.25887688e-05
Iter: 1653 loss: 2.25569402e-05
Iter: 1654 loss: 2.25825133e-05
Iter: 1655 loss: 2.25380718e-05
Iter: 1656 loss: 2.25248405e-05
Iter: 1657 loss: 2.25207295e-05
Iter: 1658 loss: 2.25081494e-05
Iter: 1659 loss: 2.2472359e-05
Iter: 1660 loss: 2.26296e-05
Iter: 1661 loss: 2.24591204e-05
Iter: 1662 loss: 2.24265095e-05
Iter: 1663 loss: 2.25572403e-05
Iter: 1664 loss: 2.24193082e-05
Iter: 1665 loss: 2.23925381e-05
Iter: 1666 loss: 2.23981478e-05
Iter: 1667 loss: 2.23723437e-05
Iter: 1668 loss: 2.23528186e-05
Iter: 1669 loss: 2.23496863e-05
Iter: 1670 loss: 2.23327515e-05
Iter: 1671 loss: 2.23067291e-05
Iter: 1672 loss: 2.23064126e-05
Iter: 1673 loss: 2.22751405e-05
Iter: 1674 loss: 2.22850795e-05
Iter: 1675 loss: 2.22528761e-05
Iter: 1676 loss: 2.22345243e-05
Iter: 1677 loss: 2.22337949e-05
Iter: 1678 loss: 2.22205745e-05
Iter: 1679 loss: 2.21877599e-05
Iter: 1680 loss: 2.2517288e-05
Iter: 1681 loss: 2.2183538e-05
Iter: 1682 loss: 2.21489645e-05
Iter: 1683 loss: 2.21419214e-05
Iter: 1684 loss: 2.2119204e-05
Iter: 1685 loss: 2.20900547e-05
Iter: 1686 loss: 2.22291055e-05
Iter: 1687 loss: 2.20847105e-05
Iter: 1688 loss: 2.2056156e-05
Iter: 1689 loss: 2.23086936e-05
Iter: 1690 loss: 2.20548154e-05
Iter: 1691 loss: 2.20350757e-05
Iter: 1692 loss: 2.20689908e-05
Iter: 1693 loss: 2.20260899e-05
Iter: 1694 loss: 2.20061793e-05
Iter: 1695 loss: 2.19769609e-05
Iter: 1696 loss: 2.19760332e-05
Iter: 1697 loss: 2.19384056e-05
Iter: 1698 loss: 2.21353621e-05
Iter: 1699 loss: 2.19324138e-05
Iter: 1700 loss: 2.19108224e-05
Iter: 1701 loss: 2.22145864e-05
Iter: 1702 loss: 2.19107933e-05
Iter: 1703 loss: 2.18858168e-05
Iter: 1704 loss: 2.18727855e-05
Iter: 1705 loss: 2.18614477e-05
Iter: 1706 loss: 2.18355472e-05
Iter: 1707 loss: 2.1831911e-05
Iter: 1708 loss: 2.18134883e-05
Iter: 1709 loss: 2.17975212e-05
Iter: 1710 loss: 2.17921497e-05
Iter: 1711 loss: 2.17711531e-05
Iter: 1712 loss: 2.1811351e-05
Iter: 1713 loss: 2.17624474e-05
Iter: 1714 loss: 2.17469424e-05
Iter: 1715 loss: 2.17366451e-05
Iter: 1716 loss: 2.17311645e-05
Iter: 1717 loss: 2.17072138e-05
Iter: 1718 loss: 2.16970438e-05
Iter: 1719 loss: 2.16846402e-05
Iter: 1720 loss: 2.16615699e-05
Iter: 1721 loss: 2.16596272e-05
Iter: 1722 loss: 2.16400149e-05
Iter: 1723 loss: 2.16979824e-05
Iter: 1724 loss: 2.16342669e-05
Iter: 1725 loss: 2.16173576e-05
Iter: 1726 loss: 2.16395219e-05
Iter: 1727 loss: 2.16085136e-05
Iter: 1728 loss: 2.15912987e-05
Iter: 1729 loss: 2.15778382e-05
Iter: 1730 loss: 2.15722321e-05
Iter: 1731 loss: 2.15412074e-05
Iter: 1732 loss: 2.15868713e-05
Iter: 1733 loss: 2.15263863e-05
Iter: 1734 loss: 2.15103792e-05
Iter: 1735 loss: 2.15082855e-05
Iter: 1736 loss: 2.14901847e-05
Iter: 1737 loss: 2.14579486e-05
Iter: 1738 loss: 2.1457925e-05
Iter: 1739 loss: 2.1433656e-05
Iter: 1740 loss: 2.14177562e-05
Iter: 1741 loss: 2.14081301e-05
Iter: 1742 loss: 2.13967e-05
Iter: 1743 loss: 2.13861349e-05
Iter: 1744 loss: 2.13733802e-05
Iter: 1745 loss: 2.13726016e-05
Iter: 1746 loss: 2.13630046e-05
Iter: 1747 loss: 2.13433923e-05
Iter: 1748 loss: 2.13625517e-05
Iter: 1749 loss: 2.13327548e-05
Iter: 1750 loss: 2.13131807e-05
Iter: 1751 loss: 2.12985069e-05
Iter: 1752 loss: 2.12923751e-05
Iter: 1753 loss: 2.12915838e-05
Iter: 1754 loss: 2.12806517e-05
Iter: 1755 loss: 2.12705199e-05
Iter: 1756 loss: 2.12626219e-05
Iter: 1757 loss: 2.12593513e-05
Iter: 1758 loss: 2.12388895e-05
Iter: 1759 loss: 2.12676314e-05
Iter: 1760 loss: 2.12288815e-05
Iter: 1761 loss: 2.12085506e-05
Iter: 1762 loss: 2.11931529e-05
Iter: 1763 loss: 2.11865554e-05
Iter: 1764 loss: 2.11555198e-05
Iter: 1765 loss: 2.13863168e-05
Iter: 1766 loss: 2.11530532e-05
Iter: 1767 loss: 2.11318875e-05
Iter: 1768 loss: 2.12005943e-05
Iter: 1769 loss: 2.11259103e-05
Iter: 1770 loss: 2.11034858e-05
Iter: 1771 loss: 2.13279345e-05
Iter: 1772 loss: 2.11024599e-05
Iter: 1773 loss: 2.10900944e-05
Iter: 1774 loss: 2.10579128e-05
Iter: 1775 loss: 2.13242529e-05
Iter: 1776 loss: 2.10521412e-05
Iter: 1777 loss: 2.10509679e-05
Iter: 1778 loss: 2.10357612e-05
Iter: 1779 loss: 2.10195321e-05
Iter: 1780 loss: 2.10010367e-05
Iter: 1781 loss: 2.09985519e-05
Iter: 1782 loss: 2.09803802e-05
Iter: 1783 loss: 2.09993032e-05
Iter: 1784 loss: 2.09705213e-05
Iter: 1785 loss: 2.09435166e-05
Iter: 1786 loss: 2.10718426e-05
Iter: 1787 loss: 2.09389909e-05
Iter: 1788 loss: 2.09211939e-05
Iter: 1789 loss: 2.11059232e-05
Iter: 1790 loss: 2.09206555e-05
Iter: 1791 loss: 2.09041173e-05
Iter: 1792 loss: 2.09002046e-05
Iter: 1793 loss: 2.08896508e-05
Iter: 1794 loss: 2.08709207e-05
Iter: 1795 loss: 2.09671562e-05
Iter: 1796 loss: 2.08678812e-05
Iter: 1797 loss: 2.08503388e-05
Iter: 1798 loss: 2.08392557e-05
Iter: 1799 loss: 2.08324309e-05
Iter: 1800 loss: 2.08058736e-05
Iter: 1801 loss: 2.08514411e-05
Iter: 1802 loss: 2.07942685e-05
Iter: 1803 loss: 2.076686e-05
Iter: 1804 loss: 2.10330945e-05
Iter: 1805 loss: 2.07659068e-05
Iter: 1806 loss: 2.07462072e-05
Iter: 1807 loss: 2.08892925e-05
Iter: 1808 loss: 2.07444318e-05
Iter: 1809 loss: 2.07320954e-05
Iter: 1810 loss: 2.07047433e-05
Iter: 1811 loss: 2.10837934e-05
Iter: 1812 loss: 2.07033263e-05
Iter: 1813 loss: 2.07210505e-05
Iter: 1814 loss: 2.06924196e-05
Iter: 1815 loss: 2.06863879e-05
Iter: 1816 loss: 2.0674117e-05
Iter: 1817 loss: 2.09075624e-05
Iter: 1818 loss: 2.06742061e-05
Iter: 1819 loss: 2.06521436e-05
Iter: 1820 loss: 2.06809236e-05
Iter: 1821 loss: 2.06408968e-05
Iter: 1822 loss: 2.06205841e-05
Iter: 1823 loss: 2.06204732e-05
Iter: 1824 loss: 2.06037039e-05
Iter: 1825 loss: 2.06517234e-05
Iter: 1826 loss: 2.05985089e-05
Iter: 1827 loss: 2.05856195e-05
Iter: 1828 loss: 2.06106706e-05
Iter: 1829 loss: 2.0579977e-05
Iter: 1830 loss: 2.05603865e-05
Iter: 1831 loss: 2.05591477e-05
Iter: 1832 loss: 2.0543881e-05
Iter: 1833 loss: 2.05188935e-05
Iter: 1834 loss: 2.0543288e-05
Iter: 1835 loss: 2.05045853e-05
Iter: 1836 loss: 2.04794906e-05
Iter: 1837 loss: 2.0483445e-05
Iter: 1838 loss: 2.04605458e-05
Iter: 1839 loss: 2.04382741e-05
Iter: 1840 loss: 2.06821824e-05
Iter: 1841 loss: 2.04379394e-05
Iter: 1842 loss: 2.04241678e-05
Iter: 1843 loss: 2.04025127e-05
Iter: 1844 loss: 2.04024e-05
Iter: 1845 loss: 2.03640629e-05
Iter: 1846 loss: 2.03712152e-05
Iter: 1847 loss: 2.0335794e-05
Iter: 1848 loss: 2.0420186e-05
Iter: 1849 loss: 2.03228101e-05
Iter: 1850 loss: 2.03134987e-05
Iter: 1851 loss: 2.0298e-05
Iter: 1852 loss: 2.0298e-05
Iter: 1853 loss: 2.02678257e-05
Iter: 1854 loss: 2.04252447e-05
Iter: 1855 loss: 2.02630581e-05
Iter: 1856 loss: 2.02500523e-05
Iter: 1857 loss: 2.0247051e-05
Iter: 1858 loss: 2.0237454e-05
Iter: 1859 loss: 2.02201882e-05
Iter: 1860 loss: 2.06496916e-05
Iter: 1861 loss: 2.02202737e-05
Iter: 1862 loss: 2.01963139e-05
Iter: 1863 loss: 2.03086602e-05
Iter: 1864 loss: 2.01919793e-05
Iter: 1865 loss: 2.0167161e-05
Iter: 1866 loss: 2.02042793e-05
Iter: 1867 loss: 2.01550029e-05
Iter: 1868 loss: 2.01325674e-05
Iter: 1869 loss: 2.01313032e-05
Iter: 1870 loss: 2.01142211e-05
Iter: 1871 loss: 2.008917e-05
Iter: 1872 loss: 2.01126222e-05
Iter: 1873 loss: 2.0074669e-05
Iter: 1874 loss: 2.00392969e-05
Iter: 1875 loss: 2.01931e-05
Iter: 1876 loss: 2.00321647e-05
Iter: 1877 loss: 2.00033101e-05
Iter: 1878 loss: 2.00352661e-05
Iter: 1879 loss: 1.99875158e-05
Iter: 1880 loss: 1.99508322e-05
Iter: 1881 loss: 2.01480416e-05
Iter: 1882 loss: 1.99455681e-05
Iter: 1883 loss: 1.99195129e-05
Iter: 1884 loss: 1.99798633e-05
Iter: 1885 loss: 1.99099068e-05
Iter: 1886 loss: 1.99049136e-05
Iter: 1887 loss: 1.98980633e-05
Iter: 1888 loss: 1.98884936e-05
Iter: 1889 loss: 1.98700436e-05
Iter: 1890 loss: 2.02353549e-05
Iter: 1891 loss: 1.98697471e-05
Iter: 1892 loss: 1.98489e-05
Iter: 1893 loss: 1.99078204e-05
Iter: 1894 loss: 1.98419857e-05
Iter: 1895 loss: 1.98252801e-05
Iter: 1896 loss: 1.98779453e-05
Iter: 1897 loss: 1.98204216e-05
Iter: 1898 loss: 1.98034249e-05
Iter: 1899 loss: 1.97958107e-05
Iter: 1900 loss: 1.97871195e-05
Iter: 1901 loss: 1.97628287e-05
Iter: 1902 loss: 1.98176476e-05
Iter: 1903 loss: 1.97538629e-05
Iter: 1904 loss: 1.97339123e-05
Iter: 1905 loss: 1.98941707e-05
Iter: 1906 loss: 1.97328191e-05
Iter: 1907 loss: 1.97143218e-05
Iter: 1908 loss: 1.97128356e-05
Iter: 1909 loss: 1.96992714e-05
Iter: 1910 loss: 1.96760629e-05
Iter: 1911 loss: 1.97685713e-05
Iter: 1912 loss: 1.96707661e-05
Iter: 1913 loss: 1.96490637e-05
Iter: 1914 loss: 1.96601395e-05
Iter: 1915 loss: 1.963459e-05
Iter: 1916 loss: 1.96091605e-05
Iter: 1917 loss: 1.9648669e-05
Iter: 1918 loss: 1.95971515e-05
Iter: 1919 loss: 1.95746434e-05
Iter: 1920 loss: 1.99173483e-05
Iter: 1921 loss: 1.95748726e-05
Iter: 1922 loss: 1.95589528e-05
Iter: 1923 loss: 1.97684167e-05
Iter: 1924 loss: 1.9559e-05
Iter: 1925 loss: 1.954861e-05
Iter: 1926 loss: 1.95334651e-05
Iter: 1927 loss: 1.95332141e-05
Iter: 1928 loss: 1.95118664e-05
Iter: 1929 loss: 1.96167839e-05
Iter: 1930 loss: 1.95080866e-05
Iter: 1931 loss: 1.94970198e-05
Iter: 1932 loss: 1.94784334e-05
Iter: 1933 loss: 1.94785152e-05
Iter: 1934 loss: 1.94674376e-05
Iter: 1935 loss: 1.94624499e-05
Iter: 1936 loss: 1.94509958e-05
Iter: 1937 loss: 1.94657241e-05
Iter: 1938 loss: 1.94450022e-05
Iter: 1939 loss: 1.94230106e-05
Iter: 1940 loss: 1.9484105e-05
Iter: 1941 loss: 1.94160984e-05
Iter: 1942 loss: 1.94003078e-05
Iter: 1943 loss: 1.93997985e-05
Iter: 1944 loss: 1.93882061e-05
Iter: 1945 loss: 1.93621963e-05
Iter: 1946 loss: 1.97282279e-05
Iter: 1947 loss: 1.9360994e-05
Iter: 1948 loss: 1.93401283e-05
Iter: 1949 loss: 1.93388041e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
