+ RUN=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ LAYERS=300_300_300_1
+ case $RUN in
+ PSI=-1
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f2
+ case $fn in
+ OPT=--alpha
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi0
+ date
Wed Nov  4 10:45:08 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f2 --psi -1 --alpha 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c0d6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c0ce950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c16ed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c16eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c12a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c12ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c12aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c16ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c075400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c075c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0707ea400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0707dbbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0707dbae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07079bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07076dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07076d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c01f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07076d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c037d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0706569d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd070656598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0705dea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0706bf378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0706d6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0706f8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd070582d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd070599d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd070582730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0706bf8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd070577268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0706b4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0704739d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd070473e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0704259d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0703cfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0703ceea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0032118955
test_loss: 0.003353792
train_loss: 0.0028512878
test_loss: 0.0026291416
train_loss: 0.0023016415
test_loss: 0.002361026
train_loss: 0.0023573453
test_loss: 0.0026904023
train_loss: 0.002429064
test_loss: 0.0022737938
train_loss: 0.0023995324
test_loss: 0.0023048315
train_loss: 0.0020962732
test_loss: 0.002123617
train_loss: 0.002051036
test_loss: 0.0020420318
train_loss: 0.0022154003
test_loss: 0.0023550694
train_loss: 0.0021384112
test_loss: 0.0020509292
train_loss: 0.0020756682
test_loss: 0.0021555366
train_loss: 0.0020234282
test_loss: 0.0020103154
train_loss: 0.0024181963
test_loss: 0.0023161825
train_loss: 0.002133666
test_loss: 0.0020990686
train_loss: 0.0020554767
test_loss: 0.0023016313
train_loss: 0.0018088782
test_loss: 0.0018522129
train_loss: 0.0019406022
test_loss: 0.0019388653
train_loss: 0.0020061075
test_loss: 0.001910428
train_loss: 0.0018612552
test_loss: 0.0019666941
train_loss: 0.0019409314
test_loss: 0.00205051
train_loss: 0.0017611043
test_loss: 0.0020020162
train_loss: 0.002065941
test_loss: 0.002173836
train_loss: 0.00201622
test_loss: 0.0021636998
train_loss: 0.0023606396
test_loss: 0.0020825467
train_loss: 0.0017949819
test_loss: 0.0020222932
train_loss: 0.001697123
test_loss: 0.0018441364
train_loss: 0.001846872
test_loss: 0.0019096762
train_loss: 0.0019018308
test_loss: 0.0020121466
train_loss: 0.001721202
test_loss: 0.0019132473
train_loss: 0.0018311695
test_loss: 0.0018498003
train_loss: 0.0017630205
test_loss: 0.0019993086
train_loss: 0.0017305267
test_loss: 0.0017922726
train_loss: 0.0016571389
test_loss: 0.0019284193
train_loss: 0.0019411661
test_loss: 0.0019802412
train_loss: 0.0017618458
test_loss: 0.0019545238
train_loss: 0.0016655191
test_loss: 0.0017063734
train_loss: 0.0016840945
test_loss: 0.0017464862
train_loss: 0.0018942277
test_loss: 0.0018879423
train_loss: 0.0017241843
test_loss: 0.001781714
train_loss: 0.0017622395
test_loss: 0.0017859746
train_loss: 0.001933459
test_loss: 0.0019428075
train_loss: 0.0018770006
test_loss: 0.0020803688
train_loss: 0.0017431793
test_loss: 0.0018540744
train_loss: 0.0020296094
test_loss: 0.0020789858
train_loss: 0.0016696363
test_loss: 0.0018613967
train_loss: 0.0015869348
test_loss: 0.0017059714
train_loss: 0.0017250872
test_loss: 0.0018437129
train_loss: 0.0019948364
test_loss: 0.0019690732
train_loss: 0.0016667724
test_loss: 0.0018071879
train_loss: 0.0015441391
test_loss: 0.0016908279
train_loss: 0.0016340147
test_loss: 0.0017790804
train_loss: 0.0023202617
test_loss: 0.001922969
train_loss: 0.0017774063
test_loss: 0.00198575
train_loss: 0.0016233849
test_loss: 0.001902658
train_loss: 0.0016745635
test_loss: 0.0017659094
train_loss: 0.0016329223
test_loss: 0.0020369452
train_loss: 0.0017352196
test_loss: 0.0017033346
train_loss: 0.0016111936
test_loss: 0.0016576035
train_loss: 0.0018800204
test_loss: 0.0018253057
train_loss: 0.0016052499
test_loss: 0.001930473
train_loss: 0.001640661
test_loss: 0.0016951228
train_loss: 0.0016200589
test_loss: 0.0017970486
train_loss: 0.0015825627
test_loss: 0.0017533523
train_loss: 0.0017242964
test_loss: 0.0020195858
train_loss: 0.0015439083
test_loss: 0.001608757
train_loss: 0.001520945
test_loss: 0.0016784018
train_loss: 0.0016819315
test_loss: 0.0018180893
train_loss: 0.0016937964
test_loss: 0.0017892286
train_loss: 0.0014658307
test_loss: 0.0017616535
train_loss: 0.0014715653
test_loss: 0.0017917792
train_loss: 0.0018218715
test_loss: 0.0018070866
train_loss: 0.0016473163
test_loss: 0.0019107062
train_loss: 0.0017624191
test_loss: 0.002254823
train_loss: 0.0016284082
test_loss: 0.0018139983
train_loss: 0.0014919388
test_loss: 0.0018021949
train_loss: 0.001484083
test_loss: 0.0016965052
train_loss: 0.0014522475
test_loss: 0.0015982898
train_loss: 0.0015705682
test_loss: 0.0016919054
train_loss: 0.001564983
test_loss: 0.001710447
train_loss: 0.0018931474
test_loss: 0.001760312
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0/300_300_300_1 --optimizer lbfgs --function f2 --psi -1 --alpha 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi0/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248830ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f424892a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f424892abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248897268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248894268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248894620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42488aa8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248894ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42488aab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42487718c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248749378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248807d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42486f0ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f424871b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f424871bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f424867ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42486887b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248688a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248688c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248666ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248666f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42485df6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42485df730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f424861c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42485a4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42485a4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248563e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248519d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248519bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42484e92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248519158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4248449d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42484496a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f424840dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f423d26f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f423d273f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.26656607e-06
Iter: 2 loss: 3.01414229e-06
Iter: 3 loss: 1.73828394e-05
Iter: 4 loss: 2.99435305e-06
Iter: 5 loss: 2.73237902e-06
Iter: 6 loss: 2.73222713e-06
Iter: 7 loss: 2.58293926e-06
Iter: 8 loss: 2.55332475e-06
Iter: 9 loss: 2.45413867e-06
Iter: 10 loss: 2.3643338e-06
Iter: 11 loss: 2.36255937e-06
Iter: 12 loss: 2.29028683e-06
Iter: 13 loss: 2.2265815e-06
Iter: 14 loss: 2.20771631e-06
Iter: 15 loss: 2.16974058e-06
Iter: 16 loss: 2.30291039e-06
Iter: 17 loss: 2.15991486e-06
Iter: 18 loss: 2.09538803e-06
Iter: 19 loss: 2.05946344e-06
Iter: 20 loss: 2.03110471e-06
Iter: 21 loss: 1.95285293e-06
Iter: 22 loss: 1.8776168e-06
Iter: 23 loss: 1.86022896e-06
Iter: 24 loss: 1.89537957e-06
Iter: 25 loss: 1.81234032e-06
Iter: 26 loss: 1.78678158e-06
Iter: 27 loss: 1.72953173e-06
Iter: 28 loss: 2.51936353e-06
Iter: 29 loss: 1.72632588e-06
Iter: 30 loss: 1.68683459e-06
Iter: 31 loss: 1.72007356e-06
Iter: 32 loss: 1.66344773e-06
Iter: 33 loss: 1.6265144e-06
Iter: 34 loss: 1.7128724e-06
Iter: 35 loss: 1.61295895e-06
Iter: 36 loss: 1.56606836e-06
Iter: 37 loss: 1.80596635e-06
Iter: 38 loss: 1.558441e-06
Iter: 39 loss: 1.50349615e-06
Iter: 40 loss: 1.85096951e-06
Iter: 41 loss: 1.49726714e-06
Iter: 42 loss: 1.4607574e-06
Iter: 43 loss: 1.54956797e-06
Iter: 44 loss: 1.44776777e-06
Iter: 45 loss: 1.42421936e-06
Iter: 46 loss: 1.68963811e-06
Iter: 47 loss: 1.42380884e-06
Iter: 48 loss: 1.40342968e-06
Iter: 49 loss: 1.36601147e-06
Iter: 50 loss: 2.2356935e-06
Iter: 51 loss: 1.36597782e-06
Iter: 52 loss: 1.34068182e-06
Iter: 53 loss: 1.47723188e-06
Iter: 54 loss: 1.33690037e-06
Iter: 55 loss: 1.31030811e-06
Iter: 56 loss: 1.50995152e-06
Iter: 57 loss: 1.30820445e-06
Iter: 58 loss: 1.29518787e-06
Iter: 59 loss: 1.26107273e-06
Iter: 60 loss: 1.52624489e-06
Iter: 61 loss: 1.25452118e-06
Iter: 62 loss: 1.24833014e-06
Iter: 63 loss: 1.2342183e-06
Iter: 64 loss: 1.21534731e-06
Iter: 65 loss: 1.17307582e-06
Iter: 66 loss: 1.76148626e-06
Iter: 67 loss: 1.17083096e-06
Iter: 68 loss: 1.14157569e-06
Iter: 69 loss: 1.18733044e-06
Iter: 70 loss: 1.12787166e-06
Iter: 71 loss: 1.10190581e-06
Iter: 72 loss: 1.14813201e-06
Iter: 73 loss: 1.09054417e-06
Iter: 74 loss: 1.07712663e-06
Iter: 75 loss: 1.07387689e-06
Iter: 76 loss: 1.05690378e-06
Iter: 77 loss: 1.07672042e-06
Iter: 78 loss: 1.04787409e-06
Iter: 79 loss: 1.0341588e-06
Iter: 80 loss: 1.10571273e-06
Iter: 81 loss: 1.03203138e-06
Iter: 82 loss: 1.01834576e-06
Iter: 83 loss: 1.04642731e-06
Iter: 84 loss: 1.01281421e-06
Iter: 85 loss: 9.98251e-07
Iter: 86 loss: 9.8801388e-07
Iter: 87 loss: 9.8280907e-07
Iter: 88 loss: 9.77245577e-07
Iter: 89 loss: 9.7689076e-07
Iter: 90 loss: 9.70254e-07
Iter: 91 loss: 9.641e-07
Iter: 92 loss: 9.625046e-07
Iter: 93 loss: 9.52848609e-07
Iter: 94 loss: 9.42874749e-07
Iter: 95 loss: 9.41060136e-07
Iter: 96 loss: 9.34987952e-07
Iter: 97 loss: 9.31277327e-07
Iter: 98 loss: 9.25638858e-07
Iter: 99 loss: 9.09069854e-07
Iter: 100 loss: 9.72546559e-07
Iter: 101 loss: 9.02075726e-07
Iter: 102 loss: 8.849305e-07
Iter: 103 loss: 9.5799669e-07
Iter: 104 loss: 8.81334188e-07
Iter: 105 loss: 8.69530368e-07
Iter: 106 loss: 9.71770419e-07
Iter: 107 loss: 8.68883546e-07
Iter: 108 loss: 8.68377754e-07
Iter: 109 loss: 8.65061111e-07
Iter: 110 loss: 8.62403795e-07
Iter: 111 loss: 8.55316841e-07
Iter: 112 loss: 9.06882292e-07
Iter: 113 loss: 8.53781387e-07
Iter: 114 loss: 8.43886312e-07
Iter: 115 loss: 9.45626141e-07
Iter: 116 loss: 8.43594648e-07
Iter: 117 loss: 8.35383844e-07
Iter: 118 loss: 8.56000838e-07
Iter: 119 loss: 8.32482442e-07
Iter: 120 loss: 8.27918768e-07
Iter: 121 loss: 8.21176513e-07
Iter: 122 loss: 8.21032586e-07
Iter: 123 loss: 8.160211e-07
Iter: 124 loss: 8.1529339e-07
Iter: 125 loss: 8.11132793e-07
Iter: 126 loss: 8.02881402e-07
Iter: 127 loss: 9.64209221e-07
Iter: 128 loss: 8.02787497e-07
Iter: 129 loss: 7.97809378e-07
Iter: 130 loss: 8.52048174e-07
Iter: 131 loss: 7.97719508e-07
Iter: 132 loss: 7.91327807e-07
Iter: 133 loss: 7.93360243e-07
Iter: 134 loss: 7.86796249e-07
Iter: 135 loss: 7.81734798e-07
Iter: 136 loss: 7.74820478e-07
Iter: 137 loss: 7.7447578e-07
Iter: 138 loss: 7.67343863e-07
Iter: 139 loss: 8.01659326e-07
Iter: 140 loss: 7.66053745e-07
Iter: 141 loss: 7.61248316e-07
Iter: 142 loss: 8.00756311e-07
Iter: 143 loss: 7.60971e-07
Iter: 144 loss: 7.54563189e-07
Iter: 145 loss: 7.64306719e-07
Iter: 146 loss: 7.51544803e-07
Iter: 147 loss: 7.47414333e-07
Iter: 148 loss: 7.59122827e-07
Iter: 149 loss: 7.46069247e-07
Iter: 150 loss: 7.4126666e-07
Iter: 151 loss: 7.67531048e-07
Iter: 152 loss: 7.40549638e-07
Iter: 153 loss: 7.36942184e-07
Iter: 154 loss: 7.34066134e-07
Iter: 155 loss: 7.32955186e-07
Iter: 156 loss: 7.27656e-07
Iter: 157 loss: 7.43492819e-07
Iter: 158 loss: 7.26029725e-07
Iter: 159 loss: 7.20927119e-07
Iter: 160 loss: 7.80158416e-07
Iter: 161 loss: 7.2083651e-07
Iter: 162 loss: 7.18727392e-07
Iter: 163 loss: 7.12896849e-07
Iter: 164 loss: 7.50510821e-07
Iter: 165 loss: 7.11455755e-07
Iter: 166 loss: 7.10781194e-07
Iter: 167 loss: 7.07890308e-07
Iter: 168 loss: 7.05095772e-07
Iter: 169 loss: 6.98305598e-07
Iter: 170 loss: 7.70192344e-07
Iter: 171 loss: 6.97533778e-07
Iter: 172 loss: 6.91993591e-07
Iter: 173 loss: 7.09610958e-07
Iter: 174 loss: 6.90376453e-07
Iter: 175 loss: 6.86811632e-07
Iter: 176 loss: 7.0232295e-07
Iter: 177 loss: 6.86057092e-07
Iter: 178 loss: 6.83967642e-07
Iter: 179 loss: 6.8367e-07
Iter: 180 loss: 6.82247332e-07
Iter: 181 loss: 6.79189839e-07
Iter: 182 loss: 7.31333444e-07
Iter: 183 loss: 6.79117875e-07
Iter: 184 loss: 6.77042635e-07
Iter: 185 loss: 6.76914908e-07
Iter: 186 loss: 6.74964838e-07
Iter: 187 loss: 6.71901034e-07
Iter: 188 loss: 6.71860903e-07
Iter: 189 loss: 6.67783581e-07
Iter: 190 loss: 6.73344346e-07
Iter: 191 loss: 6.65744324e-07
Iter: 192 loss: 6.64043e-07
Iter: 193 loss: 6.63524418e-07
Iter: 194 loss: 6.62052344e-07
Iter: 195 loss: 6.59347393e-07
Iter: 196 loss: 7.23177e-07
Iter: 197 loss: 6.59326247e-07
Iter: 198 loss: 6.57284488e-07
Iter: 199 loss: 6.68683356e-07
Iter: 200 loss: 6.57007e-07
Iter: 201 loss: 6.54206872e-07
Iter: 202 loss: 6.58318527e-07
Iter: 203 loss: 6.52859171e-07
Iter: 204 loss: 6.50904099e-07
Iter: 205 loss: 6.47249124e-07
Iter: 206 loss: 7.28240593e-07
Iter: 207 loss: 6.47247248e-07
Iter: 208 loss: 6.42682721e-07
Iter: 209 loss: 6.51025687e-07
Iter: 210 loss: 6.40714916e-07
Iter: 211 loss: 6.42290843e-07
Iter: 212 loss: 6.39249095e-07
Iter: 213 loss: 6.37895369e-07
Iter: 214 loss: 6.36340758e-07
Iter: 215 loss: 6.36167158e-07
Iter: 216 loss: 6.35042625e-07
Iter: 217 loss: 6.52435062e-07
Iter: 218 loss: 6.35034723e-07
Iter: 219 loss: 6.33882337e-07
Iter: 220 loss: 6.33041452e-07
Iter: 221 loss: 6.32684191e-07
Iter: 222 loss: 6.30786872e-07
Iter: 223 loss: 6.30569787e-07
Iter: 224 loss: 6.29218789e-07
Iter: 225 loss: 6.26679764e-07
Iter: 226 loss: 6.47293405e-07
Iter: 227 loss: 6.26515885e-07
Iter: 228 loss: 6.24005679e-07
Iter: 229 loss: 6.30455247e-07
Iter: 230 loss: 6.23134497e-07
Iter: 231 loss: 6.21665549e-07
Iter: 232 loss: 6.18908302e-07
Iter: 233 loss: 6.79734399e-07
Iter: 234 loss: 6.18886816e-07
Iter: 235 loss: 6.17621765e-07
Iter: 236 loss: 6.16882176e-07
Iter: 237 loss: 6.15984789e-07
Iter: 238 loss: 6.1412095e-07
Iter: 239 loss: 6.4586834e-07
Iter: 240 loss: 6.14066721e-07
Iter: 241 loss: 6.12078395e-07
Iter: 242 loss: 6.08504763e-07
Iter: 243 loss: 6.08505047e-07
Iter: 244 loss: 6.07320089e-07
Iter: 245 loss: 6.06268543e-07
Iter: 246 loss: 6.0386617e-07
Iter: 247 loss: 6.12322538e-07
Iter: 248 loss: 6.03260105e-07
Iter: 249 loss: 6.02061e-07
Iter: 250 loss: 6.02815703e-07
Iter: 251 loss: 6.01317197e-07
Iter: 252 loss: 5.99362579e-07
Iter: 253 loss: 6.06348067e-07
Iter: 254 loss: 5.98847578e-07
Iter: 255 loss: 5.9758e-07
Iter: 256 loss: 5.97370104e-07
Iter: 257 loss: 5.96471864e-07
Iter: 258 loss: 5.94914866e-07
Iter: 259 loss: 6.04023455e-07
Iter: 260 loss: 5.947403e-07
Iter: 261 loss: 5.93453819e-07
Iter: 262 loss: 6.04535785e-07
Iter: 263 loss: 5.93396862e-07
Iter: 264 loss: 5.92514652e-07
Iter: 265 loss: 5.90026559e-07
Iter: 266 loss: 6.03883166e-07
Iter: 267 loss: 5.8932028e-07
Iter: 268 loss: 5.8993021e-07
Iter: 269 loss: 5.8815283e-07
Iter: 270 loss: 5.87237651e-07
Iter: 271 loss: 5.85819237e-07
Iter: 272 loss: 5.85811279e-07
Iter: 273 loss: 5.84378199e-07
Iter: 274 loss: 5.81585709e-07
Iter: 275 loss: 6.37380197e-07
Iter: 276 loss: 5.8157093e-07
Iter: 277 loss: 5.79289804e-07
Iter: 278 loss: 5.79289576e-07
Iter: 279 loss: 5.78429308e-07
Iter: 280 loss: 5.78184313e-07
Iter: 281 loss: 5.7751987e-07
Iter: 282 loss: 5.75836566e-07
Iter: 283 loss: 5.88955174e-07
Iter: 284 loss: 5.7549903e-07
Iter: 285 loss: 5.74408034e-07
Iter: 286 loss: 5.74169803e-07
Iter: 287 loss: 5.73486773e-07
Iter: 288 loss: 5.72289878e-07
Iter: 289 loss: 5.72284762e-07
Iter: 290 loss: 5.70802399e-07
Iter: 291 loss: 5.72497413e-07
Iter: 292 loss: 5.70015e-07
Iter: 293 loss: 5.68121209e-07
Iter: 294 loss: 5.85661041e-07
Iter: 295 loss: 5.68027076e-07
Iter: 296 loss: 5.66321205e-07
Iter: 297 loss: 5.65354071e-07
Iter: 298 loss: 5.64603624e-07
Iter: 299 loss: 5.63352273e-07
Iter: 300 loss: 5.71989403e-07
Iter: 301 loss: 5.63239155e-07
Iter: 302 loss: 5.61947672e-07
Iter: 303 loss: 5.67411348e-07
Iter: 304 loss: 5.61652598e-07
Iter: 305 loss: 5.60864351e-07
Iter: 306 loss: 5.59422574e-07
Iter: 307 loss: 5.92067749e-07
Iter: 308 loss: 5.59428372e-07
Iter: 309 loss: 5.58033605e-07
Iter: 310 loss: 5.5834505e-07
Iter: 311 loss: 5.57018723e-07
Iter: 312 loss: 5.56569034e-07
Iter: 313 loss: 5.56037833e-07
Iter: 314 loss: 5.5494e-07
Iter: 315 loss: 5.53532e-07
Iter: 316 loss: 5.53429174e-07
Iter: 317 loss: 5.52407187e-07
Iter: 318 loss: 5.52406675e-07
Iter: 319 loss: 5.51430844e-07
Iter: 320 loss: 5.50328934e-07
Iter: 321 loss: 5.50206209e-07
Iter: 322 loss: 5.49318202e-07
Iter: 323 loss: 5.52050551e-07
Iter: 324 loss: 5.49074571e-07
Iter: 325 loss: 5.48178775e-07
Iter: 326 loss: 5.52893596e-07
Iter: 327 loss: 5.48024786e-07
Iter: 328 loss: 5.47005811e-07
Iter: 329 loss: 5.49283527e-07
Iter: 330 loss: 5.46628598e-07
Iter: 331 loss: 5.45887076e-07
Iter: 332 loss: 5.45382875e-07
Iter: 333 loss: 5.45098885e-07
Iter: 334 loss: 5.44211048e-07
Iter: 335 loss: 5.44188e-07
Iter: 336 loss: 5.43429906e-07
Iter: 337 loss: 5.41800205e-07
Iter: 338 loss: 5.66449103e-07
Iter: 339 loss: 5.41724489e-07
Iter: 340 loss: 5.40436417e-07
Iter: 341 loss: 5.42791099e-07
Iter: 342 loss: 5.39889129e-07
Iter: 343 loss: 5.3896332e-07
Iter: 344 loss: 5.48402113e-07
Iter: 345 loss: 5.38928361e-07
Iter: 346 loss: 5.38027905e-07
Iter: 347 loss: 5.45830289e-07
Iter: 348 loss: 5.37962478e-07
Iter: 349 loss: 5.37393e-07
Iter: 350 loss: 5.36702885e-07
Iter: 351 loss: 5.36631774e-07
Iter: 352 loss: 5.35307436e-07
Iter: 353 loss: 5.42936107e-07
Iter: 354 loss: 5.35128095e-07
Iter: 355 loss: 5.34572e-07
Iter: 356 loss: 5.33632772e-07
Iter: 357 loss: 5.57266389e-07
Iter: 358 loss: 5.33623563e-07
Iter: 359 loss: 5.32706053e-07
Iter: 360 loss: 5.45709895e-07
Iter: 361 loss: 5.32699914e-07
Iter: 362 loss: 5.32014496e-07
Iter: 363 loss: 5.36503762e-07
Iter: 364 loss: 5.31941851e-07
Iter: 365 loss: 5.31464934e-07
Iter: 366 loss: 5.31305602e-07
Iter: 367 loss: 5.31023488e-07
Iter: 368 loss: 5.30468242e-07
Iter: 369 loss: 5.33575587e-07
Iter: 370 loss: 5.30380248e-07
Iter: 371 loss: 5.2959075e-07
Iter: 372 loss: 5.28573082e-07
Iter: 373 loss: 5.28503563e-07
Iter: 374 loss: 5.272978e-07
Iter: 375 loss: 5.27336681e-07
Iter: 376 loss: 5.26324868e-07
Iter: 377 loss: 5.25301687e-07
Iter: 378 loss: 5.30708576e-07
Iter: 379 loss: 5.25156224e-07
Iter: 380 loss: 5.25196299e-07
Iter: 381 loss: 5.24779693e-07
Iter: 382 loss: 5.24504912e-07
Iter: 383 loss: 5.23887536e-07
Iter: 384 loss: 5.32590832e-07
Iter: 385 loss: 5.23863378e-07
Iter: 386 loss: 5.23453082e-07
Iter: 387 loss: 5.23413291e-07
Iter: 388 loss: 5.23137373e-07
Iter: 389 loss: 5.22336e-07
Iter: 390 loss: 5.2469403e-07
Iter: 391 loss: 5.21904553e-07
Iter: 392 loss: 5.20616e-07
Iter: 393 loss: 5.28859857e-07
Iter: 394 loss: 5.20481308e-07
Iter: 395 loss: 5.19345804e-07
Iter: 396 loss: 5.29870192e-07
Iter: 397 loss: 5.19291802e-07
Iter: 398 loss: 5.18525894e-07
Iter: 399 loss: 5.19834202e-07
Iter: 400 loss: 5.18189665e-07
Iter: 401 loss: 5.17604121e-07
Iter: 402 loss: 5.19075172e-07
Iter: 403 loss: 5.17429e-07
Iter: 404 loss: 5.16742318e-07
Iter: 405 loss: 5.20479944e-07
Iter: 406 loss: 5.16627665e-07
Iter: 407 loss: 5.16196678e-07
Iter: 408 loss: 5.15256488e-07
Iter: 409 loss: 5.28746909e-07
Iter: 410 loss: 5.15212946e-07
Iter: 411 loss: 5.14246381e-07
Iter: 412 loss: 5.15327088e-07
Iter: 413 loss: 5.13707505e-07
Iter: 414 loss: 5.12916131e-07
Iter: 415 loss: 5.12880092e-07
Iter: 416 loss: 5.11952294e-07
Iter: 417 loss: 5.13797e-07
Iter: 418 loss: 5.11589235e-07
Iter: 419 loss: 5.11238056e-07
Iter: 420 loss: 5.14453689e-07
Iter: 421 loss: 5.11209578e-07
Iter: 422 loss: 5.10806274e-07
Iter: 423 loss: 5.10058271e-07
Iter: 424 loss: 5.26186739e-07
Iter: 425 loss: 5.10057e-07
Iter: 426 loss: 5.09401502e-07
Iter: 427 loss: 5.10254267e-07
Iter: 428 loss: 5.0905885e-07
Iter: 429 loss: 5.08358198e-07
Iter: 430 loss: 5.19253945e-07
Iter: 431 loss: 5.08358937e-07
Iter: 432 loss: 5.07797267e-07
Iter: 433 loss: 5.08006906e-07
Iter: 434 loss: 5.07385835e-07
Iter: 435 loss: 5.06782612e-07
Iter: 436 loss: 5.08625931e-07
Iter: 437 loss: 5.0658366e-07
Iter: 438 loss: 5.06216338e-07
Iter: 439 loss: 5.12375e-07
Iter: 440 loss: 5.06204515e-07
Iter: 441 loss: 5.05914159e-07
Iter: 442 loss: 5.05176388e-07
Iter: 443 loss: 5.11586e-07
Iter: 444 loss: 5.05081744e-07
Iter: 445 loss: 5.04263312e-07
Iter: 446 loss: 5.05911e-07
Iter: 447 loss: 5.03951071e-07
Iter: 448 loss: 5.03077331e-07
Iter: 449 loss: 5.04280081e-07
Iter: 450 loss: 5.0265453e-07
Iter: 451 loss: 5.02524472e-07
Iter: 452 loss: 5.02121e-07
Iter: 453 loss: 5.01869636e-07
Iter: 454 loss: 5.01384932e-07
Iter: 455 loss: 5.12503902e-07
Iter: 456 loss: 5.01378963e-07
Iter: 457 loss: 5.00874535e-07
Iter: 458 loss: 5.08181813e-07
Iter: 459 loss: 5.00866406e-07
Iter: 460 loss: 5.00647843e-07
Iter: 461 loss: 5.00083161e-07
Iter: 462 loss: 5.03545152e-07
Iter: 463 loss: 4.9991246e-07
Iter: 464 loss: 4.99086354e-07
Iter: 465 loss: 5.01783461e-07
Iter: 466 loss: 4.98866825e-07
Iter: 467 loss: 4.98432541e-07
Iter: 468 loss: 4.98282247e-07
Iter: 469 loss: 4.98026793e-07
Iter: 470 loss: 4.9763878e-07
Iter: 471 loss: 4.97632698e-07
Iter: 472 loss: 4.97159192e-07
Iter: 473 loss: 5.02098487e-07
Iter: 474 loss: 4.97136284e-07
Iter: 475 loss: 4.96729285e-07
Iter: 476 loss: 4.96914481e-07
Iter: 477 loss: 4.96444e-07
Iter: 478 loss: 4.9605e-07
Iter: 479 loss: 4.95585141e-07
Iter: 480 loss: 4.95568315e-07
Iter: 481 loss: 4.9490086e-07
Iter: 482 loss: 4.94541439e-07
Iter: 483 loss: 4.9425006e-07
Iter: 484 loss: 4.93498419e-07
Iter: 485 loss: 4.93471e-07
Iter: 486 loss: 4.92677373e-07
Iter: 487 loss: 4.96449616e-07
Iter: 488 loss: 4.92538561e-07
Iter: 489 loss: 4.92245135e-07
Iter: 490 loss: 4.92998311e-07
Iter: 491 loss: 4.92141794e-07
Iter: 492 loss: 4.91698643e-07
Iter: 493 loss: 4.91422441e-07
Iter: 494 loss: 4.91254241e-07
Iter: 495 loss: 4.9091625e-07
Iter: 496 loss: 4.90608954e-07
Iter: 497 loss: 4.90532557e-07
Iter: 498 loss: 4.90064565e-07
Iter: 499 loss: 4.95842528e-07
Iter: 500 loss: 4.90043931e-07
Iter: 501 loss: 4.89631e-07
Iter: 502 loss: 4.91918e-07
Iter: 503 loss: 4.89531374e-07
Iter: 504 loss: 4.89186846e-07
Iter: 505 loss: 4.8856316e-07
Iter: 506 loss: 5.03111437e-07
Iter: 507 loss: 4.88570663e-07
Iter: 508 loss: 4.8799069e-07
Iter: 509 loss: 4.87974376e-07
Iter: 510 loss: 4.8755328e-07
Iter: 511 loss: 4.8774308e-07
Iter: 512 loss: 4.87266618e-07
Iter: 513 loss: 4.86930105e-07
Iter: 514 loss: 4.8667556e-07
Iter: 515 loss: 4.86570912e-07
Iter: 516 loss: 4.85971668e-07
Iter: 517 loss: 4.86256681e-07
Iter: 518 loss: 4.85566659e-07
Iter: 519 loss: 4.85682961e-07
Iter: 520 loss: 4.85356566e-07
Iter: 521 loss: 4.8509753e-07
Iter: 522 loss: 4.8476096e-07
Iter: 523 loss: 4.84739871e-07
Iter: 524 loss: 4.84478392e-07
Iter: 525 loss: 4.87572265e-07
Iter: 526 loss: 4.8445861e-07
Iter: 527 loss: 4.84176439e-07
Iter: 528 loss: 4.84065595e-07
Iter: 529 loss: 4.83895178e-07
Iter: 530 loss: 4.83649046e-07
Iter: 531 loss: 4.83168208e-07
Iter: 532 loss: 4.9331436e-07
Iter: 533 loss: 4.83167071e-07
Iter: 534 loss: 4.8261e-07
Iter: 535 loss: 4.87941747e-07
Iter: 536 loss: 4.82586358e-07
Iter: 537 loss: 4.82002179e-07
Iter: 538 loss: 4.85479518e-07
Iter: 539 loss: 4.8191913e-07
Iter: 540 loss: 4.81538734e-07
Iter: 541 loss: 4.81321877e-07
Iter: 542 loss: 4.811601e-07
Iter: 543 loss: 4.80859285e-07
Iter: 544 loss: 4.80844506e-07
Iter: 545 loss: 4.80618269e-07
Iter: 546 loss: 4.80703704e-07
Iter: 547 loss: 4.80440349e-07
Iter: 548 loss: 4.80190067e-07
Iter: 549 loss: 4.80011295e-07
Iter: 550 loss: 4.7993035e-07
Iter: 551 loss: 4.79559e-07
Iter: 552 loss: 4.80009e-07
Iter: 553 loss: 4.79368907e-07
Iter: 554 loss: 4.789473e-07
Iter: 555 loss: 4.78740731e-07
Iter: 556 loss: 4.78521599e-07
Iter: 557 loss: 4.78675418e-07
Iter: 558 loss: 4.78196625e-07
Iter: 559 loss: 4.78050197e-07
Iter: 560 loss: 4.77686228e-07
Iter: 561 loss: 4.81255256e-07
Iter: 562 loss: 4.77639105e-07
Iter: 563 loss: 4.77391552e-07
Iter: 564 loss: 4.77391382e-07
Iter: 565 loss: 4.77094261e-07
Iter: 566 loss: 4.76696187e-07
Iter: 567 loss: 4.76674813e-07
Iter: 568 loss: 4.76362487e-07
Iter: 569 loss: 4.77002573e-07
Iter: 570 loss: 4.76246498e-07
Iter: 571 loss: 4.75828074e-07
Iter: 572 loss: 4.78534844e-07
Iter: 573 loss: 4.75745281e-07
Iter: 574 loss: 4.7548815e-07
Iter: 575 loss: 4.75378783e-07
Iter: 576 loss: 4.75228916e-07
Iter: 577 loss: 4.75032323e-07
Iter: 578 loss: 4.75024649e-07
Iter: 579 loss: 4.74846729e-07
Iter: 580 loss: 4.74536876e-07
Iter: 581 loss: 4.74542929e-07
Iter: 582 loss: 4.74251465e-07
Iter: 583 loss: 4.74442629e-07
Iter: 584 loss: 4.74065473e-07
Iter: 585 loss: 4.73671633e-07
Iter: 586 loss: 4.73551665e-07
Iter: 587 loss: 4.73328271e-07
Iter: 588 loss: 4.7288512e-07
Iter: 589 loss: 4.72885063e-07
Iter: 590 loss: 4.724061e-07
Iter: 591 loss: 4.72572367e-07
Iter: 592 loss: 4.72084906e-07
Iter: 593 loss: 4.71777241e-07
Iter: 594 loss: 4.72951285e-07
Iter: 595 loss: 4.71718124e-07
Iter: 596 loss: 4.7139909e-07
Iter: 597 loss: 4.73269381e-07
Iter: 598 loss: 4.7137695e-07
Iter: 599 loss: 4.7121938e-07
Iter: 600 loss: 4.71016847e-07
Iter: 601 loss: 4.71012299e-07
Iter: 602 loss: 4.70826734e-07
Iter: 603 loss: 4.70816474e-07
Iter: 604 loss: 4.70693038e-07
Iter: 605 loss: 4.704508e-07
Iter: 606 loss: 4.75741302e-07
Iter: 607 loss: 4.70448498e-07
Iter: 608 loss: 4.70174143e-07
Iter: 609 loss: 4.7282316e-07
Iter: 610 loss: 4.70171756e-07
Iter: 611 loss: 4.69969734e-07
Iter: 612 loss: 4.69672415e-07
Iter: 613 loss: 4.69640696e-07
Iter: 614 loss: 4.69185096e-07
Iter: 615 loss: 4.69684551e-07
Iter: 616 loss: 4.68939334e-07
Iter: 617 loss: 4.68472479e-07
Iter: 618 loss: 4.69799517e-07
Iter: 619 loss: 4.68304108e-07
Iter: 620 loss: 4.68032482e-07
Iter: 621 loss: 4.70816559e-07
Iter: 622 loss: 4.6802495e-07
Iter: 623 loss: 4.67806387e-07
Iter: 624 loss: 4.70516312e-07
Iter: 625 loss: 4.67802721e-07
Iter: 626 loss: 4.6769992e-07
Iter: 627 loss: 4.67444522e-07
Iter: 628 loss: 4.71401904e-07
Iter: 629 loss: 4.6744691e-07
Iter: 630 loss: 4.67208906e-07
Iter: 631 loss: 4.67212089e-07
Iter: 632 loss: 4.67042895e-07
Iter: 633 loss: 4.66686231e-07
Iter: 634 loss: 4.72723855e-07
Iter: 635 loss: 4.66672816e-07
Iter: 636 loss: 4.66337582e-07
Iter: 637 loss: 4.71145938e-07
Iter: 638 loss: 4.66343977e-07
Iter: 639 loss: 4.65987e-07
Iter: 640 loss: 4.65712731e-07
Iter: 641 loss: 4.65589096e-07
Iter: 642 loss: 4.6530775e-07
Iter: 643 loss: 4.6530684e-07
Iter: 644 loss: 4.65050448e-07
Iter: 645 loss: 4.64977461e-07
Iter: 646 loss: 4.64811336e-07
Iter: 647 loss: 4.6455466e-07
Iter: 648 loss: 4.65309057e-07
Iter: 649 loss: 4.64493098e-07
Iter: 650 loss: 4.64213e-07
Iter: 651 loss: 4.64351274e-07
Iter: 652 loss: 4.64034969e-07
Iter: 653 loss: 4.63646131e-07
Iter: 654 loss: 4.63126241e-07
Iter: 655 loss: 4.63106517e-07
Iter: 656 loss: 4.6372827e-07
Iter: 657 loss: 4.62942353e-07
Iter: 658 loss: 4.62742946e-07
Iter: 659 loss: 4.62296185e-07
Iter: 660 loss: 4.66293386e-07
Iter: 661 loss: 4.62212512e-07
Iter: 662 loss: 4.62135802e-07
Iter: 663 loss: 4.61996251e-07
Iter: 664 loss: 4.61849e-07
Iter: 665 loss: 4.61579504e-07
Iter: 666 loss: 4.67971859e-07
Iter: 667 loss: 4.61591839e-07
Iter: 668 loss: 4.61355853e-07
Iter: 669 loss: 4.62386765e-07
Iter: 670 loss: 4.61317882e-07
Iter: 671 loss: 4.61067117e-07
Iter: 672 loss: 4.63599719e-07
Iter: 673 loss: 4.61059983e-07
Iter: 674 loss: 4.6089724e-07
Iter: 675 loss: 4.60630247e-07
Iter: 676 loss: 4.60622772e-07
Iter: 677 loss: 4.60378089e-07
Iter: 678 loss: 4.60351259e-07
Iter: 679 loss: 4.60218018e-07
Iter: 680 loss: 4.60019123e-07
Iter: 681 loss: 4.60023898e-07
Iter: 682 loss: 4.59797945e-07
Iter: 683 loss: 4.61163779e-07
Iter: 684 loss: 4.59789021e-07
Iter: 685 loss: 4.59568781e-07
Iter: 686 loss: 4.59899553e-07
Iter: 687 loss: 4.59462655e-07
Iter: 688 loss: 4.5928121e-07
Iter: 689 loss: 4.60514e-07
Iter: 690 loss: 4.59253471e-07
Iter: 691 loss: 4.59022317e-07
Iter: 692 loss: 4.58994094e-07
Iter: 693 loss: 4.5884633e-07
Iter: 694 loss: 4.58621912e-07
Iter: 695 loss: 4.58610316e-07
Iter: 696 loss: 4.58438478e-07
Iter: 697 loss: 4.58213549e-07
Iter: 698 loss: 4.58203687e-07
Iter: 699 loss: 4.58074624e-07
Iter: 700 loss: 4.57808625e-07
Iter: 701 loss: 4.61180207e-07
Iter: 702 loss: 4.57802656e-07
Iter: 703 loss: 4.57775485e-07
Iter: 704 loss: 4.5770318e-07
Iter: 705 loss: 4.57595803e-07
Iter: 706 loss: 4.57409499e-07
Iter: 707 loss: 4.60915572e-07
Iter: 708 loss: 4.5740444e-07
Iter: 709 loss: 4.57207648e-07
Iter: 710 loss: 4.59477064e-07
Iter: 711 loss: 4.57209808e-07
Iter: 712 loss: 4.57043797e-07
Iter: 713 loss: 4.56864854e-07
Iter: 714 loss: 4.56817446e-07
Iter: 715 loss: 4.56617386e-07
Iter: 716 loss: 4.56885914e-07
Iter: 717 loss: 4.56487442e-07
Iter: 718 loss: 4.56250689e-07
Iter: 719 loss: 4.57579802e-07
Iter: 720 loss: 4.56229571e-07
Iter: 721 loss: 4.56025134e-07
Iter: 722 loss: 4.56283942e-07
Iter: 723 loss: 4.55896725e-07
Iter: 724 loss: 4.55676457e-07
Iter: 725 loss: 4.55668953e-07
Iter: 726 loss: 4.55566322e-07
Iter: 727 loss: 4.5523683e-07
Iter: 728 loss: 4.56995224e-07
Iter: 729 loss: 4.5511598e-07
Iter: 730 loss: 4.55121437e-07
Iter: 731 loss: 4.54965971e-07
Iter: 732 loss: 4.54803398e-07
Iter: 733 loss: 4.54484621e-07
Iter: 734 loss: 4.59783394e-07
Iter: 735 loss: 4.54470381e-07
Iter: 736 loss: 4.54253438e-07
Iter: 737 loss: 4.55299954e-07
Iter: 738 loss: 4.54201711e-07
Iter: 739 loss: 4.54023535e-07
Iter: 740 loss: 4.54025695e-07
Iter: 741 loss: 4.53926418e-07
Iter: 742 loss: 4.53640354e-07
Iter: 743 loss: 4.56205e-07
Iter: 744 loss: 4.53594112e-07
Iter: 745 loss: 4.53285196e-07
Iter: 746 loss: 4.53284599e-07
Iter: 747 loss: 4.53149369e-07
Iter: 748 loss: 4.52820245e-07
Iter: 749 loss: 4.56712144e-07
Iter: 750 loss: 4.52805182e-07
Iter: 751 loss: 4.5245028e-07
Iter: 752 loss: 4.52709259e-07
Iter: 753 loss: 4.52241665e-07
Iter: 754 loss: 4.51994964e-07
Iter: 755 loss: 4.54746669e-07
Iter: 756 loss: 4.52007953e-07
Iter: 757 loss: 4.51811673e-07
Iter: 758 loss: 4.54036098e-07
Iter: 759 loss: 4.5181946e-07
Iter: 760 loss: 4.51703585e-07
Iter: 761 loss: 4.51512051e-07
Iter: 762 loss: 4.51505116e-07
Iter: 763 loss: 4.51336803e-07
Iter: 764 loss: 4.52584857e-07
Iter: 765 loss: 4.51306221e-07
Iter: 766 loss: 4.51094024e-07
Iter: 767 loss: 4.51149617e-07
Iter: 768 loss: 4.50935772e-07
Iter: 769 loss: 4.50676055e-07
Iter: 770 loss: 4.50433703e-07
Iter: 771 loss: 4.50366656e-07
Iter: 772 loss: 4.50138714e-07
Iter: 773 loss: 4.5102135e-07
Iter: 774 loss: 4.50063112e-07
Iter: 775 loss: 4.49798193e-07
Iter: 776 loss: 4.53612444e-07
Iter: 777 loss: 4.49804958e-07
Iter: 778 loss: 4.49682602e-07
Iter: 779 loss: 4.49455541e-07
Iter: 780 loss: 4.52381585e-07
Iter: 781 loss: 4.49439739e-07
Iter: 782 loss: 4.49330287e-07
Iter: 783 loss: 4.49297261e-07
Iter: 784 loss: 4.49138525e-07
Iter: 785 loss: 4.48741844e-07
Iter: 786 loss: 4.50374955e-07
Iter: 787 loss: 4.48559121e-07
Iter: 788 loss: 4.48190519e-07
Iter: 789 loss: 4.50686031e-07
Iter: 790 loss: 4.48132539e-07
Iter: 791 loss: 4.4813288e-07
Iter: 792 loss: 4.48024707e-07
Iter: 793 loss: 4.47932479e-07
Iter: 794 loss: 4.48146466e-07
Iter: 795 loss: 4.47887828e-07
Iter: 796 loss: 4.47797561e-07
Iter: 797 loss: 4.47640559e-07
Iter: 798 loss: 4.47644339e-07
Iter: 799 loss: 4.47469858e-07
Iter: 800 loss: 4.48619915e-07
Iter: 801 loss: 4.47465879e-07
Iter: 802 loss: 4.47317063e-07
Iter: 803 loss: 4.48309237e-07
Iter: 804 loss: 4.47283639e-07
Iter: 805 loss: 4.47151194e-07
Iter: 806 loss: 4.46876697e-07
Iter: 807 loss: 4.51268534e-07
Iter: 808 loss: 4.46879568e-07
Iter: 809 loss: 4.46644549e-07
Iter: 810 loss: 4.4901202e-07
Iter: 811 loss: 4.46642616e-07
Iter: 812 loss: 4.46412059e-07
Iter: 813 loss: 4.47529544e-07
Iter: 814 loss: 4.46372411e-07
Iter: 815 loss: 4.4627825e-07
Iter: 816 loss: 4.46168485e-07
Iter: 817 loss: 4.4616371e-07
Iter: 818 loss: 4.46011313e-07
Iter: 819 loss: 4.4601785e-07
Iter: 820 loss: 4.45925025e-07
Iter: 821 loss: 4.45702113e-07
Iter: 822 loss: 4.47866171e-07
Iter: 823 loss: 4.45653939e-07
Iter: 824 loss: 4.45525075e-07
Iter: 825 loss: 4.45531668e-07
Iter: 826 loss: 4.45376685e-07
Iter: 827 loss: 4.4584533e-07
Iter: 828 loss: 4.45326577e-07
Iter: 829 loss: 4.45196264e-07
Iter: 830 loss: 4.45073169e-07
Iter: 831 loss: 4.45032327e-07
Iter: 832 loss: 4.44843e-07
Iter: 833 loss: 4.45133367e-07
Iter: 834 loss: 4.44764453e-07
Iter: 835 loss: 4.44664209e-07
Iter: 836 loss: 4.44637635e-07
Iter: 837 loss: 4.44556406e-07
Iter: 838 loss: 4.4438417e-07
Iter: 839 loss: 4.44388633e-07
Iter: 840 loss: 4.44205654e-07
Iter: 841 loss: 4.44636584e-07
Iter: 842 loss: 4.44142557e-07
Iter: 843 loss: 4.43982032e-07
Iter: 844 loss: 4.43994281e-07
Iter: 845 loss: 4.43902081e-07
Iter: 846 loss: 4.43726975e-07
Iter: 847 loss: 4.47068857e-07
Iter: 848 loss: 4.4372922e-07
Iter: 849 loss: 4.43673684e-07
Iter: 850 loss: 4.43640715e-07
Iter: 851 loss: 4.43536265e-07
Iter: 852 loss: 4.43307044e-07
Iter: 853 loss: 4.45225169e-07
Iter: 854 loss: 4.43263758e-07
Iter: 855 loss: 4.4308257e-07
Iter: 856 loss: 4.44768318e-07
Iter: 857 loss: 4.43083195e-07
Iter: 858 loss: 4.4294535e-07
Iter: 859 loss: 4.43573697e-07
Iter: 860 loss: 4.42920737e-07
Iter: 861 loss: 4.42806311e-07
Iter: 862 loss: 4.42776241e-07
Iter: 863 loss: 4.42714907e-07
Iter: 864 loss: 4.4251189e-07
Iter: 865 loss: 4.42639077e-07
Iter: 866 loss: 4.42376347e-07
Iter: 867 loss: 4.42255725e-07
Iter: 868 loss: 4.42260102e-07
Iter: 869 loss: 4.42149769e-07
Iter: 870 loss: 4.42012833e-07
Iter: 871 loss: 4.42012748e-07
Iter: 872 loss: 4.41829854e-07
Iter: 873 loss: 4.42035287e-07
Iter: 874 loss: 4.41739445e-07
Iter: 875 loss: 4.41581676e-07
Iter: 876 loss: 4.43711656e-07
Iter: 877 loss: 4.41587247e-07
Iter: 878 loss: 4.41471855e-07
Iter: 879 loss: 4.41260681e-07
Iter: 880 loss: 4.46062387e-07
Iter: 881 loss: 4.41257896e-07
Iter: 882 loss: 4.41103623e-07
Iter: 883 loss: 4.41107545e-07
Iter: 884 loss: 4.40939573e-07
Iter: 885 loss: 4.40849618e-07
Iter: 886 loss: 4.40784447e-07
Iter: 887 loss: 4.40674e-07
Iter: 888 loss: 4.41634825e-07
Iter: 889 loss: 4.40662916e-07
Iter: 890 loss: 4.40540646e-07
Iter: 891 loss: 4.41424476e-07
Iter: 892 loss: 4.40549513e-07
Iter: 893 loss: 4.40474167e-07
Iter: 894 loss: 4.40358747e-07
Iter: 895 loss: 4.40345787e-07
Iter: 896 loss: 4.40155674e-07
Iter: 897 loss: 4.40548661e-07
Iter: 898 loss: 4.40088513e-07
Iter: 899 loss: 4.39906017e-07
Iter: 900 loss: 4.4068446e-07
Iter: 901 loss: 4.39866938e-07
Iter: 902 loss: 4.39649398e-07
Iter: 903 loss: 4.4036392e-07
Iter: 904 loss: 4.39569646e-07
Iter: 905 loss: 4.39470853e-07
Iter: 906 loss: 4.39512206e-07
Iter: 907 loss: 4.39399969e-07
Iter: 908 loss: 4.39290716e-07
Iter: 909 loss: 4.3928992e-07
Iter: 910 loss: 4.39222902e-07
Iter: 911 loss: 4.39151677e-07
Iter: 912 loss: 4.39126779e-07
Iter: 913 loss: 4.39035631e-07
Iter: 914 loss: 4.39272952e-07
Iter: 915 loss: 4.38995784e-07
Iter: 916 loss: 4.38848957e-07
Iter: 917 loss: 4.39483159e-07
Iter: 918 loss: 4.3880695e-07
Iter: 919 loss: 4.38727682e-07
Iter: 920 loss: 4.38573977e-07
Iter: 921 loss: 4.41687689e-07
Iter: 922 loss: 4.38570225e-07
Iter: 923 loss: 4.3846768e-07
Iter: 924 loss: 4.38436047e-07
Iter: 925 loss: 4.38341601e-07
Iter: 926 loss: 4.38146031e-07
Iter: 927 loss: 4.41641191e-07
Iter: 928 loss: 4.38142251e-07
Iter: 929 loss: 4.37994231e-07
Iter: 930 loss: 4.3989661e-07
Iter: 931 loss: 4.3798309e-07
Iter: 932 loss: 4.3788549e-07
Iter: 933 loss: 4.38013529e-07
Iter: 934 loss: 4.37800566e-07
Iter: 935 loss: 4.37685884e-07
Iter: 936 loss: 4.38438718e-07
Iter: 937 loss: 4.37660674e-07
Iter: 938 loss: 4.37557048e-07
Iter: 939 loss: 4.37361393e-07
Iter: 940 loss: 4.38630963e-07
Iter: 941 loss: 4.37296023e-07
Iter: 942 loss: 4.37140272e-07
Iter: 943 loss: 4.37142972e-07
Iter: 944 loss: 4.36961358e-07
Iter: 945 loss: 4.37614204e-07
Iter: 946 loss: 4.36933533e-07
Iter: 947 loss: 4.36839514e-07
Iter: 948 loss: 4.36703431e-07
Iter: 949 loss: 4.36705847e-07
Iter: 950 loss: 4.36607252e-07
Iter: 951 loss: 4.36613078e-07
Iter: 952 loss: 4.36511783e-07
Iter: 953 loss: 4.36625186e-07
Iter: 954 loss: 4.36462415e-07
Iter: 955 loss: 4.36399063e-07
Iter: 956 loss: 4.36465854e-07
Iter: 957 loss: 4.3635805e-07
Iter: 958 loss: 4.36245273e-07
Iter: 959 loss: 4.36446044e-07
Iter: 960 loss: 4.36206221e-07
Iter: 961 loss: 4.36121127e-07
Iter: 962 loss: 4.36088072e-07
Iter: 963 loss: 4.36049334e-07
Iter: 964 loss: 4.35941274e-07
Iter: 965 loss: 4.36543758e-07
Iter: 966 loss: 4.35897761e-07
Iter: 967 loss: 4.35833954e-07
Iter: 968 loss: 4.36538699e-07
Iter: 969 loss: 4.35814059e-07
Iter: 970 loss: 4.3571589e-07
Iter: 971 loss: 4.35516625e-07
Iter: 972 loss: 4.39698596e-07
Iter: 973 loss: 4.35512646e-07
Iter: 974 loss: 4.35327109e-07
Iter: 975 loss: 4.35544649e-07
Iter: 976 loss: 4.35244374e-07
Iter: 977 loss: 4.35076174e-07
Iter: 978 loss: 4.36476057e-07
Iter: 979 loss: 4.35060116e-07
Iter: 980 loss: 4.34857043e-07
Iter: 981 loss: 4.35529699e-07
Iter: 982 loss: 4.34804662e-07
Iter: 983 loss: 4.34710557e-07
Iter: 984 loss: 4.3452053e-07
Iter: 985 loss: 4.38102433e-07
Iter: 986 loss: 4.34507399e-07
Iter: 987 loss: 4.34487674e-07
Iter: 988 loss: 4.34423129e-07
Iter: 989 loss: 4.34331668e-07
Iter: 990 loss: 4.34155936e-07
Iter: 991 loss: 4.38122044e-07
Iter: 992 loss: 4.34152582e-07
Iter: 993 loss: 4.33987481e-07
Iter: 994 loss: 4.33736062e-07
Iter: 995 loss: 4.33710198e-07
Iter: 996 loss: 4.34019483e-07
Iter: 997 loss: 4.33604725e-07
Iter: 998 loss: 4.3355206e-07
Iter: 999 loss: 4.33413277e-07
Iter: 1000 loss: 4.34277467e-07
Iter: 1001 loss: 4.3338423e-07
Iter: 1002 loss: 4.33208925e-07
Iter: 1003 loss: 4.33553225e-07
Iter: 1004 loss: 4.33128434e-07
Iter: 1005 loss: 4.32964214e-07
Iter: 1006 loss: 4.34835812e-07
Iter: 1007 loss: 4.32970353e-07
Iter: 1008 loss: 4.32840864e-07
Iter: 1009 loss: 4.33519205e-07
Iter: 1010 loss: 4.32822219e-07
Iter: 1011 loss: 4.32686477e-07
Iter: 1012 loss: 4.32686676e-07
Iter: 1013 loss: 4.32589559e-07
Iter: 1014 loss: 4.32459586e-07
Iter: 1015 loss: 4.33820304e-07
Iter: 1016 loss: 4.32460894e-07
Iter: 1017 loss: 4.32369404e-07
Iter: 1018 loss: 4.32138336e-07
Iter: 1019 loss: 4.3593846e-07
Iter: 1020 loss: 4.32139387e-07
Iter: 1021 loss: 4.32031015e-07
Iter: 1022 loss: 4.33284981e-07
Iter: 1023 loss: 4.32033687e-07
Iter: 1024 loss: 4.31913094e-07
Iter: 1025 loss: 4.31918494e-07
Iter: 1026 loss: 4.31832802e-07
Iter: 1027 loss: 4.31682849e-07
Iter: 1028 loss: 4.31419e-07
Iter: 1029 loss: 4.37505889e-07
Iter: 1030 loss: 4.31421341e-07
Iter: 1031 loss: 4.31235094e-07
Iter: 1032 loss: 4.33702724e-07
Iter: 1033 loss: 4.31230944e-07
Iter: 1034 loss: 4.31035119e-07
Iter: 1035 loss: 4.31810548e-07
Iter: 1036 loss: 4.30981686e-07
Iter: 1037 loss: 4.30857199e-07
Iter: 1038 loss: 4.30644832e-07
Iter: 1039 loss: 4.33861601e-07
Iter: 1040 loss: 4.30618769e-07
Iter: 1041 loss: 4.30433033e-07
Iter: 1042 loss: 4.3275827e-07
Iter: 1043 loss: 4.30434511e-07
Iter: 1044 loss: 4.30308205e-07
Iter: 1045 loss: 4.30164476e-07
Iter: 1046 loss: 4.301437e-07
Iter: 1047 loss: 4.29929173e-07
Iter: 1048 loss: 4.30443833e-07
Iter: 1049 loss: 4.29866162e-07
Iter: 1050 loss: 4.29695802e-07
Iter: 1051 loss: 4.2969998e-07
Iter: 1052 loss: 4.29614943e-07
Iter: 1053 loss: 4.29386319e-07
Iter: 1054 loss: 4.3316615e-07
Iter: 1055 loss: 4.2939692e-07
Iter: 1056 loss: 4.29315492e-07
Iter: 1057 loss: 4.29295397e-07
Iter: 1058 loss: 4.29185e-07
Iter: 1059 loss: 4.29020247e-07
Iter: 1060 loss: 4.29015813e-07
Iter: 1061 loss: 4.28896755e-07
Iter: 1062 loss: 4.28901757e-07
Iter: 1063 loss: 4.28799865e-07
Iter: 1064 loss: 4.28604949e-07
Iter: 1065 loss: 4.28615209e-07
Iter: 1066 loss: 4.28533838e-07
Iter: 1067 loss: 4.28519712e-07
Iter: 1068 loss: 4.28428592e-07
Iter: 1069 loss: 4.28229953e-07
Iter: 1070 loss: 4.29252793e-07
Iter: 1071 loss: 4.28148383e-07
Iter: 1072 loss: 4.28015483e-07
Iter: 1073 loss: 4.2800724e-07
Iter: 1074 loss: 4.27882867e-07
Iter: 1075 loss: 4.28052658e-07
Iter: 1076 loss: 4.27815678e-07
Iter: 1077 loss: 4.27714411e-07
Iter: 1078 loss: 4.27583586e-07
Iter: 1079 loss: 4.27585547e-07
Iter: 1080 loss: 4.27363773e-07
Iter: 1081 loss: 4.27456513e-07
Iter: 1082 loss: 4.27222403e-07
Iter: 1083 loss: 4.27074553e-07
Iter: 1084 loss: 4.27057e-07
Iter: 1085 loss: 4.2695936e-07
Iter: 1086 loss: 4.26900044e-07
Iter: 1087 loss: 4.26871907e-07
Iter: 1088 loss: 4.26749352e-07
Iter: 1089 loss: 4.26747306e-07
Iter: 1090 loss: 4.26684e-07
Iter: 1091 loss: 4.26519364e-07
Iter: 1092 loss: 4.28091113e-07
Iter: 1093 loss: 4.26501e-07
Iter: 1094 loss: 4.26361908e-07
Iter: 1095 loss: 4.27814768e-07
Iter: 1096 loss: 4.26371e-07
Iter: 1097 loss: 4.26204167e-07
Iter: 1098 loss: 4.26434667e-07
Iter: 1099 loss: 4.26120465e-07
Iter: 1100 loss: 4.25985235e-07
Iter: 1101 loss: 4.25966476e-07
Iter: 1102 loss: 4.25849379e-07
Iter: 1103 loss: 4.25687915e-07
Iter: 1104 loss: 4.25689e-07
Iter: 1105 loss: 4.25641474e-07
Iter: 1106 loss: 4.25588723e-07
Iter: 1107 loss: 4.25578605e-07
Iter: 1108 loss: 4.25442948e-07
Iter: 1109 loss: 4.26047592e-07
Iter: 1110 loss: 4.25426833e-07
Iter: 1111 loss: 4.2531272e-07
Iter: 1112 loss: 4.25571557e-07
Iter: 1113 loss: 4.25275914e-07
Iter: 1114 loss: 4.2516379e-07
Iter: 1115 loss: 4.25261959e-07
Iter: 1116 loss: 4.25088928e-07
Iter: 1117 loss: 4.24957335e-07
Iter: 1118 loss: 4.25624705e-07
Iter: 1119 loss: 4.24924764e-07
Iter: 1120 loss: 4.24796127e-07
Iter: 1121 loss: 4.25174136e-07
Iter: 1122 loss: 4.24752045e-07
Iter: 1123 loss: 4.24568128e-07
Iter: 1124 loss: 4.2448255e-07
Iter: 1125 loss: 4.24406721e-07
Iter: 1126 loss: 4.24247105e-07
Iter: 1127 loss: 4.24428748e-07
Iter: 1128 loss: 4.24147572e-07
Iter: 1129 loss: 4.24091866e-07
Iter: 1130 loss: 4.24057589e-07
Iter: 1131 loss: 4.24009e-07
Iter: 1132 loss: 4.23854232e-07
Iter: 1133 loss: 4.25719406e-07
Iter: 1134 loss: 4.23851816e-07
Iter: 1135 loss: 4.23714653e-07
Iter: 1136 loss: 4.245627e-07
Iter: 1137 loss: 4.23688931e-07
Iter: 1138 loss: 4.23525165e-07
Iter: 1139 loss: 4.24378129e-07
Iter: 1140 loss: 4.23502826e-07
Iter: 1141 loss: 4.23413326e-07
Iter: 1142 loss: 4.23303334e-07
Iter: 1143 loss: 4.23292533e-07
Iter: 1144 loss: 4.23128199e-07
Iter: 1145 loss: 4.24517339e-07
Iter: 1146 loss: 4.231253e-07
Iter: 1147 loss: 4.23021959e-07
Iter: 1148 loss: 4.23239385e-07
Iter: 1149 loss: 4.22978189e-07
Iter: 1150 loss: 4.22869505e-07
Iter: 1151 loss: 4.23358358e-07
Iter: 1152 loss: 4.22858164e-07
Iter: 1153 loss: 4.22801236e-07
Iter: 1154 loss: 4.23101511e-07
Iter: 1155 loss: 4.22798109e-07
Iter: 1156 loss: 4.227185e-07
Iter: 1157 loss: 4.22814679e-07
Iter: 1158 loss: 4.22689624e-07
Iter: 1159 loss: 4.22599328e-07
Iter: 1160 loss: 4.22434482e-07
Iter: 1161 loss: 4.25194912e-07
Iter: 1162 loss: 4.22443321e-07
Iter: 1163 loss: 4.22488654e-07
Iter: 1164 loss: 4.22380396e-07
Iter: 1165 loss: 4.22326679e-07
Iter: 1166 loss: 4.22200458e-07
Iter: 1167 loss: 4.23580872e-07
Iter: 1168 loss: 4.22198809e-07
Iter: 1169 loss: 4.22068069e-07
Iter: 1170 loss: 4.22423255e-07
Iter: 1171 loss: 4.22033565e-07
Iter: 1172 loss: 4.21959896e-07
Iter: 1173 loss: 4.21954525e-07
Iter: 1174 loss: 4.21905384e-07
Iter: 1175 loss: 4.21762394e-07
Iter: 1176 loss: 4.22669586e-07
Iter: 1177 loss: 4.21741362e-07
Iter: 1178 loss: 4.21620598e-07
Iter: 1179 loss: 4.21626055e-07
Iter: 1180 loss: 4.21539312e-07
Iter: 1181 loss: 4.21790759e-07
Iter: 1182 loss: 4.21517882e-07
Iter: 1183 loss: 4.21439637e-07
Iter: 1184 loss: 4.21425625e-07
Iter: 1185 loss: 4.21368412e-07
Iter: 1186 loss: 4.21295795e-07
Iter: 1187 loss: 4.220731e-07
Iter: 1188 loss: 4.21291958e-07
Iter: 1189 loss: 4.21225536e-07
Iter: 1190 loss: 4.2126635e-07
Iter: 1191 loss: 4.21183017e-07
Iter: 1192 loss: 4.21103067e-07
Iter: 1193 loss: 4.20968547e-07
Iter: 1194 loss: 4.2095445e-07
Iter: 1195 loss: 4.20878393e-07
Iter: 1196 loss: 4.20872766e-07
Iter: 1197 loss: 4.2078392e-07
Iter: 1198 loss: 4.20820527e-07
Iter: 1199 loss: 4.20746431e-07
Iter: 1200 loss: 4.20636212e-07
Iter: 1201 loss: 4.20520053e-07
Iter: 1202 loss: 4.20498338e-07
Iter: 1203 loss: 4.20500442e-07
Iter: 1204 loss: 4.20448401e-07
Iter: 1205 loss: 4.2036487e-07
Iter: 1206 loss: 4.20217845e-07
Iter: 1207 loss: 4.23606252e-07
Iter: 1208 loss: 4.20212189e-07
Iter: 1209 loss: 4.20083467e-07
Iter: 1210 loss: 4.20159608e-07
Iter: 1211 loss: 4.20000333e-07
Iter: 1212 loss: 4.19948122e-07
Iter: 1213 loss: 4.1992655e-07
Iter: 1214 loss: 4.19839523e-07
Iter: 1215 loss: 4.1982176e-07
Iter: 1216 loss: 4.19789302e-07
Iter: 1217 loss: 4.19677264e-07
Iter: 1218 loss: 4.19758067e-07
Iter: 1219 loss: 4.19605755e-07
Iter: 1220 loss: 4.19523815e-07
Iter: 1221 loss: 4.19524383e-07
Iter: 1222 loss: 4.19475498e-07
Iter: 1223 loss: 4.19308606e-07
Iter: 1224 loss: 4.21195296e-07
Iter: 1225 loss: 4.19301358e-07
Iter: 1226 loss: 4.19160472e-07
Iter: 1227 loss: 4.19269611e-07
Iter: 1228 loss: 4.19075803e-07
Iter: 1229 loss: 4.1907748e-07
Iter: 1230 loss: 4.19000855e-07
Iter: 1231 loss: 4.18921189e-07
Iter: 1232 loss: 4.18783344e-07
Iter: 1233 loss: 4.21931617e-07
Iter: 1234 loss: 4.18779905e-07
Iter: 1235 loss: 4.18672641e-07
Iter: 1236 loss: 4.18903682e-07
Iter: 1237 loss: 4.18633959e-07
Iter: 1238 loss: 4.18510865e-07
Iter: 1239 loss: 4.2027861e-07
Iter: 1240 loss: 4.18512599e-07
Iter: 1241 loss: 4.18449247e-07
Iter: 1242 loss: 4.18317455e-07
Iter: 1243 loss: 4.19753945e-07
Iter: 1244 loss: 4.18282127e-07
Iter: 1245 loss: 4.18111568e-07
Iter: 1246 loss: 4.1909e-07
Iter: 1247 loss: 4.18080617e-07
Iter: 1248 loss: 4.1797e-07
Iter: 1249 loss: 4.17985859e-07
Iter: 1250 loss: 4.1790787e-07
Iter: 1251 loss: 4.17875924e-07
Iter: 1252 loss: 4.17839374e-07
Iter: 1253 loss: 4.17767467e-07
Iter: 1254 loss: 4.1818214e-07
Iter: 1255 loss: 4.17740608e-07
Iter: 1256 loss: 4.17642696e-07
Iter: 1257 loss: 4.17561125e-07
Iter: 1258 loss: 4.17511245e-07
Iter: 1259 loss: 4.17372178e-07
Iter: 1260 loss: 4.17321587e-07
Iter: 1261 loss: 4.17239761e-07
Iter: 1262 loss: 4.17042543e-07
Iter: 1263 loss: 4.17500218e-07
Iter: 1264 loss: 4.16973023e-07
Iter: 1265 loss: 4.16885399e-07
Iter: 1266 loss: 4.16850298e-07
Iter: 1267 loss: 4.16779386e-07
Iter: 1268 loss: 4.16605189e-07
Iter: 1269 loss: 4.17158617e-07
Iter: 1270 loss: 4.16529872e-07
Iter: 1271 loss: 4.16476581e-07
Iter: 1272 loss: 4.16415816e-07
Iter: 1273 loss: 4.16296444e-07
Iter: 1274 loss: 4.16244745e-07
Iter: 1275 loss: 4.16168234e-07
Iter: 1276 loss: 4.16065262e-07
Iter: 1277 loss: 4.15833341e-07
Iter: 1278 loss: 4.20755612e-07
Iter: 1279 loss: 4.15845506e-07
Iter: 1280 loss: 4.15645729e-07
Iter: 1281 loss: 4.17549643e-07
Iter: 1282 loss: 4.15632456e-07
Iter: 1283 loss: 4.15548755e-07
Iter: 1284 loss: 4.15540455e-07
Iter: 1285 loss: 4.15465763e-07
Iter: 1286 loss: 4.15352844e-07
Iter: 1287 loss: 4.15344317e-07
Iter: 1288 loss: 4.15246177e-07
Iter: 1289 loss: 4.16434688e-07
Iter: 1290 loss: 4.1524865e-07
Iter: 1291 loss: 4.15173474e-07
Iter: 1292 loss: 4.15149e-07
Iter: 1293 loss: 4.15075903e-07
Iter: 1294 loss: 4.14947181e-07
Iter: 1295 loss: 4.14824058e-07
Iter: 1296 loss: 4.14788701e-07
Iter: 1297 loss: 4.1462323e-07
Iter: 1298 loss: 4.15714311e-07
Iter: 1299 loss: 4.14589039e-07
Iter: 1300 loss: 4.14419731e-07
Iter: 1301 loss: 4.15331357e-07
Iter: 1302 loss: 4.14398016e-07
Iter: 1303 loss: 4.14308573e-07
Iter: 1304 loss: 4.14230186e-07
Iter: 1305 loss: 4.14196506e-07
Iter: 1306 loss: 4.14153959e-07
Iter: 1307 loss: 4.14137844e-07
Iter: 1308 loss: 4.14063067e-07
Iter: 1309 loss: 4.1389643e-07
Iter: 1310 loss: 4.15225486e-07
Iter: 1311 loss: 4.13857663e-07
Iter: 1312 loss: 4.13697165e-07
Iter: 1313 loss: 4.13721466e-07
Iter: 1314 loss: 4.13582512e-07
Iter: 1315 loss: 4.13399391e-07
Iter: 1316 loss: 4.13401835e-07
Iter: 1317 loss: 4.13219084e-07
Iter: 1318 loss: 4.13637537e-07
Iter: 1319 loss: 4.13121768e-07
Iter: 1320 loss: 4.13019649e-07
Iter: 1321 loss: 4.13217919e-07
Iter: 1322 loss: 4.12954137e-07
Iter: 1323 loss: 4.12818821e-07
Iter: 1324 loss: 4.13883413e-07
Iter: 1325 loss: 4.12820128e-07
Iter: 1326 loss: 4.12753963e-07
Iter: 1327 loss: 4.12721732e-07
Iter: 1328 loss: 4.12663297e-07
Iter: 1329 loss: 4.12577776e-07
Iter: 1330 loss: 4.13297812e-07
Iter: 1331 loss: 4.12566806e-07
Iter: 1332 loss: 4.12516982e-07
Iter: 1333 loss: 4.12691406e-07
Iter: 1334 loss: 4.12486713e-07
Iter: 1335 loss: 4.12427482e-07
Iter: 1336 loss: 4.12342729e-07
Iter: 1337 loss: 4.12344946e-07
Iter: 1338 loss: 4.12243935e-07
Iter: 1339 loss: 4.12247886e-07
Iter: 1340 loss: 4.12129708e-07
Iter: 1341 loss: 4.1195878e-07
Iter: 1342 loss: 4.11969665e-07
Iter: 1343 loss: 4.11818633e-07
Iter: 1344 loss: 4.11782167e-07
Iter: 1345 loss: 4.11693861e-07
Iter: 1346 loss: 4.11542231e-07
Iter: 1347 loss: 4.1251775e-07
Iter: 1348 loss: 4.11526798e-07
Iter: 1349 loss: 4.11405949e-07
Iter: 1350 loss: 4.11412117e-07
Iter: 1351 loss: 4.11341745e-07
Iter: 1352 loss: 4.11213e-07
Iter: 1353 loss: 4.13947049e-07
Iter: 1354 loss: 4.11215751e-07
Iter: 1355 loss: 4.11100018e-07
Iter: 1356 loss: 4.11090952e-07
Iter: 1357 loss: 4.11027315e-07
Iter: 1358 loss: 4.10957284e-07
Iter: 1359 loss: 4.10942846e-07
Iter: 1360 loss: 4.10859e-07
Iter: 1361 loss: 4.11700228e-07
Iter: 1362 loss: 4.10862128e-07
Iter: 1363 loss: 4.10792808e-07
Iter: 1364 loss: 4.10933353e-07
Iter: 1365 loss: 4.1076629e-07
Iter: 1366 loss: 4.10695293e-07
Iter: 1367 loss: 4.10629497e-07
Iter: 1368 loss: 4.10603633e-07
Iter: 1369 loss: 4.10513394e-07
Iter: 1370 loss: 4.1138918e-07
Iter: 1371 loss: 4.10514446e-07
Iter: 1372 loss: 4.10425827e-07
Iter: 1373 loss: 4.10581578e-07
Iter: 1374 loss: 4.10381801e-07
Iter: 1375 loss: 4.10310321e-07
Iter: 1376 loss: 4.10111056e-07
Iter: 1377 loss: 4.11627695e-07
Iter: 1378 loss: 4.10078201e-07
Iter: 1379 loss: 4.0992316e-07
Iter: 1380 loss: 4.09914151e-07
Iter: 1381 loss: 4.09872797e-07
Iter: 1382 loss: 4.09867681e-07
Iter: 1383 loss: 4.09815385e-07
Iter: 1384 loss: 4.09709202e-07
Iter: 1385 loss: 4.10761231e-07
Iter: 1386 loss: 4.09680524e-07
Iter: 1387 loss: 4.09631753e-07
Iter: 1388 loss: 4.09606116e-07
Iter: 1389 loss: 4.09549216e-07
Iter: 1390 loss: 4.09482709e-07
Iter: 1391 loss: 4.0946594e-07
Iter: 1392 loss: 4.09375303e-07
Iter: 1393 loss: 4.09981055e-07
Iter: 1394 loss: 4.09376071e-07
Iter: 1395 loss: 4.09295e-07
Iter: 1396 loss: 4.09465315e-07
Iter: 1397 loss: 4.09278584e-07
Iter: 1398 loss: 4.09210031e-07
Iter: 1399 loss: 4.09207729e-07
Iter: 1400 loss: 4.09150289e-07
Iter: 1401 loss: 4.09085e-07
Iter: 1402 loss: 4.09311269e-07
Iter: 1403 loss: 4.09063773e-07
Iter: 1404 loss: 4.08993e-07
Iter: 1405 loss: 4.09595231e-07
Iter: 1406 loss: 4.08974415e-07
Iter: 1407 loss: 4.08936671e-07
Iter: 1408 loss: 4.08826622e-07
Iter: 1409 loss: 4.10308019e-07
Iter: 1410 loss: 4.08818863e-07
Iter: 1411 loss: 4.08703613e-07
Iter: 1412 loss: 4.08825514e-07
Iter: 1413 loss: 4.08650379e-07
Iter: 1414 loss: 4.08619798e-07
Iter: 1415 loss: 4.08590182e-07
Iter: 1416 loss: 4.08513614e-07
Iter: 1417 loss: 4.08435625e-07
Iter: 1418 loss: 4.08430822e-07
Iter: 1419 loss: 4.08366162e-07
Iter: 1420 loss: 4.0937789e-07
Iter: 1421 loss: 4.08359767e-07
Iter: 1422 loss: 4.08290731e-07
Iter: 1423 loss: 4.08225787e-07
Iter: 1424 loss: 4.0820521e-07
Iter: 1425 loss: 4.0810329e-07
Iter: 1426 loss: 4.08590807e-07
Iter: 1427 loss: 4.08090557e-07
Iter: 1428 loss: 4.07988352e-07
Iter: 1429 loss: 4.08509834e-07
Iter: 1430 loss: 4.07959305e-07
Iter: 1431 loss: 4.07887455e-07
Iter: 1432 loss: 4.07923608e-07
Iter: 1433 loss: 4.07838627e-07
Iter: 1434 loss: 4.07758762e-07
Iter: 1435 loss: 4.07859261e-07
Iter: 1436 loss: 4.07718289e-07
Iter: 1437 loss: 4.07619325e-07
Iter: 1438 loss: 4.08887331e-07
Iter: 1439 loss: 4.07622508e-07
Iter: 1440 loss: 4.07573481e-07
Iter: 1441 loss: 4.07481593e-07
Iter: 1442 loss: 4.09066473e-07
Iter: 1443 loss: 4.07476222e-07
Iter: 1444 loss: 4.07380043e-07
Iter: 1445 loss: 4.07451864e-07
Iter: 1446 loss: 4.07327548e-07
Iter: 1447 loss: 4.07250525e-07
Iter: 1448 loss: 4.07239412e-07
Iter: 1449 loss: 4.07148548e-07
Iter: 1450 loss: 4.07156449e-07
Iter: 1451 loss: 4.07068171e-07
Iter: 1452 loss: 4.07006752e-07
Iter: 1453 loss: 4.07427763e-07
Iter: 1454 loss: 4.06990267e-07
Iter: 1455 loss: 4.06900284e-07
Iter: 1456 loss: 4.07020224e-07
Iter: 1457 loss: 4.06860806e-07
Iter: 1458 loss: 4.06816e-07
Iter: 1459 loss: 4.06933907e-07
Iter: 1460 loss: 4.06782817e-07
Iter: 1461 loss: 4.06705283e-07
Iter: 1462 loss: 4.0719712e-07
Iter: 1463 loss: 4.06698177e-07
Iter: 1464 loss: 4.06651964e-07
Iter: 1465 loss: 4.06683341e-07
Iter: 1466 loss: 4.06592221e-07
Iter: 1467 loss: 4.06526254e-07
Iter: 1468 loss: 4.06586821e-07
Iter: 1469 loss: 4.06489278e-07
Iter: 1470 loss: 4.06412823e-07
Iter: 1471 loss: 4.06406173e-07
Iter: 1472 loss: 4.06358396e-07
Iter: 1473 loss: 4.06236694e-07
Iter: 1474 loss: 4.07834449e-07
Iter: 1475 loss: 4.06257499e-07
Iter: 1476 loss: 4.06098337e-07
Iter: 1477 loss: 4.06328553e-07
Iter: 1478 loss: 4.06044762e-07
Iter: 1479 loss: 4.05963533e-07
Iter: 1480 loss: 4.06748285e-07
Iter: 1481 loss: 4.05947105e-07
Iter: 1482 loss: 4.05852688e-07
Iter: 1483 loss: 4.06131761e-07
Iter: 1484 loss: 4.05823982e-07
Iter: 1485 loss: 4.05747784e-07
Iter: 1486 loss: 4.05747159e-07
Iter: 1487 loss: 4.05685796e-07
Iter: 1488 loss: 4.05546757e-07
Iter: 1489 loss: 4.06199348e-07
Iter: 1490 loss: 4.05512765e-07
Iter: 1491 loss: 4.0541633e-07
Iter: 1492 loss: 4.05345389e-07
Iter: 1493 loss: 4.05321316e-07
Iter: 1494 loss: 4.05175911e-07
Iter: 1495 loss: 4.06705851e-07
Iter: 1496 loss: 4.05174688e-07
Iter: 1497 loss: 4.05066373e-07
Iter: 1498 loss: 4.05126741e-07
Iter: 1499 loss: 4.04992676e-07
Iter: 1500 loss: 4.04878847e-07
Iter: 1501 loss: 4.05015783e-07
Iter: 1502 loss: 4.04807849e-07
Iter: 1503 loss: 4.04691747e-07
Iter: 1504 loss: 4.04684442e-07
Iter: 1505 loss: 4.04601309e-07
Iter: 1506 loss: 4.04430949e-07
Iter: 1507 loss: 4.08394271e-07
Iter: 1508 loss: 4.04426885e-07
Iter: 1509 loss: 4.04271503e-07
Iter: 1510 loss: 4.04450958e-07
Iter: 1511 loss: 4.0418206e-07
Iter: 1512 loss: 4.04059477e-07
Iter: 1513 loss: 4.05192054e-07
Iter: 1514 loss: 4.04028157e-07
Iter: 1515 loss: 4.03913873e-07
Iter: 1516 loss: 4.05268679e-07
Iter: 1517 loss: 4.0390816e-07
Iter: 1518 loss: 4.03851828e-07
Iter: 1519 loss: 4.03759032e-07
Iter: 1520 loss: 4.03746867e-07
Iter: 1521 loss: 4.03623744e-07
Iter: 1522 loss: 4.04773317e-07
Iter: 1523 loss: 4.03619197e-07
Iter: 1524 loss: 4.03533193e-07
Iter: 1525 loss: 4.03385059e-07
Iter: 1526 loss: 4.03378067e-07
Iter: 1527 loss: 4.03258582e-07
Iter: 1528 loss: 4.03259776e-07
Iter: 1529 loss: 4.03151859e-07
Iter: 1530 loss: 4.03223311e-07
Iter: 1531 loss: 4.03082907e-07
Iter: 1532 loss: 4.02981783e-07
Iter: 1533 loss: 4.03077252e-07
Iter: 1534 loss: 4.02944067e-07
Iter: 1535 loss: 4.0281958e-07
Iter: 1536 loss: 4.02831859e-07
Iter: 1537 loss: 4.02760236e-07
Iter: 1538 loss: 4.02610169e-07
Iter: 1539 loss: 4.06020433e-07
Iter: 1540 loss: 4.02606389e-07
Iter: 1541 loss: 4.02455242e-07
Iter: 1542 loss: 4.0240127e-07
Iter: 1543 loss: 4.02292329e-07
Iter: 1544 loss: 4.02121714e-07
Iter: 1545 loss: 4.03092486e-07
Iter: 1546 loss: 4.02080076e-07
Iter: 1547 loss: 4.01924694e-07
Iter: 1548 loss: 4.01920374e-07
Iter: 1549 loss: 4.01824138e-07
Iter: 1550 loss: 4.01624021e-07
Iter: 1551 loss: 4.05657602e-07
Iter: 1552 loss: 4.01612738e-07
Iter: 1553 loss: 4.01469379e-07
Iter: 1554 loss: 4.01475035e-07
Iter: 1555 loss: 4.01382579e-07
Iter: 1556 loss: 4.01167711e-07
Iter: 1557 loss: 4.04507148e-07
Iter: 1558 loss: 4.01160719e-07
Iter: 1559 loss: 4.00984788e-07
Iter: 1560 loss: 4.03507386e-07
Iter: 1561 loss: 4.00973875e-07
Iter: 1562 loss: 4.00790015e-07
Iter: 1563 loss: 4.01305385e-07
Iter: 1564 loss: 4.00713702e-07
Iter: 1565 loss: 4.00585293e-07
Iter: 1566 loss: 4.00661861e-07
Iter: 1567 loss: 4.00511851e-07
Iter: 1568 loss: 4.00387762e-07
Iter: 1569 loss: 4.02261605e-07
Iter: 1570 loss: 4.00379633e-07
Iter: 1571 loss: 4.00281891e-07
Iter: 1572 loss: 4.00160616e-07
Iter: 1573 loss: 4.00141772e-07
Iter: 1574 loss: 3.99997759e-07
Iter: 1575 loss: 3.99945293e-07
Iter: 1576 loss: 3.99867702e-07
Iter: 1577 loss: 3.99658916e-07
Iter: 1578 loss: 4.00209416e-07
Iter: 1579 loss: 3.9958806e-07
Iter: 1580 loss: 3.99462721e-07
Iter: 1581 loss: 3.99454223e-07
Iter: 1582 loss: 3.99347812e-07
Iter: 1583 loss: 3.99162957e-07
Iter: 1584 loss: 4.03109823e-07
Iter: 1585 loss: 3.99159831e-07
Iter: 1586 loss: 3.99018489e-07
Iter: 1587 loss: 3.99006751e-07
Iter: 1588 loss: 3.98901278e-07
Iter: 1589 loss: 3.98792963e-07
Iter: 1590 loss: 3.98786824e-07
Iter: 1591 loss: 3.98654947e-07
Iter: 1592 loss: 3.9868533e-07
Iter: 1593 loss: 3.98550412e-07
Iter: 1594 loss: 3.98346913e-07
Iter: 1595 loss: 4.00505314e-07
Iter: 1596 loss: 3.98327046e-07
Iter: 1597 loss: 3.98219754e-07
Iter: 1598 loss: 3.98060223e-07
Iter: 1599 loss: 3.98065424e-07
Iter: 1600 loss: 3.97888414e-07
Iter: 1601 loss: 4.00330748e-07
Iter: 1602 loss: 3.978937e-07
Iter: 1603 loss: 3.97725955e-07
Iter: 1604 loss: 3.97921838e-07
Iter: 1605 loss: 3.97655185e-07
Iter: 1606 loss: 3.97526236e-07
Iter: 1607 loss: 3.97419228e-07
Iter: 1608 loss: 3.97383388e-07
Iter: 1609 loss: 3.97229542e-07
Iter: 1610 loss: 3.97143651e-07
Iter: 1611 loss: 3.97063161e-07
Iter: 1612 loss: 3.97043436e-07
Iter: 1613 loss: 3.97014844e-07
Iter: 1614 loss: 3.96924719e-07
Iter: 1615 loss: 3.96771952e-07
Iter: 1616 loss: 4.00583474e-07
Iter: 1617 loss: 3.9675956e-07
Iter: 1618 loss: 3.96681543e-07
Iter: 1619 loss: 3.96680832e-07
Iter: 1620 loss: 3.96594913e-07
Iter: 1621 loss: 3.96529771e-07
Iter: 1622 loss: 3.96490236e-07
Iter: 1623 loss: 3.96383655e-07
Iter: 1624 loss: 3.96259907e-07
Iter: 1625 loss: 3.96248367e-07
Iter: 1626 loss: 3.96138773e-07
Iter: 1627 loss: 3.96139598e-07
Iter: 1628 loss: 3.96023324e-07
Iter: 1629 loss: 3.96055668e-07
Iter: 1630 loss: 3.95965515e-07
Iter: 1631 loss: 3.9584387e-07
Iter: 1632 loss: 3.9585791e-07
Iter: 1633 loss: 3.95759287e-07
Iter: 1634 loss: 3.95571703e-07
Iter: 1635 loss: 3.96258145e-07
Iter: 1636 loss: 3.95546067e-07
Iter: 1637 loss: 3.9537656e-07
Iter: 1638 loss: 3.95273219e-07
Iter: 1639 loss: 3.95205063e-07
Iter: 1640 loss: 3.9503152e-07
Iter: 1641 loss: 3.95000882e-07
Iter: 1642 loss: 3.94840526e-07
Iter: 1643 loss: 3.94573959e-07
Iter: 1644 loss: 3.94995908e-07
Iter: 1645 loss: 3.94428753e-07
Iter: 1646 loss: 3.94253846e-07
Iter: 1647 loss: 3.94206552e-07
Iter: 1648 loss: 3.94000551e-07
Iter: 1649 loss: 3.93660287e-07
Iter: 1650 loss: 4.02213914e-07
Iter: 1651 loss: 3.9366094e-07
Iter: 1652 loss: 3.93433112e-07
Iter: 1653 loss: 3.93561095e-07
Iter: 1654 loss: 3.93295977e-07
Iter: 1655 loss: 3.93310017e-07
Iter: 1656 loss: 3.93162509e-07
Iter: 1657 loss: 3.9306758e-07
Iter: 1658 loss: 3.92847369e-07
Iter: 1659 loss: 3.95403731e-07
Iter: 1660 loss: 3.92803543e-07
Iter: 1661 loss: 3.92605955e-07
Iter: 1662 loss: 3.92877894e-07
Iter: 1663 loss: 3.92468792e-07
Iter: 1664 loss: 3.92557752e-07
Iter: 1665 loss: 3.92373721e-07
Iter: 1666 loss: 3.92287973e-07
Iter: 1667 loss: 3.92147911e-07
Iter: 1668 loss: 3.94653455e-07
Iter: 1669 loss: 3.92146205e-07
Iter: 1670 loss: 3.92017427e-07
Iter: 1671 loss: 3.92008246e-07
Iter: 1672 loss: 3.91898652e-07
Iter: 1673 loss: 3.92272796e-07
Iter: 1674 loss: 3.91893622e-07
Iter: 1675 loss: 3.91786358e-07
Iter: 1676 loss: 3.91577373e-07
Iter: 1677 loss: 3.9527356e-07
Iter: 1678 loss: 3.91574019e-07
Iter: 1679 loss: 3.91308e-07
Iter: 1680 loss: 3.92060315e-07
Iter: 1681 loss: 3.912495e-07
Iter: 1682 loss: 3.90953346e-07
Iter: 1683 loss: 3.93830476e-07
Iter: 1684 loss: 3.90935838e-07
Iter: 1685 loss: 3.90814535e-07
Iter: 1686 loss: 3.90606687e-07
Iter: 1687 loss: 3.90600974e-07
Iter: 1688 loss: 3.9043482e-07
Iter: 1689 loss: 3.91045432e-07
Iter: 1690 loss: 3.90381445e-07
Iter: 1691 loss: 3.90336197e-07
Iter: 1692 loss: 3.9028302e-07
Iter: 1693 loss: 3.90241411e-07
Iter: 1694 loss: 3.90159613e-07
Iter: 1695 loss: 3.90275375e-07
Iter: 1696 loss: 3.90047205e-07
Iter: 1697 loss: 3.89976094e-07
Iter: 1698 loss: 3.89943921e-07
Iter: 1699 loss: 3.89830063e-07
Iter: 1700 loss: 3.89827505e-07
Iter: 1701 loss: 3.89740791e-07
Iter: 1702 loss: 3.89623835e-07
Iter: 1703 loss: 3.89999059e-07
Iter: 1704 loss: 3.89582908e-07
Iter: 1705 loss: 3.89478771e-07
Iter: 1706 loss: 3.89891738e-07
Iter: 1707 loss: 3.89450889e-07
Iter: 1708 loss: 3.89383672e-07
Iter: 1709 loss: 3.89589928e-07
Iter: 1710 loss: 3.89363265e-07
Iter: 1711 loss: 3.89302727e-07
Iter: 1712 loss: 3.89201233e-07
Iter: 1713 loss: 3.91339881e-07
Iter: 1714 loss: 3.89199e-07
Iter: 1715 loss: 3.89158402e-07
Iter: 1716 loss: 3.89134641e-07
Iter: 1717 loss: 3.89056453e-07
Iter: 1718 loss: 3.88946518e-07
Iter: 1719 loss: 3.91491369e-07
Iter: 1720 loss: 3.88942169e-07
Iter: 1721 loss: 3.88807052e-07
Iter: 1722 loss: 3.88600881e-07
Iter: 1723 loss: 3.88575074e-07
Iter: 1724 loss: 3.88422507e-07
Iter: 1725 loss: 3.89384354e-07
Iter: 1726 loss: 3.8841165e-07
Iter: 1727 loss: 3.883315e-07
Iter: 1728 loss: 3.88308678e-07
Iter: 1729 loss: 3.88226397e-07
Iter: 1730 loss: 3.8803708e-07
Iter: 1731 loss: 3.90860237e-07
Iter: 1732 loss: 3.88037137e-07
Iter: 1733 loss: 3.87876554e-07
Iter: 1734 loss: 3.88249191e-07
Iter: 1735 loss: 3.87818375e-07
Iter: 1736 loss: 3.87584237e-07
Iter: 1737 loss: 3.89136346e-07
Iter: 1738 loss: 3.87561386e-07
Iter: 1739 loss: 3.87478337e-07
Iter: 1740 loss: 3.87750362e-07
Iter: 1741 loss: 3.87440025e-07
Iter: 1742 loss: 3.8732523e-07
Iter: 1743 loss: 3.87560419e-07
Iter: 1744 loss: 3.87272507e-07
Iter: 1745 loss: 3.87196792e-07
Iter: 1746 loss: 3.8743724e-07
Iter: 1747 loss: 3.87169194e-07
Iter: 1748 loss: 3.87084327e-07
Iter: 1749 loss: 3.87053717e-07
Iter: 1750 loss: 3.87021316e-07
Iter: 1751 loss: 3.86916383e-07
Iter: 1752 loss: 3.88564189e-07
Iter: 1753 loss: 3.86919908e-07
Iter: 1754 loss: 3.86841208e-07
Iter: 1755 loss: 3.86643507e-07
Iter: 1756 loss: 3.88165574e-07
Iter: 1757 loss: 3.86598686e-07
Iter: 1758 loss: 3.86373529e-07
Iter: 1759 loss: 3.86962398e-07
Iter: 1760 loss: 3.86307676e-07
Iter: 1761 loss: 3.86120689e-07
Iter: 1762 loss: 3.87064262e-07
Iter: 1763 loss: 3.86100055e-07
Iter: 1764 loss: 3.86065068e-07
Iter: 1765 loss: 3.86020247e-07
Iter: 1766 loss: 3.85970452e-07
Iter: 1767 loss: 3.85881151e-07
Iter: 1768 loss: 3.88265164e-07
Iter: 1769 loss: 3.85876945e-07
Iter: 1770 loss: 3.85799126e-07
Iter: 1771 loss: 3.86661497e-07
Iter: 1772 loss: 3.85796426e-07
Iter: 1773 loss: 3.85706e-07
Iter: 1774 loss: 3.85673559e-07
Iter: 1775 loss: 3.85648548e-07
Iter: 1776 loss: 3.85567404e-07
Iter: 1777 loss: 3.855898e-07
Iter: 1778 loss: 3.85523947e-07
Iter: 1779 loss: 3.85482537e-07
Iter: 1780 loss: 3.85473e-07
Iter: 1781 loss: 3.85430269e-07
Iter: 1782 loss: 3.85420321e-07
Iter: 1783 loss: 3.85369248e-07
Iter: 1784 loss: 3.85319908e-07
Iter: 1785 loss: 3.85304077e-07
Iter: 1786 loss: 3.85266901e-07
Iter: 1787 loss: 3.85162707e-07
Iter: 1788 loss: 3.85162451e-07
Iter: 1789 loss: 3.85067267e-07
Iter: 1790 loss: 3.85043933e-07
Iter: 1791 loss: 3.84976659e-07
Iter: 1792 loss: 3.84878e-07
Iter: 1793 loss: 3.86091045e-07
Iter: 1794 loss: 3.8487704e-07
Iter: 1795 loss: 3.84752923e-07
Iter: 1796 loss: 3.85019035e-07
Iter: 1797 loss: 3.84708954e-07
Iter: 1798 loss: 3.84646228e-07
Iter: 1799 loss: 3.8464475e-07
Iter: 1800 loss: 3.84598906e-07
Iter: 1801 loss: 3.84515914e-07
Iter: 1802 loss: 3.85375756e-07
Iter: 1803 loss: 3.84492722e-07
Iter: 1804 loss: 3.84453e-07
Iter: 1805 loss: 3.84424652e-07
Iter: 1806 loss: 3.84386453e-07
Iter: 1807 loss: 3.8427919e-07
Iter: 1808 loss: 3.86759268e-07
Iter: 1809 loss: 3.84271232e-07
Iter: 1810 loss: 3.84233829e-07
Iter: 1811 loss: 3.84223654e-07
Iter: 1812 loss: 3.84177952e-07
Iter: 1813 loss: 3.84078e-07
Iter: 1814 loss: 3.84693777e-07
Iter: 1815 loss: 3.84043858e-07
Iter: 1816 loss: 3.83945e-07
Iter: 1817 loss: 3.8394063e-07
Iter: 1818 loss: 3.8391255e-07
Iter: 1819 loss: 3.83939835e-07
Iter: 1820 loss: 3.83882906e-07
Iter: 1821 loss: 3.83855763e-07
Iter: 1822 loss: 3.83786187e-07
Iter: 1823 loss: 3.83770725e-07
Iter: 1824 loss: 3.83709647e-07
Iter: 1825 loss: 3.83646722e-07
Iter: 1826 loss: 3.83637598e-07
Iter: 1827 loss: 3.83492477e-07
Iter: 1828 loss: 3.84370708e-07
Iter: 1829 loss: 3.83481762e-07
Iter: 1830 loss: 3.83376573e-07
Iter: 1831 loss: 3.83400959e-07
Iter: 1832 loss: 3.83292985e-07
Iter: 1833 loss: 3.8317657e-07
Iter: 1834 loss: 3.83726842e-07
Iter: 1835 loss: 3.83157698e-07
Iter: 1836 loss: 3.83100257e-07
Iter: 1837 loss: 3.838297e-07
Iter: 1838 loss: 3.8309048e-07
Iter: 1839 loss: 3.83049326e-07
Iter: 1840 loss: 3.83616481e-07
Iter: 1841 loss: 3.83060865e-07
Iter: 1842 loss: 3.83017607e-07
Iter: 1843 loss: 3.82949963e-07
Iter: 1844 loss: 3.83824982e-07
Iter: 1845 loss: 3.82932853e-07
Iter: 1846 loss: 3.82881524e-07
Iter: 1847 loss: 3.82873793e-07
Iter: 1848 loss: 3.82831757e-07
Iter: 1849 loss: 3.82740211e-07
Iter: 1850 loss: 3.83787494e-07
Iter: 1851 loss: 3.82725091e-07
Iter: 1852 loss: 3.82668873e-07
Iter: 1853 loss: 3.82663643e-07
Iter: 1854 loss: 3.82605492e-07
Iter: 1855 loss: 3.82594123e-07
Iter: 1856 loss: 3.82559e-07
Iter: 1857 loss: 3.82444853e-07
Iter: 1858 loss: 3.82898918e-07
Iter: 1859 loss: 3.82433313e-07
Iter: 1860 loss: 3.82372576e-07
Iter: 1861 loss: 3.82347508e-07
Iter: 1862 loss: 3.82311953e-07
Iter: 1863 loss: 3.82221e-07
Iter: 1864 loss: 3.82262357e-07
Iter: 1865 loss: 3.82163478e-07
Iter: 1866 loss: 3.8203973e-07
Iter: 1867 loss: 3.8276022e-07
Iter: 1868 loss: 3.82032425e-07
Iter: 1869 loss: 3.81883439e-07
Iter: 1870 loss: 3.81873804e-07
Iter: 1871 loss: 3.81750908e-07
Iter: 1872 loss: 3.81621703e-07
Iter: 1873 loss: 3.82988105e-07
Iter: 1874 loss: 3.81606412e-07
Iter: 1875 loss: 3.81590723e-07
Iter: 1876 loss: 3.81579525e-07
Iter: 1877 loss: 3.81535671e-07
Iter: 1878 loss: 3.81468737e-07
Iter: 1879 loss: 3.82789665e-07
Iter: 1880 loss: 3.81457937e-07
Iter: 1881 loss: 3.81398024e-07
Iter: 1882 loss: 3.82038536e-07
Iter: 1883 loss: 3.81394898e-07
Iter: 1884 loss: 3.8132174e-07
Iter: 1885 loss: 3.8141809e-07
Iter: 1886 loss: 3.81292409e-07
Iter: 1887 loss: 3.81243922e-07
Iter: 1888 loss: 3.81400554e-07
Iter: 1889 loss: 3.81226414e-07
Iter: 1890 loss: 3.81164341e-07
Iter: 1891 loss: 3.81197594e-07
Iter: 1892 loss: 3.81128473e-07
Iter: 1893 loss: 3.81068446e-07
Iter: 1894 loss: 3.8145069e-07
Iter: 1895 loss: 3.81064979e-07
Iter: 1896 loss: 3.80999779e-07
Iter: 1897 loss: 3.8101e-07
Iter: 1898 loss: 3.80973518e-07
Iter: 1899 loss: 3.80927474e-07
Iter: 1900 loss: 3.80804124e-07
Iter: 1901 loss: 3.82934729e-07
Iter: 1902 loss: 3.80796109e-07
Iter: 1903 loss: 3.80651471e-07
Iter: 1904 loss: 3.80892e-07
Iter: 1905 loss: 3.80584027e-07
Iter: 1906 loss: 3.80425263e-07
Iter: 1907 loss: 3.80430407e-07
Iter: 1908 loss: 3.80365464e-07
Iter: 1909 loss: 3.80802476e-07
Iter: 1910 loss: 3.8032465e-07
Iter: 1911 loss: 3.80335848e-07
Iter: 1912 loss: 3.80319847e-07
Iter: 1913 loss: 3.80297536e-07
Iter: 1914 loss: 3.80241488e-07
Iter: 1915 loss: 3.80956578e-07
Iter: 1916 loss: 3.80232905e-07
Iter: 1917 loss: 3.80186293e-07
Iter: 1918 loss: 3.80437598e-07
Iter: 1919 loss: 3.80171173e-07
Iter: 1920 loss: 3.8013934e-07
Iter: 1921 loss: 3.80383199e-07
Iter: 1922 loss: 3.8013215e-07
Iter: 1923 loss: 3.80066354e-07
Iter: 1924 loss: 3.80033612e-07
Iter: 1925 loss: 3.80016104e-07
Iter: 1926 loss: 3.79963353e-07
Iter: 1927 loss: 3.80367283e-07
Iter: 1928 loss: 3.79952439e-07
Iter: 1929 loss: 3.79887922e-07
Iter: 1930 loss: 3.79952724e-07
Iter: 1931 loss: 3.79851969e-07
Iter: 1932 loss: 3.7979828e-07
Iter: 1933 loss: 3.79812604e-07
Iter: 1934 loss: 3.79774264e-07
Iter: 1935 loss: 3.79702783e-07
Iter: 1936 loss: 3.79698e-07
Iter: 1937 loss: 3.79668734e-07
Iter: 1938 loss: 3.79555e-07
Iter: 1939 loss: 3.80030485e-07
Iter: 1940 loss: 3.79517928e-07
Iter: 1941 loss: 3.79386393e-07
Iter: 1942 loss: 3.80241545e-07
Iter: 1943 loss: 3.79372636e-07
Iter: 1944 loss: 3.79292771e-07
Iter: 1945 loss: 3.80084373e-07
Iter: 1946 loss: 3.7929243e-07
Iter: 1947 loss: 3.79256562e-07
Iter: 1948 loss: 3.79247069e-07
Iter: 1949 loss: 3.79210462e-07
Iter: 1950 loss: 3.79164362e-07
Iter: 1951 loss: 3.7916152e-07
Iter: 1952 loss: 3.7909706e-07
Iter: 1953 loss: 3.78991331e-07
Iter: 1954 loss: 3.78983913e-07
Iter: 1955 loss: 3.78925279e-07
Iter: 1956 loss: 3.78900097e-07
Iter: 1957 loss: 3.78848029e-07
Iter: 1958 loss: 3.78806817e-07
Iter: 1959 loss: 3.78785472e-07
Iter: 1960 loss: 3.78734796e-07
Iter: 1961 loss: 3.78726355e-07
Iter: 1962 loss: 3.78688014e-07
Iter: 1963 loss: 3.7863137e-07
Iter: 1964 loss: 3.78623724e-07
Iter: 1965 loss: 3.78552841e-07
Iter: 1966 loss: 3.78616619e-07
Iter: 1967 loss: 3.78514414e-07
Iter: 1968 loss: 3.78424602e-07
Iter: 1969 loss: 3.79506218e-07
Iter: 1970 loss: 3.7842949e-07
Iter: 1971 loss: 3.78379127e-07
Iter: 1972 loss: 3.78242845e-07
Iter: 1973 loss: 3.79645485e-07
Iter: 1974 loss: 3.78248899e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi0.4
+ date
Wed Nov  4 11:34:34 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0/300_300_300_1 --function f2 --psi -1 --alpha 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f491176b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f491176b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f491167e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f491176b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f491167ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f491167e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec6570d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec657bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec63c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec7537b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec753bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec5d4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec5d4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec57c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec57c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec5280d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec6ba158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec4f9378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec4ef620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec50a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec4d12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec4d17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec608620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec5ef268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec5efb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4971c7ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec406510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec5ef9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec5ef620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec415620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec415840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec3730d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec36cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec3116a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec36ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48ec2b5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.024378665
test_loss: 0.024001833
train_loss: 0.013858917
test_loss: 0.014591056
train_loss: 0.010746863
test_loss: 0.011153476
train_loss: 0.008872955
test_loss: 0.009231837
train_loss: 0.008073085
test_loss: 0.008116151
train_loss: 0.0069544357
test_loss: 0.0074765375
train_loss: 0.0065723574
test_loss: 0.007577809
train_loss: 0.0061596427
test_loss: 0.0065181977
train_loss: 0.00573263
test_loss: 0.006227425
train_loss: 0.005522483
test_loss: 0.0059785913
train_loss: 0.005208905
test_loss: 0.0057623866
train_loss: 0.005091174
test_loss: 0.0061275405
train_loss: 0.004968222
test_loss: 0.005822281
train_loss: 0.0049671736
test_loss: 0.005340385
train_loss: 0.0044043325
test_loss: 0.005258704
train_loss: 0.004570373
test_loss: 0.005099402
train_loss: 0.0043989993
test_loss: 0.0049751233
train_loss: 0.004117496
test_loss: 0.004838286
train_loss: 0.004506031
test_loss: 0.0050319056
train_loss: 0.004246692
test_loss: 0.004812335
train_loss: 0.004219776
test_loss: 0.0051962147
train_loss: 0.0041647437
test_loss: 0.00470033
train_loss: 0.0041627823
test_loss: 0.0047706924
train_loss: 0.004458723
test_loss: 0.0048928657
train_loss: 0.0040371735
test_loss: 0.0046820305
train_loss: 0.0039524673
test_loss: 0.004509702
train_loss: 0.0040123328
test_loss: 0.0045298818
train_loss: 0.0039303186
test_loss: 0.0045860186
train_loss: 0.0037475624
test_loss: 0.004367195
train_loss: 0.0036432147
test_loss: 0.004438202
train_loss: 0.0043181414
test_loss: 0.0046556466
train_loss: 0.0037643567
test_loss: 0.004196517
train_loss: 0.003716393
test_loss: 0.004397661
train_loss: 0.0037253737
test_loss: 0.0043760287
train_loss: 0.0036177202
test_loss: 0.004297474
train_loss: 0.0035234212
test_loss: 0.0040165554
train_loss: 0.003550623
test_loss: 0.004044245
train_loss: 0.0034660383
test_loss: 0.004066926
train_loss: 0.0033214898
test_loss: 0.0041390643
train_loss: 0.0034489662
test_loss: 0.0041167173
train_loss: 0.0034378031
test_loss: 0.004046361
train_loss: 0.0034630229
test_loss: 0.003989244
train_loss: 0.0030477366
test_loss: 0.0039727837
train_loss: 0.0033441712
test_loss: 0.003795623
train_loss: 0.0032719937
test_loss: 0.003997551
train_loss: 0.0033378364
test_loss: 0.004175268
train_loss: 0.0032111602
test_loss: 0.00403164
train_loss: 0.003178305
test_loss: 0.0038364786
train_loss: 0.0036207056
test_loss: 0.0041360618
train_loss: 0.0029960778
test_loss: 0.003878947
train_loss: 0.0033012964
test_loss: 0.0038588417
train_loss: 0.0031884245
test_loss: 0.0038463275
train_loss: 0.0031513148
test_loss: 0.0036991898
train_loss: 0.003021125
test_loss: 0.0038633207
train_loss: 0.0030938888
test_loss: 0.0038143194
train_loss: 0.003231474
test_loss: 0.0039341217
train_loss: 0.003047387
test_loss: 0.0038091713
train_loss: 0.0029438892
test_loss: 0.003757454
train_loss: 0.0029632119
test_loss: 0.003761125
train_loss: 0.0030913842
test_loss: 0.0038114435
train_loss: 0.0029604612
test_loss: 0.0036766573
train_loss: 0.0030213096
test_loss: 0.003763711
train_loss: 0.0032170534
test_loss: 0.0037127612
train_loss: 0.0030208952
test_loss: 0.003696677
train_loss: 0.0031809933
test_loss: 0.0036641818
train_loss: 0.0029494492
test_loss: 0.0036859454
train_loss: 0.003161957
test_loss: 0.0035722738
train_loss: 0.0028045499
test_loss: 0.0035504717
train_loss: 0.0029626405
test_loss: 0.0036704405
train_loss: 0.0028477039
test_loss: 0.003452292
train_loss: 0.003267223
test_loss: 0.003687552
train_loss: 0.0028483
test_loss: 0.0035217067
train_loss: 0.0029041376
test_loss: 0.0034437506
train_loss: 0.0027851383
test_loss: 0.0034972143
train_loss: 0.00286326
test_loss: 0.0035622993
train_loss: 0.0028358204
test_loss: 0.0036957986
train_loss: 0.002845936
test_loss: 0.0035006548
train_loss: 0.002980201
test_loss: 0.003655445
train_loss: 0.0033105374
test_loss: 0.003714892
train_loss: 0.0027291446
test_loss: 0.003483043
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.4/300_300_300_1 --optimizer lbfgs --function f2 --psi -1 --alpha 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi0.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae3fac2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae3fa6aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae3fa5f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae3fa5f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae3f9b2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae3f9dc400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae3f93c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae3f93c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f73e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f73ee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f75d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f7192f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f714400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f714510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f6a2488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f654598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f654488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f65e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f5cc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f5cc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f622c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f5e0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f53e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f56b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f56bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f56bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f4d62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f4d6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f483620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f483730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f4ad730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f56b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae3fa6c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f3fb268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f3fb378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1f3d5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.31782454e-05
Iter: 2 loss: 1.11192294e-05
Iter: 3 loss: 1.11089712e-05
Iter: 4 loss: 9.99889926e-06
Iter: 5 loss: 1.14775212e-05
Iter: 6 loss: 9.4397783e-06
Iter: 7 loss: 8.84476685e-06
Iter: 8 loss: 1.07270807e-05
Iter: 9 loss: 8.6727141e-06
Iter: 10 loss: 8.13122824e-06
Iter: 11 loss: 1.13094829e-05
Iter: 12 loss: 8.06079242e-06
Iter: 13 loss: 7.62848413e-06
Iter: 14 loss: 8.94387085e-06
Iter: 15 loss: 7.50012396e-06
Iter: 16 loss: 7.25221435e-06
Iter: 17 loss: 7.8566045e-06
Iter: 18 loss: 7.16391241e-06
Iter: 19 loss: 6.80944868e-06
Iter: 20 loss: 7.24697338e-06
Iter: 21 loss: 6.62440107e-06
Iter: 22 loss: 6.36222921e-06
Iter: 23 loss: 6.33196123e-06
Iter: 24 loss: 6.14312921e-06
Iter: 25 loss: 6.14562032e-06
Iter: 26 loss: 6.03338185e-06
Iter: 27 loss: 5.94945777e-06
Iter: 28 loss: 5.7728812e-06
Iter: 29 loss: 8.72294277e-06
Iter: 30 loss: 5.76835919e-06
Iter: 31 loss: 5.59166847e-06
Iter: 32 loss: 5.43083388e-06
Iter: 33 loss: 5.38716677e-06
Iter: 34 loss: 5.09843539e-06
Iter: 35 loss: 6.59013949e-06
Iter: 36 loss: 5.0523995e-06
Iter: 37 loss: 5.00199167e-06
Iter: 38 loss: 4.91366973e-06
Iter: 39 loss: 4.8363122e-06
Iter: 40 loss: 4.69189308e-06
Iter: 41 loss: 7.94291191e-06
Iter: 42 loss: 4.6915743e-06
Iter: 43 loss: 4.57924898e-06
Iter: 44 loss: 5.91726075e-06
Iter: 45 loss: 4.57778424e-06
Iter: 46 loss: 4.50294692e-06
Iter: 47 loss: 5.29723502e-06
Iter: 48 loss: 4.50100788e-06
Iter: 49 loss: 4.44911393e-06
Iter: 50 loss: 4.37082235e-06
Iter: 51 loss: 4.36930441e-06
Iter: 52 loss: 4.27968e-06
Iter: 53 loss: 4.97879455e-06
Iter: 54 loss: 4.27323084e-06
Iter: 55 loss: 4.17731189e-06
Iter: 56 loss: 4.14728038e-06
Iter: 57 loss: 4.09079257e-06
Iter: 58 loss: 4.00897625e-06
Iter: 59 loss: 4.1763592e-06
Iter: 60 loss: 3.97611075e-06
Iter: 61 loss: 3.91732283e-06
Iter: 62 loss: 3.91416688e-06
Iter: 63 loss: 3.88282524e-06
Iter: 64 loss: 3.82062854e-06
Iter: 65 loss: 5.01940258e-06
Iter: 66 loss: 3.81989548e-06
Iter: 67 loss: 3.76867365e-06
Iter: 68 loss: 3.83347333e-06
Iter: 69 loss: 3.74233719e-06
Iter: 70 loss: 3.68764017e-06
Iter: 71 loss: 4.33265086e-06
Iter: 72 loss: 3.68690826e-06
Iter: 73 loss: 3.62563469e-06
Iter: 74 loss: 3.77370679e-06
Iter: 75 loss: 3.60363e-06
Iter: 76 loss: 3.56589135e-06
Iter: 77 loss: 3.54989197e-06
Iter: 78 loss: 3.5304065e-06
Iter: 79 loss: 3.49440552e-06
Iter: 80 loss: 3.49305583e-06
Iter: 81 loss: 3.47050218e-06
Iter: 82 loss: 3.54761642e-06
Iter: 83 loss: 3.46446632e-06
Iter: 84 loss: 3.44649311e-06
Iter: 85 loss: 3.44574346e-06
Iter: 86 loss: 3.43186184e-06
Iter: 87 loss: 3.4058819e-06
Iter: 88 loss: 3.57845238e-06
Iter: 89 loss: 3.40315114e-06
Iter: 90 loss: 3.38286759e-06
Iter: 91 loss: 3.34037532e-06
Iter: 92 loss: 4.05759238e-06
Iter: 93 loss: 3.33928665e-06
Iter: 94 loss: 3.31230831e-06
Iter: 95 loss: 3.31050046e-06
Iter: 96 loss: 3.2807668e-06
Iter: 97 loss: 3.26926784e-06
Iter: 98 loss: 3.25327028e-06
Iter: 99 loss: 3.22528012e-06
Iter: 100 loss: 3.20625895e-06
Iter: 101 loss: 3.19596097e-06
Iter: 102 loss: 3.16371325e-06
Iter: 103 loss: 3.38027621e-06
Iter: 104 loss: 3.16046021e-06
Iter: 105 loss: 3.15051784e-06
Iter: 106 loss: 3.14645399e-06
Iter: 107 loss: 3.13524242e-06
Iter: 108 loss: 3.10877249e-06
Iter: 109 loss: 3.41550913e-06
Iter: 110 loss: 3.10630094e-06
Iter: 111 loss: 3.08543622e-06
Iter: 112 loss: 3.36363246e-06
Iter: 113 loss: 3.08527069e-06
Iter: 114 loss: 3.065089e-06
Iter: 115 loss: 3.11649296e-06
Iter: 116 loss: 3.05815638e-06
Iter: 117 loss: 3.04155174e-06
Iter: 118 loss: 3.11012309e-06
Iter: 119 loss: 3.03788e-06
Iter: 120 loss: 3.02466788e-06
Iter: 121 loss: 3.06001e-06
Iter: 122 loss: 3.02014223e-06
Iter: 123 loss: 3.00409056e-06
Iter: 124 loss: 3.0137644e-06
Iter: 125 loss: 2.99376e-06
Iter: 126 loss: 2.98004966e-06
Iter: 127 loss: 2.98865461e-06
Iter: 128 loss: 2.97140514e-06
Iter: 129 loss: 2.95696509e-06
Iter: 130 loss: 2.95686391e-06
Iter: 131 loss: 2.94723895e-06
Iter: 132 loss: 2.92220102e-06
Iter: 133 loss: 3.11705662e-06
Iter: 134 loss: 2.91746733e-06
Iter: 135 loss: 2.89119293e-06
Iter: 136 loss: 2.9741359e-06
Iter: 137 loss: 2.8837494e-06
Iter: 138 loss: 2.87366379e-06
Iter: 139 loss: 2.87164721e-06
Iter: 140 loss: 2.86011527e-06
Iter: 141 loss: 2.8917957e-06
Iter: 142 loss: 2.85633791e-06
Iter: 143 loss: 2.84980069e-06
Iter: 144 loss: 2.84048383e-06
Iter: 145 loss: 2.84013549e-06
Iter: 146 loss: 2.82953442e-06
Iter: 147 loss: 2.82945712e-06
Iter: 148 loss: 2.82143901e-06
Iter: 149 loss: 2.8144716e-06
Iter: 150 loss: 2.81234338e-06
Iter: 151 loss: 2.79875576e-06
Iter: 152 loss: 2.87520243e-06
Iter: 153 loss: 2.79684309e-06
Iter: 154 loss: 2.78625748e-06
Iter: 155 loss: 2.81685402e-06
Iter: 156 loss: 2.78296648e-06
Iter: 157 loss: 2.77231857e-06
Iter: 158 loss: 2.76273613e-06
Iter: 159 loss: 2.76018454e-06
Iter: 160 loss: 2.75435377e-06
Iter: 161 loss: 2.75334469e-06
Iter: 162 loss: 2.74654394e-06
Iter: 163 loss: 2.73787782e-06
Iter: 164 loss: 2.73716955e-06
Iter: 165 loss: 2.72733428e-06
Iter: 166 loss: 2.71693784e-06
Iter: 167 loss: 2.71517388e-06
Iter: 168 loss: 2.69880138e-06
Iter: 169 loss: 2.77873437e-06
Iter: 170 loss: 2.69592738e-06
Iter: 171 loss: 2.68967801e-06
Iter: 172 loss: 2.68702433e-06
Iter: 173 loss: 2.68174836e-06
Iter: 174 loss: 2.66942152e-06
Iter: 175 loss: 2.8164643e-06
Iter: 176 loss: 2.66838128e-06
Iter: 177 loss: 2.66256166e-06
Iter: 178 loss: 2.66190727e-06
Iter: 179 loss: 2.65543667e-06
Iter: 180 loss: 2.65928884e-06
Iter: 181 loss: 2.65127892e-06
Iter: 182 loss: 2.64511959e-06
Iter: 183 loss: 2.66882967e-06
Iter: 184 loss: 2.64373193e-06
Iter: 185 loss: 2.63626566e-06
Iter: 186 loss: 2.63740321e-06
Iter: 187 loss: 2.63063384e-06
Iter: 188 loss: 2.61955233e-06
Iter: 189 loss: 2.64273285e-06
Iter: 190 loss: 2.61513196e-06
Iter: 191 loss: 2.6067446e-06
Iter: 192 loss: 2.61674e-06
Iter: 193 loss: 2.60228e-06
Iter: 194 loss: 2.59497165e-06
Iter: 195 loss: 2.59483954e-06
Iter: 196 loss: 2.59126864e-06
Iter: 197 loss: 2.58299974e-06
Iter: 198 loss: 2.68706026e-06
Iter: 199 loss: 2.58236832e-06
Iter: 200 loss: 2.57357988e-06
Iter: 201 loss: 2.57651254e-06
Iter: 202 loss: 2.56734279e-06
Iter: 203 loss: 2.56115345e-06
Iter: 204 loss: 2.56037538e-06
Iter: 205 loss: 2.55238729e-06
Iter: 206 loss: 2.55518216e-06
Iter: 207 loss: 2.54682141e-06
Iter: 208 loss: 2.54141059e-06
Iter: 209 loss: 2.54332576e-06
Iter: 210 loss: 2.53759276e-06
Iter: 211 loss: 2.53412895e-06
Iter: 212 loss: 2.53339135e-06
Iter: 213 loss: 2.53073586e-06
Iter: 214 loss: 2.52576569e-06
Iter: 215 loss: 2.63586844e-06
Iter: 216 loss: 2.52572681e-06
Iter: 217 loss: 2.52005202e-06
Iter: 218 loss: 2.58507e-06
Iter: 219 loss: 2.5199829e-06
Iter: 220 loss: 2.5160416e-06
Iter: 221 loss: 2.51458e-06
Iter: 222 loss: 2.5123943e-06
Iter: 223 loss: 2.50525181e-06
Iter: 224 loss: 2.50603398e-06
Iter: 225 loss: 2.49978893e-06
Iter: 226 loss: 2.49405548e-06
Iter: 227 loss: 2.49406321e-06
Iter: 228 loss: 2.48847095e-06
Iter: 229 loss: 2.48730885e-06
Iter: 230 loss: 2.48363949e-06
Iter: 231 loss: 2.47896855e-06
Iter: 232 loss: 2.47511207e-06
Iter: 233 loss: 2.47376784e-06
Iter: 234 loss: 2.46576587e-06
Iter: 235 loss: 2.47266689e-06
Iter: 236 loss: 2.4611395e-06
Iter: 237 loss: 2.46190348e-06
Iter: 238 loss: 2.45673436e-06
Iter: 239 loss: 2.45352248e-06
Iter: 240 loss: 2.44456646e-06
Iter: 241 loss: 2.49422783e-06
Iter: 242 loss: 2.44181319e-06
Iter: 243 loss: 2.43614477e-06
Iter: 244 loss: 2.43582963e-06
Iter: 245 loss: 2.43002933e-06
Iter: 246 loss: 2.44821649e-06
Iter: 247 loss: 2.42835631e-06
Iter: 248 loss: 2.42509191e-06
Iter: 249 loss: 2.42927126e-06
Iter: 250 loss: 2.42341889e-06
Iter: 251 loss: 2.41806674e-06
Iter: 252 loss: 2.42659394e-06
Iter: 253 loss: 2.41564385e-06
Iter: 254 loss: 2.41144744e-06
Iter: 255 loss: 2.42310125e-06
Iter: 256 loss: 2.4101214e-06
Iter: 257 loss: 2.4055812e-06
Iter: 258 loss: 2.4058586e-06
Iter: 259 loss: 2.40200188e-06
Iter: 260 loss: 2.3969144e-06
Iter: 261 loss: 2.39693054e-06
Iter: 262 loss: 2.39400879e-06
Iter: 263 loss: 2.38833627e-06
Iter: 264 loss: 2.49772802e-06
Iter: 265 loss: 2.38825351e-06
Iter: 266 loss: 2.38286884e-06
Iter: 267 loss: 2.38840812e-06
Iter: 268 loss: 2.37976678e-06
Iter: 269 loss: 2.37493305e-06
Iter: 270 loss: 2.43106433e-06
Iter: 271 loss: 2.37487711e-06
Iter: 272 loss: 2.36974074e-06
Iter: 273 loss: 2.39730343e-06
Iter: 274 loss: 2.36894607e-06
Iter: 275 loss: 2.36616074e-06
Iter: 276 loss: 2.35987181e-06
Iter: 277 loss: 2.44570629e-06
Iter: 278 loss: 2.35947482e-06
Iter: 279 loss: 2.35951234e-06
Iter: 280 loss: 2.35640505e-06
Iter: 281 loss: 2.35387597e-06
Iter: 282 loss: 2.34818503e-06
Iter: 283 loss: 2.42842657e-06
Iter: 284 loss: 2.34791241e-06
Iter: 285 loss: 2.34523304e-06
Iter: 286 loss: 2.34466188e-06
Iter: 287 loss: 2.3422931e-06
Iter: 288 loss: 2.33792161e-06
Iter: 289 loss: 2.44049352e-06
Iter: 290 loss: 2.33791752e-06
Iter: 291 loss: 2.33330297e-06
Iter: 292 loss: 2.36229744e-06
Iter: 293 loss: 2.33269293e-06
Iter: 294 loss: 2.32849561e-06
Iter: 295 loss: 2.34200979e-06
Iter: 296 loss: 2.32726143e-06
Iter: 297 loss: 2.32210277e-06
Iter: 298 loss: 2.33740639e-06
Iter: 299 loss: 2.32057209e-06
Iter: 300 loss: 2.31716331e-06
Iter: 301 loss: 2.31222248e-06
Iter: 302 loss: 2.31206855e-06
Iter: 303 loss: 2.30673868e-06
Iter: 304 loss: 2.33291689e-06
Iter: 305 loss: 2.30575847e-06
Iter: 306 loss: 2.30463343e-06
Iter: 307 loss: 2.30348496e-06
Iter: 308 loss: 2.30192131e-06
Iter: 309 loss: 2.29783473e-06
Iter: 310 loss: 2.32716798e-06
Iter: 311 loss: 2.29693842e-06
Iter: 312 loss: 2.29238458e-06
Iter: 313 loss: 2.32401e-06
Iter: 314 loss: 2.2919919e-06
Iter: 315 loss: 2.2866825e-06
Iter: 316 loss: 2.30938713e-06
Iter: 317 loss: 2.28562135e-06
Iter: 318 loss: 2.28280874e-06
Iter: 319 loss: 2.28289468e-06
Iter: 320 loss: 2.28057252e-06
Iter: 321 loss: 2.27539294e-06
Iter: 322 loss: 2.29351645e-06
Iter: 323 loss: 2.27399869e-06
Iter: 324 loss: 2.27141709e-06
Iter: 325 loss: 2.2732404e-06
Iter: 326 loss: 2.26974748e-06
Iter: 327 loss: 2.26679208e-06
Iter: 328 loss: 2.28736099e-06
Iter: 329 loss: 2.26648672e-06
Iter: 330 loss: 2.26402108e-06
Iter: 331 loss: 2.2765596e-06
Iter: 332 loss: 2.26361863e-06
Iter: 333 loss: 2.26120869e-06
Iter: 334 loss: 2.25577037e-06
Iter: 335 loss: 2.33497212e-06
Iter: 336 loss: 2.25543863e-06
Iter: 337 loss: 2.24963787e-06
Iter: 338 loss: 2.25845156e-06
Iter: 339 loss: 2.2468314e-06
Iter: 340 loss: 2.24521909e-06
Iter: 341 loss: 2.24393398e-06
Iter: 342 loss: 2.24099904e-06
Iter: 343 loss: 2.239693e-06
Iter: 344 loss: 2.23823599e-06
Iter: 345 loss: 2.23557049e-06
Iter: 346 loss: 2.23694769e-06
Iter: 347 loss: 2.23381085e-06
Iter: 348 loss: 2.2323261e-06
Iter: 349 loss: 2.23183906e-06
Iter: 350 loss: 2.2305303e-06
Iter: 351 loss: 2.22682115e-06
Iter: 352 loss: 2.24990572e-06
Iter: 353 loss: 2.22587505e-06
Iter: 354 loss: 2.22253288e-06
Iter: 355 loss: 2.22236349e-06
Iter: 356 loss: 2.2200004e-06
Iter: 357 loss: 2.21583036e-06
Iter: 358 loss: 2.31676313e-06
Iter: 359 loss: 2.21578739e-06
Iter: 360 loss: 2.21243863e-06
Iter: 361 loss: 2.25484632e-06
Iter: 362 loss: 2.21242408e-06
Iter: 363 loss: 2.20987977e-06
Iter: 364 loss: 2.21718119e-06
Iter: 365 loss: 2.20903894e-06
Iter: 366 loss: 2.20617949e-06
Iter: 367 loss: 2.21145592e-06
Iter: 368 loss: 2.20495394e-06
Iter: 369 loss: 2.20273751e-06
Iter: 370 loss: 2.19855201e-06
Iter: 371 loss: 2.29313923e-06
Iter: 372 loss: 2.19855792e-06
Iter: 373 loss: 2.19283538e-06
Iter: 374 loss: 2.19418393e-06
Iter: 375 loss: 2.18864761e-06
Iter: 376 loss: 2.1822666e-06
Iter: 377 loss: 2.22611538e-06
Iter: 378 loss: 2.18162495e-06
Iter: 379 loss: 2.182348e-06
Iter: 380 loss: 2.17976549e-06
Iter: 381 loss: 2.17810202e-06
Iter: 382 loss: 2.17532715e-06
Iter: 383 loss: 2.17530942e-06
Iter: 384 loss: 2.17315755e-06
Iter: 385 loss: 2.18399418e-06
Iter: 386 loss: 2.17276511e-06
Iter: 387 loss: 2.16985086e-06
Iter: 388 loss: 2.17491743e-06
Iter: 389 loss: 2.16848e-06
Iter: 390 loss: 2.16624107e-06
Iter: 391 loss: 2.16636022e-06
Iter: 392 loss: 2.1643923e-06
Iter: 393 loss: 2.16042622e-06
Iter: 394 loss: 2.18298919e-06
Iter: 395 loss: 2.159926e-06
Iter: 396 loss: 2.15798946e-06
Iter: 397 loss: 2.15541922e-06
Iter: 398 loss: 2.15532964e-06
Iter: 399 loss: 2.15381533e-06
Iter: 400 loss: 2.15358e-06
Iter: 401 loss: 2.15202635e-06
Iter: 402 loss: 2.1529379e-06
Iter: 403 loss: 2.15108048e-06
Iter: 404 loss: 2.1490348e-06
Iter: 405 loss: 2.15145838e-06
Iter: 406 loss: 2.14802662e-06
Iter: 407 loss: 2.14585225e-06
Iter: 408 loss: 2.14162083e-06
Iter: 409 loss: 2.22877225e-06
Iter: 410 loss: 2.14159672e-06
Iter: 411 loss: 2.13619592e-06
Iter: 412 loss: 2.15278078e-06
Iter: 413 loss: 2.13449357e-06
Iter: 414 loss: 2.13618068e-06
Iter: 415 loss: 2.13294129e-06
Iter: 416 loss: 2.1316514e-06
Iter: 417 loss: 2.12903865e-06
Iter: 418 loss: 2.18302011e-06
Iter: 419 loss: 2.12899749e-06
Iter: 420 loss: 2.12755913e-06
Iter: 421 loss: 2.12757e-06
Iter: 422 loss: 2.12593864e-06
Iter: 423 loss: 2.125126e-06
Iter: 424 loss: 2.12433088e-06
Iter: 425 loss: 2.12247778e-06
Iter: 426 loss: 2.12396844e-06
Iter: 427 loss: 2.1213441e-06
Iter: 428 loss: 2.11805741e-06
Iter: 429 loss: 2.12818099e-06
Iter: 430 loss: 2.11709084e-06
Iter: 431 loss: 2.1149408e-06
Iter: 432 loss: 2.11131191e-06
Iter: 433 loss: 2.11131805e-06
Iter: 434 loss: 2.11012571e-06
Iter: 435 loss: 2.10953272e-06
Iter: 436 loss: 2.10776261e-06
Iter: 437 loss: 2.10822532e-06
Iter: 438 loss: 2.1065257e-06
Iter: 439 loss: 2.10482494e-06
Iter: 440 loss: 2.10808389e-06
Iter: 441 loss: 2.10408916e-06
Iter: 442 loss: 2.10199255e-06
Iter: 443 loss: 2.10181315e-06
Iter: 444 loss: 2.10020175e-06
Iter: 445 loss: 2.09694826e-06
Iter: 446 loss: 2.09508244e-06
Iter: 447 loss: 2.09366908e-06
Iter: 448 loss: 2.09425389e-06
Iter: 449 loss: 2.09154882e-06
Iter: 450 loss: 2.08998836e-06
Iter: 451 loss: 2.08767733e-06
Iter: 452 loss: 2.08763481e-06
Iter: 453 loss: 2.08616575e-06
Iter: 454 loss: 2.10333974e-06
Iter: 455 loss: 2.08619076e-06
Iter: 456 loss: 2.0845e-06
Iter: 457 loss: 2.08443362e-06
Iter: 458 loss: 2.08310894e-06
Iter: 459 loss: 2.08167376e-06
Iter: 460 loss: 2.08288702e-06
Iter: 461 loss: 2.08082747e-06
Iter: 462 loss: 2.07823928e-06
Iter: 463 loss: 2.08695315e-06
Iter: 464 loss: 2.07755102e-06
Iter: 465 loss: 2.07575204e-06
Iter: 466 loss: 2.0720463e-06
Iter: 467 loss: 2.14255488e-06
Iter: 468 loss: 2.07199628e-06
Iter: 469 loss: 2.07069456e-06
Iter: 470 loss: 2.06963068e-06
Iter: 471 loss: 2.0683924e-06
Iter: 472 loss: 2.06645927e-06
Iter: 473 loss: 2.066452e-06
Iter: 474 loss: 2.06432219e-06
Iter: 475 loss: 2.06953678e-06
Iter: 476 loss: 2.06366212e-06
Iter: 477 loss: 2.06131517e-06
Iter: 478 loss: 2.06436926e-06
Iter: 479 loss: 2.06009713e-06
Iter: 480 loss: 2.05810966e-06
Iter: 481 loss: 2.07956964e-06
Iter: 482 loss: 2.05804167e-06
Iter: 483 loss: 2.05587435e-06
Iter: 484 loss: 2.0548564e-06
Iter: 485 loss: 2.05376818e-06
Iter: 486 loss: 2.05145807e-06
Iter: 487 loss: 2.05881815e-06
Iter: 488 loss: 2.05073411e-06
Iter: 489 loss: 2.04876142e-06
Iter: 490 loss: 2.04870389e-06
Iter: 491 loss: 2.04766729e-06
Iter: 492 loss: 2.04552225e-06
Iter: 493 loss: 2.0859145e-06
Iter: 494 loss: 2.04552725e-06
Iter: 495 loss: 2.04429921e-06
Iter: 496 loss: 2.04414278e-06
Iter: 497 loss: 2.04312528e-06
Iter: 498 loss: 2.04084381e-06
Iter: 499 loss: 2.06859841e-06
Iter: 500 loss: 2.04062144e-06
Iter: 501 loss: 2.03884929e-06
Iter: 502 loss: 2.03884565e-06
Iter: 503 loss: 2.03676541e-06
Iter: 504 loss: 2.03507034e-06
Iter: 505 loss: 2.03445734e-06
Iter: 506 loss: 2.03182799e-06
Iter: 507 loss: 2.03421837e-06
Iter: 508 loss: 2.03030095e-06
Iter: 509 loss: 2.02733554e-06
Iter: 510 loss: 2.03873287e-06
Iter: 511 loss: 2.02657407e-06
Iter: 512 loss: 2.02447177e-06
Iter: 513 loss: 2.0374664e-06
Iter: 514 loss: 2.02425963e-06
Iter: 515 loss: 2.02252841e-06
Iter: 516 loss: 2.03561217e-06
Iter: 517 loss: 2.02244883e-06
Iter: 518 loss: 2.02150659e-06
Iter: 519 loss: 2.01926969e-06
Iter: 520 loss: 2.04596813e-06
Iter: 521 loss: 2.01908324e-06
Iter: 522 loss: 2.01761281e-06
Iter: 523 loss: 2.01739908e-06
Iter: 524 loss: 2.01570515e-06
Iter: 525 loss: 2.01258536e-06
Iter: 526 loss: 2.08533493e-06
Iter: 527 loss: 2.01262242e-06
Iter: 528 loss: 2.01068224e-06
Iter: 529 loss: 2.01066e-06
Iter: 530 loss: 2.00889758e-06
Iter: 531 loss: 2.00767136e-06
Iter: 532 loss: 2.00700197e-06
Iter: 533 loss: 2.00536033e-06
Iter: 534 loss: 2.00752879e-06
Iter: 535 loss: 2.00449699e-06
Iter: 536 loss: 2.00286217e-06
Iter: 537 loss: 2.00286513e-06
Iter: 538 loss: 2.00169961e-06
Iter: 539 loss: 1.99905935e-06
Iter: 540 loss: 2.03089826e-06
Iter: 541 loss: 1.99885881e-06
Iter: 542 loss: 1.99582655e-06
Iter: 543 loss: 2.0175321e-06
Iter: 544 loss: 1.99552437e-06
Iter: 545 loss: 1.99239776e-06
Iter: 546 loss: 1.99366718e-06
Iter: 547 loss: 1.99028227e-06
Iter: 548 loss: 1.98775069e-06
Iter: 549 loss: 2.02023557e-06
Iter: 550 loss: 1.98772636e-06
Iter: 551 loss: 1.98524367e-06
Iter: 552 loss: 1.99376564e-06
Iter: 553 loss: 1.98459247e-06
Iter: 554 loss: 1.98325233e-06
Iter: 555 loss: 1.980859e-06
Iter: 556 loss: 2.03985428e-06
Iter: 557 loss: 1.98084217e-06
Iter: 558 loss: 1.97973714e-06
Iter: 559 loss: 1.97920826e-06
Iter: 560 loss: 1.97803956e-06
Iter: 561 loss: 1.97552708e-06
Iter: 562 loss: 2.01570401e-06
Iter: 563 loss: 1.9754757e-06
Iter: 564 loss: 1.97330019e-06
Iter: 565 loss: 1.97324675e-06
Iter: 566 loss: 1.97163672e-06
Iter: 567 loss: 1.96828705e-06
Iter: 568 loss: 2.02810611e-06
Iter: 569 loss: 1.96823544e-06
Iter: 570 loss: 1.96620613e-06
Iter: 571 loss: 1.96619112e-06
Iter: 572 loss: 1.96409542e-06
Iter: 573 loss: 1.96832343e-06
Iter: 574 loss: 1.96332485e-06
Iter: 575 loss: 1.96213e-06
Iter: 576 loss: 1.95978964e-06
Iter: 577 loss: 2.00520162e-06
Iter: 578 loss: 1.95973462e-06
Iter: 579 loss: 1.95699477e-06
Iter: 580 loss: 1.97713712e-06
Iter: 581 loss: 1.95677603e-06
Iter: 582 loss: 1.95400185e-06
Iter: 583 loss: 1.96195015e-06
Iter: 584 loss: 1.95313714e-06
Iter: 585 loss: 1.95122539e-06
Iter: 586 loss: 1.96317114e-06
Iter: 587 loss: 1.95096163e-06
Iter: 588 loss: 1.94850509e-06
Iter: 589 loss: 1.94701397e-06
Iter: 590 loss: 1.94596669e-06
Iter: 591 loss: 1.94404424e-06
Iter: 592 loss: 1.94687118e-06
Iter: 593 loss: 1.94298082e-06
Iter: 594 loss: 1.94093832e-06
Iter: 595 loss: 1.94093354e-06
Iter: 596 loss: 1.9399381e-06
Iter: 597 loss: 1.93792243e-06
Iter: 598 loss: 1.97294958e-06
Iter: 599 loss: 1.93785809e-06
Iter: 600 loss: 1.93550886e-06
Iter: 601 loss: 1.96847827e-06
Iter: 602 loss: 1.93552046e-06
Iter: 603 loss: 1.93409619e-06
Iter: 604 loss: 1.9311924e-06
Iter: 605 loss: 1.98241332e-06
Iter: 606 loss: 1.93116739e-06
Iter: 607 loss: 1.92972311e-06
Iter: 608 loss: 1.92943753e-06
Iter: 609 loss: 1.92779908e-06
Iter: 610 loss: 1.92608513e-06
Iter: 611 loss: 1.92579364e-06
Iter: 612 loss: 1.92400466e-06
Iter: 613 loss: 1.92230436e-06
Iter: 614 loss: 1.92187144e-06
Iter: 615 loss: 1.91903473e-06
Iter: 616 loss: 1.92875086e-06
Iter: 617 loss: 1.91830077e-06
Iter: 618 loss: 1.91623576e-06
Iter: 619 loss: 1.91618028e-06
Iter: 620 loss: 1.91443769e-06
Iter: 621 loss: 1.91996014e-06
Iter: 622 loss: 1.91391473e-06
Iter: 623 loss: 1.91191793e-06
Iter: 624 loss: 1.91162917e-06
Iter: 625 loss: 1.9101924e-06
Iter: 626 loss: 1.90867468e-06
Iter: 627 loss: 1.91684194e-06
Iter: 628 loss: 1.90845367e-06
Iter: 629 loss: 1.90658807e-06
Iter: 630 loss: 1.91254958e-06
Iter: 631 loss: 1.90608398e-06
Iter: 632 loss: 1.90491551e-06
Iter: 633 loss: 1.9033622e-06
Iter: 634 loss: 1.9032193e-06
Iter: 635 loss: 1.90124979e-06
Iter: 636 loss: 1.90123785e-06
Iter: 637 loss: 1.90019909e-06
Iter: 638 loss: 1.89756588e-06
Iter: 639 loss: 1.92681705e-06
Iter: 640 loss: 1.89738807e-06
Iter: 641 loss: 1.89646482e-06
Iter: 642 loss: 1.89592276e-06
Iter: 643 loss: 1.89451725e-06
Iter: 644 loss: 1.89234549e-06
Iter: 645 loss: 1.89234868e-06
Iter: 646 loss: 1.89064667e-06
Iter: 647 loss: 1.8925374e-06
Iter: 648 loss: 1.88966408e-06
Iter: 649 loss: 1.88745696e-06
Iter: 650 loss: 1.88851072e-06
Iter: 651 loss: 1.88599847e-06
Iter: 652 loss: 1.88294894e-06
Iter: 653 loss: 1.89510445e-06
Iter: 654 loss: 1.882231e-06
Iter: 655 loss: 1.88110744e-06
Iter: 656 loss: 1.88053355e-06
Iter: 657 loss: 1.87946068e-06
Iter: 658 loss: 1.87776845e-06
Iter: 659 loss: 1.87775083e-06
Iter: 660 loss: 1.87587079e-06
Iter: 661 loss: 1.88757701e-06
Iter: 662 loss: 1.87562978e-06
Iter: 663 loss: 1.87438854e-06
Iter: 664 loss: 1.88749641e-06
Iter: 665 loss: 1.87437377e-06
Iter: 666 loss: 1.87332148e-06
Iter: 667 loss: 1.87162209e-06
Iter: 668 loss: 1.87160765e-06
Iter: 669 loss: 1.86953275e-06
Iter: 670 loss: 1.86872671e-06
Iter: 671 loss: 1.8676842e-06
Iter: 672 loss: 1.8668818e-06
Iter: 673 loss: 1.86612635e-06
Iter: 674 loss: 1.86499221e-06
Iter: 675 loss: 1.8624562e-06
Iter: 676 loss: 1.89771822e-06
Iter: 677 loss: 1.86228328e-06
Iter: 678 loss: 1.86025682e-06
Iter: 679 loss: 1.86561647e-06
Iter: 680 loss: 1.8596013e-06
Iter: 681 loss: 1.85859335e-06
Iter: 682 loss: 1.858274e-06
Iter: 683 loss: 1.85740566e-06
Iter: 684 loss: 1.85502381e-06
Iter: 685 loss: 1.86829539e-06
Iter: 686 loss: 1.85438216e-06
Iter: 687 loss: 1.8513656e-06
Iter: 688 loss: 1.85541239e-06
Iter: 689 loss: 1.84990711e-06
Iter: 690 loss: 1.8491221e-06
Iter: 691 loss: 1.84824273e-06
Iter: 692 loss: 1.84670671e-06
Iter: 693 loss: 1.84642249e-06
Iter: 694 loss: 1.8453336e-06
Iter: 695 loss: 1.84402097e-06
Iter: 696 loss: 1.84808187e-06
Iter: 697 loss: 1.84371561e-06
Iter: 698 loss: 1.84226974e-06
Iter: 699 loss: 1.84950613e-06
Iter: 700 loss: 1.84198336e-06
Iter: 701 loss: 1.84059536e-06
Iter: 702 loss: 1.84111093e-06
Iter: 703 loss: 1.83960935e-06
Iter: 704 loss: 1.83783288e-06
Iter: 705 loss: 1.83517977e-06
Iter: 706 loss: 1.83509314e-06
Iter: 707 loss: 1.83185682e-06
Iter: 708 loss: 1.84591704e-06
Iter: 709 loss: 1.8312046e-06
Iter: 710 loss: 1.82948099e-06
Iter: 711 loss: 1.82921565e-06
Iter: 712 loss: 1.82821782e-06
Iter: 713 loss: 1.82591839e-06
Iter: 714 loss: 1.85363808e-06
Iter: 715 loss: 1.8257391e-06
Iter: 716 loss: 1.823807e-06
Iter: 717 loss: 1.8326682e-06
Iter: 718 loss: 1.82341728e-06
Iter: 719 loss: 1.82198846e-06
Iter: 720 loss: 1.82192389e-06
Iter: 721 loss: 1.82089752e-06
Iter: 722 loss: 1.81792382e-06
Iter: 723 loss: 1.83181282e-06
Iter: 724 loss: 1.81693883e-06
Iter: 725 loss: 1.81438122e-06
Iter: 726 loss: 1.84593205e-06
Iter: 727 loss: 1.81440112e-06
Iter: 728 loss: 1.81232963e-06
Iter: 729 loss: 1.84126736e-06
Iter: 730 loss: 1.81235248e-06
Iter: 731 loss: 1.81142161e-06
Iter: 732 loss: 1.80989127e-06
Iter: 733 loss: 1.80987877e-06
Iter: 734 loss: 1.80888424e-06
Iter: 735 loss: 1.80876282e-06
Iter: 736 loss: 1.80793211e-06
Iter: 737 loss: 1.80668962e-06
Iter: 738 loss: 1.80668633e-06
Iter: 739 loss: 1.80469328e-06
Iter: 740 loss: 1.80590882e-06
Iter: 741 loss: 1.80342045e-06
Iter: 742 loss: 1.80110044e-06
Iter: 743 loss: 1.80269899e-06
Iter: 744 loss: 1.79971244e-06
Iter: 745 loss: 1.79781966e-06
Iter: 746 loss: 1.81798077e-06
Iter: 747 loss: 1.79778613e-06
Iter: 748 loss: 1.79603876e-06
Iter: 749 loss: 1.80682719e-06
Iter: 750 loss: 1.7958364e-06
Iter: 751 loss: 1.79491849e-06
Iter: 752 loss: 1.79298263e-06
Iter: 753 loss: 1.82172744e-06
Iter: 754 loss: 1.79286815e-06
Iter: 755 loss: 1.79037522e-06
Iter: 756 loss: 1.79188169e-06
Iter: 757 loss: 1.78877019e-06
Iter: 758 loss: 1.78858204e-06
Iter: 759 loss: 1.78728192e-06
Iter: 760 loss: 1.78594462e-06
Iter: 761 loss: 1.78362973e-06
Iter: 762 loss: 1.78362143e-06
Iter: 763 loss: 1.78184541e-06
Iter: 764 loss: 1.7834866e-06
Iter: 765 loss: 1.78079688e-06
Iter: 766 loss: 1.77950574e-06
Iter: 767 loss: 1.77949619e-06
Iter: 768 loss: 1.77799768e-06
Iter: 769 loss: 1.77931565e-06
Iter: 770 loss: 1.7770858e-06
Iter: 771 loss: 1.77577203e-06
Iter: 772 loss: 1.77509128e-06
Iter: 773 loss: 1.77445736e-06
Iter: 774 loss: 1.77217885e-06
Iter: 775 loss: 1.78045457e-06
Iter: 776 loss: 1.77162974e-06
Iter: 777 loss: 1.76999697e-06
Iter: 778 loss: 1.78189703e-06
Iter: 779 loss: 1.76983463e-06
Iter: 780 loss: 1.76794731e-06
Iter: 781 loss: 1.77013703e-06
Iter: 782 loss: 1.76696324e-06
Iter: 783 loss: 1.76578078e-06
Iter: 784 loss: 1.76447111e-06
Iter: 785 loss: 1.76428671e-06
Iter: 786 loss: 1.76272783e-06
Iter: 787 loss: 1.78022492e-06
Iter: 788 loss: 1.76268804e-06
Iter: 789 loss: 1.76107164e-06
Iter: 790 loss: 1.76665833e-06
Iter: 791 loss: 1.76060621e-06
Iter: 792 loss: 1.75955881e-06
Iter: 793 loss: 1.75697664e-06
Iter: 794 loss: 1.78593166e-06
Iter: 795 loss: 1.75668629e-06
Iter: 796 loss: 1.75503624e-06
Iter: 797 loss: 1.75493449e-06
Iter: 798 loss: 1.75326431e-06
Iter: 799 loss: 1.76168987e-06
Iter: 800 loss: 1.75298646e-06
Iter: 801 loss: 1.75220111e-06
Iter: 802 loss: 1.7504392e-06
Iter: 803 loss: 1.77715992e-06
Iter: 804 loss: 1.75039293e-06
Iter: 805 loss: 1.74904449e-06
Iter: 806 loss: 1.74904108e-06
Iter: 807 loss: 1.74736965e-06
Iter: 808 loss: 1.74843956e-06
Iter: 809 loss: 1.74635306e-06
Iter: 810 loss: 1.74500462e-06
Iter: 811 loss: 1.74317017e-06
Iter: 812 loss: 1.74309355e-06
Iter: 813 loss: 1.7404767e-06
Iter: 814 loss: 1.76257822e-06
Iter: 815 loss: 1.7402939e-06
Iter: 816 loss: 1.73882881e-06
Iter: 817 loss: 1.74606453e-06
Iter: 818 loss: 1.73853653e-06
Iter: 819 loss: 1.73710725e-06
Iter: 820 loss: 1.7480088e-06
Iter: 821 loss: 1.73700107e-06
Iter: 822 loss: 1.73628405e-06
Iter: 823 loss: 1.73437343e-06
Iter: 824 loss: 1.74803461e-06
Iter: 825 loss: 1.73390413e-06
Iter: 826 loss: 1.73164267e-06
Iter: 827 loss: 1.74525189e-06
Iter: 828 loss: 1.73133e-06
Iter: 829 loss: 1.72978048e-06
Iter: 830 loss: 1.72974489e-06
Iter: 831 loss: 1.72879447e-06
Iter: 832 loss: 1.72648356e-06
Iter: 833 loss: 1.75224091e-06
Iter: 834 loss: 1.726238e-06
Iter: 835 loss: 1.72424052e-06
Iter: 836 loss: 1.73413059e-06
Iter: 837 loss: 1.7238724e-06
Iter: 838 loss: 1.72226498e-06
Iter: 839 loss: 1.72657963e-06
Iter: 840 loss: 1.72173736e-06
Iter: 841 loss: 1.72051375e-06
Iter: 842 loss: 1.72042223e-06
Iter: 843 loss: 1.71965485e-06
Iter: 844 loss: 1.71771683e-06
Iter: 845 loss: 1.73609624e-06
Iter: 846 loss: 1.71743761e-06
Iter: 847 loss: 1.71660247e-06
Iter: 848 loss: 1.71616239e-06
Iter: 849 loss: 1.71530644e-06
Iter: 850 loss: 1.71338786e-06
Iter: 851 loss: 1.74448269e-06
Iter: 852 loss: 1.71334216e-06
Iter: 853 loss: 1.71178283e-06
Iter: 854 loss: 1.72600664e-06
Iter: 855 loss: 1.7116937e-06
Iter: 856 loss: 1.7104885e-06
Iter: 857 loss: 1.7103157e-06
Iter: 858 loss: 1.70950307e-06
Iter: 859 loss: 1.70827389e-06
Iter: 860 loss: 1.71508805e-06
Iter: 861 loss: 1.70812564e-06
Iter: 862 loss: 1.70654039e-06
Iter: 863 loss: 1.71088368e-06
Iter: 864 loss: 1.70603437e-06
Iter: 865 loss: 1.70489227e-06
Iter: 866 loss: 1.70468297e-06
Iter: 867 loss: 1.70399517e-06
Iter: 868 loss: 1.70221915e-06
Iter: 869 loss: 1.71641716e-06
Iter: 870 loss: 1.7021157e-06
Iter: 871 loss: 1.70128624e-06
Iter: 872 loss: 1.6996504e-06
Iter: 873 loss: 1.72860814e-06
Iter: 874 loss: 1.69959367e-06
Iter: 875 loss: 1.69927364e-06
Iter: 876 loss: 1.69883401e-06
Iter: 877 loss: 1.69809869e-06
Iter: 878 loss: 1.69657471e-06
Iter: 879 loss: 1.72299542e-06
Iter: 880 loss: 1.69654709e-06
Iter: 881 loss: 1.6951152e-06
Iter: 882 loss: 1.70235705e-06
Iter: 883 loss: 1.69482269e-06
Iter: 884 loss: 1.69295231e-06
Iter: 885 loss: 1.69683608e-06
Iter: 886 loss: 1.69215991e-06
Iter: 887 loss: 1.69089526e-06
Iter: 888 loss: 1.68943575e-06
Iter: 889 loss: 1.68928193e-06
Iter: 890 loss: 1.68736847e-06
Iter: 891 loss: 1.70577653e-06
Iter: 892 loss: 1.68729207e-06
Iter: 893 loss: 1.6858163e-06
Iter: 894 loss: 1.68536781e-06
Iter: 895 loss: 1.684523e-06
Iter: 896 loss: 1.68280167e-06
Iter: 897 loss: 1.68489282e-06
Iter: 898 loss: 1.68186671e-06
Iter: 899 loss: 1.68038378e-06
Iter: 900 loss: 1.68030158e-06
Iter: 901 loss: 1.67930682e-06
Iter: 902 loss: 1.67727183e-06
Iter: 903 loss: 1.71090437e-06
Iter: 904 loss: 1.67722976e-06
Iter: 905 loss: 1.67624876e-06
Iter: 906 loss: 1.67601138e-06
Iter: 907 loss: 1.67507108e-06
Iter: 908 loss: 1.67319172e-06
Iter: 909 loss: 1.70912938e-06
Iter: 910 loss: 1.67319513e-06
Iter: 911 loss: 1.67191251e-06
Iter: 912 loss: 1.68461293e-06
Iter: 913 loss: 1.67187034e-06
Iter: 914 loss: 1.67059557e-06
Iter: 915 loss: 1.67802523e-06
Iter: 916 loss: 1.67041696e-06
Iter: 917 loss: 1.66965788e-06
Iter: 918 loss: 1.66865425e-06
Iter: 919 loss: 1.66852874e-06
Iter: 920 loss: 1.66639245e-06
Iter: 921 loss: 1.67355461e-06
Iter: 922 loss: 1.66582367e-06
Iter: 923 loss: 1.66458744e-06
Iter: 924 loss: 1.66245923e-06
Iter: 925 loss: 1.71623617e-06
Iter: 926 loss: 1.66244627e-06
Iter: 927 loss: 1.66012569e-06
Iter: 928 loss: 1.68819201e-06
Iter: 929 loss: 1.6601125e-06
Iter: 930 loss: 1.65871211e-06
Iter: 931 loss: 1.66013967e-06
Iter: 932 loss: 1.65792221e-06
Iter: 933 loss: 1.65661652e-06
Iter: 934 loss: 1.65996175e-06
Iter: 935 loss: 1.65615586e-06
Iter: 936 loss: 1.65425718e-06
Iter: 937 loss: 1.66507812e-06
Iter: 938 loss: 1.65401741e-06
Iter: 939 loss: 1.65288589e-06
Iter: 940 loss: 1.6514382e-06
Iter: 941 loss: 1.6512904e-06
Iter: 942 loss: 1.64938228e-06
Iter: 943 loss: 1.67988503e-06
Iter: 944 loss: 1.64939081e-06
Iter: 945 loss: 1.64848802e-06
Iter: 946 loss: 1.64658434e-06
Iter: 947 loss: 1.68232486e-06
Iter: 948 loss: 1.64653966e-06
Iter: 949 loss: 1.64524204e-06
Iter: 950 loss: 1.6530995e-06
Iter: 951 loss: 1.64496487e-06
Iter: 952 loss: 1.64416952e-06
Iter: 953 loss: 1.64413711e-06
Iter: 954 loss: 1.64358676e-06
Iter: 955 loss: 1.64226253e-06
Iter: 956 loss: 1.65464701e-06
Iter: 957 loss: 1.64209246e-06
Iter: 958 loss: 1.64085827e-06
Iter: 959 loss: 1.64082621e-06
Iter: 960 loss: 1.64006462e-06
Iter: 961 loss: 1.6381465e-06
Iter: 962 loss: 1.65798349e-06
Iter: 963 loss: 1.63796278e-06
Iter: 964 loss: 1.63629591e-06
Iter: 965 loss: 1.64626044e-06
Iter: 966 loss: 1.63608956e-06
Iter: 967 loss: 1.63462937e-06
Iter: 968 loss: 1.64112839e-06
Iter: 969 loss: 1.6343638e-06
Iter: 970 loss: 1.63319783e-06
Iter: 971 loss: 1.6322582e-06
Iter: 972 loss: 1.63194727e-06
Iter: 973 loss: 1.63011418e-06
Iter: 974 loss: 1.635486e-06
Iter: 975 loss: 1.62958918e-06
Iter: 976 loss: 1.62783772e-06
Iter: 977 loss: 1.6278093e-06
Iter: 978 loss: 1.62704112e-06
Iter: 979 loss: 1.62514573e-06
Iter: 980 loss: 1.64625237e-06
Iter: 981 loss: 1.6249345e-06
Iter: 982 loss: 1.62319327e-06
Iter: 983 loss: 1.62975186e-06
Iter: 984 loss: 1.62277479e-06
Iter: 985 loss: 1.62234949e-06
Iter: 986 loss: 1.62201434e-06
Iter: 987 loss: 1.62125821e-06
Iter: 988 loss: 1.61969183e-06
Iter: 989 loss: 1.6474371e-06
Iter: 990 loss: 1.61966886e-06
Iter: 991 loss: 1.61834487e-06
Iter: 992 loss: 1.63198013e-06
Iter: 993 loss: 1.6183069e-06
Iter: 994 loss: 1.61675825e-06
Iter: 995 loss: 1.61784328e-06
Iter: 996 loss: 1.61582955e-06
Iter: 997 loss: 1.61477033e-06
Iter: 998 loss: 1.61370417e-06
Iter: 999 loss: 1.61346259e-06
Iter: 1000 loss: 1.61222329e-06
Iter: 1001 loss: 1.61215758e-06
Iter: 1002 loss: 1.61153821e-06
Iter: 1003 loss: 1.61019875e-06
Iter: 1004 loss: 1.62999822e-06
Iter: 1005 loss: 1.61011735e-06
Iter: 1006 loss: 1.60866568e-06
Iter: 1007 loss: 1.61943876e-06
Iter: 1008 loss: 1.60851391e-06
Iter: 1009 loss: 1.60728905e-06
Iter: 1010 loss: 1.60781451e-06
Iter: 1011 loss: 1.60645629e-06
Iter: 1012 loss: 1.60491e-06
Iter: 1013 loss: 1.6224792e-06
Iter: 1014 loss: 1.60493983e-06
Iter: 1015 loss: 1.60379614e-06
Iter: 1016 loss: 1.60207856e-06
Iter: 1017 loss: 1.60205389e-06
Iter: 1018 loss: 1.60050115e-06
Iter: 1019 loss: 1.60675199e-06
Iter: 1020 loss: 1.60014986e-06
Iter: 1021 loss: 1.59882552e-06
Iter: 1022 loss: 1.61576372e-06
Iter: 1023 loss: 1.59878198e-06
Iter: 1024 loss: 1.59802846e-06
Iter: 1025 loss: 1.5969373e-06
Iter: 1026 loss: 1.59690546e-06
Iter: 1027 loss: 1.59624142e-06
Iter: 1028 loss: 1.5961466e-06
Iter: 1029 loss: 1.59540105e-06
Iter: 1030 loss: 1.59369938e-06
Iter: 1031 loss: 1.61225682e-06
Iter: 1032 loss: 1.59352317e-06
Iter: 1033 loss: 1.59160072e-06
Iter: 1034 loss: 1.59104638e-06
Iter: 1035 loss: 1.58990633e-06
Iter: 1036 loss: 1.59056253e-06
Iter: 1037 loss: 1.58887246e-06
Iter: 1038 loss: 1.58818739e-06
Iter: 1039 loss: 1.58667319e-06
Iter: 1040 loss: 1.61092885e-06
Iter: 1041 loss: 1.58659986e-06
Iter: 1042 loss: 1.58527621e-06
Iter: 1043 loss: 1.59790511e-06
Iter: 1044 loss: 1.58525438e-06
Iter: 1045 loss: 1.58408136e-06
Iter: 1046 loss: 1.58421426e-06
Iter: 1047 loss: 1.58320859e-06
Iter: 1048 loss: 1.58173805e-06
Iter: 1049 loss: 1.59459137e-06
Iter: 1050 loss: 1.58168928e-06
Iter: 1051 loss: 1.58067201e-06
Iter: 1052 loss: 1.5787773e-06
Iter: 1053 loss: 1.6202581e-06
Iter: 1054 loss: 1.57878605e-06
Iter: 1055 loss: 1.5771293e-06
Iter: 1056 loss: 1.59167928e-06
Iter: 1057 loss: 1.57702743e-06
Iter: 1058 loss: 1.57549118e-06
Iter: 1059 loss: 1.5887889e-06
Iter: 1060 loss: 1.5754149e-06
Iter: 1061 loss: 1.57464569e-06
Iter: 1062 loss: 1.57447244e-06
Iter: 1063 loss: 1.57404497e-06
Iter: 1064 loss: 1.57270131e-06
Iter: 1065 loss: 1.57943e-06
Iter: 1066 loss: 1.57248178e-06
Iter: 1067 loss: 1.57162367e-06
Iter: 1068 loss: 1.56983026e-06
Iter: 1069 loss: 1.59784372e-06
Iter: 1070 loss: 1.56974636e-06
Iter: 1071 loss: 1.56760279e-06
Iter: 1072 loss: 1.5699635e-06
Iter: 1073 loss: 1.5664632e-06
Iter: 1074 loss: 1.56641272e-06
Iter: 1075 loss: 1.56539863e-06
Iter: 1076 loss: 1.56444855e-06
Iter: 1077 loss: 1.56267743e-06
Iter: 1078 loss: 1.5992631e-06
Iter: 1079 loss: 1.56265946e-06
Iter: 1080 loss: 1.56158262e-06
Iter: 1081 loss: 1.57412273e-06
Iter: 1082 loss: 1.56160047e-06
Iter: 1083 loss: 1.56066289e-06
Iter: 1084 loss: 1.56453166e-06
Iter: 1085 loss: 1.56044393e-06
Iter: 1086 loss: 1.55971497e-06
Iter: 1087 loss: 1.56038698e-06
Iter: 1088 loss: 1.55924067e-06
Iter: 1089 loss: 1.55816201e-06
Iter: 1090 loss: 1.55717498e-06
Iter: 1091 loss: 1.55691009e-06
Iter: 1092 loss: 1.55557223e-06
Iter: 1093 loss: 1.56735973e-06
Iter: 1094 loss: 1.55547116e-06
Iter: 1095 loss: 1.55401347e-06
Iter: 1096 loss: 1.55822727e-06
Iter: 1097 loss: 1.55351381e-06
Iter: 1098 loss: 1.55278781e-06
Iter: 1099 loss: 1.55643829e-06
Iter: 1100 loss: 1.55266957e-06
Iter: 1101 loss: 1.55172711e-06
Iter: 1102 loss: 1.55193754e-06
Iter: 1103 loss: 1.55104897e-06
Iter: 1104 loss: 1.55021769e-06
Iter: 1105 loss: 1.54899487e-06
Iter: 1106 loss: 1.54896111e-06
Iter: 1107 loss: 1.54711347e-06
Iter: 1108 loss: 1.54973338e-06
Iter: 1109 loss: 1.54623217e-06
Iter: 1110 loss: 1.54539771e-06
Iter: 1111 loss: 1.54496718e-06
Iter: 1112 loss: 1.54413351e-06
Iter: 1113 loss: 1.5426308e-06
Iter: 1114 loss: 1.5426341e-06
Iter: 1115 loss: 1.54173279e-06
Iter: 1116 loss: 1.54174688e-06
Iter: 1117 loss: 1.54087058e-06
Iter: 1118 loss: 1.54086979e-06
Iter: 1119 loss: 1.54017539e-06
Iter: 1120 loss: 1.5392327e-06
Iter: 1121 loss: 1.54243935e-06
Iter: 1122 loss: 1.53897849e-06
Iter: 1123 loss: 1.53799135e-06
Iter: 1124 loss: 1.53717451e-06
Iter: 1125 loss: 1.5369252e-06
Iter: 1126 loss: 1.53590463e-06
Iter: 1127 loss: 1.53589758e-06
Iter: 1128 loss: 1.53489191e-06
Iter: 1129 loss: 1.53277688e-06
Iter: 1130 loss: 1.56824012e-06
Iter: 1131 loss: 1.53267126e-06
Iter: 1132 loss: 1.5324988e-06
Iter: 1133 loss: 1.53183385e-06
Iter: 1134 loss: 1.53116275e-06
Iter: 1135 loss: 1.52977839e-06
Iter: 1136 loss: 1.5569193e-06
Iter: 1137 loss: 1.52974985e-06
Iter: 1138 loss: 1.52844882e-06
Iter: 1139 loss: 1.52937537e-06
Iter: 1140 loss: 1.52768837e-06
Iter: 1141 loss: 1.52619168e-06
Iter: 1142 loss: 1.53879023e-06
Iter: 1143 loss: 1.52618259e-06
Iter: 1144 loss: 1.52455982e-06
Iter: 1145 loss: 1.53140911e-06
Iter: 1146 loss: 1.52422831e-06
Iter: 1147 loss: 1.52335178e-06
Iter: 1148 loss: 1.52275754e-06
Iter: 1149 loss: 1.52239272e-06
Iter: 1150 loss: 1.52124517e-06
Iter: 1151 loss: 1.52123835e-06
Iter: 1152 loss: 1.52054895e-06
Iter: 1153 loss: 1.51969198e-06
Iter: 1154 loss: 1.51961262e-06
Iter: 1155 loss: 1.51847371e-06
Iter: 1156 loss: 1.52432358e-06
Iter: 1157 loss: 1.51823497e-06
Iter: 1158 loss: 1.51732513e-06
Iter: 1159 loss: 1.51820598e-06
Iter: 1160 loss: 1.51681786e-06
Iter: 1161 loss: 1.51533368e-06
Iter: 1162 loss: 1.52344774e-06
Iter: 1163 loss: 1.51513518e-06
Iter: 1164 loss: 1.51419204e-06
Iter: 1165 loss: 1.51323957e-06
Iter: 1166 loss: 1.51304084e-06
Iter: 1167 loss: 1.51151107e-06
Iter: 1168 loss: 1.53572955e-06
Iter: 1169 loss: 1.51152562e-06
Iter: 1170 loss: 1.5108069e-06
Iter: 1171 loss: 1.50898484e-06
Iter: 1172 loss: 1.52395819e-06
Iter: 1173 loss: 1.50864753e-06
Iter: 1174 loss: 1.50697019e-06
Iter: 1175 loss: 1.51598067e-06
Iter: 1176 loss: 1.50673543e-06
Iter: 1177 loss: 1.50614494e-06
Iter: 1178 loss: 1.50605115e-06
Iter: 1179 loss: 1.50532628e-06
Iter: 1180 loss: 1.50375377e-06
Iter: 1181 loss: 1.52508346e-06
Iter: 1182 loss: 1.50363871e-06
Iter: 1183 loss: 1.50244455e-06
Iter: 1184 loss: 1.50241226e-06
Iter: 1185 loss: 1.50120286e-06
Iter: 1186 loss: 1.50171149e-06
Iter: 1187 loss: 1.50033202e-06
Iter: 1188 loss: 1.49904008e-06
Iter: 1189 loss: 1.49966695e-06
Iter: 1190 loss: 1.49819539e-06
Iter: 1191 loss: 1.49683865e-06
Iter: 1192 loss: 1.50762639e-06
Iter: 1193 loss: 1.49679772e-06
Iter: 1194 loss: 1.49598452e-06
Iter: 1195 loss: 1.50086032e-06
Iter: 1196 loss: 1.49590142e-06
Iter: 1197 loss: 1.49494201e-06
Iter: 1198 loss: 1.49444782e-06
Iter: 1199 loss: 1.49404275e-06
Iter: 1200 loss: 1.49311199e-06
Iter: 1201 loss: 1.50160099e-06
Iter: 1202 loss: 1.49306504e-06
Iter: 1203 loss: 1.49196762e-06
Iter: 1204 loss: 1.49093148e-06
Iter: 1205 loss: 1.49067455e-06
Iter: 1206 loss: 1.48935328e-06
Iter: 1207 loss: 1.48763456e-06
Iter: 1208 loss: 1.48749234e-06
Iter: 1209 loss: 1.48553602e-06
Iter: 1210 loss: 1.50234018e-06
Iter: 1211 loss: 1.4854603e-06
Iter: 1212 loss: 1.48494621e-06
Iter: 1213 loss: 1.48462e-06
Iter: 1214 loss: 1.48414438e-06
Iter: 1215 loss: 1.4827765e-06
Iter: 1216 loss: 1.49099469e-06
Iter: 1217 loss: 1.48243862e-06
Iter: 1218 loss: 1.481445e-06
Iter: 1219 loss: 1.4813636e-06
Iter: 1220 loss: 1.48021911e-06
Iter: 1221 loss: 1.47829064e-06
Iter: 1222 loss: 1.47827814e-06
Iter: 1223 loss: 1.47655282e-06
Iter: 1224 loss: 1.4786126e-06
Iter: 1225 loss: 1.47562059e-06
Iter: 1226 loss: 1.47414903e-06
Iter: 1227 loss: 1.47415176e-06
Iter: 1228 loss: 1.47322021e-06
Iter: 1229 loss: 1.47941159e-06
Iter: 1230 loss: 1.4731346e-06
Iter: 1231 loss: 1.47222602e-06
Iter: 1232 loss: 1.47194078e-06
Iter: 1233 loss: 1.47145408e-06
Iter: 1234 loss: 1.47079641e-06
Iter: 1235 loss: 1.47079049e-06
Iter: 1236 loss: 1.47017408e-06
Iter: 1237 loss: 1.46858406e-06
Iter: 1238 loss: 1.47931871e-06
Iter: 1239 loss: 1.46812977e-06
Iter: 1240 loss: 1.46620278e-06
Iter: 1241 loss: 1.46880507e-06
Iter: 1242 loss: 1.46524553e-06
Iter: 1243 loss: 1.46342506e-06
Iter: 1244 loss: 1.47042965e-06
Iter: 1245 loss: 1.46296952e-06
Iter: 1246 loss: 1.46154366e-06
Iter: 1247 loss: 1.46438936e-06
Iter: 1248 loss: 1.46087154e-06
Iter: 1249 loss: 1.45937202e-06
Iter: 1250 loss: 1.46374396e-06
Iter: 1251 loss: 1.45889248e-06
Iter: 1252 loss: 1.45787749e-06
Iter: 1253 loss: 1.45773834e-06
Iter: 1254 loss: 1.45688864e-06
Iter: 1255 loss: 1.45469721e-06
Iter: 1256 loss: 1.47477613e-06
Iter: 1257 loss: 1.45440788e-06
Iter: 1258 loss: 1.45255547e-06
Iter: 1259 loss: 1.46146533e-06
Iter: 1260 loss: 1.45221929e-06
Iter: 1261 loss: 1.45155957e-06
Iter: 1262 loss: 1.45137108e-06
Iter: 1263 loss: 1.45059892e-06
Iter: 1264 loss: 1.44938008e-06
Iter: 1265 loss: 1.44937314e-06
Iter: 1266 loss: 1.44814146e-06
Iter: 1267 loss: 1.45711476e-06
Iter: 1268 loss: 1.44801743e-06
Iter: 1269 loss: 1.44714363e-06
Iter: 1270 loss: 1.44969908e-06
Iter: 1271 loss: 1.446841e-06
Iter: 1272 loss: 1.4454705e-06
Iter: 1273 loss: 1.44642377e-06
Iter: 1274 loss: 1.4446714e-06
Iter: 1275 loss: 1.44381499e-06
Iter: 1276 loss: 1.44943215e-06
Iter: 1277 loss: 1.44371666e-06
Iter: 1278 loss: 1.44272337e-06
Iter: 1279 loss: 1.44305545e-06
Iter: 1280 loss: 1.44204591e-06
Iter: 1281 loss: 1.44124226e-06
Iter: 1282 loss: 1.44021578e-06
Iter: 1283 loss: 1.44013177e-06
Iter: 1284 loss: 1.43889679e-06
Iter: 1285 loss: 1.44758212e-06
Iter: 1286 loss: 1.43880129e-06
Iter: 1287 loss: 1.43770467e-06
Iter: 1288 loss: 1.45162289e-06
Iter: 1289 loss: 1.43767295e-06
Iter: 1290 loss: 1.43708166e-06
Iter: 1291 loss: 1.43558304e-06
Iter: 1292 loss: 1.45101535e-06
Iter: 1293 loss: 1.43536522e-06
Iter: 1294 loss: 1.43387979e-06
Iter: 1295 loss: 1.43925558e-06
Iter: 1296 loss: 1.43347893e-06
Iter: 1297 loss: 1.43278066e-06
Iter: 1298 loss: 1.43252123e-06
Iter: 1299 loss: 1.43208354e-06
Iter: 1300 loss: 1.43104796e-06
Iter: 1301 loss: 1.44439377e-06
Iter: 1302 loss: 1.43098555e-06
Iter: 1303 loss: 1.42978638e-06
Iter: 1304 loss: 1.44012074e-06
Iter: 1305 loss: 1.42973761e-06
Iter: 1306 loss: 1.42890963e-06
Iter: 1307 loss: 1.43454713e-06
Iter: 1308 loss: 1.42879867e-06
Iter: 1309 loss: 1.4279633e-06
Iter: 1310 loss: 1.42704994e-06
Iter: 1311 loss: 1.42691306e-06
Iter: 1312 loss: 1.42599663e-06
Iter: 1313 loss: 1.43973398e-06
Iter: 1314 loss: 1.42598651e-06
Iter: 1315 loss: 1.4250719e-06
Iter: 1316 loss: 1.42406043e-06
Iter: 1317 loss: 1.42392855e-06
Iter: 1318 loss: 1.42292424e-06
Iter: 1319 loss: 1.42269096e-06
Iter: 1320 loss: 1.42205954e-06
Iter: 1321 loss: 1.42097497e-06
Iter: 1322 loss: 1.43496572e-06
Iter: 1323 loss: 1.42096519e-06
Iter: 1324 loss: 1.41980888e-06
Iter: 1325 loss: 1.4239738e-06
Iter: 1326 loss: 1.41950306e-06
Iter: 1327 loss: 1.41888381e-06
Iter: 1328 loss: 1.41731653e-06
Iter: 1329 loss: 1.43432317e-06
Iter: 1330 loss: 1.41714145e-06
Iter: 1331 loss: 1.41578448e-06
Iter: 1332 loss: 1.41577834e-06
Iter: 1333 loss: 1.41458838e-06
Iter: 1334 loss: 1.42549675e-06
Iter: 1335 loss: 1.41455507e-06
Iter: 1336 loss: 1.41409487e-06
Iter: 1337 loss: 1.4132122e-06
Iter: 1338 loss: 1.43136197e-06
Iter: 1339 loss: 1.41321743e-06
Iter: 1340 loss: 1.41245971e-06
Iter: 1341 loss: 1.41243845e-06
Iter: 1342 loss: 1.41184501e-06
Iter: 1343 loss: 1.41251462e-06
Iter: 1344 loss: 1.41149746e-06
Iter: 1345 loss: 1.41067244e-06
Iter: 1346 loss: 1.41081944e-06
Iter: 1347 loss: 1.41004841e-06
Iter: 1348 loss: 1.40921861e-06
Iter: 1349 loss: 1.41939233e-06
Iter: 1350 loss: 1.40921429e-06
Iter: 1351 loss: 1.40848465e-06
Iter: 1352 loss: 1.40702764e-06
Iter: 1353 loss: 1.43177886e-06
Iter: 1354 loss: 1.40696397e-06
Iter: 1355 loss: 1.40573343e-06
Iter: 1356 loss: 1.41000885e-06
Iter: 1357 loss: 1.40542761e-06
Iter: 1358 loss: 1.40497139e-06
Iter: 1359 loss: 1.40488885e-06
Iter: 1360 loss: 1.40426573e-06
Iter: 1361 loss: 1.40326983e-06
Iter: 1362 loss: 1.40328711e-06
Iter: 1363 loss: 1.4022296e-06
Iter: 1364 loss: 1.4018965e-06
Iter: 1365 loss: 1.40127918e-06
Iter: 1366 loss: 1.40053908e-06
Iter: 1367 loss: 1.40047609e-06
Iter: 1368 loss: 1.3995118e-06
Iter: 1369 loss: 1.39901886e-06
Iter: 1370 loss: 1.39862709e-06
Iter: 1371 loss: 1.39776455e-06
Iter: 1372 loss: 1.39817644e-06
Iter: 1373 loss: 1.39721294e-06
Iter: 1374 loss: 1.39658459e-06
Iter: 1375 loss: 1.396485e-06
Iter: 1376 loss: 1.3959941e-06
Iter: 1377 loss: 1.39539065e-06
Iter: 1378 loss: 1.39530732e-06
Iter: 1379 loss: 1.39425958e-06
Iter: 1380 loss: 1.39707447e-06
Iter: 1381 loss: 1.39392614e-06
Iter: 1382 loss: 1.39285612e-06
Iter: 1383 loss: 1.39810197e-06
Iter: 1384 loss: 1.39266626e-06
Iter: 1385 loss: 1.39184863e-06
Iter: 1386 loss: 1.39030885e-06
Iter: 1387 loss: 1.42782051e-06
Iter: 1388 loss: 1.39032431e-06
Iter: 1389 loss: 1.38908524e-06
Iter: 1390 loss: 1.39680662e-06
Iter: 1391 loss: 1.38891528e-06
Iter: 1392 loss: 1.38828398e-06
Iter: 1393 loss: 1.38820837e-06
Iter: 1394 loss: 1.3878207e-06
Iter: 1395 loss: 1.38678524e-06
Iter: 1396 loss: 1.39500685e-06
Iter: 1397 loss: 1.38659698e-06
Iter: 1398 loss: 1.38511768e-06
Iter: 1399 loss: 1.38644384e-06
Iter: 1400 loss: 1.38429345e-06
Iter: 1401 loss: 1.38385849e-06
Iter: 1402 loss: 1.38336031e-06
Iter: 1403 loss: 1.38273617e-06
Iter: 1404 loss: 1.38122323e-06
Iter: 1405 loss: 1.4012893e-06
Iter: 1406 loss: 1.38116502e-06
Iter: 1407 loss: 1.38038513e-06
Iter: 1408 loss: 1.38039286e-06
Iter: 1409 loss: 1.37963548e-06
Iter: 1410 loss: 1.38216831e-06
Iter: 1411 loss: 1.37936968e-06
Iter: 1412 loss: 1.37889469e-06
Iter: 1413 loss: 1.37942038e-06
Iter: 1414 loss: 1.37856671e-06
Iter: 1415 loss: 1.37780614e-06
Iter: 1416 loss: 1.37900577e-06
Iter: 1417 loss: 1.37739653e-06
Iter: 1418 loss: 1.37639859e-06
Iter: 1419 loss: 1.37766415e-06
Iter: 1420 loss: 1.3758804e-06
Iter: 1421 loss: 1.37477298e-06
Iter: 1422 loss: 1.37325878e-06
Iter: 1423 loss: 1.37316692e-06
Iter: 1424 loss: 1.37234542e-06
Iter: 1425 loss: 1.37226084e-06
Iter: 1426 loss: 1.37126585e-06
Iter: 1427 loss: 1.37277607e-06
Iter: 1428 loss: 1.37081872e-06
Iter: 1429 loss: 1.37026609e-06
Iter: 1430 loss: 1.36921062e-06
Iter: 1431 loss: 1.39277267e-06
Iter: 1432 loss: 1.36923745e-06
Iter: 1433 loss: 1.3683682e-06
Iter: 1434 loss: 1.368364e-06
Iter: 1435 loss: 1.36738413e-06
Iter: 1436 loss: 1.36756512e-06
Iter: 1437 loss: 1.36662709e-06
Iter: 1438 loss: 1.36568815e-06
Iter: 1439 loss: 1.3653538e-06
Iter: 1440 loss: 1.3648546e-06
Iter: 1441 loss: 1.36414405e-06
Iter: 1442 loss: 1.36403037e-06
Iter: 1443 loss: 1.36348638e-06
Iter: 1444 loss: 1.36275287e-06
Iter: 1445 loss: 1.36273411e-06
Iter: 1446 loss: 1.36199321e-06
Iter: 1447 loss: 1.3619931e-06
Iter: 1448 loss: 1.36156507e-06
Iter: 1449 loss: 1.36139852e-06
Iter: 1450 loss: 1.36109702e-06
Iter: 1451 loss: 1.36031656e-06
Iter: 1452 loss: 1.36077415e-06
Iter: 1453 loss: 1.35977677e-06
Iter: 1454 loss: 1.35885443e-06
Iter: 1455 loss: 1.359301e-06
Iter: 1456 loss: 1.35819914e-06
Iter: 1457 loss: 1.35786036e-06
Iter: 1458 loss: 1.35762548e-06
Iter: 1459 loss: 1.35716436e-06
Iter: 1460 loss: 1.35603068e-06
Iter: 1461 loss: 1.36741323e-06
Iter: 1462 loss: 1.35585515e-06
Iter: 1463 loss: 1.35476739e-06
Iter: 1464 loss: 1.35549658e-06
Iter: 1465 loss: 1.35410028e-06
Iter: 1466 loss: 1.35390792e-06
Iter: 1467 loss: 1.35356447e-06
Iter: 1468 loss: 1.35294715e-06
Iter: 1469 loss: 1.35208677e-06
Iter: 1470 loss: 1.35204482e-06
Iter: 1471 loss: 1.35106643e-06
Iter: 1472 loss: 1.35167e-06
Iter: 1473 loss: 1.35048163e-06
Iter: 1474 loss: 1.34929269e-06
Iter: 1475 loss: 1.36768631e-06
Iter: 1476 loss: 1.34929508e-06
Iter: 1477 loss: 1.34883362e-06
Iter: 1478 loss: 1.34810386e-06
Iter: 1479 loss: 1.34811512e-06
Iter: 1480 loss: 1.34719471e-06
Iter: 1481 loss: 1.35762116e-06
Iter: 1482 loss: 1.34717664e-06
Iter: 1483 loss: 1.3466356e-06
Iter: 1484 loss: 1.34611741e-06
Iter: 1485 loss: 1.34599122e-06
Iter: 1486 loss: 1.34502841e-06
Iter: 1487 loss: 1.34817697e-06
Iter: 1488 loss: 1.3447393e-06
Iter: 1489 loss: 1.3439294e-06
Iter: 1490 loss: 1.34691732e-06
Iter: 1491 loss: 1.34373136e-06
Iter: 1492 loss: 1.34256459e-06
Iter: 1493 loss: 1.34467882e-06
Iter: 1494 loss: 1.34209927e-06
Iter: 1495 loss: 1.34125264e-06
Iter: 1496 loss: 1.34009315e-06
Iter: 1497 loss: 1.34004074e-06
Iter: 1498 loss: 1.33888398e-06
Iter: 1499 loss: 1.34489073e-06
Iter: 1500 loss: 1.33863045e-06
Iter: 1501 loss: 1.33797505e-06
Iter: 1502 loss: 1.33791991e-06
Iter: 1503 loss: 1.3374738e-06
Iter: 1504 loss: 1.33627816e-06
Iter: 1505 loss: 1.34637321e-06
Iter: 1506 loss: 1.33611286e-06
Iter: 1507 loss: 1.3357751e-06
Iter: 1508 loss: 1.3354927e-06
Iter: 1509 loss: 1.33484764e-06
Iter: 1510 loss: 1.33350943e-06
Iter: 1511 loss: 1.35478376e-06
Iter: 1512 loss: 1.33348681e-06
Iter: 1513 loss: 1.33274523e-06
Iter: 1514 loss: 1.3326835e-06
Iter: 1515 loss: 1.33202184e-06
Iter: 1516 loss: 1.33096876e-06
Iter: 1517 loss: 1.33095818e-06
Iter: 1518 loss: 1.32995683e-06
Iter: 1519 loss: 1.33295566e-06
Iter: 1520 loss: 1.32968739e-06
Iter: 1521 loss: 1.32848515e-06
Iter: 1522 loss: 1.33023627e-06
Iter: 1523 loss: 1.32801119e-06
Iter: 1524 loss: 1.32688444e-06
Iter: 1525 loss: 1.3310439e-06
Iter: 1526 loss: 1.32663786e-06
Iter: 1527 loss: 1.32533228e-06
Iter: 1528 loss: 1.33153821e-06
Iter: 1529 loss: 1.3250891e-06
Iter: 1530 loss: 1.32438822e-06
Iter: 1531 loss: 1.32303467e-06
Iter: 1532 loss: 1.35402763e-06
Iter: 1533 loss: 1.32306013e-06
Iter: 1534 loss: 1.32205287e-06
Iter: 1535 loss: 1.32204536e-06
Iter: 1536 loss: 1.32103992e-06
Iter: 1537 loss: 1.32415039e-06
Iter: 1538 loss: 1.32074979e-06
Iter: 1539 loss: 1.32019045e-06
Iter: 1540 loss: 1.31970091e-06
Iter: 1541 loss: 1.31956949e-06
Iter: 1542 loss: 1.31842103e-06
Iter: 1543 loss: 1.32772254e-06
Iter: 1544 loss: 1.31836646e-06
Iter: 1545 loss: 1.31775232e-06
Iter: 1546 loss: 1.3166989e-06
Iter: 1547 loss: 1.31668946e-06
Iter: 1548 loss: 1.3158151e-06
Iter: 1549 loss: 1.31581055e-06
Iter: 1550 loss: 1.31526281e-06
Iter: 1551 loss: 1.31427782e-06
Iter: 1552 loss: 1.3357901e-06
Iter: 1553 loss: 1.31427896e-06
Iter: 1554 loss: 1.31337265e-06
Iter: 1555 loss: 1.32264927e-06
Iter: 1556 loss: 1.31335401e-06
Iter: 1557 loss: 1.31262277e-06
Iter: 1558 loss: 1.31292018e-06
Iter: 1559 loss: 1.31209561e-06
Iter: 1560 loss: 1.31140609e-06
Iter: 1561 loss: 1.31140143e-06
Iter: 1562 loss: 1.31077149e-06
Iter: 1563 loss: 1.30924877e-06
Iter: 1564 loss: 1.32323385e-06
Iter: 1565 loss: 1.30899582e-06
Iter: 1566 loss: 1.30740727e-06
Iter: 1567 loss: 1.31558841e-06
Iter: 1568 loss: 1.30712419e-06
Iter: 1569 loss: 1.30659487e-06
Iter: 1570 loss: 1.30643366e-06
Iter: 1571 loss: 1.30597732e-06
Iter: 1572 loss: 1.30495187e-06
Iter: 1573 loss: 1.31715638e-06
Iter: 1574 loss: 1.30485796e-06
Iter: 1575 loss: 1.30453407e-06
Iter: 1576 loss: 1.30428793e-06
Iter: 1577 loss: 1.3038391e-06
Iter: 1578 loss: 1.30277704e-06
Iter: 1579 loss: 1.31574302e-06
Iter: 1580 loss: 1.30265641e-06
Iter: 1581 loss: 1.30178523e-06
Iter: 1582 loss: 1.30178523e-06
Iter: 1583 loss: 1.30089632e-06
Iter: 1584 loss: 1.29955356e-06
Iter: 1585 loss: 1.2995049e-06
Iter: 1586 loss: 1.29835109e-06
Iter: 1587 loss: 1.3025857e-06
Iter: 1588 loss: 1.29803959e-06
Iter: 1589 loss: 1.29697844e-06
Iter: 1590 loss: 1.30590092e-06
Iter: 1591 loss: 1.29689249e-06
Iter: 1592 loss: 1.29618093e-06
Iter: 1593 loss: 1.29922569e-06
Iter: 1594 loss: 1.2960453e-06
Iter: 1595 loss: 1.29517662e-06
Iter: 1596 loss: 1.29540331e-06
Iter: 1597 loss: 1.29453554e-06
Iter: 1598 loss: 1.29365412e-06
Iter: 1599 loss: 1.292447e-06
Iter: 1600 loss: 1.29237776e-06
Iter: 1601 loss: 1.29164459e-06
Iter: 1602 loss: 1.29147566e-06
Iter: 1603 loss: 1.29044531e-06
Iter: 1604 loss: 1.29034743e-06
Iter: 1605 loss: 1.28961881e-06
Iter: 1606 loss: 1.28890565e-06
Iter: 1607 loss: 1.29308535e-06
Iter: 1608 loss: 1.28881857e-06
Iter: 1609 loss: 1.28790202e-06
Iter: 1610 loss: 1.28806846e-06
Iter: 1611 loss: 1.28720308e-06
Iter: 1612 loss: 1.28649117e-06
Iter: 1613 loss: 1.28839929e-06
Iter: 1614 loss: 1.28627505e-06
Iter: 1615 loss: 1.28526665e-06
Iter: 1616 loss: 1.28771171e-06
Iter: 1617 loss: 1.28493571e-06
Iter: 1618 loss: 1.28409704e-06
Iter: 1619 loss: 1.28297756e-06
Iter: 1620 loss: 1.282933e-06
Iter: 1621 loss: 1.28170814e-06
Iter: 1622 loss: 1.29746991e-06
Iter: 1623 loss: 1.28171519e-06
Iter: 1624 loss: 1.28084901e-06
Iter: 1625 loss: 1.28241527e-06
Iter: 1626 loss: 1.28049169e-06
Iter: 1627 loss: 1.27964017e-06
Iter: 1628 loss: 1.28762952e-06
Iter: 1629 loss: 1.27958174e-06
Iter: 1630 loss: 1.27905423e-06
Iter: 1631 loss: 1.27802855e-06
Iter: 1632 loss: 1.30030458e-06
Iter: 1633 loss: 1.2780165e-06
Iter: 1634 loss: 1.2769176e-06
Iter: 1635 loss: 1.28052204e-06
Iter: 1636 loss: 1.27662588e-06
Iter: 1637 loss: 1.27547878e-06
Iter: 1638 loss: 1.29219291e-06
Iter: 1639 loss: 1.27548378e-06
Iter: 1640 loss: 1.27496082e-06
Iter: 1641 loss: 1.2737919e-06
Iter: 1642 loss: 1.29422415e-06
Iter: 1643 loss: 1.27373403e-06
Iter: 1644 loss: 1.27279873e-06
Iter: 1645 loss: 1.27274848e-06
Iter: 1646 loss: 1.27220756e-06
Iter: 1647 loss: 1.27112708e-06
Iter: 1648 loss: 1.28908289e-06
Iter: 1649 loss: 1.27109615e-06
Iter: 1650 loss: 1.26981695e-06
Iter: 1651 loss: 1.28520742e-06
Iter: 1652 loss: 1.26982241e-06
Iter: 1653 loss: 1.26920838e-06
Iter: 1654 loss: 1.26806799e-06
Iter: 1655 loss: 1.29099044e-06
Iter: 1656 loss: 1.26804548e-06
Iter: 1657 loss: 1.26674877e-06
Iter: 1658 loss: 1.2696961e-06
Iter: 1659 loss: 1.26624434e-06
Iter: 1660 loss: 1.26477971e-06
Iter: 1661 loss: 1.27963585e-06
Iter: 1662 loss: 1.26474379e-06
Iter: 1663 loss: 1.26394775e-06
Iter: 1664 loss: 1.26727377e-06
Iter: 1665 loss: 1.26374698e-06
Iter: 1666 loss: 1.26290183e-06
Iter: 1667 loss: 1.2629896e-06
Iter: 1668 loss: 1.26228201e-06
Iter: 1669 loss: 1.26137036e-06
Iter: 1670 loss: 1.26029386e-06
Iter: 1671 loss: 1.26020257e-06
Iter: 1672 loss: 1.25995871e-06
Iter: 1673 loss: 1.2595392e-06
Iter: 1674 loss: 1.25885538e-06
Iter: 1675 loss: 1.25782788e-06
Iter: 1676 loss: 1.25781673e-06
Iter: 1677 loss: 1.25714064e-06
Iter: 1678 loss: 1.25712279e-06
Iter: 1679 loss: 1.25643146e-06
Iter: 1680 loss: 1.25538531e-06
Iter: 1681 loss: 1.25537974e-06
Iter: 1682 loss: 1.25454847e-06
Iter: 1683 loss: 1.25768793e-06
Iter: 1684 loss: 1.25434315e-06
Iter: 1685 loss: 1.25323118e-06
Iter: 1686 loss: 1.25728479e-06
Iter: 1687 loss: 1.25297856e-06
Iter: 1688 loss: 1.25229201e-06
Iter: 1689 loss: 1.25109273e-06
Iter: 1690 loss: 1.27820408e-06
Iter: 1691 loss: 1.25110682e-06
Iter: 1692 loss: 1.25007136e-06
Iter: 1693 loss: 1.25007591e-06
Iter: 1694 loss: 1.24916528e-06
Iter: 1695 loss: 1.25012218e-06
Iter: 1696 loss: 1.24872827e-06
Iter: 1697 loss: 1.24785367e-06
Iter: 1698 loss: 1.25496206e-06
Iter: 1699 loss: 1.24783969e-06
Iter: 1700 loss: 1.24713654e-06
Iter: 1701 loss: 1.24639007e-06
Iter: 1702 loss: 1.24633539e-06
Iter: 1703 loss: 1.24559e-06
Iter: 1704 loss: 1.2545454e-06
Iter: 1705 loss: 1.24557278e-06
Iter: 1706 loss: 1.2448694e-06
Iter: 1707 loss: 1.24702683e-06
Iter: 1708 loss: 1.24467135e-06
Iter: 1709 loss: 1.24418943e-06
Iter: 1710 loss: 1.24484757e-06
Iter: 1711 loss: 1.24388862e-06
Iter: 1712 loss: 1.24307849e-06
Iter: 1713 loss: 1.24372662e-06
Iter: 1714 loss: 1.24262647e-06
Iter: 1715 loss: 1.24183475e-06
Iter: 1716 loss: 1.2408666e-06
Iter: 1717 loss: 1.24077974e-06
Iter: 1718 loss: 1.24070209e-06
Iter: 1719 loss: 1.24020664e-06
Iter: 1720 loss: 1.23980431e-06
Iter: 1721 loss: 1.2388432e-06
Iter: 1722 loss: 1.24839607e-06
Iter: 1723 loss: 1.23873087e-06
Iter: 1724 loss: 1.23772747e-06
Iter: 1725 loss: 1.23867551e-06
Iter: 1726 loss: 1.23721361e-06
Iter: 1727 loss: 1.23668178e-06
Iter: 1728 loss: 1.23644668e-06
Iter: 1729 loss: 1.2358397e-06
Iter: 1730 loss: 1.23559721e-06
Iter: 1731 loss: 1.23525888e-06
Iter: 1732 loss: 1.23432733e-06
Iter: 1733 loss: 1.23785901e-06
Iter: 1734 loss: 1.23410064e-06
Iter: 1735 loss: 1.23339748e-06
Iter: 1736 loss: 1.23286418e-06
Iter: 1737 loss: 1.23261748e-06
Iter: 1738 loss: 1.23205609e-06
Iter: 1739 loss: 1.23194263e-06
Iter: 1740 loss: 1.2314232e-06
Iter: 1741 loss: 1.23092218e-06
Iter: 1742 loss: 1.23077075e-06
Iter: 1743 loss: 1.23007703e-06
Iter: 1744 loss: 1.23950031e-06
Iter: 1745 loss: 1.23003565e-06
Iter: 1746 loss: 1.22956806e-06
Iter: 1747 loss: 1.22838219e-06
Iter: 1748 loss: 1.24492226e-06
Iter: 1749 loss: 1.22833285e-06
Iter: 1750 loss: 1.22744814e-06
Iter: 1751 loss: 1.23631207e-06
Iter: 1752 loss: 1.22739664e-06
Iter: 1753 loss: 1.22643928e-06
Iter: 1754 loss: 1.22958227e-06
Iter: 1755 loss: 1.22618508e-06
Iter: 1756 loss: 1.22561266e-06
Iter: 1757 loss: 1.22475433e-06
Iter: 1758 loss: 1.22472363e-06
Iter: 1759 loss: 1.22369602e-06
Iter: 1760 loss: 1.22507367e-06
Iter: 1761 loss: 1.22321239e-06
Iter: 1762 loss: 1.22268739e-06
Iter: 1763 loss: 1.22251572e-06
Iter: 1764 loss: 1.22195308e-06
Iter: 1765 loss: 1.22142148e-06
Iter: 1766 loss: 1.22127096e-06
Iter: 1767 loss: 1.22051244e-06
Iter: 1768 loss: 1.22430663e-06
Iter: 1769 loss: 1.22038466e-06
Iter: 1770 loss: 1.21961341e-06
Iter: 1771 loss: 1.21937592e-06
Iter: 1772 loss: 1.21895948e-06
Iter: 1773 loss: 1.21823973e-06
Iter: 1774 loss: 1.21816197e-06
Iter: 1775 loss: 1.217722e-06
Iter: 1776 loss: 1.21696939e-06
Iter: 1777 loss: 1.21697258e-06
Iter: 1778 loss: 1.21599987e-06
Iter: 1779 loss: 1.22684128e-06
Iter: 1780 loss: 1.21595554e-06
Iter: 1781 loss: 1.21540063e-06
Iter: 1782 loss: 1.21406902e-06
Iter: 1783 loss: 1.22906181e-06
Iter: 1784 loss: 1.2139019e-06
Iter: 1785 loss: 1.21345249e-06
Iter: 1786 loss: 1.21319692e-06
Iter: 1787 loss: 1.21252731e-06
Iter: 1788 loss: 1.21155585e-06
Iter: 1789 loss: 1.21150185e-06
Iter: 1790 loss: 1.2105354e-06
Iter: 1791 loss: 1.21029711e-06
Iter: 1792 loss: 1.20967434e-06
Iter: 1793 loss: 1.20845323e-06
Iter: 1794 loss: 1.21283756e-06
Iter: 1795 loss: 1.20813388e-06
Iter: 1796 loss: 1.20778031e-06
Iter: 1797 loss: 1.20748928e-06
Iter: 1798 loss: 1.20702759e-06
Iter: 1799 loss: 1.20615994e-06
Iter: 1800 loss: 1.22789538e-06
Iter: 1801 loss: 1.20616255e-06
Iter: 1802 loss: 1.20532059e-06
Iter: 1803 loss: 1.21132643e-06
Iter: 1804 loss: 1.20525692e-06
Iter: 1805 loss: 1.20462096e-06
Iter: 1806 loss: 1.20856544e-06
Iter: 1807 loss: 1.20454638e-06
Iter: 1808 loss: 1.203946e-06
Iter: 1809 loss: 1.20620643e-06
Iter: 1810 loss: 1.20380446e-06
Iter: 1811 loss: 1.20339337e-06
Iter: 1812 loss: 1.20321624e-06
Iter: 1813 loss: 1.20302161e-06
Iter: 1814 loss: 1.20220864e-06
Iter: 1815 loss: 1.20634229e-06
Iter: 1816 loss: 1.20205812e-06
Iter: 1817 loss: 1.20162156e-06
Iter: 1818 loss: 1.20047457e-06
Iter: 1819 loss: 1.21077289e-06
Iter: 1820 loss: 1.20028028e-06
Iter: 1821 loss: 1.19989261e-06
Iter: 1822 loss: 1.19949209e-06
Iter: 1823 loss: 1.19895265e-06
Iter: 1824 loss: 1.19770698e-06
Iter: 1825 loss: 1.2166181e-06
Iter: 1826 loss: 1.19767128e-06
Iter: 1827 loss: 1.19653873e-06
Iter: 1828 loss: 1.19636218e-06
Iter: 1829 loss: 1.19553169e-06
Iter: 1830 loss: 1.19475453e-06
Iter: 1831 loss: 1.19464687e-06
Iter: 1832 loss: 1.19373419e-06
Iter: 1833 loss: 1.19678668e-06
Iter: 1834 loss: 1.19349647e-06
Iter: 1835 loss: 1.19282709e-06
Iter: 1836 loss: 1.19194965e-06
Iter: 1837 loss: 1.19189008e-06
Iter: 1838 loss: 1.19078868e-06
Iter: 1839 loss: 1.20315065e-06
Iter: 1840 loss: 1.19078982e-06
Iter: 1841 loss: 1.19009223e-06
Iter: 1842 loss: 1.19589993e-06
Iter: 1843 loss: 1.19004983e-06
Iter: 1844 loss: 1.18950743e-06
Iter: 1845 loss: 1.18908724e-06
Iter: 1846 loss: 1.18885941e-06
Iter: 1847 loss: 1.18838534e-06
Iter: 1848 loss: 1.18841217e-06
Iter: 1849 loss: 1.18795128e-06
Iter: 1850 loss: 1.18695311e-06
Iter: 1851 loss: 1.19959088e-06
Iter: 1852 loss: 1.18689059e-06
Iter: 1853 loss: 1.1859338e-06
Iter: 1854 loss: 1.18936e-06
Iter: 1855 loss: 1.18570881e-06
Iter: 1856 loss: 1.18445041e-06
Iter: 1857 loss: 1.19067067e-06
Iter: 1858 loss: 1.18426476e-06
Iter: 1859 loss: 1.18357616e-06
Iter: 1860 loss: 1.18233891e-06
Iter: 1861 loss: 1.21178834e-06
Iter: 1862 loss: 1.18234664e-06
Iter: 1863 loss: 1.18107107e-06
Iter: 1864 loss: 1.18684761e-06
Iter: 1865 loss: 1.1808138e-06
Iter: 1866 loss: 1.18044636e-06
Iter: 1867 loss: 1.18031221e-06
Iter: 1868 loss: 1.17980267e-06
Iter: 1869 loss: 1.17874993e-06
Iter: 1870 loss: 1.19735409e-06
Iter: 1871 loss: 1.17874447e-06
Iter: 1872 loss: 1.17772913e-06
Iter: 1873 loss: 1.1811851e-06
Iter: 1874 loss: 1.17747356e-06
Iter: 1875 loss: 1.17629713e-06
Iter: 1876 loss: 1.1828032e-06
Iter: 1877 loss: 1.17614354e-06
Iter: 1878 loss: 1.17517129e-06
Iter: 1879 loss: 1.17918103e-06
Iter: 1880 loss: 1.17497234e-06
Iter: 1881 loss: 1.17425475e-06
Iter: 1882 loss: 1.17475304e-06
Iter: 1883 loss: 1.17383615e-06
Iter: 1884 loss: 1.1729428e-06
Iter: 1885 loss: 1.18083676e-06
Iter: 1886 loss: 1.17291438e-06
Iter: 1887 loss: 1.17252875e-06
Iter: 1888 loss: 1.17154491e-06
Iter: 1889 loss: 1.18296259e-06
Iter: 1890 loss: 1.17148011e-06
Iter: 1891 loss: 1.17115644e-06
Iter: 1892 loss: 1.1709617e-06
Iter: 1893 loss: 1.17042464e-06
Iter: 1894 loss: 1.16929846e-06
Iter: 1895 loss: 1.18722232e-06
Iter: 1896 loss: 1.16927538e-06
Iter: 1897 loss: 1.16804608e-06
Iter: 1898 loss: 1.16791773e-06
Iter: 1899 loss: 1.16703632e-06
Iter: 1900 loss: 1.16609135e-06
Iter: 1901 loss: 1.1660826e-06
Iter: 1902 loss: 1.16517708e-06
Iter: 1903 loss: 1.17022171e-06
Iter: 1904 loss: 1.16504702e-06
Iter: 1905 loss: 1.16458227e-06
Iter: 1906 loss: 1.16377691e-06
Iter: 1907 loss: 1.16376259e-06
Iter: 1908 loss: 1.16327101e-06
Iter: 1909 loss: 1.16324509e-06
Iter: 1910 loss: 1.16270678e-06
Iter: 1911 loss: 1.16274498e-06
Iter: 1912 loss: 1.1623024e-06
Iter: 1913 loss: 1.16144076e-06
Iter: 1914 loss: 1.16228716e-06
Iter: 1915 loss: 1.1609485e-06
Iter: 1916 loss: 1.16023534e-06
Iter: 1917 loss: 1.16974491e-06
Iter: 1918 loss: 1.16024376e-06
Iter: 1919 loss: 1.15963985e-06
Iter: 1920 loss: 1.15888133e-06
Iter: 1921 loss: 1.15882483e-06
Iter: 1922 loss: 1.15813714e-06
Iter: 1923 loss: 1.1594185e-06
Iter: 1924 loss: 1.15787225e-06
Iter: 1925 loss: 1.15713306e-06
Iter: 1926 loss: 1.1675844e-06
Iter: 1927 loss: 1.15713442e-06
Iter: 1928 loss: 1.15674925e-06
Iter: 1929 loss: 1.15582e-06
Iter: 1930 loss: 1.16665842e-06
Iter: 1931 loss: 1.15574403e-06
Iter: 1932 loss: 1.15466287e-06
Iter: 1933 loss: 1.15591524e-06
Iter: 1934 loss: 1.15405794e-06
Iter: 1935 loss: 1.15332705e-06
Iter: 1936 loss: 1.15327111e-06
Iter: 1937 loss: 1.15241096e-06
Iter: 1938 loss: 1.15319312e-06
Iter: 1939 loss: 1.15188243e-06
Iter: 1940 loss: 1.15130103e-06
Iter: 1941 loss: 1.15111186e-06
Iter: 1942 loss: 1.15081752e-06
Iter: 1943 loss: 1.15042621e-06
Iter: 1944 loss: 1.15036664e-06
Iter: 1945 loss: 1.14999125e-06
Iter: 1946 loss: 1.14933573e-06
Iter: 1947 loss: 1.16443528e-06
Iter: 1948 loss: 1.14933596e-06
Iter: 1949 loss: 1.14854379e-06
Iter: 1950 loss: 1.1556042e-06
Iter: 1951 loss: 1.14854811e-06
Iter: 1952 loss: 1.14790441e-06
Iter: 1953 loss: 1.14944987e-06
Iter: 1954 loss: 1.14764862e-06
Iter: 1955 loss: 1.14700981e-06
Iter: 1956 loss: 1.14597049e-06
Iter: 1957 loss: 1.14596799e-06
Iter: 1958 loss: 1.14498494e-06
Iter: 1959 loss: 1.14907152e-06
Iter: 1960 loss: 1.14476086e-06
Iter: 1961 loss: 1.14406532e-06
Iter: 1962 loss: 1.14404804e-06
Iter: 1963 loss: 1.14362877e-06
Iter: 1964 loss: 1.14268937e-06
Iter: 1965 loss: 1.15401099e-06
Iter: 1966 loss: 1.14263025e-06
Iter: 1967 loss: 1.1416239e-06
Iter: 1968 loss: 1.14275099e-06
Iter: 1969 loss: 1.1411388e-06
Iter: 1970 loss: 1.13985993e-06
Iter: 1971 loss: 1.14591455e-06
Iter: 1972 loss: 1.13967474e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi0.8
+ date
Wed Nov  4 12:20:08 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.4/300_300_300_1 --function f2 --psi -1 --alpha 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8abdcf9048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a7b7f0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a7b7a5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a546b8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a546b89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a546b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a54636158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a54645a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a5466a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a7b7446a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a7b744ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a5457a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a54537400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a54537510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a54609488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a545ae400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a545bd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a5449f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a544af1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a544be8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a544af488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a544dec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a544996a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a5447e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a54482840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a54441c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a543632f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a54363488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a54363378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a54347730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a54347510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a542ee0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a5430c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a542ee268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a5430cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a54233620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.012187172
test_loss: 0.012797089
train_loss: 0.008935785
test_loss: 0.010301734
train_loss: 0.008017338
test_loss: 0.009031319
train_loss: 0.0077084615
test_loss: 0.008712277
train_loss: 0.007246914
test_loss: 0.00869348
train_loss: 0.0064941933
test_loss: 0.007967698
train_loss: 0.00685287
test_loss: 0.008039702
train_loss: 0.0066658147
test_loss: 0.00796481
train_loss: 0.006170042
test_loss: 0.0077198236
train_loss: 0.0063716704
test_loss: 0.008154973
train_loss: 0.006290659
test_loss: 0.0076487684
train_loss: 0.006052671
test_loss: 0.0073680785
train_loss: 0.006442066
test_loss: 0.0076605272
train_loss: 0.005913713
test_loss: 0.0073353555
train_loss: 0.00563679
test_loss: 0.0070726136
train_loss: 0.0057558613
test_loss: 0.007959182
train_loss: 0.0057274145
test_loss: 0.007136784
train_loss: 0.005876719
test_loss: 0.007019095
train_loss: 0.0053829835
test_loss: 0.0069554946
train_loss: 0.005978873
test_loss: 0.0070693204
train_loss: 0.005305451
test_loss: 0.0070174653
train_loss: 0.0059055444
test_loss: 0.0069721304
train_loss: 0.0065849116
test_loss: 0.0075052315
train_loss: 0.0052133007
test_loss: 0.006705751
train_loss: 0.0054681688
test_loss: 0.006700419
train_loss: 0.0052987
test_loss: 0.006654737
train_loss: 0.0051944377
test_loss: 0.0066335234
train_loss: 0.0048441025
test_loss: 0.0066626384
train_loss: 0.0055790935
test_loss: 0.006870881
train_loss: 0.0049984762
test_loss: 0.0067010242
train_loss: 0.005754776
test_loss: 0.0071727796
train_loss: 0.0054019755
test_loss: 0.006743869
train_loss: 0.005455954
test_loss: 0.006645472
train_loss: 0.0050256997
test_loss: 0.0067494535
train_loss: 0.0049421717
test_loss: 0.006594939
train_loss: 0.005197197
test_loss: 0.0064788754
train_loss: 0.004976707
test_loss: 0.0071418704
train_loss: 0.004861808
test_loss: 0.006747532
train_loss: 0.004711401
test_loss: 0.0064192954
train_loss: 0.0049336157
test_loss: 0.006642318
train_loss: 0.004946605
test_loss: 0.006283282
train_loss: 0.005018412
test_loss: 0.006473719
train_loss: 0.0045685945
test_loss: 0.006243578
train_loss: 0.0048506437
test_loss: 0.0066415607
train_loss: 0.0047589843
test_loss: 0.006381447
train_loss: 0.0044888183
test_loss: 0.0061821276
train_loss: 0.004963674
test_loss: 0.006443191
train_loss: 0.0051475954
test_loss: 0.0065045026
train_loss: 0.0049035894
test_loss: 0.0066500683
train_loss: 0.0045092474
test_loss: 0.006567348
train_loss: 0.004734658
test_loss: 0.006435123
train_loss: 0.0050144647
test_loss: 0.0062600644
train_loss: 0.004862016
test_loss: 0.006334297
train_loss: 0.004560621
test_loss: 0.0061398204
train_loss: 0.004326961
test_loss: 0.0060657198
train_loss: 0.0047681425
test_loss: 0.006331659
train_loss: 0.0046731587
test_loss: 0.0063055297
train_loss: 0.004555136
test_loss: 0.0063265315
train_loss: 0.004579155
test_loss: 0.006306766
train_loss: 0.0044794213
test_loss: 0.0063022785
train_loss: 0.0046647144
test_loss: 0.006272543
train_loss: 0.0052927565
test_loss: 0.0063364645
train_loss: 0.0046562385
test_loss: 0.0062338095
train_loss: 0.0046266783
test_loss: 0.0065869773
train_loss: 0.004663377
test_loss: 0.006171723
train_loss: 0.0049447916
test_loss: 0.0063863327
train_loss: 0.004960728
test_loss: 0.0062919487
train_loss: 0.0045032883
test_loss: 0.0060694674
train_loss: 0.0045257965
test_loss: 0.0060779904
train_loss: 0.004607935
test_loss: 0.0062096952
train_loss: 0.0045377254
test_loss: 0.006124255
train_loss: 0.004261088
test_loss: 0.006026877
train_loss: 0.004820203
test_loss: 0.0063157715
train_loss: 0.0044586007
test_loss: 0.006243589
train_loss: 0.004596297
test_loss: 0.006069304
train_loss: 0.0044172974
test_loss: 0.0059111323
train_loss: 0.0043770364
test_loss: 0.0060034245
train_loss: 0.004254244
test_loss: 0.0061394228
train_loss: 0.004492242
test_loss: 0.006330651
train_loss: 0.004535593
test_loss: 0.0060513774
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.8/300_300_300_1 --optimizer lbfgs --function f2 --psi -1 --alpha 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi0.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc4b8048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc4bc8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc4136a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc413158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc42b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc3d02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc3b90d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc3b9ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc34e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc34ee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc413378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc32f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc32e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc32e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc32e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc271378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc26e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc26e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc1be400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc232ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc1e9730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc1e9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc1b6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc1787b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc179840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc179c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc0e9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc0e9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc0d29d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc0476a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc047ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11dc178d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11d451ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11d451e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11d451e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11d44cfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.44422879e-05
Iter: 2 loss: 2.74911654e-05
Iter: 3 loss: 8.84197871e-05
Iter: 4 loss: 2.71438785e-05
Iter: 5 loss: 2.37479817e-05
Iter: 6 loss: 4.12598019e-05
Iter: 7 loss: 2.31942831e-05
Iter: 8 loss: 2.16528224e-05
Iter: 9 loss: 2.21210048e-05
Iter: 10 loss: 2.05472406e-05
Iter: 11 loss: 1.87517307e-05
Iter: 12 loss: 3.84402447e-05
Iter: 13 loss: 1.87156566e-05
Iter: 14 loss: 1.76620943e-05
Iter: 15 loss: 1.81680552e-05
Iter: 16 loss: 1.69554696e-05
Iter: 17 loss: 1.59609335e-05
Iter: 18 loss: 2.67043815e-05
Iter: 19 loss: 1.59375704e-05
Iter: 20 loss: 1.5309728e-05
Iter: 21 loss: 1.58250059e-05
Iter: 22 loss: 1.49341658e-05
Iter: 23 loss: 1.41567434e-05
Iter: 24 loss: 1.73238332e-05
Iter: 25 loss: 1.39866679e-05
Iter: 26 loss: 1.34496895e-05
Iter: 27 loss: 1.3615454e-05
Iter: 28 loss: 1.30660155e-05
Iter: 29 loss: 1.25854767e-05
Iter: 30 loss: 1.25850474e-05
Iter: 31 loss: 1.21707071e-05
Iter: 32 loss: 1.16934089e-05
Iter: 33 loss: 1.16357296e-05
Iter: 34 loss: 1.11630652e-05
Iter: 35 loss: 1.44631504e-05
Iter: 36 loss: 1.11193285e-05
Iter: 37 loss: 1.07348778e-05
Iter: 38 loss: 1.2545368e-05
Iter: 39 loss: 1.06642892e-05
Iter: 40 loss: 1.03419579e-05
Iter: 41 loss: 1.07639817e-05
Iter: 42 loss: 1.01783116e-05
Iter: 43 loss: 9.93368212e-06
Iter: 44 loss: 1.25095858e-05
Iter: 45 loss: 9.9273293e-06
Iter: 46 loss: 9.71006375e-06
Iter: 47 loss: 1.00886227e-05
Iter: 48 loss: 9.61390469e-06
Iter: 49 loss: 9.43684245e-06
Iter: 50 loss: 9.85390761e-06
Iter: 51 loss: 9.37197728e-06
Iter: 52 loss: 9.15932469e-06
Iter: 53 loss: 9.8654582e-06
Iter: 54 loss: 9.10084782e-06
Iter: 55 loss: 8.95060566e-06
Iter: 56 loss: 9.61860678e-06
Iter: 57 loss: 8.92101889e-06
Iter: 58 loss: 8.78048377e-06
Iter: 59 loss: 8.85774352e-06
Iter: 60 loss: 8.68801362e-06
Iter: 61 loss: 8.5437714e-06
Iter: 62 loss: 8.94065488e-06
Iter: 63 loss: 8.49696062e-06
Iter: 64 loss: 8.33776448e-06
Iter: 65 loss: 9.51693255e-06
Iter: 66 loss: 8.32480146e-06
Iter: 67 loss: 8.23391383e-06
Iter: 68 loss: 8.15571366e-06
Iter: 69 loss: 8.13098177e-06
Iter: 70 loss: 8.01614897e-06
Iter: 71 loss: 9.30280294e-06
Iter: 72 loss: 8.01397528e-06
Iter: 73 loss: 7.91094226e-06
Iter: 74 loss: 7.90831109e-06
Iter: 75 loss: 7.82761e-06
Iter: 76 loss: 7.70112183e-06
Iter: 77 loss: 7.99976e-06
Iter: 78 loss: 7.65501318e-06
Iter: 79 loss: 7.56859208e-06
Iter: 80 loss: 7.56802638e-06
Iter: 81 loss: 7.50185382e-06
Iter: 82 loss: 7.41291569e-06
Iter: 83 loss: 7.40829773e-06
Iter: 84 loss: 7.3115034e-06
Iter: 85 loss: 8.57575651e-06
Iter: 86 loss: 7.31103773e-06
Iter: 87 loss: 7.24362963e-06
Iter: 88 loss: 7.30043485e-06
Iter: 89 loss: 7.20379057e-06
Iter: 90 loss: 7.13177269e-06
Iter: 91 loss: 7.46733349e-06
Iter: 92 loss: 7.11852226e-06
Iter: 93 loss: 7.05364209e-06
Iter: 94 loss: 7.04185459e-06
Iter: 95 loss: 6.99815519e-06
Iter: 96 loss: 6.93470201e-06
Iter: 97 loss: 7.86988e-06
Iter: 98 loss: 6.93465336e-06
Iter: 99 loss: 6.8755071e-06
Iter: 100 loss: 6.83528469e-06
Iter: 101 loss: 6.81360825e-06
Iter: 102 loss: 6.74436797e-06
Iter: 103 loss: 6.94471237e-06
Iter: 104 loss: 6.72276065e-06
Iter: 105 loss: 6.65705284e-06
Iter: 106 loss: 7.36622042e-06
Iter: 107 loss: 6.65571042e-06
Iter: 108 loss: 6.61582499e-06
Iter: 109 loss: 6.58278623e-06
Iter: 110 loss: 6.57131432e-06
Iter: 111 loss: 6.51924756e-06
Iter: 112 loss: 6.869358e-06
Iter: 113 loss: 6.51411119e-06
Iter: 114 loss: 6.46424724e-06
Iter: 115 loss: 6.71249e-06
Iter: 116 loss: 6.4558044e-06
Iter: 117 loss: 6.42051145e-06
Iter: 118 loss: 6.41951237e-06
Iter: 119 loss: 6.39195787e-06
Iter: 120 loss: 6.33817035e-06
Iter: 121 loss: 6.67061659e-06
Iter: 122 loss: 6.33189757e-06
Iter: 123 loss: 6.29365559e-06
Iter: 124 loss: 6.31594776e-06
Iter: 125 loss: 6.26885685e-06
Iter: 126 loss: 6.21609524e-06
Iter: 127 loss: 6.39379914e-06
Iter: 128 loss: 6.20170886e-06
Iter: 129 loss: 6.16522902e-06
Iter: 130 loss: 6.24587028e-06
Iter: 131 loss: 6.15121189e-06
Iter: 132 loss: 6.11187e-06
Iter: 133 loss: 6.41120732e-06
Iter: 134 loss: 6.10885945e-06
Iter: 135 loss: 6.08008531e-06
Iter: 136 loss: 6.04207344e-06
Iter: 137 loss: 6.03985927e-06
Iter: 138 loss: 6.00156545e-06
Iter: 139 loss: 6.41112047e-06
Iter: 140 loss: 6.0004827e-06
Iter: 141 loss: 5.96176278e-06
Iter: 142 loss: 6.03290027e-06
Iter: 143 loss: 5.94530911e-06
Iter: 144 loss: 5.91621074e-06
Iter: 145 loss: 5.91040862e-06
Iter: 146 loss: 5.89121282e-06
Iter: 147 loss: 5.85963789e-06
Iter: 148 loss: 5.85964881e-06
Iter: 149 loss: 5.83259407e-06
Iter: 150 loss: 5.82759139e-06
Iter: 151 loss: 5.8093965e-06
Iter: 152 loss: 5.77912579e-06
Iter: 153 loss: 5.96603377e-06
Iter: 154 loss: 5.77560331e-06
Iter: 155 loss: 5.74626347e-06
Iter: 156 loss: 5.78771096e-06
Iter: 157 loss: 5.73180569e-06
Iter: 158 loss: 5.7077018e-06
Iter: 159 loss: 5.80804408e-06
Iter: 160 loss: 5.70239172e-06
Iter: 161 loss: 5.67704728e-06
Iter: 162 loss: 5.69349641e-06
Iter: 163 loss: 5.66106428e-06
Iter: 164 loss: 5.63884e-06
Iter: 165 loss: 5.84563577e-06
Iter: 166 loss: 5.63794e-06
Iter: 167 loss: 5.61507568e-06
Iter: 168 loss: 5.60543049e-06
Iter: 169 loss: 5.59355431e-06
Iter: 170 loss: 5.56585883e-06
Iter: 171 loss: 5.59488672e-06
Iter: 172 loss: 5.55052338e-06
Iter: 173 loss: 5.52854453e-06
Iter: 174 loss: 5.52819347e-06
Iter: 175 loss: 5.51029098e-06
Iter: 176 loss: 5.48517573e-06
Iter: 177 loss: 5.48421394e-06
Iter: 178 loss: 5.45575949e-06
Iter: 179 loss: 5.56285249e-06
Iter: 180 loss: 5.44895e-06
Iter: 181 loss: 5.42349062e-06
Iter: 182 loss: 5.73731e-06
Iter: 183 loss: 5.42313956e-06
Iter: 184 loss: 5.40840301e-06
Iter: 185 loss: 5.3981239e-06
Iter: 186 loss: 5.39281336e-06
Iter: 187 loss: 5.36926291e-06
Iter: 188 loss: 5.55236375e-06
Iter: 189 loss: 5.36760308e-06
Iter: 190 loss: 5.349817e-06
Iter: 191 loss: 5.34784704e-06
Iter: 192 loss: 5.3349595e-06
Iter: 193 loss: 5.31126352e-06
Iter: 194 loss: 5.43383157e-06
Iter: 195 loss: 5.30741045e-06
Iter: 196 loss: 5.2877067e-06
Iter: 197 loss: 5.32043623e-06
Iter: 198 loss: 5.27876728e-06
Iter: 199 loss: 5.26172471e-06
Iter: 200 loss: 5.44785917e-06
Iter: 201 loss: 5.26131544e-06
Iter: 202 loss: 5.24751249e-06
Iter: 203 loss: 5.22188066e-06
Iter: 204 loss: 5.80294454e-06
Iter: 205 loss: 5.221832e-06
Iter: 206 loss: 5.20211415e-06
Iter: 207 loss: 5.41443387e-06
Iter: 208 loss: 5.20165395e-06
Iter: 209 loss: 5.18138768e-06
Iter: 210 loss: 5.23987728e-06
Iter: 211 loss: 5.17526951e-06
Iter: 212 loss: 5.15871579e-06
Iter: 213 loss: 5.14907242e-06
Iter: 214 loss: 5.14202929e-06
Iter: 215 loss: 5.12567658e-06
Iter: 216 loss: 5.12568568e-06
Iter: 217 loss: 5.11024291e-06
Iter: 218 loss: 5.12143743e-06
Iter: 219 loss: 5.1007296e-06
Iter: 220 loss: 5.08650737e-06
Iter: 221 loss: 5.13216901e-06
Iter: 222 loss: 5.08248377e-06
Iter: 223 loss: 5.06449396e-06
Iter: 224 loss: 5.09055e-06
Iter: 225 loss: 5.05551589e-06
Iter: 226 loss: 5.04069612e-06
Iter: 227 loss: 5.0917879e-06
Iter: 228 loss: 5.0366184e-06
Iter: 229 loss: 5.02192e-06
Iter: 230 loss: 5.05850494e-06
Iter: 231 loss: 5.01669274e-06
Iter: 232 loss: 5.0031058e-06
Iter: 233 loss: 5.05076878e-06
Iter: 234 loss: 4.99962198e-06
Iter: 235 loss: 4.98363806e-06
Iter: 236 loss: 5.01591967e-06
Iter: 237 loss: 4.97711335e-06
Iter: 238 loss: 4.9632e-06
Iter: 239 loss: 4.95155973e-06
Iter: 240 loss: 4.94762571e-06
Iter: 241 loss: 4.93583366e-06
Iter: 242 loss: 4.93516154e-06
Iter: 243 loss: 4.92316758e-06
Iter: 244 loss: 4.91583933e-06
Iter: 245 loss: 4.91091441e-06
Iter: 246 loss: 4.89544118e-06
Iter: 247 loss: 4.90514958e-06
Iter: 248 loss: 4.88569867e-06
Iter: 249 loss: 4.87090801e-06
Iter: 250 loss: 4.87070793e-06
Iter: 251 loss: 4.860638e-06
Iter: 252 loss: 4.85030159e-06
Iter: 253 loss: 4.8484053e-06
Iter: 254 loss: 4.83492568e-06
Iter: 255 loss: 4.99372163e-06
Iter: 256 loss: 4.83467602e-06
Iter: 257 loss: 4.82336918e-06
Iter: 258 loss: 4.81500319e-06
Iter: 259 loss: 4.8111724e-06
Iter: 260 loss: 4.79714026e-06
Iter: 261 loss: 4.90101502e-06
Iter: 262 loss: 4.79589744e-06
Iter: 263 loss: 4.7847393e-06
Iter: 264 loss: 4.8104921e-06
Iter: 265 loss: 4.78037782e-06
Iter: 266 loss: 4.77074263e-06
Iter: 267 loss: 4.83997428e-06
Iter: 268 loss: 4.76999867e-06
Iter: 269 loss: 4.76078048e-06
Iter: 270 loss: 4.74958415e-06
Iter: 271 loss: 4.74850458e-06
Iter: 272 loss: 4.73350701e-06
Iter: 273 loss: 4.74609715e-06
Iter: 274 loss: 4.7245303e-06
Iter: 275 loss: 4.71321664e-06
Iter: 276 loss: 4.71231033e-06
Iter: 277 loss: 4.70234e-06
Iter: 278 loss: 4.68425924e-06
Iter: 279 loss: 5.12370752e-06
Iter: 280 loss: 4.6841933e-06
Iter: 281 loss: 4.67038944e-06
Iter: 282 loss: 4.82014184e-06
Iter: 283 loss: 4.67015889e-06
Iter: 284 loss: 4.65718767e-06
Iter: 285 loss: 4.70908753e-06
Iter: 286 loss: 4.65434096e-06
Iter: 287 loss: 4.64442383e-06
Iter: 288 loss: 4.64084223e-06
Iter: 289 loss: 4.63523702e-06
Iter: 290 loss: 4.62099524e-06
Iter: 291 loss: 4.76727746e-06
Iter: 292 loss: 4.62066146e-06
Iter: 293 loss: 4.61188e-06
Iter: 294 loss: 4.60226056e-06
Iter: 295 loss: 4.60076171e-06
Iter: 296 loss: 4.58725e-06
Iter: 297 loss: 4.71419844e-06
Iter: 298 loss: 4.58669047e-06
Iter: 299 loss: 4.57636452e-06
Iter: 300 loss: 4.59059e-06
Iter: 301 loss: 4.57118767e-06
Iter: 302 loss: 4.56020098e-06
Iter: 303 loss: 4.64231334e-06
Iter: 304 loss: 4.559231e-06
Iter: 305 loss: 4.55176678e-06
Iter: 306 loss: 4.54079327e-06
Iter: 307 loss: 4.54060819e-06
Iter: 308 loss: 4.52688118e-06
Iter: 309 loss: 4.58301975e-06
Iter: 310 loss: 4.52394306e-06
Iter: 311 loss: 4.51406459e-06
Iter: 312 loss: 4.51400228e-06
Iter: 313 loss: 4.50734206e-06
Iter: 314 loss: 4.49110848e-06
Iter: 315 loss: 4.6661371e-06
Iter: 316 loss: 4.489395e-06
Iter: 317 loss: 4.48415722e-06
Iter: 318 loss: 4.48093397e-06
Iter: 319 loss: 4.47312868e-06
Iter: 320 loss: 4.46588319e-06
Iter: 321 loss: 4.46407967e-06
Iter: 322 loss: 4.45396699e-06
Iter: 323 loss: 4.51731557e-06
Iter: 324 loss: 4.45291153e-06
Iter: 325 loss: 4.44193029e-06
Iter: 326 loss: 4.45065325e-06
Iter: 327 loss: 4.43541285e-06
Iter: 328 loss: 4.42534974e-06
Iter: 329 loss: 4.44633815e-06
Iter: 330 loss: 4.42138798e-06
Iter: 331 loss: 4.41167185e-06
Iter: 332 loss: 4.47713546e-06
Iter: 333 loss: 4.41065504e-06
Iter: 334 loss: 4.40239864e-06
Iter: 335 loss: 4.4101389e-06
Iter: 336 loss: 4.39758924e-06
Iter: 337 loss: 4.38553934e-06
Iter: 338 loss: 4.4177059e-06
Iter: 339 loss: 4.38148345e-06
Iter: 340 loss: 4.3720911e-06
Iter: 341 loss: 4.36407e-06
Iter: 342 loss: 4.36138953e-06
Iter: 343 loss: 4.34999401e-06
Iter: 344 loss: 4.50014659e-06
Iter: 345 loss: 4.34993035e-06
Iter: 346 loss: 4.33919422e-06
Iter: 347 loss: 4.36955e-06
Iter: 348 loss: 4.33572723e-06
Iter: 349 loss: 4.32878733e-06
Iter: 350 loss: 4.32616343e-06
Iter: 351 loss: 4.32226807e-06
Iter: 352 loss: 4.31541685e-06
Iter: 353 loss: 4.31525132e-06
Iter: 354 loss: 4.3093919e-06
Iter: 355 loss: 4.2999236e-06
Iter: 356 loss: 4.29988813e-06
Iter: 357 loss: 4.29302509e-06
Iter: 358 loss: 4.29288684e-06
Iter: 359 loss: 4.28661133e-06
Iter: 360 loss: 4.27762825e-06
Iter: 361 loss: 4.27735495e-06
Iter: 362 loss: 4.26694805e-06
Iter: 363 loss: 4.30368209e-06
Iter: 364 loss: 4.26425959e-06
Iter: 365 loss: 4.2540737e-06
Iter: 366 loss: 4.33438936e-06
Iter: 367 loss: 4.25342569e-06
Iter: 368 loss: 4.24663585e-06
Iter: 369 loss: 4.26168481e-06
Iter: 370 loss: 4.24401787e-06
Iter: 371 loss: 4.23519305e-06
Iter: 372 loss: 4.23604388e-06
Iter: 373 loss: 4.22832136e-06
Iter: 374 loss: 4.21853065e-06
Iter: 375 loss: 4.22806079e-06
Iter: 376 loss: 4.21298137e-06
Iter: 377 loss: 4.20503375e-06
Iter: 378 loss: 4.20502829e-06
Iter: 379 loss: 4.19755816e-06
Iter: 380 loss: 4.19612206e-06
Iter: 381 loss: 4.19107027e-06
Iter: 382 loss: 4.18317131e-06
Iter: 383 loss: 4.19658727e-06
Iter: 384 loss: 4.17974616e-06
Iter: 385 loss: 4.17215142e-06
Iter: 386 loss: 4.29019246e-06
Iter: 387 loss: 4.17219462e-06
Iter: 388 loss: 4.1675994e-06
Iter: 389 loss: 4.1589683e-06
Iter: 390 loss: 4.34600861e-06
Iter: 391 loss: 4.15889e-06
Iter: 392 loss: 4.15071281e-06
Iter: 393 loss: 4.15069098e-06
Iter: 394 loss: 4.14484839e-06
Iter: 395 loss: 4.13655471e-06
Iter: 396 loss: 4.13622183e-06
Iter: 397 loss: 4.12785585e-06
Iter: 398 loss: 4.18036188e-06
Iter: 399 loss: 4.12694544e-06
Iter: 400 loss: 4.11786823e-06
Iter: 401 loss: 4.14258e-06
Iter: 402 loss: 4.11476867e-06
Iter: 403 loss: 4.10686289e-06
Iter: 404 loss: 4.13868156e-06
Iter: 405 loss: 4.10511848e-06
Iter: 406 loss: 4.09703534e-06
Iter: 407 loss: 4.09923223e-06
Iter: 408 loss: 4.09116456e-06
Iter: 409 loss: 4.08173855e-06
Iter: 410 loss: 4.08449159e-06
Iter: 411 loss: 4.07498101e-06
Iter: 412 loss: 4.06960726e-06
Iter: 413 loss: 4.06825893e-06
Iter: 414 loss: 4.06311392e-06
Iter: 415 loss: 4.0557743e-06
Iter: 416 loss: 4.05550463e-06
Iter: 417 loss: 4.04793582e-06
Iter: 418 loss: 4.09141057e-06
Iter: 419 loss: 4.04684761e-06
Iter: 420 loss: 4.03833383e-06
Iter: 421 loss: 4.07107973e-06
Iter: 422 loss: 4.03632293e-06
Iter: 423 loss: 4.03087597e-06
Iter: 424 loss: 4.03107151e-06
Iter: 425 loss: 4.02660271e-06
Iter: 426 loss: 4.01862644e-06
Iter: 427 loss: 4.07927337e-06
Iter: 428 loss: 4.01795796e-06
Iter: 429 loss: 4.01214584e-06
Iter: 430 loss: 4.00505178e-06
Iter: 431 loss: 4.00451108e-06
Iter: 432 loss: 3.99687269e-06
Iter: 433 loss: 4.06836625e-06
Iter: 434 loss: 3.99660439e-06
Iter: 435 loss: 3.98865404e-06
Iter: 436 loss: 4.00090721e-06
Iter: 437 loss: 3.98491329e-06
Iter: 438 loss: 3.97833719e-06
Iter: 439 loss: 4.01411808e-06
Iter: 440 loss: 3.97732765e-06
Iter: 441 loss: 3.97088752e-06
Iter: 442 loss: 3.96585892e-06
Iter: 443 loss: 3.96382438e-06
Iter: 444 loss: 3.95566531e-06
Iter: 445 loss: 3.98831571e-06
Iter: 446 loss: 3.95386451e-06
Iter: 447 loss: 3.94610151e-06
Iter: 448 loss: 4.02538262e-06
Iter: 449 loss: 3.94591461e-06
Iter: 450 loss: 3.94088e-06
Iter: 451 loss: 3.93329492e-06
Iter: 452 loss: 3.93317941e-06
Iter: 453 loss: 3.92578841e-06
Iter: 454 loss: 4.03644572e-06
Iter: 455 loss: 3.92572429e-06
Iter: 456 loss: 3.91856793e-06
Iter: 457 loss: 3.920155e-06
Iter: 458 loss: 3.91334288e-06
Iter: 459 loss: 3.90743662e-06
Iter: 460 loss: 3.93094706e-06
Iter: 461 loss: 3.90617652e-06
Iter: 462 loss: 3.8988037e-06
Iter: 463 loss: 3.91066351e-06
Iter: 464 loss: 3.89552724e-06
Iter: 465 loss: 3.8889566e-06
Iter: 466 loss: 3.88502212e-06
Iter: 467 loss: 3.8824146e-06
Iter: 468 loss: 3.87546197e-06
Iter: 469 loss: 3.87544969e-06
Iter: 470 loss: 3.8692142e-06
Iter: 471 loss: 3.86867032e-06
Iter: 472 loss: 3.86399279e-06
Iter: 473 loss: 3.85766543e-06
Iter: 474 loss: 3.90926607e-06
Iter: 475 loss: 3.85721796e-06
Iter: 476 loss: 3.8519438e-06
Iter: 477 loss: 3.84625628e-06
Iter: 478 loss: 3.84534633e-06
Iter: 479 loss: 3.83905e-06
Iter: 480 loss: 3.92407219e-06
Iter: 481 loss: 3.83908446e-06
Iter: 482 loss: 3.83317501e-06
Iter: 483 loss: 3.84298437e-06
Iter: 484 loss: 3.8304961e-06
Iter: 485 loss: 3.82509e-06
Iter: 486 loss: 3.81923383e-06
Iter: 487 loss: 3.8182875e-06
Iter: 488 loss: 3.81178916e-06
Iter: 489 loss: 3.8114033e-06
Iter: 490 loss: 3.80688925e-06
Iter: 491 loss: 3.80113943e-06
Iter: 492 loss: 3.80078473e-06
Iter: 493 loss: 3.79584321e-06
Iter: 494 loss: 3.87303e-06
Iter: 495 loss: 3.79586595e-06
Iter: 496 loss: 3.79117728e-06
Iter: 497 loss: 3.78546201e-06
Iter: 498 loss: 3.78492314e-06
Iter: 499 loss: 3.77732476e-06
Iter: 500 loss: 3.79364337e-06
Iter: 501 loss: 3.7744785e-06
Iter: 502 loss: 3.7678351e-06
Iter: 503 loss: 3.86277497e-06
Iter: 504 loss: 3.76779235e-06
Iter: 505 loss: 3.76286084e-06
Iter: 506 loss: 3.7591094e-06
Iter: 507 loss: 3.7574373e-06
Iter: 508 loss: 3.75072705e-06
Iter: 509 loss: 3.8040173e-06
Iter: 510 loss: 3.75034347e-06
Iter: 511 loss: 3.74492538e-06
Iter: 512 loss: 3.74023716e-06
Iter: 513 loss: 3.73881448e-06
Iter: 514 loss: 3.73382841e-06
Iter: 515 loss: 3.73353714e-06
Iter: 516 loss: 3.72906311e-06
Iter: 517 loss: 3.72235831e-06
Iter: 518 loss: 3.72222303e-06
Iter: 519 loss: 3.71535862e-06
Iter: 520 loss: 3.75744457e-06
Iter: 521 loss: 3.71441206e-06
Iter: 522 loss: 3.7070472e-06
Iter: 523 loss: 3.73785952e-06
Iter: 524 loss: 3.70545195e-06
Iter: 525 loss: 3.70082762e-06
Iter: 526 loss: 3.69947793e-06
Iter: 527 loss: 3.69665099e-06
Iter: 528 loss: 3.69093527e-06
Iter: 529 loss: 3.7668874e-06
Iter: 530 loss: 3.69088207e-06
Iter: 531 loss: 3.68637166e-06
Iter: 532 loss: 3.67963275e-06
Iter: 533 loss: 3.67955568e-06
Iter: 534 loss: 3.6737847e-06
Iter: 535 loss: 3.73091552e-06
Iter: 536 loss: 3.67360417e-06
Iter: 537 loss: 3.66764925e-06
Iter: 538 loss: 3.68104611e-06
Iter: 539 loss: 3.66545396e-06
Iter: 540 loss: 3.6599231e-06
Iter: 541 loss: 3.66225322e-06
Iter: 542 loss: 3.65603933e-06
Iter: 543 loss: 3.64826974e-06
Iter: 544 loss: 3.67971484e-06
Iter: 545 loss: 3.64655762e-06
Iter: 546 loss: 3.64057382e-06
Iter: 547 loss: 3.64860807e-06
Iter: 548 loss: 3.63757499e-06
Iter: 549 loss: 3.63174195e-06
Iter: 550 loss: 3.71111628e-06
Iter: 551 loss: 3.63169033e-06
Iter: 552 loss: 3.62797755e-06
Iter: 553 loss: 3.62128958e-06
Iter: 554 loss: 3.78708387e-06
Iter: 555 loss: 3.6212723e-06
Iter: 556 loss: 3.61669163e-06
Iter: 557 loss: 3.61642924e-06
Iter: 558 loss: 3.61203365e-06
Iter: 559 loss: 3.60842455e-06
Iter: 560 loss: 3.60715967e-06
Iter: 561 loss: 3.60206695e-06
Iter: 562 loss: 3.62376591e-06
Iter: 563 loss: 3.60094532e-06
Iter: 564 loss: 3.59475666e-06
Iter: 565 loss: 3.61021034e-06
Iter: 566 loss: 3.59259138e-06
Iter: 567 loss: 3.58776538e-06
Iter: 568 loss: 3.58471425e-06
Iter: 569 loss: 3.58292891e-06
Iter: 570 loss: 3.57776639e-06
Iter: 571 loss: 3.57774957e-06
Iter: 572 loss: 3.57320869e-06
Iter: 573 loss: 3.57226736e-06
Iter: 574 loss: 3.56942746e-06
Iter: 575 loss: 3.56408282e-06
Iter: 576 loss: 3.58138232e-06
Iter: 577 loss: 3.56259716e-06
Iter: 578 loss: 3.55648444e-06
Iter: 579 loss: 3.56359169e-06
Iter: 580 loss: 3.55318252e-06
Iter: 581 loss: 3.54731901e-06
Iter: 582 loss: 3.57709928e-06
Iter: 583 loss: 3.54635586e-06
Iter: 584 loss: 3.53960945e-06
Iter: 585 loss: 3.55578959e-06
Iter: 586 loss: 3.53727137e-06
Iter: 587 loss: 3.53271889e-06
Iter: 588 loss: 3.53143264e-06
Iter: 589 loss: 3.52863412e-06
Iter: 590 loss: 3.52265215e-06
Iter: 591 loss: 3.61712591e-06
Iter: 592 loss: 3.52263874e-06
Iter: 593 loss: 3.51888366e-06
Iter: 594 loss: 3.51383551e-06
Iter: 595 loss: 3.51356084e-06
Iter: 596 loss: 3.50967161e-06
Iter: 597 loss: 3.50958476e-06
Iter: 598 loss: 3.50599203e-06
Iter: 599 loss: 3.50035066e-06
Iter: 600 loss: 3.50029768e-06
Iter: 601 loss: 3.4940731e-06
Iter: 602 loss: 3.51216113e-06
Iter: 603 loss: 3.49207653e-06
Iter: 604 loss: 3.48530511e-06
Iter: 605 loss: 3.54653343e-06
Iter: 606 loss: 3.48491267e-06
Iter: 607 loss: 3.4802647e-06
Iter: 608 loss: 3.47747118e-06
Iter: 609 loss: 3.4754878e-06
Iter: 610 loss: 3.46991146e-06
Iter: 611 loss: 3.51972562e-06
Iter: 612 loss: 3.46965771e-06
Iter: 613 loss: 3.46503407e-06
Iter: 614 loss: 3.46413844e-06
Iter: 615 loss: 3.46105094e-06
Iter: 616 loss: 3.45611807e-06
Iter: 617 loss: 3.52800407e-06
Iter: 618 loss: 3.45606577e-06
Iter: 619 loss: 3.45161811e-06
Iter: 620 loss: 3.44425075e-06
Iter: 621 loss: 3.44420323e-06
Iter: 622 loss: 3.43901252e-06
Iter: 623 loss: 3.43898728e-06
Iter: 624 loss: 3.43428633e-06
Iter: 625 loss: 3.44379714e-06
Iter: 626 loss: 3.432413e-06
Iter: 627 loss: 3.42809176e-06
Iter: 628 loss: 3.42274689e-06
Iter: 629 loss: 3.42227645e-06
Iter: 630 loss: 3.41697114e-06
Iter: 631 loss: 3.41673172e-06
Iter: 632 loss: 3.41365694e-06
Iter: 633 loss: 3.40725273e-06
Iter: 634 loss: 3.51203221e-06
Iter: 635 loss: 3.40706356e-06
Iter: 636 loss: 3.40211e-06
Iter: 637 loss: 3.40206907e-06
Iter: 638 loss: 3.39684652e-06
Iter: 639 loss: 3.39962457e-06
Iter: 640 loss: 3.39344e-06
Iter: 641 loss: 3.38828977e-06
Iter: 642 loss: 3.3992244e-06
Iter: 643 loss: 3.38634891e-06
Iter: 644 loss: 3.38085943e-06
Iter: 645 loss: 3.40623887e-06
Iter: 646 loss: 3.37982556e-06
Iter: 647 loss: 3.37542224e-06
Iter: 648 loss: 3.38348354e-06
Iter: 649 loss: 3.37340748e-06
Iter: 650 loss: 3.36790868e-06
Iter: 651 loss: 3.39452595e-06
Iter: 652 loss: 3.36687663e-06
Iter: 653 loss: 3.36246808e-06
Iter: 654 loss: 3.3573649e-06
Iter: 655 loss: 3.35675622e-06
Iter: 656 loss: 3.35246477e-06
Iter: 657 loss: 3.3522083e-06
Iter: 658 loss: 3.34821038e-06
Iter: 659 loss: 3.3415572e-06
Iter: 660 loss: 3.341514e-06
Iter: 661 loss: 3.33636967e-06
Iter: 662 loss: 3.39796929e-06
Iter: 663 loss: 3.33635398e-06
Iter: 664 loss: 3.33169328e-06
Iter: 665 loss: 3.33917524e-06
Iter: 666 loss: 3.32937634e-06
Iter: 667 loss: 3.32569516e-06
Iter: 668 loss: 3.32298964e-06
Iter: 669 loss: 3.32158288e-06
Iter: 670 loss: 3.3175404e-06
Iter: 671 loss: 3.31742331e-06
Iter: 672 loss: 3.31401839e-06
Iter: 673 loss: 3.30953799e-06
Iter: 674 loss: 3.30931334e-06
Iter: 675 loss: 3.30363605e-06
Iter: 676 loss: 3.31820183e-06
Iter: 677 loss: 3.30164062e-06
Iter: 678 loss: 3.29451132e-06
Iter: 679 loss: 3.32218747e-06
Iter: 680 loss: 3.29281647e-06
Iter: 681 loss: 3.2882142e-06
Iter: 682 loss: 3.32001969e-06
Iter: 683 loss: 3.28783608e-06
Iter: 684 loss: 3.28342094e-06
Iter: 685 loss: 3.28650594e-06
Iter: 686 loss: 3.28067176e-06
Iter: 687 loss: 3.2763046e-06
Iter: 688 loss: 3.28015494e-06
Iter: 689 loss: 3.2738219e-06
Iter: 690 loss: 3.26932377e-06
Iter: 691 loss: 3.33831781e-06
Iter: 692 loss: 3.26931945e-06
Iter: 693 loss: 3.26635927e-06
Iter: 694 loss: 3.2600849e-06
Iter: 695 loss: 3.35741288e-06
Iter: 696 loss: 3.25977771e-06
Iter: 697 loss: 3.25491601e-06
Iter: 698 loss: 3.25481324e-06
Iter: 699 loss: 3.25027986e-06
Iter: 700 loss: 3.24741495e-06
Iter: 701 loss: 3.24560301e-06
Iter: 702 loss: 3.24060602e-06
Iter: 703 loss: 3.25666588e-06
Iter: 704 loss: 3.23915901e-06
Iter: 705 loss: 3.23475842e-06
Iter: 706 loss: 3.28831538e-06
Iter: 707 loss: 3.23470863e-06
Iter: 708 loss: 3.23169797e-06
Iter: 709 loss: 3.22609276e-06
Iter: 710 loss: 3.35470395e-06
Iter: 711 loss: 3.2260898e-06
Iter: 712 loss: 3.22129927e-06
Iter: 713 loss: 3.27764224e-06
Iter: 714 loss: 3.22123105e-06
Iter: 715 loss: 3.21695325e-06
Iter: 716 loss: 3.22675078e-06
Iter: 717 loss: 3.2152841e-06
Iter: 718 loss: 3.21175935e-06
Iter: 719 loss: 3.23747486e-06
Iter: 720 loss: 3.21148013e-06
Iter: 721 loss: 3.20815798e-06
Iter: 722 loss: 3.20469712e-06
Iter: 723 loss: 3.20409e-06
Iter: 724 loss: 3.19946025e-06
Iter: 725 loss: 3.2299231e-06
Iter: 726 loss: 3.19891888e-06
Iter: 727 loss: 3.1942277e-06
Iter: 728 loss: 3.21131847e-06
Iter: 729 loss: 3.19304286e-06
Iter: 730 loss: 3.18999e-06
Iter: 731 loss: 3.18634579e-06
Iter: 732 loss: 3.18604202e-06
Iter: 733 loss: 3.18151228e-06
Iter: 734 loss: 3.18151342e-06
Iter: 735 loss: 3.17762033e-06
Iter: 736 loss: 3.17190688e-06
Iter: 737 loss: 3.17180275e-06
Iter: 738 loss: 3.16701016e-06
Iter: 739 loss: 3.21716925e-06
Iter: 740 loss: 3.16682281e-06
Iter: 741 loss: 3.1623e-06
Iter: 742 loss: 3.1783884e-06
Iter: 743 loss: 3.1610939e-06
Iter: 744 loss: 3.15801458e-06
Iter: 745 loss: 3.15341686e-06
Iter: 746 loss: 3.15331545e-06
Iter: 747 loss: 3.14928047e-06
Iter: 748 loss: 3.14924637e-06
Iter: 749 loss: 3.14598446e-06
Iter: 750 loss: 3.14921749e-06
Iter: 751 loss: 3.14417275e-06
Iter: 752 loss: 3.13997e-06
Iter: 753 loss: 3.15424268e-06
Iter: 754 loss: 3.13886267e-06
Iter: 755 loss: 3.13461305e-06
Iter: 756 loss: 3.13558894e-06
Iter: 757 loss: 3.1313466e-06
Iter: 758 loss: 3.12773591e-06
Iter: 759 loss: 3.1736613e-06
Iter: 760 loss: 3.12772e-06
Iter: 761 loss: 3.12397106e-06
Iter: 762 loss: 3.11967028e-06
Iter: 763 loss: 3.11909753e-06
Iter: 764 loss: 3.11470149e-06
Iter: 765 loss: 3.12930752e-06
Iter: 766 loss: 3.11343911e-06
Iter: 767 loss: 3.10890823e-06
Iter: 768 loss: 3.15028456e-06
Iter: 769 loss: 3.10869632e-06
Iter: 770 loss: 3.10564633e-06
Iter: 771 loss: 3.10001769e-06
Iter: 772 loss: 3.23183e-06
Iter: 773 loss: 3.09995448e-06
Iter: 774 loss: 3.09596612e-06
Iter: 775 loss: 3.09579e-06
Iter: 776 loss: 3.09200277e-06
Iter: 777 loss: 3.09195366e-06
Iter: 778 loss: 3.08892982e-06
Iter: 779 loss: 3.08516974e-06
Iter: 780 loss: 3.08458925e-06
Iter: 781 loss: 3.081979e-06
Iter: 782 loss: 3.0780725e-06
Iter: 783 loss: 3.07803498e-06
Iter: 784 loss: 3.0748231e-06
Iter: 785 loss: 3.07541313e-06
Iter: 786 loss: 3.07241476e-06
Iter: 787 loss: 3.06805532e-06
Iter: 788 loss: 3.09039251e-06
Iter: 789 loss: 3.06739048e-06
Iter: 790 loss: 3.06378752e-06
Iter: 791 loss: 3.06215134e-06
Iter: 792 loss: 3.06042148e-06
Iter: 793 loss: 3.05701064e-06
Iter: 794 loss: 3.05695198e-06
Iter: 795 loss: 3.0542069e-06
Iter: 796 loss: 3.04968148e-06
Iter: 797 loss: 3.04966761e-06
Iter: 798 loss: 3.04537025e-06
Iter: 799 loss: 3.06879974e-06
Iter: 800 loss: 3.04478499e-06
Iter: 801 loss: 3.0406618e-06
Iter: 802 loss: 3.06988454e-06
Iter: 803 loss: 3.0403221e-06
Iter: 804 loss: 3.03767251e-06
Iter: 805 loss: 3.03350453e-06
Iter: 806 loss: 3.03346815e-06
Iter: 807 loss: 3.02990702e-06
Iter: 808 loss: 3.02983108e-06
Iter: 809 loss: 3.02657963e-06
Iter: 810 loss: 3.02243689e-06
Iter: 811 loss: 3.02212629e-06
Iter: 812 loss: 3.01733098e-06
Iter: 813 loss: 3.02371427e-06
Iter: 814 loss: 3.01497448e-06
Iter: 815 loss: 3.01118098e-06
Iter: 816 loss: 3.01100772e-06
Iter: 817 loss: 3.00813349e-06
Iter: 818 loss: 3.00751299e-06
Iter: 819 loss: 3.00568558e-06
Iter: 820 loss: 3.0015126e-06
Iter: 821 loss: 3.02522494e-06
Iter: 822 loss: 3.00100805e-06
Iter: 823 loss: 2.99813269e-06
Iter: 824 loss: 2.99780618e-06
Iter: 825 loss: 2.99574458e-06
Iter: 826 loss: 2.99167505e-06
Iter: 827 loss: 3.03444904e-06
Iter: 828 loss: 2.99160297e-06
Iter: 829 loss: 2.98888654e-06
Iter: 830 loss: 2.98462646e-06
Iter: 831 loss: 2.98463738e-06
Iter: 832 loss: 2.97999782e-06
Iter: 833 loss: 3.0211263e-06
Iter: 834 loss: 2.97981182e-06
Iter: 835 loss: 2.97534598e-06
Iter: 836 loss: 2.98634222e-06
Iter: 837 loss: 2.97377028e-06
Iter: 838 loss: 2.97079873e-06
Iter: 839 loss: 2.9692e-06
Iter: 840 loss: 2.96783e-06
Iter: 841 loss: 2.96439885e-06
Iter: 842 loss: 2.96431654e-06
Iter: 843 loss: 2.96204553e-06
Iter: 844 loss: 2.95768905e-06
Iter: 845 loss: 3.05687308e-06
Iter: 846 loss: 2.95766563e-06
Iter: 847 loss: 2.95336827e-06
Iter: 848 loss: 2.97975703e-06
Iter: 849 loss: 2.95288601e-06
Iter: 850 loss: 2.9487569e-06
Iter: 851 loss: 2.97450561e-06
Iter: 852 loss: 2.94827805e-06
Iter: 853 loss: 2.94538631e-06
Iter: 854 loss: 2.94751817e-06
Iter: 855 loss: 2.94351958e-06
Iter: 856 loss: 2.93933e-06
Iter: 857 loss: 2.94987149e-06
Iter: 858 loss: 2.93784296e-06
Iter: 859 loss: 2.9344792e-06
Iter: 860 loss: 2.94630036e-06
Iter: 861 loss: 2.93361109e-06
Iter: 862 loss: 2.92972368e-06
Iter: 863 loss: 2.94201891e-06
Iter: 864 loss: 2.92866343e-06
Iter: 865 loss: 2.92571576e-06
Iter: 866 loss: 2.9217922e-06
Iter: 867 loss: 2.92159825e-06
Iter: 868 loss: 2.91813194e-06
Iter: 869 loss: 2.91803258e-06
Iter: 870 loss: 2.91488232e-06
Iter: 871 loss: 2.91083143e-06
Iter: 872 loss: 2.91057495e-06
Iter: 873 loss: 2.90629328e-06
Iter: 874 loss: 2.93177254e-06
Iter: 875 loss: 2.90567596e-06
Iter: 876 loss: 2.90164371e-06
Iter: 877 loss: 2.92713094e-06
Iter: 878 loss: 2.90108665e-06
Iter: 879 loss: 2.89831905e-06
Iter: 880 loss: 2.89360241e-06
Iter: 881 loss: 2.8935824e-06
Iter: 882 loss: 2.88911178e-06
Iter: 883 loss: 2.94946881e-06
Iter: 884 loss: 2.88904948e-06
Iter: 885 loss: 2.88492947e-06
Iter: 886 loss: 2.89669606e-06
Iter: 887 loss: 2.88361753e-06
Iter: 888 loss: 2.88083652e-06
Iter: 889 loss: 2.88977799e-06
Iter: 890 loss: 2.88001706e-06
Iter: 891 loss: 2.87661669e-06
Iter: 892 loss: 2.87632315e-06
Iter: 893 loss: 2.87382e-06
Iter: 894 loss: 2.87070498e-06
Iter: 895 loss: 2.91870879e-06
Iter: 896 loss: 2.87071111e-06
Iter: 897 loss: 2.86806699e-06
Iter: 898 loss: 2.86524892e-06
Iter: 899 loss: 2.86481236e-06
Iter: 900 loss: 2.86090653e-06
Iter: 901 loss: 2.86940258e-06
Iter: 902 loss: 2.85932515e-06
Iter: 903 loss: 2.85497913e-06
Iter: 904 loss: 2.89640184e-06
Iter: 905 loss: 2.85476972e-06
Iter: 906 loss: 2.85218539e-06
Iter: 907 loss: 2.84869907e-06
Iter: 908 loss: 2.84849875e-06
Iter: 909 loss: 2.8451675e-06
Iter: 910 loss: 2.84512612e-06
Iter: 911 loss: 2.84196e-06
Iter: 912 loss: 2.84071029e-06
Iter: 913 loss: 2.83894792e-06
Iter: 914 loss: 2.83529107e-06
Iter: 915 loss: 2.83656345e-06
Iter: 916 loss: 2.8327911e-06
Iter: 917 loss: 2.82957376e-06
Iter: 918 loss: 2.82940937e-06
Iter: 919 loss: 2.82682822e-06
Iter: 920 loss: 2.82413384e-06
Iter: 921 loss: 2.8236575e-06
Iter: 922 loss: 2.81969096e-06
Iter: 923 loss: 2.85210763e-06
Iter: 924 loss: 2.81950224e-06
Iter: 925 loss: 2.81648272e-06
Iter: 926 loss: 2.81865073e-06
Iter: 927 loss: 2.81464077e-06
Iter: 928 loss: 2.81115763e-06
Iter: 929 loss: 2.83966938e-06
Iter: 930 loss: 2.81090979e-06
Iter: 931 loss: 2.80842573e-06
Iter: 932 loss: 2.8046411e-06
Iter: 933 loss: 2.80463701e-06
Iter: 934 loss: 2.80107861e-06
Iter: 935 loss: 2.85470969e-06
Iter: 936 loss: 2.80108407e-06
Iter: 937 loss: 2.79779943e-06
Iter: 938 loss: 2.79851247e-06
Iter: 939 loss: 2.79534515e-06
Iter: 940 loss: 2.79194819e-06
Iter: 941 loss: 2.79312303e-06
Iter: 942 loss: 2.78961716e-06
Iter: 943 loss: 2.78537527e-06
Iter: 944 loss: 2.84371117e-06
Iter: 945 loss: 2.78536163e-06
Iter: 946 loss: 2.78291168e-06
Iter: 947 loss: 2.77943263e-06
Iter: 948 loss: 2.77931395e-06
Iter: 949 loss: 2.77531672e-06
Iter: 950 loss: 2.79115829e-06
Iter: 951 loss: 2.77443064e-06
Iter: 952 loss: 2.77068216e-06
Iter: 953 loss: 2.81334496e-06
Iter: 954 loss: 2.77055233e-06
Iter: 955 loss: 2.76820674e-06
Iter: 956 loss: 2.76641185e-06
Iter: 957 loss: 2.76563787e-06
Iter: 958 loss: 2.7621677e-06
Iter: 959 loss: 2.79178403e-06
Iter: 960 loss: 2.7618994e-06
Iter: 961 loss: 2.75920684e-06
Iter: 962 loss: 2.76093897e-06
Iter: 963 loss: 2.7574888e-06
Iter: 964 loss: 2.75337425e-06
Iter: 965 loss: 2.7663832e-06
Iter: 966 loss: 2.75219827e-06
Iter: 967 loss: 2.74922922e-06
Iter: 968 loss: 2.7474548e-06
Iter: 969 loss: 2.74624972e-06
Iter: 970 loss: 2.74277772e-06
Iter: 971 loss: 2.7427966e-06
Iter: 972 loss: 2.74049671e-06
Iter: 973 loss: 2.73748424e-06
Iter: 974 loss: 2.73731e-06
Iter: 975 loss: 2.73422143e-06
Iter: 976 loss: 2.76377386e-06
Iter: 977 loss: 2.73408205e-06
Iter: 978 loss: 2.73073101e-06
Iter: 979 loss: 2.7336207e-06
Iter: 980 loss: 2.72871262e-06
Iter: 981 loss: 2.72586703e-06
Iter: 982 loss: 2.72428429e-06
Iter: 983 loss: 2.72297484e-06
Iter: 984 loss: 2.71927911e-06
Iter: 985 loss: 2.76330798e-06
Iter: 986 loss: 2.71921931e-06
Iter: 987 loss: 2.71555382e-06
Iter: 988 loss: 2.72268471e-06
Iter: 989 loss: 2.71400904e-06
Iter: 990 loss: 2.71105955e-06
Iter: 991 loss: 2.71455065e-06
Iter: 992 loss: 2.70943929e-06
Iter: 993 loss: 2.70561668e-06
Iter: 994 loss: 2.72387888e-06
Iter: 995 loss: 2.70491864e-06
Iter: 996 loss: 2.70219653e-06
Iter: 997 loss: 2.71342833e-06
Iter: 998 loss: 2.70159762e-06
Iter: 999 loss: 2.69866223e-06
Iter: 1000 loss: 2.69932889e-06
Iter: 1001 loss: 2.6965181e-06
Iter: 1002 loss: 2.69394945e-06
Iter: 1003 loss: 2.70080886e-06
Iter: 1004 loss: 2.69305428e-06
Iter: 1005 loss: 2.68949202e-06
Iter: 1006 loss: 2.70150349e-06
Iter: 1007 loss: 2.68847793e-06
Iter: 1008 loss: 2.68566419e-06
Iter: 1009 loss: 2.68441136e-06
Iter: 1010 loss: 2.68294161e-06
Iter: 1011 loss: 2.68037411e-06
Iter: 1012 loss: 2.68034546e-06
Iter: 1013 loss: 2.67815699e-06
Iter: 1014 loss: 2.6747673e-06
Iter: 1015 loss: 2.67470659e-06
Iter: 1016 loss: 2.67098267e-06
Iter: 1017 loss: 2.67641326e-06
Iter: 1018 loss: 2.66920392e-06
Iter: 1019 loss: 2.66677534e-06
Iter: 1020 loss: 2.66652546e-06
Iter: 1021 loss: 2.66428287e-06
Iter: 1022 loss: 2.66120787e-06
Iter: 1023 loss: 2.66111647e-06
Iter: 1024 loss: 2.6579728e-06
Iter: 1025 loss: 2.6880748e-06
Iter: 1026 loss: 2.6578598e-06
Iter: 1027 loss: 2.65517338e-06
Iter: 1028 loss: 2.65811605e-06
Iter: 1029 loss: 2.65358517e-06
Iter: 1030 loss: 2.65066114e-06
Iter: 1031 loss: 2.6669295e-06
Iter: 1032 loss: 2.65022913e-06
Iter: 1033 loss: 2.64765367e-06
Iter: 1034 loss: 2.64575e-06
Iter: 1035 loss: 2.64482969e-06
Iter: 1036 loss: 2.64204073e-06
Iter: 1037 loss: 2.67432165e-06
Iter: 1038 loss: 2.64198525e-06
Iter: 1039 loss: 2.63908169e-06
Iter: 1040 loss: 2.63969059e-06
Iter: 1041 loss: 2.63695347e-06
Iter: 1042 loss: 2.63388142e-06
Iter: 1043 loss: 2.63882612e-06
Iter: 1044 loss: 2.6325481e-06
Iter: 1045 loss: 2.6290686e-06
Iter: 1046 loss: 2.66219763e-06
Iter: 1047 loss: 2.62889466e-06
Iter: 1048 loss: 2.62671324e-06
Iter: 1049 loss: 2.62197659e-06
Iter: 1050 loss: 2.69426573e-06
Iter: 1051 loss: 2.6217308e-06
Iter: 1052 loss: 2.61795503e-06
Iter: 1053 loss: 2.67718042e-06
Iter: 1054 loss: 2.61792707e-06
Iter: 1055 loss: 2.61498803e-06
Iter: 1056 loss: 2.63842912e-06
Iter: 1057 loss: 2.61474634e-06
Iter: 1058 loss: 2.61292871e-06
Iter: 1059 loss: 2.6108064e-06
Iter: 1060 loss: 2.61055015e-06
Iter: 1061 loss: 2.60734168e-06
Iter: 1062 loss: 2.63622064e-06
Iter: 1063 loss: 2.60720958e-06
Iter: 1064 loss: 2.60465276e-06
Iter: 1065 loss: 2.60729485e-06
Iter: 1066 loss: 2.60330103e-06
Iter: 1067 loss: 2.60024331e-06
Iter: 1068 loss: 2.61157493e-06
Iter: 1069 loss: 2.59951685e-06
Iter: 1070 loss: 2.5970312e-06
Iter: 1071 loss: 2.59624767e-06
Iter: 1072 loss: 2.59479202e-06
Iter: 1073 loss: 2.59212425e-06
Iter: 1074 loss: 2.59214744e-06
Iter: 1075 loss: 2.59006492e-06
Iter: 1076 loss: 2.5873087e-06
Iter: 1077 loss: 2.58718319e-06
Iter: 1078 loss: 2.58462705e-06
Iter: 1079 loss: 2.62063236e-06
Iter: 1080 loss: 2.58466298e-06
Iter: 1081 loss: 2.58197133e-06
Iter: 1082 loss: 2.5797915e-06
Iter: 1083 loss: 2.57899455e-06
Iter: 1084 loss: 2.57537567e-06
Iter: 1085 loss: 2.57625516e-06
Iter: 1086 loss: 2.57275042e-06
Iter: 1087 loss: 2.57014653e-06
Iter: 1088 loss: 2.56991825e-06
Iter: 1089 loss: 2.56724866e-06
Iter: 1090 loss: 2.56507337e-06
Iter: 1091 loss: 2.56429303e-06
Iter: 1092 loss: 2.56113481e-06
Iter: 1093 loss: 2.57923739e-06
Iter: 1094 loss: 2.56064959e-06
Iter: 1095 loss: 2.55772738e-06
Iter: 1096 loss: 2.57100919e-06
Iter: 1097 loss: 2.5571976e-06
Iter: 1098 loss: 2.55500959e-06
Iter: 1099 loss: 2.55879831e-06
Iter: 1100 loss: 2.55412215e-06
Iter: 1101 loss: 2.55116879e-06
Iter: 1102 loss: 2.55218447e-06
Iter: 1103 loss: 2.54923225e-06
Iter: 1104 loss: 2.54603719e-06
Iter: 1105 loss: 2.55703162e-06
Iter: 1106 loss: 2.54523547e-06
Iter: 1107 loss: 2.54152701e-06
Iter: 1108 loss: 2.5567e-06
Iter: 1109 loss: 2.54071347e-06
Iter: 1110 loss: 2.53832468e-06
Iter: 1111 loss: 2.53755979e-06
Iter: 1112 loss: 2.53599569e-06
Iter: 1113 loss: 2.53283747e-06
Iter: 1114 loss: 2.5778204e-06
Iter: 1115 loss: 2.53283542e-06
Iter: 1116 loss: 2.53078e-06
Iter: 1117 loss: 2.52706286e-06
Iter: 1118 loss: 2.61922401e-06
Iter: 1119 loss: 2.52705e-06
Iter: 1120 loss: 2.52303971e-06
Iter: 1121 loss: 2.54153224e-06
Iter: 1122 loss: 2.52229893e-06
Iter: 1123 loss: 2.51923257e-06
Iter: 1124 loss: 2.51921733e-06
Iter: 1125 loss: 2.51735355e-06
Iter: 1126 loss: 2.5137565e-06
Iter: 1127 loss: 2.58824775e-06
Iter: 1128 loss: 2.51372967e-06
Iter: 1129 loss: 2.51092843e-06
Iter: 1130 loss: 2.51083225e-06
Iter: 1131 loss: 2.50885432e-06
Iter: 1132 loss: 2.50919857e-06
Iter: 1133 loss: 2.50733274e-06
Iter: 1134 loss: 2.50472522e-06
Iter: 1135 loss: 2.51624124e-06
Iter: 1136 loss: 2.50424455e-06
Iter: 1137 loss: 2.50196e-06
Iter: 1138 loss: 2.50128755e-06
Iter: 1139 loss: 2.4998858e-06
Iter: 1140 loss: 2.49728714e-06
Iter: 1141 loss: 2.52948871e-06
Iter: 1142 loss: 2.49730465e-06
Iter: 1143 loss: 2.49492064e-06
Iter: 1144 loss: 2.49294044e-06
Iter: 1145 loss: 2.49222194e-06
Iter: 1146 loss: 2.48940205e-06
Iter: 1147 loss: 2.51115125e-06
Iter: 1148 loss: 2.48921515e-06
Iter: 1149 loss: 2.48620722e-06
Iter: 1150 loss: 2.49010282e-06
Iter: 1151 loss: 2.48467495e-06
Iter: 1152 loss: 2.48212882e-06
Iter: 1153 loss: 2.48020433e-06
Iter: 1154 loss: 2.4794474e-06
Iter: 1155 loss: 2.47695061e-06
Iter: 1156 loss: 2.47685762e-06
Iter: 1157 loss: 2.47435491e-06
Iter: 1158 loss: 2.47353432e-06
Iter: 1159 loss: 2.47205526e-06
Iter: 1160 loss: 2.46921036e-06
Iter: 1161 loss: 2.47621597e-06
Iter: 1162 loss: 2.46812942e-06
Iter: 1163 loss: 2.46481886e-06
Iter: 1164 loss: 2.4867295e-06
Iter: 1165 loss: 2.46452828e-06
Iter: 1166 loss: 2.46252739e-06
Iter: 1167 loss: 2.46431637e-06
Iter: 1168 loss: 2.46137847e-06
Iter: 1169 loss: 2.45838601e-06
Iter: 1170 loss: 2.46124046e-06
Iter: 1171 loss: 2.4567064e-06
Iter: 1172 loss: 2.45338333e-06
Iter: 1173 loss: 2.46093668e-06
Iter: 1174 loss: 2.45206684e-06
Iter: 1175 loss: 2.44878606e-06
Iter: 1176 loss: 2.48156812e-06
Iter: 1177 loss: 2.44870694e-06
Iter: 1178 loss: 2.44697321e-06
Iter: 1179 loss: 2.44551234e-06
Iter: 1180 loss: 2.44504599e-06
Iter: 1181 loss: 2.44234889e-06
Iter: 1182 loss: 2.47049775e-06
Iter: 1183 loss: 2.44223838e-06
Iter: 1184 loss: 2.44006242e-06
Iter: 1185 loss: 2.43597424e-06
Iter: 1186 loss: 2.53320559e-06
Iter: 1187 loss: 2.43595082e-06
Iter: 1188 loss: 2.43250906e-06
Iter: 1189 loss: 2.45860519e-06
Iter: 1190 loss: 2.43220802e-06
Iter: 1191 loss: 2.42934e-06
Iter: 1192 loss: 2.45885349e-06
Iter: 1193 loss: 2.42916281e-06
Iter: 1194 loss: 2.42734382e-06
Iter: 1195 loss: 2.42482633e-06
Iter: 1196 loss: 2.42470969e-06
Iter: 1197 loss: 2.42211718e-06
Iter: 1198 loss: 2.42208353e-06
Iter: 1199 loss: 2.41998941e-06
Iter: 1200 loss: 2.41944463e-06
Iter: 1201 loss: 2.41809403e-06
Iter: 1202 loss: 2.41558837e-06
Iter: 1203 loss: 2.43092313e-06
Iter: 1204 loss: 2.41530802e-06
Iter: 1205 loss: 2.41292673e-06
Iter: 1206 loss: 2.41209159e-06
Iter: 1207 loss: 2.41081398e-06
Iter: 1208 loss: 2.40823738e-06
Iter: 1209 loss: 2.44058037e-06
Iter: 1210 loss: 2.4081919e-06
Iter: 1211 loss: 2.40590089e-06
Iter: 1212 loss: 2.40431268e-06
Iter: 1213 loss: 2.4034066e-06
Iter: 1214 loss: 2.40065037e-06
Iter: 1215 loss: 2.42000351e-06
Iter: 1216 loss: 2.40042527e-06
Iter: 1217 loss: 2.39748238e-06
Iter: 1218 loss: 2.39991277e-06
Iter: 1219 loss: 2.39573092e-06
Iter: 1220 loss: 2.39287556e-06
Iter: 1221 loss: 2.39222186e-06
Iter: 1222 loss: 2.39036081e-06
Iter: 1223 loss: 2.38810298e-06
Iter: 1224 loss: 2.38793314e-06
Iter: 1225 loss: 2.38571124e-06
Iter: 1226 loss: 2.3837938e-06
Iter: 1227 loss: 2.38311759e-06
Iter: 1228 loss: 2.38043299e-06
Iter: 1229 loss: 2.3897237e-06
Iter: 1230 loss: 2.37976155e-06
Iter: 1231 loss: 2.37677295e-06
Iter: 1232 loss: 2.3960888e-06
Iter: 1233 loss: 2.37644167e-06
Iter: 1234 loss: 2.37453219e-06
Iter: 1235 loss: 2.37429822e-06
Iter: 1236 loss: 2.37286577e-06
Iter: 1237 loss: 2.36982373e-06
Iter: 1238 loss: 2.38217444e-06
Iter: 1239 loss: 2.36910364e-06
Iter: 1240 loss: 2.36673486e-06
Iter: 1241 loss: 2.37026302e-06
Iter: 1242 loss: 2.36555115e-06
Iter: 1243 loss: 2.36274605e-06
Iter: 1244 loss: 2.38225834e-06
Iter: 1245 loss: 2.3625098e-06
Iter: 1246 loss: 2.36056667e-06
Iter: 1247 loss: 2.35887046e-06
Iter: 1248 loss: 2.35836342e-06
Iter: 1249 loss: 2.35543894e-06
Iter: 1250 loss: 2.39039537e-06
Iter: 1251 loss: 2.35538255e-06
Iter: 1252 loss: 2.35325342e-06
Iter: 1253 loss: 2.35027778e-06
Iter: 1254 loss: 2.35011112e-06
Iter: 1255 loss: 2.34675827e-06
Iter: 1256 loss: 2.35872767e-06
Iter: 1257 loss: 2.34585923e-06
Iter: 1258 loss: 2.34232857e-06
Iter: 1259 loss: 2.38091843e-06
Iter: 1260 loss: 2.34227264e-06
Iter: 1261 loss: 2.34043432e-06
Iter: 1262 loss: 2.33820447e-06
Iter: 1263 loss: 2.33799938e-06
Iter: 1264 loss: 2.33569631e-06
Iter: 1265 loss: 2.33567857e-06
Iter: 1266 loss: 2.33357082e-06
Iter: 1267 loss: 2.33180526e-06
Iter: 1268 loss: 2.33114838e-06
Iter: 1269 loss: 2.32837147e-06
Iter: 1270 loss: 2.34448862e-06
Iter: 1271 loss: 2.3280752e-06
Iter: 1272 loss: 2.32527282e-06
Iter: 1273 loss: 2.32716434e-06
Iter: 1274 loss: 2.32349407e-06
Iter: 1275 loss: 2.32103298e-06
Iter: 1276 loss: 2.34522872e-06
Iter: 1277 loss: 2.3209559e-06
Iter: 1278 loss: 2.31866579e-06
Iter: 1279 loss: 2.31769718e-06
Iter: 1280 loss: 2.31657123e-06
Iter: 1281 loss: 2.31402919e-06
Iter: 1282 loss: 2.33157471e-06
Iter: 1283 loss: 2.31384729e-06
Iter: 1284 loss: 2.31126364e-06
Iter: 1285 loss: 2.31413924e-06
Iter: 1286 loss: 2.30983642e-06
Iter: 1287 loss: 2.30733531e-06
Iter: 1288 loss: 2.305115e-06
Iter: 1289 loss: 2.30448813e-06
Iter: 1290 loss: 2.30271462e-06
Iter: 1291 loss: 2.30230899e-06
Iter: 1292 loss: 2.30034175e-06
Iter: 1293 loss: 2.29862599e-06
Iter: 1294 loss: 2.2980596e-06
Iter: 1295 loss: 2.29593365e-06
Iter: 1296 loss: 2.30654882e-06
Iter: 1297 loss: 2.29562693e-06
Iter: 1298 loss: 2.29322177e-06
Iter: 1299 loss: 2.30066462e-06
Iter: 1300 loss: 2.29260218e-06
Iter: 1301 loss: 2.29053808e-06
Iter: 1302 loss: 2.28916952e-06
Iter: 1303 loss: 2.28836643e-06
Iter: 1304 loss: 2.28545741e-06
Iter: 1305 loss: 2.3157088e-06
Iter: 1306 loss: 2.28539875e-06
Iter: 1307 loss: 2.28332556e-06
Iter: 1308 loss: 2.28324961e-06
Iter: 1309 loss: 2.28172462e-06
Iter: 1310 loss: 2.27888086e-06
Iter: 1311 loss: 2.30301703e-06
Iter: 1312 loss: 2.27866167e-06
Iter: 1313 loss: 2.27664191e-06
Iter: 1314 loss: 2.27482838e-06
Iter: 1315 loss: 2.27430382e-06
Iter: 1316 loss: 2.27187047e-06
Iter: 1317 loss: 2.27189776e-06
Iter: 1318 loss: 2.27023361e-06
Iter: 1319 loss: 2.26739166e-06
Iter: 1320 loss: 2.26740326e-06
Iter: 1321 loss: 2.26426755e-06
Iter: 1322 loss: 2.27662349e-06
Iter: 1323 loss: 2.26357906e-06
Iter: 1324 loss: 2.26110524e-06
Iter: 1325 loss: 2.26108978e-06
Iter: 1326 loss: 2.25974077e-06
Iter: 1327 loss: 2.25729582e-06
Iter: 1328 loss: 2.3128091e-06
Iter: 1329 loss: 2.25728104e-06
Iter: 1330 loss: 2.25508029e-06
Iter: 1331 loss: 2.2550546e-06
Iter: 1332 loss: 2.25324698e-06
Iter: 1333 loss: 2.25058056e-06
Iter: 1334 loss: 2.2505028e-06
Iter: 1335 loss: 2.24768337e-06
Iter: 1336 loss: 2.26849306e-06
Iter: 1337 loss: 2.24742394e-06
Iter: 1338 loss: 2.24460541e-06
Iter: 1339 loss: 2.24837049e-06
Iter: 1340 loss: 2.24325186e-06
Iter: 1341 loss: 2.24082169e-06
Iter: 1342 loss: 2.25583972e-06
Iter: 1343 loss: 2.24048836e-06
Iter: 1344 loss: 2.23806865e-06
Iter: 1345 loss: 2.24004953e-06
Iter: 1346 loss: 2.23659936e-06
Iter: 1347 loss: 2.23428083e-06
Iter: 1348 loss: 2.24406949e-06
Iter: 1349 loss: 2.23381289e-06
Iter: 1350 loss: 2.23132247e-06
Iter: 1351 loss: 2.23633856e-06
Iter: 1352 loss: 2.23028815e-06
Iter: 1353 loss: 2.22809967e-06
Iter: 1354 loss: 2.22621293e-06
Iter: 1355 loss: 2.22573044e-06
Iter: 1356 loss: 2.22358062e-06
Iter: 1357 loss: 2.22352082e-06
Iter: 1358 loss: 2.22136191e-06
Iter: 1359 loss: 2.22118524e-06
Iter: 1360 loss: 2.21951723e-06
Iter: 1361 loss: 2.21727328e-06
Iter: 1362 loss: 2.22286621e-06
Iter: 1363 loss: 2.21657092e-06
Iter: 1364 loss: 2.21378286e-06
Iter: 1365 loss: 2.22731342e-06
Iter: 1366 loss: 2.21331379e-06
Iter: 1367 loss: 2.21155778e-06
Iter: 1368 loss: 2.20991501e-06
Iter: 1369 loss: 2.20950187e-06
Iter: 1370 loss: 2.20674565e-06
Iter: 1371 loss: 2.23479242e-06
Iter: 1372 loss: 2.20669108e-06
Iter: 1373 loss: 2.20467564e-06
Iter: 1374 loss: 2.2049403e-06
Iter: 1375 loss: 2.20308448e-06
Iter: 1376 loss: 2.20071934e-06
Iter: 1377 loss: 2.22372273e-06
Iter: 1378 loss: 2.20061247e-06
Iter: 1379 loss: 2.19872322e-06
Iter: 1380 loss: 2.19754656e-06
Iter: 1381 loss: 2.19680805e-06
Iter: 1382 loss: 2.19446792e-06
Iter: 1383 loss: 2.22509289e-06
Iter: 1384 loss: 2.19448134e-06
Iter: 1385 loss: 2.19281242e-06
Iter: 1386 loss: 2.1907083e-06
Iter: 1387 loss: 2.19053572e-06
Iter: 1388 loss: 2.18792638e-06
Iter: 1389 loss: 2.18995706e-06
Iter: 1390 loss: 2.18634523e-06
Iter: 1391 loss: 2.18476703e-06
Iter: 1392 loss: 2.18434866e-06
Iter: 1393 loss: 2.18289961e-06
Iter: 1394 loss: 2.18048626e-06
Iter: 1395 loss: 2.18049331e-06
Iter: 1396 loss: 2.17841398e-06
Iter: 1397 loss: 2.20933225e-06
Iter: 1398 loss: 2.17845331e-06
Iter: 1399 loss: 2.1764904e-06
Iter: 1400 loss: 2.17457568e-06
Iter: 1401 loss: 2.17421666e-06
Iter: 1402 loss: 2.17217053e-06
Iter: 1403 loss: 2.1860219e-06
Iter: 1404 loss: 2.17192246e-06
Iter: 1405 loss: 2.16969602e-06
Iter: 1406 loss: 2.17063462e-06
Iter: 1407 loss: 2.16804801e-06
Iter: 1408 loss: 2.16533e-06
Iter: 1409 loss: 2.17395177e-06
Iter: 1410 loss: 2.16446279e-06
Iter: 1411 loss: 2.16147373e-06
Iter: 1412 loss: 2.17592742e-06
Iter: 1413 loss: 2.16092894e-06
Iter: 1414 loss: 2.15869431e-06
Iter: 1415 loss: 2.1596129e-06
Iter: 1416 loss: 2.15718978e-06
Iter: 1417 loss: 2.1542653e-06
Iter: 1418 loss: 2.17766819e-06
Iter: 1419 loss: 2.15404771e-06
Iter: 1420 loss: 2.1523e-06
Iter: 1421 loss: 2.14951706e-06
Iter: 1422 loss: 2.14947295e-06
Iter: 1423 loss: 2.14717647e-06
Iter: 1424 loss: 2.17703951e-06
Iter: 1425 loss: 2.14715851e-06
Iter: 1426 loss: 2.14496731e-06
Iter: 1427 loss: 2.15566706e-06
Iter: 1428 loss: 2.14458555e-06
Iter: 1429 loss: 2.14298052e-06
Iter: 1430 loss: 2.14181318e-06
Iter: 1431 loss: 2.14130068e-06
Iter: 1432 loss: 2.13860949e-06
Iter: 1433 loss: 2.16198691e-06
Iter: 1434 loss: 2.13848421e-06
Iter: 1435 loss: 2.13670546e-06
Iter: 1436 loss: 2.13453563e-06
Iter: 1437 loss: 2.13436056e-06
Iter: 1438 loss: 2.13206931e-06
Iter: 1439 loss: 2.16728e-06
Iter: 1440 loss: 2.13206658e-06
Iter: 1441 loss: 2.13026965e-06
Iter: 1442 loss: 2.12913096e-06
Iter: 1443 loss: 2.12841928e-06
Iter: 1444 loss: 2.12610962e-06
Iter: 1445 loss: 2.14913644e-06
Iter: 1446 loss: 2.12602254e-06
Iter: 1447 loss: 2.12397936e-06
Iter: 1448 loss: 2.12334157e-06
Iter: 1449 loss: 2.12211307e-06
Iter: 1450 loss: 2.11964266e-06
Iter: 1451 loss: 2.1373578e-06
Iter: 1452 loss: 2.11944325e-06
Iter: 1453 loss: 2.11701627e-06
Iter: 1454 loss: 2.11787824e-06
Iter: 1455 loss: 2.11528049e-06
Iter: 1456 loss: 2.11310203e-06
Iter: 1457 loss: 2.11246629e-06
Iter: 1458 loss: 2.11109023e-06
Iter: 1459 loss: 2.10959934e-06
Iter: 1460 loss: 2.10926237e-06
Iter: 1461 loss: 2.10757344e-06
Iter: 1462 loss: 2.10700045e-06
Iter: 1463 loss: 2.10600456e-06
Iter: 1464 loss: 2.10411554e-06
Iter: 1465 loss: 2.11329e-06
Iter: 1466 loss: 2.10386065e-06
Iter: 1467 loss: 2.10154599e-06
Iter: 1468 loss: 2.10111193e-06
Iter: 1469 loss: 2.09960422e-06
Iter: 1470 loss: 2.09728978e-06
Iter: 1471 loss: 2.10374765e-06
Iter: 1472 loss: 2.09653899e-06
Iter: 1473 loss: 2.09399968e-06
Iter: 1474 loss: 2.10860799e-06
Iter: 1475 loss: 2.09364453e-06
Iter: 1476 loss: 2.09176551e-06
Iter: 1477 loss: 2.09156724e-06
Iter: 1478 loss: 2.0902196e-06
Iter: 1479 loss: 2.08755978e-06
Iter: 1480 loss: 2.11164297e-06
Iter: 1481 loss: 2.08743813e-06
Iter: 1482 loss: 2.08567803e-06
Iter: 1483 loss: 2.08511074e-06
Iter: 1484 loss: 2.08411802e-06
Iter: 1485 loss: 2.08146866e-06
Iter: 1486 loss: 2.10061603e-06
Iter: 1487 loss: 2.08120741e-06
Iter: 1488 loss: 2.07919538e-06
Iter: 1489 loss: 2.07703306e-06
Iter: 1490 loss: 2.07669291e-06
Iter: 1491 loss: 2.07378662e-06
Iter: 1492 loss: 2.08113647e-06
Iter: 1493 loss: 2.07278799e-06
Iter: 1494 loss: 2.07125527e-06
Iter: 1495 loss: 2.0709806e-06
Iter: 1496 loss: 2.0697405e-06
Iter: 1497 loss: 2.06766572e-06
Iter: 1498 loss: 2.06766845e-06
Iter: 1499 loss: 2.0655425e-06
Iter: 1500 loss: 2.09463087e-06
Iter: 1501 loss: 2.06553636e-06
Iter: 1502 loss: 2.06387858e-06
Iter: 1503 loss: 2.06098593e-06
Iter: 1504 loss: 2.06099548e-06
Iter: 1505 loss: 2.05873403e-06
Iter: 1506 loss: 2.0924e-06
Iter: 1507 loss: 2.05872379e-06
Iter: 1508 loss: 2.05687866e-06
Iter: 1509 loss: 2.05946526e-06
Iter: 1510 loss: 2.05592869e-06
Iter: 1511 loss: 2.0542584e-06
Iter: 1512 loss: 2.05935453e-06
Iter: 1513 loss: 2.05377546e-06
Iter: 1514 loss: 2.05172887e-06
Iter: 1515 loss: 2.05591959e-06
Iter: 1516 loss: 2.050886e-06
Iter: 1517 loss: 2.04925982e-06
Iter: 1518 loss: 2.05386641e-06
Iter: 1519 loss: 2.0486932e-06
Iter: 1520 loss: 2.04665025e-06
Iter: 1521 loss: 2.05006086e-06
Iter: 1522 loss: 2.04567596e-06
Iter: 1523 loss: 2.04356911e-06
Iter: 1524 loss: 2.04162097e-06
Iter: 1525 loss: 2.04109801e-06
Iter: 1526 loss: 2.03807349e-06
Iter: 1527 loss: 2.05142533e-06
Iter: 1528 loss: 2.03746163e-06
Iter: 1529 loss: 2.03574791e-06
Iter: 1530 loss: 2.03556738e-06
Iter: 1531 loss: 2.0343366e-06
Iter: 1532 loss: 2.03202717e-06
Iter: 1533 loss: 2.0826842e-06
Iter: 1534 loss: 2.03202671e-06
Iter: 1535 loss: 2.03013929e-06
Iter: 1536 loss: 2.03011632e-06
Iter: 1537 loss: 2.02882575e-06
Iter: 1538 loss: 2.02647607e-06
Iter: 1539 loss: 2.07800804e-06
Iter: 1540 loss: 2.02646243e-06
Iter: 1541 loss: 2.02409888e-06
Iter: 1542 loss: 2.0485304e-06
Iter: 1543 loss: 2.02404726e-06
Iter: 1544 loss: 2.02183855e-06
Iter: 1545 loss: 2.02359865e-06
Iter: 1546 loss: 2.02044589e-06
Iter: 1547 loss: 2.01842386e-06
Iter: 1548 loss: 2.02937645e-06
Iter: 1549 loss: 2.01818648e-06
Iter: 1550 loss: 2.01605417e-06
Iter: 1551 loss: 2.01816101e-06
Iter: 1552 loss: 2.01486455e-06
Iter: 1553 loss: 2.01305784e-06
Iter: 1554 loss: 2.01933267e-06
Iter: 1555 loss: 2.01261605e-06
Iter: 1556 loss: 2.0102666e-06
Iter: 1557 loss: 2.01456533e-06
Iter: 1558 loss: 2.00935347e-06
Iter: 1559 loss: 2.00718864e-06
Iter: 1560 loss: 2.00654e-06
Iter: 1561 loss: 2.00523391e-06
Iter: 1562 loss: 2.00269983e-06
Iter: 1563 loss: 2.00588147e-06
Iter: 1564 loss: 2.00132081e-06
Iter: 1565 loss: 2.00036038e-06
Iter: 1566 loss: 1.99982014e-06
Iter: 1567 loss: 1.9984891e-06
Iter: 1568 loss: 1.99753754e-06
Iter: 1569 loss: 1.99706164e-06
Iter: 1570 loss: 1.99538408e-06
Iter: 1571 loss: 2.00368777e-06
Iter: 1572 loss: 1.99507963e-06
Iter: 1573 loss: 1.99309875e-06
Iter: 1574 loss: 1.99410147e-06
Iter: 1575 loss: 1.99170199e-06
Iter: 1576 loss: 1.98981593e-06
Iter: 1577 loss: 1.9910708e-06
Iter: 1578 loss: 1.98858697e-06
Iter: 1579 loss: 1.98647876e-06
Iter: 1580 loss: 2.0168527e-06
Iter: 1581 loss: 1.98648604e-06
Iter: 1582 loss: 1.98530779e-06
Iter: 1583 loss: 1.98508474e-06
Iter: 1584 loss: 1.98428233e-06
Iter: 1585 loss: 1.98232e-06
Iter: 1586 loss: 1.99103374e-06
Iter: 1587 loss: 1.98192106e-06
Iter: 1588 loss: 1.98026032e-06
Iter: 1589 loss: 1.97945087e-06
Iter: 1590 loss: 1.97869349e-06
Iter: 1591 loss: 1.9766162e-06
Iter: 1592 loss: 2.00671889e-06
Iter: 1593 loss: 1.97660052e-06
Iter: 1594 loss: 1.97532154e-06
Iter: 1595 loss: 1.97345798e-06
Iter: 1596 loss: 1.97334703e-06
Iter: 1597 loss: 1.97082773e-06
Iter: 1598 loss: 1.97317036e-06
Iter: 1599 loss: 1.96933047e-06
Iter: 1600 loss: 1.96659494e-06
Iter: 1601 loss: 1.9768961e-06
Iter: 1602 loss: 1.96593282e-06
Iter: 1603 loss: 1.96397968e-06
Iter: 1604 loss: 1.9639881e-06
Iter: 1605 loss: 1.96203018e-06
Iter: 1606 loss: 1.96448946e-06
Iter: 1607 loss: 1.96105975e-06
Iter: 1608 loss: 1.95955568e-06
Iter: 1609 loss: 1.96129986e-06
Iter: 1610 loss: 1.95876783e-06
Iter: 1611 loss: 1.95674147e-06
Iter: 1612 loss: 1.9702486e-06
Iter: 1613 loss: 1.95658208e-06
Iter: 1614 loss: 1.95529537e-06
Iter: 1615 loss: 1.95449e-06
Iter: 1616 loss: 1.95401844e-06
Iter: 1617 loss: 1.95218445e-06
Iter: 1618 loss: 1.97147938e-06
Iter: 1619 loss: 1.95216558e-06
Iter: 1620 loss: 1.9509755e-06
Iter: 1621 loss: 1.95067651e-06
Iter: 1622 loss: 1.94993913e-06
Iter: 1623 loss: 1.94815902e-06
Iter: 1624 loss: 1.9606041e-06
Iter: 1625 loss: 1.94795894e-06
Iter: 1626 loss: 1.94658219e-06
Iter: 1627 loss: 1.94565519e-06
Iter: 1628 loss: 1.94509244e-06
Iter: 1629 loss: 1.94334689e-06
Iter: 1630 loss: 1.96726478e-06
Iter: 1631 loss: 1.9433628e-06
Iter: 1632 loss: 1.94209724e-06
Iter: 1633 loss: 1.94050881e-06
Iter: 1634 loss: 1.940429e-06
Iter: 1635 loss: 1.93817232e-06
Iter: 1636 loss: 1.93714141e-06
Iter: 1637 loss: 1.93607571e-06
Iter: 1638 loss: 1.93308711e-06
Iter: 1639 loss: 1.95818529e-06
Iter: 1640 loss: 1.93298683e-06
Iter: 1641 loss: 1.9313552e-06
Iter: 1642 loss: 1.93134474e-06
Iter: 1643 loss: 1.92980724e-06
Iter: 1644 loss: 1.92893231e-06
Iter: 1645 loss: 1.92825496e-06
Iter: 1646 loss: 1.92664857e-06
Iter: 1647 loss: 1.93457595e-06
Iter: 1648 loss: 1.92636026e-06
Iter: 1649 loss: 1.92460038e-06
Iter: 1650 loss: 1.93106598e-06
Iter: 1651 loss: 1.92414359e-06
Iter: 1652 loss: 1.9230215e-06
Iter: 1653 loss: 1.92398261e-06
Iter: 1654 loss: 1.92228072e-06
Iter: 1655 loss: 1.92060429e-06
Iter: 1656 loss: 1.92602033e-06
Iter: 1657 loss: 1.92007428e-06
Iter: 1658 loss: 1.91869412e-06
Iter: 1659 loss: 1.92145399e-06
Iter: 1660 loss: 1.91816343e-06
Iter: 1661 loss: 1.91643949e-06
Iter: 1662 loss: 1.92121502e-06
Iter: 1663 loss: 1.91585946e-06
Iter: 1664 loss: 1.91439358e-06
Iter: 1665 loss: 1.91595154e-06
Iter: 1666 loss: 1.91353956e-06
Iter: 1667 loss: 1.91157915e-06
Iter: 1668 loss: 1.9216634e-06
Iter: 1669 loss: 1.91128515e-06
Iter: 1670 loss: 1.90992614e-06
Iter: 1671 loss: 1.90821265e-06
Iter: 1672 loss: 1.90808396e-06
Iter: 1673 loss: 1.90577589e-06
Iter: 1674 loss: 1.91080608e-06
Iter: 1675 loss: 1.90495211e-06
Iter: 1676 loss: 1.90270748e-06
Iter: 1677 loss: 1.91515119e-06
Iter: 1678 loss: 1.90236665e-06
Iter: 1679 loss: 1.900435e-06
Iter: 1680 loss: 1.92653829e-06
Iter: 1681 loss: 1.90045228e-06
Iter: 1682 loss: 1.89929881e-06
Iter: 1683 loss: 1.89746197e-06
Iter: 1684 loss: 1.8974489e-06
Iter: 1685 loss: 1.89571233e-06
Iter: 1686 loss: 1.89569755e-06
Iter: 1687 loss: 1.89432865e-06
Iter: 1688 loss: 1.89366438e-06
Iter: 1689 loss: 1.89299726e-06
Iter: 1690 loss: 1.8913903e-06
Iter: 1691 loss: 1.9037717e-06
Iter: 1692 loss: 1.89124489e-06
Iter: 1693 loss: 1.88971251e-06
Iter: 1694 loss: 1.89012167e-06
Iter: 1695 loss: 1.88861259e-06
Iter: 1696 loss: 1.88700312e-06
Iter: 1697 loss: 1.90203605e-06
Iter: 1698 loss: 1.88694412e-06
Iter: 1699 loss: 1.88571494e-06
Iter: 1700 loss: 1.88526292e-06
Iter: 1701 loss: 1.8845534e-06
Iter: 1702 loss: 1.88311151e-06
Iter: 1703 loss: 1.89929051e-06
Iter: 1704 loss: 1.88313129e-06
Iter: 1705 loss: 1.88189244e-06
Iter: 1706 loss: 1.87978515e-06
Iter: 1707 loss: 1.87977582e-06
Iter: 1708 loss: 1.87753562e-06
Iter: 1709 loss: 1.88039496e-06
Iter: 1710 loss: 1.8763219e-06
Iter: 1711 loss: 1.87412411e-06
Iter: 1712 loss: 1.888003e-06
Iter: 1713 loss: 1.87387775e-06
Iter: 1714 loss: 1.87217e-06
Iter: 1715 loss: 1.87216096e-06
Iter: 1716 loss: 1.87113415e-06
Iter: 1717 loss: 1.86927082e-06
Iter: 1718 loss: 1.86926411e-06
Iter: 1719 loss: 1.8678395e-06
Iter: 1720 loss: 1.89044817e-06
Iter: 1721 loss: 1.86781278e-06
Iter: 1722 loss: 1.86626471e-06
Iter: 1723 loss: 1.86575448e-06
Iter: 1724 loss: 1.86484635e-06
Iter: 1725 loss: 1.86323143e-06
Iter: 1726 loss: 1.87012768e-06
Iter: 1727 loss: 1.86284865e-06
Iter: 1728 loss: 1.86112811e-06
Iter: 1729 loss: 1.86646741e-06
Iter: 1730 loss: 1.86065427e-06
Iter: 1731 loss: 1.85929514e-06
Iter: 1732 loss: 1.86036073e-06
Iter: 1733 loss: 1.85844692e-06
Iter: 1734 loss: 1.85661645e-06
Iter: 1735 loss: 1.86801253e-06
Iter: 1736 loss: 1.85638817e-06
Iter: 1737 loss: 1.85512181e-06
Iter: 1738 loss: 1.85506838e-06
Iter: 1739 loss: 1.85405179e-06
Iter: 1740 loss: 1.85205522e-06
Iter: 1741 loss: 1.86084378e-06
Iter: 1742 loss: 1.85164561e-06
Iter: 1743 loss: 1.85014233e-06
Iter: 1744 loss: 1.84872397e-06
Iter: 1745 loss: 1.84837813e-06
Iter: 1746 loss: 1.84594114e-06
Iter: 1747 loss: 1.85224235e-06
Iter: 1748 loss: 1.84516591e-06
Iter: 1749 loss: 1.84286114e-06
Iter: 1750 loss: 1.85259159e-06
Iter: 1751 loss: 1.84242231e-06
Iter: 1752 loss: 1.84093e-06
Iter: 1753 loss: 1.84084251e-06
Iter: 1754 loss: 1.84006444e-06
Iter: 1755 loss: 1.83824989e-06
Iter: 1756 loss: 1.86361694e-06
Iter: 1757 loss: 1.838137e-06
Iter: 1758 loss: 1.83640873e-06
Iter: 1759 loss: 1.83642055e-06
Iter: 1760 loss: 1.83485656e-06
Iter: 1761 loss: 1.83375869e-06
Iter: 1762 loss: 1.83316649e-06
Iter: 1763 loss: 1.83140105e-06
Iter: 1764 loss: 1.84079022e-06
Iter: 1765 loss: 1.83107841e-06
Iter: 1766 loss: 1.82948281e-06
Iter: 1767 loss: 1.83772067e-06
Iter: 1768 loss: 1.82924009e-06
Iter: 1769 loss: 1.82801296e-06
Iter: 1770 loss: 1.82801409e-06
Iter: 1771 loss: 1.8270697e-06
Iter: 1772 loss: 1.8251867e-06
Iter: 1773 loss: 1.83649843e-06
Iter: 1774 loss: 1.82495728e-06
Iter: 1775 loss: 1.82375686e-06
Iter: 1776 loss: 1.82513747e-06
Iter: 1777 loss: 1.82313715e-06
Iter: 1778 loss: 1.82136432e-06
Iter: 1779 loss: 1.82419456e-06
Iter: 1780 loss: 1.82053384e-06
Iter: 1781 loss: 1.81882774e-06
Iter: 1782 loss: 1.8177991e-06
Iter: 1783 loss: 1.81707264e-06
Iter: 1784 loss: 1.81464679e-06
Iter: 1785 loss: 1.82362669e-06
Iter: 1786 loss: 1.81404721e-06
Iter: 1787 loss: 1.81227915e-06
Iter: 1788 loss: 1.82564338e-06
Iter: 1789 loss: 1.8121591e-06
Iter: 1790 loss: 1.81015434e-06
Iter: 1791 loss: 1.81737755e-06
Iter: 1792 loss: 1.80959705e-06
Iter: 1793 loss: 1.80827681e-06
Iter: 1794 loss: 1.80728148e-06
Iter: 1795 loss: 1.8068888e-06
Iter: 1796 loss: 1.80551024e-06
Iter: 1797 loss: 1.80545749e-06
Iter: 1798 loss: 1.8044218e-06
Iter: 1799 loss: 1.80261782e-06
Iter: 1800 loss: 1.80261293e-06
Iter: 1801 loss: 1.8006524e-06
Iter: 1802 loss: 1.81465782e-06
Iter: 1803 loss: 1.80045481e-06
Iter: 1804 loss: 1.79856602e-06
Iter: 1805 loss: 1.80338725e-06
Iter: 1806 loss: 1.7978864e-06
Iter: 1807 loss: 1.79651556e-06
Iter: 1808 loss: 1.80250686e-06
Iter: 1809 loss: 1.79628751e-06
Iter: 1810 loss: 1.79478752e-06
Iter: 1811 loss: 1.79508277e-06
Iter: 1812 loss: 1.79365713e-06
Iter: 1813 loss: 1.7918818e-06
Iter: 1814 loss: 1.80155632e-06
Iter: 1815 loss: 1.79159929e-06
Iter: 1816 loss: 1.78988353e-06
Iter: 1817 loss: 1.79100584e-06
Iter: 1818 loss: 1.78878213e-06
Iter: 1819 loss: 1.7871314e-06
Iter: 1820 loss: 1.78670859e-06
Iter: 1821 loss: 1.78570633e-06
Iter: 1822 loss: 1.78335983e-06
Iter: 1823 loss: 1.78994605e-06
Iter: 1824 loss: 1.78261587e-06
Iter: 1825 loss: 1.78119137e-06
Iter: 1826 loss: 1.78110463e-06
Iter: 1827 loss: 1.77976858e-06
Iter: 1828 loss: 1.77848744e-06
Iter: 1829 loss: 1.77814877e-06
Iter: 1830 loss: 1.77654374e-06
Iter: 1831 loss: 1.77976437e-06
Iter: 1832 loss: 1.77586548e-06
Iter: 1833 loss: 1.77415586e-06
Iter: 1834 loss: 1.79662788e-06
Iter: 1835 loss: 1.77418622e-06
Iter: 1836 loss: 1.77312427e-06
Iter: 1837 loss: 1.77162542e-06
Iter: 1838 loss: 1.77158006e-06
Iter: 1839 loss: 1.76999811e-06
Iter: 1840 loss: 1.79428173e-06
Iter: 1841 loss: 1.76999856e-06
Iter: 1842 loss: 1.76884112e-06
Iter: 1843 loss: 1.76820515e-06
Iter: 1844 loss: 1.7676706e-06
Iter: 1845 loss: 1.76602452e-06
Iter: 1846 loss: 1.77952688e-06
Iter: 1847 loss: 1.76590299e-06
Iter: 1848 loss: 1.76464039e-06
Iter: 1849 loss: 1.76554659e-06
Iter: 1850 loss: 1.76384287e-06
Iter: 1851 loss: 1.76229992e-06
Iter: 1852 loss: 1.77023412e-06
Iter: 1853 loss: 1.76207561e-06
Iter: 1854 loss: 1.76076094e-06
Iter: 1855 loss: 1.75952346e-06
Iter: 1856 loss: 1.75917057e-06
Iter: 1857 loss: 1.75719492e-06
Iter: 1858 loss: 1.75932109e-06
Iter: 1859 loss: 1.75609807e-06
Iter: 1860 loss: 1.7553283e-06
Iter: 1861 loss: 1.75499326e-06
Iter: 1862 loss: 1.75387777e-06
Iter: 1863 loss: 1.75208936e-06
Iter: 1864 loss: 1.75210153e-06
Iter: 1865 loss: 1.75061155e-06
Iter: 1866 loss: 1.76004994e-06
Iter: 1867 loss: 1.75046705e-06
Iter: 1868 loss: 1.7491202e-06
Iter: 1869 loss: 1.7586101e-06
Iter: 1870 loss: 1.74898503e-06
Iter: 1871 loss: 1.74808861e-06
Iter: 1872 loss: 1.74650427e-06
Iter: 1873 loss: 1.74647062e-06
Iter: 1874 loss: 1.7451913e-06
Iter: 1875 loss: 1.74519437e-06
Iter: 1876 loss: 1.74412071e-06
Iter: 1877 loss: 1.74308536e-06
Iter: 1878 loss: 1.7428622e-06
Iter: 1879 loss: 1.74145941e-06
Iter: 1880 loss: 1.75540094e-06
Iter: 1881 loss: 1.74141667e-06
Iter: 1882 loss: 1.74007027e-06
Iter: 1883 loss: 1.73950502e-06
Iter: 1884 loss: 1.73876424e-06
Iter: 1885 loss: 1.73726187e-06
Iter: 1886 loss: 1.7486791e-06
Iter: 1887 loss: 1.73715637e-06
Iter: 1888 loss: 1.73580725e-06
Iter: 1889 loss: 1.73550336e-06
Iter: 1890 loss: 1.73459523e-06
Iter: 1891 loss: 1.73294404e-06
Iter: 1892 loss: 1.73368312e-06
Iter: 1893 loss: 1.73182627e-06
Iter: 1894 loss: 1.72954913e-06
Iter: 1895 loss: 1.73600654e-06
Iter: 1896 loss: 1.72882403e-06
Iter: 1897 loss: 1.72790578e-06
Iter: 1898 loss: 1.72768591e-06
Iter: 1899 loss: 1.72658815e-06
Iter: 1900 loss: 1.72501677e-06
Iter: 1901 loss: 1.72497164e-06
Iter: 1902 loss: 1.72331875e-06
Iter: 1903 loss: 1.73203182e-06
Iter: 1904 loss: 1.72305852e-06
Iter: 1905 loss: 1.72110072e-06
Iter: 1906 loss: 1.72783211e-06
Iter: 1907 loss: 1.72061334e-06
Iter: 1908 loss: 1.71951729e-06
Iter: 1909 loss: 1.7182449e-06
Iter: 1910 loss: 1.71803356e-06
Iter: 1911 loss: 1.71622196e-06
Iter: 1912 loss: 1.74337094e-06
Iter: 1913 loss: 1.71621e-06
Iter: 1914 loss: 1.71501415e-06
Iter: 1915 loss: 1.71373892e-06
Iter: 1916 loss: 1.71351815e-06
Iter: 1917 loss: 1.71182785e-06
Iter: 1918 loss: 1.73492663e-06
Iter: 1919 loss: 1.71182319e-06
Iter: 1920 loss: 1.71059332e-06
Iter: 1921 loss: 1.71002785e-06
Iter: 1922 loss: 1.70940632e-06
Iter: 1923 loss: 1.70775297e-06
Iter: 1924 loss: 1.71981083e-06
Iter: 1925 loss: 1.70761837e-06
Iter: 1926 loss: 1.70633189e-06
Iter: 1927 loss: 1.70504859e-06
Iter: 1928 loss: 1.70477529e-06
Iter: 1929 loss: 1.70275075e-06
Iter: 1930 loss: 1.70835585e-06
Iter: 1931 loss: 1.70214e-06
Iter: 1932 loss: 1.70083342e-06
Iter: 1933 loss: 1.70078943e-06
Iter: 1934 loss: 1.69968393e-06
Iter: 1935 loss: 1.69832208e-06
Iter: 1936 loss: 1.6982118e-06
Iter: 1937 loss: 1.69690691e-06
Iter: 1938 loss: 1.71181705e-06
Iter: 1939 loss: 1.69690554e-06
Iter: 1940 loss: 1.69558905e-06
Iter: 1941 loss: 1.69640316e-06
Iter: 1942 loss: 1.69476357e-06
Iter: 1943 loss: 1.69346356e-06
Iter: 1944 loss: 1.69300836e-06
Iter: 1945 loss: 1.69230623e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi1.2
+ date
Wed Nov  4 13:06:27 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi0.8/300_300_300_1 --function f2 --psi -1 --alpha 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69d4c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69d4cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69cf1488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69d4cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf71fbd158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69cf1158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69bcd0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69bcd8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69ba9400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69c679d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69c67d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69c67840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69c60950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69b19488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69b192f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69b5a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69b87488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69b147b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69b140d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69b14950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf699fabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf699ea7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69a28400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69a20950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69a20b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf71fbd950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf698e72f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf698e7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf698e7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf698a96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf698ab400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69889730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf6986b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69889b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf6986b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf69968268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0115282275
test_loss: 0.013030234
train_loss: 0.009373288
test_loss: 0.010791886
train_loss: 0.008895096
test_loss: 0.010027104
train_loss: 0.0076073157
test_loss: 0.009407986
train_loss: 0.0074353567
test_loss: 0.009567014
train_loss: 0.007184971
test_loss: 0.009027006
train_loss: 0.007272579
test_loss: 0.009594518
train_loss: 0.0069292537
test_loss: 0.008819372
train_loss: 0.006847284
test_loss: 0.008556696
train_loss: 0.0068196263
test_loss: 0.008777112
train_loss: 0.007056892
test_loss: 0.008661836
train_loss: 0.0070009176
test_loss: 0.008929171
train_loss: 0.0062109753
test_loss: 0.008247379
train_loss: 0.0066389097
test_loss: 0.008389423
train_loss: 0.0061720274
test_loss: 0.008221908
train_loss: 0.006359417
test_loss: 0.0084368605
train_loss: 0.0066953283
test_loss: 0.008321263
train_loss: 0.0063812835
test_loss: 0.0083548315
train_loss: 0.006385921
test_loss: 0.008564393
train_loss: 0.006508594
test_loss: 0.008187058
train_loss: 0.0059590777
test_loss: 0.00801983
train_loss: 0.005892163
test_loss: 0.008127666
train_loss: 0.0054754745
test_loss: 0.007904205
train_loss: 0.0058870567
test_loss: 0.008213079
train_loss: 0.005866672
test_loss: 0.007984301
train_loss: 0.0055821375
test_loss: 0.007852293
train_loss: 0.005868923
test_loss: 0.007965507
train_loss: 0.0060766246
test_loss: 0.007916969
train_loss: 0.006256519
test_loss: 0.008029326
train_loss: 0.0061839134
test_loss: 0.007820891
train_loss: 0.005737901
test_loss: 0.007913108
train_loss: 0.0057574874
test_loss: 0.007887003
train_loss: 0.0056330143
test_loss: 0.0077617676
train_loss: 0.005851497
test_loss: 0.007754969
train_loss: 0.005609744
test_loss: 0.00792896
train_loss: 0.0056880135
test_loss: 0.007986905
train_loss: 0.005463251
test_loss: 0.007647625
train_loss: 0.006076928
test_loss: 0.007971406
train_loss: 0.0056769447
test_loss: 0.007713423
train_loss: 0.0054326933
test_loss: 0.007723949
train_loss: 0.0064404793
test_loss: 0.007883442
train_loss: 0.00518419
test_loss: 0.0075801616
train_loss: 0.0059221718
test_loss: 0.007768919
train_loss: 0.0051927418
test_loss: 0.0076203863
train_loss: 0.0052140295
test_loss: 0.0077158157
train_loss: 0.0054728724
test_loss: 0.00775742
train_loss: 0.0053411652
test_loss: 0.0075485823
train_loss: 0.0056214165
test_loss: 0.0075492645
train_loss: 0.0055079833
test_loss: 0.007586718
train_loss: 0.0052019497
test_loss: 0.007531523
train_loss: 0.0050759204
test_loss: 0.0075717536
train_loss: 0.005506343
test_loss: 0.007447681
train_loss: 0.0051537408
test_loss: 0.007668263
train_loss: 0.0054206587
test_loss: 0.0075160894
train_loss: 0.005609124
test_loss: 0.0076241195
train_loss: 0.005763477
test_loss: 0.0075982437
train_loss: 0.0050496636
test_loss: 0.0075490084
train_loss: 0.005581371
test_loss: 0.0077114683
train_loss: 0.005365664
test_loss: 0.0075898627
train_loss: 0.0052734753
test_loss: 0.0077386093
train_loss: 0.0052606845
test_loss: 0.007674856
train_loss: 0.005331166
test_loss: 0.0075573847
train_loss: 0.005614256
test_loss: 0.007531527
train_loss: 0.005415546
test_loss: 0.0076478766
train_loss: 0.005456229
test_loss: 0.0077153994
train_loss: 0.00472839
test_loss: 0.0075854873
train_loss: 0.0049104886
test_loss: 0.0075201495
train_loss: 0.005475928
test_loss: 0.007480073
train_loss: 0.005117774
test_loss: 0.007803031
train_loss: 0.005095311
test_loss: 0.007407358
train_loss: 0.0049246633
test_loss: 0.0072451406
train_loss: 0.005020353
test_loss: 0.0073346286
train_loss: 0.004950079
test_loss: 0.0075136144
train_loss: 0.005036245
test_loss: 0.007416032
train_loss: 0.0054061026
test_loss: 0.0075364197
train_loss: 0.004929715
test_loss: 0.007377658
train_loss: 0.0052649747
test_loss: 0.0073711467
train_loss: 0.0053685685
test_loss: 0.007518673
train_loss: 0.00502133
test_loss: 0.0075138053
train_loss: 0.0047818944
test_loss: 0.0072628534
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.2/300_300_300_1 --optimizer lbfgs --function f2 --psi -1 --alpha 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b4201b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b4201ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b41f46510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b41f46400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b41f05840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b41ecf400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b41f46268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b41eb2400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bc9c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b41eb2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bcb96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bc7d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bc46d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bc46bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bbe8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bbae268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bba8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bb816a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bb172f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bb88158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bae3598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bae3d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3ba9b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bac4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bac7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3bac79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3ba3b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3ba22400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3ba3b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3b993840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3ba01400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3b9a1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3b9a1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3b9a1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3b973ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b3b949268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.04107668e-05
Iter: 2 loss: 3.43622632e-05
Iter: 3 loss: 0.000111155299
Iter: 4 loss: 3.43231441e-05
Iter: 5 loss: 3.14714634e-05
Iter: 6 loss: 2.82015899e-05
Iter: 7 loss: 2.77935542e-05
Iter: 8 loss: 2.51054353e-05
Iter: 9 loss: 5.17931767e-05
Iter: 10 loss: 2.50163903e-05
Iter: 11 loss: 2.28147619e-05
Iter: 12 loss: 2.91869401e-05
Iter: 13 loss: 2.21324244e-05
Iter: 14 loss: 2.09854e-05
Iter: 15 loss: 2.26224893e-05
Iter: 16 loss: 2.0422096e-05
Iter: 17 loss: 1.93091491e-05
Iter: 18 loss: 3.23857821e-05
Iter: 19 loss: 1.92936968e-05
Iter: 20 loss: 1.84995824e-05
Iter: 21 loss: 1.79249928e-05
Iter: 22 loss: 1.76523827e-05
Iter: 23 loss: 1.68953356e-05
Iter: 24 loss: 1.68937731e-05
Iter: 25 loss: 1.6308848e-05
Iter: 26 loss: 1.61576863e-05
Iter: 27 loss: 1.57926206e-05
Iter: 28 loss: 1.51347431e-05
Iter: 29 loss: 1.87936676e-05
Iter: 30 loss: 1.50414207e-05
Iter: 31 loss: 1.44543392e-05
Iter: 32 loss: 1.68152e-05
Iter: 33 loss: 1.43226434e-05
Iter: 34 loss: 1.39227932e-05
Iter: 35 loss: 1.35615683e-05
Iter: 36 loss: 1.34616566e-05
Iter: 37 loss: 1.29922355e-05
Iter: 38 loss: 1.71998508e-05
Iter: 39 loss: 1.29701366e-05
Iter: 40 loss: 1.26518535e-05
Iter: 41 loss: 1.73781154e-05
Iter: 42 loss: 1.26513005e-05
Iter: 43 loss: 1.24679364e-05
Iter: 44 loss: 1.21403555e-05
Iter: 45 loss: 2.02551164e-05
Iter: 46 loss: 1.21402609e-05
Iter: 47 loss: 1.19157476e-05
Iter: 48 loss: 1.1905915e-05
Iter: 49 loss: 1.17147601e-05
Iter: 50 loss: 1.15227685e-05
Iter: 51 loss: 1.14842896e-05
Iter: 52 loss: 1.12744237e-05
Iter: 53 loss: 1.27762451e-05
Iter: 54 loss: 1.1256232e-05
Iter: 55 loss: 1.10485571e-05
Iter: 56 loss: 1.19726355e-05
Iter: 57 loss: 1.10076044e-05
Iter: 58 loss: 1.08802169e-05
Iter: 59 loss: 1.09055072e-05
Iter: 60 loss: 1.07856395e-05
Iter: 61 loss: 1.05966055e-05
Iter: 62 loss: 1.19597335e-05
Iter: 63 loss: 1.05803265e-05
Iter: 64 loss: 1.04810124e-05
Iter: 65 loss: 1.0425967e-05
Iter: 66 loss: 1.03823013e-05
Iter: 67 loss: 1.02358372e-05
Iter: 68 loss: 1.17213331e-05
Iter: 69 loss: 1.02313579e-05
Iter: 70 loss: 1.01306505e-05
Iter: 71 loss: 1.00905854e-05
Iter: 72 loss: 1.00365987e-05
Iter: 73 loss: 9.9064091e-06
Iter: 74 loss: 1.00041725e-05
Iter: 75 loss: 9.82662732e-06
Iter: 76 loss: 9.67027427e-06
Iter: 77 loss: 1.0437343e-05
Iter: 78 loss: 9.64307219e-06
Iter: 79 loss: 9.5357118e-06
Iter: 80 loss: 9.53482049e-06
Iter: 81 loss: 9.47669741e-06
Iter: 82 loss: 9.3836e-06
Iter: 83 loss: 9.38271933e-06
Iter: 84 loss: 9.29297948e-06
Iter: 85 loss: 1.04111532e-05
Iter: 86 loss: 9.29218913e-06
Iter: 87 loss: 9.20914772e-06
Iter: 88 loss: 9.23067455e-06
Iter: 89 loss: 9.14890916e-06
Iter: 90 loss: 9.07138201e-06
Iter: 91 loss: 9.3614226e-06
Iter: 92 loss: 9.0526737e-06
Iter: 93 loss: 8.97034806e-06
Iter: 94 loss: 9.41888084e-06
Iter: 95 loss: 8.95815447e-06
Iter: 96 loss: 8.89745e-06
Iter: 97 loss: 8.8747347e-06
Iter: 98 loss: 8.84129895e-06
Iter: 99 loss: 8.78974151e-06
Iter: 100 loss: 8.7890221e-06
Iter: 101 loss: 8.74493708e-06
Iter: 102 loss: 8.66822847e-06
Iter: 103 loss: 8.66822847e-06
Iter: 104 loss: 8.600613e-06
Iter: 105 loss: 9.47513e-06
Iter: 106 loss: 8.6000764e-06
Iter: 107 loss: 8.53887104e-06
Iter: 108 loss: 8.56580118e-06
Iter: 109 loss: 8.49693515e-06
Iter: 110 loss: 8.43585167e-06
Iter: 111 loss: 8.42193276e-06
Iter: 112 loss: 8.38218693e-06
Iter: 113 loss: 8.31683428e-06
Iter: 114 loss: 9.33146e-06
Iter: 115 loss: 8.31682701e-06
Iter: 116 loss: 8.25925417e-06
Iter: 117 loss: 8.47156753e-06
Iter: 118 loss: 8.24525705e-06
Iter: 119 loss: 8.20785135e-06
Iter: 120 loss: 8.15774183e-06
Iter: 121 loss: 8.15486874e-06
Iter: 122 loss: 8.10217352e-06
Iter: 123 loss: 8.10153142e-06
Iter: 124 loss: 8.06344724e-06
Iter: 125 loss: 8.0377722e-06
Iter: 126 loss: 8.02370141e-06
Iter: 127 loss: 7.98675046e-06
Iter: 128 loss: 8.45949398e-06
Iter: 129 loss: 7.98650217e-06
Iter: 130 loss: 7.95014785e-06
Iter: 131 loss: 7.9377387e-06
Iter: 132 loss: 7.91705406e-06
Iter: 133 loss: 7.86636701e-06
Iter: 134 loss: 7.97789562e-06
Iter: 135 loss: 7.84692293e-06
Iter: 136 loss: 7.80306618e-06
Iter: 137 loss: 8.30169e-06
Iter: 138 loss: 7.80210121e-06
Iter: 139 loss: 7.77086643e-06
Iter: 140 loss: 7.72638668e-06
Iter: 141 loss: 7.72475414e-06
Iter: 142 loss: 7.68785594e-06
Iter: 143 loss: 7.68770133e-06
Iter: 144 loss: 7.65410914e-06
Iter: 145 loss: 7.61409228e-06
Iter: 146 loss: 7.60975308e-06
Iter: 147 loss: 7.56304053e-06
Iter: 148 loss: 7.6224851e-06
Iter: 149 loss: 7.53909353e-06
Iter: 150 loss: 7.51594507e-06
Iter: 151 loss: 7.51025391e-06
Iter: 152 loss: 7.48374e-06
Iter: 153 loss: 7.44602221e-06
Iter: 154 loss: 7.44481122e-06
Iter: 155 loss: 7.40433461e-06
Iter: 156 loss: 7.53998802e-06
Iter: 157 loss: 7.39344659e-06
Iter: 158 loss: 7.36226684e-06
Iter: 159 loss: 7.81578456e-06
Iter: 160 loss: 7.36219363e-06
Iter: 161 loss: 7.3404326e-06
Iter: 162 loss: 7.30627198e-06
Iter: 163 loss: 7.30578176e-06
Iter: 164 loss: 7.27117049e-06
Iter: 165 loss: 7.77728928e-06
Iter: 166 loss: 7.27108e-06
Iter: 167 loss: 7.24211168e-06
Iter: 168 loss: 7.26143298e-06
Iter: 169 loss: 7.22381355e-06
Iter: 170 loss: 7.19773288e-06
Iter: 171 loss: 7.25612608e-06
Iter: 172 loss: 7.18790579e-06
Iter: 173 loss: 7.15449141e-06
Iter: 174 loss: 7.29478916e-06
Iter: 175 loss: 7.14728139e-06
Iter: 176 loss: 7.12266456e-06
Iter: 177 loss: 7.11089797e-06
Iter: 178 loss: 7.09894039e-06
Iter: 179 loss: 7.06950868e-06
Iter: 180 loss: 7.45337911e-06
Iter: 181 loss: 7.069194e-06
Iter: 182 loss: 7.04436752e-06
Iter: 183 loss: 7.01739464e-06
Iter: 184 loss: 7.01329418e-06
Iter: 185 loss: 6.97248424e-06
Iter: 186 loss: 7.05065e-06
Iter: 187 loss: 6.95557628e-06
Iter: 188 loss: 6.92883441e-06
Iter: 189 loss: 6.92885e-06
Iter: 190 loss: 6.90298748e-06
Iter: 191 loss: 6.93903e-06
Iter: 192 loss: 6.89005219e-06
Iter: 193 loss: 6.86828525e-06
Iter: 194 loss: 6.84706538e-06
Iter: 195 loss: 6.84225051e-06
Iter: 196 loss: 6.82018754e-06
Iter: 197 loss: 6.8187378e-06
Iter: 198 loss: 6.80051e-06
Iter: 199 loss: 6.78588322e-06
Iter: 200 loss: 6.78042761e-06
Iter: 201 loss: 6.75419369e-06
Iter: 202 loss: 6.86100793e-06
Iter: 203 loss: 6.74847888e-06
Iter: 204 loss: 6.71576117e-06
Iter: 205 loss: 6.79259074e-06
Iter: 206 loss: 6.70383224e-06
Iter: 207 loss: 6.6813318e-06
Iter: 208 loss: 6.69214114e-06
Iter: 209 loss: 6.66631695e-06
Iter: 210 loss: 6.63746232e-06
Iter: 211 loss: 6.9276216e-06
Iter: 212 loss: 6.63664332e-06
Iter: 213 loss: 6.61919557e-06
Iter: 214 loss: 6.60024307e-06
Iter: 215 loss: 6.59719717e-06
Iter: 216 loss: 6.57689907e-06
Iter: 217 loss: 6.90117577e-06
Iter: 218 loss: 6.57692453e-06
Iter: 219 loss: 6.55798431e-06
Iter: 220 loss: 6.53233474e-06
Iter: 221 loss: 6.5311765e-06
Iter: 222 loss: 6.49999402e-06
Iter: 223 loss: 6.61072272e-06
Iter: 224 loss: 6.49199319e-06
Iter: 225 loss: 6.46822491e-06
Iter: 226 loss: 6.6092357e-06
Iter: 227 loss: 6.4651249e-06
Iter: 228 loss: 6.44286865e-06
Iter: 229 loss: 6.61534887e-06
Iter: 230 loss: 6.44126658e-06
Iter: 231 loss: 6.42689247e-06
Iter: 232 loss: 6.40024e-06
Iter: 233 loss: 7.00483906e-06
Iter: 234 loss: 6.40020698e-06
Iter: 235 loss: 6.38348047e-06
Iter: 236 loss: 6.38307392e-06
Iter: 237 loss: 6.36421555e-06
Iter: 238 loss: 6.34255639e-06
Iter: 239 loss: 6.33980881e-06
Iter: 240 loss: 6.3176949e-06
Iter: 241 loss: 6.51923938e-06
Iter: 242 loss: 6.31663625e-06
Iter: 243 loss: 6.29630813e-06
Iter: 244 loss: 6.36293635e-06
Iter: 245 loss: 6.29058741e-06
Iter: 246 loss: 6.27507416e-06
Iter: 247 loss: 6.2680474e-06
Iter: 248 loss: 6.26038036e-06
Iter: 249 loss: 6.24269251e-06
Iter: 250 loss: 6.2426534e-06
Iter: 251 loss: 6.22911e-06
Iter: 252 loss: 6.21022173e-06
Iter: 253 loss: 6.20941955e-06
Iter: 254 loss: 6.18884042e-06
Iter: 255 loss: 6.35210381e-06
Iter: 256 loss: 6.18752438e-06
Iter: 257 loss: 6.16622719e-06
Iter: 258 loss: 6.1878e-06
Iter: 259 loss: 6.15435783e-06
Iter: 260 loss: 6.13482644e-06
Iter: 261 loss: 6.1407477e-06
Iter: 262 loss: 6.12086842e-06
Iter: 263 loss: 6.09968e-06
Iter: 264 loss: 6.2746567e-06
Iter: 265 loss: 6.09835024e-06
Iter: 266 loss: 6.07829315e-06
Iter: 267 loss: 6.18802369e-06
Iter: 268 loss: 6.0752227e-06
Iter: 269 loss: 6.0626453e-06
Iter: 270 loss: 6.04772504e-06
Iter: 271 loss: 6.04616707e-06
Iter: 272 loss: 6.02732553e-06
Iter: 273 loss: 6.19424827e-06
Iter: 274 loss: 6.02634418e-06
Iter: 275 loss: 6.00714111e-06
Iter: 276 loss: 6.0445409e-06
Iter: 277 loss: 5.99929081e-06
Iter: 278 loss: 5.9840263e-06
Iter: 279 loss: 6.00294516e-06
Iter: 280 loss: 5.97638063e-06
Iter: 281 loss: 5.95981464e-06
Iter: 282 loss: 6.13065549e-06
Iter: 283 loss: 5.9591739e-06
Iter: 284 loss: 5.94727135e-06
Iter: 285 loss: 5.92786546e-06
Iter: 286 loss: 5.92778724e-06
Iter: 287 loss: 5.91055687e-06
Iter: 288 loss: 5.9105223e-06
Iter: 289 loss: 5.89679257e-06
Iter: 290 loss: 5.90122954e-06
Iter: 291 loss: 5.88712101e-06
Iter: 292 loss: 5.87146587e-06
Iter: 293 loss: 5.87790737e-06
Iter: 294 loss: 5.86060651e-06
Iter: 295 loss: 5.84001282e-06
Iter: 296 loss: 6.04408797e-06
Iter: 297 loss: 5.83942528e-06
Iter: 298 loss: 5.82614666e-06
Iter: 299 loss: 5.80247615e-06
Iter: 300 loss: 6.37837638e-06
Iter: 301 loss: 5.80251708e-06
Iter: 302 loss: 5.78508298e-06
Iter: 303 loss: 6.05528612e-06
Iter: 304 loss: 5.78512117e-06
Iter: 305 loss: 5.76875027e-06
Iter: 306 loss: 5.85043e-06
Iter: 307 loss: 5.76605362e-06
Iter: 308 loss: 5.75480544e-06
Iter: 309 loss: 5.73537272e-06
Iter: 310 loss: 5.73533544e-06
Iter: 311 loss: 5.72005683e-06
Iter: 312 loss: 5.71984674e-06
Iter: 313 loss: 5.70571501e-06
Iter: 314 loss: 5.71005967e-06
Iter: 315 loss: 5.69573331e-06
Iter: 316 loss: 5.6799613e-06
Iter: 317 loss: 5.71123383e-06
Iter: 318 loss: 5.67352163e-06
Iter: 319 loss: 5.65674736e-06
Iter: 320 loss: 5.80026608e-06
Iter: 321 loss: 5.65575374e-06
Iter: 322 loss: 5.64343054e-06
Iter: 323 loss: 5.63301182e-06
Iter: 324 loss: 5.62963805e-06
Iter: 325 loss: 5.61727029e-06
Iter: 326 loss: 5.81357654e-06
Iter: 327 loss: 5.61728575e-06
Iter: 328 loss: 5.60549324e-06
Iter: 329 loss: 5.5935725e-06
Iter: 330 loss: 5.59132786e-06
Iter: 331 loss: 5.57384783e-06
Iter: 332 loss: 5.60691342e-06
Iter: 333 loss: 5.56646864e-06
Iter: 334 loss: 5.54627513e-06
Iter: 335 loss: 5.72138651e-06
Iter: 336 loss: 5.54518419e-06
Iter: 337 loss: 5.53314885e-06
Iter: 338 loss: 5.5202654e-06
Iter: 339 loss: 5.51813355e-06
Iter: 340 loss: 5.50204732e-06
Iter: 341 loss: 5.6527947e-06
Iter: 342 loss: 5.50131108e-06
Iter: 343 loss: 5.48551043e-06
Iter: 344 loss: 5.55306633e-06
Iter: 345 loss: 5.48218622e-06
Iter: 346 loss: 5.47139098e-06
Iter: 347 loss: 5.45896728e-06
Iter: 348 loss: 5.45734792e-06
Iter: 349 loss: 5.44335035e-06
Iter: 350 loss: 5.60611807e-06
Iter: 351 loss: 5.44300383e-06
Iter: 352 loss: 5.42840417e-06
Iter: 353 loss: 5.44572686e-06
Iter: 354 loss: 5.42062162e-06
Iter: 355 loss: 5.40928568e-06
Iter: 356 loss: 5.43094029e-06
Iter: 357 loss: 5.40444307e-06
Iter: 358 loss: 5.38925769e-06
Iter: 359 loss: 5.4631073e-06
Iter: 360 loss: 5.38672339e-06
Iter: 361 loss: 5.37562846e-06
Iter: 362 loss: 5.37533924e-06
Iter: 363 loss: 5.36673542e-06
Iter: 364 loss: 5.35497429e-06
Iter: 365 loss: 5.47443e-06
Iter: 366 loss: 5.35467461e-06
Iter: 367 loss: 5.34290712e-06
Iter: 368 loss: 5.33210368e-06
Iter: 369 loss: 5.32927606e-06
Iter: 370 loss: 5.31480691e-06
Iter: 371 loss: 5.36561083e-06
Iter: 372 loss: 5.31106161e-06
Iter: 373 loss: 5.29642693e-06
Iter: 374 loss: 5.40290102e-06
Iter: 375 loss: 5.29526e-06
Iter: 376 loss: 5.28474811e-06
Iter: 377 loss: 5.27651173e-06
Iter: 378 loss: 5.27326756e-06
Iter: 379 loss: 5.25873384e-06
Iter: 380 loss: 5.35691e-06
Iter: 381 loss: 5.25728228e-06
Iter: 382 loss: 5.24377083e-06
Iter: 383 loss: 5.32918511e-06
Iter: 384 loss: 5.24230927e-06
Iter: 385 loss: 5.23323524e-06
Iter: 386 loss: 5.21924449e-06
Iter: 387 loss: 5.21921083e-06
Iter: 388 loss: 5.20598405e-06
Iter: 389 loss: 5.32719469e-06
Iter: 390 loss: 5.20528056e-06
Iter: 391 loss: 5.19109744e-06
Iter: 392 loss: 5.23110975e-06
Iter: 393 loss: 5.18657589e-06
Iter: 394 loss: 5.1772854e-06
Iter: 395 loss: 5.17608623e-06
Iter: 396 loss: 5.16933505e-06
Iter: 397 loss: 5.15311831e-06
Iter: 398 loss: 5.26365238e-06
Iter: 399 loss: 5.15140937e-06
Iter: 400 loss: 5.14194153e-06
Iter: 401 loss: 5.13257055e-06
Iter: 402 loss: 5.13063e-06
Iter: 403 loss: 5.11875078e-06
Iter: 404 loss: 5.11869484e-06
Iter: 405 loss: 5.11006783e-06
Iter: 406 loss: 5.09965594e-06
Iter: 407 loss: 5.09867914e-06
Iter: 408 loss: 5.08640778e-06
Iter: 409 loss: 5.15370721e-06
Iter: 410 loss: 5.08460744e-06
Iter: 411 loss: 5.07275581e-06
Iter: 412 loss: 5.13016403e-06
Iter: 413 loss: 5.07074401e-06
Iter: 414 loss: 5.06137894e-06
Iter: 415 loss: 5.05609432e-06
Iter: 416 loss: 5.0519e-06
Iter: 417 loss: 5.04059881e-06
Iter: 418 loss: 5.1808106e-06
Iter: 419 loss: 5.0404783e-06
Iter: 420 loss: 5.02966031e-06
Iter: 421 loss: 5.03935e-06
Iter: 422 loss: 5.02326975e-06
Iter: 423 loss: 5.01311661e-06
Iter: 424 loss: 5.00392025e-06
Iter: 425 loss: 5.00144961e-06
Iter: 426 loss: 4.99259113e-06
Iter: 427 loss: 4.99174803e-06
Iter: 428 loss: 4.98292229e-06
Iter: 429 loss: 4.9769e-06
Iter: 430 loss: 4.97356e-06
Iter: 431 loss: 4.96268967e-06
Iter: 432 loss: 5.02041e-06
Iter: 433 loss: 4.96107e-06
Iter: 434 loss: 4.95003405e-06
Iter: 435 loss: 4.99126782e-06
Iter: 436 loss: 4.9474138e-06
Iter: 437 loss: 4.93903372e-06
Iter: 438 loss: 4.92979552e-06
Iter: 439 loss: 4.9285e-06
Iter: 440 loss: 4.91945229e-06
Iter: 441 loss: 4.91900619e-06
Iter: 442 loss: 4.91176161e-06
Iter: 443 loss: 4.89926924e-06
Iter: 444 loss: 4.8992556e-06
Iter: 445 loss: 4.88707792e-06
Iter: 446 loss: 4.96843586e-06
Iter: 447 loss: 4.88580736e-06
Iter: 448 loss: 4.87404577e-06
Iter: 449 loss: 4.9262e-06
Iter: 450 loss: 4.87173747e-06
Iter: 451 loss: 4.86225781e-06
Iter: 452 loss: 4.86059889e-06
Iter: 453 loss: 4.8542247e-06
Iter: 454 loss: 4.84506972e-06
Iter: 455 loss: 4.84508e-06
Iter: 456 loss: 4.83634813e-06
Iter: 457 loss: 4.82630776e-06
Iter: 458 loss: 4.82506857e-06
Iter: 459 loss: 4.81217376e-06
Iter: 460 loss: 4.82566702e-06
Iter: 461 loss: 4.80504877e-06
Iter: 462 loss: 4.7963731e-06
Iter: 463 loss: 4.79583832e-06
Iter: 464 loss: 4.78787479e-06
Iter: 465 loss: 4.77782e-06
Iter: 466 loss: 4.77705362e-06
Iter: 467 loss: 4.76680816e-06
Iter: 468 loss: 4.87736952e-06
Iter: 469 loss: 4.76657488e-06
Iter: 470 loss: 4.75753768e-06
Iter: 471 loss: 4.77456615e-06
Iter: 472 loss: 4.75364413e-06
Iter: 473 loss: 4.74525586e-06
Iter: 474 loss: 4.74218405e-06
Iter: 475 loss: 4.73752198e-06
Iter: 476 loss: 4.72694046e-06
Iter: 477 loss: 4.86679892e-06
Iter: 478 loss: 4.72689271e-06
Iter: 479 loss: 4.71869362e-06
Iter: 480 loss: 4.71416615e-06
Iter: 481 loss: 4.71028125e-06
Iter: 482 loss: 4.70027862e-06
Iter: 483 loss: 4.70234409e-06
Iter: 484 loss: 4.69283623e-06
Iter: 485 loss: 4.67864902e-06
Iter: 486 loss: 4.85662076e-06
Iter: 487 loss: 4.67851896e-06
Iter: 488 loss: 4.67082555e-06
Iter: 489 loss: 4.66339043e-06
Iter: 490 loss: 4.66166148e-06
Iter: 491 loss: 4.65368703e-06
Iter: 492 loss: 4.65346466e-06
Iter: 493 loss: 4.64624418e-06
Iter: 494 loss: 4.63587958e-06
Iter: 495 loss: 4.63549668e-06
Iter: 496 loss: 4.62425396e-06
Iter: 497 loss: 4.63199831e-06
Iter: 498 loss: 4.61731452e-06
Iter: 499 loss: 4.60950287e-06
Iter: 500 loss: 4.60849969e-06
Iter: 501 loss: 4.60178671e-06
Iter: 502 loss: 4.59522198e-06
Iter: 503 loss: 4.59381317e-06
Iter: 504 loss: 4.58423528e-06
Iter: 505 loss: 4.64209552e-06
Iter: 506 loss: 4.58310342e-06
Iter: 507 loss: 4.57304304e-06
Iter: 508 loss: 4.59450757e-06
Iter: 509 loss: 4.56919088e-06
Iter: 510 loss: 4.56055432e-06
Iter: 511 loss: 4.56367479e-06
Iter: 512 loss: 4.55453119e-06
Iter: 513 loss: 4.54299425e-06
Iter: 514 loss: 4.63979268e-06
Iter: 515 loss: 4.54242399e-06
Iter: 516 loss: 4.53438497e-06
Iter: 517 loss: 4.52751e-06
Iter: 518 loss: 4.52530321e-06
Iter: 519 loss: 4.51420192e-06
Iter: 520 loss: 4.55898635e-06
Iter: 521 loss: 4.51172673e-06
Iter: 522 loss: 4.50054085e-06
Iter: 523 loss: 4.57665374e-06
Iter: 524 loss: 4.49936397e-06
Iter: 525 loss: 4.49112031e-06
Iter: 526 loss: 4.49278559e-06
Iter: 527 loss: 4.48510491e-06
Iter: 528 loss: 4.47876664e-06
Iter: 529 loss: 4.47866842e-06
Iter: 530 loss: 4.47318416e-06
Iter: 531 loss: 4.45878595e-06
Iter: 532 loss: 4.57261967e-06
Iter: 533 loss: 4.45607566e-06
Iter: 534 loss: 4.44595844e-06
Iter: 535 loss: 4.55957888e-06
Iter: 536 loss: 4.44568195e-06
Iter: 537 loss: 4.43587487e-06
Iter: 538 loss: 4.49603613e-06
Iter: 539 loss: 4.43472254e-06
Iter: 540 loss: 4.42651617e-06
Iter: 541 loss: 4.4215185e-06
Iter: 542 loss: 4.41817247e-06
Iter: 543 loss: 4.41061093e-06
Iter: 544 loss: 4.41060638e-06
Iter: 545 loss: 4.40370059e-06
Iter: 546 loss: 4.39750966e-06
Iter: 547 loss: 4.39582e-06
Iter: 548 loss: 4.3852192e-06
Iter: 549 loss: 4.41303519e-06
Iter: 550 loss: 4.38182042e-06
Iter: 551 loss: 4.37119e-06
Iter: 552 loss: 4.46574268e-06
Iter: 553 loss: 4.37062499e-06
Iter: 554 loss: 4.36456912e-06
Iter: 555 loss: 4.35294851e-06
Iter: 556 loss: 4.60110186e-06
Iter: 557 loss: 4.35296533e-06
Iter: 558 loss: 4.34320282e-06
Iter: 559 loss: 4.34321692e-06
Iter: 560 loss: 4.33490686e-06
Iter: 561 loss: 4.35242191e-06
Iter: 562 loss: 4.33155856e-06
Iter: 563 loss: 4.32372235e-06
Iter: 564 loss: 4.33307741e-06
Iter: 565 loss: 4.31958597e-06
Iter: 566 loss: 4.31055787e-06
Iter: 567 loss: 4.39270934e-06
Iter: 568 loss: 4.31014905e-06
Iter: 569 loss: 4.30380533e-06
Iter: 570 loss: 4.29759302e-06
Iter: 571 loss: 4.29620741e-06
Iter: 572 loss: 4.28757e-06
Iter: 573 loss: 4.29773809e-06
Iter: 574 loss: 4.28290059e-06
Iter: 575 loss: 4.27431269e-06
Iter: 576 loss: 4.27423402e-06
Iter: 577 loss: 4.26805582e-06
Iter: 578 loss: 4.26072575e-06
Iter: 579 loss: 4.25994813e-06
Iter: 580 loss: 4.25380858e-06
Iter: 581 loss: 4.25383359e-06
Iter: 582 loss: 4.24734844e-06
Iter: 583 loss: 4.23670872e-06
Iter: 584 loss: 4.23666e-06
Iter: 585 loss: 4.22819267e-06
Iter: 586 loss: 4.31048693e-06
Iter: 587 loss: 4.22780431e-06
Iter: 588 loss: 4.21977757e-06
Iter: 589 loss: 4.24361315e-06
Iter: 590 loss: 4.21729555e-06
Iter: 591 loss: 4.21036e-06
Iter: 592 loss: 4.20402102e-06
Iter: 593 loss: 4.2023712e-06
Iter: 594 loss: 4.19363732e-06
Iter: 595 loss: 4.28218755e-06
Iter: 596 loss: 4.1933763e-06
Iter: 597 loss: 4.18503896e-06
Iter: 598 loss: 4.20542528e-06
Iter: 599 loss: 4.18207674e-06
Iter: 600 loss: 4.17498131e-06
Iter: 601 loss: 4.18998025e-06
Iter: 602 loss: 4.17226875e-06
Iter: 603 loss: 4.16344756e-06
Iter: 604 loss: 4.20029028e-06
Iter: 605 loss: 4.16152852e-06
Iter: 606 loss: 4.15505792e-06
Iter: 607 loss: 4.15003524e-06
Iter: 608 loss: 4.14792703e-06
Iter: 609 loss: 4.13912858e-06
Iter: 610 loss: 4.17191768e-06
Iter: 611 loss: 4.13704265e-06
Iter: 612 loss: 4.12834515e-06
Iter: 613 loss: 4.21819868e-06
Iter: 614 loss: 4.12821691e-06
Iter: 615 loss: 4.12201189e-06
Iter: 616 loss: 4.11649853e-06
Iter: 617 loss: 4.11479186e-06
Iter: 618 loss: 4.10869097e-06
Iter: 619 loss: 4.20535571e-06
Iter: 620 loss: 4.10867733e-06
Iter: 621 loss: 4.10234225e-06
Iter: 622 loss: 4.09353743e-06
Iter: 623 loss: 4.09318818e-06
Iter: 624 loss: 4.08528103e-06
Iter: 625 loss: 4.1396388e-06
Iter: 626 loss: 4.08446522e-06
Iter: 627 loss: 4.07688731e-06
Iter: 628 loss: 4.1117205e-06
Iter: 629 loss: 4.07549578e-06
Iter: 630 loss: 4.06914069e-06
Iter: 631 loss: 4.06103663e-06
Iter: 632 loss: 4.06051095e-06
Iter: 633 loss: 4.05153787e-06
Iter: 634 loss: 4.12618874e-06
Iter: 635 loss: 4.05093806e-06
Iter: 636 loss: 4.04282264e-06
Iter: 637 loss: 4.08693404e-06
Iter: 638 loss: 4.04146522e-06
Iter: 639 loss: 4.035031e-06
Iter: 640 loss: 4.03630929e-06
Iter: 641 loss: 4.03028025e-06
Iter: 642 loss: 4.02209935e-06
Iter: 643 loss: 4.10437497e-06
Iter: 644 loss: 4.02192563e-06
Iter: 645 loss: 4.01642865e-06
Iter: 646 loss: 4.00630324e-06
Iter: 647 loss: 4.2329666e-06
Iter: 648 loss: 4.00616636e-06
Iter: 649 loss: 3.9967249e-06
Iter: 650 loss: 4.05513219e-06
Iter: 651 loss: 3.99560804e-06
Iter: 652 loss: 3.98602515e-06
Iter: 653 loss: 4.06101e-06
Iter: 654 loss: 3.9853e-06
Iter: 655 loss: 3.97961185e-06
Iter: 656 loss: 3.97544318e-06
Iter: 657 loss: 3.97351687e-06
Iter: 658 loss: 3.96633277e-06
Iter: 659 loss: 4.06419394e-06
Iter: 660 loss: 3.96630185e-06
Iter: 661 loss: 3.96074029e-06
Iter: 662 loss: 3.96211499e-06
Iter: 663 loss: 3.95671668e-06
Iter: 664 loss: 3.95027655e-06
Iter: 665 loss: 3.95290226e-06
Iter: 666 loss: 3.94581502e-06
Iter: 667 loss: 3.93820619e-06
Iter: 668 loss: 4.04241291e-06
Iter: 669 loss: 3.93823348e-06
Iter: 670 loss: 3.93311075e-06
Iter: 671 loss: 3.92250786e-06
Iter: 672 loss: 4.0975965e-06
Iter: 673 loss: 3.92211541e-06
Iter: 674 loss: 3.91263711e-06
Iter: 675 loss: 3.99743931e-06
Iter: 676 loss: 3.91231151e-06
Iter: 677 loss: 3.90323385e-06
Iter: 678 loss: 3.95610277e-06
Iter: 679 loss: 3.90210334e-06
Iter: 680 loss: 3.89684283e-06
Iter: 681 loss: 3.90804234e-06
Iter: 682 loss: 3.8947278e-06
Iter: 683 loss: 3.88831268e-06
Iter: 684 loss: 3.91107915e-06
Iter: 685 loss: 3.8868111e-06
Iter: 686 loss: 3.88098761e-06
Iter: 687 loss: 3.87221917e-06
Iter: 688 loss: 3.87208456e-06
Iter: 689 loss: 3.86286911e-06
Iter: 690 loss: 3.93452501e-06
Iter: 691 loss: 3.86209831e-06
Iter: 692 loss: 3.85463863e-06
Iter: 693 loss: 3.93158e-06
Iter: 694 loss: 3.85438e-06
Iter: 695 loss: 3.85004387e-06
Iter: 696 loss: 3.84304349e-06
Iter: 697 loss: 3.84295208e-06
Iter: 698 loss: 3.83660245e-06
Iter: 699 loss: 3.83651104e-06
Iter: 700 loss: 3.83148836e-06
Iter: 701 loss: 3.82738517e-06
Iter: 702 loss: 3.82586086e-06
Iter: 703 loss: 3.81866221e-06
Iter: 704 loss: 3.8350463e-06
Iter: 705 loss: 3.81590507e-06
Iter: 706 loss: 3.80811957e-06
Iter: 707 loss: 3.87745877e-06
Iter: 708 loss: 3.80783786e-06
Iter: 709 loss: 3.80239157e-06
Iter: 710 loss: 3.79553717e-06
Iter: 711 loss: 3.79502148e-06
Iter: 712 loss: 3.78640812e-06
Iter: 713 loss: 3.82175676e-06
Iter: 714 loss: 3.78454388e-06
Iter: 715 loss: 3.7777354e-06
Iter: 716 loss: 3.87020373e-06
Iter: 717 loss: 3.77761353e-06
Iter: 718 loss: 3.77282845e-06
Iter: 719 loss: 3.77037e-06
Iter: 720 loss: 3.76812955e-06
Iter: 721 loss: 3.76096227e-06
Iter: 722 loss: 3.81914697e-06
Iter: 723 loss: 3.76052731e-06
Iter: 724 loss: 3.75451782e-06
Iter: 725 loss: 3.74905449e-06
Iter: 726 loss: 3.74765068e-06
Iter: 727 loss: 3.74041019e-06
Iter: 728 loss: 3.76463527e-06
Iter: 729 loss: 3.73845342e-06
Iter: 730 loss: 3.73288253e-06
Iter: 731 loss: 3.73287594e-06
Iter: 732 loss: 3.72828026e-06
Iter: 733 loss: 3.71915485e-06
Iter: 734 loss: 3.89468778e-06
Iter: 735 loss: 3.71905298e-06
Iter: 736 loss: 3.7142288e-06
Iter: 737 loss: 3.71366855e-06
Iter: 738 loss: 3.70866928e-06
Iter: 739 loss: 3.70272096e-06
Iter: 740 loss: 3.70200928e-06
Iter: 741 loss: 3.69460713e-06
Iter: 742 loss: 3.7134896e-06
Iter: 743 loss: 3.69186546e-06
Iter: 744 loss: 3.68559e-06
Iter: 745 loss: 3.77835772e-06
Iter: 746 loss: 3.68557767e-06
Iter: 747 loss: 3.68131623e-06
Iter: 748 loss: 3.67531356e-06
Iter: 749 loss: 3.67510802e-06
Iter: 750 loss: 3.66712766e-06
Iter: 751 loss: 3.68549877e-06
Iter: 752 loss: 3.66405948e-06
Iter: 753 loss: 3.6561687e-06
Iter: 754 loss: 3.74096248e-06
Iter: 755 loss: 3.65596679e-06
Iter: 756 loss: 3.64969765e-06
Iter: 757 loss: 3.6565998e-06
Iter: 758 loss: 3.64638981e-06
Iter: 759 loss: 3.64139191e-06
Iter: 760 loss: 3.68456517e-06
Iter: 761 loss: 3.64112134e-06
Iter: 762 loss: 3.63573872e-06
Iter: 763 loss: 3.62616356e-06
Iter: 764 loss: 3.85265866e-06
Iter: 765 loss: 3.6261697e-06
Iter: 766 loss: 3.61847015e-06
Iter: 767 loss: 3.65364372e-06
Iter: 768 loss: 3.61706452e-06
Iter: 769 loss: 3.61083357e-06
Iter: 770 loss: 3.6952772e-06
Iter: 771 loss: 3.61088814e-06
Iter: 772 loss: 3.6055394e-06
Iter: 773 loss: 3.60251079e-06
Iter: 774 loss: 3.60015838e-06
Iter: 775 loss: 3.59412161e-06
Iter: 776 loss: 3.61982165e-06
Iter: 777 loss: 3.59279875e-06
Iter: 778 loss: 3.58564716e-06
Iter: 779 loss: 3.6078975e-06
Iter: 780 loss: 3.58356783e-06
Iter: 781 loss: 3.57841213e-06
Iter: 782 loss: 3.57469344e-06
Iter: 783 loss: 3.57290946e-06
Iter: 784 loss: 3.56679811e-06
Iter: 785 loss: 3.56681312e-06
Iter: 786 loss: 3.56174223e-06
Iter: 787 loss: 3.55776683e-06
Iter: 788 loss: 3.55618522e-06
Iter: 789 loss: 3.55010297e-06
Iter: 790 loss: 3.559373e-06
Iter: 791 loss: 3.54720964e-06
Iter: 792 loss: 3.54031386e-06
Iter: 793 loss: 3.60412923e-06
Iter: 794 loss: 3.5400858e-06
Iter: 795 loss: 3.53382393e-06
Iter: 796 loss: 3.53868109e-06
Iter: 797 loss: 3.53005908e-06
Iter: 798 loss: 3.52486904e-06
Iter: 799 loss: 3.57090221e-06
Iter: 800 loss: 3.52465531e-06
Iter: 801 loss: 3.51948302e-06
Iter: 802 loss: 3.51723429e-06
Iter: 803 loss: 3.51463791e-06
Iter: 804 loss: 3.50773848e-06
Iter: 805 loss: 3.50900564e-06
Iter: 806 loss: 3.50255641e-06
Iter: 807 loss: 3.49616312e-06
Iter: 808 loss: 3.59440855e-06
Iter: 809 loss: 3.49622906e-06
Iter: 810 loss: 3.49029256e-06
Iter: 811 loss: 3.50226014e-06
Iter: 812 loss: 3.48777553e-06
Iter: 813 loss: 3.48336562e-06
Iter: 814 loss: 3.4852726e-06
Iter: 815 loss: 3.48032245e-06
Iter: 816 loss: 3.47472201e-06
Iter: 817 loss: 3.53158839e-06
Iter: 818 loss: 3.47450441e-06
Iter: 819 loss: 3.47033642e-06
Iter: 820 loss: 3.46367e-06
Iter: 821 loss: 3.46365778e-06
Iter: 822 loss: 3.45843455e-06
Iter: 823 loss: 3.45842773e-06
Iter: 824 loss: 3.45318017e-06
Iter: 825 loss: 3.45196941e-06
Iter: 826 loss: 3.44858381e-06
Iter: 827 loss: 3.44253749e-06
Iter: 828 loss: 3.44214595e-06
Iter: 829 loss: 3.43762804e-06
Iter: 830 loss: 3.4325335e-06
Iter: 831 loss: 3.43242527e-06
Iter: 832 loss: 3.4276386e-06
Iter: 833 loss: 3.42607768e-06
Iter: 834 loss: 3.42330486e-06
Iter: 835 loss: 3.41743e-06
Iter: 836 loss: 3.45411559e-06
Iter: 837 loss: 3.41676218e-06
Iter: 838 loss: 3.4112104e-06
Iter: 839 loss: 3.42599287e-06
Iter: 840 loss: 3.40929728e-06
Iter: 841 loss: 3.4040836e-06
Iter: 842 loss: 3.39670851e-06
Iter: 843 loss: 3.39644066e-06
Iter: 844 loss: 3.38855671e-06
Iter: 845 loss: 3.42630574e-06
Iter: 846 loss: 3.3871861e-06
Iter: 847 loss: 3.38310974e-06
Iter: 848 loss: 3.38284576e-06
Iter: 849 loss: 3.37902748e-06
Iter: 850 loss: 3.37257052e-06
Iter: 851 loss: 3.3725687e-06
Iter: 852 loss: 3.36648145e-06
Iter: 853 loss: 3.43332704e-06
Iter: 854 loss: 3.3663066e-06
Iter: 855 loss: 3.36130211e-06
Iter: 856 loss: 3.37949587e-06
Iter: 857 loss: 3.36009725e-06
Iter: 858 loss: 3.35605819e-06
Iter: 859 loss: 3.35158074e-06
Iter: 860 loss: 3.35097e-06
Iter: 861 loss: 3.34552e-06
Iter: 862 loss: 3.34550396e-06
Iter: 863 loss: 3.34150741e-06
Iter: 864 loss: 3.33469166e-06
Iter: 865 loss: 3.33473463e-06
Iter: 866 loss: 3.32768309e-06
Iter: 867 loss: 3.35069058e-06
Iter: 868 loss: 3.32576064e-06
Iter: 869 loss: 3.31943897e-06
Iter: 870 loss: 3.40429301e-06
Iter: 871 loss: 3.3194e-06
Iter: 872 loss: 3.31560545e-06
Iter: 873 loss: 3.31351976e-06
Iter: 874 loss: 3.31187084e-06
Iter: 875 loss: 3.30764055e-06
Iter: 876 loss: 3.30762532e-06
Iter: 877 loss: 3.30401758e-06
Iter: 878 loss: 3.30044463e-06
Iter: 879 loss: 3.2996777e-06
Iter: 880 loss: 3.29414343e-06
Iter: 881 loss: 3.29553131e-06
Iter: 882 loss: 3.2901487e-06
Iter: 883 loss: 3.28373585e-06
Iter: 884 loss: 3.32707032e-06
Iter: 885 loss: 3.28313945e-06
Iter: 886 loss: 3.27639395e-06
Iter: 887 loss: 3.30954981e-06
Iter: 888 loss: 3.27529028e-06
Iter: 889 loss: 3.27065504e-06
Iter: 890 loss: 3.27230259e-06
Iter: 891 loss: 3.2674543e-06
Iter: 892 loss: 3.26263103e-06
Iter: 893 loss: 3.31197907e-06
Iter: 894 loss: 3.26265217e-06
Iter: 895 loss: 3.25806172e-06
Iter: 896 loss: 3.25444967e-06
Iter: 897 loss: 3.25301244e-06
Iter: 898 loss: 3.24910252e-06
Iter: 899 loss: 3.29191334e-06
Iter: 900 loss: 3.24903021e-06
Iter: 901 loss: 3.24486882e-06
Iter: 902 loss: 3.24395319e-06
Iter: 903 loss: 3.24130019e-06
Iter: 904 loss: 3.2363132e-06
Iter: 905 loss: 3.23778022e-06
Iter: 906 loss: 3.23269273e-06
Iter: 907 loss: 3.2283358e-06
Iter: 908 loss: 3.22834103e-06
Iter: 909 loss: 3.2244493e-06
Iter: 910 loss: 3.21919106e-06
Iter: 911 loss: 3.21889843e-06
Iter: 912 loss: 3.21307743e-06
Iter: 913 loss: 3.26062059e-06
Iter: 914 loss: 3.21269863e-06
Iter: 915 loss: 3.20781419e-06
Iter: 916 loss: 3.24221764e-06
Iter: 917 loss: 3.20734171e-06
Iter: 918 loss: 3.20401591e-06
Iter: 919 loss: 3.19807123e-06
Iter: 920 loss: 3.19808441e-06
Iter: 921 loss: 3.19154492e-06
Iter: 922 loss: 3.21122184e-06
Iter: 923 loss: 3.18957382e-06
Iter: 924 loss: 3.1851705e-06
Iter: 925 loss: 3.1851323e-06
Iter: 926 loss: 3.18143066e-06
Iter: 927 loss: 3.1779723e-06
Iter: 928 loss: 3.17717149e-06
Iter: 929 loss: 3.17218974e-06
Iter: 930 loss: 3.20016807e-06
Iter: 931 loss: 3.17150079e-06
Iter: 932 loss: 3.16722662e-06
Iter: 933 loss: 3.19605988e-06
Iter: 934 loss: 3.16678825e-06
Iter: 935 loss: 3.16332307e-06
Iter: 936 loss: 3.15735315e-06
Iter: 937 loss: 3.15729926e-06
Iter: 938 loss: 3.15290231e-06
Iter: 939 loss: 3.15250963e-06
Iter: 940 loss: 3.14926251e-06
Iter: 941 loss: 3.14355248e-06
Iter: 942 loss: 3.14357567e-06
Iter: 943 loss: 3.13752571e-06
Iter: 944 loss: 3.1694376e-06
Iter: 945 loss: 3.13662986e-06
Iter: 946 loss: 3.13107739e-06
Iter: 947 loss: 3.16726096e-06
Iter: 948 loss: 3.13052374e-06
Iter: 949 loss: 3.12688644e-06
Iter: 950 loss: 3.12158227e-06
Iter: 951 loss: 3.12147631e-06
Iter: 952 loss: 3.11544363e-06
Iter: 953 loss: 3.16078376e-06
Iter: 954 loss: 3.11498343e-06
Iter: 955 loss: 3.1085392e-06
Iter: 956 loss: 3.13885448e-06
Iter: 957 loss: 3.10734731e-06
Iter: 958 loss: 3.10318137e-06
Iter: 959 loss: 3.09995e-06
Iter: 960 loss: 3.09861798e-06
Iter: 961 loss: 3.09296161e-06
Iter: 962 loss: 3.12731254e-06
Iter: 963 loss: 3.09231427e-06
Iter: 964 loss: 3.08705944e-06
Iter: 965 loss: 3.11986105e-06
Iter: 966 loss: 3.0864278e-06
Iter: 967 loss: 3.0829342e-06
Iter: 968 loss: 3.07869641e-06
Iter: 969 loss: 3.07829805e-06
Iter: 970 loss: 3.07328401e-06
Iter: 971 loss: 3.14795716e-06
Iter: 972 loss: 3.07329628e-06
Iter: 973 loss: 3.06886795e-06
Iter: 974 loss: 3.06858738e-06
Iter: 975 loss: 3.06526522e-06
Iter: 976 loss: 3.06038351e-06
Iter: 977 loss: 3.08082281e-06
Iter: 978 loss: 3.05937078e-06
Iter: 979 loss: 3.05480035e-06
Iter: 980 loss: 3.08507424e-06
Iter: 981 loss: 3.05425442e-06
Iter: 982 loss: 3.05144727e-06
Iter: 983 loss: 3.04539799e-06
Iter: 984 loss: 3.1480497e-06
Iter: 985 loss: 3.04529703e-06
Iter: 986 loss: 3.03969409e-06
Iter: 987 loss: 3.03965e-06
Iter: 988 loss: 3.03575e-06
Iter: 989 loss: 3.03386082e-06
Iter: 990 loss: 3.03201386e-06
Iter: 991 loss: 3.02720127e-06
Iter: 992 loss: 3.02677381e-06
Iter: 993 loss: 3.02303556e-06
Iter: 994 loss: 3.01721207e-06
Iter: 995 loss: 3.08371727e-06
Iter: 996 loss: 3.01707769e-06
Iter: 997 loss: 3.01311024e-06
Iter: 998 loss: 3.06108541e-06
Iter: 999 loss: 3.01303407e-06
Iter: 1000 loss: 3.0103779e-06
Iter: 1001 loss: 3.00409465e-06
Iter: 1002 loss: 3.08281437e-06
Iter: 1003 loss: 3.00363399e-06
Iter: 1004 loss: 2.999667e-06
Iter: 1005 loss: 2.99930844e-06
Iter: 1006 loss: 2.99552266e-06
Iter: 1007 loss: 2.99389876e-06
Iter: 1008 loss: 2.99201656e-06
Iter: 1009 loss: 2.98764166e-06
Iter: 1010 loss: 2.99793828e-06
Iter: 1011 loss: 2.98596933e-06
Iter: 1012 loss: 2.98029772e-06
Iter: 1013 loss: 3.01136242e-06
Iter: 1014 loss: 2.97950191e-06
Iter: 1015 loss: 2.97557608e-06
Iter: 1016 loss: 2.97752536e-06
Iter: 1017 loss: 2.97297311e-06
Iter: 1018 loss: 2.96914982e-06
Iter: 1019 loss: 2.9961684e-06
Iter: 1020 loss: 2.96876487e-06
Iter: 1021 loss: 2.96451549e-06
Iter: 1022 loss: 2.96696e-06
Iter: 1023 loss: 2.96174039e-06
Iter: 1024 loss: 2.95778636e-06
Iter: 1025 loss: 2.96993517e-06
Iter: 1026 loss: 2.95668315e-06
Iter: 1027 loss: 2.9523153e-06
Iter: 1028 loss: 2.96995745e-06
Iter: 1029 loss: 2.95133987e-06
Iter: 1030 loss: 2.94822939e-06
Iter: 1031 loss: 2.94167648e-06
Iter: 1032 loss: 3.05774802e-06
Iter: 1033 loss: 2.94155848e-06
Iter: 1034 loss: 2.9359021e-06
Iter: 1035 loss: 3.02279204e-06
Iter: 1036 loss: 2.93591665e-06
Iter: 1037 loss: 2.93084554e-06
Iter: 1038 loss: 2.96054372e-06
Iter: 1039 loss: 2.9301832e-06
Iter: 1040 loss: 2.92675099e-06
Iter: 1041 loss: 2.92333834e-06
Iter: 1042 loss: 2.92263348e-06
Iter: 1043 loss: 2.91839751e-06
Iter: 1044 loss: 2.97434372e-06
Iter: 1045 loss: 2.91838842e-06
Iter: 1046 loss: 2.91435117e-06
Iter: 1047 loss: 2.91465244e-06
Iter: 1048 loss: 2.91127753e-06
Iter: 1049 loss: 2.90695698e-06
Iter: 1050 loss: 2.91299193e-06
Iter: 1051 loss: 2.90496882e-06
Iter: 1052 loss: 2.89984905e-06
Iter: 1053 loss: 2.94683446e-06
Iter: 1054 loss: 2.89968966e-06
Iter: 1055 loss: 2.89627724e-06
Iter: 1056 loss: 2.89358627e-06
Iter: 1057 loss: 2.89253512e-06
Iter: 1058 loss: 2.88782621e-06
Iter: 1059 loss: 2.92712843e-06
Iter: 1060 loss: 2.88751221e-06
Iter: 1061 loss: 2.88293654e-06
Iter: 1062 loss: 2.89006357e-06
Iter: 1063 loss: 2.88069441e-06
Iter: 1064 loss: 2.87702687e-06
Iter: 1065 loss: 2.88317483e-06
Iter: 1066 loss: 2.87543162e-06
Iter: 1067 loss: 2.87085868e-06
Iter: 1068 loss: 2.89165814e-06
Iter: 1069 loss: 2.87005923e-06
Iter: 1070 loss: 2.86630871e-06
Iter: 1071 loss: 2.86186787e-06
Iter: 1072 loss: 2.86143973e-06
Iter: 1073 loss: 2.85652573e-06
Iter: 1074 loss: 2.89899936e-06
Iter: 1075 loss: 2.85618148e-06
Iter: 1076 loss: 2.85114356e-06
Iter: 1077 loss: 2.87543708e-06
Iter: 1078 loss: 2.85023e-06
Iter: 1079 loss: 2.84659836e-06
Iter: 1080 loss: 2.84335101e-06
Iter: 1081 loss: 2.84237694e-06
Iter: 1082 loss: 2.83797772e-06
Iter: 1083 loss: 2.89825584e-06
Iter: 1084 loss: 2.83800182e-06
Iter: 1085 loss: 2.83384338e-06
Iter: 1086 loss: 2.83343888e-06
Iter: 1087 loss: 2.83046893e-06
Iter: 1088 loss: 2.82608812e-06
Iter: 1089 loss: 2.83402551e-06
Iter: 1090 loss: 2.82431438e-06
Iter: 1091 loss: 2.81971552e-06
Iter: 1092 loss: 2.87111811e-06
Iter: 1093 loss: 2.81961502e-06
Iter: 1094 loss: 2.81686289e-06
Iter: 1095 loss: 2.81373059e-06
Iter: 1096 loss: 2.81333814e-06
Iter: 1097 loss: 2.81035227e-06
Iter: 1098 loss: 2.81034181e-06
Iter: 1099 loss: 2.8073996e-06
Iter: 1100 loss: 2.80316476e-06
Iter: 1101 loss: 2.80303857e-06
Iter: 1102 loss: 2.79886126e-06
Iter: 1103 loss: 2.83525515e-06
Iter: 1104 loss: 2.79864594e-06
Iter: 1105 loss: 2.79494247e-06
Iter: 1106 loss: 2.80305358e-06
Iter: 1107 loss: 2.79361393e-06
Iter: 1108 loss: 2.79011e-06
Iter: 1109 loss: 2.78972266e-06
Iter: 1110 loss: 2.78720699e-06
Iter: 1111 loss: 2.78256357e-06
Iter: 1112 loss: 2.78895413e-06
Iter: 1113 loss: 2.78026755e-06
Iter: 1114 loss: 2.77777963e-06
Iter: 1115 loss: 2.77714889e-06
Iter: 1116 loss: 2.77481286e-06
Iter: 1117 loss: 2.7703486e-06
Iter: 1118 loss: 2.87020748e-06
Iter: 1119 loss: 2.77039544e-06
Iter: 1120 loss: 2.76596393e-06
Iter: 1121 loss: 2.7885992e-06
Iter: 1122 loss: 2.76528272e-06
Iter: 1123 loss: 2.76061292e-06
Iter: 1124 loss: 2.78472316e-06
Iter: 1125 loss: 2.75987e-06
Iter: 1126 loss: 2.75684442e-06
Iter: 1127 loss: 2.75368711e-06
Iter: 1128 loss: 2.75309026e-06
Iter: 1129 loss: 2.74968897e-06
Iter: 1130 loss: 2.74964282e-06
Iter: 1131 loss: 2.7466981e-06
Iter: 1132 loss: 2.74420404e-06
Iter: 1133 loss: 2.74339845e-06
Iter: 1134 loss: 2.73911792e-06
Iter: 1135 loss: 2.75167667e-06
Iter: 1136 loss: 2.73780415e-06
Iter: 1137 loss: 2.73350838e-06
Iter: 1138 loss: 2.77552226e-06
Iter: 1139 loss: 2.73332898e-06
Iter: 1140 loss: 2.73072465e-06
Iter: 1141 loss: 2.72783245e-06
Iter: 1142 loss: 2.72746547e-06
Iter: 1143 loss: 2.72316379e-06
Iter: 1144 loss: 2.76737114e-06
Iter: 1145 loss: 2.7230667e-06
Iter: 1146 loss: 2.71983117e-06
Iter: 1147 loss: 2.71647787e-06
Iter: 1148 loss: 2.71582621e-06
Iter: 1149 loss: 2.71079216e-06
Iter: 1150 loss: 2.7193214e-06
Iter: 1151 loss: 2.70844907e-06
Iter: 1152 loss: 2.70353803e-06
Iter: 1153 loss: 2.72707439e-06
Iter: 1154 loss: 2.70265969e-06
Iter: 1155 loss: 2.69948896e-06
Iter: 1156 loss: 2.69945167e-06
Iter: 1157 loss: 2.69655925e-06
Iter: 1158 loss: 2.69348811e-06
Iter: 1159 loss: 2.69297925e-06
Iter: 1160 loss: 2.68913072e-06
Iter: 1161 loss: 2.70189958e-06
Iter: 1162 loss: 2.68805888e-06
Iter: 1163 loss: 2.684131e-06
Iter: 1164 loss: 2.71435079e-06
Iter: 1165 loss: 2.68379449e-06
Iter: 1166 loss: 2.68126132e-06
Iter: 1167 loss: 2.67796213e-06
Iter: 1168 loss: 2.67767905e-06
Iter: 1169 loss: 2.6736052e-06
Iter: 1170 loss: 2.70857345e-06
Iter: 1171 loss: 2.67330665e-06
Iter: 1172 loss: 2.66951724e-06
Iter: 1173 loss: 2.68773397e-06
Iter: 1174 loss: 2.66877214e-06
Iter: 1175 loss: 2.6665416e-06
Iter: 1176 loss: 2.66549068e-06
Iter: 1177 loss: 2.66441612e-06
Iter: 1178 loss: 2.66010079e-06
Iter: 1179 loss: 2.68095187e-06
Iter: 1180 loss: 2.65933022e-06
Iter: 1181 loss: 2.65642439e-06
Iter: 1182 loss: 2.65447261e-06
Iter: 1183 loss: 2.65345284e-06
Iter: 1184 loss: 2.64972641e-06
Iter: 1185 loss: 2.6497064e-06
Iter: 1186 loss: 2.64692903e-06
Iter: 1187 loss: 2.64352957e-06
Iter: 1188 loss: 2.64320556e-06
Iter: 1189 loss: 2.63855554e-06
Iter: 1190 loss: 2.64980508e-06
Iter: 1191 loss: 2.63683251e-06
Iter: 1192 loss: 2.63355059e-06
Iter: 1193 loss: 2.67470682e-06
Iter: 1194 loss: 2.63346283e-06
Iter: 1195 loss: 2.62996127e-06
Iter: 1196 loss: 2.63174707e-06
Iter: 1197 loss: 2.62755839e-06
Iter: 1198 loss: 2.62425533e-06
Iter: 1199 loss: 2.62851154e-06
Iter: 1200 loss: 2.62251569e-06
Iter: 1201 loss: 2.61889727e-06
Iter: 1202 loss: 2.65388189e-06
Iter: 1203 loss: 2.61877608e-06
Iter: 1204 loss: 2.6158325e-06
Iter: 1205 loss: 2.61129844e-06
Iter: 1206 loss: 2.61128184e-06
Iter: 1207 loss: 2.60819161e-06
Iter: 1208 loss: 2.6081218e-06
Iter: 1209 loss: 2.60519664e-06
Iter: 1210 loss: 2.60822435e-06
Iter: 1211 loss: 2.60346701e-06
Iter: 1212 loss: 2.60030174e-06
Iter: 1213 loss: 2.60169236e-06
Iter: 1214 loss: 2.598018e-06
Iter: 1215 loss: 2.59417084e-06
Iter: 1216 loss: 2.63300581e-06
Iter: 1217 loss: 2.59408193e-06
Iter: 1218 loss: 2.5914353e-06
Iter: 1219 loss: 2.58802902e-06
Iter: 1220 loss: 2.58784166e-06
Iter: 1221 loss: 2.58473437e-06
Iter: 1222 loss: 2.58471073e-06
Iter: 1223 loss: 2.58188129e-06
Iter: 1224 loss: 2.57775173e-06
Iter: 1225 loss: 2.57766851e-06
Iter: 1226 loss: 2.57349757e-06
Iter: 1227 loss: 2.58437444e-06
Iter: 1228 loss: 2.57216425e-06
Iter: 1229 loss: 2.56786689e-06
Iter: 1230 loss: 2.58186628e-06
Iter: 1231 loss: 2.56676799e-06
Iter: 1232 loss: 2.56279873e-06
Iter: 1233 loss: 2.61491255e-06
Iter: 1234 loss: 2.56278281e-06
Iter: 1235 loss: 2.56026874e-06
Iter: 1236 loss: 2.55717623e-06
Iter: 1237 loss: 2.55698501e-06
Iter: 1238 loss: 2.55332429e-06
Iter: 1239 loss: 2.56321209e-06
Iter: 1240 loss: 2.55217583e-06
Iter: 1241 loss: 2.54945598e-06
Iter: 1242 loss: 2.54943666e-06
Iter: 1243 loss: 2.5471104e-06
Iter: 1244 loss: 2.54448423e-06
Iter: 1245 loss: 2.54425208e-06
Iter: 1246 loss: 2.54112229e-06
Iter: 1247 loss: 2.5644531e-06
Iter: 1248 loss: 2.54089e-06
Iter: 1249 loss: 2.53772873e-06
Iter: 1250 loss: 2.54040924e-06
Iter: 1251 loss: 2.5359036e-06
Iter: 1252 loss: 2.53288772e-06
Iter: 1253 loss: 2.53559324e-06
Iter: 1254 loss: 2.53108647e-06
Iter: 1255 loss: 2.52728159e-06
Iter: 1256 loss: 2.55930559e-06
Iter: 1257 loss: 2.52699647e-06
Iter: 1258 loss: 2.52398922e-06
Iter: 1259 loss: 2.51940537e-06
Iter: 1260 loss: 2.51936422e-06
Iter: 1261 loss: 2.51617098e-06
Iter: 1262 loss: 2.51608526e-06
Iter: 1263 loss: 2.51298229e-06
Iter: 1264 loss: 2.51204801e-06
Iter: 1265 loss: 2.51021379e-06
Iter: 1266 loss: 2.50657104e-06
Iter: 1267 loss: 2.51597248e-06
Iter: 1268 loss: 2.5053605e-06
Iter: 1269 loss: 2.50297e-06
Iter: 1270 loss: 2.50295147e-06
Iter: 1271 loss: 2.50060066e-06
Iter: 1272 loss: 2.49837603e-06
Iter: 1273 loss: 2.49789014e-06
Iter: 1274 loss: 2.49488539e-06
Iter: 1275 loss: 2.49486538e-06
Iter: 1276 loss: 2.49248751e-06
Iter: 1277 loss: 2.48907872e-06
Iter: 1278 loss: 2.48908555e-06
Iter: 1279 loss: 2.48576225e-06
Iter: 1280 loss: 2.48642209e-06
Iter: 1281 loss: 2.48325068e-06
Iter: 1282 loss: 2.47979438e-06
Iter: 1283 loss: 2.47993785e-06
Iter: 1284 loss: 2.47706248e-06
Iter: 1285 loss: 2.47341882e-06
Iter: 1286 loss: 2.51005486e-06
Iter: 1287 loss: 2.47336288e-06
Iter: 1288 loss: 2.46981358e-06
Iter: 1289 loss: 2.47856883e-06
Iter: 1290 loss: 2.46850141e-06
Iter: 1291 loss: 2.4659164e-06
Iter: 1292 loss: 2.46639638e-06
Iter: 1293 loss: 2.46401123e-06
Iter: 1294 loss: 2.4606054e-06
Iter: 1295 loss: 2.49421078e-06
Iter: 1296 loss: 2.46047784e-06
Iter: 1297 loss: 2.45799015e-06
Iter: 1298 loss: 2.45431943e-06
Iter: 1299 loss: 2.45425372e-06
Iter: 1300 loss: 2.45029401e-06
Iter: 1301 loss: 2.47961043e-06
Iter: 1302 loss: 2.44992088e-06
Iter: 1303 loss: 2.44602529e-06
Iter: 1304 loss: 2.47000526e-06
Iter: 1305 loss: 2.44564012e-06
Iter: 1306 loss: 2.4433466e-06
Iter: 1307 loss: 2.44209014e-06
Iter: 1308 loss: 2.44100738e-06
Iter: 1309 loss: 2.43758132e-06
Iter: 1310 loss: 2.46673176e-06
Iter: 1311 loss: 2.43735985e-06
Iter: 1312 loss: 2.43466229e-06
Iter: 1313 loss: 2.43547493e-06
Iter: 1314 loss: 2.43272507e-06
Iter: 1315 loss: 2.42972806e-06
Iter: 1316 loss: 2.43092472e-06
Iter: 1317 loss: 2.4276635e-06
Iter: 1318 loss: 2.42391934e-06
Iter: 1319 loss: 2.48226297e-06
Iter: 1320 loss: 2.4238982e-06
Iter: 1321 loss: 2.42189685e-06
Iter: 1322 loss: 2.41860698e-06
Iter: 1323 loss: 2.41855969e-06
Iter: 1324 loss: 2.41442604e-06
Iter: 1325 loss: 2.41898306e-06
Iter: 1326 loss: 2.41227e-06
Iter: 1327 loss: 2.40861141e-06
Iter: 1328 loss: 2.40863665e-06
Iter: 1329 loss: 2.40548661e-06
Iter: 1330 loss: 2.41324187e-06
Iter: 1331 loss: 2.40436566e-06
Iter: 1332 loss: 2.40197824e-06
Iter: 1333 loss: 2.40151348e-06
Iter: 1334 loss: 2.39991505e-06
Iter: 1335 loss: 2.39629935e-06
Iter: 1336 loss: 2.43218597e-06
Iter: 1337 loss: 2.39620113e-06
Iter: 1338 loss: 2.39400924e-06
Iter: 1339 loss: 2.3915909e-06
Iter: 1340 loss: 2.39125075e-06
Iter: 1341 loss: 2.38881557e-06
Iter: 1342 loss: 2.38871166e-06
Iter: 1343 loss: 2.38665598e-06
Iter: 1344 loss: 2.3827829e-06
Iter: 1345 loss: 2.4728904e-06
Iter: 1346 loss: 2.38282701e-06
Iter: 1347 loss: 2.37850145e-06
Iter: 1348 loss: 2.3908924e-06
Iter: 1349 loss: 2.37718791e-06
Iter: 1350 loss: 2.37425866e-06
Iter: 1351 loss: 2.41478983e-06
Iter: 1352 loss: 2.37425184e-06
Iter: 1353 loss: 2.37105723e-06
Iter: 1354 loss: 2.36996766e-06
Iter: 1355 loss: 2.36812753e-06
Iter: 1356 loss: 2.36480332e-06
Iter: 1357 loss: 2.37138238e-06
Iter: 1358 loss: 2.3634102e-06
Iter: 1359 loss: 2.36050573e-06
Iter: 1360 loss: 2.39950168e-06
Iter: 1361 loss: 2.36048345e-06
Iter: 1362 loss: 2.35830885e-06
Iter: 1363 loss: 2.35529e-06
Iter: 1364 loss: 2.35512562e-06
Iter: 1365 loss: 2.35159587e-06
Iter: 1366 loss: 2.36005667e-06
Iter: 1367 loss: 2.3502439e-06
Iter: 1368 loss: 2.34734102e-06
Iter: 1369 loss: 2.38797884e-06
Iter: 1370 loss: 2.34737672e-06
Iter: 1371 loss: 2.3443838e-06
Iter: 1372 loss: 2.34386948e-06
Iter: 1373 loss: 2.34188292e-06
Iter: 1374 loss: 2.33887113e-06
Iter: 1375 loss: 2.34855929e-06
Iter: 1376 loss: 2.33794117e-06
Iter: 1377 loss: 2.33434957e-06
Iter: 1378 loss: 2.35054245e-06
Iter: 1379 loss: 2.33368246e-06
Iter: 1380 loss: 2.33128276e-06
Iter: 1381 loss: 2.3336986e-06
Iter: 1382 loss: 2.32992465e-06
Iter: 1383 loss: 2.32647949e-06
Iter: 1384 loss: 2.33952596e-06
Iter: 1385 loss: 2.32563343e-06
Iter: 1386 loss: 2.323025e-06
Iter: 1387 loss: 2.32258708e-06
Iter: 1388 loss: 2.32076036e-06
Iter: 1389 loss: 2.31767103e-06
Iter: 1390 loss: 2.33301762e-06
Iter: 1391 loss: 2.31708623e-06
Iter: 1392 loss: 2.31363947e-06
Iter: 1393 loss: 2.33131277e-06
Iter: 1394 loss: 2.31303125e-06
Iter: 1395 loss: 2.3109319e-06
Iter: 1396 loss: 2.30825049e-06
Iter: 1397 loss: 2.3080654e-06
Iter: 1398 loss: 2.30390447e-06
Iter: 1399 loss: 2.30703e-06
Iter: 1400 loss: 2.30139267e-06
Iter: 1401 loss: 2.29717762e-06
Iter: 1402 loss: 2.33346054e-06
Iter: 1403 loss: 2.29690977e-06
Iter: 1404 loss: 2.29361922e-06
Iter: 1405 loss: 2.32840193e-06
Iter: 1406 loss: 2.29359148e-06
Iter: 1407 loss: 2.29113061e-06
Iter: 1408 loss: 2.28803219e-06
Iter: 1409 loss: 2.2878105e-06
Iter: 1410 loss: 2.28501744e-06
Iter: 1411 loss: 2.28500357e-06
Iter: 1412 loss: 2.28231897e-06
Iter: 1413 loss: 2.28101339e-06
Iter: 1414 loss: 2.27968985e-06
Iter: 1415 loss: 2.27688179e-06
Iter: 1416 loss: 2.30091473e-06
Iter: 1417 loss: 2.27672308e-06
Iter: 1418 loss: 2.27391411e-06
Iter: 1419 loss: 2.27451392e-06
Iter: 1420 loss: 2.27188116e-06
Iter: 1421 loss: 2.26899783e-06
Iter: 1422 loss: 2.27316923e-06
Iter: 1423 loss: 2.26768134e-06
Iter: 1424 loss: 2.26484281e-06
Iter: 1425 loss: 2.30026149e-06
Iter: 1426 loss: 2.26484735e-06
Iter: 1427 loss: 2.26284101e-06
Iter: 1428 loss: 2.26131897e-06
Iter: 1429 loss: 2.26058933e-06
Iter: 1430 loss: 2.2577774e-06
Iter: 1431 loss: 2.28070508e-06
Iter: 1432 loss: 2.25760232e-06
Iter: 1433 loss: 2.25482745e-06
Iter: 1434 loss: 2.25554459e-06
Iter: 1435 loss: 2.2528352e-06
Iter: 1436 loss: 2.24968176e-06
Iter: 1437 loss: 2.24739642e-06
Iter: 1438 loss: 2.24641644e-06
Iter: 1439 loss: 2.2417687e-06
Iter: 1440 loss: 2.26254861e-06
Iter: 1441 loss: 2.24083942e-06
Iter: 1442 loss: 2.2383856e-06
Iter: 1443 loss: 2.23819302e-06
Iter: 1444 loss: 2.23616394e-06
Iter: 1445 loss: 2.23388679e-06
Iter: 1446 loss: 2.23357847e-06
Iter: 1447 loss: 2.23070629e-06
Iter: 1448 loss: 2.24364476e-06
Iter: 1449 loss: 2.23012944e-06
Iter: 1450 loss: 2.22714607e-06
Iter: 1451 loss: 2.24486212e-06
Iter: 1452 loss: 2.2266704e-06
Iter: 1453 loss: 2.22454673e-06
Iter: 1454 loss: 2.22140989e-06
Iter: 1455 loss: 2.22129529e-06
Iter: 1456 loss: 2.21909931e-06
Iter: 1457 loss: 2.21891582e-06
Iter: 1458 loss: 2.2171489e-06
Iter: 1459 loss: 2.21427626e-06
Iter: 1460 loss: 2.21421783e-06
Iter: 1461 loss: 2.21141454e-06
Iter: 1462 loss: 2.24938867e-06
Iter: 1463 loss: 2.21140454e-06
Iter: 1464 loss: 2.20878837e-06
Iter: 1465 loss: 2.20785523e-06
Iter: 1466 loss: 2.2063623e-06
Iter: 1467 loss: 2.20364063e-06
Iter: 1468 loss: 2.214201e-06
Iter: 1469 loss: 2.20303e-06
Iter: 1470 loss: 2.20015681e-06
Iter: 1471 loss: 2.21262053e-06
Iter: 1472 loss: 2.19960521e-06
Iter: 1473 loss: 2.1970759e-06
Iter: 1474 loss: 2.19516437e-06
Iter: 1475 loss: 2.19435083e-06
Iter: 1476 loss: 2.19115964e-06
Iter: 1477 loss: 2.21206369e-06
Iter: 1478 loss: 2.19078356e-06
Iter: 1479 loss: 2.18824016e-06
Iter: 1480 loss: 2.21448772e-06
Iter: 1481 loss: 2.18817718e-06
Iter: 1482 loss: 2.18641935e-06
Iter: 1483 loss: 2.18290052e-06
Iter: 1484 loss: 2.25581562e-06
Iter: 1485 loss: 2.18289574e-06
Iter: 1486 loss: 2.18101422e-06
Iter: 1487 loss: 2.18075638e-06
Iter: 1488 loss: 2.17876368e-06
Iter: 1489 loss: 2.17626689e-06
Iter: 1490 loss: 2.17612478e-06
Iter: 1491 loss: 2.1731596e-06
Iter: 1492 loss: 2.1802598e-06
Iter: 1493 loss: 2.17209413e-06
Iter: 1494 loss: 2.16914304e-06
Iter: 1495 loss: 2.19997514e-06
Iter: 1496 loss: 2.16905255e-06
Iter: 1497 loss: 2.16674266e-06
Iter: 1498 loss: 2.16553531e-06
Iter: 1499 loss: 2.16452e-06
Iter: 1500 loss: 2.16178137e-06
Iter: 1501 loss: 2.19502726e-06
Iter: 1502 loss: 2.16177227e-06
Iter: 1503 loss: 2.15955288e-06
Iter: 1504 loss: 2.15682189e-06
Iter: 1505 loss: 2.1565761e-06
Iter: 1506 loss: 2.15380214e-06
Iter: 1507 loss: 2.16695162e-06
Iter: 1508 loss: 2.15329487e-06
Iter: 1509 loss: 2.15032742e-06
Iter: 1510 loss: 2.16665876e-06
Iter: 1511 loss: 2.14994543e-06
Iter: 1512 loss: 2.14770307e-06
Iter: 1513 loss: 2.14531974e-06
Iter: 1514 loss: 2.14490501e-06
Iter: 1515 loss: 2.14202646e-06
Iter: 1516 loss: 2.17863953e-06
Iter: 1517 loss: 2.1420251e-06
Iter: 1518 loss: 2.13945532e-06
Iter: 1519 loss: 2.14724378e-06
Iter: 1520 loss: 2.13863109e-06
Iter: 1521 loss: 2.13633098e-06
Iter: 1522 loss: 2.1343003e-06
Iter: 1523 loss: 2.13364046e-06
Iter: 1524 loss: 2.1313972e-06
Iter: 1525 loss: 2.13129351e-06
Iter: 1526 loss: 2.12961368e-06
Iter: 1527 loss: 2.1270564e-06
Iter: 1528 loss: 2.12704708e-06
Iter: 1529 loss: 2.12400573e-06
Iter: 1530 loss: 2.13100111e-06
Iter: 1531 loss: 2.12298892e-06
Iter: 1532 loss: 2.12026134e-06
Iter: 1533 loss: 2.16392846e-06
Iter: 1534 loss: 2.12026202e-06
Iter: 1535 loss: 2.11860743e-06
Iter: 1536 loss: 2.11578617e-06
Iter: 1537 loss: 2.18417654e-06
Iter: 1538 loss: 2.11580254e-06
Iter: 1539 loss: 2.11282486e-06
Iter: 1540 loss: 2.15516729e-06
Iter: 1541 loss: 2.11283623e-06
Iter: 1542 loss: 2.1106498e-06
Iter: 1543 loss: 2.11106021e-06
Iter: 1544 loss: 2.10901203e-06
Iter: 1545 loss: 2.10642179e-06
Iter: 1546 loss: 2.10658072e-06
Iter: 1547 loss: 2.10439475e-06
Iter: 1548 loss: 2.10135067e-06
Iter: 1549 loss: 2.15075852e-06
Iter: 1550 loss: 2.10133931e-06
Iter: 1551 loss: 2.09944756e-06
Iter: 1552 loss: 2.09653808e-06
Iter: 1553 loss: 2.09645896e-06
Iter: 1554 loss: 2.09413747e-06
Iter: 1555 loss: 2.0941402e-06
Iter: 1556 loss: 2.09173618e-06
Iter: 1557 loss: 2.09035511e-06
Iter: 1558 loss: 2.08952792e-06
Iter: 1559 loss: 2.08660708e-06
Iter: 1560 loss: 2.09285395e-06
Iter: 1561 loss: 2.08550318e-06
Iter: 1562 loss: 2.08336746e-06
Iter: 1563 loss: 2.08336451e-06
Iter: 1564 loss: 2.08161623e-06
Iter: 1565 loss: 2.07960284e-06
Iter: 1566 loss: 2.07933908e-06
Iter: 1567 loss: 2.07660537e-06
Iter: 1568 loss: 2.08158554e-06
Iter: 1569 loss: 2.07547805e-06
Iter: 1570 loss: 2.07268795e-06
Iter: 1571 loss: 2.1107694e-06
Iter: 1572 loss: 2.07269545e-06
Iter: 1573 loss: 2.07076823e-06
Iter: 1574 loss: 2.0678342e-06
Iter: 1575 loss: 2.06778304e-06
Iter: 1576 loss: 2.06503501e-06
Iter: 1577 loss: 2.08820916e-06
Iter: 1578 loss: 2.06483332e-06
Iter: 1579 loss: 2.06217396e-06
Iter: 1580 loss: 2.0678815e-06
Iter: 1581 loss: 2.06109371e-06
Iter: 1582 loss: 2.05862716e-06
Iter: 1583 loss: 2.05695551e-06
Iter: 1584 loss: 2.05605784e-06
Iter: 1585 loss: 2.0530897e-06
Iter: 1586 loss: 2.09156178e-06
Iter: 1587 loss: 2.05307106e-06
Iter: 1588 loss: 2.05034712e-06
Iter: 1589 loss: 2.05526612e-06
Iter: 1590 loss: 2.04908815e-06
Iter: 1591 loss: 2.04709386e-06
Iter: 1592 loss: 2.05364631e-06
Iter: 1593 loss: 2.04647358e-06
Iter: 1594 loss: 2.04381308e-06
Iter: 1595 loss: 2.04837784e-06
Iter: 1596 loss: 2.04260323e-06
Iter: 1597 loss: 2.04039793e-06
Iter: 1598 loss: 2.04104072e-06
Iter: 1599 loss: 2.03879836e-06
Iter: 1600 loss: 2.03623495e-06
Iter: 1601 loss: 2.07059338e-06
Iter: 1602 loss: 2.03621607e-06
Iter: 1603 loss: 2.03433956e-06
Iter: 1604 loss: 2.03249806e-06
Iter: 1605 loss: 2.03204e-06
Iter: 1606 loss: 2.02960268e-06
Iter: 1607 loss: 2.04029106e-06
Iter: 1608 loss: 2.0291136e-06
Iter: 1609 loss: 2.02639512e-06
Iter: 1610 loss: 2.04172352e-06
Iter: 1611 loss: 2.02603883e-06
Iter: 1612 loss: 2.02397473e-06
Iter: 1613 loss: 2.02132287e-06
Iter: 1614 loss: 2.02115666e-06
Iter: 1615 loss: 2.01837679e-06
Iter: 1616 loss: 2.03761465e-06
Iter: 1617 loss: 2.01810235e-06
Iter: 1618 loss: 2.01539365e-06
Iter: 1619 loss: 2.02912429e-06
Iter: 1620 loss: 2.01495914e-06
Iter: 1621 loss: 2.01270495e-06
Iter: 1622 loss: 2.0108273e-06
Iter: 1623 loss: 2.01014382e-06
Iter: 1624 loss: 2.00773366e-06
Iter: 1625 loss: 2.03680338e-06
Iter: 1626 loss: 2.00770637e-06
Iter: 1627 loss: 2.00518252e-06
Iter: 1628 loss: 2.00821341e-06
Iter: 1629 loss: 2.00379827e-06
Iter: 1630 loss: 2.00132081e-06
Iter: 1631 loss: 2.004012e-06
Iter: 1632 loss: 2.00000363e-06
Iter: 1633 loss: 1.99736564e-06
Iter: 1634 loss: 2.02465776e-06
Iter: 1635 loss: 1.99729402e-06
Iter: 1636 loss: 1.99557189e-06
Iter: 1637 loss: 1.99334477e-06
Iter: 1638 loss: 1.99320607e-06
Iter: 1639 loss: 1.99080728e-06
Iter: 1640 loss: 1.99080023e-06
Iter: 1641 loss: 1.98905036e-06
Iter: 1642 loss: 1.98573684e-06
Iter: 1643 loss: 2.05979904e-06
Iter: 1644 loss: 1.9856916e-06
Iter: 1645 loss: 1.98372027e-06
Iter: 1646 loss: 1.98357338e-06
Iter: 1647 loss: 1.9815734e-06
Iter: 1648 loss: 1.98116e-06
Iter: 1649 loss: 1.97995541e-06
Iter: 1650 loss: 1.97737586e-06
Iter: 1651 loss: 1.97666691e-06
Iter: 1652 loss: 1.97511599e-06
Iter: 1653 loss: 1.97208647e-06
Iter: 1654 loss: 1.99138094e-06
Iter: 1655 loss: 1.97172244e-06
Iter: 1656 loss: 1.96920246e-06
Iter: 1657 loss: 1.99221608e-06
Iter: 1658 loss: 1.96912151e-06
Iter: 1659 loss: 1.96722704e-06
Iter: 1660 loss: 1.96460314e-06
Iter: 1661 loss: 1.96453175e-06
Iter: 1662 loss: 1.96247538e-06
Iter: 1663 loss: 1.9624581e-06
Iter: 1664 loss: 1.9604895e-06
Iter: 1665 loss: 1.96240353e-06
Iter: 1666 loss: 1.9593831e-06
Iter: 1667 loss: 1.95719099e-06
Iter: 1668 loss: 1.95562961e-06
Iter: 1669 loss: 1.95484154e-06
Iter: 1670 loss: 1.95255916e-06
Iter: 1671 loss: 1.9525055e-06
Iter: 1672 loss: 1.95078337e-06
Iter: 1673 loss: 1.94802828e-06
Iter: 1674 loss: 1.94794325e-06
Iter: 1675 loss: 1.94564791e-06
Iter: 1676 loss: 1.94561244e-06
Iter: 1677 loss: 1.94374502e-06
Iter: 1678 loss: 1.94169775e-06
Iter: 1679 loss: 1.94135509e-06
Iter: 1680 loss: 1.93922051e-06
Iter: 1681 loss: 1.96823976e-06
Iter: 1682 loss: 1.93921051e-06
Iter: 1683 loss: 1.93723668e-06
Iter: 1684 loss: 1.9371e-06
Iter: 1685 loss: 1.9355316e-06
Iter: 1686 loss: 1.93310143e-06
Iter: 1687 loss: 1.93138635e-06
Iter: 1688 loss: 1.9305221e-06
Iter: 1689 loss: 1.9271522e-06
Iter: 1690 loss: 1.94196173e-06
Iter: 1691 loss: 1.92650373e-06
Iter: 1692 loss: 1.92416655e-06
Iter: 1693 loss: 1.95600205e-06
Iter: 1694 loss: 1.92414382e-06
Iter: 1695 loss: 1.92207926e-06
Iter: 1696 loss: 1.92599146e-06
Iter: 1697 loss: 1.92124094e-06
Iter: 1698 loss: 1.91943036e-06
Iter: 1699 loss: 1.91964045e-06
Iter: 1700 loss: 1.9180552e-06
Iter: 1701 loss: 1.91526465e-06
Iter: 1702 loss: 1.9367194e-06
Iter: 1703 loss: 1.91505205e-06
Iter: 1704 loss: 1.91317167e-06
Iter: 1705 loss: 1.9115846e-06
Iter: 1706 loss: 1.91110075e-06
Iter: 1707 loss: 1.9085669e-06
Iter: 1708 loss: 1.92878315e-06
Iter: 1709 loss: 1.90839523e-06
Iter: 1710 loss: 1.9057552e-06
Iter: 1711 loss: 1.91114168e-06
Iter: 1712 loss: 1.90470212e-06
Iter: 1713 loss: 1.90263631e-06
Iter: 1714 loss: 1.90127855e-06
Iter: 1715 loss: 1.90055687e-06
Iter: 1716 loss: 1.89844332e-06
Iter: 1717 loss: 1.89830075e-06
Iter: 1718 loss: 1.89680907e-06
Iter: 1719 loss: 1.89390494e-06
Iter: 1720 loss: 1.95282473e-06
Iter: 1721 loss: 1.89392381e-06
Iter: 1722 loss: 1.89143839e-06
Iter: 1723 loss: 1.92643711e-06
Iter: 1724 loss: 1.89146488e-06
Iter: 1725 loss: 1.88946274e-06
Iter: 1726 loss: 1.89241928e-06
Iter: 1727 loss: 1.88850686e-06
Iter: 1728 loss: 1.8862429e-06
Iter: 1729 loss: 1.8842876e-06
Iter: 1730 loss: 1.8836995e-06
Iter: 1731 loss: 1.88169452e-06
Iter: 1732 loss: 1.88157901e-06
Iter: 1733 loss: 1.87958392e-06
Iter: 1734 loss: 1.87968442e-06
Iter: 1735 loss: 1.87806097e-06
Iter: 1736 loss: 1.87567343e-06
Iter: 1737 loss: 1.87981664e-06
Iter: 1738 loss: 1.87467231e-06
Iter: 1739 loss: 1.87204978e-06
Iter: 1740 loss: 1.89429147e-06
Iter: 1741 loss: 1.87194746e-06
Iter: 1742 loss: 1.87034482e-06
Iter: 1743 loss: 1.86823115e-06
Iter: 1744 loss: 1.86802947e-06
Iter: 1745 loss: 1.86569957e-06
Iter: 1746 loss: 1.89968932e-06
Iter: 1747 loss: 1.86571526e-06
Iter: 1748 loss: 1.86372517e-06
Iter: 1749 loss: 1.86675425e-06
Iter: 1750 loss: 1.86279919e-06
Iter: 1751 loss: 1.86104103e-06
Iter: 1752 loss: 1.86096941e-06
Iter: 1753 loss: 1.85961562e-06
Iter: 1754 loss: 1.85710712e-06
Iter: 1755 loss: 1.88483409e-06
Iter: 1756 loss: 1.85704607e-06
Iter: 1757 loss: 1.85550266e-06
Iter: 1758 loss: 1.85290799e-06
Iter: 1759 loss: 1.8529289e-06
Iter: 1760 loss: 1.85065073e-06
Iter: 1761 loss: 1.87606383e-06
Iter: 1762 loss: 1.85062493e-06
Iter: 1763 loss: 1.84834573e-06
Iter: 1764 loss: 1.84874716e-06
Iter: 1765 loss: 1.84657881e-06
Iter: 1766 loss: 1.8436258e-06
Iter: 1767 loss: 1.84650969e-06
Iter: 1768 loss: 1.84200871e-06
Iter: 1769 loss: 1.83948396e-06
Iter: 1770 loss: 1.84974056e-06
Iter: 1771 loss: 1.83889779e-06
Iter: 1772 loss: 1.83662735e-06
Iter: 1773 loss: 1.86904197e-06
Iter: 1774 loss: 1.83663781e-06
Iter: 1775 loss: 1.83513771e-06
Iter: 1776 loss: 1.83346253e-06
Iter: 1777 loss: 1.83323334e-06
Iter: 1778 loss: 1.83107113e-06
Iter: 1779 loss: 1.83888494e-06
Iter: 1780 loss: 1.83055272e-06
Iter: 1781 loss: 1.82835197e-06
Iter: 1782 loss: 1.84519627e-06
Iter: 1783 loss: 1.82817621e-06
Iter: 1784 loss: 1.82666452e-06
Iter: 1785 loss: 1.82513941e-06
Iter: 1786 loss: 1.8248121e-06
Iter: 1787 loss: 1.82301858e-06
Iter: 1788 loss: 1.82301324e-06
Iter: 1789 loss: 1.82143674e-06
Iter: 1790 loss: 1.82074905e-06
Iter: 1791 loss: 1.81993687e-06
Iter: 1792 loss: 1.8177783e-06
Iter: 1793 loss: 1.81885889e-06
Iter: 1794 loss: 1.81633663e-06
Iter: 1795 loss: 1.81382109e-06
Iter: 1796 loss: 1.84850717e-06
Iter: 1797 loss: 1.81378368e-06
Iter: 1798 loss: 1.81229416e-06
Iter: 1799 loss: 1.80985592e-06
Iter: 1800 loss: 1.80985376e-06
Iter: 1801 loss: 1.80729376e-06
Iter: 1802 loss: 1.820752e-06
Iter: 1803 loss: 1.80688812e-06
Iter: 1804 loss: 1.80409893e-06
Iter: 1805 loss: 1.81720168e-06
Iter: 1806 loss: 1.80355187e-06
Iter: 1807 loss: 1.80149618e-06
Iter: 1808 loss: 1.80179416e-06
Iter: 1809 loss: 1.79995118e-06
Iter: 1810 loss: 1.79794677e-06
Iter: 1811 loss: 1.81780808e-06
Iter: 1812 loss: 1.79792892e-06
Iter: 1813 loss: 1.79584663e-06
Iter: 1814 loss: 1.7999273e-06
Iter: 1815 loss: 1.79503445e-06
Iter: 1816 loss: 1.79353083e-06
Iter: 1817 loss: 1.79185713e-06
Iter: 1818 loss: 1.79159701e-06
Iter: 1819 loss: 1.78927564e-06
Iter: 1820 loss: 1.80987195e-06
Iter: 1821 loss: 1.7892263e-06
Iter: 1822 loss: 1.78699747e-06
Iter: 1823 loss: 1.79697508e-06
Iter: 1824 loss: 1.78653715e-06
Iter: 1825 loss: 1.78497771e-06
Iter: 1826 loss: 1.78379378e-06
Iter: 1827 loss: 1.78329515e-06
Iter: 1828 loss: 1.78111e-06
Iter: 1829 loss: 1.79276594e-06
Iter: 1830 loss: 1.78081734e-06
Iter: 1831 loss: 1.77823654e-06
Iter: 1832 loss: 1.78585606e-06
Iter: 1833 loss: 1.77750394e-06
Iter: 1834 loss: 1.77547486e-06
Iter: 1835 loss: 1.77412585e-06
Iter: 1836 loss: 1.77340269e-06
Iter: 1837 loss: 1.77152515e-06
Iter: 1838 loss: 1.77141601e-06
Iter: 1839 loss: 1.76989943e-06
Iter: 1840 loss: 1.76770141e-06
Iter: 1841 loss: 1.76762944e-06
Iter: 1842 loss: 1.76547485e-06
Iter: 1843 loss: 1.78153255e-06
Iter: 1844 loss: 1.76532126e-06
Iter: 1845 loss: 1.76339859e-06
Iter: 1846 loss: 1.77283164e-06
Iter: 1847 loss: 1.76306878e-06
Iter: 1848 loss: 1.76158164e-06
Iter: 1849 loss: 1.76126969e-06
Iter: 1850 loss: 1.76023832e-06
Iter: 1851 loss: 1.75828313e-06
Iter: 1852 loss: 1.78168818e-06
Iter: 1853 loss: 1.75829359e-06
Iter: 1854 loss: 1.75679645e-06
Iter: 1855 loss: 1.75470484e-06
Iter: 1856 loss: 1.75466641e-06
Iter: 1857 loss: 1.7521071e-06
Iter: 1858 loss: 1.75460298e-06
Iter: 1859 loss: 1.75055538e-06
Iter: 1860 loss: 1.74801562e-06
Iter: 1861 loss: 1.76650838e-06
Iter: 1862 loss: 1.74780314e-06
Iter: 1863 loss: 1.74618015e-06
Iter: 1864 loss: 1.77059587e-06
Iter: 1865 loss: 1.74618174e-06
Iter: 1866 loss: 1.74475576e-06
Iter: 1867 loss: 1.74326237e-06
Iter: 1868 loss: 1.74301613e-06
Iter: 1869 loss: 1.74104662e-06
Iter: 1870 loss: 1.75643834e-06
Iter: 1871 loss: 1.7409111e-06
Iter: 1872 loss: 1.73898809e-06
Iter: 1873 loss: 1.74158549e-06
Iter: 1874 loss: 1.738072e-06
Iter: 1875 loss: 1.7363501e-06
Iter: 1876 loss: 1.73609192e-06
Iter: 1877 loss: 1.73489673e-06
Iter: 1878 loss: 1.7327684e-06
Iter: 1879 loss: 1.76506182e-06
Iter: 1880 loss: 1.73278249e-06
Iter: 1881 loss: 1.7314253e-06
Iter: 1882 loss: 1.72962382e-06
Iter: 1883 loss: 1.72951047e-06
Iter: 1884 loss: 1.72741761e-06
Iter: 1885 loss: 1.75000878e-06
Iter: 1886 loss: 1.72736145e-06
Iter: 1887 loss: 1.72546879e-06
Iter: 1888 loss: 1.72615432e-06
Iter: 1889 loss: 1.72411831e-06
Iter: 1890 loss: 1.72213527e-06
Iter: 1891 loss: 1.72312537e-06
Iter: 1892 loss: 1.72079638e-06
Iter: 1893 loss: 1.71874876e-06
Iter: 1894 loss: 1.74781405e-06
Iter: 1895 loss: 1.71877468e-06
Iter: 1896 loss: 1.71687475e-06
Iter: 1897 loss: 1.71832414e-06
Iter: 1898 loss: 1.71569332e-06
Iter: 1899 loss: 1.71402928e-06
Iter: 1900 loss: 1.71353372e-06
Iter: 1901 loss: 1.71260785e-06
Iter: 1902 loss: 1.71022418e-06
Iter: 1903 loss: 1.71698116e-06
Iter: 1904 loss: 1.70946237e-06
Iter: 1905 loss: 1.70770875e-06
Iter: 1906 loss: 1.70768431e-06
Iter: 1907 loss: 1.70628073e-06
Iter: 1908 loss: 1.7052763e-06
Iter: 1909 loss: 1.70477927e-06
Iter: 1910 loss: 1.70301678e-06
Iter: 1911 loss: 1.72054911e-06
Iter: 1912 loss: 1.70292878e-06
Iter: 1913 loss: 1.70134558e-06
Iter: 1914 loss: 1.70089686e-06
Iter: 1915 loss: 1.69990221e-06
Iter: 1916 loss: 1.69796226e-06
Iter: 1917 loss: 1.69980603e-06
Iter: 1918 loss: 1.69686427e-06
Iter: 1919 loss: 1.69453813e-06
Iter: 1920 loss: 1.71887018e-06
Iter: 1921 loss: 1.69451152e-06
Iter: 1922 loss: 1.69317195e-06
Iter: 1923 loss: 1.69216992e-06
Iter: 1924 loss: 1.69162388e-06
Iter: 1925 loss: 1.68980534e-06
Iter: 1926 loss: 1.70933106e-06
Iter: 1927 loss: 1.68974861e-06
Iter: 1928 loss: 1.68798317e-06
Iter: 1929 loss: 1.68724648e-06
Iter: 1930 loss: 1.68629481e-06
Iter: 1931 loss: 1.68421639e-06
Iter: 1932 loss: 1.68671818e-06
Iter: 1933 loss: 1.68313568e-06
Iter: 1934 loss: 1.68162842e-06
Iter: 1935 loss: 1.68160318e-06
Iter: 1936 loss: 1.68024667e-06
Iter: 1937 loss: 1.67904295e-06
Iter: 1938 loss: 1.67870735e-06
Iter: 1939 loss: 1.67678684e-06
Iter: 1940 loss: 1.67793257e-06
Iter: 1941 loss: 1.67556561e-06
Iter: 1942 loss: 1.67374719e-06
Iter: 1943 loss: 1.67374333e-06
Iter: 1944 loss: 1.67225016e-06
Iter: 1945 loss: 1.67172846e-06
Iter: 1946 loss: 1.670855e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi1.6
+ date
Wed Nov  4 13:48:54 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.2/300_300_300_1 --function f2 --psi -1 --alpha 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f37e29268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f37e29d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f377510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f2edd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f2ed950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f2e8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f22e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f22eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f239400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f293730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f293d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f265488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f265b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f1f78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f1be620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f15a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f199158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f0928c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f0ba620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f094730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f1482f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f148bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f016400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f1487b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f016378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f016d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2ef56510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2ef529d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2ef521e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2ef3b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2ef2c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f0062f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2efecb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2efecae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2f006268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2f2ef0a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.009855814
test_loss: 0.0115176635
train_loss: 0.007898484
test_loss: 0.010079191
train_loss: 0.007815019
test_loss: 0.009467858
train_loss: 0.0076885317
test_loss: 0.009080897
train_loss: 0.0069736103
test_loss: 0.0090693785
train_loss: 0.0066211075
test_loss: 0.00883849
train_loss: 0.008038056
test_loss: 0.008711784
train_loss: 0.006753583
test_loss: 0.0086960625
train_loss: 0.0067284442
test_loss: 0.008518412
train_loss: 0.0068709594
test_loss: 0.008499646
train_loss: 0.0065436736
test_loss: 0.00879433
train_loss: 0.0060071107
test_loss: 0.008429843
train_loss: 0.0061697196
test_loss: 0.008846617
train_loss: 0.006061131
test_loss: 0.008279711
train_loss: 0.0061100065
test_loss: 0.008244813
train_loss: 0.0063722488
test_loss: 0.008574907
train_loss: 0.005879454
test_loss: 0.008319821
train_loss: 0.0057012597
test_loss: 0.008147663
train_loss: 0.0062027085
test_loss: 0.008570133
train_loss: 0.006519519
test_loss: 0.008221542
train_loss: 0.005912132
test_loss: 0.008311887
train_loss: 0.0058049895
test_loss: 0.008118969
train_loss: 0.005810825
test_loss: 0.008165176
train_loss: 0.005858962
test_loss: 0.008083012
train_loss: 0.005561359
test_loss: 0.007897669
train_loss: 0.005531039
test_loss: 0.007902555
train_loss: 0.0053114914
test_loss: 0.0077930316
train_loss: 0.005836952
test_loss: 0.007993903
train_loss: 0.005693933
test_loss: 0.008100496
train_loss: 0.0054449183
test_loss: 0.008084146
train_loss: 0.005941921
test_loss: 0.0080111995
train_loss: 0.0060787397
test_loss: 0.00812506
train_loss: 0.0053526773
test_loss: 0.0080377
train_loss: 0.0055537694
test_loss: 0.007833065
train_loss: 0.0056528007
test_loss: 0.00796161
train_loss: 0.005606971
test_loss: 0.007819112
train_loss: 0.005245704
test_loss: 0.007857484
train_loss: 0.005508851
test_loss: 0.0077485996
train_loss: 0.0055814385
test_loss: 0.0077424813
train_loss: 0.0051130326
test_loss: 0.007995127
train_loss: 0.005225671
test_loss: 0.007811969
train_loss: 0.005405626
test_loss: 0.008018747
train_loss: 0.0053598494
test_loss: 0.007837826
train_loss: 0.005428909
test_loss: 0.007927416
train_loss: 0.0052257143
test_loss: 0.007834884
train_loss: 0.004929804
test_loss: 0.007845135
train_loss: 0.0054928483
test_loss: 0.0077643394
train_loss: 0.00508315
test_loss: 0.00789044
train_loss: 0.005545023
test_loss: 0.007666277
train_loss: 0.0052914834
test_loss: 0.007783225
train_loss: 0.004961457
test_loss: 0.007582316
train_loss: 0.004986587
test_loss: 0.00785492
train_loss: 0.004932163
test_loss: 0.007614258
train_loss: 0.005120593
test_loss: 0.007913782
train_loss: 0.0055850497
test_loss: 0.008000228
train_loss: 0.0050748675
test_loss: 0.00797447
train_loss: 0.0049198456
test_loss: 0.0076584234
train_loss: 0.0052634277
test_loss: 0.0076632644
train_loss: 0.0050885463
test_loss: 0.0076429066
train_loss: 0.0052570193
test_loss: 0.0076801176
train_loss: 0.005043307
test_loss: 0.007761828
train_loss: 0.0049353675
test_loss: 0.0076676286
train_loss: 0.0054436694
test_loss: 0.0077813296
train_loss: 0.0049067247
test_loss: 0.0075721787
train_loss: 0.004736562
test_loss: 0.007582242
train_loss: 0.0050632088
test_loss: 0.0077411816
train_loss: 0.005244008
test_loss: 0.00767345
train_loss: 0.0051479167
test_loss: 0.007609994
train_loss: 0.0049892026
test_loss: 0.007584808
train_loss: 0.005021016
test_loss: 0.007791222
train_loss: 0.005106542
test_loss: 0.0078120246
train_loss: 0.004630474
test_loss: 0.0074088415
train_loss: 0.005650369
test_loss: 0.0076382025
train_loss: 0.004745239
test_loss: 0.0075599845
train_loss: 0.004983087
test_loss: 0.0075337123
train_loss: 0.0048718937
test_loss: 0.0075696805
train_loss: 0.0052624573
test_loss: 0.0077475784
train_loss: 0.0046957983
test_loss: 0.0074915644
train_loss: 0.0050359475
test_loss: 0.0075933808
train_loss: 0.004644064
test_loss: 0.0076267393
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.6/300_300_300_1 --optimizer lbfgs --function f2 --psi -1 --alpha 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f080d86bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f080d817a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f080d7bb7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f080d7bb488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f080d7bb268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f080d75a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f080d70f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f080d70f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f080d6c48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f080d6c4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f080d6de840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07d098f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07d09ec400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07d09ec598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07d0986620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07d092f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07d092f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07d0900620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07ac18d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07d090aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07ac148598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07ac1486a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07ac1146a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07ac144840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07ac1318c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07ac0db7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07ac0b61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07ac09d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07ac047158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07ac012620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07ac012c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07ac03a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07907a2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07907a27b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07907a2f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07907571e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.79296357e-05
Iter: 2 loss: 4.81032621e-05
Iter: 3 loss: 5.04236232e-05
Iter: 4 loss: 4.13277194e-05
Iter: 5 loss: 3.61984166e-05
Iter: 6 loss: 0.000106198189
Iter: 7 loss: 3.61712591e-05
Iter: 8 loss: 3.23172499e-05
Iter: 9 loss: 3.49444199e-05
Iter: 10 loss: 2.99052499e-05
Iter: 11 loss: 2.793981e-05
Iter: 12 loss: 4.19933203e-05
Iter: 13 loss: 2.7770373e-05
Iter: 14 loss: 2.60901325e-05
Iter: 15 loss: 3.08432609e-05
Iter: 16 loss: 2.55515733e-05
Iter: 17 loss: 2.4101284e-05
Iter: 18 loss: 2.50084122e-05
Iter: 19 loss: 2.31772101e-05
Iter: 20 loss: 2.15659165e-05
Iter: 21 loss: 4.25084036e-05
Iter: 22 loss: 2.15548025e-05
Iter: 23 loss: 2.0793279e-05
Iter: 24 loss: 2.00779723e-05
Iter: 25 loss: 1.99019378e-05
Iter: 26 loss: 1.87641017e-05
Iter: 27 loss: 2.39521505e-05
Iter: 28 loss: 1.85472709e-05
Iter: 29 loss: 1.75707373e-05
Iter: 30 loss: 2.82012697e-05
Iter: 31 loss: 1.75486257e-05
Iter: 32 loss: 1.70067433e-05
Iter: 33 loss: 1.66818645e-05
Iter: 34 loss: 1.64575e-05
Iter: 35 loss: 1.5844822e-05
Iter: 36 loss: 2.17580928e-05
Iter: 37 loss: 1.58222792e-05
Iter: 38 loss: 1.53218643e-05
Iter: 39 loss: 1.66756181e-05
Iter: 40 loss: 1.5157595e-05
Iter: 41 loss: 1.47523115e-05
Iter: 42 loss: 1.49208417e-05
Iter: 43 loss: 1.4473505e-05
Iter: 44 loss: 1.40602324e-05
Iter: 45 loss: 1.56385686e-05
Iter: 46 loss: 1.39626363e-05
Iter: 47 loss: 1.36373546e-05
Iter: 48 loss: 1.36370963e-05
Iter: 49 loss: 1.34454613e-05
Iter: 50 loss: 1.31889665e-05
Iter: 51 loss: 1.31750912e-05
Iter: 52 loss: 1.28840202e-05
Iter: 53 loss: 1.69401956e-05
Iter: 54 loss: 1.28831589e-05
Iter: 55 loss: 1.26790546e-05
Iter: 56 loss: 1.26915293e-05
Iter: 57 loss: 1.25194e-05
Iter: 58 loss: 1.23176305e-05
Iter: 59 loss: 1.4355428e-05
Iter: 60 loss: 1.23115387e-05
Iter: 61 loss: 1.21428502e-05
Iter: 62 loss: 1.21327121e-05
Iter: 63 loss: 1.20048862e-05
Iter: 64 loss: 1.17962e-05
Iter: 65 loss: 1.1774895e-05
Iter: 66 loss: 1.1622702e-05
Iter: 67 loss: 1.14552695e-05
Iter: 68 loss: 1.14491449e-05
Iter: 69 loss: 1.12957732e-05
Iter: 70 loss: 1.13117403e-05
Iter: 71 loss: 1.11777972e-05
Iter: 72 loss: 1.10199362e-05
Iter: 73 loss: 1.12354246e-05
Iter: 74 loss: 1.09410876e-05
Iter: 75 loss: 1.08036984e-05
Iter: 76 loss: 1.28843267e-05
Iter: 77 loss: 1.08037229e-05
Iter: 78 loss: 1.07037704e-05
Iter: 79 loss: 1.0595405e-05
Iter: 80 loss: 1.05787585e-05
Iter: 81 loss: 1.04152914e-05
Iter: 82 loss: 1.07718051e-05
Iter: 83 loss: 1.03518178e-05
Iter: 84 loss: 1.02791828e-05
Iter: 85 loss: 1.02609747e-05
Iter: 86 loss: 1.01944024e-05
Iter: 87 loss: 1.01131445e-05
Iter: 88 loss: 1.01055448e-05
Iter: 89 loss: 1.00011384e-05
Iter: 90 loss: 1.05441377e-05
Iter: 91 loss: 9.98469113e-06
Iter: 92 loss: 9.87686417e-06
Iter: 93 loss: 1.02061977e-05
Iter: 94 loss: 9.84498183e-06
Iter: 95 loss: 9.76788e-06
Iter: 96 loss: 9.90348235e-06
Iter: 97 loss: 9.73375154e-06
Iter: 98 loss: 9.63759e-06
Iter: 99 loss: 1.00204143e-05
Iter: 100 loss: 9.61578553e-06
Iter: 101 loss: 9.54015923e-06
Iter: 102 loss: 9.50477079e-06
Iter: 103 loss: 9.46748e-06
Iter: 104 loss: 9.37444747e-06
Iter: 105 loss: 9.84764938e-06
Iter: 106 loss: 9.3589988e-06
Iter: 107 loss: 9.27864403e-06
Iter: 108 loss: 1.00141879e-05
Iter: 109 loss: 9.27470319e-06
Iter: 110 loss: 9.21647e-06
Iter: 111 loss: 9.14170778e-06
Iter: 112 loss: 9.13642361e-06
Iter: 113 loss: 9.06316745e-06
Iter: 114 loss: 9.628111e-06
Iter: 115 loss: 9.05776687e-06
Iter: 116 loss: 8.98770122e-06
Iter: 117 loss: 9.2727887e-06
Iter: 118 loss: 8.97229347e-06
Iter: 119 loss: 8.91143463e-06
Iter: 120 loss: 8.87130773e-06
Iter: 121 loss: 8.84800284e-06
Iter: 122 loss: 8.76257764e-06
Iter: 123 loss: 9.06129753e-06
Iter: 124 loss: 8.74050056e-06
Iter: 125 loss: 8.68953794e-06
Iter: 126 loss: 8.68758161e-06
Iter: 127 loss: 8.64201138e-06
Iter: 128 loss: 8.60668661e-06
Iter: 129 loss: 8.59233296e-06
Iter: 130 loss: 8.53436541e-06
Iter: 131 loss: 8.57549912e-06
Iter: 132 loss: 8.49842945e-06
Iter: 133 loss: 8.45165778e-06
Iter: 134 loss: 8.4494568e-06
Iter: 135 loss: 8.41605288e-06
Iter: 136 loss: 8.36733e-06
Iter: 137 loss: 8.36585605e-06
Iter: 138 loss: 8.31052694e-06
Iter: 139 loss: 8.52219455e-06
Iter: 140 loss: 8.29740384e-06
Iter: 141 loss: 8.24230847e-06
Iter: 142 loss: 8.63653077e-06
Iter: 143 loss: 8.23759365e-06
Iter: 144 loss: 8.19898287e-06
Iter: 145 loss: 8.14820669e-06
Iter: 146 loss: 8.1452e-06
Iter: 147 loss: 8.08057121e-06
Iter: 148 loss: 8.41732162e-06
Iter: 149 loss: 8.07035849e-06
Iter: 150 loss: 8.02017075e-06
Iter: 151 loss: 8.69231644e-06
Iter: 152 loss: 8.02003706e-06
Iter: 153 loss: 7.98867313e-06
Iter: 154 loss: 7.94820153e-06
Iter: 155 loss: 7.94547668e-06
Iter: 156 loss: 7.89606e-06
Iter: 157 loss: 8.20472269e-06
Iter: 158 loss: 7.89026763e-06
Iter: 159 loss: 7.84498752e-06
Iter: 160 loss: 8.0823047e-06
Iter: 161 loss: 7.83799351e-06
Iter: 162 loss: 7.80656592e-06
Iter: 163 loss: 7.79912625e-06
Iter: 164 loss: 7.77914647e-06
Iter: 165 loss: 7.74072396e-06
Iter: 166 loss: 8.32164915e-06
Iter: 167 loss: 7.74068758e-06
Iter: 168 loss: 7.70974111e-06
Iter: 169 loss: 7.68135e-06
Iter: 170 loss: 7.67384063e-06
Iter: 171 loss: 7.63360822e-06
Iter: 172 loss: 7.66553876e-06
Iter: 173 loss: 7.60922057e-06
Iter: 174 loss: 7.57160069e-06
Iter: 175 loss: 8.09591529e-06
Iter: 176 loss: 7.57149064e-06
Iter: 177 loss: 7.53431141e-06
Iter: 178 loss: 7.57391717e-06
Iter: 179 loss: 7.51375137e-06
Iter: 180 loss: 7.4833456e-06
Iter: 181 loss: 7.46189107e-06
Iter: 182 loss: 7.45122543e-06
Iter: 183 loss: 7.40600808e-06
Iter: 184 loss: 7.78083177e-06
Iter: 185 loss: 7.40318228e-06
Iter: 186 loss: 7.36345464e-06
Iter: 187 loss: 7.5330986e-06
Iter: 188 loss: 7.35526464e-06
Iter: 189 loss: 7.32565695e-06
Iter: 190 loss: 7.30239844e-06
Iter: 191 loss: 7.29336e-06
Iter: 192 loss: 7.25103291e-06
Iter: 193 loss: 7.47056947e-06
Iter: 194 loss: 7.24431447e-06
Iter: 195 loss: 7.21425977e-06
Iter: 196 loss: 7.4890977e-06
Iter: 197 loss: 7.21282731e-06
Iter: 198 loss: 7.18302454e-06
Iter: 199 loss: 7.19060108e-06
Iter: 200 loss: 7.16132627e-06
Iter: 201 loss: 7.13331065e-06
Iter: 202 loss: 7.36629772e-06
Iter: 203 loss: 7.13160262e-06
Iter: 204 loss: 7.10302174e-06
Iter: 205 loss: 7.13585177e-06
Iter: 206 loss: 7.08770131e-06
Iter: 207 loss: 7.06418177e-06
Iter: 208 loss: 7.07610934e-06
Iter: 209 loss: 7.04862896e-06
Iter: 210 loss: 7.01744648e-06
Iter: 211 loss: 7.28815439e-06
Iter: 212 loss: 7.01582121e-06
Iter: 213 loss: 6.99293241e-06
Iter: 214 loss: 6.9655589e-06
Iter: 215 loss: 6.96281131e-06
Iter: 216 loss: 6.93134916e-06
Iter: 217 loss: 7.10403174e-06
Iter: 218 loss: 6.92663434e-06
Iter: 219 loss: 6.89416e-06
Iter: 220 loss: 7.10657287e-06
Iter: 221 loss: 6.89074113e-06
Iter: 222 loss: 6.86801513e-06
Iter: 223 loss: 6.84435508e-06
Iter: 224 loss: 6.83996586e-06
Iter: 225 loss: 6.80727817e-06
Iter: 226 loss: 6.85113537e-06
Iter: 227 loss: 6.79050709e-06
Iter: 228 loss: 6.76016498e-06
Iter: 229 loss: 7.15655688e-06
Iter: 230 loss: 6.76002719e-06
Iter: 231 loss: 6.73393833e-06
Iter: 232 loss: 6.80049652e-06
Iter: 233 loss: 6.72509896e-06
Iter: 234 loss: 6.70104055e-06
Iter: 235 loss: 6.69568408e-06
Iter: 236 loss: 6.68019948e-06
Iter: 237 loss: 6.65051584e-06
Iter: 238 loss: 6.80561425e-06
Iter: 239 loss: 6.64581648e-06
Iter: 240 loss: 6.62323873e-06
Iter: 241 loss: 6.94807841e-06
Iter: 242 loss: 6.62317643e-06
Iter: 243 loss: 6.60961496e-06
Iter: 244 loss: 6.59647867e-06
Iter: 245 loss: 6.59368197e-06
Iter: 246 loss: 6.56518296e-06
Iter: 247 loss: 6.68496159e-06
Iter: 248 loss: 6.55926624e-06
Iter: 249 loss: 6.54101905e-06
Iter: 250 loss: 6.52481685e-06
Iter: 251 loss: 6.52020526e-06
Iter: 252 loss: 6.4885694e-06
Iter: 253 loss: 6.61206741e-06
Iter: 254 loss: 6.48133664e-06
Iter: 255 loss: 6.45767432e-06
Iter: 256 loss: 6.45767e-06
Iter: 257 loss: 6.44175589e-06
Iter: 258 loss: 6.41575434e-06
Iter: 259 loss: 6.41551969e-06
Iter: 260 loss: 6.39115842e-06
Iter: 261 loss: 6.60811293e-06
Iter: 262 loss: 6.38979373e-06
Iter: 263 loss: 6.36706409e-06
Iter: 264 loss: 6.4529936e-06
Iter: 265 loss: 6.36186132e-06
Iter: 266 loss: 6.34273e-06
Iter: 267 loss: 6.32834417e-06
Iter: 268 loss: 6.32192723e-06
Iter: 269 loss: 6.29607939e-06
Iter: 270 loss: 6.34745e-06
Iter: 271 loss: 6.28549e-06
Iter: 272 loss: 6.26205201e-06
Iter: 273 loss: 6.52900189e-06
Iter: 274 loss: 6.26146129e-06
Iter: 275 loss: 6.24078484e-06
Iter: 276 loss: 6.2863769e-06
Iter: 277 loss: 6.23258529e-06
Iter: 278 loss: 6.21506933e-06
Iter: 279 loss: 6.22946027e-06
Iter: 280 loss: 6.20459741e-06
Iter: 281 loss: 6.17939077e-06
Iter: 282 loss: 6.37688754e-06
Iter: 283 loss: 6.1776941e-06
Iter: 284 loss: 6.16313173e-06
Iter: 285 loss: 6.1429555e-06
Iter: 286 loss: 6.14222336e-06
Iter: 287 loss: 6.12296708e-06
Iter: 288 loss: 6.41389624e-06
Iter: 289 loss: 6.12288568e-06
Iter: 290 loss: 6.10719053e-06
Iter: 291 loss: 6.11036467e-06
Iter: 292 loss: 6.09551444e-06
Iter: 293 loss: 6.07440325e-06
Iter: 294 loss: 6.08614e-06
Iter: 295 loss: 6.06039976e-06
Iter: 296 loss: 6.04615116e-06
Iter: 297 loss: 6.04593743e-06
Iter: 298 loss: 6.03073659e-06
Iter: 299 loss: 6.02063938e-06
Iter: 300 loss: 6.01489592e-06
Iter: 301 loss: 5.99385112e-06
Iter: 302 loss: 6.00142539e-06
Iter: 303 loss: 5.97894e-06
Iter: 304 loss: 5.95499841e-06
Iter: 305 loss: 6.01910324e-06
Iter: 306 loss: 5.94707308e-06
Iter: 307 loss: 5.93089499e-06
Iter: 308 loss: 5.93061077e-06
Iter: 309 loss: 5.91610387e-06
Iter: 310 loss: 5.90180753e-06
Iter: 311 loss: 5.89863839e-06
Iter: 312 loss: 5.8789351e-06
Iter: 313 loss: 5.94973699e-06
Iter: 314 loss: 5.873801e-06
Iter: 315 loss: 5.85694e-06
Iter: 316 loss: 6.03678063e-06
Iter: 317 loss: 5.85655198e-06
Iter: 318 loss: 5.84209465e-06
Iter: 319 loss: 5.83583233e-06
Iter: 320 loss: 5.82846587e-06
Iter: 321 loss: 5.81093309e-06
Iter: 322 loss: 5.99867963e-06
Iter: 323 loss: 5.81049244e-06
Iter: 324 loss: 5.79750622e-06
Iter: 325 loss: 5.77704577e-06
Iter: 326 loss: 5.77664559e-06
Iter: 327 loss: 5.75494505e-06
Iter: 328 loss: 5.87133582e-06
Iter: 329 loss: 5.75164358e-06
Iter: 330 loss: 5.73486795e-06
Iter: 331 loss: 5.85304679e-06
Iter: 332 loss: 5.73326088e-06
Iter: 333 loss: 5.71657347e-06
Iter: 334 loss: 5.72677254e-06
Iter: 335 loss: 5.70591146e-06
Iter: 336 loss: 5.68973155e-06
Iter: 337 loss: 5.72754061e-06
Iter: 338 loss: 5.68403e-06
Iter: 339 loss: 5.66681592e-06
Iter: 340 loss: 5.8398191e-06
Iter: 341 loss: 5.66634617e-06
Iter: 342 loss: 5.65613936e-06
Iter: 343 loss: 5.63524372e-06
Iter: 344 loss: 6.01685451e-06
Iter: 345 loss: 5.6349304e-06
Iter: 346 loss: 5.61033e-06
Iter: 347 loss: 5.68752193e-06
Iter: 348 loss: 5.60322405e-06
Iter: 349 loss: 5.58400188e-06
Iter: 350 loss: 5.67566076e-06
Iter: 351 loss: 5.58058e-06
Iter: 352 loss: 5.56415307e-06
Iter: 353 loss: 5.70840575e-06
Iter: 354 loss: 5.56322902e-06
Iter: 355 loss: 5.54616463e-06
Iter: 356 loss: 5.56954501e-06
Iter: 357 loss: 5.53780728e-06
Iter: 358 loss: 5.52460187e-06
Iter: 359 loss: 5.59222462e-06
Iter: 360 loss: 5.52243682e-06
Iter: 361 loss: 5.50900313e-06
Iter: 362 loss: 5.51291851e-06
Iter: 363 loss: 5.4992588e-06
Iter: 364 loss: 5.48233493e-06
Iter: 365 loss: 5.47060881e-06
Iter: 366 loss: 5.46445881e-06
Iter: 367 loss: 5.44875456e-06
Iter: 368 loss: 5.44861859e-06
Iter: 369 loss: 5.43505e-06
Iter: 370 loss: 5.46104047e-06
Iter: 371 loss: 5.4293414e-06
Iter: 372 loss: 5.41505869e-06
Iter: 373 loss: 5.4129132e-06
Iter: 374 loss: 5.40296787e-06
Iter: 375 loss: 5.39037137e-06
Iter: 376 loss: 5.38981294e-06
Iter: 377 loss: 5.38123e-06
Iter: 378 loss: 5.36375501e-06
Iter: 379 loss: 5.68420819e-06
Iter: 380 loss: 5.36355128e-06
Iter: 381 loss: 5.34795345e-06
Iter: 382 loss: 5.52769234e-06
Iter: 383 loss: 5.34764058e-06
Iter: 384 loss: 5.33361708e-06
Iter: 385 loss: 5.37825872e-06
Iter: 386 loss: 5.32955346e-06
Iter: 387 loss: 5.31814203e-06
Iter: 388 loss: 5.3038e-06
Iter: 389 loss: 5.30260286e-06
Iter: 390 loss: 5.28369492e-06
Iter: 391 loss: 5.37823917e-06
Iter: 392 loss: 5.28064356e-06
Iter: 393 loss: 5.26777058e-06
Iter: 394 loss: 5.26770555e-06
Iter: 395 loss: 5.25788528e-06
Iter: 396 loss: 5.24382631e-06
Iter: 397 loss: 5.24330198e-06
Iter: 398 loss: 5.22634173e-06
Iter: 399 loss: 5.24186544e-06
Iter: 400 loss: 5.21646598e-06
Iter: 401 loss: 5.20215917e-06
Iter: 402 loss: 5.31667911e-06
Iter: 403 loss: 5.20128651e-06
Iter: 404 loss: 5.1871757e-06
Iter: 405 loss: 5.24425741e-06
Iter: 406 loss: 5.18392426e-06
Iter: 407 loss: 5.17032504e-06
Iter: 408 loss: 5.17636545e-06
Iter: 409 loss: 5.16102045e-06
Iter: 410 loss: 5.14962721e-06
Iter: 411 loss: 5.31324213e-06
Iter: 412 loss: 5.14955354e-06
Iter: 413 loss: 5.13899522e-06
Iter: 414 loss: 5.12518091e-06
Iter: 415 loss: 5.12431279e-06
Iter: 416 loss: 5.10947393e-06
Iter: 417 loss: 5.19924697e-06
Iter: 418 loss: 5.1077e-06
Iter: 419 loss: 5.09631855e-06
Iter: 420 loss: 5.18864817e-06
Iter: 421 loss: 5.09558049e-06
Iter: 422 loss: 5.08613812e-06
Iter: 423 loss: 5.07017421e-06
Iter: 424 loss: 5.07002687e-06
Iter: 425 loss: 5.05436083e-06
Iter: 426 loss: 5.18556681e-06
Iter: 427 loss: 5.0534527e-06
Iter: 428 loss: 5.04127183e-06
Iter: 429 loss: 5.13503028e-06
Iter: 430 loss: 5.04043601e-06
Iter: 431 loss: 5.03134106e-06
Iter: 432 loss: 5.02701914e-06
Iter: 433 loss: 5.02247167e-06
Iter: 434 loss: 5.00920305e-06
Iter: 435 loss: 5.15197098e-06
Iter: 436 loss: 5.0088538e-06
Iter: 437 loss: 5.00042097e-06
Iter: 438 loss: 4.98623422e-06
Iter: 439 loss: 4.98612098e-06
Iter: 440 loss: 4.97087785e-06
Iter: 441 loss: 5.00122678e-06
Iter: 442 loss: 4.96466237e-06
Iter: 443 loss: 4.94776577e-06
Iter: 444 loss: 4.99161251e-06
Iter: 445 loss: 4.94209689e-06
Iter: 446 loss: 4.93106654e-06
Iter: 447 loss: 4.93102789e-06
Iter: 448 loss: 4.92015806e-06
Iter: 449 loss: 4.93063635e-06
Iter: 450 loss: 4.91395895e-06
Iter: 451 loss: 4.90334742e-06
Iter: 452 loss: 4.92928e-06
Iter: 453 loss: 4.89953891e-06
Iter: 454 loss: 4.8859838e-06
Iter: 455 loss: 4.91856372e-06
Iter: 456 loss: 4.88108071e-06
Iter: 457 loss: 4.87015723e-06
Iter: 458 loss: 4.87595116e-06
Iter: 459 loss: 4.86295357e-06
Iter: 460 loss: 4.8509396e-06
Iter: 461 loss: 4.91369065e-06
Iter: 462 loss: 4.84902102e-06
Iter: 463 loss: 4.83696749e-06
Iter: 464 loss: 4.90104412e-06
Iter: 465 loss: 4.83508211e-06
Iter: 466 loss: 4.82626774e-06
Iter: 467 loss: 4.81959796e-06
Iter: 468 loss: 4.81657344e-06
Iter: 469 loss: 4.80675499e-06
Iter: 470 loss: 4.80665858e-06
Iter: 471 loss: 4.79904975e-06
Iter: 472 loss: 4.78341826e-06
Iter: 473 loss: 5.05673961e-06
Iter: 474 loss: 4.78318134e-06
Iter: 475 loss: 4.77106096e-06
Iter: 476 loss: 4.94152573e-06
Iter: 477 loss: 4.77098365e-06
Iter: 478 loss: 4.7595945e-06
Iter: 479 loss: 4.76384366e-06
Iter: 480 loss: 4.75173465e-06
Iter: 481 loss: 4.73828095e-06
Iter: 482 loss: 4.74885292e-06
Iter: 483 loss: 4.73028285e-06
Iter: 484 loss: 4.71655039e-06
Iter: 485 loss: 4.76186e-06
Iter: 486 loss: 4.71271505e-06
Iter: 487 loss: 4.70298119e-06
Iter: 488 loss: 4.70279792e-06
Iter: 489 loss: 4.69533734e-06
Iter: 490 loss: 4.68581948e-06
Iter: 491 loss: 4.68499184e-06
Iter: 492 loss: 4.67233713e-06
Iter: 493 loss: 4.70482291e-06
Iter: 494 loss: 4.66798383e-06
Iter: 495 loss: 4.65595667e-06
Iter: 496 loss: 4.77411413e-06
Iter: 497 loss: 4.65551329e-06
Iter: 498 loss: 4.64692403e-06
Iter: 499 loss: 4.64031427e-06
Iter: 500 loss: 4.63761626e-06
Iter: 501 loss: 4.62614389e-06
Iter: 502 loss: 4.7356034e-06
Iter: 503 loss: 4.62580374e-06
Iter: 504 loss: 4.61510353e-06
Iter: 505 loss: 4.63872948e-06
Iter: 506 loss: 4.61103673e-06
Iter: 507 loss: 4.60302635e-06
Iter: 508 loss: 4.62241815e-06
Iter: 509 loss: 4.6001237e-06
Iter: 510 loss: 4.58924e-06
Iter: 511 loss: 4.61124728e-06
Iter: 512 loss: 4.58474051e-06
Iter: 513 loss: 4.5763627e-06
Iter: 514 loss: 4.56755515e-06
Iter: 515 loss: 4.56605676e-06
Iter: 516 loss: 4.55499503e-06
Iter: 517 loss: 4.68794588e-06
Iter: 518 loss: 4.55485406e-06
Iter: 519 loss: 4.54473138e-06
Iter: 520 loss: 4.56864927e-06
Iter: 521 loss: 4.54105293e-06
Iter: 522 loss: 4.53146595e-06
Iter: 523 loss: 4.52554195e-06
Iter: 524 loss: 4.52164159e-06
Iter: 525 loss: 4.5086731e-06
Iter: 526 loss: 4.5796819e-06
Iter: 527 loss: 4.50669404e-06
Iter: 528 loss: 4.49406571e-06
Iter: 529 loss: 4.59792045e-06
Iter: 530 loss: 4.49331083e-06
Iter: 531 loss: 4.48483479e-06
Iter: 532 loss: 4.48156788e-06
Iter: 533 loss: 4.476919e-06
Iter: 534 loss: 4.4667222e-06
Iter: 535 loss: 4.46040576e-06
Iter: 536 loss: 4.45624e-06
Iter: 537 loss: 4.45194428e-06
Iter: 538 loss: 4.44834677e-06
Iter: 539 loss: 4.44223951e-06
Iter: 540 loss: 4.43738418e-06
Iter: 541 loss: 4.43555291e-06
Iter: 542 loss: 4.42591818e-06
Iter: 543 loss: 4.47707907e-06
Iter: 544 loss: 4.42453256e-06
Iter: 545 loss: 4.41420661e-06
Iter: 546 loss: 4.42607916e-06
Iter: 547 loss: 4.40876829e-06
Iter: 548 loss: 4.40076292e-06
Iter: 549 loss: 4.42580085e-06
Iter: 550 loss: 4.39846e-06
Iter: 551 loss: 4.38906818e-06
Iter: 552 loss: 4.41201337e-06
Iter: 553 loss: 4.3856985e-06
Iter: 554 loss: 4.37696144e-06
Iter: 555 loss: 4.36852861e-06
Iter: 556 loss: 4.36664504e-06
Iter: 557 loss: 4.35469792e-06
Iter: 558 loss: 4.41047723e-06
Iter: 559 loss: 4.35248467e-06
Iter: 560 loss: 4.3422524e-06
Iter: 561 loss: 4.42666851e-06
Iter: 562 loss: 4.34160484e-06
Iter: 563 loss: 4.3324153e-06
Iter: 564 loss: 4.36059781e-06
Iter: 565 loss: 4.32984143e-06
Iter: 566 loss: 4.32235538e-06
Iter: 567 loss: 4.32994239e-06
Iter: 568 loss: 4.3182381e-06
Iter: 569 loss: 4.30849923e-06
Iter: 570 loss: 4.36549271e-06
Iter: 571 loss: 4.30718319e-06
Iter: 572 loss: 4.29956117e-06
Iter: 573 loss: 4.28737e-06
Iter: 574 loss: 4.28719932e-06
Iter: 575 loss: 4.27433042e-06
Iter: 576 loss: 4.34529193e-06
Iter: 577 loss: 4.27261557e-06
Iter: 578 loss: 4.26529414e-06
Iter: 579 loss: 4.26514816e-06
Iter: 580 loss: 4.25885264e-06
Iter: 581 loss: 4.24816471e-06
Iter: 582 loss: 4.24808877e-06
Iter: 583 loss: 4.23986876e-06
Iter: 584 loss: 4.23991514e-06
Iter: 585 loss: 4.23311e-06
Iter: 586 loss: 4.23743859e-06
Iter: 587 loss: 4.2288857e-06
Iter: 588 loss: 4.22075709e-06
Iter: 589 loss: 4.21279719e-06
Iter: 590 loss: 4.2110978e-06
Iter: 591 loss: 4.19995058e-06
Iter: 592 loss: 4.29904685e-06
Iter: 593 loss: 4.19955768e-06
Iter: 594 loss: 4.1899093e-06
Iter: 595 loss: 4.23895199e-06
Iter: 596 loss: 4.18825948e-06
Iter: 597 loss: 4.1803778e-06
Iter: 598 loss: 4.17317733e-06
Iter: 599 loss: 4.17115825e-06
Iter: 600 loss: 4.160755e-06
Iter: 601 loss: 4.28243084e-06
Iter: 602 loss: 4.16054445e-06
Iter: 603 loss: 4.15155409e-06
Iter: 604 loss: 4.1857279e-06
Iter: 605 loss: 4.14943452e-06
Iter: 606 loss: 4.14326314e-06
Iter: 607 loss: 4.1335943e-06
Iter: 608 loss: 4.133537e-06
Iter: 609 loss: 4.12456939e-06
Iter: 610 loss: 4.12443387e-06
Iter: 611 loss: 4.11782048e-06
Iter: 612 loss: 4.11829433e-06
Iter: 613 loss: 4.1125968e-06
Iter: 614 loss: 4.10497069e-06
Iter: 615 loss: 4.12818417e-06
Iter: 616 loss: 4.10274652e-06
Iter: 617 loss: 4.09246468e-06
Iter: 618 loss: 4.12948157e-06
Iter: 619 loss: 4.08985488e-06
Iter: 620 loss: 4.08377446e-06
Iter: 621 loss: 4.0769869e-06
Iter: 622 loss: 4.07616972e-06
Iter: 623 loss: 4.06775689e-06
Iter: 624 loss: 4.18397667e-06
Iter: 625 loss: 4.06769232e-06
Iter: 626 loss: 4.0600039e-06
Iter: 627 loss: 4.06351e-06
Iter: 628 loss: 4.0546247e-06
Iter: 629 loss: 4.04635875e-06
Iter: 630 loss: 4.04782531e-06
Iter: 631 loss: 4.04013281e-06
Iter: 632 loss: 4.02890964e-06
Iter: 633 loss: 4.04455523e-06
Iter: 634 loss: 4.02326259e-06
Iter: 635 loss: 4.01490752e-06
Iter: 636 loss: 4.01483658e-06
Iter: 637 loss: 4.00712815e-06
Iter: 638 loss: 4.0255336e-06
Iter: 639 loss: 4.00440922e-06
Iter: 640 loss: 3.99773762e-06
Iter: 641 loss: 3.99387636e-06
Iter: 642 loss: 3.99109103e-06
Iter: 643 loss: 3.98399334e-06
Iter: 644 loss: 4.09693166e-06
Iter: 645 loss: 3.98398379e-06
Iter: 646 loss: 3.9778397e-06
Iter: 647 loss: 3.97239955e-06
Iter: 648 loss: 3.97065151e-06
Iter: 649 loss: 3.9628876e-06
Iter: 650 loss: 4.01944271e-06
Iter: 651 loss: 3.9622164e-06
Iter: 652 loss: 3.95465668e-06
Iter: 653 loss: 3.99043711e-06
Iter: 654 loss: 3.95330244e-06
Iter: 655 loss: 3.94779863e-06
Iter: 656 loss: 3.94218659e-06
Iter: 657 loss: 3.94112658e-06
Iter: 658 loss: 3.93265236e-06
Iter: 659 loss: 3.98954671e-06
Iter: 660 loss: 3.93168875e-06
Iter: 661 loss: 3.92317497e-06
Iter: 662 loss: 3.93971959e-06
Iter: 663 loss: 3.91962931e-06
Iter: 664 loss: 3.9130482e-06
Iter: 665 loss: 3.92077345e-06
Iter: 666 loss: 3.90948026e-06
Iter: 667 loss: 3.90128844e-06
Iter: 668 loss: 3.96254836e-06
Iter: 669 loss: 3.90063815e-06
Iter: 670 loss: 3.89457318e-06
Iter: 671 loss: 3.89042634e-06
Iter: 672 loss: 3.88837771e-06
Iter: 673 loss: 3.87925502e-06
Iter: 674 loss: 3.88517947e-06
Iter: 675 loss: 3.87347427e-06
Iter: 676 loss: 3.86526108e-06
Iter: 677 loss: 3.8652106e-06
Iter: 678 loss: 3.85697513e-06
Iter: 679 loss: 3.86659713e-06
Iter: 680 loss: 3.85251315e-06
Iter: 681 loss: 3.84627901e-06
Iter: 682 loss: 3.8404296e-06
Iter: 683 loss: 3.83892166e-06
Iter: 684 loss: 3.82921098e-06
Iter: 685 loss: 3.9065244e-06
Iter: 686 loss: 3.82857434e-06
Iter: 687 loss: 3.81983864e-06
Iter: 688 loss: 3.89145544e-06
Iter: 689 loss: 3.81923564e-06
Iter: 690 loss: 3.81466225e-06
Iter: 691 loss: 3.80895699e-06
Iter: 692 loss: 3.8084313e-06
Iter: 693 loss: 3.79997255e-06
Iter: 694 loss: 3.85990279e-06
Iter: 695 loss: 3.79919538e-06
Iter: 696 loss: 3.791273e-06
Iter: 697 loss: 3.79861012e-06
Iter: 698 loss: 3.7868781e-06
Iter: 699 loss: 3.77976767e-06
Iter: 700 loss: 3.78851064e-06
Iter: 701 loss: 3.77607284e-06
Iter: 702 loss: 3.76895264e-06
Iter: 703 loss: 3.86609781e-06
Iter: 704 loss: 3.76894423e-06
Iter: 705 loss: 3.76371372e-06
Iter: 706 loss: 3.75492709e-06
Iter: 707 loss: 3.75496757e-06
Iter: 708 loss: 3.74508636e-06
Iter: 709 loss: 3.77923698e-06
Iter: 710 loss: 3.7424486e-06
Iter: 711 loss: 3.73535545e-06
Iter: 712 loss: 3.81917744e-06
Iter: 713 loss: 3.73523835e-06
Iter: 714 loss: 3.72827503e-06
Iter: 715 loss: 3.73744251e-06
Iter: 716 loss: 3.72477211e-06
Iter: 717 loss: 3.71918236e-06
Iter: 718 loss: 3.74119645e-06
Iter: 719 loss: 3.71786859e-06
Iter: 720 loss: 3.71150372e-06
Iter: 721 loss: 3.72162503e-06
Iter: 722 loss: 3.70849466e-06
Iter: 723 loss: 3.70260273e-06
Iter: 724 loss: 3.69827694e-06
Iter: 725 loss: 3.69607869e-06
Iter: 726 loss: 3.6900858e-06
Iter: 727 loss: 3.68978885e-06
Iter: 728 loss: 3.68429141e-06
Iter: 729 loss: 3.68152541e-06
Iter: 730 loss: 3.67882694e-06
Iter: 731 loss: 3.67211896e-06
Iter: 732 loss: 3.66743461e-06
Iter: 733 loss: 3.66496852e-06
Iter: 734 loss: 3.65536471e-06
Iter: 735 loss: 3.73939474e-06
Iter: 736 loss: 3.65489041e-06
Iter: 737 loss: 3.64872903e-06
Iter: 738 loss: 3.71694523e-06
Iter: 739 loss: 3.64861444e-06
Iter: 740 loss: 3.64331845e-06
Iter: 741 loss: 3.63501431e-06
Iter: 742 loss: 3.63493109e-06
Iter: 743 loss: 3.62624633e-06
Iter: 744 loss: 3.66176687e-06
Iter: 745 loss: 3.62429751e-06
Iter: 746 loss: 3.61726552e-06
Iter: 747 loss: 3.71167926e-06
Iter: 748 loss: 3.61725688e-06
Iter: 749 loss: 3.61280877e-06
Iter: 750 loss: 3.60753165e-06
Iter: 751 loss: 3.60695071e-06
Iter: 752 loss: 3.59972796e-06
Iter: 753 loss: 3.65671212e-06
Iter: 754 loss: 3.5992316e-06
Iter: 755 loss: 3.5922983e-06
Iter: 756 loss: 3.61440516e-06
Iter: 757 loss: 3.59029764e-06
Iter: 758 loss: 3.58503053e-06
Iter: 759 loss: 3.57807266e-06
Iter: 760 loss: 3.57772069e-06
Iter: 761 loss: 3.56992314e-06
Iter: 762 loss: 3.67687903e-06
Iter: 763 loss: 3.5698622e-06
Iter: 764 loss: 3.56281453e-06
Iter: 765 loss: 3.58551392e-06
Iter: 766 loss: 3.56079454e-06
Iter: 767 loss: 3.55618863e-06
Iter: 768 loss: 3.55367501e-06
Iter: 769 loss: 3.55165253e-06
Iter: 770 loss: 3.54378631e-06
Iter: 771 loss: 3.59519026e-06
Iter: 772 loss: 3.54284589e-06
Iter: 773 loss: 3.53710584e-06
Iter: 774 loss: 3.53319115e-06
Iter: 775 loss: 3.53108271e-06
Iter: 776 loss: 3.52230199e-06
Iter: 777 loss: 3.5416233e-06
Iter: 778 loss: 3.51903509e-06
Iter: 779 loss: 3.51210247e-06
Iter: 780 loss: 3.57775161e-06
Iter: 781 loss: 3.51177755e-06
Iter: 782 loss: 3.50433857e-06
Iter: 783 loss: 3.51800281e-06
Iter: 784 loss: 3.50108712e-06
Iter: 785 loss: 3.49553716e-06
Iter: 786 loss: 3.49221773e-06
Iter: 787 loss: 3.48989784e-06
Iter: 788 loss: 3.48307799e-06
Iter: 789 loss: 3.55438124e-06
Iter: 790 loss: 3.48286153e-06
Iter: 791 loss: 3.47567493e-06
Iter: 792 loss: 3.4938289e-06
Iter: 793 loss: 3.47315154e-06
Iter: 794 loss: 3.4675902e-06
Iter: 795 loss: 3.46643242e-06
Iter: 796 loss: 3.46282445e-06
Iter: 797 loss: 3.4556e-06
Iter: 798 loss: 3.53805171e-06
Iter: 799 loss: 3.45543981e-06
Iter: 800 loss: 3.45023295e-06
Iter: 801 loss: 3.45124749e-06
Iter: 802 loss: 3.44624095e-06
Iter: 803 loss: 3.44081127e-06
Iter: 804 loss: 3.50399864e-06
Iter: 805 loss: 3.44071259e-06
Iter: 806 loss: 3.43639113e-06
Iter: 807 loss: 3.4332088e-06
Iter: 808 loss: 3.43168244e-06
Iter: 809 loss: 3.42526687e-06
Iter: 810 loss: 3.4223076e-06
Iter: 811 loss: 3.41912278e-06
Iter: 812 loss: 3.41152418e-06
Iter: 813 loss: 3.49942388e-06
Iter: 814 loss: 3.41137184e-06
Iter: 815 loss: 3.40475844e-06
Iter: 816 loss: 3.42897511e-06
Iter: 817 loss: 3.40316706e-06
Iter: 818 loss: 3.39762414e-06
Iter: 819 loss: 3.39649455e-06
Iter: 820 loss: 3.3927779e-06
Iter: 821 loss: 3.38595305e-06
Iter: 822 loss: 3.40515089e-06
Iter: 823 loss: 3.38368818e-06
Iter: 824 loss: 3.37799543e-06
Iter: 825 loss: 3.45185e-06
Iter: 826 loss: 3.377947e-06
Iter: 827 loss: 3.37245797e-06
Iter: 828 loss: 3.37091114e-06
Iter: 829 loss: 3.36747416e-06
Iter: 830 loss: 3.36099561e-06
Iter: 831 loss: 3.37433585e-06
Iter: 832 loss: 3.35841924e-06
Iter: 833 loss: 3.35225786e-06
Iter: 834 loss: 3.43145985e-06
Iter: 835 loss: 3.35221762e-06
Iter: 836 loss: 3.34792526e-06
Iter: 837 loss: 3.34238075e-06
Iter: 838 loss: 3.34186598e-06
Iter: 839 loss: 3.33598132e-06
Iter: 840 loss: 3.39706912e-06
Iter: 841 loss: 3.33587423e-06
Iter: 842 loss: 3.33010144e-06
Iter: 843 loss: 3.34525566e-06
Iter: 844 loss: 3.32828517e-06
Iter: 845 loss: 3.32386753e-06
Iter: 846 loss: 3.31936508e-06
Iter: 847 loss: 3.3184715e-06
Iter: 848 loss: 3.31233582e-06
Iter: 849 loss: 3.40850647e-06
Iter: 850 loss: 3.31233787e-06
Iter: 851 loss: 3.30771763e-06
Iter: 852 loss: 3.30019702e-06
Iter: 853 loss: 3.30019134e-06
Iter: 854 loss: 3.2916073e-06
Iter: 855 loss: 3.30639796e-06
Iter: 856 loss: 3.28778538e-06
Iter: 857 loss: 3.2815442e-06
Iter: 858 loss: 3.28140754e-06
Iter: 859 loss: 3.27666066e-06
Iter: 860 loss: 3.28511987e-06
Iter: 861 loss: 3.27441649e-06
Iter: 862 loss: 3.26920281e-06
Iter: 863 loss: 3.26175018e-06
Iter: 864 loss: 3.26150712e-06
Iter: 865 loss: 3.25556243e-06
Iter: 866 loss: 3.25543283e-06
Iter: 867 loss: 3.25029828e-06
Iter: 868 loss: 3.2688456e-06
Iter: 869 loss: 3.24896769e-06
Iter: 870 loss: 3.24509097e-06
Iter: 871 loss: 3.23882182e-06
Iter: 872 loss: 3.23878476e-06
Iter: 873 loss: 3.23260338e-06
Iter: 874 loss: 3.23255767e-06
Iter: 875 loss: 3.22768778e-06
Iter: 876 loss: 3.22847586e-06
Iter: 877 loss: 3.22400479e-06
Iter: 878 loss: 3.21913058e-06
Iter: 879 loss: 3.23615313e-06
Iter: 880 loss: 3.21785092e-06
Iter: 881 loss: 3.21123639e-06
Iter: 882 loss: 3.21701691e-06
Iter: 883 loss: 3.20742447e-06
Iter: 884 loss: 3.2019002e-06
Iter: 885 loss: 3.20728532e-06
Iter: 886 loss: 3.19872061e-06
Iter: 887 loss: 3.19285232e-06
Iter: 888 loss: 3.21631796e-06
Iter: 889 loss: 3.19158607e-06
Iter: 890 loss: 3.18456932e-06
Iter: 891 loss: 3.21099469e-06
Iter: 892 loss: 3.18302637e-06
Iter: 893 loss: 3.17811714e-06
Iter: 894 loss: 3.17587728e-06
Iter: 895 loss: 3.17346917e-06
Iter: 896 loss: 3.16625824e-06
Iter: 897 loss: 3.18969023e-06
Iter: 898 loss: 3.1641739e-06
Iter: 899 loss: 3.15809098e-06
Iter: 900 loss: 3.23360109e-06
Iter: 901 loss: 3.15815441e-06
Iter: 902 loss: 3.15384705e-06
Iter: 903 loss: 3.15382431e-06
Iter: 904 loss: 3.15043599e-06
Iter: 905 loss: 3.1445843e-06
Iter: 906 loss: 3.15278976e-06
Iter: 907 loss: 3.1417444e-06
Iter: 908 loss: 3.13703413e-06
Iter: 909 loss: 3.13696478e-06
Iter: 910 loss: 3.13396549e-06
Iter: 911 loss: 3.12785278e-06
Iter: 912 loss: 3.23266113e-06
Iter: 913 loss: 3.12777047e-06
Iter: 914 loss: 3.12208158e-06
Iter: 915 loss: 3.20257777e-06
Iter: 916 loss: 3.12206635e-06
Iter: 917 loss: 3.11660961e-06
Iter: 918 loss: 3.12026123e-06
Iter: 919 loss: 3.11311123e-06
Iter: 920 loss: 3.1084378e-06
Iter: 921 loss: 3.10234054e-06
Iter: 922 loss: 3.1019681e-06
Iter: 923 loss: 3.09405141e-06
Iter: 924 loss: 3.171358e-06
Iter: 925 loss: 3.09379811e-06
Iter: 926 loss: 3.08799e-06
Iter: 927 loss: 3.13900682e-06
Iter: 928 loss: 3.08771223e-06
Iter: 929 loss: 3.08393328e-06
Iter: 930 loss: 3.08063318e-06
Iter: 931 loss: 3.07960318e-06
Iter: 932 loss: 3.07453251e-06
Iter: 933 loss: 3.12084399e-06
Iter: 934 loss: 3.07428672e-06
Iter: 935 loss: 3.0689007e-06
Iter: 936 loss: 3.06856646e-06
Iter: 937 loss: 3.06454922e-06
Iter: 938 loss: 3.05953e-06
Iter: 939 loss: 3.07111532e-06
Iter: 940 loss: 3.05764115e-06
Iter: 941 loss: 3.05207845e-06
Iter: 942 loss: 3.07262872e-06
Iter: 943 loss: 3.05079902e-06
Iter: 944 loss: 3.04411788e-06
Iter: 945 loss: 3.06289394e-06
Iter: 946 loss: 3.04202558e-06
Iter: 947 loss: 3.03753677e-06
Iter: 948 loss: 3.05210983e-06
Iter: 949 loss: 3.03624074e-06
Iter: 950 loss: 3.03111619e-06
Iter: 951 loss: 3.05498247e-06
Iter: 952 loss: 3.03012439e-06
Iter: 953 loss: 3.02647845e-06
Iter: 954 loss: 3.02241824e-06
Iter: 955 loss: 3.0218373e-06
Iter: 956 loss: 3.01696741e-06
Iter: 957 loss: 3.0881381e-06
Iter: 958 loss: 3.01696969e-06
Iter: 959 loss: 3.01264618e-06
Iter: 960 loss: 3.01438808e-06
Iter: 961 loss: 3.00952297e-06
Iter: 962 loss: 3.00537113e-06
Iter: 963 loss: 2.99928365e-06
Iter: 964 loss: 2.9990556e-06
Iter: 965 loss: 2.99230896e-06
Iter: 966 loss: 3.05071535e-06
Iter: 967 loss: 2.99192016e-06
Iter: 968 loss: 2.98602095e-06
Iter: 969 loss: 3.00359511e-06
Iter: 970 loss: 2.9842306e-06
Iter: 971 loss: 2.97756856e-06
Iter: 972 loss: 3.01581281e-06
Iter: 973 loss: 2.976627e-06
Iter: 974 loss: 2.97196425e-06
Iter: 975 loss: 2.9718251e-06
Iter: 976 loss: 2.96834401e-06
Iter: 977 loss: 2.96334451e-06
Iter: 978 loss: 3.01347018e-06
Iter: 979 loss: 2.96316125e-06
Iter: 980 loss: 2.95820746e-06
Iter: 981 loss: 2.95841573e-06
Iter: 982 loss: 2.95433301e-06
Iter: 983 loss: 2.94972733e-06
Iter: 984 loss: 2.96141661e-06
Iter: 985 loss: 2.94820211e-06
Iter: 986 loss: 2.94390202e-06
Iter: 987 loss: 2.99844805e-06
Iter: 988 loss: 2.94384927e-06
Iter: 989 loss: 2.94061829e-06
Iter: 990 loss: 2.93615039e-06
Iter: 991 loss: 2.9359403e-06
Iter: 992 loss: 2.93082621e-06
Iter: 993 loss: 2.99490398e-06
Iter: 994 loss: 2.93073867e-06
Iter: 995 loss: 2.9271805e-06
Iter: 996 loss: 2.92423488e-06
Iter: 997 loss: 2.92318782e-06
Iter: 998 loss: 2.91849665e-06
Iter: 999 loss: 2.96413668e-06
Iter: 1000 loss: 2.91842389e-06
Iter: 1001 loss: 2.9137384e-06
Iter: 1002 loss: 2.91710171e-06
Iter: 1003 loss: 2.91094352e-06
Iter: 1004 loss: 2.9066041e-06
Iter: 1005 loss: 2.90361868e-06
Iter: 1006 loss: 2.90204298e-06
Iter: 1007 loss: 2.89562604e-06
Iter: 1008 loss: 2.91273136e-06
Iter: 1009 loss: 2.89345644e-06
Iter: 1010 loss: 2.88789852e-06
Iter: 1011 loss: 2.94113966e-06
Iter: 1012 loss: 2.88770707e-06
Iter: 1013 loss: 2.88294905e-06
Iter: 1014 loss: 2.9109242e-06
Iter: 1015 loss: 2.88241608e-06
Iter: 1016 loss: 2.87872217e-06
Iter: 1017 loss: 2.87501371e-06
Iter: 1018 loss: 2.87426383e-06
Iter: 1019 loss: 2.8698405e-06
Iter: 1020 loss: 2.93434641e-06
Iter: 1021 loss: 2.86983823e-06
Iter: 1022 loss: 2.86591012e-06
Iter: 1023 loss: 2.86806e-06
Iter: 1024 loss: 2.86328668e-06
Iter: 1025 loss: 2.8595382e-06
Iter: 1026 loss: 2.87933244e-06
Iter: 1027 loss: 2.85900524e-06
Iter: 1028 loss: 2.8548543e-06
Iter: 1029 loss: 2.86101886e-06
Iter: 1030 loss: 2.85277883e-06
Iter: 1031 loss: 2.84888347e-06
Iter: 1032 loss: 2.84514726e-06
Iter: 1033 loss: 2.84426687e-06
Iter: 1034 loss: 2.84018961e-06
Iter: 1035 loss: 2.84001317e-06
Iter: 1036 loss: 2.83659301e-06
Iter: 1037 loss: 2.83273243e-06
Iter: 1038 loss: 2.83228246e-06
Iter: 1039 loss: 2.82690576e-06
Iter: 1040 loss: 2.87704279e-06
Iter: 1041 loss: 2.82671704e-06
Iter: 1042 loss: 2.82272572e-06
Iter: 1043 loss: 2.82742712e-06
Iter: 1044 loss: 2.8206241e-06
Iter: 1045 loss: 2.81641314e-06
Iter: 1046 loss: 2.814661e-06
Iter: 1047 loss: 2.8124e-06
Iter: 1048 loss: 2.80672793e-06
Iter: 1049 loss: 2.81633675e-06
Iter: 1050 loss: 2.80411109e-06
Iter: 1051 loss: 2.79767846e-06
Iter: 1052 loss: 2.82638689e-06
Iter: 1053 loss: 2.79646406e-06
Iter: 1054 loss: 2.7912597e-06
Iter: 1055 loss: 2.81225243e-06
Iter: 1056 loss: 2.78999983e-06
Iter: 1057 loss: 2.78520702e-06
Iter: 1058 loss: 2.83739291e-06
Iter: 1059 loss: 2.78511743e-06
Iter: 1060 loss: 2.78219727e-06
Iter: 1061 loss: 2.77900699e-06
Iter: 1062 loss: 2.77860681e-06
Iter: 1063 loss: 2.77493496e-06
Iter: 1064 loss: 2.77486379e-06
Iter: 1065 loss: 2.77220761e-06
Iter: 1066 loss: 2.7688543e-06
Iter: 1067 loss: 2.76853052e-06
Iter: 1068 loss: 2.76454e-06
Iter: 1069 loss: 2.80296035e-06
Iter: 1070 loss: 2.76434412e-06
Iter: 1071 loss: 2.76030823e-06
Iter: 1072 loss: 2.75670914e-06
Iter: 1073 loss: 2.75570619e-06
Iter: 1074 loss: 2.75094e-06
Iter: 1075 loss: 2.76852e-06
Iter: 1076 loss: 2.74973e-06
Iter: 1077 loss: 2.74450213e-06
Iter: 1078 loss: 2.77556956e-06
Iter: 1079 loss: 2.74390209e-06
Iter: 1080 loss: 2.73986097e-06
Iter: 1081 loss: 2.74344347e-06
Iter: 1082 loss: 2.73758724e-06
Iter: 1083 loss: 2.73388264e-06
Iter: 1084 loss: 2.77080312e-06
Iter: 1085 loss: 2.73372598e-06
Iter: 1086 loss: 2.73079513e-06
Iter: 1087 loss: 2.72802254e-06
Iter: 1088 loss: 2.72734269e-06
Iter: 1089 loss: 2.72279954e-06
Iter: 1090 loss: 2.72320517e-06
Iter: 1091 loss: 2.71935573e-06
Iter: 1092 loss: 2.71326053e-06
Iter: 1093 loss: 2.73502565e-06
Iter: 1094 loss: 2.71165641e-06
Iter: 1095 loss: 2.7070023e-06
Iter: 1096 loss: 2.75843513e-06
Iter: 1097 loss: 2.70692635e-06
Iter: 1098 loss: 2.70252326e-06
Iter: 1099 loss: 2.71336739e-06
Iter: 1100 loss: 2.70081159e-06
Iter: 1101 loss: 2.69712746e-06
Iter: 1102 loss: 2.70348323e-06
Iter: 1103 loss: 2.69551947e-06
Iter: 1104 loss: 2.69155544e-06
Iter: 1105 loss: 2.72653733e-06
Iter: 1106 loss: 2.6914031e-06
Iter: 1107 loss: 2.68856093e-06
Iter: 1108 loss: 2.6864509e-06
Iter: 1109 loss: 2.68555323e-06
Iter: 1110 loss: 2.68188523e-06
Iter: 1111 loss: 2.72217858e-06
Iter: 1112 loss: 2.68174972e-06
Iter: 1113 loss: 2.67876908e-06
Iter: 1114 loss: 2.67571909e-06
Iter: 1115 loss: 2.67508449e-06
Iter: 1116 loss: 2.67027144e-06
Iter: 1117 loss: 2.67828909e-06
Iter: 1118 loss: 2.66816846e-06
Iter: 1119 loss: 2.6635239e-06
Iter: 1120 loss: 2.72785087e-06
Iter: 1121 loss: 2.66348411e-06
Iter: 1122 loss: 2.66040661e-06
Iter: 1123 loss: 2.66268012e-06
Iter: 1124 loss: 2.65852532e-06
Iter: 1125 loss: 2.65524432e-06
Iter: 1126 loss: 2.67127234e-06
Iter: 1127 loss: 2.65470499e-06
Iter: 1128 loss: 2.6511334e-06
Iter: 1129 loss: 2.6507746e-06
Iter: 1130 loss: 2.64813525e-06
Iter: 1131 loss: 2.64385039e-06
Iter: 1132 loss: 2.63976767e-06
Iter: 1133 loss: 2.63884294e-06
Iter: 1134 loss: 2.63199763e-06
Iter: 1135 loss: 2.66499183e-06
Iter: 1136 loss: 2.6307921e-06
Iter: 1137 loss: 2.62719868e-06
Iter: 1138 loss: 2.62705294e-06
Iter: 1139 loss: 2.62374192e-06
Iter: 1140 loss: 2.62317963e-06
Iter: 1141 loss: 2.6208545e-06
Iter: 1142 loss: 2.61728e-06
Iter: 1143 loss: 2.63959123e-06
Iter: 1144 loss: 2.61689593e-06
Iter: 1145 loss: 2.61282162e-06
Iter: 1146 loss: 2.61725427e-06
Iter: 1147 loss: 2.61072137e-06
Iter: 1148 loss: 2.60698926e-06
Iter: 1149 loss: 2.61681589e-06
Iter: 1150 loss: 2.60570482e-06
Iter: 1151 loss: 2.60202069e-06
Iter: 1152 loss: 2.62404819e-06
Iter: 1153 loss: 2.60159982e-06
Iter: 1154 loss: 2.59870626e-06
Iter: 1155 loss: 2.59480976e-06
Iter: 1156 loss: 2.59462331e-06
Iter: 1157 loss: 2.59014632e-06
Iter: 1158 loss: 2.62749973e-06
Iter: 1159 loss: 2.58986893e-06
Iter: 1160 loss: 2.58585942e-06
Iter: 1161 loss: 2.60396746e-06
Iter: 1162 loss: 2.58500677e-06
Iter: 1163 loss: 2.58188948e-06
Iter: 1164 loss: 2.58299474e-06
Iter: 1165 loss: 2.57961369e-06
Iter: 1166 loss: 2.5754166e-06
Iter: 1167 loss: 2.60443358e-06
Iter: 1168 loss: 2.57510192e-06
Iter: 1169 loss: 2.57195825e-06
Iter: 1170 loss: 2.56947123e-06
Iter: 1171 loss: 2.56849421e-06
Iter: 1172 loss: 2.56411658e-06
Iter: 1173 loss: 2.57330748e-06
Iter: 1174 loss: 2.56239218e-06
Iter: 1175 loss: 2.55784971e-06
Iter: 1176 loss: 2.56847e-06
Iter: 1177 loss: 2.55622217e-06
Iter: 1178 loss: 2.55260193e-06
Iter: 1179 loss: 2.55260193e-06
Iter: 1180 loss: 2.54956876e-06
Iter: 1181 loss: 2.54744282e-06
Iter: 1182 loss: 2.54640736e-06
Iter: 1183 loss: 2.54279621e-06
Iter: 1184 loss: 2.58487489e-06
Iter: 1185 loss: 2.54274937e-06
Iter: 1186 loss: 2.539462e-06
Iter: 1187 loss: 2.53879148e-06
Iter: 1188 loss: 2.5366603e-06
Iter: 1189 loss: 2.53311282e-06
Iter: 1190 loss: 2.54877796e-06
Iter: 1191 loss: 2.53239796e-06
Iter: 1192 loss: 2.52851464e-06
Iter: 1193 loss: 2.53849794e-06
Iter: 1194 loss: 2.52730842e-06
Iter: 1195 loss: 2.52398422e-06
Iter: 1196 loss: 2.5237382e-06
Iter: 1197 loss: 2.52125415e-06
Iter: 1198 loss: 2.51780102e-06
Iter: 1199 loss: 2.55451369e-06
Iter: 1200 loss: 2.51769052e-06
Iter: 1201 loss: 2.51404117e-06
Iter: 1202 loss: 2.51489791e-06
Iter: 1203 loss: 2.5114557e-06
Iter: 1204 loss: 2.50776066e-06
Iter: 1205 loss: 2.5225163e-06
Iter: 1206 loss: 2.50700214e-06
Iter: 1207 loss: 2.50259563e-06
Iter: 1208 loss: 2.50834773e-06
Iter: 1209 loss: 2.5003169e-06
Iter: 1210 loss: 2.49657978e-06
Iter: 1211 loss: 2.49532536e-06
Iter: 1212 loss: 2.49327513e-06
Iter: 1213 loss: 2.48781089e-06
Iter: 1214 loss: 2.50678795e-06
Iter: 1215 loss: 2.48642436e-06
Iter: 1216 loss: 2.48203355e-06
Iter: 1217 loss: 2.49946402e-06
Iter: 1218 loss: 2.48103015e-06
Iter: 1219 loss: 2.47707794e-06
Iter: 1220 loss: 2.50880612e-06
Iter: 1221 loss: 2.47675052e-06
Iter: 1222 loss: 2.47308844e-06
Iter: 1223 loss: 2.47834555e-06
Iter: 1224 loss: 2.47125354e-06
Iter: 1225 loss: 2.46773743e-06
Iter: 1226 loss: 2.46865261e-06
Iter: 1227 loss: 2.4651913e-06
Iter: 1228 loss: 2.46013974e-06
Iter: 1229 loss: 2.50602079e-06
Iter: 1230 loss: 2.4599924e-06
Iter: 1231 loss: 2.45653928e-06
Iter: 1232 loss: 2.45643332e-06
Iter: 1233 loss: 2.45375622e-06
Iter: 1234 loss: 2.45136175e-06
Iter: 1235 loss: 2.45134424e-06
Iter: 1236 loss: 2.44896819e-06
Iter: 1237 loss: 2.44606554e-06
Iter: 1238 loss: 2.44581088e-06
Iter: 1239 loss: 2.44204148e-06
Iter: 1240 loss: 2.45761339e-06
Iter: 1241 loss: 2.44125113e-06
Iter: 1242 loss: 2.43755494e-06
Iter: 1243 loss: 2.4582896e-06
Iter: 1244 loss: 2.43704267e-06
Iter: 1245 loss: 2.43467912e-06
Iter: 1246 loss: 2.43462023e-06
Iter: 1247 loss: 2.4327951e-06
Iter: 1248 loss: 2.42829947e-06
Iter: 1249 loss: 2.43993691e-06
Iter: 1250 loss: 2.42667284e-06
Iter: 1251 loss: 2.42323267e-06
Iter: 1252 loss: 2.42313149e-06
Iter: 1253 loss: 2.42042756e-06
Iter: 1254 loss: 2.41591829e-06
Iter: 1255 loss: 2.4150288e-06
Iter: 1256 loss: 2.41193561e-06
Iter: 1257 loss: 2.40639952e-06
Iter: 1258 loss: 2.46206855e-06
Iter: 1259 loss: 2.40618283e-06
Iter: 1260 loss: 2.40262284e-06
Iter: 1261 loss: 2.42685473e-06
Iter: 1262 loss: 2.40232043e-06
Iter: 1263 loss: 2.39875158e-06
Iter: 1264 loss: 2.41078851e-06
Iter: 1265 loss: 2.39787437e-06
Iter: 1266 loss: 2.3948287e-06
Iter: 1267 loss: 2.39417705e-06
Iter: 1268 loss: 2.39220935e-06
Iter: 1269 loss: 2.38926418e-06
Iter: 1270 loss: 2.43121826e-06
Iter: 1271 loss: 2.38926532e-06
Iter: 1272 loss: 2.38620123e-06
Iter: 1273 loss: 2.38425264e-06
Iter: 1274 loss: 2.38308621e-06
Iter: 1275 loss: 2.37951872e-06
Iter: 1276 loss: 2.40247209e-06
Iter: 1277 loss: 2.37904942e-06
Iter: 1278 loss: 2.37588642e-06
Iter: 1279 loss: 2.38462576e-06
Iter: 1280 loss: 2.37476161e-06
Iter: 1281 loss: 2.37205677e-06
Iter: 1282 loss: 2.3718087e-06
Iter: 1283 loss: 2.36979e-06
Iter: 1284 loss: 2.36601e-06
Iter: 1285 loss: 2.40901863e-06
Iter: 1286 loss: 2.36588494e-06
Iter: 1287 loss: 2.36361711e-06
Iter: 1288 loss: 2.35928474e-06
Iter: 1289 loss: 2.45820229e-06
Iter: 1290 loss: 2.35925518e-06
Iter: 1291 loss: 2.35426114e-06
Iter: 1292 loss: 2.36652977e-06
Iter: 1293 loss: 2.35250309e-06
Iter: 1294 loss: 2.34758545e-06
Iter: 1295 loss: 2.3640057e-06
Iter: 1296 loss: 2.34625304e-06
Iter: 1297 loss: 2.3433663e-06
Iter: 1298 loss: 2.34334902e-06
Iter: 1299 loss: 2.3404109e-06
Iter: 1300 loss: 2.33820083e-06
Iter: 1301 loss: 2.33722017e-06
Iter: 1302 loss: 2.3328912e-06
Iter: 1303 loss: 2.35238599e-06
Iter: 1304 loss: 2.33210449e-06
Iter: 1305 loss: 2.32867046e-06
Iter: 1306 loss: 2.35872653e-06
Iter: 1307 loss: 2.32849015e-06
Iter: 1308 loss: 2.32574121e-06
Iter: 1309 loss: 2.3228647e-06
Iter: 1310 loss: 2.32230605e-06
Iter: 1311 loss: 2.31933927e-06
Iter: 1312 loss: 2.31930676e-06
Iter: 1313 loss: 2.3167886e-06
Iter: 1314 loss: 2.31304739e-06
Iter: 1315 loss: 2.31305557e-06
Iter: 1316 loss: 2.30936053e-06
Iter: 1317 loss: 2.32384855e-06
Iter: 1318 loss: 2.30852561e-06
Iter: 1319 loss: 2.30446676e-06
Iter: 1320 loss: 2.33189326e-06
Iter: 1321 loss: 2.30401224e-06
Iter: 1322 loss: 2.30108458e-06
Iter: 1323 loss: 2.29976831e-06
Iter: 1324 loss: 2.29829971e-06
Iter: 1325 loss: 2.29494344e-06
Iter: 1326 loss: 2.30910496e-06
Iter: 1327 loss: 2.29428178e-06
Iter: 1328 loss: 2.29080524e-06
Iter: 1329 loss: 2.30573937e-06
Iter: 1330 loss: 2.29009333e-06
Iter: 1331 loss: 2.28691647e-06
Iter: 1332 loss: 2.28637919e-06
Iter: 1333 loss: 2.28416684e-06
Iter: 1334 loss: 2.28010094e-06
Iter: 1335 loss: 2.27802389e-06
Iter: 1336 loss: 2.2761983e-06
Iter: 1337 loss: 2.27115697e-06
Iter: 1338 loss: 2.31743024e-06
Iter: 1339 loss: 2.27096052e-06
Iter: 1340 loss: 2.26748125e-06
Iter: 1341 loss: 2.29069769e-06
Iter: 1342 loss: 2.26718362e-06
Iter: 1343 loss: 2.2635511e-06
Iter: 1344 loss: 2.28033696e-06
Iter: 1345 loss: 2.26292696e-06
Iter: 1346 loss: 2.26049315e-06
Iter: 1347 loss: 2.26167072e-06
Iter: 1348 loss: 2.25881695e-06
Iter: 1349 loss: 2.2552681e-06
Iter: 1350 loss: 2.27452938e-06
Iter: 1351 loss: 2.25475787e-06
Iter: 1352 loss: 2.25215058e-06
Iter: 1353 loss: 2.24993096e-06
Iter: 1354 loss: 2.24922769e-06
Iter: 1355 loss: 2.24548421e-06
Iter: 1356 loss: 2.29545344e-06
Iter: 1357 loss: 2.24545e-06
Iter: 1358 loss: 2.24312089e-06
Iter: 1359 loss: 2.24153746e-06
Iter: 1360 loss: 2.2406839e-06
Iter: 1361 loss: 2.23790857e-06
Iter: 1362 loss: 2.27210603e-06
Iter: 1363 loss: 2.23790948e-06
Iter: 1364 loss: 2.23529196e-06
Iter: 1365 loss: 2.23288635e-06
Iter: 1366 loss: 2.23216e-06
Iter: 1367 loss: 2.22899553e-06
Iter: 1368 loss: 2.22924155e-06
Iter: 1369 loss: 2.22649851e-06
Iter: 1370 loss: 2.22202516e-06
Iter: 1371 loss: 2.25020221e-06
Iter: 1372 loss: 2.22145172e-06
Iter: 1373 loss: 2.21812525e-06
Iter: 1374 loss: 2.24747464e-06
Iter: 1375 loss: 2.21799064e-06
Iter: 1376 loss: 2.21515347e-06
Iter: 1377 loss: 2.21552546e-06
Iter: 1378 loss: 2.2129816e-06
Iter: 1379 loss: 2.20970151e-06
Iter: 1380 loss: 2.20957872e-06
Iter: 1381 loss: 2.20705897e-06
Iter: 1382 loss: 2.20502466e-06
Iter: 1383 loss: 2.20446782e-06
Iter: 1384 loss: 2.20224683e-06
Iter: 1385 loss: 2.20059655e-06
Iter: 1386 loss: 2.19985668e-06
Iter: 1387 loss: 2.19688718e-06
Iter: 1388 loss: 2.20314e-06
Iter: 1389 loss: 2.1957519e-06
Iter: 1390 loss: 2.19241429e-06
Iter: 1391 loss: 2.21630489e-06
Iter: 1392 loss: 2.19208e-06
Iter: 1393 loss: 2.18956734e-06
Iter: 1394 loss: 2.18674018e-06
Iter: 1395 loss: 2.18638729e-06
Iter: 1396 loss: 2.18341756e-06
Iter: 1397 loss: 2.22340736e-06
Iter: 1398 loss: 2.18339801e-06
Iter: 1399 loss: 2.1805381e-06
Iter: 1400 loss: 2.18316109e-06
Iter: 1401 loss: 2.17888805e-06
Iter: 1402 loss: 2.17607339e-06
Iter: 1403 loss: 2.17374645e-06
Iter: 1404 loss: 2.17290449e-06
Iter: 1405 loss: 2.1688511e-06
Iter: 1406 loss: 2.19905905e-06
Iter: 1407 loss: 2.16857234e-06
Iter: 1408 loss: 2.1653525e-06
Iter: 1409 loss: 2.18919081e-06
Iter: 1410 loss: 2.16515855e-06
Iter: 1411 loss: 2.16275112e-06
Iter: 1412 loss: 2.1595597e-06
Iter: 1413 loss: 2.15940418e-06
Iter: 1414 loss: 2.15512091e-06
Iter: 1415 loss: 2.16911599e-06
Iter: 1416 loss: 2.15403406e-06
Iter: 1417 loss: 2.15118371e-06
Iter: 1418 loss: 2.15113823e-06
Iter: 1419 loss: 2.14855299e-06
Iter: 1420 loss: 2.14844454e-06
Iter: 1421 loss: 2.1464657e-06
Iter: 1422 loss: 2.14416241e-06
Iter: 1423 loss: 2.17546017e-06
Iter: 1424 loss: 2.14412921e-06
Iter: 1425 loss: 2.14209649e-06
Iter: 1426 loss: 2.13857356e-06
Iter: 1427 loss: 2.13854264e-06
Iter: 1428 loss: 2.13471594e-06
Iter: 1429 loss: 2.14963507e-06
Iter: 1430 loss: 2.13374119e-06
Iter: 1431 loss: 2.13062913e-06
Iter: 1432 loss: 2.17327397e-06
Iter: 1433 loss: 2.13062185e-06
Iter: 1434 loss: 2.12853843e-06
Iter: 1435 loss: 2.12638042e-06
Iter: 1436 loss: 2.12606983e-06
Iter: 1437 loss: 2.12289842e-06
Iter: 1438 loss: 2.14199e-06
Iter: 1439 loss: 2.12252462e-06
Iter: 1440 loss: 2.11945598e-06
Iter: 1441 loss: 2.13025578e-06
Iter: 1442 loss: 2.11863198e-06
Iter: 1443 loss: 2.11610632e-06
Iter: 1444 loss: 2.11360248e-06
Iter: 1445 loss: 2.11308975e-06
Iter: 1446 loss: 2.10930216e-06
Iter: 1447 loss: 2.11642305e-06
Iter: 1448 loss: 2.107706e-06
Iter: 1449 loss: 2.10375447e-06
Iter: 1450 loss: 2.12774512e-06
Iter: 1451 loss: 2.10329108e-06
Iter: 1452 loss: 2.10009262e-06
Iter: 1453 loss: 2.13061253e-06
Iter: 1454 loss: 2.09995665e-06
Iter: 1455 loss: 2.09752397e-06
Iter: 1456 loss: 2.09522295e-06
Iter: 1457 loss: 2.094602e-06
Iter: 1458 loss: 2.09199743e-06
Iter: 1459 loss: 2.09199084e-06
Iter: 1460 loss: 2.08932579e-06
Iter: 1461 loss: 2.08859615e-06
Iter: 1462 loss: 2.08696451e-06
Iter: 1463 loss: 2.08416259e-06
Iter: 1464 loss: 2.08555957e-06
Iter: 1465 loss: 2.0823386e-06
Iter: 1466 loss: 2.07913308e-06
Iter: 1467 loss: 2.12641e-06
Iter: 1468 loss: 2.07914036e-06
Iter: 1469 loss: 2.07733342e-06
Iter: 1470 loss: 2.07514859e-06
Iter: 1471 loss: 2.07498147e-06
Iter: 1472 loss: 2.07181733e-06
Iter: 1473 loss: 2.10519624e-06
Iter: 1474 loss: 2.07173662e-06
Iter: 1475 loss: 2.06930417e-06
Iter: 1476 loss: 2.06938489e-06
Iter: 1477 loss: 2.06736786e-06
Iter: 1478 loss: 2.0647185e-06
Iter: 1479 loss: 2.07110816e-06
Iter: 1480 loss: 2.06367667e-06
Iter: 1481 loss: 2.06026016e-06
Iter: 1482 loss: 2.07758103e-06
Iter: 1483 loss: 2.0596467e-06
Iter: 1484 loss: 2.05706783e-06
Iter: 1485 loss: 2.05568858e-06
Iter: 1486 loss: 2.05449896e-06
Iter: 1487 loss: 2.05139145e-06
Iter: 1488 loss: 2.05775768e-06
Iter: 1489 loss: 2.05008655e-06
Iter: 1490 loss: 2.04718276e-06
Iter: 1491 loss: 2.08146412e-06
Iter: 1492 loss: 2.04714502e-06
Iter: 1493 loss: 2.04439061e-06
Iter: 1494 loss: 2.04527078e-06
Iter: 1495 loss: 2.0424809e-06
Iter: 1496 loss: 2.03991181e-06
Iter: 1497 loss: 2.06303594e-06
Iter: 1498 loss: 2.03983905e-06
Iter: 1499 loss: 2.03729587e-06
Iter: 1500 loss: 2.03962622e-06
Iter: 1501 loss: 2.03593618e-06
Iter: 1502 loss: 2.03327136e-06
Iter: 1503 loss: 2.03218042e-06
Iter: 1504 loss: 2.03079071e-06
Iter: 1505 loss: 2.02803403e-06
Iter: 1506 loss: 2.06578375e-06
Iter: 1507 loss: 2.02799242e-06
Iter: 1508 loss: 2.02562501e-06
Iter: 1509 loss: 2.02889851e-06
Iter: 1510 loss: 2.02444221e-06
Iter: 1511 loss: 2.02218143e-06
Iter: 1512 loss: 2.02514889e-06
Iter: 1513 loss: 2.02110732e-06
Iter: 1514 loss: 2.01810531e-06
Iter: 1515 loss: 2.03439e-06
Iter: 1516 loss: 2.01763896e-06
Iter: 1517 loss: 2.01558214e-06
Iter: 1518 loss: 2.01281841e-06
Iter: 1519 loss: 2.01258968e-06
Iter: 1520 loss: 2.00965565e-06
Iter: 1521 loss: 2.00963768e-06
Iter: 1522 loss: 2.00708541e-06
Iter: 1523 loss: 2.00611748e-06
Iter: 1524 loss: 2.00469185e-06
Iter: 1525 loss: 2.00186423e-06
Iter: 1526 loss: 2.00291424e-06
Iter: 1527 loss: 1.99980173e-06
Iter: 1528 loss: 1.99614465e-06
Iter: 1529 loss: 2.00909881e-06
Iter: 1530 loss: 1.99512192e-06
Iter: 1531 loss: 1.99289434e-06
Iter: 1532 loss: 1.99287456e-06
Iter: 1533 loss: 1.99093301e-06
Iter: 1534 loss: 1.98903626e-06
Iter: 1535 loss: 1.98864427e-06
Iter: 1536 loss: 1.98609678e-06
Iter: 1537 loss: 2.02472847e-06
Iter: 1538 loss: 1.98606108e-06
Iter: 1539 loss: 1.98418525e-06
Iter: 1540 loss: 1.98158477e-06
Iter: 1541 loss: 1.98149701e-06
Iter: 1542 loss: 1.97854888e-06
Iter: 1543 loss: 1.99684564e-06
Iter: 1544 loss: 1.97818e-06
Iter: 1545 loss: 1.97531517e-06
Iter: 1546 loss: 1.98702446e-06
Iter: 1547 loss: 1.97467057e-06
Iter: 1548 loss: 1.9726167e-06
Iter: 1549 loss: 1.9732895e-06
Iter: 1550 loss: 1.97119289e-06
Iter: 1551 loss: 1.96872475e-06
Iter: 1552 loss: 1.99062652e-06
Iter: 1553 loss: 1.96862493e-06
Iter: 1554 loss: 1.96648853e-06
Iter: 1555 loss: 1.96486599e-06
Iter: 1556 loss: 1.96423025e-06
Iter: 1557 loss: 1.96124392e-06
Iter: 1558 loss: 1.97703775e-06
Iter: 1559 loss: 1.96074461e-06
Iter: 1560 loss: 1.95836719e-06
Iter: 1561 loss: 1.97604959e-06
Iter: 1562 loss: 1.95817188e-06
Iter: 1563 loss: 1.95623079e-06
Iter: 1564 loss: 1.9519548e-06
Iter: 1565 loss: 2.01479816e-06
Iter: 1566 loss: 1.95174971e-06
Iter: 1567 loss: 1.94760878e-06
Iter: 1568 loss: 1.96682072e-06
Iter: 1569 loss: 1.94688027e-06
Iter: 1570 loss: 1.94322979e-06
Iter: 1571 loss: 1.95529856e-06
Iter: 1572 loss: 1.94221229e-06
Iter: 1573 loss: 1.93999381e-06
Iter: 1574 loss: 1.93995152e-06
Iter: 1575 loss: 1.93786263e-06
Iter: 1576 loss: 1.93863821e-06
Iter: 1577 loss: 1.93628739e-06
Iter: 1578 loss: 1.93417873e-06
Iter: 1579 loss: 1.93545475e-06
Iter: 1580 loss: 1.93280539e-06
Iter: 1581 loss: 1.93011329e-06
Iter: 1582 loss: 1.95216353e-06
Iter: 1583 loss: 1.92994048e-06
Iter: 1584 loss: 1.92773314e-06
Iter: 1585 loss: 1.92679818e-06
Iter: 1586 loss: 1.92560128e-06
Iter: 1587 loss: 1.92358857e-06
Iter: 1588 loss: 1.9235672e-06
Iter: 1589 loss: 1.92180369e-06
Iter: 1590 loss: 1.91869367e-06
Iter: 1591 loss: 1.9187205e-06
Iter: 1592 loss: 1.9154013e-06
Iter: 1593 loss: 1.9306317e-06
Iter: 1594 loss: 1.91477784e-06
Iter: 1595 loss: 1.91233767e-06
Iter: 1596 loss: 1.94222457e-06
Iter: 1597 loss: 1.91223762e-06
Iter: 1598 loss: 1.91042591e-06
Iter: 1599 loss: 1.90855758e-06
Iter: 1600 loss: 1.90813398e-06
Iter: 1601 loss: 1.90517744e-06
Iter: 1602 loss: 1.90797073e-06
Iter: 1603 loss: 1.90348044e-06
Iter: 1604 loss: 1.90137484e-06
Iter: 1605 loss: 1.90121705e-06
Iter: 1606 loss: 1.89949378e-06
Iter: 1607 loss: 1.8978111e-06
Iter: 1608 loss: 1.89744128e-06
Iter: 1609 loss: 1.89432785e-06
Iter: 1610 loss: 1.89673574e-06
Iter: 1611 loss: 1.89236562e-06
Iter: 1612 loss: 1.89056391e-06
Iter: 1613 loss: 1.89024888e-06
Iter: 1614 loss: 1.88851311e-06
Iter: 1615 loss: 1.8867064e-06
Iter: 1616 loss: 1.88634635e-06
Iter: 1617 loss: 1.88376146e-06
Iter: 1618 loss: 1.88508432e-06
Iter: 1619 loss: 1.88204535e-06
Iter: 1620 loss: 1.87934438e-06
Iter: 1621 loss: 1.89560694e-06
Iter: 1622 loss: 1.87898706e-06
Iter: 1623 loss: 1.87615319e-06
Iter: 1624 loss: 1.89072705e-06
Iter: 1625 loss: 1.87566627e-06
Iter: 1626 loss: 1.87380078e-06
Iter: 1627 loss: 1.87422938e-06
Iter: 1628 loss: 1.87253772e-06
Iter: 1629 loss: 1.86947898e-06
Iter: 1630 loss: 1.8812666e-06
Iter: 1631 loss: 1.86867976e-06
Iter: 1632 loss: 1.86662714e-06
Iter: 1633 loss: 1.864257e-06
Iter: 1634 loss: 1.86398199e-06
Iter: 1635 loss: 1.86104853e-06
Iter: 1636 loss: 1.88490992e-06
Iter: 1637 loss: 1.8608514e-06
Iter: 1638 loss: 1.85869169e-06
Iter: 1639 loss: 1.8809767e-06
Iter: 1640 loss: 1.85864747e-06
Iter: 1641 loss: 1.85691056e-06
Iter: 1642 loss: 1.85534077e-06
Iter: 1643 loss: 1.85490853e-06
Iter: 1644 loss: 1.8523483e-06
Iter: 1645 loss: 1.85933641e-06
Iter: 1646 loss: 1.85154613e-06
Iter: 1647 loss: 1.84875773e-06
Iter: 1648 loss: 1.87160106e-06
Iter: 1649 loss: 1.84858618e-06
Iter: 1650 loss: 1.84655573e-06
Iter: 1651 loss: 1.8475e-06
Iter: 1652 loss: 1.84511555e-06
Iter: 1653 loss: 1.84277906e-06
Iter: 1654 loss: 1.86363775e-06
Iter: 1655 loss: 1.84265355e-06
Iter: 1656 loss: 1.84076862e-06
Iter: 1657 loss: 1.8394137e-06
Iter: 1658 loss: 1.83877899e-06
Iter: 1659 loss: 1.83640634e-06
Iter: 1660 loss: 1.83711325e-06
Iter: 1661 loss: 1.83471911e-06
Iter: 1662 loss: 1.8316673e-06
Iter: 1663 loss: 1.85147724e-06
Iter: 1664 loss: 1.83140378e-06
Iter: 1665 loss: 1.82919848e-06
Iter: 1666 loss: 1.85922181e-06
Iter: 1667 loss: 1.82913345e-06
Iter: 1668 loss: 1.82770304e-06
Iter: 1669 loss: 1.82459098e-06
Iter: 1670 loss: 1.87148828e-06
Iter: 1671 loss: 1.82446252e-06
Iter: 1672 loss: 1.82199551e-06
Iter: 1673 loss: 1.82199256e-06
Iter: 1674 loss: 1.81969835e-06
Iter: 1675 loss: 1.82014605e-06
Iter: 1676 loss: 1.81804535e-06
Iter: 1677 loss: 1.81557823e-06
Iter: 1678 loss: 1.81598443e-06
Iter: 1679 loss: 1.81369955e-06
Iter: 1680 loss: 1.81046403e-06
Iter: 1681 loss: 1.8275789e-06
Iter: 1682 loss: 1.80994698e-06
Iter: 1683 loss: 1.80739539e-06
Iter: 1684 loss: 1.841678e-06
Iter: 1685 loss: 1.80739539e-06
Iter: 1686 loss: 1.80590598e-06
Iter: 1687 loss: 1.80468169e-06
Iter: 1688 loss: 1.80421239e-06
Iter: 1689 loss: 1.8019341e-06
Iter: 1690 loss: 1.8218476e-06
Iter: 1691 loss: 1.80181178e-06
Iter: 1692 loss: 1.79963229e-06
Iter: 1693 loss: 1.79964036e-06
Iter: 1694 loss: 1.79797303e-06
Iter: 1695 loss: 1.7958663e-06
Iter: 1696 loss: 1.80009329e-06
Iter: 1697 loss: 1.79509163e-06
Iter: 1698 loss: 1.79224901e-06
Iter: 1699 loss: 1.80041457e-06
Iter: 1700 loss: 1.79130734e-06
Iter: 1701 loss: 1.7890668e-06
Iter: 1702 loss: 1.78949381e-06
Iter: 1703 loss: 1.78729829e-06
Iter: 1704 loss: 1.78450489e-06
Iter: 1705 loss: 1.79280892e-06
Iter: 1706 loss: 1.78364212e-06
Iter: 1707 loss: 1.78179266e-06
Iter: 1708 loss: 1.78175026e-06
Iter: 1709 loss: 1.78007781e-06
Iter: 1710 loss: 1.77793459e-06
Iter: 1711 loss: 1.77787888e-06
Iter: 1712 loss: 1.77511242e-06
Iter: 1713 loss: 1.77691857e-06
Iter: 1714 loss: 1.77335392e-06
Iter: 1715 loss: 1.77026141e-06
Iter: 1716 loss: 1.78864479e-06
Iter: 1717 loss: 1.76987714e-06
Iter: 1718 loss: 1.76725007e-06
Iter: 1719 loss: 1.78960659e-06
Iter: 1720 loss: 1.76708772e-06
Iter: 1721 loss: 1.7651812e-06
Iter: 1722 loss: 1.7630656e-06
Iter: 1723 loss: 1.76279127e-06
Iter: 1724 loss: 1.76006154e-06
Iter: 1725 loss: 1.77125105e-06
Iter: 1726 loss: 1.7594848e-06
Iter: 1727 loss: 1.75644391e-06
Iter: 1728 loss: 1.78464518e-06
Iter: 1729 loss: 1.75634955e-06
Iter: 1730 loss: 1.75467676e-06
Iter: 1731 loss: 1.75324067e-06
Iter: 1732 loss: 1.75278342e-06
Iter: 1733 loss: 1.75065531e-06
Iter: 1734 loss: 1.76626736e-06
Iter: 1735 loss: 1.75051525e-06
Iter: 1736 loss: 1.74822412e-06
Iter: 1737 loss: 1.75056107e-06
Iter: 1738 loss: 1.74696595e-06
Iter: 1739 loss: 1.74495653e-06
Iter: 1740 loss: 1.74696754e-06
Iter: 1741 loss: 1.74373e-06
Iter: 1742 loss: 1.74161255e-06
Iter: 1743 loss: 1.76766287e-06
Iter: 1744 loss: 1.74161141e-06
Iter: 1745 loss: 1.7399301e-06
Iter: 1746 loss: 1.73721537e-06
Iter: 1747 loss: 1.73718217e-06
Iter: 1748 loss: 1.73392277e-06
Iter: 1749 loss: 1.75679509e-06
Iter: 1750 loss: 1.73359206e-06
Iter: 1751 loss: 1.73165176e-06
Iter: 1752 loss: 1.75973037e-06
Iter: 1753 loss: 1.73165108e-06
Iter: 1754 loss: 1.73043668e-06
Iter: 1755 loss: 1.72755836e-06
Iter: 1756 loss: 1.76361573e-06
Iter: 1757 loss: 1.72737271e-06
Iter: 1758 loss: 1.72386183e-06
Iter: 1759 loss: 1.73528633e-06
Iter: 1760 loss: 1.722962e-06
Iter: 1761 loss: 1.71996737e-06
Iter: 1762 loss: 1.72709156e-06
Iter: 1763 loss: 1.71881629e-06
Iter: 1764 loss: 1.71627062e-06
Iter: 1765 loss: 1.75608488e-06
Iter: 1766 loss: 1.71628858e-06
Iter: 1767 loss: 1.7140884e-06
Iter: 1768 loss: 1.71599982e-06
Iter: 1769 loss: 1.71277304e-06
Iter: 1770 loss: 1.71075715e-06
Iter: 1771 loss: 1.72922762e-06
Iter: 1772 loss: 1.71072338e-06
Iter: 1773 loss: 1.70886142e-06
Iter: 1774 loss: 1.70738042e-06
Iter: 1775 loss: 1.70683893e-06
Iter: 1776 loss: 1.70425233e-06
Iter: 1777 loss: 1.70488386e-06
Iter: 1778 loss: 1.70236012e-06
Iter: 1779 loss: 1.69925067e-06
Iter: 1780 loss: 1.70863632e-06
Iter: 1781 loss: 1.6983588e-06
Iter: 1782 loss: 1.69627901e-06
Iter: 1783 loss: 1.69628129e-06
Iter: 1784 loss: 1.69450846e-06
Iter: 1785 loss: 1.69336636e-06
Iter: 1786 loss: 1.69270049e-06
Iter: 1787 loss: 1.69008126e-06
Iter: 1788 loss: 1.69532177e-06
Iter: 1789 loss: 1.68904421e-06
Iter: 1790 loss: 1.68631391e-06
Iter: 1791 loss: 1.71245904e-06
Iter: 1792 loss: 1.68619817e-06
Iter: 1793 loss: 1.68422036e-06
Iter: 1794 loss: 1.68265e-06
Iter: 1795 loss: 1.68204099e-06
Iter: 1796 loss: 1.67989128e-06
Iter: 1797 loss: 1.70752912e-06
Iter: 1798 loss: 1.67989276e-06
Iter: 1799 loss: 1.67775011e-06
Iter: 1800 loss: 1.67653275e-06
Iter: 1801 loss: 1.67549683e-06
Iter: 1802 loss: 1.67288306e-06
Iter: 1803 loss: 1.6755605e-06
Iter: 1804 loss: 1.67141275e-06
Iter: 1805 loss: 1.6689504e-06
Iter: 1806 loss: 1.67837266e-06
Iter: 1807 loss: 1.66845552e-06
Iter: 1808 loss: 1.66610835e-06
Iter: 1809 loss: 1.68862402e-06
Iter: 1810 loss: 1.66608197e-06
Iter: 1811 loss: 1.66393784e-06
Iter: 1812 loss: 1.6671238e-06
Iter: 1813 loss: 1.66292568e-06
Iter: 1814 loss: 1.6613219e-06
Iter: 1815 loss: 1.66217251e-06
Iter: 1816 loss: 1.66020482e-06
Iter: 1817 loss: 1.65787674e-06
Iter: 1818 loss: 1.67072858e-06
Iter: 1819 loss: 1.65754898e-06
Iter: 1820 loss: 1.65564654e-06
Iter: 1821 loss: 1.65292136e-06
Iter: 1822 loss: 1.65288805e-06
Iter: 1823 loss: 1.65005952e-06
Iter: 1824 loss: 1.65764334e-06
Iter: 1825 loss: 1.64913592e-06
Iter: 1826 loss: 1.6465558e-06
Iter: 1827 loss: 1.68282781e-06
Iter: 1828 loss: 1.64651624e-06
Iter: 1829 loss: 1.64431515e-06
Iter: 1830 loss: 1.64961261e-06
Iter: 1831 loss: 1.64348683e-06
Iter: 1832 loss: 1.64179232e-06
Iter: 1833 loss: 1.64095377e-06
Iter: 1834 loss: 1.64006406e-06
Iter: 1835 loss: 1.63793584e-06
Iter: 1836 loss: 1.63792492e-06
Iter: 1837 loss: 1.63644825e-06
Iter: 1838 loss: 1.63497748e-06
Iter: 1839 loss: 1.63464711e-06
Iter: 1840 loss: 1.63280754e-06
Iter: 1841 loss: 1.65700521e-06
Iter: 1842 loss: 1.63279117e-06
Iter: 1843 loss: 1.63124014e-06
Iter: 1844 loss: 1.63088225e-06
Iter: 1845 loss: 1.62986362e-06
Iter: 1846 loss: 1.62779588e-06
Iter: 1847 loss: 1.62789763e-06
Iter: 1848 loss: 1.62616095e-06
Iter: 1849 loss: 1.62485662e-06
Iter: 1850 loss: 1.62457331e-06
Iter: 1851 loss: 1.62322169e-06
Iter: 1852 loss: 1.62130675e-06
Iter: 1853 loss: 1.62124923e-06
Iter: 1854 loss: 1.61891523e-06
Iter: 1855 loss: 1.62198262e-06
Iter: 1856 loss: 1.61761579e-06
Iter: 1857 loss: 1.61495427e-06
Iter: 1858 loss: 1.61717242e-06
Iter: 1859 loss: 1.61339972e-06
Iter: 1860 loss: 1.61140667e-06
Iter: 1861 loss: 1.61138041e-06
Iter: 1862 loss: 1.60949685e-06
Iter: 1863 loss: 1.6112399e-06
Iter: 1864 loss: 1.60836623e-06
Iter: 1865 loss: 1.60661887e-06
Iter: 1866 loss: 1.60804552e-06
Iter: 1867 loss: 1.60551781e-06
Iter: 1868 loss: 1.60354557e-06
Iter: 1869 loss: 1.6282886e-06
Iter: 1870 loss: 1.60352931e-06
Iter: 1871 loss: 1.60190928e-06
Iter: 1872 loss: 1.60141337e-06
Iter: 1873 loss: 1.60044738e-06
Iter: 1874 loss: 1.59855085e-06
Iter: 1875 loss: 1.60449417e-06
Iter: 1876 loss: 1.59798674e-06
Iter: 1877 loss: 1.59581668e-06
Iter: 1878 loss: 1.60870843e-06
Iter: 1879 loss: 1.59550189e-06
Iter: 1880 loss: 1.5938931e-06
Iter: 1881 loss: 1.59184469e-06
Iter: 1882 loss: 1.59165756e-06
Iter: 1883 loss: 1.58956607e-06
Iter: 1884 loss: 1.58955618e-06
Iter: 1885 loss: 1.58784883e-06
Iter: 1886 loss: 1.58774901e-06
Iter: 1887 loss: 1.58642433e-06
Iter: 1888 loss: 1.58454986e-06
Iter: 1889 loss: 1.58882494e-06
Iter: 1890 loss: 1.58379e-06
Iter: 1891 loss: 1.58144042e-06
Iter: 1892 loss: 1.60045374e-06
Iter: 1893 loss: 1.5812609e-06
Iter: 1894 loss: 1.57993145e-06
Iter: 1895 loss: 1.57792113e-06
Iter: 1896 loss: 1.57789782e-06
Iter: 1897 loss: 1.57521208e-06
Iter: 1898 loss: 1.57805243e-06
Iter: 1899 loss: 1.57375246e-06
Iter: 1900 loss: 1.57112e-06
Iter: 1901 loss: 1.59728017e-06
Iter: 1902 loss: 1.57105501e-06
Iter: 1903 loss: 1.56865167e-06
Iter: 1904 loss: 1.57802367e-06
Iter: 1905 loss: 1.56803446e-06
Iter: 1906 loss: 1.56610713e-06
Iter: 1907 loss: 1.56751548e-06
Iter: 1908 loss: 1.5649324e-06
Iter: 1909 loss: 1.56296869e-06
Iter: 1910 loss: 1.58498324e-06
Iter: 1911 loss: 1.56293231e-06
Iter: 1912 loss: 1.56140709e-06
Iter: 1913 loss: 1.56114049e-06
Iter: 1914 loss: 1.56010947e-06
Iter: 1915 loss: 1.5581777e-06
Iter: 1916 loss: 1.56327121e-06
Iter: 1917 loss: 1.55752275e-06
Iter: 1918 loss: 1.55518728e-06
Iter: 1919 loss: 1.56649e-06
Iter: 1920 loss: 1.55477755e-06
Iter: 1921 loss: 1.55302473e-06
Iter: 1922 loss: 1.55221676e-06
Iter: 1923 loss: 1.55138127e-06
Iter: 1924 loss: 1.5495192e-06
Iter: 1925 loss: 1.56365411e-06
Iter: 1926 loss: 1.54937129e-06
Iter: 1927 loss: 1.54738439e-06
Iter: 1928 loss: 1.55121393e-06
Iter: 1929 loss: 1.54654958e-06
Iter: 1930 loss: 1.54491079e-06
Iter: 1931 loss: 1.55159069e-06
Iter: 1932 loss: 1.54454824e-06
Iter: 1933 loss: 1.54262761e-06
Iter: 1934 loss: 1.54636655e-06
Iter: 1935 loss: 1.54182487e-06
Iter: 1936 loss: 1.54034865e-06
Iter: 1937 loss: 1.53788983e-06
Iter: 1938 loss: 1.53792166e-06
Iter: 1939 loss: 1.53487235e-06
Iter: 1940 loss: 1.54975135e-06
Iter: 1941 loss: 1.53437986e-06
Iter: 1942 loss: 1.53183055e-06
Iter: 1943 loss: 1.53595681e-06
Iter: 1944 loss: 1.5306955e-06
Iter: 1945 loss: 1.52843677e-06
Iter: 1946 loss: 1.55274074e-06
Iter: 1947 loss: 1.52842892e-06
Iter: 1948 loss: 1.52647317e-06
Iter: 1949 loss: 1.53426345e-06
Iter: 1950 loss: 1.52601922e-06
Iter: 1951 loss: 1.5245671e-06
Iter: 1952 loss: 1.52413168e-06
Iter: 1953 loss: 1.52320831e-06
Iter: 1954 loss: 1.52147618e-06
Iter: 1955 loss: 1.54749102e-06
Iter: 1956 loss: 1.52143832e-06
Iter: 1957 loss: 1.51985978e-06
Iter: 1958 loss: 1.51878885e-06
Iter: 1959 loss: 1.51818972e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi2
+ date
Wed Nov  4 14:30:58 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi1.6/300_300_300_1 --function f2 --psi -1 --alpha 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2abc21158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb288f0f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb288ec9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb288ec92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb288eb8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb288eb8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2646ca2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb288e55620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb288eac378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb288eac9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb26463aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb26463a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb264648378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb264648488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb26460a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb288df87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb288e271e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb288e272f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2645a5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2645702f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2645a5158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2644b1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb264518400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb264566378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2645667b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb288ec9488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb264459158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2645487b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2644a6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2644a6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb26437b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb26437be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb264404950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb264422620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb264422950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb264404bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.008910155
test_loss: 0.011070216
train_loss: 0.0071254694
test_loss: 0.009794232
train_loss: 0.006803022
test_loss: 0.009766425
train_loss: 0.0066419635
test_loss: 0.009306601
train_loss: 0.0064253528
test_loss: 0.00925681
train_loss: 0.006236526
test_loss: 0.00918679
train_loss: 0.006505396
test_loss: 0.009408876
train_loss: 0.006151502
test_loss: 0.009084973
train_loss: 0.0059842
test_loss: 0.008860129
train_loss: 0.0061062323
test_loss: 0.008884946
train_loss: 0.006076712
test_loss: 0.008899057
train_loss: 0.00610692
test_loss: 0.00879544
train_loss: 0.005894504
test_loss: 0.008752504
train_loss: 0.005910119
test_loss: 0.00870287
train_loss: 0.0063059367
test_loss: 0.0089806225
train_loss: 0.005660379
test_loss: 0.008640988
train_loss: 0.0062883524
test_loss: 0.008559852
train_loss: 0.005466716
test_loss: 0.0085775815
train_loss: 0.0057817902
test_loss: 0.008793852
train_loss: 0.0055431444
test_loss: 0.008684231
train_loss: 0.006073126
test_loss: 0.00862113
train_loss: 0.0055576675
test_loss: 0.00870508
train_loss: 0.0056791427
test_loss: 0.008479522
train_loss: 0.006000762
test_loss: 0.008848785
train_loss: 0.0051695523
test_loss: 0.008448911
train_loss: 0.005240588
test_loss: 0.008584589
train_loss: 0.005476027
test_loss: 0.008616256
train_loss: 0.005813718
test_loss: 0.008408587
train_loss: 0.005503563
test_loss: 0.008567426
train_loss: 0.0051945536
test_loss: 0.008377574
train_loss: 0.0051245987
test_loss: 0.008496445
train_loss: 0.0054057324
test_loss: 0.008435392
train_loss: 0.005088344
test_loss: 0.008404563
train_loss: 0.005659958
test_loss: 0.008486928
train_loss: 0.00509594
test_loss: 0.008345649
train_loss: 0.005427995
test_loss: 0.00843972
train_loss: 0.005406173
test_loss: 0.008405947
train_loss: 0.005453541
test_loss: 0.008633199
train_loss: 0.0053496426
test_loss: 0.008382387
train_loss: 0.0055294638
test_loss: 0.008288355
train_loss: 0.005059747
test_loss: 0.008451938
train_loss: 0.0052277083
test_loss: 0.008508315
train_loss: 0.0053742384
test_loss: 0.008398119
train_loss: 0.0052720895
test_loss: 0.00839167
train_loss: 0.005056296
test_loss: 0.008346221
train_loss: 0.0049187667
test_loss: 0.0083198445
train_loss: 0.0050646905
test_loss: 0.008350103
train_loss: 0.0050752917
test_loss: 0.008350024
train_loss: 0.005103473
test_loss: 0.008366423
train_loss: 0.0050265808
test_loss: 0.008641827
train_loss: 0.0049839807
test_loss: 0.008415949
train_loss: 0.0053557786
test_loss: 0.008388861
train_loss: 0.0053297076
test_loss: 0.00826756
train_loss: 0.0049161497
test_loss: 0.008249037
train_loss: 0.005246597
test_loss: 0.008297595
train_loss: 0.0051726676
test_loss: 0.00859365
train_loss: 0.005556609
test_loss: 0.008329177
train_loss: 0.0051940037
test_loss: 0.008223967
train_loss: 0.0053997273
test_loss: 0.008318868
train_loss: 0.005009683
test_loss: 0.008177271
train_loss: 0.0052514467
test_loss: 0.00834634
train_loss: 0.0050113495
test_loss: 0.008233413
train_loss: 0.005184696
test_loss: 0.0084029455
train_loss: 0.0049490123
test_loss: 0.00844665
train_loss: 0.004939547
test_loss: 0.008208685
train_loss: 0.0052260887
test_loss: 0.0082654
train_loss: 0.0049240645
test_loss: 0.008206675
train_loss: 0.0049505574
test_loss: 0.008152976
train_loss: 0.0050911424
test_loss: 0.008232938
train_loss: 0.005142975
test_loss: 0.008348634
train_loss: 0.0048732655
test_loss: 0.0080496445
train_loss: 0.004854606
test_loss: 0.0083372
train_loss: 0.004987992
test_loss: 0.008115508
train_loss: 0.0049458635
test_loss: 0.00822365
train_loss: 0.005185504
test_loss: 0.008057347
train_loss: 0.0046982807
test_loss: 0.008107512
train_loss: 0.004839524
test_loss: 0.008237637
train_loss: 0.0049921735
test_loss: 0.008166607
train_loss: 0.00499395
test_loss: 0.008043425
train_loss: 0.004806284
test_loss: 0.008026497
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2/300_300_300_1 --optimizer lbfgs --function f2 --psi -1 --alpha 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29177d2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29177f9ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29177b6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29177b6268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29177be7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29177be510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ed801e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ed80d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ed33488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ed33598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ed1df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ed020d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ed029d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ecbfe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ec87620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290eca5510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ec94598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ebfa620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ec1f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ec0f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ec1f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ebf2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290eba10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290eb499d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290eb63048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290eb63bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290eb281e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290eb28510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290eb63ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ead3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290eaad8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290eca5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ecbb730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ecbbbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290ecbba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f290e9baa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.82472135e-05
Iter: 2 loss: 3.45316585e-05
Iter: 3 loss: 3.19301798e-05
Iter: 4 loss: 3.0583069e-05
Iter: 5 loss: 2.71730423e-05
Iter: 6 loss: 6.74699259e-05
Iter: 7 loss: 2.71280878e-05
Iter: 8 loss: 2.51240308e-05
Iter: 9 loss: 2.62770373e-05
Iter: 10 loss: 2.38214361e-05
Iter: 11 loss: 2.20192323e-05
Iter: 12 loss: 4.43262616e-05
Iter: 13 loss: 2.20002421e-05
Iter: 14 loss: 2.09891732e-05
Iter: 15 loss: 2.05663364e-05
Iter: 16 loss: 2.00393151e-05
Iter: 17 loss: 1.88089653e-05
Iter: 18 loss: 2.77385934e-05
Iter: 19 loss: 1.87031037e-05
Iter: 20 loss: 1.76163285e-05
Iter: 21 loss: 1.86408906e-05
Iter: 22 loss: 1.69944869e-05
Iter: 23 loss: 1.60108211e-05
Iter: 24 loss: 1.85509853e-05
Iter: 25 loss: 1.56747265e-05
Iter: 26 loss: 1.48999989e-05
Iter: 27 loss: 2.23150691e-05
Iter: 28 loss: 1.48700265e-05
Iter: 29 loss: 1.42175668e-05
Iter: 30 loss: 1.54652425e-05
Iter: 31 loss: 1.39443291e-05
Iter: 32 loss: 1.34782595e-05
Iter: 33 loss: 1.35712435e-05
Iter: 34 loss: 1.31317092e-05
Iter: 35 loss: 1.26792092e-05
Iter: 36 loss: 1.49663283e-05
Iter: 37 loss: 1.2604618e-05
Iter: 38 loss: 1.22288911e-05
Iter: 39 loss: 1.64838129e-05
Iter: 40 loss: 1.22219972e-05
Iter: 41 loss: 1.19654815e-05
Iter: 42 loss: 1.18845146e-05
Iter: 43 loss: 1.17338723e-05
Iter: 44 loss: 1.14527656e-05
Iter: 45 loss: 1.23777008e-05
Iter: 46 loss: 1.13747537e-05
Iter: 47 loss: 1.11672416e-05
Iter: 48 loss: 1.11652025e-05
Iter: 49 loss: 1.10175715e-05
Iter: 50 loss: 1.08798777e-05
Iter: 51 loss: 1.08450786e-05
Iter: 52 loss: 1.06593798e-05
Iter: 53 loss: 1.12938815e-05
Iter: 54 loss: 1.06094358e-05
Iter: 55 loss: 1.0409658e-05
Iter: 56 loss: 1.15688781e-05
Iter: 57 loss: 1.03833972e-05
Iter: 58 loss: 1.02370759e-05
Iter: 59 loss: 1.01827973e-05
Iter: 60 loss: 1.01018513e-05
Iter: 61 loss: 9.93353751e-06
Iter: 62 loss: 1.05731142e-05
Iter: 63 loss: 9.89332511e-06
Iter: 64 loss: 9.7537195e-06
Iter: 65 loss: 1.12094176e-05
Iter: 66 loss: 9.75012426e-06
Iter: 67 loss: 9.63207094e-06
Iter: 68 loss: 9.62905688e-06
Iter: 69 loss: 9.53670951e-06
Iter: 70 loss: 9.42491715e-06
Iter: 71 loss: 9.66821426e-06
Iter: 72 loss: 9.38169615e-06
Iter: 73 loss: 9.25692e-06
Iter: 74 loss: 1.02541162e-05
Iter: 75 loss: 9.24858068e-06
Iter: 76 loss: 9.16269e-06
Iter: 77 loss: 9.08918264e-06
Iter: 78 loss: 9.065634e-06
Iter: 79 loss: 8.95228641e-06
Iter: 80 loss: 9.25683889e-06
Iter: 81 loss: 8.91467e-06
Iter: 82 loss: 8.82686254e-06
Iter: 83 loss: 8.82685163e-06
Iter: 84 loss: 8.74653233e-06
Iter: 85 loss: 8.88480736e-06
Iter: 86 loss: 8.71087195e-06
Iter: 87 loss: 8.65031052e-06
Iter: 88 loss: 8.93835e-06
Iter: 89 loss: 8.63950572e-06
Iter: 90 loss: 8.57567647e-06
Iter: 91 loss: 8.56718725e-06
Iter: 92 loss: 8.52207268e-06
Iter: 93 loss: 8.43835642e-06
Iter: 94 loss: 8.58997919e-06
Iter: 95 loss: 8.40208577e-06
Iter: 96 loss: 8.32437e-06
Iter: 97 loss: 8.71321936e-06
Iter: 98 loss: 8.31145462e-06
Iter: 99 loss: 8.231621e-06
Iter: 100 loss: 8.58714702e-06
Iter: 101 loss: 8.21589674e-06
Iter: 102 loss: 8.16478496e-06
Iter: 103 loss: 8.13027327e-06
Iter: 104 loss: 8.11127211e-06
Iter: 105 loss: 8.0371392e-06
Iter: 106 loss: 8.31053876e-06
Iter: 107 loss: 8.01891292e-06
Iter: 108 loss: 7.95352662e-06
Iter: 109 loss: 8.77954517e-06
Iter: 110 loss: 7.95287e-06
Iter: 111 loss: 7.9091169e-06
Iter: 112 loss: 7.85762131e-06
Iter: 113 loss: 7.85191605e-06
Iter: 114 loss: 7.78545837e-06
Iter: 115 loss: 7.83118321e-06
Iter: 116 loss: 7.74386e-06
Iter: 117 loss: 7.69090093e-06
Iter: 118 loss: 7.68981408e-06
Iter: 119 loss: 7.64307151e-06
Iter: 120 loss: 7.72167186e-06
Iter: 121 loss: 7.62199579e-06
Iter: 122 loss: 7.57913767e-06
Iter: 123 loss: 7.82299139e-06
Iter: 124 loss: 7.57313546e-06
Iter: 125 loss: 7.52776032e-06
Iter: 126 loss: 7.55137353e-06
Iter: 127 loss: 7.49773881e-06
Iter: 128 loss: 7.45518537e-06
Iter: 129 loss: 7.48465436e-06
Iter: 130 loss: 7.42879683e-06
Iter: 131 loss: 7.37821665e-06
Iter: 132 loss: 7.53662289e-06
Iter: 133 loss: 7.36352558e-06
Iter: 134 loss: 7.32186436e-06
Iter: 135 loss: 7.84290751e-06
Iter: 136 loss: 7.32143644e-06
Iter: 137 loss: 7.28857503e-06
Iter: 138 loss: 7.24283746e-06
Iter: 139 loss: 7.24105394e-06
Iter: 140 loss: 7.19468153e-06
Iter: 141 loss: 7.55323208e-06
Iter: 142 loss: 7.19116179e-06
Iter: 143 loss: 7.14584075e-06
Iter: 144 loss: 7.30857118e-06
Iter: 145 loss: 7.13426e-06
Iter: 146 loss: 7.10226323e-06
Iter: 147 loss: 7.09049255e-06
Iter: 148 loss: 7.07284289e-06
Iter: 149 loss: 7.0407832e-06
Iter: 150 loss: 7.49177889e-06
Iter: 151 loss: 7.04045669e-06
Iter: 152 loss: 7.00861801e-06
Iter: 153 loss: 7.0168926e-06
Iter: 154 loss: 6.98525673e-06
Iter: 155 loss: 6.95205745e-06
Iter: 156 loss: 6.99649e-06
Iter: 157 loss: 6.93531274e-06
Iter: 158 loss: 6.90185698e-06
Iter: 159 loss: 7.02530133e-06
Iter: 160 loss: 6.89352191e-06
Iter: 161 loss: 6.85992472e-06
Iter: 162 loss: 7.20073467e-06
Iter: 163 loss: 6.8589361e-06
Iter: 164 loss: 6.8336667e-06
Iter: 165 loss: 6.81768688e-06
Iter: 166 loss: 6.80775338e-06
Iter: 167 loss: 6.77943899e-06
Iter: 168 loss: 6.76594073e-06
Iter: 169 loss: 6.75222918e-06
Iter: 170 loss: 6.71437783e-06
Iter: 171 loss: 7.16863542e-06
Iter: 172 loss: 6.71384805e-06
Iter: 173 loss: 6.68171742e-06
Iter: 174 loss: 6.83260851e-06
Iter: 175 loss: 6.67574614e-06
Iter: 176 loss: 6.65359767e-06
Iter: 177 loss: 6.62943103e-06
Iter: 178 loss: 6.62567527e-06
Iter: 179 loss: 6.60003298e-06
Iter: 180 loss: 6.60002661e-06
Iter: 181 loss: 6.57686041e-06
Iter: 182 loss: 6.57143e-06
Iter: 183 loss: 6.55668964e-06
Iter: 184 loss: 6.52950393e-06
Iter: 185 loss: 6.54346604e-06
Iter: 186 loss: 6.51142636e-06
Iter: 187 loss: 6.48293735e-06
Iter: 188 loss: 6.48285186e-06
Iter: 189 loss: 6.46398e-06
Iter: 190 loss: 6.44243755e-06
Iter: 191 loss: 6.43964631e-06
Iter: 192 loss: 6.41213046e-06
Iter: 193 loss: 6.55726e-06
Iter: 194 loss: 6.40779717e-06
Iter: 195 loss: 6.38362826e-06
Iter: 196 loss: 6.53838697e-06
Iter: 197 loss: 6.38091751e-06
Iter: 198 loss: 6.35908464e-06
Iter: 199 loss: 6.39171958e-06
Iter: 200 loss: 6.34866183e-06
Iter: 201 loss: 6.32687079e-06
Iter: 202 loss: 6.47091883e-06
Iter: 203 loss: 6.32433148e-06
Iter: 204 loss: 6.3076468e-06
Iter: 205 loss: 6.28654743e-06
Iter: 206 loss: 6.28462794e-06
Iter: 207 loss: 6.25652774e-06
Iter: 208 loss: 6.31024068e-06
Iter: 209 loss: 6.24457334e-06
Iter: 210 loss: 6.21670824e-06
Iter: 211 loss: 6.33521768e-06
Iter: 212 loss: 6.21105937e-06
Iter: 213 loss: 6.19057118e-06
Iter: 214 loss: 6.46152466e-06
Iter: 215 loss: 6.19046114e-06
Iter: 216 loss: 6.17190108e-06
Iter: 217 loss: 6.16139e-06
Iter: 218 loss: 6.15348654e-06
Iter: 219 loss: 6.13064731e-06
Iter: 220 loss: 6.14570172e-06
Iter: 221 loss: 6.11620135e-06
Iter: 222 loss: 6.09461404e-06
Iter: 223 loss: 6.40339931e-06
Iter: 224 loss: 6.09440849e-06
Iter: 225 loss: 6.07367156e-06
Iter: 226 loss: 6.08255823e-06
Iter: 227 loss: 6.05954119e-06
Iter: 228 loss: 6.03703938e-06
Iter: 229 loss: 6.05626747e-06
Iter: 230 loss: 6.02387081e-06
Iter: 231 loss: 5.99574741e-06
Iter: 232 loss: 6.23391725e-06
Iter: 233 loss: 5.99420082e-06
Iter: 234 loss: 5.97460166e-06
Iter: 235 loss: 6.02003729e-06
Iter: 236 loss: 5.96734e-06
Iter: 237 loss: 5.94966286e-06
Iter: 238 loss: 5.99444866e-06
Iter: 239 loss: 5.94354105e-06
Iter: 240 loss: 5.91988646e-06
Iter: 241 loss: 5.9916897e-06
Iter: 242 loss: 5.91289609e-06
Iter: 243 loss: 5.89910087e-06
Iter: 244 loss: 5.87043633e-06
Iter: 245 loss: 6.36675213e-06
Iter: 246 loss: 5.86987107e-06
Iter: 247 loss: 5.84838926e-06
Iter: 248 loss: 5.84801091e-06
Iter: 249 loss: 5.82927078e-06
Iter: 250 loss: 5.86783699e-06
Iter: 251 loss: 5.82168377e-06
Iter: 252 loss: 5.80191181e-06
Iter: 253 loss: 5.79734797e-06
Iter: 254 loss: 5.78447953e-06
Iter: 255 loss: 5.76315233e-06
Iter: 256 loss: 5.96113614e-06
Iter: 257 loss: 5.76232378e-06
Iter: 258 loss: 5.73998659e-06
Iter: 259 loss: 5.78456e-06
Iter: 260 loss: 5.73077477e-06
Iter: 261 loss: 5.71456803e-06
Iter: 262 loss: 5.69194526e-06
Iter: 263 loss: 5.69099348e-06
Iter: 264 loss: 5.66358e-06
Iter: 265 loss: 5.93593586e-06
Iter: 266 loss: 5.66266635e-06
Iter: 267 loss: 5.63958884e-06
Iter: 268 loss: 5.78289837e-06
Iter: 269 loss: 5.63690355e-06
Iter: 270 loss: 5.61946308e-06
Iter: 271 loss: 5.62492551e-06
Iter: 272 loss: 5.60695116e-06
Iter: 273 loss: 5.58750253e-06
Iter: 274 loss: 5.68337327e-06
Iter: 275 loss: 5.58420106e-06
Iter: 276 loss: 5.56663053e-06
Iter: 277 loss: 5.70104203e-06
Iter: 278 loss: 5.56535315e-06
Iter: 279 loss: 5.55515453e-06
Iter: 280 loss: 5.55071028e-06
Iter: 281 loss: 5.54551434e-06
Iter: 282 loss: 5.52861093e-06
Iter: 283 loss: 5.61917523e-06
Iter: 284 loss: 5.52611164e-06
Iter: 285 loss: 5.51284074e-06
Iter: 286 loss: 5.49352535e-06
Iter: 287 loss: 5.49308243e-06
Iter: 288 loss: 5.46925639e-06
Iter: 289 loss: 5.53305563e-06
Iter: 290 loss: 5.46132924e-06
Iter: 291 loss: 5.43711667e-06
Iter: 292 loss: 5.54552435e-06
Iter: 293 loss: 5.43240094e-06
Iter: 294 loss: 5.41103782e-06
Iter: 295 loss: 5.65217215e-06
Iter: 296 loss: 5.41066584e-06
Iter: 297 loss: 5.39876e-06
Iter: 298 loss: 5.40121e-06
Iter: 299 loss: 5.38985023e-06
Iter: 300 loss: 5.37332562e-06
Iter: 301 loss: 5.42969065e-06
Iter: 302 loss: 5.36891503e-06
Iter: 303 loss: 5.3494673e-06
Iter: 304 loss: 5.37339611e-06
Iter: 305 loss: 5.3392514e-06
Iter: 306 loss: 5.32254035e-06
Iter: 307 loss: 5.34090077e-06
Iter: 308 loss: 5.31367e-06
Iter: 309 loss: 5.29382487e-06
Iter: 310 loss: 5.39276743e-06
Iter: 311 loss: 5.29043155e-06
Iter: 312 loss: 5.27122256e-06
Iter: 313 loss: 5.39422763e-06
Iter: 314 loss: 5.26894655e-06
Iter: 315 loss: 5.25837277e-06
Iter: 316 loss: 5.27345856e-06
Iter: 317 loss: 5.25299902e-06
Iter: 318 loss: 5.23918152e-06
Iter: 319 loss: 5.30542547e-06
Iter: 320 loss: 5.23677591e-06
Iter: 321 loss: 5.2255109e-06
Iter: 322 loss: 5.2030814e-06
Iter: 323 loss: 5.6427416e-06
Iter: 324 loss: 5.20292178e-06
Iter: 325 loss: 5.18550132e-06
Iter: 326 loss: 5.44444538e-06
Iter: 327 loss: 5.18547586e-06
Iter: 328 loss: 5.17179251e-06
Iter: 329 loss: 5.22984828e-06
Iter: 330 loss: 5.16889213e-06
Iter: 331 loss: 5.15549254e-06
Iter: 332 loss: 5.13578107e-06
Iter: 333 loss: 5.13528266e-06
Iter: 334 loss: 5.11536518e-06
Iter: 335 loss: 5.17542048e-06
Iter: 336 loss: 5.10933387e-06
Iter: 337 loss: 5.09254096e-06
Iter: 338 loss: 5.09254733e-06
Iter: 339 loss: 5.0797762e-06
Iter: 340 loss: 5.11649e-06
Iter: 341 loss: 5.07582899e-06
Iter: 342 loss: 5.06490551e-06
Iter: 343 loss: 5.04656873e-06
Iter: 344 loss: 5.04661148e-06
Iter: 345 loss: 5.02713465e-06
Iter: 346 loss: 5.23527206e-06
Iter: 347 loss: 5.02665216e-06
Iter: 348 loss: 5.01357408e-06
Iter: 349 loss: 5.12938595e-06
Iter: 350 loss: 5.01285149e-06
Iter: 351 loss: 5.0019944e-06
Iter: 352 loss: 5.00411443e-06
Iter: 353 loss: 4.99393445e-06
Iter: 354 loss: 4.98015788e-06
Iter: 355 loss: 5.07095365e-06
Iter: 356 loss: 4.97878409e-06
Iter: 357 loss: 4.96645862e-06
Iter: 358 loss: 4.96665507e-06
Iter: 359 loss: 4.95669065e-06
Iter: 360 loss: 4.94377446e-06
Iter: 361 loss: 4.99937778e-06
Iter: 362 loss: 4.94115829e-06
Iter: 363 loss: 4.92493791e-06
Iter: 364 loss: 4.92889103e-06
Iter: 365 loss: 4.91305445e-06
Iter: 366 loss: 4.89827653e-06
Iter: 367 loss: 4.89087506e-06
Iter: 368 loss: 4.8838674e-06
Iter: 369 loss: 4.86850195e-06
Iter: 370 loss: 4.86853696e-06
Iter: 371 loss: 4.85484907e-06
Iter: 372 loss: 4.89580816e-06
Iter: 373 loss: 4.85076316e-06
Iter: 374 loss: 4.83904114e-06
Iter: 375 loss: 4.8348561e-06
Iter: 376 loss: 4.82812266e-06
Iter: 377 loss: 4.81475399e-06
Iter: 378 loss: 4.92800245e-06
Iter: 379 loss: 4.81402913e-06
Iter: 380 loss: 4.79905384e-06
Iter: 381 loss: 4.81781e-06
Iter: 382 loss: 4.79126356e-06
Iter: 383 loss: 4.77668e-06
Iter: 384 loss: 4.7902904e-06
Iter: 385 loss: 4.76834657e-06
Iter: 386 loss: 4.75498382e-06
Iter: 387 loss: 4.81801e-06
Iter: 388 loss: 4.75263732e-06
Iter: 389 loss: 4.73916225e-06
Iter: 390 loss: 4.84184875e-06
Iter: 391 loss: 4.73814771e-06
Iter: 392 loss: 4.72776355e-06
Iter: 393 loss: 4.72069223e-06
Iter: 394 loss: 4.716896e-06
Iter: 395 loss: 4.70542e-06
Iter: 396 loss: 4.8196448e-06
Iter: 397 loss: 4.70518717e-06
Iter: 398 loss: 4.69389e-06
Iter: 399 loss: 4.68862163e-06
Iter: 400 loss: 4.68309463e-06
Iter: 401 loss: 4.6690393e-06
Iter: 402 loss: 4.66804067e-06
Iter: 403 loss: 4.6576024e-06
Iter: 404 loss: 4.64565574e-06
Iter: 405 loss: 4.64549839e-06
Iter: 406 loss: 4.63417337e-06
Iter: 407 loss: 4.63052766e-06
Iter: 408 loss: 4.62378421e-06
Iter: 409 loss: 4.61006721e-06
Iter: 410 loss: 4.63348351e-06
Iter: 411 loss: 4.60379579e-06
Iter: 412 loss: 4.59106195e-06
Iter: 413 loss: 4.71480143e-06
Iter: 414 loss: 4.59059811e-06
Iter: 415 loss: 4.57884653e-06
Iter: 416 loss: 4.59519924e-06
Iter: 417 loss: 4.57316264e-06
Iter: 418 loss: 4.55975896e-06
Iter: 419 loss: 4.57571969e-06
Iter: 420 loss: 4.55265763e-06
Iter: 421 loss: 4.54158635e-06
Iter: 422 loss: 4.66577148e-06
Iter: 423 loss: 4.5413617e-06
Iter: 424 loss: 4.53107941e-06
Iter: 425 loss: 4.52161657e-06
Iter: 426 loss: 4.51913911e-06
Iter: 427 loss: 4.50662901e-06
Iter: 428 loss: 4.62870867e-06
Iter: 429 loss: 4.5061015e-06
Iter: 430 loss: 4.49497929e-06
Iter: 431 loss: 4.53873099e-06
Iter: 432 loss: 4.49253366e-06
Iter: 433 loss: 4.48530591e-06
Iter: 434 loss: 4.47077e-06
Iter: 435 loss: 4.7387266e-06
Iter: 436 loss: 4.47049069e-06
Iter: 437 loss: 4.46052763e-06
Iter: 438 loss: 4.46009471e-06
Iter: 439 loss: 4.45037495e-06
Iter: 440 loss: 4.45354044e-06
Iter: 441 loss: 4.44347188e-06
Iter: 442 loss: 4.43224872e-06
Iter: 443 loss: 4.42719693e-06
Iter: 444 loss: 4.42157125e-06
Iter: 445 loss: 4.40645817e-06
Iter: 446 loss: 4.46976219e-06
Iter: 447 loss: 4.40336316e-06
Iter: 448 loss: 4.39060386e-06
Iter: 449 loss: 4.46897047e-06
Iter: 450 loss: 4.38902862e-06
Iter: 451 loss: 4.37530025e-06
Iter: 452 loss: 4.41356497e-06
Iter: 453 loss: 4.37084418e-06
Iter: 454 loss: 4.35990296e-06
Iter: 455 loss: 4.36647952e-06
Iter: 456 loss: 4.35275797e-06
Iter: 457 loss: 4.34199092e-06
Iter: 458 loss: 4.49355866e-06
Iter: 459 loss: 4.34193953e-06
Iter: 460 loss: 4.33404875e-06
Iter: 461 loss: 4.3344e-06
Iter: 462 loss: 4.32789602e-06
Iter: 463 loss: 4.3169839e-06
Iter: 464 loss: 4.33553942e-06
Iter: 465 loss: 4.31202079e-06
Iter: 466 loss: 4.29898228e-06
Iter: 467 loss: 4.4111589e-06
Iter: 468 loss: 4.2983e-06
Iter: 469 loss: 4.2913307e-06
Iter: 470 loss: 4.29383817e-06
Iter: 471 loss: 4.28625845e-06
Iter: 472 loss: 4.27578152e-06
Iter: 473 loss: 4.30353566e-06
Iter: 474 loss: 4.27241503e-06
Iter: 475 loss: 4.26134193e-06
Iter: 476 loss: 4.26204224e-06
Iter: 477 loss: 4.25275084e-06
Iter: 478 loss: 4.24249674e-06
Iter: 479 loss: 4.27626674e-06
Iter: 480 loss: 4.23965e-06
Iter: 481 loss: 4.22918265e-06
Iter: 482 loss: 4.32990055e-06
Iter: 483 loss: 4.22871926e-06
Iter: 484 loss: 4.22120411e-06
Iter: 485 loss: 4.21496952e-06
Iter: 486 loss: 4.21277809e-06
Iter: 487 loss: 4.20194874e-06
Iter: 488 loss: 4.2112124e-06
Iter: 489 loss: 4.19557864e-06
Iter: 490 loss: 4.18128457e-06
Iter: 491 loss: 4.22771882e-06
Iter: 492 loss: 4.17719457e-06
Iter: 493 loss: 4.16507692e-06
Iter: 494 loss: 4.31456374e-06
Iter: 495 loss: 4.16496187e-06
Iter: 496 loss: 4.15606564e-06
Iter: 497 loss: 4.18064e-06
Iter: 498 loss: 4.15325621e-06
Iter: 499 loss: 4.14561327e-06
Iter: 500 loss: 4.1428234e-06
Iter: 501 loss: 4.13852104e-06
Iter: 502 loss: 4.12843838e-06
Iter: 503 loss: 4.28264912e-06
Iter: 504 loss: 4.12845475e-06
Iter: 505 loss: 4.12277404e-06
Iter: 506 loss: 4.12187364e-06
Iter: 507 loss: 4.11791325e-06
Iter: 508 loss: 4.10877783e-06
Iter: 509 loss: 4.15379964e-06
Iter: 510 loss: 4.10726534e-06
Iter: 511 loss: 4.10001758e-06
Iter: 512 loss: 4.08605774e-06
Iter: 513 loss: 4.37741528e-06
Iter: 514 loss: 4.08608093e-06
Iter: 515 loss: 4.07506741e-06
Iter: 516 loss: 4.07508469e-06
Iter: 517 loss: 4.06605659e-06
Iter: 518 loss: 4.09754784e-06
Iter: 519 loss: 4.06376466e-06
Iter: 520 loss: 4.05602805e-06
Iter: 521 loss: 4.04759567e-06
Iter: 522 loss: 4.04627826e-06
Iter: 523 loss: 4.03852709e-06
Iter: 524 loss: 4.03786635e-06
Iter: 525 loss: 4.032167e-06
Iter: 526 loss: 4.02362048e-06
Iter: 527 loss: 4.02342403e-06
Iter: 528 loss: 4.01284706e-06
Iter: 529 loss: 4.02514524e-06
Iter: 530 loss: 4.00727822e-06
Iter: 531 loss: 3.99530791e-06
Iter: 532 loss: 4.06223398e-06
Iter: 533 loss: 3.99358305e-06
Iter: 534 loss: 3.98274824e-06
Iter: 535 loss: 3.9892484e-06
Iter: 536 loss: 3.97586382e-06
Iter: 537 loss: 3.97117765e-06
Iter: 538 loss: 3.96893665e-06
Iter: 539 loss: 3.96355153e-06
Iter: 540 loss: 3.95299412e-06
Iter: 541 loss: 4.16305556e-06
Iter: 542 loss: 3.95298139e-06
Iter: 543 loss: 3.94357039e-06
Iter: 544 loss: 3.94362269e-06
Iter: 545 loss: 3.93711343e-06
Iter: 546 loss: 3.93467508e-06
Iter: 547 loss: 3.93103619e-06
Iter: 548 loss: 3.9228803e-06
Iter: 549 loss: 3.93403525e-06
Iter: 550 loss: 3.91868798e-06
Iter: 551 loss: 3.90961077e-06
Iter: 552 loss: 4.00233239e-06
Iter: 553 loss: 3.90927835e-06
Iter: 554 loss: 3.90372134e-06
Iter: 555 loss: 3.89590105e-06
Iter: 556 loss: 3.89563229e-06
Iter: 557 loss: 3.88571607e-06
Iter: 558 loss: 3.96059841e-06
Iter: 559 loss: 3.88495164e-06
Iter: 560 loss: 3.87530599e-06
Iter: 561 loss: 3.903e-06
Iter: 562 loss: 3.87216187e-06
Iter: 563 loss: 3.86392458e-06
Iter: 564 loss: 3.86900638e-06
Iter: 565 loss: 3.85866042e-06
Iter: 566 loss: 3.85012663e-06
Iter: 567 loss: 3.9343804e-06
Iter: 568 loss: 3.84981058e-06
Iter: 569 loss: 3.84268242e-06
Iter: 570 loss: 3.84509303e-06
Iter: 571 loss: 3.83757606e-06
Iter: 572 loss: 3.82875396e-06
Iter: 573 loss: 3.83346742e-06
Iter: 574 loss: 3.82296139e-06
Iter: 575 loss: 3.81295e-06
Iter: 576 loss: 3.85132535e-06
Iter: 577 loss: 3.81059476e-06
Iter: 578 loss: 3.80324605e-06
Iter: 579 loss: 3.80312167e-06
Iter: 580 loss: 3.79867561e-06
Iter: 581 loss: 3.7893742e-06
Iter: 582 loss: 3.9388633e-06
Iter: 583 loss: 3.78907e-06
Iter: 584 loss: 3.77954439e-06
Iter: 585 loss: 3.86251031e-06
Iter: 586 loss: 3.77913261e-06
Iter: 587 loss: 3.77056358e-06
Iter: 588 loss: 3.80556958e-06
Iter: 589 loss: 3.76877733e-06
Iter: 590 loss: 3.7624327e-06
Iter: 591 loss: 3.75672039e-06
Iter: 592 loss: 3.75521813e-06
Iter: 593 loss: 3.74519186e-06
Iter: 594 loss: 3.76056255e-06
Iter: 595 loss: 3.74052115e-06
Iter: 596 loss: 3.73182775e-06
Iter: 597 loss: 3.73178614e-06
Iter: 598 loss: 3.72584873e-06
Iter: 599 loss: 3.72894806e-06
Iter: 600 loss: 3.72199293e-06
Iter: 601 loss: 3.71457872e-06
Iter: 602 loss: 3.72847921e-06
Iter: 603 loss: 3.71130841e-06
Iter: 604 loss: 3.7030768e-06
Iter: 605 loss: 3.74933552e-06
Iter: 606 loss: 3.70184353e-06
Iter: 607 loss: 3.69399208e-06
Iter: 608 loss: 3.68768428e-06
Iter: 609 loss: 3.68530527e-06
Iter: 610 loss: 3.6758961e-06
Iter: 611 loss: 3.69639361e-06
Iter: 612 loss: 3.67230268e-06
Iter: 613 loss: 3.66525273e-06
Iter: 614 loss: 3.66499034e-06
Iter: 615 loss: 3.65902315e-06
Iter: 616 loss: 3.65332653e-06
Iter: 617 loss: 3.65191386e-06
Iter: 618 loss: 3.64445782e-06
Iter: 619 loss: 3.65715323e-06
Iter: 620 loss: 3.64104903e-06
Iter: 621 loss: 3.63276581e-06
Iter: 622 loss: 3.71846136e-06
Iter: 623 loss: 3.63258505e-06
Iter: 624 loss: 3.62705259e-06
Iter: 625 loss: 3.61916113e-06
Iter: 626 loss: 3.61891125e-06
Iter: 627 loss: 3.61170055e-06
Iter: 628 loss: 3.61171851e-06
Iter: 629 loss: 3.60502145e-06
Iter: 630 loss: 3.59879459e-06
Iter: 631 loss: 3.59712089e-06
Iter: 632 loss: 3.58849456e-06
Iter: 633 loss: 3.60474314e-06
Iter: 634 loss: 3.58480565e-06
Iter: 635 loss: 3.5756093e-06
Iter: 636 loss: 3.60123931e-06
Iter: 637 loss: 3.57264867e-06
Iter: 638 loss: 3.56165629e-06
Iter: 639 loss: 3.63007894e-06
Iter: 640 loss: 3.56027704e-06
Iter: 641 loss: 3.55228e-06
Iter: 642 loss: 3.58223315e-06
Iter: 643 loss: 3.55031125e-06
Iter: 644 loss: 3.54448e-06
Iter: 645 loss: 3.54754525e-06
Iter: 646 loss: 3.54060739e-06
Iter: 647 loss: 3.53207952e-06
Iter: 648 loss: 3.58412512e-06
Iter: 649 loss: 3.53106407e-06
Iter: 650 loss: 3.52529969e-06
Iter: 651 loss: 3.5287494e-06
Iter: 652 loss: 3.52157576e-06
Iter: 653 loss: 3.51331505e-06
Iter: 654 loss: 3.57220551e-06
Iter: 655 loss: 3.51263793e-06
Iter: 656 loss: 3.50774553e-06
Iter: 657 loss: 3.49933839e-06
Iter: 658 loss: 3.49934817e-06
Iter: 659 loss: 3.48917911e-06
Iter: 660 loss: 3.51400945e-06
Iter: 661 loss: 3.48560116e-06
Iter: 662 loss: 3.47869309e-06
Iter: 663 loss: 3.47865011e-06
Iter: 664 loss: 3.47264881e-06
Iter: 665 loss: 3.47591117e-06
Iter: 666 loss: 3.46876504e-06
Iter: 667 loss: 3.46199295e-06
Iter: 668 loss: 3.45981e-06
Iter: 669 loss: 3.45588114e-06
Iter: 670 loss: 3.44827345e-06
Iter: 671 loss: 3.51659082e-06
Iter: 672 loss: 3.4478619e-06
Iter: 673 loss: 3.44105274e-06
Iter: 674 loss: 3.46416391e-06
Iter: 675 loss: 3.43915781e-06
Iter: 676 loss: 3.43196052e-06
Iter: 677 loss: 3.4324346e-06
Iter: 678 loss: 3.42631756e-06
Iter: 679 loss: 3.41888472e-06
Iter: 680 loss: 3.43579381e-06
Iter: 681 loss: 3.41613531e-06
Iter: 682 loss: 3.40835413e-06
Iter: 683 loss: 3.47485684e-06
Iter: 684 loss: 3.40796305e-06
Iter: 685 loss: 3.40080555e-06
Iter: 686 loss: 3.41324244e-06
Iter: 687 loss: 3.39773896e-06
Iter: 688 loss: 3.39253188e-06
Iter: 689 loss: 3.43292254e-06
Iter: 690 loss: 3.39219514e-06
Iter: 691 loss: 3.38713448e-06
Iter: 692 loss: 3.38424366e-06
Iter: 693 loss: 3.38209475e-06
Iter: 694 loss: 3.37582151e-06
Iter: 695 loss: 3.40095744e-06
Iter: 696 loss: 3.37439042e-06
Iter: 697 loss: 3.36738344e-06
Iter: 698 loss: 3.38478367e-06
Iter: 699 loss: 3.36498715e-06
Iter: 700 loss: 3.35922505e-06
Iter: 701 loss: 3.3511235e-06
Iter: 702 loss: 3.35075129e-06
Iter: 703 loss: 3.33984417e-06
Iter: 704 loss: 3.39743656e-06
Iter: 705 loss: 3.33810476e-06
Iter: 706 loss: 3.33204093e-06
Iter: 707 loss: 3.39895041e-06
Iter: 708 loss: 3.33178173e-06
Iter: 709 loss: 3.3249114e-06
Iter: 710 loss: 3.33069829e-06
Iter: 711 loss: 3.320823e-06
Iter: 712 loss: 3.31397132e-06
Iter: 713 loss: 3.32300237e-06
Iter: 714 loss: 3.310382e-06
Iter: 715 loss: 3.30320654e-06
Iter: 716 loss: 3.31295223e-06
Iter: 717 loss: 3.29943623e-06
Iter: 718 loss: 3.2921339e-06
Iter: 719 loss: 3.35774962e-06
Iter: 720 loss: 3.29177919e-06
Iter: 721 loss: 3.28543115e-06
Iter: 722 loss: 3.30648118e-06
Iter: 723 loss: 3.28368151e-06
Iter: 724 loss: 3.27839689e-06
Iter: 725 loss: 3.28172769e-06
Iter: 726 loss: 3.27493e-06
Iter: 727 loss: 3.26839654e-06
Iter: 728 loss: 3.34306924e-06
Iter: 729 loss: 3.26838153e-06
Iter: 730 loss: 3.26428426e-06
Iter: 731 loss: 3.25536939e-06
Iter: 732 loss: 3.37898655e-06
Iter: 733 loss: 3.25493943e-06
Iter: 734 loss: 3.24990856e-06
Iter: 735 loss: 3.24935672e-06
Iter: 736 loss: 3.24420716e-06
Iter: 737 loss: 3.23706445e-06
Iter: 738 loss: 3.23687482e-06
Iter: 739 loss: 3.23068798e-06
Iter: 740 loss: 3.29879595e-06
Iter: 741 loss: 3.23058066e-06
Iter: 742 loss: 3.22459027e-06
Iter: 743 loss: 3.23195377e-06
Iter: 744 loss: 3.2216285e-06
Iter: 745 loss: 3.2153157e-06
Iter: 746 loss: 3.21289099e-06
Iter: 747 loss: 3.20949266e-06
Iter: 748 loss: 3.20202662e-06
Iter: 749 loss: 3.24573512e-06
Iter: 750 loss: 3.20108711e-06
Iter: 751 loss: 3.19498668e-06
Iter: 752 loss: 3.24342591e-06
Iter: 753 loss: 3.1946106e-06
Iter: 754 loss: 3.18878347e-06
Iter: 755 loss: 3.19362857e-06
Iter: 756 loss: 3.18527054e-06
Iter: 757 loss: 3.17868489e-06
Iter: 758 loss: 3.17640684e-06
Iter: 759 loss: 3.17262356e-06
Iter: 760 loss: 3.16449314e-06
Iter: 761 loss: 3.18716684e-06
Iter: 762 loss: 3.1618822e-06
Iter: 763 loss: 3.15547049e-06
Iter: 764 loss: 3.22632377e-06
Iter: 765 loss: 3.15531474e-06
Iter: 766 loss: 3.14795534e-06
Iter: 767 loss: 3.16031901e-06
Iter: 768 loss: 3.14463091e-06
Iter: 769 loss: 3.14001863e-06
Iter: 770 loss: 3.14885847e-06
Iter: 771 loss: 3.13815394e-06
Iter: 772 loss: 3.13215037e-06
Iter: 773 loss: 3.146464e-06
Iter: 774 loss: 3.12995462e-06
Iter: 775 loss: 3.12404063e-06
Iter: 776 loss: 3.12737166e-06
Iter: 777 loss: 3.12019074e-06
Iter: 778 loss: 3.11388544e-06
Iter: 779 loss: 3.13983628e-06
Iter: 780 loss: 3.11257054e-06
Iter: 781 loss: 3.10556402e-06
Iter: 782 loss: 3.13806322e-06
Iter: 783 loss: 3.10429823e-06
Iter: 784 loss: 3.09968027e-06
Iter: 785 loss: 3.09768802e-06
Iter: 786 loss: 3.09537108e-06
Iter: 787 loss: 3.08805647e-06
Iter: 788 loss: 3.10056362e-06
Iter: 789 loss: 3.08474227e-06
Iter: 790 loss: 3.07788696e-06
Iter: 791 loss: 3.14960698e-06
Iter: 792 loss: 3.0777137e-06
Iter: 793 loss: 3.07210371e-06
Iter: 794 loss: 3.08539529e-06
Iter: 795 loss: 3.0701442e-06
Iter: 796 loss: 3.06490074e-06
Iter: 797 loss: 3.06486413e-06
Iter: 798 loss: 3.06067523e-06
Iter: 799 loss: 3.05350773e-06
Iter: 800 loss: 3.13386113e-06
Iter: 801 loss: 3.05337153e-06
Iter: 802 loss: 3.04967193e-06
Iter: 803 loss: 3.04538753e-06
Iter: 804 loss: 3.04477044e-06
Iter: 805 loss: 3.03796423e-06
Iter: 806 loss: 3.04736113e-06
Iter: 807 loss: 3.03462116e-06
Iter: 808 loss: 3.03083198e-06
Iter: 809 loss: 3.03028582e-06
Iter: 810 loss: 3.02590342e-06
Iter: 811 loss: 3.02060607e-06
Iter: 812 loss: 3.02006538e-06
Iter: 813 loss: 3.01414207e-06
Iter: 814 loss: 3.01854084e-06
Iter: 815 loss: 3.01058299e-06
Iter: 816 loss: 3.00438728e-06
Iter: 817 loss: 3.04927244e-06
Iter: 818 loss: 3.00391048e-06
Iter: 819 loss: 2.99787098e-06
Iter: 820 loss: 3.02270246e-06
Iter: 821 loss: 2.99662679e-06
Iter: 822 loss: 2.99208477e-06
Iter: 823 loss: 2.99110161e-06
Iter: 824 loss: 2.98819486e-06
Iter: 825 loss: 2.98184341e-06
Iter: 826 loss: 3.01320802e-06
Iter: 827 loss: 2.98075042e-06
Iter: 828 loss: 2.97581164e-06
Iter: 829 loss: 3.01905538e-06
Iter: 830 loss: 2.97562474e-06
Iter: 831 loss: 2.97141946e-06
Iter: 832 loss: 2.96447752e-06
Iter: 833 loss: 2.96448025e-06
Iter: 834 loss: 2.95806603e-06
Iter: 835 loss: 2.9981486e-06
Iter: 836 loss: 2.9573489e-06
Iter: 837 loss: 2.95136306e-06
Iter: 838 loss: 2.99224166e-06
Iter: 839 loss: 2.95079712e-06
Iter: 840 loss: 2.9461753e-06
Iter: 841 loss: 2.94256142e-06
Iter: 842 loss: 2.94102711e-06
Iter: 843 loss: 2.93625908e-06
Iter: 844 loss: 2.93620087e-06
Iter: 845 loss: 2.93244852e-06
Iter: 846 loss: 2.93165635e-06
Iter: 847 loss: 2.9289663e-06
Iter: 848 loss: 2.92373647e-06
Iter: 849 loss: 2.93124594e-06
Iter: 850 loss: 2.92120535e-06
Iter: 851 loss: 2.91589822e-06
Iter: 852 loss: 2.99955059e-06
Iter: 853 loss: 2.91591186e-06
Iter: 854 loss: 2.91283118e-06
Iter: 855 loss: 2.90699631e-06
Iter: 856 loss: 3.03610295e-06
Iter: 857 loss: 2.90696971e-06
Iter: 858 loss: 2.90001253e-06
Iter: 859 loss: 2.92121854e-06
Iter: 860 loss: 2.89799709e-06
Iter: 861 loss: 2.89219201e-06
Iter: 862 loss: 2.90269236e-06
Iter: 863 loss: 2.88967226e-06
Iter: 864 loss: 2.88379647e-06
Iter: 865 loss: 2.95608174e-06
Iter: 866 loss: 2.88371712e-06
Iter: 867 loss: 2.87887906e-06
Iter: 868 loss: 2.87880948e-06
Iter: 869 loss: 2.87497096e-06
Iter: 870 loss: 2.86947056e-06
Iter: 871 loss: 2.87257467e-06
Iter: 872 loss: 2.86582326e-06
Iter: 873 loss: 2.8614254e-06
Iter: 874 loss: 2.86118484e-06
Iter: 875 loss: 2.85766032e-06
Iter: 876 loss: 2.85361693e-06
Iter: 877 loss: 2.85315946e-06
Iter: 878 loss: 2.84733051e-06
Iter: 879 loss: 2.85071155e-06
Iter: 880 loss: 2.84347379e-06
Iter: 881 loss: 2.83663712e-06
Iter: 882 loss: 2.92625464e-06
Iter: 883 loss: 2.83665031e-06
Iter: 884 loss: 2.8315817e-06
Iter: 885 loss: 2.83610916e-06
Iter: 886 loss: 2.82865722e-06
Iter: 887 loss: 2.82394603e-06
Iter: 888 loss: 2.84162752e-06
Iter: 889 loss: 2.82278052e-06
Iter: 890 loss: 2.81719349e-06
Iter: 891 loss: 2.83354075e-06
Iter: 892 loss: 2.81549887e-06
Iter: 893 loss: 2.81109828e-06
Iter: 894 loss: 2.81052871e-06
Iter: 895 loss: 2.80739596e-06
Iter: 896 loss: 2.80221593e-06
Iter: 897 loss: 2.85122042e-06
Iter: 898 loss: 2.80210679e-06
Iter: 899 loss: 2.79694086e-06
Iter: 900 loss: 2.80920631e-06
Iter: 901 loss: 2.79498749e-06
Iter: 902 loss: 2.79088817e-06
Iter: 903 loss: 2.7867618e-06
Iter: 904 loss: 2.78594644e-06
Iter: 905 loss: 2.77904223e-06
Iter: 906 loss: 2.79717619e-06
Iter: 907 loss: 2.77670324e-06
Iter: 908 loss: 2.77136178e-06
Iter: 909 loss: 2.82500582e-06
Iter: 910 loss: 2.77115032e-06
Iter: 911 loss: 2.76601509e-06
Iter: 912 loss: 2.77549657e-06
Iter: 913 loss: 2.76385458e-06
Iter: 914 loss: 2.75951766e-06
Iter: 915 loss: 2.75933553e-06
Iter: 916 loss: 2.75603315e-06
Iter: 917 loss: 2.75156026e-06
Iter: 918 loss: 2.79697929e-06
Iter: 919 loss: 2.75140292e-06
Iter: 920 loss: 2.74628906e-06
Iter: 921 loss: 2.7478975e-06
Iter: 922 loss: 2.74271588e-06
Iter: 923 loss: 2.73746173e-06
Iter: 924 loss: 2.75178945e-06
Iter: 925 loss: 2.73580554e-06
Iter: 926 loss: 2.73059891e-06
Iter: 927 loss: 2.74159902e-06
Iter: 928 loss: 2.72845409e-06
Iter: 929 loss: 2.72322677e-06
Iter: 930 loss: 2.76633909e-06
Iter: 931 loss: 2.72286388e-06
Iter: 932 loss: 2.7192425e-06
Iter: 933 loss: 2.71436693e-06
Iter: 934 loss: 2.71414615e-06
Iter: 935 loss: 2.71088265e-06
Iter: 936 loss: 2.71039107e-06
Iter: 937 loss: 2.70691635e-06
Iter: 938 loss: 2.70286e-06
Iter: 939 loss: 2.70239411e-06
Iter: 940 loss: 2.69813563e-06
Iter: 941 loss: 2.74802824e-06
Iter: 942 loss: 2.69807151e-06
Iter: 943 loss: 2.69396446e-06
Iter: 944 loss: 2.69162797e-06
Iter: 945 loss: 2.68987174e-06
Iter: 946 loss: 2.68443455e-06
Iter: 947 loss: 2.68174e-06
Iter: 948 loss: 2.67913515e-06
Iter: 949 loss: 2.67254859e-06
Iter: 950 loss: 2.7113706e-06
Iter: 951 loss: 2.67171617e-06
Iter: 952 loss: 2.66578422e-06
Iter: 953 loss: 2.67836595e-06
Iter: 954 loss: 2.66337383e-06
Iter: 955 loss: 2.65991048e-06
Iter: 956 loss: 2.65981816e-06
Iter: 957 loss: 2.65653807e-06
Iter: 958 loss: 2.65079211e-06
Iter: 959 loss: 2.65078324e-06
Iter: 960 loss: 2.64467531e-06
Iter: 961 loss: 2.68791609e-06
Iter: 962 loss: 2.64416531e-06
Iter: 963 loss: 2.63996162e-06
Iter: 964 loss: 2.66484722e-06
Iter: 965 loss: 2.63943912e-06
Iter: 966 loss: 2.63493939e-06
Iter: 967 loss: 2.63757647e-06
Iter: 968 loss: 2.63200377e-06
Iter: 969 loss: 2.62717549e-06
Iter: 970 loss: 2.63057291e-06
Iter: 971 loss: 2.62414369e-06
Iter: 972 loss: 2.61924151e-06
Iter: 973 loss: 2.67341443e-06
Iter: 974 loss: 2.61918967e-06
Iter: 975 loss: 2.61466698e-06
Iter: 976 loss: 2.62023332e-06
Iter: 977 loss: 2.61243918e-06
Iter: 978 loss: 2.60868478e-06
Iter: 979 loss: 2.64499295e-06
Iter: 980 loss: 2.60856814e-06
Iter: 981 loss: 2.60534489e-06
Iter: 982 loss: 2.60298839e-06
Iter: 983 loss: 2.60195247e-06
Iter: 984 loss: 2.5973352e-06
Iter: 985 loss: 2.59866556e-06
Iter: 986 loss: 2.59407329e-06
Iter: 987 loss: 2.58874775e-06
Iter: 988 loss: 2.58876094e-06
Iter: 989 loss: 2.58548653e-06
Iter: 990 loss: 2.58102455e-06
Iter: 991 loss: 2.58081354e-06
Iter: 992 loss: 2.57552392e-06
Iter: 993 loss: 2.58644036e-06
Iter: 994 loss: 2.57331385e-06
Iter: 995 loss: 2.5672166e-06
Iter: 996 loss: 2.58843329e-06
Iter: 997 loss: 2.56564272e-06
Iter: 998 loss: 2.56134081e-06
Iter: 999 loss: 2.62100866e-06
Iter: 1000 loss: 2.56126077e-06
Iter: 1001 loss: 2.55773875e-06
Iter: 1002 loss: 2.5582508e-06
Iter: 1003 loss: 2.55499663e-06
Iter: 1004 loss: 2.55012105e-06
Iter: 1005 loss: 2.55187774e-06
Iter: 1006 loss: 2.54672273e-06
Iter: 1007 loss: 2.54311954e-06
Iter: 1008 loss: 2.54296856e-06
Iter: 1009 loss: 2.54016186e-06
Iter: 1010 loss: 2.5362915e-06
Iter: 1011 loss: 2.53612643e-06
Iter: 1012 loss: 2.53097937e-06
Iter: 1013 loss: 2.53980738e-06
Iter: 1014 loss: 2.52867471e-06
Iter: 1015 loss: 2.52346035e-06
Iter: 1016 loss: 2.52344216e-06
Iter: 1017 loss: 2.52107884e-06
Iter: 1018 loss: 2.51986171e-06
Iter: 1019 loss: 2.5188051e-06
Iter: 1020 loss: 2.514274e-06
Iter: 1021 loss: 2.52574728e-06
Iter: 1022 loss: 2.512842e-06
Iter: 1023 loss: 2.5086249e-06
Iter: 1024 loss: 2.51148094e-06
Iter: 1025 loss: 2.50621792e-06
Iter: 1026 loss: 2.50324206e-06
Iter: 1027 loss: 2.54988026e-06
Iter: 1028 loss: 2.50325161e-06
Iter: 1029 loss: 2.50042012e-06
Iter: 1030 loss: 2.49768755e-06
Iter: 1031 loss: 2.49713526e-06
Iter: 1032 loss: 2.4923479e-06
Iter: 1033 loss: 2.49237928e-06
Iter: 1034 loss: 2.48845754e-06
Iter: 1035 loss: 2.48256106e-06
Iter: 1036 loss: 2.51448955e-06
Iter: 1037 loss: 2.48165225e-06
Iter: 1038 loss: 2.47686899e-06
Iter: 1039 loss: 2.4944602e-06
Iter: 1040 loss: 2.47560365e-06
Iter: 1041 loss: 2.47157e-06
Iter: 1042 loss: 2.5086963e-06
Iter: 1043 loss: 2.47135654e-06
Iter: 1044 loss: 2.46774243e-06
Iter: 1045 loss: 2.46880813e-06
Iter: 1046 loss: 2.46526292e-06
Iter: 1047 loss: 2.46169611e-06
Iter: 1048 loss: 2.47014395e-06
Iter: 1049 loss: 2.46039735e-06
Iter: 1050 loss: 2.45636579e-06
Iter: 1051 loss: 2.4839635e-06
Iter: 1052 loss: 2.45597403e-06
Iter: 1053 loss: 2.4527676e-06
Iter: 1054 loss: 2.44951866e-06
Iter: 1055 loss: 2.44893249e-06
Iter: 1056 loss: 2.44553235e-06
Iter: 1057 loss: 2.44543526e-06
Iter: 1058 loss: 2.44231251e-06
Iter: 1059 loss: 2.44252078e-06
Iter: 1060 loss: 2.4397923e-06
Iter: 1061 loss: 2.43643649e-06
Iter: 1062 loss: 2.44015882e-06
Iter: 1063 loss: 2.43459522e-06
Iter: 1064 loss: 2.43064733e-06
Iter: 1065 loss: 2.46022e-06
Iter: 1066 loss: 2.43033128e-06
Iter: 1067 loss: 2.42722967e-06
Iter: 1068 loss: 2.42346914e-06
Iter: 1069 loss: 2.42303531e-06
Iter: 1070 loss: 2.41939324e-06
Iter: 1071 loss: 2.4193489e-06
Iter: 1072 loss: 2.41614862e-06
Iter: 1073 loss: 2.41220141e-06
Iter: 1074 loss: 2.41181647e-06
Iter: 1075 loss: 2.40714e-06
Iter: 1076 loss: 2.41138105e-06
Iter: 1077 loss: 2.40433906e-06
Iter: 1078 loss: 2.39916517e-06
Iter: 1079 loss: 2.42501642e-06
Iter: 1080 loss: 2.39823976e-06
Iter: 1081 loss: 2.39413112e-06
Iter: 1082 loss: 2.4201513e-06
Iter: 1083 loss: 2.39369365e-06
Iter: 1084 loss: 2.38942562e-06
Iter: 1085 loss: 2.40275222e-06
Iter: 1086 loss: 2.38821258e-06
Iter: 1087 loss: 2.3845223e-06
Iter: 1088 loss: 2.39067958e-06
Iter: 1089 loss: 2.38284724e-06
Iter: 1090 loss: 2.37933682e-06
Iter: 1091 loss: 2.39153428e-06
Iter: 1092 loss: 2.37839822e-06
Iter: 1093 loss: 2.37373752e-06
Iter: 1094 loss: 2.38284724e-06
Iter: 1095 loss: 2.37191443e-06
Iter: 1096 loss: 2.36882943e-06
Iter: 1097 loss: 2.3905659e-06
Iter: 1098 loss: 2.36845221e-06
Iter: 1099 loss: 2.36543224e-06
Iter: 1100 loss: 2.36679625e-06
Iter: 1101 loss: 2.3634e-06
Iter: 1102 loss: 2.36031019e-06
Iter: 1103 loss: 2.35551852e-06
Iter: 1104 loss: 2.35548964e-06
Iter: 1105 loss: 2.35191828e-06
Iter: 1106 loss: 2.35161929e-06
Iter: 1107 loss: 2.34832441e-06
Iter: 1108 loss: 2.34974891e-06
Iter: 1109 loss: 2.34601202e-06
Iter: 1110 loss: 2.34237154e-06
Iter: 1111 loss: 2.35186849e-06
Iter: 1112 loss: 2.34111781e-06
Iter: 1113 loss: 2.33762421e-06
Iter: 1114 loss: 2.35573361e-06
Iter: 1115 loss: 2.33700644e-06
Iter: 1116 loss: 2.33357878e-06
Iter: 1117 loss: 2.33396258e-06
Iter: 1118 loss: 2.33092442e-06
Iter: 1119 loss: 2.32670163e-06
Iter: 1120 loss: 2.33117271e-06
Iter: 1121 loss: 2.32446496e-06
Iter: 1122 loss: 2.3199691e-06
Iter: 1123 loss: 2.33644073e-06
Iter: 1124 loss: 2.3189059e-06
Iter: 1125 loss: 2.3143416e-06
Iter: 1126 loss: 2.35781e-06
Iter: 1127 loss: 2.31415333e-06
Iter: 1128 loss: 2.31138574e-06
Iter: 1129 loss: 2.31062586e-06
Iter: 1130 loss: 2.30896262e-06
Iter: 1131 loss: 2.30575188e-06
Iter: 1132 loss: 2.34672234e-06
Iter: 1133 loss: 2.30575051e-06
Iter: 1134 loss: 2.3029661e-06
Iter: 1135 loss: 2.30142382e-06
Iter: 1136 loss: 2.30009891e-06
Iter: 1137 loss: 2.2966035e-06
Iter: 1138 loss: 2.31444142e-06
Iter: 1139 loss: 2.29603165e-06
Iter: 1140 loss: 2.29245188e-06
Iter: 1141 loss: 2.29797274e-06
Iter: 1142 loss: 2.29069133e-06
Iter: 1143 loss: 2.28744057e-06
Iter: 1144 loss: 2.28368503e-06
Iter: 1145 loss: 2.2832437e-06
Iter: 1146 loss: 2.27918167e-06
Iter: 1147 loss: 2.27913301e-06
Iter: 1148 loss: 2.27514511e-06
Iter: 1149 loss: 2.28094905e-06
Iter: 1150 loss: 2.27314013e-06
Iter: 1151 loss: 2.26985526e-06
Iter: 1152 loss: 2.26957673e-06
Iter: 1153 loss: 2.26728162e-06
Iter: 1154 loss: 2.26289194e-06
Iter: 1155 loss: 2.29955322e-06
Iter: 1156 loss: 2.26258931e-06
Iter: 1157 loss: 2.25930103e-06
Iter: 1158 loss: 2.2709055e-06
Iter: 1159 loss: 2.25847089e-06
Iter: 1160 loss: 2.25550707e-06
Iter: 1161 loss: 2.25322401e-06
Iter: 1162 loss: 2.25228837e-06
Iter: 1163 loss: 2.24799714e-06
Iter: 1164 loss: 2.28198019e-06
Iter: 1165 loss: 2.247667e-06
Iter: 1166 loss: 2.24434689e-06
Iter: 1167 loss: 2.26993961e-06
Iter: 1168 loss: 2.24408359e-06
Iter: 1169 loss: 2.24163796e-06
Iter: 1170 loss: 2.24060159e-06
Iter: 1171 loss: 2.2393524e-06
Iter: 1172 loss: 2.23574216e-06
Iter: 1173 loss: 2.27205055e-06
Iter: 1174 loss: 2.23559891e-06
Iter: 1175 loss: 2.23320171e-06
Iter: 1176 loss: 2.22997141e-06
Iter: 1177 loss: 2.22983135e-06
Iter: 1178 loss: 2.2267202e-06
Iter: 1179 loss: 2.26498878e-06
Iter: 1180 loss: 2.22667563e-06
Iter: 1181 loss: 2.22341191e-06
Iter: 1182 loss: 2.22015888e-06
Iter: 1183 loss: 2.21957043e-06
Iter: 1184 loss: 2.21555433e-06
Iter: 1185 loss: 2.23031884e-06
Iter: 1186 loss: 2.2146246e-06
Iter: 1187 loss: 2.21112168e-06
Iter: 1188 loss: 2.24190399e-06
Iter: 1189 loss: 2.21094865e-06
Iter: 1190 loss: 2.20772472e-06
Iter: 1191 loss: 2.20950415e-06
Iter: 1192 loss: 2.20564675e-06
Iter: 1193 loss: 2.20243828e-06
Iter: 1194 loss: 2.20376751e-06
Iter: 1195 loss: 2.20021775e-06
Iter: 1196 loss: 2.19627805e-06
Iter: 1197 loss: 2.22199742e-06
Iter: 1198 loss: 2.19587764e-06
Iter: 1199 loss: 2.19272533e-06
Iter: 1200 loss: 2.21094206e-06
Iter: 1201 loss: 2.19237427e-06
Iter: 1202 loss: 2.18944729e-06
Iter: 1203 loss: 2.1847618e-06
Iter: 1204 loss: 2.18476157e-06
Iter: 1205 loss: 2.18338664e-06
Iter: 1206 loss: 2.18220475e-06
Iter: 1207 loss: 2.18004243e-06
Iter: 1208 loss: 2.17717479e-06
Iter: 1209 loss: 2.17697948e-06
Iter: 1210 loss: 2.17406819e-06
Iter: 1211 loss: 2.21214555e-06
Iter: 1212 loss: 2.17403863e-06
Iter: 1213 loss: 2.17156571e-06
Iter: 1214 loss: 2.16953867e-06
Iter: 1215 loss: 2.16886929e-06
Iter: 1216 loss: 2.1652113e-06
Iter: 1217 loss: 2.16971716e-06
Iter: 1218 loss: 2.1633914e-06
Iter: 1219 loss: 2.15992077e-06
Iter: 1220 loss: 2.21300434e-06
Iter: 1221 loss: 2.15994123e-06
Iter: 1222 loss: 2.15783166e-06
Iter: 1223 loss: 2.15586761e-06
Iter: 1224 loss: 2.15533964e-06
Iter: 1225 loss: 2.15194632e-06
Iter: 1226 loss: 2.15809928e-06
Iter: 1227 loss: 2.15038767e-06
Iter: 1228 loss: 2.1471451e-06
Iter: 1229 loss: 2.14714169e-06
Iter: 1230 loss: 2.14495094e-06
Iter: 1231 loss: 2.14152897e-06
Iter: 1232 loss: 2.14146621e-06
Iter: 1233 loss: 2.13724502e-06
Iter: 1234 loss: 2.15015166e-06
Iter: 1235 loss: 2.13603198e-06
Iter: 1236 loss: 2.13286194e-06
Iter: 1237 loss: 2.16164563e-06
Iter: 1238 loss: 2.13261092e-06
Iter: 1239 loss: 2.12976829e-06
Iter: 1240 loss: 2.13952603e-06
Iter: 1241 loss: 2.1290241e-06
Iter: 1242 loss: 2.12621399e-06
Iter: 1243 loss: 2.13174349e-06
Iter: 1244 loss: 2.12511941e-06
Iter: 1245 loss: 2.12203349e-06
Iter: 1246 loss: 2.13762064e-06
Iter: 1247 loss: 2.12158375e-06
Iter: 1248 loss: 2.11907263e-06
Iter: 1249 loss: 2.11563201e-06
Iter: 1250 loss: 2.11544057e-06
Iter: 1251 loss: 2.11201e-06
Iter: 1252 loss: 2.16655371e-06
Iter: 1253 loss: 2.11201723e-06
Iter: 1254 loss: 2.10910935e-06
Iter: 1255 loss: 2.10804092e-06
Iter: 1256 loss: 2.10649478e-06
Iter: 1257 loss: 2.10332723e-06
Iter: 1258 loss: 2.10602593e-06
Iter: 1259 loss: 2.10156622e-06
Iter: 1260 loss: 2.09776772e-06
Iter: 1261 loss: 2.13471731e-06
Iter: 1262 loss: 2.09768587e-06
Iter: 1263 loss: 2.09438713e-06
Iter: 1264 loss: 2.09864379e-06
Iter: 1265 loss: 2.09278915e-06
Iter: 1266 loss: 2.09027257e-06
Iter: 1267 loss: 2.09593372e-06
Iter: 1268 loss: 2.08935967e-06
Iter: 1269 loss: 2.086226e-06
Iter: 1270 loss: 2.09753057e-06
Iter: 1271 loss: 2.08542815e-06
Iter: 1272 loss: 2.08245888e-06
Iter: 1273 loss: 2.08363326e-06
Iter: 1274 loss: 2.08036045e-06
Iter: 1275 loss: 2.07716676e-06
Iter: 1276 loss: 2.07985318e-06
Iter: 1277 loss: 2.07521634e-06
Iter: 1278 loss: 2.07185781e-06
Iter: 1279 loss: 2.10989833e-06
Iter: 1280 loss: 2.07180437e-06
Iter: 1281 loss: 2.06865616e-06
Iter: 1282 loss: 2.08172833e-06
Iter: 1283 loss: 2.06791856e-06
Iter: 1284 loss: 2.06580125e-06
Iter: 1285 loss: 2.06179175e-06
Iter: 1286 loss: 2.1520973e-06
Iter: 1287 loss: 2.06182654e-06
Iter: 1288 loss: 2.05853894e-06
Iter: 1289 loss: 2.05841479e-06
Iter: 1290 loss: 2.05587025e-06
Iter: 1291 loss: 2.05657739e-06
Iter: 1292 loss: 2.05391893e-06
Iter: 1293 loss: 2.05114338e-06
Iter: 1294 loss: 2.05061e-06
Iter: 1295 loss: 2.04874141e-06
Iter: 1296 loss: 2.04540561e-06
Iter: 1297 loss: 2.0454579e-06
Iter: 1298 loss: 2.04310277e-06
Iter: 1299 loss: 2.0414252e-06
Iter: 1300 loss: 2.0406469e-06
Iter: 1301 loss: 2.03710442e-06
Iter: 1302 loss: 2.05380093e-06
Iter: 1303 loss: 2.03652e-06
Iter: 1304 loss: 2.03347577e-06
Iter: 1305 loss: 2.05187393e-06
Iter: 1306 loss: 2.03311697e-06
Iter: 1307 loss: 2.03014793e-06
Iter: 1308 loss: 2.02863066e-06
Iter: 1309 loss: 2.02724868e-06
Iter: 1310 loss: 2.02385854e-06
Iter: 1311 loss: 2.02818273e-06
Iter: 1312 loss: 2.02205706e-06
Iter: 1313 loss: 2.01957619e-06
Iter: 1314 loss: 2.0194675e-06
Iter: 1315 loss: 2.01691546e-06
Iter: 1316 loss: 2.01508465e-06
Iter: 1317 loss: 2.01414741e-06
Iter: 1318 loss: 2.01111652e-06
Iter: 1319 loss: 2.0459463e-06
Iter: 1320 loss: 2.01105831e-06
Iter: 1321 loss: 2.00857266e-06
Iter: 1322 loss: 2.00542331e-06
Iter: 1323 loss: 2.00519662e-06
Iter: 1324 loss: 2.00138902e-06
Iter: 1325 loss: 2.00602813e-06
Iter: 1326 loss: 1.99935835e-06
Iter: 1327 loss: 1.9959939e-06
Iter: 1328 loss: 2.04159824e-06
Iter: 1329 loss: 1.99599708e-06
Iter: 1330 loss: 1.99348733e-06
Iter: 1331 loss: 2.00520253e-06
Iter: 1332 loss: 1.99301894e-06
Iter: 1333 loss: 1.99059923e-06
Iter: 1334 loss: 1.98814382e-06
Iter: 1335 loss: 1.98773159e-06
Iter: 1336 loss: 1.98442171e-06
Iter: 1337 loss: 2.01037324e-06
Iter: 1338 loss: 1.98420457e-06
Iter: 1339 loss: 1.98101e-06
Iter: 1340 loss: 1.98617568e-06
Iter: 1341 loss: 1.97952704e-06
Iter: 1342 loss: 1.97623604e-06
Iter: 1343 loss: 1.98223825e-06
Iter: 1344 loss: 1.97488203e-06
Iter: 1345 loss: 1.97249551e-06
Iter: 1346 loss: 1.99891247e-06
Iter: 1347 loss: 1.97242252e-06
Iter: 1348 loss: 1.97023246e-06
Iter: 1349 loss: 1.97017584e-06
Iter: 1350 loss: 1.9683921e-06
Iter: 1351 loss: 1.96582278e-06
Iter: 1352 loss: 1.97116606e-06
Iter: 1353 loss: 1.96467226e-06
Iter: 1354 loss: 1.96212227e-06
Iter: 1355 loss: 1.99624355e-06
Iter: 1356 loss: 1.96213659e-06
Iter: 1357 loss: 1.96044198e-06
Iter: 1358 loss: 1.95745633e-06
Iter: 1359 loss: 2.02029605e-06
Iter: 1360 loss: 1.95737289e-06
Iter: 1361 loss: 1.95372604e-06
Iter: 1362 loss: 1.96501151e-06
Iter: 1363 loss: 1.95260054e-06
Iter: 1364 loss: 1.94980703e-06
Iter: 1365 loss: 1.98418184e-06
Iter: 1366 loss: 1.9498184e-06
Iter: 1367 loss: 1.94731933e-06
Iter: 1368 loss: 1.94785594e-06
Iter: 1369 loss: 1.94558265e-06
Iter: 1370 loss: 1.9423735e-06
Iter: 1371 loss: 1.9454485e-06
Iter: 1372 loss: 1.94058453e-06
Iter: 1373 loss: 1.93788924e-06
Iter: 1374 loss: 1.9742206e-06
Iter: 1375 loss: 1.9379363e-06
Iter: 1376 loss: 1.93552228e-06
Iter: 1377 loss: 1.93712413e-06
Iter: 1378 loss: 1.93402684e-06
Iter: 1379 loss: 1.93135565e-06
Iter: 1380 loss: 1.92964353e-06
Iter: 1381 loss: 1.92866764e-06
Iter: 1382 loss: 1.92615471e-06
Iter: 1383 loss: 1.92604148e-06
Iter: 1384 loss: 1.924049e-06
Iter: 1385 loss: 1.92239304e-06
Iter: 1386 loss: 1.92182461e-06
Iter: 1387 loss: 1.91919889e-06
Iter: 1388 loss: 1.94630684e-06
Iter: 1389 loss: 1.91914955e-06
Iter: 1390 loss: 1.91663753e-06
Iter: 1391 loss: 1.91926983e-06
Iter: 1392 loss: 1.91522986e-06
Iter: 1393 loss: 1.91323284e-06
Iter: 1394 loss: 1.92105017e-06
Iter: 1395 loss: 1.91274967e-06
Iter: 1396 loss: 1.91032359e-06
Iter: 1397 loss: 1.91205208e-06
Iter: 1398 loss: 1.90883065e-06
Iter: 1399 loss: 1.9059936e-06
Iter: 1400 loss: 1.90549088e-06
Iter: 1401 loss: 1.90360231e-06
Iter: 1402 loss: 1.89982222e-06
Iter: 1403 loss: 1.91272989e-06
Iter: 1404 loss: 1.89886077e-06
Iter: 1405 loss: 1.89554078e-06
Iter: 1406 loss: 1.90691321e-06
Iter: 1407 loss: 1.8946663e-06
Iter: 1408 loss: 1.8912865e-06
Iter: 1409 loss: 1.9197787e-06
Iter: 1410 loss: 1.89111074e-06
Iter: 1411 loss: 1.8890272e-06
Iter: 1412 loss: 1.88707202e-06
Iter: 1413 loss: 1.88661488e-06
Iter: 1414 loss: 1.88409524e-06
Iter: 1415 loss: 1.92183074e-06
Iter: 1416 loss: 1.88408376e-06
Iter: 1417 loss: 1.88177364e-06
Iter: 1418 loss: 1.88080321e-06
Iter: 1419 loss: 1.87957073e-06
Iter: 1420 loss: 1.87693854e-06
Iter: 1421 loss: 1.88123727e-06
Iter: 1422 loss: 1.87576245e-06
Iter: 1423 loss: 1.87359501e-06
Iter: 1424 loss: 1.87360934e-06
Iter: 1425 loss: 1.87157013e-06
Iter: 1426 loss: 1.87068554e-06
Iter: 1427 loss: 1.86957709e-06
Iter: 1428 loss: 1.86687078e-06
Iter: 1429 loss: 1.87892431e-06
Iter: 1430 loss: 1.86639454e-06
Iter: 1431 loss: 1.86397438e-06
Iter: 1432 loss: 1.8735501e-06
Iter: 1433 loss: 1.86343391e-06
Iter: 1434 loss: 1.86120644e-06
Iter: 1435 loss: 1.85893975e-06
Iter: 1436 loss: 1.85855265e-06
Iter: 1437 loss: 1.85619456e-06
Iter: 1438 loss: 1.85619842e-06
Iter: 1439 loss: 1.85400438e-06
Iter: 1440 loss: 1.85132831e-06
Iter: 1441 loss: 1.85111105e-06
Iter: 1442 loss: 1.84777423e-06
Iter: 1443 loss: 1.856876e-06
Iter: 1444 loss: 1.84667306e-06
Iter: 1445 loss: 1.84352609e-06
Iter: 1446 loss: 1.85415104e-06
Iter: 1447 loss: 1.84270459e-06
Iter: 1448 loss: 1.84013152e-06
Iter: 1449 loss: 1.87480236e-06
Iter: 1450 loss: 1.84010241e-06
Iter: 1451 loss: 1.83799466e-06
Iter: 1452 loss: 1.83691486e-06
Iter: 1453 loss: 1.83587349e-06
Iter: 1454 loss: 1.83327177e-06
Iter: 1455 loss: 1.83988493e-06
Iter: 1456 loss: 1.83245822e-06
Iter: 1457 loss: 1.82988174e-06
Iter: 1458 loss: 1.85073372e-06
Iter: 1459 loss: 1.82974372e-06
Iter: 1460 loss: 1.82748454e-06
Iter: 1461 loss: 1.82922486e-06
Iter: 1462 loss: 1.82616066e-06
Iter: 1463 loss: 1.82368956e-06
Iter: 1464 loss: 1.84005421e-06
Iter: 1465 loss: 1.82350141e-06
Iter: 1466 loss: 1.82122608e-06
Iter: 1467 loss: 1.82187091e-06
Iter: 1468 loss: 1.81960775e-06
Iter: 1469 loss: 1.81750534e-06
Iter: 1470 loss: 1.81988833e-06
Iter: 1471 loss: 1.81632117e-06
Iter: 1472 loss: 1.81363248e-06
Iter: 1473 loss: 1.83580153e-06
Iter: 1474 loss: 1.81348616e-06
Iter: 1475 loss: 1.81132179e-06
Iter: 1476 loss: 1.80969789e-06
Iter: 1477 loss: 1.80900042e-06
Iter: 1478 loss: 1.80606662e-06
Iter: 1479 loss: 1.80617974e-06
Iter: 1480 loss: 1.80374809e-06
Iter: 1481 loss: 1.79984215e-06
Iter: 1482 loss: 1.82421627e-06
Iter: 1483 loss: 1.79931203e-06
Iter: 1484 loss: 1.79647782e-06
Iter: 1485 loss: 1.82685721e-06
Iter: 1486 loss: 1.79644439e-06
Iter: 1487 loss: 1.79415758e-06
Iter: 1488 loss: 1.79633071e-06
Iter: 1489 loss: 1.79276662e-06
Iter: 1490 loss: 1.79027711e-06
Iter: 1491 loss: 1.79623146e-06
Iter: 1492 loss: 1.78928713e-06
Iter: 1493 loss: 1.78676748e-06
Iter: 1494 loss: 1.80489906e-06
Iter: 1495 loss: 1.78648179e-06
Iter: 1496 loss: 1.7844103e-06
Iter: 1497 loss: 1.78258142e-06
Iter: 1498 loss: 1.78209848e-06
Iter: 1499 loss: 1.78008395e-06
Iter: 1500 loss: 1.77999607e-06
Iter: 1501 loss: 1.77836796e-06
Iter: 1502 loss: 1.77726963e-06
Iter: 1503 loss: 1.77671279e-06
Iter: 1504 loss: 1.77443417e-06
Iter: 1505 loss: 1.77651987e-06
Iter: 1506 loss: 1.77314803e-06
Iter: 1507 loss: 1.77038362e-06
Iter: 1508 loss: 1.79713265e-06
Iter: 1509 loss: 1.77025754e-06
Iter: 1510 loss: 1.76844651e-06
Iter: 1511 loss: 1.76587253e-06
Iter: 1512 loss: 1.76584876e-06
Iter: 1513 loss: 1.76311346e-06
Iter: 1514 loss: 1.80072686e-06
Iter: 1515 loss: 1.76308095e-06
Iter: 1516 loss: 1.76079504e-06
Iter: 1517 loss: 1.76346794e-06
Iter: 1518 loss: 1.75961895e-06
Iter: 1519 loss: 1.75735386e-06
Iter: 1520 loss: 1.75859191e-06
Iter: 1521 loss: 1.75593891e-06
Iter: 1522 loss: 1.75287641e-06
Iter: 1523 loss: 1.75717616e-06
Iter: 1524 loss: 1.75137268e-06
Iter: 1525 loss: 1.74895274e-06
Iter: 1526 loss: 1.7782645e-06
Iter: 1527 loss: 1.74892079e-06
Iter: 1528 loss: 1.74656134e-06
Iter: 1529 loss: 1.75157743e-06
Iter: 1530 loss: 1.74559841e-06
Iter: 1531 loss: 1.74372008e-06
Iter: 1532 loss: 1.744055e-06
Iter: 1533 loss: 1.74222987e-06
Iter: 1534 loss: 1.73923445e-06
Iter: 1535 loss: 1.76002084e-06
Iter: 1536 loss: 1.73890737e-06
Iter: 1537 loss: 1.73701028e-06
Iter: 1538 loss: 1.74218962e-06
Iter: 1539 loss: 1.73644935e-06
Iter: 1540 loss: 1.73449564e-06
Iter: 1541 loss: 1.74071261e-06
Iter: 1542 loss: 1.73394551e-06
Iter: 1543 loss: 1.73221531e-06
Iter: 1544 loss: 1.72867135e-06
Iter: 1545 loss: 1.78901007e-06
Iter: 1546 loss: 1.72860655e-06
Iter: 1547 loss: 1.72597356e-06
Iter: 1548 loss: 1.72589e-06
Iter: 1549 loss: 1.72353202e-06
Iter: 1550 loss: 1.72789703e-06
Iter: 1551 loss: 1.7225492e-06
Iter: 1552 loss: 1.72053069e-06
Iter: 1553 loss: 1.71931833e-06
Iter: 1554 loss: 1.71842919e-06
Iter: 1555 loss: 1.71628983e-06
Iter: 1556 loss: 1.71625584e-06
Iter: 1557 loss: 1.71445845e-06
Iter: 1558 loss: 1.71365946e-06
Iter: 1559 loss: 1.71283341e-06
Iter: 1560 loss: 1.71034503e-06
Iter: 1561 loss: 1.71178158e-06
Iter: 1562 loss: 1.70881071e-06
Iter: 1563 loss: 1.70587009e-06
Iter: 1564 loss: 1.72710259e-06
Iter: 1565 loss: 1.70559542e-06
Iter: 1566 loss: 1.70361409e-06
Iter: 1567 loss: 1.72459704e-06
Iter: 1568 loss: 1.70355952e-06
Iter: 1569 loss: 1.70190935e-06
Iter: 1570 loss: 1.69911914e-06
Iter: 1571 loss: 1.69909629e-06
Iter: 1572 loss: 1.69728696e-06
Iter: 1573 loss: 1.69705118e-06
Iter: 1574 loss: 1.69556779e-06
Iter: 1575 loss: 1.69441444e-06
Iter: 1576 loss: 1.69387658e-06
Iter: 1577 loss: 1.69183181e-06
Iter: 1578 loss: 1.70282078e-06
Iter: 1579 loss: 1.69150803e-06
Iter: 1580 loss: 1.68936913e-06
Iter: 1581 loss: 1.69046166e-06
Iter: 1582 loss: 1.6879128e-06
Iter: 1583 loss: 1.68571717e-06
Iter: 1584 loss: 1.68983593e-06
Iter: 1585 loss: 1.6848187e-06
Iter: 1586 loss: 1.68222118e-06
Iter: 1587 loss: 1.70114436e-06
Iter: 1588 loss: 1.68193264e-06
Iter: 1589 loss: 1.68018164e-06
Iter: 1590 loss: 1.67881217e-06
Iter: 1591 loss: 1.67829273e-06
Iter: 1592 loss: 1.67568669e-06
Iter: 1593 loss: 1.68754923e-06
Iter: 1594 loss: 1.67516532e-06
Iter: 1595 loss: 1.67277494e-06
Iter: 1596 loss: 1.68538475e-06
Iter: 1597 loss: 1.67241114e-06
Iter: 1598 loss: 1.67003247e-06
Iter: 1599 loss: 1.6719664e-06
Iter: 1600 loss: 1.66859661e-06
Iter: 1601 loss: 1.66633095e-06
Iter: 1602 loss: 1.66556674e-06
Iter: 1603 loss: 1.66423092e-06
Iter: 1604 loss: 1.66113296e-06
Iter: 1605 loss: 1.69245425e-06
Iter: 1606 loss: 1.66105417e-06
Iter: 1607 loss: 1.65851702e-06
Iter: 1608 loss: 1.67551286e-06
Iter: 1609 loss: 1.65828646e-06
Iter: 1610 loss: 1.65675885e-06
Iter: 1611 loss: 1.65736719e-06
Iter: 1612 loss: 1.65575693e-06
Iter: 1613 loss: 1.65358256e-06
Iter: 1614 loss: 1.66678535e-06
Iter: 1615 loss: 1.65331869e-06
Iter: 1616 loss: 1.65183656e-06
Iter: 1617 loss: 1.65041615e-06
Iter: 1618 loss: 1.65017946e-06
Iter: 1619 loss: 1.64798848e-06
Iter: 1620 loss: 1.65923814e-06
Iter: 1621 loss: 1.64764242e-06
Iter: 1622 loss: 1.6450889e-06
Iter: 1623 loss: 1.64882874e-06
Iter: 1624 loss: 1.64382698e-06
Iter: 1625 loss: 1.64176697e-06
Iter: 1626 loss: 1.64565768e-06
Iter: 1627 loss: 1.64088101e-06
Iter: 1628 loss: 1.63839354e-06
Iter: 1629 loss: 1.65335871e-06
Iter: 1630 loss: 1.63809023e-06
Iter: 1631 loss: 1.63609639e-06
Iter: 1632 loss: 1.63508037e-06
Iter: 1633 loss: 1.63411028e-06
Iter: 1634 loss: 1.63188463e-06
Iter: 1635 loss: 1.64233415e-06
Iter: 1636 loss: 1.63144762e-06
Iter: 1637 loss: 1.6292737e-06
Iter: 1638 loss: 1.64121798e-06
Iter: 1639 loss: 1.62905587e-06
Iter: 1640 loss: 1.62708125e-06
Iter: 1641 loss: 1.6291018e-06
Iter: 1642 loss: 1.62594085e-06
Iter: 1643 loss: 1.62380172e-06
Iter: 1644 loss: 1.62415336e-06
Iter: 1645 loss: 1.62219e-06
Iter: 1646 loss: 1.62029437e-06
Iter: 1647 loss: 1.62021581e-06
Iter: 1648 loss: 1.61871844e-06
Iter: 1649 loss: 1.61793639e-06
Iter: 1650 loss: 1.61726268e-06
Iter: 1651 loss: 1.61541084e-06
Iter: 1652 loss: 1.62576509e-06
Iter: 1653 loss: 1.61514072e-06
Iter: 1654 loss: 1.61309026e-06
Iter: 1655 loss: 1.61420394e-06
Iter: 1656 loss: 1.61179059e-06
Iter: 1657 loss: 1.6098486e-06
Iter: 1658 loss: 1.60795753e-06
Iter: 1659 loss: 1.60755565e-06
Iter: 1660 loss: 1.60502896e-06
Iter: 1661 loss: 1.64195956e-06
Iter: 1662 loss: 1.60502202e-06
Iter: 1663 loss: 1.60295292e-06
Iter: 1664 loss: 1.61082505e-06
Iter: 1665 loss: 1.60245349e-06
Iter: 1666 loss: 1.60086813e-06
Iter: 1667 loss: 1.60062473e-06
Iter: 1668 loss: 1.59949877e-06
Iter: 1669 loss: 1.59713193e-06
Iter: 1670 loss: 1.61425e-06
Iter: 1671 loss: 1.59699061e-06
Iter: 1672 loss: 1.5950618e-06
Iter: 1673 loss: 1.59653337e-06
Iter: 1674 loss: 1.59390424e-06
Iter: 1675 loss: 1.59160777e-06
Iter: 1676 loss: 1.59209719e-06
Iter: 1677 loss: 1.58995556e-06
Iter: 1678 loss: 1.58724356e-06
Iter: 1679 loss: 1.6013521e-06
Iter: 1680 loss: 1.5867987e-06
Iter: 1681 loss: 1.58473961e-06
Iter: 1682 loss: 1.61036007e-06
Iter: 1683 loss: 1.58469709e-06
Iter: 1684 loss: 1.58317232e-06
Iter: 1685 loss: 1.58166324e-06
Iter: 1686 loss: 1.5813954e-06
Iter: 1687 loss: 1.57929503e-06
Iter: 1688 loss: 1.5792715e-06
Iter: 1689 loss: 1.57814725e-06
Iter: 1690 loss: 1.57574584e-06
Iter: 1691 loss: 1.61981006e-06
Iter: 1692 loss: 1.57570139e-06
Iter: 1693 loss: 1.57384136e-06
Iter: 1694 loss: 1.5738392e-06
Iter: 1695 loss: 1.57186946e-06
Iter: 1696 loss: 1.56958777e-06
Iter: 1697 loss: 1.56935573e-06
Iter: 1698 loss: 1.56690874e-06
Iter: 1699 loss: 1.57825707e-06
Iter: 1700 loss: 1.56645274e-06
Iter: 1701 loss: 1.56420663e-06
Iter: 1702 loss: 1.56478382e-06
Iter: 1703 loss: 1.56260921e-06
Iter: 1704 loss: 1.55989471e-06
Iter: 1705 loss: 1.57367072e-06
Iter: 1706 loss: 1.55946691e-06
Iter: 1707 loss: 1.5576361e-06
Iter: 1708 loss: 1.5852022e-06
Iter: 1709 loss: 1.55764633e-06
Iter: 1710 loss: 1.55604357e-06
Iter: 1711 loss: 1.55390694e-06
Iter: 1712 loss: 1.55374323e-06
Iter: 1713 loss: 1.55114299e-06
Iter: 1714 loss: 1.5614155e-06
Iter: 1715 loss: 1.5506e-06
Iter: 1716 loss: 1.5484386e-06
Iter: 1717 loss: 1.56828264e-06
Iter: 1718 loss: 1.54831991e-06
Iter: 1719 loss: 1.546666e-06
Iter: 1720 loss: 1.5455621e-06
Iter: 1721 loss: 1.54483723e-06
Iter: 1722 loss: 1.54335271e-06
Iter: 1723 loss: 1.54336385e-06
Iter: 1724 loss: 1.54180771e-06
Iter: 1725 loss: 1.54040208e-06
Iter: 1726 loss: 1.53996962e-06
Iter: 1727 loss: 1.53775454e-06
Iter: 1728 loss: 1.54431336e-06
Iter: 1729 loss: 1.53699909e-06
Iter: 1730 loss: 1.53490691e-06
Iter: 1731 loss: 1.55146e-06
Iter: 1732 loss: 1.53481972e-06
Iter: 1733 loss: 1.53338351e-06
Iter: 1734 loss: 1.53238011e-06
Iter: 1735 loss: 1.53187034e-06
Iter: 1736 loss: 1.52980101e-06
Iter: 1737 loss: 1.54465943e-06
Iter: 1738 loss: 1.52961411e-06
Iter: 1739 loss: 1.52773191e-06
Iter: 1740 loss: 1.5319265e-06
Iter: 1741 loss: 1.52702785e-06
Iter: 1742 loss: 1.52500138e-06
Iter: 1743 loss: 1.52321775e-06
Iter: 1744 loss: 1.52279654e-06
Iter: 1745 loss: 1.5198566e-06
Iter: 1746 loss: 1.53030919e-06
Iter: 1747 loss: 1.51912286e-06
Iter: 1748 loss: 1.51693075e-06
Iter: 1749 loss: 1.5396547e-06
Iter: 1750 loss: 1.51689096e-06
Iter: 1751 loss: 1.51493464e-06
Iter: 1752 loss: 1.5218402e-06
Iter: 1753 loss: 1.51447455e-06
Iter: 1754 loss: 1.51295535e-06
Iter: 1755 loss: 1.51111203e-06
Iter: 1756 loss: 1.51099539e-06
Iter: 1757 loss: 1.50867572e-06
Iter: 1758 loss: 1.53640258e-06
Iter: 1759 loss: 1.50863548e-06
Iter: 1760 loss: 1.50676976e-06
Iter: 1761 loss: 1.51070549e-06
Iter: 1762 loss: 1.50604706e-06
Iter: 1763 loss: 1.50434437e-06
Iter: 1764 loss: 1.50252913e-06
Iter: 1765 loss: 1.50224605e-06
Iter: 1766 loss: 1.49973221e-06
Iter: 1767 loss: 1.49971765e-06
Iter: 1768 loss: 1.4977735e-06
Iter: 1769 loss: 1.50372011e-06
Iter: 1770 loss: 1.49721632e-06
Iter: 1771 loss: 1.49561083e-06
Iter: 1772 loss: 1.49973539e-06
Iter: 1773 loss: 1.49513539e-06
Iter: 1774 loss: 1.49320113e-06
Iter: 1775 loss: 1.4960018e-06
Iter: 1776 loss: 1.49226321e-06
Iter: 1777 loss: 1.4904308e-06
Iter: 1778 loss: 1.49251377e-06
Iter: 1779 loss: 1.48948413e-06
Iter: 1780 loss: 1.48771392e-06
Iter: 1781 loss: 1.50494611e-06
Iter: 1782 loss: 1.48770164e-06
Iter: 1783 loss: 1.48617426e-06
Iter: 1784 loss: 1.4838929e-06
Iter: 1785 loss: 1.48383788e-06
Iter: 1786 loss: 1.48110712e-06
Iter: 1787 loss: 1.48594609e-06
Iter: 1788 loss: 1.47996752e-06
Iter: 1789 loss: 1.47724654e-06
Iter: 1790 loss: 1.47961021e-06
Iter: 1791 loss: 1.4756248e-06
Iter: 1792 loss: 1.47456831e-06
Iter: 1793 loss: 1.47395906e-06
Iter: 1794 loss: 1.47257811e-06
Iter: 1795 loss: 1.47194e-06
Iter: 1796 loss: 1.47132891e-06
Iter: 1797 loss: 1.46926061e-06
Iter: 1798 loss: 1.47029732e-06
Iter: 1799 loss: 1.46788193e-06
Iter: 1800 loss: 1.4658923e-06
Iter: 1801 loss: 1.49122945e-06
Iter: 1802 loss: 1.46583466e-06
Iter: 1803 loss: 1.46409798e-06
Iter: 1804 loss: 1.46401158e-06
Iter: 1805 loss: 1.46262278e-06
Iter: 1806 loss: 1.46099489e-06
Iter: 1807 loss: 1.46099489e-06
Iter: 1808 loss: 1.45966374e-06
Iter: 1809 loss: 1.45956517e-06
Iter: 1810 loss: 1.45859303e-06
Iter: 1811 loss: 1.45693116e-06
Iter: 1812 loss: 1.45776107e-06
Iter: 1813 loss: 1.45586728e-06
Iter: 1814 loss: 1.45392585e-06
Iter: 1815 loss: 1.4677812e-06
Iter: 1816 loss: 1.45377635e-06
Iter: 1817 loss: 1.45177171e-06
Iter: 1818 loss: 1.453089e-06
Iter: 1819 loss: 1.45052957e-06
Iter: 1820 loss: 1.44874207e-06
Iter: 1821 loss: 1.45209424e-06
Iter: 1822 loss: 1.44794012e-06
Iter: 1823 loss: 1.4462081e-06
Iter: 1824 loss: 1.46332798e-06
Iter: 1825 loss: 1.44617275e-06
Iter: 1826 loss: 1.44467276e-06
Iter: 1827 loss: 1.44302589e-06
Iter: 1828 loss: 1.44280784e-06
Iter: 1829 loss: 1.44061505e-06
Iter: 1830 loss: 1.44317471e-06
Iter: 1831 loss: 1.43942e-06
Iter: 1832 loss: 1.43701459e-06
Iter: 1833 loss: 1.45677177e-06
Iter: 1834 loss: 1.43692694e-06
Iter: 1835 loss: 1.43535453e-06
Iter: 1836 loss: 1.45338163e-06
Iter: 1837 loss: 1.435336e-06
Iter: 1838 loss: 1.43405464e-06
Iter: 1839 loss: 1.43221973e-06
Iter: 1840 loss: 1.43216448e-06
Iter: 1841 loss: 1.43004854e-06
Iter: 1842 loss: 1.44623891e-06
Iter: 1843 loss: 1.42985982e-06
Iter: 1844 loss: 1.42771137e-06
Iter: 1845 loss: 1.43678767e-06
Iter: 1846 loss: 1.42727617e-06
Iter: 1847 loss: 1.42584213e-06
Iter: 1848 loss: 1.42567706e-06
Iter: 1849 loss: 1.42462227e-06
Iter: 1850 loss: 1.42286876e-06
Iter: 1851 loss: 1.4324271e-06
Iter: 1852 loss: 1.42258773e-06
Iter: 1853 loss: 1.42064914e-06
Iter: 1854 loss: 1.42131034e-06
Iter: 1855 loss: 1.41928876e-06
Iter: 1856 loss: 1.41722569e-06
Iter: 1857 loss: 1.42140971e-06
Iter: 1858 loss: 1.41640817e-06
Iter: 1859 loss: 1.41478279e-06
Iter: 1860 loss: 1.41480439e-06
Iter: 1861 loss: 1.41360647e-06
Iter: 1862 loss: 1.41138412e-06
Iter: 1863 loss: 1.4592863e-06
Iter: 1864 loss: 1.4113765e-06
Iter: 1865 loss: 1.4090632e-06
Iter: 1866 loss: 1.41631642e-06
Iter: 1867 loss: 1.40841246e-06
Iter: 1868 loss: 1.40649831e-06
Iter: 1869 loss: 1.42895465e-06
Iter: 1870 loss: 1.40651139e-06
Iter: 1871 loss: 1.40458155e-06
Iter: 1872 loss: 1.40407701e-06
Iter: 1873 loss: 1.40292025e-06
Iter: 1874 loss: 1.40069255e-06
Iter: 1875 loss: 1.40533484e-06
Iter: 1876 loss: 1.39986787e-06
Iter: 1877 loss: 1.39833742e-06
Iter: 1878 loss: 1.39829501e-06
Iter: 1879 loss: 1.39701831e-06
Iter: 1880 loss: 1.39596727e-06
Iter: 1881 loss: 1.39555777e-06
Iter: 1882 loss: 1.39408132e-06
Iter: 1883 loss: 1.41089708e-06
Iter: 1884 loss: 1.3940969e-06
Iter: 1885 loss: 1.392738e-06
Iter: 1886 loss: 1.3921391e-06
Iter: 1887 loss: 1.39150143e-06
Iter: 1888 loss: 1.38965834e-06
Iter: 1889 loss: 1.39096414e-06
Iter: 1890 loss: 1.38855262e-06
Iter: 1891 loss: 1.38655344e-06
Iter: 1892 loss: 1.39741633e-06
Iter: 1893 loss: 1.38625683e-06
Iter: 1894 loss: 1.38451264e-06
Iter: 1895 loss: 1.39576787e-06
Iter: 1896 loss: 1.38434348e-06
Iter: 1897 loss: 1.3828851e-06
Iter: 1898 loss: 1.38175471e-06
Iter: 1899 loss: 1.38129712e-06
Iter: 1900 loss: 1.37933193e-06
Iter: 1901 loss: 1.39971871e-06
Iter: 1902 loss: 1.37931329e-06
Iter: 1903 loss: 1.37758616e-06
Iter: 1904 loss: 1.38024438e-06
Iter: 1905 loss: 1.37676864e-06
Iter: 1906 loss: 1.37537393e-06
Iter: 1907 loss: 1.37387769e-06
Iter: 1908 loss: 1.37370625e-06
Iter: 1909 loss: 1.37112568e-06
Iter: 1910 loss: 1.38224516e-06
Iter: 1911 loss: 1.3705635e-06
Iter: 1912 loss: 1.36878657e-06
Iter: 1913 loss: 1.39605947e-06
Iter: 1914 loss: 1.36881329e-06
Iter: 1915 loss: 1.36723702e-06
Iter: 1916 loss: 1.36865606e-06
Iter: 1917 loss: 1.36634117e-06
Iter: 1918 loss: 1.36464064e-06
Iter: 1919 loss: 1.37157303e-06
Iter: 1920 loss: 1.36432038e-06
Iter: 1921 loss: 1.36266283e-06
Iter: 1922 loss: 1.36474887e-06
Iter: 1923 loss: 1.36173173e-06
Iter: 1924 loss: 1.36012693e-06
Iter: 1925 loss: 1.36022265e-06
Iter: 1926 loss: 1.35880759e-06
Iter: 1927 loss: 1.35700543e-06
Iter: 1928 loss: 1.38257724e-06
Iter: 1929 loss: 1.35704283e-06
Iter: 1930 loss: 1.35565551e-06
Iter: 1931 loss: 1.35595087e-06
Iter: 1932 loss: 1.35462426e-06
Iter: 1933 loss: 1.35313508e-06
Iter: 1934 loss: 1.35267237e-06
Iter: 1935 loss: 1.3517872e-06
Iter: 1936 loss: 1.34962784e-06
Iter: 1937 loss: 1.36560493e-06
Iter: 1938 loss: 1.34943798e-06
Iter: 1939 loss: 1.3477237e-06
Iter: 1940 loss: 1.35887171e-06
Iter: 1941 loss: 1.34753714e-06
Iter: 1942 loss: 1.34627339e-06
Iter: 1943 loss: 1.3439286e-06
Iter: 1944 loss: 1.3979751e-06
Iter: 1945 loss: 1.34391075e-06
Iter: 1946 loss: 1.34179254e-06
Iter: 1947 loss: 1.36338417e-06
Iter: 1948 loss: 1.34170068e-06
Iter: 1949 loss: 1.34020183e-06
Iter: 1950 loss: 1.35806454e-06
Iter: 1951 loss: 1.34018376e-06
Iter: 1952 loss: 1.33905723e-06
Iter: 1953 loss: 1.33769458e-06
Iter: 1954 loss: 1.33759204e-06
Iter: 1955 loss: 1.33589651e-06
Iter: 1956 loss: 1.35811536e-06
Iter: 1957 loss: 1.33589674e-06
Iter: 1958 loss: 1.33453807e-06
Iter: 1959 loss: 1.33614867e-06
Iter: 1960 loss: 1.3338306e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.4 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi2.4
+ date
Wed Nov  4 15:12:31 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2/300_300_300_1 --function f2 --psi -1 --alpha 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff422c33400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff422c33a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc28f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc2e2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc2e2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc2d8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc1640d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc2a4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc1937b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc1386a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc13f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc143510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc143d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc09cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc10c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc203378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc214378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc0c8048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3e0208378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc0d28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3e01e5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3e017e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc047488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc058840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc02b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc02b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3fc058620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3e011cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3e011ce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3e012e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3e0126730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3e0126950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3e00bc6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3e00f16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3e00bca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff3e009aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.007600253
test_loss: 0.010116898
train_loss: 0.006396172
test_loss: 0.009552796
train_loss: 0.0066831466
test_loss: 0.00935194
train_loss: 0.005946901
test_loss: 0.009275509
train_loss: 0.005731436
test_loss: 0.00901641
train_loss: 0.0057008285
test_loss: 0.00897878
train_loss: 0.0061903996
test_loss: 0.0087923845
train_loss: 0.006013434
test_loss: 0.009190917
train_loss: 0.0054444475
test_loss: 0.008860134
train_loss: 0.005437985
test_loss: 0.008793977
train_loss: 0.005372184
test_loss: 0.008768953
train_loss: 0.0055268966
test_loss: 0.008867608
train_loss: 0.0052660895
test_loss: 0.00876961
train_loss: 0.005318297
test_loss: 0.008729867
train_loss: 0.005270544
test_loss: 0.008651035
train_loss: 0.005206539
test_loss: 0.00852601
train_loss: 0.0053741476
test_loss: 0.008640514
train_loss: 0.005161108
test_loss: 0.008602812
train_loss: 0.0050887614
test_loss: 0.008459926
train_loss: 0.005049985
test_loss: 0.008715416
train_loss: 0.005454017
test_loss: 0.008695049
train_loss: 0.0051182825
test_loss: 0.008503536
train_loss: 0.0052750697
test_loss: 0.008668494
train_loss: 0.005641234
test_loss: 0.008693905
train_loss: 0.005583948
test_loss: 0.0086322175
train_loss: 0.0052658115
test_loss: 0.008556999
train_loss: 0.0050625135
test_loss: 0.008508119
train_loss: 0.0050275344
test_loss: 0.0086153895
train_loss: 0.0052411426
test_loss: 0.008675999
train_loss: 0.0049885646
test_loss: 0.0085078105
train_loss: 0.0049522705
test_loss: 0.0084048845
train_loss: 0.0051456885
test_loss: 0.008531203
train_loss: 0.0049306275
test_loss: 0.008528336
train_loss: 0.005059419
test_loss: 0.008342515
train_loss: 0.004889812
test_loss: 0.008504394
train_loss: 0.004769771
test_loss: 0.008435909
train_loss: 0.004828965
test_loss: 0.008355461
train_loss: 0.0049890713
test_loss: 0.008639679
train_loss: 0.0051016053
test_loss: 0.008309843
train_loss: 0.004579867
test_loss: 0.008340946
train_loss: 0.004909017
test_loss: 0.008391567
train_loss: 0.00484981
test_loss: 0.0084182955
train_loss: 0.005433689
test_loss: 0.0084673595
train_loss: 0.004804668
test_loss: 0.008379372
train_loss: 0.004956685
test_loss: 0.00846134
train_loss: 0.0051151896
test_loss: 0.008393343
train_loss: 0.0048010335
test_loss: 0.008414733
train_loss: 0.004650426
test_loss: 0.008407018
train_loss: 0.0049209255
test_loss: 0.00843785
train_loss: 0.0053092428
test_loss: 0.008450063
train_loss: 0.004737609
test_loss: 0.0084161805
train_loss: 0.0049183895
test_loss: 0.008279949
train_loss: 0.004688054
test_loss: 0.008218342
train_loss: 0.0051546833
test_loss: 0.00828844
train_loss: 0.0047234343
test_loss: 0.008328633
train_loss: 0.004751198
test_loss: 0.0083916625
train_loss: 0.0046479036
test_loss: 0.008304214
train_loss: 0.004929881
test_loss: 0.008285557
train_loss: 0.004806371
test_loss: 0.008354163
train_loss: 0.004519836
test_loss: 0.008137704
train_loss: 0.004522142
test_loss: 0.0082447855
train_loss: 0.004912716
test_loss: 0.0082703885
train_loss: 0.0048633213
test_loss: 0.008287644
train_loss: 0.004685695
test_loss: 0.008152986
train_loss: 0.0044731596
test_loss: 0.008244012
train_loss: 0.0046772854
test_loss: 0.008271444
train_loss: 0.0045850812
test_loss: 0.00837295
train_loss: 0.004484754
test_loss: 0.008240311
train_loss: 0.004463336
test_loss: 0.008219922
train_loss: 0.0050632614
test_loss: 0.008328154
train_loss: 0.0044276514
test_loss: 0.008158919
train_loss: 0.0046625948
test_loss: 0.008288642
train_loss: 0.0047660996
test_loss: 0.008628057
train_loss: 0.0044585173
test_loss: 0.008284869
train_loss: 0.0045755003
test_loss: 0.008327674
train_loss: 0.00452041
test_loss: 0.008107541
train_loss: 0.0044108164
test_loss: 0.008058084
train_loss: 0.004799539
test_loss: 0.008258194
train_loss: 0.004612681
test_loss: 0.008196539
train_loss: 0.004489751
test_loss: 0.008114033
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.4/300_300_300_1 --optimizer lbfgs --function f2 --psi -1 --alpha 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc535ec510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc535ec9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc535e3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc535e3378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc535309d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc53516488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc534c21e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc534c2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc53470488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc53495840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc53528598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5343e048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc53415c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc534159d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc533cb7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5337b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc53376268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc53340488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f9441e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f944598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc533401e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc53537620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f8c1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f8df730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f8e28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f884048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f84a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f84a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f7fa510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f7c56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f80f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f7dac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f7da7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f780488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f73bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f774378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.41648411e-05
Iter: 2 loss: 3.11296499e-05
Iter: 3 loss: 2.91744527e-05
Iter: 4 loss: 2.79956766e-05
Iter: 5 loss: 2.43532304e-05
Iter: 6 loss: 4.28794301e-05
Iter: 7 loss: 2.37612076e-05
Iter: 8 loss: 2.1676904e-05
Iter: 9 loss: 2.65584185e-05
Iter: 10 loss: 2.09081991e-05
Iter: 11 loss: 1.88060549e-05
Iter: 12 loss: 3.27738235e-05
Iter: 13 loss: 1.8589677e-05
Iter: 14 loss: 1.76037356e-05
Iter: 15 loss: 1.73486951e-05
Iter: 16 loss: 1.6732989e-05
Iter: 17 loss: 1.56269416e-05
Iter: 18 loss: 1.85747667e-05
Iter: 19 loss: 1.52579423e-05
Iter: 20 loss: 1.42732015e-05
Iter: 21 loss: 2.4552628e-05
Iter: 22 loss: 1.42470653e-05
Iter: 23 loss: 1.35395076e-05
Iter: 24 loss: 1.41884566e-05
Iter: 25 loss: 1.31305032e-05
Iter: 26 loss: 1.25435508e-05
Iter: 27 loss: 1.61743192e-05
Iter: 28 loss: 1.24744874e-05
Iter: 29 loss: 1.20013392e-05
Iter: 30 loss: 1.41704286e-05
Iter: 31 loss: 1.19109e-05
Iter: 32 loss: 1.15405182e-05
Iter: 33 loss: 1.17199197e-05
Iter: 34 loss: 1.12923772e-05
Iter: 35 loss: 1.09430903e-05
Iter: 36 loss: 1.33532276e-05
Iter: 37 loss: 1.09096109e-05
Iter: 38 loss: 1.05933095e-05
Iter: 39 loss: 1.16723386e-05
Iter: 40 loss: 1.05091149e-05
Iter: 41 loss: 1.02664781e-05
Iter: 42 loss: 1.01691821e-05
Iter: 43 loss: 1.00395891e-05
Iter: 44 loss: 9.7412285e-06
Iter: 45 loss: 1.05828931e-05
Iter: 46 loss: 9.6462818e-06
Iter: 47 loss: 9.5108835e-06
Iter: 48 loss: 9.47333319e-06
Iter: 49 loss: 9.35497246e-06
Iter: 50 loss: 9.31585782e-06
Iter: 51 loss: 9.24733104e-06
Iter: 52 loss: 9.11331881e-06
Iter: 53 loss: 9.8846067e-06
Iter: 54 loss: 9.09515802e-06
Iter: 55 loss: 8.96542224e-06
Iter: 56 loss: 9.07231515e-06
Iter: 57 loss: 8.88826798e-06
Iter: 58 loss: 8.75852675e-06
Iter: 59 loss: 8.90756201e-06
Iter: 60 loss: 8.68916868e-06
Iter: 61 loss: 8.54741847e-06
Iter: 62 loss: 8.76336617e-06
Iter: 63 loss: 8.48007585e-06
Iter: 64 loss: 8.37820789e-06
Iter: 65 loss: 8.37807147e-06
Iter: 66 loss: 8.27912299e-06
Iter: 67 loss: 8.22332913e-06
Iter: 68 loss: 8.18042099e-06
Iter: 69 loss: 8.0736545e-06
Iter: 70 loss: 8.69368887e-06
Iter: 71 loss: 8.05955187e-06
Iter: 72 loss: 7.96213408e-06
Iter: 73 loss: 8.37520838e-06
Iter: 74 loss: 7.94164e-06
Iter: 75 loss: 7.85556495e-06
Iter: 76 loss: 7.86443525e-06
Iter: 77 loss: 7.78919912e-06
Iter: 78 loss: 7.70649876e-06
Iter: 79 loss: 8.38006144e-06
Iter: 80 loss: 7.70122733e-06
Iter: 81 loss: 7.62117043e-06
Iter: 82 loss: 7.75366789e-06
Iter: 83 loss: 7.5846151e-06
Iter: 84 loss: 7.50666641e-06
Iter: 85 loss: 7.560825e-06
Iter: 86 loss: 7.45798e-06
Iter: 87 loss: 7.38066865e-06
Iter: 88 loss: 7.64148e-06
Iter: 89 loss: 7.35980029e-06
Iter: 90 loss: 7.28847681e-06
Iter: 91 loss: 8.42209647e-06
Iter: 92 loss: 7.28833038e-06
Iter: 93 loss: 7.2504522e-06
Iter: 94 loss: 7.1760428e-06
Iter: 95 loss: 8.67757262e-06
Iter: 96 loss: 7.17544663e-06
Iter: 97 loss: 7.11586199e-06
Iter: 98 loss: 7.96565473e-06
Iter: 99 loss: 7.11569737e-06
Iter: 100 loss: 7.05855928e-06
Iter: 101 loss: 7.09015058e-06
Iter: 102 loss: 7.02110037e-06
Iter: 103 loss: 6.96282405e-06
Iter: 104 loss: 6.96359803e-06
Iter: 105 loss: 6.91641071e-06
Iter: 106 loss: 6.84878023e-06
Iter: 107 loss: 7.24045276e-06
Iter: 108 loss: 6.83970666e-06
Iter: 109 loss: 6.78917422e-06
Iter: 110 loss: 7.26621238e-06
Iter: 111 loss: 6.78728975e-06
Iter: 112 loss: 6.74589046e-06
Iter: 113 loss: 6.78875631e-06
Iter: 114 loss: 6.72275e-06
Iter: 115 loss: 6.67951235e-06
Iter: 116 loss: 6.75719457e-06
Iter: 117 loss: 6.66057304e-06
Iter: 118 loss: 6.61652302e-06
Iter: 119 loss: 6.94319033e-06
Iter: 120 loss: 6.61292142e-06
Iter: 121 loss: 6.57661803e-06
Iter: 122 loss: 6.57662576e-06
Iter: 123 loss: 6.54767973e-06
Iter: 124 loss: 6.50595621e-06
Iter: 125 loss: 6.69871e-06
Iter: 126 loss: 6.49807407e-06
Iter: 127 loss: 6.46196622e-06
Iter: 128 loss: 6.67248514e-06
Iter: 129 loss: 6.45719228e-06
Iter: 130 loss: 6.42465329e-06
Iter: 131 loss: 6.39142672e-06
Iter: 132 loss: 6.38513347e-06
Iter: 133 loss: 6.35352262e-06
Iter: 134 loss: 6.35294418e-06
Iter: 135 loss: 6.32111e-06
Iter: 136 loss: 6.3336538e-06
Iter: 137 loss: 6.29916121e-06
Iter: 138 loss: 6.26842757e-06
Iter: 139 loss: 6.24703853e-06
Iter: 140 loss: 6.23609367e-06
Iter: 141 loss: 6.1962719e-06
Iter: 142 loss: 6.41825909e-06
Iter: 143 loss: 6.19064122e-06
Iter: 144 loss: 6.15438921e-06
Iter: 145 loss: 6.47247634e-06
Iter: 146 loss: 6.15258705e-06
Iter: 147 loss: 6.12751683e-06
Iter: 148 loss: 6.10948473e-06
Iter: 149 loss: 6.10077268e-06
Iter: 150 loss: 6.05765217e-06
Iter: 151 loss: 6.13822067e-06
Iter: 152 loss: 6.03929038e-06
Iter: 153 loss: 6.00688782e-06
Iter: 154 loss: 6.1499e-06
Iter: 155 loss: 6.00037129e-06
Iter: 156 loss: 5.96672544e-06
Iter: 157 loss: 6.21832123e-06
Iter: 158 loss: 5.96397422e-06
Iter: 159 loss: 5.94378344e-06
Iter: 160 loss: 5.92834203e-06
Iter: 161 loss: 5.92177366e-06
Iter: 162 loss: 5.89381216e-06
Iter: 163 loss: 6.21734671e-06
Iter: 164 loss: 5.89357933e-06
Iter: 165 loss: 5.8697542e-06
Iter: 166 loss: 5.86519127e-06
Iter: 167 loss: 5.84912505e-06
Iter: 168 loss: 5.82346365e-06
Iter: 169 loss: 5.90615491e-06
Iter: 170 loss: 5.81630684e-06
Iter: 171 loss: 5.78694926e-06
Iter: 172 loss: 5.94658741e-06
Iter: 173 loss: 5.78266463e-06
Iter: 174 loss: 5.76207049e-06
Iter: 175 loss: 5.76820457e-06
Iter: 176 loss: 5.7472721e-06
Iter: 177 loss: 5.72620957e-06
Iter: 178 loss: 5.72609133e-06
Iter: 179 loss: 5.71221426e-06
Iter: 180 loss: 5.68461382e-06
Iter: 181 loss: 6.21751133e-06
Iter: 182 loss: 5.68439646e-06
Iter: 183 loss: 5.65716709e-06
Iter: 184 loss: 5.72680165e-06
Iter: 185 loss: 5.64773472e-06
Iter: 186 loss: 5.61644083e-06
Iter: 187 loss: 5.68068754e-06
Iter: 188 loss: 5.60387434e-06
Iter: 189 loss: 5.58184365e-06
Iter: 190 loss: 5.58184729e-06
Iter: 191 loss: 5.56161604e-06
Iter: 192 loss: 5.55468432e-06
Iter: 193 loss: 5.54310327e-06
Iter: 194 loss: 5.52010761e-06
Iter: 195 loss: 5.56283339e-06
Iter: 196 loss: 5.51031508e-06
Iter: 197 loss: 5.48208e-06
Iter: 198 loss: 5.53679729e-06
Iter: 199 loss: 5.47050149e-06
Iter: 200 loss: 5.44656268e-06
Iter: 201 loss: 5.76950606e-06
Iter: 202 loss: 5.44649447e-06
Iter: 203 loss: 5.4278853e-06
Iter: 204 loss: 5.41720055e-06
Iter: 205 loss: 5.40938026e-06
Iter: 206 loss: 5.38874247e-06
Iter: 207 loss: 5.51145376e-06
Iter: 208 loss: 5.38621316e-06
Iter: 209 loss: 5.3629542e-06
Iter: 210 loss: 5.40667133e-06
Iter: 211 loss: 5.35302024e-06
Iter: 212 loss: 5.33607181e-06
Iter: 213 loss: 5.3360618e-06
Iter: 214 loss: 5.32248851e-06
Iter: 215 loss: 5.29913541e-06
Iter: 216 loss: 5.55965198e-06
Iter: 217 loss: 5.29873796e-06
Iter: 218 loss: 5.28395321e-06
Iter: 219 loss: 5.28403052e-06
Iter: 220 loss: 5.2721216e-06
Iter: 221 loss: 5.25374389e-06
Iter: 222 loss: 5.39563916e-06
Iter: 223 loss: 5.25253199e-06
Iter: 224 loss: 5.23194285e-06
Iter: 225 loss: 5.24144252e-06
Iter: 226 loss: 5.21772608e-06
Iter: 227 loss: 5.20155891e-06
Iter: 228 loss: 5.18808e-06
Iter: 229 loss: 5.18348861e-06
Iter: 230 loss: 5.15664306e-06
Iter: 231 loss: 5.25789892e-06
Iter: 232 loss: 5.15019337e-06
Iter: 233 loss: 5.13420036e-06
Iter: 234 loss: 5.13379337e-06
Iter: 235 loss: 5.12013321e-06
Iter: 236 loss: 5.10432483e-06
Iter: 237 loss: 5.10235e-06
Iter: 238 loss: 5.08257563e-06
Iter: 239 loss: 5.11204507e-06
Iter: 240 loss: 5.07308141e-06
Iter: 241 loss: 5.05083335e-06
Iter: 242 loss: 5.18899742e-06
Iter: 243 loss: 5.04811396e-06
Iter: 244 loss: 5.02917646e-06
Iter: 245 loss: 5.15371721e-06
Iter: 246 loss: 5.02696e-06
Iter: 247 loss: 5.01402e-06
Iter: 248 loss: 5.01832073e-06
Iter: 249 loss: 5.00454371e-06
Iter: 250 loss: 4.98718509e-06
Iter: 251 loss: 5.03386354e-06
Iter: 252 loss: 4.98120153e-06
Iter: 253 loss: 4.96244e-06
Iter: 254 loss: 5.06936249e-06
Iter: 255 loss: 4.95993436e-06
Iter: 256 loss: 4.9476489e-06
Iter: 257 loss: 4.94232881e-06
Iter: 258 loss: 4.93602556e-06
Iter: 259 loss: 4.91875926e-06
Iter: 260 loss: 5.04612808e-06
Iter: 261 loss: 4.91727405e-06
Iter: 262 loss: 4.90081084e-06
Iter: 263 loss: 4.92980143e-06
Iter: 264 loss: 4.89372451e-06
Iter: 265 loss: 4.88314708e-06
Iter: 266 loss: 4.98862482e-06
Iter: 267 loss: 4.88281876e-06
Iter: 268 loss: 4.87086345e-06
Iter: 269 loss: 4.85413739e-06
Iter: 270 loss: 4.85349165e-06
Iter: 271 loss: 4.83761596e-06
Iter: 272 loss: 4.88302931e-06
Iter: 273 loss: 4.83275062e-06
Iter: 274 loss: 4.81650886e-06
Iter: 275 loss: 4.82509722e-06
Iter: 276 loss: 4.80566814e-06
Iter: 277 loss: 4.78673519e-06
Iter: 278 loss: 4.95971653e-06
Iter: 279 loss: 4.78574e-06
Iter: 280 loss: 4.77120693e-06
Iter: 281 loss: 4.86500903e-06
Iter: 282 loss: 4.76959667e-06
Iter: 283 loss: 4.75932393e-06
Iter: 284 loss: 4.74336048e-06
Iter: 285 loss: 4.74318131e-06
Iter: 286 loss: 4.72486772e-06
Iter: 287 loss: 4.811488e-06
Iter: 288 loss: 4.72151169e-06
Iter: 289 loss: 4.70685427e-06
Iter: 290 loss: 4.85744e-06
Iter: 291 loss: 4.70644954e-06
Iter: 292 loss: 4.69386214e-06
Iter: 293 loss: 4.70391251e-06
Iter: 294 loss: 4.6863961e-06
Iter: 295 loss: 4.6720379e-06
Iter: 296 loss: 4.67592236e-06
Iter: 297 loss: 4.66164283e-06
Iter: 298 loss: 4.64851837e-06
Iter: 299 loss: 4.64849836e-06
Iter: 300 loss: 4.63830293e-06
Iter: 301 loss: 4.62005119e-06
Iter: 302 loss: 5.06208289e-06
Iter: 303 loss: 4.620048e-06
Iter: 304 loss: 4.60410138e-06
Iter: 305 loss: 4.79353275e-06
Iter: 306 loss: 4.60384672e-06
Iter: 307 loss: 4.59038165e-06
Iter: 308 loss: 4.64563027e-06
Iter: 309 loss: 4.58739032e-06
Iter: 310 loss: 4.5782117e-06
Iter: 311 loss: 4.63066408e-06
Iter: 312 loss: 4.57703936e-06
Iter: 313 loss: 4.56674888e-06
Iter: 314 loss: 4.55774261e-06
Iter: 315 loss: 4.55512873e-06
Iter: 316 loss: 4.54088968e-06
Iter: 317 loss: 4.55914142e-06
Iter: 318 loss: 4.53368102e-06
Iter: 319 loss: 4.51816095e-06
Iter: 320 loss: 4.5262741e-06
Iter: 321 loss: 4.5079687e-06
Iter: 322 loss: 4.49528034e-06
Iter: 323 loss: 4.49514937e-06
Iter: 324 loss: 4.48197716e-06
Iter: 325 loss: 4.47941e-06
Iter: 326 loss: 4.47044931e-06
Iter: 327 loss: 4.45678e-06
Iter: 328 loss: 4.46438662e-06
Iter: 329 loss: 4.44767193e-06
Iter: 330 loss: 4.43257e-06
Iter: 331 loss: 4.50238e-06
Iter: 332 loss: 4.42963756e-06
Iter: 333 loss: 4.41292923e-06
Iter: 334 loss: 4.51497863e-06
Iter: 335 loss: 4.41084512e-06
Iter: 336 loss: 4.39955102e-06
Iter: 337 loss: 4.41927114e-06
Iter: 338 loss: 4.39452924e-06
Iter: 339 loss: 4.3825994e-06
Iter: 340 loss: 4.38760071e-06
Iter: 341 loss: 4.37421886e-06
Iter: 342 loss: 4.3637724e-06
Iter: 343 loss: 4.36360915e-06
Iter: 344 loss: 4.3566497e-06
Iter: 345 loss: 4.34395224e-06
Iter: 346 loss: 4.65434096e-06
Iter: 347 loss: 4.34396225e-06
Iter: 348 loss: 4.33174591e-06
Iter: 349 loss: 4.3317259e-06
Iter: 350 loss: 4.32120305e-06
Iter: 351 loss: 4.34012418e-06
Iter: 352 loss: 4.31656827e-06
Iter: 353 loss: 4.30697355e-06
Iter: 354 loss: 4.31392709e-06
Iter: 355 loss: 4.30112732e-06
Iter: 356 loss: 4.28619569e-06
Iter: 357 loss: 4.31088574e-06
Iter: 358 loss: 4.27934265e-06
Iter: 359 loss: 4.26850784e-06
Iter: 360 loss: 4.26565839e-06
Iter: 361 loss: 4.25878716e-06
Iter: 362 loss: 4.24239352e-06
Iter: 363 loss: 4.3359687e-06
Iter: 364 loss: 4.24019208e-06
Iter: 365 loss: 4.22970516e-06
Iter: 366 loss: 4.30177033e-06
Iter: 367 loss: 4.22875564e-06
Iter: 368 loss: 4.21752793e-06
Iter: 369 loss: 4.23620713e-06
Iter: 370 loss: 4.21250479e-06
Iter: 371 loss: 4.20071865e-06
Iter: 372 loss: 4.19813205e-06
Iter: 373 loss: 4.19048092e-06
Iter: 374 loss: 4.17690808e-06
Iter: 375 loss: 4.22503854e-06
Iter: 376 loss: 4.1734238e-06
Iter: 377 loss: 4.16008152e-06
Iter: 378 loss: 4.22476296e-06
Iter: 379 loss: 4.1577814e-06
Iter: 380 loss: 4.14542365e-06
Iter: 381 loss: 4.23150595e-06
Iter: 382 loss: 4.14425904e-06
Iter: 383 loss: 4.13655607e-06
Iter: 384 loss: 4.12701775e-06
Iter: 385 loss: 4.12621e-06
Iter: 386 loss: 4.11398e-06
Iter: 387 loss: 4.22380253e-06
Iter: 388 loss: 4.11319343e-06
Iter: 389 loss: 4.10185794e-06
Iter: 390 loss: 4.13467478e-06
Iter: 391 loss: 4.09834638e-06
Iter: 392 loss: 4.08986534e-06
Iter: 393 loss: 4.08577262e-06
Iter: 394 loss: 4.08173037e-06
Iter: 395 loss: 4.07230436e-06
Iter: 396 loss: 4.07216521e-06
Iter: 397 loss: 4.0640607e-06
Iter: 398 loss: 4.05920127e-06
Iter: 399 loss: 4.05585433e-06
Iter: 400 loss: 4.04596085e-06
Iter: 401 loss: 4.12863665e-06
Iter: 402 loss: 4.04552338e-06
Iter: 403 loss: 4.03771082e-06
Iter: 404 loss: 4.02306659e-06
Iter: 405 loss: 4.34845e-06
Iter: 406 loss: 4.02301521e-06
Iter: 407 loss: 4.0075538e-06
Iter: 408 loss: 4.07990456e-06
Iter: 409 loss: 4.00455247e-06
Iter: 410 loss: 3.9926349e-06
Iter: 411 loss: 4.05697301e-06
Iter: 412 loss: 3.99080182e-06
Iter: 413 loss: 3.97864414e-06
Iter: 414 loss: 4.04463208e-06
Iter: 415 loss: 3.97688e-06
Iter: 416 loss: 3.96710766e-06
Iter: 417 loss: 3.96403675e-06
Iter: 418 loss: 3.95823736e-06
Iter: 419 loss: 3.94689869e-06
Iter: 420 loss: 3.95661755e-06
Iter: 421 loss: 3.94010567e-06
Iter: 422 loss: 3.92540778e-06
Iter: 423 loss: 4.02531e-06
Iter: 424 loss: 3.92388756e-06
Iter: 425 loss: 3.91460117e-06
Iter: 426 loss: 4.00857425e-06
Iter: 427 loss: 3.91430194e-06
Iter: 428 loss: 3.90646437e-06
Iter: 429 loss: 3.90100513e-06
Iter: 430 loss: 3.89821025e-06
Iter: 431 loss: 3.88678791e-06
Iter: 432 loss: 3.92678112e-06
Iter: 433 loss: 3.88381886e-06
Iter: 434 loss: 3.87371801e-06
Iter: 435 loss: 3.97526674e-06
Iter: 436 loss: 3.87344107e-06
Iter: 437 loss: 3.86650754e-06
Iter: 438 loss: 3.86911279e-06
Iter: 439 loss: 3.86163902e-06
Iter: 440 loss: 3.85135718e-06
Iter: 441 loss: 3.9055476e-06
Iter: 442 loss: 3.84980876e-06
Iter: 443 loss: 3.84258692e-06
Iter: 444 loss: 3.83215547e-06
Iter: 445 loss: 3.83187762e-06
Iter: 446 loss: 3.8243752e-06
Iter: 447 loss: 3.82408689e-06
Iter: 448 loss: 3.81745349e-06
Iter: 449 loss: 3.80386791e-06
Iter: 450 loss: 4.0360228e-06
Iter: 451 loss: 3.80360461e-06
Iter: 452 loss: 3.79034e-06
Iter: 453 loss: 3.90396872e-06
Iter: 454 loss: 3.78963978e-06
Iter: 455 loss: 3.77953415e-06
Iter: 456 loss: 3.78606842e-06
Iter: 457 loss: 3.77326865e-06
Iter: 458 loss: 3.76287335e-06
Iter: 459 loss: 3.76286198e-06
Iter: 460 loss: 3.75438026e-06
Iter: 461 loss: 3.75366676e-06
Iter: 462 loss: 3.74740239e-06
Iter: 463 loss: 3.73813077e-06
Iter: 464 loss: 3.75077593e-06
Iter: 465 loss: 3.73363582e-06
Iter: 466 loss: 3.7226e-06
Iter: 467 loss: 3.74033175e-06
Iter: 468 loss: 3.71755209e-06
Iter: 469 loss: 3.70729435e-06
Iter: 470 loss: 3.84231544e-06
Iter: 471 loss: 3.70717885e-06
Iter: 472 loss: 3.69908639e-06
Iter: 473 loss: 3.7124903e-06
Iter: 474 loss: 3.69553845e-06
Iter: 475 loss: 3.68823976e-06
Iter: 476 loss: 3.69430381e-06
Iter: 477 loss: 3.68390033e-06
Iter: 478 loss: 3.67527809e-06
Iter: 479 loss: 3.78051595e-06
Iter: 480 loss: 3.67526e-06
Iter: 481 loss: 3.66966174e-06
Iter: 482 loss: 3.66774339e-06
Iter: 483 loss: 3.66472796e-06
Iter: 484 loss: 3.65627e-06
Iter: 485 loss: 3.70487828e-06
Iter: 486 loss: 3.65510869e-06
Iter: 487 loss: 3.64903667e-06
Iter: 488 loss: 3.63921527e-06
Iter: 489 loss: 3.63906679e-06
Iter: 490 loss: 3.62930291e-06
Iter: 491 loss: 3.76853222e-06
Iter: 492 loss: 3.62926789e-06
Iter: 493 loss: 3.62146147e-06
Iter: 494 loss: 3.62840456e-06
Iter: 495 loss: 3.61668913e-06
Iter: 496 loss: 3.60781837e-06
Iter: 497 loss: 3.60376703e-06
Iter: 498 loss: 3.59934347e-06
Iter: 499 loss: 3.58858983e-06
Iter: 500 loss: 3.66425115e-06
Iter: 501 loss: 3.58759826e-06
Iter: 502 loss: 3.57810904e-06
Iter: 503 loss: 3.60731519e-06
Iter: 504 loss: 3.57528688e-06
Iter: 505 loss: 3.5672806e-06
Iter: 506 loss: 3.66347649e-06
Iter: 507 loss: 3.567146e-06
Iter: 508 loss: 3.56150258e-06
Iter: 509 loss: 3.55293037e-06
Iter: 510 loss: 3.55279371e-06
Iter: 511 loss: 3.54293593e-06
Iter: 512 loss: 3.59241972e-06
Iter: 513 loss: 3.54125063e-06
Iter: 514 loss: 3.53311111e-06
Iter: 515 loss: 3.5724911e-06
Iter: 516 loss: 3.53149858e-06
Iter: 517 loss: 3.52150141e-06
Iter: 518 loss: 3.55560519e-06
Iter: 519 loss: 3.51893596e-06
Iter: 520 loss: 3.51411609e-06
Iter: 521 loss: 3.51992685e-06
Iter: 522 loss: 3.51164044e-06
Iter: 523 loss: 3.50343225e-06
Iter: 524 loss: 3.5048713e-06
Iter: 525 loss: 3.49720517e-06
Iter: 526 loss: 3.49035872e-06
Iter: 527 loss: 3.51207359e-06
Iter: 528 loss: 3.48830531e-06
Iter: 529 loss: 3.4814359e-06
Iter: 530 loss: 3.52189772e-06
Iter: 531 loss: 3.48053754e-06
Iter: 532 loss: 3.47409809e-06
Iter: 533 loss: 3.46959723e-06
Iter: 534 loss: 3.4673576e-06
Iter: 535 loss: 3.45892204e-06
Iter: 536 loss: 3.49936636e-06
Iter: 537 loss: 3.45741591e-06
Iter: 538 loss: 3.44904197e-06
Iter: 539 loss: 3.4934933e-06
Iter: 540 loss: 3.44773298e-06
Iter: 541 loss: 3.44098e-06
Iter: 542 loss: 3.43491365e-06
Iter: 543 loss: 3.43322426e-06
Iter: 544 loss: 3.42260137e-06
Iter: 545 loss: 3.46123352e-06
Iter: 546 loss: 3.41989835e-06
Iter: 547 loss: 3.41217719e-06
Iter: 548 loss: 3.5249509e-06
Iter: 549 loss: 3.41215559e-06
Iter: 550 loss: 3.40593942e-06
Iter: 551 loss: 3.40699171e-06
Iter: 552 loss: 3.40132055e-06
Iter: 553 loss: 3.39350709e-06
Iter: 554 loss: 3.39845451e-06
Iter: 555 loss: 3.38869449e-06
Iter: 556 loss: 3.38395125e-06
Iter: 557 loss: 3.38295536e-06
Iter: 558 loss: 3.37933034e-06
Iter: 559 loss: 3.37181928e-06
Iter: 560 loss: 3.51389667e-06
Iter: 561 loss: 3.37169377e-06
Iter: 562 loss: 3.36294784e-06
Iter: 563 loss: 3.39084272e-06
Iter: 564 loss: 3.36040807e-06
Iter: 565 loss: 3.35376217e-06
Iter: 566 loss: 3.42085923e-06
Iter: 567 loss: 3.35350887e-06
Iter: 568 loss: 3.34726406e-06
Iter: 569 loss: 3.34528659e-06
Iter: 570 loss: 3.34161268e-06
Iter: 571 loss: 3.33368484e-06
Iter: 572 loss: 3.33376101e-06
Iter: 573 loss: 3.32735522e-06
Iter: 574 loss: 3.32174545e-06
Iter: 575 loss: 3.3211868e-06
Iter: 576 loss: 3.31565343e-06
Iter: 577 loss: 3.30775151e-06
Iter: 578 loss: 3.30742773e-06
Iter: 579 loss: 3.2993903e-06
Iter: 580 loss: 3.34596552e-06
Iter: 581 loss: 3.29817976e-06
Iter: 582 loss: 3.29118438e-06
Iter: 583 loss: 3.33697926e-06
Iter: 584 loss: 3.29042859e-06
Iter: 585 loss: 3.28445913e-06
Iter: 586 loss: 3.2832279e-06
Iter: 587 loss: 3.27932798e-06
Iter: 588 loss: 3.27109683e-06
Iter: 589 loss: 3.29414524e-06
Iter: 590 loss: 3.26840859e-06
Iter: 591 loss: 3.26175814e-06
Iter: 592 loss: 3.35311597e-06
Iter: 593 loss: 3.26181703e-06
Iter: 594 loss: 3.25677161e-06
Iter: 595 loss: 3.25535461e-06
Iter: 596 loss: 3.25232895e-06
Iter: 597 loss: 3.24610164e-06
Iter: 598 loss: 3.30941361e-06
Iter: 599 loss: 3.24599569e-06
Iter: 600 loss: 3.24038274e-06
Iter: 601 loss: 3.2371006e-06
Iter: 602 loss: 3.23473114e-06
Iter: 603 loss: 3.22786013e-06
Iter: 604 loss: 3.22666142e-06
Iter: 605 loss: 3.2219109e-06
Iter: 606 loss: 3.2132466e-06
Iter: 607 loss: 3.28228361e-06
Iter: 608 loss: 3.21271568e-06
Iter: 609 loss: 3.20685376e-06
Iter: 610 loss: 3.26525401e-06
Iter: 611 loss: 3.20671666e-06
Iter: 612 loss: 3.2015098e-06
Iter: 613 loss: 3.19174569e-06
Iter: 614 loss: 3.40218185e-06
Iter: 615 loss: 3.19179912e-06
Iter: 616 loss: 3.18222919e-06
Iter: 617 loss: 3.20720483e-06
Iter: 618 loss: 3.17900162e-06
Iter: 619 loss: 3.17119225e-06
Iter: 620 loss: 3.27363136e-06
Iter: 621 loss: 3.17113336e-06
Iter: 622 loss: 3.16480191e-06
Iter: 623 loss: 3.18714274e-06
Iter: 624 loss: 3.16324918e-06
Iter: 625 loss: 3.15774969e-06
Iter: 626 loss: 3.15290617e-06
Iter: 627 loss: 3.15152556e-06
Iter: 628 loss: 3.14254476e-06
Iter: 629 loss: 3.20371919e-06
Iter: 630 loss: 3.14169824e-06
Iter: 631 loss: 3.13615942e-06
Iter: 632 loss: 3.18229763e-06
Iter: 633 loss: 3.13578084e-06
Iter: 634 loss: 3.13083706e-06
Iter: 635 loss: 3.1274833e-06
Iter: 636 loss: 3.12570069e-06
Iter: 637 loss: 3.11948679e-06
Iter: 638 loss: 3.20653771e-06
Iter: 639 loss: 3.11939493e-06
Iter: 640 loss: 3.11408166e-06
Iter: 641 loss: 3.1127272e-06
Iter: 642 loss: 3.10938458e-06
Iter: 643 loss: 3.10252904e-06
Iter: 644 loss: 3.14434828e-06
Iter: 645 loss: 3.10172322e-06
Iter: 646 loss: 3.09565712e-06
Iter: 647 loss: 3.09740494e-06
Iter: 648 loss: 3.09136249e-06
Iter: 649 loss: 3.08489825e-06
Iter: 650 loss: 3.08285917e-06
Iter: 651 loss: 3.07904406e-06
Iter: 652 loss: 3.06998527e-06
Iter: 653 loss: 3.13019245e-06
Iter: 654 loss: 3.06906713e-06
Iter: 655 loss: 3.06328911e-06
Iter: 656 loss: 3.14375734e-06
Iter: 657 loss: 3.06322636e-06
Iter: 658 loss: 3.05877666e-06
Iter: 659 loss: 3.05152639e-06
Iter: 660 loss: 3.05141612e-06
Iter: 661 loss: 3.04241826e-06
Iter: 662 loss: 3.05406434e-06
Iter: 663 loss: 3.03768343e-06
Iter: 664 loss: 3.03016873e-06
Iter: 665 loss: 3.09406187e-06
Iter: 666 loss: 3.02979242e-06
Iter: 667 loss: 3.02207172e-06
Iter: 668 loss: 3.05450862e-06
Iter: 669 loss: 3.02045487e-06
Iter: 670 loss: 3.01548425e-06
Iter: 671 loss: 3.01883256e-06
Iter: 672 loss: 3.01237742e-06
Iter: 673 loss: 3.00554029e-06
Iter: 674 loss: 3.01456112e-06
Iter: 675 loss: 3.00204192e-06
Iter: 676 loss: 2.99585395e-06
Iter: 677 loss: 2.99577414e-06
Iter: 678 loss: 2.99182511e-06
Iter: 679 loss: 2.99568342e-06
Iter: 680 loss: 2.98953137e-06
Iter: 681 loss: 2.98448685e-06
Iter: 682 loss: 3.00278634e-06
Iter: 683 loss: 2.98323789e-06
Iter: 684 loss: 2.97827091e-06
Iter: 685 loss: 2.97541851e-06
Iter: 686 loss: 2.97323118e-06
Iter: 687 loss: 2.96739381e-06
Iter: 688 loss: 3.02381477e-06
Iter: 689 loss: 2.96710505e-06
Iter: 690 loss: 2.9621774e-06
Iter: 691 loss: 2.96328631e-06
Iter: 692 loss: 2.95851964e-06
Iter: 693 loss: 2.9522871e-06
Iter: 694 loss: 2.94544088e-06
Iter: 695 loss: 2.94444953e-06
Iter: 696 loss: 2.93965422e-06
Iter: 697 loss: 2.93874814e-06
Iter: 698 loss: 2.93390121e-06
Iter: 699 loss: 2.94120309e-06
Iter: 700 loss: 2.93162248e-06
Iter: 701 loss: 2.92638515e-06
Iter: 702 loss: 2.92209825e-06
Iter: 703 loss: 2.92054347e-06
Iter: 704 loss: 2.91284687e-06
Iter: 705 loss: 2.9491805e-06
Iter: 706 loss: 2.91137781e-06
Iter: 707 loss: 2.90442313e-06
Iter: 708 loss: 2.91214019e-06
Iter: 709 loss: 2.90075877e-06
Iter: 710 loss: 2.89351829e-06
Iter: 711 loss: 2.98289342e-06
Iter: 712 loss: 2.89340574e-06
Iter: 713 loss: 2.88826323e-06
Iter: 714 loss: 2.90230378e-06
Iter: 715 loss: 2.88654974e-06
Iter: 716 loss: 2.88164938e-06
Iter: 717 loss: 2.87788657e-06
Iter: 718 loss: 2.87642638e-06
Iter: 719 loss: 2.86989734e-06
Iter: 720 loss: 2.92882487e-06
Iter: 721 loss: 2.86964041e-06
Iter: 722 loss: 2.86448767e-06
Iter: 723 loss: 2.90133607e-06
Iter: 724 loss: 2.86394857e-06
Iter: 725 loss: 2.86038585e-06
Iter: 726 loss: 2.86031e-06
Iter: 727 loss: 2.85757096e-06
Iter: 728 loss: 2.85039232e-06
Iter: 729 loss: 2.86652676e-06
Iter: 730 loss: 2.84774433e-06
Iter: 731 loss: 2.84344424e-06
Iter: 732 loss: 2.85285523e-06
Iter: 733 loss: 2.84183761e-06
Iter: 734 loss: 2.83621193e-06
Iter: 735 loss: 2.84544649e-06
Iter: 736 loss: 2.83367444e-06
Iter: 737 loss: 2.82825454e-06
Iter: 738 loss: 2.82485803e-06
Iter: 739 loss: 2.82275346e-06
Iter: 740 loss: 2.81658413e-06
Iter: 741 loss: 2.89689729e-06
Iter: 742 loss: 2.81662528e-06
Iter: 743 loss: 2.81232542e-06
Iter: 744 loss: 2.83992131e-06
Iter: 745 loss: 2.81175016e-06
Iter: 746 loss: 2.80774884e-06
Iter: 747 loss: 2.79983215e-06
Iter: 748 loss: 2.95697646e-06
Iter: 749 loss: 2.79976643e-06
Iter: 750 loss: 2.79234519e-06
Iter: 751 loss: 2.84106795e-06
Iter: 752 loss: 2.79150549e-06
Iter: 753 loss: 2.78543257e-06
Iter: 754 loss: 2.79402479e-06
Iter: 755 loss: 2.7823794e-06
Iter: 756 loss: 2.77670642e-06
Iter: 757 loss: 2.86506634e-06
Iter: 758 loss: 2.77669915e-06
Iter: 759 loss: 2.77179811e-06
Iter: 760 loss: 2.76949982e-06
Iter: 761 loss: 2.7671108e-06
Iter: 762 loss: 2.76172022e-06
Iter: 763 loss: 2.7810845e-06
Iter: 764 loss: 2.7602739e-06
Iter: 765 loss: 2.75467e-06
Iter: 766 loss: 2.78180619e-06
Iter: 767 loss: 2.75368097e-06
Iter: 768 loss: 2.74815898e-06
Iter: 769 loss: 2.77073968e-06
Iter: 770 loss: 2.74683862e-06
Iter: 771 loss: 2.74312424e-06
Iter: 772 loss: 2.75872071e-06
Iter: 773 loss: 2.74240529e-06
Iter: 774 loss: 2.73819933e-06
Iter: 775 loss: 2.73814135e-06
Iter: 776 loss: 2.73471187e-06
Iter: 777 loss: 2.73007981e-06
Iter: 778 loss: 2.73394426e-06
Iter: 779 loss: 2.72723423e-06
Iter: 780 loss: 2.72095326e-06
Iter: 781 loss: 2.76596825e-06
Iter: 782 loss: 2.72036368e-06
Iter: 783 loss: 2.71670751e-06
Iter: 784 loss: 2.71034969e-06
Iter: 785 loss: 2.71030103e-06
Iter: 786 loss: 2.70483292e-06
Iter: 787 loss: 2.70475584e-06
Iter: 788 loss: 2.69995849e-06
Iter: 789 loss: 2.7087026e-06
Iter: 790 loss: 2.69780776e-06
Iter: 791 loss: 2.69297948e-06
Iter: 792 loss: 2.69575912e-06
Iter: 793 loss: 2.68983126e-06
Iter: 794 loss: 2.68471058e-06
Iter: 795 loss: 2.69539669e-06
Iter: 796 loss: 2.68259691e-06
Iter: 797 loss: 2.67664745e-06
Iter: 798 loss: 2.68266558e-06
Iter: 799 loss: 2.6733403e-06
Iter: 800 loss: 2.66779352e-06
Iter: 801 loss: 2.66779307e-06
Iter: 802 loss: 2.66346524e-06
Iter: 803 loss: 2.66756706e-06
Iter: 804 loss: 2.66083453e-06
Iter: 805 loss: 2.65545532e-06
Iter: 806 loss: 2.659144e-06
Iter: 807 loss: 2.65212111e-06
Iter: 808 loss: 2.64822279e-06
Iter: 809 loss: 2.6479006e-06
Iter: 810 loss: 2.64524624e-06
Iter: 811 loss: 2.63974061e-06
Iter: 812 loss: 2.73366504e-06
Iter: 813 loss: 2.63958918e-06
Iter: 814 loss: 2.63564198e-06
Iter: 815 loss: 2.63558559e-06
Iter: 816 loss: 2.63226411e-06
Iter: 817 loss: 2.62896037e-06
Iter: 818 loss: 2.62827689e-06
Iter: 819 loss: 2.62332856e-06
Iter: 820 loss: 2.65143262e-06
Iter: 821 loss: 2.62268645e-06
Iter: 822 loss: 2.6179905e-06
Iter: 823 loss: 2.63354059e-06
Iter: 824 loss: 2.6167545e-06
Iter: 825 loss: 2.61275704e-06
Iter: 826 loss: 2.60646902e-06
Iter: 827 loss: 2.60638694e-06
Iter: 828 loss: 2.60106344e-06
Iter: 829 loss: 2.60108254e-06
Iter: 830 loss: 2.59617582e-06
Iter: 831 loss: 2.61126888e-06
Iter: 832 loss: 2.59485478e-06
Iter: 833 loss: 2.59009471e-06
Iter: 834 loss: 2.5950776e-06
Iter: 835 loss: 2.58749401e-06
Iter: 836 loss: 2.58302612e-06
Iter: 837 loss: 2.58433829e-06
Iter: 838 loss: 2.5798322e-06
Iter: 839 loss: 2.57375905e-06
Iter: 840 loss: 2.59119224e-06
Iter: 841 loss: 2.57172496e-06
Iter: 842 loss: 2.56745761e-06
Iter: 843 loss: 2.56744534e-06
Iter: 844 loss: 2.56359431e-06
Iter: 845 loss: 2.56458497e-06
Iter: 846 loss: 2.56078556e-06
Iter: 847 loss: 2.55619443e-06
Iter: 848 loss: 2.58448927e-06
Iter: 849 loss: 2.55557848e-06
Iter: 850 loss: 2.55159421e-06
Iter: 851 loss: 2.55056193e-06
Iter: 852 loss: 2.54815041e-06
Iter: 853 loss: 2.54343399e-06
Iter: 854 loss: 2.54685096e-06
Iter: 855 loss: 2.54057477e-06
Iter: 856 loss: 2.53518874e-06
Iter: 857 loss: 2.56973158e-06
Iter: 858 loss: 2.53448161e-06
Iter: 859 loss: 2.52969858e-06
Iter: 860 loss: 2.55306668e-06
Iter: 861 loss: 2.52891459e-06
Iter: 862 loss: 2.52512268e-06
Iter: 863 loss: 2.52164773e-06
Iter: 864 loss: 2.52074028e-06
Iter: 865 loss: 2.5152317e-06
Iter: 866 loss: 2.56276826e-06
Iter: 867 loss: 2.51496795e-06
Iter: 868 loss: 2.51067672e-06
Iter: 869 loss: 2.53917915e-06
Iter: 870 loss: 2.51024903e-06
Iter: 871 loss: 2.50683615e-06
Iter: 872 loss: 2.50518588e-06
Iter: 873 loss: 2.50357834e-06
Iter: 874 loss: 2.49822733e-06
Iter: 875 loss: 2.49854224e-06
Iter: 876 loss: 2.49402592e-06
Iter: 877 loss: 2.48875517e-06
Iter: 878 loss: 2.56001476e-06
Iter: 879 loss: 2.48871447e-06
Iter: 880 loss: 2.48409333e-06
Iter: 881 loss: 2.50330049e-06
Iter: 882 loss: 2.48305719e-06
Iter: 883 loss: 2.47958656e-06
Iter: 884 loss: 2.48280503e-06
Iter: 885 loss: 2.47756543e-06
Iter: 886 loss: 2.47228672e-06
Iter: 887 loss: 2.49597724e-06
Iter: 888 loss: 2.47125399e-06
Iter: 889 loss: 2.46817353e-06
Iter: 890 loss: 2.46598688e-06
Iter: 891 loss: 2.46494483e-06
Iter: 892 loss: 2.4601627e-06
Iter: 893 loss: 2.47984235e-06
Iter: 894 loss: 2.45910519e-06
Iter: 895 loss: 2.45346109e-06
Iter: 896 loss: 2.46798209e-06
Iter: 897 loss: 2.45159708e-06
Iter: 898 loss: 2.44760781e-06
Iter: 899 loss: 2.44553439e-06
Iter: 900 loss: 2.44361399e-06
Iter: 901 loss: 2.43805107e-06
Iter: 902 loss: 2.49288928e-06
Iter: 903 loss: 2.43786644e-06
Iter: 904 loss: 2.43320528e-06
Iter: 905 loss: 2.44505509e-06
Iter: 906 loss: 2.43167142e-06
Iter: 907 loss: 2.4272731e-06
Iter: 908 loss: 2.42835358e-06
Iter: 909 loss: 2.42418128e-06
Iter: 910 loss: 2.41974453e-06
Iter: 911 loss: 2.44038756e-06
Iter: 912 loss: 2.41889734e-06
Iter: 913 loss: 2.41360249e-06
Iter: 914 loss: 2.43262048e-06
Iter: 915 loss: 2.41229191e-06
Iter: 916 loss: 2.4079784e-06
Iter: 917 loss: 2.41882117e-06
Iter: 918 loss: 2.40658846e-06
Iter: 919 loss: 2.4027263e-06
Iter: 920 loss: 2.39878455e-06
Iter: 921 loss: 2.39799238e-06
Iter: 922 loss: 2.39465885e-06
Iter: 923 loss: 2.39438623e-06
Iter: 924 loss: 2.39070891e-06
Iter: 925 loss: 2.39232099e-06
Iter: 926 loss: 2.38817165e-06
Iter: 927 loss: 2.38489201e-06
Iter: 928 loss: 2.4108972e-06
Iter: 929 loss: 2.38463781e-06
Iter: 930 loss: 2.38126e-06
Iter: 931 loss: 2.37789163e-06
Iter: 932 loss: 2.37720337e-06
Iter: 933 loss: 2.37280938e-06
Iter: 934 loss: 2.3746436e-06
Iter: 935 loss: 2.36978121e-06
Iter: 936 loss: 2.36550477e-06
Iter: 937 loss: 2.36546703e-06
Iter: 938 loss: 2.3624375e-06
Iter: 939 loss: 2.3587811e-06
Iter: 940 loss: 2.35844391e-06
Iter: 941 loss: 2.35403104e-06
Iter: 942 loss: 2.3695211e-06
Iter: 943 loss: 2.35281095e-06
Iter: 944 loss: 2.34818071e-06
Iter: 945 loss: 2.3760449e-06
Iter: 946 loss: 2.34764639e-06
Iter: 947 loss: 2.34366871e-06
Iter: 948 loss: 2.35132302e-06
Iter: 949 loss: 2.34205982e-06
Iter: 950 loss: 2.3382222e-06
Iter: 951 loss: 2.33606943e-06
Iter: 952 loss: 2.33439982e-06
Iter: 953 loss: 2.33087485e-06
Iter: 954 loss: 2.330602e-06
Iter: 955 loss: 2.32745924e-06
Iter: 956 loss: 2.3255202e-06
Iter: 957 loss: 2.32422371e-06
Iter: 958 loss: 2.32024877e-06
Iter: 959 loss: 2.32181446e-06
Iter: 960 loss: 2.31751574e-06
Iter: 961 loss: 2.31172385e-06
Iter: 962 loss: 2.35362131e-06
Iter: 963 loss: 2.31126e-06
Iter: 964 loss: 2.30773662e-06
Iter: 965 loss: 2.34257323e-06
Iter: 966 loss: 2.30760384e-06
Iter: 967 loss: 2.30529031e-06
Iter: 968 loss: 2.30139472e-06
Iter: 969 loss: 2.3014486e-06
Iter: 970 loss: 2.29860984e-06
Iter: 971 loss: 2.29833086e-06
Iter: 972 loss: 2.29588886e-06
Iter: 973 loss: 2.29339526e-06
Iter: 974 loss: 2.2928848e-06
Iter: 975 loss: 2.28897397e-06
Iter: 976 loss: 2.2963477e-06
Iter: 977 loss: 2.28733325e-06
Iter: 978 loss: 2.2829945e-06
Iter: 979 loss: 2.31566696e-06
Iter: 980 loss: 2.28268118e-06
Iter: 981 loss: 2.28039789e-06
Iter: 982 loss: 2.27619967e-06
Iter: 983 loss: 2.37722361e-06
Iter: 984 loss: 2.27618125e-06
Iter: 985 loss: 2.27103033e-06
Iter: 986 loss: 2.32053208e-06
Iter: 987 loss: 2.27090572e-06
Iter: 988 loss: 2.26712336e-06
Iter: 989 loss: 2.28199679e-06
Iter: 990 loss: 2.26618363e-06
Iter: 991 loss: 2.26271504e-06
Iter: 992 loss: 2.26028169e-06
Iter: 993 loss: 2.25900908e-06
Iter: 994 loss: 2.25378335e-06
Iter: 995 loss: 2.2788945e-06
Iter: 996 loss: 2.25278222e-06
Iter: 997 loss: 2.24941846e-06
Iter: 998 loss: 2.29726538e-06
Iter: 999 loss: 2.2493814e-06
Iter: 1000 loss: 2.24640257e-06
Iter: 1001 loss: 2.24341511e-06
Iter: 1002 loss: 2.24289033e-06
Iter: 1003 loss: 2.23811321e-06
Iter: 1004 loss: 2.24935911e-06
Iter: 1005 loss: 2.23636835e-06
Iter: 1006 loss: 2.23279426e-06
Iter: 1007 loss: 2.26146972e-06
Iter: 1008 loss: 2.23252505e-06
Iter: 1009 loss: 2.22881772e-06
Iter: 1010 loss: 2.24217501e-06
Iter: 1011 loss: 2.22790459e-06
Iter: 1012 loss: 2.22517e-06
Iter: 1013 loss: 2.23096436e-06
Iter: 1014 loss: 2.22414246e-06
Iter: 1015 loss: 2.22052654e-06
Iter: 1016 loss: 2.22685458e-06
Iter: 1017 loss: 2.21894925e-06
Iter: 1018 loss: 2.21606888e-06
Iter: 1019 loss: 2.21305686e-06
Iter: 1020 loss: 2.21258824e-06
Iter: 1021 loss: 2.20905349e-06
Iter: 1022 loss: 2.2090062e-06
Iter: 1023 loss: 2.20606171e-06
Iter: 1024 loss: 2.20295055e-06
Iter: 1025 loss: 2.20240622e-06
Iter: 1026 loss: 2.19837625e-06
Iter: 1027 loss: 2.21120422e-06
Iter: 1028 loss: 2.19726e-06
Iter: 1029 loss: 2.19351614e-06
Iter: 1030 loss: 2.22323843e-06
Iter: 1031 loss: 2.19318099e-06
Iter: 1032 loss: 2.18963669e-06
Iter: 1033 loss: 2.18775676e-06
Iter: 1034 loss: 2.18610421e-06
Iter: 1035 loss: 2.18178661e-06
Iter: 1036 loss: 2.19303593e-06
Iter: 1037 loss: 2.18035211e-06
Iter: 1038 loss: 2.17602337e-06
Iter: 1039 loss: 2.19680851e-06
Iter: 1040 loss: 2.17530942e-06
Iter: 1041 loss: 2.17153524e-06
Iter: 1042 loss: 2.2044992e-06
Iter: 1043 loss: 2.17134857e-06
Iter: 1044 loss: 2.16877606e-06
Iter: 1045 loss: 2.16783928e-06
Iter: 1046 loss: 2.16624289e-06
Iter: 1047 loss: 2.16353396e-06
Iter: 1048 loss: 2.20453148e-06
Iter: 1049 loss: 2.16356057e-06
Iter: 1050 loss: 2.16097351e-06
Iter: 1051 loss: 2.15873706e-06
Iter: 1052 loss: 2.1579549e-06
Iter: 1053 loss: 2.15471596e-06
Iter: 1054 loss: 2.15991577e-06
Iter: 1055 loss: 2.15305727e-06
Iter: 1056 loss: 2.1495689e-06
Iter: 1057 loss: 2.16523e-06
Iter: 1058 loss: 2.14881811e-06
Iter: 1059 loss: 2.14462671e-06
Iter: 1060 loss: 2.15531963e-06
Iter: 1061 loss: 2.14319562e-06
Iter: 1062 loss: 2.14008105e-06
Iter: 1063 loss: 2.14193074e-06
Iter: 1064 loss: 2.13801491e-06
Iter: 1065 loss: 2.13440899e-06
Iter: 1066 loss: 2.13896737e-06
Iter: 1067 loss: 2.1324629e-06
Iter: 1068 loss: 2.12767645e-06
Iter: 1069 loss: 2.16815988e-06
Iter: 1070 loss: 2.12738678e-06
Iter: 1071 loss: 2.1243834e-06
Iter: 1072 loss: 2.12371765e-06
Iter: 1073 loss: 2.12168061e-06
Iter: 1074 loss: 2.11815586e-06
Iter: 1075 loss: 2.14170564e-06
Iter: 1076 loss: 2.11779297e-06
Iter: 1077 loss: 2.11418387e-06
Iter: 1078 loss: 2.12339546e-06
Iter: 1079 loss: 2.11298357e-06
Iter: 1080 loss: 2.10987446e-06
Iter: 1081 loss: 2.11382985e-06
Iter: 1082 loss: 2.10824805e-06
Iter: 1083 loss: 2.10522671e-06
Iter: 1084 loss: 2.1315891e-06
Iter: 1085 loss: 2.10503254e-06
Iter: 1086 loss: 2.10208282e-06
Iter: 1087 loss: 2.10681173e-06
Iter: 1088 loss: 2.10063718e-06
Iter: 1089 loss: 2.09782684e-06
Iter: 1090 loss: 2.09990412e-06
Iter: 1091 loss: 2.09612699e-06
Iter: 1092 loss: 2.09212158e-06
Iter: 1093 loss: 2.10960525e-06
Iter: 1094 loss: 2.0912039e-06
Iter: 1095 loss: 2.08851952e-06
Iter: 1096 loss: 2.0882926e-06
Iter: 1097 loss: 2.08622851e-06
Iter: 1098 loss: 2.08256188e-06
Iter: 1099 loss: 2.08681831e-06
Iter: 1100 loss: 2.08062829e-06
Iter: 1101 loss: 2.07716403e-06
Iter: 1102 loss: 2.10807048e-06
Iter: 1103 loss: 2.07700259e-06
Iter: 1104 loss: 2.07352559e-06
Iter: 1105 loss: 2.08397523e-06
Iter: 1106 loss: 2.07241965e-06
Iter: 1107 loss: 2.06934101e-06
Iter: 1108 loss: 2.06651771e-06
Iter: 1109 loss: 2.06574578e-06
Iter: 1110 loss: 2.06173013e-06
Iter: 1111 loss: 2.07147286e-06
Iter: 1112 loss: 2.06029426e-06
Iter: 1113 loss: 2.05645438e-06
Iter: 1114 loss: 2.11782367e-06
Iter: 1115 loss: 2.05644483e-06
Iter: 1116 loss: 2.05430979e-06
Iter: 1117 loss: 2.05237711e-06
Iter: 1118 loss: 2.05182755e-06
Iter: 1119 loss: 2.04908292e-06
Iter: 1120 loss: 2.08438951e-06
Iter: 1121 loss: 2.04908929e-06
Iter: 1122 loss: 2.04670096e-06
Iter: 1123 loss: 2.05419701e-06
Iter: 1124 loss: 2.04600315e-06
Iter: 1125 loss: 2.0438647e-06
Iter: 1126 loss: 2.04220464e-06
Iter: 1127 loss: 2.04166327e-06
Iter: 1128 loss: 2.03799345e-06
Iter: 1129 loss: 2.06159621e-06
Iter: 1130 loss: 2.03753712e-06
Iter: 1131 loss: 2.03460195e-06
Iter: 1132 loss: 2.0362836e-06
Iter: 1133 loss: 2.03261834e-06
Iter: 1134 loss: 2.02920023e-06
Iter: 1135 loss: 2.0284715e-06
Iter: 1136 loss: 2.02620322e-06
Iter: 1137 loss: 2.02337128e-06
Iter: 1138 loss: 2.0232992e-06
Iter: 1139 loss: 2.02058618e-06
Iter: 1140 loss: 2.02059687e-06
Iter: 1141 loss: 2.01839839e-06
Iter: 1142 loss: 2.0149896e-06
Iter: 1143 loss: 2.01571402e-06
Iter: 1144 loss: 2.01249713e-06
Iter: 1145 loss: 2.00953787e-06
Iter: 1146 loss: 2.00943259e-06
Iter: 1147 loss: 2.00686304e-06
Iter: 1148 loss: 2.00508566e-06
Iter: 1149 loss: 2.00416434e-06
Iter: 1150 loss: 2.00015438e-06
Iter: 1151 loss: 2.00690738e-06
Iter: 1152 loss: 1.99842407e-06
Iter: 1153 loss: 1.99476153e-06
Iter: 1154 loss: 1.99962551e-06
Iter: 1155 loss: 1.99294527e-06
Iter: 1156 loss: 1.99011492e-06
Iter: 1157 loss: 1.98986709e-06
Iter: 1158 loss: 1.98802695e-06
Iter: 1159 loss: 1.9854831e-06
Iter: 1160 loss: 1.98537668e-06
Iter: 1161 loss: 1.98175576e-06
Iter: 1162 loss: 2.00604245e-06
Iter: 1163 loss: 1.98146131e-06
Iter: 1164 loss: 1.97922327e-06
Iter: 1165 loss: 1.97634836e-06
Iter: 1166 loss: 1.9760862e-06
Iter: 1167 loss: 1.97344525e-06
Iter: 1168 loss: 1.9734357e-06
Iter: 1169 loss: 1.97083091e-06
Iter: 1170 loss: 1.9690342e-06
Iter: 1171 loss: 1.96814244e-06
Iter: 1172 loss: 1.96468864e-06
Iter: 1173 loss: 1.97834197e-06
Iter: 1174 loss: 1.96385645e-06
Iter: 1175 loss: 1.96071096e-06
Iter: 1176 loss: 1.96038513e-06
Iter: 1177 loss: 1.95812208e-06
Iter: 1178 loss: 1.95451139e-06
Iter: 1179 loss: 1.95456755e-06
Iter: 1180 loss: 1.95191842e-06
Iter: 1181 loss: 1.95033181e-06
Iter: 1182 loss: 1.94938457e-06
Iter: 1183 loss: 1.94576046e-06
Iter: 1184 loss: 1.95537882e-06
Iter: 1185 loss: 1.94466429e-06
Iter: 1186 loss: 1.94178438e-06
Iter: 1187 loss: 1.9799752e-06
Iter: 1188 loss: 1.94181871e-06
Iter: 1189 loss: 1.93971619e-06
Iter: 1190 loss: 1.93616893e-06
Iter: 1191 loss: 1.93612959e-06
Iter: 1192 loss: 1.93381675e-06
Iter: 1193 loss: 1.93368669e-06
Iter: 1194 loss: 1.93118331e-06
Iter: 1195 loss: 1.93092865e-06
Iter: 1196 loss: 1.92912398e-06
Iter: 1197 loss: 1.92615539e-06
Iter: 1198 loss: 1.92600146e-06
Iter: 1199 loss: 1.92371454e-06
Iter: 1200 loss: 1.92022435e-06
Iter: 1201 loss: 1.94521658e-06
Iter: 1202 loss: 1.91989e-06
Iter: 1203 loss: 1.9166805e-06
Iter: 1204 loss: 1.92519201e-06
Iter: 1205 loss: 1.91570234e-06
Iter: 1206 loss: 1.91263416e-06
Iter: 1207 loss: 1.91315303e-06
Iter: 1208 loss: 1.9103968e-06
Iter: 1209 loss: 1.90667606e-06
Iter: 1210 loss: 1.92481593e-06
Iter: 1211 loss: 1.90604464e-06
Iter: 1212 loss: 1.90289575e-06
Iter: 1213 loss: 1.92963876e-06
Iter: 1214 loss: 1.90271385e-06
Iter: 1215 loss: 1.9004176e-06
Iter: 1216 loss: 1.89903085e-06
Iter: 1217 loss: 1.89809725e-06
Iter: 1218 loss: 1.89454431e-06
Iter: 1219 loss: 1.89852631e-06
Iter: 1220 loss: 1.89258276e-06
Iter: 1221 loss: 1.88941738e-06
Iter: 1222 loss: 1.91623803e-06
Iter: 1223 loss: 1.88921535e-06
Iter: 1224 loss: 1.8855593e-06
Iter: 1225 loss: 1.89015213e-06
Iter: 1226 loss: 1.88366744e-06
Iter: 1227 loss: 1.88137074e-06
Iter: 1228 loss: 1.90019841e-06
Iter: 1229 loss: 1.88112358e-06
Iter: 1230 loss: 1.8785837e-06
Iter: 1231 loss: 1.88124432e-06
Iter: 1232 loss: 1.87716159e-06
Iter: 1233 loss: 1.87458591e-06
Iter: 1234 loss: 1.87314322e-06
Iter: 1235 loss: 1.87201556e-06
Iter: 1236 loss: 1.86928969e-06
Iter: 1237 loss: 1.86925217e-06
Iter: 1238 loss: 1.86713964e-06
Iter: 1239 loss: 1.86712748e-06
Iter: 1240 loss: 1.86545e-06
Iter: 1241 loss: 1.86265743e-06
Iter: 1242 loss: 1.86270768e-06
Iter: 1243 loss: 1.86036209e-06
Iter: 1244 loss: 1.85690419e-06
Iter: 1245 loss: 1.90121114e-06
Iter: 1246 loss: 1.85688737e-06
Iter: 1247 loss: 1.8546491e-06
Iter: 1248 loss: 1.85545969e-06
Iter: 1249 loss: 1.85305157e-06
Iter: 1250 loss: 1.84986641e-06
Iter: 1251 loss: 1.8491271e-06
Iter: 1252 loss: 1.8470904e-06
Iter: 1253 loss: 1.84317412e-06
Iter: 1254 loss: 1.87441947e-06
Iter: 1255 loss: 1.84292628e-06
Iter: 1256 loss: 1.84047417e-06
Iter: 1257 loss: 1.86579905e-06
Iter: 1258 loss: 1.84035014e-06
Iter: 1259 loss: 1.83811085e-06
Iter: 1260 loss: 1.83695624e-06
Iter: 1261 loss: 1.835896e-06
Iter: 1262 loss: 1.83303132e-06
Iter: 1263 loss: 1.84141709e-06
Iter: 1264 loss: 1.83213444e-06
Iter: 1265 loss: 1.82950657e-06
Iter: 1266 loss: 1.86915474e-06
Iter: 1267 loss: 1.82951658e-06
Iter: 1268 loss: 1.82806298e-06
Iter: 1269 loss: 1.82486201e-06
Iter: 1270 loss: 1.87086107e-06
Iter: 1271 loss: 1.82469012e-06
Iter: 1272 loss: 1.8212653e-06
Iter: 1273 loss: 1.86815407e-06
Iter: 1274 loss: 1.82123654e-06
Iter: 1275 loss: 1.8190043e-06
Iter: 1276 loss: 1.817167e-06
Iter: 1277 loss: 1.81646544e-06
Iter: 1278 loss: 1.81307155e-06
Iter: 1279 loss: 1.81884604e-06
Iter: 1280 loss: 1.81154553e-06
Iter: 1281 loss: 1.8084778e-06
Iter: 1282 loss: 1.84973146e-06
Iter: 1283 loss: 1.80844961e-06
Iter: 1284 loss: 1.80610323e-06
Iter: 1285 loss: 1.80733855e-06
Iter: 1286 loss: 1.80453389e-06
Iter: 1287 loss: 1.80157622e-06
Iter: 1288 loss: 1.80751647e-06
Iter: 1289 loss: 1.80038489e-06
Iter: 1290 loss: 1.7975126e-06
Iter: 1291 loss: 1.81872042e-06
Iter: 1292 loss: 1.79727249e-06
Iter: 1293 loss: 1.79452559e-06
Iter: 1294 loss: 1.79475569e-06
Iter: 1295 loss: 1.7924657e-06
Iter: 1296 loss: 1.7894763e-06
Iter: 1297 loss: 1.79316976e-06
Iter: 1298 loss: 1.787923e-06
Iter: 1299 loss: 1.78524169e-06
Iter: 1300 loss: 1.80720758e-06
Iter: 1301 loss: 1.78505502e-06
Iter: 1302 loss: 1.78239713e-06
Iter: 1303 loss: 1.79540825e-06
Iter: 1304 loss: 1.78187418e-06
Iter: 1305 loss: 1.7796425e-06
Iter: 1306 loss: 1.77777872e-06
Iter: 1307 loss: 1.77716288e-06
Iter: 1308 loss: 1.77455286e-06
Iter: 1309 loss: 1.81063479e-06
Iter: 1310 loss: 1.77455468e-06
Iter: 1311 loss: 1.77247466e-06
Iter: 1312 loss: 1.76985827e-06
Iter: 1313 loss: 1.76968592e-06
Iter: 1314 loss: 1.7669845e-06
Iter: 1315 loss: 1.77759682e-06
Iter: 1316 loss: 1.76639173e-06
Iter: 1317 loss: 1.7634186e-06
Iter: 1318 loss: 1.78083712e-06
Iter: 1319 loss: 1.76305969e-06
Iter: 1320 loss: 1.76082131e-06
Iter: 1321 loss: 1.76004812e-06
Iter: 1322 loss: 1.75879973e-06
Iter: 1323 loss: 1.7560983e-06
Iter: 1324 loss: 1.77166248e-06
Iter: 1325 loss: 1.75568277e-06
Iter: 1326 loss: 1.75326943e-06
Iter: 1327 loss: 1.76208368e-06
Iter: 1328 loss: 1.7526952e-06
Iter: 1329 loss: 1.75022865e-06
Iter: 1330 loss: 1.75300033e-06
Iter: 1331 loss: 1.74890624e-06
Iter: 1332 loss: 1.74611614e-06
Iter: 1333 loss: 1.74719776e-06
Iter: 1334 loss: 1.74416346e-06
Iter: 1335 loss: 1.7417052e-06
Iter: 1336 loss: 1.74160573e-06
Iter: 1337 loss: 1.73993362e-06
Iter: 1338 loss: 1.73858302e-06
Iter: 1339 loss: 1.73802e-06
Iter: 1340 loss: 1.7355984e-06
Iter: 1341 loss: 1.76368076e-06
Iter: 1342 loss: 1.735512e-06
Iter: 1343 loss: 1.73345404e-06
Iter: 1344 loss: 1.73188573e-06
Iter: 1345 loss: 1.73115211e-06
Iter: 1346 loss: 1.72878083e-06
Iter: 1347 loss: 1.72901548e-06
Iter: 1348 loss: 1.72681962e-06
Iter: 1349 loss: 1.72405976e-06
Iter: 1350 loss: 1.75599848e-06
Iter: 1351 loss: 1.72396267e-06
Iter: 1352 loss: 1.72140346e-06
Iter: 1353 loss: 1.72709224e-06
Iter: 1354 loss: 1.72041837e-06
Iter: 1355 loss: 1.71830436e-06
Iter: 1356 loss: 1.71776765e-06
Iter: 1357 loss: 1.71641454e-06
Iter: 1358 loss: 1.71327486e-06
Iter: 1359 loss: 1.71778231e-06
Iter: 1360 loss: 1.71175304e-06
Iter: 1361 loss: 1.70837404e-06
Iter: 1362 loss: 1.75171795e-06
Iter: 1363 loss: 1.70834642e-06
Iter: 1364 loss: 1.70635758e-06
Iter: 1365 loss: 1.70793328e-06
Iter: 1366 loss: 1.70511873e-06
Iter: 1367 loss: 1.7024588e-06
Iter: 1368 loss: 1.70480064e-06
Iter: 1369 loss: 1.7008648e-06
Iter: 1370 loss: 1.69851273e-06
Iter: 1371 loss: 1.73612466e-06
Iter: 1372 loss: 1.6985208e-06
Iter: 1373 loss: 1.69681107e-06
Iter: 1374 loss: 1.69825728e-06
Iter: 1375 loss: 1.69572445e-06
Iter: 1376 loss: 1.69394741e-06
Iter: 1377 loss: 1.7072332e-06
Iter: 1378 loss: 1.6937995e-06
Iter: 1379 loss: 1.69193197e-06
Iter: 1380 loss: 1.68900908e-06
Iter: 1381 loss: 1.68895201e-06
Iter: 1382 loss: 1.68658369e-06
Iter: 1383 loss: 1.70841417e-06
Iter: 1384 loss: 1.68649012e-06
Iter: 1385 loss: 1.68394035e-06
Iter: 1386 loss: 1.68645897e-06
Iter: 1387 loss: 1.68252154e-06
Iter: 1388 loss: 1.67996313e-06
Iter: 1389 loss: 1.68099041e-06
Iter: 1390 loss: 1.67822623e-06
Iter: 1391 loss: 1.67523274e-06
Iter: 1392 loss: 1.68719157e-06
Iter: 1393 loss: 1.67462349e-06
Iter: 1394 loss: 1.67245332e-06
Iter: 1395 loss: 1.70110729e-06
Iter: 1396 loss: 1.67246526e-06
Iter: 1397 loss: 1.67065707e-06
Iter: 1398 loss: 1.66903101e-06
Iter: 1399 loss: 1.6685849e-06
Iter: 1400 loss: 1.66599375e-06
Iter: 1401 loss: 1.66722498e-06
Iter: 1402 loss: 1.66417726e-06
Iter: 1403 loss: 1.66115103e-06
Iter: 1404 loss: 1.68687677e-06
Iter: 1405 loss: 1.66100222e-06
Iter: 1406 loss: 1.65850292e-06
Iter: 1407 loss: 1.66796758e-06
Iter: 1408 loss: 1.65787378e-06
Iter: 1409 loss: 1.65552967e-06
Iter: 1410 loss: 1.6630961e-06
Iter: 1411 loss: 1.65490951e-06
Iter: 1412 loss: 1.65308461e-06
Iter: 1413 loss: 1.66914856e-06
Iter: 1414 loss: 1.65297274e-06
Iter: 1415 loss: 1.65121628e-06
Iter: 1416 loss: 1.64851076e-06
Iter: 1417 loss: 1.64847415e-06
Iter: 1418 loss: 1.64611265e-06
Iter: 1419 loss: 1.65724919e-06
Iter: 1420 loss: 1.64571509e-06
Iter: 1421 loss: 1.6436e-06
Iter: 1422 loss: 1.66120356e-06
Iter: 1423 loss: 1.64346307e-06
Iter: 1424 loss: 1.64178846e-06
Iter: 1425 loss: 1.63905861e-06
Iter: 1426 loss: 1.63909453e-06
Iter: 1427 loss: 1.63676236e-06
Iter: 1428 loss: 1.66516759e-06
Iter: 1429 loss: 1.63672416e-06
Iter: 1430 loss: 1.63430718e-06
Iter: 1431 loss: 1.63643415e-06
Iter: 1432 loss: 1.63289167e-06
Iter: 1433 loss: 1.63044183e-06
Iter: 1434 loss: 1.63128982e-06
Iter: 1435 loss: 1.62874107e-06
Iter: 1436 loss: 1.62594768e-06
Iter: 1437 loss: 1.64184121e-06
Iter: 1438 loss: 1.62555921e-06
Iter: 1439 loss: 1.62322976e-06
Iter: 1440 loss: 1.64357948e-06
Iter: 1441 loss: 1.62307788e-06
Iter: 1442 loss: 1.62142783e-06
Iter: 1443 loss: 1.62046172e-06
Iter: 1444 loss: 1.61975595e-06
Iter: 1445 loss: 1.61739479e-06
Iter: 1446 loss: 1.62287404e-06
Iter: 1447 loss: 1.61655555e-06
Iter: 1448 loss: 1.61448452e-06
Iter: 1449 loss: 1.61449861e-06
Iter: 1450 loss: 1.61295009e-06
Iter: 1451 loss: 1.61207799e-06
Iter: 1452 loss: 1.61134665e-06
Iter: 1453 loss: 1.60876721e-06
Iter: 1454 loss: 1.61727951e-06
Iter: 1455 loss: 1.60804279e-06
Iter: 1456 loss: 1.60586762e-06
Iter: 1457 loss: 1.60846287e-06
Iter: 1458 loss: 1.60474053e-06
Iter: 1459 loss: 1.60219247e-06
Iter: 1460 loss: 1.60250374e-06
Iter: 1461 loss: 1.60035142e-06
Iter: 1462 loss: 1.59766569e-06
Iter: 1463 loss: 1.62174115e-06
Iter: 1464 loss: 1.59752096e-06
Iter: 1465 loss: 1.59487149e-06
Iter: 1466 loss: 1.6042319e-06
Iter: 1467 loss: 1.59419744e-06
Iter: 1468 loss: 1.59223885e-06
Iter: 1469 loss: 1.59066371e-06
Iter: 1470 loss: 1.59008187e-06
Iter: 1471 loss: 1.58738203e-06
Iter: 1472 loss: 1.61060325e-06
Iter: 1473 loss: 1.58723117e-06
Iter: 1474 loss: 1.58493458e-06
Iter: 1475 loss: 1.59461229e-06
Iter: 1476 loss: 1.58449166e-06
Iter: 1477 loss: 1.58238913e-06
Iter: 1478 loss: 1.58069463e-06
Iter: 1479 loss: 1.5800282e-06
Iter: 1480 loss: 1.57773729e-06
Iter: 1481 loss: 1.60403897e-06
Iter: 1482 loss: 1.57770364e-06
Iter: 1483 loss: 1.57559293e-06
Iter: 1484 loss: 1.58202761e-06
Iter: 1485 loss: 1.57494424e-06
Iter: 1486 loss: 1.57312866e-06
Iter: 1487 loss: 1.57889133e-06
Iter: 1488 loss: 1.5725343e-06
Iter: 1489 loss: 1.57077636e-06
Iter: 1490 loss: 1.5765678e-06
Iter: 1491 loss: 1.57027102e-06
Iter: 1492 loss: 1.56856026e-06
Iter: 1493 loss: 1.5666285e-06
Iter: 1494 loss: 1.56634201e-06
Iter: 1495 loss: 1.56391025e-06
Iter: 1496 loss: 1.581329e-06
Iter: 1497 loss: 1.56374199e-06
Iter: 1498 loss: 1.56140436e-06
Iter: 1499 loss: 1.56591454e-06
Iter: 1500 loss: 1.56047224e-06
Iter: 1501 loss: 1.55800944e-06
Iter: 1502 loss: 1.56272972e-06
Iter: 1503 loss: 1.55703594e-06
Iter: 1504 loss: 1.55481041e-06
Iter: 1505 loss: 1.55348789e-06
Iter: 1506 loss: 1.55250291e-06
Iter: 1507 loss: 1.55176247e-06
Iter: 1508 loss: 1.55091311e-06
Iter: 1509 loss: 1.54972963e-06
Iter: 1510 loss: 1.54731595e-06
Iter: 1511 loss: 1.59241085e-06
Iter: 1512 loss: 1.54731583e-06
Iter: 1513 loss: 1.54470536e-06
Iter: 1514 loss: 1.55227201e-06
Iter: 1515 loss: 1.54395991e-06
Iter: 1516 loss: 1.5417711e-06
Iter: 1517 loss: 1.55642624e-06
Iter: 1518 loss: 1.54155805e-06
Iter: 1519 loss: 1.53911219e-06
Iter: 1520 loss: 1.5482467e-06
Iter: 1521 loss: 1.53853921e-06
Iter: 1522 loss: 1.53667452e-06
Iter: 1523 loss: 1.54254576e-06
Iter: 1524 loss: 1.53605652e-06
Iter: 1525 loss: 1.53423719e-06
Iter: 1526 loss: 1.54106954e-06
Iter: 1527 loss: 1.53381325e-06
Iter: 1528 loss: 1.5321815e-06
Iter: 1529 loss: 1.53041594e-06
Iter: 1530 loss: 1.53013275e-06
Iter: 1531 loss: 1.52759549e-06
Iter: 1532 loss: 1.53741962e-06
Iter: 1533 loss: 1.52695679e-06
Iter: 1534 loss: 1.52487326e-06
Iter: 1535 loss: 1.54430177e-06
Iter: 1536 loss: 1.52478151e-06
Iter: 1537 loss: 1.52297025e-06
Iter: 1538 loss: 1.52143775e-06
Iter: 1539 loss: 1.52090547e-06
Iter: 1540 loss: 1.5185725e-06
Iter: 1541 loss: 1.53460451e-06
Iter: 1542 loss: 1.51837298e-06
Iter: 1543 loss: 1.51624545e-06
Iter: 1544 loss: 1.52585176e-06
Iter: 1545 loss: 1.51584732e-06
Iter: 1546 loss: 1.5140406e-06
Iter: 1547 loss: 1.51289578e-06
Iter: 1548 loss: 1.51223594e-06
Iter: 1549 loss: 1.51008794e-06
Iter: 1550 loss: 1.53210476e-06
Iter: 1551 loss: 1.51007862e-06
Iter: 1552 loss: 1.50808569e-06
Iter: 1553 loss: 1.5126268e-06
Iter: 1554 loss: 1.50734832e-06
Iter: 1555 loss: 1.50547362e-06
Iter: 1556 loss: 1.50583469e-06
Iter: 1557 loss: 1.50418782e-06
Iter: 1558 loss: 1.50251208e-06
Iter: 1559 loss: 1.52645021e-06
Iter: 1560 loss: 1.50247365e-06
Iter: 1561 loss: 1.50085202e-06
Iter: 1562 loss: 1.50164533e-06
Iter: 1563 loss: 1.49967605e-06
Iter: 1564 loss: 1.49818959e-06
Iter: 1565 loss: 1.49801849e-06
Iter: 1566 loss: 1.49697098e-06
Iter: 1567 loss: 1.49459265e-06
Iter: 1568 loss: 1.50855055e-06
Iter: 1569 loss: 1.49428536e-06
Iter: 1570 loss: 1.49222365e-06
Iter: 1571 loss: 1.49743687e-06
Iter: 1572 loss: 1.49144398e-06
Iter: 1573 loss: 1.48968286e-06
Iter: 1574 loss: 1.48883851e-06
Iter: 1575 loss: 1.48806441e-06
Iter: 1576 loss: 1.48530648e-06
Iter: 1577 loss: 1.48901063e-06
Iter: 1578 loss: 1.4839585e-06
Iter: 1579 loss: 1.48205595e-06
Iter: 1580 loss: 1.48196727e-06
Iter: 1581 loss: 1.48051095e-06
Iter: 1582 loss: 1.48027107e-06
Iter: 1583 loss: 1.47921207e-06
Iter: 1584 loss: 1.47721698e-06
Iter: 1585 loss: 1.47933918e-06
Iter: 1586 loss: 1.4761315e-06
Iter: 1587 loss: 1.47351579e-06
Iter: 1588 loss: 1.49490336e-06
Iter: 1589 loss: 1.47334322e-06
Iter: 1590 loss: 1.47176e-06
Iter: 1591 loss: 1.47191474e-06
Iter: 1592 loss: 1.47058836e-06
Iter: 1593 loss: 1.46856632e-06
Iter: 1594 loss: 1.48419895e-06
Iter: 1595 loss: 1.46839466e-06
Iter: 1596 loss: 1.46648517e-06
Iter: 1597 loss: 1.46936281e-06
Iter: 1598 loss: 1.46557124e-06
Iter: 1599 loss: 1.4638731e-06
Iter: 1600 loss: 1.4740508e-06
Iter: 1601 loss: 1.46363459e-06
Iter: 1602 loss: 1.46212142e-06
Iter: 1603 loss: 1.46127729e-06
Iter: 1604 loss: 1.46060052e-06
Iter: 1605 loss: 1.45852096e-06
Iter: 1606 loss: 1.46019875e-06
Iter: 1607 loss: 1.45727711e-06
Iter: 1608 loss: 1.45505487e-06
Iter: 1609 loss: 1.46818024e-06
Iter: 1610 loss: 1.45479578e-06
Iter: 1611 loss: 1.45282149e-06
Iter: 1612 loss: 1.46324e-06
Iter: 1613 loss: 1.45246224e-06
Iter: 1614 loss: 1.45038018e-06
Iter: 1615 loss: 1.45014451e-06
Iter: 1616 loss: 1.44866135e-06
Iter: 1617 loss: 1.44654746e-06
Iter: 1618 loss: 1.44691421e-06
Iter: 1619 loss: 1.44486376e-06
Iter: 1620 loss: 1.4422053e-06
Iter: 1621 loss: 1.45685112e-06
Iter: 1622 loss: 1.44179182e-06
Iter: 1623 loss: 1.44004389e-06
Iter: 1624 loss: 1.44006083e-06
Iter: 1625 loss: 1.4384724e-06
Iter: 1626 loss: 1.43699503e-06
Iter: 1627 loss: 1.43673128e-06
Iter: 1628 loss: 1.43451734e-06
Iter: 1629 loss: 1.44997989e-06
Iter: 1630 loss: 1.43431657e-06
Iter: 1631 loss: 1.43227669e-06
Iter: 1632 loss: 1.44264425e-06
Iter: 1633 loss: 1.43203147e-06
Iter: 1634 loss: 1.43060993e-06
Iter: 1635 loss: 1.43194154e-06
Iter: 1636 loss: 1.4298098e-06
Iter: 1637 loss: 1.42780016e-06
Iter: 1638 loss: 1.43031036e-06
Iter: 1639 loss: 1.42680165e-06
Iter: 1640 loss: 1.42476802e-06
Iter: 1641 loss: 1.42593649e-06
Iter: 1642 loss: 1.42340616e-06
Iter: 1643 loss: 1.42131194e-06
Iter: 1644 loss: 1.43986654e-06
Iter: 1645 loss: 1.42123713e-06
Iter: 1646 loss: 1.41948067e-06
Iter: 1647 loss: 1.42371948e-06
Iter: 1648 loss: 1.41887222e-06
Iter: 1649 loss: 1.41710905e-06
Iter: 1650 loss: 1.41608325e-06
Iter: 1651 loss: 1.41528221e-06
Iter: 1652 loss: 1.41337296e-06
Iter: 1653 loss: 1.44111402e-06
Iter: 1654 loss: 1.41335977e-06
Iter: 1655 loss: 1.41171063e-06
Iter: 1656 loss: 1.41348255e-06
Iter: 1657 loss: 1.41078203e-06
Iter: 1658 loss: 1.40893633e-06
Iter: 1659 loss: 1.40997327e-06
Iter: 1660 loss: 1.40768361e-06
Iter: 1661 loss: 1.40548968e-06
Iter: 1662 loss: 1.40622296e-06
Iter: 1663 loss: 1.40398561e-06
Iter: 1664 loss: 1.40237603e-06
Iter: 1665 loss: 1.40225529e-06
Iter: 1666 loss: 1.40059387e-06
Iter: 1667 loss: 1.40193629e-06
Iter: 1668 loss: 1.3995782e-06
Iter: 1669 loss: 1.39789927e-06
Iter: 1670 loss: 1.4060895e-06
Iter: 1671 loss: 1.39759391e-06
Iter: 1672 loss: 1.39581243e-06
Iter: 1673 loss: 1.39703241e-06
Iter: 1674 loss: 1.39459121e-06
Iter: 1675 loss: 1.39295048e-06
Iter: 1676 loss: 1.39299107e-06
Iter: 1677 loss: 1.39164342e-06
Iter: 1678 loss: 1.38923724e-06
Iter: 1679 loss: 1.39627059e-06
Iter: 1680 loss: 1.38847849e-06
Iter: 1681 loss: 1.38648613e-06
Iter: 1682 loss: 1.40956945e-06
Iter: 1683 loss: 1.38645248e-06
Iter: 1684 loss: 1.38490896e-06
Iter: 1685 loss: 1.38495841e-06
Iter: 1686 loss: 1.38368353e-06
Iter: 1687 loss: 1.38162352e-06
Iter: 1688 loss: 1.38340056e-06
Iter: 1689 loss: 1.38037342e-06
Iter: 1690 loss: 1.37819393e-06
Iter: 1691 loss: 1.40999362e-06
Iter: 1692 loss: 1.37818756e-06
Iter: 1693 loss: 1.37681968e-06
Iter: 1694 loss: 1.37514371e-06
Iter: 1695 loss: 1.37501979e-06
Iter: 1696 loss: 1.37296843e-06
Iter: 1697 loss: 1.38653e-06
Iter: 1698 loss: 1.37274287e-06
Iter: 1699 loss: 1.37076893e-06
Iter: 1700 loss: 1.37716881e-06
Iter: 1701 loss: 1.37024722e-06
Iter: 1702 loss: 1.36849837e-06
Iter: 1703 loss: 1.36989615e-06
Iter: 1704 loss: 1.36744825e-06
Iter: 1705 loss: 1.36566427e-06
Iter: 1706 loss: 1.36568542e-06
Iter: 1707 loss: 1.36451092e-06
Iter: 1708 loss: 1.36262952e-06
Iter: 1709 loss: 1.36261156e-06
Iter: 1710 loss: 1.3603601e-06
Iter: 1711 loss: 1.37087795e-06
Iter: 1712 loss: 1.36000745e-06
Iter: 1713 loss: 1.35772734e-06
Iter: 1714 loss: 1.36806068e-06
Iter: 1715 loss: 1.35732648e-06
Iter: 1716 loss: 1.35595394e-06
Iter: 1717 loss: 1.3541e-06
Iter: 1718 loss: 1.35400444e-06
Iter: 1719 loss: 1.35173696e-06
Iter: 1720 loss: 1.37421875e-06
Iter: 1721 loss: 1.35167579e-06
Iter: 1722 loss: 1.3500013e-06
Iter: 1723 loss: 1.35845858e-06
Iter: 1724 loss: 1.34970787e-06
Iter: 1725 loss: 1.34799973e-06
Iter: 1726 loss: 1.34833385e-06
Iter: 1727 loss: 1.34674383e-06
Iter: 1728 loss: 1.34479683e-06
Iter: 1729 loss: 1.34572383e-06
Iter: 1730 loss: 1.34347511e-06
Iter: 1731 loss: 1.34112906e-06
Iter: 1732 loss: 1.37545567e-06
Iter: 1733 loss: 1.34111588e-06
Iter: 1734 loss: 1.33982314e-06
Iter: 1735 loss: 1.33827893e-06
Iter: 1736 loss: 1.33815206e-06
Iter: 1737 loss: 1.33600133e-06
Iter: 1738 loss: 1.34973789e-06
Iter: 1739 loss: 1.33581648e-06
Iter: 1740 loss: 1.33402227e-06
Iter: 1741 loss: 1.35102255e-06
Iter: 1742 loss: 1.33387402e-06
Iter: 1743 loss: 1.33272431e-06
Iter: 1744 loss: 1.33206402e-06
Iter: 1745 loss: 1.33164554e-06
Iter: 1746 loss: 1.32995092e-06
Iter: 1747 loss: 1.33956712e-06
Iter: 1748 loss: 1.32972514e-06
Iter: 1749 loss: 1.32823675e-06
Iter: 1750 loss: 1.32712353e-06
Iter: 1751 loss: 1.32662012e-06
Iter: 1752 loss: 1.32481784e-06
Iter: 1753 loss: 1.3472943e-06
Iter: 1754 loss: 1.32477646e-06
Iter: 1755 loss: 1.32322316e-06
Iter: 1756 loss: 1.32344621e-06
Iter: 1757 loss: 1.32200125e-06
Iter: 1758 loss: 1.32023411e-06
Iter: 1759 loss: 1.31970046e-06
Iter: 1760 loss: 1.31870297e-06
Iter: 1761 loss: 1.31634408e-06
Iter: 1762 loss: 1.3407855e-06
Iter: 1763 loss: 1.31637762e-06
Iter: 1764 loss: 1.31467664e-06
Iter: 1765 loss: 1.32515152e-06
Iter: 1766 loss: 1.31449872e-06
Iter: 1767 loss: 1.31314857e-06
Iter: 1768 loss: 1.31151819e-06
Iter: 1769 loss: 1.31132424e-06
Iter: 1770 loss: 1.30914327e-06
Iter: 1771 loss: 1.32351897e-06
Iter: 1772 loss: 1.30892715e-06
Iter: 1773 loss: 1.30728245e-06
Iter: 1774 loss: 1.31906e-06
Iter: 1775 loss: 1.30713784e-06
Iter: 1776 loss: 1.30561148e-06
Iter: 1777 loss: 1.30492185e-06
Iter: 1778 loss: 1.30413378e-06
Iter: 1779 loss: 1.30230922e-06
Iter: 1780 loss: 1.33093761e-06
Iter: 1781 loss: 1.30229989e-06
Iter: 1782 loss: 1.30110527e-06
Iter: 1783 loss: 1.29942373e-06
Iter: 1784 loss: 1.29942009e-06
Iter: 1785 loss: 1.29760633e-06
Iter: 1786 loss: 1.29827174e-06
Iter: 1787 loss: 1.29642524e-06
Iter: 1788 loss: 1.29478599e-06
Iter: 1789 loss: 1.29477121e-06
Iter: 1790 loss: 1.29343323e-06
Iter: 1791 loss: 1.29190619e-06
Iter: 1792 loss: 1.29167211e-06
Iter: 1793 loss: 1.28949478e-06
Iter: 1794 loss: 1.29647992e-06
Iter: 1795 loss: 1.28883607e-06
Iter: 1796 loss: 1.28715578e-06
Iter: 1797 loss: 1.29640705e-06
Iter: 1798 loss: 1.28690488e-06
Iter: 1799 loss: 1.28492024e-06
Iter: 1800 loss: 1.28785518e-06
Iter: 1801 loss: 1.28402303e-06
Iter: 1802 loss: 1.28228271e-06
Iter: 1803 loss: 1.28713009e-06
Iter: 1804 loss: 1.2817253e-06
Iter: 1805 loss: 1.28001511e-06
Iter: 1806 loss: 1.2893463e-06
Iter: 1807 loss: 1.27974579e-06
Iter: 1808 loss: 1.27819339e-06
Iter: 1809 loss: 1.27957128e-06
Iter: 1810 loss: 1.27730198e-06
Iter: 1811 loss: 1.27578801e-06
Iter: 1812 loss: 1.28110401e-06
Iter: 1813 loss: 1.2753087e-06
Iter: 1814 loss: 1.27335807e-06
Iter: 1815 loss: 1.27952649e-06
Iter: 1816 loss: 1.27275416e-06
Iter: 1817 loss: 1.27132103e-06
Iter: 1818 loss: 1.27331737e-06
Iter: 1819 loss: 1.27064095e-06
Iter: 1820 loss: 1.26910538e-06
Iter: 1821 loss: 1.27636804e-06
Iter: 1822 loss: 1.26883424e-06
Iter: 1823 loss: 1.26740565e-06
Iter: 1824 loss: 1.26594955e-06
Iter: 1825 loss: 1.26567693e-06
Iter: 1826 loss: 1.26383691e-06
Iter: 1827 loss: 1.27361636e-06
Iter: 1828 loss: 1.26351847e-06
Iter: 1829 loss: 1.26169596e-06
Iter: 1830 loss: 1.27452358e-06
Iter: 1831 loss: 1.26155328e-06
Iter: 1832 loss: 1.26011628e-06
Iter: 1833 loss: 1.25828421e-06
Iter: 1834 loss: 1.25815404e-06
Iter: 1835 loss: 1.25600582e-06
Iter: 1836 loss: 1.26725593e-06
Iter: 1837 loss: 1.25566635e-06
Iter: 1838 loss: 1.25362203e-06
Iter: 1839 loss: 1.25734232e-06
Iter: 1840 loss: 1.25271163e-06
Iter: 1841 loss: 1.25077884e-06
Iter: 1842 loss: 1.27488704e-06
Iter: 1843 loss: 1.25073416e-06
Iter: 1844 loss: 1.24938651e-06
Iter: 1845 loss: 1.24812891e-06
Iter: 1846 loss: 1.24782434e-06
Iter: 1847 loss: 1.24587871e-06
Iter: 1848 loss: 1.26422901e-06
Iter: 1849 loss: 1.24583153e-06
Iter: 1850 loss: 1.24422695e-06
Iter: 1851 loss: 1.25077054e-06
Iter: 1852 loss: 1.24389908e-06
Iter: 1853 loss: 1.24279143e-06
Iter: 1854 loss: 1.24616304e-06
Iter: 1855 loss: 1.24251642e-06
Iter: 1856 loss: 1.24112239e-06
Iter: 1857 loss: 1.23970085e-06
Iter: 1858 loss: 1.23941879e-06
Iter: 1859 loss: 1.23749646e-06
Iter: 1860 loss: 1.24649409e-06
Iter: 1861 loss: 1.237069e-06
Iter: 1862 loss: 1.23569453e-06
Iter: 1863 loss: 1.24662222e-06
Iter: 1864 loss: 1.2355963e-06
Iter: 1865 loss: 1.23413474e-06
Iter: 1866 loss: 1.23190682e-06
Iter: 1867 loss: 1.23186362e-06
Iter: 1868 loss: 1.23010341e-06
Iter: 1869 loss: 1.2451394e-06
Iter: 1870 loss: 1.23005361e-06
Iter: 1871 loss: 1.22834467e-06
Iter: 1872 loss: 1.2370183e-06
Iter: 1873 loss: 1.22805545e-06
Iter: 1874 loss: 1.2266072e-06
Iter: 1875 loss: 1.22482641e-06
Iter: 1876 loss: 1.22467145e-06
Iter: 1877 loss: 1.22265783e-06
Iter: 1878 loss: 1.23414202e-06
Iter: 1879 loss: 1.22234837e-06
Iter: 1880 loss: 1.22087908e-06
Iter: 1881 loss: 1.22982692e-06
Iter: 1882 loss: 1.22073493e-06
Iter: 1883 loss: 1.21920039e-06
Iter: 1884 loss: 1.22358529e-06
Iter: 1885 loss: 1.21874052e-06
Iter: 1886 loss: 1.21716369e-06
Iter: 1887 loss: 1.21870789e-06
Iter: 1888 loss: 1.21633718e-06
Iter: 1889 loss: 1.21473226e-06
Iter: 1890 loss: 1.23340783e-06
Iter: 1891 loss: 1.21474909e-06
Iter: 1892 loss: 1.21368794e-06
Iter: 1893 loss: 1.21206267e-06
Iter: 1894 loss: 1.21201811e-06
Iter: 1895 loss: 1.21025403e-06
Iter: 1896 loss: 1.21799121e-06
Iter: 1897 loss: 1.20996106e-06
Iter: 1898 loss: 1.20820755e-06
Iter: 1899 loss: 1.21772564e-06
Iter: 1900 loss: 1.20801963e-06
Iter: 1901 loss: 1.20684103e-06
Iter: 1902 loss: 1.20573327e-06
Iter: 1903 loss: 1.20549112e-06
Iter: 1904 loss: 1.20353525e-06
Iter: 1905 loss: 1.2215861e-06
Iter: 1906 loss: 1.20349955e-06
Iter: 1907 loss: 1.20188849e-06
Iter: 1908 loss: 1.20434936e-06
Iter: 1909 loss: 1.20114612e-06
Iter: 1910 loss: 1.19970946e-06
Iter: 1911 loss: 1.2017872e-06
Iter: 1912 loss: 1.19896208e-06
Iter: 1913 loss: 1.19744618e-06
Iter: 1914 loss: 1.21258063e-06
Iter: 1915 loss: 1.19740344e-06
Iter: 1916 loss: 1.19627259e-06
Iter: 1917 loss: 1.19472111e-06
Iter: 1918 loss: 1.19464937e-06
Iter: 1919 loss: 1.19265701e-06
Iter: 1920 loss: 1.19783181e-06
Iter: 1921 loss: 1.19203128e-06
Iter: 1922 loss: 1.19021854e-06
Iter: 1923 loss: 1.21551216e-06
Iter: 1924 loss: 1.19021615e-06
Iter: 1925 loss: 1.18865728e-06
Iter: 1926 loss: 1.19000742e-06
Iter: 1927 loss: 1.18777461e-06
Iter: 1928 loss: 1.18643061e-06
Iter: 1929 loss: 1.19594199e-06
Iter: 1930 loss: 1.18630714e-06
Iter: 1931 loss: 1.18523974e-06
Iter: 1932 loss: 1.18610194e-06
Iter: 1933 loss: 1.18455978e-06
Iter: 1934 loss: 1.18319656e-06
Iter: 1935 loss: 1.18170408e-06
Iter: 1936 loss: 1.18152298e-06
Iter: 1937 loss: 1.17949912e-06
Iter: 1938 loss: 1.19721199e-06
Iter: 1939 loss: 1.17944228e-06
Iter: 1940 loss: 1.17795901e-06
Iter: 1941 loss: 1.18922435e-06
Iter: 1942 loss: 1.17783509e-06
Iter: 1943 loss: 1.17654713e-06
Iter: 1944 loss: 1.17493119e-06
Iter: 1945 loss: 1.17484478e-06
Iter: 1946 loss: 1.1733905e-06
Iter: 1947 loss: 1.19043784e-06
Iter: 1948 loss: 1.17335196e-06
Iter: 1949 loss: 1.17168599e-06
Iter: 1950 loss: 1.17117895e-06
Iter: 1951 loss: 1.17023922e-06
Iter: 1952 loss: 1.16865863e-06
Iter: 1953 loss: 1.17646823e-06
Iter: 1954 loss: 1.1684341e-06
Iter: 1955 loss: 1.16686908e-06
Iter: 1956 loss: 1.17367767e-06
Iter: 1957 loss: 1.16656975e-06
Iter: 1958 loss: 1.16534773e-06
Iter: 1959 loss: 1.16636159e-06
Iter: 1960 loss: 1.16463571e-06
Iter: 1961 loss: 1.16345916e-06
Iter: 1962 loss: 1.17608693e-06
Iter: 1963 loss: 1.16341732e-06
Iter: 1964 loss: 1.16227488e-06
Iter: 1965 loss: 1.16243211e-06
Iter: 1966 loss: 1.1613472e-06
Iter: 1967 loss: 1.1600664e-06
Iter: 1968 loss: 1.16117485e-06
Iter: 1969 loss: 1.15931221e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.8 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi2.8
+ date
Wed Nov  4 15:51:29 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.4/300_300_300_1 --function f2 --psi -1 --alpha 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5130c2f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f510a7248c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f510a688488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f510a724620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f510a688c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f510a7247b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e467c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e467cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e467e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e45fb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e45fbb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e45fb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e4627378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f510a5ff268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e46447b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e4550158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e455e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e44b56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e450a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e44c51e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e450c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e450cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e447f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e448a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e448a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e448ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e43b91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e43b0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e43af0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e43f7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e441a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e441a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e42d2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e437d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e42d27b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e434b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.006717499
test_loss: 0.009372512
train_loss: 0.0059793503
test_loss: 0.008946436
train_loss: 0.0053328834
test_loss: 0.008668368
train_loss: 0.0052522914
test_loss: 0.008662435
train_loss: 0.005395429
test_loss: 0.008927118
train_loss: 0.005239272
test_loss: 0.008592324
train_loss: 0.0049735974
test_loss: 0.008340942
train_loss: 0.0051936535
test_loss: 0.008341181
train_loss: 0.0052695223
test_loss: 0.008500897
train_loss: 0.0051608495
test_loss: 0.008508767
train_loss: 0.005273385
test_loss: 0.008482496
train_loss: 0.0048993505
test_loss: 0.0083945915
train_loss: 0.0047310265
test_loss: 0.008319989
train_loss: 0.0049745236
test_loss: 0.008463179
train_loss: 0.0053790435
test_loss: 0.008373885
train_loss: 0.0054962486
test_loss: 0.008336463
train_loss: 0.005044606
test_loss: 0.008404023
train_loss: 0.004744139
test_loss: 0.008291433
train_loss: 0.005602034
test_loss: 0.008413539
train_loss: 0.004844565
test_loss: 0.008260946
train_loss: 0.004643667
test_loss: 0.008290133
train_loss: 0.0050987913
test_loss: 0.0083155995
train_loss: 0.0046876394
test_loss: 0.00818226
train_loss: 0.004870916
test_loss: 0.008135701
train_loss: 0.004892884
test_loss: 0.008214858
train_loss: 0.0046454794
test_loss: 0.008155119
train_loss: 0.0048784013
test_loss: 0.008284449
train_loss: 0.00487303
test_loss: 0.00809107
train_loss: 0.0047427015
test_loss: 0.00809128
train_loss: 0.004777363
test_loss: 0.008187864
train_loss: 0.00503136
test_loss: 0.008144972
train_loss: 0.004521717
test_loss: 0.008100678
train_loss: 0.0047815912
test_loss: 0.008026676
train_loss: 0.004637397
test_loss: 0.00823492
train_loss: 0.004454428
test_loss: 0.008135855
train_loss: 0.004964021
test_loss: 0.008065031
train_loss: 0.0047839903
test_loss: 0.008315276
train_loss: 0.0047477325
test_loss: 0.008089115
train_loss: 0.004469796
test_loss: 0.008061281
train_loss: 0.0050177975
test_loss: 0.008306551
train_loss: 0.004701741
test_loss: 0.008191894
train_loss: 0.004579927
test_loss: 0.008118717
train_loss: 0.004220665
test_loss: 0.008089642
train_loss: 0.0049616396
test_loss: 0.008231552
train_loss: 0.0047654714
test_loss: 0.008279926
train_loss: 0.004675368
test_loss: 0.008079517
train_loss: 0.004360947
test_loss: 0.008076835
train_loss: 0.0049457503
test_loss: 0.008145593
train_loss: 0.0048135696
test_loss: 0.007986543
train_loss: 0.0044409717
test_loss: 0.008159692
train_loss: 0.0044247755
test_loss: 0.008071563
train_loss: 0.004547528
test_loss: 0.008036352
train_loss: 0.0045086113
test_loss: 0.008052015
train_loss: 0.0045137303
test_loss: 0.00803903
train_loss: 0.0048541673
test_loss: 0.008197801
train_loss: 0.0046363743
test_loss: 0.008054874
train_loss: 0.0043227277
test_loss: 0.007979796
train_loss: 0.004559684
test_loss: 0.008269652
train_loss: 0.004894818
test_loss: 0.008184994
train_loss: 0.0046687587
test_loss: 0.008103622
train_loss: 0.00435019
test_loss: 0.008053449
train_loss: 0.0043893177
test_loss: 0.007994704
train_loss: 0.0048761973
test_loss: 0.008109114
train_loss: 0.00446582
test_loss: 0.00796997
train_loss: 0.0044895113
test_loss: 0.008037506
train_loss: 0.004667483
test_loss: 0.00830631
train_loss: 0.0049829073
test_loss: 0.008105682
train_loss: 0.0043695746
test_loss: 0.007963794
train_loss: 0.0044162003
test_loss: 0.0080285985
train_loss: 0.004257912
test_loss: 0.007898056
train_loss: 0.00437306
test_loss: 0.008051716
train_loss: 0.004307727
test_loss: 0.008039368
train_loss: 0.0045205676
test_loss: 0.008132296
train_loss: 0.004173022
test_loss: 0.007839514
train_loss: 0.0044763545
test_loss: 0.00822646
train_loss: 0.0045155087
test_loss: 0.00795471
train_loss: 0.0044550067
test_loss: 0.008044512
train_loss: 0.0041669663
test_loss: 0.007889559
train_loss: 0.004560938
test_loss: 0.008052243
train_loss: 0.004455113
test_loss: 0.007921215
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.8/300_300_300_1 --optimizer lbfgs --function f2 --psi -1 --alpha 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7eceb0048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7ece92e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7ecea9730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7ece05e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7ece05598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7ece34f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7ecd940d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7ecd94268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7a88c8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7a8868840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7a88b4c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7a8884048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7a8857b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7a8857c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7a88207b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7a87bc268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7a8820c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc784092ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc78402e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc784094048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7840946a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c25b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c232510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c23a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c22b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c22b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c1b6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c1b6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c160510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c1141e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c159950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c13f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c13f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c13f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c13f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc77c0bf488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.56961209e-05
Iter: 2 loss: 3.3053173e-05
Iter: 3 loss: 2.79974811e-05
Iter: 4 loss: 0.000134659596
Iter: 5 loss: 2.7973856e-05
Iter: 6 loss: 2.36621709e-05
Iter: 7 loss: 5.7269026e-05
Iter: 8 loss: 2.33489427e-05
Iter: 9 loss: 2.09184109e-05
Iter: 10 loss: 2.50102985e-05
Iter: 11 loss: 1.98225134e-05
Iter: 12 loss: 1.82616459e-05
Iter: 13 loss: 2.99101066e-05
Iter: 14 loss: 1.81364521e-05
Iter: 15 loss: 1.69784817e-05
Iter: 16 loss: 2.20290312e-05
Iter: 17 loss: 1.67465514e-05
Iter: 18 loss: 1.59022729e-05
Iter: 19 loss: 2.20470356e-05
Iter: 20 loss: 1.58292478e-05
Iter: 21 loss: 1.51364984e-05
Iter: 22 loss: 1.61520074e-05
Iter: 23 loss: 1.48014515e-05
Iter: 24 loss: 1.42193539e-05
Iter: 25 loss: 1.4777348e-05
Iter: 26 loss: 1.38879777e-05
Iter: 27 loss: 1.32575369e-05
Iter: 28 loss: 1.86618054e-05
Iter: 29 loss: 1.32225478e-05
Iter: 30 loss: 1.27606827e-05
Iter: 31 loss: 1.25592733e-05
Iter: 32 loss: 1.23233349e-05
Iter: 33 loss: 1.17952295e-05
Iter: 34 loss: 1.31284987e-05
Iter: 35 loss: 1.16120909e-05
Iter: 36 loss: 1.12337302e-05
Iter: 37 loss: 1.67822654e-05
Iter: 38 loss: 1.12331563e-05
Iter: 39 loss: 1.09454377e-05
Iter: 40 loss: 1.12250518e-05
Iter: 41 loss: 1.07820852e-05
Iter: 42 loss: 1.04971241e-05
Iter: 43 loss: 1.09341636e-05
Iter: 44 loss: 1.03625243e-05
Iter: 45 loss: 1.00933412e-05
Iter: 46 loss: 1.02178728e-05
Iter: 47 loss: 9.91126763e-06
Iter: 48 loss: 9.64421633e-06
Iter: 49 loss: 1.27850399e-05
Iter: 50 loss: 9.64032552e-06
Iter: 51 loss: 9.43134728e-06
Iter: 52 loss: 1.09602361e-05
Iter: 53 loss: 9.41390681e-06
Iter: 54 loss: 9.25672884e-06
Iter: 55 loss: 9.19894046e-06
Iter: 56 loss: 9.11154712e-06
Iter: 57 loss: 8.9840305e-06
Iter: 58 loss: 8.97907194e-06
Iter: 59 loss: 8.87511e-06
Iter: 60 loss: 9.00909254e-06
Iter: 61 loss: 8.82211225e-06
Iter: 62 loss: 8.72367491e-06
Iter: 63 loss: 8.845e-06
Iter: 64 loss: 8.67259769e-06
Iter: 65 loss: 8.54618065e-06
Iter: 66 loss: 8.81213418e-06
Iter: 67 loss: 8.49590833e-06
Iter: 68 loss: 8.39322638e-06
Iter: 69 loss: 8.72088822e-06
Iter: 70 loss: 8.36386243e-06
Iter: 71 loss: 8.24245762e-06
Iter: 72 loss: 8.60023101e-06
Iter: 73 loss: 8.20547575e-06
Iter: 74 loss: 8.11560858e-06
Iter: 75 loss: 8.03088733e-06
Iter: 76 loss: 8.01000169e-06
Iter: 77 loss: 7.88981743e-06
Iter: 78 loss: 8.86105e-06
Iter: 79 loss: 7.88169e-06
Iter: 80 loss: 7.78569847e-06
Iter: 81 loss: 8.40579469e-06
Iter: 82 loss: 7.77507375e-06
Iter: 83 loss: 7.69642793e-06
Iter: 84 loss: 7.7330933e-06
Iter: 85 loss: 7.64335346e-06
Iter: 86 loss: 7.55979272e-06
Iter: 87 loss: 7.74736873e-06
Iter: 88 loss: 7.52812775e-06
Iter: 89 loss: 7.4405084e-06
Iter: 90 loss: 7.63237495e-06
Iter: 91 loss: 7.40673931e-06
Iter: 92 loss: 7.32911303e-06
Iter: 93 loss: 8.24290237e-06
Iter: 94 loss: 7.32790704e-06
Iter: 95 loss: 7.26524604e-06
Iter: 96 loss: 7.43473447e-06
Iter: 97 loss: 7.24478741e-06
Iter: 98 loss: 7.20091748e-06
Iter: 99 loss: 7.60513103e-06
Iter: 100 loss: 7.19896252e-06
Iter: 101 loss: 7.15865917e-06
Iter: 102 loss: 7.09995311e-06
Iter: 103 loss: 7.09820324e-06
Iter: 104 loss: 7.04235936e-06
Iter: 105 loss: 7.22817549e-06
Iter: 106 loss: 7.02704256e-06
Iter: 107 loss: 6.96567258e-06
Iter: 108 loss: 7.32514309e-06
Iter: 109 loss: 6.95746166e-06
Iter: 110 loss: 6.91262721e-06
Iter: 111 loss: 6.95136441e-06
Iter: 112 loss: 6.88612909e-06
Iter: 113 loss: 6.83623375e-06
Iter: 114 loss: 7.1333011e-06
Iter: 115 loss: 6.82992049e-06
Iter: 116 loss: 6.78631113e-06
Iter: 117 loss: 6.76847685e-06
Iter: 118 loss: 6.74523e-06
Iter: 119 loss: 6.69144129e-06
Iter: 120 loss: 6.88841146e-06
Iter: 121 loss: 6.67814083e-06
Iter: 122 loss: 6.62496859e-06
Iter: 123 loss: 6.71911812e-06
Iter: 124 loss: 6.60196201e-06
Iter: 125 loss: 6.55854774e-06
Iter: 126 loss: 7.12026031e-06
Iter: 127 loss: 6.55822942e-06
Iter: 128 loss: 6.52262133e-06
Iter: 129 loss: 6.52341305e-06
Iter: 130 loss: 6.49451977e-06
Iter: 131 loss: 6.44983857e-06
Iter: 132 loss: 6.50291304e-06
Iter: 133 loss: 6.42637406e-06
Iter: 134 loss: 6.37887661e-06
Iter: 135 loss: 6.49965e-06
Iter: 136 loss: 6.36235973e-06
Iter: 137 loss: 6.33711079e-06
Iter: 138 loss: 6.3333755e-06
Iter: 139 loss: 6.30926206e-06
Iter: 140 loss: 6.29716669e-06
Iter: 141 loss: 6.28602811e-06
Iter: 142 loss: 6.25259781e-06
Iter: 143 loss: 6.43737303e-06
Iter: 144 loss: 6.24798122e-06
Iter: 145 loss: 6.21822e-06
Iter: 146 loss: 6.19975708e-06
Iter: 147 loss: 6.18772629e-06
Iter: 148 loss: 6.15309182e-06
Iter: 149 loss: 6.24419363e-06
Iter: 150 loss: 6.14131113e-06
Iter: 151 loss: 6.10745064e-06
Iter: 152 loss: 6.48795958e-06
Iter: 153 loss: 6.10672851e-06
Iter: 154 loss: 6.08198388e-06
Iter: 155 loss: 6.06164167e-06
Iter: 156 loss: 6.0545226e-06
Iter: 157 loss: 6.01819e-06
Iter: 158 loss: 6.16801299e-06
Iter: 159 loss: 6.01030069e-06
Iter: 160 loss: 5.9724166e-06
Iter: 161 loss: 6.11210407e-06
Iter: 162 loss: 5.96295968e-06
Iter: 163 loss: 5.93517234e-06
Iter: 164 loss: 5.91899516e-06
Iter: 165 loss: 5.90717536e-06
Iter: 166 loss: 5.8641217e-06
Iter: 167 loss: 6.08512755e-06
Iter: 168 loss: 5.85715225e-06
Iter: 169 loss: 5.82547818e-06
Iter: 170 loss: 6.05411697e-06
Iter: 171 loss: 5.82277289e-06
Iter: 172 loss: 5.79567222e-06
Iter: 173 loss: 5.87096e-06
Iter: 174 loss: 5.78721938e-06
Iter: 175 loss: 5.76409e-06
Iter: 176 loss: 5.7640982e-06
Iter: 177 loss: 5.74542e-06
Iter: 178 loss: 5.7313905e-06
Iter: 179 loss: 5.72873614e-06
Iter: 180 loss: 5.71247392e-06
Iter: 181 loss: 5.68991072e-06
Iter: 182 loss: 5.68893392e-06
Iter: 183 loss: 5.66068366e-06
Iter: 184 loss: 5.74350179e-06
Iter: 185 loss: 5.65212167e-06
Iter: 186 loss: 5.63027834e-06
Iter: 187 loss: 5.80074311e-06
Iter: 188 loss: 5.62850937e-06
Iter: 189 loss: 5.60774834e-06
Iter: 190 loss: 5.58377815e-06
Iter: 191 loss: 5.58092324e-06
Iter: 192 loss: 5.55308634e-06
Iter: 193 loss: 5.69307e-06
Iter: 194 loss: 5.54854432e-06
Iter: 195 loss: 5.52308757e-06
Iter: 196 loss: 5.74801106e-06
Iter: 197 loss: 5.52182109e-06
Iter: 198 loss: 5.50451477e-06
Iter: 199 loss: 5.47996888e-06
Iter: 200 loss: 5.47912623e-06
Iter: 201 loss: 5.4534421e-06
Iter: 202 loss: 5.69271606e-06
Iter: 203 loss: 5.45239345e-06
Iter: 204 loss: 5.42981297e-06
Iter: 205 loss: 5.48314301e-06
Iter: 206 loss: 5.42153612e-06
Iter: 207 loss: 5.4005377e-06
Iter: 208 loss: 5.40861765e-06
Iter: 209 loss: 5.38580298e-06
Iter: 210 loss: 5.35616027e-06
Iter: 211 loss: 5.42275484e-06
Iter: 212 loss: 5.34507853e-06
Iter: 213 loss: 5.3210224e-06
Iter: 214 loss: 5.54360395e-06
Iter: 215 loss: 5.31992146e-06
Iter: 216 loss: 5.2995947e-06
Iter: 217 loss: 5.34433366e-06
Iter: 218 loss: 5.29190856e-06
Iter: 219 loss: 5.27039811e-06
Iter: 220 loss: 5.29913177e-06
Iter: 221 loss: 5.25964379e-06
Iter: 222 loss: 5.24597908e-06
Iter: 223 loss: 5.24452662e-06
Iter: 224 loss: 5.2365167e-06
Iter: 225 loss: 5.21325637e-06
Iter: 226 loss: 5.32269405e-06
Iter: 227 loss: 5.20497906e-06
Iter: 228 loss: 5.18427396e-06
Iter: 229 loss: 5.18407614e-06
Iter: 230 loss: 5.16723549e-06
Iter: 231 loss: 5.20446793e-06
Iter: 232 loss: 5.16068394e-06
Iter: 233 loss: 5.14110661e-06
Iter: 234 loss: 5.13026544e-06
Iter: 235 loss: 5.12171573e-06
Iter: 236 loss: 5.10145901e-06
Iter: 237 loss: 5.30532907e-06
Iter: 238 loss: 5.10080054e-06
Iter: 239 loss: 5.07830055e-06
Iter: 240 loss: 5.09441816e-06
Iter: 241 loss: 5.0645076e-06
Iter: 242 loss: 5.04801301e-06
Iter: 243 loss: 5.03974e-06
Iter: 244 loss: 5.03213278e-06
Iter: 245 loss: 5.0106355e-06
Iter: 246 loss: 5.34565925e-06
Iter: 247 loss: 5.01066916e-06
Iter: 248 loss: 4.9958e-06
Iter: 249 loss: 5.00108945e-06
Iter: 250 loss: 4.98533836e-06
Iter: 251 loss: 4.96732446e-06
Iter: 252 loss: 5.01633349e-06
Iter: 253 loss: 4.96158282e-06
Iter: 254 loss: 4.94283267e-06
Iter: 255 loss: 4.97366455e-06
Iter: 256 loss: 4.93438074e-06
Iter: 257 loss: 4.91936134e-06
Iter: 258 loss: 4.91941728e-06
Iter: 259 loss: 4.90879529e-06
Iter: 260 loss: 4.92277741e-06
Iter: 261 loss: 4.90349339e-06
Iter: 262 loss: 4.88806836e-06
Iter: 263 loss: 4.91191759e-06
Iter: 264 loss: 4.88096885e-06
Iter: 265 loss: 4.86816225e-06
Iter: 266 loss: 4.85809e-06
Iter: 267 loss: 4.85406099e-06
Iter: 268 loss: 4.83557415e-06
Iter: 269 loss: 4.92801792e-06
Iter: 270 loss: 4.83246185e-06
Iter: 271 loss: 4.81435836e-06
Iter: 272 loss: 4.93188236e-06
Iter: 273 loss: 4.81251118e-06
Iter: 274 loss: 4.79868231e-06
Iter: 275 loss: 4.78628181e-06
Iter: 276 loss: 4.78263064e-06
Iter: 277 loss: 4.76351943e-06
Iter: 278 loss: 4.9245e-06
Iter: 279 loss: 4.76248943e-06
Iter: 280 loss: 4.74526405e-06
Iter: 281 loss: 4.80342169e-06
Iter: 282 loss: 4.74046965e-06
Iter: 283 loss: 4.72616e-06
Iter: 284 loss: 4.71904e-06
Iter: 285 loss: 4.71232579e-06
Iter: 286 loss: 4.69449151e-06
Iter: 287 loss: 4.74905482e-06
Iter: 288 loss: 4.68915641e-06
Iter: 289 loss: 4.67157497e-06
Iter: 290 loss: 4.80614608e-06
Iter: 291 loss: 4.67030077e-06
Iter: 292 loss: 4.65588482e-06
Iter: 293 loss: 4.67605878e-06
Iter: 294 loss: 4.64839513e-06
Iter: 295 loss: 4.63349079e-06
Iter: 296 loss: 4.63856e-06
Iter: 297 loss: 4.62282742e-06
Iter: 298 loss: 4.6093237e-06
Iter: 299 loss: 4.60897536e-06
Iter: 300 loss: 4.59765533e-06
Iter: 301 loss: 4.61818536e-06
Iter: 302 loss: 4.592609e-06
Iter: 303 loss: 4.58161139e-06
Iter: 304 loss: 4.58224895e-06
Iter: 305 loss: 4.57287706e-06
Iter: 306 loss: 4.55563213e-06
Iter: 307 loss: 4.60950196e-06
Iter: 308 loss: 4.5507245e-06
Iter: 309 loss: 4.53925941e-06
Iter: 310 loss: 4.53652319e-06
Iter: 311 loss: 4.52930772e-06
Iter: 312 loss: 4.51194455e-06
Iter: 313 loss: 4.62950084e-06
Iter: 314 loss: 4.51010101e-06
Iter: 315 loss: 4.49708432e-06
Iter: 316 loss: 4.58877093e-06
Iter: 317 loss: 4.49597292e-06
Iter: 318 loss: 4.48644323e-06
Iter: 319 loss: 4.47217417e-06
Iter: 320 loss: 4.47199136e-06
Iter: 321 loss: 4.45685964e-06
Iter: 322 loss: 4.673283e-06
Iter: 323 loss: 4.45677415e-06
Iter: 324 loss: 4.44362468e-06
Iter: 325 loss: 4.45424075e-06
Iter: 326 loss: 4.43573026e-06
Iter: 327 loss: 4.4227545e-06
Iter: 328 loss: 4.41397879e-06
Iter: 329 loss: 4.40911344e-06
Iter: 330 loss: 4.38907227e-06
Iter: 331 loss: 4.48757874e-06
Iter: 332 loss: 4.38568e-06
Iter: 333 loss: 4.37043491e-06
Iter: 334 loss: 4.50823973e-06
Iter: 335 loss: 4.36983919e-06
Iter: 336 loss: 4.3566979e-06
Iter: 337 loss: 4.36502432e-06
Iter: 338 loss: 4.34844378e-06
Iter: 339 loss: 4.33519381e-06
Iter: 340 loss: 4.40137364e-06
Iter: 341 loss: 4.33302739e-06
Iter: 342 loss: 4.32151592e-06
Iter: 343 loss: 4.43031558e-06
Iter: 344 loss: 4.32099296e-06
Iter: 345 loss: 4.31217359e-06
Iter: 346 loss: 4.30753789e-06
Iter: 347 loss: 4.303477e-06
Iter: 348 loss: 4.29328156e-06
Iter: 349 loss: 4.3142436e-06
Iter: 350 loss: 4.28919884e-06
Iter: 351 loss: 4.27448276e-06
Iter: 352 loss: 4.29698866e-06
Iter: 353 loss: 4.26738598e-06
Iter: 354 loss: 4.25566759e-06
Iter: 355 loss: 4.26726092e-06
Iter: 356 loss: 4.24917835e-06
Iter: 357 loss: 4.23711936e-06
Iter: 358 loss: 4.35682523e-06
Iter: 359 loss: 4.23657411e-06
Iter: 360 loss: 4.22460971e-06
Iter: 361 loss: 4.23014535e-06
Iter: 362 loss: 4.21656569e-06
Iter: 363 loss: 4.20386414e-06
Iter: 364 loss: 4.21528512e-06
Iter: 365 loss: 4.19656226e-06
Iter: 366 loss: 4.18349691e-06
Iter: 367 loss: 4.37088192e-06
Iter: 368 loss: 4.18351647e-06
Iter: 369 loss: 4.17418369e-06
Iter: 370 loss: 4.16243347e-06
Iter: 371 loss: 4.1615e-06
Iter: 372 loss: 4.14630904e-06
Iter: 373 loss: 4.17125557e-06
Iter: 374 loss: 4.13947055e-06
Iter: 375 loss: 4.12683676e-06
Iter: 376 loss: 4.22755e-06
Iter: 377 loss: 4.12590452e-06
Iter: 378 loss: 4.11402e-06
Iter: 379 loss: 4.15422437e-06
Iter: 380 loss: 4.11086785e-06
Iter: 381 loss: 4.09945096e-06
Iter: 382 loss: 4.12762893e-06
Iter: 383 loss: 4.09535551e-06
Iter: 384 loss: 4.08617143e-06
Iter: 385 loss: 4.14056649e-06
Iter: 386 loss: 4.08494907e-06
Iter: 387 loss: 4.07394509e-06
Iter: 388 loss: 4.09416816e-06
Iter: 389 loss: 4.06923e-06
Iter: 390 loss: 4.05961237e-06
Iter: 391 loss: 4.05462151e-06
Iter: 392 loss: 4.05021456e-06
Iter: 393 loss: 4.03724744e-06
Iter: 394 loss: 4.13139696e-06
Iter: 395 loss: 4.03611148e-06
Iter: 396 loss: 4.02438582e-06
Iter: 397 loss: 4.02715432e-06
Iter: 398 loss: 4.01573152e-06
Iter: 399 loss: 4.00177669e-06
Iter: 400 loss: 4.04178218e-06
Iter: 401 loss: 3.99735154e-06
Iter: 402 loss: 3.98665088e-06
Iter: 403 loss: 4.06356685e-06
Iter: 404 loss: 3.98566044e-06
Iter: 405 loss: 3.97469739e-06
Iter: 406 loss: 3.9942629e-06
Iter: 407 loss: 3.96988344e-06
Iter: 408 loss: 3.95975303e-06
Iter: 409 loss: 3.96346059e-06
Iter: 410 loss: 3.95269217e-06
Iter: 411 loss: 3.94171093e-06
Iter: 412 loss: 4.06147319e-06
Iter: 413 loss: 3.94141216e-06
Iter: 414 loss: 3.93090249e-06
Iter: 415 loss: 3.92561378e-06
Iter: 416 loss: 3.92048969e-06
Iter: 417 loss: 3.9098959e-06
Iter: 418 loss: 3.92924312e-06
Iter: 419 loss: 3.9052984e-06
Iter: 420 loss: 3.89121942e-06
Iter: 421 loss: 3.91052345e-06
Iter: 422 loss: 3.88431272e-06
Iter: 423 loss: 3.8739e-06
Iter: 424 loss: 3.87384262e-06
Iter: 425 loss: 3.86566398e-06
Iter: 426 loss: 3.86519696e-06
Iter: 427 loss: 3.85906242e-06
Iter: 428 loss: 3.8494386e-06
Iter: 429 loss: 4.00132194e-06
Iter: 430 loss: 3.84943951e-06
Iter: 431 loss: 3.84340183e-06
Iter: 432 loss: 3.82875351e-06
Iter: 433 loss: 3.98923839e-06
Iter: 434 loss: 3.82727831e-06
Iter: 435 loss: 3.81755353e-06
Iter: 436 loss: 3.81746213e-06
Iter: 437 loss: 3.80861366e-06
Iter: 438 loss: 3.8073822e-06
Iter: 439 loss: 3.8011342e-06
Iter: 440 loss: 3.7881241e-06
Iter: 441 loss: 3.81843665e-06
Iter: 442 loss: 3.78335108e-06
Iter: 443 loss: 3.77320544e-06
Iter: 444 loss: 3.80315078e-06
Iter: 445 loss: 3.77001402e-06
Iter: 446 loss: 3.7594159e-06
Iter: 447 loss: 3.8138287e-06
Iter: 448 loss: 3.7577056e-06
Iter: 449 loss: 3.74693946e-06
Iter: 450 loss: 3.77847709e-06
Iter: 451 loss: 3.74368301e-06
Iter: 452 loss: 3.73590842e-06
Iter: 453 loss: 3.7323357e-06
Iter: 454 loss: 3.72848763e-06
Iter: 455 loss: 3.71858437e-06
Iter: 456 loss: 3.71855981e-06
Iter: 457 loss: 3.71187798e-06
Iter: 458 loss: 3.70344515e-06
Iter: 459 loss: 3.70276121e-06
Iter: 460 loss: 3.69149e-06
Iter: 461 loss: 3.69495456e-06
Iter: 462 loss: 3.68344445e-06
Iter: 463 loss: 3.67159782e-06
Iter: 464 loss: 3.85511839e-06
Iter: 465 loss: 3.67163898e-06
Iter: 466 loss: 3.66469885e-06
Iter: 467 loss: 3.74981892e-06
Iter: 468 loss: 3.66455401e-06
Iter: 469 loss: 3.65898086e-06
Iter: 470 loss: 3.65563733e-06
Iter: 471 loss: 3.65328788e-06
Iter: 472 loss: 3.6449635e-06
Iter: 473 loss: 3.67228586e-06
Iter: 474 loss: 3.64266566e-06
Iter: 475 loss: 3.63416484e-06
Iter: 476 loss: 3.62750779e-06
Iter: 477 loss: 3.6249171e-06
Iter: 478 loss: 3.61357502e-06
Iter: 479 loss: 3.68208794e-06
Iter: 480 loss: 3.61217917e-06
Iter: 481 loss: 3.60290619e-06
Iter: 482 loss: 3.65048118e-06
Iter: 483 loss: 3.60126842e-06
Iter: 484 loss: 3.59224077e-06
Iter: 485 loss: 3.60474132e-06
Iter: 486 loss: 3.58779198e-06
Iter: 487 loss: 3.5786918e-06
Iter: 488 loss: 3.58275656e-06
Iter: 489 loss: 3.57250246e-06
Iter: 490 loss: 3.56195369e-06
Iter: 491 loss: 3.59777482e-06
Iter: 492 loss: 3.55900556e-06
Iter: 493 loss: 3.54605436e-06
Iter: 494 loss: 3.62579954e-06
Iter: 495 loss: 3.54450435e-06
Iter: 496 loss: 3.53822e-06
Iter: 497 loss: 3.53659152e-06
Iter: 498 loss: 3.53266864e-06
Iter: 499 loss: 3.52391157e-06
Iter: 500 loss: 3.57561566e-06
Iter: 501 loss: 3.52281199e-06
Iter: 502 loss: 3.51460881e-06
Iter: 503 loss: 3.55776433e-06
Iter: 504 loss: 3.51337485e-06
Iter: 505 loss: 3.50734172e-06
Iter: 506 loss: 3.50583832e-06
Iter: 507 loss: 3.502e-06
Iter: 508 loss: 3.49203674e-06
Iter: 509 loss: 3.57784052e-06
Iter: 510 loss: 3.49153879e-06
Iter: 511 loss: 3.48527215e-06
Iter: 512 loss: 3.47988907e-06
Iter: 513 loss: 3.47819423e-06
Iter: 514 loss: 3.46905767e-06
Iter: 515 loss: 3.49033667e-06
Iter: 516 loss: 3.46578827e-06
Iter: 517 loss: 3.45656349e-06
Iter: 518 loss: 3.51747849e-06
Iter: 519 loss: 3.45562e-06
Iter: 520 loss: 3.44844557e-06
Iter: 521 loss: 3.4453717e-06
Iter: 522 loss: 3.44170621e-06
Iter: 523 loss: 3.43139573e-06
Iter: 524 loss: 3.46845218e-06
Iter: 525 loss: 3.42877638e-06
Iter: 526 loss: 3.42083058e-06
Iter: 527 loss: 3.4921095e-06
Iter: 528 loss: 3.42058047e-06
Iter: 529 loss: 3.41310079e-06
Iter: 530 loss: 3.41110808e-06
Iter: 531 loss: 3.40659494e-06
Iter: 532 loss: 3.39657095e-06
Iter: 533 loss: 3.42258772e-06
Iter: 534 loss: 3.39318535e-06
Iter: 535 loss: 3.38363589e-06
Iter: 536 loss: 3.39297912e-06
Iter: 537 loss: 3.37835058e-06
Iter: 538 loss: 3.37106212e-06
Iter: 539 loss: 3.37072606e-06
Iter: 540 loss: 3.36488461e-06
Iter: 541 loss: 3.35808295e-06
Iter: 542 loss: 3.35729e-06
Iter: 543 loss: 3.34956076e-06
Iter: 544 loss: 3.44931459e-06
Iter: 545 loss: 3.34944571e-06
Iter: 546 loss: 3.34188803e-06
Iter: 547 loss: 3.35404775e-06
Iter: 548 loss: 3.33824073e-06
Iter: 549 loss: 3.33200546e-06
Iter: 550 loss: 3.32208697e-06
Iter: 551 loss: 3.32204081e-06
Iter: 552 loss: 3.31243882e-06
Iter: 553 loss: 3.42709291e-06
Iter: 554 loss: 3.31228944e-06
Iter: 555 loss: 3.30460352e-06
Iter: 556 loss: 3.32763784e-06
Iter: 557 loss: 3.30227203e-06
Iter: 558 loss: 3.29439899e-06
Iter: 559 loss: 3.31232332e-06
Iter: 560 loss: 3.29126374e-06
Iter: 561 loss: 3.28498572e-06
Iter: 562 loss: 3.32438754e-06
Iter: 563 loss: 3.28410556e-06
Iter: 564 loss: 3.27812745e-06
Iter: 565 loss: 3.2756011e-06
Iter: 566 loss: 3.27239491e-06
Iter: 567 loss: 3.26303757e-06
Iter: 568 loss: 3.26941677e-06
Iter: 569 loss: 3.25715087e-06
Iter: 570 loss: 3.24918437e-06
Iter: 571 loss: 3.35461323e-06
Iter: 572 loss: 3.24909342e-06
Iter: 573 loss: 3.24165467e-06
Iter: 574 loss: 3.24427583e-06
Iter: 575 loss: 3.2364203e-06
Iter: 576 loss: 3.226844e-06
Iter: 577 loss: 3.23680365e-06
Iter: 578 loss: 3.22161304e-06
Iter: 579 loss: 3.21320158e-06
Iter: 580 loss: 3.24671328e-06
Iter: 581 loss: 3.21134371e-06
Iter: 582 loss: 3.20549452e-06
Iter: 583 loss: 3.20544791e-06
Iter: 584 loss: 3.20027948e-06
Iter: 585 loss: 3.19485594e-06
Iter: 586 loss: 3.19384071e-06
Iter: 587 loss: 3.18646744e-06
Iter: 588 loss: 3.24676421e-06
Iter: 589 loss: 3.1860352e-06
Iter: 590 loss: 3.18039838e-06
Iter: 591 loss: 3.17937156e-06
Iter: 592 loss: 3.17556146e-06
Iter: 593 loss: 3.16794512e-06
Iter: 594 loss: 3.16474348e-06
Iter: 595 loss: 3.16075966e-06
Iter: 596 loss: 3.15091779e-06
Iter: 597 loss: 3.25827182e-06
Iter: 598 loss: 3.15081479e-06
Iter: 599 loss: 3.14456543e-06
Iter: 600 loss: 3.1668892e-06
Iter: 601 loss: 3.14306862e-06
Iter: 602 loss: 3.13560167e-06
Iter: 603 loss: 3.14322028e-06
Iter: 604 loss: 3.13158694e-06
Iter: 605 loss: 3.12478983e-06
Iter: 606 loss: 3.1608804e-06
Iter: 607 loss: 3.12363659e-06
Iter: 608 loss: 3.11735471e-06
Iter: 609 loss: 3.14459953e-06
Iter: 610 loss: 3.11608028e-06
Iter: 611 loss: 3.11046279e-06
Iter: 612 loss: 3.10090809e-06
Iter: 613 loss: 3.10085579e-06
Iter: 614 loss: 3.09282086e-06
Iter: 615 loss: 3.20676259e-06
Iter: 616 loss: 3.09283405e-06
Iter: 617 loss: 3.08533299e-06
Iter: 618 loss: 3.09894563e-06
Iter: 619 loss: 3.08206745e-06
Iter: 620 loss: 3.07522077e-06
Iter: 621 loss: 3.08497647e-06
Iter: 622 loss: 3.07195705e-06
Iter: 623 loss: 3.06550487e-06
Iter: 624 loss: 3.06544871e-06
Iter: 625 loss: 3.06159814e-06
Iter: 626 loss: 3.05281083e-06
Iter: 627 loss: 3.17189779e-06
Iter: 628 loss: 3.05230878e-06
Iter: 629 loss: 3.04313153e-06
Iter: 630 loss: 3.08435619e-06
Iter: 631 loss: 3.04130344e-06
Iter: 632 loss: 3.0333988e-06
Iter: 633 loss: 3.09231518e-06
Iter: 634 loss: 3.03275283e-06
Iter: 635 loss: 3.02595708e-06
Iter: 636 loss: 3.03869047e-06
Iter: 637 loss: 3.02300941e-06
Iter: 638 loss: 3.01673117e-06
Iter: 639 loss: 3.01490741e-06
Iter: 640 loss: 3.01111595e-06
Iter: 641 loss: 3.00343982e-06
Iter: 642 loss: 3.0608262e-06
Iter: 643 loss: 3.0029e-06
Iter: 644 loss: 2.99592057e-06
Iter: 645 loss: 3.0189949e-06
Iter: 646 loss: 2.99397925e-06
Iter: 647 loss: 2.98759051e-06
Iter: 648 loss: 3.00059e-06
Iter: 649 loss: 2.98499845e-06
Iter: 650 loss: 2.97885936e-06
Iter: 651 loss: 3.00122429e-06
Iter: 652 loss: 2.97738052e-06
Iter: 653 loss: 2.97080328e-06
Iter: 654 loss: 2.99903286e-06
Iter: 655 loss: 2.96954704e-06
Iter: 656 loss: 2.96407279e-06
Iter: 657 loss: 2.95695463e-06
Iter: 658 loss: 2.95660311e-06
Iter: 659 loss: 2.95289169e-06
Iter: 660 loss: 2.95174686e-06
Iter: 661 loss: 2.94735742e-06
Iter: 662 loss: 2.94473739e-06
Iter: 663 loss: 2.942935e-06
Iter: 664 loss: 2.93533458e-06
Iter: 665 loss: 2.96299686e-06
Iter: 666 loss: 2.93340827e-06
Iter: 667 loss: 2.92858385e-06
Iter: 668 loss: 2.93771836e-06
Iter: 669 loss: 2.92649861e-06
Iter: 670 loss: 2.9211194e-06
Iter: 671 loss: 2.91145398e-06
Iter: 672 loss: 3.14405725e-06
Iter: 673 loss: 2.91142351e-06
Iter: 674 loss: 2.90422327e-06
Iter: 675 loss: 2.90418802e-06
Iter: 676 loss: 2.89763466e-06
Iter: 677 loss: 2.91810534e-06
Iter: 678 loss: 2.8958284e-06
Iter: 679 loss: 2.88873434e-06
Iter: 680 loss: 2.89645891e-06
Iter: 681 loss: 2.88494334e-06
Iter: 682 loss: 2.87849025e-06
Iter: 683 loss: 2.88881188e-06
Iter: 684 loss: 2.875463e-06
Iter: 685 loss: 2.86850445e-06
Iter: 686 loss: 2.90672779e-06
Iter: 687 loss: 2.86753084e-06
Iter: 688 loss: 2.86129534e-06
Iter: 689 loss: 2.88290721e-06
Iter: 690 loss: 2.85963051e-06
Iter: 691 loss: 2.85377337e-06
Iter: 692 loss: 2.8502684e-06
Iter: 693 loss: 2.84781709e-06
Iter: 694 loss: 2.84258158e-06
Iter: 695 loss: 2.84241014e-06
Iter: 696 loss: 2.83729923e-06
Iter: 697 loss: 2.83729787e-06
Iter: 698 loss: 2.83322674e-06
Iter: 699 loss: 2.82874862e-06
Iter: 700 loss: 2.89557875e-06
Iter: 701 loss: 2.82874771e-06
Iter: 702 loss: 2.82481869e-06
Iter: 703 loss: 2.81940584e-06
Iter: 704 loss: 2.81913049e-06
Iter: 705 loss: 2.81333928e-06
Iter: 706 loss: 2.85143847e-06
Iter: 707 loss: 2.81260873e-06
Iter: 708 loss: 2.8068871e-06
Iter: 709 loss: 2.80812083e-06
Iter: 710 loss: 2.80253789e-06
Iter: 711 loss: 2.79626875e-06
Iter: 712 loss: 2.80278209e-06
Iter: 713 loss: 2.79284177e-06
Iter: 714 loss: 2.78532457e-06
Iter: 715 loss: 2.81045141e-06
Iter: 716 loss: 2.78321659e-06
Iter: 717 loss: 2.77689787e-06
Iter: 718 loss: 2.84505404e-06
Iter: 719 loss: 2.77678555e-06
Iter: 720 loss: 2.7715887e-06
Iter: 721 loss: 2.77211166e-06
Iter: 722 loss: 2.76753326e-06
Iter: 723 loss: 2.76126366e-06
Iter: 724 loss: 2.77477284e-06
Iter: 725 loss: 2.75893422e-06
Iter: 726 loss: 2.75253501e-06
Iter: 727 loss: 2.76728497e-06
Iter: 728 loss: 2.75019647e-06
Iter: 729 loss: 2.74425474e-06
Iter: 730 loss: 2.80204449e-06
Iter: 731 loss: 2.74398326e-06
Iter: 732 loss: 2.73975138e-06
Iter: 733 loss: 2.73635101e-06
Iter: 734 loss: 2.73508294e-06
Iter: 735 loss: 2.72957232e-06
Iter: 736 loss: 2.79710866e-06
Iter: 737 loss: 2.72953957e-06
Iter: 738 loss: 2.72393368e-06
Iter: 739 loss: 2.73709338e-06
Iter: 740 loss: 2.72198054e-06
Iter: 741 loss: 2.71803856e-06
Iter: 742 loss: 2.71886688e-06
Iter: 743 loss: 2.71507201e-06
Iter: 744 loss: 2.70948749e-06
Iter: 745 loss: 2.74055355e-06
Iter: 746 loss: 2.70877581e-06
Iter: 747 loss: 2.70439205e-06
Iter: 748 loss: 2.7038468e-06
Iter: 749 loss: 2.70070268e-06
Iter: 750 loss: 2.6950554e-06
Iter: 751 loss: 2.74336026e-06
Iter: 752 loss: 2.69474867e-06
Iter: 753 loss: 2.68999747e-06
Iter: 754 loss: 2.68594681e-06
Iter: 755 loss: 2.68467033e-06
Iter: 756 loss: 2.67802716e-06
Iter: 757 loss: 2.69459088e-06
Iter: 758 loss: 2.67582254e-06
Iter: 759 loss: 2.66909e-06
Iter: 760 loss: 2.68251142e-06
Iter: 761 loss: 2.66627876e-06
Iter: 762 loss: 2.66077154e-06
Iter: 763 loss: 2.73347632e-06
Iter: 764 loss: 2.66070356e-06
Iter: 765 loss: 2.65612516e-06
Iter: 766 loss: 2.66321308e-06
Iter: 767 loss: 2.65390645e-06
Iter: 768 loss: 2.64828e-06
Iter: 769 loss: 2.64810296e-06
Iter: 770 loss: 2.64386426e-06
Iter: 771 loss: 2.63823699e-06
Iter: 772 loss: 2.68682084e-06
Iter: 773 loss: 2.63793299e-06
Iter: 774 loss: 2.63248194e-06
Iter: 775 loss: 2.64880373e-06
Iter: 776 loss: 2.63082825e-06
Iter: 777 loss: 2.62602293e-06
Iter: 778 loss: 2.65946233e-06
Iter: 779 loss: 2.62561753e-06
Iter: 780 loss: 2.62201365e-06
Iter: 781 loss: 2.61874811e-06
Iter: 782 loss: 2.6178277e-06
Iter: 783 loss: 2.61216474e-06
Iter: 784 loss: 2.61873311e-06
Iter: 785 loss: 2.60919614e-06
Iter: 786 loss: 2.60350134e-06
Iter: 787 loss: 2.62754475e-06
Iter: 788 loss: 2.60242336e-06
Iter: 789 loss: 2.59644912e-06
Iter: 790 loss: 2.62736739e-06
Iter: 791 loss: 2.59548483e-06
Iter: 792 loss: 2.59086846e-06
Iter: 793 loss: 2.59320632e-06
Iter: 794 loss: 2.58777345e-06
Iter: 795 loss: 2.58307409e-06
Iter: 796 loss: 2.63414358e-06
Iter: 797 loss: 2.58298132e-06
Iter: 798 loss: 2.57866e-06
Iter: 799 loss: 2.57354077e-06
Iter: 800 loss: 2.57302781e-06
Iter: 801 loss: 2.56689646e-06
Iter: 802 loss: 2.58274576e-06
Iter: 803 loss: 2.56468729e-06
Iter: 804 loss: 2.5583447e-06
Iter: 805 loss: 2.56541171e-06
Iter: 806 loss: 2.5549366e-06
Iter: 807 loss: 2.54965926e-06
Iter: 808 loss: 2.54967131e-06
Iter: 809 loss: 2.54618476e-06
Iter: 810 loss: 2.5686677e-06
Iter: 811 loss: 2.5456925e-06
Iter: 812 loss: 2.54224051e-06
Iter: 813 loss: 2.54291785e-06
Iter: 814 loss: 2.53956318e-06
Iter: 815 loss: 2.5351419e-06
Iter: 816 loss: 2.56006524e-06
Iter: 817 loss: 2.53461121e-06
Iter: 818 loss: 2.53116218e-06
Iter: 819 loss: 2.52535665e-06
Iter: 820 loss: 2.52538507e-06
Iter: 821 loss: 2.52085783e-06
Iter: 822 loss: 2.52082395e-06
Iter: 823 loss: 2.51704705e-06
Iter: 824 loss: 2.51223014e-06
Iter: 825 loss: 2.51191068e-06
Iter: 826 loss: 2.50561197e-06
Iter: 827 loss: 2.52053201e-06
Iter: 828 loss: 2.50329344e-06
Iter: 829 loss: 2.49959817e-06
Iter: 830 loss: 2.49944446e-06
Iter: 831 loss: 2.49591949e-06
Iter: 832 loss: 2.49246523e-06
Iter: 833 loss: 2.49164214e-06
Iter: 834 loss: 2.48550464e-06
Iter: 835 loss: 2.49923e-06
Iter: 836 loss: 2.48310516e-06
Iter: 837 loss: 2.47888693e-06
Iter: 838 loss: 2.54129782e-06
Iter: 839 loss: 2.47890466e-06
Iter: 840 loss: 2.4755991e-06
Iter: 841 loss: 2.46959189e-06
Iter: 842 loss: 2.61233481e-06
Iter: 843 loss: 2.46960053e-06
Iter: 844 loss: 2.46427635e-06
Iter: 845 loss: 2.51064239e-06
Iter: 846 loss: 2.46400941e-06
Iter: 847 loss: 2.45998831e-06
Iter: 848 loss: 2.513053e-06
Iter: 849 loss: 2.4599658e-06
Iter: 850 loss: 2.45657657e-06
Iter: 851 loss: 2.45128012e-06
Iter: 852 loss: 2.45126648e-06
Iter: 853 loss: 2.4459473e-06
Iter: 854 loss: 2.46140621e-06
Iter: 855 loss: 2.44437115e-06
Iter: 856 loss: 2.4394e-06
Iter: 857 loss: 2.47585695e-06
Iter: 858 loss: 2.43894328e-06
Iter: 859 loss: 2.43479e-06
Iter: 860 loss: 2.44164403e-06
Iter: 861 loss: 2.43298291e-06
Iter: 862 loss: 2.42884948e-06
Iter: 863 loss: 2.42705187e-06
Iter: 864 loss: 2.42495889e-06
Iter: 865 loss: 2.42009173e-06
Iter: 866 loss: 2.49376308e-06
Iter: 867 loss: 2.4201172e-06
Iter: 868 loss: 2.41635371e-06
Iter: 869 loss: 2.41692965e-06
Iter: 870 loss: 2.41346788e-06
Iter: 871 loss: 2.40935924e-06
Iter: 872 loss: 2.43565319e-06
Iter: 873 loss: 2.40881309e-06
Iter: 874 loss: 2.40465488e-06
Iter: 875 loss: 2.41374937e-06
Iter: 876 loss: 2.40300187e-06
Iter: 877 loss: 2.39904284e-06
Iter: 878 loss: 2.3973098e-06
Iter: 879 loss: 2.39532233e-06
Iter: 880 loss: 2.38963889e-06
Iter: 881 loss: 2.4017836e-06
Iter: 882 loss: 2.38736334e-06
Iter: 883 loss: 2.38449479e-06
Iter: 884 loss: 2.38398479e-06
Iter: 885 loss: 2.38094822e-06
Iter: 886 loss: 2.38037228e-06
Iter: 887 loss: 2.37830227e-06
Iter: 888 loss: 2.3744592e-06
Iter: 889 loss: 2.38648681e-06
Iter: 890 loss: 2.37329141e-06
Iter: 891 loss: 2.3693658e-06
Iter: 892 loss: 2.37459903e-06
Iter: 893 loss: 2.36744245e-06
Iter: 894 loss: 2.36351661e-06
Iter: 895 loss: 2.3600669e-06
Iter: 896 loss: 2.35904054e-06
Iter: 897 loss: 2.35361404e-06
Iter: 898 loss: 2.40942768e-06
Iter: 899 loss: 2.35346579e-06
Iter: 900 loss: 2.34882418e-06
Iter: 901 loss: 2.3762268e-06
Iter: 902 loss: 2.34821528e-06
Iter: 903 loss: 2.34513072e-06
Iter: 904 loss: 2.34444633e-06
Iter: 905 loss: 2.34245454e-06
Iter: 906 loss: 2.33777405e-06
Iter: 907 loss: 2.34753e-06
Iter: 908 loss: 2.33594392e-06
Iter: 909 loss: 2.33133233e-06
Iter: 910 loss: 2.37887639e-06
Iter: 911 loss: 2.33120545e-06
Iter: 912 loss: 2.32786806e-06
Iter: 913 loss: 2.33104174e-06
Iter: 914 loss: 2.32606135e-06
Iter: 915 loss: 2.32216917e-06
Iter: 916 loss: 2.3219377e-06
Iter: 917 loss: 2.3190546e-06
Iter: 918 loss: 2.31368767e-06
Iter: 919 loss: 2.36470351e-06
Iter: 920 loss: 2.31354579e-06
Iter: 921 loss: 2.31031436e-06
Iter: 922 loss: 2.32304478e-06
Iter: 923 loss: 2.30967385e-06
Iter: 924 loss: 2.30580827e-06
Iter: 925 loss: 2.3091568e-06
Iter: 926 loss: 2.30351066e-06
Iter: 927 loss: 2.29997431e-06
Iter: 928 loss: 2.29588159e-06
Iter: 929 loss: 2.29534044e-06
Iter: 930 loss: 2.28948124e-06
Iter: 931 loss: 2.30548085e-06
Iter: 932 loss: 2.28755744e-06
Iter: 933 loss: 2.28252065e-06
Iter: 934 loss: 2.32294474e-06
Iter: 935 loss: 2.282271e-06
Iter: 936 loss: 2.27847e-06
Iter: 937 loss: 2.29760076e-06
Iter: 938 loss: 2.27788405e-06
Iter: 939 loss: 2.27362079e-06
Iter: 940 loss: 2.27639293e-06
Iter: 941 loss: 2.27093119e-06
Iter: 942 loss: 2.266705e-06
Iter: 943 loss: 2.27050987e-06
Iter: 944 loss: 2.26424481e-06
Iter: 945 loss: 2.25946178e-06
Iter: 946 loss: 2.30222531e-06
Iter: 947 loss: 2.25925442e-06
Iter: 948 loss: 2.25539316e-06
Iter: 949 loss: 2.26921065e-06
Iter: 950 loss: 2.25451663e-06
Iter: 951 loss: 2.25125405e-06
Iter: 952 loss: 2.2481986e-06
Iter: 953 loss: 2.2473846e-06
Iter: 954 loss: 2.24417113e-06
Iter: 955 loss: 2.24407904e-06
Iter: 956 loss: 2.24147107e-06
Iter: 957 loss: 2.24316955e-06
Iter: 958 loss: 2.23982079e-06
Iter: 959 loss: 2.23596476e-06
Iter: 960 loss: 2.25080021e-06
Iter: 961 loss: 2.2349775e-06
Iter: 962 loss: 2.23197321e-06
Iter: 963 loss: 2.22797735e-06
Iter: 964 loss: 2.22772769e-06
Iter: 965 loss: 2.22308677e-06
Iter: 966 loss: 2.24819223e-06
Iter: 967 loss: 2.22237441e-06
Iter: 968 loss: 2.21713026e-06
Iter: 969 loss: 2.23085408e-06
Iter: 970 loss: 2.2153381e-06
Iter: 971 loss: 2.21153527e-06
Iter: 972 loss: 2.21343157e-06
Iter: 973 loss: 2.20892935e-06
Iter: 974 loss: 2.20409379e-06
Iter: 975 loss: 2.21216669e-06
Iter: 976 loss: 2.20193101e-06
Iter: 977 loss: 2.19752428e-06
Iter: 978 loss: 2.22790959e-06
Iter: 979 loss: 2.197095e-06
Iter: 980 loss: 2.19310368e-06
Iter: 981 loss: 2.21560413e-06
Iter: 982 loss: 2.19247295e-06
Iter: 983 loss: 2.18919058e-06
Iter: 984 loss: 2.19645244e-06
Iter: 985 loss: 2.18801165e-06
Iter: 986 loss: 2.18451851e-06
Iter: 987 loss: 2.18104447e-06
Iter: 988 loss: 2.18036939e-06
Iter: 989 loss: 2.1770411e-06
Iter: 990 loss: 2.17672232e-06
Iter: 991 loss: 2.17391653e-06
Iter: 992 loss: 2.17601064e-06
Iter: 993 loss: 2.17224988e-06
Iter: 994 loss: 2.16892568e-06
Iter: 995 loss: 2.18515765e-06
Iter: 996 loss: 2.16828948e-06
Iter: 997 loss: 2.16537228e-06
Iter: 998 loss: 2.1640094e-06
Iter: 999 loss: 2.16249805e-06
Iter: 1000 loss: 2.15879345e-06
Iter: 1001 loss: 2.18968671e-06
Iter: 1002 loss: 2.15860746e-06
Iter: 1003 loss: 2.15585806e-06
Iter: 1004 loss: 2.15599221e-06
Iter: 1005 loss: 2.15362297e-06
Iter: 1006 loss: 2.14979173e-06
Iter: 1007 loss: 2.15179671e-06
Iter: 1008 loss: 2.14726219e-06
Iter: 1009 loss: 2.14363217e-06
Iter: 1010 loss: 2.19035519e-06
Iter: 1011 loss: 2.14356032e-06
Iter: 1012 loss: 2.14033594e-06
Iter: 1013 loss: 2.13971907e-06
Iter: 1014 loss: 2.13745125e-06
Iter: 1015 loss: 2.13297426e-06
Iter: 1016 loss: 2.13791577e-06
Iter: 1017 loss: 2.13049157e-06
Iter: 1018 loss: 2.12574014e-06
Iter: 1019 loss: 2.13851899e-06
Iter: 1020 loss: 2.12413329e-06
Iter: 1021 loss: 2.11961128e-06
Iter: 1022 loss: 2.12940472e-06
Iter: 1023 loss: 2.11787233e-06
Iter: 1024 loss: 2.11432098e-06
Iter: 1025 loss: 2.11417955e-06
Iter: 1026 loss: 2.11138354e-06
Iter: 1027 loss: 2.11378665e-06
Iter: 1028 loss: 2.10964868e-06
Iter: 1029 loss: 2.10689745e-06
Iter: 1030 loss: 2.13381372e-06
Iter: 1031 loss: 2.10685607e-06
Iter: 1032 loss: 2.10484495e-06
Iter: 1033 loss: 2.10115786e-06
Iter: 1034 loss: 2.18745186e-06
Iter: 1035 loss: 2.101154e-06
Iter: 1036 loss: 2.09720338e-06
Iter: 1037 loss: 2.13134263e-06
Iter: 1038 loss: 2.09706241e-06
Iter: 1039 loss: 2.09391942e-06
Iter: 1040 loss: 2.10189478e-06
Iter: 1041 loss: 2.09285e-06
Iter: 1042 loss: 2.08964639e-06
Iter: 1043 loss: 2.08856522e-06
Iter: 1044 loss: 2.0866953e-06
Iter: 1045 loss: 2.0824923e-06
Iter: 1046 loss: 2.11798692e-06
Iter: 1047 loss: 2.08221581e-06
Iter: 1048 loss: 2.07880976e-06
Iter: 1049 loss: 2.08157167e-06
Iter: 1050 loss: 2.07670496e-06
Iter: 1051 loss: 2.07263656e-06
Iter: 1052 loss: 2.07465064e-06
Iter: 1053 loss: 2.06979871e-06
Iter: 1054 loss: 2.06630943e-06
Iter: 1055 loss: 2.10949293e-06
Iter: 1056 loss: 2.06624895e-06
Iter: 1057 loss: 2.06293271e-06
Iter: 1058 loss: 2.06475397e-06
Iter: 1059 loss: 2.06072355e-06
Iter: 1060 loss: 2.05643619e-06
Iter: 1061 loss: 2.06332925e-06
Iter: 1062 loss: 2.05440392e-06
Iter: 1063 loss: 2.05060678e-06
Iter: 1064 loss: 2.05328161e-06
Iter: 1065 loss: 2.04810317e-06
Iter: 1066 loss: 2.04615594e-06
Iter: 1067 loss: 2.04532034e-06
Iter: 1068 loss: 2.043015e-06
Iter: 1069 loss: 2.03937111e-06
Iter: 1070 loss: 2.03929267e-06
Iter: 1071 loss: 2.03578202e-06
Iter: 1072 loss: 2.05080642e-06
Iter: 1073 loss: 2.03501713e-06
Iter: 1074 loss: 2.0313505e-06
Iter: 1075 loss: 2.04284788e-06
Iter: 1076 loss: 2.03031027e-06
Iter: 1077 loss: 2.02710726e-06
Iter: 1078 loss: 2.0352395e-06
Iter: 1079 loss: 2.02616457e-06
Iter: 1080 loss: 2.02330966e-06
Iter: 1081 loss: 2.02429101e-06
Iter: 1082 loss: 2.02139699e-06
Iter: 1083 loss: 2.01730199e-06
Iter: 1084 loss: 2.05147012e-06
Iter: 1085 loss: 2.01714693e-06
Iter: 1086 loss: 2.01490616e-06
Iter: 1087 loss: 2.01624243e-06
Iter: 1088 loss: 2.01352623e-06
Iter: 1089 loss: 2.01020521e-06
Iter: 1090 loss: 2.0220923e-06
Iter: 1091 loss: 2.00938757e-06
Iter: 1092 loss: 2.00599698e-06
Iter: 1093 loss: 2.00428713e-06
Iter: 1094 loss: 2.00278646e-06
Iter: 1095 loss: 1.99886927e-06
Iter: 1096 loss: 2.01335411e-06
Iter: 1097 loss: 1.99795522e-06
Iter: 1098 loss: 1.99446936e-06
Iter: 1099 loss: 2.03154423e-06
Iter: 1100 loss: 1.99438409e-06
Iter: 1101 loss: 1.99185183e-06
Iter: 1102 loss: 1.98808e-06
Iter: 1103 loss: 1.98795055e-06
Iter: 1104 loss: 1.98607677e-06
Iter: 1105 loss: 1.98564976e-06
Iter: 1106 loss: 1.98348653e-06
Iter: 1107 loss: 1.98518592e-06
Iter: 1108 loss: 1.98220368e-06
Iter: 1109 loss: 1.97971394e-06
Iter: 1110 loss: 1.97659961e-06
Iter: 1111 loss: 1.97632471e-06
Iter: 1112 loss: 1.97221084e-06
Iter: 1113 loss: 1.98537828e-06
Iter: 1114 loss: 1.97105373e-06
Iter: 1115 loss: 1.96763381e-06
Iter: 1116 loss: 2.00871614e-06
Iter: 1117 loss: 1.96762721e-06
Iter: 1118 loss: 1.96475708e-06
Iter: 1119 loss: 1.96593828e-06
Iter: 1120 loss: 1.96287169e-06
Iter: 1121 loss: 1.95936764e-06
Iter: 1122 loss: 1.9664426e-06
Iter: 1123 loss: 1.95795315e-06
Iter: 1124 loss: 1.95492521e-06
Iter: 1125 loss: 1.97454619e-06
Iter: 1126 loss: 1.95461439e-06
Iter: 1127 loss: 1.95144639e-06
Iter: 1128 loss: 1.95075268e-06
Iter: 1129 loss: 1.94876202e-06
Iter: 1130 loss: 1.94487211e-06
Iter: 1131 loss: 1.95643497e-06
Iter: 1132 loss: 1.94375662e-06
Iter: 1133 loss: 1.94050745e-06
Iter: 1134 loss: 1.98045382e-06
Iter: 1135 loss: 1.940527e-06
Iter: 1136 loss: 1.93827691e-06
Iter: 1137 loss: 1.9384081e-06
Iter: 1138 loss: 1.93644973e-06
Iter: 1139 loss: 1.93338838e-06
Iter: 1140 loss: 1.94017184e-06
Iter: 1141 loss: 1.93220239e-06
Iter: 1142 loss: 1.92871653e-06
Iter: 1143 loss: 1.95142684e-06
Iter: 1144 loss: 1.92834477e-06
Iter: 1145 loss: 1.92636048e-06
Iter: 1146 loss: 1.93986898e-06
Iter: 1147 loss: 1.92621633e-06
Iter: 1148 loss: 1.92412881e-06
Iter: 1149 loss: 1.9205695e-06
Iter: 1150 loss: 1.92054586e-06
Iter: 1151 loss: 1.91694858e-06
Iter: 1152 loss: 1.92499579e-06
Iter: 1153 loss: 1.9155641e-06
Iter: 1154 loss: 1.91192794e-06
Iter: 1155 loss: 1.91571098e-06
Iter: 1156 loss: 1.90986202e-06
Iter: 1157 loss: 1.90596779e-06
Iter: 1158 loss: 1.94155564e-06
Iter: 1159 loss: 1.90578453e-06
Iter: 1160 loss: 1.90322737e-06
Iter: 1161 loss: 1.9241761e-06
Iter: 1162 loss: 1.90310311e-06
Iter: 1163 loss: 1.90077321e-06
Iter: 1164 loss: 1.89703121e-06
Iter: 1165 loss: 1.8970087e-06
Iter: 1166 loss: 1.89291745e-06
Iter: 1167 loss: 1.9374379e-06
Iter: 1168 loss: 1.89282548e-06
Iter: 1169 loss: 1.88988815e-06
Iter: 1170 loss: 1.89779632e-06
Iter: 1171 loss: 1.88891886e-06
Iter: 1172 loss: 1.88583363e-06
Iter: 1173 loss: 1.88631861e-06
Iter: 1174 loss: 1.88355261e-06
Iter: 1175 loss: 1.87997705e-06
Iter: 1176 loss: 1.89643742e-06
Iter: 1177 loss: 1.8793412e-06
Iter: 1178 loss: 1.87629098e-06
Iter: 1179 loss: 1.90597063e-06
Iter: 1180 loss: 1.87617979e-06
Iter: 1181 loss: 1.87402418e-06
Iter: 1182 loss: 1.87933517e-06
Iter: 1183 loss: 1.87314561e-06
Iter: 1184 loss: 1.87078808e-06
Iter: 1185 loss: 1.87610226e-06
Iter: 1186 loss: 1.86985847e-06
Iter: 1187 loss: 1.86772218e-06
Iter: 1188 loss: 1.86455645e-06
Iter: 1189 loss: 1.86448119e-06
Iter: 1190 loss: 1.86119644e-06
Iter: 1191 loss: 1.86112788e-06
Iter: 1192 loss: 1.85948011e-06
Iter: 1193 loss: 1.85532804e-06
Iter: 1194 loss: 1.89393768e-06
Iter: 1195 loss: 1.85469651e-06
Iter: 1196 loss: 1.85089277e-06
Iter: 1197 loss: 1.90483036e-06
Iter: 1198 loss: 1.85089766e-06
Iter: 1199 loss: 1.84805128e-06
Iter: 1200 loss: 1.85689532e-06
Iter: 1201 loss: 1.84718783e-06
Iter: 1202 loss: 1.84396254e-06
Iter: 1203 loss: 1.85475642e-06
Iter: 1204 loss: 1.84313978e-06
Iter: 1205 loss: 1.84026896e-06
Iter: 1206 loss: 1.84103897e-06
Iter: 1207 loss: 1.83812972e-06
Iter: 1208 loss: 1.83494603e-06
Iter: 1209 loss: 1.85591409e-06
Iter: 1210 loss: 1.83456314e-06
Iter: 1211 loss: 1.83147256e-06
Iter: 1212 loss: 1.83525469e-06
Iter: 1213 loss: 1.82983683e-06
Iter: 1214 loss: 1.82694271e-06
Iter: 1215 loss: 1.83048144e-06
Iter: 1216 loss: 1.82531164e-06
Iter: 1217 loss: 1.8229739e-06
Iter: 1218 loss: 1.82293627e-06
Iter: 1219 loss: 1.82095471e-06
Iter: 1220 loss: 1.82125927e-06
Iter: 1221 loss: 1.81945086e-06
Iter: 1222 loss: 1.81691462e-06
Iter: 1223 loss: 1.82773636e-06
Iter: 1224 loss: 1.81635414e-06
Iter: 1225 loss: 1.81402856e-06
Iter: 1226 loss: 1.81188364e-06
Iter: 1227 loss: 1.81135658e-06
Iter: 1228 loss: 1.80873758e-06
Iter: 1229 loss: 1.80876395e-06
Iter: 1230 loss: 1.80657685e-06
Iter: 1231 loss: 1.80362781e-06
Iter: 1232 loss: 1.80344432e-06
Iter: 1233 loss: 1.79973006e-06
Iter: 1234 loss: 1.80648738e-06
Iter: 1235 loss: 1.79810013e-06
Iter: 1236 loss: 1.79442861e-06
Iter: 1237 loss: 1.80264419e-06
Iter: 1238 loss: 1.79293829e-06
Iter: 1239 loss: 1.7894356e-06
Iter: 1240 loss: 1.83017505e-06
Iter: 1241 loss: 1.78935841e-06
Iter: 1242 loss: 1.78660684e-06
Iter: 1243 loss: 1.7945523e-06
Iter: 1244 loss: 1.78574669e-06
Iter: 1245 loss: 1.78342543e-06
Iter: 1246 loss: 1.78195774e-06
Iter: 1247 loss: 1.78099447e-06
Iter: 1248 loss: 1.77753213e-06
Iter: 1249 loss: 1.80930385e-06
Iter: 1250 loss: 1.77742049e-06
Iter: 1251 loss: 1.77493723e-06
Iter: 1252 loss: 1.78134326e-06
Iter: 1253 loss: 1.77408515e-06
Iter: 1254 loss: 1.77146762e-06
Iter: 1255 loss: 1.77306788e-06
Iter: 1256 loss: 1.76991648e-06
Iter: 1257 loss: 1.76769345e-06
Iter: 1258 loss: 1.76761716e-06
Iter: 1259 loss: 1.76619039e-06
Iter: 1260 loss: 1.76380843e-06
Iter: 1261 loss: 1.82338545e-06
Iter: 1262 loss: 1.76381297e-06
Iter: 1263 loss: 1.76086576e-06
Iter: 1264 loss: 1.77148752e-06
Iter: 1265 loss: 1.7601775e-06
Iter: 1266 loss: 1.75746607e-06
Iter: 1267 loss: 1.77183927e-06
Iter: 1268 loss: 1.75703235e-06
Iter: 1269 loss: 1.75497757e-06
Iter: 1270 loss: 1.75213052e-06
Iter: 1271 loss: 1.75209061e-06
Iter: 1272 loss: 1.74929494e-06
Iter: 1273 loss: 1.7492697e-06
Iter: 1274 loss: 1.74657907e-06
Iter: 1275 loss: 1.7499766e-06
Iter: 1276 loss: 1.74521415e-06
Iter: 1277 loss: 1.74286265e-06
Iter: 1278 loss: 1.74023444e-06
Iter: 1279 loss: 1.73983096e-06
Iter: 1280 loss: 1.7361142e-06
Iter: 1281 loss: 1.76719277e-06
Iter: 1282 loss: 1.73586818e-06
Iter: 1283 loss: 1.73318836e-06
Iter: 1284 loss: 1.75417676e-06
Iter: 1285 loss: 1.73304113e-06
Iter: 1286 loss: 1.73071385e-06
Iter: 1287 loss: 1.73142882e-06
Iter: 1288 loss: 1.72910006e-06
Iter: 1289 loss: 1.72611544e-06
Iter: 1290 loss: 1.73172475e-06
Iter: 1291 loss: 1.72490127e-06
Iter: 1292 loss: 1.72228113e-06
Iter: 1293 loss: 1.74602201e-06
Iter: 1294 loss: 1.72220871e-06
Iter: 1295 loss: 1.72022919e-06
Iter: 1296 loss: 1.72367515e-06
Iter: 1297 loss: 1.71928252e-06
Iter: 1298 loss: 1.71739987e-06
Iter: 1299 loss: 1.73606236e-06
Iter: 1300 loss: 1.71733313e-06
Iter: 1301 loss: 1.71553359e-06
Iter: 1302 loss: 1.71178169e-06
Iter: 1303 loss: 1.77689128e-06
Iter: 1304 loss: 1.71166312e-06
Iter: 1305 loss: 1.70856163e-06
Iter: 1306 loss: 1.73016485e-06
Iter: 1307 loss: 1.70825842e-06
Iter: 1308 loss: 1.70533576e-06
Iter: 1309 loss: 1.71526767e-06
Iter: 1310 loss: 1.70456042e-06
Iter: 1311 loss: 1.7017303e-06
Iter: 1312 loss: 1.70430121e-06
Iter: 1313 loss: 1.70003682e-06
Iter: 1314 loss: 1.69729208e-06
Iter: 1315 loss: 1.70430735e-06
Iter: 1316 loss: 1.6963038e-06
Iter: 1317 loss: 1.69418558e-06
Iter: 1318 loss: 1.69419388e-06
Iter: 1319 loss: 1.69235159e-06
Iter: 1320 loss: 1.69028158e-06
Iter: 1321 loss: 1.69005557e-06
Iter: 1322 loss: 1.68695522e-06
Iter: 1323 loss: 1.69085115e-06
Iter: 1324 loss: 1.68542874e-06
Iter: 1325 loss: 1.6828767e-06
Iter: 1326 loss: 1.70430656e-06
Iter: 1327 loss: 1.6827953e-06
Iter: 1328 loss: 1.68011195e-06
Iter: 1329 loss: 1.68503e-06
Iter: 1330 loss: 1.67890062e-06
Iter: 1331 loss: 1.67618498e-06
Iter: 1332 loss: 1.68021916e-06
Iter: 1333 loss: 1.67488247e-06
Iter: 1334 loss: 1.67265136e-06
Iter: 1335 loss: 1.69590317e-06
Iter: 1336 loss: 1.67254336e-06
Iter: 1337 loss: 1.67043174e-06
Iter: 1338 loss: 1.67222083e-06
Iter: 1339 loss: 1.66915038e-06
Iter: 1340 loss: 1.66627319e-06
Iter: 1341 loss: 1.67671556e-06
Iter: 1342 loss: 1.6655805e-06
Iter: 1343 loss: 1.6633569e-06
Iter: 1344 loss: 1.66329517e-06
Iter: 1345 loss: 1.66154916e-06
Iter: 1346 loss: 1.6587627e-06
Iter: 1347 loss: 1.66451844e-06
Iter: 1348 loss: 1.65768e-06
Iter: 1349 loss: 1.6553696e-06
Iter: 1350 loss: 1.67518601e-06
Iter: 1351 loss: 1.65525148e-06
Iter: 1352 loss: 1.65310439e-06
Iter: 1353 loss: 1.65312622e-06
Iter: 1354 loss: 1.6513477e-06
Iter: 1355 loss: 1.64856408e-06
Iter: 1356 loss: 1.6512148e-06
Iter: 1357 loss: 1.64694825e-06
Iter: 1358 loss: 1.64341543e-06
Iter: 1359 loss: 1.65400172e-06
Iter: 1360 loss: 1.64225708e-06
Iter: 1361 loss: 1.63990501e-06
Iter: 1362 loss: 1.63989648e-06
Iter: 1363 loss: 1.6379422e-06
Iter: 1364 loss: 1.63517745e-06
Iter: 1365 loss: 1.63512073e-06
Iter: 1366 loss: 1.63204061e-06
Iter: 1367 loss: 1.63660866e-06
Iter: 1368 loss: 1.63061236e-06
Iter: 1369 loss: 1.62788092e-06
Iter: 1370 loss: 1.6278716e-06
Iter: 1371 loss: 1.62611741e-06
Iter: 1372 loss: 1.62622723e-06
Iter: 1373 loss: 1.62477295e-06
Iter: 1374 loss: 1.62240553e-06
Iter: 1375 loss: 1.63933123e-06
Iter: 1376 loss: 1.62222034e-06
Iter: 1377 loss: 1.6200695e-06
Iter: 1378 loss: 1.6254487e-06
Iter: 1379 loss: 1.61935031e-06
Iter: 1380 loss: 1.61762796e-06
Iter: 1381 loss: 1.61771925e-06
Iter: 1382 loss: 1.61626519e-06
Iter: 1383 loss: 1.61340176e-06
Iter: 1384 loss: 1.6212914e-06
Iter: 1385 loss: 1.61259732e-06
Iter: 1386 loss: 1.61027185e-06
Iter: 1387 loss: 1.61061098e-06
Iter: 1388 loss: 1.60861418e-06
Iter: 1389 loss: 1.60552406e-06
Iter: 1390 loss: 1.61979301e-06
Iter: 1391 loss: 1.60505226e-06
Iter: 1392 loss: 1.60240415e-06
Iter: 1393 loss: 1.62119818e-06
Iter: 1394 loss: 1.60224226e-06
Iter: 1395 loss: 1.60049956e-06
Iter: 1396 loss: 1.598577e-06
Iter: 1397 loss: 1.59830552e-06
Iter: 1398 loss: 1.5952661e-06
Iter: 1399 loss: 1.6020075e-06
Iter: 1400 loss: 1.59407614e-06
Iter: 1401 loss: 1.59176307e-06
Iter: 1402 loss: 1.61726e-06
Iter: 1403 loss: 1.59170077e-06
Iter: 1404 loss: 1.58931289e-06
Iter: 1405 loss: 1.59571118e-06
Iter: 1406 loss: 1.58849298e-06
Iter: 1407 loss: 1.58663602e-06
Iter: 1408 loss: 1.58742159e-06
Iter: 1409 loss: 1.58529781e-06
Iter: 1410 loss: 1.58288776e-06
Iter: 1411 loss: 1.59195667e-06
Iter: 1412 loss: 1.5823415e-06
Iter: 1413 loss: 1.57960233e-06
Iter: 1414 loss: 1.59760771e-06
Iter: 1415 loss: 1.57923921e-06
Iter: 1416 loss: 1.57765749e-06
Iter: 1417 loss: 1.57743966e-06
Iter: 1418 loss: 1.57639795e-06
Iter: 1419 loss: 1.57384784e-06
Iter: 1420 loss: 1.5810781e-06
Iter: 1421 loss: 1.57306613e-06
Iter: 1422 loss: 1.5710151e-06
Iter: 1423 loss: 1.57357454e-06
Iter: 1424 loss: 1.56992814e-06
Iter: 1425 loss: 1.56797819e-06
Iter: 1426 loss: 1.58248326e-06
Iter: 1427 loss: 1.56781311e-06
Iter: 1428 loss: 1.56575538e-06
Iter: 1429 loss: 1.56500118e-06
Iter: 1430 loss: 1.56392093e-06
Iter: 1431 loss: 1.56133365e-06
Iter: 1432 loss: 1.56712406e-06
Iter: 1433 loss: 1.56032752e-06
Iter: 1434 loss: 1.55760836e-06
Iter: 1435 loss: 1.57604507e-06
Iter: 1436 loss: 1.55730856e-06
Iter: 1437 loss: 1.55517932e-06
Iter: 1438 loss: 1.56062458e-06
Iter: 1439 loss: 1.55444604e-06
Iter: 1440 loss: 1.55222153e-06
Iter: 1441 loss: 1.5513026e-06
Iter: 1442 loss: 1.55015982e-06
Iter: 1443 loss: 1.54747613e-06
Iter: 1444 loss: 1.56223177e-06
Iter: 1445 loss: 1.54701593e-06
Iter: 1446 loss: 1.54452835e-06
Iter: 1447 loss: 1.56829697e-06
Iter: 1448 loss: 1.54434747e-06
Iter: 1449 loss: 1.5427961e-06
Iter: 1450 loss: 1.5421964e-06
Iter: 1451 loss: 1.54130362e-06
Iter: 1452 loss: 1.53931626e-06
Iter: 1453 loss: 1.57014574e-06
Iter: 1454 loss: 1.53930341e-06
Iter: 1455 loss: 1.53791132e-06
Iter: 1456 loss: 1.53530789e-06
Iter: 1457 loss: 1.59451452e-06
Iter: 1458 loss: 1.53529345e-06
Iter: 1459 loss: 1.53228825e-06
Iter: 1460 loss: 1.53632323e-06
Iter: 1461 loss: 1.53081669e-06
Iter: 1462 loss: 1.5282659e-06
Iter: 1463 loss: 1.55405678e-06
Iter: 1464 loss: 1.52816e-06
Iter: 1465 loss: 1.52578548e-06
Iter: 1466 loss: 1.53057863e-06
Iter: 1467 loss: 1.52491589e-06
Iter: 1468 loss: 1.52254779e-06
Iter: 1469 loss: 1.52673488e-06
Iter: 1470 loss: 1.5214323e-06
Iter: 1471 loss: 1.51954089e-06
Iter: 1472 loss: 1.529982e-06
Iter: 1473 loss: 1.519286e-06
Iter: 1474 loss: 1.51713471e-06
Iter: 1475 loss: 1.51976792e-06
Iter: 1476 loss: 1.51603081e-06
Iter: 1477 loss: 1.51367772e-06
Iter: 1478 loss: 1.51862037e-06
Iter: 1479 loss: 1.51266863e-06
Iter: 1480 loss: 1.51043719e-06
Iter: 1481 loss: 1.51527354e-06
Iter: 1482 loss: 1.50960602e-06
Iter: 1483 loss: 1.50692154e-06
Iter: 1484 loss: 1.52038751e-06
Iter: 1485 loss: 1.50646747e-06
Iter: 1486 loss: 1.50455776e-06
Iter: 1487 loss: 1.50269818e-06
Iter: 1488 loss: 1.50230892e-06
Iter: 1489 loss: 1.50121218e-06
Iter: 1490 loss: 1.50080314e-06
Iter: 1491 loss: 1.49930338e-06
Iter: 1492 loss: 1.49759171e-06
Iter: 1493 loss: 1.4973474e-06
Iter: 1494 loss: 1.49531104e-06
Iter: 1495 loss: 1.49609866e-06
Iter: 1496 loss: 1.49395248e-06
Iter: 1497 loss: 1.49179709e-06
Iter: 1498 loss: 1.51878351e-06
Iter: 1499 loss: 1.49180619e-06
Iter: 1500 loss: 1.4898967e-06
Iter: 1501 loss: 1.49105983e-06
Iter: 1502 loss: 1.48871811e-06
Iter: 1503 loss: 1.48637378e-06
Iter: 1504 loss: 1.48584718e-06
Iter: 1505 loss: 1.48434094e-06
Iter: 1506 loss: 1.48186678e-06
Iter: 1507 loss: 1.50218216e-06
Iter: 1508 loss: 1.48165691e-06
Iter: 1509 loss: 1.4794025e-06
Iter: 1510 loss: 1.48720346e-06
Iter: 1511 loss: 1.47889227e-06
Iter: 1512 loss: 1.47695289e-06
Iter: 1513 loss: 1.47881497e-06
Iter: 1514 loss: 1.47587843e-06
Iter: 1515 loss: 1.47366779e-06
Iter: 1516 loss: 1.47900221e-06
Iter: 1517 loss: 1.47278899e-06
Iter: 1518 loss: 1.47032119e-06
Iter: 1519 loss: 1.48656477e-06
Iter: 1520 loss: 1.47006813e-06
Iter: 1521 loss: 1.46826062e-06
Iter: 1522 loss: 1.46837533e-06
Iter: 1523 loss: 1.46687967e-06
Iter: 1524 loss: 1.46460206e-06
Iter: 1525 loss: 1.47555465e-06
Iter: 1526 loss: 1.46425464e-06
Iter: 1527 loss: 1.46197181e-06
Iter: 1528 loss: 1.46984269e-06
Iter: 1529 loss: 1.46131788e-06
Iter: 1530 loss: 1.45945194e-06
Iter: 1531 loss: 1.47205856e-06
Iter: 1532 loss: 1.4592847e-06
Iter: 1533 loss: 1.45748436e-06
Iter: 1534 loss: 1.45826345e-06
Iter: 1535 loss: 1.45626893e-06
Iter: 1536 loss: 1.454325e-06
Iter: 1537 loss: 1.45330546e-06
Iter: 1538 loss: 1.45242916e-06
Iter: 1539 loss: 1.44992327e-06
Iter: 1540 loss: 1.45152228e-06
Iter: 1541 loss: 1.44839737e-06
Iter: 1542 loss: 1.44584806e-06
Iter: 1543 loss: 1.48447339e-06
Iter: 1544 loss: 1.44586079e-06
Iter: 1545 loss: 1.44406897e-06
Iter: 1546 loss: 1.45323349e-06
Iter: 1547 loss: 1.44383762e-06
Iter: 1548 loss: 1.44196952e-06
Iter: 1549 loss: 1.44177682e-06
Iter: 1550 loss: 1.44045509e-06
Iter: 1551 loss: 1.43848365e-06
Iter: 1552 loss: 1.44002115e-06
Iter: 1553 loss: 1.43734235e-06
Iter: 1554 loss: 1.43475472e-06
Iter: 1555 loss: 1.45176955e-06
Iter: 1556 loss: 1.43448858e-06
Iter: 1557 loss: 1.43211741e-06
Iter: 1558 loss: 1.43643092e-06
Iter: 1559 loss: 1.43103193e-06
Iter: 1560 loss: 1.42893941e-06
Iter: 1561 loss: 1.42813963e-06
Iter: 1562 loss: 1.42693739e-06
Iter: 1563 loss: 1.42435874e-06
Iter: 1564 loss: 1.42438557e-06
Iter: 1565 loss: 1.42266481e-06
Iter: 1566 loss: 1.42764657e-06
Iter: 1567 loss: 1.4221107e-06
Iter: 1568 loss: 1.42064187e-06
Iter: 1569 loss: 1.43115972e-06
Iter: 1570 loss: 1.42047406e-06
Iter: 1571 loss: 1.41905116e-06
Iter: 1572 loss: 1.41657824e-06
Iter: 1573 loss: 1.47848311e-06
Iter: 1574 loss: 1.41656847e-06
Iter: 1575 loss: 1.41446799e-06
Iter: 1576 loss: 1.43824059e-06
Iter: 1577 loss: 1.41440523e-06
Iter: 1578 loss: 1.41258374e-06
Iter: 1579 loss: 1.41452131e-06
Iter: 1580 loss: 1.41167357e-06
Iter: 1581 loss: 1.40957445e-06
Iter: 1582 loss: 1.40891643e-06
Iter: 1583 loss: 1.40774557e-06
Iter: 1584 loss: 1.40493535e-06
Iter: 1585 loss: 1.41430348e-06
Iter: 1586 loss: 1.40422969e-06
Iter: 1587 loss: 1.40205418e-06
Iter: 1588 loss: 1.42750071e-06
Iter: 1589 loss: 1.402012e-06
Iter: 1590 loss: 1.40002385e-06
Iter: 1591 loss: 1.40354746e-06
Iter: 1592 loss: 1.39915483e-06
Iter: 1593 loss: 1.39737904e-06
Iter: 1594 loss: 1.39785107e-06
Iter: 1595 loss: 1.39608392e-06
Iter: 1596 loss: 1.39368194e-06
Iter: 1597 loss: 1.39538281e-06
Iter: 1598 loss: 1.39217536e-06
Iter: 1599 loss: 1.38986911e-06
Iter: 1600 loss: 1.41112037e-06
Iter: 1601 loss: 1.38976759e-06
Iter: 1602 loss: 1.38753398e-06
Iter: 1603 loss: 1.39533313e-06
Iter: 1604 loss: 1.38696589e-06
Iter: 1605 loss: 1.38533665e-06
Iter: 1606 loss: 1.39687018e-06
Iter: 1607 loss: 1.38517612e-06
Iter: 1608 loss: 1.383567e-06
Iter: 1609 loss: 1.38641326e-06
Iter: 1610 loss: 1.38286475e-06
Iter: 1611 loss: 1.38138284e-06
Iter: 1612 loss: 1.38032306e-06
Iter: 1613 loss: 1.37981237e-06
Iter: 1614 loss: 1.37800794e-06
Iter: 1615 loss: 1.40055567e-06
Iter: 1616 loss: 1.37799839e-06
Iter: 1617 loss: 1.37653717e-06
Iter: 1618 loss: 1.37533993e-06
Iter: 1619 loss: 1.37498625e-06
Iter: 1620 loss: 1.37235315e-06
Iter: 1621 loss: 1.37750715e-06
Iter: 1622 loss: 1.37131065e-06
Iter: 1623 loss: 1.36929066e-06
Iter: 1624 loss: 1.39454619e-06
Iter: 1625 loss: 1.3692777e-06
Iter: 1626 loss: 1.36765107e-06
Iter: 1627 loss: 1.3656113e-06
Iter: 1628 loss: 1.36547351e-06
Iter: 1629 loss: 1.36280426e-06
Iter: 1630 loss: 1.37426264e-06
Iter: 1631 loss: 1.36231802e-06
Iter: 1632 loss: 1.36024209e-06
Iter: 1633 loss: 1.38106475e-06
Iter: 1634 loss: 1.36019924e-06
Iter: 1635 loss: 1.35855839e-06
Iter: 1636 loss: 1.36265407e-06
Iter: 1637 loss: 1.35804419e-06
Iter: 1638 loss: 1.35647588e-06
Iter: 1639 loss: 1.35418145e-06
Iter: 1640 loss: 1.35411824e-06
Iter: 1641 loss: 1.35161747e-06
Iter: 1642 loss: 1.36477183e-06
Iter: 1643 loss: 1.35119546e-06
Iter: 1644 loss: 1.34901086e-06
Iter: 1645 loss: 1.35962864e-06
Iter: 1646 loss: 1.34871561e-06
Iter: 1647 loss: 1.34621177e-06
Iter: 1648 loss: 1.36341123e-06
Iter: 1649 loss: 1.345991e-06
Iter: 1650 loss: 1.34504398e-06
Iter: 1651 loss: 1.34413381e-06
Iter: 1652 loss: 1.34389825e-06
Iter: 1653 loss: 1.3419849e-06
Iter: 1654 loss: 1.34874017e-06
Iter: 1655 loss: 1.34143374e-06
Iter: 1656 loss: 1.33946514e-06
Iter: 1657 loss: 1.33962976e-06
Iter: 1658 loss: 1.33796414e-06
Iter: 1659 loss: 1.33592562e-06
Iter: 1660 loss: 1.35321397e-06
Iter: 1661 loss: 1.33582091e-06
Iter: 1662 loss: 1.33416916e-06
Iter: 1663 loss: 1.33730168e-06
Iter: 1664 loss: 1.33353683e-06
Iter: 1665 loss: 1.33136632e-06
Iter: 1666 loss: 1.33132039e-06
Iter: 1667 loss: 1.32965283e-06
Iter: 1668 loss: 1.32733953e-06
Iter: 1669 loss: 1.33640788e-06
Iter: 1670 loss: 1.32684988e-06
Iter: 1671 loss: 1.32465607e-06
Iter: 1672 loss: 1.34070433e-06
Iter: 1673 loss: 1.32447417e-06
Iter: 1674 loss: 1.3229278e-06
Iter: 1675 loss: 1.32243349e-06
Iter: 1676 loss: 1.32149592e-06
Iter: 1677 loss: 1.31951924e-06
Iter: 1678 loss: 1.33812352e-06
Iter: 1679 loss: 1.31945683e-06
Iter: 1680 loss: 1.31773493e-06
Iter: 1681 loss: 1.32185e-06
Iter: 1682 loss: 1.31714353e-06
Iter: 1683 loss: 1.3154505e-06
Iter: 1684 loss: 1.31361799e-06
Iter: 1685 loss: 1.31334514e-06
Iter: 1686 loss: 1.31082129e-06
Iter: 1687 loss: 1.32252035e-06
Iter: 1688 loss: 1.31039292e-06
Iter: 1689 loss: 1.30849457e-06
Iter: 1690 loss: 1.32865534e-06
Iter: 1691 loss: 1.30846104e-06
Iter: 1692 loss: 1.30656269e-06
Iter: 1693 loss: 1.31218292e-06
Iter: 1694 loss: 1.30599415e-06
Iter: 1695 loss: 1.30466628e-06
Iter: 1696 loss: 1.30301237e-06
Iter: 1697 loss: 1.30280227e-06
Iter: 1698 loss: 1.30071714e-06
Iter: 1699 loss: 1.30537728e-06
Iter: 1700 loss: 1.29993316e-06
Iter: 1701 loss: 1.29758553e-06
Iter: 1702 loss: 1.32055266e-06
Iter: 1703 loss: 1.29753948e-06
Iter: 1704 loss: 1.29599835e-06
Iter: 1705 loss: 1.2977041e-06
Iter: 1706 loss: 1.29518321e-06
Iter: 1707 loss: 1.29334762e-06
Iter: 1708 loss: 1.29370869e-06
Iter: 1709 loss: 1.29195746e-06
Iter: 1710 loss: 1.28998454e-06
Iter: 1711 loss: 1.32009927e-06
Iter: 1712 loss: 1.28994679e-06
Iter: 1713 loss: 1.28862189e-06
Iter: 1714 loss: 1.28661304e-06
Iter: 1715 loss: 1.28659599e-06
Iter: 1716 loss: 1.28415263e-06
Iter: 1717 loss: 1.30016065e-06
Iter: 1718 loss: 1.28387819e-06
Iter: 1719 loss: 1.2821306e-06
Iter: 1720 loss: 1.29419436e-06
Iter: 1721 loss: 1.28197371e-06
Iter: 1722 loss: 1.2805433e-06
Iter: 1723 loss: 1.28028728e-06
Iter: 1724 loss: 1.27928729e-06
Iter: 1725 loss: 1.27754811e-06
Iter: 1726 loss: 1.28856323e-06
Iter: 1727 loss: 1.27729356e-06
Iter: 1728 loss: 1.2757896e-06
Iter: 1729 loss: 1.28512511e-06
Iter: 1730 loss: 1.27560293e-06
Iter: 1731 loss: 1.27441353e-06
Iter: 1732 loss: 1.27241856e-06
Iter: 1733 loss: 1.27236422e-06
Iter: 1734 loss: 1.27044905e-06
Iter: 1735 loss: 1.29322166e-06
Iter: 1736 loss: 1.27044746e-06
Iter: 1737 loss: 1.26901091e-06
Iter: 1738 loss: 1.28271336e-06
Iter: 1739 loss: 1.26894588e-06
Iter: 1740 loss: 1.26788655e-06
Iter: 1741 loss: 1.26591351e-06
Iter: 1742 loss: 1.31008437e-06
Iter: 1743 loss: 1.26592704e-06
Iter: 1744 loss: 1.26373106e-06
Iter: 1745 loss: 1.26845066e-06
Iter: 1746 loss: 1.26287523e-06
Iter: 1747 loss: 1.26099155e-06
Iter: 1748 loss: 1.2765903e-06
Iter: 1749 loss: 1.26089049e-06
Iter: 1750 loss: 1.25921429e-06
Iter: 1751 loss: 1.26555017e-06
Iter: 1752 loss: 1.25885458e-06
Iter: 1753 loss: 1.25744771e-06
Iter: 1754 loss: 1.25674285e-06
Iter: 1755 loss: 1.25617726e-06
Iter: 1756 loss: 1.25412475e-06
Iter: 1757 loss: 1.27234966e-06
Iter: 1758 loss: 1.25404574e-06
Iter: 1759 loss: 1.25268593e-06
Iter: 1760 loss: 1.25620545e-06
Iter: 1761 loss: 1.25216297e-06
Iter: 1762 loss: 1.25069471e-06
Iter: 1763 loss: 1.24970666e-06
Iter: 1764 loss: 1.24913709e-06
Iter: 1765 loss: 1.24692247e-06
Iter: 1766 loss: 1.26183215e-06
Iter: 1767 loss: 1.24674352e-06
Iter: 1768 loss: 1.24484598e-06
Iter: 1769 loss: 1.25218253e-06
Iter: 1770 loss: 1.24441783e-06
Iter: 1771 loss: 1.24282519e-06
Iter: 1772 loss: 1.24204598e-06
Iter: 1773 loss: 1.24134044e-06
Iter: 1774 loss: 1.23968664e-06
Iter: 1775 loss: 1.26522445e-06
Iter: 1776 loss: 1.23969869e-06
Iter: 1777 loss: 1.23802124e-06
Iter: 1778 loss: 1.2409788e-06
Iter: 1779 loss: 1.23733275e-06
Iter: 1780 loss: 1.23606515e-06
Iter: 1781 loss: 1.24424719e-06
Iter: 1782 loss: 1.23590962e-06
Iter: 1783 loss: 1.23462189e-06
Iter: 1784 loss: 1.23407085e-06
Iter: 1785 loss: 1.23342977e-06
Iter: 1786 loss: 1.23160976e-06
Iter: 1787 loss: 1.2318867e-06
Iter: 1788 loss: 1.23027621e-06
Iter: 1789 loss: 1.22828351e-06
Iter: 1790 loss: 1.232062e-06
Iter: 1791 loss: 1.22739198e-06
Iter: 1792 loss: 1.22481833e-06
Iter: 1793 loss: 1.2375458e-06
Iter: 1794 loss: 1.22443635e-06
Iter: 1795 loss: 1.2227531e-06
Iter: 1796 loss: 1.24187272e-06
Iter: 1797 loss: 1.22272729e-06
Iter: 1798 loss: 1.2215537e-06
Iter: 1799 loss: 1.21967537e-06
Iter: 1800 loss: 1.21962762e-06
Iter: 1801 loss: 1.21745791e-06
Iter: 1802 loss: 1.23790051e-06
Iter: 1803 loss: 1.21735684e-06
Iter: 1804 loss: 1.21589596e-06
Iter: 1805 loss: 1.22442054e-06
Iter: 1806 loss: 1.21567564e-06
Iter: 1807 loss: 1.21451319e-06
Iter: 1808 loss: 1.21280482e-06
Iter: 1809 loss: 1.21275934e-06
Iter: 1810 loss: 1.21057201e-06
Iter: 1811 loss: 1.22707183e-06
Iter: 1812 loss: 1.21040921e-06
Iter: 1813 loss: 1.20850484e-06
Iter: 1814 loss: 1.2139551e-06
Iter: 1815 loss: 1.20780805e-06
Iter: 1816 loss: 1.20609593e-06
Iter: 1817 loss: 1.20814843e-06
Iter: 1818 loss: 1.20520417e-06
Iter: 1819 loss: 1.20384482e-06
Iter: 1820 loss: 1.22470749e-06
Iter: 1821 loss: 1.20385312e-06
Iter: 1822 loss: 1.20266395e-06
Iter: 1823 loss: 1.20442803e-06
Iter: 1824 loss: 1.20207983e-06
Iter: 1825 loss: 1.20083394e-06
Iter: 1826 loss: 1.20064328e-06
Iter: 1827 loss: 1.19982258e-06
Iter: 1828 loss: 1.19814251e-06
Iter: 1829 loss: 1.21448318e-06
Iter: 1830 loss: 1.19805929e-06
Iter: 1831 loss: 1.19710342e-06
Iter: 1832 loss: 1.19521724e-06
Iter: 1833 loss: 1.23518089e-06
Iter: 1834 loss: 1.19520064e-06
Iter: 1835 loss: 1.1927101e-06
Iter: 1836 loss: 1.19820265e-06
Iter: 1837 loss: 1.19172569e-06
Iter: 1838 loss: 1.18954472e-06
Iter: 1839 loss: 1.20001846e-06
Iter: 1840 loss: 1.18917183e-06
Iter: 1841 loss: 1.18721664e-06
Iter: 1842 loss: 1.19693595e-06
Iter: 1843 loss: 1.18689491e-06
Iter: 1844 loss: 1.18531318e-06
Iter: 1845 loss: 1.1954437e-06
Iter: 1846 loss: 1.18511639e-06
Iter: 1847 loss: 1.18376124e-06
Iter: 1848 loss: 1.18314165e-06
Iter: 1849 loss: 1.18250114e-06
Iter: 1850 loss: 1.18046069e-06
Iter: 1851 loss: 1.19031176e-06
Iter: 1852 loss: 1.18014009e-06
Iter: 1853 loss: 1.17848538e-06
Iter: 1854 loss: 1.18881246e-06
Iter: 1855 loss: 1.17831848e-06
Iter: 1856 loss: 1.17701711e-06
Iter: 1857 loss: 1.17637364e-06
Iter: 1858 loss: 1.17576735e-06
Iter: 1859 loss: 1.17440436e-06
Iter: 1860 loss: 1.1952327e-06
Iter: 1861 loss: 1.17441891e-06
Iter: 1862 loss: 1.17323316e-06
Iter: 1863 loss: 1.17482591e-06
Iter: 1864 loss: 1.17261891e-06
Iter: 1865 loss: 1.17111199e-06
Iter: 1866 loss: 1.17412458e-06
Iter: 1867 loss: 1.17052514e-06
Iter: 1868 loss: 1.16913066e-06
Iter: 1869 loss: 1.17119851e-06
Iter: 1870 loss: 1.16837873e-06
Iter: 1871 loss: 1.16684475e-06
Iter: 1872 loss: 1.1663999e-06
Iter: 1873 loss: 1.16551678e-06
Iter: 1874 loss: 1.16403612e-06
Iter: 1875 loss: 1.16400292e-06
Iter: 1876 loss: 1.16295e-06
Iter: 1877 loss: 1.16163187e-06
Iter: 1878 loss: 1.16156798e-06
Iter: 1879 loss: 1.15962939e-06
Iter: 1880 loss: 1.16281501e-06
Iter: 1881 loss: 1.15879538e-06
Iter: 1882 loss: 1.15686203e-06
Iter: 1883 loss: 1.15968896e-06
Iter: 1884 loss: 1.15589228e-06
Iter: 1885 loss: 1.15390526e-06
Iter: 1886 loss: 1.16825879e-06
Iter: 1887 loss: 1.15372927e-06
Iter: 1888 loss: 1.15204807e-06
Iter: 1889 loss: 1.15578064e-06
Iter: 1890 loss: 1.1513655e-06
Iter: 1891 loss: 1.14952763e-06
Iter: 1892 loss: 1.16285685e-06
Iter: 1893 loss: 1.14938496e-06
Iter: 1894 loss: 1.14816885e-06
Iter: 1895 loss: 1.14823865e-06
Iter: 1896 loss: 1.14722252e-06
Iter: 1897 loss: 1.14594263e-06
Iter: 1898 loss: 1.1459872e-06
Iter: 1899 loss: 1.14497902e-06
Iter: 1900 loss: 1.14443969e-06
Iter: 1901 loss: 1.1440095e-06
Iter: 1902 loss: 1.14239208e-06
Iter: 1903 loss: 1.14851366e-06
Iter: 1904 loss: 1.14201168e-06
Iter: 1905 loss: 1.14080513e-06
Iter: 1906 loss: 1.14072031e-06
Iter: 1907 loss: 1.13984231e-06
Iter: 1908 loss: 1.13813371e-06
Iter: 1909 loss: 1.14441991e-06
Iter: 1910 loss: 1.13776764e-06
Iter: 1911 loss: 1.1363328e-06
Iter: 1912 loss: 1.14674413e-06
Iter: 1913 loss: 1.13621581e-06
Iter: 1914 loss: 1.13510214e-06
Iter: 1915 loss: 1.13457759e-06
Iter: 1916 loss: 1.13404519e-06
Iter: 1917 loss: 1.13230954e-06
Iter: 1918 loss: 1.14309353e-06
Iter: 1919 loss: 1.13210524e-06
Iter: 1920 loss: 1.13052897e-06
Iter: 1921 loss: 1.13314127e-06
Iter: 1922 loss: 1.12979615e-06
Iter: 1923 loss: 1.12848966e-06
Iter: 1924 loss: 1.12782845e-06
Iter: 1925 loss: 1.12723e-06
Iter: 1926 loss: 1.12538805e-06
Iter: 1927 loss: 1.13236183e-06
Iter: 1928 loss: 1.1249906e-06
Iter: 1929 loss: 1.12322209e-06
Iter: 1930 loss: 1.13033479e-06
Iter: 1931 loss: 1.12286989e-06
Iter: 1932 loss: 1.12163616e-06
Iter: 1933 loss: 1.13214378e-06
Iter: 1934 loss: 1.12150212e-06
Iter: 1935 loss: 1.12000748e-06
Iter: 1936 loss: 1.12136127e-06
Iter: 1937 loss: 1.11911118e-06
Iter: 1938 loss: 1.11776865e-06
Iter: 1939 loss: 1.12754924e-06
Iter: 1940 loss: 1.11770225e-06
Iter: 1941 loss: 1.11670749e-06
Iter: 1942 loss: 1.11647932e-06
Iter: 1943 loss: 1.11589452e-06
Iter: 1944 loss: 1.11432269e-06
Iter: 1945 loss: 1.11469785e-06
Iter: 1946 loss: 1.11316831e-06
Iter: 1947 loss: 1.11185204e-06
Iter: 1948 loss: 1.12826183e-06
Iter: 1949 loss: 1.11181589e-06
Iter: 1950 loss: 1.11056283e-06
Iter: 1951 loss: 1.1116731e-06
Iter: 1952 loss: 1.10985889e-06
Iter: 1953 loss: 1.10848737e-06
Iter: 1954 loss: 1.11080351e-06
Iter: 1955 loss: 1.1078962e-06
Iter: 1956 loss: 1.10644623e-06
Iter: 1957 loss: 1.11965846e-06
Iter: 1958 loss: 1.10634983e-06
Iter: 1959 loss: 1.10540839e-06
Iter: 1960 loss: 1.1041617e-06
Iter: 1961 loss: 1.10410974e-06
Iter: 1962 loss: 1.10225619e-06
Iter: 1963 loss: 1.11162012e-06
Iter: 1964 loss: 1.10190445e-06
Iter: 1965 loss: 1.10064968e-06
Iter: 1966 loss: 1.10931092e-06
Iter: 1967 loss: 1.10049962e-06
Iter: 1968 loss: 1.09931989e-06
Iter: 1969 loss: 1.0986696e-06
Iter: 1970 loss: 1.09811549e-06
Iter: 1971 loss: 1.09656332e-06
Iter: 1972 loss: 1.10673159e-06
Iter: 1973 loss: 1.09635471e-06
Iter: 1974 loss: 1.09511984e-06
Iter: 1975 loss: 1.11048234e-06
Iter: 1976 loss: 1.09510142e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi3 /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi3
+ date
Wed Nov  4 16:31:08 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi2.8/300_300_300_1 --function f2 --psi -1 --alpha 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi3/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd09be400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd09be598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd08926a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd0892400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd08a32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd08b8ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd080d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd080d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd0855598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fad29eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fad29e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fad28e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fad1f7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fad1f1488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fad216620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fad308840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fad2ef488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f882c26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f882ad6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f882ad488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f882ad400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f8823c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fad24e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f8823c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd07d5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd07c2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f8817d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f8818e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f8818e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f881e5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f880ee8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f880f3048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f880f32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f8813e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f8813eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f880f3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.004726739
test_loss: 0.008375158
train_loss: 0.0046246313
test_loss: 0.008213192
train_loss: 0.0043669674
test_loss: 0.008283212
train_loss: 0.0045147883
test_loss: 0.008407823
train_loss: 0.0048126783
test_loss: 0.008324364
train_loss: 0.0047036353
test_loss: 0.008319232
train_loss: 0.004592169
test_loss: 0.008170131
train_loss: 0.0046818387
test_loss: 0.008163063
train_loss: 0.005555956
test_loss: 0.008497046
train_loss: 0.004579472
test_loss: 0.008357614
train_loss: 0.0049397233
test_loss: 0.008288737
train_loss: 0.0044997376
test_loss: 0.008293477
train_loss: 0.0046106554
test_loss: 0.008232785
train_loss: 0.0048215864
test_loss: 0.008242208
train_loss: 0.004363613
test_loss: 0.008188877
train_loss: 0.0046518277
test_loss: 0.008225875
train_loss: 0.0043068808
test_loss: 0.008175629
train_loss: 0.0043267813
test_loss: 0.008023718
train_loss: 0.0043323124
test_loss: 0.008069845
train_loss: 0.0054986337
test_loss: 0.008160061
train_loss: 0.004614498
test_loss: 0.008277836
train_loss: 0.004439518
test_loss: 0.008088316
train_loss: 0.004740263
test_loss: 0.008088656
train_loss: 0.0041395654
test_loss: 0.008073253
train_loss: 0.005474683
test_loss: 0.008357403
train_loss: 0.0043426887
test_loss: 0.008258485
train_loss: 0.004422604
test_loss: 0.008045421
train_loss: 0.004515716
test_loss: 0.00829145
train_loss: 0.004264657
test_loss: 0.008278984
train_loss: 0.0046609114
test_loss: 0.008215901
train_loss: 0.004202746
test_loss: 0.00808278
train_loss: 0.0042207055
test_loss: 0.00800243
train_loss: 0.0045718215
test_loss: 0.008243407
train_loss: 0.0043043694
test_loss: 0.008187232
train_loss: 0.0041783163
test_loss: 0.008120844
train_loss: 0.0043209526
test_loss: 0.007952295
train_loss: 0.0042923586
test_loss: 0.008097301
train_loss: 0.0040982887
test_loss: 0.008024394
train_loss: 0.0043118554
test_loss: 0.008080688
train_loss: 0.0044120457
test_loss: 0.008285579
train_loss: 0.0045084003
test_loss: 0.008074565
train_loss: 0.004477566
test_loss: 0.008089571
train_loss: 0.004091817
test_loss: 0.007972255
train_loss: 0.0045887455
test_loss: 0.008100942
train_loss: 0.00481499
test_loss: 0.00812683
train_loss: 0.0040755216
test_loss: 0.007949254
train_loss: 0.0041053724
test_loss: 0.008140527
train_loss: 0.0041520065
test_loss: 0.008087661
train_loss: 0.0042952965
test_loss: 0.007933352
train_loss: 0.0044935737
test_loss: 0.008132116
train_loss: 0.0041253087
test_loss: 0.007961309
train_loss: 0.004396543
test_loss: 0.007971882
train_loss: 0.0040655583
test_loss: 0.008103989
train_loss: 0.0041244514
test_loss: 0.007927926
train_loss: 0.00432333
test_loss: 0.008082558
train_loss: 0.003983984
test_loss: 0.007848808
train_loss: 0.0043948125
test_loss: 0.00813338
train_loss: 0.004714259
test_loss: 0.008014889
train_loss: 0.004151622
test_loss: 0.008010523
train_loss: 0.004070779
test_loss: 0.008095669
train_loss: 0.004487945
test_loss: 0.008212004
train_loss: 0.0040898374
test_loss: 0.007883299
train_loss: 0.0041230926
test_loss: 0.008002527
train_loss: 0.004644208
test_loss: 0.0081845205
train_loss: 0.003919268
test_loss: 0.007935131
train_loss: 0.003960244
test_loss: 0.0080154855
train_loss: 0.0038948942
test_loss: 0.007967923
train_loss: 0.0039444715
test_loss: 0.007909014
train_loss: 0.0043089488
test_loss: 0.0081166
train_loss: 0.0041804886
test_loss: 0.008042894
train_loss: 0.004123791
test_loss: 0.008101859
train_loss: 0.0042219264
test_loss: 0.008084142
train_loss: 0.004070569
test_loss: 0.007844135
train_loss: 0.0039747916
test_loss: 0.007906225
train_loss: 0.003917517
test_loss: 0.0079231495
train_loss: 0.0042969943
test_loss: 0.00804851
train_loss: 0.0041560717
test_loss: 0.008011613
train_loss: 0.0037731077
test_loss: 0.007902557
train_loss: 0.0041524414
test_loss: 0.008031014
train_loss: 0.0040200283
test_loss: 0.00808291
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi3/300_300_300_1 --optimizer lbfgs --function f2 --psi -1 --alpha 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f2_psi-1_phi3/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7ecc4048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7ec66e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7ebed6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7ebed840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7ebedd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7ebfe598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7eb5c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7eb5cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7eb1e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7eb1ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7eb18840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c741490d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c740fdc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c740fd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c740b87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c740b8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c740b8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c74036510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c74052730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7405a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c74013048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c74013730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73fd72f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73f7a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73f8f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73fa12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73f62620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73f62a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73f60950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73f06d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73ec3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73e83048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73e832f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73e89598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73e89840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c73defea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.86381507e-05
Iter: 2 loss: 3.44365544e-05
Iter: 3 loss: 3.25776782e-05
Iter: 4 loss: 3.04328405e-05
Iter: 5 loss: 2.57803731e-05
Iter: 6 loss: 4.27092964e-05
Iter: 7 loss: 2.46502459e-05
Iter: 8 loss: 2.21083e-05
Iter: 9 loss: 3.01489436e-05
Iter: 10 loss: 2.13771891e-05
Iter: 11 loss: 1.92265925e-05
Iter: 12 loss: 2.45011e-05
Iter: 13 loss: 1.84585952e-05
Iter: 14 loss: 1.72583132e-05
Iter: 15 loss: 2.23410316e-05
Iter: 16 loss: 1.7006867e-05
Iter: 17 loss: 1.58479434e-05
Iter: 18 loss: 1.91930303e-05
Iter: 19 loss: 1.54837871e-05
Iter: 20 loss: 1.45334352e-05
Iter: 21 loss: 1.61128773e-05
Iter: 22 loss: 1.41035698e-05
Iter: 23 loss: 1.32310324e-05
Iter: 24 loss: 1.804977e-05
Iter: 25 loss: 1.3103634e-05
Iter: 26 loss: 1.23424015e-05
Iter: 27 loss: 1.56424394e-05
Iter: 28 loss: 1.21878375e-05
Iter: 29 loss: 1.16344181e-05
Iter: 30 loss: 1.24958497e-05
Iter: 31 loss: 1.13744445e-05
Iter: 32 loss: 1.090355e-05
Iter: 33 loss: 1.21961e-05
Iter: 34 loss: 1.07497017e-05
Iter: 35 loss: 1.03278962e-05
Iter: 36 loss: 1.46472821e-05
Iter: 37 loss: 1.03153761e-05
Iter: 38 loss: 1.00885463e-05
Iter: 39 loss: 9.97195912e-06
Iter: 40 loss: 9.86640589e-06
Iter: 41 loss: 9.54910138e-06
Iter: 42 loss: 1.06659354e-05
Iter: 43 loss: 9.46728e-06
Iter: 44 loss: 9.28876761e-06
Iter: 45 loss: 9.27965084e-06
Iter: 46 loss: 9.11686493e-06
Iter: 47 loss: 8.95008088e-06
Iter: 48 loss: 8.91881336e-06
Iter: 49 loss: 8.72098371e-06
Iter: 50 loss: 9.30456918e-06
Iter: 51 loss: 8.6602322e-06
Iter: 52 loss: 8.50626657e-06
Iter: 53 loss: 9.88604461e-06
Iter: 54 loss: 8.49885055e-06
Iter: 55 loss: 8.35761421e-06
Iter: 56 loss: 8.30680801e-06
Iter: 57 loss: 8.22786569e-06
Iter: 58 loss: 8.05908167e-06
Iter: 59 loss: 9.07359117e-06
Iter: 60 loss: 8.03773582e-06
Iter: 61 loss: 7.91765888e-06
Iter: 62 loss: 8.58502426e-06
Iter: 63 loss: 7.90055674e-06
Iter: 64 loss: 7.80152732e-06
Iter: 65 loss: 7.86756755e-06
Iter: 66 loss: 7.7390705e-06
Iter: 67 loss: 7.63244316e-06
Iter: 68 loss: 8.27809708e-06
Iter: 69 loss: 7.61938418e-06
Iter: 70 loss: 7.52165852e-06
Iter: 71 loss: 7.70993302e-06
Iter: 72 loss: 7.48072034e-06
Iter: 73 loss: 7.39326106e-06
Iter: 74 loss: 7.42775683e-06
Iter: 75 loss: 7.332515e-06
Iter: 76 loss: 7.23660696e-06
Iter: 77 loss: 7.84144231e-06
Iter: 78 loss: 7.22557888e-06
Iter: 79 loss: 7.13063673e-06
Iter: 80 loss: 7.46692331e-06
Iter: 81 loss: 7.10600261e-06
Iter: 82 loss: 7.04017748e-06
Iter: 83 loss: 7.04230342e-06
Iter: 84 loss: 6.98837903e-06
Iter: 85 loss: 6.92526828e-06
Iter: 86 loss: 6.92236426e-06
Iter: 87 loss: 6.87608735e-06
Iter: 88 loss: 6.80059611e-06
Iter: 89 loss: 6.80029189e-06
Iter: 90 loss: 6.72631359e-06
Iter: 91 loss: 6.90875504e-06
Iter: 92 loss: 6.69988458e-06
Iter: 93 loss: 6.62913862e-06
Iter: 94 loss: 7.08855032e-06
Iter: 95 loss: 6.62153161e-06
Iter: 96 loss: 6.55875e-06
Iter: 97 loss: 6.85229e-06
Iter: 98 loss: 6.54716314e-06
Iter: 99 loss: 6.50208221e-06
Iter: 100 loss: 6.47918114e-06
Iter: 101 loss: 6.45796536e-06
Iter: 102 loss: 6.39106156e-06
Iter: 103 loss: 6.85262694e-06
Iter: 104 loss: 6.3847142e-06
Iter: 105 loss: 6.33402942e-06
Iter: 106 loss: 6.55880922e-06
Iter: 107 loss: 6.32394267e-06
Iter: 108 loss: 6.28087219e-06
Iter: 109 loss: 6.301133e-06
Iter: 110 loss: 6.25201119e-06
Iter: 111 loss: 6.20090805e-06
Iter: 112 loss: 6.54108953e-06
Iter: 113 loss: 6.19563889e-06
Iter: 114 loss: 6.15456975e-06
Iter: 115 loss: 6.19084904e-06
Iter: 116 loss: 6.13048542e-06
Iter: 117 loss: 6.08465143e-06
Iter: 118 loss: 6.13282418e-06
Iter: 119 loss: 6.05920195e-06
Iter: 120 loss: 6.01869306e-06
Iter: 121 loss: 6.49956e-06
Iter: 122 loss: 6.01824831e-06
Iter: 123 loss: 5.97927919e-06
Iter: 124 loss: 5.98478073e-06
Iter: 125 loss: 5.94951553e-06
Iter: 126 loss: 5.91834214e-06
Iter: 127 loss: 5.9177255e-06
Iter: 128 loss: 5.89400634e-06
Iter: 129 loss: 5.85824955e-06
Iter: 130 loss: 5.85745784e-06
Iter: 131 loss: 5.8162268e-06
Iter: 132 loss: 5.87162322e-06
Iter: 133 loss: 5.79538937e-06
Iter: 134 loss: 5.75097465e-06
Iter: 135 loss: 5.95372967e-06
Iter: 136 loss: 5.74258274e-06
Iter: 137 loss: 5.71012652e-06
Iter: 138 loss: 5.99056511e-06
Iter: 139 loss: 5.70832344e-06
Iter: 140 loss: 5.67661209e-06
Iter: 141 loss: 5.6803e-06
Iter: 142 loss: 5.65238861e-06
Iter: 143 loss: 5.61584602e-06
Iter: 144 loss: 5.65186474e-06
Iter: 145 loss: 5.59522732e-06
Iter: 146 loss: 5.56228588e-06
Iter: 147 loss: 6.03284661e-06
Iter: 148 loss: 5.5622304e-06
Iter: 149 loss: 5.53411928e-06
Iter: 150 loss: 5.52994788e-06
Iter: 151 loss: 5.51052881e-06
Iter: 152 loss: 5.47307172e-06
Iter: 153 loss: 5.5333785e-06
Iter: 154 loss: 5.455684e-06
Iter: 155 loss: 5.42202451e-06
Iter: 156 loss: 5.85368616e-06
Iter: 157 loss: 5.42174257e-06
Iter: 158 loss: 5.40099609e-06
Iter: 159 loss: 5.36959578e-06
Iter: 160 loss: 5.36883363e-06
Iter: 161 loss: 5.34202445e-06
Iter: 162 loss: 5.34194896e-06
Iter: 163 loss: 5.31564865e-06
Iter: 164 loss: 5.36640255e-06
Iter: 165 loss: 5.30454281e-06
Iter: 166 loss: 5.27792827e-06
Iter: 167 loss: 5.39243047e-06
Iter: 168 loss: 5.2723467e-06
Iter: 169 loss: 5.24641564e-06
Iter: 170 loss: 5.23046037e-06
Iter: 171 loss: 5.22008759e-06
Iter: 172 loss: 5.18686829e-06
Iter: 173 loss: 5.31317073e-06
Iter: 174 loss: 5.17903391e-06
Iter: 175 loss: 5.15126385e-06
Iter: 176 loss: 5.17911167e-06
Iter: 177 loss: 5.13568057e-06
Iter: 178 loss: 5.10896e-06
Iter: 179 loss: 5.50059758e-06
Iter: 180 loss: 5.10885639e-06
Iter: 181 loss: 5.08763833e-06
Iter: 182 loss: 5.08855919e-06
Iter: 183 loss: 5.07081677e-06
Iter: 184 loss: 5.0473077e-06
Iter: 185 loss: 5.09402071e-06
Iter: 186 loss: 5.03763567e-06
Iter: 187 loss: 5.01256636e-06
Iter: 188 loss: 5.14787553e-06
Iter: 189 loss: 5.00876331e-06
Iter: 190 loss: 4.9822047e-06
Iter: 191 loss: 5.02654711e-06
Iter: 192 loss: 4.9700875e-06
Iter: 193 loss: 4.94677079e-06
Iter: 194 loss: 4.96595339e-06
Iter: 195 loss: 4.93276e-06
Iter: 196 loss: 4.91052106e-06
Iter: 197 loss: 5.10976952e-06
Iter: 198 loss: 4.90953198e-06
Iter: 199 loss: 4.88827254e-06
Iter: 200 loss: 4.89901822e-06
Iter: 201 loss: 4.87408488e-06
Iter: 202 loss: 4.85725559e-06
Iter: 203 loss: 5.07278764e-06
Iter: 204 loss: 4.85708279e-06
Iter: 205 loss: 4.84194243e-06
Iter: 206 loss: 4.86824365e-06
Iter: 207 loss: 4.83499662e-06
Iter: 208 loss: 4.81986262e-06
Iter: 209 loss: 4.81903044e-06
Iter: 210 loss: 4.80771405e-06
Iter: 211 loss: 4.78696893e-06
Iter: 212 loss: 4.87718125e-06
Iter: 213 loss: 4.78278798e-06
Iter: 214 loss: 4.76235573e-06
Iter: 215 loss: 4.77665662e-06
Iter: 216 loss: 4.74951685e-06
Iter: 217 loss: 4.72677175e-06
Iter: 218 loss: 4.76619925e-06
Iter: 219 loss: 4.71673e-06
Iter: 220 loss: 4.69323459e-06
Iter: 221 loss: 4.80403196e-06
Iter: 222 loss: 4.68876078e-06
Iter: 223 loss: 4.66536494e-06
Iter: 224 loss: 4.81315692e-06
Iter: 225 loss: 4.66269557e-06
Iter: 226 loss: 4.64940695e-06
Iter: 227 loss: 4.63099968e-06
Iter: 228 loss: 4.63010065e-06
Iter: 229 loss: 4.60978936e-06
Iter: 230 loss: 4.86891258e-06
Iter: 231 loss: 4.60957881e-06
Iter: 232 loss: 4.59266494e-06
Iter: 233 loss: 4.64254572e-06
Iter: 234 loss: 4.58741351e-06
Iter: 235 loss: 4.57228452e-06
Iter: 236 loss: 4.56708585e-06
Iter: 237 loss: 4.5584693e-06
Iter: 238 loss: 4.53566099e-06
Iter: 239 loss: 4.57877832e-06
Iter: 240 loss: 4.52596032e-06
Iter: 241 loss: 4.50657262e-06
Iter: 242 loss: 4.50646849e-06
Iter: 243 loss: 4.49426625e-06
Iter: 244 loss: 4.51790038e-06
Iter: 245 loss: 4.48927904e-06
Iter: 246 loss: 4.47290677e-06
Iter: 247 loss: 4.49166691e-06
Iter: 248 loss: 4.46404e-06
Iter: 249 loss: 4.4494891e-06
Iter: 250 loss: 4.4503463e-06
Iter: 251 loss: 4.43816407e-06
Iter: 252 loss: 4.4175622e-06
Iter: 253 loss: 4.5155557e-06
Iter: 254 loss: 4.41386192e-06
Iter: 255 loss: 4.39760743e-06
Iter: 256 loss: 4.47571256e-06
Iter: 257 loss: 4.39466703e-06
Iter: 258 loss: 4.38046163e-06
Iter: 259 loss: 4.37316521e-06
Iter: 260 loss: 4.36634173e-06
Iter: 261 loss: 4.34653703e-06
Iter: 262 loss: 4.48490846e-06
Iter: 263 loss: 4.34476942e-06
Iter: 264 loss: 4.32946126e-06
Iter: 265 loss: 4.43167664e-06
Iter: 266 loss: 4.328047e-06
Iter: 267 loss: 4.31542e-06
Iter: 268 loss: 4.30385398e-06
Iter: 269 loss: 4.30093314e-06
Iter: 270 loss: 4.28332351e-06
Iter: 271 loss: 4.32874094e-06
Iter: 272 loss: 4.27724353e-06
Iter: 273 loss: 4.2604056e-06
Iter: 274 loss: 4.49344861e-06
Iter: 275 loss: 4.26050428e-06
Iter: 276 loss: 4.25120561e-06
Iter: 277 loss: 4.23562324e-06
Iter: 278 loss: 4.23556185e-06
Iter: 279 loss: 4.2147858e-06
Iter: 280 loss: 4.31320268e-06
Iter: 281 loss: 4.21117966e-06
Iter: 282 loss: 4.19626667e-06
Iter: 283 loss: 4.36545724e-06
Iter: 284 loss: 4.19608659e-06
Iter: 285 loss: 4.18315039e-06
Iter: 286 loss: 4.21220784e-06
Iter: 287 loss: 4.17824776e-06
Iter: 288 loss: 4.16738658e-06
Iter: 289 loss: 4.17410502e-06
Iter: 290 loss: 4.16060084e-06
Iter: 291 loss: 4.14410533e-06
Iter: 292 loss: 4.17452156e-06
Iter: 293 loss: 4.13695852e-06
Iter: 294 loss: 4.12540703e-06
Iter: 295 loss: 4.12371e-06
Iter: 296 loss: 4.11557767e-06
Iter: 297 loss: 4.09921313e-06
Iter: 298 loss: 4.28682506e-06
Iter: 299 loss: 4.09891345e-06
Iter: 300 loss: 4.08807864e-06
Iter: 301 loss: 4.0944351e-06
Iter: 302 loss: 4.08113237e-06
Iter: 303 loss: 4.06643e-06
Iter: 304 loss: 4.07226526e-06
Iter: 305 loss: 4.05620176e-06
Iter: 306 loss: 4.03962849e-06
Iter: 307 loss: 4.29955026e-06
Iter: 308 loss: 4.0396344e-06
Iter: 309 loss: 4.03028116e-06
Iter: 310 loss: 4.01843863e-06
Iter: 311 loss: 4.01756461e-06
Iter: 312 loss: 4.00085946e-06
Iter: 313 loss: 4.0783334e-06
Iter: 314 loss: 3.99780674e-06
Iter: 315 loss: 3.9863221e-06
Iter: 316 loss: 4.12467625e-06
Iter: 317 loss: 3.9862125e-06
Iter: 318 loss: 3.97688336e-06
Iter: 319 loss: 3.96339101e-06
Iter: 320 loss: 3.96308769e-06
Iter: 321 loss: 3.94642439e-06
Iter: 322 loss: 3.99826513e-06
Iter: 323 loss: 3.94147219e-06
Iter: 324 loss: 3.92931133e-06
Iter: 325 loss: 4.11375049e-06
Iter: 326 loss: 3.92926449e-06
Iter: 327 loss: 3.91891081e-06
Iter: 328 loss: 3.94920153e-06
Iter: 329 loss: 3.91576077e-06
Iter: 330 loss: 3.9071947e-06
Iter: 331 loss: 3.90399828e-06
Iter: 332 loss: 3.89925208e-06
Iter: 333 loss: 3.88616e-06
Iter: 334 loss: 3.9462675e-06
Iter: 335 loss: 3.88374247e-06
Iter: 336 loss: 3.87179352e-06
Iter: 337 loss: 3.87980936e-06
Iter: 338 loss: 3.86433931e-06
Iter: 339 loss: 3.85179646e-06
Iter: 340 loss: 3.87145747e-06
Iter: 341 loss: 3.84595023e-06
Iter: 342 loss: 3.83096e-06
Iter: 343 loss: 3.94352264e-06
Iter: 344 loss: 3.82961571e-06
Iter: 345 loss: 3.8188582e-06
Iter: 346 loss: 3.82861526e-06
Iter: 347 loss: 3.81265863e-06
Iter: 348 loss: 3.80093047e-06
Iter: 349 loss: 3.84804753e-06
Iter: 350 loss: 3.79838457e-06
Iter: 351 loss: 3.78568893e-06
Iter: 352 loss: 3.83665611e-06
Iter: 353 loss: 3.78301343e-06
Iter: 354 loss: 3.77466677e-06
Iter: 355 loss: 3.76910634e-06
Iter: 356 loss: 3.76603543e-06
Iter: 357 loss: 3.75171339e-06
Iter: 358 loss: 3.81608197e-06
Iter: 359 loss: 3.74892966e-06
Iter: 360 loss: 3.73669172e-06
Iter: 361 loss: 3.82605776e-06
Iter: 362 loss: 3.73570583e-06
Iter: 363 loss: 3.72616796e-06
Iter: 364 loss: 3.72316936e-06
Iter: 365 loss: 3.71764531e-06
Iter: 366 loss: 3.70310522e-06
Iter: 367 loss: 3.70550242e-06
Iter: 368 loss: 3.69223358e-06
Iter: 369 loss: 3.67852e-06
Iter: 370 loss: 3.80147503e-06
Iter: 371 loss: 3.67792336e-06
Iter: 372 loss: 3.66712902e-06
Iter: 373 loss: 3.75427499e-06
Iter: 374 loss: 3.66645099e-06
Iter: 375 loss: 3.65847131e-06
Iter: 376 loss: 3.68771589e-06
Iter: 377 loss: 3.65652204e-06
Iter: 378 loss: 3.64888797e-06
Iter: 379 loss: 3.67082475e-06
Iter: 380 loss: 3.64652442e-06
Iter: 381 loss: 3.63844424e-06
Iter: 382 loss: 3.62582705e-06
Iter: 383 loss: 3.62567357e-06
Iter: 384 loss: 3.61281536e-06
Iter: 385 loss: 3.70057637e-06
Iter: 386 loss: 3.61154889e-06
Iter: 387 loss: 3.5994376e-06
Iter: 388 loss: 3.64681978e-06
Iter: 389 loss: 3.59661863e-06
Iter: 390 loss: 3.58641637e-06
Iter: 391 loss: 3.59248679e-06
Iter: 392 loss: 3.57985527e-06
Iter: 393 loss: 3.5699411e-06
Iter: 394 loss: 3.70448129e-06
Iter: 395 loss: 3.56990813e-06
Iter: 396 loss: 3.56225814e-06
Iter: 397 loss: 3.54782014e-06
Iter: 398 loss: 3.86443708e-06
Iter: 399 loss: 3.54781673e-06
Iter: 400 loss: 3.53556925e-06
Iter: 401 loss: 3.68374367e-06
Iter: 402 loss: 3.53538326e-06
Iter: 403 loss: 3.52585039e-06
Iter: 404 loss: 3.55905422e-06
Iter: 405 loss: 3.52325105e-06
Iter: 406 loss: 3.51264953e-06
Iter: 407 loss: 3.52170491e-06
Iter: 408 loss: 3.50631535e-06
Iter: 409 loss: 3.49567381e-06
Iter: 410 loss: 3.51445897e-06
Iter: 411 loss: 3.4910081e-06
Iter: 412 loss: 3.47930313e-06
Iter: 413 loss: 3.48300114e-06
Iter: 414 loss: 3.47103082e-06
Iter: 415 loss: 3.46285719e-06
Iter: 416 loss: 3.46191837e-06
Iter: 417 loss: 3.45408625e-06
Iter: 418 loss: 3.45911826e-06
Iter: 419 loss: 3.44898717e-06
Iter: 420 loss: 3.43977058e-06
Iter: 421 loss: 3.48892263e-06
Iter: 422 loss: 3.43839883e-06
Iter: 423 loss: 3.43108377e-06
Iter: 424 loss: 3.42336e-06
Iter: 425 loss: 3.42198928e-06
Iter: 426 loss: 3.40986116e-06
Iter: 427 loss: 3.45598301e-06
Iter: 428 loss: 3.40701285e-06
Iter: 429 loss: 3.39642475e-06
Iter: 430 loss: 3.45523654e-06
Iter: 431 loss: 3.39491498e-06
Iter: 432 loss: 3.38475206e-06
Iter: 433 loss: 3.39888697e-06
Iter: 434 loss: 3.37982306e-06
Iter: 435 loss: 3.37126744e-06
Iter: 436 loss: 3.42035332e-06
Iter: 437 loss: 3.37022607e-06
Iter: 438 loss: 3.36219978e-06
Iter: 439 loss: 3.37129904e-06
Iter: 440 loss: 3.35807545e-06
Iter: 441 loss: 3.34796596e-06
Iter: 442 loss: 3.35107893e-06
Iter: 443 loss: 3.3408478e-06
Iter: 444 loss: 3.3314941e-06
Iter: 445 loss: 3.42127396e-06
Iter: 446 loss: 3.33117237e-06
Iter: 447 loss: 3.32271475e-06
Iter: 448 loss: 3.34263882e-06
Iter: 449 loss: 3.31957563e-06
Iter: 450 loss: 3.31097681e-06
Iter: 451 loss: 3.30731632e-06
Iter: 452 loss: 3.30286503e-06
Iter: 453 loss: 3.2912651e-06
Iter: 454 loss: 3.32481613e-06
Iter: 455 loss: 3.2876992e-06
Iter: 456 loss: 3.27971702e-06
Iter: 457 loss: 3.27971475e-06
Iter: 458 loss: 3.271997e-06
Iter: 459 loss: 3.28024407e-06
Iter: 460 loss: 3.26760664e-06
Iter: 461 loss: 3.26103395e-06
Iter: 462 loss: 3.27261432e-06
Iter: 463 loss: 3.258177e-06
Iter: 464 loss: 3.24864845e-06
Iter: 465 loss: 3.25493693e-06
Iter: 466 loss: 3.24281791e-06
Iter: 467 loss: 3.23367158e-06
Iter: 468 loss: 3.23706422e-06
Iter: 469 loss: 3.22743472e-06
Iter: 470 loss: 3.2171688e-06
Iter: 471 loss: 3.35655818e-06
Iter: 472 loss: 3.21708558e-06
Iter: 473 loss: 3.20999175e-06
Iter: 474 loss: 3.22385904e-06
Iter: 475 loss: 3.20713934e-06
Iter: 476 loss: 3.20042068e-06
Iter: 477 loss: 3.20894787e-06
Iter: 478 loss: 3.19706805e-06
Iter: 479 loss: 3.18733032e-06
Iter: 480 loss: 3.20693789e-06
Iter: 481 loss: 3.18344564e-06
Iter: 482 loss: 3.17454123e-06
Iter: 483 loss: 3.20852405e-06
Iter: 484 loss: 3.17258605e-06
Iter: 485 loss: 3.16539035e-06
Iter: 486 loss: 3.17343574e-06
Iter: 487 loss: 3.16142e-06
Iter: 488 loss: 3.15153511e-06
Iter: 489 loss: 3.20228264e-06
Iter: 490 loss: 3.14991348e-06
Iter: 491 loss: 3.14168665e-06
Iter: 492 loss: 3.14220824e-06
Iter: 493 loss: 3.13522287e-06
Iter: 494 loss: 3.12619522e-06
Iter: 495 loss: 3.13984947e-06
Iter: 496 loss: 3.12198972e-06
Iter: 497 loss: 3.11541589e-06
Iter: 498 loss: 3.11492272e-06
Iter: 499 loss: 3.10955556e-06
Iter: 500 loss: 3.10298196e-06
Iter: 501 loss: 3.10246037e-06
Iter: 502 loss: 3.09457732e-06
Iter: 503 loss: 3.12699694e-06
Iter: 504 loss: 3.09276948e-06
Iter: 505 loss: 3.08516564e-06
Iter: 506 loss: 3.10699215e-06
Iter: 507 loss: 3.08292169e-06
Iter: 508 loss: 3.07450068e-06
Iter: 509 loss: 3.07734899e-06
Iter: 510 loss: 3.06841866e-06
Iter: 511 loss: 3.05996969e-06
Iter: 512 loss: 3.101125e-06
Iter: 513 loss: 3.05844037e-06
Iter: 514 loss: 3.04994842e-06
Iter: 515 loss: 3.09170878e-06
Iter: 516 loss: 3.04850778e-06
Iter: 517 loss: 3.04185255e-06
Iter: 518 loss: 3.04481773e-06
Iter: 519 loss: 3.03718843e-06
Iter: 520 loss: 3.02993976e-06
Iter: 521 loss: 3.06611355e-06
Iter: 522 loss: 3.02858598e-06
Iter: 523 loss: 3.02191302e-06
Iter: 524 loss: 3.04185528e-06
Iter: 525 loss: 3.01991236e-06
Iter: 526 loss: 3.01230739e-06
Iter: 527 loss: 3.00603028e-06
Iter: 528 loss: 3.00382021e-06
Iter: 529 loss: 2.99798603e-06
Iter: 530 loss: 2.99732619e-06
Iter: 531 loss: 2.99231374e-06
Iter: 532 loss: 2.982149e-06
Iter: 533 loss: 3.15795592e-06
Iter: 534 loss: 2.98190525e-06
Iter: 535 loss: 2.97374663e-06
Iter: 536 loss: 3.08543054e-06
Iter: 537 loss: 2.97372208e-06
Iter: 538 loss: 2.96675762e-06
Iter: 539 loss: 3.01648788e-06
Iter: 540 loss: 2.96613916e-06
Iter: 541 loss: 2.96117014e-06
Iter: 542 loss: 2.9537332e-06
Iter: 543 loss: 2.95358973e-06
Iter: 544 loss: 2.94603092e-06
Iter: 545 loss: 2.97021711e-06
Iter: 546 loss: 2.94388633e-06
Iter: 547 loss: 2.93425455e-06
Iter: 548 loss: 2.97445786e-06
Iter: 549 loss: 2.93228322e-06
Iter: 550 loss: 2.92529262e-06
Iter: 551 loss: 2.93398716e-06
Iter: 552 loss: 2.92173763e-06
Iter: 553 loss: 2.91455626e-06
Iter: 554 loss: 2.94965616e-06
Iter: 555 loss: 2.91317338e-06
Iter: 556 loss: 2.90616754e-06
Iter: 557 loss: 2.93082439e-06
Iter: 558 loss: 2.90429944e-06
Iter: 559 loss: 2.89776381e-06
Iter: 560 loss: 2.89874197e-06
Iter: 561 loss: 2.89294712e-06
Iter: 562 loss: 2.88622232e-06
Iter: 563 loss: 2.9234543e-06
Iter: 564 loss: 2.88527576e-06
Iter: 565 loss: 2.87819398e-06
Iter: 566 loss: 2.891657e-06
Iter: 567 loss: 2.87521652e-06
Iter: 568 loss: 2.86854356e-06
Iter: 569 loss: 2.8895256e-06
Iter: 570 loss: 2.8666143e-06
Iter: 571 loss: 2.86019144e-06
Iter: 572 loss: 2.87828379e-06
Iter: 573 loss: 2.85824422e-06
Iter: 574 loss: 2.85032047e-06
Iter: 575 loss: 2.85725946e-06
Iter: 576 loss: 2.84579664e-06
Iter: 577 loss: 2.83940244e-06
Iter: 578 loss: 2.90102321e-06
Iter: 579 loss: 2.83919167e-06
Iter: 580 loss: 2.83298095e-06
Iter: 581 loss: 2.84597968e-06
Iter: 582 loss: 2.83055692e-06
Iter: 583 loss: 2.82539963e-06
Iter: 584 loss: 2.81997e-06
Iter: 585 loss: 2.81903795e-06
Iter: 586 loss: 2.81051075e-06
Iter: 587 loss: 2.83366739e-06
Iter: 588 loss: 2.80777817e-06
Iter: 589 loss: 2.80045515e-06
Iter: 590 loss: 2.88679e-06
Iter: 591 loss: 2.80041354e-06
Iter: 592 loss: 2.79486608e-06
Iter: 593 loss: 2.79758251e-06
Iter: 594 loss: 2.79114556e-06
Iter: 595 loss: 2.78398284e-06
Iter: 596 loss: 2.80135396e-06
Iter: 597 loss: 2.78158245e-06
Iter: 598 loss: 2.77564413e-06
Iter: 599 loss: 2.82702354e-06
Iter: 600 loss: 2.77531808e-06
Iter: 601 loss: 2.77003983e-06
Iter: 602 loss: 2.76538e-06
Iter: 603 loss: 2.76401715e-06
Iter: 604 loss: 2.75661387e-06
Iter: 605 loss: 2.77399022e-06
Iter: 606 loss: 2.75386583e-06
Iter: 607 loss: 2.7481592e-06
Iter: 608 loss: 2.74807803e-06
Iter: 609 loss: 2.7436356e-06
Iter: 610 loss: 2.73430987e-06
Iter: 611 loss: 2.89409445e-06
Iter: 612 loss: 2.7340825e-06
Iter: 613 loss: 2.72920124e-06
Iter: 614 loss: 2.72828083e-06
Iter: 615 loss: 2.72412217e-06
Iter: 616 loss: 2.72358307e-06
Iter: 617 loss: 2.72066427e-06
Iter: 618 loss: 2.71432441e-06
Iter: 619 loss: 2.76152377e-06
Iter: 620 loss: 2.71390081e-06
Iter: 621 loss: 2.70932424e-06
Iter: 622 loss: 2.70204873e-06
Iter: 623 loss: 2.70200121e-06
Iter: 624 loss: 2.69457587e-06
Iter: 625 loss: 2.71753561e-06
Iter: 626 loss: 2.69233965e-06
Iter: 627 loss: 2.68544318e-06
Iter: 628 loss: 2.74404283e-06
Iter: 629 loss: 2.68503481e-06
Iter: 630 loss: 2.67879796e-06
Iter: 631 loss: 2.68412805e-06
Iter: 632 loss: 2.6750929e-06
Iter: 633 loss: 2.6685907e-06
Iter: 634 loss: 2.69074e-06
Iter: 635 loss: 2.66681764e-06
Iter: 636 loss: 2.66153756e-06
Iter: 637 loss: 2.69967904e-06
Iter: 638 loss: 2.6610403e-06
Iter: 639 loss: 2.65597419e-06
Iter: 640 loss: 2.64979508e-06
Iter: 641 loss: 2.64926916e-06
Iter: 642 loss: 2.64165919e-06
Iter: 643 loss: 2.68941676e-06
Iter: 644 loss: 2.640822e-06
Iter: 645 loss: 2.63536754e-06
Iter: 646 loss: 2.66598909e-06
Iter: 647 loss: 2.63459265e-06
Iter: 648 loss: 2.62869571e-06
Iter: 649 loss: 2.63092124e-06
Iter: 650 loss: 2.6246023e-06
Iter: 651 loss: 2.61922401e-06
Iter: 652 loss: 2.65902509e-06
Iter: 653 loss: 2.61882224e-06
Iter: 654 loss: 2.61432206e-06
Iter: 655 loss: 2.63902621e-06
Iter: 656 loss: 2.61371861e-06
Iter: 657 loss: 2.60967249e-06
Iter: 658 loss: 2.60823799e-06
Iter: 659 loss: 2.60603429e-06
Iter: 660 loss: 2.59985927e-06
Iter: 661 loss: 2.62314688e-06
Iter: 662 loss: 2.59837498e-06
Iter: 663 loss: 2.59341732e-06
Iter: 664 loss: 2.58972682e-06
Iter: 665 loss: 2.58816976e-06
Iter: 666 loss: 2.58112368e-06
Iter: 667 loss: 2.61765899e-06
Iter: 668 loss: 2.57994634e-06
Iter: 669 loss: 2.57416332e-06
Iter: 670 loss: 2.60828347e-06
Iter: 671 loss: 2.57346096e-06
Iter: 672 loss: 2.5678039e-06
Iter: 673 loss: 2.58312389e-06
Iter: 674 loss: 2.56600697e-06
Iter: 675 loss: 2.56103317e-06
Iter: 676 loss: 2.56741487e-06
Iter: 677 loss: 2.55852183e-06
Iter: 678 loss: 2.55243185e-06
Iter: 679 loss: 2.57035731e-06
Iter: 680 loss: 2.5505874e-06
Iter: 681 loss: 2.54398765e-06
Iter: 682 loss: 2.56556e-06
Iter: 683 loss: 2.54213455e-06
Iter: 684 loss: 2.53747521e-06
Iter: 685 loss: 2.535965e-06
Iter: 686 loss: 2.53337976e-06
Iter: 687 loss: 2.52688505e-06
Iter: 688 loss: 2.5850477e-06
Iter: 689 loss: 2.52661948e-06
Iter: 690 loss: 2.52074346e-06
Iter: 691 loss: 2.53600683e-06
Iter: 692 loss: 2.51871074e-06
Iter: 693 loss: 2.51475785e-06
Iter: 694 loss: 2.54837755e-06
Iter: 695 loss: 2.51451e-06
Iter: 696 loss: 2.51046367e-06
Iter: 697 loss: 2.50517769e-06
Iter: 698 loss: 2.50485641e-06
Iter: 699 loss: 2.49900745e-06
Iter: 700 loss: 2.52143263e-06
Iter: 701 loss: 2.4975775e-06
Iter: 702 loss: 2.49235336e-06
Iter: 703 loss: 2.51533311e-06
Iter: 704 loss: 2.49123809e-06
Iter: 705 loss: 2.48608944e-06
Iter: 706 loss: 2.48887181e-06
Iter: 707 loss: 2.48268907e-06
Iter: 708 loss: 2.47668e-06
Iter: 709 loss: 2.48137781e-06
Iter: 710 loss: 2.47305024e-06
Iter: 711 loss: 2.46805848e-06
Iter: 712 loss: 2.46812601e-06
Iter: 713 loss: 2.46349919e-06
Iter: 714 loss: 2.46451327e-06
Iter: 715 loss: 2.46008267e-06
Iter: 716 loss: 2.45494698e-06
Iter: 717 loss: 2.46224727e-06
Iter: 718 loss: 2.45248361e-06
Iter: 719 loss: 2.44662078e-06
Iter: 720 loss: 2.48972719e-06
Iter: 721 loss: 2.44611056e-06
Iter: 722 loss: 2.44171133e-06
Iter: 723 loss: 2.44709554e-06
Iter: 724 loss: 2.43934664e-06
Iter: 725 loss: 2.43460272e-06
Iter: 726 loss: 2.4388446e-06
Iter: 727 loss: 2.43182603e-06
Iter: 728 loss: 2.42647684e-06
Iter: 729 loss: 2.4878309e-06
Iter: 730 loss: 2.42642409e-06
Iter: 731 loss: 2.42177907e-06
Iter: 732 loss: 2.42531632e-06
Iter: 733 loss: 2.418956e-06
Iter: 734 loss: 2.41411135e-06
Iter: 735 loss: 2.43793306e-06
Iter: 736 loss: 2.413316e-06
Iter: 737 loss: 2.40890222e-06
Iter: 738 loss: 2.40919712e-06
Iter: 739 loss: 2.40550321e-06
Iter: 740 loss: 2.39980204e-06
Iter: 741 loss: 2.39598216e-06
Iter: 742 loss: 2.393861e-06
Iter: 743 loss: 2.38982739e-06
Iter: 744 loss: 2.38926532e-06
Iter: 745 loss: 2.38558687e-06
Iter: 746 loss: 2.38236021e-06
Iter: 747 loss: 2.38136636e-06
Iter: 748 loss: 2.37513814e-06
Iter: 749 loss: 2.39692122e-06
Iter: 750 loss: 2.37353743e-06
Iter: 751 loss: 2.36867641e-06
Iter: 752 loss: 2.4065223e-06
Iter: 753 loss: 2.36834262e-06
Iter: 754 loss: 2.36380447e-06
Iter: 755 loss: 2.36996652e-06
Iter: 756 loss: 2.36152641e-06
Iter: 757 loss: 2.35700509e-06
Iter: 758 loss: 2.36234814e-06
Iter: 759 loss: 2.35469338e-06
Iter: 760 loss: 2.34941331e-06
Iter: 761 loss: 2.36659776e-06
Iter: 762 loss: 2.34800859e-06
Iter: 763 loss: 2.34274739e-06
Iter: 764 loss: 2.37239078e-06
Iter: 765 loss: 2.34203299e-06
Iter: 766 loss: 2.3378991e-06
Iter: 767 loss: 2.33730225e-06
Iter: 768 loss: 2.33442552e-06
Iter: 769 loss: 2.32870707e-06
Iter: 770 loss: 2.40603549e-06
Iter: 771 loss: 2.3286716e-06
Iter: 772 loss: 2.32623938e-06
Iter: 773 loss: 2.3223954e-06
Iter: 774 loss: 2.32235061e-06
Iter: 775 loss: 2.31695822e-06
Iter: 776 loss: 2.35562493e-06
Iter: 777 loss: 2.31648255e-06
Iter: 778 loss: 2.31249646e-06
Iter: 779 loss: 2.31067429e-06
Iter: 780 loss: 2.30866544e-06
Iter: 781 loss: 2.30360024e-06
Iter: 782 loss: 2.31810623e-06
Iter: 783 loss: 2.30200021e-06
Iter: 784 loss: 2.29652233e-06
Iter: 785 loss: 2.3122243e-06
Iter: 786 loss: 2.29480497e-06
Iter: 787 loss: 2.28976e-06
Iter: 788 loss: 2.33425135e-06
Iter: 789 loss: 2.28952285e-06
Iter: 790 loss: 2.28615522e-06
Iter: 791 loss: 2.28077579e-06
Iter: 792 loss: 2.28067938e-06
Iter: 793 loss: 2.27641294e-06
Iter: 794 loss: 2.27615965e-06
Iter: 795 loss: 2.27294231e-06
Iter: 796 loss: 2.27033297e-06
Iter: 797 loss: 2.26943621e-06
Iter: 798 loss: 2.2638269e-06
Iter: 799 loss: 2.27490364e-06
Iter: 800 loss: 2.2615734e-06
Iter: 801 loss: 2.25702661e-06
Iter: 802 loss: 2.28614158e-06
Iter: 803 loss: 2.25651434e-06
Iter: 804 loss: 2.25202211e-06
Iter: 805 loss: 2.26312704e-06
Iter: 806 loss: 2.25040912e-06
Iter: 807 loss: 2.24644441e-06
Iter: 808 loss: 2.25218332e-06
Iter: 809 loss: 2.24451151e-06
Iter: 810 loss: 2.24066025e-06
Iter: 811 loss: 2.29760144e-06
Iter: 812 loss: 2.24067753e-06
Iter: 813 loss: 2.23786947e-06
Iter: 814 loss: 2.23305733e-06
Iter: 815 loss: 2.23309053e-06
Iter: 816 loss: 2.22801032e-06
Iter: 817 loss: 2.28766021e-06
Iter: 818 loss: 2.22796893e-06
Iter: 819 loss: 2.22434619e-06
Iter: 820 loss: 2.22068775e-06
Iter: 821 loss: 2.22000631e-06
Iter: 822 loss: 2.21401933e-06
Iter: 823 loss: 2.22209701e-06
Iter: 824 loss: 2.21093751e-06
Iter: 825 loss: 2.20551647e-06
Iter: 826 loss: 2.25119584e-06
Iter: 827 loss: 2.20522838e-06
Iter: 828 loss: 2.20080119e-06
Iter: 829 loss: 2.21255846e-06
Iter: 830 loss: 2.19930894e-06
Iter: 831 loss: 2.19399021e-06
Iter: 832 loss: 2.20362426e-06
Iter: 833 loss: 2.1916203e-06
Iter: 834 loss: 2.18691821e-06
Iter: 835 loss: 2.2018985e-06
Iter: 836 loss: 2.18565219e-06
Iter: 837 loss: 2.18121068e-06
Iter: 838 loss: 2.20710217e-06
Iter: 839 loss: 2.18064361e-06
Iter: 840 loss: 2.1763949e-06
Iter: 841 loss: 2.17883758e-06
Iter: 842 loss: 2.17358274e-06
Iter: 843 loss: 2.16931312e-06
Iter: 844 loss: 2.181627e-06
Iter: 845 loss: 2.16812032e-06
Iter: 846 loss: 2.16381227e-06
Iter: 847 loss: 2.17921956e-06
Iter: 848 loss: 2.16271656e-06
Iter: 849 loss: 2.157361e-06
Iter: 850 loss: 2.16031208e-06
Iter: 851 loss: 2.15388764e-06
Iter: 852 loss: 2.14982742e-06
Iter: 853 loss: 2.16671219e-06
Iter: 854 loss: 2.1489077e-06
Iter: 855 loss: 2.14406759e-06
Iter: 856 loss: 2.17100819e-06
Iter: 857 loss: 2.14346051e-06
Iter: 858 loss: 2.1403348e-06
Iter: 859 loss: 2.14692977e-06
Iter: 860 loss: 2.13915246e-06
Iter: 861 loss: 2.13517592e-06
Iter: 862 loss: 2.1363121e-06
Iter: 863 loss: 2.1322985e-06
Iter: 864 loss: 2.12809141e-06
Iter: 865 loss: 2.13175417e-06
Iter: 866 loss: 2.12549753e-06
Iter: 867 loss: 2.12042846e-06
Iter: 868 loss: 2.12884061e-06
Iter: 869 loss: 2.11797783e-06
Iter: 870 loss: 2.11293e-06
Iter: 871 loss: 2.14631746e-06
Iter: 872 loss: 2.11242491e-06
Iter: 873 loss: 2.10859525e-06
Iter: 874 loss: 2.12954228e-06
Iter: 875 loss: 2.10801909e-06
Iter: 876 loss: 2.10452799e-06
Iter: 877 loss: 2.10680355e-06
Iter: 878 loss: 2.10228109e-06
Iter: 879 loss: 2.09834707e-06
Iter: 880 loss: 2.11511e-06
Iter: 881 loss: 2.09745667e-06
Iter: 882 loss: 2.09325799e-06
Iter: 883 loss: 2.10860344e-06
Iter: 884 loss: 2.09215182e-06
Iter: 885 loss: 2.08902588e-06
Iter: 886 loss: 2.08707934e-06
Iter: 887 loss: 2.08584333e-06
Iter: 888 loss: 2.08170081e-06
Iter: 889 loss: 2.12605437e-06
Iter: 890 loss: 2.08162828e-06
Iter: 891 loss: 2.07771245e-06
Iter: 892 loss: 2.07783864e-06
Iter: 893 loss: 2.07471385e-06
Iter: 894 loss: 2.06970572e-06
Iter: 895 loss: 2.08189044e-06
Iter: 896 loss: 2.06787263e-06
Iter: 897 loss: 2.06438744e-06
Iter: 898 loss: 2.11239944e-06
Iter: 899 loss: 2.06433606e-06
Iter: 900 loss: 2.06104278e-06
Iter: 901 loss: 2.06157665e-06
Iter: 902 loss: 2.05855986e-06
Iter: 903 loss: 2.05482297e-06
Iter: 904 loss: 2.08593565e-06
Iter: 905 loss: 2.05459787e-06
Iter: 906 loss: 2.05174456e-06
Iter: 907 loss: 2.04866637e-06
Iter: 908 loss: 2.04820799e-06
Iter: 909 loss: 2.04366552e-06
Iter: 910 loss: 2.05161405e-06
Iter: 911 loss: 2.04167964e-06
Iter: 912 loss: 2.03739205e-06
Iter: 913 loss: 2.05565698e-06
Iter: 914 loss: 2.03642912e-06
Iter: 915 loss: 2.03219815e-06
Iter: 916 loss: 2.04825392e-06
Iter: 917 loss: 2.03110721e-06
Iter: 918 loss: 2.0272264e-06
Iter: 919 loss: 2.04582125e-06
Iter: 920 loss: 2.02649653e-06
Iter: 921 loss: 2.02305932e-06
Iter: 922 loss: 2.02434376e-06
Iter: 923 loss: 2.02061892e-06
Iter: 924 loss: 2.01664625e-06
Iter: 925 loss: 2.05786455e-06
Iter: 926 loss: 2.01655712e-06
Iter: 927 loss: 2.01368221e-06
Iter: 928 loss: 2.00953923e-06
Iter: 929 loss: 2.00943691e-06
Iter: 930 loss: 2.00507588e-06
Iter: 931 loss: 2.03883565e-06
Iter: 932 loss: 2.0047205e-06
Iter: 933 loss: 2.00077693e-06
Iter: 934 loss: 2.01886724e-06
Iter: 935 loss: 2.00004069e-06
Iter: 936 loss: 1.99693045e-06
Iter: 937 loss: 1.99824e-06
Iter: 938 loss: 1.99480633e-06
Iter: 939 loss: 1.99171768e-06
Iter: 940 loss: 2.03219247e-06
Iter: 941 loss: 1.99171291e-06
Iter: 942 loss: 1.9889676e-06
Iter: 943 loss: 1.99161013e-06
Iter: 944 loss: 1.98740781e-06
Iter: 945 loss: 1.98437465e-06
Iter: 946 loss: 1.98912312e-06
Iter: 947 loss: 1.98295766e-06
Iter: 948 loss: 1.97885879e-06
Iter: 949 loss: 1.98298881e-06
Iter: 950 loss: 1.97651684e-06
Iter: 951 loss: 1.97224881e-06
Iter: 952 loss: 1.97492727e-06
Iter: 953 loss: 1.96950532e-06
Iter: 954 loss: 1.96454494e-06
Iter: 955 loss: 1.98240014e-06
Iter: 956 loss: 1.96324299e-06
Iter: 957 loss: 1.95999951e-06
Iter: 958 loss: 1.9840345e-06
Iter: 959 loss: 1.95974667e-06
Iter: 960 loss: 1.95626785e-06
Iter: 961 loss: 1.96173119e-06
Iter: 962 loss: 1.954666e-06
Iter: 963 loss: 1.95132043e-06
Iter: 964 loss: 1.96151336e-06
Iter: 965 loss: 1.95018151e-06
Iter: 966 loss: 1.94672407e-06
Iter: 967 loss: 1.95743e-06
Iter: 968 loss: 1.94575e-06
Iter: 969 loss: 1.94187328e-06
Iter: 970 loss: 1.94144468e-06
Iter: 971 loss: 1.9387046e-06
Iter: 972 loss: 1.93465212e-06
Iter: 973 loss: 1.95452913e-06
Iter: 974 loss: 1.93389906e-06
Iter: 975 loss: 1.93002506e-06
Iter: 976 loss: 1.95348093e-06
Iter: 977 loss: 1.92957259e-06
Iter: 978 loss: 1.92659263e-06
Iter: 979 loss: 1.92864695e-06
Iter: 980 loss: 1.92477182e-06
Iter: 981 loss: 1.92196649e-06
Iter: 982 loss: 1.92198513e-06
Iter: 983 loss: 1.92005427e-06
Iter: 984 loss: 1.91693243e-06
Iter: 985 loss: 1.91685785e-06
Iter: 986 loss: 1.91358549e-06
Iter: 987 loss: 1.94558788e-06
Iter: 988 loss: 1.91354502e-06
Iter: 989 loss: 1.91067898e-06
Iter: 990 loss: 1.90766036e-06
Iter: 991 loss: 1.90717356e-06
Iter: 992 loss: 1.90274693e-06
Iter: 993 loss: 1.91874165e-06
Iter: 994 loss: 1.90161279e-06
Iter: 995 loss: 1.89779507e-06
Iter: 996 loss: 1.90463982e-06
Iter: 997 loss: 1.89612933e-06
Iter: 998 loss: 1.89155389e-06
Iter: 999 loss: 1.90919e-06
Iter: 1000 loss: 1.89046307e-06
Iter: 1001 loss: 1.8871757e-06
Iter: 1002 loss: 1.93026108e-06
Iter: 1003 loss: 1.88705235e-06
Iter: 1004 loss: 1.88488957e-06
Iter: 1005 loss: 1.88266597e-06
Iter: 1006 loss: 1.88218428e-06
Iter: 1007 loss: 1.8787764e-06
Iter: 1008 loss: 1.91358708e-06
Iter: 1009 loss: 1.87869341e-06
Iter: 1010 loss: 1.87593196e-06
Iter: 1011 loss: 1.87290107e-06
Iter: 1012 loss: 1.87248156e-06
Iter: 1013 loss: 1.8689442e-06
Iter: 1014 loss: 1.90160051e-06
Iter: 1015 loss: 1.8687922e-06
Iter: 1016 loss: 1.8656508e-06
Iter: 1017 loss: 1.87359728e-06
Iter: 1018 loss: 1.86446141e-06
Iter: 1019 loss: 1.86079285e-06
Iter: 1020 loss: 1.8666168e-06
Iter: 1021 loss: 1.85910608e-06
Iter: 1022 loss: 1.85623662e-06
Iter: 1023 loss: 1.85623776e-06
Iter: 1024 loss: 1.85435147e-06
Iter: 1025 loss: 1.85370254e-06
Iter: 1026 loss: 1.85263889e-06
Iter: 1027 loss: 1.85005229e-06
Iter: 1028 loss: 1.85111969e-06
Iter: 1029 loss: 1.84822738e-06
Iter: 1030 loss: 1.84391149e-06
Iter: 1031 loss: 1.8612277e-06
Iter: 1032 loss: 1.84295732e-06
Iter: 1033 loss: 1.84037231e-06
Iter: 1034 loss: 1.84202099e-06
Iter: 1035 loss: 1.83880138e-06
Iter: 1036 loss: 1.83513498e-06
Iter: 1037 loss: 1.83814109e-06
Iter: 1038 loss: 1.83288557e-06
Iter: 1039 loss: 1.82876556e-06
Iter: 1040 loss: 1.84411022e-06
Iter: 1041 loss: 1.82780082e-06
Iter: 1042 loss: 1.82412464e-06
Iter: 1043 loss: 1.87058777e-06
Iter: 1044 loss: 1.82407723e-06
Iter: 1045 loss: 1.82177541e-06
Iter: 1046 loss: 1.81828989e-06
Iter: 1047 loss: 1.81822611e-06
Iter: 1048 loss: 1.81450901e-06
Iter: 1049 loss: 1.87224077e-06
Iter: 1050 loss: 1.81451401e-06
Iter: 1051 loss: 1.81210316e-06
Iter: 1052 loss: 1.80994516e-06
Iter: 1053 loss: 1.80931875e-06
Iter: 1054 loss: 1.80549932e-06
Iter: 1055 loss: 1.81974679e-06
Iter: 1056 loss: 1.80456288e-06
Iter: 1057 loss: 1.80075835e-06
Iter: 1058 loss: 1.82705958e-06
Iter: 1059 loss: 1.80036557e-06
Iter: 1060 loss: 1.79797405e-06
Iter: 1061 loss: 1.79881295e-06
Iter: 1062 loss: 1.79626841e-06
Iter: 1063 loss: 1.79347762e-06
Iter: 1064 loss: 1.81150347e-06
Iter: 1065 loss: 1.79315975e-06
Iter: 1066 loss: 1.79015751e-06
Iter: 1067 loss: 1.79710821e-06
Iter: 1068 loss: 1.789007e-06
Iter: 1069 loss: 1.78640403e-06
Iter: 1070 loss: 1.79790584e-06
Iter: 1071 loss: 1.78584708e-06
Iter: 1072 loss: 1.78339758e-06
Iter: 1073 loss: 1.78415019e-06
Iter: 1074 loss: 1.78174832e-06
Iter: 1075 loss: 1.7784356e-06
Iter: 1076 loss: 1.77743118e-06
Iter: 1077 loss: 1.77543507e-06
Iter: 1078 loss: 1.77155266e-06
Iter: 1079 loss: 1.78728419e-06
Iter: 1080 loss: 1.77068114e-06
Iter: 1081 loss: 1.76728054e-06
Iter: 1082 loss: 1.78669018e-06
Iter: 1083 loss: 1.76678293e-06
Iter: 1084 loss: 1.76393087e-06
Iter: 1085 loss: 1.78181904e-06
Iter: 1086 loss: 1.7636238e-06
Iter: 1087 loss: 1.76094295e-06
Iter: 1088 loss: 1.75992318e-06
Iter: 1089 loss: 1.75847208e-06
Iter: 1090 loss: 1.75533387e-06
Iter: 1091 loss: 1.7803751e-06
Iter: 1092 loss: 1.75513571e-06
Iter: 1093 loss: 1.75223931e-06
Iter: 1094 loss: 1.75398327e-06
Iter: 1095 loss: 1.75045011e-06
Iter: 1096 loss: 1.74746015e-06
Iter: 1097 loss: 1.75012929e-06
Iter: 1098 loss: 1.7457462e-06
Iter: 1099 loss: 1.74249578e-06
Iter: 1100 loss: 1.78495429e-06
Iter: 1101 loss: 1.74248316e-06
Iter: 1102 loss: 1.74039121e-06
Iter: 1103 loss: 1.73875355e-06
Iter: 1104 loss: 1.73810872e-06
Iter: 1105 loss: 1.73481419e-06
Iter: 1106 loss: 1.75953335e-06
Iter: 1107 loss: 1.73449985e-06
Iter: 1108 loss: 1.731976e-06
Iter: 1109 loss: 1.75049195e-06
Iter: 1110 loss: 1.73170861e-06
Iter: 1111 loss: 1.73005049e-06
Iter: 1112 loss: 1.72957198e-06
Iter: 1113 loss: 1.72851344e-06
Iter: 1114 loss: 1.72557043e-06
Iter: 1115 loss: 1.7330176e-06
Iter: 1116 loss: 1.72461523e-06
Iter: 1117 loss: 1.72176874e-06
Iter: 1118 loss: 1.72323257e-06
Iter: 1119 loss: 1.71993236e-06
Iter: 1120 loss: 1.71646991e-06
Iter: 1121 loss: 1.71590955e-06
Iter: 1122 loss: 1.71346028e-06
Iter: 1123 loss: 1.70996611e-06
Iter: 1124 loss: 1.75825039e-06
Iter: 1125 loss: 1.70996134e-06
Iter: 1126 loss: 1.70730038e-06
Iter: 1127 loss: 1.7216837e-06
Iter: 1128 loss: 1.70680914e-06
Iter: 1129 loss: 1.70439296e-06
Iter: 1130 loss: 1.70514204e-06
Iter: 1131 loss: 1.70265821e-06
Iter: 1132 loss: 1.69968212e-06
Iter: 1133 loss: 1.70993121e-06
Iter: 1134 loss: 1.69883742e-06
Iter: 1135 loss: 1.69533075e-06
Iter: 1136 loss: 1.70224189e-06
Iter: 1137 loss: 1.69392e-06
Iter: 1138 loss: 1.69141788e-06
Iter: 1139 loss: 1.69444775e-06
Iter: 1140 loss: 1.69005955e-06
Iter: 1141 loss: 1.68689621e-06
Iter: 1142 loss: 1.70777241e-06
Iter: 1143 loss: 1.68655879e-06
Iter: 1144 loss: 1.68371275e-06
Iter: 1145 loss: 1.68580823e-06
Iter: 1146 loss: 1.6818949e-06
Iter: 1147 loss: 1.67906308e-06
Iter: 1148 loss: 1.68467454e-06
Iter: 1149 loss: 1.67789108e-06
Iter: 1150 loss: 1.67447615e-06
Iter: 1151 loss: 1.69445252e-06
Iter: 1152 loss: 1.67406938e-06
Iter: 1153 loss: 1.67146572e-06
Iter: 1154 loss: 1.6799969e-06
Iter: 1155 loss: 1.6707578e-06
Iter: 1156 loss: 1.66877635e-06
Iter: 1157 loss: 1.68481597e-06
Iter: 1158 loss: 1.66871484e-06
Iter: 1159 loss: 1.66669975e-06
Iter: 1160 loss: 1.6641161e-06
Iter: 1161 loss: 1.66401105e-06
Iter: 1162 loss: 1.66085613e-06
Iter: 1163 loss: 1.66610641e-06
Iter: 1164 loss: 1.65939105e-06
Iter: 1165 loss: 1.6563688e-06
Iter: 1166 loss: 1.66652205e-06
Iter: 1167 loss: 1.65550614e-06
Iter: 1168 loss: 1.65208689e-06
Iter: 1169 loss: 1.67545227e-06
Iter: 1170 loss: 1.65179972e-06
Iter: 1171 loss: 1.64927201e-06
Iter: 1172 loss: 1.64866037e-06
Iter: 1173 loss: 1.64701714e-06
Iter: 1174 loss: 1.64439234e-06
Iter: 1175 loss: 1.67594362e-06
Iter: 1176 loss: 1.64438518e-06
Iter: 1177 loss: 1.64211224e-06
Iter: 1178 loss: 1.63958111e-06
Iter: 1179 loss: 1.63919583e-06
Iter: 1180 loss: 1.6361405e-06
Iter: 1181 loss: 1.65785491e-06
Iter: 1182 loss: 1.63592324e-06
Iter: 1183 loss: 1.63312438e-06
Iter: 1184 loss: 1.64391429e-06
Iter: 1185 loss: 1.63243112e-06
Iter: 1186 loss: 1.62993649e-06
Iter: 1187 loss: 1.63039192e-06
Iter: 1188 loss: 1.62815684e-06
Iter: 1189 loss: 1.62506683e-06
Iter: 1190 loss: 1.64109645e-06
Iter: 1191 loss: 1.62466893e-06
Iter: 1192 loss: 1.6220622e-06
Iter: 1193 loss: 1.63440586e-06
Iter: 1194 loss: 1.6215788e-06
Iter: 1195 loss: 1.6192289e-06
Iter: 1196 loss: 1.61972343e-06
Iter: 1197 loss: 1.61751132e-06
Iter: 1198 loss: 1.61473451e-06
Iter: 1199 loss: 1.65450092e-06
Iter: 1200 loss: 1.61476555e-06
Iter: 1201 loss: 1.61304217e-06
Iter: 1202 loss: 1.61117055e-06
Iter: 1203 loss: 1.61092021e-06
Iter: 1204 loss: 1.60820173e-06
Iter: 1205 loss: 1.61205116e-06
Iter: 1206 loss: 1.60679508e-06
Iter: 1207 loss: 1.60454488e-06
Iter: 1208 loss: 1.63020934e-06
Iter: 1209 loss: 1.60451941e-06
Iter: 1210 loss: 1.60230798e-06
Iter: 1211 loss: 1.60377829e-06
Iter: 1212 loss: 1.60097841e-06
Iter: 1213 loss: 1.59824094e-06
Iter: 1214 loss: 1.60090769e-06
Iter: 1215 loss: 1.59678575e-06
Iter: 1216 loss: 1.59388367e-06
Iter: 1217 loss: 1.61404103e-06
Iter: 1218 loss: 1.59357944e-06
Iter: 1219 loss: 1.59094179e-06
Iter: 1220 loss: 1.59021874e-06
Iter: 1221 loss: 1.58852913e-06
Iter: 1222 loss: 1.5854755e-06
Iter: 1223 loss: 1.60569448e-06
Iter: 1224 loss: 1.58513103e-06
Iter: 1225 loss: 1.58270404e-06
Iter: 1226 loss: 1.59539184e-06
Iter: 1227 loss: 1.58235059e-06
Iter: 1228 loss: 1.58009402e-06
Iter: 1229 loss: 1.57807222e-06
Iter: 1230 loss: 1.57753948e-06
Iter: 1231 loss: 1.57445106e-06
Iter: 1232 loss: 1.60569846e-06
Iter: 1233 loss: 1.57433374e-06
Iter: 1234 loss: 1.57226884e-06
Iter: 1235 loss: 1.58165e-06
Iter: 1236 loss: 1.57186014e-06
Iter: 1237 loss: 1.5697683e-06
Iter: 1238 loss: 1.57270404e-06
Iter: 1239 loss: 1.56874376e-06
Iter: 1240 loss: 1.56626425e-06
Iter: 1241 loss: 1.58106832e-06
Iter: 1242 loss: 1.56606256e-06
Iter: 1243 loss: 1.5644257e-06
Iter: 1244 loss: 1.56198575e-06
Iter: 1245 loss: 1.56191356e-06
Iter: 1246 loss: 1.55875546e-06
Iter: 1247 loss: 1.5647887e-06
Iter: 1248 loss: 1.55745931e-06
Iter: 1249 loss: 1.55459225e-06
Iter: 1250 loss: 1.58793637e-06
Iter: 1251 loss: 1.55453108e-06
Iter: 1252 loss: 1.55173439e-06
Iter: 1253 loss: 1.55246357e-06
Iter: 1254 loss: 1.5497028e-06
Iter: 1255 loss: 1.54718009e-06
Iter: 1256 loss: 1.55733369e-06
Iter: 1257 loss: 1.54652275e-06
Iter: 1258 loss: 1.54426266e-06
Iter: 1259 loss: 1.55451471e-06
Iter: 1260 loss: 1.54382758e-06
Iter: 1261 loss: 1.54134773e-06
Iter: 1262 loss: 1.53979795e-06
Iter: 1263 loss: 1.53879432e-06
Iter: 1264 loss: 1.5361951e-06
Iter: 1265 loss: 1.56664225e-06
Iter: 1266 loss: 1.53610381e-06
Iter: 1267 loss: 1.53383189e-06
Iter: 1268 loss: 1.5362815e-06
Iter: 1269 loss: 1.53257702e-06
Iter: 1270 loss: 1.52965e-06
Iter: 1271 loss: 1.53194446e-06
Iter: 1272 loss: 1.52781092e-06
Iter: 1273 loss: 1.52569146e-06
Iter: 1274 loss: 1.55296857e-06
Iter: 1275 loss: 1.52567168e-06
Iter: 1276 loss: 1.52354221e-06
Iter: 1277 loss: 1.52327175e-06
Iter: 1278 loss: 1.52174516e-06
Iter: 1279 loss: 1.51964252e-06
Iter: 1280 loss: 1.51962354e-06
Iter: 1281 loss: 1.51808217e-06
Iter: 1282 loss: 1.51555957e-06
Iter: 1283 loss: 1.51553263e-06
Iter: 1284 loss: 1.51252766e-06
Iter: 1285 loss: 1.52285293e-06
Iter: 1286 loss: 1.51179029e-06
Iter: 1287 loss: 1.50909659e-06
Iter: 1288 loss: 1.51097493e-06
Iter: 1289 loss: 1.50743631e-06
Iter: 1290 loss: 1.50524147e-06
Iter: 1291 loss: 1.50520793e-06
Iter: 1292 loss: 1.5033545e-06
Iter: 1293 loss: 1.50102471e-06
Iter: 1294 loss: 1.50080064e-06
Iter: 1295 loss: 1.4981938e-06
Iter: 1296 loss: 1.51802328e-06
Iter: 1297 loss: 1.49802838e-06
Iter: 1298 loss: 1.4956338e-06
Iter: 1299 loss: 1.50052051e-06
Iter: 1300 loss: 1.49465984e-06
Iter: 1301 loss: 1.49246671e-06
Iter: 1302 loss: 1.49491223e-06
Iter: 1303 loss: 1.49126731e-06
Iter: 1304 loss: 1.48842378e-06
Iter: 1305 loss: 1.49715163e-06
Iter: 1306 loss: 1.48753861e-06
Iter: 1307 loss: 1.48490381e-06
Iter: 1308 loss: 1.50158985e-06
Iter: 1309 loss: 1.48457707e-06
Iter: 1310 loss: 1.48267145e-06
Iter: 1311 loss: 1.48288882e-06
Iter: 1312 loss: 1.48117988e-06
Iter: 1313 loss: 1.47870276e-06
Iter: 1314 loss: 1.49093796e-06
Iter: 1315 loss: 1.47827438e-06
Iter: 1316 loss: 1.47578271e-06
Iter: 1317 loss: 1.49040852e-06
Iter: 1318 loss: 1.47548883e-06
Iter: 1319 loss: 1.47391017e-06
Iter: 1320 loss: 1.47806213e-06
Iter: 1321 loss: 1.47333799e-06
Iter: 1322 loss: 1.47138803e-06
Iter: 1323 loss: 1.47082403e-06
Iter: 1324 loss: 1.46965533e-06
Iter: 1325 loss: 1.46765615e-06
Iter: 1326 loss: 1.46881916e-06
Iter: 1327 loss: 1.46635375e-06
Iter: 1328 loss: 1.46354341e-06
Iter: 1329 loss: 1.47045375e-06
Iter: 1330 loss: 1.46240745e-06
Iter: 1331 loss: 1.4603728e-06
Iter: 1332 loss: 1.46035518e-06
Iter: 1333 loss: 1.45846036e-06
Iter: 1334 loss: 1.45760691e-06
Iter: 1335 loss: 1.45661716e-06
Iter: 1336 loss: 1.45419631e-06
Iter: 1337 loss: 1.45836498e-06
Iter: 1338 loss: 1.45306478e-06
Iter: 1339 loss: 1.4507184e-06
Iter: 1340 loss: 1.47772732e-06
Iter: 1341 loss: 1.45066815e-06
Iter: 1342 loss: 1.44892829e-06
Iter: 1343 loss: 1.44679666e-06
Iter: 1344 loss: 1.44658975e-06
Iter: 1345 loss: 1.4440451e-06
Iter: 1346 loss: 1.47122978e-06
Iter: 1347 loss: 1.44392061e-06
Iter: 1348 loss: 1.4418415e-06
Iter: 1349 loss: 1.44627052e-06
Iter: 1350 loss: 1.44098487e-06
Iter: 1351 loss: 1.43905606e-06
Iter: 1352 loss: 1.44069531e-06
Iter: 1353 loss: 1.4378719e-06
Iter: 1354 loss: 1.43590012e-06
Iter: 1355 loss: 1.46338971e-06
Iter: 1356 loss: 1.43588454e-06
Iter: 1357 loss: 1.43425359e-06
Iter: 1358 loss: 1.43502962e-06
Iter: 1359 loss: 1.43318925e-06
Iter: 1360 loss: 1.4310815e-06
Iter: 1361 loss: 1.43643433e-06
Iter: 1362 loss: 1.43032457e-06
Iter: 1363 loss: 1.42825729e-06
Iter: 1364 loss: 1.42833437e-06
Iter: 1365 loss: 1.42654801e-06
Iter: 1366 loss: 1.42383851e-06
Iter: 1367 loss: 1.43226873e-06
Iter: 1368 loss: 1.42307408e-06
Iter: 1369 loss: 1.42084104e-06
Iter: 1370 loss: 1.42429917e-06
Iter: 1371 loss: 1.41976e-06
Iter: 1372 loss: 1.41717078e-06
Iter: 1373 loss: 1.44510659e-06
Iter: 1374 loss: 1.41712076e-06
Iter: 1375 loss: 1.41545172e-06
Iter: 1376 loss: 1.41487885e-06
Iter: 1377 loss: 1.41390774e-06
Iter: 1378 loss: 1.41162445e-06
Iter: 1379 loss: 1.41511418e-06
Iter: 1380 loss: 1.41052419e-06
Iter: 1381 loss: 1.40777274e-06
Iter: 1382 loss: 1.42766089e-06
Iter: 1383 loss: 1.40752388e-06
Iter: 1384 loss: 1.40558086e-06
Iter: 1385 loss: 1.40910083e-06
Iter: 1386 loss: 1.40473048e-06
Iter: 1387 loss: 1.4028401e-06
Iter: 1388 loss: 1.40505904e-06
Iter: 1389 loss: 1.4017412e-06
Iter: 1390 loss: 1.3992194e-06
Iter: 1391 loss: 1.41325199e-06
Iter: 1392 loss: 1.39882638e-06
Iter: 1393 loss: 1.39692906e-06
Iter: 1394 loss: 1.40274699e-06
Iter: 1395 loss: 1.39633903e-06
Iter: 1396 loss: 1.39445615e-06
Iter: 1397 loss: 1.40574116e-06
Iter: 1398 loss: 1.39426709e-06
Iter: 1399 loss: 1.39254894e-06
Iter: 1400 loss: 1.39018994e-06
Iter: 1401 loss: 1.39005101e-06
Iter: 1402 loss: 1.38760186e-06
Iter: 1403 loss: 1.41131682e-06
Iter: 1404 loss: 1.38754194e-06
Iter: 1405 loss: 1.38554674e-06
Iter: 1406 loss: 1.38837345e-06
Iter: 1407 loss: 1.38461928e-06
Iter: 1408 loss: 1.38256314e-06
Iter: 1409 loss: 1.38193934e-06
Iter: 1410 loss: 1.38070527e-06
Iter: 1411 loss: 1.37771883e-06
Iter: 1412 loss: 1.3874253e-06
Iter: 1413 loss: 1.37693689e-06
Iter: 1414 loss: 1.37470829e-06
Iter: 1415 loss: 1.40751899e-06
Iter: 1416 loss: 1.37470465e-06
Iter: 1417 loss: 1.37273787e-06
Iter: 1418 loss: 1.37286372e-06
Iter: 1419 loss: 1.37118582e-06
Iter: 1420 loss: 1.36907568e-06
Iter: 1421 loss: 1.36835376e-06
Iter: 1422 loss: 1.36720246e-06
Iter: 1423 loss: 1.36513358e-06
Iter: 1424 loss: 1.36512847e-06
Iter: 1425 loss: 1.36322171e-06
Iter: 1426 loss: 1.36279164e-06
Iter: 1427 loss: 1.36159815e-06
Iter: 1428 loss: 1.35939058e-06
Iter: 1429 loss: 1.36847552e-06
Iter: 1430 loss: 1.35890912e-06
Iter: 1431 loss: 1.35701771e-06
Iter: 1432 loss: 1.36984772e-06
Iter: 1433 loss: 1.35674918e-06
Iter: 1434 loss: 1.35513937e-06
Iter: 1435 loss: 1.35766959e-06
Iter: 1436 loss: 1.35436585e-06
Iter: 1437 loss: 1.352244e-06
Iter: 1438 loss: 1.36027199e-06
Iter: 1439 loss: 1.35178072e-06
Iter: 1440 loss: 1.35017342e-06
Iter: 1441 loss: 1.3494041e-06
Iter: 1442 loss: 1.34866514e-06
Iter: 1443 loss: 1.34649326e-06
Iter: 1444 loss: 1.35763833e-06
Iter: 1445 loss: 1.34619415e-06
Iter: 1446 loss: 1.34405013e-06
Iter: 1447 loss: 1.34739958e-06
Iter: 1448 loss: 1.34307948e-06
Iter: 1449 loss: 1.34104198e-06
Iter: 1450 loss: 1.34273182e-06
Iter: 1451 loss: 1.33977255e-06
Iter: 1452 loss: 1.33737421e-06
Iter: 1453 loss: 1.34884579e-06
Iter: 1454 loss: 1.33692117e-06
Iter: 1455 loss: 1.33509616e-06
Iter: 1456 loss: 1.35419305e-06
Iter: 1457 loss: 1.33506921e-06
Iter: 1458 loss: 1.33353353e-06
Iter: 1459 loss: 1.33094977e-06
Iter: 1460 loss: 1.33098649e-06
Iter: 1461 loss: 1.32855894e-06
Iter: 1462 loss: 1.34865979e-06
Iter: 1463 loss: 1.32844195e-06
Iter: 1464 loss: 1.32662103e-06
Iter: 1465 loss: 1.33095182e-06
Iter: 1466 loss: 1.32590174e-06
Iter: 1467 loss: 1.32357593e-06
Iter: 1468 loss: 1.33329627e-06
Iter: 1469 loss: 1.32310174e-06
Iter: 1470 loss: 1.32154287e-06
Iter: 1471 loss: 1.32338539e-06
Iter: 1472 loss: 1.32073887e-06
Iter: 1473 loss: 1.31859531e-06
Iter: 1474 loss: 1.33104186e-06
Iter: 1475 loss: 1.31831985e-06
Iter: 1476 loss: 1.31694412e-06
Iter: 1477 loss: 1.31873139e-06
Iter: 1478 loss: 1.31623165e-06
Iter: 1479 loss: 1.31453328e-06
Iter: 1480 loss: 1.3154712e-06
Iter: 1481 loss: 1.31337765e-06
Iter: 1482 loss: 1.31111904e-06
Iter: 1483 loss: 1.31448792e-06
Iter: 1484 loss: 1.310024e-06
Iter: 1485 loss: 1.30790431e-06
Iter: 1486 loss: 1.32184778e-06
Iter: 1487 loss: 1.30771718e-06
Iter: 1488 loss: 1.30608373e-06
Iter: 1489 loss: 1.31015315e-06
Iter: 1490 loss: 1.30556884e-06
Iter: 1491 loss: 1.3036115e-06
Iter: 1492 loss: 1.30275203e-06
Iter: 1493 loss: 1.30178228e-06
Iter: 1494 loss: 1.29971045e-06
Iter: 1495 loss: 1.31984223e-06
Iter: 1496 loss: 1.29958948e-06
Iter: 1497 loss: 1.2978e-06
Iter: 1498 loss: 1.306076e-06
Iter: 1499 loss: 1.29750333e-06
Iter: 1500 loss: 1.29587397e-06
Iter: 1501 loss: 1.2928964e-06
Iter: 1502 loss: 1.36428696e-06
Iter: 1503 loss: 1.29289856e-06
Iter: 1504 loss: 1.29017246e-06
Iter: 1505 loss: 1.303624e-06
Iter: 1506 loss: 1.28965462e-06
Iter: 1507 loss: 1.2879251e-06
Iter: 1508 loss: 1.28791885e-06
Iter: 1509 loss: 1.28647957e-06
Iter: 1510 loss: 1.28724014e-06
Iter: 1511 loss: 1.28558838e-06
Iter: 1512 loss: 1.28385909e-06
Iter: 1513 loss: 1.29452644e-06
Iter: 1514 loss: 1.28362717e-06
Iter: 1515 loss: 1.28194642e-06
Iter: 1516 loss: 1.28253168e-06
Iter: 1517 loss: 1.28080137e-06
Iter: 1518 loss: 1.27918622e-06
Iter: 1519 loss: 1.27996088e-06
Iter: 1520 loss: 1.27814405e-06
Iter: 1521 loss: 1.27597968e-06
Iter: 1522 loss: 1.29330056e-06
Iter: 1523 loss: 1.2759026e-06
Iter: 1524 loss: 1.27440944e-06
Iter: 1525 loss: 1.27364933e-06
Iter: 1526 loss: 1.2730203e-06
Iter: 1527 loss: 1.27068324e-06
Iter: 1528 loss: 1.27728242e-06
Iter: 1529 loss: 1.26999078e-06
Iter: 1530 loss: 1.26726104e-06
Iter: 1531 loss: 1.28067063e-06
Iter: 1532 loss: 1.26686291e-06
Iter: 1533 loss: 1.26512214e-06
Iter: 1534 loss: 1.26723148e-06
Iter: 1535 loss: 1.26417854e-06
Iter: 1536 loss: 1.26241821e-06
Iter: 1537 loss: 1.27695989e-06
Iter: 1538 loss: 1.26228974e-06
Iter: 1539 loss: 1.26070563e-06
Iter: 1540 loss: 1.26179e-06
Iter: 1541 loss: 1.2596571e-06
Iter: 1542 loss: 1.25787619e-06
Iter: 1543 loss: 1.25951749e-06
Iter: 1544 loss: 1.25687e-06
Iter: 1545 loss: 1.25478e-06
Iter: 1546 loss: 1.25946508e-06
Iter: 1547 loss: 1.25392194e-06
Iter: 1548 loss: 1.25230804e-06
Iter: 1549 loss: 1.2523202e-06
Iter: 1550 loss: 1.2509006e-06
Iter: 1551 loss: 1.25016959e-06
Iter: 1552 loss: 1.24954954e-06
Iter: 1553 loss: 1.2476396e-06
Iter: 1554 loss: 1.26352916e-06
Iter: 1555 loss: 1.24750511e-06
Iter: 1556 loss: 1.24607391e-06
Iter: 1557 loss: 1.24408677e-06
Iter: 1558 loss: 1.24401276e-06
Iter: 1559 loss: 1.24195572e-06
Iter: 1560 loss: 1.25547558e-06
Iter: 1561 loss: 1.24169901e-06
Iter: 1562 loss: 1.24006249e-06
Iter: 1563 loss: 1.25135284e-06
Iter: 1564 loss: 1.24001e-06
Iter: 1565 loss: 1.23855887e-06
Iter: 1566 loss: 1.23737e-06
Iter: 1567 loss: 1.23693712e-06
Iter: 1568 loss: 1.23488098e-06
Iter: 1569 loss: 1.24517692e-06
Iter: 1570 loss: 1.23451809e-06
Iter: 1571 loss: 1.23260861e-06
Iter: 1572 loss: 1.2414414e-06
Iter: 1573 loss: 1.23230188e-06
Iter: 1574 loss: 1.23044083e-06
Iter: 1575 loss: 1.2314315e-06
Iter: 1576 loss: 1.22920062e-06
Iter: 1577 loss: 1.22706285e-06
Iter: 1578 loss: 1.24086591e-06
Iter: 1579 loss: 1.22683628e-06
Iter: 1580 loss: 1.22507959e-06
Iter: 1581 loss: 1.23024824e-06
Iter: 1582 loss: 1.22453173e-06
Iter: 1583 loss: 1.22298172e-06
Iter: 1584 loss: 1.22282222e-06
Iter: 1585 loss: 1.22166557e-06
Iter: 1586 loss: 1.21974813e-06
Iter: 1587 loss: 1.23225118e-06
Iter: 1588 loss: 1.21954281e-06
Iter: 1589 loss: 1.21786138e-06
Iter: 1590 loss: 1.23219343e-06
Iter: 1591 loss: 1.21777066e-06
Iter: 1592 loss: 1.21671133e-06
Iter: 1593 loss: 1.21602989e-06
Iter: 1594 loss: 1.2155981e-06
Iter: 1595 loss: 1.21388098e-06
Iter: 1596 loss: 1.2210204e-06
Iter: 1597 loss: 1.21358016e-06
Iter: 1598 loss: 1.21169819e-06
Iter: 1599 loss: 1.21084747e-06
Iter: 1600 loss: 1.2099548e-06
Iter: 1601 loss: 1.20803486e-06
Iter: 1602 loss: 1.20987261e-06
Iter: 1603 loss: 1.20698428e-06
Iter: 1604 loss: 1.20468053e-06
Iter: 1605 loss: 1.23242444e-06
Iter: 1606 loss: 1.20462778e-06
Iter: 1607 loss: 1.20299956e-06
Iter: 1608 loss: 1.20252912e-06
Iter: 1609 loss: 1.20157392e-06
Iter: 1610 loss: 1.19981883e-06
Iter: 1611 loss: 1.20644461e-06
Iter: 1612 loss: 1.19942979e-06
Iter: 1613 loss: 1.19758852e-06
Iter: 1614 loss: 1.20822438e-06
Iter: 1615 loss: 1.19729407e-06
Iter: 1616 loss: 1.19588049e-06
Iter: 1617 loss: 1.19769072e-06
Iter: 1618 loss: 1.19508684e-06
Iter: 1619 loss: 1.19347089e-06
Iter: 1620 loss: 1.20010327e-06
Iter: 1621 loss: 1.19316508e-06
Iter: 1622 loss: 1.19148274e-06
Iter: 1623 loss: 1.1922632e-06
Iter: 1624 loss: 1.19035735e-06
Iter: 1625 loss: 1.18871606e-06
Iter: 1626 loss: 1.19828485e-06
Iter: 1627 loss: 1.18851028e-06
Iter: 1628 loss: 1.18695971e-06
Iter: 1629 loss: 1.19614356e-06
Iter: 1630 loss: 1.18678486e-06
Iter: 1631 loss: 1.1855152e-06
Iter: 1632 loss: 1.18423236e-06
Iter: 1633 loss: 1.18393359e-06
Iter: 1634 loss: 1.18239109e-06
Iter: 1635 loss: 1.19289439e-06
Iter: 1636 loss: 1.18222044e-06
Iter: 1637 loss: 1.18069829e-06
Iter: 1638 loss: 1.18361731e-06
Iter: 1639 loss: 1.18005084e-06
Iter: 1640 loss: 1.17848435e-06
Iter: 1641 loss: 1.17772879e-06
Iter: 1642 loss: 1.17702132e-06
Iter: 1643 loss: 1.17496791e-06
Iter: 1644 loss: 1.18819696e-06
Iter: 1645 loss: 1.17474553e-06
Iter: 1646 loss: 1.17311879e-06
Iter: 1647 loss: 1.18447849e-06
Iter: 1648 loss: 1.1729835e-06
Iter: 1649 loss: 1.17158925e-06
Iter: 1650 loss: 1.17055424e-06
Iter: 1651 loss: 1.17004629e-06
Iter: 1652 loss: 1.16820365e-06
Iter: 1653 loss: 1.17879119e-06
Iter: 1654 loss: 1.16788885e-06
Iter: 1655 loss: 1.16636147e-06
Iter: 1656 loss: 1.17891216e-06
Iter: 1657 loss: 1.16627211e-06
Iter: 1658 loss: 1.16500041e-06
Iter: 1659 loss: 1.16389697e-06
Iter: 1660 loss: 1.16354875e-06
Iter: 1661 loss: 1.1619295e-06
Iter: 1662 loss: 1.17692537e-06
Iter: 1663 loss: 1.16185743e-06
Iter: 1664 loss: 1.16046465e-06
Iter: 1665 loss: 1.16296519e-06
Iter: 1666 loss: 1.15988098e-06
Iter: 1667 loss: 1.15839498e-06
Iter: 1668 loss: 1.16790625e-06
Iter: 1669 loss: 1.15822763e-06
Iter: 1670 loss: 1.15698697e-06
Iter: 1671 loss: 1.15792443e-06
Iter: 1672 loss: 1.15617581e-06
Iter: 1673 loss: 1.1548459e-06
Iter: 1674 loss: 1.15420812e-06
Iter: 1675 loss: 1.1534928e-06
Iter: 1676 loss: 1.15156979e-06
Iter: 1677 loss: 1.16683884e-06
Iter: 1678 loss: 1.15144007e-06
Iter: 1679 loss: 1.15005423e-06
Iter: 1680 loss: 1.15773855e-06
Iter: 1681 loss: 1.14986301e-06
Iter: 1682 loss: 1.14871159e-06
Iter: 1683 loss: 1.14609e-06
Iter: 1684 loss: 1.18530988e-06
Iter: 1685 loss: 1.1460304e-06
Iter: 1686 loss: 1.14379247e-06
Iter: 1687 loss: 1.1721686e-06
Iter: 1688 loss: 1.14376519e-06
Iter: 1689 loss: 1.14223667e-06
Iter: 1690 loss: 1.15550426e-06
Iter: 1691 loss: 1.14220347e-06
Iter: 1692 loss: 1.14087857e-06
Iter: 1693 loss: 1.13967735e-06
Iter: 1694 loss: 1.13935903e-06
Iter: 1695 loss: 1.13754891e-06
Iter: 1696 loss: 1.15064495e-06
Iter: 1697 loss: 1.13741032e-06
Iter: 1698 loss: 1.13577505e-06
Iter: 1699 loss: 1.14293641e-06
Iter: 1700 loss: 1.13546048e-06
Iter: 1701 loss: 1.13414262e-06
Iter: 1702 loss: 1.13400426e-06
Iter: 1703 loss: 1.13302212e-06
Iter: 1704 loss: 1.13168903e-06
Iter: 1705 loss: 1.15038074e-06
Iter: 1706 loss: 1.13170597e-06
Iter: 1707 loss: 1.13043109e-06
Iter: 1708 loss: 1.13060844e-06
Iter: 1709 loss: 1.12948101e-06
Iter: 1710 loss: 1.12808561e-06
Iter: 1711 loss: 1.13417593e-06
Iter: 1712 loss: 1.12783687e-06
Iter: 1713 loss: 1.12636894e-06
Iter: 1714 loss: 1.12471844e-06
Iter: 1715 loss: 1.12452733e-06
Iter: 1716 loss: 1.12271664e-06
Iter: 1717 loss: 1.13713884e-06
Iter: 1718 loss: 1.12255543e-06
Iter: 1719 loss: 1.12108864e-06
Iter: 1720 loss: 1.12940359e-06
Iter: 1721 loss: 1.12087264e-06
Iter: 1722 loss: 1.11954398e-06
Iter: 1723 loss: 1.11847976e-06
Iter: 1724 loss: 1.11806605e-06
Iter: 1725 loss: 1.1160829e-06
Iter: 1726 loss: 1.12470889e-06
Iter: 1727 loss: 1.11564793e-06
Iter: 1728 loss: 1.1139349e-06
Iter: 1729 loss: 1.11816735e-06
Iter: 1730 loss: 1.11331019e-06
Iter: 1731 loss: 1.11160932e-06
Iter: 1732 loss: 1.12544376e-06
Iter: 1733 loss: 1.11153179e-06
Iter: 1734 loss: 1.11017982e-06
Iter: 1735 loss: 1.10964584e-06
Iter: 1736 loss: 1.10891358e-06
Iter: 1737 loss: 1.10738824e-06
Iter: 1738 loss: 1.12059206e-06
Iter: 1739 loss: 1.10727296e-06
Iter: 1740 loss: 1.10573887e-06
Iter: 1741 loss: 1.10702535e-06
Iter: 1742 loss: 1.10482392e-06
Iter: 1743 loss: 1.10340443e-06
Iter: 1744 loss: 1.11090503e-06
Iter: 1745 loss: 1.10321446e-06
Iter: 1746 loss: 1.10161477e-06
Iter: 1747 loss: 1.10527526e-06
Iter: 1748 loss: 1.10102576e-06
Iter: 1749 loss: 1.09975804e-06
Iter: 1750 loss: 1.10087558e-06
Iter: 1751 loss: 1.09907774e-06
Iter: 1752 loss: 1.09770917e-06
Iter: 1753 loss: 1.10057624e-06
Iter: 1754 loss: 1.09718508e-06
Iter: 1755 loss: 1.09534903e-06
Iter: 1756 loss: 1.09841892e-06
Iter: 1757 loss: 1.09453254e-06
Iter: 1758 loss: 1.09310213e-06
Iter: 1759 loss: 1.09901418e-06
Iter: 1760 loss: 1.09277153e-06
Iter: 1761 loss: 1.091331e-06
Iter: 1762 loss: 1.09649932e-06
Iter: 1763 loss: 1.091017e-06
Iter: 1764 loss: 1.08943198e-06
Iter: 1765 loss: 1.08950155e-06
Iter: 1766 loss: 1.08817198e-06
Iter: 1767 loss: 1.08640381e-06
Iter: 1768 loss: 1.09066536e-06
Iter: 1769 loss: 1.0857774e-06
Iter: 1770 loss: 1.0841668e-06
Iter: 1771 loss: 1.09063262e-06
Iter: 1772 loss: 1.08371148e-06
Iter: 1773 loss: 1.0818984e-06
Iter: 1774 loss: 1.09076336e-06
Iter: 1775 loss: 1.08162078e-06
Iter: 1776 loss: 1.08036681e-06
Iter: 1777 loss: 1.08137465e-06
Iter: 1778 loss: 1.07963626e-06
Iter: 1779 loss: 1.07816618e-06
Iter: 1780 loss: 1.08878828e-06
Iter: 1781 loss: 1.07811286e-06
Iter: 1782 loss: 1.07678375e-06
Iter: 1783 loss: 1.07862263e-06
Iter: 1784 loss: 1.07618609e-06
Iter: 1785 loss: 1.07499659e-06
Iter: 1786 loss: 1.08459881e-06
Iter: 1787 loss: 1.07492974e-06
Iter: 1788 loss: 1.07382562e-06
Iter: 1789 loss: 1.07228311e-06
Iter: 1790 loss: 1.07226595e-06
Iter: 1791 loss: 1.07072378e-06
Iter: 1792 loss: 1.07480332e-06
Iter: 1793 loss: 1.07026221e-06
Iter: 1794 loss: 1.06836387e-06
Iter: 1795 loss: 1.07593644e-06
Iter: 1796 loss: 1.06796529e-06
Iter: 1797 loss: 1.06642915e-06
Iter: 1798 loss: 1.0712813e-06
Iter: 1799 loss: 1.06597577e-06
Iter: 1800 loss: 1.06460914e-06
Iter: 1801 loss: 1.06867469e-06
Iter: 1802 loss: 1.06424454e-06
Iter: 1803 loss: 1.06288485e-06
Iter: 1804 loss: 1.06761422e-06
Iter: 1805 loss: 1.06252799e-06
Iter: 1806 loss: 1.06125117e-06
Iter: 1807 loss: 1.06196217e-06
Iter: 1808 loss: 1.06033826e-06
Iter: 1809 loss: 1.05874778e-06
Iter: 1810 loss: 1.05957986e-06
Iter: 1811 loss: 1.05770789e-06
Iter: 1812 loss: 1.05616709e-06
Iter: 1813 loss: 1.07787218e-06
Iter: 1814 loss: 1.05615493e-06
Iter: 1815 loss: 1.0548124e-06
Iter: 1816 loss: 1.05706908e-06
Iter: 1817 loss: 1.05420008e-06
Iter: 1818 loss: 1.05294907e-06
Iter: 1819 loss: 1.0555018e-06
Iter: 1820 loss: 1.05246716e-06
Iter: 1821 loss: 1.0508503e-06
Iter: 1822 loss: 1.0572553e-06
Iter: 1823 loss: 1.05049492e-06
Iter: 1824 loss: 1.04919377e-06
Iter: 1825 loss: 1.05245863e-06
Iter: 1826 loss: 1.04868036e-06
Iter: 1827 loss: 1.04734033e-06
Iter: 1828 loss: 1.0502938e-06
Iter: 1829 loss: 1.04679293e-06
Iter: 1830 loss: 1.04557557e-06
Iter: 1831 loss: 1.0450492e-06
Iter: 1832 loss: 1.0444096e-06
Iter: 1833 loss: 1.04258572e-06
Iter: 1834 loss: 1.0466141e-06
Iter: 1835 loss: 1.04186222e-06
Iter: 1836 loss: 1.04033722e-06
Iter: 1837 loss: 1.06143511e-06
Iter: 1838 loss: 1.04034143e-06
Iter: 1839 loss: 1.03913305e-06
Iter: 1840 loss: 1.03870047e-06
Iter: 1841 loss: 1.03799175e-06
Iter: 1842 loss: 1.03647812e-06
Iter: 1843 loss: 1.04631772e-06
Iter: 1844 loss: 1.03631555e-06
Iter: 1845 loss: 1.03484876e-06
Iter: 1846 loss: 1.03836987e-06
Iter: 1847 loss: 1.0343482e-06
Iter: 1848 loss: 1.03307843e-06
Iter: 1849 loss: 1.03339687e-06
Iter: 1850 loss: 1.03217758e-06
Iter: 1851 loss: 1.03033631e-06
Iter: 1852 loss: 1.0358533e-06
Iter: 1853 loss: 1.02978822e-06
Iter: 1854 loss: 1.02852641e-06
Iter: 1855 loss: 1.04220862e-06
Iter: 1856 loss: 1.02851789e-06
Iter: 1857 loss: 1.02727859e-06
Iter: 1858 loss: 1.02700858e-06
Iter: 1859 loss: 1.02617946e-06
Iter: 1860 loss: 1.02511876e-06
Iter: 1861 loss: 1.0414991e-06
Iter: 1862 loss: 1.02512331e-06
Iter: 1863 loss: 1.02410593e-06
Iter: 1864 loss: 1.02412594e-06
Iter: 1865 loss: 1.02341517e-06
Iter: 1866 loss: 1.02209435e-06
Iter: 1867 loss: 1.02532977e-06
Iter: 1868 loss: 1.02162164e-06
Iter: 1869 loss: 1.02009653e-06
Iter: 1870 loss: 1.02232968e-06
Iter: 1871 loss: 1.01938622e-06
Iter: 1872 loss: 1.01794717e-06
Iter: 1873 loss: 1.01924866e-06
Iter: 1874 loss: 1.01714886e-06
Iter: 1875 loss: 1.01558476e-06
Iter: 1876 loss: 1.0200946e-06
Iter: 1877 loss: 1.01508249e-06
Iter: 1878 loss: 1.01341811e-06
Iter: 1879 loss: 1.02022375e-06
Iter: 1880 loss: 1.01302942e-06
Iter: 1881 loss: 1.01139403e-06
Iter: 1882 loss: 1.01923206e-06
Iter: 1883 loss: 1.01110095e-06
Iter: 1884 loss: 1.01001251e-06
Iter: 1885 loss: 1.01019327e-06
Iter: 1886 loss: 1.00918191e-06
Iter: 1887 loss: 1.00773116e-06
Iter: 1888 loss: 1.01838486e-06
Iter: 1889 loss: 1.00756688e-06
Iter: 1890 loss: 1.00639522e-06
Iter: 1891 loss: 1.00597254e-06
Iter: 1892 loss: 1.0053277e-06
Iter: 1893 loss: 1.00361081e-06
Iter: 1894 loss: 1.00628972e-06
Iter: 1895 loss: 1.0029122e-06
Iter: 1896 loss: 1.00149373e-06
Iter: 1897 loss: 1.00145621e-06
Iter: 1898 loss: 1.00052193e-06
Iter: 1899 loss: 1.00119e-06
Iter: 1900 loss: 9.99912913e-07
Iter: 1901 loss: 9.98793666e-07
Iter: 1902 loss: 1.00585589e-06
Iter: 1903 loss: 9.98644623e-07
Iter: 1904 loss: 9.97672601e-07
Iter: 1905 loss: 9.97225129e-07
Iter: 1906 loss: 9.96747303e-07
Iter: 1907 loss: 9.95574851e-07
Iter: 1908 loss: 1.00105899e-06
Iter: 1909 loss: 9.95348387e-07
Iter: 1910 loss: 9.93975846e-07
Iter: 1911 loss: 9.94067e-07
Iter: 1912 loss: 9.92902e-07
Iter: 1913 loss: 9.91514753e-07
Iter: 1914 loss: 9.95559276e-07
Iter: 1915 loss: 9.91082629e-07
Iter: 1916 loss: 9.89719e-07
Iter: 1917 loss: 9.91507704e-07
Iter: 1918 loss: 9.89061732e-07
Iter: 1919 loss: 9.8735336e-07
Iter: 1920 loss: 1.00057775e-06
Iter: 1921 loss: 9.87221824e-07
Iter: 1922 loss: 9.85992415e-07
Iter: 1923 loss: 9.88968623e-07
Iter: 1924 loss: 9.85569159e-07
Iter: 1925 loss: 9.8430155e-07
Iter: 1926 loss: 9.83909217e-07
Iter: 1927 loss: 9.83129439e-07
Iter: 1928 loss: 9.81717903e-07
Iter: 1929 loss: 1.00016348e-06
Iter: 1930 loss: 9.81727908e-07
Iter: 1931 loss: 9.80580808e-07
Iter: 1932 loss: 9.79950073e-07
Iter: 1933 loss: 9.7943348e-07
Iter: 1934 loss: 9.77908712e-07
Iter: 1935 loss: 9.86340069e-07
Iter: 1936 loss: 9.77679747e-07
Iter: 1937 loss: 9.7630209e-07
Iter: 1938 loss: 9.88969873e-07
Iter: 1939 loss: 9.76286856e-07
Iter: 1940 loss: 9.75530384e-07
Iter: 1941 loss: 9.75817329e-07
Iter: 1942 loss: 9.7495365e-07
Iter: 1943 loss: 9.73559e-07
Iter: 1944 loss: 9.74644081e-07
Iter: 1945 loss: 9.72701628e-07
Iter: 1946 loss: 9.71482336e-07
Iter: 1947 loss: 9.75226158e-07
Iter: 1948 loss: 9.71097e-07
Iter: 1949 loss: 9.69979283e-07
Iter: 1950 loss: 9.72995622e-07
Iter: 1951 loss: 9.69611619e-07
Iter: 1952 loss: 9.68155746e-07
Iter: 1953 loss: 9.6974486e-07
Iter: 1954 loss: 9.67335382e-07
Iter: 1955 loss: 9.66066864e-07
Iter: 1956 loss: 9.67658e-07
Iter: 1957 loss: 9.65391564e-07
Iter: 1958 loss: 9.63915454e-07
Iter: 1959 loss: 9.68449399e-07
Iter: 1960 loss: 9.63483899e-07
Iter: 1961 loss: 9.62105446e-07
Iter: 1962 loss: 9.75213879e-07
Iter: 1963 loss: 9.6201768e-07
Iter: 1964 loss: 9.61066235e-07
Iter: 1965 loss: 9.62261424e-07
Iter: 1966 loss: 9.60552256e-07
Iter: 1967 loss: 9.59392082e-07
Iter: 1968 loss: 9.58730539e-07
Iter: 1969 loss: 9.58195415e-07
Iter: 1970 loss: 9.56564804e-07
Iter: 1971 loss: 9.76684532e-07
Iter: 1972 loss: 9.56573558e-07
Iter: 1973 loss: 9.55474661e-07
Iter: 1974 loss: 9.56346639e-07
Iter: 1975 loss: 9.54854727e-07
Iter: 1976 loss: 9.53704102e-07
Iter: 1977 loss: 9.64094852e-07
Iter: 1978 loss: 9.53641916e-07
Iter: 1979 loss: 9.5263465e-07
Iter: 1980 loss: 9.53052563e-07
Iter: 1981 loss: 9.51859306e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script67
+ '[' -r STOP.script67 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi-1_phi3/300_300_300_1
