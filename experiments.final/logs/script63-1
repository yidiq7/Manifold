+ RUN=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='-2 -1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 100000 				 --batch_size 5000 				 --max_epochs 1000 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0
+ date
Tue Oct 20 16:32:33 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a79e5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a7ab8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a79e5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a7ab82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a7a3a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a7a3a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a7a3ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a7a3b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a7964a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a7964ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a7947ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a79192f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a78b9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a78b92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a78da378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a7831730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a78b96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a78b9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a7813e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a77d6620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1822fd048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1a77d6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1822e8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1822eb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1822eb048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd182298730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1822eb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd182275f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd18221f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd15c2c0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd15c2dbae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd15c284b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd18221f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd15c252378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd15c204400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd15c218e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 8.3197865e-06
Iter: 2 loss: 6.57087276e-06
Iter: 3 loss: 5.00797796e-06
Iter: 4 loss: 4.44629677e-06
Iter: 5 loss: 5.52134861e-06
Iter: 6 loss: 4.20995502e-06
Iter: 7 loss: 3.85992689e-06
Iter: 8 loss: 5.71025294e-06
Iter: 9 loss: 3.80585266e-06
Iter: 10 loss: 3.48111189e-06
Iter: 11 loss: 3.95913321e-06
Iter: 12 loss: 3.32463719e-06
Iter: 13 loss: 3.08398512e-06
Iter: 14 loss: 2.84299927e-06
Iter: 15 loss: 2.79401183e-06
Iter: 16 loss: 2.62987032e-06
Iter: 17 loss: 2.62765e-06
Iter: 18 loss: 2.41918383e-06
Iter: 19 loss: 2.2687484e-06
Iter: 20 loss: 2.19695539e-06
Iter: 21 loss: 2.00116574e-06
Iter: 22 loss: 2.06229333e-06
Iter: 23 loss: 1.86155501e-06
Iter: 24 loss: 1.90533729e-06
Iter: 25 loss: 1.80628547e-06
Iter: 26 loss: 1.75881314e-06
Iter: 27 loss: 1.70307533e-06
Iter: 28 loss: 1.69689361e-06
Iter: 29 loss: 1.64279254e-06
Iter: 30 loss: 1.55849966e-06
Iter: 31 loss: 1.55739178e-06
Iter: 32 loss: 1.49610764e-06
Iter: 33 loss: 1.49591813e-06
Iter: 34 loss: 1.45092929e-06
Iter: 35 loss: 1.35019286e-06
Iter: 36 loss: 2.75372213e-06
Iter: 37 loss: 1.344731e-06
Iter: 38 loss: 1.26306236e-06
Iter: 39 loss: 2.03466379e-06
Iter: 40 loss: 1.25979636e-06
Iter: 41 loss: 1.18837374e-06
Iter: 42 loss: 1.34353127e-06
Iter: 43 loss: 1.16067906e-06
Iter: 44 loss: 1.1219887e-06
Iter: 45 loss: 1.12301689e-06
Iter: 46 loss: 1.0913152e-06
Iter: 47 loss: 1.05393883e-06
Iter: 48 loss: 1.30271098e-06
Iter: 49 loss: 1.05008462e-06
Iter: 50 loss: 1.01148362e-06
Iter: 51 loss: 1.01275896e-06
Iter: 52 loss: 9.80912773e-07
Iter: 53 loss: 9.52860432e-07
Iter: 54 loss: 9.5265824e-07
Iter: 55 loss: 9.22683967e-07
Iter: 56 loss: 8.6793375e-07
Iter: 57 loss: 2.16001763e-06
Iter: 58 loss: 8.67936251e-07
Iter: 59 loss: 8.30562044e-07
Iter: 60 loss: 1.12733039e-06
Iter: 61 loss: 8.28014777e-07
Iter: 62 loss: 7.90963554e-07
Iter: 63 loss: 1.05124059e-06
Iter: 64 loss: 7.87599731e-07
Iter: 65 loss: 7.74966e-07
Iter: 66 loss: 7.54728717e-07
Iter: 67 loss: 7.54572966e-07
Iter: 68 loss: 7.46395472e-07
Iter: 69 loss: 7.44562e-07
Iter: 70 loss: 7.33585466e-07
Iter: 71 loss: 7.27381575e-07
Iter: 72 loss: 7.22614118e-07
Iter: 73 loss: 7.07423055e-07
Iter: 74 loss: 6.9180652e-07
Iter: 75 loss: 6.88873342e-07
Iter: 76 loss: 6.91248829e-07
Iter: 77 loss: 6.77788762e-07
Iter: 78 loss: 6.72442525e-07
Iter: 79 loss: 6.58391059e-07
Iter: 80 loss: 7.63932121e-07
Iter: 81 loss: 6.55481813e-07
Iter: 82 loss: 6.3932805e-07
Iter: 83 loss: 6.7280962e-07
Iter: 84 loss: 6.32889055e-07
Iter: 85 loss: 6.19159948e-07
Iter: 86 loss: 6.30124532e-07
Iter: 87 loss: 6.10866778e-07
Iter: 88 loss: 6.03693252e-07
Iter: 89 loss: 6.01133e-07
Iter: 90 loss: 5.90026389e-07
Iter: 91 loss: 5.77641345e-07
Iter: 92 loss: 5.759278e-07
Iter: 93 loss: 5.64068046e-07
Iter: 94 loss: 6.97525138e-07
Iter: 95 loss: 5.63852552e-07
Iter: 96 loss: 5.5606921e-07
Iter: 97 loss: 5.6317208e-07
Iter: 98 loss: 5.51555786e-07
Iter: 99 loss: 5.43621582e-07
Iter: 100 loss: 6.56355496e-07
Iter: 101 loss: 5.43621e-07
Iter: 102 loss: 5.39629468e-07
Iter: 103 loss: 5.30384568e-07
Iter: 104 loss: 6.44976e-07
Iter: 105 loss: 5.29698355e-07
Iter: 106 loss: 5.31134674e-07
Iter: 107 loss: 5.25741939e-07
Iter: 108 loss: 5.23274537e-07
Iter: 109 loss: 5.18070124e-07
Iter: 110 loss: 6.02393072e-07
Iter: 111 loss: 5.17908347e-07
Iter: 112 loss: 5.08967901e-07
Iter: 113 loss: 5.45561306e-07
Iter: 114 loss: 5.07006405e-07
Iter: 115 loss: 5.02083253e-07
Iter: 116 loss: 4.953352e-07
Iter: 117 loss: 4.95003633e-07
Iter: 118 loss: 4.89111585e-07
Iter: 119 loss: 4.98254394e-07
Iter: 120 loss: 4.86339275e-07
Iter: 121 loss: 4.80116285e-07
Iter: 122 loss: 5.10334701e-07
Iter: 123 loss: 4.79007326e-07
Iter: 124 loss: 4.7382639e-07
Iter: 125 loss: 4.73794046e-07
Iter: 126 loss: 4.70989306e-07
Iter: 127 loss: 4.65759456e-07
Iter: 128 loss: 5.84219265e-07
Iter: 129 loss: 4.6575164e-07
Iter: 130 loss: 4.60227852e-07
Iter: 131 loss: 5.23947449e-07
Iter: 132 loss: 4.60133833e-07
Iter: 133 loss: 4.56619603e-07
Iter: 134 loss: 4.75143622e-07
Iter: 135 loss: 4.56077629e-07
Iter: 136 loss: 4.52354584e-07
Iter: 137 loss: 4.53456607e-07
Iter: 138 loss: 4.49700224e-07
Iter: 139 loss: 4.47302398e-07
Iter: 140 loss: 4.57873227e-07
Iter: 141 loss: 4.46819087e-07
Iter: 142 loss: 4.43679198e-07
Iter: 143 loss: 4.4939776e-07
Iter: 144 loss: 4.42327661e-07
Iter: 145 loss: 4.39990515e-07
Iter: 146 loss: 4.55827461e-07
Iter: 147 loss: 4.39739665e-07
Iter: 148 loss: 4.36927e-07
Iter: 149 loss: 4.3050855e-07
Iter: 150 loss: 5.14449084e-07
Iter: 151 loss: 4.30094559e-07
Iter: 152 loss: 4.2579623e-07
Iter: 153 loss: 4.36956e-07
Iter: 154 loss: 4.24334701e-07
Iter: 155 loss: 4.20269771e-07
Iter: 156 loss: 4.27585519e-07
Iter: 157 loss: 4.18514105e-07
Iter: 158 loss: 4.19287318e-07
Iter: 159 loss: 4.17047971e-07
Iter: 160 loss: 4.15689442e-07
Iter: 161 loss: 4.12614071e-07
Iter: 162 loss: 4.5376882e-07
Iter: 163 loss: 4.12417734e-07
Iter: 164 loss: 4.09297115e-07
Iter: 165 loss: 4.25200199e-07
Iter: 166 loss: 4.08782711e-07
Iter: 167 loss: 4.05307901e-07
Iter: 168 loss: 4.17397757e-07
Iter: 169 loss: 4.04394228e-07
Iter: 170 loss: 4.01362058e-07
Iter: 171 loss: 4.2121e-07
Iter: 172 loss: 4.01046918e-07
Iter: 173 loss: 3.98944707e-07
Iter: 174 loss: 3.9807415e-07
Iter: 175 loss: 3.96976247e-07
Iter: 176 loss: 3.9567368e-07
Iter: 177 loss: 3.95460745e-07
Iter: 178 loss: 3.94467293e-07
Iter: 179 loss: 3.93231289e-07
Iter: 180 loss: 3.93118967e-07
Iter: 181 loss: 3.90356121e-07
Iter: 182 loss: 3.92851518e-07
Iter: 183 loss: 3.88744752e-07
Iter: 184 loss: 3.86691028e-07
Iter: 185 loss: 3.84196767e-07
Iter: 186 loss: 3.83968597e-07
Iter: 187 loss: 3.8050365e-07
Iter: 188 loss: 3.81828443e-07
Iter: 189 loss: 3.78109689e-07
Iter: 190 loss: 3.76527026e-07
Iter: 191 loss: 3.75962969e-07
Iter: 192 loss: 3.73855926e-07
Iter: 193 loss: 3.81311565e-07
Iter: 194 loss: 3.73310399e-07
Iter: 195 loss: 3.72034435e-07
Iter: 196 loss: 3.6930544e-07
Iter: 197 loss: 4.11389834e-07
Iter: 198 loss: 3.69209602e-07
Iter: 199 loss: 3.66214238e-07
Iter: 200 loss: 3.66216057e-07
Iter: 201 loss: 3.6421153e-07
Iter: 202 loss: 3.7072391e-07
Iter: 203 loss: 3.63664526e-07
Iter: 204 loss: 3.61603298e-07
Iter: 205 loss: 3.6313071e-07
Iter: 206 loss: 3.60340579e-07
Iter: 207 loss: 3.58859353e-07
Iter: 208 loss: 3.73933176e-07
Iter: 209 loss: 3.58811207e-07
Iter: 210 loss: 3.57347773e-07
Iter: 211 loss: 3.57594047e-07
Iter: 212 loss: 3.56246971e-07
Iter: 213 loss: 3.54973196e-07
Iter: 214 loss: 3.74893204e-07
Iter: 215 loss: 3.54977487e-07
Iter: 216 loss: 3.5419481e-07
Iter: 217 loss: 3.5210968e-07
Iter: 218 loss: 3.66878822e-07
Iter: 219 loss: 3.51650385e-07
Iter: 220 loss: 3.49171444e-07
Iter: 221 loss: 3.49521372e-07
Iter: 222 loss: 3.47291405e-07
Iter: 223 loss: 3.4380966e-07
Iter: 224 loss: 3.68309344e-07
Iter: 225 loss: 3.43512795e-07
Iter: 226 loss: 3.43833676e-07
Iter: 227 loss: 3.42443911e-07
Iter: 228 loss: 3.41936186e-07
Iter: 229 loss: 3.40468773e-07
Iter: 230 loss: 3.47334549e-07
Iter: 231 loss: 3.39966135e-07
Iter: 232 loss: 3.38636255e-07
Iter: 233 loss: 3.38595271e-07
Iter: 234 loss: 3.3741307e-07
Iter: 235 loss: 3.39030692e-07
Iter: 236 loss: 3.36814509e-07
Iter: 237 loss: 3.35578591e-07
Iter: 238 loss: 3.42824023e-07
Iter: 239 loss: 3.35422243e-07
Iter: 240 loss: 3.34567176e-07
Iter: 241 loss: 3.35933407e-07
Iter: 242 loss: 3.34186609e-07
Iter: 243 loss: 3.33047637e-07
Iter: 244 loss: 3.37481112e-07
Iter: 245 loss: 3.32778768e-07
Iter: 246 loss: 3.32023319e-07
Iter: 247 loss: 3.35931645e-07
Iter: 248 loss: 3.31906762e-07
Iter: 249 loss: 3.31123431e-07
Iter: 250 loss: 3.30013393e-07
Iter: 251 loss: 3.29962546e-07
Iter: 252 loss: 3.2898194e-07
Iter: 253 loss: 3.27858572e-07
Iter: 254 loss: 3.2773346e-07
Iter: 255 loss: 3.25559341e-07
Iter: 256 loss: 3.29735769e-07
Iter: 257 loss: 3.24650557e-07
Iter: 258 loss: 3.25936071e-07
Iter: 259 loss: 3.24050973e-07
Iter: 260 loss: 3.23518833e-07
Iter: 261 loss: 3.22460295e-07
Iter: 262 loss: 3.40789967e-07
Iter: 263 loss: 3.22430367e-07
Iter: 264 loss: 3.21438336e-07
Iter: 265 loss: 3.25092458e-07
Iter: 266 loss: 3.21212497e-07
Iter: 267 loss: 3.20253179e-07
Iter: 268 loss: 3.30722315e-07
Iter: 269 loss: 3.20231663e-07
Iter: 270 loss: 3.19758783e-07
Iter: 271 loss: 3.21258426e-07
Iter: 272 loss: 3.19622472e-07
Iter: 273 loss: 3.1908732e-07
Iter: 274 loss: 3.18449338e-07
Iter: 275 loss: 3.18387777e-07
Iter: 276 loss: 3.17389777e-07
Iter: 277 loss: 3.2794145e-07
Iter: 278 loss: 3.17349816e-07
Iter: 279 loss: 3.16670253e-07
Iter: 280 loss: 3.1765228e-07
Iter: 281 loss: 3.16335672e-07
Iter: 282 loss: 3.15468014e-07
Iter: 283 loss: 3.17402055e-07
Iter: 284 loss: 3.15136674e-07
Iter: 285 loss: 3.14614567e-07
Iter: 286 loss: 3.13421424e-07
Iter: 287 loss: 3.2878819e-07
Iter: 288 loss: 3.13327291e-07
Iter: 289 loss: 3.12030693e-07
Iter: 290 loss: 3.18332582e-07
Iter: 291 loss: 3.1180025e-07
Iter: 292 loss: 3.11007568e-07
Iter: 293 loss: 3.11000861e-07
Iter: 294 loss: 3.1010282e-07
Iter: 295 loss: 3.11453107e-07
Iter: 296 loss: 3.09659583e-07
Iter: 297 loss: 3.09032657e-07
Iter: 298 loss: 3.08216215e-07
Iter: 299 loss: 3.08155563e-07
Iter: 300 loss: 3.07681177e-07
Iter: 301 loss: 3.07512e-07
Iter: 302 loss: 3.07145569e-07
Iter: 303 loss: 3.07298052e-07
Iter: 304 loss: 3.06903132e-07
Iter: 305 loss: 3.06363432e-07
Iter: 306 loss: 3.06997038e-07
Iter: 307 loss: 3.06081233e-07
Iter: 308 loss: 3.05527578e-07
Iter: 309 loss: 3.0934882e-07
Iter: 310 loss: 3.05471758e-07
Iter: 311 loss: 3.04847731e-07
Iter: 312 loss: 3.04502e-07
Iter: 313 loss: 3.04235044e-07
Iter: 314 loss: 3.03423576e-07
Iter: 315 loss: 3.12179282e-07
Iter: 316 loss: 3.03395723e-07
Iter: 317 loss: 3.03014161e-07
Iter: 318 loss: 3.02121293e-07
Iter: 319 loss: 3.11556676e-07
Iter: 320 loss: 3.01999137e-07
Iter: 321 loss: 3.00989541e-07
Iter: 322 loss: 3.04152564e-07
Iter: 323 loss: 3.00701259e-07
Iter: 324 loss: 2.99957208e-07
Iter: 325 loss: 3.03962423e-07
Iter: 326 loss: 2.99864894e-07
Iter: 327 loss: 2.99146791e-07
Iter: 328 loss: 3.0879022e-07
Iter: 329 loss: 2.99142613e-07
Iter: 330 loss: 2.98728793e-07
Iter: 331 loss: 2.97590219e-07
Iter: 332 loss: 3.05073286e-07
Iter: 333 loss: 2.97315e-07
Iter: 334 loss: 2.97066663e-07
Iter: 335 loss: 2.96704059e-07
Iter: 336 loss: 2.96167883e-07
Iter: 337 loss: 2.96732026e-07
Iter: 338 loss: 2.9587153e-07
Iter: 339 loss: 2.9537614e-07
Iter: 340 loss: 2.97463941e-07
Iter: 341 loss: 2.95276095e-07
Iter: 342 loss: 2.94867107e-07
Iter: 343 loss: 2.97696261e-07
Iter: 344 loss: 2.94823622e-07
Iter: 345 loss: 2.94397097e-07
Iter: 346 loss: 2.94401559e-07
Iter: 347 loss: 2.94057429e-07
Iter: 348 loss: 2.93633093e-07
Iter: 349 loss: 2.98424482e-07
Iter: 350 loss: 2.93636958e-07
Iter: 351 loss: 2.93345437e-07
Iter: 352 loss: 2.92662605e-07
Iter: 353 loss: 3.01095866e-07
Iter: 354 loss: 2.92604483e-07
Iter: 355 loss: 2.91785682e-07
Iter: 356 loss: 2.9175726e-07
Iter: 357 loss: 2.91122262e-07
Iter: 358 loss: 2.90283424e-07
Iter: 359 loss: 2.97699728e-07
Iter: 360 loss: 2.90245282e-07
Iter: 361 loss: 2.90096239e-07
Iter: 362 loss: 2.89926845e-07
Iter: 363 loss: 2.89684806e-07
Iter: 364 loss: 2.89006266e-07
Iter: 365 loss: 2.92873096e-07
Iter: 366 loss: 2.88804472e-07
Iter: 367 loss: 2.88219042e-07
Iter: 368 loss: 2.88216654e-07
Iter: 369 loss: 2.87684259e-07
Iter: 370 loss: 2.90281861e-07
Iter: 371 loss: 2.87594361e-07
Iter: 372 loss: 2.87276436e-07
Iter: 373 loss: 2.88092366e-07
Iter: 374 loss: 2.87161612e-07
Iter: 375 loss: 2.86851446e-07
Iter: 376 loss: 2.87652512e-07
Iter: 377 loss: 2.86737958e-07
Iter: 378 loss: 2.86352417e-07
Iter: 379 loss: 2.87379862e-07
Iter: 380 loss: 2.8620704e-07
Iter: 381 loss: 2.85917793e-07
Iter: 382 loss: 2.87301418e-07
Iter: 383 loss: 2.8584094e-07
Iter: 384 loss: 2.85556609e-07
Iter: 385 loss: 2.85401427e-07
Iter: 386 loss: 2.85272051e-07
Iter: 387 loss: 2.84921413e-07
Iter: 388 loss: 2.84170028e-07
Iter: 389 loss: 2.96417824e-07
Iter: 390 loss: 2.84141379e-07
Iter: 391 loss: 2.83437203e-07
Iter: 392 loss: 2.9242085e-07
Iter: 393 loss: 2.83429102e-07
Iter: 394 loss: 2.83235465e-07
Iter: 395 loss: 2.83203576e-07
Iter: 396 loss: 2.82928e-07
Iter: 397 loss: 2.82406688e-07
Iter: 398 loss: 2.92828673e-07
Iter: 399 loss: 2.82408394e-07
Iter: 400 loss: 2.82015719e-07
Iter: 401 loss: 2.84726e-07
Iter: 402 loss: 2.81969676e-07
Iter: 403 loss: 2.81656696e-07
Iter: 404 loss: 2.84288575e-07
Iter: 405 loss: 2.81643224e-07
Iter: 406 loss: 2.81409228e-07
Iter: 407 loss: 2.81469113e-07
Iter: 408 loss: 2.81255893e-07
Iter: 409 loss: 2.80907017e-07
Iter: 410 loss: 2.81402464e-07
Iter: 411 loss: 2.80724976e-07
Iter: 412 loss: 2.80366976e-07
Iter: 413 loss: 2.84229031e-07
Iter: 414 loss: 2.80352026e-07
Iter: 415 loss: 2.80128262e-07
Iter: 416 loss: 2.80311752e-07
Iter: 417 loss: 2.80002297e-07
Iter: 418 loss: 2.79708388e-07
Iter: 419 loss: 2.8029794e-07
Iter: 420 loss: 2.7959112e-07
Iter: 421 loss: 2.79392367e-07
Iter: 422 loss: 2.7892753e-07
Iter: 423 loss: 2.85536032e-07
Iter: 424 loss: 2.78909198e-07
Iter: 425 loss: 2.78321068e-07
Iter: 426 loss: 2.80132383e-07
Iter: 427 loss: 2.78150509e-07
Iter: 428 loss: 2.77932315e-07
Iter: 429 loss: 2.77906935e-07
Iter: 430 loss: 2.77617744e-07
Iter: 431 loss: 2.77804105e-07
Iter: 432 loss: 2.77429024e-07
Iter: 433 loss: 2.77175673e-07
Iter: 434 loss: 2.77137332e-07
Iter: 435 loss: 2.76979534e-07
Iter: 436 loss: 2.7679917e-07
Iter: 437 loss: 2.76763274e-07
Iter: 438 loss: 2.76620909e-07
Iter: 439 loss: 2.76424601e-07
Iter: 440 loss: 2.76412862e-07
Iter: 441 loss: 2.76081e-07
Iter: 442 loss: 2.76885032e-07
Iter: 443 loss: 2.75949759e-07
Iter: 444 loss: 2.75654827e-07
Iter: 445 loss: 2.79081803e-07
Iter: 446 loss: 2.75652695e-07
Iter: 447 loss: 2.75423616e-07
Iter: 448 loss: 2.75421883e-07
Iter: 449 loss: 2.75238563e-07
Iter: 450 loss: 2.74969068e-07
Iter: 451 loss: 2.76594733e-07
Iter: 452 loss: 2.74938202e-07
Iter: 453 loss: 2.7476176e-07
Iter: 454 loss: 2.74415896e-07
Iter: 455 loss: 2.80609385e-07
Iter: 456 loss: 2.74420501e-07
Iter: 457 loss: 2.73973825e-07
Iter: 458 loss: 2.73898934e-07
Iter: 459 loss: 2.73596243e-07
Iter: 460 loss: 2.73159372e-07
Iter: 461 loss: 2.79976973e-07
Iter: 462 loss: 2.73159912e-07
Iter: 463 loss: 2.72846563e-07
Iter: 464 loss: 2.77416376e-07
Iter: 465 loss: 2.72844773e-07
Iter: 466 loss: 2.72676374e-07
Iter: 467 loss: 2.72253089e-07
Iter: 468 loss: 2.77317099e-07
Iter: 469 loss: 2.72242e-07
Iter: 470 loss: 2.72190448e-07
Iter: 471 loss: 2.72043337e-07
Iter: 472 loss: 2.71918594e-07
Iter: 473 loss: 2.71716203e-07
Iter: 474 loss: 2.71710405e-07
Iter: 475 loss: 2.71437813e-07
Iter: 476 loss: 2.72668785e-07
Iter: 477 loss: 2.71386824e-07
Iter: 478 loss: 2.71112697e-07
Iter: 479 loss: 2.71892702e-07
Iter: 480 loss: 2.71029137e-07
Iter: 481 loss: 2.70699331e-07
Iter: 482 loss: 2.71774979e-07
Iter: 483 loss: 2.70605597e-07
Iter: 484 loss: 2.70382827e-07
Iter: 485 loss: 2.71202538e-07
Iter: 486 loss: 2.70314786e-07
Iter: 487 loss: 2.70087071e-07
Iter: 488 loss: 2.69824938e-07
Iter: 489 loss: 2.69788927e-07
Iter: 490 loss: 2.69510849e-07
Iter: 491 loss: 2.69616322e-07
Iter: 492 loss: 2.69319571e-07
Iter: 493 loss: 2.68976436e-07
Iter: 494 loss: 2.70058138e-07
Iter: 495 loss: 2.68878324e-07
Iter: 496 loss: 2.68866984e-07
Iter: 497 loss: 2.68741246e-07
Iter: 498 loss: 2.68614883e-07
Iter: 499 loss: 2.68294912e-07
Iter: 500 loss: 2.7066244e-07
Iter: 501 loss: 2.68221356e-07
Iter: 502 loss: 2.68044346e-07
Iter: 503 loss: 2.68033119e-07
Iter: 504 loss: 2.67849032e-07
Iter: 505 loss: 2.68276779e-07
Iter: 506 loss: 2.67769e-07
Iter: 507 loss: 2.67652609e-07
Iter: 508 loss: 2.67686573e-07
Iter: 509 loss: 2.67570385e-07
Iter: 510 loss: 2.67379477e-07
Iter: 511 loss: 2.68198534e-07
Iter: 512 loss: 2.67349975e-07
Iter: 513 loss: 2.67165461e-07
Iter: 514 loss: 2.68325806e-07
Iter: 515 loss: 2.67144912e-07
Iter: 516 loss: 2.67006385e-07
Iter: 517 loss: 2.66994533e-07
Iter: 518 loss: 2.66906511e-07
Iter: 519 loss: 2.66685674e-07
Iter: 520 loss: 2.67054645e-07
Iter: 521 loss: 2.66590405e-07
Iter: 522 loss: 2.66406175e-07
Iter: 523 loss: 2.66137107e-07
Iter: 524 loss: 2.66128438e-07
Iter: 525 loss: 2.65818869e-07
Iter: 526 loss: 2.67150938e-07
Iter: 527 loss: 2.65752789e-07
Iter: 528 loss: 2.65609856e-07
Iter: 529 loss: 2.65606445e-07
Iter: 530 loss: 2.65438587e-07
Iter: 531 loss: 2.6536614e-07
Iter: 532 loss: 2.65262486e-07
Iter: 533 loss: 2.65081439e-07
Iter: 534 loss: 2.65090705e-07
Iter: 535 loss: 2.64929e-07
Iter: 536 loss: 2.64822916e-07
Iter: 537 loss: 2.64775679e-07
Iter: 538 loss: 2.64702805e-07
Iter: 539 loss: 2.64564079e-07
Iter: 540 loss: 2.67994e-07
Iter: 541 loss: 2.6456118e-07
Iter: 542 loss: 2.64395794e-07
Iter: 543 loss: 2.65621111e-07
Iter: 544 loss: 2.64375103e-07
Iter: 545 loss: 2.6424695e-07
Iter: 546 loss: 2.6476124e-07
Iter: 547 loss: 2.64217874e-07
Iter: 548 loss: 2.6407497e-07
Iter: 549 loss: 2.64186838e-07
Iter: 550 loss: 2.63994338e-07
Iter: 551 loss: 2.63872465e-07
Iter: 552 loss: 2.64104585e-07
Iter: 553 loss: 2.63800956e-07
Iter: 554 loss: 2.63632529e-07
Iter: 555 loss: 2.63526317e-07
Iter: 556 loss: 2.63460208e-07
Iter: 557 loss: 2.6325074e-07
Iter: 558 loss: 2.63425022e-07
Iter: 559 loss: 2.63130914e-07
Iter: 560 loss: 2.62959674e-07
Iter: 561 loss: 2.63610929e-07
Iter: 562 loss: 2.62909168e-07
Iter: 563 loss: 2.62783601e-07
Iter: 564 loss: 2.62778173e-07
Iter: 565 loss: 2.62676139e-07
Iter: 566 loss: 2.62408e-07
Iter: 567 loss: 2.65176112e-07
Iter: 568 loss: 2.6239357e-07
Iter: 569 loss: 2.62220055e-07
Iter: 570 loss: 2.62219032e-07
Iter: 571 loss: 2.62010417e-07
Iter: 572 loss: 2.62025537e-07
Iter: 573 loss: 2.61851056e-07
Iter: 574 loss: 2.61688768e-07
Iter: 575 loss: 2.62510298e-07
Iter: 576 loss: 2.61645368e-07
Iter: 577 loss: 2.61465686e-07
Iter: 578 loss: 2.6219297e-07
Iter: 579 loss: 2.61434252e-07
Iter: 580 loss: 2.6132227e-07
Iter: 581 loss: 2.62640555e-07
Iter: 582 loss: 2.61320707e-07
Iter: 583 loss: 2.61251387e-07
Iter: 584 loss: 2.61116099e-07
Iter: 585 loss: 2.63533963e-07
Iter: 586 loss: 2.61109506e-07
Iter: 587 loss: 2.60949889e-07
Iter: 588 loss: 2.6190591e-07
Iter: 589 loss: 2.60921922e-07
Iter: 590 loss: 2.60771486e-07
Iter: 591 loss: 2.60465299e-07
Iter: 592 loss: 2.66189943e-07
Iter: 593 loss: 2.60464589e-07
Iter: 594 loss: 2.60154849e-07
Iter: 595 loss: 2.61474952e-07
Iter: 596 loss: 2.60085983e-07
Iter: 597 loss: 2.59887912e-07
Iter: 598 loss: 2.62698848e-07
Iter: 599 loss: 2.59882142e-07
Iter: 600 loss: 2.59686601e-07
Iter: 601 loss: 2.60624745e-07
Iter: 602 loss: 2.59643969e-07
Iter: 603 loss: 2.59572744e-07
Iter: 604 loss: 2.59448313e-07
Iter: 605 loss: 2.59446779e-07
Iter: 606 loss: 2.59349179e-07
Iter: 607 loss: 2.59346393e-07
Iter: 608 loss: 2.59233758e-07
Iter: 609 loss: 2.59056577e-07
Iter: 610 loss: 2.59059959e-07
Iter: 611 loss: 2.5889824e-07
Iter: 612 loss: 2.59969852e-07
Iter: 613 loss: 2.58888406e-07
Iter: 614 loss: 2.58775e-07
Iter: 615 loss: 2.59731621e-07
Iter: 616 loss: 2.58765226e-07
Iter: 617 loss: 2.58649635e-07
Iter: 618 loss: 2.58485159e-07
Iter: 619 loss: 2.58474188e-07
Iter: 620 loss: 2.58332022e-07
Iter: 621 loss: 2.58614648e-07
Iter: 622 loss: 2.58266851e-07
Iter: 623 loss: 2.58114142e-07
Iter: 624 loss: 2.59286594e-07
Iter: 625 loss: 2.58097884e-07
Iter: 626 loss: 2.57980815e-07
Iter: 627 loss: 2.57812871e-07
Iter: 628 loss: 2.57814833e-07
Iter: 629 loss: 2.57584844e-07
Iter: 630 loss: 2.57431736e-07
Iter: 631 loss: 2.57323109e-07
Iter: 632 loss: 2.57389871e-07
Iter: 633 loss: 2.57190607e-07
Iter: 634 loss: 2.57054438e-07
Iter: 635 loss: 2.57087038e-07
Iter: 636 loss: 2.56944588e-07
Iter: 637 loss: 2.56864723e-07
Iter: 638 loss: 2.56736513e-07
Iter: 639 loss: 2.56737309e-07
Iter: 640 loss: 2.56655881e-07
Iter: 641 loss: 2.56632887e-07
Iter: 642 loss: 2.56562856e-07
Iter: 643 loss: 2.56473697e-07
Iter: 644 loss: 2.56466336e-07
Iter: 645 loss: 2.56345373e-07
Iter: 646 loss: 2.57363638e-07
Iter: 647 loss: 2.56341593e-07
Iter: 648 loss: 2.56245926e-07
Iter: 649 loss: 2.56123769e-07
Iter: 650 loss: 2.56118881e-07
Iter: 651 loss: 2.55985526e-07
Iter: 652 loss: 2.57811337e-07
Iter: 653 loss: 2.5598797e-07
Iter: 654 loss: 2.55895031e-07
Iter: 655 loss: 2.56852189e-07
Iter: 656 loss: 2.55885027e-07
Iter: 657 loss: 2.55848732e-07
Iter: 658 loss: 2.55801126e-07
Iter: 659 loss: 2.55796721e-07
Iter: 660 loss: 2.55716031e-07
Iter: 661 loss: 2.55665867e-07
Iter: 662 loss: 2.556298e-07
Iter: 663 loss: 2.55507302e-07
Iter: 664 loss: 2.55573326e-07
Iter: 665 loss: 2.55421668e-07
Iter: 666 loss: 2.55392052e-07
Iter: 667 loss: 2.55335181e-07
Iter: 668 loss: 2.5527811e-07
Iter: 669 loss: 2.55095671e-07
Iter: 670 loss: 2.55222972e-07
Iter: 671 loss: 2.54933866e-07
Iter: 672 loss: 2.54640611e-07
Iter: 673 loss: 2.56564448e-07
Iter: 674 loss: 2.54590844e-07
Iter: 675 loss: 2.54529141e-07
Iter: 676 loss: 2.5448395e-07
Iter: 677 loss: 2.54374015e-07
Iter: 678 loss: 2.54225426e-07
Iter: 679 loss: 2.54216644e-07
Iter: 680 loss: 2.54047109e-07
Iter: 681 loss: 2.55131084e-07
Iter: 682 loss: 2.54025252e-07
Iter: 683 loss: 2.5382468e-07
Iter: 684 loss: 2.53849407e-07
Iter: 685 loss: 2.53682e-07
Iter: 686 loss: 2.53514145e-07
Iter: 687 loss: 2.54964135e-07
Iter: 688 loss: 2.53510962e-07
Iter: 689 loss: 2.53331308e-07
Iter: 690 loss: 2.53798504e-07
Iter: 691 loss: 2.53274607e-07
Iter: 692 loss: 2.53158589e-07
Iter: 693 loss: 2.53179138e-07
Iter: 694 loss: 2.53059767e-07
Iter: 695 loss: 2.52906318e-07
Iter: 696 loss: 2.53010967e-07
Iter: 697 loss: 2.5280832e-07
Iter: 698 loss: 2.52699778e-07
Iter: 699 loss: 2.53349157e-07
Iter: 700 loss: 2.52682383e-07
Iter: 701 loss: 2.52544794e-07
Iter: 702 loss: 2.53317467e-07
Iter: 703 loss: 2.525193e-07
Iter: 704 loss: 2.52430567e-07
Iter: 705 loss: 2.52158173e-07
Iter: 706 loss: 2.53013155e-07
Iter: 707 loss: 2.52026609e-07
Iter: 708 loss: 2.51747736e-07
Iter: 709 loss: 2.51744154e-07
Iter: 710 loss: 2.51550034e-07
Iter: 711 loss: 2.54108443e-07
Iter: 712 loss: 2.51541792e-07
Iter: 713 loss: 2.51441122e-07
Iter: 714 loss: 2.5124092e-07
Iter: 715 loss: 2.55642789e-07
Iter: 716 loss: 2.51238902e-07
Iter: 717 loss: 2.51081985e-07
Iter: 718 loss: 2.51072436e-07
Iter: 719 loss: 2.50986119e-07
Iter: 720 loss: 2.50962273e-07
Iter: 721 loss: 2.50882977e-07
Iter: 722 loss: 2.50805329e-07
Iter: 723 loss: 2.50808682e-07
Iter: 724 loss: 2.50735837e-07
Iter: 725 loss: 2.50600692e-07
Iter: 726 loss: 2.53695191e-07
Iter: 727 loss: 2.50604e-07
Iter: 728 loss: 2.50409556e-07
Iter: 729 loss: 2.50659411e-07
Iter: 730 loss: 2.50310933e-07
Iter: 731 loss: 2.50133411e-07
Iter: 732 loss: 2.50253095e-07
Iter: 733 loss: 2.50016058e-07
Iter: 734 loss: 2.49871022e-07
Iter: 735 loss: 2.51741739e-07
Iter: 736 loss: 2.49874745e-07
Iter: 737 loss: 2.49729396e-07
Iter: 738 loss: 2.50394066e-07
Iter: 739 loss: 2.4971115e-07
Iter: 740 loss: 2.49638276e-07
Iter: 741 loss: 2.49467291e-07
Iter: 742 loss: 2.50814765e-07
Iter: 743 loss: 2.49427671e-07
Iter: 744 loss: 2.49246739e-07
Iter: 745 loss: 2.51688562e-07
Iter: 746 loss: 2.49248842e-07
Iter: 747 loss: 2.49049123e-07
Iter: 748 loss: 2.49497816e-07
Iter: 749 loss: 2.4897713e-07
Iter: 750 loss: 2.4878841e-07
Iter: 751 loss: 2.48620836e-07
Iter: 752 loss: 2.48584314e-07
Iter: 753 loss: 2.48435441e-07
Iter: 754 loss: 2.4840665e-07
Iter: 755 loss: 2.48336477e-07
Iter: 756 loss: 2.48257606e-07
Iter: 757 loss: 2.48238194e-07
Iter: 758 loss: 2.48171119e-07
Iter: 759 loss: 2.48173308e-07
Iter: 760 loss: 2.48106119e-07
Iter: 761 loss: 2.4799516e-07
Iter: 762 loss: 2.47991039e-07
Iter: 763 loss: 2.47863511e-07
Iter: 764 loss: 2.48310869e-07
Iter: 765 loss: 2.47823777e-07
Iter: 766 loss: 2.47693066e-07
Iter: 767 loss: 2.47754542e-07
Iter: 768 loss: 2.47606039e-07
Iter: 769 loss: 2.47497042e-07
Iter: 770 loss: 2.47500139e-07
Iter: 771 loss: 2.47378239e-07
Iter: 772 loss: 2.47172721e-07
Iter: 773 loss: 2.51606593e-07
Iter: 774 loss: 2.47178065e-07
Iter: 775 loss: 2.47042976e-07
Iter: 776 loss: 2.47299909e-07
Iter: 777 loss: 2.46982637e-07
Iter: 778 loss: 2.46976185e-07
Iter: 779 loss: 2.46943955e-07
Iter: 780 loss: 2.46884895e-07
Iter: 781 loss: 2.46790222e-07
Iter: 782 loss: 2.4907547e-07
Iter: 783 loss: 2.46786982e-07
Iter: 784 loss: 2.46710613e-07
Iter: 785 loss: 2.47543369e-07
Iter: 786 loss: 2.46709163e-07
Iter: 787 loss: 2.46617276e-07
Iter: 788 loss: 2.46508137e-07
Iter: 789 loss: 2.46495659e-07
Iter: 790 loss: 2.46391409e-07
Iter: 791 loss: 2.46850334e-07
Iter: 792 loss: 2.46366255e-07
Iter: 793 loss: 2.46268598e-07
Iter: 794 loss: 2.46266438e-07
Iter: 795 loss: 2.46223692e-07
Iter: 796 loss: 2.46146925e-07
Iter: 797 loss: 2.46146016e-07
Iter: 798 loss: 2.46072659e-07
Iter: 799 loss: 2.46575269e-07
Iter: 800 loss: 2.46069021e-07
Iter: 801 loss: 2.46012888e-07
Iter: 802 loss: 2.45958802e-07
Iter: 803 loss: 2.45945387e-07
Iter: 804 loss: 2.45908723e-07
Iter: 805 loss: 2.45892352e-07
Iter: 806 loss: 2.45851936e-07
Iter: 807 loss: 2.45728472e-07
Iter: 808 loss: 2.47315711e-07
Iter: 809 loss: 2.45724408e-07
Iter: 810 loss: 2.4565486e-07
Iter: 811 loss: 2.46634727e-07
Iter: 812 loss: 2.45651506e-07
Iter: 813 loss: 2.45574313e-07
Iter: 814 loss: 2.45827437e-07
Iter: 815 loss: 2.45562575e-07
Iter: 816 loss: 2.45511842e-07
Iter: 817 loss: 2.45391334e-07
Iter: 818 loss: 2.47236187e-07
Iter: 819 loss: 2.45387668e-07
Iter: 820 loss: 2.45251414e-07
Iter: 821 loss: 2.45447808e-07
Iter: 822 loss: 2.45179649e-07
Iter: 823 loss: 2.45202e-07
Iter: 824 loss: 2.451103e-07
Iter: 825 loss: 2.45046522e-07
Iter: 826 loss: 2.44925246e-07
Iter: 827 loss: 2.47391313e-07
Iter: 828 loss: 2.4492806e-07
Iter: 829 loss: 2.4490879e-07
Iter: 830 loss: 2.4486414e-07
Iter: 831 loss: 2.44829181e-07
Iter: 832 loss: 2.44779983e-07
Iter: 833 loss: 2.4477103e-07
Iter: 834 loss: 2.44718194e-07
Iter: 835 loss: 2.44755938e-07
Iter: 836 loss: 2.44672066e-07
Iter: 837 loss: 2.4459672e-07
Iter: 838 loss: 2.4502765e-07
Iter: 839 loss: 2.4458771e-07
Iter: 840 loss: 2.44532089e-07
Iter: 841 loss: 2.44529019e-07
Iter: 842 loss: 2.44499574e-07
Iter: 843 loss: 2.4442e-07
Iter: 844 loss: 2.45478333e-07
Iter: 845 loss: 2.44422381e-07
Iter: 846 loss: 2.44278851e-07
Iter: 847 loss: 2.44513501e-07
Iter: 848 loss: 2.44219791e-07
Iter: 849 loss: 2.4410906e-07
Iter: 850 loss: 2.44115967e-07
Iter: 851 loss: 2.44011801e-07
Iter: 852 loss: 2.44021578e-07
Iter: 853 loss: 2.43970192e-07
Iter: 854 loss: 2.43920169e-07
Iter: 855 loss: 2.43808e-07
Iter: 856 loss: 2.45172771e-07
Iter: 857 loss: 2.43800372e-07
Iter: 858 loss: 2.43744978e-07
Iter: 859 loss: 2.43746143e-07
Iter: 860 loss: 2.43672446e-07
Iter: 861 loss: 2.43567172e-07
Iter: 862 loss: 2.43563818e-07
Iter: 863 loss: 2.43471e-07
Iter: 864 loss: 2.44381681e-07
Iter: 865 loss: 2.43471021e-07
Iter: 866 loss: 2.43360319e-07
Iter: 867 loss: 2.43281193e-07
Iter: 868 loss: 2.43234751e-07
Iter: 869 loss: 2.43163981e-07
Iter: 870 loss: 2.43215169e-07
Iter: 871 loss: 2.4312115e-07
Iter: 872 loss: 2.43074652e-07
Iter: 873 loss: 2.43047737e-07
Iter: 874 loss: 2.43025056e-07
Iter: 875 loss: 2.42969492e-07
Iter: 876 loss: 2.42970373e-07
Iter: 877 loss: 2.4292936e-07
Iter: 878 loss: 2.42881924e-07
Iter: 879 loss: 2.42875558e-07
Iter: 880 loss: 2.42804191e-07
Iter: 881 loss: 2.42700878e-07
Iter: 882 loss: 2.42710371e-07
Iter: 883 loss: 2.42569e-07
Iter: 884 loss: 2.4284725e-07
Iter: 885 loss: 2.42517558e-07
Iter: 886 loss: 2.42504939e-07
Iter: 887 loss: 2.42460885e-07
Iter: 888 loss: 2.42406941e-07
Iter: 889 loss: 2.42401512e-07
Iter: 890 loss: 2.42353934e-07
Iter: 891 loss: 2.42312126e-07
Iter: 892 loss: 2.42384459e-07
Iter: 893 loss: 2.42297119e-07
Iter: 894 loss: 2.42230982e-07
Iter: 895 loss: 2.42319913e-07
Iter: 896 loss: 2.42189401e-07
Iter: 897 loss: 2.42139947e-07
Iter: 898 loss: 2.42174565e-07
Iter: 899 loss: 2.42116556e-07
Iter: 900 loss: 2.4203797e-07
Iter: 901 loss: 2.4280456e-07
Iter: 902 loss: 2.42033735e-07
Iter: 903 loss: 2.42002272e-07
Iter: 904 loss: 2.41873209e-07
Iter: 905 loss: 2.42104363e-07
Iter: 906 loss: 2.4179613e-07
Iter: 907 loss: 2.41789138e-07
Iter: 908 loss: 2.41725985e-07
Iter: 909 loss: 2.41673035e-07
Iter: 910 loss: 2.41981667e-07
Iter: 911 loss: 2.41674911e-07
Iter: 912 loss: 2.41641033e-07
Iter: 913 loss: 2.41731556e-07
Iter: 914 loss: 2.41621251e-07
Iter: 915 loss: 2.41598684e-07
Iter: 916 loss: 2.41542438e-07
Iter: 917 loss: 2.41536497e-07
Iter: 918 loss: 2.41467944e-07
Iter: 919 loss: 2.41415506e-07
Iter: 920 loss: 2.41392627e-07
Iter: 921 loss: 2.41443161e-07
Iter: 922 loss: 2.41339194e-07
Iter: 923 loss: 2.4130884e-07
Iter: 924 loss: 2.41237785e-07
Iter: 925 loss: 2.42139947e-07
Iter: 926 loss: 2.41230566e-07
Iter: 927 loss: 2.41162297e-07
Iter: 928 loss: 2.4141508e-07
Iter: 929 loss: 2.41160791e-07
Iter: 930 loss: 2.41101105e-07
Iter: 931 loss: 2.41101731e-07
Iter: 932 loss: 2.41077203e-07
Iter: 933 loss: 2.4115e-07
Iter: 934 loss: 2.41072911e-07
Iter: 935 loss: 2.41048639e-07
Iter: 936 loss: 2.40954137e-07
Iter: 937 loss: 2.41114606e-07
Iter: 938 loss: 2.40895332e-07
Iter: 939 loss: 2.40783436e-07
Iter: 940 loss: 2.42155437e-07
Iter: 941 loss: 2.40779258e-07
Iter: 942 loss: 2.40699336e-07
Iter: 943 loss: 2.41319526e-07
Iter: 944 loss: 2.40688337e-07
Iter: 945 loss: 2.40606653e-07
Iter: 946 loss: 2.40835675e-07
Iter: 947 loss: 2.40584427e-07
Iter: 948 loss: 2.40519199e-07
Iter: 949 loss: 2.4053216e-07
Iter: 950 loss: 2.40454767e-07
Iter: 951 loss: 2.40391557e-07
Iter: 952 loss: 2.40430779e-07
Iter: 953 loss: 2.40348953e-07
Iter: 954 loss: 2.40281508e-07
Iter: 955 loss: 2.4053881e-07
Iter: 956 loss: 2.40273152e-07
Iter: 957 loss: 2.40235863e-07
Iter: 958 loss: 2.40235039e-07
Iter: 959 loss: 2.40213893e-07
Iter: 960 loss: 2.40139315e-07
Iter: 961 loss: 2.40264626e-07
Iter: 962 loss: 2.40088667e-07
Iter: 963 loss: 2.40023269e-07
Iter: 964 loss: 2.4001551e-07
Iter: 965 loss: 2.39959121e-07
Iter: 966 loss: 2.40278979e-07
Iter: 967 loss: 2.39947468e-07
Iter: 968 loss: 2.39911373e-07
Iter: 969 loss: 2.40130049e-07
Iter: 970 loss: 2.3990583e-07
Iter: 971 loss: 2.39863368e-07
Iter: 972 loss: 2.39789898e-07
Iter: 973 loss: 2.41104942e-07
Iter: 974 loss: 2.39793053e-07
Iter: 975 loss: 2.39735016e-07
Iter: 976 loss: 2.39847196e-07
Iter: 977 loss: 2.39721146e-07
Iter: 978 loss: 2.39675956e-07
Iter: 979 loss: 2.40100064e-07
Iter: 980 loss: 2.39666861e-07
Iter: 981 loss: 2.3960834e-07
Iter: 982 loss: 2.39671493e-07
Iter: 983 loss: 2.39564656e-07
Iter: 984 loss: 2.39516737e-07
Iter: 985 loss: 2.39487065e-07
Iter: 986 loss: 2.39468392e-07
Iter: 987 loss: 2.39387305e-07
Iter: 988 loss: 2.40143692e-07
Iter: 989 loss: 2.3938378e-07
Iter: 990 loss: 2.39345184e-07
Iter: 991 loss: 2.39286521e-07
Iter: 992 loss: 2.39283281e-07
Iter: 993 loss: 2.39231184e-07
Iter: 994 loss: 2.39800812e-07
Iter: 995 loss: 2.39227575e-07
Iter: 996 loss: 2.39196936e-07
Iter: 997 loss: 2.39193e-07
Iter: 998 loss: 2.39177496e-07
Iter: 999 loss: 2.39120311e-07
Iter: 1000 loss: 2.39187727e-07
Iter: 1001 loss: 2.39069379e-07
Iter: 1002 loss: 2.39094732e-07
Iter: 1003 loss: 2.39030129e-07
Iter: 1004 loss: 2.39002588e-07
Iter: 1005 loss: 2.3897303e-07
Iter: 1006 loss: 2.38971182e-07
Iter: 1007 loss: 2.38933183e-07
Iter: 1008 loss: 2.39396513e-07
Iter: 1009 loss: 2.38933751e-07
Iter: 1010 loss: 2.38914538e-07
Iter: 1011 loss: 2.38862299e-07
Iter: 1012 loss: 2.39431643e-07
Iter: 1013 loss: 2.38853659e-07
Iter: 1014 loss: 2.38833024e-07
Iter: 1015 loss: 2.38831177e-07
Iter: 1016 loss: 2.38802301e-07
Iter: 1017 loss: 2.38818046e-07
Iter: 1018 loss: 2.38781212e-07
Iter: 1019 loss: 2.38750829e-07
Iter: 1020 loss: 2.38704e-07
Iter: 1021 loss: 2.38692706e-07
Iter: 1022 loss: 2.38633163e-07
Iter: 1023 loss: 2.38836719e-07
Iter: 1024 loss: 2.38615172e-07
Iter: 1025 loss: 2.38560233e-07
Iter: 1026 loss: 2.39291666e-07
Iter: 1027 loss: 2.38563501e-07
Iter: 1028 loss: 2.38532948e-07
Iter: 1029 loss: 2.38554122e-07
Iter: 1030 loss: 2.38510097e-07
Iter: 1031 loss: 2.38480709e-07
Iter: 1032 loss: 2.38473604e-07
Iter: 1033 loss: 2.38454788e-07
Iter: 1034 loss: 2.38408035e-07
Iter: 1035 loss: 2.39348765e-07
Iter: 1036 loss: 2.38409186e-07
Iter: 1037 loss: 2.38357131e-07
Iter: 1038 loss: 2.38357771e-07
Iter: 1039 loss: 2.38334025e-07
Iter: 1040 loss: 2.38326294e-07
Iter: 1041 loss: 2.38305702e-07
Iter: 1042 loss: 2.38260071e-07
Iter: 1043 loss: 2.38227784e-07
Iter: 1044 loss: 2.3820715e-07
Iter: 1045 loss: 2.38137858e-07
Iter: 1046 loss: 2.38307933e-07
Iter: 1047 loss: 2.38117195e-07
Iter: 1048 loss: 2.38061702e-07
Iter: 1049 loss: 2.38926333e-07
Iter: 1050 loss: 2.38057311e-07
Iter: 1051 loss: 2.38047903e-07
Iter: 1052 loss: 2.38009363e-07
Iter: 1053 loss: 2.38011353e-07
Iter: 1054 loss: 2.37954168e-07
Iter: 1055 loss: 2.38048983e-07
Iter: 1056 loss: 2.37935581e-07
Iter: 1057 loss: 2.37872314e-07
Iter: 1058 loss: 2.37880627e-07
Iter: 1059 loss: 2.37830363e-07
Iter: 1060 loss: 2.37741347e-07
Iter: 1061 loss: 2.3835625e-07
Iter: 1062 loss: 2.37730887e-07
Iter: 1063 loss: 2.37651804e-07
Iter: 1064 loss: 2.38083317e-07
Iter: 1065 loss: 2.37636868e-07
Iter: 1066 loss: 2.3755959e-07
Iter: 1067 loss: 2.37640194e-07
Iter: 1068 loss: 2.37502334e-07
Iter: 1069 loss: 2.37455325e-07
Iter: 1070 loss: 2.37782231e-07
Iter: 1071 loss: 2.37445278e-07
Iter: 1072 loss: 2.37387553e-07
Iter: 1073 loss: 2.37530642e-07
Iter: 1074 loss: 2.37374351e-07
Iter: 1075 loss: 2.37340387e-07
Iter: 1076 loss: 2.37405175e-07
Iter: 1077 loss: 2.37325494e-07
Iter: 1078 loss: 2.37280332e-07
Iter: 1079 loss: 2.37286883e-07
Iter: 1080 loss: 2.37247122e-07
Iter: 1081 loss: 2.37199401e-07
Iter: 1082 loss: 2.3711047e-07
Iter: 1083 loss: 2.37105638e-07
Iter: 1084 loss: 2.37055275e-07
Iter: 1085 loss: 2.37037909e-07
Iter: 1086 loss: 2.37005125e-07
Iter: 1087 loss: 2.3689077e-07
Iter: 1088 loss: 2.37536398e-07
Iter: 1089 loss: 2.3685476e-07
Iter: 1090 loss: 2.36767534e-07
Iter: 1091 loss: 2.3676607e-07
Iter: 1092 loss: 2.367143e-07
Iter: 1093 loss: 2.37018668e-07
Iter: 1094 loss: 2.36692244e-07
Iter: 1095 loss: 2.36634065e-07
Iter: 1096 loss: 2.366057e-07
Iter: 1097 loss: 2.3658211e-07
Iter: 1098 loss: 2.36516314e-07
Iter: 1099 loss: 2.36521828e-07
Iter: 1100 loss: 2.36470242e-07
Iter: 1101 loss: 2.36385731e-07
Iter: 1102 loss: 2.36382277e-07
Iter: 1103 loss: 2.36318357e-07
Iter: 1104 loss: 2.36310029e-07
Iter: 1105 loss: 2.36246422e-07
Iter: 1106 loss: 2.36132408e-07
Iter: 1107 loss: 2.36128756e-07
Iter: 1108 loss: 2.36061013e-07
Iter: 1109 loss: 2.37199359e-07
Iter: 1110 loss: 2.36061425e-07
Iter: 1111 loss: 2.35996367e-07
Iter: 1112 loss: 2.35927402e-07
Iter: 1113 loss: 2.35919416e-07
Iter: 1114 loss: 2.35839025e-07
Iter: 1115 loss: 2.35905048e-07
Iter: 1116 loss: 2.35790964e-07
Iter: 1117 loss: 2.35727526e-07
Iter: 1118 loss: 2.35728223e-07
Iter: 1119 loss: 2.3567371e-07
Iter: 1120 loss: 2.35513596e-07
Iter: 1121 loss: 2.36108917e-07
Iter: 1122 loss: 2.35454124e-07
Iter: 1123 loss: 2.35318723e-07
Iter: 1124 loss: 2.35319519e-07
Iter: 1125 loss: 2.35214742e-07
Iter: 1126 loss: 2.35602897e-07
Iter: 1127 loss: 2.35211672e-07
Iter: 1128 loss: 2.35130202e-07
Iter: 1129 loss: 2.35230914e-07
Iter: 1130 loss: 2.35090468e-07
Iter: 1131 loss: 2.34998353e-07
Iter: 1132 loss: 2.36093356e-07
Iter: 1133 loss: 2.34998566e-07
Iter: 1134 loss: 2.34956374e-07
Iter: 1135 loss: 2.34950363e-07
Iter: 1136 loss: 2.34930198e-07
Iter: 1137 loss: 2.34858845e-07
Iter: 1138 loss: 2.35174326e-07
Iter: 1139 loss: 2.34843014e-07
Iter: 1140 loss: 2.34765906e-07
Iter: 1141 loss: 2.34643721e-07
Iter: 1142 loss: 2.37346796e-07
Iter: 1143 loss: 2.34644119e-07
Iter: 1144 loss: 2.345312e-07
Iter: 1145 loss: 2.34527946e-07
Iter: 1146 loss: 2.34450667e-07
Iter: 1147 loss: 2.34303783e-07
Iter: 1148 loss: 2.37575961e-07
Iter: 1149 loss: 2.3430907e-07
Iter: 1150 loss: 2.34223862e-07
Iter: 1151 loss: 2.35065585e-07
Iter: 1152 loss: 2.34217183e-07
Iter: 1153 loss: 2.3412656e-07
Iter: 1154 loss: 2.34830424e-07
Iter: 1155 loss: 2.34136351e-07
Iter: 1156 loss: 2.34078982e-07
Iter: 1157 loss: 2.33956115e-07
Iter: 1158 loss: 2.35399838e-07
Iter: 1159 loss: 2.33951624e-07
Iter: 1160 loss: 2.33824949e-07
Iter: 1161 loss: 2.33821538e-07
Iter: 1162 loss: 2.3372364e-07
Iter: 1163 loss: 2.33633244e-07
Iter: 1164 loss: 2.33605988e-07
Iter: 1165 loss: 2.3348079e-07
Iter: 1166 loss: 2.34110217e-07
Iter: 1167 loss: 2.33453846e-07
Iter: 1168 loss: 2.33427443e-07
Iter: 1169 loss: 2.33400527e-07
Iter: 1170 loss: 2.33357824e-07
Iter: 1171 loss: 2.333216e-07
Iter: 1172 loss: 2.33320819e-07
Iter: 1173 loss: 2.33249779e-07
Iter: 1174 loss: 2.33505489e-07
Iter: 1175 loss: 2.33218486e-07
Iter: 1176 loss: 2.33155106e-07
Iter: 1177 loss: 2.33076193e-07
Iter: 1178 loss: 2.33064171e-07
Iter: 1179 loss: 2.32931768e-07
Iter: 1180 loss: 2.3414006e-07
Iter: 1181 loss: 2.32928926e-07
Iter: 1182 loss: 2.32829038e-07
Iter: 1183 loss: 2.32745052e-07
Iter: 1184 loss: 2.32728155e-07
Iter: 1185 loss: 2.32606169e-07
Iter: 1186 loss: 2.33543261e-07
Iter: 1187 loss: 2.32601252e-07
Iter: 1188 loss: 2.3247344e-07
Iter: 1189 loss: 2.33173665e-07
Iter: 1190 loss: 2.32457467e-07
Iter: 1191 loss: 2.3239069e-07
Iter: 1192 loss: 2.32256255e-07
Iter: 1193 loss: 2.34868267e-07
Iter: 1194 loss: 2.32256355e-07
Iter: 1195 loss: 2.32119618e-07
Iter: 1196 loss: 2.33077515e-07
Iter: 1197 loss: 2.32117401e-07
Iter: 1198 loss: 2.31955056e-07
Iter: 1199 loss: 2.31929803e-07
Iter: 1200 loss: 2.31823137e-07
Iter: 1201 loss: 2.31619467e-07
Iter: 1202 loss: 2.33261161e-07
Iter: 1203 loss: 2.31615417e-07
Iter: 1204 loss: 2.31443437e-07
Iter: 1205 loss: 2.32414806e-07
Iter: 1206 loss: 2.3142637e-07
Iter: 1207 loss: 2.31330546e-07
Iter: 1208 loss: 2.31775132e-07
Iter: 1209 loss: 2.3131193e-07
Iter: 1210 loss: 2.31186547e-07
Iter: 1211 loss: 2.31131594e-07
Iter: 1212 loss: 2.31070132e-07
Iter: 1213 loss: 2.30965369e-07
Iter: 1214 loss: 2.31442527e-07
Iter: 1215 loss: 2.30940756e-07
Iter: 1216 loss: 2.3082805e-07
Iter: 1217 loss: 2.31264949e-07
Iter: 1218 loss: 2.30807956e-07
Iter: 1219 loss: 2.30737584e-07
Iter: 1220 loss: 2.30606233e-07
Iter: 1221 loss: 2.33518961e-07
Iter: 1222 loss: 2.30610453e-07
Iter: 1223 loss: 2.30461239e-07
Iter: 1224 loss: 2.32652397e-07
Iter: 1225 loss: 2.30453708e-07
Iter: 1226 loss: 2.30317198e-07
Iter: 1227 loss: 2.30160254e-07
Iter: 1228 loss: 2.30136351e-07
Iter: 1229 loss: 2.29987307e-07
Iter: 1230 loss: 2.30224941e-07
Iter: 1231 loss: 2.29913866e-07
Iter: 1232 loss: 2.29752089e-07
Iter: 1233 loss: 2.31209896e-07
Iter: 1234 loss: 2.29750242e-07
Iter: 1235 loss: 2.29629308e-07
Iter: 1236 loss: 2.29935878e-07
Iter: 1237 loss: 2.29587329e-07
Iter: 1238 loss: 2.29514498e-07
Iter: 1239 loss: 2.30644588e-07
Iter: 1240 loss: 2.29509126e-07
Iter: 1241 loss: 2.29459133e-07
Iter: 1242 loss: 2.29489473e-07
Iter: 1243 loss: 2.294193e-07
Iter: 1244 loss: 2.29339079e-07
Iter: 1245 loss: 2.29562573e-07
Iter: 1246 loss: 2.29314637e-07
Iter: 1247 loss: 2.29229698e-07
Iter: 1248 loss: 2.29121042e-07
Iter: 1249 loss: 2.29115926e-07
Iter: 1250 loss: 2.28960531e-07
Iter: 1251 loss: 2.3066481e-07
Iter: 1252 loss: 2.2896154e-07
Iter: 1253 loss: 2.28862348e-07
Iter: 1254 loss: 2.28756e-07
Iter: 1255 loss: 2.2873779e-07
Iter: 1256 loss: 2.28668398e-07
Iter: 1257 loss: 2.28657484e-07
Iter: 1258 loss: 2.28598353e-07
Iter: 1259 loss: 2.28780038e-07
Iter: 1260 loss: 2.28568823e-07
Iter: 1261 loss: 2.28497584e-07
Iter: 1262 loss: 2.28421939e-07
Iter: 1263 loss: 2.28419708e-07
Iter: 1264 loss: 2.28323501e-07
Iter: 1265 loss: 2.28769807e-07
Iter: 1266 loss: 2.2830335e-07
Iter: 1267 loss: 2.28195546e-07
Iter: 1268 loss: 2.28583758e-07
Iter: 1269 loss: 2.28169512e-07
Iter: 1270 loss: 2.28075095e-07
Iter: 1271 loss: 2.28466604e-07
Iter: 1272 loss: 2.28057218e-07
Iter: 1273 loss: 2.27950181e-07
Iter: 1274 loss: 2.28080296e-07
Iter: 1275 loss: 2.27895555e-07
Iter: 1276 loss: 2.27818376e-07
Iter: 1277 loss: 2.2900285e-07
Iter: 1278 loss: 2.27812635e-07
Iter: 1279 loss: 2.27774308e-07
Iter: 1280 loss: 2.27718203e-07
Iter: 1281 loss: 2.27709819e-07
Iter: 1282 loss: 2.27640669e-07
Iter: 1283 loss: 2.28083181e-07
Iter: 1284 loss: 2.27637457e-07
Iter: 1285 loss: 2.27572059e-07
Iter: 1286 loss: 2.27571292e-07
Iter: 1287 loss: 2.2750794e-07
Iter: 1288 loss: 2.27439855e-07
Iter: 1289 loss: 2.2743194e-07
Iter: 1290 loss: 2.273718e-07
Iter: 1291 loss: 2.27328201e-07
Iter: 1292 loss: 2.27309471e-07
Iter: 1293 loss: 2.27258482e-07
Iter: 1294 loss: 2.27150821e-07
Iter: 1295 loss: 2.28946845e-07
Iter: 1296 loss: 2.27149513e-07
Iter: 1297 loss: 2.27022284e-07
Iter: 1298 loss: 2.27799475e-07
Iter: 1299 loss: 2.27010219e-07
Iter: 1300 loss: 2.26939278e-07
Iter: 1301 loss: 2.27576635e-07
Iter: 1302 loss: 2.2694303e-07
Iter: 1303 loss: 2.26897228e-07
Iter: 1304 loss: 2.2721e-07
Iter: 1305 loss: 2.26889853e-07
Iter: 1306 loss: 2.26855462e-07
Iter: 1307 loss: 2.26952736e-07
Iter: 1308 loss: 2.26838267e-07
Iter: 1309 loss: 2.26799528e-07
Iter: 1310 loss: 2.26932016e-07
Iter: 1311 loss: 2.26799727e-07
Iter: 1312 loss: 2.26750132e-07
Iter: 1313 loss: 2.2666498e-07
Iter: 1314 loss: 2.26670537e-07
Iter: 1315 loss: 2.26588753e-07
Iter: 1316 loss: 2.27006609e-07
Iter: 1317 loss: 2.26578265e-07
Iter: 1318 loss: 2.26510551e-07
Iter: 1319 loss: 2.26795265e-07
Iter: 1320 loss: 2.2650147e-07
Iter: 1321 loss: 2.26445863e-07
Iter: 1322 loss: 2.26338301e-07
Iter: 1323 loss: 2.28689501e-07
Iter: 1324 loss: 2.26340745e-07
Iter: 1325 loss: 2.26276839e-07
Iter: 1326 loss: 2.26279013e-07
Iter: 1327 loss: 2.26220578e-07
Iter: 1328 loss: 2.26492347e-07
Iter: 1329 loss: 2.26222213e-07
Iter: 1330 loss: 2.26189272e-07
Iter: 1331 loss: 2.26091174e-07
Iter: 1332 loss: 2.26844151e-07
Iter: 1333 loss: 2.26078697e-07
Iter: 1334 loss: 2.25982305e-07
Iter: 1335 loss: 2.26926176e-07
Iter: 1336 loss: 2.25974688e-07
Iter: 1337 loss: 2.2589721e-07
Iter: 1338 loss: 2.26208584e-07
Iter: 1339 loss: 2.25886254e-07
Iter: 1340 loss: 2.25811121e-07
Iter: 1341 loss: 2.2632031e-07
Iter: 1342 loss: 2.25806659e-07
Iter: 1343 loss: 2.25746106e-07
Iter: 1344 loss: 2.25705435e-07
Iter: 1345 loss: 2.25683408e-07
Iter: 1346 loss: 2.25633869e-07
Iter: 1347 loss: 2.25628725e-07
Iter: 1348 loss: 2.25596153e-07
Iter: 1349 loss: 2.25528396e-07
Iter: 1350 loss: 2.26958491e-07
Iter: 1351 loss: 2.25521219e-07
Iter: 1352 loss: 2.25460184e-07
Iter: 1353 loss: 2.26005426e-07
Iter: 1354 loss: 2.25461349e-07
Iter: 1355 loss: 2.25414738e-07
Iter: 1356 loss: 2.25290677e-07
Iter: 1357 loss: 2.26028334e-07
Iter: 1358 loss: 2.25247049e-07
Iter: 1359 loss: 2.25109801e-07
Iter: 1360 loss: 2.26446616e-07
Iter: 1361 loss: 2.2510703e-07
Iter: 1362 loss: 2.25053554e-07
Iter: 1363 loss: 2.2531944e-07
Iter: 1364 loss: 2.25039173e-07
Iter: 1365 loss: 2.24989051e-07
Iter: 1366 loss: 2.25506653e-07
Iter: 1367 loss: 2.24985e-07
Iter: 1368 loss: 2.24947911e-07
Iter: 1369 loss: 2.24889348e-07
Iter: 1370 loss: 2.24886691e-07
Iter: 1371 loss: 2.24818777e-07
Iter: 1372 loss: 2.25309961e-07
Iter: 1373 loss: 2.2481683e-07
Iter: 1374 loss: 2.24750039e-07
Iter: 1375 loss: 2.24828312e-07
Iter: 1376 loss: 2.24723905e-07
Iter: 1377 loss: 2.246409e-07
Iter: 1378 loss: 2.25107229e-07
Iter: 1379 loss: 2.24625666e-07
Iter: 1380 loss: 2.2456706e-07
Iter: 1381 loss: 2.24527611e-07
Iter: 1382 loss: 2.24494713e-07
Iter: 1383 loss: 2.24462795e-07
Iter: 1384 loss: 2.24456699e-07
Iter: 1385 loss: 2.24434572e-07
Iter: 1386 loss: 2.24412588e-07
Iter: 1387 loss: 2.24404417e-07
Iter: 1388 loss: 2.243643e-07
Iter: 1389 loss: 2.24492979e-07
Iter: 1390 loss: 2.24345882e-07
Iter: 1391 loss: 2.24304983e-07
Iter: 1392 loss: 2.24230249e-07
Iter: 1393 loss: 2.25681632e-07
Iter: 1394 loss: 2.24234242e-07
Iter: 1395 loss: 2.24141218e-07
Iter: 1396 loss: 2.24529359e-07
Iter: 1397 loss: 2.24135022e-07
Iter: 1398 loss: 2.24082427e-07
Iter: 1399 loss: 2.23959063e-07
Iter: 1400 loss: 2.26091544e-07
Iter: 1401 loss: 2.23952242e-07
Iter: 1402 loss: 2.23856773e-07
Iter: 1403 loss: 2.24770588e-07
Iter: 1404 loss: 2.23855736e-07
Iter: 1405 loss: 2.23790011e-07
Iter: 1406 loss: 2.23976457e-07
Iter: 1407 loss: 2.2377256e-07
Iter: 1408 loss: 2.23714579e-07
Iter: 1409 loss: 2.24093839e-07
Iter: 1410 loss: 2.23700226e-07
Iter: 1411 loss: 2.23643724e-07
Iter: 1412 loss: 2.23706763e-07
Iter: 1413 loss: 2.23625491e-07
Iter: 1414 loss: 2.23575057e-07
Iter: 1415 loss: 2.24013959e-07
Iter: 1416 loss: 2.23568179e-07
Iter: 1417 loss: 2.23523216e-07
Iter: 1418 loss: 2.23511051e-07
Iter: 1419 loss: 2.23465094e-07
Iter: 1420 loss: 2.23409174e-07
Iter: 1421 loss: 2.23695409e-07
Iter: 1422 loss: 2.2339654e-07
Iter: 1423 loss: 2.23332236e-07
Iter: 1424 loss: 2.23515755e-07
Iter: 1425 loss: 2.23307012e-07
Iter: 1426 loss: 2.23253892e-07
Iter: 1427 loss: 2.23480427e-07
Iter: 1428 loss: 2.23250169e-07
Iter: 1429 loss: 2.23200459e-07
Iter: 1430 loss: 2.23154871e-07
Iter: 1431 loss: 2.23153208e-07
Iter: 1432 loss: 2.23093124e-07
Iter: 1433 loss: 2.23104493e-07
Iter: 1434 loss: 2.23047365e-07
Iter: 1435 loss: 2.23022624e-07
Iter: 1436 loss: 2.23006538e-07
Iter: 1437 loss: 2.22968467e-07
Iter: 1438 loss: 2.22888545e-07
Iter: 1439 loss: 2.24740077e-07
Iter: 1440 loss: 2.22888886e-07
Iter: 1441 loss: 2.22786653e-07
Iter: 1442 loss: 2.23184855e-07
Iter: 1443 loss: 2.22774375e-07
Iter: 1444 loss: 2.22707598e-07
Iter: 1445 loss: 2.22971792e-07
Iter: 1446 loss: 2.22688e-07
Iter: 1447 loss: 2.22618354e-07
Iter: 1448 loss: 2.22952409e-07
Iter: 1449 loss: 2.22610623e-07
Iter: 1450 loss: 2.2258331e-07
Iter: 1451 loss: 2.22959756e-07
Iter: 1452 loss: 2.22577739e-07
Iter: 1453 loss: 2.22540706e-07
Iter: 1454 loss: 2.22561141e-07
Iter: 1455 loss: 2.22524889e-07
Iter: 1456 loss: 2.22492176e-07
Iter: 1457 loss: 2.22566456e-07
Iter: 1458 loss: 2.22467023e-07
Iter: 1459 loss: 2.22414315e-07
Iter: 1460 loss: 2.2249364e-07
Iter: 1461 loss: 2.22398555e-07
Iter: 1462 loss: 2.22344084e-07
Iter: 1463 loss: 2.225879e-07
Iter: 1464 loss: 2.22332389e-07
Iter: 1465 loss: 2.22277748e-07
Iter: 1466 loss: 2.22215903e-07
Iter: 1467 loss: 2.22210929e-07
Iter: 1468 loss: 2.22146554e-07
Iter: 1469 loss: 2.22379157e-07
Iter: 1470 loss: 2.221214e-07
Iter: 1471 loss: 2.22106905e-07
Iter: 1472 loss: 2.22102159e-07
Iter: 1473 loss: 2.22075855e-07
Iter: 1474 loss: 2.22016894e-07
Iter: 1475 loss: 2.22553581e-07
Iter: 1476 loss: 2.22016695e-07
Iter: 1477 loss: 2.219507e-07
Iter: 1478 loss: 2.22295554e-07
Iter: 1479 loss: 2.21937171e-07
Iter: 1480 loss: 2.21892336e-07
Iter: 1481 loss: 2.21980727e-07
Iter: 1482 loss: 2.21856737e-07
Iter: 1483 loss: 2.21801e-07
Iter: 1484 loss: 2.2228302e-07
Iter: 1485 loss: 2.21795631e-07
Iter: 1486 loss: 2.21759279e-07
Iter: 1487 loss: 2.21997482e-07
Iter: 1488 loss: 2.21763713e-07
Iter: 1489 loss: 2.21730147e-07
Iter: 1490 loss: 2.21778322e-07
Iter: 1491 loss: 2.21711517e-07
Iter: 1492 loss: 2.21687756e-07
Iter: 1493 loss: 2.21824379e-07
Iter: 1494 loss: 2.21682271e-07
Iter: 1495 loss: 2.21656222e-07
Iter: 1496 loss: 2.21682683e-07
Iter: 1497 loss: 2.21640633e-07
Iter: 1498 loss: 2.21602647e-07
Iter: 1499 loss: 2.21670604e-07
Iter: 1500 loss: 2.2159675e-07
Iter: 1501 loss: 2.21564079e-07
Iter: 1502 loss: 2.21510447e-07
Iter: 1503 loss: 2.21507705e-07
Iter: 1504 loss: 2.21458123e-07
Iter: 1505 loss: 2.22003152e-07
Iter: 1506 loss: 2.21458109e-07
Iter: 1507 loss: 2.21404562e-07
Iter: 1508 loss: 2.2160377e-07
Iter: 1509 loss: 2.21391616e-07
Iter: 1510 loss: 2.2136598e-07
Iter: 1511 loss: 2.2133878e-07
Iter: 1512 loss: 2.21329316e-07
Iter: 1513 loss: 2.21294954e-07
Iter: 1514 loss: 2.21545037e-07
Iter: 1515 loss: 2.21289525e-07
Iter: 1516 loss: 2.21264557e-07
Iter: 1517 loss: 2.21343939e-07
Iter: 1518 loss: 2.2125181e-07
Iter: 1519 loss: 2.21222734e-07
Iter: 1520 loss: 2.21374592e-07
Iter: 1521 loss: 2.21215146e-07
Iter: 1522 loss: 2.21184166e-07
Iter: 1523 loss: 2.2122218e-07
Iter: 1524 loss: 2.21166488e-07
Iter: 1525 loss: 2.21138478e-07
Iter: 1526 loss: 2.21293789e-07
Iter: 1527 loss: 2.21132069e-07
Iter: 1528 loss: 2.21101203e-07
Iter: 1529 loss: 2.21187463e-07
Iter: 1530 loss: 2.21094922e-07
Iter: 1531 loss: 2.21056325e-07
Iter: 1532 loss: 2.21183612e-07
Iter: 1533 loss: 2.21048026e-07
Iter: 1534 loss: 2.2102806e-07
Iter: 1535 loss: 2.20991552e-07
Iter: 1536 loss: 2.20983793e-07
Iter: 1537 loss: 2.20952785e-07
Iter: 1538 loss: 2.21028699e-07
Iter: 1539 loss: 2.20943434e-07
Iter: 1540 loss: 2.20912952e-07
Iter: 1541 loss: 2.20918281e-07
Iter: 1542 loss: 2.20897334e-07
Iter: 1543 loss: 2.20853437e-07
Iter: 1544 loss: 2.21295949e-07
Iter: 1545 loss: 2.20843759e-07
Iter: 1546 loss: 2.2079189e-07
Iter: 1547 loss: 2.21060787e-07
Iter: 1548 loss: 2.20779e-07
Iter: 1549 loss: 2.20728282e-07
Iter: 1550 loss: 2.20870035e-07
Iter: 1551 loss: 2.20716657e-07
Iter: 1552 loss: 2.2068653e-07
Iter: 1553 loss: 2.21093202e-07
Iter: 1554 loss: 2.20681727e-07
Iter: 1555 loss: 2.20649554e-07
Iter: 1556 loss: 2.20754885e-07
Iter: 1557 loss: 2.20643216e-07
Iter: 1558 loss: 2.20611781e-07
Iter: 1559 loss: 2.20678842e-07
Iter: 1560 loss: 2.20610218e-07
Iter: 1561 loss: 2.20584496e-07
Iter: 1562 loss: 2.20590081e-07
Iter: 1563 loss: 2.20560764e-07
Iter: 1564 loss: 2.20519809e-07
Iter: 1565 loss: 2.20649582e-07
Iter: 1566 loss: 2.20508653e-07
Iter: 1567 loss: 2.20470483e-07
Iter: 1568 loss: 2.20491927e-07
Iter: 1569 loss: 2.20434629e-07
Iter: 1570 loss: 2.20381992e-07
Iter: 1571 loss: 2.20370964e-07
Iter: 1572 loss: 2.20345981e-07
Iter: 1573 loss: 2.20291753e-07
Iter: 1574 loss: 2.20764065e-07
Iter: 1575 loss: 2.20291909e-07
Iter: 1576 loss: 2.20246761e-07
Iter: 1577 loss: 2.20636935e-07
Iter: 1578 loss: 2.20246818e-07
Iter: 1579 loss: 2.20222034e-07
Iter: 1580 loss: 2.20169738e-07
Iter: 1581 loss: 2.20488729e-07
Iter: 1582 loss: 2.20164836e-07
Iter: 1583 loss: 2.20090783e-07
Iter: 1584 loss: 2.20666692e-07
Iter: 1585 loss: 2.20090186e-07
Iter: 1586 loss: 2.20031566e-07
Iter: 1587 loss: 2.20128669e-07
Iter: 1588 loss: 2.20003244e-07
Iter: 1589 loss: 2.19956348e-07
Iter: 1590 loss: 2.19955808e-07
Iter: 1591 loss: 2.1992183e-07
Iter: 1592 loss: 2.20003272e-07
Iter: 1593 loss: 2.19907605e-07
Iter: 1594 loss: 2.19876583e-07
Iter: 1595 loss: 2.1998828e-07
Iter: 1596 loss: 2.19875616e-07
Iter: 1597 loss: 2.19842434e-07
Iter: 1598 loss: 2.19868298e-07
Iter: 1599 loss: 2.19836778e-07
Iter: 1600 loss: 2.19797968e-07
Iter: 1601 loss: 2.19910689e-07
Iter: 1602 loss: 2.19789e-07
Iter: 1603 loss: 2.19748131e-07
Iter: 1604 loss: 2.19740414e-07
Iter: 1605 loss: 2.19722807e-07
Iter: 1606 loss: 2.19672387e-07
Iter: 1607 loss: 2.19668067e-07
Iter: 1608 loss: 2.19643653e-07
Iter: 1609 loss: 2.19591186e-07
Iter: 1610 loss: 2.19591541e-07
Iter: 1611 loss: 2.19550174e-07
Iter: 1612 loss: 2.19565152e-07
Iter: 1613 loss: 2.19520956e-07
Iter: 1614 loss: 2.19493728e-07
Iter: 1615 loss: 2.19471104e-07
Iter: 1616 loss: 2.19456041e-07
Iter: 1617 loss: 2.19419505e-07
Iter: 1618 loss: 2.19831733e-07
Iter: 1619 loss: 2.19405e-07
Iter: 1620 loss: 2.19365859e-07
Iter: 1621 loss: 2.19356266e-07
Iter: 1622 loss: 2.19322601e-07
Iter: 1623 loss: 2.19280707e-07
Iter: 1624 loss: 2.1928652e-07
Iter: 1625 loss: 2.19248292e-07
Iter: 1626 loss: 2.19290527e-07
Iter: 1627 loss: 2.19231225e-07
Iter: 1628 loss: 2.19197773e-07
Iter: 1629 loss: 2.19354533e-07
Iter: 1630 loss: 2.19181146e-07
Iter: 1631 loss: 2.19158068e-07
Iter: 1632 loss: 2.19186504e-07
Iter: 1633 loss: 2.19147282e-07
Iter: 1634 loss: 2.19107349e-07
Iter: 1635 loss: 2.19357361e-07
Iter: 1636 loss: 2.19106724e-07
Iter: 1637 loss: 2.19083589e-07
Iter: 1638 loss: 2.19062912e-07
Iter: 1639 loss: 2.19055892e-07
Iter: 1640 loss: 2.19022127e-07
Iter: 1641 loss: 2.19005841e-07
Iter: 1642 loss: 2.18982535e-07
Iter: 1643 loss: 2.18961816e-07
Iter: 1644 loss: 2.18959642e-07
Iter: 1645 loss: 2.18927781e-07
Iter: 1646 loss: 2.18903367e-07
Iter: 1647 loss: 2.18895707e-07
Iter: 1648 loss: 2.18858034e-07
Iter: 1649 loss: 2.18880189e-07
Iter: 1650 loss: 2.18848811e-07
Iter: 1651 loss: 2.18801603e-07
Iter: 1652 loss: 2.1902828e-07
Iter: 1653 loss: 2.18794398e-07
Iter: 1654 loss: 2.18771874e-07
Iter: 1655 loss: 2.18998238e-07
Iter: 1656 loss: 2.18765805e-07
Iter: 1657 loss: 2.18744802e-07
Iter: 1658 loss: 2.18994643e-07
Iter: 1659 loss: 2.1874682e-07
Iter: 1660 loss: 2.18738279e-07
Iter: 1661 loss: 2.18708109e-07
Iter: 1662 loss: 2.1932037e-07
Iter: 1663 loss: 2.18708237e-07
Iter: 1664 loss: 2.18679503e-07
Iter: 1665 loss: 2.1903611e-07
Iter: 1666 loss: 2.1867973e-07
Iter: 1667 loss: 2.18658286e-07
Iter: 1668 loss: 2.18669854e-07
Iter: 1669 loss: 2.18651252e-07
Iter: 1670 loss: 2.18623327e-07
Iter: 1671 loss: 2.18700691e-07
Iter: 1672 loss: 2.1861085e-07
Iter: 1673 loss: 2.1859276e-07
Iter: 1674 loss: 2.18567465e-07
Iter: 1675 loss: 2.18561127e-07
Iter: 1676 loss: 2.18544528e-07
Iter: 1677 loss: 2.18762693e-07
Iter: 1678 loss: 2.18540436e-07
Iter: 1679 loss: 2.1851011e-07
Iter: 1680 loss: 2.18684264e-07
Iter: 1681 loss: 2.18514558e-07
Iter: 1682 loss: 2.1849587e-07
Iter: 1683 loss: 2.18452698e-07
Iter: 1684 loss: 2.19049696e-07
Iter: 1685 loss: 2.18450026e-07
Iter: 1686 loss: 2.18403045e-07
Iter: 1687 loss: 2.18591254e-07
Iter: 1688 loss: 2.18400828e-07
Iter: 1689 loss: 2.18362757e-07
Iter: 1690 loss: 2.18615867e-07
Iter: 1691 loss: 2.18362e-07
Iter: 1692 loss: 2.1832625e-07
Iter: 1693 loss: 2.18400302e-07
Iter: 1694 loss: 2.18313602e-07
Iter: 1695 loss: 2.18287809e-07
Iter: 1696 loss: 2.18287866e-07
Iter: 1697 loss: 2.18275346e-07
Iter: 1698 loss: 2.18256531e-07
Iter: 1699 loss: 2.18252126e-07
Iter: 1700 loss: 2.18226688e-07
Iter: 1701 loss: 2.18251358e-07
Iter: 1702 loss: 2.18204278e-07
Iter: 1703 loss: 2.18185306e-07
Iter: 1704 loss: 2.18531966e-07
Iter: 1705 loss: 2.18184582e-07
Iter: 1706 loss: 2.18172573e-07
Iter: 1707 loss: 2.18138268e-07
Iter: 1708 loss: 2.18877219e-07
Iter: 1709 loss: 2.18134261e-07
Iter: 1710 loss: 2.1809393e-07
Iter: 1711 loss: 2.18137089e-07
Iter: 1712 loss: 2.18075556e-07
Iter: 1713 loss: 2.18052818e-07
Iter: 1714 loss: 2.18051355e-07
Iter: 1715 loss: 2.18025207e-07
Iter: 1716 loss: 2.17996302e-07
Iter: 1717 loss: 2.17995336e-07
Iter: 1718 loss: 2.17964924e-07
Iter: 1719 loss: 2.17947388e-07
Iter: 1720 loss: 2.17920956e-07
Iter: 1721 loss: 2.17887e-07
Iter: 1722 loss: 2.1828393e-07
Iter: 1723 loss: 2.17879531e-07
Iter: 1724 loss: 2.17837297e-07
Iter: 1725 loss: 2.18024525e-07
Iter: 1726 loss: 2.17821636e-07
Iter: 1727 loss: 2.17800689e-07
Iter: 1728 loss: 2.18178542e-07
Iter: 1729 loss: 2.1780599e-07
Iter: 1730 loss: 2.17776304e-07
Iter: 1731 loss: 2.17773234e-07
Iter: 1732 loss: 2.17755598e-07
Iter: 1733 loss: 2.17740137e-07
Iter: 1734 loss: 2.1790072e-07
Iter: 1735 loss: 2.17734268e-07
Iter: 1736 loss: 2.17707978e-07
Iter: 1737 loss: 2.17673346e-07
Iter: 1738 loss: 2.17663739e-07
Iter: 1739 loss: 2.17624802e-07
Iter: 1740 loss: 2.18071264e-07
Iter: 1741 loss: 2.17622315e-07
Iter: 1742 loss: 2.17601595e-07
Iter: 1743 loss: 2.17548376e-07
Iter: 1744 loss: 2.18672582e-07
Iter: 1745 loss: 2.17540972e-07
Iter: 1746 loss: 2.1748599e-07
Iter: 1747 loss: 2.17557954e-07
Iter: 1748 loss: 2.17458478e-07
Iter: 1749 loss: 2.17394216e-07
Iter: 1750 loss: 2.17489387e-07
Iter: 1751 loss: 2.17379679e-07
Iter: 1752 loss: 2.17335582e-07
Iter: 1753 loss: 2.17322793e-07
Iter: 1754 loss: 2.17308624e-07
Iter: 1755 loss: 2.17254097e-07
Iter: 1756 loss: 2.1822359e-07
Iter: 1757 loss: 2.17254168e-07
Iter: 1758 loss: 2.17190234e-07
Iter: 1759 loss: 2.1727692e-07
Iter: 1760 loss: 2.17168946e-07
Iter: 1761 loss: 2.17142485e-07
Iter: 1762 loss: 2.17129681e-07
Iter: 1763 loss: 2.17107953e-07
Iter: 1764 loss: 2.17228632e-07
Iter: 1765 loss: 2.17112884e-07
Iter: 1766 loss: 2.17088143e-07
Iter: 1767 loss: 2.1707487e-07
Iter: 1768 loss: 2.17069527e-07
Iter: 1769 loss: 2.17035591e-07
Iter: 1770 loss: 2.17160647e-07
Iter: 1771 loss: 2.17035307e-07
Iter: 1772 loss: 2.16999013e-07
Iter: 1773 loss: 2.17009955e-07
Iter: 1774 loss: 2.16967663e-07
Iter: 1775 loss: 2.16918266e-07
Iter: 1776 loss: 2.17113197e-07
Iter: 1777 loss: 2.16903373e-07
Iter: 1778 loss: 2.16863143e-07
Iter: 1779 loss: 2.16794575e-07
Iter: 1780 loss: 2.1680205e-07
Iter: 1781 loss: 2.16736368e-07
Iter: 1782 loss: 2.17002338e-07
Iter: 1783 loss: 2.167221e-07
Iter: 1784 loss: 2.16678586e-07
Iter: 1785 loss: 2.17090843e-07
Iter: 1786 loss: 2.16674778e-07
Iter: 1787 loss: 2.16608356e-07
Iter: 1788 loss: 2.16693621e-07
Iter: 1789 loss: 2.1657435e-07
Iter: 1790 loss: 2.16519396e-07
Iter: 1791 loss: 2.16431602e-07
Iter: 1792 loss: 2.1643703e-07
Iter: 1793 loss: 2.16332182e-07
Iter: 1794 loss: 2.17063231e-07
Iter: 1795 loss: 2.16326072e-07
Iter: 1796 loss: 2.16256836e-07
Iter: 1797 loss: 2.17232838e-07
Iter: 1798 loss: 2.1625354e-07
Iter: 1799 loss: 2.16226198e-07
Iter: 1800 loss: 2.16438266e-07
Iter: 1801 loss: 2.16218098e-07
Iter: 1802 loss: 2.16198828e-07
Iter: 1803 loss: 2.16182784e-07
Iter: 1804 loss: 2.16169198e-07
Iter: 1805 loss: 2.16135021e-07
Iter: 1806 loss: 2.16236614e-07
Iter: 1807 loss: 2.16119574e-07
Iter: 1808 loss: 2.16081276e-07
Iter: 1809 loss: 2.16210935e-07
Iter: 1810 loss: 2.16066923e-07
Iter: 1811 loss: 2.16018606e-07
Iter: 1812 loss: 2.16025e-07
Iter: 1813 loss: 2.15979185e-07
Iter: 1814 loss: 2.15917453e-07
Iter: 1815 loss: 2.15837659e-07
Iter: 1816 loss: 2.15829402e-07
Iter: 1817 loss: 2.15741892e-07
Iter: 1818 loss: 2.16998046e-07
Iter: 1819 loss: 2.15741153e-07
Iter: 1820 loss: 2.15676138e-07
Iter: 1821 loss: 2.16135462e-07
Iter: 1822 loss: 2.15689283e-07
Iter: 1823 loss: 2.15647361e-07
Iter: 1824 loss: 2.15586169e-07
Iter: 1825 loss: 2.15590291e-07
Iter: 1826 loss: 2.15512898e-07
Iter: 1827 loss: 2.15608992e-07
Iter: 1828 loss: 2.1548135e-07
Iter: 1829 loss: 2.15424393e-07
Iter: 1830 loss: 2.15424436e-07
Iter: 1831 loss: 2.15369354e-07
Iter: 1832 loss: 2.15379032e-07
Iter: 1833 loss: 2.15332136e-07
Iter: 1834 loss: 2.15251205e-07
Iter: 1835 loss: 2.15632937e-07
Iter: 1836 loss: 2.15229875e-07
Iter: 1837 loss: 2.15184571e-07
Iter: 1838 loss: 2.15334808e-07
Iter: 1839 loss: 2.15161648e-07
Iter: 1840 loss: 2.15123265e-07
Iter: 1841 loss: 2.15146017e-07
Iter: 1842 loss: 2.15086203e-07
Iter: 1843 loss: 2.15019341e-07
Iter: 1844 loss: 2.15278661e-07
Iter: 1845 loss: 2.15005031e-07
Iter: 1846 loss: 2.14952408e-07
Iter: 1847 loss: 2.14918217e-07
Iter: 1848 loss: 2.14895522e-07
Iter: 1849 loss: 2.14831886e-07
Iter: 1850 loss: 2.14944308e-07
Iter: 1851 loss: 2.14802611e-07
Iter: 1852 loss: 2.14722604e-07
Iter: 1853 loss: 2.15491809e-07
Iter: 1854 loss: 2.14719151e-07
Iter: 1855 loss: 2.14654193e-07
Iter: 1856 loss: 2.14600647e-07
Iter: 1857 loss: 2.14575735e-07
Iter: 1858 loss: 2.14502748e-07
Iter: 1859 loss: 2.1461841e-07
Iter: 1860 loss: 2.14465558e-07
Iter: 1861 loss: 2.14369962e-07
Iter: 1862 loss: 2.1538473e-07
Iter: 1863 loss: 2.14374538e-07
Iter: 1864 loss: 2.14290708e-07
Iter: 1865 loss: 2.14369877e-07
Iter: 1866 loss: 2.14252907e-07
Iter: 1867 loss: 2.14178442e-07
Iter: 1868 loss: 2.14887621e-07
Iter: 1869 loss: 2.14169773e-07
Iter: 1870 loss: 2.14120334e-07
Iter: 1871 loss: 2.14121556e-07
Iter: 1872 loss: 2.14069047e-07
Iter: 1873 loss: 2.13980044e-07
Iter: 1874 loss: 2.14118586e-07
Iter: 1875 loss: 2.13939657e-07
Iter: 1876 loss: 2.1384183e-07
Iter: 1877 loss: 2.14362245e-07
Iter: 1878 loss: 2.13837168e-07
Iter: 1879 loss: 2.13749232e-07
Iter: 1880 loss: 2.13707068e-07
Iter: 1881 loss: 2.13673061e-07
Iter: 1882 loss: 2.13595015e-07
Iter: 1883 loss: 2.13915712e-07
Iter: 1884 loss: 2.13584258e-07
Iter: 1885 loss: 2.13537561e-07
Iter: 1886 loss: 2.13541952e-07
Iter: 1887 loss: 2.13496236e-07
Iter: 1888 loss: 2.13411127e-07
Iter: 1889 loss: 2.13411056e-07
Iter: 1890 loss: 2.13313285e-07
Iter: 1891 loss: 2.13392369e-07
Iter: 1892 loss: 2.13259668e-07
Iter: 1893 loss: 2.13183597e-07
Iter: 1894 loss: 2.13181437e-07
Iter: 1895 loss: 2.13117147e-07
Iter: 1896 loss: 2.1337496e-07
Iter: 1897 loss: 2.13108322e-07
Iter: 1898 loss: 2.13070109e-07
Iter: 1899 loss: 2.1327736e-07
Iter: 1900 loss: 2.13057803e-07
Iter: 1901 loss: 2.13008263e-07
Iter: 1902 loss: 2.13024393e-07
Iter: 1903 loss: 2.12974228e-07
Iter: 1904 loss: 2.12924718e-07
Iter: 1905 loss: 2.13258389e-07
Iter: 1906 loss: 2.129172e-07
Iter: 1907 loss: 2.12883549e-07
Iter: 1908 loss: 2.12989221e-07
Iter: 1909 loss: 2.12877751e-07
Iter: 1910 loss: 2.12838785e-07
Iter: 1911 loss: 2.12800671e-07
Iter: 1912 loss: 2.12795428e-07
Iter: 1913 loss: 2.12728509e-07
Iter: 1914 loss: 2.12838714e-07
Iter: 1915 loss: 2.12704748e-07
Iter: 1916 loss: 2.12678017e-07
Iter: 1917 loss: 2.12669576e-07
Iter: 1918 loss: 2.12635783e-07
Iter: 1919 loss: 2.12618687e-07
Iter: 1920 loss: 2.12604547e-07
Iter: 1921 loss: 2.12567755e-07
Iter: 1922 loss: 2.12570342e-07
Iter: 1923 loss: 2.12525649e-07
Iter: 1924 loss: 2.12481226e-07
Iter: 1925 loss: 2.12941842e-07
Iter: 1926 loss: 2.12483627e-07
Iter: 1927 loss: 2.12439915e-07
Iter: 1928 loss: 2.12663025e-07
Iter: 1929 loss: 2.12430166e-07
Iter: 1930 loss: 2.12407542e-07
Iter: 1931 loss: 2.12481893e-07
Iter: 1932 loss: 2.12392933e-07
Iter: 1933 loss: 2.12365251e-07
Iter: 1934 loss: 2.12382361e-07
Iter: 1935 loss: 2.12335607e-07
Iter: 1936 loss: 2.12294111e-07
Iter: 1937 loss: 2.12523389e-07
Iter: 1938 loss: 2.12297337e-07
Iter: 1939 loss: 2.12262634e-07
Iter: 1940 loss: 2.12324991e-07
Iter: 1941 loss: 2.12257248e-07
Iter: 1942 loss: 2.12230589e-07
Iter: 1943 loss: 2.12267167e-07
Iter: 1944 loss: 2.12216236e-07
Iter: 1945 loss: 2.12186336e-07
Iter: 1946 loss: 2.12165702e-07
Iter: 1947 loss: 2.12158483e-07
Iter: 1948 loss: 2.12116817e-07
Iter: 1949 loss: 2.12551242e-07
Iter: 1950 loss: 2.1212125e-07
Iter: 1951 loss: 2.12084387e-07
Iter: 1952 loss: 2.1207812e-07
Iter: 1953 loss: 2.12058012e-07
Iter: 1954 loss: 2.12018008e-07
Iter: 1955 loss: 2.12086363e-07
Iter: 1956 loss: 2.11994774e-07
Iter: 1957 loss: 2.11969152e-07
Iter: 1958 loss: 2.12136257e-07
Iter: 1959 loss: 2.11952838e-07
Iter: 1960 loss: 2.11924728e-07
Iter: 1961 loss: 2.12130743e-07
Iter: 1962 loss: 2.11914895e-07
Iter: 1963 loss: 2.11887553e-07
Iter: 1964 loss: 2.11957371e-07
Iter: 1965 loss: 2.11886032e-07
Iter: 1966 loss: 2.11859685e-07
Iter: 1967 loss: 2.11902076e-07
Iter: 1968 loss: 2.11846682e-07
Iter: 1969 loss: 2.11824897e-07
Iter: 1970 loss: 2.11915491e-07
Iter: 1971 loss: 2.11816868e-07
Iter: 1972 loss: 2.11795395e-07
Iter: 1973 loss: 2.11806395e-07
Iter: 1974 loss: 2.11776509e-07
Iter: 1975 loss: 2.11737103e-07
Iter: 1976 loss: 2.1180594e-07
Iter: 1977 loss: 2.11716014e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4
+ date
Tue Oct 20 16:39:03 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f344143cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3441346598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3441346378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f344124bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f344124b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f344124bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f344123d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f344123dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f344123d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34411b1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3441151598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f344116be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f344116e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3441115ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34410d98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34410d9730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3441103400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34410a2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34410809d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3441082ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34266819d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34266927b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34265e36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34266059d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34265f9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34265b6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f342656ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f342658a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f342658a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3426533378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34265338c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34264987b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34264982f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34264c3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3426477d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34264259d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.63421633e-06
Iter: 2 loss: 5.60758463e-06
Iter: 3 loss: 5.06884362e-06
Iter: 4 loss: 4.54195879e-06
Iter: 5 loss: 4.25702819e-06
Iter: 6 loss: 4.02092292e-06
Iter: 7 loss: 3.79968901e-06
Iter: 8 loss: 3.76575144e-06
Iter: 9 loss: 3.59990963e-06
Iter: 10 loss: 3.2005637e-06
Iter: 11 loss: 7.53166114e-06
Iter: 12 loss: 3.15866146e-06
Iter: 13 loss: 2.97758493e-06
Iter: 14 loss: 2.93640278e-06
Iter: 15 loss: 2.78124708e-06
Iter: 16 loss: 2.9199357e-06
Iter: 17 loss: 2.69075167e-06
Iter: 18 loss: 2.50508583e-06
Iter: 19 loss: 2.4206015e-06
Iter: 20 loss: 2.32795355e-06
Iter: 21 loss: 2.05203378e-06
Iter: 22 loss: 2.39399833e-06
Iter: 23 loss: 1.90851983e-06
Iter: 24 loss: 1.89971013e-06
Iter: 25 loss: 1.83487373e-06
Iter: 26 loss: 1.76494871e-06
Iter: 27 loss: 1.72985347e-06
Iter: 28 loss: 1.6968055e-06
Iter: 29 loss: 1.63468167e-06
Iter: 30 loss: 1.54968916e-06
Iter: 31 loss: 1.54564316e-06
Iter: 32 loss: 1.4408181e-06
Iter: 33 loss: 1.9537e-06
Iter: 34 loss: 1.42263286e-06
Iter: 35 loss: 1.35058383e-06
Iter: 36 loss: 1.34560378e-06
Iter: 37 loss: 1.3014976e-06
Iter: 38 loss: 1.22305312e-06
Iter: 39 loss: 1.22305244e-06
Iter: 40 loss: 1.17036143e-06
Iter: 41 loss: 1.16318665e-06
Iter: 42 loss: 1.14069053e-06
Iter: 43 loss: 1.10179303e-06
Iter: 44 loss: 1.10176211e-06
Iter: 45 loss: 1.0781996e-06
Iter: 46 loss: 1.07585583e-06
Iter: 47 loss: 1.05576521e-06
Iter: 48 loss: 1.01040234e-06
Iter: 49 loss: 1.62497417e-06
Iter: 50 loss: 1.00770501e-06
Iter: 51 loss: 9.55767405e-07
Iter: 52 loss: 1.27639805e-06
Iter: 53 loss: 9.49627292e-07
Iter: 54 loss: 9.08401262e-07
Iter: 55 loss: 9.01612509e-07
Iter: 56 loss: 8.73274871e-07
Iter: 57 loss: 8.42186353e-07
Iter: 58 loss: 8.4007263e-07
Iter: 59 loss: 8.06778701e-07
Iter: 60 loss: 8.13659085e-07
Iter: 61 loss: 7.82123607e-07
Iter: 62 loss: 7.60493663e-07
Iter: 63 loss: 7.50425443e-07
Iter: 64 loss: 7.39779466e-07
Iter: 65 loss: 7.1710042e-07
Iter: 66 loss: 8.96577205e-07
Iter: 67 loss: 7.15538079e-07
Iter: 68 loss: 6.91730577e-07
Iter: 69 loss: 8.76818149e-07
Iter: 70 loss: 6.90042384e-07
Iter: 71 loss: 6.79271068e-07
Iter: 72 loss: 6.73988609e-07
Iter: 73 loss: 6.68870882e-07
Iter: 74 loss: 6.44313729e-07
Iter: 75 loss: 6.95735878e-07
Iter: 76 loss: 6.34542744e-07
Iter: 77 loss: 6.19858042e-07
Iter: 78 loss: 6.25254302e-07
Iter: 79 loss: 6.09583253e-07
Iter: 80 loss: 5.94747803e-07
Iter: 81 loss: 5.94663788e-07
Iter: 82 loss: 5.88084163e-07
Iter: 83 loss: 5.78389916e-07
Iter: 84 loss: 5.78156346e-07
Iter: 85 loss: 5.68151222e-07
Iter: 86 loss: 6.35898175e-07
Iter: 87 loss: 5.67186532e-07
Iter: 88 loss: 5.58112106e-07
Iter: 89 loss: 5.4565254e-07
Iter: 90 loss: 5.45084049e-07
Iter: 91 loss: 5.3475685e-07
Iter: 92 loss: 5.33046773e-07
Iter: 93 loss: 5.24757752e-07
Iter: 94 loss: 5.07337631e-07
Iter: 95 loss: 7.99208408e-07
Iter: 96 loss: 5.06892206e-07
Iter: 97 loss: 4.90494472e-07
Iter: 98 loss: 5.02681644e-07
Iter: 99 loss: 4.80421306e-07
Iter: 100 loss: 4.73763549e-07
Iter: 101 loss: 4.72858574e-07
Iter: 102 loss: 4.65405464e-07
Iter: 103 loss: 5.04935258e-07
Iter: 104 loss: 4.64249212e-07
Iter: 105 loss: 4.6014847e-07
Iter: 106 loss: 4.58312627e-07
Iter: 107 loss: 4.56252678e-07
Iter: 108 loss: 4.48136205e-07
Iter: 109 loss: 4.86900262e-07
Iter: 110 loss: 4.4668468e-07
Iter: 111 loss: 4.41770396e-07
Iter: 112 loss: 4.31739977e-07
Iter: 113 loss: 6.14106625e-07
Iter: 114 loss: 4.31574904e-07
Iter: 115 loss: 4.23281932e-07
Iter: 116 loss: 4.22676749e-07
Iter: 117 loss: 4.17945728e-07
Iter: 118 loss: 4.10384104e-07
Iter: 119 loss: 4.10320553e-07
Iter: 120 loss: 4.04830757e-07
Iter: 121 loss: 4.38510313e-07
Iter: 122 loss: 4.04175523e-07
Iter: 123 loss: 3.98541147e-07
Iter: 124 loss: 4.11871213e-07
Iter: 125 loss: 3.96512775e-07
Iter: 126 loss: 3.93450875e-07
Iter: 127 loss: 3.93243738e-07
Iter: 128 loss: 3.90815444e-07
Iter: 129 loss: 3.83858691e-07
Iter: 130 loss: 4.16490678e-07
Iter: 131 loss: 3.81395239e-07
Iter: 132 loss: 3.71434282e-07
Iter: 133 loss: 3.90783839e-07
Iter: 134 loss: 3.67279824e-07
Iter: 135 loss: 3.62016266e-07
Iter: 136 loss: 3.61906814e-07
Iter: 137 loss: 3.57358857e-07
Iter: 138 loss: 4.02285337e-07
Iter: 139 loss: 3.57206318e-07
Iter: 140 loss: 3.55606915e-07
Iter: 141 loss: 3.54504493e-07
Iter: 142 loss: 3.53918097e-07
Iter: 143 loss: 3.50629819e-07
Iter: 144 loss: 3.66292227e-07
Iter: 145 loss: 3.50022788e-07
Iter: 146 loss: 3.47806179e-07
Iter: 147 loss: 3.42897039e-07
Iter: 148 loss: 4.14227259e-07
Iter: 149 loss: 3.42664407e-07
Iter: 150 loss: 3.39828631e-07
Iter: 151 loss: 3.39031715e-07
Iter: 152 loss: 3.36607684e-07
Iter: 153 loss: 3.29790197e-07
Iter: 154 loss: 3.6484056e-07
Iter: 155 loss: 3.27570092e-07
Iter: 156 loss: 3.21186405e-07
Iter: 157 loss: 3.72171144e-07
Iter: 158 loss: 3.20740099e-07
Iter: 159 loss: 3.18168844e-07
Iter: 160 loss: 3.18171857e-07
Iter: 161 loss: 3.159345e-07
Iter: 162 loss: 3.22402968e-07
Iter: 163 loss: 3.15255534e-07
Iter: 164 loss: 3.12413533e-07
Iter: 165 loss: 3.17092116e-07
Iter: 166 loss: 3.11114718e-07
Iter: 167 loss: 3.08488268e-07
Iter: 168 loss: 3.02910337e-07
Iter: 169 loss: 3.93621235e-07
Iter: 170 loss: 3.02757257e-07
Iter: 171 loss: 2.96843666e-07
Iter: 172 loss: 3.26689246e-07
Iter: 173 loss: 2.95846291e-07
Iter: 174 loss: 2.97027441e-07
Iter: 175 loss: 2.94307682e-07
Iter: 176 loss: 2.92868464e-07
Iter: 177 loss: 2.90282173e-07
Iter: 178 loss: 3.51931362e-07
Iter: 179 loss: 2.90279587e-07
Iter: 180 loss: 2.89390925e-07
Iter: 181 loss: 2.8919419e-07
Iter: 182 loss: 2.88153558e-07
Iter: 183 loss: 2.8559765e-07
Iter: 184 loss: 3.11066117e-07
Iter: 185 loss: 2.85289161e-07
Iter: 186 loss: 2.82292717e-07
Iter: 187 loss: 2.94078063e-07
Iter: 188 loss: 2.81615257e-07
Iter: 189 loss: 2.7776278e-07
Iter: 190 loss: 2.94143831e-07
Iter: 191 loss: 2.76971747e-07
Iter: 192 loss: 2.75281e-07
Iter: 193 loss: 2.72015086e-07
Iter: 194 loss: 3.38481584e-07
Iter: 195 loss: 2.71982685e-07
Iter: 196 loss: 2.69119369e-07
Iter: 197 loss: 2.84615595e-07
Iter: 198 loss: 2.68688467e-07
Iter: 199 loss: 2.67917784e-07
Iter: 200 loss: 2.67655878e-07
Iter: 201 loss: 2.66486381e-07
Iter: 202 loss: 2.6570234e-07
Iter: 203 loss: 2.65242335e-07
Iter: 204 loss: 2.63666038e-07
Iter: 205 loss: 2.60250857e-07
Iter: 206 loss: 3.13017267e-07
Iter: 207 loss: 2.60130719e-07
Iter: 208 loss: 2.5645889e-07
Iter: 209 loss: 2.91650252e-07
Iter: 210 loss: 2.56324199e-07
Iter: 211 loss: 2.56520423e-07
Iter: 212 loss: 2.55218481e-07
Iter: 213 loss: 2.54657692e-07
Iter: 214 loss: 2.53332729e-07
Iter: 215 loss: 2.69045699e-07
Iter: 216 loss: 2.53205485e-07
Iter: 217 loss: 2.51955441e-07
Iter: 218 loss: 2.51958227e-07
Iter: 219 loss: 2.51192319e-07
Iter: 220 loss: 2.50849098e-07
Iter: 221 loss: 2.50455855e-07
Iter: 222 loss: 2.49283687e-07
Iter: 223 loss: 2.5132573e-07
Iter: 224 loss: 2.48772608e-07
Iter: 225 loss: 2.47419308e-07
Iter: 226 loss: 2.55940307e-07
Iter: 227 loss: 2.47250398e-07
Iter: 228 loss: 2.45894853e-07
Iter: 229 loss: 2.43118592e-07
Iter: 230 loss: 2.91953199e-07
Iter: 231 loss: 2.430655e-07
Iter: 232 loss: 2.41066203e-07
Iter: 233 loss: 2.51073232e-07
Iter: 234 loss: 2.40733982e-07
Iter: 235 loss: 2.39749738e-07
Iter: 236 loss: 2.39731037e-07
Iter: 237 loss: 2.38633561e-07
Iter: 238 loss: 2.39978931e-07
Iter: 239 loss: 2.38057936e-07
Iter: 240 loss: 2.3722184e-07
Iter: 241 loss: 2.35479362e-07
Iter: 242 loss: 2.6364836e-07
Iter: 243 loss: 2.35405679e-07
Iter: 244 loss: 2.33273255e-07
Iter: 245 loss: 2.3965913e-07
Iter: 246 loss: 2.32623449e-07
Iter: 247 loss: 2.31640627e-07
Iter: 248 loss: 2.31189887e-07
Iter: 249 loss: 2.30517912e-07
Iter: 250 loss: 2.29225364e-07
Iter: 251 loss: 2.56761325e-07
Iter: 252 loss: 2.29211381e-07
Iter: 253 loss: 2.28729164e-07
Iter: 254 loss: 2.28499303e-07
Iter: 255 loss: 2.28116818e-07
Iter: 256 loss: 2.2726509e-07
Iter: 257 loss: 2.39367097e-07
Iter: 258 loss: 2.27228867e-07
Iter: 259 loss: 2.26373501e-07
Iter: 260 loss: 2.36627528e-07
Iter: 261 loss: 2.2636776e-07
Iter: 262 loss: 2.25556448e-07
Iter: 263 loss: 2.2578736e-07
Iter: 264 loss: 2.24978834e-07
Iter: 265 loss: 2.23958523e-07
Iter: 266 loss: 2.25399e-07
Iter: 267 loss: 2.23474729e-07
Iter: 268 loss: 2.22383548e-07
Iter: 269 loss: 2.21332e-07
Iter: 270 loss: 2.21098503e-07
Iter: 271 loss: 2.21022091e-07
Iter: 272 loss: 2.20296158e-07
Iter: 273 loss: 2.1975805e-07
Iter: 274 loss: 2.1892572e-07
Iter: 275 loss: 2.18915233e-07
Iter: 276 loss: 2.18169916e-07
Iter: 277 loss: 2.17242572e-07
Iter: 278 loss: 2.1716329e-07
Iter: 279 loss: 2.16166512e-07
Iter: 280 loss: 2.16160288e-07
Iter: 281 loss: 2.15038881e-07
Iter: 282 loss: 2.2033565e-07
Iter: 283 loss: 2.14837129e-07
Iter: 284 loss: 2.14223761e-07
Iter: 285 loss: 2.1350499e-07
Iter: 286 loss: 2.13442632e-07
Iter: 287 loss: 2.12090242e-07
Iter: 288 loss: 2.22193663e-07
Iter: 289 loss: 2.11973131e-07
Iter: 290 loss: 2.11364309e-07
Iter: 291 loss: 2.10824766e-07
Iter: 292 loss: 2.10667864e-07
Iter: 293 loss: 2.10298694e-07
Iter: 294 loss: 2.10175813e-07
Iter: 295 loss: 2.09854875e-07
Iter: 296 loss: 2.09274532e-07
Iter: 297 loss: 2.22747417e-07
Iter: 298 loss: 2.09270851e-07
Iter: 299 loss: 2.08472144e-07
Iter: 300 loss: 2.10328409e-07
Iter: 301 loss: 2.08188567e-07
Iter: 302 loss: 2.07270233e-07
Iter: 303 loss: 2.06542936e-07
Iter: 304 loss: 2.06246582e-07
Iter: 305 loss: 2.05955303e-07
Iter: 306 loss: 2.05499731e-07
Iter: 307 loss: 2.05053823e-07
Iter: 308 loss: 2.03949568e-07
Iter: 309 loss: 2.16481112e-07
Iter: 310 loss: 2.03838383e-07
Iter: 311 loss: 2.03113217e-07
Iter: 312 loss: 2.10039588e-07
Iter: 313 loss: 2.03075302e-07
Iter: 314 loss: 2.02712158e-07
Iter: 315 loss: 2.02458352e-07
Iter: 316 loss: 2.02314737e-07
Iter: 317 loss: 2.01811275e-07
Iter: 318 loss: 2.02806191e-07
Iter: 319 loss: 2.01603783e-07
Iter: 320 loss: 2.01074158e-07
Iter: 321 loss: 2.04577688e-07
Iter: 322 loss: 2.01016135e-07
Iter: 323 loss: 2.00304697e-07
Iter: 324 loss: 1.99862228e-07
Iter: 325 loss: 1.99584349e-07
Iter: 326 loss: 1.98922663e-07
Iter: 327 loss: 2.01087929e-07
Iter: 328 loss: 1.98749177e-07
Iter: 329 loss: 1.97969783e-07
Iter: 330 loss: 2.02139091e-07
Iter: 331 loss: 1.9785675e-07
Iter: 332 loss: 1.97448145e-07
Iter: 333 loss: 1.97213538e-07
Iter: 334 loss: 1.97042823e-07
Iter: 335 loss: 1.96491712e-07
Iter: 336 loss: 1.99265585e-07
Iter: 337 loss: 1.96404287e-07
Iter: 338 loss: 1.95953774e-07
Iter: 339 loss: 1.96787624e-07
Iter: 340 loss: 1.95735922e-07
Iter: 341 loss: 1.95265017e-07
Iter: 342 loss: 2.01698526e-07
Iter: 343 loss: 1.95258011e-07
Iter: 344 loss: 1.94941833e-07
Iter: 345 loss: 1.94002041e-07
Iter: 346 loss: 1.96551468e-07
Iter: 347 loss: 1.93470925e-07
Iter: 348 loss: 1.93375655e-07
Iter: 349 loss: 1.92762755e-07
Iter: 350 loss: 1.92322631e-07
Iter: 351 loss: 1.9182815e-07
Iter: 352 loss: 1.91761416e-07
Iter: 353 loss: 1.91219229e-07
Iter: 354 loss: 1.91456806e-07
Iter: 355 loss: 1.90830548e-07
Iter: 356 loss: 1.90548803e-07
Iter: 357 loss: 1.90506881e-07
Iter: 358 loss: 1.90198051e-07
Iter: 359 loss: 1.90164585e-07
Iter: 360 loss: 1.89939826e-07
Iter: 361 loss: 1.89620636e-07
Iter: 362 loss: 1.89398477e-07
Iter: 363 loss: 1.8928128e-07
Iter: 364 loss: 1.88644805e-07
Iter: 365 loss: 1.93904796e-07
Iter: 366 loss: 1.88603835e-07
Iter: 367 loss: 1.8812176e-07
Iter: 368 loss: 1.87534454e-07
Iter: 369 loss: 1.8746961e-07
Iter: 370 loss: 1.8681007e-07
Iter: 371 loss: 1.91999732e-07
Iter: 372 loss: 1.86773732e-07
Iter: 373 loss: 1.86323803e-07
Iter: 374 loss: 1.87320325e-07
Iter: 375 loss: 1.8614449e-07
Iter: 376 loss: 1.85713986e-07
Iter: 377 loss: 1.91333612e-07
Iter: 378 loss: 1.85717823e-07
Iter: 379 loss: 1.85509435e-07
Iter: 380 loss: 1.85095047e-07
Iter: 381 loss: 1.92964379e-07
Iter: 382 loss: 1.8507987e-07
Iter: 383 loss: 1.84803369e-07
Iter: 384 loss: 1.84768808e-07
Iter: 385 loss: 1.84486254e-07
Iter: 386 loss: 1.83754764e-07
Iter: 387 loss: 1.88630082e-07
Iter: 388 loss: 1.83580312e-07
Iter: 389 loss: 1.83033762e-07
Iter: 390 loss: 1.83020902e-07
Iter: 391 loss: 1.82563952e-07
Iter: 392 loss: 1.82620241e-07
Iter: 393 loss: 1.82219168e-07
Iter: 394 loss: 1.81786e-07
Iter: 395 loss: 1.82773078e-07
Iter: 396 loss: 1.81612023e-07
Iter: 397 loss: 1.81433677e-07
Iter: 398 loss: 1.81408382e-07
Iter: 399 loss: 1.81216194e-07
Iter: 400 loss: 1.80889487e-07
Iter: 401 loss: 1.80888776e-07
Iter: 402 loss: 1.80522107e-07
Iter: 403 loss: 1.81488701e-07
Iter: 404 loss: 1.80415668e-07
Iter: 405 loss: 1.80011e-07
Iter: 406 loss: 1.80176798e-07
Iter: 407 loss: 1.79734741e-07
Iter: 408 loss: 1.79324985e-07
Iter: 409 loss: 1.85213665e-07
Iter: 410 loss: 1.7931805e-07
Iter: 411 loss: 1.78908209e-07
Iter: 412 loss: 1.78545747e-07
Iter: 413 loss: 1.78429815e-07
Iter: 414 loss: 1.78139317e-07
Iter: 415 loss: 1.78131842e-07
Iter: 416 loss: 1.77830884e-07
Iter: 417 loss: 1.77888239e-07
Iter: 418 loss: 1.77591119e-07
Iter: 419 loss: 1.77375455e-07
Iter: 420 loss: 1.78198619e-07
Iter: 421 loss: 1.77327e-07
Iter: 422 loss: 1.77067747e-07
Iter: 423 loss: 1.78013991e-07
Iter: 424 loss: 1.76994675e-07
Iter: 425 loss: 1.76761176e-07
Iter: 426 loss: 1.76332492e-07
Iter: 427 loss: 1.86370301e-07
Iter: 428 loss: 1.76336471e-07
Iter: 429 loss: 1.75753854e-07
Iter: 430 loss: 1.76187015e-07
Iter: 431 loss: 1.75413675e-07
Iter: 432 loss: 1.75339707e-07
Iter: 433 loss: 1.75141182e-07
Iter: 434 loss: 1.74892193e-07
Iter: 435 loss: 1.74502247e-07
Iter: 436 loss: 1.74508912e-07
Iter: 437 loss: 1.74219252e-07
Iter: 438 loss: 1.76319475e-07
Iter: 439 loss: 1.74193985e-07
Iter: 440 loss: 1.73971898e-07
Iter: 441 loss: 1.74670674e-07
Iter: 442 loss: 1.73894989e-07
Iter: 443 loss: 1.73672106e-07
Iter: 444 loss: 1.75332445e-07
Iter: 445 loss: 1.73649013e-07
Iter: 446 loss: 1.73436774e-07
Iter: 447 loss: 1.73056662e-07
Iter: 448 loss: 1.73060243e-07
Iter: 449 loss: 1.72821757e-07
Iter: 450 loss: 1.72786372e-07
Iter: 451 loss: 1.72569685e-07
Iter: 452 loss: 1.72100499e-07
Iter: 453 loss: 1.78757631e-07
Iter: 454 loss: 1.72072447e-07
Iter: 455 loss: 1.71784265e-07
Iter: 456 loss: 1.71766686e-07
Iter: 457 loss: 1.71521805e-07
Iter: 458 loss: 1.7125106e-07
Iter: 459 loss: 1.71212037e-07
Iter: 460 loss: 1.70939501e-07
Iter: 461 loss: 1.71485965e-07
Iter: 462 loss: 1.70833516e-07
Iter: 463 loss: 1.70547537e-07
Iter: 464 loss: 1.70484583e-07
Iter: 465 loss: 1.70315914e-07
Iter: 466 loss: 1.69962632e-07
Iter: 467 loss: 1.73842224e-07
Iter: 468 loss: 1.69966427e-07
Iter: 469 loss: 1.69559769e-07
Iter: 470 loss: 1.69923339e-07
Iter: 471 loss: 1.69312841e-07
Iter: 472 loss: 1.68938129e-07
Iter: 473 loss: 1.68592592e-07
Iter: 474 loss: 1.68492619e-07
Iter: 475 loss: 1.68052694e-07
Iter: 476 loss: 1.70752202e-07
Iter: 477 loss: 1.68008256e-07
Iter: 478 loss: 1.6766019e-07
Iter: 479 loss: 1.70293021e-07
Iter: 480 loss: 1.67626979e-07
Iter: 481 loss: 1.67407762e-07
Iter: 482 loss: 1.68735824e-07
Iter: 483 loss: 1.67372932e-07
Iter: 484 loss: 1.67082362e-07
Iter: 485 loss: 1.67188716e-07
Iter: 486 loss: 1.66883922e-07
Iter: 487 loss: 1.66586233e-07
Iter: 488 loss: 1.66774115e-07
Iter: 489 loss: 1.66396248e-07
Iter: 490 loss: 1.65929237e-07
Iter: 491 loss: 1.6886726e-07
Iter: 492 loss: 1.65870588e-07
Iter: 493 loss: 1.65642149e-07
Iter: 494 loss: 1.65232763e-07
Iter: 495 loss: 1.74427413e-07
Iter: 496 loss: 1.65241772e-07
Iter: 497 loss: 1.64937035e-07
Iter: 498 loss: 1.6493567e-07
Iter: 499 loss: 1.64679605e-07
Iter: 500 loss: 1.66282661e-07
Iter: 501 loss: 1.64651652e-07
Iter: 502 loss: 1.64517019e-07
Iter: 503 loss: 1.64195654e-07
Iter: 504 loss: 1.67604682e-07
Iter: 505 loss: 1.64161e-07
Iter: 506 loss: 1.6399872e-07
Iter: 507 loss: 1.63951853e-07
Iter: 508 loss: 1.6372104e-07
Iter: 509 loss: 1.63316955e-07
Iter: 510 loss: 1.63314112e-07
Iter: 511 loss: 1.62824236e-07
Iter: 512 loss: 1.6275952e-07
Iter: 513 loss: 1.62414466e-07
Iter: 514 loss: 1.62020172e-07
Iter: 515 loss: 1.65338378e-07
Iter: 516 loss: 1.61999452e-07
Iter: 517 loss: 1.61906428e-07
Iter: 518 loss: 1.61846401e-07
Iter: 519 loss: 1.61705401e-07
Iter: 520 loss: 1.61465977e-07
Iter: 521 loss: 1.61465721e-07
Iter: 522 loss: 1.6124045e-07
Iter: 523 loss: 1.62324469e-07
Iter: 524 loss: 1.61201896e-07
Iter: 525 loss: 1.60965385e-07
Iter: 526 loss: 1.61888167e-07
Iter: 527 loss: 1.60913771e-07
Iter: 528 loss: 1.60646835e-07
Iter: 529 loss: 1.6048287e-07
Iter: 530 loss: 1.60373943e-07
Iter: 531 loss: 1.60057084e-07
Iter: 532 loss: 1.60509245e-07
Iter: 533 loss: 1.598913e-07
Iter: 534 loss: 1.59772057e-07
Iter: 535 loss: 1.59721282e-07
Iter: 536 loss: 1.59594379e-07
Iter: 537 loss: 1.59325623e-07
Iter: 538 loss: 1.62896669e-07
Iter: 539 loss: 1.59305699e-07
Iter: 540 loss: 1.59052036e-07
Iter: 541 loss: 1.59857819e-07
Iter: 542 loss: 1.58980299e-07
Iter: 543 loss: 1.58675931e-07
Iter: 544 loss: 1.61422022e-07
Iter: 545 loss: 1.58680891e-07
Iter: 546 loss: 1.58490266e-07
Iter: 547 loss: 1.58012682e-07
Iter: 548 loss: 1.62238322e-07
Iter: 549 loss: 1.57946687e-07
Iter: 550 loss: 1.57442955e-07
Iter: 551 loss: 1.60206426e-07
Iter: 552 loss: 1.57373734e-07
Iter: 553 loss: 1.57245921e-07
Iter: 554 loss: 1.57131552e-07
Iter: 555 loss: 1.57003797e-07
Iter: 556 loss: 1.56833948e-07
Iter: 557 loss: 1.56833195e-07
Iter: 558 loss: 1.5670858e-07
Iter: 559 loss: 1.56708907e-07
Iter: 560 loss: 1.56579887e-07
Iter: 561 loss: 1.56435519e-07
Iter: 562 loss: 1.56413847e-07
Iter: 563 loss: 1.56190367e-07
Iter: 564 loss: 1.5649519e-07
Iter: 565 loss: 1.56075743e-07
Iter: 566 loss: 1.55832879e-07
Iter: 567 loss: 1.56336341e-07
Iter: 568 loss: 1.55735549e-07
Iter: 569 loss: 1.5541913e-07
Iter: 570 loss: 1.57373549e-07
Iter: 571 loss: 1.55382764e-07
Iter: 572 loss: 1.55174575e-07
Iter: 573 loss: 1.54859009e-07
Iter: 574 loss: 1.54858697e-07
Iter: 575 loss: 1.54718833e-07
Iter: 576 loss: 1.54693979e-07
Iter: 577 loss: 1.54521359e-07
Iter: 578 loss: 1.54434204e-07
Iter: 579 loss: 1.54351127e-07
Iter: 580 loss: 1.54163075e-07
Iter: 581 loss: 1.5394582e-07
Iter: 582 loss: 1.53918279e-07
Iter: 583 loss: 1.53778416e-07
Iter: 584 loss: 1.53749809e-07
Iter: 585 loss: 1.53572529e-07
Iter: 586 loss: 1.53306985e-07
Iter: 587 loss: 1.53297492e-07
Iter: 588 loss: 1.53042805e-07
Iter: 589 loss: 1.53824857e-07
Iter: 590 loss: 1.52962244e-07
Iter: 591 loss: 1.52724695e-07
Iter: 592 loss: 1.55848227e-07
Iter: 593 loss: 1.52718513e-07
Iter: 594 loss: 1.5259576e-07
Iter: 595 loss: 1.5247204e-07
Iter: 596 loss: 1.52454106e-07
Iter: 597 loss: 1.52262373e-07
Iter: 598 loss: 1.53331044e-07
Iter: 599 loss: 1.52237675e-07
Iter: 600 loss: 1.5210766e-07
Iter: 601 loss: 1.52884439e-07
Iter: 602 loss: 1.52087338e-07
Iter: 603 loss: 1.51934444e-07
Iter: 604 loss: 1.51725956e-07
Iter: 605 loss: 1.51721437e-07
Iter: 606 loss: 1.5146253e-07
Iter: 607 loss: 1.51320847e-07
Iter: 608 loss: 1.51207473e-07
Iter: 609 loss: 1.51161089e-07
Iter: 610 loss: 1.51025176e-07
Iter: 611 loss: 1.5089158e-07
Iter: 612 loss: 1.50638016e-07
Iter: 613 loss: 1.55600901e-07
Iter: 614 loss: 1.50635e-07
Iter: 615 loss: 1.50484695e-07
Iter: 616 loss: 1.52646464e-07
Iter: 617 loss: 1.50488233e-07
Iter: 618 loss: 1.50343737e-07
Iter: 619 loss: 1.5109724e-07
Iter: 620 loss: 1.50326343e-07
Iter: 621 loss: 1.50238918e-07
Iter: 622 loss: 1.50056422e-07
Iter: 623 loss: 1.53045249e-07
Iter: 624 loss: 1.50043931e-07
Iter: 625 loss: 1.49922499e-07
Iter: 626 loss: 1.49914499e-07
Iter: 627 loss: 1.49772461e-07
Iter: 628 loss: 1.49546111e-07
Iter: 629 loss: 1.49546167e-07
Iter: 630 loss: 1.49284787e-07
Iter: 631 loss: 1.49406546e-07
Iter: 632 loss: 1.49106725e-07
Iter: 633 loss: 1.48961149e-07
Iter: 634 loss: 1.48936408e-07
Iter: 635 loss: 1.48803707e-07
Iter: 636 loss: 1.49101112e-07
Iter: 637 loss: 1.48754637e-07
Iter: 638 loss: 1.48608137e-07
Iter: 639 loss: 1.48505165e-07
Iter: 640 loss: 1.48456436e-07
Iter: 641 loss: 1.48244567e-07
Iter: 642 loss: 1.48210091e-07
Iter: 643 loss: 1.48080403e-07
Iter: 644 loss: 1.4791388e-07
Iter: 645 loss: 1.47878097e-07
Iter: 646 loss: 1.47763487e-07
Iter: 647 loss: 1.47455779e-07
Iter: 648 loss: 1.49369811e-07
Iter: 649 loss: 1.47377079e-07
Iter: 650 loss: 1.47253672e-07
Iter: 651 loss: 1.47164798e-07
Iter: 652 loss: 1.47052916e-07
Iter: 653 loss: 1.46864792e-07
Iter: 654 loss: 1.51146224e-07
Iter: 655 loss: 1.46858113e-07
Iter: 656 loss: 1.46767476e-07
Iter: 657 loss: 1.46752768e-07
Iter: 658 loss: 1.46666764e-07
Iter: 659 loss: 1.4643949e-07
Iter: 660 loss: 1.48661101e-07
Iter: 661 loss: 1.46410116e-07
Iter: 662 loss: 1.46153113e-07
Iter: 663 loss: 1.46751717e-07
Iter: 664 loss: 1.46045565e-07
Iter: 665 loss: 1.45753617e-07
Iter: 666 loss: 1.46395664e-07
Iter: 667 loss: 1.45621399e-07
Iter: 668 loss: 1.45384149e-07
Iter: 669 loss: 1.49172791e-07
Iter: 670 loss: 1.45384689e-07
Iter: 671 loss: 1.45166638e-07
Iter: 672 loss: 1.4575042e-07
Iter: 673 loss: 1.45082993e-07
Iter: 674 loss: 1.44926503e-07
Iter: 675 loss: 1.45457236e-07
Iter: 676 loss: 1.44882947e-07
Iter: 677 loss: 1.44733519e-07
Iter: 678 loss: 1.44736134e-07
Iter: 679 loss: 1.44632295e-07
Iter: 680 loss: 1.44467563e-07
Iter: 681 loss: 1.44961405e-07
Iter: 682 loss: 1.44416745e-07
Iter: 683 loss: 1.44186558e-07
Iter: 684 loss: 1.45666959e-07
Iter: 685 loss: 1.44164346e-07
Iter: 686 loss: 1.44032526e-07
Iter: 687 loss: 1.43766982e-07
Iter: 688 loss: 1.47621805e-07
Iter: 689 loss: 1.43751606e-07
Iter: 690 loss: 1.43695672e-07
Iter: 691 loss: 1.43579555e-07
Iter: 692 loss: 1.4350222e-07
Iter: 693 loss: 1.43286698e-07
Iter: 694 loss: 1.44949638e-07
Iter: 695 loss: 1.43229954e-07
Iter: 696 loss: 1.43045497e-07
Iter: 697 loss: 1.43193176e-07
Iter: 698 loss: 1.42922019e-07
Iter: 699 loss: 1.42830913e-07
Iter: 700 loss: 1.42792516e-07
Iter: 701 loss: 1.42683973e-07
Iter: 702 loss: 1.42491558e-07
Iter: 703 loss: 1.4248738e-07
Iter: 704 loss: 1.42292549e-07
Iter: 705 loss: 1.42383314e-07
Iter: 706 loss: 1.42161525e-07
Iter: 707 loss: 1.42027389e-07
Iter: 708 loss: 1.42018365e-07
Iter: 709 loss: 1.41911087e-07
Iter: 710 loss: 1.42223357e-07
Iter: 711 loss: 1.41871027e-07
Iter: 712 loss: 1.41773299e-07
Iter: 713 loss: 1.41726588e-07
Iter: 714 loss: 1.41677162e-07
Iter: 715 loss: 1.41527948e-07
Iter: 716 loss: 1.41640527e-07
Iter: 717 loss: 1.41437184e-07
Iter: 718 loss: 1.41287103e-07
Iter: 719 loss: 1.42919149e-07
Iter: 720 loss: 1.41286819e-07
Iter: 721 loss: 1.41105389e-07
Iter: 722 loss: 1.40966989e-07
Iter: 723 loss: 1.40920946e-07
Iter: 724 loss: 1.40705566e-07
Iter: 725 loss: 1.40689735e-07
Iter: 726 loss: 1.40534013e-07
Iter: 727 loss: 1.40334578e-07
Iter: 728 loss: 1.41994946e-07
Iter: 729 loss: 1.40320566e-07
Iter: 730 loss: 1.40269691e-07
Iter: 731 loss: 1.40222966e-07
Iter: 732 loss: 1.40181555e-07
Iter: 733 loss: 1.40059697e-07
Iter: 734 loss: 1.40885362e-07
Iter: 735 loss: 1.40033052e-07
Iter: 736 loss: 1.3993332e-07
Iter: 737 loss: 1.39930108e-07
Iter: 738 loss: 1.39832338e-07
Iter: 739 loss: 1.3971669e-07
Iter: 740 loss: 1.39717159e-07
Iter: 741 loss: 1.39542095e-07
Iter: 742 loss: 1.4002768e-07
Iter: 743 loss: 1.3949591e-07
Iter: 744 loss: 1.39287991e-07
Iter: 745 loss: 1.40050787e-07
Iter: 746 loss: 1.39235141e-07
Iter: 747 loss: 1.39031513e-07
Iter: 748 loss: 1.39120772e-07
Iter: 749 loss: 1.38902976e-07
Iter: 750 loss: 1.38726506e-07
Iter: 751 loss: 1.39004641e-07
Iter: 752 loss: 1.38651032e-07
Iter: 753 loss: 1.38495139e-07
Iter: 754 loss: 1.39088698e-07
Iter: 755 loss: 1.38458191e-07
Iter: 756 loss: 1.38334e-07
Iter: 757 loss: 1.40309652e-07
Iter: 758 loss: 1.38333959e-07
Iter: 759 loss: 1.38249362e-07
Iter: 760 loss: 1.38031055e-07
Iter: 761 loss: 1.39712071e-07
Iter: 762 loss: 1.37976429e-07
Iter: 763 loss: 1.37697072e-07
Iter: 764 loss: 1.38314533e-07
Iter: 765 loss: 1.37587108e-07
Iter: 766 loss: 1.3736701e-07
Iter: 767 loss: 1.37348025e-07
Iter: 768 loss: 1.37215977e-07
Iter: 769 loss: 1.36937729e-07
Iter: 770 loss: 1.40752434e-07
Iter: 771 loss: 1.36910941e-07
Iter: 772 loss: 1.36852492e-07
Iter: 773 loss: 1.36773622e-07
Iter: 774 loss: 1.36686552e-07
Iter: 775 loss: 1.36599496e-07
Iter: 776 loss: 1.36575565e-07
Iter: 777 loss: 1.36455654e-07
Iter: 778 loss: 1.37158e-07
Iter: 779 loss: 1.36449117e-07
Iter: 780 loss: 1.36329305e-07
Iter: 781 loss: 1.36661157e-07
Iter: 782 loss: 1.36277038e-07
Iter: 783 loss: 1.36141736e-07
Iter: 784 loss: 1.36009191e-07
Iter: 785 loss: 1.35973664e-07
Iter: 786 loss: 1.35749616e-07
Iter: 787 loss: 1.3598239e-07
Iter: 788 loss: 1.35611486e-07
Iter: 789 loss: 1.35336848e-07
Iter: 790 loss: 1.36192796e-07
Iter: 791 loss: 1.35260905e-07
Iter: 792 loss: 1.35134798e-07
Iter: 793 loss: 1.35107172e-07
Iter: 794 loss: 1.35017046e-07
Iter: 795 loss: 1.34864052e-07
Iter: 796 loss: 1.34861267e-07
Iter: 797 loss: 1.3471093e-07
Iter: 798 loss: 1.34619853e-07
Iter: 799 loss: 1.34561787e-07
Iter: 800 loss: 1.34582791e-07
Iter: 801 loss: 1.34458503e-07
Iter: 802 loss: 1.3436788e-07
Iter: 803 loss: 1.3411838e-07
Iter: 804 loss: 1.35796043e-07
Iter: 805 loss: 1.34067889e-07
Iter: 806 loss: 1.33889813e-07
Iter: 807 loss: 1.33892584e-07
Iter: 808 loss: 1.33688104e-07
Iter: 809 loss: 1.34020354e-07
Iter: 810 loss: 1.33610058e-07
Iter: 811 loss: 1.33485372e-07
Iter: 812 loss: 1.33578325e-07
Iter: 813 loss: 1.33408719e-07
Iter: 814 loss: 1.3329e-07
Iter: 815 loss: 1.34108973e-07
Iter: 816 loss: 1.3327903e-07
Iter: 817 loss: 1.33185395e-07
Iter: 818 loss: 1.34075293e-07
Iter: 819 loss: 1.33175021e-07
Iter: 820 loss: 1.33110831e-07
Iter: 821 loss: 1.32941338e-07
Iter: 822 loss: 1.34808076e-07
Iter: 823 loss: 1.32923276e-07
Iter: 824 loss: 1.32705011e-07
Iter: 825 loss: 1.33758789e-07
Iter: 826 loss: 1.3267281e-07
Iter: 827 loss: 1.32543079e-07
Iter: 828 loss: 1.32541956e-07
Iter: 829 loss: 1.32427772e-07
Iter: 830 loss: 1.32223988e-07
Iter: 831 loss: 1.36602821e-07
Iter: 832 loss: 1.32224613e-07
Iter: 833 loss: 1.32060649e-07
Iter: 834 loss: 1.32904532e-07
Iter: 835 loss: 1.32042942e-07
Iter: 836 loss: 1.31962253e-07
Iter: 837 loss: 1.31963034e-07
Iter: 838 loss: 1.3188091e-07
Iter: 839 loss: 1.31831e-07
Iter: 840 loss: 1.3179168e-07
Iter: 841 loss: 1.31686818e-07
Iter: 842 loss: 1.31633698e-07
Iter: 843 loss: 1.315845e-07
Iter: 844 loss: 1.31440643e-07
Iter: 845 loss: 1.31437744e-07
Iter: 846 loss: 1.31350831e-07
Iter: 847 loss: 1.31174673e-07
Iter: 848 loss: 1.33682775e-07
Iter: 849 loss: 1.31164768e-07
Iter: 850 loss: 1.31039229e-07
Iter: 851 loss: 1.31037368e-07
Iter: 852 loss: 1.30945125e-07
Iter: 853 loss: 1.31524232e-07
Iter: 854 loss: 1.30940677e-07
Iter: 855 loss: 1.30868187e-07
Iter: 856 loss: 1.30811685e-07
Iter: 857 loss: 1.30790497e-07
Iter: 858 loss: 1.30683532e-07
Iter: 859 loss: 1.30796607e-07
Iter: 860 loss: 1.30621572e-07
Iter: 861 loss: 1.30505356e-07
Iter: 862 loss: 1.31865747e-07
Iter: 863 loss: 1.30495778e-07
Iter: 864 loss: 1.30377074e-07
Iter: 865 loss: 1.30228059e-07
Iter: 866 loss: 1.30208221e-07
Iter: 867 loss: 1.30023395e-07
Iter: 868 loss: 1.30117485e-07
Iter: 869 loss: 1.29903057e-07
Iter: 870 loss: 1.29889742e-07
Iter: 871 loss: 1.29819341e-07
Iter: 872 loss: 1.2975238e-07
Iter: 873 loss: 1.29598121e-07
Iter: 874 loss: 1.3164599e-07
Iter: 875 loss: 1.29593104e-07
Iter: 876 loss: 1.29489081e-07
Iter: 877 loss: 1.29923393e-07
Iter: 878 loss: 1.29460062e-07
Iter: 879 loss: 1.29366811e-07
Iter: 880 loss: 1.30884814e-07
Iter: 881 loss: 1.29365617e-07
Iter: 882 loss: 1.2930326e-07
Iter: 883 loss: 1.29107519e-07
Iter: 884 loss: 1.30071086e-07
Iter: 885 loss: 1.29051855e-07
Iter: 886 loss: 1.28847773e-07
Iter: 887 loss: 1.30696009e-07
Iter: 888 loss: 1.2883433e-07
Iter: 889 loss: 1.28729681e-07
Iter: 890 loss: 1.28717872e-07
Iter: 891 loss: 1.28657746e-07
Iter: 892 loss: 1.28578534e-07
Iter: 893 loss: 1.28572623e-07
Iter: 894 loss: 1.28492132e-07
Iter: 895 loss: 1.28865992e-07
Iter: 896 loss: 1.28478e-07
Iter: 897 loss: 1.28402206e-07
Iter: 898 loss: 1.28981725e-07
Iter: 899 loss: 1.28400714e-07
Iter: 900 loss: 1.28326363e-07
Iter: 901 loss: 1.28284157e-07
Iter: 902 loss: 1.28261405e-07
Iter: 903 loss: 1.28158689e-07
Iter: 904 loss: 1.28205471e-07
Iter: 905 loss: 1.28089937e-07
Iter: 906 loss: 1.28012402e-07
Iter: 907 loss: 1.27999385e-07
Iter: 908 loss: 1.27941206e-07
Iter: 909 loss: 1.27787459e-07
Iter: 910 loss: 1.28825945e-07
Iter: 911 loss: 1.27752301e-07
Iter: 912 loss: 1.27627402e-07
Iter: 913 loss: 1.27627487e-07
Iter: 914 loss: 1.27529972e-07
Iter: 915 loss: 1.28294786e-07
Iter: 916 loss: 1.27526206e-07
Iter: 917 loss: 1.27481826e-07
Iter: 918 loss: 1.27395396e-07
Iter: 919 loss: 1.28730633e-07
Iter: 920 loss: 1.27396717e-07
Iter: 921 loss: 1.27274404e-07
Iter: 922 loss: 1.27992791e-07
Iter: 923 loss: 1.27258687e-07
Iter: 924 loss: 1.27156838e-07
Iter: 925 loss: 1.2824222e-07
Iter: 926 loss: 1.27153868e-07
Iter: 927 loss: 1.27082785e-07
Iter: 928 loss: 1.26963371e-07
Iter: 929 loss: 1.29775131e-07
Iter: 930 loss: 1.26967237e-07
Iter: 931 loss: 1.26837193e-07
Iter: 932 loss: 1.27218044e-07
Iter: 933 loss: 1.26792955e-07
Iter: 934 loss: 1.2670472e-07
Iter: 935 loss: 1.26708045e-07
Iter: 936 loss: 1.26629701e-07
Iter: 937 loss: 1.26655252e-07
Iter: 938 loss: 1.26578072e-07
Iter: 939 loss: 1.26514095e-07
Iter: 940 loss: 1.26525066e-07
Iter: 941 loss: 1.26470397e-07
Iter: 942 loss: 1.26405524e-07
Iter: 943 loss: 1.27345075e-07
Iter: 944 loss: 1.26403847e-07
Iter: 945 loss: 1.26326398e-07
Iter: 946 loss: 1.26203744e-07
Iter: 947 loss: 1.26202892e-07
Iter: 948 loss: 1.26092473e-07
Iter: 949 loss: 1.26352504e-07
Iter: 950 loss: 1.26043091e-07
Iter: 951 loss: 1.25906297e-07
Iter: 952 loss: 1.27027732e-07
Iter: 953 loss: 1.25890836e-07
Iter: 954 loss: 1.25834404e-07
Iter: 955 loss: 1.25705228e-07
Iter: 956 loss: 1.2784345e-07
Iter: 957 loss: 1.25697483e-07
Iter: 958 loss: 1.25575397e-07
Iter: 959 loss: 1.2593641e-07
Iter: 960 loss: 1.25533603e-07
Iter: 961 loss: 1.25436841e-07
Iter: 962 loss: 1.26999566e-07
Iter: 963 loss: 1.25438476e-07
Iter: 964 loss: 1.25352145e-07
Iter: 965 loss: 1.25782037e-07
Iter: 966 loss: 1.25335276e-07
Iter: 967 loss: 1.25258282e-07
Iter: 968 loss: 1.25111441e-07
Iter: 969 loss: 1.28453024e-07
Iter: 970 loss: 1.25112834e-07
Iter: 971 loss: 1.2493318e-07
Iter: 972 loss: 1.25367507e-07
Iter: 973 loss: 1.24869672e-07
Iter: 974 loss: 1.24761613e-07
Iter: 975 loss: 1.2474969e-07
Iter: 976 loss: 1.24659579e-07
Iter: 977 loss: 1.2473609e-07
Iter: 978 loss: 1.24591168e-07
Iter: 979 loss: 1.24507878e-07
Iter: 980 loss: 1.24636145e-07
Iter: 981 loss: 1.24465075e-07
Iter: 982 loss: 1.24382098e-07
Iter: 983 loss: 1.25618485e-07
Iter: 984 loss: 1.24384826e-07
Iter: 985 loss: 1.24321403e-07
Iter: 986 loss: 1.24201421e-07
Iter: 987 loss: 1.2591083e-07
Iter: 988 loss: 1.24197641e-07
Iter: 989 loss: 1.24115928e-07
Iter: 990 loss: 1.24115815e-07
Iter: 991 loss: 1.24030393e-07
Iter: 992 loss: 1.23834809e-07
Iter: 993 loss: 1.26112084e-07
Iter: 994 loss: 1.23817159e-07
Iter: 995 loss: 1.2364373e-07
Iter: 996 loss: 1.24190024e-07
Iter: 997 loss: 1.23596521e-07
Iter: 998 loss: 1.23496108e-07
Iter: 999 loss: 1.24163677e-07
Iter: 1000 loss: 1.23481712e-07
Iter: 1001 loss: 1.23403197e-07
Iter: 1002 loss: 1.24552e-07
Iter: 1003 loss: 1.23399843e-07
Iter: 1004 loss: 1.23347249e-07
Iter: 1005 loss: 1.23299728e-07
Iter: 1006 loss: 1.23285602e-07
Iter: 1007 loss: 1.23194567e-07
Iter: 1008 loss: 1.23302726e-07
Iter: 1009 loss: 1.23144261e-07
Iter: 1010 loss: 1.23039229e-07
Iter: 1011 loss: 1.23937227e-07
Iter: 1012 loss: 1.23030901e-07
Iter: 1013 loss: 1.22927801e-07
Iter: 1014 loss: 1.22822499e-07
Iter: 1015 loss: 1.22792443e-07
Iter: 1016 loss: 1.22703923e-07
Iter: 1017 loss: 1.22697898e-07
Iter: 1018 loss: 1.22604661e-07
Iter: 1019 loss: 1.22626403e-07
Iter: 1020 loss: 1.2253021e-07
Iter: 1021 loss: 1.22453301e-07
Iter: 1022 loss: 1.22625735e-07
Iter: 1023 loss: 1.22420971e-07
Iter: 1024 loss: 1.22344943e-07
Iter: 1025 loss: 1.2332265e-07
Iter: 1026 loss: 1.2234247e-07
Iter: 1027 loss: 1.22302836e-07
Iter: 1028 loss: 1.22163456e-07
Iter: 1029 loss: 1.22885851e-07
Iter: 1030 loss: 1.22132178e-07
Iter: 1031 loss: 1.21990951e-07
Iter: 1032 loss: 1.22474873e-07
Iter: 1033 loss: 1.21944353e-07
Iter: 1034 loss: 1.21782122e-07
Iter: 1035 loss: 1.22024204e-07
Iter: 1036 loss: 1.21704275e-07
Iter: 1037 loss: 1.21679193e-07
Iter: 1038 loss: 1.21630393e-07
Iter: 1039 loss: 1.21569812e-07
Iter: 1040 loss: 1.21436145e-07
Iter: 1041 loss: 1.22867988e-07
Iter: 1042 loss: 1.21424009e-07
Iter: 1043 loss: 1.21356805e-07
Iter: 1044 loss: 1.22120895e-07
Iter: 1045 loss: 1.21360614e-07
Iter: 1046 loss: 1.21288295e-07
Iter: 1047 loss: 1.21340321e-07
Iter: 1048 loss: 1.21249087e-07
Iter: 1049 loss: 1.21162273e-07
Iter: 1050 loss: 1.21153391e-07
Iter: 1051 loss: 1.21083815e-07
Iter: 1052 loss: 1.20993064e-07
Iter: 1053 loss: 1.20892068e-07
Iter: 1054 loss: 1.20872755e-07
Iter: 1055 loss: 1.20722248e-07
Iter: 1056 loss: 1.21871764e-07
Iter: 1057 loss: 1.20709316e-07
Iter: 1058 loss: 1.20632023e-07
Iter: 1059 loss: 1.21604813e-07
Iter: 1060 loss: 1.20620598e-07
Iter: 1061 loss: 1.20541642e-07
Iter: 1062 loss: 1.20662918e-07
Iter: 1063 loss: 1.20507508e-07
Iter: 1064 loss: 1.20431665e-07
Iter: 1065 loss: 1.20428709e-07
Iter: 1066 loss: 1.20378274e-07
Iter: 1067 loss: 1.20278088e-07
Iter: 1068 loss: 1.20291958e-07
Iter: 1069 loss: 1.20205897e-07
Iter: 1070 loss: 1.20068236e-07
Iter: 1071 loss: 1.20585511e-07
Iter: 1072 loss: 1.20040283e-07
Iter: 1073 loss: 1.19930206e-07
Iter: 1074 loss: 1.19928444e-07
Iter: 1075 loss: 1.19853809e-07
Iter: 1076 loss: 1.19745977e-07
Iter: 1077 loss: 1.19743731e-07
Iter: 1078 loss: 1.19665813e-07
Iter: 1079 loss: 1.19660655e-07
Iter: 1080 loss: 1.19605716e-07
Iter: 1081 loss: 1.1950425e-07
Iter: 1082 loss: 1.21828393e-07
Iter: 1083 loss: 1.19501806e-07
Iter: 1084 loss: 1.19447861e-07
Iter: 1085 loss: 1.19439932e-07
Iter: 1086 loss: 1.19391572e-07
Iter: 1087 loss: 1.19284209e-07
Iter: 1088 loss: 1.20661525e-07
Iter: 1089 loss: 1.19273437e-07
Iter: 1090 loss: 1.19158678e-07
Iter: 1091 loss: 1.19608444e-07
Iter: 1092 loss: 1.19139663e-07
Iter: 1093 loss: 1.19001157e-07
Iter: 1094 loss: 1.19930093e-07
Iter: 1095 loss: 1.18993107e-07
Iter: 1096 loss: 1.18869153e-07
Iter: 1097 loss: 1.18952727e-07
Iter: 1098 loss: 1.18799761e-07
Iter: 1099 loss: 1.18714141e-07
Iter: 1100 loss: 1.18779319e-07
Iter: 1101 loss: 1.18665028e-07
Iter: 1102 loss: 1.18548293e-07
Iter: 1103 loss: 1.18600454e-07
Iter: 1104 loss: 1.18482049e-07
Iter: 1105 loss: 1.18391156e-07
Iter: 1106 loss: 1.18391803e-07
Iter: 1107 loss: 1.18283253e-07
Iter: 1108 loss: 1.18181639e-07
Iter: 1109 loss: 1.18167605e-07
Iter: 1110 loss: 1.18026207e-07
Iter: 1111 loss: 1.18517534e-07
Iter: 1112 loss: 1.17996976e-07
Iter: 1113 loss: 1.17850547e-07
Iter: 1114 loss: 1.18498242e-07
Iter: 1115 loss: 1.17821145e-07
Iter: 1116 loss: 1.17732434e-07
Iter: 1117 loss: 1.17933254e-07
Iter: 1118 loss: 1.17698661e-07
Iter: 1119 loss: 1.17568312e-07
Iter: 1120 loss: 1.17926206e-07
Iter: 1121 loss: 1.17525786e-07
Iter: 1122 loss: 1.17468382e-07
Iter: 1123 loss: 1.17427192e-07
Iter: 1124 loss: 1.17408874e-07
Iter: 1125 loss: 1.17324177e-07
Iter: 1126 loss: 1.18150112e-07
Iter: 1127 loss: 1.17317128e-07
Iter: 1128 loss: 1.17241306e-07
Iter: 1129 loss: 1.17402188e-07
Iter: 1130 loss: 1.17211201e-07
Iter: 1131 loss: 1.17123882e-07
Iter: 1132 loss: 1.17036095e-07
Iter: 1133 loss: 1.17030766e-07
Iter: 1134 loss: 1.16896231e-07
Iter: 1135 loss: 1.17300871e-07
Iter: 1136 loss: 1.16855986e-07
Iter: 1137 loss: 1.16728536e-07
Iter: 1138 loss: 1.16840191e-07
Iter: 1139 loss: 1.16652863e-07
Iter: 1140 loss: 1.16648707e-07
Iter: 1141 loss: 1.16596446e-07
Iter: 1142 loss: 1.1655893e-07
Iter: 1143 loss: 1.16472904e-07
Iter: 1144 loss: 1.17892711e-07
Iter: 1145 loss: 1.16474233e-07
Iter: 1146 loss: 1.16417468e-07
Iter: 1147 loss: 1.16415436e-07
Iter: 1148 loss: 1.16352041e-07
Iter: 1149 loss: 1.16266023e-07
Iter: 1150 loss: 1.16268517e-07
Iter: 1151 loss: 1.16168721e-07
Iter: 1152 loss: 1.16168678e-07
Iter: 1153 loss: 1.16103493e-07
Iter: 1154 loss: 1.15975453e-07
Iter: 1155 loss: 1.18885865e-07
Iter: 1156 loss: 1.15975311e-07
Iter: 1157 loss: 1.1587322e-07
Iter: 1158 loss: 1.16856782e-07
Iter: 1159 loss: 1.15875217e-07
Iter: 1160 loss: 1.15792254e-07
Iter: 1161 loss: 1.16036219e-07
Iter: 1162 loss: 1.15768913e-07
Iter: 1163 loss: 1.15704985e-07
Iter: 1164 loss: 1.15863777e-07
Iter: 1165 loss: 1.15680152e-07
Iter: 1166 loss: 1.1562274e-07
Iter: 1167 loss: 1.15657542e-07
Iter: 1168 loss: 1.15575688e-07
Iter: 1169 loss: 1.15483189e-07
Iter: 1170 loss: 1.15584342e-07
Iter: 1171 loss: 1.15430069e-07
Iter: 1172 loss: 1.1533789e-07
Iter: 1173 loss: 1.16175443e-07
Iter: 1174 loss: 1.1533006e-07
Iter: 1175 loss: 1.15201928e-07
Iter: 1176 loss: 1.15216395e-07
Iter: 1177 loss: 1.15109081e-07
Iter: 1178 loss: 1.15023646e-07
Iter: 1179 loss: 1.15560091e-07
Iter: 1180 loss: 1.15008056e-07
Iter: 1181 loss: 1.14932895e-07
Iter: 1182 loss: 1.15180029e-07
Iter: 1183 loss: 1.14907223e-07
Iter: 1184 loss: 1.14852327e-07
Iter: 1185 loss: 1.15197061e-07
Iter: 1186 loss: 1.148492e-07
Iter: 1187 loss: 1.14797039e-07
Iter: 1188 loss: 1.14741482e-07
Iter: 1189 loss: 1.14726433e-07
Iter: 1190 loss: 1.1466004e-07
Iter: 1191 loss: 1.14949721e-07
Iter: 1192 loss: 1.14641793e-07
Iter: 1193 loss: 1.14581923e-07
Iter: 1194 loss: 1.14886006e-07
Iter: 1195 loss: 1.14568721e-07
Iter: 1196 loss: 1.14504289e-07
Iter: 1197 loss: 1.14463717e-07
Iter: 1198 loss: 1.14437739e-07
Iter: 1199 loss: 1.14339983e-07
Iter: 1200 loss: 1.14659159e-07
Iter: 1201 loss: 1.14308762e-07
Iter: 1202 loss: 1.1422388e-07
Iter: 1203 loss: 1.14461386e-07
Iter: 1204 loss: 1.14194251e-07
Iter: 1205 loss: 1.14121875e-07
Iter: 1206 loss: 1.14161345e-07
Iter: 1207 loss: 1.14075888e-07
Iter: 1208 loss: 1.14006113e-07
Iter: 1209 loss: 1.1400288e-07
Iter: 1210 loss: 1.13960382e-07
Iter: 1211 loss: 1.13862797e-07
Iter: 1212 loss: 1.14953522e-07
Iter: 1213 loss: 1.1384266e-07
Iter: 1214 loss: 1.13769147e-07
Iter: 1215 loss: 1.13760905e-07
Iter: 1216 loss: 1.13708225e-07
Iter: 1217 loss: 1.13628431e-07
Iter: 1218 loss: 1.13625816e-07
Iter: 1219 loss: 1.13566315e-07
Iter: 1220 loss: 1.1355759e-07
Iter: 1221 loss: 1.13518574e-07
Iter: 1222 loss: 1.13467955e-07
Iter: 1223 loss: 1.13469561e-07
Iter: 1224 loss: 1.13398563e-07
Iter: 1225 loss: 1.13393547e-07
Iter: 1226 loss: 1.13358539e-07
Iter: 1227 loss: 1.13415197e-07
Iter: 1228 loss: 1.13333641e-07
Iter: 1229 loss: 1.13272591e-07
Iter: 1230 loss: 1.13259176e-07
Iter: 1231 loss: 1.13210305e-07
Iter: 1232 loss: 1.13108335e-07
Iter: 1233 loss: 1.13269095e-07
Iter: 1234 loss: 1.13056295e-07
Iter: 1235 loss: 1.12935865e-07
Iter: 1236 loss: 1.13565847e-07
Iter: 1237 loss: 1.12922905e-07
Iter: 1238 loss: 1.12837427e-07
Iter: 1239 loss: 1.1324255e-07
Iter: 1240 loss: 1.12832041e-07
Iter: 1241 loss: 1.1278766e-07
Iter: 1242 loss: 1.12791071e-07
Iter: 1243 loss: 1.12757277e-07
Iter: 1244 loss: 1.12690195e-07
Iter: 1245 loss: 1.13766e-07
Iter: 1246 loss: 1.12693328e-07
Iter: 1247 loss: 1.12636712e-07
Iter: 1248 loss: 1.13305376e-07
Iter: 1249 loss: 1.12636393e-07
Iter: 1250 loss: 1.12572785e-07
Iter: 1251 loss: 1.12539091e-07
Iter: 1252 loss: 1.12509497e-07
Iter: 1253 loss: 1.12437981e-07
Iter: 1254 loss: 1.12369527e-07
Iter: 1255 loss: 1.12361e-07
Iter: 1256 loss: 1.12328969e-07
Iter: 1257 loss: 1.12295808e-07
Iter: 1258 loss: 1.122512e-07
Iter: 1259 loss: 1.12259187e-07
Iter: 1260 loss: 1.12219908e-07
Iter: 1261 loss: 1.12171982e-07
Iter: 1262 loss: 1.1244191e-07
Iter: 1263 loss: 1.12169609e-07
Iter: 1264 loss: 1.12126742e-07
Iter: 1265 loss: 1.12146189e-07
Iter: 1266 loss: 1.12095947e-07
Iter: 1267 loss: 1.12039722e-07
Iter: 1268 loss: 1.11964695e-07
Iter: 1269 loss: 1.11964745e-07
Iter: 1270 loss: 1.11854497e-07
Iter: 1271 loss: 1.12076094e-07
Iter: 1272 loss: 1.11810209e-07
Iter: 1273 loss: 1.11733463e-07
Iter: 1274 loss: 1.11730991e-07
Iter: 1275 loss: 1.11665834e-07
Iter: 1276 loss: 1.11711181e-07
Iter: 1277 loss: 1.11629163e-07
Iter: 1278 loss: 1.11576547e-07
Iter: 1279 loss: 1.11589358e-07
Iter: 1280 loss: 1.11525964e-07
Iter: 1281 loss: 1.11504683e-07
Iter: 1282 loss: 1.11487196e-07
Iter: 1283 loss: 1.11466512e-07
Iter: 1284 loss: 1.11401405e-07
Iter: 1285 loss: 1.12041917e-07
Iter: 1286 loss: 1.11387052e-07
Iter: 1287 loss: 1.11335339e-07
Iter: 1288 loss: 1.12043018e-07
Iter: 1289 loss: 1.11331616e-07
Iter: 1290 loss: 1.11271717e-07
Iter: 1291 loss: 1.11370206e-07
Iter: 1292 loss: 1.11242734e-07
Iter: 1293 loss: 1.11180022e-07
Iter: 1294 loss: 1.11340817e-07
Iter: 1295 loss: 1.11159466e-07
Iter: 1296 loss: 1.11079643e-07
Iter: 1297 loss: 1.11180697e-07
Iter: 1298 loss: 1.11036641e-07
Iter: 1299 loss: 1.10967434e-07
Iter: 1300 loss: 1.11095275e-07
Iter: 1301 loss: 1.1093757e-07
Iter: 1302 loss: 1.10868733e-07
Iter: 1303 loss: 1.10928937e-07
Iter: 1304 loss: 1.10840958e-07
Iter: 1305 loss: 1.107787e-07
Iter: 1306 loss: 1.1155123e-07
Iter: 1307 loss: 1.10782089e-07
Iter: 1308 loss: 1.10735328e-07
Iter: 1309 loss: 1.10857158e-07
Iter: 1310 loss: 1.10721615e-07
Iter: 1311 loss: 1.10669781e-07
Iter: 1312 loss: 1.10612739e-07
Iter: 1313 loss: 1.1060731e-07
Iter: 1314 loss: 1.10545344e-07
Iter: 1315 loss: 1.10544903e-07
Iter: 1316 loss: 1.10493175e-07
Iter: 1317 loss: 1.10424452e-07
Iter: 1318 loss: 1.10417488e-07
Iter: 1319 loss: 1.1035452e-07
Iter: 1320 loss: 1.10667713e-07
Iter: 1321 loss: 1.10338377e-07
Iter: 1322 loss: 1.10288902e-07
Iter: 1323 loss: 1.10288681e-07
Iter: 1324 loss: 1.10268658e-07
Iter: 1325 loss: 1.1030086e-07
Iter: 1326 loss: 1.10254618e-07
Iter: 1327 loss: 1.10231376e-07
Iter: 1328 loss: 1.1028402e-07
Iter: 1329 loss: 1.10213058e-07
Iter: 1330 loss: 1.10180288e-07
Iter: 1331 loss: 1.10174696e-07
Iter: 1332 loss: 1.1015107e-07
Iter: 1333 loss: 1.10093623e-07
Iter: 1334 loss: 1.10131957e-07
Iter: 1335 loss: 1.10059347e-07
Iter: 1336 loss: 1.09999078e-07
Iter: 1337 loss: 1.10289648e-07
Iter: 1338 loss: 1.09994851e-07
Iter: 1339 loss: 1.09922631e-07
Iter: 1340 loss: 1.10334071e-07
Iter: 1341 loss: 1.09915412e-07
Iter: 1342 loss: 1.09871195e-07
Iter: 1343 loss: 1.09896369e-07
Iter: 1344 loss: 1.09844159e-07
Iter: 1345 loss: 1.09813222e-07
Iter: 1346 loss: 1.10200439e-07
Iter: 1347 loss: 1.09811509e-07
Iter: 1348 loss: 1.09776536e-07
Iter: 1349 loss: 1.0978809e-07
Iter: 1350 loss: 1.09754552e-07
Iter: 1351 loss: 1.09717092e-07
Iter: 1352 loss: 1.09692394e-07
Iter: 1353 loss: 1.09679192e-07
Iter: 1354 loss: 1.09629717e-07
Iter: 1355 loss: 1.09630477e-07
Iter: 1356 loss: 1.09595518e-07
Iter: 1357 loss: 1.09597906e-07
Iter: 1358 loss: 1.09570912e-07
Iter: 1359 loss: 1.09524443e-07
Iter: 1360 loss: 1.09677245e-07
Iter: 1361 loss: 1.09518083e-07
Iter: 1362 loss: 1.0946831e-07
Iter: 1363 loss: 1.09491339e-07
Iter: 1364 loss: 1.09448429e-07
Iter: 1365 loss: 1.09392914e-07
Iter: 1366 loss: 1.09497769e-07
Iter: 1367 loss: 1.09366454e-07
Iter: 1368 loss: 1.0932358e-07
Iter: 1369 loss: 1.09422928e-07
Iter: 1370 loss: 1.09313227e-07
Iter: 1371 loss: 1.09267617e-07
Iter: 1372 loss: 1.09690994e-07
Iter: 1373 loss: 1.09268079e-07
Iter: 1374 loss: 1.09233682e-07
Iter: 1375 loss: 1.09182302e-07
Iter: 1376 loss: 1.09174437e-07
Iter: 1377 loss: 1.09108541e-07
Iter: 1378 loss: 1.09511021e-07
Iter: 1379 loss: 1.0910167e-07
Iter: 1380 loss: 1.09044791e-07
Iter: 1381 loss: 1.09181755e-07
Iter: 1382 loss: 1.09033017e-07
Iter: 1383 loss: 1.08990882e-07
Iter: 1384 loss: 1.09007203e-07
Iter: 1385 loss: 1.08957138e-07
Iter: 1386 loss: 1.08923871e-07
Iter: 1387 loss: 1.08925981e-07
Iter: 1388 loss: 1.08897538e-07
Iter: 1389 loss: 1.08878169e-07
Iter: 1390 loss: 1.08871518e-07
Iter: 1391 loss: 1.08826256e-07
Iter: 1392 loss: 1.08990562e-07
Iter: 1393 loss: 1.08814788e-07
Iter: 1394 loss: 1.08775737e-07
Iter: 1395 loss: 1.08771324e-07
Iter: 1396 loss: 1.08744572e-07
Iter: 1397 loss: 1.08695815e-07
Iter: 1398 loss: 1.08796527e-07
Iter: 1399 loss: 1.08666448e-07
Iter: 1400 loss: 1.08604283e-07
Iter: 1401 loss: 1.08664658e-07
Iter: 1402 loss: 1.08562652e-07
Iter: 1403 loss: 1.08528297e-07
Iter: 1404 loss: 1.08524461e-07
Iter: 1405 loss: 1.08495144e-07
Iter: 1406 loss: 1.08487406e-07
Iter: 1407 loss: 1.08467525e-07
Iter: 1408 loss: 1.08438698e-07
Iter: 1409 loss: 1.08625912e-07
Iter: 1410 loss: 1.08432516e-07
Iter: 1411 loss: 1.08392996e-07
Iter: 1412 loss: 1.0858281e-07
Iter: 1413 loss: 1.08396478e-07
Iter: 1414 loss: 1.08361249e-07
Iter: 1415 loss: 1.08301649e-07
Iter: 1416 loss: 1.09374163e-07
Iter: 1417 loss: 1.08298657e-07
Iter: 1418 loss: 1.08264338e-07
Iter: 1419 loss: 1.08258128e-07
Iter: 1420 loss: 1.08209491e-07
Iter: 1421 loss: 1.08171228e-07
Iter: 1422 loss: 1.08156442e-07
Iter: 1423 loss: 1.08102959e-07
Iter: 1424 loss: 1.08863809e-07
Iter: 1425 loss: 1.08102071e-07
Iter: 1426 loss: 1.08065123e-07
Iter: 1427 loss: 1.08017211e-07
Iter: 1428 loss: 1.08013282e-07
Iter: 1429 loss: 1.07946931e-07
Iter: 1430 loss: 1.08233699e-07
Iter: 1431 loss: 1.0793309e-07
Iter: 1432 loss: 1.0788559e-07
Iter: 1433 loss: 1.07977364e-07
Iter: 1434 loss: 1.07856522e-07
Iter: 1435 loss: 1.07804389e-07
Iter: 1436 loss: 1.08389195e-07
Iter: 1437 loss: 1.078e-07
Iter: 1438 loss: 1.07743944e-07
Iter: 1439 loss: 1.07758524e-07
Iter: 1440 loss: 1.07704551e-07
Iter: 1441 loss: 1.07652568e-07
Iter: 1442 loss: 1.07796879e-07
Iter: 1443 loss: 1.076384e-07
Iter: 1444 loss: 1.07587809e-07
Iter: 1445 loss: 1.08331683e-07
Iter: 1446 loss: 1.07585514e-07
Iter: 1447 loss: 1.07558115e-07
Iter: 1448 loss: 1.0748056e-07
Iter: 1449 loss: 1.08336224e-07
Iter: 1450 loss: 1.07469475e-07
Iter: 1451 loss: 1.07430871e-07
Iter: 1452 loss: 1.07425635e-07
Iter: 1453 loss: 1.07373658e-07
Iter: 1454 loss: 1.07377197e-07
Iter: 1455 loss: 1.07341805e-07
Iter: 1456 loss: 1.07293772e-07
Iter: 1457 loss: 1.07589585e-07
Iter: 1458 loss: 1.0728634e-07
Iter: 1459 loss: 1.07243274e-07
Iter: 1460 loss: 1.07245704e-07
Iter: 1461 loss: 1.07217339e-07
Iter: 1462 loss: 1.07145766e-07
Iter: 1463 loss: 1.07108107e-07
Iter: 1464 loss: 1.07081867e-07
Iter: 1465 loss: 1.07001156e-07
Iter: 1466 loss: 1.07517138e-07
Iter: 1467 loss: 1.06989255e-07
Iter: 1468 loss: 1.0695419e-07
Iter: 1469 loss: 1.06961409e-07
Iter: 1470 loss: 1.06925626e-07
Iter: 1471 loss: 1.06880357e-07
Iter: 1472 loss: 1.06879028e-07
Iter: 1473 loss: 1.06827933e-07
Iter: 1474 loss: 1.06928454e-07
Iter: 1475 loss: 1.0680553e-07
Iter: 1476 loss: 1.06748736e-07
Iter: 1477 loss: 1.06747223e-07
Iter: 1478 loss: 1.06708043e-07
Iter: 1479 loss: 1.06617236e-07
Iter: 1480 loss: 1.08323242e-07
Iter: 1481 loss: 1.06614529e-07
Iter: 1482 loss: 1.06558915e-07
Iter: 1483 loss: 1.07480169e-07
Iter: 1484 loss: 1.0655824e-07
Iter: 1485 loss: 1.06501034e-07
Iter: 1486 loss: 1.06671301e-07
Iter: 1487 loss: 1.06473493e-07
Iter: 1488 loss: 1.06441561e-07
Iter: 1489 loss: 1.06546509e-07
Iter: 1490 loss: 1.0641989e-07
Iter: 1491 loss: 1.0636554e-07
Iter: 1492 loss: 1.06544846e-07
Iter: 1493 loss: 1.06358883e-07
Iter: 1494 loss: 1.06326326e-07
Iter: 1495 loss: 1.06309685e-07
Iter: 1496 loss: 1.06294436e-07
Iter: 1497 loss: 1.06239455e-07
Iter: 1498 loss: 1.06270249e-07
Iter: 1499 loss: 1.06201263e-07
Iter: 1500 loss: 1.06144427e-07
Iter: 1501 loss: 1.06980316e-07
Iter: 1502 loss: 1.06143155e-07
Iter: 1503 loss: 1.06085039e-07
Iter: 1504 loss: 1.06125981e-07
Iter: 1505 loss: 1.0604775e-07
Iter: 1506 loss: 1.05997785e-07
Iter: 1507 loss: 1.05977378e-07
Iter: 1508 loss: 1.05946718e-07
Iter: 1509 loss: 1.05898607e-07
Iter: 1510 loss: 1.05898081e-07
Iter: 1511 loss: 1.05854788e-07
Iter: 1512 loss: 1.05859158e-07
Iter: 1513 loss: 1.0581909e-07
Iter: 1514 loss: 1.05782576e-07
Iter: 1515 loss: 1.05809718e-07
Iter: 1516 loss: 1.05758133e-07
Iter: 1517 loss: 1.05700835e-07
Iter: 1518 loss: 1.06133271e-07
Iter: 1519 loss: 1.0569287e-07
Iter: 1520 loss: 1.05657108e-07
Iter: 1521 loss: 1.05629184e-07
Iter: 1522 loss: 1.05609232e-07
Iter: 1523 loss: 1.05548814e-07
Iter: 1524 loss: 1.06078097e-07
Iter: 1525 loss: 1.05534312e-07
Iter: 1526 loss: 1.05490571e-07
Iter: 1527 loss: 1.05475173e-07
Iter: 1528 loss: 1.05450731e-07
Iter: 1529 loss: 1.05399906e-07
Iter: 1530 loss: 1.05562918e-07
Iter: 1531 loss: 1.0538227e-07
Iter: 1532 loss: 1.05351901e-07
Iter: 1533 loss: 1.05713937e-07
Iter: 1534 loss: 1.05353543e-07
Iter: 1535 loss: 1.0531884e-07
Iter: 1536 loss: 1.05376557e-07
Iter: 1537 loss: 1.05305617e-07
Iter: 1538 loss: 1.05261591e-07
Iter: 1539 loss: 1.05194871e-07
Iter: 1540 loss: 1.06747933e-07
Iter: 1541 loss: 1.05193074e-07
Iter: 1542 loss: 1.05129445e-07
Iter: 1543 loss: 1.06161039e-07
Iter: 1544 loss: 1.05129814e-07
Iter: 1545 loss: 1.05065368e-07
Iter: 1546 loss: 1.05346587e-07
Iter: 1547 loss: 1.05053964e-07
Iter: 1548 loss: 1.05020618e-07
Iter: 1549 loss: 1.04965949e-07
Iter: 1550 loss: 1.04959597e-07
Iter: 1551 loss: 1.04948775e-07
Iter: 1552 loss: 1.04926876e-07
Iter: 1553 loss: 1.04910626e-07
Iter: 1554 loss: 1.0487355e-07
Iter: 1555 loss: 1.05605778e-07
Iter: 1556 loss: 1.04872633e-07
Iter: 1557 loss: 1.04826732e-07
Iter: 1558 loss: 1.05369935e-07
Iter: 1559 loss: 1.04829319e-07
Iter: 1560 loss: 1.04795646e-07
Iter: 1561 loss: 1.04745141e-07
Iter: 1562 loss: 1.04743052e-07
Iter: 1563 loss: 1.04675436e-07
Iter: 1564 loss: 1.04821105e-07
Iter: 1565 loss: 1.04655193e-07
Iter: 1566 loss: 1.04593909e-07
Iter: 1567 loss: 1.05220579e-07
Iter: 1568 loss: 1.04587443e-07
Iter: 1569 loss: 1.04540483e-07
Iter: 1570 loss: 1.04803618e-07
Iter: 1571 loss: 1.04534024e-07
Iter: 1572 loss: 1.04513539e-07
Iter: 1573 loss: 1.04498675e-07
Iter: 1574 loss: 1.04483725e-07
Iter: 1575 loss: 1.04451374e-07
Iter: 1576 loss: 1.0449633e-07
Iter: 1577 loss: 1.04427897e-07
Iter: 1578 loss: 1.04397941e-07
Iter: 1579 loss: 1.04397323e-07
Iter: 1580 loss: 1.04374806e-07
Iter: 1581 loss: 1.04326929e-07
Iter: 1582 loss: 1.05164979e-07
Iter: 1583 loss: 1.04327e-07
Iter: 1584 loss: 1.04301414e-07
Iter: 1585 loss: 1.04291033e-07
Iter: 1586 loss: 1.04259655e-07
Iter: 1587 loss: 1.04222259e-07
Iter: 1588 loss: 1.05397703e-07
Iter: 1589 loss: 1.04216859e-07
Iter: 1590 loss: 1.04177651e-07
Iter: 1591 loss: 1.04178426e-07
Iter: 1592 loss: 1.04151056e-07
Iter: 1593 loss: 1.0419096e-07
Iter: 1594 loss: 1.04131651e-07
Iter: 1595 loss: 1.04101439e-07
Iter: 1596 loss: 1.04043139e-07
Iter: 1597 loss: 1.04050649e-07
Iter: 1598 loss: 1.03976504e-07
Iter: 1599 loss: 1.04394978e-07
Iter: 1600 loss: 1.03964545e-07
Iter: 1601 loss: 1.03904e-07
Iter: 1602 loss: 1.04473585e-07
Iter: 1603 loss: 1.03901208e-07
Iter: 1604 loss: 1.03848876e-07
Iter: 1605 loss: 1.04117163e-07
Iter: 1606 loss: 1.03839369e-07
Iter: 1607 loss: 1.03798598e-07
Iter: 1608 loss: 1.03739083e-07
Iter: 1609 loss: 1.03738671e-07
Iter: 1610 loss: 1.03723593e-07
Iter: 1611 loss: 1.03708174e-07
Iter: 1612 loss: 1.03687164e-07
Iter: 1613 loss: 1.03652283e-07
Iter: 1614 loss: 1.0365283e-07
Iter: 1615 loss: 1.03627123e-07
Iter: 1616 loss: 1.03625268e-07
Iter: 1617 loss: 1.03595575e-07
Iter: 1618 loss: 1.03591532e-07
Iter: 1619 loss: 1.03568453e-07
Iter: 1620 loss: 1.03534845e-07
Iter: 1621 loss: 1.03662877e-07
Iter: 1622 loss: 1.0352796e-07
Iter: 1623 loss: 1.03483245e-07
Iter: 1624 loss: 1.03492837e-07
Iter: 1625 loss: 1.03454028e-07
Iter: 1626 loss: 1.03389787e-07
Iter: 1627 loss: 1.03318442e-07
Iter: 1628 loss: 1.03314193e-07
Iter: 1629 loss: 1.03221169e-07
Iter: 1630 loss: 1.0371113e-07
Iter: 1631 loss: 1.03209274e-07
Iter: 1632 loss: 1.03169185e-07
Iter: 1633 loss: 1.03172212e-07
Iter: 1634 loss: 1.03130319e-07
Iter: 1635 loss: 1.03172141e-07
Iter: 1636 loss: 1.03117159e-07
Iter: 1637 loss: 1.03072047e-07
Iter: 1638 loss: 1.03101279e-07
Iter: 1639 loss: 1.03039312e-07
Iter: 1640 loss: 1.02990413e-07
Iter: 1641 loss: 1.03106792e-07
Iter: 1642 loss: 1.02981609e-07
Iter: 1643 loss: 1.02901467e-07
Iter: 1644 loss: 1.02914477e-07
Iter: 1645 loss: 1.02851438e-07
Iter: 1646 loss: 1.02776937e-07
Iter: 1647 loss: 1.02883206e-07
Iter: 1648 loss: 1.02740728e-07
Iter: 1649 loss: 1.02670761e-07
Iter: 1650 loss: 1.0266892e-07
Iter: 1651 loss: 1.02637927e-07
Iter: 1652 loss: 1.02618301e-07
Iter: 1653 loss: 1.02602684e-07
Iter: 1654 loss: 1.02564535e-07
Iter: 1655 loss: 1.03158982e-07
Iter: 1656 loss: 1.02566318e-07
Iter: 1657 loss: 1.025454e-07
Iter: 1658 loss: 1.02504757e-07
Iter: 1659 loss: 1.03450674e-07
Iter: 1660 loss: 1.02505432e-07
Iter: 1661 loss: 1.02458614e-07
Iter: 1662 loss: 1.02415569e-07
Iter: 1663 loss: 1.02406716e-07
Iter: 1664 loss: 1.02351031e-07
Iter: 1665 loss: 1.02608013e-07
Iter: 1666 loss: 1.02333317e-07
Iter: 1667 loss: 1.02279827e-07
Iter: 1668 loss: 1.02854145e-07
Iter: 1669 loss: 1.02281405e-07
Iter: 1670 loss: 1.02253111e-07
Iter: 1671 loss: 1.02342156e-07
Iter: 1672 loss: 1.02242922e-07
Iter: 1673 loss: 1.0222081e-07
Iter: 1674 loss: 1.02189198e-07
Iter: 1675 loss: 1.02185105e-07
Iter: 1676 loss: 1.02157607e-07
Iter: 1677 loss: 1.02159262e-07
Iter: 1678 loss: 1.02138443e-07
Iter: 1679 loss: 1.02106313e-07
Iter: 1680 loss: 1.02623417e-07
Iter: 1681 loss: 1.02104053e-07
Iter: 1682 loss: 1.02052809e-07
Iter: 1683 loss: 1.02376674e-07
Iter: 1684 loss: 1.02042115e-07
Iter: 1685 loss: 1.02006361e-07
Iter: 1686 loss: 1.01978358e-07
Iter: 1687 loss: 1.01975836e-07
Iter: 1688 loss: 1.01936116e-07
Iter: 1689 loss: 1.02349773e-07
Iter: 1690 loss: 1.01930112e-07
Iter: 1691 loss: 1.019e-07
Iter: 1692 loss: 1.01849501e-07
Iter: 1693 loss: 1.03165689e-07
Iter: 1694 loss: 1.01846574e-07
Iter: 1695 loss: 1.0178411e-07
Iter: 1696 loss: 1.01803565e-07
Iter: 1697 loss: 1.01741236e-07
Iter: 1698 loss: 1.01682673e-07
Iter: 1699 loss: 1.01663964e-07
Iter: 1700 loss: 1.01632871e-07
Iter: 1701 loss: 1.01635059e-07
Iter: 1702 loss: 1.01600087e-07
Iter: 1703 loss: 1.01568702e-07
Iter: 1704 loss: 1.01523298e-07
Iter: 1705 loss: 1.02504686e-07
Iter: 1706 loss: 1.01516882e-07
Iter: 1707 loss: 1.01462589e-07
Iter: 1708 loss: 1.01453637e-07
Iter: 1709 loss: 1.01417719e-07
Iter: 1710 loss: 1.01359745e-07
Iter: 1711 loss: 1.01621922e-07
Iter: 1712 loss: 1.01350921e-07
Iter: 1713 loss: 1.01318022e-07
Iter: 1714 loss: 1.01316971e-07
Iter: 1715 loss: 1.01281046e-07
Iter: 1716 loss: 1.01241518e-07
Iter: 1717 loss: 1.01239522e-07
Iter: 1718 loss: 1.01193621e-07
Iter: 1719 loss: 1.01394349e-07
Iter: 1720 loss: 1.01185847e-07
Iter: 1721 loss: 1.01146242e-07
Iter: 1722 loss: 1.01417456e-07
Iter: 1723 loss: 1.01145218e-07
Iter: 1724 loss: 1.01117422e-07
Iter: 1725 loss: 1.0107459e-07
Iter: 1726 loss: 1.01074292e-07
Iter: 1727 loss: 1.01033123e-07
Iter: 1728 loss: 1.01034956e-07
Iter: 1729 loss: 1.01002783e-07
Iter: 1730 loss: 1.00972628e-07
Iter: 1731 loss: 1.00970453e-07
Iter: 1732 loss: 1.00913638e-07
Iter: 1733 loss: 1.01235571e-07
Iter: 1734 loss: 1.00909453e-07
Iter: 1735 loss: 1.00878395e-07
Iter: 1736 loss: 1.01199781e-07
Iter: 1737 loss: 1.00874836e-07
Iter: 1738 loss: 1.0085266e-07
Iter: 1739 loss: 1.00822447e-07
Iter: 1740 loss: 1.00817346e-07
Iter: 1741 loss: 1.00769633e-07
Iter: 1742 loss: 1.0079259e-07
Iter: 1743 loss: 1.00744018e-07
Iter: 1744 loss: 1.00690812e-07
Iter: 1745 loss: 1.00752047e-07
Iter: 1746 loss: 1.00673176e-07
Iter: 1747 loss: 1.00650496e-07
Iter: 1748 loss: 1.00641955e-07
Iter: 1749 loss: 1.00610407e-07
Iter: 1750 loss: 1.00563057e-07
Iter: 1751 loss: 1.00559788e-07
Iter: 1752 loss: 1.00540305e-07
Iter: 1753 loss: 1.00538685e-07
Iter: 1754 loss: 1.00514221e-07
Iter: 1755 loss: 1.00485046e-07
Iter: 1756 loss: 1.00476292e-07
Iter: 1757 loss: 1.00453136e-07
Iter: 1758 loss: 1.00753745e-07
Iter: 1759 loss: 1.00452681e-07
Iter: 1760 loss: 1.00429126e-07
Iter: 1761 loss: 1.00409636e-07
Iter: 1762 loss: 1.0040327e-07
Iter: 1763 loss: 1.00356303e-07
Iter: 1764 loss: 1.00405678e-07
Iter: 1765 loss: 1.00332954e-07
Iter: 1766 loss: 1.00259754e-07
Iter: 1767 loss: 1.00593965e-07
Iter: 1768 loss: 1.00253345e-07
Iter: 1769 loss: 1.00196281e-07
Iter: 1770 loss: 1.00314473e-07
Iter: 1771 loss: 1.001779e-07
Iter: 1772 loss: 1.00119408e-07
Iter: 1773 loss: 1.00076846e-07
Iter: 1774 loss: 1.00051906e-07
Iter: 1775 loss: 9.99835947e-08
Iter: 1776 loss: 1.0015588e-07
Iter: 1777 loss: 9.99655114e-08
Iter: 1778 loss: 9.99345247e-08
Iter: 1779 loss: 9.99336294e-08
Iter: 1780 loss: 9.9903751e-08
Iter: 1781 loss: 9.9926e-08
Iter: 1782 loss: 9.98767149e-08
Iter: 1783 loss: 9.98432625e-08
Iter: 1784 loss: 9.98549226e-08
Iter: 1785 loss: 9.9828604e-08
Iter: 1786 loss: 9.97788874e-08
Iter: 1787 loss: 1.00084534e-07
Iter: 1788 loss: 9.97699843e-08
Iter: 1789 loss: 9.973121e-08
Iter: 1790 loss: 9.96799372e-08
Iter: 1791 loss: 9.96769742e-08
Iter: 1792 loss: 9.96158462e-08
Iter: 1793 loss: 9.96167842e-08
Iter: 1794 loss: 9.95797507e-08
Iter: 1795 loss: 9.95425466e-08
Iter: 1796 loss: 9.95331675e-08
Iter: 1797 loss: 9.94883749e-08
Iter: 1798 loss: 1.00146821e-07
Iter: 1799 loss: 9.94884459e-08
Iter: 1800 loss: 9.94540201e-08
Iter: 1801 loss: 9.94938674e-08
Iter: 1802 loss: 9.9440939e-08
Iter: 1803 loss: 9.9398143e-08
Iter: 1804 loss: 9.94477318e-08
Iter: 1805 loss: 9.93814524e-08
Iter: 1806 loss: 9.93209568e-08
Iter: 1807 loss: 9.93132616e-08
Iter: 1808 loss: 9.92697693e-08
Iter: 1809 loss: 9.92128335e-08
Iter: 1810 loss: 9.92138212e-08
Iter: 1811 loss: 9.91623068e-08
Iter: 1812 loss: 9.95548248e-08
Iter: 1813 loss: 9.91578872e-08
Iter: 1814 loss: 9.91305313e-08
Iter: 1815 loss: 9.90345512e-08
Iter: 1816 loss: 9.97905119e-08
Iter: 1817 loss: 9.90278295e-08
Iter: 1818 loss: 9.90795854e-08
Iter: 1819 loss: 9.89873e-08
Iter: 1820 loss: 9.89623672e-08
Iter: 1821 loss: 9.89337536e-08
Iter: 1822 loss: 9.89319631e-08
Iter: 1823 loss: 9.88995481e-08
Iter: 1824 loss: 9.92224329e-08
Iter: 1825 loss: 9.88990365e-08
Iter: 1826 loss: 9.88732e-08
Iter: 1827 loss: 9.88720927e-08
Iter: 1828 loss: 9.88490569e-08
Iter: 1829 loss: 9.88047475e-08
Iter: 1830 loss: 9.87700091e-08
Iter: 1831 loss: 9.87581785e-08
Iter: 1832 loss: 9.87002e-08
Iter: 1833 loss: 9.86884601e-08
Iter: 1834 loss: 9.8652464e-08
Iter: 1835 loss: 9.85980719e-08
Iter: 1836 loss: 9.8725053e-08
Iter: 1837 loss: 9.85705171e-08
Iter: 1838 loss: 9.85142492e-08
Iter: 1839 loss: 9.87811646e-08
Iter: 1840 loss: 9.85036053e-08
Iter: 1841 loss: 9.8449874e-08
Iter: 1842 loss: 9.88921158e-08
Iter: 1843 loss: 9.84455042e-08
Iter: 1844 loss: 9.83993829e-08
Iter: 1845 loss: 9.84059767e-08
Iter: 1846 loss: 9.83589814e-08
Iter: 1847 loss: 9.83195747e-08
Iter: 1848 loss: 9.83182957e-08
Iter: 1849 loss: 9.82850565e-08
Iter: 1850 loss: 9.8216077e-08
Iter: 1851 loss: 9.9171217e-08
Iter: 1852 loss: 9.82124533e-08
Iter: 1853 loss: 9.82428361e-08
Iter: 1854 loss: 9.8192011e-08
Iter: 1855 loss: 9.81760522e-08
Iter: 1856 loss: 9.81364e-08
Iter: 1857 loss: 9.8682932e-08
Iter: 1858 loss: 9.81310606e-08
Iter: 1859 loss: 9.81107533e-08
Iter: 1860 loss: 9.83912543e-08
Iter: 1861 loss: 9.81068524e-08
Iter: 1862 loss: 9.80815713e-08
Iter: 1863 loss: 9.81330572e-08
Iter: 1864 loss: 9.80691084e-08
Iter: 1865 loss: 9.80426194e-08
Iter: 1866 loss: 9.80390951e-08
Iter: 1867 loss: 9.80174804e-08
Iter: 1868 loss: 9.7986387e-08
Iter: 1869 loss: 9.79890942e-08
Iter: 1870 loss: 9.79594645e-08
Iter: 1871 loss: 9.79304247e-08
Iter: 1872 loss: 9.79261188e-08
Iter: 1873 loss: 9.78871668e-08
Iter: 1874 loss: 9.79529133e-08
Iter: 1875 loss: 9.78738512e-08
Iter: 1876 loss: 9.78364696e-08
Iter: 1877 loss: 9.82058737e-08
Iter: 1878 loss: 9.78345511e-08
Iter: 1879 loss: 9.78183579e-08
Iter: 1880 loss: 9.80052874e-08
Iter: 1881 loss: 9.78176473e-08
Iter: 1882 loss: 9.78004948e-08
Iter: 1883 loss: 9.77847634e-08
Iter: 1884 loss: 9.77782832e-08
Iter: 1885 loss: 9.77516947e-08
Iter: 1886 loss: 9.76963079e-08
Iter: 1887 loss: 9.88264617e-08
Iter: 1888 loss: 9.76949934e-08
Iter: 1889 loss: 9.76196759e-08
Iter: 1890 loss: 9.78656232e-08
Iter: 1891 loss: 9.76053443e-08
Iter: 1892 loss: 9.75618804e-08
Iter: 1893 loss: 9.75575958e-08
Iter: 1894 loss: 9.75295791e-08
Iter: 1895 loss: 9.74871e-08
Iter: 1896 loss: 9.80644472e-08
Iter: 1897 loss: 9.74790737e-08
Iter: 1898 loss: 9.74727357e-08
Iter: 1899 loss: 9.74576153e-08
Iter: 1900 loss: 9.74399939e-08
Iter: 1901 loss: 9.7415608e-08
Iter: 1902 loss: 9.74131353e-08
Iter: 1903 loss: 9.73837e-08
Iter: 1904 loss: 9.76477494e-08
Iter: 1905 loss: 9.73864047e-08
Iter: 1906 loss: 9.73572227e-08
Iter: 1907 loss: 9.73298881e-08
Iter: 1908 loss: 9.73195711e-08
Iter: 1909 loss: 9.72691936e-08
Iter: 1910 loss: 9.7241788e-08
Iter: 1911 loss: 9.72183614e-08
Iter: 1912 loss: 9.71707692e-08
Iter: 1913 loss: 9.7511581e-08
Iter: 1914 loss: 9.71605729e-08
Iter: 1915 loss: 9.71109841e-08
Iter: 1916 loss: 9.71729577e-08
Iter: 1917 loss: 9.7088531e-08
Iter: 1918 loss: 9.70536291e-08
Iter: 1919 loss: 9.72365939e-08
Iter: 1920 loss: 9.7048563e-08
Iter: 1921 loss: 9.7031787e-08
Iter: 1922 loss: 9.70203047e-08
Iter: 1923 loss: 9.7010485e-08
Iter: 1924 loss: 9.69814948e-08
Iter: 1925 loss: 9.7134162e-08
Iter: 1926 loss: 9.69728e-08
Iter: 1927 loss: 9.69314158e-08
Iter: 1928 loss: 9.69174536e-08
Iter: 1929 loss: 9.68951213e-08
Iter: 1930 loss: 9.68573346e-08
Iter: 1931 loss: 9.68501581e-08
Iter: 1932 loss: 9.68233067e-08
Iter: 1933 loss: 9.68092948e-08
Iter: 1934 loss: 9.68013438e-08
Iter: 1935 loss: 9.67815765e-08
Iter: 1936 loss: 9.67616529e-08
Iter: 1937 loss: 9.67610418e-08
Iter: 1938 loss: 9.6723042e-08
Iter: 1939 loss: 9.70268133e-08
Iter: 1940 loss: 9.67265805e-08
Iter: 1941 loss: 9.67007452e-08
Iter: 1942 loss: 9.66848646e-08
Iter: 1943 loss: 9.66735385e-08
Iter: 1944 loss: 9.66259748e-08
Iter: 1945 loss: 9.66884244e-08
Iter: 1946 loss: 9.66040261e-08
Iter: 1947 loss: 9.65589066e-08
Iter: 1948 loss: 9.65554534e-08
Iter: 1949 loss: 9.65294404e-08
Iter: 1950 loss: 9.65182778e-08
Iter: 1951 loss: 9.65061844e-08
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8
+ date
Tue Oct 20 16:45:19 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c82647840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e3bd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e3bdb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e4530d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e401620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e401730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e31dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e2d4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e2d4ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e2b2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e2b2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e265ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e2856a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e285620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e211ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e1d7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e18f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e1bfbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e177840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e183f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e1168c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e0da598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e08a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e0b2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e0b2400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e08aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e083b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e044950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7dfd6048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7e0441e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7dfa1d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7dfc31e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7dfc3378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7df7a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7df22bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c7df22b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.51272148e-06
Iter: 2 loss: 5.27405e-06
Iter: 3 loss: 5.24466532e-06
Iter: 4 loss: 4.62596654e-06
Iter: 5 loss: 5.27708562e-06
Iter: 6 loss: 4.28425938e-06
Iter: 7 loss: 3.95583811e-06
Iter: 8 loss: 6.29530359e-06
Iter: 9 loss: 3.9268175e-06
Iter: 10 loss: 3.59980686e-06
Iter: 11 loss: 3.88259696e-06
Iter: 12 loss: 3.40720862e-06
Iter: 13 loss: 3.14089175e-06
Iter: 14 loss: 4.82834366e-06
Iter: 15 loss: 3.1110244e-06
Iter: 16 loss: 2.93023e-06
Iter: 17 loss: 3.93418122e-06
Iter: 18 loss: 2.90450475e-06
Iter: 19 loss: 2.74563058e-06
Iter: 20 loss: 2.7030726e-06
Iter: 21 loss: 2.60462161e-06
Iter: 22 loss: 2.40120426e-06
Iter: 23 loss: 2.24505948e-06
Iter: 24 loss: 2.18008131e-06
Iter: 25 loss: 2.15946693e-06
Iter: 26 loss: 2.07463086e-06
Iter: 27 loss: 1.98360476e-06
Iter: 28 loss: 2.04370644e-06
Iter: 29 loss: 1.92606058e-06
Iter: 30 loss: 1.863443e-06
Iter: 31 loss: 1.75934019e-06
Iter: 32 loss: 1.75900095e-06
Iter: 33 loss: 1.6323271e-06
Iter: 34 loss: 2.11082124e-06
Iter: 35 loss: 1.6019859e-06
Iter: 36 loss: 1.49047696e-06
Iter: 37 loss: 2.14982629e-06
Iter: 38 loss: 1.47620517e-06
Iter: 39 loss: 1.36515337e-06
Iter: 40 loss: 2.48475089e-06
Iter: 41 loss: 1.36176732e-06
Iter: 42 loss: 1.31807269e-06
Iter: 43 loss: 1.33255048e-06
Iter: 44 loss: 1.28712873e-06
Iter: 45 loss: 1.23234622e-06
Iter: 46 loss: 1.81374048e-06
Iter: 47 loss: 1.23100631e-06
Iter: 48 loss: 1.20528648e-06
Iter: 49 loss: 1.19173455e-06
Iter: 50 loss: 1.18005914e-06
Iter: 51 loss: 1.13661486e-06
Iter: 52 loss: 1.45935212e-06
Iter: 53 loss: 1.13309875e-06
Iter: 54 loss: 1.10892722e-06
Iter: 55 loss: 1.09597886e-06
Iter: 56 loss: 1.08498386e-06
Iter: 57 loss: 1.03980051e-06
Iter: 58 loss: 1.05583604e-06
Iter: 59 loss: 1.00808666e-06
Iter: 60 loss: 9.6643123e-07
Iter: 61 loss: 1.20533571e-06
Iter: 62 loss: 9.60773377e-07
Iter: 63 loss: 9.19067475e-07
Iter: 64 loss: 1.25581914e-06
Iter: 65 loss: 9.16318072e-07
Iter: 66 loss: 8.93309732e-07
Iter: 67 loss: 8.5764e-07
Iter: 68 loss: 8.57147086e-07
Iter: 69 loss: 8.26494102e-07
Iter: 70 loss: 9.13984593e-07
Iter: 71 loss: 8.16892e-07
Iter: 72 loss: 7.88988e-07
Iter: 73 loss: 8.17139437e-07
Iter: 74 loss: 7.73369777e-07
Iter: 75 loss: 7.45665943e-07
Iter: 76 loss: 1.05874278e-06
Iter: 77 loss: 7.45157422e-07
Iter: 78 loss: 7.20982257e-07
Iter: 79 loss: 9.68086738e-07
Iter: 80 loss: 7.20252842e-07
Iter: 81 loss: 7.08684183e-07
Iter: 82 loss: 6.98336521e-07
Iter: 83 loss: 6.95438303e-07
Iter: 84 loss: 6.76432592e-07
Iter: 85 loss: 9.11784355e-07
Iter: 86 loss: 6.76246032e-07
Iter: 87 loss: 6.68431767e-07
Iter: 88 loss: 6.63077117e-07
Iter: 89 loss: 6.60244382e-07
Iter: 90 loss: 6.47474053e-07
Iter: 91 loss: 7.74352543e-07
Iter: 92 loss: 6.4707092e-07
Iter: 93 loss: 6.40629651e-07
Iter: 94 loss: 6.24843267e-07
Iter: 95 loss: 7.88616831e-07
Iter: 96 loss: 6.23049402e-07
Iter: 97 loss: 6.06210392e-07
Iter: 98 loss: 7.99974202e-07
Iter: 99 loss: 6.05960622e-07
Iter: 100 loss: 5.92381866e-07
Iter: 101 loss: 6.05518721e-07
Iter: 102 loss: 5.84659517e-07
Iter: 103 loss: 5.71128965e-07
Iter: 104 loss: 5.71126179e-07
Iter: 105 loss: 5.64283368e-07
Iter: 106 loss: 5.49234755e-07
Iter: 107 loss: 7.72026681e-07
Iter: 108 loss: 5.48571848e-07
Iter: 109 loss: 5.34906519e-07
Iter: 110 loss: 6.07111133e-07
Iter: 111 loss: 5.32790295e-07
Iter: 112 loss: 5.23639869e-07
Iter: 113 loss: 5.3626e-07
Iter: 114 loss: 5.19059427e-07
Iter: 115 loss: 5.14913154e-07
Iter: 116 loss: 5.13258897e-07
Iter: 117 loss: 5.07201833e-07
Iter: 118 loss: 4.94002222e-07
Iter: 119 loss: 6.92132232e-07
Iter: 120 loss: 4.93441632e-07
Iter: 121 loss: 4.89686556e-07
Iter: 122 loss: 4.8687275e-07
Iter: 123 loss: 4.82226824e-07
Iter: 124 loss: 4.71209887e-07
Iter: 125 loss: 5.96641371e-07
Iter: 126 loss: 4.70177952e-07
Iter: 127 loss: 4.66150453e-07
Iter: 128 loss: 4.64959498e-07
Iter: 129 loss: 4.60354357e-07
Iter: 130 loss: 4.63064737e-07
Iter: 131 loss: 4.57366895e-07
Iter: 132 loss: 4.53163437e-07
Iter: 133 loss: 4.47479778e-07
Iter: 134 loss: 4.47184732e-07
Iter: 135 loss: 4.42624867e-07
Iter: 136 loss: 4.42321266e-07
Iter: 137 loss: 4.37164488e-07
Iter: 138 loss: 4.31394682e-07
Iter: 139 loss: 4.30600039e-07
Iter: 140 loss: 4.23662357e-07
Iter: 141 loss: 4.69199e-07
Iter: 142 loss: 4.2295008e-07
Iter: 143 loss: 4.17237402e-07
Iter: 144 loss: 4.09026057e-07
Iter: 145 loss: 4.08775691e-07
Iter: 146 loss: 4.02259246e-07
Iter: 147 loss: 4.6396616e-07
Iter: 148 loss: 4.02015985e-07
Iter: 149 loss: 3.9937504e-07
Iter: 150 loss: 3.98999077e-07
Iter: 151 loss: 3.96618248e-07
Iter: 152 loss: 3.932212e-07
Iter: 153 loss: 3.93087134e-07
Iter: 154 loss: 3.90077133e-07
Iter: 155 loss: 4.28802394e-07
Iter: 156 loss: 3.90048342e-07
Iter: 157 loss: 3.86672411e-07
Iter: 158 loss: 3.80804067e-07
Iter: 159 loss: 3.80790709e-07
Iter: 160 loss: 3.76741269e-07
Iter: 161 loss: 4.35450033e-07
Iter: 162 loss: 3.76729872e-07
Iter: 163 loss: 3.7275376e-07
Iter: 164 loss: 3.76262676e-07
Iter: 165 loss: 3.70463823e-07
Iter: 166 loss: 3.66916424e-07
Iter: 167 loss: 3.6236159e-07
Iter: 168 loss: 3.62045739e-07
Iter: 169 loss: 3.62652543e-07
Iter: 170 loss: 3.59986274e-07
Iter: 171 loss: 3.58252805e-07
Iter: 172 loss: 3.55885959e-07
Iter: 173 loss: 3.55781538e-07
Iter: 174 loss: 3.5301e-07
Iter: 175 loss: 3.55785602e-07
Iter: 176 loss: 3.51439382e-07
Iter: 177 loss: 3.47257924e-07
Iter: 178 loss: 3.57736496e-07
Iter: 179 loss: 3.45783889e-07
Iter: 180 loss: 3.42097962e-07
Iter: 181 loss: 3.40534939e-07
Iter: 182 loss: 3.38641e-07
Iter: 183 loss: 3.3802732e-07
Iter: 184 loss: 3.36462733e-07
Iter: 185 loss: 3.34364472e-07
Iter: 186 loss: 3.33548314e-07
Iter: 187 loss: 3.32411275e-07
Iter: 188 loss: 3.30631849e-07
Iter: 189 loss: 3.42875438e-07
Iter: 190 loss: 3.30456089e-07
Iter: 191 loss: 3.28222711e-07
Iter: 192 loss: 3.27245687e-07
Iter: 193 loss: 3.26121778e-07
Iter: 194 loss: 3.24334621e-07
Iter: 195 loss: 3.41395207e-07
Iter: 196 loss: 3.24250266e-07
Iter: 197 loss: 3.22140636e-07
Iter: 198 loss: 3.18129196e-07
Iter: 199 loss: 4.05606499e-07
Iter: 200 loss: 3.18114274e-07
Iter: 201 loss: 3.1420376e-07
Iter: 202 loss: 3.17724243e-07
Iter: 203 loss: 3.11938578e-07
Iter: 204 loss: 3.09143729e-07
Iter: 205 loss: 3.51353265e-07
Iter: 206 loss: 3.09132133e-07
Iter: 207 loss: 3.06380798e-07
Iter: 208 loss: 3.18899936e-07
Iter: 209 loss: 3.05853632e-07
Iter: 210 loss: 3.04518949e-07
Iter: 211 loss: 3.02512575e-07
Iter: 212 loss: 3.0246585e-07
Iter: 213 loss: 3.00105853e-07
Iter: 214 loss: 3.18281963e-07
Iter: 215 loss: 2.99916451e-07
Iter: 216 loss: 2.97701575e-07
Iter: 217 loss: 2.97391637e-07
Iter: 218 loss: 2.95833587e-07
Iter: 219 loss: 2.93159303e-07
Iter: 220 loss: 3.16090961e-07
Iter: 221 loss: 2.93004916e-07
Iter: 222 loss: 2.90638923e-07
Iter: 223 loss: 3.08906e-07
Iter: 224 loss: 2.90443893e-07
Iter: 225 loss: 2.89290313e-07
Iter: 226 loss: 2.88104843e-07
Iter: 227 loss: 2.87886024e-07
Iter: 228 loss: 2.85647531e-07
Iter: 229 loss: 3.05608921e-07
Iter: 230 loss: 2.85549618e-07
Iter: 231 loss: 2.84539624e-07
Iter: 232 loss: 2.83643828e-07
Iter: 233 loss: 2.83387067e-07
Iter: 234 loss: 2.81625432e-07
Iter: 235 loss: 2.98967109e-07
Iter: 236 loss: 2.81551365e-07
Iter: 237 loss: 2.80524773e-07
Iter: 238 loss: 2.77664753e-07
Iter: 239 loss: 2.92541273e-07
Iter: 240 loss: 2.76752075e-07
Iter: 241 loss: 2.72885416e-07
Iter: 242 loss: 2.9317772e-07
Iter: 243 loss: 2.72303566e-07
Iter: 244 loss: 2.71428462e-07
Iter: 245 loss: 2.70731249e-07
Iter: 246 loss: 2.69467705e-07
Iter: 247 loss: 2.68240711e-07
Iter: 248 loss: 2.6795334e-07
Iter: 249 loss: 2.66676068e-07
Iter: 250 loss: 2.66334e-07
Iter: 251 loss: 2.65559464e-07
Iter: 252 loss: 2.64070081e-07
Iter: 253 loss: 2.8521481e-07
Iter: 254 loss: 2.64055359e-07
Iter: 255 loss: 2.62926392e-07
Iter: 256 loss: 2.65042843e-07
Iter: 257 loss: 2.62430717e-07
Iter: 258 loss: 2.60620681e-07
Iter: 259 loss: 2.64762889e-07
Iter: 260 loss: 2.59944613e-07
Iter: 261 loss: 2.58713499e-07
Iter: 262 loss: 2.57149679e-07
Iter: 263 loss: 2.57026358e-07
Iter: 264 loss: 2.55826734e-07
Iter: 265 loss: 2.55567386e-07
Iter: 266 loss: 2.54931933e-07
Iter: 267 loss: 2.53616719e-07
Iter: 268 loss: 2.78436914e-07
Iter: 269 loss: 2.53593896e-07
Iter: 270 loss: 2.52729194e-07
Iter: 271 loss: 2.52639495e-07
Iter: 272 loss: 2.5201993e-07
Iter: 273 loss: 2.50757466e-07
Iter: 274 loss: 2.73933892e-07
Iter: 275 loss: 2.50719665e-07
Iter: 276 loss: 2.49238326e-07
Iter: 277 loss: 2.53972104e-07
Iter: 278 loss: 2.48820243e-07
Iter: 279 loss: 2.47014782e-07
Iter: 280 loss: 2.62496229e-07
Iter: 281 loss: 2.46912975e-07
Iter: 282 loss: 2.45863788e-07
Iter: 283 loss: 2.4376277e-07
Iter: 284 loss: 2.81126916e-07
Iter: 285 loss: 2.43716784e-07
Iter: 286 loss: 2.41509156e-07
Iter: 287 loss: 2.50913274e-07
Iter: 288 loss: 2.41063645e-07
Iter: 289 loss: 2.39396854e-07
Iter: 290 loss: 2.52552127e-07
Iter: 291 loss: 2.3929141e-07
Iter: 292 loss: 2.38279313e-07
Iter: 293 loss: 2.53690445e-07
Iter: 294 loss: 2.38288195e-07
Iter: 295 loss: 2.37558083e-07
Iter: 296 loss: 2.39327221e-07
Iter: 297 loss: 2.37288646e-07
Iter: 298 loss: 2.36543102e-07
Iter: 299 loss: 2.35521128e-07
Iter: 300 loss: 2.35467596e-07
Iter: 301 loss: 2.34572312e-07
Iter: 302 loss: 2.34522986e-07
Iter: 303 loss: 2.33793642e-07
Iter: 304 loss: 2.3201882e-07
Iter: 305 loss: 2.50454065e-07
Iter: 306 loss: 2.31804634e-07
Iter: 307 loss: 2.31157244e-07
Iter: 308 loss: 2.30838396e-07
Iter: 309 loss: 2.3005579e-07
Iter: 310 loss: 2.28755056e-07
Iter: 311 loss: 2.28746316e-07
Iter: 312 loss: 2.27804165e-07
Iter: 313 loss: 2.34763775e-07
Iter: 314 loss: 2.27721216e-07
Iter: 315 loss: 2.26839376e-07
Iter: 316 loss: 2.33487427e-07
Iter: 317 loss: 2.26762637e-07
Iter: 318 loss: 2.2623118e-07
Iter: 319 loss: 2.25060688e-07
Iter: 320 loss: 2.4361816e-07
Iter: 321 loss: 2.25035819e-07
Iter: 322 loss: 2.23599983e-07
Iter: 323 loss: 2.23905303e-07
Iter: 324 loss: 2.22530815e-07
Iter: 325 loss: 2.20876942e-07
Iter: 326 loss: 2.35846585e-07
Iter: 327 loss: 2.20819658e-07
Iter: 328 loss: 2.19857952e-07
Iter: 329 loss: 2.19835073e-07
Iter: 330 loss: 2.19187427e-07
Iter: 331 loss: 2.19856673e-07
Iter: 332 loss: 2.18853501e-07
Iter: 333 loss: 2.18157027e-07
Iter: 334 loss: 2.18597307e-07
Iter: 335 loss: 2.17696098e-07
Iter: 336 loss: 2.16985882e-07
Iter: 337 loss: 2.26039063e-07
Iter: 338 loss: 2.16983437e-07
Iter: 339 loss: 2.16456158e-07
Iter: 340 loss: 2.15385313e-07
Iter: 341 loss: 2.35760467e-07
Iter: 342 loss: 2.15384915e-07
Iter: 343 loss: 2.14648978e-07
Iter: 344 loss: 2.14634412e-07
Iter: 345 loss: 2.13876191e-07
Iter: 346 loss: 2.12526942e-07
Iter: 347 loss: 2.12525549e-07
Iter: 348 loss: 2.11446917e-07
Iter: 349 loss: 2.18161759e-07
Iter: 350 loss: 2.1130414e-07
Iter: 351 loss: 2.1065685e-07
Iter: 352 loss: 2.10646675e-07
Iter: 353 loss: 2.10285293e-07
Iter: 354 loss: 2.09318614e-07
Iter: 355 loss: 2.1778591e-07
Iter: 356 loss: 2.09187021e-07
Iter: 357 loss: 2.08286735e-07
Iter: 358 loss: 2.09723694e-07
Iter: 359 loss: 2.0788967e-07
Iter: 360 loss: 2.06607638e-07
Iter: 361 loss: 2.09850214e-07
Iter: 362 loss: 2.0616497e-07
Iter: 363 loss: 2.05546456e-07
Iter: 364 loss: 2.05352151e-07
Iter: 365 loss: 2.0480438e-07
Iter: 366 loss: 2.03983191e-07
Iter: 367 loss: 2.0395882e-07
Iter: 368 loss: 2.03068652e-07
Iter: 369 loss: 2.11279058e-07
Iter: 370 loss: 2.03032954e-07
Iter: 371 loss: 2.02466467e-07
Iter: 372 loss: 2.05651986e-07
Iter: 373 loss: 2.02379084e-07
Iter: 374 loss: 2.01891979e-07
Iter: 375 loss: 2.01728824e-07
Iter: 376 loss: 2.01439349e-07
Iter: 377 loss: 2.00965403e-07
Iter: 378 loss: 2.04137066e-07
Iter: 379 loss: 2.00917867e-07
Iter: 380 loss: 2.00313863e-07
Iter: 381 loss: 1.9970129e-07
Iter: 382 loss: 1.9958037e-07
Iter: 383 loss: 1.98736188e-07
Iter: 384 loss: 1.9846641e-07
Iter: 385 loss: 1.97977656e-07
Iter: 386 loss: 1.9747273e-07
Iter: 387 loss: 1.97285473e-07
Iter: 388 loss: 1.96701663e-07
Iter: 389 loss: 1.96008159e-07
Iter: 390 loss: 1.9593044e-07
Iter: 391 loss: 1.95263681e-07
Iter: 392 loss: 1.9537606e-07
Iter: 393 loss: 1.94758485e-07
Iter: 394 loss: 1.9403538e-07
Iter: 395 loss: 1.96440226e-07
Iter: 396 loss: 1.93827489e-07
Iter: 397 loss: 1.93554769e-07
Iter: 398 loss: 1.93406905e-07
Iter: 399 loss: 1.93089818e-07
Iter: 400 loss: 1.92380398e-07
Iter: 401 loss: 2.01788723e-07
Iter: 402 loss: 1.92334625e-07
Iter: 403 loss: 1.91549304e-07
Iter: 404 loss: 1.96189916e-07
Iter: 405 loss: 1.91444528e-07
Iter: 406 loss: 1.90575804e-07
Iter: 407 loss: 1.93740902e-07
Iter: 408 loss: 1.90358946e-07
Iter: 409 loss: 1.89813306e-07
Iter: 410 loss: 1.92253083e-07
Iter: 411 loss: 1.89689928e-07
Iter: 412 loss: 1.89331701e-07
Iter: 413 loss: 1.89504e-07
Iter: 414 loss: 1.89085696e-07
Iter: 415 loss: 1.88458301e-07
Iter: 416 loss: 1.90296788e-07
Iter: 417 loss: 1.88253523e-07
Iter: 418 loss: 1.87852365e-07
Iter: 419 loss: 1.87477667e-07
Iter: 420 loss: 1.87363781e-07
Iter: 421 loss: 1.86735292e-07
Iter: 422 loss: 1.89414777e-07
Iter: 423 loss: 1.86598527e-07
Iter: 424 loss: 1.85750793e-07
Iter: 425 loss: 1.89282673e-07
Iter: 426 loss: 1.85578443e-07
Iter: 427 loss: 1.85010066e-07
Iter: 428 loss: 1.84183477e-07
Iter: 429 loss: 1.84167675e-07
Iter: 430 loss: 1.83475464e-07
Iter: 431 loss: 1.85876644e-07
Iter: 432 loss: 1.83303356e-07
Iter: 433 loss: 1.83056358e-07
Iter: 434 loss: 1.82962921e-07
Iter: 435 loss: 1.82608275e-07
Iter: 436 loss: 1.82276381e-07
Iter: 437 loss: 1.82197084e-07
Iter: 438 loss: 1.8177812e-07
Iter: 439 loss: 1.83239095e-07
Iter: 440 loss: 1.81657384e-07
Iter: 441 loss: 1.81135107e-07
Iter: 442 loss: 1.82747129e-07
Iter: 443 loss: 1.80987982e-07
Iter: 444 loss: 1.80486524e-07
Iter: 445 loss: 1.8033478e-07
Iter: 446 loss: 1.80045475e-07
Iter: 447 loss: 1.79246413e-07
Iter: 448 loss: 1.81832064e-07
Iter: 449 loss: 1.79016638e-07
Iter: 450 loss: 1.78484868e-07
Iter: 451 loss: 1.85821207e-07
Iter: 452 loss: 1.78477563e-07
Iter: 453 loss: 1.78172883e-07
Iter: 454 loss: 1.77577292e-07
Iter: 455 loss: 1.89903801e-07
Iter: 456 loss: 1.77578045e-07
Iter: 457 loss: 1.77047752e-07
Iter: 458 loss: 1.81315016e-07
Iter: 459 loss: 1.77007252e-07
Iter: 460 loss: 1.76513026e-07
Iter: 461 loss: 1.80427122e-07
Iter: 462 loss: 1.76483425e-07
Iter: 463 loss: 1.76167333e-07
Iter: 464 loss: 1.75458013e-07
Iter: 465 loss: 1.85057417e-07
Iter: 466 loss: 1.75410293e-07
Iter: 467 loss: 1.74528481e-07
Iter: 468 loss: 1.7572512e-07
Iter: 469 loss: 1.7410477e-07
Iter: 470 loss: 1.74272898e-07
Iter: 471 loss: 1.73731891e-07
Iter: 472 loss: 1.73418741e-07
Iter: 473 loss: 1.72914525e-07
Iter: 474 loss: 1.72907761e-07
Iter: 475 loss: 1.72571859e-07
Iter: 476 loss: 1.77326314e-07
Iter: 477 loss: 1.72575739e-07
Iter: 478 loss: 1.72236838e-07
Iter: 479 loss: 1.71988404e-07
Iter: 480 loss: 1.71877474e-07
Iter: 481 loss: 1.71436298e-07
Iter: 482 loss: 1.73250669e-07
Iter: 483 loss: 1.71343075e-07
Iter: 484 loss: 1.70921226e-07
Iter: 485 loss: 1.71980375e-07
Iter: 486 loss: 1.70785341e-07
Iter: 487 loss: 1.70249081e-07
Iter: 488 loss: 1.71191289e-07
Iter: 489 loss: 1.70024236e-07
Iter: 490 loss: 1.69447716e-07
Iter: 491 loss: 1.69175706e-07
Iter: 492 loss: 1.68901408e-07
Iter: 493 loss: 1.68511278e-07
Iter: 494 loss: 1.68496229e-07
Iter: 495 loss: 1.68067515e-07
Iter: 496 loss: 1.67962853e-07
Iter: 497 loss: 1.67699042e-07
Iter: 498 loss: 1.67316614e-07
Iter: 499 loss: 1.67353761e-07
Iter: 500 loss: 1.67010143e-07
Iter: 501 loss: 1.66550407e-07
Iter: 502 loss: 1.67936093e-07
Iter: 503 loss: 1.6640989e-07
Iter: 504 loss: 1.66002096e-07
Iter: 505 loss: 1.65989832e-07
Iter: 506 loss: 1.6575126e-07
Iter: 507 loss: 1.65159634e-07
Iter: 508 loss: 1.72337494e-07
Iter: 509 loss: 1.65106542e-07
Iter: 510 loss: 1.6473048e-07
Iter: 511 loss: 1.64674049e-07
Iter: 512 loss: 1.64347142e-07
Iter: 513 loss: 1.63733318e-07
Iter: 514 loss: 1.77214332e-07
Iter: 515 loss: 1.63731698e-07
Iter: 516 loss: 1.63302943e-07
Iter: 517 loss: 1.63304662e-07
Iter: 518 loss: 1.63001957e-07
Iter: 519 loss: 1.63445151e-07
Iter: 520 loss: 1.62863273e-07
Iter: 521 loss: 1.62500243e-07
Iter: 522 loss: 1.63252523e-07
Iter: 523 loss: 1.62335027e-07
Iter: 524 loss: 1.61982598e-07
Iter: 525 loss: 1.61755366e-07
Iter: 526 loss: 1.61615745e-07
Iter: 527 loss: 1.61413936e-07
Iter: 528 loss: 1.61339017e-07
Iter: 529 loss: 1.61093482e-07
Iter: 530 loss: 1.604825e-07
Iter: 531 loss: 1.67172828e-07
Iter: 532 loss: 1.60415595e-07
Iter: 533 loss: 1.59737809e-07
Iter: 534 loss: 1.60666758e-07
Iter: 535 loss: 1.59390822e-07
Iter: 536 loss: 1.59028616e-07
Iter: 537 loss: 1.59025475e-07
Iter: 538 loss: 1.58707e-07
Iter: 539 loss: 1.61170647e-07
Iter: 540 loss: 1.58683804e-07
Iter: 541 loss: 1.58503923e-07
Iter: 542 loss: 1.58122646e-07
Iter: 543 loss: 1.647375e-07
Iter: 544 loss: 1.58102281e-07
Iter: 545 loss: 1.57720038e-07
Iter: 546 loss: 1.5771667e-07
Iter: 547 loss: 1.57518301e-07
Iter: 548 loss: 1.57142324e-07
Iter: 549 loss: 1.65983664e-07
Iter: 550 loss: 1.57140079e-07
Iter: 551 loss: 1.56699315e-07
Iter: 552 loss: 1.60179638e-07
Iter: 553 loss: 1.56667511e-07
Iter: 554 loss: 1.56264377e-07
Iter: 555 loss: 1.55945813e-07
Iter: 556 loss: 1.55815812e-07
Iter: 557 loss: 1.55388165e-07
Iter: 558 loss: 1.60546804e-07
Iter: 559 loss: 1.5538798e-07
Iter: 560 loss: 1.55087378e-07
Iter: 561 loss: 1.5516386e-07
Iter: 562 loss: 1.54865901e-07
Iter: 563 loss: 1.54639523e-07
Iter: 564 loss: 1.54619485e-07
Iter: 565 loss: 1.54422963e-07
Iter: 566 loss: 1.5396094e-07
Iter: 567 loss: 1.60423639e-07
Iter: 568 loss: 1.53941357e-07
Iter: 569 loss: 1.53474431e-07
Iter: 570 loss: 1.53966937e-07
Iter: 571 loss: 1.53201455e-07
Iter: 572 loss: 1.5265023e-07
Iter: 573 loss: 1.57053492e-07
Iter: 574 loss: 1.52614405e-07
Iter: 575 loss: 1.52142462e-07
Iter: 576 loss: 1.58561448e-07
Iter: 577 loss: 1.52128848e-07
Iter: 578 loss: 1.5198097e-07
Iter: 579 loss: 1.51707951e-07
Iter: 580 loss: 1.51703787e-07
Iter: 581 loss: 1.51475049e-07
Iter: 582 loss: 1.51470232e-07
Iter: 583 loss: 1.5127226e-07
Iter: 584 loss: 1.50835973e-07
Iter: 585 loss: 1.57959988e-07
Iter: 586 loss: 1.50842624e-07
Iter: 587 loss: 1.50566606e-07
Iter: 588 loss: 1.50553547e-07
Iter: 589 loss: 1.50317433e-07
Iter: 590 loss: 1.49936824e-07
Iter: 591 loss: 1.49932688e-07
Iter: 592 loss: 1.49468e-07
Iter: 593 loss: 1.51405786e-07
Iter: 594 loss: 1.49372894e-07
Iter: 595 loss: 1.48978614e-07
Iter: 596 loss: 1.52426082e-07
Iter: 597 loss: 1.4895511e-07
Iter: 598 loss: 1.48661726e-07
Iter: 599 loss: 1.50136202e-07
Iter: 600 loss: 1.4861547e-07
Iter: 601 loss: 1.48417257e-07
Iter: 602 loss: 1.48099105e-07
Iter: 603 loss: 1.48097143e-07
Iter: 604 loss: 1.47791965e-07
Iter: 605 loss: 1.4802994e-07
Iter: 606 loss: 1.47599096e-07
Iter: 607 loss: 1.47321728e-07
Iter: 608 loss: 1.49557309e-07
Iter: 609 loss: 1.47305826e-07
Iter: 610 loss: 1.46979119e-07
Iter: 611 loss: 1.47596211e-07
Iter: 612 loss: 1.46839682e-07
Iter: 613 loss: 1.46558307e-07
Iter: 614 loss: 1.46085696e-07
Iter: 615 loss: 1.46081959e-07
Iter: 616 loss: 1.4614433e-07
Iter: 617 loss: 1.45861719e-07
Iter: 618 loss: 1.4571475e-07
Iter: 619 loss: 1.4536343e-07
Iter: 620 loss: 1.50578558e-07
Iter: 621 loss: 1.45347968e-07
Iter: 622 loss: 1.45123352e-07
Iter: 623 loss: 1.45125327e-07
Iter: 624 loss: 1.4489801e-07
Iter: 625 loss: 1.44730237e-07
Iter: 626 loss: 1.44662238e-07
Iter: 627 loss: 1.44378561e-07
Iter: 628 loss: 1.44798832e-07
Iter: 629 loss: 1.44246911e-07
Iter: 630 loss: 1.4387949e-07
Iter: 631 loss: 1.46297282e-07
Iter: 632 loss: 1.43848368e-07
Iter: 633 loss: 1.43509681e-07
Iter: 634 loss: 1.44388679e-07
Iter: 635 loss: 1.43390452e-07
Iter: 636 loss: 1.4307345e-07
Iter: 637 loss: 1.42836896e-07
Iter: 638 loss: 1.4273904e-07
Iter: 639 loss: 1.42335296e-07
Iter: 640 loss: 1.44932088e-07
Iter: 641 loss: 1.42296898e-07
Iter: 642 loss: 1.42099623e-07
Iter: 643 loss: 1.42095132e-07
Iter: 644 loss: 1.41946288e-07
Iter: 645 loss: 1.4162265e-07
Iter: 646 loss: 1.45589098e-07
Iter: 647 loss: 1.41600196e-07
Iter: 648 loss: 1.41288766e-07
Iter: 649 loss: 1.43047856e-07
Iter: 650 loss: 1.41247625e-07
Iter: 651 loss: 1.4091853e-07
Iter: 652 loss: 1.43580223e-07
Iter: 653 loss: 1.40892212e-07
Iter: 654 loss: 1.40697495e-07
Iter: 655 loss: 1.40324346e-07
Iter: 656 loss: 1.48394506e-07
Iter: 657 loss: 1.40321674e-07
Iter: 658 loss: 1.4005559e-07
Iter: 659 loss: 1.40051711e-07
Iter: 660 loss: 1.39836487e-07
Iter: 661 loss: 1.39462728e-07
Iter: 662 loss: 1.39461605e-07
Iter: 663 loss: 1.3918158e-07
Iter: 664 loss: 1.41523941e-07
Iter: 665 loss: 1.39168037e-07
Iter: 666 loss: 1.38947456e-07
Iter: 667 loss: 1.41775757e-07
Iter: 668 loss: 1.38952132e-07
Iter: 669 loss: 1.38819559e-07
Iter: 670 loss: 1.38697089e-07
Iter: 671 loss: 1.3864711e-07
Iter: 672 loss: 1.38383768e-07
Iter: 673 loss: 1.38202594e-07
Iter: 674 loss: 1.38106145e-07
Iter: 675 loss: 1.37870913e-07
Iter: 676 loss: 1.37869193e-07
Iter: 677 loss: 1.37615757e-07
Iter: 678 loss: 1.37301541e-07
Iter: 679 loss: 1.37260884e-07
Iter: 680 loss: 1.36942276e-07
Iter: 681 loss: 1.37257842e-07
Iter: 682 loss: 1.36746422e-07
Iter: 683 loss: 1.3664777e-07
Iter: 684 loss: 1.36573533e-07
Iter: 685 loss: 1.36429577e-07
Iter: 686 loss: 1.36156828e-07
Iter: 687 loss: 1.36159855e-07
Iter: 688 loss: 1.35957549e-07
Iter: 689 loss: 1.37272252e-07
Iter: 690 loss: 1.35935807e-07
Iter: 691 loss: 1.35687799e-07
Iter: 692 loss: 1.36074078e-07
Iter: 693 loss: 1.35572506e-07
Iter: 694 loss: 1.35344493e-07
Iter: 695 loss: 1.35122548e-07
Iter: 696 loss: 1.35068262e-07
Iter: 697 loss: 1.34812325e-07
Iter: 698 loss: 1.3480873e-07
Iter: 699 loss: 1.34574805e-07
Iter: 700 loss: 1.35265111e-07
Iter: 701 loss: 1.34505456e-07
Iter: 702 loss: 1.34324253e-07
Iter: 703 loss: 1.34277386e-07
Iter: 704 loss: 1.34148664e-07
Iter: 705 loss: 1.33937462e-07
Iter: 706 loss: 1.34473652e-07
Iter: 707 loss: 1.33853092e-07
Iter: 708 loss: 1.33689852e-07
Iter: 709 loss: 1.33686356e-07
Iter: 710 loss: 1.33542784e-07
Iter: 711 loss: 1.33184e-07
Iter: 712 loss: 1.36001887e-07
Iter: 713 loss: 1.33111882e-07
Iter: 714 loss: 1.32782617e-07
Iter: 715 loss: 1.34746173e-07
Iter: 716 loss: 1.327452e-07
Iter: 717 loss: 1.32403116e-07
Iter: 718 loss: 1.36090875e-07
Iter: 719 loss: 1.32397517e-07
Iter: 720 loss: 1.32217508e-07
Iter: 721 loss: 1.3197922e-07
Iter: 722 loss: 1.31967823e-07
Iter: 723 loss: 1.31836202e-07
Iter: 724 loss: 1.31825374e-07
Iter: 725 loss: 1.3169786e-07
Iter: 726 loss: 1.31454257e-07
Iter: 727 loss: 1.31454684e-07
Iter: 728 loss: 1.31192905e-07
Iter: 729 loss: 1.31143622e-07
Iter: 730 loss: 1.3096053e-07
Iter: 731 loss: 1.30693962e-07
Iter: 732 loss: 1.33942834e-07
Iter: 733 loss: 1.30693095e-07
Iter: 734 loss: 1.30390603e-07
Iter: 735 loss: 1.31027704e-07
Iter: 736 loss: 1.30271474e-07
Iter: 737 loss: 1.30051333e-07
Iter: 738 loss: 1.29926633e-07
Iter: 739 loss: 1.29832273e-07
Iter: 740 loss: 1.29669615e-07
Iter: 741 loss: 1.29663334e-07
Iter: 742 loss: 1.29506276e-07
Iter: 743 loss: 1.29928821e-07
Iter: 744 loss: 1.29452076e-07
Iter: 745 loss: 1.29316987e-07
Iter: 746 loss: 1.2935368e-07
Iter: 747 loss: 1.29215962e-07
Iter: 748 loss: 1.29035982e-07
Iter: 749 loss: 1.2905133e-07
Iter: 750 loss: 1.28900865e-07
Iter: 751 loss: 1.28687844e-07
Iter: 752 loss: 1.28689578e-07
Iter: 753 loss: 1.28543377e-07
Iter: 754 loss: 1.28273271e-07
Iter: 755 loss: 1.33803667e-07
Iter: 756 loss: 1.28270401e-07
Iter: 757 loss: 1.28061799e-07
Iter: 758 loss: 1.31128331e-07
Iter: 759 loss: 1.28066461e-07
Iter: 760 loss: 1.27850086e-07
Iter: 761 loss: 1.28029058e-07
Iter: 762 loss: 1.27738602e-07
Iter: 763 loss: 1.27533099e-07
Iter: 764 loss: 1.27231132e-07
Iter: 765 loss: 1.27221824e-07
Iter: 766 loss: 1.26849699e-07
Iter: 767 loss: 1.27673985e-07
Iter: 768 loss: 1.26719371e-07
Iter: 769 loss: 1.26422918e-07
Iter: 770 loss: 1.26418e-07
Iter: 771 loss: 1.26156976e-07
Iter: 772 loss: 1.28202515e-07
Iter: 773 loss: 1.26130587e-07
Iter: 774 loss: 1.26006981e-07
Iter: 775 loss: 1.25836323e-07
Iter: 776 loss: 1.25814296e-07
Iter: 777 loss: 1.25675029e-07
Iter: 778 loss: 1.25674518e-07
Iter: 779 loss: 1.25548041e-07
Iter: 780 loss: 1.25674916e-07
Iter: 781 loss: 1.2547541e-07
Iter: 782 loss: 1.25337834e-07
Iter: 783 loss: 1.25357573e-07
Iter: 784 loss: 1.25228738e-07
Iter: 785 loss: 1.25091248e-07
Iter: 786 loss: 1.26400124e-07
Iter: 787 loss: 1.25091105e-07
Iter: 788 loss: 1.24926544e-07
Iter: 789 loss: 1.24808793e-07
Iter: 790 loss: 1.24758884e-07
Iter: 791 loss: 1.24550567e-07
Iter: 792 loss: 1.24525215e-07
Iter: 793 loss: 1.24372036e-07
Iter: 794 loss: 1.24225963e-07
Iter: 795 loss: 1.24219824e-07
Iter: 796 loss: 1.2406295e-07
Iter: 797 loss: 1.239288e-07
Iter: 798 loss: 1.23887659e-07
Iter: 799 loss: 1.23720582e-07
Iter: 800 loss: 1.23615379e-07
Iter: 801 loss: 1.23551786e-07
Iter: 802 loss: 1.23349864e-07
Iter: 803 loss: 1.25649322e-07
Iter: 804 loss: 1.23350446e-07
Iter: 805 loss: 1.23127478e-07
Iter: 806 loss: 1.24037342e-07
Iter: 807 loss: 1.23076816e-07
Iter: 808 loss: 1.22954518e-07
Iter: 809 loss: 1.22707974e-07
Iter: 810 loss: 1.22712862e-07
Iter: 811 loss: 1.22677875e-07
Iter: 812 loss: 1.22578086e-07
Iter: 813 loss: 1.22496616e-07
Iter: 814 loss: 1.22299184e-07
Iter: 815 loss: 1.25580982e-07
Iter: 816 loss: 1.22299099e-07
Iter: 817 loss: 1.22146261e-07
Iter: 818 loss: 1.23463053e-07
Iter: 819 loss: 1.22139184e-07
Iter: 820 loss: 1.22000799e-07
Iter: 821 loss: 1.22819031e-07
Iter: 822 loss: 1.21981742e-07
Iter: 823 loss: 1.21850846e-07
Iter: 824 loss: 1.21826915e-07
Iter: 825 loss: 1.21753729e-07
Iter: 826 loss: 1.21576278e-07
Iter: 827 loss: 1.2162225e-07
Iter: 828 loss: 1.21437395e-07
Iter: 829 loss: 1.21314969e-07
Iter: 830 loss: 1.21308204e-07
Iter: 831 loss: 1.21163154e-07
Iter: 832 loss: 1.20878283e-07
Iter: 833 loss: 1.26448185e-07
Iter: 834 loss: 1.20881268e-07
Iter: 835 loss: 1.20673917e-07
Iter: 836 loss: 1.21332135e-07
Iter: 837 loss: 1.20608775e-07
Iter: 838 loss: 1.20445264e-07
Iter: 839 loss: 1.20980815e-07
Iter: 840 loss: 1.20383959e-07
Iter: 841 loss: 1.20202429e-07
Iter: 842 loss: 1.22358685e-07
Iter: 843 loss: 1.20193477e-07
Iter: 844 loss: 1.20077615e-07
Iter: 845 loss: 1.19843918e-07
Iter: 846 loss: 1.23753068e-07
Iter: 847 loss: 1.19833445e-07
Iter: 848 loss: 1.19607989e-07
Iter: 849 loss: 1.2028292e-07
Iter: 850 loss: 1.19550577e-07
Iter: 851 loss: 1.19473199e-07
Iter: 852 loss: 1.19423262e-07
Iter: 853 loss: 1.19341109e-07
Iter: 854 loss: 1.19142115e-07
Iter: 855 loss: 1.20825547e-07
Iter: 856 loss: 1.19109814e-07
Iter: 857 loss: 1.18922024e-07
Iter: 858 loss: 1.20657063e-07
Iter: 859 loss: 1.18916503e-07
Iter: 860 loss: 1.1875386e-07
Iter: 861 loss: 1.19082976e-07
Iter: 862 loss: 1.18696242e-07
Iter: 863 loss: 1.18538779e-07
Iter: 864 loss: 1.20118131e-07
Iter: 865 loss: 1.18522493e-07
Iter: 866 loss: 1.1840207e-07
Iter: 867 loss: 1.18177311e-07
Iter: 868 loss: 1.23351668e-07
Iter: 869 loss: 1.18179543e-07
Iter: 870 loss: 1.17996422e-07
Iter: 871 loss: 1.21031718e-07
Iter: 872 loss: 1.17997928e-07
Iter: 873 loss: 1.17798535e-07
Iter: 874 loss: 1.17805328e-07
Iter: 875 loss: 1.17635949e-07
Iter: 876 loss: 1.17483864e-07
Iter: 877 loss: 1.17462726e-07
Iter: 878 loss: 1.17359384e-07
Iter: 879 loss: 1.17293986e-07
Iter: 880 loss: 1.17238841e-07
Iter: 881 loss: 1.17154343e-07
Iter: 882 loss: 1.16972686e-07
Iter: 883 loss: 1.200778e-07
Iter: 884 loss: 1.16969993e-07
Iter: 885 loss: 1.16815634e-07
Iter: 886 loss: 1.18400671e-07
Iter: 887 loss: 1.16807456e-07
Iter: 888 loss: 1.16629224e-07
Iter: 889 loss: 1.16701813e-07
Iter: 890 loss: 1.16505277e-07
Iter: 891 loss: 1.16368639e-07
Iter: 892 loss: 1.16274514e-07
Iter: 893 loss: 1.16226168e-07
Iter: 894 loss: 1.16061855e-07
Iter: 895 loss: 1.17134839e-07
Iter: 896 loss: 1.16038457e-07
Iter: 897 loss: 1.15906715e-07
Iter: 898 loss: 1.15784843e-07
Iter: 899 loss: 1.15749479e-07
Iter: 900 loss: 1.15608444e-07
Iter: 901 loss: 1.15611542e-07
Iter: 902 loss: 1.1548623e-07
Iter: 903 loss: 1.15384445e-07
Iter: 904 loss: 1.153551e-07
Iter: 905 loss: 1.15152822e-07
Iter: 906 loss: 1.15018715e-07
Iter: 907 loss: 1.14946779e-07
Iter: 908 loss: 1.14851545e-07
Iter: 909 loss: 1.14782992e-07
Iter: 910 loss: 1.14667124e-07
Iter: 911 loss: 1.14429099e-07
Iter: 912 loss: 1.1739408e-07
Iter: 913 loss: 1.14403583e-07
Iter: 914 loss: 1.1420947e-07
Iter: 915 loss: 1.15369602e-07
Iter: 916 loss: 1.14183777e-07
Iter: 917 loss: 1.14085594e-07
Iter: 918 loss: 1.14079235e-07
Iter: 919 loss: 1.14011208e-07
Iter: 920 loss: 1.13871565e-07
Iter: 921 loss: 1.16004145e-07
Iter: 922 loss: 1.13867443e-07
Iter: 923 loss: 1.13777688e-07
Iter: 924 loss: 1.13771172e-07
Iter: 925 loss: 1.13656341e-07
Iter: 926 loss: 1.13421805e-07
Iter: 927 loss: 1.1751429e-07
Iter: 928 loss: 1.13407516e-07
Iter: 929 loss: 1.13193309e-07
Iter: 930 loss: 1.14611154e-07
Iter: 931 loss: 1.1316348e-07
Iter: 932 loss: 1.12962809e-07
Iter: 933 loss: 1.14302509e-07
Iter: 934 loss: 1.12940278e-07
Iter: 935 loss: 1.12796769e-07
Iter: 936 loss: 1.13248674e-07
Iter: 937 loss: 1.12758322e-07
Iter: 938 loss: 1.12652941e-07
Iter: 939 loss: 1.12473209e-07
Iter: 940 loss: 1.12470168e-07
Iter: 941 loss: 1.12250788e-07
Iter: 942 loss: 1.12626722e-07
Iter: 943 loss: 1.12163839e-07
Iter: 944 loss: 1.11913906e-07
Iter: 945 loss: 1.1260844e-07
Iter: 946 loss: 1.11833586e-07
Iter: 947 loss: 1.11601736e-07
Iter: 948 loss: 1.11604223e-07
Iter: 949 loss: 1.11456487e-07
Iter: 950 loss: 1.11183731e-07
Iter: 951 loss: 1.17416882e-07
Iter: 952 loss: 1.11175211e-07
Iter: 953 loss: 1.11109259e-07
Iter: 954 loss: 1.11055115e-07
Iter: 955 loss: 1.10936867e-07
Iter: 956 loss: 1.10753206e-07
Iter: 957 loss: 1.10749738e-07
Iter: 958 loss: 1.10636606e-07
Iter: 959 loss: 1.12453328e-07
Iter: 960 loss: 1.10640613e-07
Iter: 961 loss: 1.10516353e-07
Iter: 962 loss: 1.10670328e-07
Iter: 963 loss: 1.10449207e-07
Iter: 964 loss: 1.1034389e-07
Iter: 965 loss: 1.10213676e-07
Iter: 966 loss: 1.10207679e-07
Iter: 967 loss: 1.10030982e-07
Iter: 968 loss: 1.10031188e-07
Iter: 969 loss: 1.09899489e-07
Iter: 970 loss: 1.0969918e-07
Iter: 971 loss: 1.09698362e-07
Iter: 972 loss: 1.09439384e-07
Iter: 973 loss: 1.10467795e-07
Iter: 974 loss: 1.09376217e-07
Iter: 975 loss: 1.09182551e-07
Iter: 976 loss: 1.09953618e-07
Iter: 977 loss: 1.09139499e-07
Iter: 978 loss: 1.09012831e-07
Iter: 979 loss: 1.09304338e-07
Iter: 980 loss: 1.08967356e-07
Iter: 981 loss: 1.08820139e-07
Iter: 982 loss: 1.10109845e-07
Iter: 983 loss: 1.08818284e-07
Iter: 984 loss: 1.08731058e-07
Iter: 985 loss: 1.08516637e-07
Iter: 986 loss: 1.10666143e-07
Iter: 987 loss: 1.08487548e-07
Iter: 988 loss: 1.08261943e-07
Iter: 989 loss: 1.09260817e-07
Iter: 990 loss: 1.08213712e-07
Iter: 991 loss: 1.08063801e-07
Iter: 992 loss: 1.08055097e-07
Iter: 993 loss: 1.07936444e-07
Iter: 994 loss: 1.07732284e-07
Iter: 995 loss: 1.12735549e-07
Iter: 996 loss: 1.07724297e-07
Iter: 997 loss: 1.07705127e-07
Iter: 998 loss: 1.07633262e-07
Iter: 999 loss: 1.07581386e-07
Iter: 1000 loss: 1.07438566e-07
Iter: 1001 loss: 1.0859042e-07
Iter: 1002 loss: 1.07422366e-07
Iter: 1003 loss: 1.07307e-07
Iter: 1004 loss: 1.08524759e-07
Iter: 1005 loss: 1.07299876e-07
Iter: 1006 loss: 1.07162499e-07
Iter: 1007 loss: 1.07400048e-07
Iter: 1008 loss: 1.07106807e-07
Iter: 1009 loss: 1.06983286e-07
Iter: 1010 loss: 1.06736422e-07
Iter: 1011 loss: 1.11055336e-07
Iter: 1012 loss: 1.06730255e-07
Iter: 1013 loss: 1.06441711e-07
Iter: 1014 loss: 1.08574973e-07
Iter: 1015 loss: 1.06407441e-07
Iter: 1016 loss: 1.06227503e-07
Iter: 1017 loss: 1.0826448e-07
Iter: 1018 loss: 1.06222245e-07
Iter: 1019 loss: 1.06093339e-07
Iter: 1020 loss: 1.06483412e-07
Iter: 1021 loss: 1.06060952e-07
Iter: 1022 loss: 1.05947926e-07
Iter: 1023 loss: 1.06074786e-07
Iter: 1024 loss: 1.05892717e-07
Iter: 1025 loss: 1.05772941e-07
Iter: 1026 loss: 1.05850319e-07
Iter: 1027 loss: 1.05705894e-07
Iter: 1028 loss: 1.05605757e-07
Iter: 1029 loss: 1.05598602e-07
Iter: 1030 loss: 1.05499751e-07
Iter: 1031 loss: 1.05369395e-07
Iter: 1032 loss: 1.05359845e-07
Iter: 1033 loss: 1.05205523e-07
Iter: 1034 loss: 1.05165981e-07
Iter: 1035 loss: 1.05076921e-07
Iter: 1036 loss: 1.04923771e-07
Iter: 1037 loss: 1.06859233e-07
Iter: 1038 loss: 1.04923515e-07
Iter: 1039 loss: 1.04801401e-07
Iter: 1040 loss: 1.06146338e-07
Iter: 1041 loss: 1.04803568e-07
Iter: 1042 loss: 1.04739129e-07
Iter: 1043 loss: 1.04592317e-07
Iter: 1044 loss: 1.06421155e-07
Iter: 1045 loss: 1.04581702e-07
Iter: 1046 loss: 1.04509851e-07
Iter: 1047 loss: 1.04506668e-07
Iter: 1048 loss: 1.04420153e-07
Iter: 1049 loss: 1.04445967e-07
Iter: 1050 loss: 1.04363309e-07
Iter: 1051 loss: 1.04241678e-07
Iter: 1052 loss: 1.0415e-07
Iter: 1053 loss: 1.04119643e-07
Iter: 1054 loss: 1.03961469e-07
Iter: 1055 loss: 1.06029802e-07
Iter: 1056 loss: 1.03957404e-07
Iter: 1057 loss: 1.03831326e-07
Iter: 1058 loss: 1.03949752e-07
Iter: 1059 loss: 1.03770191e-07
Iter: 1060 loss: 1.03618405e-07
Iter: 1061 loss: 1.03865553e-07
Iter: 1062 loss: 1.03559472e-07
Iter: 1063 loss: 1.03448521e-07
Iter: 1064 loss: 1.03687221e-07
Iter: 1065 loss: 1.03412575e-07
Iter: 1066 loss: 1.03345656e-07
Iter: 1067 loss: 1.03338166e-07
Iter: 1068 loss: 1.0327642e-07
Iter: 1069 loss: 1.03141204e-07
Iter: 1070 loss: 1.05587382e-07
Iter: 1071 loss: 1.03134582e-07
Iter: 1072 loss: 1.03026096e-07
Iter: 1073 loss: 1.03899836e-07
Iter: 1074 loss: 1.03023396e-07
Iter: 1075 loss: 1.02869294e-07
Iter: 1076 loss: 1.02934884e-07
Iter: 1077 loss: 1.02768546e-07
Iter: 1078 loss: 1.0264506e-07
Iter: 1079 loss: 1.02649643e-07
Iter: 1080 loss: 1.02542572e-07
Iter: 1081 loss: 1.02466345e-07
Iter: 1082 loss: 1.02452546e-07
Iter: 1083 loss: 1.02390231e-07
Iter: 1084 loss: 1.0228257e-07
Iter: 1085 loss: 1.04868974e-07
Iter: 1086 loss: 1.02283245e-07
Iter: 1087 loss: 1.02189176e-07
Iter: 1088 loss: 1.02946842e-07
Iter: 1089 loss: 1.02179698e-07
Iter: 1090 loss: 1.02090077e-07
Iter: 1091 loss: 1.02308377e-07
Iter: 1092 loss: 1.02052368e-07
Iter: 1093 loss: 1.01943918e-07
Iter: 1094 loss: 1.01982522e-07
Iter: 1095 loss: 1.01876466e-07
Iter: 1096 loss: 1.01755987e-07
Iter: 1097 loss: 1.01605437e-07
Iter: 1098 loss: 1.0157661e-07
Iter: 1099 loss: 1.01369338e-07
Iter: 1100 loss: 1.02121753e-07
Iter: 1101 loss: 1.01316509e-07
Iter: 1102 loss: 1.01167061e-07
Iter: 1103 loss: 1.02550821e-07
Iter: 1104 loss: 1.01168318e-07
Iter: 1105 loss: 1.01106032e-07
Iter: 1106 loss: 1.01093917e-07
Iter: 1107 loss: 1.01045629e-07
Iter: 1108 loss: 1.00910988e-07
Iter: 1109 loss: 1.025251e-07
Iter: 1110 loss: 1.00899236e-07
Iter: 1111 loss: 1.00812883e-07
Iter: 1112 loss: 1.00811448e-07
Iter: 1113 loss: 1.00702195e-07
Iter: 1114 loss: 1.00580223e-07
Iter: 1115 loss: 1.0055696e-07
Iter: 1116 loss: 1.00435606e-07
Iter: 1117 loss: 1.01658337e-07
Iter: 1118 loss: 1.00427684e-07
Iter: 1119 loss: 1.00303431e-07
Iter: 1120 loss: 1.0032911e-07
Iter: 1121 loss: 1.00212702e-07
Iter: 1122 loss: 1.00089693e-07
Iter: 1123 loss: 1.00146956e-07
Iter: 1124 loss: 1.00026391e-07
Iter: 1125 loss: 9.9924236e-08
Iter: 1126 loss: 1.01405902e-07
Iter: 1127 loss: 9.99251526e-08
Iter: 1128 loss: 9.98251153e-08
Iter: 1129 loss: 9.97872931e-08
Iter: 1130 loss: 9.973e-08
Iter: 1131 loss: 9.96089824e-08
Iter: 1132 loss: 1.00287608e-07
Iter: 1133 loss: 9.95862806e-08
Iter: 1134 loss: 9.94733114e-08
Iter: 1135 loss: 9.94378908e-08
Iter: 1136 loss: 9.93691458e-08
Iter: 1137 loss: 9.92381075e-08
Iter: 1138 loss: 1.00005742e-07
Iter: 1139 loss: 9.92194131e-08
Iter: 1140 loss: 9.9066952e-08
Iter: 1141 loss: 9.98197365e-08
Iter: 1142 loss: 9.90417e-08
Iter: 1143 loss: 9.89614648e-08
Iter: 1144 loss: 9.88376598e-08
Iter: 1145 loss: 9.88383206e-08
Iter: 1146 loss: 9.87375515e-08
Iter: 1147 loss: 9.99975285e-08
Iter: 1148 loss: 9.8733075e-08
Iter: 1149 loss: 9.86116504e-08
Iter: 1150 loss: 9.87768232e-08
Iter: 1151 loss: 9.85625e-08
Iter: 1152 loss: 9.84747359e-08
Iter: 1153 loss: 9.87217e-08
Iter: 1154 loss: 9.84515935e-08
Iter: 1155 loss: 9.83284707e-08
Iter: 1156 loss: 9.83434276e-08
Iter: 1157 loss: 9.82422819e-08
Iter: 1158 loss: 9.80959811e-08
Iter: 1159 loss: 9.79953e-08
Iter: 1160 loss: 9.79411112e-08
Iter: 1161 loss: 9.7862042e-08
Iter: 1162 loss: 9.78320429e-08
Iter: 1163 loss: 9.77482e-08
Iter: 1164 loss: 9.76669696e-08
Iter: 1165 loss: 9.76458239e-08
Iter: 1166 loss: 9.75618946e-08
Iter: 1167 loss: 9.83572761e-08
Iter: 1168 loss: 9.75555707e-08
Iter: 1169 loss: 9.74852838e-08
Iter: 1170 loss: 9.742336e-08
Iter: 1171 loss: 9.74067191e-08
Iter: 1172 loss: 9.73155636e-08
Iter: 1173 loss: 9.73156773e-08
Iter: 1174 loss: 9.72193135e-08
Iter: 1175 loss: 9.71492895e-08
Iter: 1176 loss: 9.71189209e-08
Iter: 1177 loss: 9.69963736e-08
Iter: 1178 loss: 9.68927765e-08
Iter: 1179 loss: 9.68558709e-08
Iter: 1180 loss: 9.67547749e-08
Iter: 1181 loss: 9.67447704e-08
Iter: 1182 loss: 9.66227844e-08
Iter: 1183 loss: 9.67230136e-08
Iter: 1184 loss: 9.65700053e-08
Iter: 1185 loss: 9.65004148e-08
Iter: 1186 loss: 9.68079874e-08
Iter: 1187 loss: 9.64879092e-08
Iter: 1188 loss: 9.64136e-08
Iter: 1189 loss: 9.65224842e-08
Iter: 1190 loss: 9.6372311e-08
Iter: 1191 loss: 9.63000844e-08
Iter: 1192 loss: 9.61913855e-08
Iter: 1193 loss: 9.61884297e-08
Iter: 1194 loss: 9.60906661e-08
Iter: 1195 loss: 9.60850173e-08
Iter: 1196 loss: 9.59995106e-08
Iter: 1197 loss: 9.58784483e-08
Iter: 1198 loss: 9.58801252e-08
Iter: 1199 loss: 9.57340518e-08
Iter: 1200 loss: 9.57466924e-08
Iter: 1201 loss: 9.56319823e-08
Iter: 1202 loss: 9.54678896e-08
Iter: 1203 loss: 9.74394254e-08
Iter: 1204 loss: 9.54704831e-08
Iter: 1205 loss: 9.54104777e-08
Iter: 1206 loss: 9.60960804e-08
Iter: 1207 loss: 9.54091206e-08
Iter: 1208 loss: 9.5347211e-08
Iter: 1209 loss: 9.5235329e-08
Iter: 1210 loss: 9.52316e-08
Iter: 1211 loss: 9.51341121e-08
Iter: 1212 loss: 9.51795087e-08
Iter: 1213 loss: 9.50720889e-08
Iter: 1214 loss: 9.49903125e-08
Iter: 1215 loss: 9.4980777e-08
Iter: 1216 loss: 9.49024113e-08
Iter: 1217 loss: 9.48500443e-08
Iter: 1218 loss: 9.48241308e-08
Iter: 1219 loss: 9.47381764e-08
Iter: 1220 loss: 9.54733537e-08
Iter: 1221 loss: 9.47315542e-08
Iter: 1222 loss: 9.4645273e-08
Iter: 1223 loss: 9.46214e-08
Iter: 1224 loss: 9.45611447e-08
Iter: 1225 loss: 9.44862393e-08
Iter: 1226 loss: 9.46603294e-08
Iter: 1227 loss: 9.44612424e-08
Iter: 1228 loss: 9.43855198e-08
Iter: 1229 loss: 9.50462749e-08
Iter: 1230 loss: 9.43796e-08
Iter: 1231 loss: 9.432393e-08
Iter: 1232 loss: 9.42161e-08
Iter: 1233 loss: 9.67711955e-08
Iter: 1234 loss: 9.42189899e-08
Iter: 1235 loss: 9.40925702e-08
Iter: 1236 loss: 9.4113652e-08
Iter: 1237 loss: 9.39939468e-08
Iter: 1238 loss: 9.38589e-08
Iter: 1239 loss: 9.49790788e-08
Iter: 1240 loss: 9.3847504e-08
Iter: 1241 loss: 9.37397289e-08
Iter: 1242 loss: 9.42733536e-08
Iter: 1243 loss: 9.37222211e-08
Iter: 1244 loss: 9.36362525e-08
Iter: 1245 loss: 9.41113072e-08
Iter: 1246 loss: 9.36196614e-08
Iter: 1247 loss: 9.35399811e-08
Iter: 1248 loss: 9.34464381e-08
Iter: 1249 loss: 9.34491737e-08
Iter: 1250 loss: 9.33551902e-08
Iter: 1251 loss: 9.38562863e-08
Iter: 1252 loss: 9.33494562e-08
Iter: 1253 loss: 9.32522397e-08
Iter: 1254 loss: 9.40906943e-08
Iter: 1255 loss: 9.32509323e-08
Iter: 1256 loss: 9.31903514e-08
Iter: 1257 loss: 9.30314243e-08
Iter: 1258 loss: 9.44362455e-08
Iter: 1259 loss: 9.30129218e-08
Iter: 1260 loss: 9.2866955e-08
Iter: 1261 loss: 9.36185529e-08
Iter: 1262 loss: 9.28397839e-08
Iter: 1263 loss: 9.2749076e-08
Iter: 1264 loss: 9.27415726e-08
Iter: 1265 loss: 9.2684175e-08
Iter: 1266 loss: 9.25674e-08
Iter: 1267 loss: 9.44578176e-08
Iter: 1268 loss: 9.25641714e-08
Iter: 1269 loss: 9.24640915e-08
Iter: 1270 loss: 9.30244823e-08
Iter: 1271 loss: 9.24643686e-08
Iter: 1272 loss: 9.23708825e-08
Iter: 1273 loss: 9.30688e-08
Iter: 1274 loss: 9.2363436e-08
Iter: 1275 loss: 9.22807146e-08
Iter: 1276 loss: 9.23481309e-08
Iter: 1277 loss: 9.22354744e-08
Iter: 1278 loss: 9.21537762e-08
Iter: 1279 loss: 9.25850259e-08
Iter: 1280 loss: 9.21412635e-08
Iter: 1281 loss: 9.20451839e-08
Iter: 1282 loss: 9.2100521e-08
Iter: 1283 loss: 9.19819314e-08
Iter: 1284 loss: 9.18820859e-08
Iter: 1285 loss: 9.19476e-08
Iter: 1286 loss: 9.1827296e-08
Iter: 1287 loss: 9.17274434e-08
Iter: 1288 loss: 9.22903212e-08
Iter: 1289 loss: 9.17051679e-08
Iter: 1290 loss: 9.16301275e-08
Iter: 1291 loss: 9.21236e-08
Iter: 1292 loss: 9.16235123e-08
Iter: 1293 loss: 9.15547247e-08
Iter: 1294 loss: 9.17656635e-08
Iter: 1295 loss: 9.15291e-08
Iter: 1296 loss: 9.14868323e-08
Iter: 1297 loss: 9.13822547e-08
Iter: 1298 loss: 9.25674399e-08
Iter: 1299 loss: 9.13645408e-08
Iter: 1300 loss: 9.12414961e-08
Iter: 1301 loss: 9.21428409e-08
Iter: 1302 loss: 9.12391229e-08
Iter: 1303 loss: 9.11187e-08
Iter: 1304 loss: 9.22536287e-08
Iter: 1305 loss: 9.11124118e-08
Iter: 1306 loss: 9.10502109e-08
Iter: 1307 loss: 9.09386344e-08
Iter: 1308 loss: 9.31851787e-08
Iter: 1309 loss: 9.09370783e-08
Iter: 1310 loss: 9.08475855e-08
Iter: 1311 loss: 9.16911844e-08
Iter: 1312 loss: 9.08518558e-08
Iter: 1313 loss: 9.07904649e-08
Iter: 1314 loss: 9.15046101e-08
Iter: 1315 loss: 9.07852922e-08
Iter: 1316 loss: 9.07498361e-08
Iter: 1317 loss: 9.06947051e-08
Iter: 1318 loss: 9.06929785e-08
Iter: 1319 loss: 9.06295909e-08
Iter: 1320 loss: 9.12947229e-08
Iter: 1321 loss: 9.06245532e-08
Iter: 1322 loss: 9.05447e-08
Iter: 1323 loss: 9.05825317e-08
Iter: 1324 loss: 9.04919588e-08
Iter: 1325 loss: 9.04120441e-08
Iter: 1326 loss: 9.04602757e-08
Iter: 1327 loss: 9.03622919e-08
Iter: 1328 loss: 9.02566057e-08
Iter: 1329 loss: 9.0649138e-08
Iter: 1330 loss: 9.02360284e-08
Iter: 1331 loss: 9.01763428e-08
Iter: 1332 loss: 9.103222e-08
Iter: 1333 loss: 9.01784531e-08
Iter: 1334 loss: 9.01149448e-08
Iter: 1335 loss: 9.01770676e-08
Iter: 1336 loss: 9.00775774e-08
Iter: 1337 loss: 9.00405297e-08
Iter: 1338 loss: 8.99585473e-08
Iter: 1339 loss: 9.12415246e-08
Iter: 1340 loss: 8.99563517e-08
Iter: 1341 loss: 8.9922608e-08
Iter: 1342 loss: 8.99047663e-08
Iter: 1343 loss: 8.98533514e-08
Iter: 1344 loss: 8.97495198e-08
Iter: 1345 loss: 9.14551137e-08
Iter: 1346 loss: 8.97462e-08
Iter: 1347 loss: 8.96567514e-08
Iter: 1348 loss: 9.04199737e-08
Iter: 1349 loss: 8.96526444e-08
Iter: 1350 loss: 8.95704204e-08
Iter: 1351 loss: 8.97250843e-08
Iter: 1352 loss: 8.95287116e-08
Iter: 1353 loss: 8.94654875e-08
Iter: 1354 loss: 8.96836e-08
Iter: 1355 loss: 8.94488323e-08
Iter: 1356 loss: 8.93864183e-08
Iter: 1357 loss: 8.95552219e-08
Iter: 1358 loss: 8.9382155e-08
Iter: 1359 loss: 8.93340371e-08
Iter: 1360 loss: 8.9350209e-08
Iter: 1361 loss: 8.92966554e-08
Iter: 1362 loss: 8.92337653e-08
Iter: 1363 loss: 8.92359324e-08
Iter: 1364 loss: 8.91836294e-08
Iter: 1365 loss: 8.90886085e-08
Iter: 1366 loss: 8.91227e-08
Iter: 1367 loss: 8.90106691e-08
Iter: 1368 loss: 8.89245086e-08
Iter: 1369 loss: 8.8925816e-08
Iter: 1370 loss: 8.88415e-08
Iter: 1371 loss: 8.89652654e-08
Iter: 1372 loss: 8.87977905e-08
Iter: 1373 loss: 8.87384e-08
Iter: 1374 loss: 8.86794709e-08
Iter: 1375 loss: 8.86647697e-08
Iter: 1376 loss: 8.86203253e-08
Iter: 1377 loss: 8.86171563e-08
Iter: 1378 loss: 8.85596165e-08
Iter: 1379 loss: 8.86011193e-08
Iter: 1380 loss: 8.85223272e-08
Iter: 1381 loss: 8.84601548e-08
Iter: 1382 loss: 8.84436417e-08
Iter: 1383 loss: 8.84087399e-08
Iter: 1384 loss: 8.83200144e-08
Iter: 1385 loss: 8.90081964e-08
Iter: 1386 loss: 8.83060096e-08
Iter: 1387 loss: 8.82358222e-08
Iter: 1388 loss: 8.82431763e-08
Iter: 1389 loss: 8.8190788e-08
Iter: 1390 loss: 8.81096724e-08
Iter: 1391 loss: 8.9060066e-08
Iter: 1392 loss: 8.81074698e-08
Iter: 1393 loss: 8.80567654e-08
Iter: 1394 loss: 8.81136302e-08
Iter: 1395 loss: 8.80227731e-08
Iter: 1396 loss: 8.79609203e-08
Iter: 1397 loss: 8.8068667e-08
Iter: 1398 loss: 8.79360513e-08
Iter: 1399 loss: 8.78741631e-08
Iter: 1400 loss: 8.78867183e-08
Iter: 1401 loss: 8.7825029e-08
Iter: 1402 loss: 8.77612436e-08
Iter: 1403 loss: 8.81572362e-08
Iter: 1404 loss: 8.77516158e-08
Iter: 1405 loss: 8.76603963e-08
Iter: 1406 loss: 8.77851249e-08
Iter: 1407 loss: 8.76105162e-08
Iter: 1408 loss: 8.75285e-08
Iter: 1409 loss: 8.74867112e-08
Iter: 1410 loss: 8.74481572e-08
Iter: 1411 loss: 8.73591262e-08
Iter: 1412 loss: 8.78356e-08
Iter: 1413 loss: 8.73419665e-08
Iter: 1414 loss: 8.72885693e-08
Iter: 1415 loss: 8.72851444e-08
Iter: 1416 loss: 8.72457875e-08
Iter: 1417 loss: 8.71860522e-08
Iter: 1418 loss: 8.71863293e-08
Iter: 1419 loss: 8.71388295e-08
Iter: 1420 loss: 8.78274307e-08
Iter: 1421 loss: 8.71385595e-08
Iter: 1422 loss: 8.70866188e-08
Iter: 1423 loss: 8.70298535e-08
Iter: 1424 loss: 8.70249295e-08
Iter: 1425 loss: 8.69580106e-08
Iter: 1426 loss: 8.75381687e-08
Iter: 1427 loss: 8.6951367e-08
Iter: 1428 loss: 8.68735768e-08
Iter: 1429 loss: 8.69202665e-08
Iter: 1430 loss: 8.68182184e-08
Iter: 1431 loss: 8.67384529e-08
Iter: 1432 loss: 8.6985537e-08
Iter: 1433 loss: 8.67086e-08
Iter: 1434 loss: 8.66247e-08
Iter: 1435 loss: 8.68526513e-08
Iter: 1436 loss: 8.65997407e-08
Iter: 1437 loss: 8.65334542e-08
Iter: 1438 loss: 8.68487646e-08
Iter: 1439 loss: 8.65256595e-08
Iter: 1440 loss: 8.6456815e-08
Iter: 1441 loss: 8.65594529e-08
Iter: 1442 loss: 8.64211529e-08
Iter: 1443 loss: 8.63651337e-08
Iter: 1444 loss: 8.63013696e-08
Iter: 1445 loss: 8.62911378e-08
Iter: 1446 loss: 8.62038121e-08
Iter: 1447 loss: 8.65566e-08
Iter: 1448 loss: 8.61861054e-08
Iter: 1449 loss: 8.60814211e-08
Iter: 1450 loss: 8.71679475e-08
Iter: 1451 loss: 8.60820677e-08
Iter: 1452 loss: 8.6016037e-08
Iter: 1453 loss: 8.59330811e-08
Iter: 1454 loss: 8.59326761e-08
Iter: 1455 loss: 8.58501323e-08
Iter: 1456 loss: 8.58441069e-08
Iter: 1457 loss: 8.58006644e-08
Iter: 1458 loss: 8.57745803e-08
Iter: 1459 loss: 8.57571578e-08
Iter: 1460 loss: 8.57030074e-08
Iter: 1461 loss: 8.64754384e-08
Iter: 1462 loss: 8.57066169e-08
Iter: 1463 loss: 8.56591811e-08
Iter: 1464 loss: 8.55962412e-08
Iter: 1465 loss: 8.55957936e-08
Iter: 1466 loss: 8.55216e-08
Iter: 1467 loss: 8.60620233e-08
Iter: 1468 loss: 8.55152962e-08
Iter: 1469 loss: 8.54502815e-08
Iter: 1470 loss: 8.53669633e-08
Iter: 1471 loss: 8.53639648e-08
Iter: 1472 loss: 8.52484163e-08
Iter: 1473 loss: 8.60912195e-08
Iter: 1474 loss: 8.52346886e-08
Iter: 1475 loss: 8.51302815e-08
Iter: 1476 loss: 8.56750049e-08
Iter: 1477 loss: 8.511347e-08
Iter: 1478 loss: 8.50552624e-08
Iter: 1479 loss: 8.49579607e-08
Iter: 1480 loss: 8.49613286e-08
Iter: 1481 loss: 8.48873185e-08
Iter: 1482 loss: 8.48866222e-08
Iter: 1483 loss: 8.48231e-08
Iter: 1484 loss: 8.51372306e-08
Iter: 1485 loss: 8.48097699e-08
Iter: 1486 loss: 8.47613393e-08
Iter: 1487 loss: 8.46779642e-08
Iter: 1488 loss: 8.6838412e-08
Iter: 1489 loss: 8.46770476e-08
Iter: 1490 loss: 8.45775503e-08
Iter: 1491 loss: 8.45788719e-08
Iter: 1492 loss: 8.45296e-08
Iter: 1493 loss: 8.44406642e-08
Iter: 1494 loss: 8.44340633e-08
Iter: 1495 loss: 8.43468086e-08
Iter: 1496 loss: 8.4348116e-08
Iter: 1497 loss: 8.42851833e-08
Iter: 1498 loss: 8.43068122e-08
Iter: 1499 loss: 8.42408241e-08
Iter: 1500 loss: 8.4191683e-08
Iter: 1501 loss: 8.48760919e-08
Iter: 1502 loss: 8.41914627e-08
Iter: 1503 loss: 8.41592396e-08
Iter: 1504 loss: 8.41443466e-08
Iter: 1505 loss: 8.4124224e-08
Iter: 1506 loss: 8.40741876e-08
Iter: 1507 loss: 8.43095265e-08
Iter: 1508 loss: 8.40598702e-08
Iter: 1509 loss: 8.40083345e-08
Iter: 1510 loss: 8.39769854e-08
Iter: 1511 loss: 8.39462473e-08
Iter: 1512 loss: 8.38696508e-08
Iter: 1513 loss: 8.38717114e-08
Iter: 1514 loss: 8.38019929e-08
Iter: 1515 loss: 8.36938341e-08
Iter: 1516 loss: 8.39076932e-08
Iter: 1517 loss: 8.36555785e-08
Iter: 1518 loss: 8.36325427e-08
Iter: 1519 loss: 8.36159444e-08
Iter: 1520 loss: 8.35696454e-08
Iter: 1521 loss: 8.36128962e-08
Iter: 1522 loss: 8.35414511e-08
Iter: 1523 loss: 8.35070111e-08
Iter: 1524 loss: 8.34093754e-08
Iter: 1525 loss: 8.43792947e-08
Iter: 1526 loss: 8.34016944e-08
Iter: 1527 loss: 8.33914e-08
Iter: 1528 loss: 8.33562126e-08
Iter: 1529 loss: 8.33015434e-08
Iter: 1530 loss: 8.32125835e-08
Iter: 1531 loss: 8.51941095e-08
Iter: 1532 loss: 8.32069276e-08
Iter: 1533 loss: 8.3123517e-08
Iter: 1534 loss: 8.38761451e-08
Iter: 1535 loss: 8.3119474e-08
Iter: 1536 loss: 8.30438722e-08
Iter: 1537 loss: 8.335833e-08
Iter: 1538 loss: 8.30185769e-08
Iter: 1539 loss: 8.29803781e-08
Iter: 1540 loss: 8.29721785e-08
Iter: 1541 loss: 8.29474089e-08
Iter: 1542 loss: 8.29067517e-08
Iter: 1543 loss: 8.29057569e-08
Iter: 1544 loss: 8.2867146e-08
Iter: 1545 loss: 8.28485298e-08
Iter: 1546 loss: 8.28330329e-08
Iter: 1547 loss: 8.27708391e-08
Iter: 1548 loss: 8.28647657e-08
Iter: 1549 loss: 8.27471212e-08
Iter: 1550 loss: 8.26857729e-08
Iter: 1551 loss: 8.25772e-08
Iter: 1552 loss: 8.258408e-08
Iter: 1553 loss: 8.2445311e-08
Iter: 1554 loss: 8.28447639e-08
Iter: 1555 loss: 8.24146156e-08
Iter: 1556 loss: 8.23839059e-08
Iter: 1557 loss: 8.23580706e-08
Iter: 1558 loss: 8.23113879e-08
Iter: 1559 loss: 8.22805504e-08
Iter: 1560 loss: 8.2264151e-08
Iter: 1561 loss: 8.22132407e-08
Iter: 1562 loss: 8.2203e-08
Iter: 1563 loss: 8.2172086e-08
Iter: 1564 loss: 8.20992625e-08
Iter: 1565 loss: 8.32431652e-08
Iter: 1566 loss: 8.20966264e-08
Iter: 1567 loss: 8.20598132e-08
Iter: 1568 loss: 8.20051298e-08
Iter: 1569 loss: 8.20089383e-08
Iter: 1570 loss: 8.19194241e-08
Iter: 1571 loss: 8.25563689e-08
Iter: 1572 loss: 8.19121695e-08
Iter: 1573 loss: 8.18573369e-08
Iter: 1574 loss: 8.18062205e-08
Iter: 1575 loss: 8.1783e-08
Iter: 1576 loss: 8.17167702e-08
Iter: 1577 loss: 8.17176797e-08
Iter: 1578 loss: 8.1669171e-08
Iter: 1579 loss: 8.17088193e-08
Iter: 1580 loss: 8.16370544e-08
Iter: 1581 loss: 8.15829821e-08
Iter: 1582 loss: 8.16552159e-08
Iter: 1583 loss: 8.15591434e-08
Iter: 1584 loss: 8.1509e-08
Iter: 1585 loss: 8.14659913e-08
Iter: 1586 loss: 8.14538268e-08
Iter: 1587 loss: 8.13901551e-08
Iter: 1588 loss: 8.13943e-08
Iter: 1589 loss: 8.1327e-08
Iter: 1590 loss: 8.1357e-08
Iter: 1591 loss: 8.12912617e-08
Iter: 1592 loss: 8.12220264e-08
Iter: 1593 loss: 8.11414367e-08
Iter: 1594 loss: 8.1137415e-08
Iter: 1595 loss: 8.10458e-08
Iter: 1596 loss: 8.2250736e-08
Iter: 1597 loss: 8.10468705e-08
Iter: 1598 loss: 8.09613141e-08
Iter: 1599 loss: 8.15136545e-08
Iter: 1600 loss: 8.09500094e-08
Iter: 1601 loss: 8.09068581e-08
Iter: 1602 loss: 8.09792056e-08
Iter: 1603 loss: 8.08915388e-08
Iter: 1604 loss: 8.08394773e-08
Iter: 1605 loss: 8.08801062e-08
Iter: 1606 loss: 8.07990403e-08
Iter: 1607 loss: 8.07458704e-08
Iter: 1608 loss: 8.08499934e-08
Iter: 1609 loss: 8.07237868e-08
Iter: 1610 loss: 8.06633125e-08
Iter: 1611 loss: 8.09743383e-08
Iter: 1612 loss: 8.06489808e-08
Iter: 1613 loss: 8.05985039e-08
Iter: 1614 loss: 8.05620459e-08
Iter: 1615 loss: 8.05413052e-08
Iter: 1616 loss: 8.04475633e-08
Iter: 1617 loss: 8.0643396e-08
Iter: 1618 loss: 8.04123204e-08
Iter: 1619 loss: 8.0334253e-08
Iter: 1620 loss: 8.040832e-08
Iter: 1621 loss: 8.02948392e-08
Iter: 1622 loss: 8.02807e-08
Iter: 1623 loss: 8.02486468e-08
Iter: 1624 loss: 8.02173616e-08
Iter: 1625 loss: 8.01518638e-08
Iter: 1626 loss: 8.13217298e-08
Iter: 1627 loss: 8.01535194e-08
Iter: 1628 loss: 8.0078209e-08
Iter: 1629 loss: 8.03829678e-08
Iter: 1630 loss: 8.0057923e-08
Iter: 1631 loss: 8.00137485e-08
Iter: 1632 loss: 8.00164202e-08
Iter: 1633 loss: 7.99763313e-08
Iter: 1634 loss: 7.99284408e-08
Iter: 1635 loss: 7.99272897e-08
Iter: 1636 loss: 7.98556741e-08
Iter: 1637 loss: 8.05615912e-08
Iter: 1638 loss: 7.98501958e-08
Iter: 1639 loss: 7.98050408e-08
Iter: 1640 loss: 7.97099204e-08
Iter: 1641 loss: 8.15884604e-08
Iter: 1642 loss: 7.97097073e-08
Iter: 1643 loss: 7.97086e-08
Iter: 1644 loss: 7.96738e-08
Iter: 1645 loss: 7.96444795e-08
Iter: 1646 loss: 7.9606707e-08
Iter: 1647 loss: 7.95952317e-08
Iter: 1648 loss: 7.95469859e-08
Iter: 1649 loss: 7.9734356e-08
Iter: 1650 loss: 7.95436534e-08
Iter: 1651 loss: 7.94849768e-08
Iter: 1652 loss: 7.95133275e-08
Iter: 1653 loss: 7.94536845e-08
Iter: 1654 loss: 7.93949155e-08
Iter: 1655 loss: 8.0147224e-08
Iter: 1656 loss: 7.93966e-08
Iter: 1657 loss: 7.93415751e-08
Iter: 1658 loss: 7.93011665e-08
Iter: 1659 loss: 7.92826498e-08
Iter: 1660 loss: 7.91948267e-08
Iter: 1661 loss: 7.92465613e-08
Iter: 1662 loss: 7.9128327e-08
Iter: 1663 loss: 7.90588217e-08
Iter: 1664 loss: 7.90588501e-08
Iter: 1665 loss: 7.89895154e-08
Iter: 1666 loss: 7.90363686e-08
Iter: 1667 loss: 7.89452343e-08
Iter: 1668 loss: 7.88819e-08
Iter: 1669 loss: 7.94563064e-08
Iter: 1670 loss: 7.88777825e-08
Iter: 1671 loss: 7.88267229e-08
Iter: 1672 loss: 7.87713148e-08
Iter: 1673 loss: 7.87622128e-08
Iter: 1674 loss: 7.86918264e-08
Iter: 1675 loss: 7.88774628e-08
Iter: 1676 loss: 7.86788732e-08
Iter: 1677 loss: 7.85928052e-08
Iter: 1678 loss: 7.92300625e-08
Iter: 1679 loss: 7.85881e-08
Iter: 1680 loss: 7.85356136e-08
Iter: 1681 loss: 7.84501424e-08
Iter: 1682 loss: 8.06279132e-08
Iter: 1683 loss: 7.84507179e-08
Iter: 1684 loss: 7.83584113e-08
Iter: 1685 loss: 7.90745389e-08
Iter: 1686 loss: 7.83483571e-08
Iter: 1687 loss: 7.82830725e-08
Iter: 1688 loss: 7.86447742e-08
Iter: 1689 loss: 7.82771679e-08
Iter: 1690 loss: 7.82145548e-08
Iter: 1691 loss: 7.85069645e-08
Iter: 1692 loss: 7.82021772e-08
Iter: 1693 loss: 7.8163815e-08
Iter: 1694 loss: 7.81127909e-08
Iter: 1695 loss: 7.81133309e-08
Iter: 1696 loss: 7.80498226e-08
Iter: 1697 loss: 7.82895597e-08
Iter: 1698 loss: 7.80412819e-08
Iter: 1699 loss: 7.79629943e-08
Iter: 1700 loss: 7.82457761e-08
Iter: 1701 loss: 7.7947945e-08
Iter: 1702 loss: 7.7892409e-08
Iter: 1703 loss: 7.80657459e-08
Iter: 1704 loss: 7.78658631e-08
Iter: 1705 loss: 7.78056375e-08
Iter: 1706 loss: 7.78162814e-08
Iter: 1707 loss: 7.77592604e-08
Iter: 1708 loss: 7.7680312e-08
Iter: 1709 loss: 7.75904425e-08
Iter: 1710 loss: 7.75716131e-08
Iter: 1711 loss: 7.76190561e-08
Iter: 1712 loss: 7.75334144e-08
Iter: 1713 loss: 7.74990525e-08
Iter: 1714 loss: 7.74398785e-08
Iter: 1715 loss: 7.74408164e-08
Iter: 1716 loss: 7.73708848e-08
Iter: 1717 loss: 7.74404612e-08
Iter: 1718 loss: 7.7334974e-08
Iter: 1719 loss: 7.72492825e-08
Iter: 1720 loss: 7.77472e-08
Iter: 1721 loss: 7.72357e-08
Iter: 1722 loss: 7.71774324e-08
Iter: 1723 loss: 7.74795126e-08
Iter: 1724 loss: 7.71701423e-08
Iter: 1725 loss: 7.70904336e-08
Iter: 1726 loss: 7.7166419e-08
Iter: 1727 loss: 7.70466073e-08
Iter: 1728 loss: 7.69907444e-08
Iter: 1729 loss: 7.7043623e-08
Iter: 1730 loss: 7.6963147e-08
Iter: 1731 loss: 7.69129542e-08
Iter: 1732 loss: 7.74513396e-08
Iter: 1733 loss: 7.69154838e-08
Iter: 1734 loss: 7.68763257e-08
Iter: 1735 loss: 7.69950432e-08
Iter: 1736 loss: 7.68639552e-08
Iter: 1737 loss: 7.68242217e-08
Iter: 1738 loss: 7.68478756e-08
Iter: 1739 loss: 7.68007595e-08
Iter: 1740 loss: 7.6750581e-08
Iter: 1741 loss: 7.6734409e-08
Iter: 1742 loss: 7.67023707e-08
Iter: 1743 loss: 7.66375834e-08
Iter: 1744 loss: 7.68664208e-08
Iter: 1745 loss: 7.66208927e-08
Iter: 1746 loss: 7.65448647e-08
Iter: 1747 loss: 7.71227349e-08
Iter: 1748 loss: 7.65435715e-08
Iter: 1749 loss: 7.65064527e-08
Iter: 1750 loss: 7.64198589e-08
Iter: 1751 loss: 7.7785316e-08
Iter: 1752 loss: 7.64124479e-08
Iter: 1753 loss: 7.63316805e-08
Iter: 1754 loss: 7.72264386e-08
Iter: 1755 loss: 7.63316876e-08
Iter: 1756 loss: 7.62784538e-08
Iter: 1757 loss: 7.6437864e-08
Iter: 1758 loss: 7.62683925e-08
Iter: 1759 loss: 7.6235672e-08
Iter: 1760 loss: 7.62343291e-08
Iter: 1761 loss: 7.62003722e-08
Iter: 1762 loss: 7.6156e-08
Iter: 1763 loss: 7.6156077e-08
Iter: 1764 loss: 7.60983454e-08
Iter: 1765 loss: 7.61880301e-08
Iter: 1766 loss: 7.60733343e-08
Iter: 1767 loss: 7.60169172e-08
Iter: 1768 loss: 7.67073516e-08
Iter: 1769 loss: 7.60170451e-08
Iter: 1770 loss: 7.59665539e-08
Iter: 1771 loss: 7.60543273e-08
Iter: 1772 loss: 7.59482219e-08
Iter: 1773 loss: 7.59003598e-08
Iter: 1774 loss: 7.60384893e-08
Iter: 1775 loss: 7.58864616e-08
Iter: 1776 loss: 7.58437579e-08
Iter: 1777 loss: 7.58359135e-08
Iter: 1778 loss: 7.58113714e-08
Iter: 1779 loss: 7.57689662e-08
Iter: 1780 loss: 7.60142456e-08
Iter: 1781 loss: 7.57605179e-08
Iter: 1782 loss: 7.57077459e-08
Iter: 1783 loss: 7.59151675e-08
Iter: 1784 loss: 7.56969882e-08
Iter: 1785 loss: 7.56616814e-08
Iter: 1786 loss: 7.56022089e-08
Iter: 1787 loss: 7.6915839e-08
Iter: 1788 loss: 7.55978e-08
Iter: 1789 loss: 7.55246958e-08
Iter: 1790 loss: 7.58664385e-08
Iter: 1791 loss: 7.55062857e-08
Iter: 1792 loss: 7.54459322e-08
Iter: 1793 loss: 7.57986172e-08
Iter: 1794 loss: 7.54312239e-08
Iter: 1795 loss: 7.53884493e-08
Iter: 1796 loss: 7.59307781e-08
Iter: 1797 loss: 7.5387e-08
Iter: 1798 loss: 7.53467404e-08
Iter: 1799 loss: 7.53387823e-08
Iter: 1800 loss: 7.53147873e-08
Iter: 1801 loss: 7.52814131e-08
Iter: 1802 loss: 7.53268452e-08
Iter: 1803 loss: 7.5262129e-08
Iter: 1804 loss: 7.52181322e-08
Iter: 1805 loss: 7.56834879e-08
Iter: 1806 loss: 7.52172511e-08
Iter: 1807 loss: 7.51858522e-08
Iter: 1808 loss: 7.52613e-08
Iter: 1809 loss: 7.51762244e-08
Iter: 1810 loss: 7.51423954e-08
Iter: 1811 loss: 7.51588232e-08
Iter: 1812 loss: 7.51138458e-08
Iter: 1813 loss: 7.50607541e-08
Iter: 1814 loss: 7.50230242e-08
Iter: 1815 loss: 7.50052891e-08
Iter: 1816 loss: 7.49476854e-08
Iter: 1817 loss: 7.53281384e-08
Iter: 1818 loss: 7.49465343e-08
Iter: 1819 loss: 7.49089253e-08
Iter: 1820 loss: 7.49080584e-08
Iter: 1821 loss: 7.48873106e-08
Iter: 1822 loss: 7.4843669e-08
Iter: 1823 loss: 7.53148868e-08
Iter: 1824 loss: 7.48367626e-08
Iter: 1825 loss: 7.47780433e-08
Iter: 1826 loss: 7.48615463e-08
Iter: 1827 loss: 7.47551283e-08
Iter: 1828 loss: 7.47063709e-08
Iter: 1829 loss: 7.47068185e-08
Iter: 1830 loss: 7.46543e-08
Iter: 1831 loss: 7.46601359e-08
Iter: 1832 loss: 7.46200186e-08
Iter: 1833 loss: 7.45719859e-08
Iter: 1834 loss: 7.45945599e-08
Iter: 1835 loss: 7.45400612e-08
Iter: 1836 loss: 7.44980895e-08
Iter: 1837 loss: 7.45004201e-08
Iter: 1838 loss: 7.44700799e-08
Iter: 1839 loss: 7.45190576e-08
Iter: 1840 loss: 7.44568e-08
Iter: 1841 loss: 7.44226725e-08
Iter: 1842 loss: 7.44968176e-08
Iter: 1843 loss: 7.43994377e-08
Iter: 1844 loss: 7.43718758e-08
Iter: 1845 loss: 7.45107371e-08
Iter: 1846 loss: 7.43668096e-08
Iter: 1847 loss: 7.43336486e-08
Iter: 1848 loss: 7.42750075e-08
Iter: 1849 loss: 7.42755617e-08
Iter: 1850 loss: 7.42276143e-08
Iter: 1851 loss: 7.48288e-08
Iter: 1852 loss: 7.42281401e-08
Iter: 1853 loss: 7.41830846e-08
Iter: 1854 loss: 7.44199511e-08
Iter: 1855 loss: 7.41779047e-08
Iter: 1856 loss: 7.41398551e-08
Iter: 1857 loss: 7.40834452e-08
Iter: 1858 loss: 7.5258157e-08
Iter: 1859 loss: 7.40802193e-08
Iter: 1860 loss: 7.40108277e-08
Iter: 1861 loss: 7.42403188e-08
Iter: 1862 loss: 7.39962331e-08
Iter: 1863 loss: 7.39660138e-08
Iter: 1864 loss: 7.39642658e-08
Iter: 1865 loss: 7.39319148e-08
Iter: 1866 loss: 7.38867385e-08
Iter: 1867 loss: 7.38818855e-08
Iter: 1868 loss: 7.38275787e-08
Iter: 1869 loss: 7.38631911e-08
Iter: 1870 loss: 7.38013e-08
Iter: 1871 loss: 7.37549755e-08
Iter: 1872 loss: 7.37586632e-08
Iter: 1873 loss: 7.37154409e-08
Iter: 1874 loss: 7.37474792e-08
Iter: 1875 loss: 7.36940891e-08
Iter: 1876 loss: 7.36430792e-08
Iter: 1877 loss: 7.38195638e-08
Iter: 1878 loss: 7.36371248e-08
Iter: 1879 loss: 7.36033527e-08
Iter: 1880 loss: 7.36416297e-08
Iter: 1881 loss: 7.3584232e-08
Iter: 1882 loss: 7.35453369e-08
Iter: 1883 loss: 7.3544e-08
Iter: 1884 loss: 7.35055323e-08
Iter: 1885 loss: 7.34804644e-08
Iter: 1886 loss: 7.34783541e-08
Iter: 1887 loss: 7.34539753e-08
Iter: 1888 loss: 7.34020631e-08
Iter: 1889 loss: 7.44138404e-08
Iter: 1890 loss: 7.34072e-08
Iter: 1891 loss: 7.33499519e-08
Iter: 1892 loss: 7.33643546e-08
Iter: 1893 loss: 7.331208e-08
Iter: 1894 loss: 7.32576169e-08
Iter: 1895 loss: 7.38347481e-08
Iter: 1896 loss: 7.32533181e-08
Iter: 1897 loss: 7.32123908e-08
Iter: 1898 loss: 7.36861523e-08
Iter: 1899 loss: 7.32095629e-08
Iter: 1900 loss: 7.31868255e-08
Iter: 1901 loss: 7.31525631e-08
Iter: 1902 loss: 7.31500194e-08
Iter: 1903 loss: 7.31125454e-08
Iter: 1904 loss: 7.34519574e-08
Iter: 1905 loss: 7.3109689e-08
Iter: 1906 loss: 7.30734087e-08
Iter: 1907 loss: 7.30659835e-08
Iter: 1908 loss: 7.30379242e-08
Iter: 1909 loss: 7.29918597e-08
Iter: 1910 loss: 7.34469765e-08
Iter: 1911 loss: 7.29849461e-08
Iter: 1912 loss: 7.29499732e-08
Iter: 1913 loss: 7.29066727e-08
Iter: 1914 loss: 7.28989846e-08
Iter: 1915 loss: 7.28381551e-08
Iter: 1916 loss: 7.3183827e-08
Iter: 1917 loss: 7.28273e-08
Iter: 1918 loss: 7.27779224e-08
Iter: 1919 loss: 7.30057437e-08
Iter: 1920 loss: 7.27712859e-08
Iter: 1921 loss: 7.27275875e-08
Iter: 1922 loss: 7.29073051e-08
Iter: 1923 loss: 7.2718187e-08
Iter: 1924 loss: 7.26985405e-08
Iter: 1925 loss: 7.26499891e-08
Iter: 1926 loss: 7.26516873e-08
Iter: 1927 loss: 7.25943181e-08
Iter: 1928 loss: 7.26038465e-08
Iter: 1929 loss: 7.25484526e-08
Iter: 1930 loss: 7.25115541e-08
Iter: 1931 loss: 7.25045766e-08
Iter: 1932 loss: 7.24618232e-08
Iter: 1933 loss: 7.23751228e-08
Iter: 1934 loss: 7.40645305e-08
Iter: 1935 loss: 7.23732256e-08
Iter: 1936 loss: 7.23002387e-08
Iter: 1937 loss: 7.33768815e-08
Iter: 1938 loss: 7.23064915e-08
Iter: 1939 loss: 7.22605762e-08
Iter: 1940 loss: 7.2322706e-08
Iter: 1941 loss: 7.22336964e-08
Iter: 1942 loss: 7.21905309e-08
Iter: 1943 loss: 7.26079037e-08
Iter: 1944 loss: 7.2192627e-08
Iter: 1945 loss: 7.21594304e-08
Iter: 1946 loss: 7.21756948e-08
Iter: 1947 loss: 7.21405868e-08
Iter: 1948 loss: 7.21105664e-08
Iter: 1949 loss: 7.21105593e-08
Iter: 1950 loss: 7.20813631e-08
Iter: 1951 loss: 7.20258839e-08
Iter: 1952 loss: 7.22787661e-08
Iter: 1953 loss: 7.20138758e-08
Iter: 1954 loss: 7.19763804e-08
Iter: 1955 loss: 7.23008e-08
Iter: 1956 loss: 7.19753217e-08
Iter: 1957 loss: 7.1934906e-08
Iter: 1958 loss: 7.18659e-08
Iter: 1959 loss: 7.1869664e-08
Iter: 1960 loss: 7.18037114e-08
Iter: 1961 loss: 7.19348066e-08
Iter: 1962 loss: 7.17749415e-08
Iter: 1963 loss: 7.17360606e-08
Iter: 1964 loss: 7.23320568e-08
Iter: 1965 loss: 7.17378654e-08
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2
+ date
Tue Oct 20 16:51:54 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1 --function f1 --psi -2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 100000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa95e6aca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa95e667378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa95e667c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa950279d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9502c31e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9502c4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa95017cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9500ff730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa95010b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa95010bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc4759d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc6c00d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc6c0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc6be598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc6be048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc6be158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc633620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc633f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9501347b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa95014ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa95014b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc68ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc4cdae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc4d0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc4b8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc5a1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc3a67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa950230598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa950230488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9502146a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa950214488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc742510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc742400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc7441e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc2b71e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8fc2b7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.007022036
test_loss: 0.007402311
train_loss: 0.006391908
test_loss: 0.006387049
train_loss: 0.0060468456
test_loss: 0.0060657584
train_loss: 0.0055827727
test_loss: 0.0055708326
train_loss: 0.0056524063
test_loss: 0.005737335
train_loss: 0.0052915034
test_loss: 0.005527636
train_loss: 0.0049316175
test_loss: 0.005177851
train_loss: 0.004663592
test_loss: 0.0048294947
train_loss: 0.0049866596
test_loss: 0.0050341007
train_loss: 0.0044687237
test_loss: 0.004628058
train_loss: 0.0044931574
test_loss: 0.004755156
train_loss: 0.0044105593
test_loss: 0.0047740997
train_loss: 0.0041541904
test_loss: 0.0044084764
train_loss: 0.0042155865
test_loss: 0.0043005506
train_loss: 0.003990074
test_loss: 0.004300904
train_loss: 0.004237072
test_loss: 0.0043106065
train_loss: 0.004030079
test_loss: 0.0041567804
train_loss: 0.003832646
test_loss: 0.004059811
train_loss: 0.00394179
test_loss: 0.0039740116
train_loss: 0.0040194397
test_loss: 0.0039624213
train_loss: 0.0038598992
test_loss: 0.0039092805
train_loss: 0.0039673843
test_loss: 0.003821251
train_loss: 0.0040099816
test_loss: 0.0040635895
train_loss: 0.004188262
test_loss: 0.0041901204
train_loss: 0.0039073224
test_loss: 0.0038212473
train_loss: 0.0037883436
test_loss: 0.003661706
train_loss: 0.0038646467
test_loss: 0.003873164
train_loss: 0.0036212737
test_loss: 0.003614247
train_loss: 0.0037120031
test_loss: 0.004001387
train_loss: 0.0037325602
test_loss: 0.0039299633
train_loss: 0.0036117793
test_loss: 0.0036310237
train_loss: 0.0036135283
test_loss: 0.0037779235
train_loss: 0.0036476208
test_loss: 0.003967774
train_loss: 0.0037257844
test_loss: 0.0036482597
train_loss: 0.0035760524
test_loss: 0.00405278
train_loss: 0.0035432954
test_loss: 0.0037252465
train_loss: 0.0034868426
test_loss: 0.0036965532
train_loss: 0.00326708
test_loss: 0.0035258818
train_loss: 0.003476556
test_loss: 0.0034991985
train_loss: 0.0034241327
test_loss: 0.0033400378
train_loss: 0.0034324275
test_loss: 0.0036339762
train_loss: 0.0034843949
test_loss: 0.0037464655
train_loss: 0.0032819323
test_loss: 0.0034318317
train_loss: 0.003221584
test_loss: 0.0035733948
train_loss: 0.0032242592
test_loss: 0.0032582888
train_loss: 0.003297567
test_loss: 0.0034213953
train_loss: 0.0032767022
test_loss: 0.0034559043
train_loss: 0.0033031825
test_loss: 0.0033787284
train_loss: 0.0032907824
test_loss: 0.0035878182
train_loss: 0.003258306
test_loss: 0.0034000215
train_loss: 0.003148241
test_loss: 0.0033347488
train_loss: 0.0034855632
test_loss: 0.0033656263
train_loss: 0.003228475
test_loss: 0.00350705
train_loss: 0.0034623824
test_loss: 0.0032915387
train_loss: 0.0031307717
test_loss: 0.0032148738
train_loss: 0.0032814334
test_loss: 0.003440969
train_loss: 0.003335307
test_loss: 0.0037607925
train_loss: 0.0031661892
test_loss: 0.0032458964
train_loss: 0.0034384509
test_loss: 0.0034414276
train_loss: 0.0031739401
test_loss: 0.003284301
train_loss: 0.0031871563
test_loss: 0.0031871295
train_loss: 0.00341717
test_loss: 0.003547772
train_loss: 0.003140655
test_loss: 0.003095476
train_loss: 0.0031779609
test_loss: 0.0033796642
train_loss: 0.0030733063
test_loss: 0.0032562867
train_loss: 0.0032369234
test_loss: 0.003167866
train_loss: 0.0030791715
test_loss: 0.0031392202
train_loss: 0.0034470547
test_loss: 0.003264459
train_loss: 0.0032888805
test_loss: 0.0032084433
train_loss: 0.0031769648
test_loss: 0.0031894094
train_loss: 0.0034457482
test_loss: 0.003429233
train_loss: 0.0030116425
test_loss: 0.0031741767
train_loss: 0.0028819807
test_loss: 0.0030975384
train_loss: 0.0031868592
test_loss: 0.0031566028
train_loss: 0.0031869146
test_loss: 0.0032303103
train_loss: 0.003194673
test_loss: 0.0032111595
train_loss: 0.0033221317
test_loss: 0.0030962552
train_loss: 0.0030238873
test_loss: 0.0030017016
train_loss: 0.0032592793
test_loss: 0.0033383467
train_loss: 0.0031143134
test_loss: 0.00341951
train_loss: 0.003031112
test_loss: 0.0031055259
train_loss: 0.002996223
test_loss: 0.003111626
train_loss: 0.0030996702
test_loss: 0.0031313316
train_loss: 0.003101706
test_loss: 0.0031203215
train_loss: 0.003169558
test_loss: 0.0031735233
train_loss: 0.0031088185
test_loss: 0.0031202869
train_loss: 0.0031136672
test_loss: 0.0031389277
train_loss: 0.0028794715
test_loss: 0.0032548744
train_loss: 0.0033968505
test_loss: 0.0034456328
train_loss: 0.003023092
test_loss: 0.0032419413
train_loss: 0.002893646
test_loss: 0.0029676142
train_loss: 0.003161277
test_loss: 0.0031777176
train_loss: 0.0029461025
test_loss: 0.00310401
train_loss: 0.0030159282
test_loss: 0.0031133692
train_loss: 0.0028319587
test_loss: 0.0031046427
train_loss: 0.0030093584
test_loss: 0.0030255117
train_loss: 0.0028593247
test_loss: 0.0031147008
train_loss: 0.0029207678
test_loss: 0.0030115575
train_loss: 0.003093442
test_loss: 0.0029968915
train_loss: 0.0029914817
test_loss: 0.0030384385
train_loss: 0.0029908586
test_loss: 0.0032467335
train_loss: 0.0028903915
test_loss: 0.0029666678
train_loss: 0.0030055083
test_loss: 0.0032127819
train_loss: 0.0030113459
test_loss: 0.003132013
train_loss: 0.002848334
test_loss: 0.002996505
train_loss: 0.0031919114
test_loss: 0.0029817512
train_loss: 0.003054486
test_loss: 0.0030093943
train_loss: 0.0030540335
test_loss: 0.003097581
train_loss: 0.0029635222
test_loss: 0.002993688
train_loss: 0.003385757
test_loss: 0.0032866306
train_loss: 0.0029468588
test_loss: 0.0030343824
train_loss: 0.0029497598
test_loss: 0.002977659
train_loss: 0.0028605768
test_loss: 0.0029152443
train_loss: 0.0029452178
test_loss: 0.003217107
train_loss: 0.0028784135
test_loss: 0.0030819138
train_loss: 0.0030256975
test_loss: 0.0028898579
train_loss: 0.0028133697
test_loss: 0.0030346483
train_loss: 0.0027975247
test_loss: 0.0029952284
train_loss: 0.0028384645
test_loss: 0.0029801074
train_loss: 0.002830272
test_loss: 0.0029713016
train_loss: 0.0029600696
test_loss: 0.0028672472
train_loss: 0.002749456
test_loss: 0.002926622
train_loss: 0.0029731938
test_loss: 0.0029968517
train_loss: 0.0029315872
test_loss: 0.0029392259
train_loss: 0.0026717987
test_loss: 0.0028335603
train_loss: 0.0028027098
test_loss: 0.0032489647
train_loss: 0.002990325
test_loss: 0.0033878637
train_loss: 0.0027754586
test_loss: 0.00287594
train_loss: 0.0027990283
test_loss: 0.0029825885
train_loss: 0.0028580748
test_loss: 0.0030928818
train_loss: 0.0028528725
test_loss: 0.002905056
train_loss: 0.0026957646
test_loss: 0.0028272478
train_loss: 0.0027339486
test_loss: 0.0029136555
train_loss: 0.0028147625
test_loss: 0.003009249
train_loss: 0.003002482
test_loss: 0.0030249231
train_loss: 0.0028598723
test_loss: 0.0028466056
train_loss: 0.0028256287
test_loss: 0.002753956
train_loss: 0.0026872205
test_loss: 0.0028978395
train_loss: 0.0027399664
test_loss: 0.002759044
train_loss: 0.002762909
test_loss: 0.0029468609
train_loss: 0.0025530625
test_loss: 0.002653207
train_loss: 0.002667433
test_loss: 0.0027195513
train_loss: 0.0027313589
test_loss: 0.0028787884
train_loss: 0.0027378965
test_loss: 0.0027897335
train_loss: 0.0028403015
test_loss: 0.0028236816
train_loss: 0.003082391
test_loss: 0.0030225678
train_loss: 0.0028013035
test_loss: 0.0030016128
train_loss: 0.002561766
test_loss: 0.0028043212
train_loss: 0.0028695313
test_loss: 0.0026635034
train_loss: 0.0028520967
test_loss: 0.003037102
train_loss: 0.0029122601
test_loss: 0.003248176
train_loss: 0.0028079806
test_loss: 0.0029855724
train_loss: 0.002839339
test_loss: 0.0027721163
train_loss: 0.0028543663
test_loss: 0.0028741618
train_loss: 0.002842697
test_loss: 0.0029040358
train_loss: 0.0029654964
test_loss: 0.002911828
train_loss: 0.0028608355
test_loss: 0.0028719655
train_loss: 0.0027031025
test_loss: 0.0027555765
train_loss: 0.0029349318
test_loss: 0.003118329
train_loss: 0.0028169574
test_loss: 0.0028328218
train_loss: 0.002752125
test_loss: 0.002820594
train_loss: 0.0030758418
test_loss: 0.0028430873
train_loss: 0.0026604503
test_loss: 0.0030606927
train_loss: 0.0025521358
test_loss: 0.0027395226
train_loss: 0.0027715247
test_loss: 0.0027732465
train_loss: 0.0028344812
test_loss: 0.0032343704
train_loss: 0.0029400478
test_loss: 0.0028183353
train_loss: 0.0026595723
test_loss: 0.0027360586
train_loss: 0.0025255692
test_loss: 0.0027283342
train_loss: