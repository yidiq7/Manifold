+ RUN=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='0 1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1 --function f1 --psi 0 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d5006b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d50042840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4823b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d500b7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48202f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d481db620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d481dbea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4811f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4811f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4811f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48178268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480119d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4803b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480950d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480ae400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48095048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48155f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48079620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480792f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf056b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf0555378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480d10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf0555f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480d2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf04fd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d30048b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d30048730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d30066378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d30066048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf03eb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf0424268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf0496f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf03de598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf048bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf03490d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf02a9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.01446991
test_loss: 0.014563111
train_loss: 0.0061007137
test_loss: 0.0057039363
train_loss: 0.0029843245
test_loss: 0.0029687865
train_loss: 0.0022280903
test_loss: 0.0019834223
train_loss: 0.001850297
test_loss: 0.0021342314
train_loss: 0.001932413
test_loss: 0.002534642
train_loss: 0.0017468589
test_loss: 0.0018275735
train_loss: 0.0016478723
test_loss: 0.0018323604
train_loss: 0.001758661
test_loss: 0.001729004
train_loss: 0.0016869906
test_loss: 0.0016550544
train_loss: 0.001781175
test_loss: 0.002203674
train_loss: 0.001658789
test_loss: 0.0017269037
train_loss: 0.00184351
test_loss: 0.0016266324
train_loss: 0.0016099224
test_loss: 0.0017676431
train_loss: 0.0016576992
test_loss: 0.0017862144
train_loss: 0.0015958007
test_loss: 0.0018138258
train_loss: 0.0016244443
test_loss: 0.00187894
train_loss: 0.0016558589
test_loss: 0.0016680209
train_loss: 0.0015983128
test_loss: 0.0016659022
train_loss: 0.0015355722
test_loss: 0.0016872641
train_loss: 0.001711067
test_loss: 0.0016990335
train_loss: 0.0014374659
test_loss: 0.0016078943
train_loss: 0.0016087075
test_loss: 0.0015395381
train_loss: 0.0015143383
test_loss: 0.0016746816
train_loss: 0.0016346029
test_loss: 0.0017192756
train_loss: 0.0015073996
test_loss: 0.0015587525
train_loss: 0.0014686474
test_loss: 0.0016429438
train_loss: 0.0016858317
test_loss: 0.0017663288
train_loss: 0.0016277325
test_loss: 0.0016505244
train_loss: 0.0014395562
test_loss: 0.0015161858
train_loss: 0.0017073299
test_loss: 0.001642084
train_loss: 0.0015001392
test_loss: 0.0017853288
train_loss: 0.0015324022
test_loss: 0.0015694213
train_loss: 0.0017190642
test_loss: 0.002040534
train_loss: 0.0019446943
test_loss: 0.0018911173
train_loss: 0.0015931504
test_loss: 0.0017187675
train_loss: 0.0016002906
test_loss: 0.0014440895
train_loss: 0.0015964371
test_loss: 0.0017356991
train_loss: 0.0014666321
test_loss: 0.0016223755
train_loss: 0.0022661954
test_loss: 0.0016969298
train_loss: 0.002013837
test_loss: 0.0017684957
train_loss: 0.0014898067
test_loss: 0.0014933725
train_loss: 0.0014357049
test_loss: 0.0015457827
train_loss: 0.0015759708
test_loss: 0.0015848638
train_loss: 0.0015968217
test_loss: 0.0015508466
train_loss: 0.0015940558
test_loss: 0.0016074141
train_loss: 0.0014032067
test_loss: 0.0017015783
train_loss: 0.0016453989
test_loss: 0.001819515
train_loss: 0.001533316
test_loss: 0.0016596533
train_loss: 0.001744759
test_loss: 0.0015761027
train_loss: 0.0015969123
test_loss: 0.001567108
train_loss: 0.001583218
test_loss: 0.0014744926
train_loss: 0.0016172767
test_loss: 0.0014762414
train_loss: 0.0013788611
test_loss: 0.0016270675
train_loss: 0.0014433893
test_loss: 0.0016332268
train_loss: 0.0016015944
test_loss: 0.0014983468
train_loss: 0.0015836951
test_loss: 0.001656378
train_loss: 0.0019046154
test_loss: 0.0016807339
train_loss: 0.0015642921
test_loss: 0.001449934
train_loss: 0.0015236659
test_loss: 0.001625768
train_loss: 0.001438518
test_loss: 0.00159101
train_loss: 0.0017950196
test_loss: 0.0017771337
train_loss: 0.0016934378
test_loss: 0.0014408209
train_loss: 0.001451454
test_loss: 0.0014762724
train_loss: 0.0016224268
test_loss: 0.0015628005
train_loss: 0.001439381
test_loss: 0.0016408323
train_loss: 0.0014427197
test_loss: 0.0015511219
train_loss: 0.0019329096
test_loss: 0.0017298143
train_loss: 0.0014603699
test_loss: 0.0016668828
train_loss: 0.0014633517
test_loss: 0.0015038365
train_loss: 0.0016945603
test_loss: 0.0014765506
train_loss: 0.0015610064
test_loss: 0.0015900561
train_loss: 0.0014894115
test_loss: 0.0017577407
train_loss: 0.0015504151
test_loss: 0.0015772174
train_loss: 0.001785702
test_loss: 0.0015981816
train_loss: 0.0013556577
test_loss: 0.0016797732
train_loss: 0.0019596303
test_loss: 0.0016231047
train_loss: 0.0015978557
test_loss: 0.0019548875
train_loss: 0.0018089241
test_loss: 0.0017244956
train_loss: 0.0016637614
test_loss: 0.0016568156
train_loss: 0.0016396774
test_loss: 0.0016148655
train_loss: 0.0015247636
test_loss: 0.0015079884
train_loss: 0.0015650761
test_loss: 0.0013783799
train_loss: 0.0017130303
test_loss: 0.0018068748
train_loss: 0.0014303089
test_loss: 0.0014289726
train_loss: 0.0014250784
test_loss: 0.0018212582
train_loss: 0.0016418603
test_loss: 0.0018726395
train_loss: 0.0014853289
test_loss: 0.0015382114
train_loss: 0.0014621972
test_loss: 0.0015432447
train_loss: 0.0013706426
test_loss: 0.00161236
train_loss: 0.0014555722
test_loss: 0.001560037
train_loss: 0.0014903278
test_loss: 0.0014532026
train_loss: 0.0016215194
test_loss: 0.0015232835
train_loss: 0.0014487463
test_loss: 0.0014153557
train_loss: 0.001453269
test_loss: 0.001495504
train_loss: 0.001645748
test_loss: 0.0015742294
train_loss: 0.001486985
test_loss: 0.0015117292
train_loss: 0.001293146
test_loss: 0.0014536665
train_loss: 0.0012717189
test_loss: 0.0017682397
train_loss: 0.0018159614
test_loss: 0.0014019535
train_loss: 0.0015336735
test_loss: 0.0016174158
train_loss: 0.001640986
test_loss: 0.0016490876
train_loss: 0.0017285929
test_loss: 0.0015039394
train_loss: 0.0015129268
test_loss: 0.001501659
train_loss: 0.0014770599
test_loss: 0.00166554
train_loss: 0.0014768214
test_loss: 0.0015077664
train_loss: 0.0013613536
test_loss: 0.0014948576
train_loss: 0.0016425725
test_loss: 0.0014975911
train_loss: 0.0015560121
test_loss: 0.0017135025
train_loss: 0.0016344946
test_loss: 0.0017686852
train_loss: 0.0014189279
test_loss: 0.0015491813
train_loss: 0.0017162962
test_loss: 0.0013569933
train_loss: 0.0017372073
test_loss: 0.0015285186
train_loss: 0.0016153982
test_loss: 0.0020301905
train_loss: 0.0014209867
test_loss: 0.0016320463
train_loss: 0.0014127066
test_loss: 0.0014944248
train_loss: 0.0014703255
test_loss: 0.001583737
train_loss: 0.0014225662
test_loss: 0.0014884382
train_loss: 0.0012278806
test_loss: 0.0015368352
train_loss: 0.0015249306
test_loss: 0.0015929276
train_loss: 0.0014749116
test_loss: 0.001581356
train_loss: 0.0014457266
test_loss: 0.0015895407
train_loss: 0.0013781362
test_loss: 0.0016907799
train_loss: 0.0018331269
test_loss: 0.0018032367
train_loss: 0.0015140672
test_loss: 0.001694127
train_loss: 0.00144173
test_loss: 0.001309777
train_loss: 0.0013793635
test_loss: 0.0014030401
train_loss: 0.0013339465
test_loss: 0.0014826511
train_loss: 0.0014160526
test_loss: 0.0015112892
train_loss: 0.0017736193
test_loss: 0.0015836468
train_loss: 0.0014529022
test_loss: 0.0016319467
train_loss: 0.0013588046
test_loss: 0.0016108248
train_loss: 0.0016617901
test_loss: 0.0015876953
train_loss: 0.0014381143
test_loss: 0.001420244
train_loss: 0.0015028443
test_loss: 0.0014328328
train_loss: 0.0018428022
test_loss: 0.0016394155
train_loss: 0.0017813148
test_loss: 0.0014182451
train_loss: 0.0015414245
test_loss: 0.0014307202
train_loss: 0.0014657108
test_loss: 0.001655902
train_loss: 0.001583894
test_loss: 0.0016174768
train_loss: 0.0017861134
test_loss: 0.0016336904
train_loss: 0.0014783943
test_loss: 0.0015000133
train_loss: 0.0016571431
test_loss: 0.0016172334
train_loss: 0.0014215212
test_loss: 0.00190303
train_loss: 0.0014609338
test_loss: 0.0017572785
train_loss: 0.0014708242
test_loss: 0.0014933813
train_loss: 0.0016582767
test_loss: 0.0014312085
train_loss: 0.0014339054
test_loss: 0.0015002323
train_loss: 0.0016902378
test_loss: 0.0015686426
train_loss: 0.0015350088
test_loss: 0.0015101255
train_loss: 0.0013320304
test_loss: 0.001445899
train_loss: 0.0014558436
test_loss: 0.0014504981
train_loss: 0.0015751593
test_loss: 0.0017077051
train_loss: 0.0017902954
test_loss: 0.0016688957
train_loss: 0.001461013
test_loss: 0.0016848516
train_loss: 0.0015692152
test_loss: 0.0014802668
train_loss: 0.001397494
test_loss: 0.0015489759
train_loss: 0.0014356087
test_loss: 0.0017592951
train_loss: 0.00156361
test_loss: 0.001506651
train_loss: 0.0017183595
test_loss: 0.0015042296
train_loss: 0.0013784659
test_loss: 0.0014277741
train_loss: 0.0013683643
test_loss: 0.0015297084
train_loss: 0.0012907035
test_loss: 0.0014080602
train_loss: 0.0013201418
test_loss: 0.001384942
train_loss: 0.0016413741
test_loss: 0.0014957768
train_loss: 0.0013758225
test_loss: 0.0018150372
train_loss: 0.0014531198
test_loss: 0.0015570043
train_loss: 0.001414399
test_loss: 0.0014045959
train_loss: 0.0014860721
test_loss: 0.0014274438
train_loss: 0.0014137501
test_loss: 0.0015327838
train_loss: 0.001277756
test_loss: 0.0012679124
train_loss: 0.0013596637
test_loss: 0.0017831826
train_loss: 0.0015593704
test_loss: 0.0016056147
train_loss: 0.0013414015
test_loss: 0.0014384162
train_loss: 0.0015234689
test_loss: 0.0012737212
train_loss: 0.0014330641
test_loss: 0.0015948906
train_loss: 0.001573056
test_loss: 0.001617892
train_loss: 0.001447568
test_loss: 0.0014805155
train_loss: 0.0015321799
test_loss: 0.001339891
train_loss: 0.0014027252
test_loss: 0.0014441533
train_loss: 0.0015259949
test_loss: 0.0014097646
train_loss: 0.0013040403
test_loss: 0.0013511409
train_loss: 0.0016715988
test_loss: 0.0016655498
train_loss: 0.0014940179
test_loss: 0.0015317177
train_loss: 0.0016524498
test_loss: 0.0015901997
train_loss: 0.0013186787
test_loss: 0.0017084526
train_loss: 0.0013541421
test_loss: 0.0014523218
train_loss: 0.0013671687
test_loss: 0.0015558104
train_loss: 0.0015906491
test_loss: 0.0013784255
train_loss: 0.0016700743
test_loss: 0.0013970636
train_loss: 0.001479984
test_loss: 0.0014134469
train_loss: 0.0017400414
test_loss: 0.001376923
train_loss: 0.0013729646
test_loss: 0.0013080407
train_loss: 0.0014066153
test_loss: 0.0015263975
train_loss: 0.0014601744
test_loss: 0.0014722964
train_loss: 0.0012941139
test_loss: 0.00145577
train_loss: 0.0014750783
test_loss: 0.0017029109
train_loss: 0.0013822546
test_loss: 0.0013884901
train_loss: 0.0012710359
test_loss: 0.0014446508
train_loss: 0.0014942574
test_loss: 0.0014321767
train_loss: 0.0013153483
test_loss: 0.0014142843
train_loss: 0.0013886084
test_loss: 0.0017549748
train_loss: 0.0015359556
test_loss: 0.0014066724
train_loss: 0.001521497
test_loss: 0.001809925
train_loss: 0.0015140332
test_loss: 0.0014500907
train_loss: 0.0013236115
test_loss: 0.0017262682
train_loss: 0.001402145
test_loss: 0.0014286883
train_loss: 0.0012410501
test_loss: 0.0014471372
train_loss: 0.0014986605
test_loss: 0.0016517887
train_loss: 0.0015107307
test_loss: 0.0015618787
train_loss: 0.0014216696
test_loss: 0.0016765672
train_loss: 0.0014310125
test_loss: 0.001445288
train_loss: 0.0014756061
test_loss: 0.001410923
train_loss: 0.0014969462
test_loss: 0.0015486982
train_loss: 0.001447938
test_loss: 0.0016327292
train_loss: 0.0013367047
test_loss: 0.0018495074
train_loss: 0.0014273216
test_loss: 0.0014834874
train_loss: 0.0014102816
test_loss: 0.0014595977
train_loss: 0.0014819386
test_loss: 0.0016490534
train_loss: 0.0013889554
test_loss: 0.0014832243
train_loss: 0.0015212616
test_loss: 0.0016344985
train_loss: 0.0017937169
test_loss: 0.0015656346
train_loss: 0.0015375549
test_loss: 0.001715795
train_loss: 0.001305368
test_loss: 0.0015075401
train_loss: 0.0013870564
test_loss: 0.0015610573
train_loss: 0.0015742984
test_loss: 0.0014954566
train_loss: 0.0015045939
test_loss: 0.0015422287
train_loss: 0.001587027
test_loss: 0.0014604046
train_loss: 0.0013660378
test_loss: 0.0015623491
train_loss: 0.0015801725
test_loss: 0.0015435684
train_loss: 0.0014816442
test_loss: 0.0015432443
train_loss: 0.0014255899
test_loss: 0.0014864856
train_loss: 0.0014168565
test_loss: 0.0015866129
train_loss: 0.0014174237
test_loss: 0.0015370129
train_loss: 0.0013607971
test_loss: 0.0020046507
train_loss: 0.0015179964
test_loss: 0.0014220093
train_loss: 0.0013930805
test_loss: 0.0013122876
train_loss: 0.0014497729
test_loss: 0.0016451248
train_loss: 0.0015281609
test_loss: 0.0015396327
train_loss: 0.0014655908
test_loss: 0.001615487
train_loss: 0.0014467072
test_loss: 0.0014940519
train_loss: 0.0014258396
test_loss: 0.0014120623
train_loss: 0.001590949
test_loss: 0.0015562979
train_loss: 0.0013666612
test_loss: 0.001688748
train_loss: 0.0014256646
test_loss: 0.0016673609
train_loss: 0.0012837786
test_loss: 0.0014689006
train_loss: 0.0015225306
test_loss: 0.0013226365
train_loss: 0.0014435058
test_loss: 0.0016097219
train_loss: 0.001490226
test_loss: 0.0014707719
train_loss: 0.0015570254
test_loss: 0.0014763457
train_loss: 0.0014651002
test_loss: 0.0014806896
train_loss: 0.0014131137
test_loss: 0.0015715347
train_loss: 0.0013346198
test_loss: 0.0014365915
train_loss: 0.0014147961
test_loss: 0.0015735792
train_loss: 0.001558482
test_loss: 0.001689539
train_loss: 0.0015526665
test_loss: 0.0017777268
train_loss: 0.0015912428
test_loss: 0.0017169814
train_loss: 0.0015534265
test_loss: 0.0014019324
train_loss: 0.0014093252
test_loss: 0.0014856003
train_loss: 0.0012982495
test_loss: 0.0016091944
train_loss: 0.0015962397
test_loss: 0.0014576904
train_loss: 0.0014527976
test_loss: 0.0015175258
train_loss: 0.0015200137
test_loss: 0.0014382985
train_loss: 0.0016757972
test_loss: 0.0015269875
train_loss: 0.0013741413
test_loss: 0.0013454
train_loss: 0.001477249
test_loss: 0.0013973379
train_loss: 0.0013785462
test_loss: 0.0016340134
train_loss: 0.0014377218
test_loss: 0.0014360637
train_loss: 0.0013921668
test_loss: 0.0013515848
train_loss: 0.0015138204
test_loss: 0.0014131402
train_loss: 0.0013741986
test_loss: 0.0015224686
train_loss: 0.0014870095
test_loss: 0.0016747715
train_loss: 0.0014078515
test_loss: 0.0014689103
train_loss: 0.0013958886
test_loss: 0.0014381474
train_loss: 0.00137558
test_loss: 0.0015118683
train_loss: 0.0013358187
test_loss: 0.0014284201
train_loss: 0.0015065102
test_loss: 0.0013745155
train_loss: 0.0015887045
test_loss: 0.0014611683
train_loss: 0.0015606779
test_loss: 0.0015876652
train_loss: 0.0015719568
test_loss: 0.0014939316
train_loss: 0.0014262844
test_loss: 0.0014986428
train_loss: 0.0013385668
test_loss: 0.0015234804
train_loss: 0.0012120851
test_loss: 0.0012789413
train_loss: 0.0014410091
test_loss: 0.0016880564
train_loss: 0.0012948622
test_loss: 0.0017923623
train_loss: 0.0015650354
test_loss: 0.0017466852
train_loss: 0.001385052
test_loss: 0.0017402772
train_loss: 0.0014741006
test_loss: 0.0017429255
train_loss: 0.0014156213
test_loss: 0.0015291126
train_loss: 0.0016936248
test_loss: 0.0013755395
train_loss: 0.0012992825
test_loss: 0.0015683166
train_loss: 0.0013352858
test_loss: 0.0015312992
train_loss: 0.0013649424
test_loss: 0.0014869414
train_loss: 0.0013231616
test_loss: 0.0015380144
train_loss: 0.0015821371
test_loss: 0.0014820109
train_loss: 0.0012809385
test_loss: 0.0016835411
train_loss: 0.0015572383
test_loss: 0.0015560972
train_loss: 0.0016272591
test_loss: 0.0013203487
train_loss: 0.0013345317
test_loss: 0.0014885379
train_loss: 0.0014487236
test_loss: 0.0014548007
train_loss: 0.0013212552
test_loss: 0.0013025034
train_loss: 0.0014428523
test_loss: 0.0013844796
train_loss: 0.0012974569
test_loss: 0.0014775242
train_loss: 0.0014225338
test_loss: 0.0015958056
train_loss: 0.0014615837
test_loss: 0.0013792451
train_loss: 0.0013449255
test_loss: 0.0014553375
train_loss: 0.0013739971
test_loss: 0.0013114054
train_loss: 0.001374784
test_loss: 0.001417581
train_loss: 0.0013198523
test_loss: 0.0013718043
train_loss: 0.0015022773
test_loss: 0.0016950854
train_loss: 0.0019781184
test_loss: 0.001643026
train_loss: 0.0013600537
test_loss: 0.0015435726
train_loss: 0.0016295132
test_loss: 0.0015399072
train_loss: 0.001646334
test_loss: 0.0015145659
train_loss: 0.0015850936
test_loss: 0.001870616
train_loss: 0.0013257648
test_loss: 0.0017225089
train_loss: 0.0012660936
test_loss: 0.0013343855
train_loss: 0.0014383723
test_loss: 0.0015453593
train_loss: 0.0013907064
test_loss: 0.0013401497
train_loss: 0.0013601249
test_loss: 0.0014754762
train_loss: 0.001396916
test_loss: 0.0013455708
train_loss: 0.0013642213
test_loss: 0.0013115632
train_loss: 0.0015313214
test_loss: 0.0013405582
train_loss: 0.0013751647
test_loss: 0.0015248109
train_loss: 0.0013186116
test_loss: 0.0015073994
train_loss: 0.0014920167
test_loss: 0.001485834
train_loss: 0.0015281143
test_loss: 0.0014298985
train_loss: 0.0014124538
test_loss: 0.0014213847
train_loss: 0.0014022395
test_loss: 0.0014109123
train_loss: 0.0014368199
test_loss: 0.0014391667
train_loss: 0.0014725878
test_loss: 0.0015497493
train_loss: 0.0015073218
test_loss: 0.0015339466
train_loss: 0.0013636653
test_loss: 0.0013886383
train_loss: 0.001728624
test_loss: 0.0015224301
train_loss: 0.0013574834
test_loss: 0.0015514166
train_loss: 0.0014211363
test_loss: 0.0015485353
train_loss: 0.0017218156
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
test_loss: 0.0013878092
train_loss: 0.001475277
test_loss: 0.0017442113
train_loss: 0.0016256459
test_loss: 0.001633986
train_loss: 0.0012510968
test_loss: 0.0014752301
train_loss: 0.0012361784
test_loss: 0.0015759117
train_loss: 0.0013991217
test_loss: 0.001396975
train_loss: 0.0019350554
test_loss: 0.0013713478
train_loss: 0.0017251641
test_loss: 0.0014397216
train_loss: 0.001353412
test_loss: 0.0017204063
train_loss: 0.0014993977
test_loss: 0.0014968325
train_loss: 0.0015862407
test_loss: 0.0014354254
train_loss: 0.0012285576
test_loss: 0.0017759237
train_loss: 0.0015174267
test_loss: 0.0014423534
train_loss: 0.0013673684
test_loss: 0.001390212
train_loss: 0.0012749638
test_loss: 0.0014282076
train_loss: 0.0012710089
test_loss: 0.0014345777
train_loss: 0.0013052354
test_loss: 0.0016724768
train_loss: 0.0014125541
test_loss: 0.0012864624
train_loss: 0.0015152947
test_loss: 0.0015124533
train_loss: 0.0013430432
test_loss: 0.0014485155
train_loss: 0.0013324504
test_loss: 0.0013013548
train_loss: 0.0016578173
test_loss: 0.0015228215
train_loss: 0.0013796499
test_loss: 0.0018758621
train_loss: 0.0013569505
test_loss: 0.0014462544
train_loss: 0.0013229394
test_loss: 0.001368823
train_loss: 0.0016070299
test_loss: 0.0014097806
train_loss: 0.0013115944
test_loss: 0.0016082012
train_loss: 0.0014270482
test_loss: 0.0013836159
train_loss: 0.0013671007
test_loss: 0.0014289151
train_loss: 0.0013596357
test_loss: 0.001456479
train_loss: 0.0014648982
test_loss: 0.0012871297
train_loss: 0.0012566395
test_loss: 0.001533489
train_loss: 0.0014007286
test_loss: 0.0014395473
train_loss: 0.0013886758
test_loss: 0.0016520122
train_loss: 0.0014141721
test_loss: 0.0015225235
train_loss: 0.0013985448
test_loss: 0.0013241741
train_loss: 0.0014713507
test_loss: 0.0015886216
train_loss: 0.0014968038
test_loss: 0.0013955629
train_loss: 0.0014127896
test_loss: 0.0015917782
train_loss: 0.001669686
test_loss: 0.0015452713
train_loss: 0.0014676875
test_loss: 0.0014705528
train_loss: 0.0013773564
test_loss: 0.0015788996
train_loss: 0.0015142972
test_loss: 0.0015299346
train_loss: 0.0013011172
test_loss: 0.001345937
train_loss: 0.0012402565
test_loss: 0.0014224151
train_loss: 0.0013756165
test_loss: 0.0014134049
train_loss: 0.001571715
test_loss: 0.0014289744
train_loss: 0.0013906276
test_loss: 0.001414175
train_loss: 0.0014316312
test_loss: 0.0014302703
train_loss: 0.001304377
test_loss: 0.0014763335
train_loss: 0.0013441988
test_loss: 0.0015980445
train_loss: 0.0017669268
test_loss: 0.001429147
train_loss: 0.0013799511
test_loss: 0.0013702397
train_loss: 0.0014846993
test_loss: 0.0012934621
train_loss: 0.0013515005
test_loss: 0.0012363696
train_loss: 0.0012705945
test_loss: 0.0013683463
train_loss: 0.0012952298
test_loss: 0.0013100216
train_loss: 0.0013683639
test_loss: 0.0013572149
train_loss: 0.0014456001
test_loss: 0.0013699345
train_loss: 0.0013124323
test_loss: 0.0016054502
train_loss: 0.0013912058
test_loss: 0.0014344753
train_loss: 0.0012366488
test_loss: 0.0013875691
train_loss: 0.0015618025
test_loss: 0.0014748742
train_loss: 0.0015847064
test_loss: 0.0015977737
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcded400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fce0c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd819d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fce2abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd5af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd5a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd0cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd771e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fccd7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fccd7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fccae1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcc4abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcc71950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcc17268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcc71a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcbec9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcbec730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcb53ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcb69d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcbec488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbebca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe570d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbebc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe4f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe4f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe4fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe4f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbdc69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbdc6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b45057b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b4520268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b4487f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b4487730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b44d8d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b44500d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b4473f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.55788461e-06
Iter: 2 loss: 3.38037557e-06
Iter: 3 loss: 2.27885e-06
Iter: 4 loss: 1.90168623e-06
Iter: 5 loss: 1.76326103e-06
Iter: 6 loss: 1.55402813e-06
Iter: 7 loss: 1.42646286e-06
Iter: 8 loss: 1.41123576e-06
Iter: 9 loss: 1.35459095e-06
Iter: 10 loss: 1.32728883e-06
Iter: 11 loss: 1.29992065e-06
Iter: 12 loss: 1.24824055e-06
Iter: 13 loss: 2.04126036e-06
Iter: 14 loss: 1.24819985e-06
Iter: 15 loss: 1.21244136e-06
Iter: 16 loss: 1.22334723e-06
Iter: 17 loss: 1.18684898e-06
Iter: 18 loss: 1.16322008e-06
Iter: 19 loss: 1.269108e-06
Iter: 20 loss: 1.15855596e-06
Iter: 21 loss: 1.14076647e-06
Iter: 22 loss: 1.15416537e-06
Iter: 23 loss: 1.12986265e-06
Iter: 24 loss: 1.11019835e-06
Iter: 25 loss: 1.11014242e-06
Iter: 26 loss: 1.10192821e-06
Iter: 27 loss: 1.08460199e-06
Iter: 28 loss: 1.37487382e-06
Iter: 29 loss: 1.08413428e-06
Iter: 30 loss: 1.06803941e-06
Iter: 31 loss: 1.10739279e-06
Iter: 32 loss: 1.06227503e-06
Iter: 33 loss: 1.05222784e-06
Iter: 34 loss: 1.05872607e-06
Iter: 35 loss: 1.04582784e-06
Iter: 36 loss: 1.03278e-06
Iter: 37 loss: 1.07106405e-06
Iter: 38 loss: 1.02874276e-06
Iter: 39 loss: 1.02763613e-06
Iter: 40 loss: 1.02279182e-06
Iter: 41 loss: 1.01788567e-06
Iter: 42 loss: 1.00811212e-06
Iter: 43 loss: 1.19674633e-06
Iter: 44 loss: 1.00803686e-06
Iter: 45 loss: 9.98287e-07
Iter: 46 loss: 1.06929156e-06
Iter: 47 loss: 9.97500479e-07
Iter: 48 loss: 9.86287432e-07
Iter: 49 loss: 9.8971077e-07
Iter: 50 loss: 9.78229536e-07
Iter: 51 loss: 9.70720748e-07
Iter: 52 loss: 1.04609353e-06
Iter: 53 loss: 9.70449605e-07
Iter: 54 loss: 9.63264e-07
Iter: 55 loss: 9.67753635e-07
Iter: 56 loss: 9.58683131e-07
Iter: 57 loss: 9.54099278e-07
Iter: 58 loss: 9.54957727e-07
Iter: 59 loss: 9.50693277e-07
Iter: 60 loss: 9.48243724e-07
Iter: 61 loss: 9.47802e-07
Iter: 62 loss: 9.45082093e-07
Iter: 63 loss: 9.45353747e-07
Iter: 64 loss: 9.42954159e-07
Iter: 65 loss: 9.40503e-07
Iter: 66 loss: 9.42966608e-07
Iter: 67 loss: 9.3917032e-07
Iter: 68 loss: 9.36096512e-07
Iter: 69 loss: 9.37721893e-07
Iter: 70 loss: 9.34043385e-07
Iter: 71 loss: 9.29588225e-07
Iter: 72 loss: 9.36761239e-07
Iter: 73 loss: 9.27521683e-07
Iter: 74 loss: 9.24214589e-07
Iter: 75 loss: 9.24223173e-07
Iter: 76 loss: 9.20520108e-07
Iter: 77 loss: 9.28258203e-07
Iter: 78 loss: 9.19098227e-07
Iter: 79 loss: 9.17274349e-07
Iter: 80 loss: 9.15769249e-07
Iter: 81 loss: 9.15257942e-07
Iter: 82 loss: 9.12201699e-07
Iter: 83 loss: 9.51308266e-07
Iter: 84 loss: 9.12179246e-07
Iter: 85 loss: 9.10803692e-07
Iter: 86 loss: 9.100836e-07
Iter: 87 loss: 9.09432174e-07
Iter: 88 loss: 9.07177707e-07
Iter: 89 loss: 9.22794584e-07
Iter: 90 loss: 9.0697722e-07
Iter: 91 loss: 9.05090928e-07
Iter: 92 loss: 9.0153344e-07
Iter: 93 loss: 9.76295155e-07
Iter: 94 loss: 9.01504677e-07
Iter: 95 loss: 8.98078383e-07
Iter: 96 loss: 9.17606712e-07
Iter: 97 loss: 8.97591e-07
Iter: 98 loss: 8.94697e-07
Iter: 99 loss: 9.37337632e-07
Iter: 100 loss: 8.94680682e-07
Iter: 101 loss: 8.93543415e-07
Iter: 102 loss: 8.90849265e-07
Iter: 103 loss: 9.20650905e-07
Iter: 104 loss: 8.90565559e-07
Iter: 105 loss: 8.8899958e-07
Iter: 106 loss: 8.88969225e-07
Iter: 107 loss: 8.87880503e-07
Iter: 108 loss: 8.87587817e-07
Iter: 109 loss: 8.86916951e-07
Iter: 110 loss: 8.85669294e-07
Iter: 111 loss: 8.99184329e-07
Iter: 112 loss: 8.85642919e-07
Iter: 113 loss: 8.85109046e-07
Iter: 114 loss: 8.85068687e-07
Iter: 115 loss: 8.84614224e-07
Iter: 116 loss: 8.83241796e-07
Iter: 117 loss: 8.85777922e-07
Iter: 118 loss: 8.82349e-07
Iter: 119 loss: 8.8178416e-07
Iter: 120 loss: 8.8140763e-07
Iter: 121 loss: 8.80421908e-07
Iter: 122 loss: 8.79765196e-07
Iter: 123 loss: 8.7939685e-07
Iter: 124 loss: 8.78175e-07
Iter: 125 loss: 8.8497319e-07
Iter: 126 loss: 8.78003277e-07
Iter: 127 loss: 8.76744366e-07
Iter: 128 loss: 8.79324318e-07
Iter: 129 loss: 8.76249487e-07
Iter: 130 loss: 8.75310434e-07
Iter: 131 loss: 8.73980525e-07
Iter: 132 loss: 8.73951876e-07
Iter: 133 loss: 8.73367128e-07
Iter: 134 loss: 8.73066256e-07
Iter: 135 loss: 8.72184444e-07
Iter: 136 loss: 8.70418432e-07
Iter: 137 loss: 9.02885688e-07
Iter: 138 loss: 8.7038336e-07
Iter: 139 loss: 8.68725465e-07
Iter: 140 loss: 8.69348298e-07
Iter: 141 loss: 8.67552046e-07
Iter: 142 loss: 8.65892332e-07
Iter: 143 loss: 8.81092831e-07
Iter: 144 loss: 8.65786944e-07
Iter: 145 loss: 8.64818389e-07
Iter: 146 loss: 8.72926762e-07
Iter: 147 loss: 8.64763e-07
Iter: 148 loss: 8.6412615e-07
Iter: 149 loss: 8.64536844e-07
Iter: 150 loss: 8.63717673e-07
Iter: 151 loss: 8.63212449e-07
Iter: 152 loss: 8.63134915e-07
Iter: 153 loss: 8.62797833e-07
Iter: 154 loss: 8.621397e-07
Iter: 155 loss: 8.77681657e-07
Iter: 156 loss: 8.62138279e-07
Iter: 157 loss: 8.61456385e-07
Iter: 158 loss: 8.63107971e-07
Iter: 159 loss: 8.6123822e-07
Iter: 160 loss: 8.60239652e-07
Iter: 161 loss: 8.63950675e-07
Iter: 162 loss: 8.59968225e-07
Iter: 163 loss: 8.59440377e-07
Iter: 164 loss: 8.590672e-07
Iter: 165 loss: 8.5887973e-07
Iter: 166 loss: 8.57448413e-07
Iter: 167 loss: 8.59898364e-07
Iter: 168 loss: 8.56863721e-07
Iter: 169 loss: 8.55896246e-07
Iter: 170 loss: 8.55774033e-07
Iter: 171 loss: 8.55126132e-07
Iter: 172 loss: 8.54008078e-07
Iter: 173 loss: 8.66100038e-07
Iter: 174 loss: 8.5399256e-07
Iter: 175 loss: 8.52792482e-07
Iter: 176 loss: 8.54743746e-07
Iter: 177 loss: 8.52243375e-07
Iter: 178 loss: 8.51610821e-07
Iter: 179 loss: 8.50615606e-07
Iter: 180 loss: 8.50606739e-07
Iter: 181 loss: 8.49489538e-07
Iter: 182 loss: 8.5269545e-07
Iter: 183 loss: 8.49122955e-07
Iter: 184 loss: 8.48112848e-07
Iter: 185 loss: 8.56262488e-07
Iter: 186 loss: 8.48030197e-07
Iter: 187 loss: 8.47349668e-07
Iter: 188 loss: 8.50237939e-07
Iter: 189 loss: 8.47198748e-07
Iter: 190 loss: 8.46618263e-07
Iter: 191 loss: 8.55945359e-07
Iter: 192 loss: 8.46630144e-07
Iter: 193 loss: 8.4613265e-07
Iter: 194 loss: 8.45821774e-07
Iter: 195 loss: 8.45656928e-07
Iter: 196 loss: 8.45049726e-07
Iter: 197 loss: 8.44607598e-07
Iter: 198 loss: 8.44398642e-07
Iter: 199 loss: 8.4367673e-07
Iter: 200 loss: 8.4362e-07
Iter: 201 loss: 8.43250746e-07
Iter: 202 loss: 8.42464715e-07
Iter: 203 loss: 8.5726964e-07
Iter: 204 loss: 8.42453801e-07
Iter: 205 loss: 8.41706765e-07
Iter: 206 loss: 8.41706878e-07
Iter: 207 loss: 8.41135488e-07
Iter: 208 loss: 8.40259418e-07
Iter: 209 loss: 8.40248845e-07
Iter: 210 loss: 8.39491406e-07
Iter: 211 loss: 8.44182239e-07
Iter: 212 loss: 8.39384597e-07
Iter: 213 loss: 8.38573328e-07
Iter: 214 loss: 8.43321914e-07
Iter: 215 loss: 8.3847118e-07
Iter: 216 loss: 8.38004098e-07
Iter: 217 loss: 8.36914751e-07
Iter: 218 loss: 8.51928121e-07
Iter: 219 loss: 8.36833351e-07
Iter: 220 loss: 8.3572121e-07
Iter: 221 loss: 8.40432904e-07
Iter: 222 loss: 8.35482183e-07
Iter: 223 loss: 8.34520222e-07
Iter: 224 loss: 8.41150552e-07
Iter: 225 loss: 8.34421257e-07
Iter: 226 loss: 8.33635625e-07
Iter: 227 loss: 8.43692874e-07
Iter: 228 loss: 8.33625393e-07
Iter: 229 loss: 8.3323016e-07
Iter: 230 loss: 8.33016031e-07
Iter: 231 loss: 8.32848968e-07
Iter: 232 loss: 8.32352725e-07
Iter: 233 loss: 8.33742e-07
Iter: 234 loss: 8.32239436e-07
Iter: 235 loss: 8.31696184e-07
Iter: 236 loss: 8.36061133e-07
Iter: 237 loss: 8.31652301e-07
Iter: 238 loss: 8.3133591e-07
Iter: 239 loss: 8.30798683e-07
Iter: 240 loss: 8.30783222e-07
Iter: 241 loss: 8.30281692e-07
Iter: 242 loss: 8.30280896e-07
Iter: 243 loss: 8.29852809e-07
Iter: 244 loss: 8.28999362e-07
Iter: 245 loss: 8.47355409e-07
Iter: 246 loss: 8.29003341e-07
Iter: 247 loss: 8.28114253e-07
Iter: 248 loss: 8.31205966e-07
Iter: 249 loss: 8.27870906e-07
Iter: 250 loss: 8.27079248e-07
Iter: 251 loss: 8.39290578e-07
Iter: 252 loss: 8.27078281e-07
Iter: 253 loss: 8.26634391e-07
Iter: 254 loss: 8.25840459e-07
Iter: 255 loss: 8.45321495e-07
Iter: 256 loss: 8.25835798e-07
Iter: 257 loss: 8.24997301e-07
Iter: 258 loss: 8.25539e-07
Iter: 259 loss: 8.24477297e-07
Iter: 260 loss: 8.23826554e-07
Iter: 261 loss: 8.29085252e-07
Iter: 262 loss: 8.23793e-07
Iter: 263 loss: 8.23324513e-07
Iter: 264 loss: 8.23318487e-07
Iter: 265 loss: 8.2303626e-07
Iter: 266 loss: 8.22423601e-07
Iter: 267 loss: 8.28903524e-07
Iter: 268 loss: 8.2238364e-07
Iter: 269 loss: 8.21791559e-07
Iter: 270 loss: 8.2711324e-07
Iter: 271 loss: 8.21753e-07
Iter: 272 loss: 8.21271385e-07
Iter: 273 loss: 8.25322218e-07
Iter: 274 loss: 8.21245351e-07
Iter: 275 loss: 8.20999276e-07
Iter: 276 loss: 8.2109068e-07
Iter: 277 loss: 8.20814364e-07
Iter: 278 loss: 8.20515197e-07
Iter: 279 loss: 8.21900301e-07
Iter: 280 loss: 8.20493312e-07
Iter: 281 loss: 8.20151115e-07
Iter: 282 loss: 8.2075519e-07
Iter: 283 loss: 8.20058915e-07
Iter: 284 loss: 8.19761794e-07
Iter: 285 loss: 8.19194497e-07
Iter: 286 loss: 8.19214506e-07
Iter: 287 loss: 8.19013735e-07
Iter: 288 loss: 8.1891335e-07
Iter: 289 loss: 8.18578769e-07
Iter: 290 loss: 8.17841737e-07
Iter: 291 loss: 8.24795052e-07
Iter: 292 loss: 8.17692353e-07
Iter: 293 loss: 8.16774786e-07
Iter: 294 loss: 8.19630202e-07
Iter: 295 loss: 8.16564238e-07
Iter: 296 loss: 8.15775365e-07
Iter: 297 loss: 8.16136094e-07
Iter: 298 loss: 8.15231886e-07
Iter: 299 loss: 8.15809e-07
Iter: 300 loss: 8.14909185e-07
Iter: 301 loss: 8.14688462e-07
Iter: 302 loss: 8.14076884e-07
Iter: 303 loss: 8.19351271e-07
Iter: 304 loss: 8.14006171e-07
Iter: 305 loss: 8.13316547e-07
Iter: 306 loss: 8.13974225e-07
Iter: 307 loss: 8.12973326e-07
Iter: 308 loss: 8.12635676e-07
Iter: 309 loss: 8.12584403e-07
Iter: 310 loss: 8.12187864e-07
Iter: 311 loss: 8.12257326e-07
Iter: 312 loss: 8.11912173e-07
Iter: 313 loss: 8.11644554e-07
Iter: 314 loss: 8.11738801e-07
Iter: 315 loss: 8.11422296e-07
Iter: 316 loss: 8.10930715e-07
Iter: 317 loss: 8.13304837e-07
Iter: 318 loss: 8.10855795e-07
Iter: 319 loss: 8.1054111e-07
Iter: 320 loss: 8.11163375e-07
Iter: 321 loss: 8.10441861e-07
Iter: 322 loss: 8.10111828e-07
Iter: 323 loss: 8.09648839e-07
Iter: 324 loss: 8.09649123e-07
Iter: 325 loss: 8.09041694e-07
Iter: 326 loss: 8.13319843e-07
Iter: 327 loss: 8.09002074e-07
Iter: 328 loss: 8.08628556e-07
Iter: 329 loss: 8.08617187e-07
Iter: 330 loss: 8.08252e-07
Iter: 331 loss: 8.07414722e-07
Iter: 332 loss: 8.17474586e-07
Iter: 333 loss: 8.07359811e-07
Iter: 334 loss: 8.06680646e-07
Iter: 335 loss: 8.08888103e-07
Iter: 336 loss: 8.06471576e-07
Iter: 337 loss: 8.06080038e-07
Iter: 338 loss: 8.06073388e-07
Iter: 339 loss: 8.05618e-07
Iter: 340 loss: 8.05628815e-07
Iter: 341 loss: 8.05218406e-07
Iter: 342 loss: 8.04897923e-07
Iter: 343 loss: 8.04633373e-07
Iter: 344 loss: 8.04530032e-07
Iter: 345 loss: 8.04218587e-07
Iter: 346 loss: 8.04208696e-07
Iter: 347 loss: 8.03849844e-07
Iter: 348 loss: 8.03509351e-07
Iter: 349 loss: 8.03430908e-07
Iter: 350 loss: 8.03036755e-07
Iter: 351 loss: 8.03044941e-07
Iter: 352 loss: 8.0271252e-07
Iter: 353 loss: 8.02267e-07
Iter: 354 loss: 8.02260388e-07
Iter: 355 loss: 8.01921772e-07
Iter: 356 loss: 8.01351575e-07
Iter: 357 loss: 8.0139182e-07
Iter: 358 loss: 8.0097584e-07
Iter: 359 loss: 8.05510808e-07
Iter: 360 loss: 8.00984196e-07
Iter: 361 loss: 8.00749376e-07
Iter: 362 loss: 8.00898817e-07
Iter: 363 loss: 8.00574753e-07
Iter: 364 loss: 8.00330781e-07
Iter: 365 loss: 8.00328053e-07
Iter: 366 loss: 8.00118642e-07
Iter: 367 loss: 7.99859265e-07
Iter: 368 loss: 7.99876261e-07
Iter: 369 loss: 7.99472275e-07
Iter: 370 loss: 7.99037e-07
Iter: 371 loss: 7.98979499e-07
Iter: 372 loss: 7.99095915e-07
Iter: 373 loss: 7.98712335e-07
Iter: 374 loss: 7.9848428e-07
Iter: 375 loss: 7.97878101e-07
Iter: 376 loss: 8.01192527e-07
Iter: 377 loss: 7.97691655e-07
Iter: 378 loss: 7.97281359e-07
Iter: 379 loss: 7.97250777e-07
Iter: 380 loss: 7.96853158e-07
Iter: 381 loss: 7.97885605e-07
Iter: 382 loss: 7.96717814e-07
Iter: 383 loss: 7.96453094e-07
Iter: 384 loss: 7.96135623e-07
Iter: 385 loss: 7.96129768e-07
Iter: 386 loss: 7.95870733e-07
Iter: 387 loss: 7.95875394e-07
Iter: 388 loss: 7.95623066e-07
Iter: 389 loss: 7.95502501e-07
Iter: 390 loss: 7.95382277e-07
Iter: 391 loss: 7.95071571e-07
Iter: 392 loss: 7.94741254e-07
Iter: 393 loss: 7.94701236e-07
Iter: 394 loss: 7.94078233e-07
Iter: 395 loss: 7.96889367e-07
Iter: 396 loss: 7.93967558e-07
Iter: 397 loss: 7.93800609e-07
Iter: 398 loss: 7.93700963e-07
Iter: 399 loss: 7.93522872e-07
Iter: 400 loss: 7.93150207e-07
Iter: 401 loss: 7.98835572e-07
Iter: 402 loss: 7.9315555e-07
Iter: 403 loss: 7.9274821e-07
Iter: 404 loss: 7.94842094e-07
Iter: 405 loss: 7.92663855e-07
Iter: 406 loss: 7.92446713e-07
Iter: 407 loss: 7.92466949e-07
Iter: 408 loss: 7.922539e-07
Iter: 409 loss: 7.92169658e-07
Iter: 410 loss: 7.92111337e-07
Iter: 411 loss: 7.91963089e-07
Iter: 412 loss: 7.91655509e-07
Iter: 413 loss: 7.9805875e-07
Iter: 414 loss: 7.91662728e-07
Iter: 415 loss: 7.91388516e-07
Iter: 416 loss: 7.95348114e-07
Iter: 417 loss: 7.91384309e-07
Iter: 418 loss: 7.91116349e-07
Iter: 419 loss: 7.91000105e-07
Iter: 420 loss: 7.90918648e-07
Iter: 421 loss: 7.90616809e-07
Iter: 422 loss: 7.91503851e-07
Iter: 423 loss: 7.90519039e-07
Iter: 424 loss: 7.90159049e-07
Iter: 425 loss: 7.91144089e-07
Iter: 426 loss: 7.90030185e-07
Iter: 427 loss: 7.89837429e-07
Iter: 428 loss: 7.89450723e-07
Iter: 429 loss: 7.89461296e-07
Iter: 430 loss: 7.89204137e-07
Iter: 431 loss: 7.89200556e-07
Iter: 432 loss: 7.88970965e-07
Iter: 433 loss: 7.90074409e-07
Iter: 434 loss: 7.88915202e-07
Iter: 435 loss: 7.88718637e-07
Iter: 436 loss: 7.88222337e-07
Iter: 437 loss: 7.94106256e-07
Iter: 438 loss: 7.88186071e-07
Iter: 439 loss: 7.87685508e-07
Iter: 440 loss: 7.9060942e-07
Iter: 441 loss: 7.87607178e-07
Iter: 442 loss: 7.87103659e-07
Iter: 443 loss: 7.88476939e-07
Iter: 444 loss: 7.86953194e-07
Iter: 445 loss: 7.86611736e-07
Iter: 446 loss: 7.87895374e-07
Iter: 447 loss: 7.86503961e-07
Iter: 448 loss: 7.86357418e-07
Iter: 449 loss: 7.86305293e-07
Iter: 450 loss: 7.86191549e-07
Iter: 451 loss: 7.85945076e-07
Iter: 452 loss: 7.88654347e-07
Iter: 453 loss: 7.85943598e-07
Iter: 454 loss: 7.85846964e-07
Iter: 455 loss: 7.85830593e-07
Iter: 456 loss: 7.85712302e-07
Iter: 457 loss: 7.85503914e-07
Iter: 458 loss: 7.89663659e-07
Iter: 459 loss: 7.85526595e-07
Iter: 460 loss: 7.85285067e-07
Iter: 461 loss: 7.8526682e-07
Iter: 462 loss: 7.85094358e-07
Iter: 463 loss: 7.84769e-07
Iter: 464 loss: 7.853331e-07
Iter: 465 loss: 7.84622841e-07
Iter: 466 loss: 7.8410244e-07
Iter: 467 loss: 7.88177317e-07
Iter: 468 loss: 7.84103e-07
Iter: 469 loss: 7.83809526e-07
Iter: 470 loss: 7.83240353e-07
Iter: 471 loss: 7.96004088e-07
Iter: 472 loss: 7.83235407e-07
Iter: 473 loss: 7.82826874e-07
Iter: 474 loss: 7.82821076e-07
Iter: 475 loss: 7.82507414e-07
Iter: 476 loss: 7.85018074e-07
Iter: 477 loss: 7.8248587e-07
Iter: 478 loss: 7.82271457e-07
Iter: 479 loss: 7.82022653e-07
Iter: 480 loss: 7.82003042e-07
Iter: 481 loss: 7.81699214e-07
Iter: 482 loss: 7.8256943e-07
Iter: 483 loss: 7.81601159e-07
Iter: 484 loss: 7.81230881e-07
Iter: 485 loss: 7.81479912e-07
Iter: 486 loss: 7.80969572e-07
Iter: 487 loss: 7.80695245e-07
Iter: 488 loss: 7.83245468e-07
Iter: 489 loss: 7.80672735e-07
Iter: 490 loss: 7.80313144e-07
Iter: 491 loss: 7.81157041e-07
Iter: 492 loss: 7.80169785e-07
Iter: 493 loss: 7.7998061e-07
Iter: 494 loss: 7.79973902e-07
Iter: 495 loss: 7.79793e-07
Iter: 496 loss: 7.79649781e-07
Iter: 497 loss: 7.79623861e-07
Iter: 498 loss: 7.79479365e-07
Iter: 499 loss: 7.79164793e-07
Iter: 500 loss: 7.84936e-07
Iter: 501 loss: 7.7918e-07
Iter: 502 loss: 7.78955155e-07
Iter: 503 loss: 7.81127483e-07
Iter: 504 loss: 7.78937135e-07
Iter: 505 loss: 7.78712433e-07
Iter: 506 loss: 7.79240054e-07
Iter: 507 loss: 7.786374e-07
Iter: 508 loss: 7.78413153e-07
Iter: 509 loss: 7.78119841e-07
Iter: 510 loss: 7.78108188e-07
Iter: 511 loss: 7.77990067e-07
Iter: 512 loss: 7.77908554e-07
Iter: 513 loss: 7.77733646e-07
Iter: 514 loss: 7.77296805e-07
Iter: 515 loss: 7.82219161e-07
Iter: 516 loss: 7.7725764e-07
Iter: 517 loss: 7.76809e-07
Iter: 518 loss: 7.7796426e-07
Iter: 519 loss: 7.7665095e-07
Iter: 520 loss: 7.76263505e-07
Iter: 521 loss: 7.78724257e-07
Iter: 522 loss: 7.7622667e-07
Iter: 523 loss: 7.75952401e-07
Iter: 524 loss: 7.77475066e-07
Iter: 525 loss: 7.75943306e-07
Iter: 526 loss: 7.75717638e-07
Iter: 527 loss: 7.780788e-07
Iter: 528 loss: 7.75709964e-07
Iter: 529 loss: 7.7556939e-07
Iter: 530 loss: 7.75275453e-07
Iter: 531 loss: 7.79634888e-07
Iter: 532 loss: 7.75257718e-07
Iter: 533 loss: 7.75129e-07
Iter: 534 loss: 7.75074056e-07
Iter: 535 loss: 7.74921432e-07
Iter: 536 loss: 7.74606292e-07
Iter: 537 loss: 7.78774734e-07
Iter: 538 loss: 7.74597083e-07
Iter: 539 loss: 7.74259604e-07
Iter: 540 loss: 7.74810303e-07
Iter: 541 loss: 7.74102716e-07
Iter: 542 loss: 7.73833222e-07
Iter: 543 loss: 7.77567834e-07
Iter: 544 loss: 7.73829242e-07
Iter: 545 loss: 7.73578506e-07
Iter: 546 loss: 7.74343562e-07
Iter: 547 loss: 7.73483293e-07
Iter: 548 loss: 7.73345505e-07
Iter: 549 loss: 7.73238128e-07
Iter: 550 loss: 7.73177703e-07
Iter: 551 loss: 7.72893088e-07
Iter: 552 loss: 7.76071829e-07
Iter: 553 loss: 7.72902808e-07
Iter: 554 loss: 7.72764e-07
Iter: 555 loss: 7.72482508e-07
Iter: 556 loss: 7.72465967e-07
Iter: 557 loss: 7.72147303e-07
Iter: 558 loss: 7.71764917e-07
Iter: 559 loss: 7.71700627e-07
Iter: 560 loss: 7.71289933e-07
Iter: 561 loss: 7.77762807e-07
Iter: 562 loss: 7.71277769e-07
Iter: 563 loss: 7.70942222e-07
Iter: 564 loss: 7.75763169e-07
Iter: 565 loss: 7.70954841e-07
Iter: 566 loss: 7.70840472e-07
Iter: 567 loss: 7.70569045e-07
Iter: 568 loss: 7.75118792e-07
Iter: 569 loss: 7.70572342e-07
Iter: 570 loss: 7.70422218e-07
Iter: 571 loss: 7.70384759e-07
Iter: 572 loss: 7.70232646e-07
Iter: 573 loss: 7.700271e-07
Iter: 574 loss: 7.70023462e-07
Iter: 575 loss: 7.69785572e-07
Iter: 576 loss: 7.70175575e-07
Iter: 577 loss: 7.69665292e-07
Iter: 578 loss: 7.69407052e-07
Iter: 579 loss: 7.69994e-07
Iter: 580 loss: 7.69300186e-07
Iter: 581 loss: 7.69083897e-07
Iter: 582 loss: 7.72412818e-07
Iter: 583 loss: 7.69076962e-07
Iter: 584 loss: 7.68892619e-07
Iter: 585 loss: 7.68615337e-07
Iter: 586 loss: 7.6862e-07
Iter: 587 loss: 7.68301391e-07
Iter: 588 loss: 7.68350731e-07
Iter: 589 loss: 7.6804082e-07
Iter: 590 loss: 7.67912752e-07
Iter: 591 loss: 7.67845847e-07
Iter: 592 loss: 7.6765059e-07
Iter: 593 loss: 7.67406846e-07
Iter: 594 loss: 7.67396841e-07
Iter: 595 loss: 7.67126494e-07
Iter: 596 loss: 7.6723677e-07
Iter: 597 loss: 7.66953235e-07
Iter: 598 loss: 7.66631729e-07
Iter: 599 loss: 7.68530413e-07
Iter: 600 loss: 7.6660649e-07
Iter: 601 loss: 7.66341941e-07
Iter: 602 loss: 7.70209681e-07
Iter: 603 loss: 7.66335461e-07
Iter: 604 loss: 7.66202618e-07
Iter: 605 loss: 7.65908851e-07
Iter: 606 loss: 7.68910922e-07
Iter: 607 loss: 7.65886739e-07
Iter: 608 loss: 7.65817276e-07
Iter: 609 loss: 7.6572519e-07
Iter: 610 loss: 7.65609116e-07
Iter: 611 loss: 7.65444724e-07
Iter: 612 loss: 7.65426307e-07
Iter: 613 loss: 7.65223263e-07
Iter: 614 loss: 7.65796699e-07
Iter: 615 loss: 7.65169716e-07
Iter: 616 loss: 7.64980882e-07
Iter: 617 loss: 7.66623714e-07
Iter: 618 loss: 7.6495553e-07
Iter: 619 loss: 7.64823085e-07
Iter: 620 loss: 7.6478625e-07
Iter: 621 loss: 7.64729407e-07
Iter: 622 loss: 7.64474805e-07
Iter: 623 loss: 7.64445929e-07
Iter: 624 loss: 7.64270794e-07
Iter: 625 loss: 7.6394349e-07
Iter: 626 loss: 7.64518518e-07
Iter: 627 loss: 7.6383958e-07
Iter: 628 loss: 7.63622438e-07
Iter: 629 loss: 7.6359e-07
Iter: 630 loss: 7.63419678e-07
Iter: 631 loss: 7.63121875e-07
Iter: 632 loss: 7.67195786e-07
Iter: 633 loss: 7.63099195e-07
Iter: 634 loss: 7.62845332e-07
Iter: 635 loss: 7.66091944e-07
Iter: 636 loss: 7.62850448e-07
Iter: 637 loss: 7.62640411e-07
Iter: 638 loss: 7.64443939e-07
Iter: 639 loss: 7.62648881e-07
Iter: 640 loss: 7.62551736e-07
Iter: 641 loss: 7.62275818e-07
Iter: 642 loss: 7.64082927e-07
Iter: 643 loss: 7.62214142e-07
Iter: 644 loss: 7.62233924e-07
Iter: 645 loss: 7.62082095e-07
Iter: 646 loss: 7.61979e-07
Iter: 647 loss: 7.61792535e-07
Iter: 648 loss: 7.66016e-07
Iter: 649 loss: 7.61793842e-07
Iter: 650 loss: 7.61620299e-07
Iter: 651 loss: 7.61839829e-07
Iter: 652 loss: 7.61511728e-07
Iter: 653 loss: 7.61417709e-07
Iter: 654 loss: 7.61377578e-07
Iter: 655 loss: 7.61271e-07
Iter: 656 loss: 7.61043509e-07
Iter: 657 loss: 7.65215759e-07
Iter: 658 loss: 7.61053911e-07
Iter: 659 loss: 7.60838475e-07
Iter: 660 loss: 7.6157221e-07
Iter: 661 loss: 7.60768103e-07
Iter: 662 loss: 7.60491957e-07
Iter: 663 loss: 7.6088736e-07
Iter: 664 loss: 7.6041465e-07
Iter: 665 loss: 7.60274e-07
Iter: 666 loss: 7.60280216e-07
Iter: 667 loss: 7.60137e-07
Iter: 668 loss: 7.59851162e-07
Iter: 669 loss: 7.63817e-07
Iter: 670 loss: 7.5985497e-07
Iter: 671 loss: 7.59599e-07
Iter: 672 loss: 7.60107923e-07
Iter: 673 loss: 7.5948094e-07
Iter: 674 loss: 7.59381e-07
Iter: 675 loss: 7.59339912e-07
Iter: 676 loss: 7.59204227e-07
Iter: 677 loss: 7.5895e-07
Iter: 678 loss: 7.58973556e-07
Iter: 679 loss: 7.58759711e-07
Iter: 680 loss: 7.58964575e-07
Iter: 681 loss: 7.58639828e-07
Iter: 682 loss: 7.58504427e-07
Iter: 683 loss: 7.58476745e-07
Iter: 684 loss: 7.58384658e-07
Iter: 685 loss: 7.58089868e-07
Iter: 686 loss: 7.6077356e-07
Iter: 687 loss: 7.58052465e-07
Iter: 688 loss: 7.57800876e-07
Iter: 689 loss: 7.59429099e-07
Iter: 690 loss: 7.57765065e-07
Iter: 691 loss: 7.57577482e-07
Iter: 692 loss: 7.59698423e-07
Iter: 693 loss: 7.57577368e-07
Iter: 694 loss: 7.57412636e-07
Iter: 695 loss: 7.57229373e-07
Iter: 696 loss: 7.57198e-07
Iter: 697 loss: 7.56973236e-07
Iter: 698 loss: 7.58056331e-07
Iter: 699 loss: 7.56912186e-07
Iter: 700 loss: 7.56746886e-07
Iter: 701 loss: 7.58999136e-07
Iter: 702 loss: 7.56739837e-07
Iter: 703 loss: 7.5661012e-07
Iter: 704 loss: 7.56597785e-07
Iter: 705 loss: 7.56498366e-07
Iter: 706 loss: 7.56330905e-07
Iter: 707 loss: 7.56077725e-07
Iter: 708 loss: 7.56067777e-07
Iter: 709 loss: 7.5584677e-07
Iter: 710 loss: 7.56804639e-07
Iter: 711 loss: 7.55795213e-07
Iter: 712 loss: 7.5560007e-07
Iter: 713 loss: 7.55581368e-07
Iter: 714 loss: 7.55470239e-07
Iter: 715 loss: 7.55198755e-07
Iter: 716 loss: 7.59248962e-07
Iter: 717 loss: 7.55190399e-07
Iter: 718 loss: 7.54980931e-07
Iter: 719 loss: 7.5558188e-07
Iter: 720 loss: 7.54873554e-07
Iter: 721 loss: 7.54671873e-07
Iter: 722 loss: 7.54660675e-07
Iter: 723 loss: 7.54551309e-07
Iter: 724 loss: 7.54246628e-07
Iter: 725 loss: 7.5741184e-07
Iter: 726 loss: 7.54240773e-07
Iter: 727 loss: 7.53900736e-07
Iter: 728 loss: 7.55347969e-07
Iter: 729 loss: 7.53865947e-07
Iter: 730 loss: 7.53637096e-07
Iter: 731 loss: 7.55855467e-07
Iter: 732 loss: 7.53610152e-07
Iter: 733 loss: 7.53371182e-07
Iter: 734 loss: 7.53683594e-07
Iter: 735 loss: 7.53273071e-07
Iter: 736 loss: 7.53053e-07
Iter: 737 loss: 7.53500785e-07
Iter: 738 loss: 7.52986125e-07
Iter: 739 loss: 7.52767562e-07
Iter: 740 loss: 7.54108441e-07
Iter: 741 loss: 7.52767505e-07
Iter: 742 loss: 7.52535925e-07
Iter: 743 loss: 7.52649612e-07
Iter: 744 loss: 7.52449637e-07
Iter: 745 loss: 7.52260348e-07
Iter: 746 loss: 7.52046333e-07
Iter: 747 loss: 7.52021094e-07
Iter: 748 loss: 7.51884443e-07
Iter: 749 loss: 7.51838e-07
Iter: 750 loss: 7.51619496e-07
Iter: 751 loss: 7.51579933e-07
Iter: 752 loss: 7.5147193e-07
Iter: 753 loss: 7.51281277e-07
Iter: 754 loss: 7.50942263e-07
Iter: 755 loss: 7.59244699e-07
Iter: 756 loss: 7.5094465e-07
Iter: 757 loss: 7.50804475e-07
Iter: 758 loss: 7.50723075e-07
Iter: 759 loss: 7.5050923e-07
Iter: 760 loss: 7.50380082e-07
Iter: 761 loss: 7.50332561e-07
Iter: 762 loss: 7.50105926e-07
Iter: 763 loss: 7.49916921e-07
Iter: 764 loss: 7.4983808e-07
Iter: 765 loss: 7.49613832e-07
Iter: 766 loss: 7.51177936e-07
Iter: 767 loss: 7.49567789e-07
Iter: 768 loss: 7.49447565e-07
Iter: 769 loss: 7.49441938e-07
Iter: 770 loss: 7.49335868e-07
Iter: 771 loss: 7.49262426e-07
Iter: 772 loss: 7.49219396e-07
Iter: 773 loss: 7.4905563e-07
Iter: 774 loss: 7.49112417e-07
Iter: 775 loss: 7.48977243e-07
Iter: 776 loss: 7.487223e-07
Iter: 777 loss: 7.49799483e-07
Iter: 778 loss: 7.4870411e-07
Iter: 779 loss: 7.48505e-07
Iter: 780 loss: 7.49916524e-07
Iter: 781 loss: 7.48475031e-07
Iter: 782 loss: 7.48298078e-07
Iter: 783 loss: 7.47944739e-07
Iter: 784 loss: 7.5647165e-07
Iter: 785 loss: 7.47962758e-07
Iter: 786 loss: 7.47782963e-07
Iter: 787 loss: 7.47761305e-07
Iter: 788 loss: 7.47555e-07
Iter: 789 loss: 7.47625336e-07
Iter: 790 loss: 7.47406148e-07
Iter: 791 loss: 7.4726745e-07
Iter: 792 loss: 7.47025751e-07
Iter: 793 loss: 7.47013e-07
Iter: 794 loss: 7.46875e-07
Iter: 795 loss: 7.46844705e-07
Iter: 796 loss: 7.46684e-07
Iter: 797 loss: 7.4654821e-07
Iter: 798 loss: 7.46504725e-07
Iter: 799 loss: 7.46324417e-07
Iter: 800 loss: 7.46531896e-07
Iter: 801 loss: 7.46222099e-07
Iter: 802 loss: 7.46052137e-07
Iter: 803 loss: 7.48411e-07
Iter: 804 loss: 7.46037699e-07
Iter: 805 loss: 7.4593305e-07
Iter: 806 loss: 7.45983812e-07
Iter: 807 loss: 7.45889508e-07
Iter: 808 loss: 7.45756552e-07
Iter: 809 loss: 7.4576667e-07
Iter: 810 loss: 7.45663669e-07
Iter: 811 loss: 7.45511556e-07
Iter: 812 loss: 7.45528382e-07
Iter: 813 loss: 7.45408443e-07
Iter: 814 loss: 7.45700845e-07
Iter: 815 loss: 7.45383488e-07
Iter: 816 loss: 7.45231318e-07
Iter: 817 loss: 7.45020884e-07
Iter: 818 loss: 7.49936589e-07
Iter: 819 loss: 7.45026227e-07
Iter: 820 loss: 7.44941531e-07
Iter: 821 loss: 7.44875933e-07
Iter: 822 loss: 7.4473661e-07
Iter: 823 loss: 7.44497186e-07
Iter: 824 loss: 7.47929562e-07
Iter: 825 loss: 7.44473709e-07
Iter: 826 loss: 7.44149418e-07
Iter: 827 loss: 7.44534645e-07
Iter: 828 loss: 7.44010322e-07
Iter: 829 loss: 7.43811597e-07
Iter: 830 loss: 7.43805231e-07
Iter: 831 loss: 7.43605426e-07
Iter: 832 loss: 7.43929036e-07
Iter: 833 loss: 7.43566943e-07
Iter: 834 loss: 7.43422561e-07
Iter: 835 loss: 7.43200474e-07
Iter: 836 loss: 7.4319496e-07
Iter: 837 loss: 7.4305575e-07
Iter: 838 loss: 7.43044552e-07
Iter: 839 loss: 7.42889313e-07
Iter: 840 loss: 7.42974407e-07
Iter: 841 loss: 7.42783755e-07
Iter: 842 loss: 7.42604925e-07
Iter: 843 loss: 7.42416887e-07
Iter: 844 loss: 7.42399322e-07
Iter: 845 loss: 7.42225211e-07
Iter: 846 loss: 7.42236e-07
Iter: 847 loss: 7.42034899e-07
Iter: 848 loss: 7.42174507e-07
Iter: 849 loss: 7.41939118e-07
Iter: 850 loss: 7.4177683e-07
Iter: 851 loss: 7.41926e-07
Iter: 852 loss: 7.41721522e-07
Iter: 853 loss: 7.41540134e-07
Iter: 854 loss: 7.42929899e-07
Iter: 855 loss: 7.41551048e-07
Iter: 856 loss: 7.41409963e-07
Iter: 857 loss: 7.41515692e-07
Iter: 858 loss: 7.41309123e-07
Iter: 859 loss: 7.41163035e-07
Iter: 860 loss: 7.40979203e-07
Iter: 861 loss: 7.41001941e-07
Iter: 862 loss: 7.40690666e-07
Iter: 863 loss: 7.41241934e-07
Iter: 864 loss: 7.40569533e-07
Iter: 865 loss: 7.40422934e-07
Iter: 866 loss: 7.4039184e-07
Iter: 867 loss: 7.4025e-07
Iter: 868 loss: 7.39950508e-07
Iter: 869 loss: 7.42897896e-07
Iter: 870 loss: 7.39917709e-07
Iter: 871 loss: 7.39607e-07
Iter: 872 loss: 7.40366545e-07
Iter: 873 loss: 7.39483426e-07
Iter: 874 loss: 7.39238089e-07
Iter: 875 loss: 7.41004669e-07
Iter: 876 loss: 7.39220241e-07
Iter: 877 loss: 7.39071368e-07
Iter: 878 loss: 7.4115826e-07
Iter: 879 loss: 7.39065058e-07
Iter: 880 loss: 7.38896574e-07
Iter: 881 loss: 7.38820688e-07
Iter: 882 loss: 7.38748383e-07
Iter: 883 loss: 7.38598885e-07
Iter: 884 loss: 7.39157713e-07
Iter: 885 loss: 7.38556e-07
Iter: 886 loss: 7.38390668e-07
Iter: 887 loss: 7.39592167e-07
Iter: 888 loss: 7.38396466e-07
Iter: 889 loss: 7.3828528e-07
Iter: 890 loss: 7.38180802e-07
Iter: 891 loss: 7.3816085e-07
Iter: 892 loss: 7.38028575e-07
Iter: 893 loss: 7.38387598e-07
Iter: 894 loss: 7.37954792e-07
Iter: 895 loss: 7.37819846e-07
Iter: 896 loss: 7.37813139e-07
Iter: 897 loss: 7.37686435e-07
Iter: 898 loss: 7.37711105e-07
Iter: 899 loss: 7.37585708e-07
Iter: 900 loss: 7.37479581e-07
Iter: 901 loss: 7.37380788e-07
Iter: 902 loss: 7.37346113e-07
Iter: 903 loss: 7.37254254e-07
Iter: 904 loss: 7.3722731e-07
Iter: 905 loss: 7.3712954e-07
Iter: 906 loss: 7.36874e-07
Iter: 907 loss: 7.38986785e-07
Iter: 908 loss: 7.36815196e-07
Iter: 909 loss: 7.36532911e-07
Iter: 910 loss: 7.37173536e-07
Iter: 911 loss: 7.36455718e-07
Iter: 912 loss: 7.36158313e-07
Iter: 913 loss: 7.38839731e-07
Iter: 914 loss: 7.3618e-07
Iter: 915 loss: 7.3587853e-07
Iter: 916 loss: 7.36902621e-07
Iter: 917 loss: 7.35817366e-07
Iter: 918 loss: 7.35675371e-07
Iter: 919 loss: 7.35685489e-07
Iter: 920 loss: 7.35583967e-07
Iter: 921 loss: 7.35530648e-07
Iter: 922 loss: 7.35492165e-07
Iter: 923 loss: 7.35432707e-07
Iter: 924 loss: 7.35277922e-07
Iter: 925 loss: 7.36995901e-07
Iter: 926 loss: 7.3528156e-07
Iter: 927 loss: 7.35129e-07
Iter: 928 loss: 7.3606185e-07
Iter: 929 loss: 7.35120466e-07
Iter: 930 loss: 7.35036679e-07
Iter: 931 loss: 7.36162804e-07
Iter: 932 loss: 7.35040658e-07
Iter: 933 loss: 7.34909349e-07
Iter: 934 loss: 7.34668049e-07
Iter: 935 loss: 7.36050197e-07
Iter: 936 loss: 7.34589e-07
Iter: 937 loss: 7.34315961e-07
Iter: 938 loss: 7.36968218e-07
Iter: 939 loss: 7.3429635e-07
Iter: 940 loss: 7.3408728e-07
Iter: 941 loss: 7.35755066e-07
Iter: 942 loss: 7.34036e-07
Iter: 943 loss: 7.33918114e-07
Iter: 944 loss: 7.33663796e-07
Iter: 945 loss: 7.33661295e-07
Iter: 946 loss: 7.33453476e-07
Iter: 947 loss: 7.33703644e-07
Iter: 948 loss: 7.33370882e-07
Iter: 949 loss: 7.33275442e-07
Iter: 950 loss: 7.332373e-07
Iter: 951 loss: 7.33135039e-07
Iter: 952 loss: 7.33031129e-07
Iter: 953 loss: 7.33018169e-07
Iter: 954 loss: 7.32889873e-07
Iter: 955 loss: 7.33478771e-07
Iter: 956 loss: 7.32875606e-07
Iter: 957 loss: 7.32704393e-07
Iter: 958 loss: 7.33229399e-07
Iter: 959 loss: 7.32651245e-07
Iter: 960 loss: 7.32531817e-07
Iter: 961 loss: 7.32380897e-07
Iter: 962 loss: 7.32333831e-07
Iter: 963 loss: 7.32192518e-07
Iter: 964 loss: 7.32199055e-07
Iter: 965 loss: 7.32077638e-07
Iter: 966 loss: 7.3225749e-07
Iter: 967 loss: 7.31985097e-07
Iter: 968 loss: 7.31876753e-07
Iter: 969 loss: 7.31654154e-07
Iter: 970 loss: 7.31661203e-07
Iter: 971 loss: 7.31545697e-07
Iter: 972 loss: 7.31541718e-07
Iter: 973 loss: 7.31413536e-07
Iter: 974 loss: 7.31373e-07
Iter: 975 loss: 7.31288424e-07
Iter: 976 loss: 7.31115961e-07
Iter: 977 loss: 7.30959414e-07
Iter: 978 loss: 7.30923432e-07
Iter: 979 loss: 7.3069981e-07
Iter: 980 loss: 7.30891315e-07
Iter: 981 loss: 7.30557474e-07
Iter: 982 loss: 7.30416104e-07
Iter: 983 loss: 7.30420879e-07
Iter: 984 loss: 7.302325e-07
Iter: 985 loss: 7.30191687e-07
Iter: 986 loss: 7.30117563e-07
Iter: 987 loss: 7.29978069e-07
Iter: 988 loss: 7.30528825e-07
Iter: 989 loss: 7.29939813e-07
Iter: 990 loss: 7.29770363e-07
Iter: 991 loss: 7.30428042e-07
Iter: 992 loss: 7.29742169e-07
Iter: 993 loss: 7.29659064e-07
Iter: 994 loss: 7.29542194e-07
Iter: 995 loss: 7.29523435e-07
Iter: 996 loss: 7.2943908e-07
Iter: 997 loss: 7.29429246e-07
Iter: 998 loss: 7.29347335e-07
Iter: 999 loss: 7.2916896e-07
Iter: 1000 loss: 7.32148e-07
Iter: 1001 loss: 7.29184649e-07
Iter: 1002 loss: 7.28988425e-07
Iter: 1003 loss: 7.29628482e-07
Iter: 1004 loss: 7.28938232e-07
Iter: 1005 loss: 7.2878413e-07
Iter: 1006 loss: 7.3065064e-07
Iter: 1007 loss: 7.28785608e-07
Iter: 1008 loss: 7.28638383e-07
Iter: 1009 loss: 7.28410043e-07
Iter: 1010 loss: 7.28397481e-07
Iter: 1011 loss: 7.28149871e-07
Iter: 1012 loss: 7.28047667e-07
Iter: 1013 loss: 7.27873612e-07
Iter: 1014 loss: 7.2754375e-07
Iter: 1015 loss: 7.28835289e-07
Iter: 1016 loss: 7.2746252e-07
Iter: 1017 loss: 7.27245947e-07
Iter: 1018 loss: 7.27224574e-07
Iter: 1019 loss: 7.2698856e-07
Iter: 1020 loss: 7.27601e-07
Iter: 1021 loss: 7.26923645e-07
Iter: 1022 loss: 7.26796543e-07
Iter: 1023 loss: 7.26684789e-07
Iter: 1024 loss: 7.26648409e-07
Iter: 1025 loss: 7.26486064e-07
Iter: 1026 loss: 7.26465373e-07
Iter: 1027 loss: 7.26397502e-07
Iter: 1028 loss: 7.26237658e-07
Iter: 1029 loss: 7.28307839e-07
Iter: 1030 loss: 7.26241694e-07
Iter: 1031 loss: 7.26158e-07
Iter: 1032 loss: 7.26111068e-07
Iter: 1033 loss: 7.26058886e-07
Iter: 1034 loss: 7.259124e-07
Iter: 1035 loss: 7.2765e-07
Iter: 1036 loss: 7.25908933e-07
Iter: 1037 loss: 7.25711516e-07
Iter: 1038 loss: 7.2639466e-07
Iter: 1039 loss: 7.25684345e-07
Iter: 1040 loss: 7.25490281e-07
Iter: 1041 loss: 7.26599581e-07
Iter: 1042 loss: 7.25474933e-07
Iter: 1043 loss: 7.2535363e-07
Iter: 1044 loss: 7.25235054e-07
Iter: 1045 loss: 7.25191967e-07
Iter: 1046 loss: 7.25021323e-07
Iter: 1047 loss: 7.2506532e-07
Iter: 1048 loss: 7.24890299e-07
Iter: 1049 loss: 7.24665597e-07
Iter: 1050 loss: 7.25107498e-07
Iter: 1051 loss: 7.24570498e-07
Iter: 1052 loss: 7.24301344e-07
Iter: 1053 loss: 7.25093514e-07
Iter: 1054 loss: 7.24262122e-07
Iter: 1055 loss: 7.24027529e-07
Iter: 1056 loss: 7.27338943e-07
Iter: 1057 loss: 7.24038216e-07
Iter: 1058 loss: 7.23860126e-07
Iter: 1059 loss: 7.23692494e-07
Iter: 1060 loss: 7.23635708e-07
Iter: 1061 loss: 7.23422772e-07
Iter: 1062 loss: 7.26560074e-07
Iter: 1063 loss: 7.23419419e-07
Iter: 1064 loss: 7.23197502e-07
Iter: 1065 loss: 7.22963705e-07
Iter: 1066 loss: 7.22925961e-07
Iter: 1067 loss: 7.22714276e-07
Iter: 1068 loss: 7.24869835e-07
Iter: 1069 loss: 7.22708592e-07
Iter: 1070 loss: 7.22522486e-07
Iter: 1071 loss: 7.23315793e-07
Iter: 1072 loss: 7.22479115e-07
Iter: 1073 loss: 7.22390382e-07
Iter: 1074 loss: 7.22173866e-07
Iter: 1075 loss: 7.26341568e-07
Iter: 1076 loss: 7.22190748e-07
Iter: 1077 loss: 7.22101845e-07
Iter: 1078 loss: 7.22070126e-07
Iter: 1079 loss: 7.21983838e-07
Iter: 1080 loss: 7.2189323e-07
Iter: 1081 loss: 7.21842412e-07
Iter: 1082 loss: 7.21744755e-07
Iter: 1083 loss: 7.21540118e-07
Iter: 1084 loss: 7.21532047e-07
Iter: 1085 loss: 7.21315473e-07
Iter: 1086 loss: 7.21821834e-07
Iter: 1087 loss: 7.21202355e-07
Iter: 1088 loss: 7.20970206e-07
Iter: 1089 loss: 7.20982484e-07
Iter: 1090 loss: 7.20776029e-07
Iter: 1091 loss: 7.21185927e-07
Iter: 1092 loss: 7.20715377e-07
Iter: 1093 loss: 7.20517335e-07
Iter: 1094 loss: 7.20783305e-07
Iter: 1095 loss: 7.20410128e-07
Iter: 1096 loss: 7.202392e-07
Iter: 1097 loss: 7.22322341e-07
Iter: 1098 loss: 7.20216804e-07
Iter: 1099 loss: 7.20073331e-07
Iter: 1100 loss: 7.19863124e-07
Iter: 1101 loss: 7.19861873e-07
Iter: 1102 loss: 7.19681566e-07
Iter: 1103 loss: 7.19655418e-07
Iter: 1104 loss: 7.1947e-07
Iter: 1105 loss: 7.19526611e-07
Iter: 1106 loss: 7.19347042e-07
Iter: 1107 loss: 7.19216473e-07
Iter: 1108 loss: 7.1891634e-07
Iter: 1109 loss: 7.24965446e-07
Iter: 1110 loss: 7.18928163e-07
Iter: 1111 loss: 7.18857e-07
Iter: 1112 loss: 7.18749732e-07
Iter: 1113 loss: 7.18662761e-07
Iter: 1114 loss: 7.18458296e-07
Iter: 1115 loss: 7.18434535e-07
Iter: 1116 loss: 7.18302e-07
Iter: 1117 loss: 7.18581646e-07
Iter: 1118 loss: 7.18225635e-07
Iter: 1119 loss: 7.18058402e-07
Iter: 1120 loss: 7.18105071e-07
Iter: 1121 loss: 7.17955288e-07
Iter: 1122 loss: 7.17810849e-07
Iter: 1123 loss: 7.18623e-07
Iter: 1124 loss: 7.17769581e-07
Iter: 1125 loss: 7.1760212e-07
Iter: 1126 loss: 7.18309252e-07
Iter: 1127 loss: 7.17563694e-07
Iter: 1128 loss: 7.17343596e-07
Iter: 1129 loss: 7.18352567e-07
Iter: 1130 loss: 7.17304033e-07
Iter: 1131 loss: 7.17103831e-07
Iter: 1132 loss: 7.17181251e-07
Iter: 1133 loss: 7.16973375e-07
Iter: 1134 loss: 7.16737645e-07
Iter: 1135 loss: 7.18176125e-07
Iter: 1136 loss: 7.16706268e-07
Iter: 1137 loss: 7.16482191e-07
Iter: 1138 loss: 7.17772195e-07
Iter: 1139 loss: 7.16480599e-07
Iter: 1140 loss: 7.16355657e-07
Iter: 1141 loss: 7.16164777e-07
Iter: 1142 loss: 7.16151135e-07
Iter: 1143 loss: 7.15967246e-07
Iter: 1144 loss: 7.15923932e-07
Iter: 1145 loss: 7.158568e-07
Iter: 1146 loss: 7.15660917e-07
Iter: 1147 loss: 7.18454373e-07
Iter: 1148 loss: 7.15682916e-07
Iter: 1149 loss: 7.15466115e-07
Iter: 1150 loss: 7.16741738e-07
Iter: 1151 loss: 7.15442184e-07
Iter: 1152 loss: 7.15284614e-07
Iter: 1153 loss: 7.16769e-07
Iter: 1154 loss: 7.15251417e-07
Iter: 1155 loss: 7.15122269e-07
Iter: 1156 loss: 7.14821169e-07
Iter: 1157 loss: 7.18084607e-07
Iter: 1158 loss: 7.14766657e-07
Iter: 1159 loss: 7.14477551e-07
Iter: 1160 loss: 7.15722081e-07
Iter: 1161 loss: 7.14392343e-07
Iter: 1162 loss: 7.14142857e-07
Iter: 1163 loss: 7.15827582e-07
Iter: 1164 loss: 7.14119324e-07
Iter: 1165 loss: 7.13972554e-07
Iter: 1166 loss: 7.14683665e-07
Iter: 1167 loss: 7.13947e-07
Iter: 1168 loss: 7.13823169e-07
Iter: 1169 loss: 7.15135457e-07
Iter: 1170 loss: 7.13827262e-07
Iter: 1171 loss: 7.13727104e-07
Iter: 1172 loss: 7.13706584e-07
Iter: 1173 loss: 7.1365514e-07
Iter: 1174 loss: 7.13512691e-07
Iter: 1175 loss: 7.13659688e-07
Iter: 1176 loss: 7.13495069e-07
Iter: 1177 loss: 7.13336078e-07
Iter: 1178 loss: 7.15437295e-07
Iter: 1179 loss: 7.13314364e-07
Iter: 1180 loss: 7.1321017e-07
Iter: 1181 loss: 7.12995586e-07
Iter: 1182 loss: 7.16635839e-07
Iter: 1183 loss: 7.13e-07
Iter: 1184 loss: 7.12782708e-07
Iter: 1185 loss: 7.12807264e-07
Iter: 1186 loss: 7.12649921e-07
Iter: 1187 loss: 7.12365704e-07
Iter: 1188 loss: 7.16232876e-07
Iter: 1189 loss: 7.12368e-07
Iter: 1190 loss: 7.12085921e-07
Iter: 1191 loss: 7.12923793e-07
Iter: 1192 loss: 7.11968937e-07
Iter: 1193 loss: 7.11967687e-07
Iter: 1194 loss: 7.11887083e-07
Iter: 1195 loss: 7.11804e-07
Iter: 1196 loss: 7.11596158e-07
Iter: 1197 loss: 7.128736e-07
Iter: 1198 loss: 7.11551138e-07
Iter: 1199 loss: 7.11358382e-07
Iter: 1200 loss: 7.1269119e-07
Iter: 1201 loss: 7.11332461e-07
Iter: 1202 loss: 7.11149369e-07
Iter: 1203 loss: 7.11455527e-07
Iter: 1204 loss: 7.11081611e-07
Iter: 1205 loss: 7.10960705e-07
Iter: 1206 loss: 7.10962922e-07
Iter: 1207 loss: 7.10867653e-07
Iter: 1208 loss: 7.10731626e-07
Iter: 1209 loss: 7.10730319e-07
Iter: 1210 loss: 7.10564791e-07
Iter: 1211 loss: 7.111787e-07
Iter: 1212 loss: 7.10531253e-07
Iter: 1213 loss: 7.10374366e-07
Iter: 1214 loss: 7.11882535e-07
Iter: 1215 loss: 7.10388122e-07
Iter: 1216 loss: 7.10248457e-07
Iter: 1217 loss: 7.1027614e-07
Iter: 1218 loss: 7.10180927e-07
Iter: 1219 loss: 7.1005752e-07
Iter: 1220 loss: 7.10729068e-07
Iter: 1221 loss: 7.1006491e-07
Iter: 1222 loss: 7.09948552e-07
Iter: 1223 loss: 7.09792289e-07
Iter: 1224 loss: 7.09764095e-07
Iter: 1225 loss: 7.09573783e-07
Iter: 1226 loss: 7.0971123e-07
Iter: 1227 loss: 7.09468736e-07
Iter: 1228 loss: 7.09256483e-07
Iter: 1229 loss: 7.09765914e-07
Iter: 1230 loss: 7.09197479e-07
Iter: 1231 loss: 7.08936454e-07
Iter: 1232 loss: 7.11375606e-07
Iter: 1233 loss: 7.08941684e-07
Iter: 1234 loss: 7.08815946e-07
Iter: 1235 loss: 7.08602784e-07
Iter: 1236 loss: 7.08612674e-07
Iter: 1237 loss: 7.0843555e-07
Iter: 1238 loss: 7.09134611e-07
Iter: 1239 loss: 7.083745e-07
Iter: 1240 loss: 7.08267066e-07
Iter: 1241 loss: 7.08269681e-07
Iter: 1242 loss: 7.0814e-07
Iter: 1243 loss: 7.08160201e-07
Iter: 1244 loss: 7.08081416e-07
Iter: 1245 loss: 7.07948061e-07
Iter: 1246 loss: 7.08011896e-07
Iter: 1247 loss: 7.07862e-07
Iter: 1248 loss: 7.07725e-07
Iter: 1249 loss: 7.0862734e-07
Iter: 1250 loss: 7.0771938e-07
Iter: 1251 loss: 7.07593927e-07
Iter: 1252 loss: 7.08958339e-07
Iter: 1253 loss: 7.07587105e-07
Iter: 1254 loss: 7.07508e-07
Iter: 1255 loss: 7.0743755e-07
Iter: 1256 loss: 7.07427375e-07
Iter: 1257 loss: 7.07292e-07
Iter: 1258 loss: 7.08422363e-07
Iter: 1259 loss: 7.07264576e-07
Iter: 1260 loss: 7.0714043e-07
Iter: 1261 loss: 7.06925675e-07
Iter: 1262 loss: 7.10488166e-07
Iter: 1263 loss: 7.06877586e-07
Iter: 1264 loss: 7.06702167e-07
Iter: 1265 loss: 7.08721473e-07
Iter: 1266 loss: 7.06694891e-07
Iter: 1267 loss: 7.06513731e-07
Iter: 1268 loss: 7.0770318e-07
Iter: 1269 loss: 7.06516e-07
Iter: 1270 loss: 7.06390097e-07
Iter: 1271 loss: 7.06166475e-07
Iter: 1272 loss: 7.11003622e-07
Iter: 1273 loss: 7.06151354e-07
Iter: 1274 loss: 7.05974799e-07
Iter: 1275 loss: 7.06485253e-07
Iter: 1276 loss: 7.05895e-07
Iter: 1277 loss: 7.05739694e-07
Iter: 1278 loss: 7.08044411e-07
Iter: 1279 loss: 7.05728098e-07
Iter: 1280 loss: 7.05591447e-07
Iter: 1281 loss: 7.05797447e-07
Iter: 1282 loss: 7.0552926e-07
Iter: 1283 loss: 7.05403181e-07
Iter: 1284 loss: 7.05298817e-07
Iter: 1285 loss: 7.052613e-07
Iter: 1286 loss: 7.05095772e-07
Iter: 1287 loss: 7.07538106e-07
Iter: 1288 loss: 7.05095829e-07
Iter: 1289 loss: 7.04980209e-07
Iter: 1290 loss: 7.05629873e-07
Iter: 1291 loss: 7.04951958e-07
Iter: 1292 loss: 7.04819854e-07
Iter: 1293 loss: 7.04784725e-07
Iter: 1294 loss: 7.04738227e-07
Iter: 1295 loss: 7.04559284e-07
Iter: 1296 loss: 7.05871798e-07
Iter: 1297 loss: 7.04544618e-07
Iter: 1298 loss: 7.04418e-07
Iter: 1299 loss: 7.04224e-07
Iter: 1300 loss: 7.0422675e-07
Iter: 1301 loss: 7.04079753e-07
Iter: 1302 loss: 7.06125149e-07
Iter: 1303 loss: 7.04090553e-07
Iter: 1304 loss: 7.03922183e-07
Iter: 1305 loss: 7.04360332e-07
Iter: 1306 loss: 7.03876253e-07
Iter: 1307 loss: 7.03772344e-07
Iter: 1308 loss: 7.03582941e-07
Iter: 1309 loss: 7.03581634e-07
Iter: 1310 loss: 7.03456408e-07
Iter: 1311 loss: 7.03834701e-07
Iter: 1312 loss: 7.03378817e-07
Iter: 1313 loss: 7.03235457e-07
Iter: 1314 loss: 7.05057914e-07
Iter: 1315 loss: 7.03230285e-07
Iter: 1316 loss: 7.03055321e-07
Iter: 1317 loss: 7.03080104e-07
Iter: 1318 loss: 7.0295215e-07
Iter: 1319 loss: 7.02786792e-07
Iter: 1320 loss: 7.02802481e-07
Iter: 1321 loss: 7.02684702e-07
Iter: 1322 loss: 7.02558e-07
Iter: 1323 loss: 7.02536283e-07
Iter: 1324 loss: 7.02411114e-07
Iter: 1325 loss: 7.02585e-07
Iter: 1326 loss: 7.02366151e-07
Iter: 1327 loss: 7.02242858e-07
Iter: 1328 loss: 7.02562033e-07
Iter: 1329 loss: 7.02205398e-07
Iter: 1330 loss: 7.02080797e-07
Iter: 1331 loss: 7.02592388e-07
Iter: 1332 loss: 7.02069087e-07
Iter: 1333 loss: 7.01971885e-07
Iter: 1334 loss: 7.01758211e-07
Iter: 1335 loss: 7.06169203e-07
Iter: 1336 loss: 7.01760825e-07
Iter: 1337 loss: 7.01636395e-07
Iter: 1338 loss: 7.01621047e-07
Iter: 1339 loss: 7.01495594e-07
Iter: 1340 loss: 7.01458134e-07
Iter: 1341 loss: 7.01367185e-07
Iter: 1342 loss: 7.01224053e-07
Iter: 1343 loss: 7.01038459e-07
Iter: 1344 loss: 7.01027716e-07
Iter: 1345 loss: 7.00814e-07
Iter: 1346 loss: 7.02319767e-07
Iter: 1347 loss: 7.00801138e-07
Iter: 1348 loss: 7.00667044e-07
Iter: 1349 loss: 7.0066551e-07
Iter: 1350 loss: 7.00553301e-07
Iter: 1351 loss: 7.00559838e-07
Iter: 1352 loss: 7.00476221e-07
Iter: 1353 loss: 7.00377598e-07
Iter: 1354 loss: 7.0027005e-07
Iter: 1355 loss: 7.00226678e-07
Iter: 1356 loss: 7.00055125e-07
Iter: 1357 loss: 7.01792828e-07
Iter: 1358 loss: 7.00047849e-07
Iter: 1359 loss: 6.99917223e-07
Iter: 1360 loss: 7.01257193e-07
Iter: 1361 loss: 6.99908071e-07
Iter: 1362 loss: 6.99824e-07
Iter: 1363 loss: 6.99813427e-07
Iter: 1364 loss: 6.99752377e-07
Iter: 1365 loss: 6.99638463e-07
Iter: 1366 loss: 7.00024145e-07
Iter: 1367 loss: 6.9960339e-07
Iter: 1368 loss: 6.99479e-07
Iter: 1369 loss: 6.99460202e-07
Iter: 1370 loss: 6.99382667e-07
Iter: 1371 loss: 6.99226575e-07
Iter: 1372 loss: 7.00216674e-07
Iter: 1373 loss: 6.99199063e-07
Iter: 1374 loss: 6.99039902e-07
Iter: 1375 loss: 6.99213729e-07
Iter: 1376 loss: 6.98975e-07
Iter: 1377 loss: 6.98836914e-07
Iter: 1378 loss: 6.98699864e-07
Iter: 1379 loss: 6.98689178e-07
Iter: 1380 loss: 6.98468114e-07
Iter: 1381 loss: 6.99096518e-07
Iter: 1382 loss: 6.98399163e-07
Iter: 1383 loss: 6.98217946e-07
Iter: 1384 loss: 7.00085e-07
Iter: 1385 loss: 6.98219196e-07
Iter: 1386 loss: 6.98015242e-07
Iter: 1387 loss: 6.98204815e-07
Iter: 1388 loss: 6.97912924e-07
Iter: 1389 loss: 6.97790483e-07
Iter: 1390 loss: 6.9774336e-07
Iter: 1391 loss: 6.97624557e-07
Iter: 1392 loss: 6.97431e-07
Iter: 1393 loss: 6.97954363e-07
Iter: 1394 loss: 6.97358814e-07
Iter: 1395 loss: 6.97258031e-07
Iter: 1396 loss: 6.97250471e-07
Iter: 1397 loss: 6.9714639e-07
Iter: 1398 loss: 6.97462212e-07
Iter: 1399 loss: 6.97104213e-07
Iter: 1400 loss: 6.97025712e-07
Iter: 1401 loss: 6.97107794e-07
Iter: 1402 loss: 6.96975803e-07
Iter: 1403 loss: 6.96874679e-07
Iter: 1404 loss: 6.97849146e-07
Iter: 1405 loss: 6.96877862e-07
Iter: 1406 loss: 6.96773327e-07
Iter: 1407 loss: 6.96687096e-07
Iter: 1408 loss: 6.96651512e-07
Iter: 1409 loss: 6.96514064e-07
Iter: 1410 loss: 6.96854499e-07
Iter: 1411 loss: 6.96444715e-07
Iter: 1412 loss: 6.96295103e-07
Iter: 1413 loss: 6.98083795e-07
Iter: 1414 loss: 6.96301186e-07
Iter: 1415 loss: 6.96214613e-07
Iter: 1416 loss: 6.96027428e-07
Iter: 1417 loss: 6.99371924e-07
Iter: 1418 loss: 6.960218e-07
Iter: 1419 loss: 6.95804886e-07
Iter: 1420 loss: 6.95870426e-07
Iter: 1421 loss: 6.95657207e-07
Iter: 1422 loss: 6.95424433e-07
Iter: 1423 loss: 6.987392e-07
Iter: 1424 loss: 6.95423296e-07
Iter: 1425 loss: 6.95258791e-07
Iter: 1426 loss: 6.9653521e-07
Iter: 1427 loss: 6.95218e-07
Iter: 1428 loss: 6.95109179e-07
Iter: 1429 loss: 6.9485759e-07
Iter: 1430 loss: 6.99749876e-07
Iter: 1431 loss: 6.94871e-07
Iter: 1432 loss: 6.94677112e-07
Iter: 1433 loss: 6.95692108e-07
Iter: 1434 loss: 6.9463141e-07
Iter: 1435 loss: 6.9449186e-07
Iter: 1436 loss: 6.95696087e-07
Iter: 1437 loss: 6.94467417e-07
Iter: 1438 loss: 6.94368737e-07
Iter: 1439 loss: 6.94364758e-07
Iter: 1440 loss: 6.94313826e-07
Iter: 1441 loss: 6.94236576e-07
Iter: 1442 loss: 6.94237315e-07
Iter: 1443 loss: 6.94147161e-07
Iter: 1444 loss: 6.94049618e-07
Iter: 1445 loss: 6.94033758e-07
Iter: 1446 loss: 6.93927916e-07
Iter: 1447 loss: 6.93932236e-07
Iter: 1448 loss: 6.93839866e-07
Iter: 1449 loss: 6.93674224e-07
Iter: 1450 loss: 6.96109851e-07
Iter: 1451 loss: 6.93652737e-07
Iter: 1452 loss: 6.93448442e-07
Iter: 1453 loss: 6.93863512e-07
Iter: 1454 loss: 6.93371476e-07
Iter: 1455 loss: 6.93138873e-07
Iter: 1456 loss: 6.93144727e-07
Iter: 1457 loss: 6.93025413e-07
Iter: 1458 loss: 6.92835215e-07
Iter: 1459 loss: 6.92800427e-07
Iter: 1460 loss: 6.9265036e-07
Iter: 1461 loss: 6.93634206e-07
Iter: 1462 loss: 6.92623303e-07
Iter: 1463 loss: 6.92423214e-07
Iter: 1464 loss: 6.93470895e-07
Iter: 1465 loss: 6.92374556e-07
Iter: 1466 loss: 6.92289859e-07
Iter: 1467 loss: 6.92249444e-07
Iter: 1468 loss: 6.92211756e-07
Iter: 1469 loss: 6.92055664e-07
Iter: 1470 loss: 6.92003766e-07
Iter: 1471 loss: 6.91914408e-07
Iter: 1472 loss: 6.9189241e-07
Iter: 1473 loss: 6.91821e-07
Iter: 1474 loss: 6.91735181e-07
Iter: 1475 loss: 6.91659807e-07
Iter: 1476 loss: 6.91634568e-07
Iter: 1477 loss: 6.91472792e-07
Iter: 1478 loss: 6.9135217e-07
Iter: 1479 loss: 6.91323066e-07
Iter: 1480 loss: 6.91144407e-07
Iter: 1481 loss: 6.9313711e-07
Iter: 1482 loss: 6.9112366e-07
Iter: 1483 loss: 6.90926072e-07
Iter: 1484 loss: 6.91689934e-07
Iter: 1485 loss: 6.90902652e-07
Iter: 1486 loss: 6.90810339e-07
Iter: 1487 loss: 6.90646175e-07
Iter: 1488 loss: 6.90648051e-07
Iter: 1489 loss: 6.90474849e-07
Iter: 1490 loss: 6.91648e-07
Iter: 1491 loss: 6.90442107e-07
Iter: 1492 loss: 6.90318871e-07
Iter: 1493 loss: 6.91729781e-07
Iter: 1494 loss: 6.90303693e-07
Iter: 1495 loss: 6.90208822e-07
Iter: 1496 loss: 6.89969738e-07
Iter: 1497 loss: 6.92299238e-07
Iter: 1498 loss: 6.89941771e-07
Iter: 1499 loss: 6.89691092e-07
Iter: 1500 loss: 6.91978755e-07
Iter: 1501 loss: 6.8966591e-07
Iter: 1502 loss: 6.89408239e-07
Iter: 1503 loss: 6.90586944e-07
Iter: 1504 loss: 6.89383796e-07
Iter: 1505 loss: 6.89204967e-07
Iter: 1506 loss: 6.88896307e-07
Iter: 1507 loss: 6.95670508e-07
Iter: 1508 loss: 6.8890489e-07
Iter: 1509 loss: 6.88588898e-07
Iter: 1510 loss: 6.91363539e-07
Iter: 1511 loss: 6.8857662e-07
Iter: 1512 loss: 6.8854979e-07
Iter: 1513 loss: 6.88487773e-07
Iter: 1514 loss: 6.88428202e-07
Iter: 1515 loss: 6.88299451e-07
Iter: 1516 loss: 6.91269179e-07
Iter: 1517 loss: 6.8830991e-07
Iter: 1518 loss: 6.88192586e-07
Iter: 1519 loss: 6.88444629e-07
Iter: 1520 loss: 6.88146088e-07
Iter: 1521 loss: 6.88004093e-07
Iter: 1522 loss: 6.88239084e-07
Iter: 1523 loss: 6.87949694e-07
Iter: 1524 loss: 6.87818556e-07
Iter: 1525 loss: 6.89692058e-07
Iter: 1526 loss: 6.87782403e-07
Iter: 1527 loss: 6.87692136e-07
Iter: 1528 loss: 6.87523936e-07
Iter: 1529 loss: 6.91460968e-07
Iter: 1530 loss: 6.87526949e-07
Iter: 1531 loss: 6.87312593e-07
Iter: 1532 loss: 6.87571685e-07
Iter: 1533 loss: 6.87208342e-07
Iter: 1534 loss: 6.87107558e-07
Iter: 1535 loss: 6.870942e-07
Iter: 1536 loss: 6.86965e-07
Iter: 1537 loss: 6.86748763e-07
Iter: 1538 loss: 6.90687216e-07
Iter: 1539 loss: 6.86733415e-07
Iter: 1540 loss: 6.86574424e-07
Iter: 1541 loss: 6.88091e-07
Iter: 1542 loss: 6.86581302e-07
Iter: 1543 loss: 6.86396788e-07
Iter: 1544 loss: 6.86963062e-07
Iter: 1545 loss: 6.86349722e-07
Iter: 1546 loss: 6.86221426e-07
Iter: 1547 loss: 6.86102908e-07
Iter: 1548 loss: 6.86057e-07
Iter: 1549 loss: 6.85879513e-07
Iter: 1550 loss: 6.86527414e-07
Iter: 1551 loss: 6.85831594e-07
Iter: 1552 loss: 6.85582961e-07
Iter: 1553 loss: 6.88316845e-07
Iter: 1554 loss: 6.85555165e-07
Iter: 1555 loss: 6.85468251e-07
Iter: 1556 loss: 6.85252587e-07
Iter: 1557 loss: 6.88886928e-07
Iter: 1558 loss: 6.85260261e-07
Iter: 1559 loss: 6.85071768e-07
Iter: 1560 loss: 6.86322096e-07
Iter: 1561 loss: 6.85034e-07
Iter: 1562 loss: 6.84863835e-07
Iter: 1563 loss: 6.85156692e-07
Iter: 1564 loss: 6.84798351e-07
Iter: 1565 loss: 6.84654765e-07
Iter: 1566 loss: 6.85358941e-07
Iter: 1567 loss: 6.84666645e-07
Iter: 1568 loss: 6.84546819e-07
Iter: 1569 loss: 6.84524764e-07
Iter: 1570 loss: 6.84497763e-07
Iter: 1571 loss: 6.8431541e-07
Iter: 1572 loss: 6.85225e-07
Iter: 1573 loss: 6.84274e-07
Iter: 1574 loss: 6.84013457e-07
Iter: 1575 loss: 6.8456859e-07
Iter: 1576 loss: 6.8388124e-07
Iter: 1577 loss: 6.83728842e-07
Iter: 1578 loss: 6.83725773e-07
Iter: 1579 loss: 6.83557346e-07
Iter: 1580 loss: 6.83307121e-07
Iter: 1581 loss: 6.83305245e-07
Iter: 1582 loss: 6.83041776e-07
Iter: 1583 loss: 6.83967528e-07
Iter: 1584 loss: 6.82985842e-07
Iter: 1585 loss: 6.82892e-07
Iter: 1586 loss: 6.82827135e-07
Iter: 1587 loss: 6.82759037e-07
Iter: 1588 loss: 6.8264103e-07
Iter: 1589 loss: 6.82612495e-07
Iter: 1590 loss: 6.82521e-07
Iter: 1591 loss: 6.82504322e-07
Iter: 1592 loss: 6.82443e-07
Iter: 1593 loss: 6.82316227e-07
Iter: 1594 loss: 6.82325776e-07
Iter: 1595 loss: 6.82181621e-07
Iter: 1596 loss: 6.82519499e-07
Iter: 1597 loss: 6.82151267e-07
Iter: 1598 loss: 6.82019049e-07
Iter: 1599 loss: 6.82136317e-07
Iter: 1600 loss: 6.81923154e-07
Iter: 1601 loss: 6.81780875e-07
Iter: 1602 loss: 6.82459358e-07
Iter: 1603 loss: 6.8176314e-07
Iter: 1604 loss: 6.81688334e-07
Iter: 1605 loss: 6.81686117e-07
Iter: 1606 loss: 6.816216e-07
Iter: 1607 loss: 6.81473466e-07
Iter: 1608 loss: 6.84404142e-07
Iter: 1609 loss: 6.81482561e-07
Iter: 1610 loss: 6.81324082e-07
Iter: 1611 loss: 6.81554582e-07
Iter: 1612 loss: 6.81242113e-07
Iter: 1613 loss: 6.81080564e-07
Iter: 1614 loss: 6.82863345e-07
Iter: 1615 loss: 6.81085055e-07
Iter: 1616 loss: 6.80986147e-07
Iter: 1617 loss: 6.8082619e-07
Iter: 1618 loss: 6.80809876e-07
Iter: 1619 loss: 6.80586879e-07
Iter: 1620 loss: 6.80854384e-07
Iter: 1621 loss: 6.80487233e-07
Iter: 1622 loss: 6.80387757e-07
Iter: 1623 loss: 6.80330459e-07
Iter: 1624 loss: 6.80265089e-07
Iter: 1625 loss: 6.80105472e-07
Iter: 1626 loss: 6.80116273e-07
Iter: 1627 loss: 6.80003268e-07
Iter: 1628 loss: 6.80010317e-07
Iter: 1629 loss: 6.79926643e-07
Iter: 1630 loss: 6.79811308e-07
Iter: 1631 loss: 6.82619884e-07
Iter: 1632 loss: 6.79813752e-07
Iter: 1633 loss: 6.79695745e-07
Iter: 1634 loss: 6.80139578e-07
Iter: 1635 loss: 6.79648e-07
Iter: 1636 loss: 6.79564721e-07
Iter: 1637 loss: 6.7986133e-07
Iter: 1638 loss: 6.79572793e-07
Iter: 1639 loss: 6.79468201e-07
Iter: 1640 loss: 6.79342747e-07
Iter: 1641 loss: 6.79295908e-07
Iter: 1642 loss: 6.79180516e-07
Iter: 1643 loss: 6.79185177e-07
Iter: 1644 loss: 6.79065181e-07
Iter: 1645 loss: 6.794304e-07
Iter: 1646 loss: 6.79026527e-07
Iter: 1647 loss: 6.78918e-07
Iter: 1648 loss: 6.78672166e-07
Iter: 1649 loss: 6.81983579e-07
Iter: 1650 loss: 6.78686604e-07
Iter: 1651 loss: 6.78650792e-07
Iter: 1652 loss: 6.78587526e-07
Iter: 1653 loss: 6.78512606e-07
Iter: 1654 loss: 6.78333549e-07
Iter: 1655 loss: 6.78336903e-07
Iter: 1656 loss: 6.78185756e-07
Iter: 1657 loss: 6.78799836e-07
Iter: 1658 loss: 6.7814085e-07
Iter: 1659 loss: 6.7804649e-07
Iter: 1660 loss: 6.78031483e-07
Iter: 1661 loss: 6.77976232e-07
Iter: 1662 loss: 6.77728451e-07
Iter: 1663 loss: 6.80124231e-07
Iter: 1664 loss: 6.77734647e-07
Iter: 1665 loss: 6.77631874e-07
Iter: 1666 loss: 6.77609535e-07
Iter: 1667 loss: 6.77519e-07
Iter: 1668 loss: 6.77509263e-07
Iter: 1669 loss: 6.77459468e-07
Iter: 1670 loss: 6.77319292e-07
Iter: 1671 loss: 6.77430251e-07
Iter: 1672 loss: 6.77288199e-07
Iter: 1673 loss: 6.77137564e-07
Iter: 1674 loss: 6.77236926e-07
Iter: 1675 loss: 6.77051219e-07
Iter: 1676 loss: 6.76898765e-07
Iter: 1677 loss: 6.7781059e-07
Iter: 1678 loss: 6.76885634e-07
Iter: 1679 loss: 6.76780815e-07
Iter: 1680 loss: 6.77480216e-07
Iter: 1681 loss: 6.7677189e-07
Iter: 1682 loss: 6.76681111e-07
Iter: 1683 loss: 6.76792069e-07
Iter: 1684 loss: 6.76613581e-07
Iter: 1685 loss: 6.76535592e-07
Iter: 1686 loss: 6.76402806e-07
Iter: 1687 loss: 6.76372281e-07
Iter: 1688 loss: 6.76248703e-07
Iter: 1689 loss: 6.76257912e-07
Iter: 1690 loss: 6.76130583e-07
Iter: 1691 loss: 6.76136494e-07
Iter: 1692 loss: 6.7601917e-07
Iter: 1693 loss: 6.75923843e-07
Iter: 1694 loss: 6.75924866e-07
Iter: 1695 loss: 6.75843921e-07
Iter: 1696 loss: 6.75633942e-07
Iter: 1697 loss: 6.78276592e-07
Iter: 1698 loss: 6.75633885e-07
Iter: 1699 loss: 6.75477395e-07
Iter: 1700 loss: 6.76705497e-07
Iter: 1701 loss: 6.7548325e-07
Iter: 1702 loss: 6.75325225e-07
Iter: 1703 loss: 6.75943568e-07
Iter: 1704 loss: 6.75263323e-07
Iter: 1705 loss: 6.75146339e-07
Iter: 1706 loss: 6.75111664e-07
Iter: 1707 loss: 6.75052206e-07
Iter: 1708 loss: 6.74909188e-07
Iter: 1709 loss: 6.75126728e-07
Iter: 1710 loss: 6.74832336e-07
Iter: 1711 loss: 6.74689488e-07
Iter: 1712 loss: 6.75393835e-07
Iter: 1713 loss: 6.74674936e-07
Iter: 1714 loss: 6.74557441e-07
Iter: 1715 loss: 6.75106776e-07
Iter: 1716 loss: 6.74547e-07
Iter: 1717 loss: 6.7441772e-07
Iter: 1718 loss: 6.75404294e-07
Iter: 1719 loss: 6.74420448e-07
Iter: 1720 loss: 6.74353259e-07
Iter: 1721 loss: 6.74220928e-07
Iter: 1722 loss: 6.75895308e-07
Iter: 1723 loss: 6.74223656e-07
Iter: 1724 loss: 6.74060516e-07
Iter: 1725 loss: 6.75836816e-07
Iter: 1726 loss: 6.74063585e-07
Iter: 1727 loss: 6.7396553e-07
Iter: 1728 loss: 6.74178352e-07
Iter: 1729 loss: 6.73922841e-07
Iter: 1730 loss: 6.73805459e-07
Iter: 1731 loss: 6.74089449e-07
Iter: 1732 loss: 6.73760155e-07
Iter: 1733 loss: 6.73603836e-07
Iter: 1734 loss: 6.74099283e-07
Iter: 1735 loss: 6.73567797e-07
Iter: 1736 loss: 6.73463887e-07
Iter: 1737 loss: 6.73198315e-07
Iter: 1738 loss: 6.7669e-07
Iter: 1739 loss: 6.73171485e-07
Iter: 1740 loss: 6.73131069e-07
Iter: 1741 loss: 6.7305939e-07
Iter: 1742 loss: 6.7294809e-07
Iter: 1743 loss: 6.72762667e-07
Iter: 1744 loss: 6.7276676e-07
Iter: 1745 loss: 6.7260288e-07
Iter: 1746 loss: 6.7269832e-07
Iter: 1747 loss: 6.72479132e-07
Iter: 1748 loss: 6.72291947e-07
Iter: 1749 loss: 6.73851275e-07
Iter: 1750 loss: 6.72281772e-07
Iter: 1751 loss: 6.72137389e-07
Iter: 1752 loss: 6.72544502e-07
Iter: 1753 loss: 6.72120962e-07
Iter: 1754 loss: 6.71985163e-07
Iter: 1755 loss: 6.73428247e-07
Iter: 1756 loss: 6.71988232e-07
Iter: 1757 loss: 6.71899329e-07
Iter: 1758 loss: 6.71843281e-07
Iter: 1759 loss: 6.71816224e-07
Iter: 1760 loss: 6.71722205e-07
Iter: 1761 loss: 6.72124145e-07
Iter: 1762 loss: 6.71711405e-07
Iter: 1763 loss: 6.71588452e-07
Iter: 1764 loss: 6.72370902e-07
Iter: 1765 loss: 6.71602493e-07
Iter: 1766 loss: 6.71517228e-07
Iter: 1767 loss: 6.71593284e-07
Iter: 1768 loss: 6.71500743e-07
Iter: 1769 loss: 6.71372391e-07
Iter: 1770 loss: 6.7145271e-07
Iter: 1771 loss: 6.71315775e-07
Iter: 1772 loss: 6.7121448e-07
Iter: 1773 loss: 6.71212092e-07
Iter: 1774 loss: 6.71146495e-07
Iter: 1775 loss: 6.7103997e-07
Iter: 1776 loss: 6.72701503e-07
Iter: 1777 loss: 6.71039516e-07
Iter: 1778 loss: 6.70991767e-07
Iter: 1779 loss: 6.70826296e-07
Iter: 1780 loss: 6.7084909e-07
Iter: 1781 loss: 6.70698341e-07
Iter: 1782 loss: 6.70669294e-07
Iter: 1783 loss: 6.70568397e-07
Iter: 1784 loss: 6.70460622e-07
Iter: 1785 loss: 6.70466818e-07
Iter: 1786 loss: 6.70398663e-07
Iter: 1787 loss: 6.7079975e-07
Iter: 1788 loss: 6.70385361e-07
Iter: 1789 loss: 6.70303791e-07
Iter: 1790 loss: 6.70221937e-07
Iter: 1791 loss: 6.70171971e-07
Iter: 1792 loss: 6.70048962e-07
Iter: 1793 loss: 6.70185955e-07
Iter: 1794 loss: 6.69987685e-07
Iter: 1795 loss: 6.69857513e-07
Iter: 1796 loss: 6.70866314e-07
Iter: 1797 loss: 6.6985308e-07
Iter: 1798 loss: 6.69743827e-07
Iter: 1799 loss: 6.69994563e-07
Iter: 1800 loss: 6.69707561e-07
Iter: 1801 loss: 6.69632527e-07
Iter: 1802 loss: 6.70364e-07
Iter: 1803 loss: 6.69615929e-07
Iter: 1804 loss: 6.69519238e-07
Iter: 1805 loss: 6.69495648e-07
Iter: 1806 loss: 6.69436417e-07
Iter: 1807 loss: 6.69379347e-07
Iter: 1808 loss: 6.6957648e-07
Iter: 1809 loss: 6.69347799e-07
Iter: 1810 loss: 6.69262818e-07
Iter: 1811 loss: 6.69610813e-07
Iter: 1812 loss: 6.69247754e-07
Iter: 1813 loss: 6.6915652e-07
Iter: 1814 loss: 6.69002702e-07
Iter: 1815 loss: 6.71977034e-07
Iter: 1816 loss: 6.69001224e-07
Iter: 1817 loss: 6.68854796e-07
Iter: 1818 loss: 6.69190399e-07
Iter: 1819 loss: 6.68772316e-07
Iter: 1820 loss: 6.68600762e-07
Iter: 1821 loss: 6.69407882e-07
Iter: 1822 loss: 6.68593714e-07
Iter: 1823 loss: 6.68416192e-07
Iter: 1824 loss: 6.68713426e-07
Iter: 1825 loss: 6.68371285e-07
Iter: 1826 loss: 6.68232e-07
Iter: 1827 loss: 6.68252824e-07
Iter: 1828 loss: 6.68139e-07
Iter: 1829 loss: 6.6828e-07
Iter: 1830 loss: 6.68106168e-07
Iter: 1831 loss: 6.68044208e-07
Iter: 1832 loss: 6.67936e-07
Iter: 1833 loss: 6.67927907e-07
Iter: 1834 loss: 6.67884649e-07
Iter: 1835 loss: 6.67858558e-07
Iter: 1836 loss: 6.67782388e-07
Iter: 1837 loss: 6.67788527e-07
Iter: 1838 loss: 6.67723384e-07
Iter: 1839 loss: 6.67639824e-07
Iter: 1840 loss: 6.67990662e-07
Iter: 1841 loss: 6.67604866e-07
Iter: 1842 loss: 6.67490724e-07
Iter: 1843 loss: 6.67466907e-07
Iter: 1844 loss: 6.67393238e-07
Iter: 1845 loss: 6.67267159e-07
Iter: 1846 loss: 6.67759764e-07
Iter: 1847 loss: 6.67273355e-07
Iter: 1848 loss: 6.67115728e-07
Iter: 1849 loss: 6.67457812e-07
Iter: 1850 loss: 6.67064739e-07
Iter: 1851 loss: 6.66978451e-07
Iter: 1852 loss: 6.66895517e-07
Iter: 1853 loss: 6.66852941e-07
Iter: 1854 loss: 6.66728624e-07
Iter: 1855 loss: 6.67250333e-07
Iter: 1856 loss: 6.66683434e-07
Iter: 1857 loss: 6.6657725e-07
Iter: 1858 loss: 6.67523864e-07
Iter: 1859 loss: 6.6657077e-07
Iter: 1860 loss: 6.66461e-07
Iter: 1861 loss: 6.67140853e-07
Iter: 1862 loss: 6.66465212e-07
Iter: 1863 loss: 6.66381311e-07
Iter: 1864 loss: 6.6625887e-07
Iter: 1865 loss: 6.69111728e-07
Iter: 1866 loss: 6.66242499e-07
Iter: 1867 loss: 6.6607322e-07
Iter: 1868 loss: 6.66599e-07
Iter: 1869 loss: 6.65989887e-07
Iter: 1870 loss: 6.65962091e-07
Iter: 1871 loss: 6.65910932e-07
Iter: 1872 loss: 6.65857726e-07
Iter: 1873 loss: 6.65690948e-07
Iter: 1874 loss: 6.68727751e-07
Iter: 1875 loss: 6.65700213e-07
Iter: 1876 loss: 6.65580274e-07
Iter: 1877 loss: 6.65582547e-07
Iter: 1878 loss: 6.65496088e-07
Iter: 1879 loss: 6.65409175e-07
Iter: 1880 loss: 6.65381947e-07
Iter: 1881 loss: 6.65304128e-07
Iter: 1882 loss: 6.65314701e-07
Iter: 1883 loss: 6.65240634e-07
Iter: 1884 loss: 6.6518669e-07
Iter: 1885 loss: 6.6516e-07
Iter: 1886 loss: 6.65044467e-07
Iter: 1887 loss: 6.64929644e-07
Iter: 1888 loss: 6.64922197e-07
Iter: 1889 loss: 6.64706818e-07
Iter: 1890 loss: 6.65523487e-07
Iter: 1891 loss: 6.6469039e-07
Iter: 1892 loss: 6.64560389e-07
Iter: 1893 loss: 6.64541403e-07
Iter: 1894 loss: 6.64455968e-07
Iter: 1895 loss: 6.64741549e-07
Iter: 1896 loss: 6.64403501e-07
Iter: 1897 loss: 6.64337279e-07
Iter: 1898 loss: 6.64244453e-07
Iter: 1899 loss: 6.64216827e-07
Iter: 1900 loss: 6.64091601e-07
Iter: 1901 loss: 6.64788161e-07
Iter: 1902 loss: 6.64085178e-07
Iter: 1903 loss: 6.63936078e-07
Iter: 1904 loss: 6.64780544e-07
Iter: 1905 loss: 6.63910441e-07
Iter: 1906 loss: 6.63856952e-07
Iter: 1907 loss: 6.63858202e-07
Iter: 1908 loss: 6.63795845e-07
Iter: 1909 loss: 6.63692958e-07
Iter: 1910 loss: 6.64203185e-07
Iter: 1911 loss: 6.63672267e-07
Iter: 1912 loss: 6.63595642e-07
Iter: 1913 loss: 6.6366e-07
Iter: 1914 loss: 6.63569949e-07
Iter: 1915 loss: 6.63483547e-07
Iter: 1916 loss: 6.64477113e-07
Iter: 1917 loss: 6.63503045e-07
Iter: 1918 loss: 6.63434548e-07
Iter: 1919 loss: 6.63313131e-07
Iter: 1920 loss: 6.6508079e-07
Iter: 1921 loss: 6.63334049e-07
Iter: 1922 loss: 6.63213143e-07
Iter: 1923 loss: 6.63453704e-07
Iter: 1924 loss: 6.63170681e-07
Iter: 1925 loss: 6.63046535e-07
Iter: 1926 loss: 6.63879405e-07
Iter: 1927 loss: 6.63046421e-07
Iter: 1928 loss: 6.62962179e-07
Iter: 1929 loss: 6.63936476e-07
Iter: 1930 loss: 6.62943307e-07
Iter: 1931 loss: 6.62886521e-07
Iter: 1932 loss: 6.62729576e-07
Iter: 1933 loss: 6.62735204e-07
Iter: 1934 loss: 6.62558875e-07
Iter: 1935 loss: 6.62735829e-07
Iter: 1936 loss: 6.62442289e-07
Iter: 1937 loss: 6.62447746e-07
Iter: 1938 loss: 6.62375612e-07
Iter: 1939 loss: 6.62311e-07
Iter: 1940 loss: 6.62146647e-07
Iter: 1941 loss: 6.63923174e-07
Iter: 1942 loss: 6.62141076e-07
Iter: 1943 loss: 6.62056038e-07
Iter: 1944 loss: 6.62059335e-07
Iter: 1945 loss: 6.61978788e-07
Iter: 1946 loss: 6.61934678e-07
Iter: 1947 loss: 6.619e-07
Iter: 1948 loss: 6.61840943e-07
Iter: 1949 loss: 6.62558705e-07
Iter: 1950 loss: 6.61830086e-07
Iter: 1951 loss: 6.61769604e-07
Iter: 1952 loss: 6.61881927e-07
Iter: 1953 loss: 6.61731292e-07
Iter: 1954 loss: 6.61661488e-07
Iter: 1955 loss: 6.61613058e-07
Iter: 1956 loss: 6.61620732e-07
Iter: 1957 loss: 6.61550473e-07
Iter: 1958 loss: 6.61734248e-07
Iter: 1959 loss: 6.6148948e-07
Iter: 1960 loss: 6.61394324e-07
Iter: 1961 loss: 6.62252944e-07
Iter: 1962 loss: 6.61398531e-07
Iter: 1963 loss: 6.6127609e-07
Iter: 1964 loss: 6.61533818e-07
Iter: 1965 loss: 6.61255854e-07
Iter: 1966 loss: 6.61169111e-07
Iter: 1967 loss: 6.61051672e-07
Iter: 1968 loss: 6.61034164e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6
+ date
Wed Oct 21 11:11:28 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1 --function f1 --psi 0 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00059c3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00059ea950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000595c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00058f6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00058f6268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000595c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005b48158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00058201e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005820c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00058207b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057f02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00056daa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00056fcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057550d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005765510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00056fcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057cad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005755048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005696400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000567d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005687950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0000440158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005687840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0000436ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00003c36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005743bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005743840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057130d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057131e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00003ef9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000041e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00003efbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00003ef510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000026a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00002c00d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00002b6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.016922584
test_loss: 0.015464297
train_loss: 0.0063496646
test_loss: 0.0059371376
train_loss: 0.003089107
test_loss: 0.003022571
train_loss: 0.0021938388
test_loss: 0.0021872728
train_loss: 0.001975308
test_loss: 0.0027110185
train_loss: 0.0019643097
test_loss: 0.002601942
train_loss: 0.0019080758
test_loss: 0.0019133923
train_loss: 0.0021056659
test_loss: 0.0017717603
train_loss: 0.0016050718
test_loss: 0.0017009822
train_loss: 0.0016690297
test_loss: 0.0018089985
train_loss: 0.001665068
test_loss: 0.0019052639
train_loss: 0.0016067089
test_loss: 0.0016424713
train_loss: 0.002147658
test_loss: 0.0019735156
train_loss: 0.0015222677
test_loss: 0.0018868005
train_loss: 0.0015097215
test_loss: 0.0016064183
train_loss: 0.0014903806
test_loss: 0.0017969136
train_loss: 0.0016922746
test_loss: 0.001505116
train_loss: 0.0016726566
test_loss: 0.0021887969
train_loss: 0.0013760906
test_loss: 0.0016504409
train_loss: 0.001709378
test_loss: 0.0015969785
train_loss: 0.0018729742
test_loss: 0.0017861648
train_loss: 0.0015492444
test_loss: 0.001585423
train_loss: 0.0015760302
test_loss: 0.001649873
train_loss: 0.0014495514
test_loss: 0.0015191706
train_loss: 0.0017869922
test_loss: 0.0018864614
train_loss: 0.001761206
test_loss: 0.0021156517
train_loss: 0.0015662911
test_loss: 0.0017466969
train_loss: 0.0020064472
test_loss: 0.0017615614
train_loss: 0.0016644712
test_loss: 0.0016114335
train_loss: 0.0015713078
test_loss: 0.0017106709
train_loss: 0.0016948169
test_loss: 0.0016824155
train_loss: 0.0016120143
test_loss: 0.0015696279
train_loss: 0.0016563793
test_loss: 0.001630859
train_loss: 0.0014369226
test_loss: 0.0013843499
train_loss: 0.0014580477
test_loss: 0.0013461156
train_loss: 0.001629058
test_loss: 0.0018538943
train_loss: 0.0015573909
test_loss: 0.0014201169
train_loss: 0.0015552009
test_loss: 0.0014488866
train_loss: 0.00125279
test_loss: 0.0013614801
train_loss: 0.0015773998
test_loss: 0.0013969357
train_loss: 0.0014909613
test_loss: 0.0015515485
train_loss: 0.0014807436
test_loss: 0.001746128
train_loss: 0.001831281
test_loss: 0.0016882151
train_loss: 0.0015653595
test_loss: 0.001998451
train_loss: 0.0016148051
test_loss: 0.001671084
train_loss: 0.0013571302
test_loss: 0.0014476929
train_loss: 0.0014889971
test_loss: 0.0016643885
train_loss: 0.001616266
test_loss: 0.0014187436
train_loss: 0.001514863
test_loss: 0.0016089982
train_loss: 0.0014825511
test_loss: 0.0016601634
train_loss: 0.0014008313
test_loss: 0.0015862854
train_loss: 0.001678202
test_loss: 0.001606837
train_loss: 0.0014063264
test_loss: 0.0014378119
train_loss: 0.0015040135
test_loss: 0.0015010572
train_loss: 0.0014791754
test_loss: 0.0016107592
train_loss: 0.0013533931
test_loss: 0.0015067608
train_loss: 0.0016981496
test_loss: 0.0014657441
train_loss: 0.0015334378
test_loss: 0.0017094803
train_loss: 0.0018167256
test_loss: 0.0016347872
train_loss: 0.0014203442
test_loss: 0.0016187441
train_loss: 0.0015117088
test_loss: 0.0015525066
train_loss: 0.0015543187
test_loss: 0.0014492383
train_loss: 0.0014590274
test_loss: 0.0013432403
train_loss: 0.0015504617
test_loss: 0.001577946
train_loss: 0.0016386111
test_loss: 0.001490223
train_loss: 0.0015919269
test_loss: 0.0018390053
train_loss: 0.001436933
test_loss: 0.001492549
train_loss: 0.0016541091
test_loss: 0.0017509698
train_loss: 0.0016228138
test_loss: 0.0016980194
train_loss: 0.0014463514
test_loss: 0.0016705908
train_loss: 0.0012550371
test_loss: 0.0013835294
train_loss: 0.001457377
test_loss: 0.0014833568
train_loss: 0.0015047905
test_loss: 0.0015296239
train_loss: 0.0016093969
test_loss: 0.0013756385
train_loss: 0.0012608708
test_loss: 0.001434123
train_loss: 0.001690814
test_loss: 0.001616609
train_loss: 0.0013980505
test_loss: 0.0013808404
train_loss: 0.0017304267
test_loss: 0.0014900482
train_loss: 0.0016442093
test_loss: 0.0017383727
train_loss: 0.0016035688
test_loss: 0.0016672862
train_loss: 0.001699951
test_loss: 0.001395916
train_loss: 0.0014253702
test_loss: 0.0013624275
train_loss: 0.0015505585
test_loss: 0.0013897975
train_loss: 0.0017446792
test_loss: 0.0015159204
train_loss: 0.0015683649
test_loss: 0.0016961771
train_loss: 0.0016817334
test_loss: 0.0015562355
train_loss: 0.0015133785
test_loss: 0.0013989205
train_loss: 0.0017835121
test_loss: 0.0015945291
train_loss: 0.0018691765
test_loss: 0.0016857558
train_loss: 0.0015021773
test_loss: 0.001222532
train_loss: 0.0014149977
test_loss: 0.0013994314
train_loss: 0.0014292074
test_loss: 0.0017725952
train_loss: 0.0013064295
test_loss: 0.001556989
train_loss: 0.0015352789
test_loss: 0.0015697753
train_loss: 0.0016067596
test_loss: 0.0014879855
train_loss: 0.0014163388
test_loss: 0.0015228107
train_loss: 0.0014957119
test_loss: 0.0016224006
train_loss: 0.0015375295
test_loss: 0.0014367609
train_loss: 0.001469916
test_loss: 0.001765552
train_loss: 0.0017126185
test_loss: 0.0017036767
train_loss: 0.0012309423
test_loss: 0.0013365182
train_loss: 0.0013032254
test_loss: 0.0015215301
train_loss: 0.0014518165
test_loss: 0.0014942202
train_loss: 0.0023503336
test_loss: 0.0016641191
train_loss: 0.002608803
test_loss: 0.0018877266
train_loss: 0.0016163946
test_loss: 0.0018367948
train_loss: 0.0014017837
test_loss: 0.0014600441
train_loss: 0.001494664
test_loss: 0.0014045136
train_loss: 0.0013374386
test_loss: 0.0013290385
train_loss: 0.0015582292
test_loss: 0.0015251532
train_loss: 0.0018308467
test_loss: 0.001556598
train_loss: 0.0016534973
test_loss: 0.0017743967
train_loss: 0.0015802435
test_loss: 0.0017543897
train_loss: 0.001415628
test_loss: 0.0014168991
train_loss: 0.0013375932
test_loss: 0.0013343024
train_loss: 0.0015199832
test_loss: 0.0015842433
train_loss: 0.0015959642
test_loss: 0.0013427549
train_loss: 0.001436499
test_loss: 0.0014353557
train_loss: 0.0013819995
test_loss: 0.0014002947
train_loss: 0.0015829769
test_loss: 0.0015794951
train_loss: 0.0014774184
test_loss: 0.0014373057
train_loss: 0.0013282129
test_loss: 0.0017254116
train_loss: 0.0012097623
test_loss: 0.001515211
train_loss: 0.001661952
test_loss: 0.0014381306
train_loss: 0.001312728
test_loss: 0.0015822144
train_loss: 0.0013082575
test_loss: 0.0018800459
train_loss: 0.001391081
test_loss: 0.0014863592
train_loss: 0.0015928118
test_loss: 0.0015750225
train_loss: 0.0017537797
test_loss: 0.0018665208
train_loss: 0.0012924635
test_loss: 0.0014773329
train_loss: 0.001277115
test_loss: 0.0013449511
train_loss: 0.0012470348
test_loss: 0.0014733066
train_loss: 0.0015076578
test_loss: 0.0013687022
train_loss: 0.0014540675
test_loss: 0.0013542802
train_loss: 0.0016027681
test_loss: 0.0014257866
train_loss: 0.0016801774
test_loss: 0.0016586503
train_loss: 0.001751445
test_loss: 0.0015606087
train_loss: 0.0014691923
test_loss: 0.0014452144
train_loss: 0.0015131987
test_loss: 0.0015802769
train_loss: 0.001503384
test_loss: 0.0018719181
train_loss: 0.0017264546
test_loss: 0.0013557656
train_loss: 0.0014724327
test_loss: 0.0017873181
train_loss: 0.0014489254
test_loss: 0.0013847906
train_loss: 0.0014253183
test_loss: 0.0015007646
train_loss: 0.0015602747
test_loss: 0.0014432371
train_loss: 0.0015074848
test_loss: 0.0014293577
train_loss: 0.0013720643
test_loss: 0.001488991
train_loss: 0.0015257541
test_loss: 0.0014205354
train_loss: 0.0014771908
test_loss: 0.0014787721
train_loss: 0.0013509787
test_loss: 0.001591693
train_loss: 0.0014236748
test_loss: 0.0014284026
train_loss: 0.0013872355
test_loss: 0.001455978
train_loss: 0.0014261579
test_loss: 0.0015696393
train_loss: 0.0014680445
test_loss: 0.0014678705
train_loss: 0.0014584918
test_loss: 0.0012828221
train_loss: 0.0014092923
test_loss: 0.0014622549
train_loss: 0.0016629192
test_loss: 0.002077664
train_loss: 0.0012272447
test_loss: 0.0013067513
train_loss: 0.0013241839
test_loss: 0.0013261979
train_loss: 0.0013409485
test_loss: 0.001373854
train_loss: 0.0013725562
test_loss: 0.0017130424
train_loss: 0.0012811273
test_loss: 0.0013760404
train_loss: 0.0016551536
test_loss: 0.001437969
train_loss: 0.0017232173
test_loss: 0.0015198152
train_loss: 0.0011188997
test_loss: 0.0013288583
train_loss: 0.0013750118
test_loss: 0.0012872227
train_loss: 0.0013886306
test_loss: 0.0013513728
train_loss: 0.0012216235
test_loss: 0.0020118302
train_loss: 0.0014584046
test_loss: 0.0014501421
train_loss: 0.0012802893
test_loss: 0.001490633
train_loss: 0.0012149888
test_loss: 0.0015685506
train_loss: 0.001452424
test_loss: 0.0015796556
train_loss: 0.0015049299
test_loss: 0.0015773306
train_loss: 0.0013575486
test_loss: 0.0016471026
train_loss: 0.0013966772
test_loss: 0.0013467553
train_loss: 0.0015000864
test_loss: 0.0013763062
train_loss: 0.0017367135
test_loss: 0.0016560302
train_loss: 0.0016022018
test_loss: 0.0013448964
train_loss: 0.0014164273
test_loss: 0.001395188
train_loss: 0.0017147409
test_loss: 0.0014445644
train_loss: 0.0015340913
test_loss: 0.001800333
train_loss: 0.0015356683
test_loss: 0.0017019445
train_loss: 0.0012673779
test_loss: 0.0016846311
train_loss: 0.0013083556
test_loss: 0.0014498349
train_loss: 0.0013396814
test_loss: 0.0013308355
train_loss: 0.0013817691
test_loss: 0.0014955116
train_loss: 0.0014225261
test_loss: 0.0015760276
train_loss: 0.0014239026
test_loss: 0.0015069224
train_loss: 0.0015501555
test_loss: 0.001402445
train_loss: 0.0013297653
test_loss: 0.0016203086
train_loss: 0.0014156015
test_loss: 0.0015175316
train_loss: 0.0017282407
test_loss: 0.0015751172
train_loss: 0.0013798834
test_loss: 0.0014387487
train_loss: 0.00164651
test_loss: 0.0013779294
train_loss: 0.0013040913
test_loss: 0.0015441581
train_loss: 0.001321745
test_loss: 0.0016125641
train_loss: 0.0012692787
test_loss: 0.001463056
train_loss: 0.0014123239
test_loss: 0.0013847015
train_loss: 0.0014966748
test_loss: 0.0016488127
train_loss: 0.00132473
test_loss: 0.0014840197
train_loss: 0.0014807275
test_loss: 0.0014542574
train_loss: 0.0013155788
test_loss: 0.0014679137
train_loss: 0.0015213012
test_loss: 0.0015434369
train_loss: 0.0013252127
test_loss: 0.0013311154
train_loss: 0.0016238054
test_loss: 0.0014897101
train_loss: 0.0012908031
test_loss: 0.001543065
train_loss: 0.0015264873
test_loss: 0.0013498341
train_loss: 0.001633761
test_loss: 0.0015430395
train_loss: 0.0015739414
test_loss: 0.001604052
train_loss: 0.0014889279
test_loss: 0.0015858116
train_loss: 0.0012994565
test_loss: 0.0015306241
train_loss: 0.0011770177
test_loss: 0.0015719658
train_loss: 0.0015402364
test_loss: 0.001448493
train_loss: 0.0013432668
test_loss: 0.0014188776
train_loss: 0.0014614437
test_loss: 0.001577554
train_loss: 0.0016257821
test_loss: 0.0015292971
train_loss: 0.0012441145
test_loss: 0.0014539397
train_loss: 0.0012330823
test_loss: 0.001587862
train_loss: 0.0015449944
test_loss: 0.0014051842
train_loss: 0.0016765755
test_loss: 0.0013654524
train_loss: 0.0014912055
test_loss: 0.0014825221
train_loss: 0.0015913723
test_loss: 0.0015946341
train_loss: 0.0016611174
test_loss: 0.0015201087
train_loss: 0.001596886
test_loss: 0.001477695
train_loss: 0.0014049276
test_loss: 0.0017262369
train_loss: 0.0015660375
test_loss: 0.0016791009
train_loss: 0.0015475758
test_loss: 0.0014521085
train_loss: 0.0014775747
test_loss: 0.0015231619
train_loss: 0.001531827
test_loss: 0.001578575
train_loss: 0.0013302042
test_loss: 0.0012758786
train_loss: 0.0023450942
test_loss: 0.0018895536
train_loss: 0.00165673
test_loss: 0.0016972204
train_loss: 0.0016792068
test_loss: 0.0017403943
train_loss: 0.0012754467
test_loss: 0.0017439779
train_loss: 0.0014055311
test_loss: 0.001640926
train_loss: 0.001463078
test_loss: 0.0014381867
train_loss: 0.0013781944
test_loss: 0.001575314
train_loss: 0.0015353513
test_loss: 0.0015445414
train_loss: 0.001468888
test_loss: 0.001593919
train_loss: 0.0017637839
test_loss: 0.0013798762
train_loss: 0.0014539842
test_loss: 0.0015947677
train_loss: 0.001204069
test_loss: 0.0014493972
train_loss: 0.0016517015
test_loss: 0.0015245315
train_loss: 0.0012903515
test_loss: 0.0013461056
train_loss: 0.0014456187
test_loss: 0.0014642882
train_loss: 0.0013949127
test_loss: 0.0015395893
train_loss: 0.001674594
test_loss: 0.0014605705
train_loss: 0.0015067017
test_loss: 0.0014543493
train_loss: 0.001599745
test_loss: 0.0014653178
train_loss: 0.001472912
test_loss: 0.0012640479
train_loss: 0.0014644118
test_loss: 0.001668843
train_loss: 0.001297433
test_loss: 0.0014600877
train_loss: 0.0014413868
test_loss: 0.0013990005
train_loss: 0.0013880215
test_loss: 0.0013581938
train_loss: 0.001424531
test_loss: 0.0015948932
train_loss: 0.0015312403
test_loss: 0.0013536598
train_loss: 0.0012945663
test_loss: 0.0013702991
train_loss: 0.0012877707
test_loss: 0.0016350829
train_loss: 0.001673737
test_loss: 0.0014148107
train_loss: 0.0014583343
test_loss: 0.0013328424
train_loss: 0.0015257685
test_loss: 0.0016189489
train_loss: 0.0017850285
test_loss: 0.001439888
train_loss: 0.0015748064
test_loss: 0.0014320102
train_loss: 0.0014352797
test_loss: 0.0022009197
train_loss: 0.0015730768
test_loss: 0.0018778224
train_loss: 0.001388762
test_loss: 0.0017689847
train_loss: 0.0015698159
test_loss: 0.0014469388
train_loss: 0.0018500795
test_loss: 0.0019437424
train_loss: 0.0015289283
test_loss: 0.0017706858
train_loss: 0.0014095416
test_loss: 0.0016104057
train_loss: 0.0015032617
test_loss: 0.0016745807
train_loss: 0.0013683154
test_loss: 0.0015659808
train_loss: 0.0012371747
test_loss: 0.0016886488
train_loss: 0.0010801178
test_loss: 0.0014051244
train_loss: 0.0014672494
test_loss: 0.0014341665
train_loss: 0.0014049782
test_loss: 0.0013024503
train_loss: 0.0016235135
test_loss: 0.0013583913
train_loss: 0.0014002282
test_loss: 0.0014787273
train_loss: 0.0015500518
test_loss: 0.001346413
train_loss: 0.0016560353
test_loss: 0.0018179382
train_loss: 0.0014400118
test_loss: 0.0013906902
train_loss: 0.001267285
test_loss: 0.001520405
train_loss: 0.001494358
test_loss: 0.0014571564
train_loss: 0.0016935156
test_loss: 0.0013008433
train_loss: 0.0015551371
test_loss: 0.0015430052
train_loss: 0.0012946528
test_loss: 0.001511084
train_loss: 0.001231489
test_loss: 0.0013668917
train_loss: 0.0011912171
test_loss: 0.0013586886
train_loss: 0.0013569667
test_loss: 0.0015666463
train_loss: 0.0015890767
test_loss: 0.0016141132
train_loss: 0.0015018224
test_loss: 0.0016127885
train_loss: 0.0014001866
test_loss: 0.0015247817
train_loss: 0.0013752072
test_loss: 0.0014078543
train_loss: 0.0014358342
test_loss: 0.001406772
train_loss: 0.0012610734
test_loss: 0.0016445891
train_loss: 0.0012929275
test_loss: 0.0014723045
train_loss: 0.0013340383
test_loss: 0.0013695102
train_loss: 0.0015712176
test_loss: 0.0014495171
train_loss: 0.0016710183
test_loss: 0.0014492733
train_loss: 0.0013828295
test_loss: 0.0012719245
train_loss: 0.001337366
test_loss: 0.0012841054
train_loss: 0.0013890099
test_loss: 0.0014993114
train_loss: 0.0016550196
test_loss: 0.0017657408
train_loss: 0.0016594417
test_loss: 0.0018461663
train_loss: 0.0014588227
test_loss: 0.0014902338
train_loss: 0.001328009
test_loss: 0.0013774114
train_loss: 0.001249661
test_loss: 0.0015175005
train_loss: 0.0011882805
test_loss: 0.0014186902
train_loss: 0.0011757598
test_loss: 0.0015121072
train_loss: 0.0013858499
test_loss: 0.0014676728
train_loss: 0.0012283709
test_loss: 0.0013619843
train_loss: 0.0013463935
test_loss: 0.0015701469
train_loss: 0.0013195822
test_loss: 0.0012516876
train_loss: 0.0012783476
test_loss: 0.0013913248
train_loss: 0.0013592832
test_loss: 0.0011999622
train_loss: 0.0014173773
test_loss: 0.0015812849
train_loss: 0.001510446
test_loss: 0.0015270007
train_loss: 0.0014136506
test_loss: 0.0013825179
train_loss: 0.0015101341
test_loss: 0.0013817635
train_loss: 0.0012957482
test_loss: 0.0015811903
train_loss: 0.0013152575
test_loss: 0.0014296628
train_loss: 0.001495761
test_loss: 0.0016257579
train_loss: 0.0016931781
test_loss: 0.0013319232
train_loss: 0.0015897492
test_loss: 0.0017528173
train_loss: 0.0014400952
test_loss: 0.001503598
train_loss: 0.0013032975
test_loss: 0.0014513121
train_loss: 0.0013001093
test_loss: 0.001498144
train_loss: 0.0012787429
test_loss: 0.0012396291
train_loss: 0.0013602634
test_loss: 0.0015290142
train_loss: 0.0013593336
test_loss: 0.0015086288
train_loss: 0.0012473094
test_loss: 0.001448503
train_loss: 0.0012267046
test_loss: 0.0014542518
train_loss: 0.0013612706
test_loss: 0.0014592008
train_loss: 0.0017338346
test_loss: 0.0016979277
train_loss: 0.0016106345
test_loss: 0.0015233716
train_loss: 0.0014063254
test_loss: 0.0013705391
train_loss: 0.001175853
test_loss: 0.0014215696/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

train_loss: 0.001349858
test_loss: 0.0013502991
train_loss: 0.0014397176
test_loss: 0.0017013672
train_loss: 0.0016301727
test_loss: 0.001376211
train_loss: 0.0013760102
test_loss: 0.0014470856
train_loss: 0.0016027327
test_loss: 0.0014296329
train_loss: 0.0013652998
test_loss: 0.0015763638
train_loss: 0.0015627985
test_loss: 0.001286689
train_loss: 0.0013419521
test_loss: 0.0013872469
train_loss: 0.0016420328
test_loss: 0.0015574339
train_loss: 0.0012126678
test_loss: 0.0014242113
train_loss: 0.0013711075
test_loss: 0.0015151392
train_loss: 0.0013295023
test_loss: 0.0014271217
train_loss: 0.001337578
test_loss: 0.001578896
train_loss: 0.0017197067
test_loss: 0.0017009174
train_loss: 0.0015556072
test_loss: 0.0014852275
train_loss: 0.0013148505
test_loss: 0.0016218142
train_loss: 0.0012175441
test_loss: 0.001438189
train_loss: 0.0014259994
test_loss: 0.0015390974
train_loss: 0.0018140245
test_loss: 0.0014808159
train_loss: 0.0014407875
test_loss: 0.0012667786
train_loss: 0.0012273603
test_loss: 0.0014349947
train_loss: 0.0013871237
test_loss: 0.0014822838
train_loss: 0.0011827433
test_loss: 0.0013730731
train_loss: 0.0013957564
test_loss: 0.0013619129
train_loss: 0.0012280317
test_loss: 0.0013735476
train_loss: 0.001273159
test_loss: 0.0015209181
train_loss: 0.001491286
test_loss: 0.0017209142
train_loss: 0.0014648715
test_loss: 0.0015958942
train_loss: 0.0013410053
test_loss: 0.0012974691
train_loss: 0.0013341719
test_loss: 0.0015014716
train_loss: 0.0014652575
test_loss: 0.001424128
train_loss: 0.0012585017
test_loss: 0.0018846134
train_loss: 0.001580913
test_loss: 0.0013647787
train_loss: 0.0014381025
test_loss: 0.0015004886
train_loss: 0.0013167432
test_loss: 0.0015474143
train_loss: 0.0013590797
test_loss: 0.0013454137
train_loss: 0.0014641962
test_loss: 0.0015012856
train_loss: 0.0012406509
test_loss: 0.0014278528
train_loss: 0.001400263
test_loss: 0.0015334415
train_loss: 0.001547323
test_loss: 0.0014108345
train_loss: 0.0012785761
test_loss: 0.001683291
train_loss: 0.001487056
test_loss: 0.0015031291
train_loss: 0.00146004
test_loss: 0.0013781269
train_loss: 0.0013757108
test_loss: 0.0016895435
train_loss: 0.0013156289
test_loss: 0.0015654813
train_loss: 0.0014685986
test_loss: 0.0015898355
train_loss: 0.0013529727
test_loss: 0.00141687
train_loss: 0.0012826306
test_loss: 0.0014425192
train_loss: 0.001205623
test_loss: 0.0015049645
train_loss: 0.0013970996
test_loss: 0.0014691093
train_loss: 0.0014665725
test_loss: 0.0015131495
train_loss: 0.0013352314
test_loss: 0.0013118511
train_loss: 0.0013581663
test_loss: 0.0019375617
train_loss: 0.0025674752
test_loss: 0.002571135
train_loss: 0.0024057073
test_loss: 0.002015226
train_loss: 0.0019050664
test_loss: 0.0017968379
train_loss: 0.0012661112
test_loss: 0.0015596214
train_loss: 0.0014077759
test_loss: 0.0013998222
train_loss: 0.0016229561
test_loss: 0.001493405
train_loss: 0.0014516211
test_loss: 0.0016304293
train_loss: 0.001349327
test_loss: 0.0014355896
train_loss: 0.0013206949
test_loss: 0.0013440814
train_loss: 0.0013415691
test_loss: 0.0014542189
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc02a3378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc02c7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc02536a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc032a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc0209f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc0209268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc0209158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc01806a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc01809d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc0180268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc01641e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc010dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc0121a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc00e0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc0121d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc009e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc009ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc009e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc009ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa06f0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa070ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa06a50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa070c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa06dba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0692488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0624b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0624730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0659378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0659048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa05b0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa05c5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0591f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0520598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa057c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa050e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa04b0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.04875857e-06
Iter: 2 loss: 3.35923914e-06
Iter: 3 loss: 1.95891971e-06
Iter: 4 loss: 1.6219991e-06
Iter: 5 loss: 2.03794548e-06
Iter: 6 loss: 1.44654177e-06
Iter: 7 loss: 1.28987017e-06
Iter: 8 loss: 3.00532565e-06
Iter: 9 loss: 1.28655586e-06
Iter: 10 loss: 1.19389711e-06
Iter: 11 loss: 1.14961711e-06
Iter: 12 loss: 1.10464157e-06
Iter: 13 loss: 1.04875971e-06
Iter: 14 loss: 1.37738857e-06
Iter: 15 loss: 1.04147784e-06
Iter: 16 loss: 1.00935381e-06
Iter: 17 loss: 1.18029175e-06
Iter: 18 loss: 1.00451666e-06
Iter: 19 loss: 9.85606903e-07
Iter: 20 loss: 1.24373355e-06
Iter: 21 loss: 9.85559723e-07
Iter: 22 loss: 9.75490138e-07
Iter: 23 loss: 9.67629376e-07
Iter: 24 loss: 9.64467176e-07
Iter: 25 loss: 9.56505801e-07
Iter: 26 loss: 9.55882911e-07
Iter: 27 loss: 9.49360413e-07
Iter: 28 loss: 9.40651319e-07
Iter: 29 loss: 9.40162295e-07
Iter: 30 loss: 9.29916951e-07
Iter: 31 loss: 9.28597672e-07
Iter: 32 loss: 9.21252592e-07
Iter: 33 loss: 9.13004e-07
Iter: 34 loss: 9.97450684e-07
Iter: 35 loss: 9.12760697e-07
Iter: 36 loss: 9.07943e-07
Iter: 37 loss: 9.07786159e-07
Iter: 38 loss: 9.04421768e-07
Iter: 39 loss: 8.99043243e-07
Iter: 40 loss: 8.98984581e-07
Iter: 41 loss: 8.9368433e-07
Iter: 42 loss: 8.9367694e-07
Iter: 43 loss: 8.90606884e-07
Iter: 44 loss: 8.85479324e-07
Iter: 45 loss: 8.85435838e-07
Iter: 46 loss: 8.79395429e-07
Iter: 47 loss: 8.90848071e-07
Iter: 48 loss: 8.76841113e-07
Iter: 49 loss: 8.70562076e-07
Iter: 50 loss: 9.04063086e-07
Iter: 51 loss: 8.6962558e-07
Iter: 52 loss: 8.63945559e-07
Iter: 53 loss: 8.97529503e-07
Iter: 54 loss: 8.63249568e-07
Iter: 55 loss: 8.58875751e-07
Iter: 56 loss: 8.61548756e-07
Iter: 57 loss: 8.56035172e-07
Iter: 58 loss: 8.51999175e-07
Iter: 59 loss: 8.89348826e-07
Iter: 60 loss: 8.51802724e-07
Iter: 61 loss: 8.47568117e-07
Iter: 62 loss: 8.44542967e-07
Iter: 63 loss: 8.43037469e-07
Iter: 64 loss: 8.38947585e-07
Iter: 65 loss: 8.46374576e-07
Iter: 66 loss: 8.37175492e-07
Iter: 67 loss: 8.33233798e-07
Iter: 68 loss: 8.67888446e-07
Iter: 69 loss: 8.330436e-07
Iter: 70 loss: 8.29434498e-07
Iter: 71 loss: 8.60044793e-07
Iter: 72 loss: 8.29248734e-07
Iter: 73 loss: 8.28264092e-07
Iter: 74 loss: 8.29223268e-07
Iter: 75 loss: 8.27756082e-07
Iter: 76 loss: 8.26029577e-07
Iter: 77 loss: 8.25465293e-07
Iter: 78 loss: 8.24485937e-07
Iter: 79 loss: 8.2256463e-07
Iter: 80 loss: 8.24997414e-07
Iter: 81 loss: 8.21604374e-07
Iter: 82 loss: 8.19639467e-07
Iter: 83 loss: 8.22855384e-07
Iter: 84 loss: 8.18762032e-07
Iter: 85 loss: 8.16963166e-07
Iter: 86 loss: 8.40049779e-07
Iter: 87 loss: 8.16949182e-07
Iter: 88 loss: 8.15512067e-07
Iter: 89 loss: 8.17044679e-07
Iter: 90 loss: 8.14729447e-07
Iter: 91 loss: 8.12840483e-07
Iter: 92 loss: 8.13305e-07
Iter: 93 loss: 8.11499376e-07
Iter: 94 loss: 8.09694143e-07
Iter: 95 loss: 8.09685275e-07
Iter: 96 loss: 8.08574896e-07
Iter: 97 loss: 8.06167805e-07
Iter: 98 loss: 8.45636464e-07
Iter: 99 loss: 8.06102889e-07
Iter: 100 loss: 8.03696707e-07
Iter: 101 loss: 8.16736247e-07
Iter: 102 loss: 8.03332739e-07
Iter: 103 loss: 8.02628335e-07
Iter: 104 loss: 8.02247428e-07
Iter: 105 loss: 8.01368628e-07
Iter: 106 loss: 7.99191184e-07
Iter: 107 loss: 8.2202871e-07
Iter: 108 loss: 7.98944711e-07
Iter: 109 loss: 7.97269195e-07
Iter: 110 loss: 7.97217581e-07
Iter: 111 loss: 7.95919732e-07
Iter: 112 loss: 7.9417714e-07
Iter: 113 loss: 7.94086475e-07
Iter: 114 loss: 7.92336e-07
Iter: 115 loss: 7.9457925e-07
Iter: 116 loss: 7.91422281e-07
Iter: 117 loss: 7.89695036e-07
Iter: 118 loss: 8.03347575e-07
Iter: 119 loss: 7.89621708e-07
Iter: 120 loss: 7.88332841e-07
Iter: 121 loss: 7.96343045e-07
Iter: 122 loss: 7.88200566e-07
Iter: 123 loss: 7.87129181e-07
Iter: 124 loss: 7.88652414e-07
Iter: 125 loss: 7.86576209e-07
Iter: 126 loss: 7.85798306e-07
Iter: 127 loss: 7.92577225e-07
Iter: 128 loss: 7.85779434e-07
Iter: 129 loss: 7.8508134e-07
Iter: 130 loss: 7.85811835e-07
Iter: 131 loss: 7.8468554e-07
Iter: 132 loss: 7.83881717e-07
Iter: 133 loss: 7.83471705e-07
Iter: 134 loss: 7.83146902e-07
Iter: 135 loss: 7.8234109e-07
Iter: 136 loss: 7.90280865e-07
Iter: 137 loss: 7.82308121e-07
Iter: 138 loss: 7.81376457e-07
Iter: 139 loss: 7.84147232e-07
Iter: 140 loss: 7.81120093e-07
Iter: 141 loss: 7.8051761e-07
Iter: 142 loss: 7.80506184e-07
Iter: 143 loss: 7.80022106e-07
Iter: 144 loss: 7.79145296e-07
Iter: 145 loss: 7.85294731e-07
Iter: 146 loss: 7.79065772e-07
Iter: 147 loss: 7.7841969e-07
Iter: 148 loss: 7.77153218e-07
Iter: 149 loss: 8.01214924e-07
Iter: 150 loss: 7.77170385e-07
Iter: 151 loss: 7.75995204e-07
Iter: 152 loss: 7.79756078e-07
Iter: 153 loss: 7.75713147e-07
Iter: 154 loss: 7.74545128e-07
Iter: 155 loss: 7.86041255e-07
Iter: 156 loss: 7.74477257e-07
Iter: 157 loss: 7.7377581e-07
Iter: 158 loss: 7.77607738e-07
Iter: 159 loss: 7.73663714e-07
Iter: 160 loss: 7.73051738e-07
Iter: 161 loss: 7.72787075e-07
Iter: 162 loss: 7.7244772e-07
Iter: 163 loss: 7.71564032e-07
Iter: 164 loss: 7.80530456e-07
Iter: 165 loss: 7.71513612e-07
Iter: 166 loss: 7.70962e-07
Iter: 167 loss: 7.7074219e-07
Iter: 168 loss: 7.70437282e-07
Iter: 169 loss: 7.69603616e-07
Iter: 170 loss: 7.69548933e-07
Iter: 171 loss: 7.68871132e-07
Iter: 172 loss: 7.69190649e-07
Iter: 173 loss: 7.68465725e-07
Iter: 174 loss: 7.68157861e-07
Iter: 175 loss: 7.67428958e-07
Iter: 176 loss: 7.74616524e-07
Iter: 177 loss: 7.67334313e-07
Iter: 178 loss: 7.66972448e-07
Iter: 179 loss: 7.66895596e-07
Iter: 180 loss: 7.6657625e-07
Iter: 181 loss: 7.6626111e-07
Iter: 182 loss: 7.66215692e-07
Iter: 183 loss: 7.65742129e-07
Iter: 184 loss: 7.65044e-07
Iter: 185 loss: 7.65033747e-07
Iter: 186 loss: 7.64454285e-07
Iter: 187 loss: 7.6444303e-07
Iter: 188 loss: 7.63961737e-07
Iter: 189 loss: 7.6530938e-07
Iter: 190 loss: 7.63772391e-07
Iter: 191 loss: 7.63181106e-07
Iter: 192 loss: 7.63529442e-07
Iter: 193 loss: 7.62817308e-07
Iter: 194 loss: 7.62233356e-07
Iter: 195 loss: 7.65914876e-07
Iter: 196 loss: 7.62178388e-07
Iter: 197 loss: 7.61610863e-07
Iter: 198 loss: 7.62717605e-07
Iter: 199 loss: 7.6138457e-07
Iter: 200 loss: 7.60876446e-07
Iter: 201 loss: 7.60476269e-07
Iter: 202 loss: 7.60332284e-07
Iter: 203 loss: 7.59789373e-07
Iter: 204 loss: 7.68016946e-07
Iter: 205 loss: 7.5977897e-07
Iter: 206 loss: 7.59211844e-07
Iter: 207 loss: 7.61969034e-07
Iter: 208 loss: 7.59149316e-07
Iter: 209 loss: 7.58829856e-07
Iter: 210 loss: 7.58404e-07
Iter: 211 loss: 7.58388069e-07
Iter: 212 loss: 7.57856242e-07
Iter: 213 loss: 7.63792627e-07
Iter: 214 loss: 7.57802468e-07
Iter: 215 loss: 7.57416728e-07
Iter: 216 loss: 7.56458405e-07
Iter: 217 loss: 7.69220605e-07
Iter: 218 loss: 7.56420263e-07
Iter: 219 loss: 7.55489168e-07
Iter: 220 loss: 7.57733062e-07
Iter: 221 loss: 7.55158283e-07
Iter: 222 loss: 7.54232872e-07
Iter: 223 loss: 7.6465858e-07
Iter: 224 loss: 7.54237647e-07
Iter: 225 loss: 7.53668303e-07
Iter: 226 loss: 7.5857389e-07
Iter: 227 loss: 7.53590825e-07
Iter: 228 loss: 7.53299048e-07
Iter: 229 loss: 7.53121071e-07
Iter: 230 loss: 7.52995163e-07
Iter: 231 loss: 7.52479423e-07
Iter: 232 loss: 7.55626616e-07
Iter: 233 loss: 7.52420476e-07
Iter: 234 loss: 7.51973857e-07
Iter: 235 loss: 7.52918254e-07
Iter: 236 loss: 7.51830498e-07
Iter: 237 loss: 7.51435493e-07
Iter: 238 loss: 7.50784409e-07
Iter: 239 loss: 7.50788445e-07
Iter: 240 loss: 7.51077209e-07
Iter: 241 loss: 7.50502295e-07
Iter: 242 loss: 7.50257641e-07
Iter: 243 loss: 7.49726382e-07
Iter: 244 loss: 7.59023123e-07
Iter: 245 loss: 7.49705521e-07
Iter: 246 loss: 7.49243782e-07
Iter: 247 loss: 7.50997856e-07
Iter: 248 loss: 7.49146807e-07
Iter: 249 loss: 7.48607874e-07
Iter: 250 loss: 7.51245352e-07
Iter: 251 loss: 7.48542448e-07
Iter: 252 loss: 7.48165519e-07
Iter: 253 loss: 7.47138472e-07
Iter: 254 loss: 7.5516607e-07
Iter: 255 loss: 7.46951e-07
Iter: 256 loss: 7.46225169e-07
Iter: 257 loss: 7.55736664e-07
Iter: 258 loss: 7.46235401e-07
Iter: 259 loss: 7.45658838e-07
Iter: 260 loss: 7.47912736e-07
Iter: 261 loss: 7.45540433e-07
Iter: 262 loss: 7.45005082e-07
Iter: 263 loss: 7.50952665e-07
Iter: 264 loss: 7.45025318e-07
Iter: 265 loss: 7.44743943e-07
Iter: 266 loss: 7.44693807e-07
Iter: 267 loss: 7.44528336e-07
Iter: 268 loss: 7.44051704e-07
Iter: 269 loss: 7.45507e-07
Iter: 270 loss: 7.4392517e-07
Iter: 271 loss: 7.43521127e-07
Iter: 272 loss: 7.43664032e-07
Iter: 273 loss: 7.43227133e-07
Iter: 274 loss: 7.42699e-07
Iter: 275 loss: 7.43363e-07
Iter: 276 loss: 7.42447185e-07
Iter: 277 loss: 7.41897395e-07
Iter: 278 loss: 7.42735e-07
Iter: 279 loss: 7.41642964e-07
Iter: 280 loss: 7.41386202e-07
Iter: 281 loss: 7.41226359e-07
Iter: 282 loss: 7.41000179e-07
Iter: 283 loss: 7.40445842e-07
Iter: 284 loss: 7.45740181e-07
Iter: 285 loss: 7.40367796e-07
Iter: 286 loss: 7.39857683e-07
Iter: 287 loss: 7.40388771e-07
Iter: 288 loss: 7.39588927e-07
Iter: 289 loss: 7.39226493e-07
Iter: 290 loss: 7.39213078e-07
Iter: 291 loss: 7.38825634e-07
Iter: 292 loss: 7.38805625e-07
Iter: 293 loss: 7.38522e-07
Iter: 294 loss: 7.38154e-07
Iter: 295 loss: 7.37377263e-07
Iter: 296 loss: 7.48935065e-07
Iter: 297 loss: 7.37338326e-07
Iter: 298 loss: 7.36999368e-07
Iter: 299 loss: 7.3687653e-07
Iter: 300 loss: 7.36470952e-07
Iter: 301 loss: 7.38167159e-07
Iter: 302 loss: 7.36381367e-07
Iter: 303 loss: 7.36033144e-07
Iter: 304 loss: 7.35681169e-07
Iter: 305 loss: 7.35603408e-07
Iter: 306 loss: 7.35323681e-07
Iter: 307 loss: 7.35290314e-07
Iter: 308 loss: 7.35082722e-07
Iter: 309 loss: 7.34510536e-07
Iter: 310 loss: 7.37137839e-07
Iter: 311 loss: 7.34313915e-07
Iter: 312 loss: 7.33648506e-07
Iter: 313 loss: 7.36751872e-07
Iter: 314 loss: 7.33525667e-07
Iter: 315 loss: 7.3352345e-07
Iter: 316 loss: 7.332373e-07
Iter: 317 loss: 7.32991339e-07
Iter: 318 loss: 7.32662215e-07
Iter: 319 loss: 7.32695298e-07
Iter: 320 loss: 7.32262151e-07
Iter: 321 loss: 7.31857199e-07
Iter: 322 loss: 7.31757893e-07
Iter: 323 loss: 7.3113722e-07
Iter: 324 loss: 7.37013806e-07
Iter: 325 loss: 7.31085834e-07
Iter: 326 loss: 7.30792635e-07
Iter: 327 loss: 7.30782631e-07
Iter: 328 loss: 7.3055e-07
Iter: 329 loss: 7.2998148e-07
Iter: 330 loss: 7.3858206e-07
Iter: 331 loss: 7.29983697e-07
Iter: 332 loss: 7.29557712e-07
Iter: 333 loss: 7.3056151e-07
Iter: 334 loss: 7.29383657e-07
Iter: 335 loss: 7.28935106e-07
Iter: 336 loss: 7.32551939e-07
Iter: 337 loss: 7.28909072e-07
Iter: 338 loss: 7.28535724e-07
Iter: 339 loss: 7.30889496e-07
Iter: 340 loss: 7.28465523e-07
Iter: 341 loss: 7.28235875e-07
Iter: 342 loss: 7.28278678e-07
Iter: 343 loss: 7.28056364e-07
Iter: 344 loss: 7.27742872e-07
Iter: 345 loss: 7.30682359e-07
Iter: 346 loss: 7.27755435e-07
Iter: 347 loss: 7.27542329e-07
Iter: 348 loss: 7.27187626e-07
Iter: 349 loss: 7.27194731e-07
Iter: 350 loss: 7.26759538e-07
Iter: 351 loss: 7.26932512e-07
Iter: 352 loss: 7.26415578e-07
Iter: 353 loss: 7.2628211e-07
Iter: 354 loss: 7.26194969e-07
Iter: 355 loss: 7.25929794e-07
Iter: 356 loss: 7.25859707e-07
Iter: 357 loss: 7.25682412e-07
Iter: 358 loss: 7.25356927e-07
Iter: 359 loss: 7.24887968e-07
Iter: 360 loss: 7.24874667e-07
Iter: 361 loss: 7.24291624e-07
Iter: 362 loss: 7.24959307e-07
Iter: 363 loss: 7.23988e-07
Iter: 364 loss: 7.23575852e-07
Iter: 365 loss: 7.2356795e-07
Iter: 366 loss: 7.2331062e-07
Iter: 367 loss: 7.25400923e-07
Iter: 368 loss: 7.23308233e-07
Iter: 369 loss: 7.23050618e-07
Iter: 370 loss: 7.23063408e-07
Iter: 371 loss: 7.22827224e-07
Iter: 372 loss: 7.22563868e-07
Iter: 373 loss: 7.22692562e-07
Iter: 374 loss: 7.22373841e-07
Iter: 375 loss: 7.22064897e-07
Iter: 376 loss: 7.23314542e-07
Iter: 377 loss: 7.22011123e-07
Iter: 378 loss: 7.2173458e-07
Iter: 379 loss: 7.25703501e-07
Iter: 380 loss: 7.21755555e-07
Iter: 381 loss: 7.21614811e-07
Iter: 382 loss: 7.2137243e-07
Iter: 383 loss: 7.21389881e-07
Iter: 384 loss: 7.21095375e-07
Iter: 385 loss: 7.24340794e-07
Iter: 386 loss: 7.21070535e-07
Iter: 387 loss: 7.20929961e-07
Iter: 388 loss: 7.20633e-07
Iter: 389 loss: 7.20633921e-07
Iter: 390 loss: 7.20451169e-07
Iter: 391 loss: 7.20403705e-07
Iter: 392 loss: 7.20162518e-07
Iter: 393 loss: 7.1969157e-07
Iter: 394 loss: 7.29441126e-07
Iter: 395 loss: 7.19677701e-07
Iter: 396 loss: 7.19365573e-07
Iter: 397 loss: 7.18971421e-07
Iter: 398 loss: 7.18918727e-07
Iter: 399 loss: 7.1864639e-07
Iter: 400 loss: 7.18617514e-07
Iter: 401 loss: 7.18334377e-07
Iter: 402 loss: 7.186311e-07
Iter: 403 loss: 7.18209662e-07
Iter: 404 loss: 7.17921807e-07
Iter: 405 loss: 7.1812633e-07
Iter: 406 loss: 7.17709099e-07
Iter: 407 loss: 7.17431135e-07
Iter: 408 loss: 7.18866431e-07
Iter: 409 loss: 7.17408454e-07
Iter: 410 loss: 7.17232183e-07
Iter: 411 loss: 7.18865408e-07
Iter: 412 loss: 7.1720126e-07
Iter: 413 loss: 7.17002308e-07
Iter: 414 loss: 7.16871796e-07
Iter: 415 loss: 7.16820352e-07
Iter: 416 loss: 7.16572288e-07
Iter: 417 loss: 7.18636727e-07
Iter: 418 loss: 7.16539603e-07
Iter: 419 loss: 7.16286593e-07
Iter: 420 loss: 7.1632428e-07
Iter: 421 loss: 7.16098555e-07
Iter: 422 loss: 7.15822921e-07
Iter: 423 loss: 7.16404543e-07
Iter: 424 loss: 7.15739532e-07
Iter: 425 loss: 7.15541717e-07
Iter: 426 loss: 7.15523811e-07
Iter: 427 loss: 7.15417059e-07
Iter: 428 loss: 7.15095666e-07
Iter: 429 loss: 7.16541535e-07
Iter: 430 loss: 7.14981638e-07
Iter: 431 loss: 7.14617158e-07
Iter: 432 loss: 7.16035061e-07
Iter: 433 loss: 7.14524504e-07
Iter: 434 loss: 7.14167e-07
Iter: 435 loss: 7.1554075e-07
Iter: 436 loss: 7.14079306e-07
Iter: 437 loss: 7.13869724e-07
Iter: 438 loss: 7.13854149e-07
Iter: 439 loss: 7.1374086e-07
Iter: 440 loss: 7.13450731e-07
Iter: 441 loss: 7.17677153e-07
Iter: 442 loss: 7.13431803e-07
Iter: 443 loss: 7.13118254e-07
Iter: 444 loss: 7.14439466e-07
Iter: 445 loss: 7.13038389e-07
Iter: 446 loss: 7.12763324e-07
Iter: 447 loss: 7.15649435e-07
Iter: 448 loss: 7.12733765e-07
Iter: 449 loss: 7.12493261e-07
Iter: 450 loss: 7.1220262e-07
Iter: 451 loss: 7.12180281e-07
Iter: 452 loss: 7.1189163e-07
Iter: 453 loss: 7.11894472e-07
Iter: 454 loss: 7.11681537e-07
Iter: 455 loss: 7.11415112e-07
Iter: 456 loss: 7.11387315e-07
Iter: 457 loss: 7.11136863e-07
Iter: 458 loss: 7.11136295e-07
Iter: 459 loss: 7.10903237e-07
Iter: 460 loss: 7.11676194e-07
Iter: 461 loss: 7.10834456e-07
Iter: 462 loss: 7.10710196e-07
Iter: 463 loss: 7.10405e-07
Iter: 464 loss: 7.14151156e-07
Iter: 465 loss: 7.10428e-07
Iter: 466 loss: 7.10158247e-07
Iter: 467 loss: 7.12614792e-07
Iter: 468 loss: 7.10166546e-07
Iter: 469 loss: 7.0996839e-07
Iter: 470 loss: 7.10171207e-07
Iter: 471 loss: 7.09865049e-07
Iter: 472 loss: 7.09661776e-07
Iter: 473 loss: 7.12003157e-07
Iter: 474 loss: 7.09649726e-07
Iter: 475 loss: 7.09513529e-07
Iter: 476 loss: 7.09276151e-07
Iter: 477 loss: 7.11630378e-07
Iter: 478 loss: 7.09208621e-07
Iter: 479 loss: 7.08994e-07
Iter: 480 loss: 7.09017968e-07
Iter: 481 loss: 7.08733239e-07
Iter: 482 loss: 7.08868242e-07
Iter: 483 loss: 7.08614209e-07
Iter: 484 loss: 7.08387915e-07
Iter: 485 loss: 7.0840133e-07
Iter: 486 loss: 7.0820812e-07
Iter: 487 loss: 7.07883487e-07
Iter: 488 loss: 7.11583482e-07
Iter: 489 loss: 7.07876609e-07
Iter: 490 loss: 7.07699428e-07
Iter: 491 loss: 7.07503432e-07
Iter: 492 loss: 7.07461368e-07
Iter: 493 loss: 7.0724451e-07
Iter: 494 loss: 7.07219669e-07
Iter: 495 loss: 7.07048571e-07
Iter: 496 loss: 7.06764183e-07
Iter: 497 loss: 7.13302597e-07
Iter: 498 loss: 7.06751223e-07
Iter: 499 loss: 7.06491221e-07
Iter: 500 loss: 7.07042602e-07
Iter: 501 loss: 7.06424e-07
Iter: 502 loss: 7.06156584e-07
Iter: 503 loss: 7.07114111e-07
Iter: 504 loss: 7.06094681e-07
Iter: 505 loss: 7.05885e-07
Iter: 506 loss: 7.08496259e-07
Iter: 507 loss: 7.05878051e-07
Iter: 508 loss: 7.05699563e-07
Iter: 509 loss: 7.0583269e-07
Iter: 510 loss: 7.05585421e-07
Iter: 511 loss: 7.05384139e-07
Iter: 512 loss: 7.05272896e-07
Iter: 513 loss: 7.05134255e-07
Iter: 514 loss: 7.04941556e-07
Iter: 515 loss: 7.05717923e-07
Iter: 516 loss: 7.04854699e-07
Iter: 517 loss: 7.04600836e-07
Iter: 518 loss: 7.07580284e-07
Iter: 519 loss: 7.04592253e-07
Iter: 520 loss: 7.04453441e-07
Iter: 521 loss: 7.0426438e-07
Iter: 522 loss: 7.0424835e-07
Iter: 523 loss: 7.0408322e-07
Iter: 524 loss: 7.04067e-07
Iter: 525 loss: 7.03962769e-07
Iter: 526 loss: 7.04109254e-07
Iter: 527 loss: 7.03887281e-07
Iter: 528 loss: 7.03734145e-07
Iter: 529 loss: 7.04902618e-07
Iter: 530 loss: 7.03709361e-07
Iter: 531 loss: 7.03594424e-07
Iter: 532 loss: 7.03317539e-07
Iter: 533 loss: 7.05905393e-07
Iter: 534 loss: 7.03299861e-07
Iter: 535 loss: 7.02973637e-07
Iter: 536 loss: 7.03639444e-07
Iter: 537 loss: 7.02822092e-07
Iter: 538 loss: 7.02529405e-07
Iter: 539 loss: 7.03216585e-07
Iter: 540 loss: 7.02406055e-07
Iter: 541 loss: 7.02152249e-07
Iter: 542 loss: 7.02124794e-07
Iter: 543 loss: 7.01995759e-07
Iter: 544 loss: 7.01954036e-07
Iter: 545 loss: 7.01853708e-07
Iter: 546 loss: 7.0161434e-07
Iter: 547 loss: 7.01632871e-07
Iter: 548 loss: 7.01454496e-07
Iter: 549 loss: 7.01245483e-07
Iter: 550 loss: 7.01260944e-07
Iter: 551 loss: 7.01086947e-07
Iter: 552 loss: 7.01321426e-07
Iter: 553 loss: 7.00973146e-07
Iter: 554 loss: 7.00815e-07
Iter: 555 loss: 7.00873443e-07
Iter: 556 loss: 7.00714963e-07
Iter: 557 loss: 7.0053153e-07
Iter: 558 loss: 7.00536361e-07
Iter: 559 loss: 7.00464284e-07
Iter: 560 loss: 7.00514249e-07
Iter: 561 loss: 7.00386067e-07
Iter: 562 loss: 7.00193937e-07
Iter: 563 loss: 7.00163184e-07
Iter: 564 loss: 7.00050236e-07
Iter: 565 loss: 6.99917223e-07
Iter: 566 loss: 6.997426e-07
Iter: 567 loss: 6.99710142e-07
Iter: 568 loss: 6.99479756e-07
Iter: 569 loss: 7.00765554e-07
Iter: 570 loss: 6.99438203e-07
Iter: 571 loss: 6.99341115e-07
Iter: 572 loss: 6.9932139e-07
Iter: 573 loss: 6.99216343e-07
Iter: 574 loss: 6.99152451e-07
Iter: 575 loss: 6.99146312e-07
Iter: 576 loss: 6.98976123e-07
Iter: 577 loss: 6.99126872e-07
Iter: 578 loss: 6.98907684e-07
Iter: 579 loss: 6.98721749e-07
Iter: 580 loss: 6.98569693e-07
Iter: 581 loss: 6.98502163e-07
Iter: 582 loss: 6.98258305e-07
Iter: 583 loss: 6.98275812e-07
Iter: 584 loss: 6.98114491e-07
Iter: 585 loss: 6.97975338e-07
Iter: 586 loss: 6.97877681e-07
Iter: 587 loss: 6.97646328e-07
Iter: 588 loss: 6.98815825e-07
Iter: 589 loss: 6.97654286e-07
Iter: 590 loss: 6.97407131e-07
Iter: 591 loss: 6.9844657e-07
Iter: 592 loss: 6.97331643e-07
Iter: 593 loss: 6.97165603e-07
Iter: 594 loss: 6.98350163e-07
Iter: 595 loss: 6.97171e-07
Iter: 596 loss: 6.97054134e-07
Iter: 597 loss: 6.96919358e-07
Iter: 598 loss: 6.96887582e-07
Iter: 599 loss: 6.96713926e-07
Iter: 600 loss: 6.96631218e-07
Iter: 601 loss: 6.96535722e-07
Iter: 602 loss: 6.9633353e-07
Iter: 603 loss: 6.98132055e-07
Iter: 604 loss: 6.96319091e-07
Iter: 605 loss: 6.96147595e-07
Iter: 606 loss: 6.97911901e-07
Iter: 607 loss: 6.96142251e-07
Iter: 608 loss: 6.96026575e-07
Iter: 609 loss: 6.95877134e-07
Iter: 610 loss: 6.95836e-07
Iter: 611 loss: 6.95651579e-07
Iter: 612 loss: 6.95972744e-07
Iter: 613 loss: 6.95504411e-07
Iter: 614 loss: 6.95330357e-07
Iter: 615 loss: 6.95322456e-07
Iter: 616 loss: 6.95197969e-07
Iter: 617 loss: 6.95541644e-07
Iter: 618 loss: 6.95155677e-07
Iter: 619 loss: 6.94970595e-07
Iter: 620 loss: 6.94792845e-07
Iter: 621 loss: 6.94763571e-07
Iter: 622 loss: 6.94736684e-07
Iter: 623 loss: 6.94632945e-07
Iter: 624 loss: 6.94554501e-07
Iter: 625 loss: 6.94528694e-07
Iter: 626 loss: 6.94507548e-07
Iter: 627 loss: 6.94338496e-07
Iter: 628 loss: 6.94447408e-07
Iter: 629 loss: 6.94240612e-07
Iter: 630 loss: 6.9411027e-07
Iter: 631 loss: 6.9397106e-07
Iter: 632 loss: 6.93958668e-07
Iter: 633 loss: 6.93667289e-07
Iter: 634 loss: 6.94169671e-07
Iter: 635 loss: 6.93616698e-07
Iter: 636 loss: 6.93486527e-07
Iter: 637 loss: 6.93481866e-07
Iter: 638 loss: 6.93363e-07
Iter: 639 loss: 6.93172808e-07
Iter: 640 loss: 6.93163088e-07
Iter: 641 loss: 6.93010918e-07
Iter: 642 loss: 6.93240736e-07
Iter: 643 loss: 6.92918718e-07
Iter: 644 loss: 6.92698563e-07
Iter: 645 loss: 6.93244033e-07
Iter: 646 loss: 6.92639787e-07
Iter: 647 loss: 6.92423498e-07
Iter: 648 loss: 6.94256642e-07
Iter: 649 loss: 6.92435265e-07
Iter: 650 loss: 6.92266553e-07
Iter: 651 loss: 6.92521155e-07
Iter: 652 loss: 6.92179128e-07
Iter: 653 loss: 6.9203594e-07
Iter: 654 loss: 6.92534e-07
Iter: 655 loss: 6.92016101e-07
Iter: 656 loss: 6.91853074e-07
Iter: 657 loss: 6.92163042e-07
Iter: 658 loss: 6.91771845e-07
Iter: 659 loss: 6.91593584e-07
Iter: 660 loss: 6.91998252e-07
Iter: 661 loss: 6.91542652e-07
Iter: 662 loss: 6.91353e-07
Iter: 663 loss: 6.91266678e-07
Iter: 664 loss: 6.91149921e-07
Iter: 665 loss: 6.90930733e-07
Iter: 666 loss: 6.90658112e-07
Iter: 667 loss: 6.90613547e-07
Iter: 668 loss: 6.90371849e-07
Iter: 669 loss: 6.90374463e-07
Iter: 670 loss: 6.90167553e-07
Iter: 671 loss: 6.91436526e-07
Iter: 672 loss: 6.90171817e-07
Iter: 673 loss: 6.90024251e-07
Iter: 674 loss: 6.89793637e-07
Iter: 675 loss: 6.95223719e-07
Iter: 676 loss: 6.89794e-07
Iter: 677 loss: 6.89549495e-07
Iter: 678 loss: 6.90680054e-07
Iter: 679 loss: 6.89519084e-07
Iter: 680 loss: 6.89342414e-07
Iter: 681 loss: 6.89340482e-07
Iter: 682 loss: 6.89221679e-07
Iter: 683 loss: 6.8952761e-07
Iter: 684 loss: 6.89206672e-07
Iter: 685 loss: 6.89097192e-07
Iter: 686 loss: 6.8905922e-07
Iter: 687 loss: 6.89017952e-07
Iter: 688 loss: 6.88826049e-07
Iter: 689 loss: 6.90674938e-07
Iter: 690 loss: 6.8880496e-07
Iter: 691 loss: 6.88683826e-07
Iter: 692 loss: 6.88804505e-07
Iter: 693 loss: 6.8861317e-07
Iter: 694 loss: 6.88448949e-07
Iter: 695 loss: 6.88652221e-07
Iter: 696 loss: 6.88367379e-07
Iter: 697 loss: 6.88206569e-07
Iter: 698 loss: 6.87969305e-07
Iter: 699 loss: 6.87933891e-07
Iter: 700 loss: 6.87737383e-07
Iter: 701 loss: 6.8870429e-07
Iter: 702 loss: 6.87665761e-07
Iter: 703 loss: 6.87481872e-07
Iter: 704 loss: 6.87496936e-07
Iter: 705 loss: 6.87319186e-07
Iter: 706 loss: 6.87172701e-07
Iter: 707 loss: 6.87145871e-07
Iter: 708 loss: 6.86909061e-07
Iter: 709 loss: 6.86758654e-07
Iter: 710 loss: 6.86713236e-07
Iter: 711 loss: 6.86490864e-07
Iter: 712 loss: 6.8650985e-07
Iter: 713 loss: 6.86291742e-07
Iter: 714 loss: 6.86965109e-07
Iter: 715 loss: 6.86221142e-07
Iter: 716 loss: 6.86061412e-07
Iter: 717 loss: 6.86301746e-07
Iter: 718 loss: 6.85942496e-07
Iter: 719 loss: 6.8584211e-07
Iter: 720 loss: 6.85830344e-07
Iter: 721 loss: 6.8572956e-07
Iter: 722 loss: 6.85665611e-07
Iter: 723 loss: 6.85623e-07
Iter: 724 loss: 6.85449777e-07
Iter: 725 loss: 6.85870191e-07
Iter: 726 loss: 6.85376449e-07
Iter: 727 loss: 6.85221039e-07
Iter: 728 loss: 6.85310511e-07
Iter: 729 loss: 6.85099621e-07
Iter: 730 loss: 6.84941483e-07
Iter: 731 loss: 6.84907377e-07
Iter: 732 loss: 6.84807e-07
Iter: 733 loss: 6.84625661e-07
Iter: 734 loss: 6.87401496e-07
Iter: 735 loss: 6.84627537e-07
Iter: 736 loss: 6.84461156e-07
Iter: 737 loss: 6.84939721e-07
Iter: 738 loss: 6.84391864e-07
Iter: 739 loss: 6.84242877e-07
Iter: 740 loss: 6.84014708e-07
Iter: 741 loss: 6.84024712e-07
Iter: 742 loss: 6.83763687e-07
Iter: 743 loss: 6.84521808e-07
Iter: 744 loss: 6.83680241e-07
Iter: 745 loss: 6.83448718e-07
Iter: 746 loss: 6.83445307e-07
Iter: 747 loss: 6.83225949e-07
Iter: 748 loss: 6.83240842e-07
Iter: 749 loss: 6.83080259e-07
Iter: 750 loss: 6.82888697e-07
Iter: 751 loss: 6.84679662e-07
Iter: 752 loss: 6.82881648e-07
Iter: 753 loss: 6.82691962e-07
Iter: 754 loss: 6.83054054e-07
Iter: 755 loss: 6.82616474e-07
Iter: 756 loss: 6.82478685e-07
Iter: 757 loss: 6.82752614e-07
Iter: 758 loss: 6.82397342e-07
Iter: 759 loss: 6.8224216e-07
Iter: 760 loss: 6.82404448e-07
Iter: 761 loss: 6.82146606e-07
Iter: 762 loss: 6.81946858e-07
Iter: 763 loss: 6.8187984e-07
Iter: 764 loss: 6.81792415e-07
Iter: 765 loss: 6.81557822e-07
Iter: 766 loss: 6.82619486e-07
Iter: 767 loss: 6.81495521e-07
Iter: 768 loss: 6.81347274e-07
Iter: 769 loss: 6.81357506e-07
Iter: 770 loss: 6.81198856e-07
Iter: 771 loss: 6.80928451e-07
Iter: 772 loss: 6.80940957e-07
Iter: 773 loss: 6.80756273e-07
Iter: 774 loss: 6.81100175e-07
Iter: 775 loss: 6.80705512e-07
Iter: 776 loss: 6.804799e-07
Iter: 777 loss: 6.82002337e-07
Iter: 778 loss: 6.80461937e-07
Iter: 779 loss: 6.80328412e-07
Iter: 780 loss: 6.82010921e-07
Iter: 781 loss: 6.80320341e-07
Iter: 782 loss: 6.80214e-07
Iter: 783 loss: 6.80106496e-07
Iter: 784 loss: 6.8007563e-07
Iter: 785 loss: 6.79905611e-07
Iter: 786 loss: 6.79908965e-07
Iter: 787 loss: 6.7983126e-07
Iter: 788 loss: 6.79834272e-07
Iter: 789 loss: 6.79732523e-07
Iter: 790 loss: 6.79631285e-07
Iter: 791 loss: 6.79681364e-07
Iter: 792 loss: 6.79539653e-07
Iter: 793 loss: 6.79360312e-07
Iter: 794 loss: 6.79437676e-07
Iter: 795 loss: 6.79235313e-07
Iter: 796 loss: 6.78999413e-07
Iter: 797 loss: 6.79310688e-07
Iter: 798 loss: 6.78887375e-07
Iter: 799 loss: 6.78666254e-07
Iter: 800 loss: 6.81306403e-07
Iter: 801 loss: 6.78660513e-07
Iter: 802 loss: 6.78463948e-07
Iter: 803 loss: 6.79139873e-07
Iter: 804 loss: 6.78389142e-07
Iter: 805 loss: 6.78274489e-07
Iter: 806 loss: 6.77987202e-07
Iter: 807 loss: 6.82146e-07
Iter: 808 loss: 6.77968387e-07
Iter: 809 loss: 6.77733e-07
Iter: 810 loss: 6.80599328e-07
Iter: 811 loss: 6.77747948e-07
Iter: 812 loss: 6.77618345e-07
Iter: 813 loss: 6.77615731e-07
Iter: 814 loss: 6.77501e-07
Iter: 815 loss: 6.77310311e-07
Iter: 816 loss: 6.77322078e-07
Iter: 817 loss: 6.77234254e-07
Iter: 818 loss: 6.77237949e-07
Iter: 819 loss: 6.77130117e-07
Iter: 820 loss: 6.77086746e-07
Iter: 821 loss: 6.7706253e-07
Iter: 822 loss: 6.76933269e-07
Iter: 823 loss: 6.77157743e-07
Iter: 824 loss: 6.76876141e-07
Iter: 825 loss: 6.76718969e-07
Iter: 826 loss: 6.76565e-07
Iter: 827 loss: 6.76545653e-07
Iter: 828 loss: 6.76298384e-07
Iter: 829 loss: 6.77025696e-07
Iter: 830 loss: 6.76230798e-07
Iter: 831 loss: 6.7601195e-07
Iter: 832 loss: 6.77201058e-07
Iter: 833 loss: 6.75971535e-07
Iter: 834 loss: 6.75797423e-07
Iter: 835 loss: 6.77803314e-07
Iter: 836 loss: 6.75773492e-07
Iter: 837 loss: 6.7570295e-07
Iter: 838 loss: 6.75478077e-07
Iter: 839 loss: 6.79949608e-07
Iter: 840 loss: 6.75473814e-07
Iter: 841 loss: 6.75256956e-07
Iter: 842 loss: 6.75864726e-07
Iter: 843 loss: 6.7519909e-07
Iter: 844 loss: 6.75033334e-07
Iter: 845 loss: 6.77007506e-07
Iter: 846 loss: 6.75043793e-07
Iter: 847 loss: 6.74834496e-07
Iter: 848 loss: 6.74921e-07
Iter: 849 loss: 6.74671696e-07
Iter: 850 loss: 6.74561306e-07
Iter: 851 loss: 6.75877459e-07
Iter: 852 loss: 6.74574437e-07
Iter: 853 loss: 6.74469902e-07
Iter: 854 loss: 6.74656121e-07
Iter: 855 loss: 6.74400894e-07
Iter: 856 loss: 6.74277e-07
Iter: 857 loss: 6.74303294e-07
Iter: 858 loss: 6.74205637e-07
Iter: 859 loss: 6.74053e-07
Iter: 860 loss: 6.74718194e-07
Iter: 861 loss: 6.74023113e-07
Iter: 862 loss: 6.73901866e-07
Iter: 863 loss: 6.73834393e-07
Iter: 864 loss: 6.73806539e-07
Iter: 865 loss: 6.73667955e-07
Iter: 866 loss: 6.74622072e-07
Iter: 867 loss: 6.73688135e-07
Iter: 868 loss: 6.73576e-07
Iter: 869 loss: 6.74725186e-07
Iter: 870 loss: 6.73594968e-07
Iter: 871 loss: 6.73483839e-07
Iter: 872 loss: 6.7330194e-07
Iter: 873 loss: 6.73306204e-07
Iter: 874 loss: 6.73142836e-07
Iter: 875 loss: 6.73348268e-07
Iter: 876 loss: 6.73064164e-07
Iter: 877 loss: 6.72842361e-07
Iter: 878 loss: 6.73142722e-07
Iter: 879 loss: 6.72751696e-07
Iter: 880 loss: 6.72629824e-07
Iter: 881 loss: 6.72580541e-07
Iter: 882 loss: 6.7252688e-07
Iter: 883 loss: 6.7238949e-07
Iter: 884 loss: 6.7238e-07
Iter: 885 loss: 6.72215833e-07
Iter: 886 loss: 6.72235387e-07
Iter: 887 loss: 6.72132728e-07
Iter: 888 loss: 6.72039675e-07
Iter: 889 loss: 6.72021542e-07
Iter: 890 loss: 6.71890632e-07
Iter: 891 loss: 6.7301562e-07
Iter: 892 loss: 6.71875682e-07
Iter: 893 loss: 6.71788371e-07
Iter: 894 loss: 6.71659222e-07
Iter: 895 loss: 6.71617272e-07
Iter: 896 loss: 6.71461294e-07
Iter: 897 loss: 6.71620057e-07
Iter: 898 loss: 6.71391035e-07
Iter: 899 loss: 6.71262853e-07
Iter: 900 loss: 6.73094235e-07
Iter: 901 loss: 6.71269504e-07
Iter: 902 loss: 6.71149735e-07
Iter: 903 loss: 6.71473913e-07
Iter: 904 loss: 6.71111593e-07
Iter: 905 loss: 6.71017233e-07
Iter: 906 loss: 6.70950214e-07
Iter: 907 loss: 6.70899908e-07
Iter: 908 loss: 6.70750069e-07
Iter: 909 loss: 6.70852955e-07
Iter: 910 loss: 6.70664292e-07
Iter: 911 loss: 6.70488532e-07
Iter: 912 loss: 6.7100143e-07
Iter: 913 loss: 6.7044482e-07
Iter: 914 loss: 6.70272584e-07
Iter: 915 loss: 6.70283725e-07
Iter: 916 loss: 6.70175837e-07
Iter: 917 loss: 6.70151906e-07
Iter: 918 loss: 6.70076531e-07
Iter: 919 loss: 6.69988765e-07
Iter: 920 loss: 6.69980636e-07
Iter: 921 loss: 6.69948633e-07
Iter: 922 loss: 6.69767303e-07
Iter: 923 loss: 6.70677878e-07
Iter: 924 loss: 6.69716428e-07
Iter: 925 loss: 6.69587791e-07
Iter: 926 loss: 6.69569886e-07
Iter: 927 loss: 6.69464953e-07
Iter: 928 loss: 6.69352403e-07
Iter: 929 loss: 6.69329893e-07
Iter: 930 loss: 6.69167321e-07
Iter: 931 loss: 6.69547717e-07
Iter: 932 loss: 6.69089445e-07
Iter: 933 loss: 6.68920848e-07
Iter: 934 loss: 6.69373549e-07
Iter: 935 loss: 6.68857751e-07
Iter: 936 loss: 6.68723487e-07
Iter: 937 loss: 6.68834332e-07
Iter: 938 loss: 6.68570578e-07
Iter: 939 loss: 6.68436599e-07
Iter: 940 loss: 6.69317e-07
Iter: 941 loss: 6.68359803e-07
Iter: 942 loss: 6.68271e-07
Iter: 943 loss: 6.68343432e-07
Iter: 944 loss: 6.68204621e-07
Iter: 945 loss: 6.68034e-07
Iter: 946 loss: 6.68406528e-07
Iter: 947 loss: 6.6798674e-07
Iter: 948 loss: 6.67884e-07
Iter: 949 loss: 6.67888912e-07
Iter: 950 loss: 6.67805637e-07
Iter: 951 loss: 6.67765789e-07
Iter: 952 loss: 6.67713039e-07
Iter: 953 loss: 6.67592644e-07
Iter: 954 loss: 6.69233259e-07
Iter: 955 loss: 6.67600489e-07
Iter: 956 loss: 6.67536142e-07
Iter: 957 loss: 6.67345e-07
Iter: 958 loss: 6.69507358e-07
Iter: 959 loss: 6.67347479e-07
Iter: 960 loss: 6.67189738e-07
Iter: 961 loss: 6.68862526e-07
Iter: 962 loss: 6.67210315e-07
Iter: 963 loss: 6.67058657e-07
Iter: 964 loss: 6.67431e-07
Iter: 965 loss: 6.67036034e-07
Iter: 966 loss: 6.66963501e-07
Iter: 967 loss: 6.66885171e-07
Iter: 968 loss: 6.66843846e-07
Iter: 969 loss: 6.66734479e-07
Iter: 970 loss: 6.66742608e-07
Iter: 971 loss: 6.66651374e-07
Iter: 972 loss: 6.66542519e-07
Iter: 973 loss: 6.66566393e-07
Iter: 974 loss: 6.664485e-07
Iter: 975 loss: 6.6654593e-07
Iter: 976 loss: 6.66380515e-07
Iter: 977 loss: 6.661719e-07
Iter: 978 loss: 6.66213907e-07
Iter: 979 loss: 6.659875e-07
Iter: 980 loss: 6.65798666e-07
Iter: 981 loss: 6.66940252e-07
Iter: 982 loss: 6.65761604e-07
Iter: 983 loss: 6.65556399e-07
Iter: 984 loss: 6.66494543e-07
Iter: 985 loss: 6.65557366e-07
Iter: 986 loss: 6.65403491e-07
Iter: 987 loss: 6.65875859e-07
Iter: 988 loss: 6.65399853e-07
Iter: 989 loss: 6.65243533e-07
Iter: 990 loss: 6.65988182e-07
Iter: 991 loss: 6.65233301e-07
Iter: 992 loss: 6.65135644e-07
Iter: 993 loss: 6.64994957e-07
Iter: 994 loss: 6.68152836e-07
Iter: 995 loss: 6.64983e-07
Iter: 996 loss: 6.64863478e-07
Iter: 997 loss: 6.66532742e-07
Iter: 998 loss: 6.64870811e-07
Iter: 999 loss: 6.64759e-07
Iter: 1000 loss: 6.64789e-07
Iter: 1001 loss: 6.64692493e-07
Iter: 1002 loss: 6.64580966e-07
Iter: 1003 loss: 6.64773836e-07
Iter: 1004 loss: 6.64518723e-07
Iter: 1005 loss: 6.64451363e-07
Iter: 1006 loss: 6.64426238e-07
Iter: 1007 loss: 6.64349386e-07
Iter: 1008 loss: 6.64208869e-07
Iter: 1009 loss: 6.64192498e-07
Iter: 1010 loss: 6.64053289e-07
Iter: 1011 loss: 6.64033337e-07
Iter: 1012 loss: 6.63956257e-07
Iter: 1013 loss: 6.63728e-07
Iter: 1014 loss: 6.6524143e-07
Iter: 1015 loss: 6.637095e-07
Iter: 1016 loss: 6.63566311e-07
Iter: 1017 loss: 6.64462164e-07
Iter: 1018 loss: 6.63578703e-07
Iter: 1019 loss: 6.63444553e-07
Iter: 1020 loss: 6.63754747e-07
Iter: 1021 loss: 6.63377364e-07
Iter: 1022 loss: 6.63270498e-07
Iter: 1023 loss: 6.64306583e-07
Iter: 1024 loss: 6.6327209e-07
Iter: 1025 loss: 6.6320132e-07
Iter: 1026 loss: 6.63080243e-07
Iter: 1027 loss: 6.63076605e-07
Iter: 1028 loss: 6.62931143e-07
Iter: 1029 loss: 6.62946377e-07
Iter: 1030 loss: 6.62823822e-07
Iter: 1031 loss: 6.62741797e-07
Iter: 1032 loss: 6.62747425e-07
Iter: 1033 loss: 6.62650564e-07
Iter: 1034 loss: 6.6256689e-07
Iter: 1035 loss: 6.62541595e-07
Iter: 1036 loss: 6.62395337e-07
Iter: 1037 loss: 6.62784373e-07
Iter: 1038 loss: 6.62376465e-07
Iter: 1039 loss: 6.62254195e-07
Iter: 1040 loss: 6.64035099e-07
Iter: 1041 loss: 6.62252546e-07
Iter: 1042 loss: 6.62155969e-07
Iter: 1043 loss: 6.61954687e-07
Iter: 1044 loss: 6.64386448e-07
Iter: 1045 loss: 6.61928311e-07
Iter: 1046 loss: 6.61738454e-07
Iter: 1047 loss: 6.63181e-07
Iter: 1048 loss: 6.61728e-07
Iter: 1049 loss: 6.61626132e-07
Iter: 1050 loss: 6.62711727e-07
Iter: 1051 loss: 6.61623574e-07
Iter: 1052 loss: 6.61491583e-07
Iter: 1053 loss: 6.61912509e-07
Iter: 1054 loss: 6.61478339e-07
Iter: 1055 loss: 6.61397451e-07
Iter: 1056 loss: 6.61882893e-07
Iter: 1057 loss: 6.61380511e-07
Iter: 1058 loss: 6.61336401e-07
Iter: 1059 loss: 6.61460945e-07
Iter: 1060 loss: 6.61291949e-07
Iter: 1061 loss: 6.6123971e-07
Iter: 1062 loss: 6.61073955e-07
Iter: 1063 loss: 6.62878278e-07
Iter: 1064 loss: 6.61032573e-07
Iter: 1065 loss: 6.60956346e-07
Iter: 1066 loss: 6.60933893e-07
Iter: 1067 loss: 6.60826572e-07
Iter: 1068 loss: 6.6083885e-07
Iter: 1069 loss: 6.60762339e-07
Iter: 1070 loss: 6.60595106e-07
Iter: 1071 loss: 6.60462945e-07
Iter: 1072 loss: 6.60409228e-07
Iter: 1073 loss: 6.60307e-07
Iter: 1074 loss: 6.6030475e-07
Iter: 1075 loss: 6.60134219e-07
Iter: 1076 loss: 6.60040826e-07
Iter: 1077 loss: 6.60008e-07
Iter: 1078 loss: 6.5987058e-07
Iter: 1079 loss: 6.60025e-07
Iter: 1080 loss: 6.59794807e-07
Iter: 1081 loss: 6.59649686e-07
Iter: 1082 loss: 6.59836473e-07
Iter: 1083 loss: 6.59591478e-07
Iter: 1084 loss: 6.59535544e-07
Iter: 1085 loss: 6.5950934e-07
Iter: 1086 loss: 6.59447096e-07
Iter: 1087 loss: 6.5942686e-07
Iter: 1088 loss: 6.59409238e-07
Iter: 1089 loss: 6.5925866e-07
Iter: 1090 loss: 6.59472e-07
Iter: 1091 loss: 6.59252692e-07
Iter: 1092 loss: 6.59102511e-07
Iter: 1093 loss: 6.591942e-07
Iter: 1094 loss: 6.59027819e-07
Iter: 1095 loss: 6.58928286e-07
Iter: 1096 loss: 6.5872257e-07
Iter: 1097 loss: 6.63328422e-07
Iter: 1098 loss: 6.58726037e-07
Iter: 1099 loss: 6.58582508e-07
Iter: 1100 loss: 6.58535726e-07
Iter: 1101 loss: 6.58453359e-07
Iter: 1102 loss: 6.58293629e-07
Iter: 1103 loss: 6.5827561e-07
Iter: 1104 loss: 6.58104454e-07
Iter: 1105 loss: 6.58864508e-07
Iter: 1106 loss: 6.58037379e-07
Iter: 1107 loss: 6.57899363e-07
Iter: 1108 loss: 6.60153887e-07
Iter: 1109 loss: 6.57894304e-07
Iter: 1110 loss: 6.57820124e-07
Iter: 1111 loss: 6.57656528e-07
Iter: 1112 loss: 6.60299349e-07
Iter: 1113 loss: 6.57627083e-07
Iter: 1114 loss: 6.574694e-07
Iter: 1115 loss: 6.5796894e-07
Iter: 1116 loss: 6.57392093e-07
Iter: 1117 loss: 6.57221563e-07
Iter: 1118 loss: 6.58736667e-07
Iter: 1119 loss: 6.57212468e-07
Iter: 1120 loss: 6.57058706e-07
Iter: 1121 loss: 6.57923124e-07
Iter: 1122 loss: 6.57060241e-07
Iter: 1123 loss: 6.56953148e-07
Iter: 1124 loss: 6.5748111e-07
Iter: 1125 loss: 6.56944167e-07
Iter: 1126 loss: 6.56873283e-07
Iter: 1127 loss: 6.56754821e-07
Iter: 1128 loss: 6.56716793e-07
Iter: 1129 loss: 6.56598843e-07
Iter: 1130 loss: 6.56775967e-07
Iter: 1131 loss: 6.56541488e-07
Iter: 1132 loss: 6.56416887e-07
Iter: 1133 loss: 6.56626071e-07
Iter: 1134 loss: 6.56334407e-07
Iter: 1135 loss: 6.56225552e-07
Iter: 1136 loss: 6.57932219e-07
Iter: 1137 loss: 6.56217253e-07
Iter: 1138 loss: 6.56137559e-07
Iter: 1139 loss: 6.56101633e-07
Iter: 1140 loss: 6.56057409e-07
Iter: 1141 loss: 6.55976464e-07
Iter: 1142 loss: 6.56818827e-07
Iter: 1143 loss: 6.55965e-07
Iter: 1144 loss: 6.55884207e-07
Iter: 1145 loss: 6.55817189e-07
Iter: 1146 loss: 6.55773533e-07
Iter: 1147 loss: 6.55610847e-07
Iter: 1148 loss: 6.55558495e-07
Iter: 1149 loss: 6.55488407e-07
Iter: 1150 loss: 6.55271265e-07
Iter: 1151 loss: 6.5538876e-07
Iter: 1152 loss: 6.551453e-07
Iter: 1153 loss: 6.55111819e-07
Iter: 1154 loss: 6.55028771e-07
Iter: 1155 loss: 6.54903488e-07
Iter: 1156 loss: 6.54812538e-07
Iter: 1157 loss: 6.54802875e-07
Iter: 1158 loss: 6.54607e-07
Iter: 1159 loss: 6.55308611e-07
Iter: 1160 loss: 6.54549922e-07
Iter: 1161 loss: 6.54430437e-07
Iter: 1162 loss: 6.54387065e-07
Iter: 1163 loss: 6.54326698e-07
Iter: 1164 loss: 6.54126382e-07
Iter: 1165 loss: 6.54111886e-07
Iter: 1166 loss: 6.53981147e-07
Iter: 1167 loss: 6.53800271e-07
Iter: 1168 loss: 6.56490329e-07
Iter: 1169 loss: 6.53768666e-07
Iter: 1170 loss: 6.53647248e-07
Iter: 1171 loss: 6.54280427e-07
Iter: 1172 loss: 6.53651739e-07
Iter: 1173 loss: 6.53519578e-07
Iter: 1174 loss: 6.53342e-07
Iter: 1175 loss: 6.53339498e-07
Iter: 1176 loss: 6.53360303e-07
Iter: 1177 loss: 6.53243603e-07
Iter: 1178 loss: 6.53210293e-07
Iter: 1179 loss: 6.53055963e-07
Iter: 1180 loss: 6.53640029e-07
Iter: 1181 loss: 6.52964331e-07
Iter: 1182 loss: 6.52727522e-07
Iter: 1183 loss: 6.54156338e-07
Iter: 1184 loss: 6.5267875e-07
Iter: 1185 loss: 6.52552842e-07
Iter: 1186 loss: 6.53942493e-07
Iter: 1187 loss: 6.52539143e-07
Iter: 1188 loss: 6.52367646e-07
Iter: 1189 loss: 6.52769131e-07
Iter: 1190 loss: 6.523087e-07
Iter: 1191 loss: 6.52190408e-07
Iter: 1192 loss: 6.53236953e-07
Iter: 1193 loss: 6.5218444e-07
Iter: 1194 loss: 6.5209e-07
Iter: 1195 loss: 6.52031758e-07
Iter: 1196 loss: 6.52003678e-07
Iter: 1197 loss: 6.51872085e-07
Iter: 1198 loss: 6.51825758e-07
Iter: 1199 loss: 6.51735604e-07
Iter: 1200 loss: 6.51570645e-07
Iter: 1201 loss: 6.52208e-07
Iter: 1202 loss: 6.5154029e-07
Iter: 1203 loss: 6.51376808e-07
Iter: 1204 loss: 6.52021356e-07
Iter: 1205 loss: 6.5133645e-07
Iter: 1206 loss: 6.51146649e-07
Iter: 1207 loss: 6.51479809e-07
Iter: 1208 loss: 6.5104382e-07
Iter: 1209 loss: 6.50945935e-07
Iter: 1210 loss: 6.51459231e-07
Iter: 1211 loss: 6.50903075e-07
Iter: 1212 loss: 6.50713901e-07
Iter: 1213 loss: 6.51070877e-07
Iter: 1214 loss: 6.50630511e-07
Iter: 1215 loss: 6.50545303e-07
Iter: 1216 loss: 6.50453e-07
Iter: 1217 loss: 6.5044037e-07
Iter: 1218 loss: 6.50268248e-07
Iter: 1219 loss: 6.50817071e-07
Iter: 1220 loss: 6.50204242e-07
Iter: 1221 loss: 6.50195261e-07
Iter: 1222 loss: 6.50145921e-07
Iter: 1223 loss: 6.5008885e-07
Iter: 1224 loss: 6.50017228e-07
Iter: 1225 loss: 6.50013817e-07
Iter: 1226 loss: 6.49929405e-07
Iter: 1227 loss: 6.50654783e-07
Iter: 1228 loss: 6.49890808e-07
Iter: 1229 loss: 6.49828962e-07
Iter: 1230 loss: 6.49713684e-07
Iter: 1231 loss: 6.49709477e-07
Iter: 1232 loss: 6.49575952e-07
Iter: 1233 loss: 6.49871311e-07
Iter: 1234 loss: 6.49529397e-07
Iter: 1235 loss: 6.49403205e-07
Iter: 1236 loss: 6.49804633e-07
Iter: 1237 loss: 6.49354092e-07
Iter: 1238 loss: 6.49229492e-07
Iter: 1239 loss: 6.5023886e-07
Iter: 1240 loss: 6.49185893e-07
Iter: 1241 loss: 6.49097956e-07
Iter: 1242 loss: 6.49078856e-07
Iter: 1243 loss: 6.489729e-07
Iter: 1244 loss: 6.48864e-07
Iter: 1245 loss: 6.4888718e-07
Iter: 1246 loss: 6.48802768e-07
Iter: 1247 loss: 6.4870369e-07
Iter: 1248 loss: 6.48691866e-07
Iter: 1249 loss: 6.48580226e-07
Iter: 1250 loss: 6.48609785e-07
Iter: 1251 loss: 6.48489959e-07
Iter: 1252 loss: 6.4832841e-07
Iter: 1253 loss: 6.49183221e-07
Iter: 1254 loss: 6.48309594e-07
Iter: 1255 loss: 6.48168111e-07
Iter: 1256 loss: 6.4942833e-07
Iter: 1257 loss: 6.48139235e-07
Iter: 1258 loss: 6.48075684e-07
Iter: 1259 loss: 6.48278956e-07
Iter: 1260 loss: 6.48044704e-07
Iter: 1261 loss: 6.47948411e-07
Iter: 1262 loss: 6.47930278e-07
Iter: 1263 loss: 6.47889692e-07
Iter: 1264 loss: 6.47800221e-07
Iter: 1265 loss: 6.47732918e-07
Iter: 1266 loss: 6.47696197e-07
Iter: 1267 loss: 6.47553236e-07
Iter: 1268 loss: 6.48780883e-07
Iter: 1269 loss: 6.47567049e-07
Iter: 1270 loss: 6.47448928e-07
Iter: 1271 loss: 6.4768318e-07
Iter: 1272 loss: 6.4743017e-07
Iter: 1273 loss: 6.4734877e-07
Iter: 1274 loss: 6.47807383e-07
Iter: 1275 loss: 6.4732285e-07
Iter: 1276 loss: 6.47240256e-07
Iter: 1277 loss: 6.47257593e-07
Iter: 1278 loss: 6.47190063e-07
Iter: 1279 loss: 6.47064326e-07
Iter: 1280 loss: 6.47492698e-07
Iter: 1281 loss: 6.47020386e-07
Iter: 1282 loss: 6.46923183e-07
Iter: 1283 loss: 6.46857131e-07
Iter: 1284 loss: 6.46834053e-07
Iter: 1285 loss: 6.46648232e-07
Iter: 1286 loss: 6.46718718e-07
Iter: 1287 loss: 6.46537956e-07
Iter: 1288 loss: 6.46711555e-07
Iter: 1289 loss: 6.46479862e-07
Iter: 1290 loss: 6.46446892e-07
Iter: 1291 loss: 6.46318199e-07
Iter: 1292 loss: 6.48036519e-07
Iter: 1293 loss: 6.46312856e-07
Iter: 1294 loss: 6.46142666e-07
Iter: 1295 loss: 6.47096272e-07
Iter: 1296 loss: 6.46126e-07
Iter: 1297 loss: 6.46009255e-07
Iter: 1298 loss: 6.46019942e-07
Iter: 1299 loss: 6.45937575e-07
Iter: 1300 loss: 6.45824343e-07
Iter: 1301 loss: 6.45834e-07
Iter: 1302 loss: 6.45684054e-07
Iter: 1303 loss: 6.45601403e-07
Iter: 1304 loss: 6.47530783e-07
Iter: 1305 loss: 6.45596515e-07
Iter: 1306 loss: 6.45495504e-07
Iter: 1307 loss: 6.45591854e-07
Iter: 1308 loss: 6.45455884e-07
Iter: 1309 loss: 6.45338901e-07
Iter: 1310 loss: 6.45588443e-07
Iter: 1311 loss: 6.45263526e-07
Iter: 1312 loss: 6.45184798e-07
Iter: 1313 loss: 6.46213891e-07
Iter: 1314 loss: 6.45182809e-07
Iter: 1315 loss: 6.45113289e-07
Iter: 1316 loss: 6.44949125e-07
Iter: 1317 loss: 6.47235083e-07
Iter: 1318 loss: 6.44946226e-07
Iter: 1319 loss: 6.44794113e-07
Iter: 1320 loss: 6.45339298e-07
Iter: 1321 loss: 6.44766487e-07
Iter: 1322 loss: 6.4461176e-07
Iter: 1323 loss: 6.45223736e-07
Iter: 1324 loss: 6.44564466e-07
Iter: 1325 loss: 6.44452484e-07
Iter: 1326 loss: 6.44450949e-07
Iter: 1327 loss: 6.44379782e-07
Iter: 1328 loss: 6.44358238e-07
Iter: 1329 loss: 6.44366708e-07
Iter: 1330 loss: 6.44247166e-07
Iter: 1331 loss: 6.44227725e-07
Iter: 1332 loss: 6.44178442e-07
Iter: 1333 loss: 6.44037e-07
Iter: 1334 loss: 6.44014e-07
Iter: 1335 loss: 6.4393879e-07
Iter: 1336 loss: 6.43799694e-07
Iter: 1337 loss: 6.44909449e-07
Iter: 1338 loss: 6.43790486e-07
Iter: 1339 loss: 6.43676458e-07
Iter: 1340 loss: 6.44924398e-07
Iter: 1341 loss: 6.43689816e-07
Iter: 1342 loss: 6.43604267e-07
Iter: 1343 loss: 6.43628766e-07
Iter: 1344 loss: 6.43571639e-07
Iter: 1345 loss: 6.43473641e-07
Iter: 1346 loss: 6.43979604e-07
Iter: 1347 loss: 6.43459146e-07
Iter: 1348 loss: 6.43397584e-07
Iter: 1349 loss: 6.43586e-07
Iter: 1350 loss: 6.43347505e-07
Iter: 1351 loss: 6.43272415e-07
Iter: 1352 loss: 6.43239446e-07
Iter: 1353 loss: 6.43233307e-07
Iter: 1354 loss: 6.43119165e-07
Iter: 1355 loss: 6.43311523e-07
Iter: 1356 loss: 6.43082e-07
Iter: 1357 loss: 6.43088299e-07
Iter: 1358 loss: 6.43032195e-07
Iter: 1359 loss: 6.43003204e-07
Iter: 1360 loss: 6.42902478e-07
Iter: 1361 loss: 6.44158547e-07
Iter: 1362 loss: 6.42892644e-07
Iter: 1363 loss: 6.42831708e-07
Iter: 1364 loss: 6.43777355e-07
Iter: 1365 loss: 6.42810221e-07
Iter: 1366 loss: 6.42775149e-07
Iter: 1367 loss: 6.42732459e-07
Iter: 1368 loss: 6.42714099e-07
Iter: 1369 loss: 6.42640771e-07
Iter: 1370 loss: 6.42606153e-07
Iter: 1371 loss: 6.42540954e-07
Iter: 1372 loss: 6.42503778e-07
Iter: 1373 loss: 6.42492409e-07
Iter: 1374 loss: 6.42438067e-07
Iter: 1375 loss: 6.42389e-07
Iter: 1376 loss: 6.42340751e-07
Iter: 1377 loss: 6.42250825e-07
Iter: 1378 loss: 6.42554824e-07
Iter: 1379 loss: 6.42201769e-07
Iter: 1380 loss: 6.42133216e-07
Iter: 1381 loss: 6.42643954e-07
Iter: 1382 loss: 6.42135888e-07
Iter: 1383 loss: 6.4204778e-07
Iter: 1384 loss: 6.42028226e-07
Iter: 1385 loss: 6.4201231e-07
Iter: 1386 loss: 6.41876e-07
Iter: 1387 loss: 6.41995825e-07
Iter: 1388 loss: 6.41789711e-07
Iter: 1389 loss: 6.41740314e-07
Iter: 1390 loss: 6.42555733e-07
Iter: 1391 loss: 6.41716667e-07
Iter: 1392 loss: 6.41626343e-07
Iter: 1393 loss: 6.41979341e-07
Iter: 1394 loss: 6.41596159e-07
Iter: 1395 loss: 6.41560177e-07
Iter: 1396 loss: 6.41645556e-07
Iter: 1397 loss: 6.41541078e-07
Iter: 1398 loss: 6.41437339e-07
Iter: 1399 loss: 6.4154608e-07
Iter: 1400 loss: 6.41406587e-07
Iter: 1401 loss: 6.41334395e-07
Iter: 1402 loss: 6.41378392e-07
Iter: 1403 loss: 6.41283293e-07
Iter: 1404 loss: 6.41224e-07
Iter: 1405 loss: 6.41348379e-07
Iter: 1406 loss: 6.4118808e-07
Iter: 1407 loss: 6.41131862e-07
Iter: 1408 loss: 6.41141696e-07
Iter: 1409 loss: 6.41073825e-07
Iter: 1410 loss: 6.41106055e-07
Iter: 1411 loss: 6.41039094e-07
Iter: 1412 loss: 6.41004e-07
Iter: 1413 loss: 6.41191036e-07
Iter: 1414 loss: 6.40980033e-07
Iter: 1415 loss: 6.40919438e-07
Iter: 1416 loss: 6.40906137e-07
Iter: 1417 loss: 6.40863959e-07
Iter: 1418 loss: 6.40792337e-07
Iter: 1419 loss: 6.40847418e-07
Iter: 1420 loss: 6.40718895e-07
Iter: 1421 loss: 6.40598273e-07
Iter: 1422 loss: 6.40656936e-07
Iter: 1423 loss: 6.40510905e-07
Iter: 1424 loss: 6.40467533e-07
Iter: 1425 loss: 6.4042996e-07
Iter: 1426 loss: 6.40370558e-07
Iter: 1427 loss: 6.40238e-07
Iter: 1428 loss: 6.42011855e-07
Iter: 1429 loss: 6.40250164e-07
Iter: 1430 loss: 6.40144037e-07
Iter: 1431 loss: 6.40143298e-07
Iter: 1432 loss: 6.4008367e-07
Iter: 1433 loss: 6.40034159e-07
Iter: 1434 loss: 6.40013e-07
Iter: 1435 loss: 6.39955829e-07
Iter: 1436 loss: 6.39879318e-07
Iter: 1437 loss: 6.3982236e-07
Iter: 1438 loss: 6.3971936e-07
Iter: 1439 loss: 6.40570761e-07
Iter: 1440 loss: 6.39696736e-07
Iter: 1441 loss: 6.39659845e-07
Iter: 1442 loss: 6.40108453e-07
Iter: 1443 loss: 6.39665245e-07
Iter: 1444 loss: 6.39585778e-07
Iter: 1445 loss: 6.39683549e-07
Iter: 1446 loss: 6.39539792e-07
Iter: 1447 loss: 6.39487553e-07
Iter: 1448 loss: 6.39669565e-07
Iter: 1449 loss: 6.3941809e-07
Iter: 1450 loss: 6.39374548e-07
Iter: 1451 loss: 6.39364885e-07
Iter: 1452 loss: 6.39308155e-07
Iter: 1453 loss: 6.39211066e-07
Iter: 1454 loss: 6.39179518e-07
Iter: 1455 loss: 6.39116536e-07
Iter: 1456 loss: 6.39059635e-07
Iter: 1457 loss: 6.39040195e-07
Iter: 1458 loss: 6.38962433e-07
Iter: 1459 loss: 6.38952315e-07
Iter: 1460 loss: 6.38897177e-07
Iter: 1461 loss: 6.38842096e-07
Iter: 1462 loss: 6.38970619e-07
Iter: 1463 loss: 6.38800088e-07
Iter: 1464 loss: 6.38670144e-07
Iter: 1465 loss: 6.39070549e-07
Iter: 1466 loss: 6.38665426e-07
Iter: 1467 loss: 6.38584709e-07
Iter: 1468 loss: 6.38469487e-07
Iter: 1469 loss: 6.38440724e-07
Iter: 1470 loss: 6.3834392e-07
Iter: 1471 loss: 6.38355232e-07
Iter: 1472 loss: 6.38284064e-07
Iter: 1473 loss: 6.38398e-07
Iter: 1474 loss: 6.3821733e-07
Iter: 1475 loss: 6.38100232e-07
Iter: 1476 loss: 6.38325901e-07
Iter: 1477 loss: 6.38051915e-07
Iter: 1478 loss: 6.37984954e-07
Iter: 1479 loss: 6.37983703e-07
Iter: 1480 loss: 6.3795494e-07
Iter: 1481 loss: 6.3784006e-07
Iter: 1482 loss: 6.39507221e-07
Iter: 1483 loss: 6.37829601e-07
Iter: 1484 loss: 6.3770733e-07
Iter: 1485 loss: 6.38271672e-07
Iter: 1486 loss: 6.37682e-07
Iter: 1487 loss: 6.37626385e-07
Iter: 1488 loss: 6.38190954e-07
Iter: 1489 loss: 6.37628773e-07
Iter: 1490 loss: 6.37554308e-07
Iter: 1491 loss: 6.38264964e-07
Iter: 1492 loss: 6.37533503e-07
Iter: 1493 loss: 6.37539472e-07
Iter: 1494 loss: 6.37553399e-07
Iter: 1495 loss: 6.37555559e-07
Iter: 1496 loss: 6.37571873e-07
Iter: 1497 loss: 6.37553512e-07
Iter: 1498 loss: 6.37550897e-07
Iter: 1499 loss: 6.37557719e-07
Iter: 1500 loss: 6.37530206e-07
Iter: 1501 loss: 6.37549e-07
Iter: 1502 loss: 6.37532366e-07
Iter: 1503 loss: 6.37550329e-07
Iter: 1504 loss: 6.37546293e-07
Iter: 1505 loss: 6.37535265e-07
Iter: 1506 loss: 6.37536061e-07
Iter: 1507 loss: 6.37535209e-07
Iter: 1508 loss: 6.3752924e-07
Iter: 1509 loss: 6.37533503e-07
Iter: 1510 loss: 6.37535095e-07
Iter: 1511 loss: 6.37534527e-07
Iter: 1512 loss: 6.37533731e-07
Iter: 1513 loss: 6.37534299e-07
Iter: 1514 loss: 6.37533844e-07
Iter: 1515 loss: 6.37534413e-07
Iter: 1516 loss: 6.37534413e-07
Iter: 1517 loss: 6.37534413e-07
Iter: 1518 loss: 6.37533844e-07
Iter: 1519 loss: 6.37534413e-07
Iter: 1520 loss: 6.37534413e-07
Iter: 1521 loss: 6.37534413e-07
Iter: 1522 loss: 6.37534413e-07
Iter: 1523 loss: 6.37533844e-07
Iter: 1524 loss: 6.37534413e-07
Iter: 1525 loss: 6.39612665e-07
Iter: 1526 loss: 6.3746694e-07
Iter: 1527 loss: 6.37413905e-07
Iter: 1528 loss: 6.37422545e-07
Iter: 1529 loss: 6.37332732e-07
Iter: 1530 loss: 6.37415155e-07
Iter: 1531 loss: 6.37311189e-07
Iter: 1532 loss: 6.37251787e-07
Iter: 1533 loss: 6.37259745e-07
Iter: 1534 loss: 6.37208132e-07
Iter: 1535 loss: 6.37105359e-07
Iter: 1536 loss: 6.37445225e-07
Iter: 1537 loss: 6.37073128e-07
Iter: 1538 loss: 6.36965638e-07
Iter: 1539 loss: 6.37355129e-07
Iter: 1540 loss: 6.36938751e-07
Iter: 1541 loss: 6.36823643e-07
Iter: 1542 loss: 6.37293624e-07
Iter: 1543 loss: 6.36781124e-07
Iter: 1544 loss: 6.36717e-07
Iter: 1545 loss: 6.36923e-07
Iter: 1546 loss: 6.36688924e-07
Iter: 1547 loss: 6.36587174e-07
Iter: 1548 loss: 6.36745085e-07
Iter: 1549 loss: 6.36526806e-07
Iter: 1550 loss: 6.36450864e-07
Iter: 1551 loss: 6.36621678e-07
Iter: 1552 loss: 6.36381969e-07
Iter: 1553 loss: 6.36329275e-07
Iter: 1554 loss: 6.36319555e-07
Iter: 1555 loss: 6.36252878e-07
Iter: 1556 loss: 6.36247137e-07
Iter: 1557 loss: 6.36210359e-07
Iter: 1558 loss: 6.36126117e-07
Iter: 1559 loss: 6.36103778e-07
Iter: 1560 loss: 6.36086384e-07
Iter: 1561 loss: 6.35997139e-07
Iter: 1562 loss: 6.35971787e-07
Iter: 1563 loss: 6.35925346e-07
Iter: 1564 loss: 6.35799722e-07
Iter: 1565 loss: 6.36970753e-07
Iter: 1566 loss: 6.35809329e-07
Iter: 1567 loss: 6.3572918e-07
Iter: 1568 loss: 6.36007371e-07
Iter: 1569 loss: 6.35710364e-07
Iter: 1570 loss: 6.35663639e-07
Iter: 1571 loss: 6.35579738e-07
Iter: 1572 loss: 6.35560582e-07
Iter: 1573 loss: 6.35447691e-07
Iter: 1574 loss: 6.36331606e-07
Iter: 1575 loss: 6.35459401e-07
Iter: 1576 loss: 6.35362369e-07
Iter: 1577 loss: 6.3541944e-07
Iter: 1578 loss: 6.35295237e-07
Iter: 1579 loss: 6.35186439e-07
Iter: 1580 loss: 6.36172331e-07
Iter: 1581 loss: 6.3521486e-07
Iter: 1582 loss: 6.35120387e-07
Iter: 1583 loss: 6.35017955e-07
Iter: 1584 loss: 6.35005108e-07
Iter: 1585 loss: 6.34879143e-07
Iter: 1586 loss: 6.34914159e-07
Iter: 1587 loss: 6.34756702e-07
Iter: 1588 loss: 6.34590663e-07
Iter: 1589 loss: 6.34854473e-07
Iter: 1590 loss: 6.34529556e-07
Iter: 1591 loss: 6.34328444e-07
Iter: 1592 loss: 6.36875086e-07
Iter: 1593 loss: 6.34333674e-07
Iter: 1594 loss: 6.34196738e-07
Iter: 1595 loss: 6.35822289e-07
Iter: 1596 loss: 6.34207254e-07
Iter: 1597 loss: 6.34142111e-07
Iter: 1598 loss: 6.33977834e-07
Iter: 1599 loss: 6.37217227e-07
Iter: 1600 loss: 6.33985906e-07
Iter: 1601 loss: 6.33774334e-07
Iter: 1602 loss: 6.34185312e-07
Iter: 1603 loss: 6.33714649e-07
Iter: 1604 loss: 6.33561228e-07
Iter: 1605 loss: 6.33752222e-07
Iter: 1606 loss: 6.33486e-07
Iter: 1607 loss: 6.33356365e-07
Iter: 1608 loss: 6.3519667e-07
Iter: 1609 loss: 6.33346929e-07
Iter: 1610 loss: 6.33187312e-07
Iter: 1611 loss: 6.33685204e-07
Iter: 1612 loss: 6.33184357e-07
Iter: 1613 loss: 6.33117338e-07
Iter: 1614 loss: 6.33290938e-07
Iter: 1615 loss: 6.33091645e-07
Iter: 1616 loss: 6.33015759e-07
Iter: 1617 loss: 6.33387799e-07
Iter: 1618 loss: 6.32987906e-07
Iter: 1619 loss: 6.32913782e-07
Iter: 1620 loss: 6.33051059e-07
Iter: 1621 loss: 6.32901902e-07
Iter: 1622 loss: 6.32812657e-07
Iter: 1623 loss: 6.33094714e-07
Iter: 1624 loss: 6.32807e-07
Iter: 1625 loss: 6.32731826e-07
Iter: 1626 loss: 6.32591309e-07
Iter: 1627 loss: 6.32567321e-07
Iter: 1628 loss: 6.32422029e-07
Iter: 1629 loss: 6.32975684e-07
Iter: 1630 loss: 6.32384172e-07
Iter: 1631 loss: 6.322378e-07
Iter: 1632 loss: 6.32220235e-07
Iter: 1633 loss: 6.32142928e-07
Iter: 1634 loss: 6.32061415e-07
Iter: 1635 loss: 6.32051695e-07
Iter: 1636 loss: 6.31930391e-07
Iter: 1637 loss: 6.31923683e-07
Iter: 1638 loss: 6.31812441e-07
Iter: 1639 loss: 6.31647765e-07
Iter: 1640 loss: 6.32857223e-07
Iter: 1641 loss: 6.31643729e-07
Iter: 1642 loss: 6.31504577e-07
Iter: 1643 loss: 6.31450689e-07
Iter: 1644 loss: 6.31395835e-07
Iter: 1645 loss: 6.31279875e-07
Iter: 1646 loss: 6.31281694e-07
Iter: 1647 loss: 6.31157604e-07
Iter: 1648 loss: 6.31503838e-07
Iter: 1649 loss: 6.31106445e-07
Iter: 1650 loss: 6.31006856e-07
Iter: 1651 loss: 6.31129183e-07
Iter: 1652 loss: 6.30966383e-07
Iter: 1653 loss: 6.30912382e-07
Iter: 1654 loss: 6.31780381e-07
Iter: 1655 loss: 6.30878617e-07
Iter: 1656 loss: 6.30805062e-07
Iter: 1657 loss: 6.30756688e-07
Iter: 1658 loss: 6.30742e-07
Iter: 1659 loss: 6.306459e-07
Iter: 1660 loss: 6.31009243e-07
Iter: 1661 loss: 6.3061691e-07
Iter: 1662 loss: 6.30573368e-07
Iter: 1663 loss: 6.30556485e-07
Iter: 1664 loss: 6.30521299e-07
Iter: 1665 loss: 6.30436546e-07
Iter: 1666 loss: 6.32125136e-07
Iter: 1667 loss: 6.30443424e-07
Iter: 1668 loss: 6.30346392e-07
Iter: 1669 loss: 6.30906356e-07
Iter: 1670 loss: 6.30312854e-07
Iter: 1671 loss: 6.3022793e-07
Iter: 1672 loss: 6.30397722e-07
Iter: 1673 loss: 6.30179898e-07
Iter: 1674 loss: 6.30127772e-07
Iter: 1675 loss: 6.29968042e-07
Iter: 1676 loss: 6.29978558e-07
Iter: 1677 loss: 6.29817578e-07
Iter: 1678 loss: 6.30778629e-07
Iter: 1679 loss: 6.29791487e-07
Iter: 1680 loss: 6.29634656e-07
Iter: 1681 loss: 6.30300178e-07
Iter: 1682 loss: 6.29624651e-07
Iter: 1683 loss: 6.29588726e-07
Iter: 1684 loss: 6.2957281e-07
Iter: 1685 loss: 6.29510168e-07
Iter: 1686 loss: 6.29488568e-07
Iter: 1687 loss: 6.29490785e-07
Iter: 1688 loss: 6.293875e-07
Iter: 1689 loss: 6.29326451e-07
Iter: 1690 loss: 6.29337308e-07
Iter: 1691 loss: 6.29230726e-07
Iter: 1692 loss: 6.30289264e-07
Iter: 1693 loss: 6.29236922e-07
Iter: 1694 loss: 6.29148758e-07
Iter: 1695 loss: 6.29405179e-07
Iter: 1696 loss: 6.29141255e-07
Iter: 1697 loss: 6.2904e-07
Iter: 1698 loss: 6.29104477e-07
Iter: 1699 loss: 6.28983685e-07
Iter: 1700 loss: 6.28948442e-07
Iter: 1701 loss: 6.28942132e-07
Iter: 1702 loss: 6.28899102e-07
Iter: 1703 loss: 6.28760574e-07
Iter: 1704 loss: 6.29102828e-07
Iter: 1705 loss: 6.28663827e-07
Iter: 1706 loss: 6.28529449e-07
Iter: 1707 loss: 6.30086674e-07
Iter: 1708 loss: 6.28524617e-07
Iter: 1709 loss: 6.28441342e-07
Iter: 1710 loss: 6.28562134e-07
Iter: 1711 loss: 6.28384e-07
Iter: 1712 loss: 6.28292071e-07
Iter: 1713 loss: 6.28285079e-07
Iter: 1714 loss: 6.28232726e-07
Iter: 1715 loss: 6.28208568e-07
Iter: 1716 loss: 6.28148882e-07
Iter: 1717 loss: 6.2808067e-07
Iter: 1718 loss: 6.28265e-07
Iter: 1719 loss: 6.28039174e-07
Iter: 1720 loss: 6.27944e-07
Iter: 1721 loss: 6.28315e-07
Iter: 1722 loss: 6.27942256e-07
Iter: 1723 loss: 6.27845907e-07
Iter: 1724 loss: 6.2781811e-07
Iter: 1725 loss: 6.27799864e-07
Iter: 1726 loss: 6.27686632e-07
Iter: 1727 loss: 6.2824904e-07
Iter: 1728 loss: 6.27662e-07
Iter: 1729 loss: 6.27600798e-07
Iter: 1730 loss: 6.28122962e-07
Iter: 1731 loss: 6.27588804e-07
Iter: 1732 loss: 6.2749217e-07
Iter: 1733 loss: 6.27534632e-07
Iter: 1734 loss: 6.27467557e-07
Iter: 1735 loss: 6.27391387e-07
Iter: 1736 loss: 6.28699866e-07
Iter: 1737 loss: 6.27361715e-07
Iter: 1738 loss: 6.27318911e-07
Iter: 1739 loss: 6.27205964e-07
Iter: 1740 loss: 6.28994258e-07
Iter: 1741 loss: 6.27202894e-07
Iter: 1742 loss: 6.27111717e-07
Iter: 1743 loss: 6.2714787e-07
Iter: 1744 loss: 6.27032477e-07
Iter: 1745 loss: 6.26931296e-07
Iter: 1746 loss: 6.27939357e-07
Iter: 1747 loss: 6.26943915e-07
Iter: 1748 loss: 6.26841711e-07
Iter: 1749 loss: 6.27659347e-07
Iter: 1750 loss: 6.26851602e-07
Iter: 1751 loss: 6.26782594e-07
Iter: 1752 loss: 6.26598649e-07
Iter: 1753 loss: 6.29541319e-07
Iter: 1754 loss: 6.26621272e-07
Iter: 1755 loss: 6.2657756e-07
Iter: 1756 loss: 6.26544193e-07
Iter: 1757 loss: 6.26518045e-07
Iter: 1758 loss: 6.26415726e-07
Iter: 1759 loss: 6.26405267e-07
Iter: 1760 loss: 6.26298515e-07
Iter: 1761 loss: 6.2654567e-07
Iter: 1762 loss: 6.2624207e-07
Iter: 1763 loss: 6.26139297e-07
Iter: 1764 loss: 6.26531573e-07
Iter: 1765 loss: 6.26147539e-07
Iter: 1766 loss: 6.26025894e-07
Iter: 1767 loss: 6.26559086e-07
Iter: 1768 loss: 6.26030896e-07
Iter: 1769 loss: 6.25972689e-07
Iter: 1770 loss: 6.26251961e-07
Iter: 1771 loss: 6.25979681e-07
Iter: 1772 loss: 6.25910275e-07
Iter: 1773 loss: 6.26114229e-07
Iter: 1774 loss: 6.2590334e-07
Iter: 1775 loss: 6.25875259e-07
Iter: 1776 loss: 6.25728717e-07
Iter: 1777 loss: 6.26815563e-07
Iter: 1778 loss: 6.25728e-07
Iter: 1779 loss: 6.25612245e-07
Iter: 1780 loss: 6.26115252e-07
Iter: 1781 loss: 6.25610937e-07
Iter: 1782 loss: 6.25497819e-07
Iter: 1783 loss: 6.26868427e-07
Iter: 1784 loss: 6.25494067e-07
Iter: 1785 loss: 6.25418807e-07
Iter: 1786 loss: 6.25762198e-07
Iter: 1787 loss: 6.25420341e-07
Iter: 1788 loss: 6.25349116e-07
Iter: 1789 loss: 6.25271355e-07
Iter: 1790 loss: 6.25275447e-07
Iter: 1791 loss: 6.25186658e-07
Iter: 1792 loss: 6.25196662e-07
Iter: 1793 loss: 6.25144139e-07
Iter: 1794 loss: 6.25096504e-07
Iter: 1795 loss: 6.25058078e-07
Iter: 1796 loss: 6.24961217e-07
Iter: 1797 loss: 6.24868e-07
Iter: 1798 loss: 6.24848e-07
Iter: 1799 loss: 6.24737822e-07
Iter: 1800 loss: 6.26573808e-07
Iter: 1801 loss: 6.24751237e-07
Iter: 1802 loss: 6.24653239e-07
Iter: 1803 loss: 6.24912218e-07
Iter: 1804 loss: 6.24616064e-07
Iter: 1805 loss: 6.24545919e-07
Iter: 1806 loss: 6.24865265e-07
Iter: 1807 loss: 6.24499762e-07
Iter: 1808 loss: 6.24406312e-07
Iter: 1809 loss: 6.24366919e-07
Iter: 1810 loss: 6.24337304e-07
Iter: 1811 loss: 6.24198663e-07
Iter: 1812 loss: 6.2399846e-07
Iter: 1813 loss: 6.2401989e-07
Iter: 1814 loss: 6.23772792e-07
Iter: 1815 loss: 6.25221332e-07
Iter: 1816 loss: 6.23761821e-07
Iter: 1817 loss: 6.23685423e-07
Iter: 1818 loss: 6.23686e-07
Iter: 1819 loss: 6.23574806e-07
Iter: 1820 loss: 6.23684116e-07
Iter: 1821 loss: 6.2355025e-07
Iter: 1822 loss: 6.23467258e-07
Iter: 1823 loss: 6.23584e-07
Iter: 1824 loss: 6.23458732e-07
Iter: 1825 loss: 6.23347887e-07
Iter: 1826 loss: 6.23910239e-07
Iter: 1827 loss: 6.23375911e-07
Iter: 1828 loss: 6.23293772e-07
Iter: 1829 loss: 6.2328229e-07
Iter: 1830 loss: 6.23245e-07
Iter: 1831 loss: 6.2317406e-07
Iter: 1832 loss: 6.23437245e-07
Iter: 1833 loss: 6.23152289e-07
Iter: 1834 loss: 6.23104484e-07
Iter: 1835 loss: 6.23464757e-07
Iter: 1836 loss: 6.23068e-07
Iter: 1837 loss: 6.22971129e-07
Iter: 1838 loss: 6.23222263e-07
Iter: 1839 loss: 6.22935318e-07
Iter: 1840 loss: 6.22843743e-07
Iter: 1841 loss: 6.22904849e-07
Iter: 1842 loss: 6.22813445e-07
Iter: 1843 loss: 6.22723519e-07
Iter: 1844 loss: 6.22768937e-07
Iter: 1845 loss: 6.22676964e-07
Iter: 1846 loss: 6.22541791e-07
Iter: 1847 loss: 6.22354378e-07
Iter: 1848 loss: 6.22339201e-07
Iter: 1849 loss: 6.22197604e-07
Iter: 1850 loss: 6.23188384e-07
Iter: 1851 loss: 6.22193397e-07
Iter: 1852 loss: 6.22103244e-07
Iter: 1853 loss: 6.22092443e-07
Iter: 1854 loss: 6.22025823e-07
Iter: 1855 loss: 6.21948743e-07
Iter: 1856 loss: 6.21934078e-07
Iter: 1857 loss: 6.2179754e-07
Iter: 1858 loss: 6.21991489e-07
Iter: 1859 loss: 6.21738081e-07
Iter: 1860 loss: 6.21604329e-07
Iter: 1861 loss: 6.21618256e-07
Iter: 1862 loss: 6.21564823e-07
Iter: 1863 loss: 6.21478307e-07
Iter: 1864 loss: 6.214716e-07
Iter: 1865 loss: 6.21343e-07
Iter: 1866 loss: 6.22103869e-07
Iter: 1867 loss: 6.21349614e-07
Iter: 1868 loss: 6.21287768e-07
Iter: 1869 loss: 6.2204083e-07
Iter: 1870 loss: 6.21258721e-07
Iter: 1871 loss: 6.21219e-07
Iter: 1872 loss: 6.21374284e-07
Iter: 1873 loss: 6.21226832e-07
Iter: 1874 loss: 6.21157e-07
Iter: 1875 loss: 6.2104516e-07
Iter: 1876 loss: 6.21022934e-07
Iter: 1877 loss: 6.20943524e-07
Iter: 1878 loss: 6.21041181e-07
Iter: 1879 loss: 6.20896571e-07
Iter: 1880 loss: 6.20773676e-07
Iter: 1881 loss: 6.20897367e-07
Iter: 1882 loss: 6.20727064e-07
Iter: 1883 loss: 6.20573246e-07
Iter: 1884 loss: 6.20886112e-07
Iter: 1885 loss: 6.20542323e-07
Iter: 1886 loss: 6.2048673e-07
Iter: 1887 loss: 6.20447395e-07
Iter: 1888 loss: 6.20378273e-07
Iter: 1889 loss: 6.2025515e-07
Iter: 1890 loss: 6.21739787e-07
Iter: 1891 loss: 6.20247533e-07
Iter: 1892 loss: 6.20088088e-07
Iter: 1893 loss: 6.21015545e-07
Iter: 1894 loss: 6.20084336e-07
Iter: 1895 loss: 6.19931825e-07
Iter: 1896 loss: 6.20640208e-07
Iter: 1897 loss: 6.19873163e-07
Iter: 1898 loss: 6.1974481e-07
Iter: 1899 loss: 6.19787e-07
Iter: 1900 loss: 6.1966432e-07
Iter: 1901 loss: 6.19531477e-07
Iter: 1902 loss: 6.20419826e-07
Iter: 1903 loss: 6.19542e-07
Iter: 1904 loss: 6.19406592e-07
Iter: 1905 loss: 6.19857e-07
Iter: 1906 loss: 6.19383627e-07
Iter: 1907 loss: 6.19299101e-07
Iter: 1908 loss: 6.20007143e-07
Iter: 1909 loss: 6.19244304e-07
Iter: 1910 loss: 6.1921719e-07
Iter: 1911 loss: 6.19232708e-07
Iter: 1912 loss: 6.19193202e-07
Iter: 1913 loss: 6.19124876e-07
Iter: 1914 loss: 6.18966055e-07
Iter: 1915 loss: 6.18959461e-07
Iter: 1916 loss: 6.18827926e-07
Iter: 1917 loss: 6.19959678e-07
Iter: 1918 loss: 6.18829745e-07
Iter: 1919 loss: 6.18700483e-07
Iter: 1920 loss: 6.18754768e-07
Iter: 1921 loss: 6.18646823e-07
Iter: 1922 loss: 6.18519778e-07
Iter: 1923 loss: 6.18524723e-07
Iter: 1924 loss: 6.18475838e-07
Iter: 1925 loss: 6.18369882e-07
Iter: 1926 loss: 6.19697744e-07
Iter: 1927 loss: 6.1831895e-07
Iter: 1928 loss: 6.18249828e-07
Iter: 1929 loss: 6.1824511e-07
Iter: 1930 loss: 6.18181616e-07
Iter: 1931 loss: 6.1812591e-07
Iter: 1932 loss: 6.18114086e-07
Iter: 1933 loss: 6.17991304e-07
Iter: 1934 loss: 6.18347e-07
Iter: 1935 loss: 6.17971182e-07
Iter: 1936 loss: 6.1790513e-07
Iter: 1937 loss: 6.18055196e-07
Iter: 1938 loss: 6.17885803e-07
Iter: 1939 loss: 6.17777232e-07
Iter: 1940 loss: 6.1892581e-07
Iter: 1941 loss: 6.17771434e-07
Iter: 1942 loss: 6.17726187e-07
Iter: 1943 loss: 6.17659452e-07
Iter: 1944 loss: 6.17655928e-07
Iter: 1945 loss: 6.17551905e-07
Iter: 1946 loss: 6.17785531e-07
Iter: 1947 loss: 6.17543719e-07
Iter: 1948 loss: 6.17435774e-07
Iter: 1949 loss: 6.17672868e-07
Iter: 1950 loss: 6.17393084e-07
Iter: 1951 loss: 6.1725126e-07
Iter: 1952 loss: 6.17131263e-07
Iter: 1953 loss: 6.17123192e-07
Iter: 1954 loss: 6.17139449e-07
Iter: 1955 loss: 6.17059527e-07
Iter: 1956 loss: 6.1696278e-07
Iter: 1957 loss: 6.16822206e-07
Iter: 1958 loss: 6.19573939e-07
Iter: 1959 loss: 6.16840566e-07
Iter: 1960 loss: 6.1669607e-07
Iter: 1961 loss: 6.17178955e-07
Iter: 1962 loss: 6.16674242e-07
Iter: 1963 loss: 6.16616717e-07
Iter: 1964 loss: 6.16604098e-07
Iter: 1965 loss: 6.16532532e-07
Iter: 1966 loss: 6.1652031e-07
Iter: 1967 loss: 6.16497914e-07
Iter: 1968 loss: 6.16421403e-07
Iter: 1969 loss: 6.16593127e-07
Iter: 1970 loss: 6.16377235e-07
Iter: 1971 loss: 6.16315901e-07
Iter: 1972 loss: 6.16949e-07
Iter: 1973 loss: 6.16263947e-07
Iter: 1974 loss: 6.16263378e-07
Iter: 1975 loss: 6.16373e-07
Iter: 1976 loss: 6.16203693e-07
Iter: 1977 loss: 6.16121554e-07
Iter: 1978 loss: 6.16106945e-07
Iter: 1979 loss: 6.16062039e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2
+ date
Wed Oct 21 12:46:20 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/300_300_300_1 --function f1 --psi 0 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0ffa98510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0ffa5f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0ffaf9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d5522730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d556cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d556c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d554f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d54601e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d5460268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d54608c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d54b81e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b05359d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b053f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d54b67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b04fad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d54cda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b04fa598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b0474488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b0474598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b040a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b0427620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b03680d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b0427f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b0370840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b03af268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02d1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02d1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02e32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02e3598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02618c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02621e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b03078c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b01b1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02b4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b01b2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b0202f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.042416587
test_loss: 0.04139553
train_loss: 0.009857872
test_loss: 0.0097349025
train_loss: 0.0049662725
test_loss: 0.004941139
train_loss: 0.003279291
test_loss: 0.0037324051
train_loss: 0.0033179503
test_loss: 0.0029821475
train_loss: 0.0025646465
test_loss: 0.0024677187
train_loss: 0.0025650405
test_loss: 0.0026588333
train_loss: 0.0026821159
test_loss: 0.0026260447
train_loss: 0.0019910883
test_loss: 0.002125526
train_loss: 0.0025287576
test_loss: 0.0021311634
train_loss: 0.0021998337
test_loss: 0.0024981585
train_loss: 0.0020497264
test_loss: 0.0022053905
train_loss: 0.002224956
test_loss: 0.002001827
train_loss: 0.0019986124
test_loss: 0.0025447714
train_loss: 0.0019258435
test_loss: 0.002252581
train_loss: 0.0019413858
test_loss: 0.0028204299
train_loss: 0.00231431
test_loss: 0.0020137809
train_loss: 0.0022379986
test_loss: 0.0021666137
train_loss: 0.0020246883
test_loss: 0.0024672227
train_loss: 0.0019454774
test_loss: 0.0018006756
train_loss: 0.0016822859
test_loss: 0.0017636578
train_loss: 0.0022464423
test_loss: 0.0018668985
train_loss: 0.0022442539
test_loss: 0.0018224488
train_loss: 0.0021904928
test_loss: 0.0022407777
train_loss: 0.0024497379
test_loss: 0.002867468
train_loss: 0.0021002183
test_loss: 0.002209691
train_loss: 0.0018111864
test_loss: 0.001794736
train_loss: 0.002131275
test_loss: 0.0020525954
train_loss: 0.0018667819
test_loss: 0.0022992308
train_loss: 0.0018993014
test_loss: 0.0018459025
train_loss: 0.0023347056
test_loss: 0.0021350053
train_loss: 0.0016453192
test_loss: 0.001954487
train_loss: 0.002524483
test_loss: 0.00305972
train_loss: 0.003511337
test_loss: 0.0033306738
train_loss: 0.0031476808
test_loss: 0.0037367314
train_loss: 0.0023579902
test_loss: 0.0028138286
train_loss: 0.0035255696
test_loss: 0.0028400838
train_loss: 0.0027876692
test_loss: 0.002883296
train_loss: 0.0027308848
test_loss: 0.0025845212
train_loss: 0.0022093365
test_loss: 0.0019937688
train_loss: 0.002842882
test_loss: 0.0021719525
train_loss: 0.002557864
test_loss: 0.001961604
train_loss: 0.0019304573
test_loss: 0.0020076656
train_loss: 0.0024429602
test_loss: 0.0019148845
train_loss: 0.001976118
test_loss: 0.0021244634
train_loss: 0.0018249684
test_loss: 0.0024311054
train_loss: 0.0019384001
test_loss: 0.0018869451
train_loss: 0.0019590682
test_loss: 0.0021223908
train_loss: 0.0016904408
test_loss: 0.0020681554
train_loss: 0.0018385672
test_loss: 0.001619408
train_loss: 0.001882304
test_loss: 0.0016556571
train_loss: 0.0018127991
test_loss: 0.0018337858
train_loss: 0.0016711659
test_loss: 0.0015995186
train_loss: 0.0016056702
test_loss: 0.0019793184
train_loss: 0.0016807113
test_loss: 0.0016121479
train_loss: 0.0016639419
test_loss: 0.0017809843
train_loss: 0.002032394
test_loss: 0.0016948197
train_loss: 0.0020946627
test_loss: 0.001975542
train_loss: 0.0023679223
test_loss: 0.0021565207
train_loss: 0.0018711964
test_loss: 0.0020923843
train_loss: 0.002274768
test_loss: 0.0032546145
train_loss: 0.0024928818
test_loss: 0.0030991903
train_loss: 0.0025388238
test_loss: 0.0028872474
train_loss: 0.0019408856
test_loss: 0.00190508
train_loss: 0.0017086202
test_loss: 0.00172416
train_loss: 0.001539929
test_loss: 0.0016331119
train_loss: 0.001986675
test_loss: 0.001826091
train_loss: 0.0017989323
test_loss: 0.0020959587
train_loss: 0.0020630602
test_loss: 0.0020451369
train_loss: 0.0018071467
test_loss: 0.0019307403
train_loss: 0.0017687123
test_loss: 0.0018503086
train_loss: 0.0018221738
test_loss: 0.0016117974
train_loss: 0.001792416
test_loss: 0.0016202261
train_loss: 0.0021515307
test_loss: 0.0028006053
train_loss: 0.0020704714
test_loss: 0.002588455
train_loss: 0.0024512531
test_loss: 0.0027554776
train_loss: 0.00166821
test_loss: 0.0022275997
train_loss: 0.002285305
test_loss: 0.0017419494
train_loss: 0.0017354729
test_loss: 0.0018203848
train_loss: 0.0018384219
test_loss: 0.001637106
train_loss: 0.0021579266
test_loss: 0.0017753417
train_loss: 0.0018058242
test_loss: 0.001608822
train_loss: 0.0016712499
test_loss: 0.0016569749
train_loss: 0.0014993993
test_loss: 0.0016899062
train_loss: 0.0016751795
test_loss: 0.0016767209
train_loss: 0.0014455648
test_loss: 0.001631381
train_loss: 0.0016740888
test_loss: 0.0016937415
train_loss: 0.0015114147
test_loss: 0.001940931
train_loss: 0.0016413264
test_loss: 0.0019194079
train_loss: 0.0016314234
test_loss: 0.0016078468
train_loss: 0.0020148936
test_loss: 0.0016277005
train_loss: 0.0015432896
test_loss: 0.0017636399
train_loss: 0.0017367934
test_loss: 0.001964244
train_loss: 0.0019229583
test_loss: 0.0022081435
train_loss: 0.0018377504
test_loss: 0.0017186531
train_loss: 0.0015584256
test_loss: 0.0022541492
train_loss: 0.0016987226
test_loss: 0.0018897805
train_loss: 0.0017692207
test_loss: 0.0015876744
train_loss: 0.0020290324
test_loss: 0.0016579842
train_loss: 0.0019680103
test_loss: 0.0017542845
train_loss: 0.0015621508
test_loss: 0.0016407018
train_loss: 0.0017273037
test_loss: 0.0017953458
train_loss: 0.0014744096
test_loss: 0.0015936015
train_loss: 0.00190313
test_loss: 0.0016957741
train_loss: 0.0020204773
test_loss: 0.0015987777
train_loss: 0.0016995121
test_loss: 0.0018128024
train_loss: 0.0015620464
test_loss: 0.0021414468
train_loss: 0.0021142606
test_loss: 0.001917681
train_loss: 0.0018585408
test_loss: 0.0016993951
train_loss: 0.0016536074
test_loss: 0.0015911221
train_loss: 0.0020311621
test_loss: 0.0027038301
train_loss: 0.0019467036
test_loss: 0.002330311
train_loss: 0.0017962494
test_loss: 0.0018732857
train_loss: 0.0018897703
test_loss: 0.0018309602
train_loss: 0.0016266702
test_loss: 0.0019270759
train_loss: 0.0017679178
test_loss: 0.0015518677
train_loss: 0.0016396301
test_loss: 0.0017978624
train_loss: 0.0017756318
test_loss: 0.0020541584
train_loss: 0.001649586
test_loss: 0.0016750112
train_loss: 0.0019527819
test_loss: 0.0016012599
train_loss: 0.0015959012
test_loss: 0.0018556577
train_loss: 0.0022842502
test_loss: 0.0021830173
train_loss: 0.0018358813
test_loss: 0.001828471
train_loss: 0.0017152916
test_loss: 0.0019854226
train_loss: 0.001590343
test_loss: 0.0016098463
train_loss: 0.0018942216
test_loss: 0.0017149257
train_loss: 0.0016422725
test_loss: 0.0014002577
train_loss: 0.0016586324
test_loss: 0.0018785825
train_loss: 0.0022253455
test_loss: 0.0018725408
train_loss: 0.0016363865
test_loss: 0.0017698314
train_loss: 0.0021541663
test_loss: 0.0022218833
train_loss: 0.0022682333
test_loss: 0.0019263191
train_loss: 0.0017930872
test_loss: 0.0017779449
train_loss: 0.0017280998
test_loss: 0.0018828505
train_loss: 0.00207919
test_loss: 0.0019665656
train_loss: 0.001683199
test_loss: 0.001711281
train_loss: 0.0016076285
test_loss: 0.00179889
train_loss: 0.0016747594
test_loss: 0.0018168916
train_loss: 0.0022489221
test_loss: 0.0018219004
train_loss: 0.0017814899
test_loss: 0.0015374797
train_loss: 0.0018250127
test_loss: 0.0017295663
train_loss: 0.0022874193
test_loss: 0.0017969515
train_loss: 0.0016326325
test_loss: 0.0020652192
train_loss: 0.0016510999
test_loss: 0.0019866365
train_loss: 0.0020883926
test_loss: 0.0021392063
train_loss: 0.0032361876
test_loss: 0.0024249316
train_loss: 0.0028076589
test_loss: 0.0028565428
train_loss: 0.0026675418
test_loss: 0.0020266015
train_loss: 0.0018748095
test_loss: 0.0016664715
train_loss: 0.001613192
test_loss: 0.0016863288
train_loss: 0.0018933157
test_loss: 0.0017176765
train_loss: 0.0017073681
test_loss: 0.002309637
train_loss: 0.001611344
test_loss: 0.0019371123
train_loss: 0.001970003
test_loss: 0.0017136955
train_loss: 0.0016637282
test_loss: 0.0016873025
train_loss: 0.0015843962
test_loss: 0.0022258451
train_loss: 0.0020086227
test_loss: 0.0019262781
train_loss: 0.0015414294
test_loss: 0.0017540423
train_loss: 0.0016030432
test_loss: 0.0018808215
train_loss: 0.0015427447
test_loss: 0.0021475789
train_loss: 0.001812752
test_loss: 0.0019856317
train_loss: 0.0020501665
test_loss: 0.0024228077
train_loss: 0.0017230292
test_loss: 0.0017129299
train_loss: 0.0018010663
test_loss: 0.001393917
train_loss: 0.0015231608
test_loss: 0.0016031591
train_loss: 0.0017237333
test_loss: 0.0016870141
train_loss: 0.0018427407
test_loss: 0.0017763523
train_loss: 0.001707121
test_loss: 0.0023144914
train_loss: 0.0017940252
test_loss: 0.0016094762
train_loss: 0.0016109315
test_loss: 0.0020886252
train_loss: 0.0016411376
test_loss: 0.0018124237
train_loss: 0.0019333267
test_loss: 0.0019556626
train_loss: 0.0017975235
test_loss: 0.0016159329
train_loss: 0.0018503019
test_loss: 0.0017475716
train_loss: 0.0019398439
test_loss: 0.0021006984
train_loss: 0.0019830863
test_loss: 0.0015874417
train_loss: 0.001707475
test_loss: 0.0019592673
train_loss: 0.0017898488
test_loss: 0.0017107584
train_loss: 0.0019111056
test_loss: 0.0017273573
train_loss: 0.0014875765
test_loss: 0.0015465098
train_loss: 0.001835969
test_loss: 0.0021848832
train_loss: 0.0015174071
test_loss: 0.0016456885
train_loss: 0.001607694
test_loss: 0.002003151
train_loss: 0.0022411332
test_loss: 0.001574695
train_loss: 0.0016676446
test_loss: 0.0020327587
train_loss: 0.0017883439
test_loss: 0.0017311798
train_loss: 0.0017604752
test_loss: 0.0015600373
train_loss: 0.0016846218
test_loss: 0.0015226122
train_loss: 0.0016915158
test_loss: 0.0017945942
train_loss: 0.0016442479
test_loss: 0.0020207672
train_loss: 0.0019931353
test_loss: 0.0022773312
train_loss: 0.0026926906
test_loss: 0.001980295
train_loss: 0.00151742
test_loss: 0.0019190897
train_loss: 0.0017986486
test_loss: 0.0018792183
train_loss: 0.0025638822
test_loss: 0.0024571575
train_loss: 0.0017988625
test_loss: 0.0018002127
train_loss: 0.0017217057
test_loss: 0.001769533
train_loss: 0.0014243971
test_loss: 0.0015370337
train_loss: 0.0021420363
test_loss: 0.0017960343
train_loss: 0.0020291244
test_loss: 0.0018440115
train_loss: 0.0017206063
test_loss: 0.0016481085
train_loss: 0.0019494066
test_loss: 0.0017712212
train_loss: 0.0015420428
test_loss: 0.002198478
train_loss: 0.001987359
test_loss: 0.002012977
train_loss: 0.0019404148
test_loss: 0.0017896818
train_loss: 0.0015741198
test_loss: 0.0017450936
train_loss: 0.0018933095
test_loss: 0.0015688736
train_loss: 0.0018372175
test_loss: 0.0018685469
train_loss: 0.0021992442
test_loss: 0.0019560012
train_loss: 0.001779599
test_loss: 0.0017606254
train_loss: 0.0016947523
test_loss: 0.0017264847
train_loss: 0.001735568
test_loss: 0.0017616468
train_loss: 0.001682224
test_loss: 0.0018771357
train_loss: 0.0015511088
test_loss: 0.0013425712
train_loss: 0.0016881744
test_loss: 0.0016510908
train_loss: 0.0018446405
test_loss: 0.0015599336
train_loss: 0.0016445431
test_loss: 0.0016509419
train_loss: 0.0023160635
test_loss: 0.0018279812
train_loss: 0.0020026248
test_loss: 0.0018657332
train_loss: 0.0015789121
test_loss: 0.0017983424
train_loss: 0.0021070784
test_loss: 0.0021401178
train_loss: 0.0019746122
test_loss: 0.0022668268
train_loss: 0.0018765132
test_loss: 0.0018786844
train_loss: 0.0019800044
test_loss: 0.0018201419
train_loss: 0.0015015288
test_loss: 0.0014933443
train_loss: 0.0014914947
test_loss: 0.0014706483
train_loss: 0.0021939273
test_loss: 0.0018224878
train_loss: 0.0021066593
test_loss: 0.0025283182
train_loss: 0.0033423891
test_loss: 0.0026501692
train_loss: 0.0027152533
test_loss: 0.002815552
train_loss: 0.002603379
test_loss: 0.002109779
train_loss: 0.002706467
test_loss: 0.002698803
train_loss: 0.0026177242
test_loss: 0.0023334087
train_loss: 0.002599004
test_loss: 0.0024283575
train_loss: 0.0029184746
test_loss: 0.00274131
train_loss: 0.0025665509
test_loss: 0.002268018
train_loss: 0.002033954
test_loss: 0.0019314907
train_loss: 0.0016357402
test_loss: 0.001568041
train_loss: 0.0017455053
test_loss: 0.0020377832
train_loss: 0.001974842
test_loss: 0.001966537
train_loss: 0.0014623222
test_loss: 0.0021096359
train_loss: 0.0015492297
test_loss: 0.0020111115
train_loss: 0.0018896353
test_loss: 0.00185218
train_loss: 0.0014828641
test_loss: 0.002009786
train_loss: 0.0018642681
test_loss: 0.0019350682
train_loss: 0.0015392584
test_loss: 0.0015231316
train_loss: 0.0017151337
test_loss: 0.0016711652
train_loss: 0.0016291611
test_loss: 0.0015708993
train_loss: 0.0015417045
test_loss: 0.0015280126
train_loss: 0.0022749256
test_loss: 0.0017113058
train_loss: 0.0017456771
test_loss: 0.0027918345
train_loss: 0.002258473
test_loss: 0.0029745523
train_loss: 0.002263705
test_loss: 0.0025217717
train_loss: 0.0020649887
test_loss: 0.0018095337
train_loss: 0.0017527726
test_loss: 0.0022643292
train_loss: 0.0015812247
test_loss: 0.0019156131
train_loss: 0.0014904486
test_loss: 0.0016946627
train_loss: 0.0016319198
test_loss: 0.001448229
train_loss: 0.001844185
test_loss: 0.0017089456
train_loss: 0.0014647559
test_loss: 0.001854929
train_loss: 0.0016302865
test_loss: 0.0016796803
train_loss: 0.0016866399
test_loss: 0.0015797247
train_loss: 0.0017354263
test_loss: 0.0014930324
train_loss: 0.0014305932
test_loss: 0.0015768565
train_loss: 0.0016703774
test_loss: 0.0014094976
train_loss: 0.0014592662
test_loss: 0.0014212534
train_loss: 0.001728687
test_loss: 0.001636417
train_loss: 0.0016394503
test_loss: 0.001585615
train_loss: 0.002028092
test_loss: 0.001963207
train_loss: 0.0017147668
test_loss: 0.0018228355
train_loss: 0.001600907
test_loss: 0.0015807057
train_loss: 0.0017604949
test_loss: 0.0016741292
train_loss: 0.0015606701
test_loss: 0.0015693636
train_loss: 0.0018455576
test_loss: 0.0020203996
train_loss: 0.0020166906
test_loss: 0.0015183011
train_loss: 0.0016621557
test_loss: 0.0016285357
train_loss: 0.0017959625
test_loss: 0.0016377433
train_loss: 0.0015679286
test_loss: 0.0017075461
train_loss: 0.0013912593
test_loss: 0.0019233808
train_loss: 0.0018569897
test_loss: 0.0016960148
train_loss: 0.0014169803
test_loss: 0.0019337704
train_loss: 0.0018016418
test_loss: 0.001504231
train_loss: 0.0016709114
test_loss: 0.0015973654
train_loss: 0.0018798113
test_loss: 0.0020624513
train_loss: 0.0021419507
test_loss: 0.0018134452
train_loss: 0.001578904
test_loss: 0.0016081639
train_loss: 0.0015557689
test_loss: 0.0018169621
train_loss: 0.0017185452
test_loss: 0.001804663
train_loss: 0.0017948647
test_loss: 0.0017503244
train_loss: 0.0018682433
test_loss: 0.0015671909
train_loss: 0.0015899302
test_loss: 0.0018598143
train_loss: 0.0017360011
test_loss: 0.0016722707
train_loss: 0.0018938559
test_loss: 0.002098133
train_loss: 0.0014509865
test_loss: 0.0014422515
train_loss: 0.0018996892
test_loss: 0.0019311474
train_loss: 0.001508211
test_loss: 0.0016078786
train_loss: 0.0016239912
test_loss: 0.0013404964
train_loss: 0.0021609685
test_loss: 0.0015571862
train_loss: 0.0018488895
test_loss: 0.0018229483
train_loss: 0.0015386741
test_loss: 0.0016178649
train_loss: 0.0015653007
test_loss: 0.0016788129
train_loss: 0.0015952194
test_loss: 0.001689347
train_loss: 0.0019412363
test_loss: 0.0017410871
train_loss: 0.0021698491
test_loss: 0.0020270494
train_loss: 0.0021193514
test_loss: 0.0017769915
train_loss: 0.0018700949
test_loss: 0.0019114334
train_loss: 0.0020309745
test_loss: 0.0015554017
train_loss: 0.0019017162
test_loss: 0.0015795765
train_loss: 0.0021716985
test_loss: 0.0031848517
train_loss: 0.0024239025
test_loss: 0.0029914416
train_loss: 0.002330747
test_loss: 0.0022182357
train_loss: 0.001574032
test_loss: 0.0016391019
train_loss: 0.0015702824
test_loss: 0.0016732543
train_loss: 0.0016759804
test_loss: 0.0018310489
train_loss: 0.0017732417
test_loss: 0.0019100732
train_loss: 0.0016609055
test_loss: 0.0015965183
train_loss: 0.0017775988
test_loss: 0.0017048283
train_loss: 0.0016567857
test_loss: 0.0017433582
train_loss: 0.0017702953
test_loss: 0.0017357005
train_loss: 0.0014990554
test_loss: 0.0016388926
train_loss: 0.0016427597
test_loss: 0.0016435529
train_loss: 0.0018770484
test_loss: 0.0017719954
train_loss: 0.0015721954
test_loss: 0.001894413
train_loss: 0.002015405
test_loss: 0.0015027678
train_loss: 0.0014281073
test_loss: 0.0021483945
train_loss: 0.0015951224
test_loss: 0.0021121744
train_loss: 0.0021858066
test_loss: 0.0018336688
train_loss: 0.0019288564
test_loss: 0.001910683
train_loss: 0.0015607106
test_loss: 0.0016758998
train_loss: 0.0016142485
test_loss: 0.002537625
train_loss: 0.0018842551
test_loss: 0.0016486754
train_loss: 0.0015981109
test_loss: 0.0014156477
train_loss: 0.0017939374
test_loss: 0.001720991
train_loss: 0.0016633826
test_loss: 0.001358864
train_loss: 0.0016321559
test_loss: 0.0018232468
train_loss: 0.0016861092
test_loss: 0.0017893167
train_loss: 0.0015744702
test_loss: 0.0015850618
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0016896459
test_loss: 0.0017861909
train_loss: 0.0017326658
test_loss: 0.0021512476
train_loss: 0.0017774376
test_loss: 0.0015381953
train_loss: 0.0021076333
test_loss: 0.0017600888
train_loss: 0.0017391146
test_loss: 0.001492671
train_loss: 0.0016680597
test_loss: 0.0019616534
train_loss: 0.0019107346
test_loss: 0.0018579811
train_loss: 0.0017055764
test_loss: 0.0019698488
train_loss: 0.0018884059
test_loss: 0.0016008577
train_loss: 0.00219973
test_loss: 0.0018101488
train_loss: 0.0018056817
test_loss: 0.0016982694
train_loss: 0.0018181573
test_loss: 0.0014264275
train_loss: 0.0019447352
test_loss: 0.0017524667
train_loss: 0.0017145805
test_loss: 0.0018091346
train_loss: 0.0017530745
test_loss: 0.0020380658
train_loss: 0.0015640282
test_loss: 0.0017497198
train_loss: 0.001972603
test_loss: 0.0018840682
train_loss: 0.0016451849
test_loss: 0.0019779496
train_loss: 0.0018519696
test_loss: 0.0015310978
train_loss: 0.001470523
test_loss: 0.0013474916
train_loss: 0.0014531303
test_loss: 0.0014655659
train_loss: 0.0019722558
test_loss: 0.0016169827
train_loss: 0.0015551466
test_loss: 0.0016954447
train_loss: 0.0015379798
test_loss: 0.0018317521
train_loss: 0.0016312205
test_loss: 0.0017554375
train_loss: 0.0017530009
test_loss: 0.0017638106
train_loss: 0.0022684284
test_loss: 0.001740784
train_loss: 0.002200381
test_loss: 0.0017194113
train_loss: 0.0015658842
test_loss: 0.001564163
train_loss: 0.0014822453
test_loss: 0.0017933449
train_loss: 0.0015728052
test_loss: 0.0014976531
train_loss: 0.001652946
test_loss: 0.0014415368
train_loss: 0.0014704783
test_loss: 0.0017149122
train_loss: 0.0018942968
test_loss: 0.0018115971
train_loss: 0.0017146635
test_loss: 0.0019805592
train_loss: 0.00187825
test_loss: 0.0019454226
train_loss: 0.0018096899
test_loss: 0.0023496172
train_loss: 0.0015497794
test_loss: 0.002546013
train_loss: 0.0026236402
test_loss: 0.0017891711
train_loss: 0.001813586
test_loss: 0.0016461458
train_loss: 0.0016674268
test_loss: 0.0016672125
train_loss: 0.001486981
test_loss: 0.0022213743
train_loss: 0.0014753921
test_loss: 0.0016014549
train_loss: 0.0019661235
test_loss: 0.0015414532
train_loss: 0.0020347526
test_loss: 0.0015739726
train_loss: 0.0018414294
test_loss: 0.0018158897
train_loss: 0.0018493646
test_loss: 0.0016683736
train_loss: 0.001533155
test_loss: 0.0016000683
train_loss: 0.0017527041
test_loss: 0.001711548
train_loss: 0.0016850457
test_loss: 0.0019482052
train_loss: 0.0016518703
test_loss: 0.0021793768
train_loss: 0.0017433626
test_loss: 0.0015948776
train_loss: 0.0018691504
test_loss: 0.0018686998
train_loss: 0.0014951314
test_loss: 0.0019248206
train_loss: 0.0013862328
test_loss: 0.001822174
train_loss: 0.0014703576
test_loss: 0.0014195086
train_loss: 0.001576788
test_loss: 0.0017453361
train_loss: 0.0016113096
test_loss: 0.0015723238
train_loss: 0.0013901311
test_loss: 0.0018117665
train_loss: 0.0015818065
test_loss: 0.0014524597
train_loss: 0.0016293952
test_loss: 0.0019470504
train_loss: 0.0017855421
test_loss: 0.00147885
train_loss: 0.00175542
test_loss: 0.0020337384
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104607488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104621620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21045f37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104696730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21046962f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21045522f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21044fdae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21044c96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21044c92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210448af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210448a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210444dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210445bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104410158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21043bbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21043ef950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21044a99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104493268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21044937b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104303730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210431c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210432ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210431cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21042eb7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21042aa488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21042b7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21042b77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104271268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21042677b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21041ba400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21041ee158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104191f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104141620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210418d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104170ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104124ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.17827345e-06
Iter: 2 loss: 1.77248021e-05
Iter: 3 loss: 3.55970133e-06
Iter: 4 loss: 2.72410352e-06
Iter: 5 loss: 3.05278468e-06
Iter: 6 loss: 2.14603369e-06
Iter: 7 loss: 1.73092508e-06
Iter: 8 loss: 4.63419519e-06
Iter: 9 loss: 1.69269413e-06
Iter: 10 loss: 1.47459446e-06
Iter: 11 loss: 1.71476722e-06
Iter: 12 loss: 1.3560383e-06
Iter: 13 loss: 1.22775543e-06
Iter: 14 loss: 2.69888e-06
Iter: 15 loss: 1.22559e-06
Iter: 16 loss: 1.17145782e-06
Iter: 17 loss: 1.42222984e-06
Iter: 18 loss: 1.16135e-06
Iter: 19 loss: 1.11586598e-06
Iter: 20 loss: 1.10981432e-06
Iter: 21 loss: 1.07758808e-06
Iter: 22 loss: 1.04641697e-06
Iter: 23 loss: 1.04605851e-06
Iter: 24 loss: 1.02550757e-06
Iter: 25 loss: 1.01184173e-06
Iter: 26 loss: 1.00405305e-06
Iter: 27 loss: 9.84174903e-07
Iter: 28 loss: 1.0958538e-06
Iter: 29 loss: 9.8141e-07
Iter: 30 loss: 9.66862444e-07
Iter: 31 loss: 9.92395144e-07
Iter: 32 loss: 9.60422199e-07
Iter: 33 loss: 9.45689294e-07
Iter: 34 loss: 9.79569677e-07
Iter: 35 loss: 9.40183781e-07
Iter: 36 loss: 9.4261128e-07
Iter: 37 loss: 9.33992283e-07
Iter: 38 loss: 9.28177769e-07
Iter: 39 loss: 9.15646183e-07
Iter: 40 loss: 1.10933638e-06
Iter: 41 loss: 9.15210251e-07
Iter: 42 loss: 9.10302049e-07
Iter: 43 loss: 9.10184326e-07
Iter: 44 loss: 9.06023274e-07
Iter: 45 loss: 9.01076476e-07
Iter: 46 loss: 9.00629516e-07
Iter: 47 loss: 8.93600145e-07
Iter: 48 loss: 9.25604752e-07
Iter: 49 loss: 8.9225216e-07
Iter: 50 loss: 8.87392105e-07
Iter: 51 loss: 9.25587642e-07
Iter: 52 loss: 8.87081512e-07
Iter: 53 loss: 8.83193479e-07
Iter: 54 loss: 8.79058e-07
Iter: 55 loss: 8.78403114e-07
Iter: 56 loss: 8.71093e-07
Iter: 57 loss: 9.11481948e-07
Iter: 58 loss: 8.70062252e-07
Iter: 59 loss: 8.65461118e-07
Iter: 60 loss: 8.83142832e-07
Iter: 61 loss: 8.64364893e-07
Iter: 62 loss: 8.59374552e-07
Iter: 63 loss: 8.57482178e-07
Iter: 64 loss: 8.54849077e-07
Iter: 65 loss: 8.48577145e-07
Iter: 66 loss: 8.74589e-07
Iter: 67 loss: 8.47250249e-07
Iter: 68 loss: 8.4197967e-07
Iter: 69 loss: 8.40741563e-07
Iter: 70 loss: 8.37392747e-07
Iter: 71 loss: 8.45942907e-07
Iter: 72 loss: 8.35872e-07
Iter: 73 loss: 8.34303e-07
Iter: 74 loss: 8.30558861e-07
Iter: 75 loss: 8.73859619e-07
Iter: 76 loss: 8.30196711e-07
Iter: 77 loss: 8.2669635e-07
Iter: 78 loss: 8.37954076e-07
Iter: 79 loss: 8.25752579e-07
Iter: 80 loss: 8.22986522e-07
Iter: 81 loss: 8.59727095e-07
Iter: 82 loss: 8.22950824e-07
Iter: 83 loss: 8.21353296e-07
Iter: 84 loss: 8.17874081e-07
Iter: 85 loss: 8.71043142e-07
Iter: 86 loss: 8.17739647e-07
Iter: 87 loss: 8.14783334e-07
Iter: 88 loss: 8.14675161e-07
Iter: 89 loss: 8.12544727e-07
Iter: 90 loss: 8.10387291e-07
Iter: 91 loss: 8.1001167e-07
Iter: 92 loss: 8.06092942e-07
Iter: 93 loss: 8.22813206e-07
Iter: 94 loss: 8.05283889e-07
Iter: 95 loss: 8.02962404e-07
Iter: 96 loss: 8.21692765e-07
Iter: 97 loss: 8.02850309e-07
Iter: 98 loss: 8.01067529e-07
Iter: 99 loss: 7.99263e-07
Iter: 100 loss: 7.98901851e-07
Iter: 101 loss: 7.95794278e-07
Iter: 102 loss: 8.14349846e-07
Iter: 103 loss: 7.95432527e-07
Iter: 104 loss: 7.93128947e-07
Iter: 105 loss: 7.90889317e-07
Iter: 106 loss: 7.90447928e-07
Iter: 107 loss: 7.88307716e-07
Iter: 108 loss: 7.8821995e-07
Iter: 109 loss: 7.86655278e-07
Iter: 110 loss: 7.866999e-07
Iter: 111 loss: 7.85993961e-07
Iter: 112 loss: 7.84023428e-07
Iter: 113 loss: 7.91552281e-07
Iter: 114 loss: 7.83190558e-07
Iter: 115 loss: 7.81425967e-07
Iter: 116 loss: 7.9938286e-07
Iter: 117 loss: 7.81376229e-07
Iter: 118 loss: 7.80081e-07
Iter: 119 loss: 7.9575409e-07
Iter: 120 loss: 7.80099072e-07
Iter: 121 loss: 7.79042e-07
Iter: 122 loss: 7.76909e-07
Iter: 123 loss: 8.1664848e-07
Iter: 124 loss: 7.76858883e-07
Iter: 125 loss: 7.74929731e-07
Iter: 126 loss: 7.84314409e-07
Iter: 127 loss: 7.74562068e-07
Iter: 128 loss: 7.72369276e-07
Iter: 129 loss: 7.83408154e-07
Iter: 130 loss: 7.72001158e-07
Iter: 131 loss: 7.70507711e-07
Iter: 132 loss: 7.70881343e-07
Iter: 133 loss: 7.69389828e-07
Iter: 134 loss: 7.67800771e-07
Iter: 135 loss: 7.72701128e-07
Iter: 136 loss: 7.67306517e-07
Iter: 137 loss: 7.66044934e-07
Iter: 138 loss: 7.76743889e-07
Iter: 139 loss: 7.66041353e-07
Iter: 140 loss: 7.651185e-07
Iter: 141 loss: 7.65589334e-07
Iter: 142 loss: 7.6445906e-07
Iter: 143 loss: 7.63474873e-07
Iter: 144 loss: 7.76385548e-07
Iter: 145 loss: 7.63506364e-07
Iter: 146 loss: 7.63074581e-07
Iter: 147 loss: 7.63046557e-07
Iter: 148 loss: 7.62688842e-07
Iter: 149 loss: 7.61794638e-07
Iter: 150 loss: 7.6957582e-07
Iter: 151 loss: 7.61604269e-07
Iter: 152 loss: 7.60407318e-07
Iter: 153 loss: 7.60938178e-07
Iter: 154 loss: 7.59581269e-07
Iter: 155 loss: 7.58438659e-07
Iter: 156 loss: 7.59796535e-07
Iter: 157 loss: 7.57876194e-07
Iter: 158 loss: 7.56132749e-07
Iter: 159 loss: 7.61871377e-07
Iter: 160 loss: 7.55655037e-07
Iter: 161 loss: 7.55474048e-07
Iter: 162 loss: 7.55093367e-07
Iter: 163 loss: 7.54627649e-07
Iter: 164 loss: 7.53358393e-07
Iter: 165 loss: 7.59463433e-07
Iter: 166 loss: 7.52891196e-07
Iter: 167 loss: 7.52222945e-07
Iter: 168 loss: 7.52097208e-07
Iter: 169 loss: 7.51318169e-07
Iter: 170 loss: 7.50966e-07
Iter: 171 loss: 7.50535605e-07
Iter: 172 loss: 7.49250148e-07
Iter: 173 loss: 7.49333253e-07
Iter: 174 loss: 7.48208606e-07
Iter: 175 loss: 7.47219474e-07
Iter: 176 loss: 7.60363832e-07
Iter: 177 loss: 7.47200488e-07
Iter: 178 loss: 7.46321803e-07
Iter: 179 loss: 7.4682066e-07
Iter: 180 loss: 7.45781e-07
Iter: 181 loss: 7.44819317e-07
Iter: 182 loss: 7.55365249e-07
Iter: 183 loss: 7.44803515e-07
Iter: 184 loss: 7.44210297e-07
Iter: 185 loss: 7.5281082e-07
Iter: 186 loss: 7.44215185e-07
Iter: 187 loss: 7.43880037e-07
Iter: 188 loss: 7.43280452e-07
Iter: 189 loss: 7.59128454e-07
Iter: 190 loss: 7.43286478e-07
Iter: 191 loss: 7.42501243e-07
Iter: 192 loss: 7.41876079e-07
Iter: 193 loss: 7.41736358e-07
Iter: 194 loss: 7.40759504e-07
Iter: 195 loss: 7.42603675e-07
Iter: 196 loss: 7.40333689e-07
Iter: 197 loss: 7.3952765e-07
Iter: 198 loss: 7.52127391e-07
Iter: 199 loss: 7.39493146e-07
Iter: 200 loss: 7.38908625e-07
Iter: 201 loss: 7.43756289e-07
Iter: 202 loss: 7.3889197e-07
Iter: 203 loss: 7.38388678e-07
Iter: 204 loss: 7.37322e-07
Iter: 205 loss: 7.61556407e-07
Iter: 206 loss: 7.37312234e-07
Iter: 207 loss: 7.36562868e-07
Iter: 208 loss: 7.42338e-07
Iter: 209 loss: 7.36484822e-07
Iter: 210 loss: 7.3559886e-07
Iter: 211 loss: 7.37356e-07
Iter: 212 loss: 7.35256549e-07
Iter: 213 loss: 7.34595915e-07
Iter: 214 loss: 7.35338745e-07
Iter: 215 loss: 7.3431903e-07
Iter: 216 loss: 7.33366278e-07
Iter: 217 loss: 7.3273452e-07
Iter: 218 loss: 7.32428191e-07
Iter: 219 loss: 7.32049443e-07
Iter: 220 loss: 7.31810303e-07
Iter: 221 loss: 7.31460545e-07
Iter: 222 loss: 7.3521619e-07
Iter: 223 loss: 7.31444e-07
Iter: 224 loss: 7.31110674e-07
Iter: 225 loss: 7.30529337e-07
Iter: 226 loss: 7.30531724e-07
Iter: 227 loss: 7.29916621e-07
Iter: 228 loss: 7.30688271e-07
Iter: 229 loss: 7.29579597e-07
Iter: 230 loss: 7.28842792e-07
Iter: 231 loss: 7.28719158e-07
Iter: 232 loss: 7.28230361e-07
Iter: 233 loss: 7.27118618e-07
Iter: 234 loss: 7.29730232e-07
Iter: 235 loss: 7.26703e-07
Iter: 236 loss: 7.25961172e-07
Iter: 237 loss: 7.34010882e-07
Iter: 238 loss: 7.25930704e-07
Iter: 239 loss: 7.25352947e-07
Iter: 240 loss: 7.30271211e-07
Iter: 241 loss: 7.25270809e-07
Iter: 242 loss: 7.2492e-07
Iter: 243 loss: 7.24177426e-07
Iter: 244 loss: 7.41120857e-07
Iter: 245 loss: 7.24193e-07
Iter: 246 loss: 7.23530547e-07
Iter: 247 loss: 7.2760588e-07
Iter: 248 loss: 7.23433e-07
Iter: 249 loss: 7.22733887e-07
Iter: 250 loss: 7.27219174e-07
Iter: 251 loss: 7.22619234e-07
Iter: 252 loss: 7.22207403e-07
Iter: 253 loss: 7.21492597e-07
Iter: 254 loss: 7.38141694e-07
Iter: 255 loss: 7.21520109e-07
Iter: 256 loss: 7.20828211e-07
Iter: 257 loss: 7.29963745e-07
Iter: 258 loss: 7.20832531e-07
Iter: 259 loss: 7.20722255e-07
Iter: 260 loss: 7.20621756e-07
Iter: 261 loss: 7.20447417e-07
Iter: 262 loss: 7.1994674e-07
Iter: 263 loss: 7.24735571e-07
Iter: 264 loss: 7.19917921e-07
Iter: 265 loss: 7.19267689e-07
Iter: 266 loss: 7.20966852e-07
Iter: 267 loss: 7.19061745e-07
Iter: 268 loss: 7.18644e-07
Iter: 269 loss: 7.18850401e-07
Iter: 270 loss: 7.18372405e-07
Iter: 271 loss: 7.17674823e-07
Iter: 272 loss: 7.18486831e-07
Iter: 273 loss: 7.17343937e-07
Iter: 274 loss: 7.16704676e-07
Iter: 275 loss: 7.20541379e-07
Iter: 276 loss: 7.16628506e-07
Iter: 277 loss: 7.16148122e-07
Iter: 278 loss: 7.15731062e-07
Iter: 279 loss: 7.15596116e-07
Iter: 280 loss: 7.15131421e-07
Iter: 281 loss: 7.15122951e-07
Iter: 282 loss: 7.14669e-07
Iter: 283 loss: 7.15402962e-07
Iter: 284 loss: 7.14431e-07
Iter: 285 loss: 7.14013e-07
Iter: 286 loss: 7.13568511e-07
Iter: 287 loss: 7.13465056e-07
Iter: 288 loss: 7.12982398e-07
Iter: 289 loss: 7.14058046e-07
Iter: 290 loss: 7.12768383e-07
Iter: 291 loss: 7.12146516e-07
Iter: 292 loss: 7.14870964e-07
Iter: 293 loss: 7.12053293e-07
Iter: 294 loss: 7.12093481e-07
Iter: 295 loss: 7.11847179e-07
Iter: 296 loss: 7.11657435e-07
Iter: 297 loss: 7.11277949e-07
Iter: 298 loss: 7.13400198e-07
Iter: 299 loss: 7.11102757e-07
Iter: 300 loss: 7.10618224e-07
Iter: 301 loss: 7.15254487e-07
Iter: 302 loss: 7.10552513e-07
Iter: 303 loss: 7.10210486e-07
Iter: 304 loss: 7.10511642e-07
Iter: 305 loss: 7.10042e-07
Iter: 306 loss: 7.09642904e-07
Iter: 307 loss: 7.11883445e-07
Iter: 308 loss: 7.09613971e-07
Iter: 309 loss: 7.093048e-07
Iter: 310 loss: 7.09608e-07
Iter: 311 loss: 7.09082883e-07
Iter: 312 loss: 7.08788207e-07
Iter: 313 loss: 7.09547919e-07
Iter: 314 loss: 7.08649168e-07
Iter: 315 loss: 7.08301513e-07
Iter: 316 loss: 7.08358471e-07
Iter: 317 loss: 7.07990921e-07
Iter: 318 loss: 7.07596314e-07
Iter: 319 loss: 7.10999643e-07
Iter: 320 loss: 7.07626214e-07
Iter: 321 loss: 7.07241952e-07
Iter: 322 loss: 7.08559867e-07
Iter: 323 loss: 7.07136678e-07
Iter: 324 loss: 7.06906349e-07
Iter: 325 loss: 7.06911351e-07
Iter: 326 loss: 7.06682954e-07
Iter: 327 loss: 7.06313756e-07
Iter: 328 loss: 7.05676428e-07
Iter: 329 loss: 7.05690184e-07
Iter: 330 loss: 7.05932564e-07
Iter: 331 loss: 7.05415459e-07
Iter: 332 loss: 7.05147897e-07
Iter: 333 loss: 7.05286311e-07
Iter: 334 loss: 7.04947183e-07
Iter: 335 loss: 7.0466092e-07
Iter: 336 loss: 7.04087597e-07
Iter: 337 loss: 7.10655115e-07
Iter: 338 loss: 7.04016e-07
Iter: 339 loss: 7.03552246e-07
Iter: 340 loss: 7.05453e-07
Iter: 341 loss: 7.03390072e-07
Iter: 342 loss: 7.02931402e-07
Iter: 343 loss: 7.06809828e-07
Iter: 344 loss: 7.02885472e-07
Iter: 345 loss: 7.02458522e-07
Iter: 346 loss: 7.05551201e-07
Iter: 347 loss: 7.02382067e-07
Iter: 348 loss: 7.02198463e-07
Iter: 349 loss: 7.01945169e-07
Iter: 350 loss: 7.01898045e-07
Iter: 351 loss: 7.01529075e-07
Iter: 352 loss: 7.04435536e-07
Iter: 353 loss: 7.01546242e-07
Iter: 354 loss: 7.01185513e-07
Iter: 355 loss: 7.01854049e-07
Iter: 356 loss: 7.01100305e-07
Iter: 357 loss: 7.00787837e-07
Iter: 358 loss: 7.01810336e-07
Iter: 359 loss: 7.00683529e-07
Iter: 360 loss: 7.00357759e-07
Iter: 361 loss: 7.0166459e-07
Iter: 362 loss: 7.00344913e-07
Iter: 363 loss: 7.00086616e-07
Iter: 364 loss: 7.0119205e-07
Iter: 365 loss: 7.00018063e-07
Iter: 366 loss: 6.99780344e-07
Iter: 367 loss: 6.99803195e-07
Iter: 368 loss: 6.99604e-07
Iter: 369 loss: 6.99528869e-07
Iter: 370 loss: 6.99479529e-07
Iter: 371 loss: 6.99338443e-07
Iter: 372 loss: 6.99010343e-07
Iter: 373 loss: 7.01363092e-07
Iter: 374 loss: 6.98950885e-07
Iter: 375 loss: 6.9870913e-07
Iter: 376 loss: 6.98869087e-07
Iter: 377 loss: 6.98549e-07
Iter: 378 loss: 6.98205326e-07
Iter: 379 loss: 6.99124428e-07
Iter: 380 loss: 6.98047188e-07
Iter: 381 loss: 6.97799749e-07
Iter: 382 loss: 7.02039131e-07
Iter: 383 loss: 6.9777218e-07
Iter: 384 loss: 6.97524229e-07
Iter: 385 loss: 6.9731e-07
Iter: 386 loss: 6.97261498e-07
Iter: 387 loss: 6.96901623e-07
Iter: 388 loss: 6.9762973e-07
Iter: 389 loss: 6.96751897e-07
Iter: 390 loss: 6.96291409e-07
Iter: 391 loss: 6.98030135e-07
Iter: 392 loss: 6.9624565e-07
Iter: 393 loss: 6.95802669e-07
Iter: 394 loss: 6.96890936e-07
Iter: 395 loss: 6.95725419e-07
Iter: 396 loss: 6.95390725e-07
Iter: 397 loss: 6.97277528e-07
Iter: 398 loss: 6.95346841e-07
Iter: 399 loss: 6.95048243e-07
Iter: 400 loss: 6.95893732e-07
Iter: 401 loss: 6.94978496e-07
Iter: 402 loss: 6.94693881e-07
Iter: 403 loss: 6.95029769e-07
Iter: 404 loss: 6.94537732e-07
Iter: 405 loss: 6.94409209e-07
Iter: 406 loss: 6.94403866e-07
Iter: 407 loss: 6.94278e-07
Iter: 408 loss: 6.94062e-07
Iter: 409 loss: 6.94056098e-07
Iter: 410 loss: 6.93841514e-07
Iter: 411 loss: 6.93564402e-07
Iter: 412 loss: 6.93500624e-07
Iter: 413 loss: 6.93236586e-07
Iter: 414 loss: 6.93450488e-07
Iter: 415 loss: 6.93061565e-07
Iter: 416 loss: 6.92609092e-07
Iter: 417 loss: 6.94191101e-07
Iter: 418 loss: 6.9249927e-07
Iter: 419 loss: 6.9215946e-07
Iter: 420 loss: 6.92166e-07
Iter: 421 loss: 6.91885e-07
Iter: 422 loss: 6.91492119e-07
Iter: 423 loss: 6.91493597e-07
Iter: 424 loss: 6.91077901e-07
Iter: 425 loss: 6.92882281e-07
Iter: 426 loss: 6.91039475e-07
Iter: 427 loss: 6.90716263e-07
Iter: 428 loss: 6.92736e-07
Iter: 429 loss: 6.90699778e-07
Iter: 430 loss: 6.90399531e-07
Iter: 431 loss: 6.90511911e-07
Iter: 432 loss: 6.90172669e-07
Iter: 433 loss: 6.8984366e-07
Iter: 434 loss: 6.92376602e-07
Iter: 435 loss: 6.89835076e-07
Iter: 436 loss: 6.89498336e-07
Iter: 437 loss: 6.89648857e-07
Iter: 438 loss: 6.89260219e-07
Iter: 439 loss: 6.8905382e-07
Iter: 440 loss: 6.89061949e-07
Iter: 441 loss: 6.88876412e-07
Iter: 442 loss: 6.89181718e-07
Iter: 443 loss: 6.88756359e-07
Iter: 444 loss: 6.88567866e-07
Iter: 445 loss: 6.88213845e-07
Iter: 446 loss: 6.95593087e-07
Iter: 447 loss: 6.88227885e-07
Iter: 448 loss: 6.87887677e-07
Iter: 449 loss: 6.89213721e-07
Iter: 450 loss: 6.87786496e-07
Iter: 451 loss: 6.87496538e-07
Iter: 452 loss: 6.87490797e-07
Iter: 453 loss: 6.87212037e-07
Iter: 454 loss: 6.87068791e-07
Iter: 455 loss: 6.8698472e-07
Iter: 456 loss: 6.86816406e-07
Iter: 457 loss: 6.86573173e-07
Iter: 458 loss: 6.86561918e-07
Iter: 459 loss: 6.86241947e-07
Iter: 460 loss: 6.86636e-07
Iter: 461 loss: 6.8605209e-07
Iter: 462 loss: 6.85710233e-07
Iter: 463 loss: 6.86653266e-07
Iter: 464 loss: 6.85591885e-07
Iter: 465 loss: 6.85197847e-07
Iter: 466 loss: 6.88033879e-07
Iter: 467 loss: 6.85190457e-07
Iter: 468 loss: 6.84950123e-07
Iter: 469 loss: 6.85607e-07
Iter: 470 loss: 6.84781298e-07
Iter: 471 loss: 6.84564156e-07
Iter: 472 loss: 6.8675763e-07
Iter: 473 loss: 6.84559382e-07
Iter: 474 loss: 6.84387658e-07
Iter: 475 loss: 6.8480972e-07
Iter: 476 loss: 6.8433252e-07
Iter: 477 loss: 6.84100314e-07
Iter: 478 loss: 6.84763506e-07
Iter: 479 loss: 6.83983899e-07
Iter: 480 loss: 6.83861515e-07
Iter: 481 loss: 6.83578605e-07
Iter: 482 loss: 6.88641194e-07
Iter: 483 loss: 6.83601286e-07
Iter: 484 loss: 6.83249425e-07
Iter: 485 loss: 6.84361453e-07
Iter: 486 loss: 6.8311806e-07
Iter: 487 loss: 6.82827192e-07
Iter: 488 loss: 6.8360373e-07
Iter: 489 loss: 6.82721407e-07
Iter: 490 loss: 6.82465384e-07
Iter: 491 loss: 6.83517612e-07
Iter: 492 loss: 6.82371933e-07
Iter: 493 loss: 6.82019902e-07
Iter: 494 loss: 6.83785743e-07
Iter: 495 loss: 6.82048835e-07
Iter: 496 loss: 6.8178e-07
Iter: 497 loss: 6.81371716e-07
Iter: 498 loss: 6.87612101e-07
Iter: 499 loss: 6.81348581e-07
Iter: 500 loss: 6.80867743e-07
Iter: 501 loss: 6.83665689e-07
Iter: 502 loss: 6.8077577e-07
Iter: 503 loss: 6.80449091e-07
Iter: 504 loss: 6.84642487e-07
Iter: 505 loss: 6.80418282e-07
Iter: 506 loss: 6.80146343e-07
Iter: 507 loss: 6.8147574e-07
Iter: 508 loss: 6.80132189e-07
Iter: 509 loss: 6.79912148e-07
Iter: 510 loss: 6.80576761e-07
Iter: 511 loss: 6.79882874e-07
Iter: 512 loss: 6.79684717e-07
Iter: 513 loss: 6.80134121e-07
Iter: 514 loss: 6.79535049e-07
Iter: 515 loss: 6.79347295e-07
Iter: 516 loss: 6.818467e-07
Iter: 517 loss: 6.79309608e-07
Iter: 518 loss: 6.79224456e-07
Iter: 519 loss: 6.78994411e-07
Iter: 520 loss: 6.81901952e-07
Iter: 521 loss: 6.78941092e-07
Iter: 522 loss: 6.78642664e-07
Iter: 523 loss: 6.78983156e-07
Iter: 524 loss: 6.785e-07
Iter: 525 loss: 6.78259767e-07
Iter: 526 loss: 6.80593757e-07
Iter: 527 loss: 6.78268123e-07
Iter: 528 loss: 6.7805496e-07
Iter: 529 loss: 6.78064339e-07
Iter: 530 loss: 6.77902904e-07
Iter: 531 loss: 6.77603282e-07
Iter: 532 loss: 6.80216488e-07
Iter: 533 loss: 6.77561275e-07
Iter: 534 loss: 6.77291382e-07
Iter: 535 loss: 6.77176104e-07
Iter: 536 loss: 6.77058836e-07
Iter: 537 loss: 6.76705213e-07
Iter: 538 loss: 6.77097148e-07
Iter: 539 loss: 6.76539912e-07
Iter: 540 loss: 6.76192428e-07
Iter: 541 loss: 6.76725e-07
Iter: 542 loss: 6.76093407e-07
Iter: 543 loss: 6.75874958e-07
Iter: 544 loss: 6.7586393e-07
Iter: 545 loss: 6.75697493e-07
Iter: 546 loss: 6.75899798e-07
Iter: 547 loss: 6.75610863e-07
Iter: 548 loss: 6.75482738e-07
Iter: 549 loss: 6.7740973e-07
Iter: 550 loss: 6.75441242e-07
Iter: 551 loss: 6.75324827e-07
Iter: 552 loss: 6.75554e-07
Iter: 553 loss: 6.75282308e-07
Iter: 554 loss: 6.75135425e-07
Iter: 555 loss: 6.74969669e-07
Iter: 556 loss: 6.74972512e-07
Iter: 557 loss: 6.74790215e-07
Iter: 558 loss: 6.74687897e-07
Iter: 559 loss: 6.74573926e-07
Iter: 560 loss: 6.74318e-07
Iter: 561 loss: 6.75505817e-07
Iter: 562 loss: 6.74284195e-07
Iter: 563 loss: 6.74005832e-07
Iter: 564 loss: 6.74025728e-07
Iter: 565 loss: 6.73789202e-07
Iter: 566 loss: 6.73464911e-07
Iter: 567 loss: 6.73498903e-07
Iter: 568 loss: 6.73282784e-07
Iter: 569 loss: 6.73815521e-07
Iter: 570 loss: 6.73196382e-07
Iter: 571 loss: 6.72941383e-07
Iter: 572 loss: 6.73160685e-07
Iter: 573 loss: 6.72779834e-07
Iter: 574 loss: 6.72556439e-07
Iter: 575 loss: 6.72504768e-07
Iter: 576 loss: 6.72367264e-07
Iter: 577 loss: 6.72033764e-07
Iter: 578 loss: 6.733041e-07
Iter: 579 loss: 6.7194668e-07
Iter: 580 loss: 6.71796215e-07
Iter: 581 loss: 6.71772796e-07
Iter: 582 loss: 6.71598912e-07
Iter: 583 loss: 6.716744e-07
Iter: 584 loss: 6.71505063e-07
Iter: 585 loss: 6.71279167e-07
Iter: 586 loss: 6.72084411e-07
Iter: 587 loss: 6.71223347e-07
Iter: 588 loss: 6.71103635e-07
Iter: 589 loss: 6.70957e-07
Iter: 590 loss: 6.70928614e-07
Iter: 591 loss: 6.70667873e-07
Iter: 592 loss: 6.71724251e-07
Iter: 593 loss: 6.70674581e-07
Iter: 594 loss: 6.70461077e-07
Iter: 595 loss: 6.70388772e-07
Iter: 596 loss: 6.70328177e-07
Iter: 597 loss: 6.70025884e-07
Iter: 598 loss: 6.70442e-07
Iter: 599 loss: 6.69954431e-07
Iter: 600 loss: 6.69706424e-07
Iter: 601 loss: 6.71543262e-07
Iter: 602 loss: 6.69661517e-07
Iter: 603 loss: 6.69530039e-07
Iter: 604 loss: 6.7083829e-07
Iter: 605 loss: 6.69486383e-07
Iter: 606 loss: 6.6935479e-07
Iter: 607 loss: 6.69325232e-07
Iter: 608 loss: 6.69266342e-07
Iter: 609 loss: 6.69069038e-07
Iter: 610 loss: 6.69768895e-07
Iter: 611 loss: 6.69023052e-07
Iter: 612 loss: 6.68847292e-07
Iter: 613 loss: 6.6877152e-07
Iter: 614 loss: 6.68684038e-07
Iter: 615 loss: 6.68552616e-07
Iter: 616 loss: 6.68525388e-07
Iter: 617 loss: 6.68422672e-07
Iter: 618 loss: 6.68625603e-07
Iter: 619 loss: 6.68384871e-07
Iter: 620 loss: 6.68190637e-07
Iter: 621 loss: 6.68146299e-07
Iter: 622 loss: 6.68043185e-07
Iter: 623 loss: 6.67821439e-07
Iter: 624 loss: 6.67803306e-07
Iter: 625 loss: 6.67638915e-07
Iter: 626 loss: 6.67386075e-07
Iter: 627 loss: 6.69672488e-07
Iter: 628 loss: 6.67353902e-07
Iter: 629 loss: 6.67088784e-07
Iter: 630 loss: 6.6686431e-07
Iter: 631 loss: 6.66806216e-07
Iter: 632 loss: 6.66595668e-07
Iter: 633 loss: 6.68102e-07
Iter: 634 loss: 6.66550477e-07
Iter: 635 loss: 6.6628553e-07
Iter: 636 loss: 6.67650795e-07
Iter: 637 loss: 6.66247615e-07
Iter: 638 loss: 6.66054575e-07
Iter: 639 loss: 6.66877213e-07
Iter: 640 loss: 6.65997618e-07
Iter: 641 loss: 6.65813445e-07
Iter: 642 loss: 6.65922698e-07
Iter: 643 loss: 6.65722609e-07
Iter: 644 loss: 6.65535595e-07
Iter: 645 loss: 6.6572386e-07
Iter: 646 loss: 6.65416678e-07
Iter: 647 loss: 6.65206642e-07
Iter: 648 loss: 6.66882897e-07
Iter: 649 loss: 6.65186292e-07
Iter: 650 loss: 6.65029802e-07
Iter: 651 loss: 6.66899837e-07
Iter: 652 loss: 6.65009225e-07
Iter: 653 loss: 6.64915433e-07
Iter: 654 loss: 6.64869958e-07
Iter: 655 loss: 6.64784295e-07
Iter: 656 loss: 6.64579034e-07
Iter: 657 loss: 6.64838467e-07
Iter: 658 loss: 6.64468132e-07
Iter: 659 loss: 6.64358595e-07
Iter: 660 loss: 6.64386334e-07
Iter: 661 loss: 6.64257414e-07
Iter: 662 loss: 6.64085064e-07
Iter: 663 loss: 6.65212724e-07
Iter: 664 loss: 6.64082222e-07
Iter: 665 loss: 6.63917035e-07
Iter: 666 loss: 6.63901233e-07
Iter: 667 loss: 6.63810283e-07
Iter: 668 loss: 6.63552669e-07
Iter: 669 loss: 6.63571e-07
Iter: 670 loss: 6.6339021e-07
Iter: 671 loss: 6.631509e-07
Iter: 672 loss: 6.66246422e-07
Iter: 673 loss: 6.63093886e-07
Iter: 674 loss: 6.6287771e-07
Iter: 675 loss: 6.63360879e-07
Iter: 676 loss: 6.62775335e-07
Iter: 677 loss: 6.6256041e-07
Iter: 678 loss: 6.62572802e-07
Iter: 679 loss: 6.62456443e-07
Iter: 680 loss: 6.6220116e-07
Iter: 681 loss: 6.63263791e-07
Iter: 682 loss: 6.62161369e-07
Iter: 683 loss: 6.61963554e-07
Iter: 684 loss: 6.64766617e-07
Iter: 685 loss: 6.61946e-07
Iter: 686 loss: 6.61759714e-07
Iter: 687 loss: 6.61833042e-07
Iter: 688 loss: 6.61690478e-07
Iter: 689 loss: 6.61481693e-07
Iter: 690 loss: 6.62010166e-07
Iter: 691 loss: 6.61366869e-07
Iter: 692 loss: 6.61165814e-07
Iter: 693 loss: 6.60975616e-07
Iter: 694 loss: 6.60944579e-07
Iter: 695 loss: 6.60772912e-07
Iter: 696 loss: 6.61693662e-07
Iter: 697 loss: 6.60709702e-07
Iter: 698 loss: 6.60486535e-07
Iter: 699 loss: 6.61615502e-07
Iter: 700 loss: 6.60500802e-07
Iter: 701 loss: 6.60278829e-07
Iter: 702 loss: 6.60508533e-07
Iter: 703 loss: 6.60218916e-07
Iter: 704 loss: 6.60044634e-07
Iter: 705 loss: 6.59822035e-07
Iter: 706 loss: 6.597744e-07
Iter: 707 loss: 6.59702835e-07
Iter: 708 loss: 6.59650368e-07
Iter: 709 loss: 6.59515536e-07
Iter: 710 loss: 6.59605234e-07
Iter: 711 loss: 6.59415718e-07
Iter: 712 loss: 6.59293391e-07
Iter: 713 loss: 6.59112402e-07
Iter: 714 loss: 6.59096941e-07
Iter: 715 loss: 6.58951535e-07
Iter: 716 loss: 6.58954946e-07
Iter: 717 loss: 6.58805675e-07
Iter: 718 loss: 6.5931323e-07
Iter: 719 loss: 6.58763838e-07
Iter: 720 loss: 6.58554313e-07
Iter: 721 loss: 6.58617296e-07
Iter: 722 loss: 6.58515489e-07
Iter: 723 loss: 6.5833666e-07
Iter: 724 loss: 6.59066359e-07
Iter: 725 loss: 6.58287718e-07
Iter: 726 loss: 6.58170222e-07
Iter: 727 loss: 6.58120257e-07
Iter: 728 loss: 6.58065119e-07
Iter: 729 loss: 6.57898e-07
Iter: 730 loss: 6.58017257e-07
Iter: 731 loss: 6.57803184e-07
Iter: 732 loss: 6.57647092e-07
Iter: 733 loss: 6.57662952e-07
Iter: 734 loss: 6.57505e-07
Iter: 735 loss: 6.57272494e-07
Iter: 736 loss: 6.62027333e-07
Iter: 737 loss: 6.57264081e-07
Iter: 738 loss: 6.5697634e-07
Iter: 739 loss: 6.58155614e-07
Iter: 740 loss: 6.56913357e-07
Iter: 741 loss: 6.56647785e-07
Iter: 742 loss: 6.59238538e-07
Iter: 743 loss: 6.56680129e-07
Iter: 744 loss: 6.56403927e-07
Iter: 745 loss: 6.56574116e-07
Iter: 746 loss: 6.56285124e-07
Iter: 747 loss: 6.56069801e-07
Iter: 748 loss: 6.55853e-07
Iter: 749 loss: 6.55808435e-07
Iter: 750 loss: 6.55876647e-07
Iter: 751 loss: 6.55618578e-07
Iter: 752 loss: 6.55510917e-07
Iter: 753 loss: 6.55364488e-07
Iter: 754 loss: 6.55342546e-07
Iter: 755 loss: 6.55189297e-07
Iter: 756 loss: 6.56401767e-07
Iter: 757 loss: 6.55184408e-07
Iter: 758 loss: 6.55035933e-07
Iter: 759 loss: 6.55066174e-07
Iter: 760 loss: 6.5493623e-07
Iter: 761 loss: 6.54796622e-07
Iter: 762 loss: 6.55003248e-07
Iter: 763 loss: 6.54696123e-07
Iter: 764 loss: 6.54567202e-07
Iter: 765 loss: 6.54711812e-07
Iter: 766 loss: 6.5445289e-07
Iter: 767 loss: 6.54297082e-07
Iter: 768 loss: 6.5585175e-07
Iter: 769 loss: 6.54301516e-07
Iter: 770 loss: 6.54169867e-07
Iter: 771 loss: 6.54201813e-07
Iter: 772 loss: 6.54056578e-07
Iter: 773 loss: 6.53847223e-07
Iter: 774 loss: 6.53879852e-07
Iter: 775 loss: 6.53655775e-07
Iter: 776 loss: 6.5357392e-07
Iter: 777 loss: 6.53550558e-07
Iter: 778 loss: 6.53397137e-07
Iter: 779 loss: 6.53153279e-07
Iter: 780 loss: 6.53171412e-07
Iter: 781 loss: 6.52924143e-07
Iter: 782 loss: 6.54422195e-07
Iter: 783 loss: 6.52868266e-07
Iter: 784 loss: 6.5279346e-07
Iter: 785 loss: 6.527614e-07
Iter: 786 loss: 6.5267875e-07
Iter: 787 loss: 6.52485596e-07
Iter: 788 loss: 6.53580287e-07
Iter: 789 loss: 6.5240647e-07
Iter: 790 loss: 6.52374922e-07
Iter: 791 loss: 6.52297558e-07
Iter: 792 loss: 6.52227641e-07
Iter: 793 loss: 6.5202812e-07
Iter: 794 loss: 6.53629e-07
Iter: 795 loss: 6.51945584e-07
Iter: 796 loss: 6.51757773e-07
Iter: 797 loss: 6.52821427e-07
Iter: 798 loss: 6.5172685e-07
Iter: 799 loss: 6.51522e-07
Iter: 800 loss: 6.51854862e-07
Iter: 801 loss: 6.51410687e-07
Iter: 802 loss: 6.51185246e-07
Iter: 803 loss: 6.51293249e-07
Iter: 804 loss: 6.51013352e-07
Iter: 805 loss: 6.50754714e-07
Iter: 806 loss: 6.53220468e-07
Iter: 807 loss: 6.50789957e-07
Iter: 808 loss: 6.50668881e-07
Iter: 809 loss: 6.50580773e-07
Iter: 810 loss: 6.50523418e-07
Iter: 811 loss: 6.50325148e-07
Iter: 812 loss: 6.51984237e-07
Iter: 813 loss: 6.50345555e-07
Iter: 814 loss: 6.50158313e-07
Iter: 815 loss: 6.50536435e-07
Iter: 816 loss: 6.50116306e-07
Iter: 817 loss: 6.50014556e-07
Iter: 818 loss: 6.50199297e-07
Iter: 819 loss: 6.49935771e-07
Iter: 820 loss: 6.49748472e-07
Iter: 821 loss: 6.51533185e-07
Iter: 822 loss: 6.4972312e-07
Iter: 823 loss: 6.49703395e-07
Iter: 824 loss: 6.49544404e-07
Iter: 825 loss: 6.51554842e-07
Iter: 826 loss: 6.49531444e-07
Iter: 827 loss: 6.49389449e-07
Iter: 828 loss: 6.49416052e-07
Iter: 829 loss: 6.4933522e-07
Iter: 830 loss: 6.492088e-07
Iter: 831 loss: 6.49220851e-07
Iter: 832 loss: 6.49066237e-07
Iter: 833 loss: 6.48943342e-07
Iter: 834 loss: 6.48908269e-07
Iter: 835 loss: 6.4866083e-07
Iter: 836 loss: 6.50549964e-07
Iter: 837 loss: 6.48691071e-07
Iter: 838 loss: 6.48541231e-07
Iter: 839 loss: 6.48682e-07
Iter: 840 loss: 6.48484161e-07
Iter: 841 loss: 6.48245759e-07
Iter: 842 loss: 6.48704372e-07
Iter: 843 loss: 6.48189427e-07
Iter: 844 loss: 6.47970751e-07
Iter: 845 loss: 6.4932226e-07
Iter: 846 loss: 6.47979391e-07
Iter: 847 loss: 6.47854e-07
Iter: 848 loss: 6.48179139e-07
Iter: 849 loss: 6.47785214e-07
Iter: 850 loss: 6.47578531e-07
Iter: 851 loss: 6.47682214e-07
Iter: 852 loss: 6.47464731e-07
Iter: 853 loss: 6.4748906e-07
Iter: 854 loss: 6.47360366e-07
Iter: 855 loss: 6.47326715e-07
Iter: 856 loss: 6.47232923e-07
Iter: 857 loss: 6.49616254e-07
Iter: 858 loss: 6.47250374e-07
Iter: 859 loss: 6.47099341e-07
Iter: 860 loss: 6.47118497e-07
Iter: 861 loss: 6.47019306e-07
Iter: 862 loss: 6.46940805e-07
Iter: 863 loss: 6.46945921e-07
Iter: 864 loss: 6.46836554e-07
Iter: 865 loss: 6.46653177e-07
Iter: 866 loss: 6.47857348e-07
Iter: 867 loss: 6.46602302e-07
Iter: 868 loss: 6.46407898e-07
Iter: 869 loss: 6.48510081e-07
Iter: 870 loss: 6.46414037e-07
Iter: 871 loss: 6.46247372e-07
Iter: 872 loss: 6.46687624e-07
Iter: 873 loss: 6.4621679e-07
Iter: 874 loss: 6.46058652e-07
Iter: 875 loss: 6.46529031e-07
Iter: 876 loss: 6.46013916e-07
Iter: 877 loss: 6.45841453e-07
Iter: 878 loss: 6.46034721e-07
Iter: 879 loss: 6.45749651e-07
Iter: 880 loss: 6.455424e-07
Iter: 881 loss: 6.4601943e-07
Iter: 882 loss: 6.45452246e-07
Iter: 883 loss: 6.45325258e-07
Iter: 884 loss: 6.46833371e-07
Iter: 885 loss: 6.45323894e-07
Iter: 886 loss: 6.45194177e-07
Iter: 887 loss: 6.45631587e-07
Iter: 888 loss: 6.4513813e-07
Iter: 889 loss: 6.45068894e-07
Iter: 890 loss: 6.4602807e-07
Iter: 891 loss: 6.45046839e-07
Iter: 892 loss: 6.44984709e-07
Iter: 893 loss: 6.44855618e-07
Iter: 894 loss: 6.46741512e-07
Iter: 895 loss: 6.44795591e-07
Iter: 896 loss: 6.44682302e-07
Iter: 897 loss: 6.45074e-07
Iter: 898 loss: 6.4462165e-07
Iter: 899 loss: 6.44493525e-07
Iter: 900 loss: 6.46142325e-07
Iter: 901 loss: 6.44522402e-07
Iter: 902 loss: 6.44440661e-07
Iter: 903 loss: 6.4432885e-07
Iter: 904 loss: 6.44265867e-07
Iter: 905 loss: 6.44123702e-07
Iter: 906 loss: 6.44059696e-07
Iter: 907 loss: 6.43989949e-07
Iter: 908 loss: 6.43814474e-07
Iter: 909 loss: 6.43814701e-07
Iter: 910 loss: 6.43714827e-07
Iter: 911 loss: 6.43622286e-07
Iter: 912 loss: 6.43592386e-07
Iter: 913 loss: 6.43435669e-07
Iter: 914 loss: 6.44456463e-07
Iter: 915 loss: 6.43460055e-07
Iter: 916 loss: 6.43372346e-07
Iter: 917 loss: 6.43962949e-07
Iter: 918 loss: 6.43305043e-07
Iter: 919 loss: 6.4323433e-07
Iter: 920 loss: 6.43479893e-07
Iter: 921 loss: 6.43219153e-07
Iter: 922 loss: 6.43137128e-07
Iter: 923 loss: 6.44057707e-07
Iter: 924 loss: 6.43148553e-07
Iter: 925 loss: 6.43085855e-07
Iter: 926 loss: 6.43084093e-07
Iter: 927 loss: 6.43048281e-07
Iter: 928 loss: 6.42969781e-07
Iter: 929 loss: 6.42771624e-07
Iter: 930 loss: 6.45209639e-07
Iter: 931 loss: 6.42783675e-07
Iter: 932 loss: 6.4263412e-07
Iter: 933 loss: 6.44202942e-07
Iter: 934 loss: 6.42659757e-07
Iter: 935 loss: 6.42556472e-07
Iter: 936 loss: 6.43410715e-07
Iter: 937 loss: 6.42558234e-07
Iter: 938 loss: 6.424616e-07
Iter: 939 loss: 6.42281179e-07
Iter: 940 loss: 6.44554461e-07
Iter: 941 loss: 6.42279588e-07
Iter: 942 loss: 6.42148507e-07
Iter: 943 loss: 6.44122906e-07
Iter: 944 loss: 6.42144641e-07
Iter: 945 loss: 6.41988095e-07
Iter: 946 loss: 6.42168857e-07
Iter: 947 loss: 6.41972861e-07
Iter: 948 loss: 6.41838483e-07
Iter: 949 loss: 6.42429541e-07
Iter: 950 loss: 6.41825693e-07
Iter: 951 loss: 6.4176e-07
Iter: 952 loss: 6.41707629e-07
Iter: 953 loss: 6.41637712e-07
Iter: 954 loss: 6.41503e-07
Iter: 955 loss: 6.41509359e-07
Iter: 956 loss: 6.41504357e-07
Iter: 957 loss: 6.4162748e-07
Iter: 958 loss: 6.41429324e-07
Iter: 959 loss: 6.41388283e-07
Iter: 960 loss: 6.4176362e-07
Iter: 961 loss: 6.4138e-07
Iter: 962 loss: 6.41267889e-07
Iter: 963 loss: 6.41071e-07
Iter: 964 loss: 6.44463171e-07
Iter: 965 loss: 6.41084739e-07
Iter: 966 loss: 6.40891926e-07
Iter: 967 loss: 6.41777604e-07
Iter: 968 loss: 6.40875783e-07
Iter: 969 loss: 6.40751296e-07
Iter: 970 loss: 6.41736278e-07
Iter: 971 loss: 6.40767098e-07
Iter: 972 loss: 6.40603218e-07
Iter: 973 loss: 6.40561211e-07
Iter: 974 loss: 6.4052665e-07
Iter: 975 loss: 6.40389544e-07
Iter: 976 loss: 6.40445762e-07
Iter: 977 loss: 6.40292456e-07
Iter: 978 loss: 6.40138808e-07
Iter: 979 loss: 6.40968437e-07
Iter: 980 loss: 6.40104531e-07
Iter: 981 loss: 6.39960206e-07
Iter: 982 loss: 6.40555186e-07
Iter: 983 loss: 6.39950031e-07
Iter: 984 loss: 6.39763812e-07
Iter: 985 loss: 6.39860332e-07
Iter: 986 loss: 6.39694804e-07
Iter: 987 loss: 6.39564e-07
Iter: 988 loss: 6.40967414e-07
Iter: 989 loss: 6.39531436e-07
Iter: 990 loss: 6.39455379e-07
Iter: 991 loss: 6.39971347e-07
Iter: 992 loss: 6.39463451e-07
Iter: 993 loss: 6.39348912e-07
Iter: 994 loss: 6.39670077e-07
Iter: 995 loss: 6.39351128e-07
Iter: 996 loss: 6.39209475e-07
Iter: 997 loss: 6.39205552e-07
Iter: 998 loss: 6.39118184e-07
Iter: 999 loss: 6.38958454e-07
Iter: 1000 loss: 6.39020641e-07
Iter: 1001 loss: 6.38879555e-07
Iter: 1002 loss: 6.38762e-07
Iter: 1003 loss: 6.39101245e-07
Iter: 1004 loss: 6.38681229e-07
Iter: 1005 loss: 6.38591587e-07
Iter: 1006 loss: 6.40283361e-07
Iter: 1007 loss: 6.38606139e-07
Iter: 1008 loss: 6.38524625e-07
Iter: 1009 loss: 6.38351707e-07
Iter: 1010 loss: 6.40379085e-07
Iter: 1011 loss: 6.38376207e-07
Iter: 1012 loss: 6.38204824e-07
Iter: 1013 loss: 6.39069413e-07
Iter: 1014 loss: 6.38182314e-07
Iter: 1015 loss: 6.38031452e-07
Iter: 1016 loss: 6.38338861e-07
Iter: 1017 loss: 6.37957555e-07
Iter: 1018 loss: 6.37792368e-07
Iter: 1019 loss: 6.39333734e-07
Iter: 1020 loss: 6.3778765e-07
Iter: 1021 loss: 6.37687094e-07
Iter: 1022 loss: 6.37736321e-07
Iter: 1023 loss: 6.37652477e-07
Iter: 1024 loss: 6.37479047e-07
Iter: 1025 loss: 6.38444078e-07
Iter: 1026 loss: 6.37463813e-07
Iter: 1027 loss: 6.3736195e-07
Iter: 1028 loss: 6.38469032e-07
Iter: 1029 loss: 6.37387075e-07
Iter: 1030 loss: 6.37324263e-07
Iter: 1031 loss: 6.37412086e-07
Iter: 1032 loss: 6.37275036e-07
Iter: 1033 loss: 6.37232574e-07
Iter: 1034 loss: 6.37105e-07
Iter: 1035 loss: 6.37108883e-07
Iter: 1036 loss: 6.37020435e-07
Iter: 1037 loss: 6.3754112e-07
Iter: 1038 loss: 6.36951711e-07
Iter: 1039 loss: 6.36888672e-07
Iter: 1040 loss: 6.37858591e-07
Iter: 1041 loss: 6.36889467e-07
Iter: 1042 loss: 6.36808863e-07
Iter: 1043 loss: 6.36669427e-07
Iter: 1044 loss: 6.3922073e-07
Iter: 1045 loss: 6.36692448e-07
Iter: 1046 loss: 6.36504694e-07
Iter: 1047 loss: 6.37053176e-07
Iter: 1048 loss: 6.36510094e-07
Iter: 1049 loss: 6.36342861e-07
Iter: 1050 loss: 6.36421589e-07
Iter: 1051 loss: 6.36242305e-07
Iter: 1052 loss: 6.36146751e-07
Iter: 1053 loss: 6.3613669e-07
Iter: 1054 loss: 6.36089737e-07
Iter: 1055 loss: 6.35897493e-07
Iter: 1056 loss: 6.35904598e-07
Iter: 1057 loss: 6.35802849e-07
Iter: 1058 loss: 6.37526171e-07
Iter: 1059 loss: 6.35804327e-07
Iter: 1060 loss: 6.35724916e-07
Iter: 1061 loss: 6.36024254e-07
Iter: 1062 loss: 6.35700758e-07
Iter: 1063 loss: 6.35589913e-07
Iter: 1064 loss: 6.35907213e-07
Iter: 1065 loss: 6.35572633e-07
Iter: 1066 loss: 6.35503625e-07
Iter: 1067 loss: 6.35330593e-07
Iter: 1068 loss: 6.35340029e-07
Iter: 1069 loss: 6.35187e-07
Iter: 1070 loss: 6.35693709e-07
Iter: 1071 loss: 6.35126412e-07
Iter: 1072 loss: 6.35012668e-07
Iter: 1073 loss: 6.36121229e-07
Iter: 1074 loss: 6.35018182e-07
Iter: 1075 loss: 6.34885509e-07
Iter: 1076 loss: 6.34993569e-07
Iter: 1077 loss: 6.34825767e-07
Iter: 1078 loss: 6.34724188e-07
Iter: 1079 loss: 6.34836908e-07
Iter: 1080 loss: 6.34703156e-07
Iter: 1081 loss: 6.34536605e-07
Iter: 1082 loss: 6.34497496e-07
Iter: 1083 loss: 6.34423714e-07
Iter: 1084 loss: 6.34296782e-07
Iter: 1085 loss: 6.35712922e-07
Iter: 1086 loss: 6.34300818e-07
Iter: 1087 loss: 6.34163e-07
Iter: 1088 loss: 6.34509e-07
Iter: 1089 loss: 6.34115395e-07
Iter: 1090 loss: 6.34008074e-07
Iter: 1091 loss: 6.3437983e-07
Iter: 1092 loss: 6.33983859e-07
Iter: 1093 loss: 6.33929403e-07
Iter: 1094 loss: 6.3522765e-07
Iter: 1095 loss: 6.33939294e-07
Iter: 1096 loss: 6.33848458e-07
Iter: 1097 loss: 6.33881e-07
Iter: 1098 loss: 6.33798095e-07
Iter: 1099 loss: 6.33687705e-07
Iter: 1100 loss: 6.33754098e-07
Iter: 1101 loss: 6.33618e-07
Iter: 1102 loss: 6.33576519e-07
Iter: 1103 loss: 6.33512286e-07
Iter: 1104 loss: 6.33441687e-07
Iter: 1105 loss: 6.3332584e-07
Iter: 1106 loss: 6.33679065e-07
Iter: 1107 loss: 6.3331504e-07
Iter: 1108 loss: 6.33138598e-07
Iter: 1109 loss: 6.34792855e-07
Iter: 1110 loss: 6.33134505e-07
Iter: 1111 loss: 6.33100399e-07
Iter: 1112 loss: 6.33035427e-07
Iter: 1113 loss: 6.33023433e-07
Iter: 1114 loss: 6.32902129e-07
Iter: 1115 loss: 6.32879789e-07
Iter: 1116 loss: 6.32753085e-07
Iter: 1117 loss: 6.32656054e-07
Iter: 1118 loss: 6.33201e-07
Iter: 1119 loss: 6.32630076e-07
Iter: 1120 loss: 6.32423735e-07
Iter: 1121 loss: 6.33378363e-07
Iter: 1122 loss: 6.32422598e-07
Iter: 1123 loss: 6.32319939e-07
Iter: 1124 loss: 6.329455e-07
Iter: 1125 loss: 6.323462e-07
Iter: 1126 loss: 6.32251442e-07
Iter: 1127 loss: 6.32469153e-07
Iter: 1128 loss: 6.32229671e-07
Iter: 1129 loss: 6.32116553e-07
Iter: 1130 loss: 6.32695901e-07
Iter: 1131 loss: 6.32105071e-07
Iter: 1132 loss: 6.32044873e-07
Iter: 1133 loss: 6.32281512e-07
Iter: 1134 loss: 6.32003605e-07
Iter: 1135 loss: 6.31935563e-07
Iter: 1136 loss: 6.31883154e-07
Iter: 1137 loss: 6.33529794e-07
Iter: 1138 loss: 6.31875537e-07
Iter: 1139 loss: 6.31765374e-07
Iter: 1140 loss: 6.3242e-07
Iter: 1141 loss: 6.31746616e-07
Iter: 1142 loss: 6.31660328e-07
Iter: 1143 loss: 6.32621436e-07
Iter: 1144 loss: 6.316792e-07
Iter: 1145 loss: 6.31587511e-07
Iter: 1146 loss: 6.31473654e-07
Iter: 1147 loss: 6.3147462e-07
Iter: 1148 loss: 6.31338366e-07
Iter: 1149 loss: 6.31723879e-07
Iter: 1150 loss: 6.31285388e-07
Iter: 1151 loss: 6.31172668e-07
Iter: 1152 loss: 6.31041758e-07
Iter: 1153 loss: 6.3102334e-07
Iter: 1154 loss: 6.30900558e-07
Iter: 1155 loss: 6.30900217e-07
Iter: 1156 loss: 6.30847353e-07
Iter: 1157 loss: 6.31121566e-07
Iter: 1158 loss: 6.30822058e-07
Iter: 1159 loss: 6.30758223e-07
Iter: 1160 loss: 6.30859802e-07
Iter: 1161 loss: 6.30696093e-07
Iter: 1162 loss: 6.30599629e-07
Iter: 1163 loss: 6.306351e-07
Iter: 1164 loss: 6.30597697e-07
Iter: 1165 loss: 6.30589625e-07
Iter: 1166 loss: 6.30571776e-07
Iter: 1167 loss: 6.30499756e-07
Iter: 1168 loss: 6.30378793e-07
Iter: 1169 loss: 6.30370096e-07
Iter: 1170 loss: 6.30305294e-07
Iter: 1171 loss: 6.30695e-07
Iter: 1172 loss: 6.3028051e-07
Iter: 1173 loss: 6.30157729e-07
Iter: 1174 loss: 6.30678869e-07
Iter: 1175 loss: 6.30127374e-07
Iter: 1176 loss: 6.29972646e-07
Iter: 1177 loss: 6.301334e-07
Iter: 1178 loss: 6.29895794e-07
Iter: 1179 loss: 6.2977324e-07
Iter: 1180 loss: 6.29974522e-07
Iter: 1181 loss: 6.29727879e-07
Iter: 1182 loss: 6.29610781e-07
Iter: 1183 loss: 6.29541e-07
Iter: 1184 loss: 6.29483395e-07
Iter: 1185 loss: 6.29356919e-07
Iter: 1186 loss: 6.30710588e-07
Iter: 1187 loss: 6.2938534e-07
Iter: 1188 loss: 6.29233853e-07
Iter: 1189 loss: 6.29526824e-07
Iter: 1190 loss: 6.29197416e-07
Iter: 1191 loss: 6.29052636e-07
Iter: 1192 loss: 6.29802457e-07
Iter: 1193 loss: 6.29066e-07
Iter: 1194 loss: 6.29005626e-07
Iter: 1195 loss: 6.28978682e-07
Iter: 1196 loss: 6.28955e-07
Iter: 1197 loss: 6.28863177e-07
Iter: 1198 loss: 6.28849421e-07
Iter: 1199 loss: 6.28695261e-07
Iter: 1200 loss: 6.29147962e-07
Iter: 1201 loss: 6.28684802e-07
Iter: 1202 loss: 6.28610337e-07
Iter: 1203 loss: 6.28535e-07
Iter: 1204 loss: 6.28527687e-07
Iter: 1205 loss: 6.28361704e-07
Iter: 1206 loss: 6.28926045e-07
Iter: 1207 loss: 6.28342946e-07
Iter: 1208 loss: 6.2828974e-07
Iter: 1209 loss: 6.29217766e-07
Iter: 1210 loss: 6.28285477e-07
Iter: 1211 loss: 6.28186626e-07
Iter: 1212 loss: 6.28091925e-07
Iter: 1213 loss: 6.28082717e-07
Iter: 1214 loss: 6.27943e-07
Iter: 1215 loss: 6.28239491e-07
Iter: 1216 loss: 6.27899283e-07
Iter: 1217 loss: 6.27772692e-07
Iter: 1218 loss: 6.27829593e-07
Iter: 1219 loss: 6.27687655e-07
Iter: 1220 loss: 6.27607506e-07
Iter: 1221 loss: 6.29274382e-07
Iter: 1222 loss: 6.27605914e-07
Iter: 1223 loss: 6.27519057e-07
Iter: 1224 loss: 6.27803843e-07
Iter: 1225 loss: 6.27478755e-07
Iter: 1226 loss: 6.27402414e-07
Iter: 1227 loss: 6.27894906e-07
Iter: 1228 loss: 6.27368308e-07
Iter: 1229 loss: 6.27234158e-07
Iter: 1230 loss: 6.27439817e-07
Iter: 1231 loss: 6.27206e-07
Iter: 1232 loss: 6.2712536e-07
Iter: 1233 loss: 6.27493e-07
Iter: 1234 loss: 6.27118538e-07
Iter: 1235 loss: 6.27057148e-07
Iter: 1236 loss: 6.26948633e-07
Iter: 1237 loss: 6.2906912e-07
Iter: 1238 loss: 6.26937833e-07
Iter: 1239 loss: 6.2681454e-07
Iter: 1240 loss: 6.274513e-07
Iter: 1241 loss: 6.26812437e-07
Iter: 1242 loss: 6.26683288e-07
Iter: 1243 loss: 6.27793554e-07
Iter: 1244 loss: 6.26708697e-07
Iter: 1245 loss: 6.26651968e-07
Iter: 1246 loss: 6.26542203e-07
Iter: 1247 loss: 6.26543851e-07
Iter: 1248 loss: 6.26410213e-07
Iter: 1249 loss: 6.26785209e-07
Iter: 1250 loss: 6.2639117e-07
Iter: 1251 loss: 6.2629465e-07
Iter: 1252 loss: 6.26319661e-07
Iter: 1253 loss: 6.2624332e-07
Iter: 1254 loss: 6.26125086e-07
Iter: 1255 loss: 6.26641281e-07
Iter: 1256 loss: 6.2608882e-07
Iter: 1257 loss: 6.2603192e-07
Iter: 1258 loss: 6.26685505e-07
Iter: 1259 loss: 6.26006795e-07
Iter: 1260 loss: 6.25913799e-07
Iter: 1261 loss: 6.2611997e-07
Iter: 1262 loss: 6.25928067e-07
Iter: 1263 loss: 6.25793064e-07
Iter: 1264 loss: 6.26540213e-07
Iter: 1265 loss: 6.25820576e-07
Iter: 1266 loss: 6.25764187e-07
Iter: 1267 loss: 6.25722123e-07
Iter: 1268 loss: 6.25680968e-07
Iter: 1269 loss: 6.25575353e-07
Iter: 1270 loss: 6.25682787e-07
Iter: 1271 loss: 6.25517828e-07
Iter: 1272 loss: 6.25418238e-07
Iter: 1273 loss: 6.25415623e-07
Iter: 1274 loss: 6.2533e-07
Iter: 1275 loss: 6.25240489e-07
Iter: 1276 loss: 6.26547603e-07
Iter: 1277 loss: 6.25251062e-07
Iter: 1278 loss: 6.25117138e-07
Iter: 1279 loss: 6.25181656e-07
Iter: 1280 loss: 6.25068083e-07
Iter: 1281 loss: 6.2495559e-07
Iter: 1282 loss: 6.25189614e-07
Iter: 1283 loss: 6.24880954e-07
Iter: 1284 loss: 6.24789379e-07
Iter: 1285 loss: 6.24884819e-07
Iter: 1286 loss: 6.24737822e-07
Iter: 1287 loss: 6.24575136e-07
Iter: 1288 loss: 6.24955874e-07
Iter: 1289 loss: 6.24514257e-07
Iter: 1290 loss: 6.24454856e-07
Iter: 1291 loss: 6.24914549e-07
Iter: 1292 loss: 6.24449171e-07
Iter: 1293 loss: 6.24304164e-07
Iter: 1294 loss: 6.24779e-07
Iter: 1295 loss: 6.24297854e-07
Iter: 1296 loss: 6.24200766e-07
Iter: 1297 loss: 6.25344853e-07
Iter: 1298 loss: 6.24212873e-07
Iter: 1299 loss: 6.24118684e-07
Iter: 1300 loss: 6.24065763e-07
Iter: 1301 loss: 6.24064796e-07
Iter: 1302 loss: 6.23969186e-07
Iter: 1303 loss: 6.24194172e-07
Iter: 1304 loss: 6.23923086e-07
Iter: 1305 loss: 6.2386863e-07
Iter: 1306 loss: 6.23949177e-07
Iter: 1307 loss: 6.23781148e-07
Iter: 1308 loss: 6.23712651e-07
Iter: 1309 loss: 6.23576e-07
Iter: 1310 loss: 6.2357492e-07
Iter: 1311 loss: 6.23486358e-07
Iter: 1312 loss: 6.2344759e-07
Iter: 1313 loss: 6.23396886e-07
Iter: 1314 loss: 6.23321057e-07
Iter: 1315 loss: 6.23330038e-07
Iter: 1316 loss: 6.23132451e-07
Iter: 1317 loss: 6.23470783e-07
Iter: 1318 loss: 6.2308419e-07
Iter: 1319 loss: 6.23051619e-07
Iter: 1320 loss: 6.230498e-07
Iter: 1321 loss: 6.22957373e-07
Iter: 1322 loss: 6.22807e-07
Iter: 1323 loss: 6.22875632e-07
Iter: 1324 loss: 6.22710957e-07
Iter: 1325 loss: 6.22550772e-07
Iter: 1326 loss: 6.22790822e-07
Iter: 1327 loss: 6.22449079e-07
Iter: 1328 loss: 6.22429354e-07
Iter: 1329 loss: 6.22370635e-07
Iter: 1330 loss: 6.2228969e-07
Iter: 1331 loss: 6.22557195e-07
Iter: 1332 loss: 6.22268431e-07
Iter: 1333 loss: 6.22232903e-07
Iter: 1334 loss: 6.22233e-07
Iter: 1335 loss: 6.22160542e-07
Iter: 1336 loss: 6.22062771e-07
Iter: 1337 loss: 6.22055495e-07
Iter: 1338 loss: 6.21979439e-07
Iter: 1339 loss: 6.21877348e-07
Iter: 1340 loss: 6.22710331e-07
Iter: 1341 loss: 6.21881895e-07
Iter: 1342 loss: 6.21793333e-07
Iter: 1343 loss: 6.2169596e-07
Iter: 1344 loss: 6.21672484e-07
Iter: 1345 loss: 6.21546292e-07
Iter: 1346 loss: 6.22867674e-07
Iter: 1347 loss: 6.21566301e-07
Iter: 1348 loss: 6.21436925e-07
Iter: 1349 loss: 6.21805498e-07
Iter: 1350 loss: 6.2138929e-07
Iter: 1351 loss: 6.21319941e-07
Iter: 1352 loss: 6.21182608e-07
Iter: 1353 loss: 6.23208621e-07
Iter: 1354 loss: 6.21180618e-07
Iter: 1355 loss: 6.21039419e-07
Iter: 1356 loss: 6.2292645e-07
Iter: 1357 loss: 6.21021741e-07
Iter: 1358 loss: 6.20940966e-07
Iter: 1359 loss: 6.20953756e-07
Iter: 1360 loss: 6.20845242e-07
Iter: 1361 loss: 6.20694436e-07
Iter: 1362 loss: 6.21453864e-07
Iter: 1363 loss: 6.20695459e-07
Iter: 1364 loss: 6.20643618e-07
Iter: 1365 loss: 6.20655101e-07
Iter: 1366 loss: 6.20570177e-07
Iter: 1367 loss: 6.20445462e-07
Iter: 1368 loss: 6.2240747e-07
Iter: 1369 loss: 6.20415278e-07
Iter: 1370 loss: 6.2033655e-07
Iter: 1371 loss: 6.21233482e-07
Iter: 1372 loss: 6.20318929e-07
Iter: 1373 loss: 6.20243441e-07
Iter: 1374 loss: 6.203108e-07
Iter: 1375 loss: 6.2019069e-07
Iter: 1376 loss: 6.20109063e-07
Iter: 1377 loss: 6.20148171e-07
Iter: 1378 loss: 6.20025673e-07
Iter: 1379 loss: 6.19944899e-07
Iter: 1380 loss: 6.19951265e-07
Iter: 1381 loss: 6.19895786e-07
Iter: 1382 loss: 6.19856735e-07
Iter: 1383 loss: 6.19832178e-07
Iter: 1384 loss: 6.1975976e-07
Iter: 1385 loss: 6.20575406e-07
Iter: 1386 loss: 6.19711216e-07
Iter: 1387 loss: 6.19642094e-07
Iter: 1388 loss: 6.1954205e-07
Iter: 1389 loss: 6.21697779e-07
Iter: 1390 loss: 6.19502714e-07
Iter: 1391 loss: 6.19392836e-07
Iter: 1392 loss: 6.20253331e-07
Iter: 1393 loss: 6.19364471e-07
Iter: 1394 loss: 6.19276591e-07
Iter: 1395 loss: 6.19377658e-07
Iter: 1396 loss: 6.19249477e-07
Iter: 1397 loss: 6.19138234e-07
Iter: 1398 loss: 6.1988942e-07
Iter: 1399 loss: 6.19135335e-07
Iter: 1400 loss: 6.19071159e-07
Iter: 1401 loss: 6.19032562e-07
Iter: 1402 loss: 6.18993511e-07
Iter: 1403 loss: 6.18879596e-07
Iter: 1404 loss: 6.18876129e-07
Iter: 1405 loss: 6.18788192e-07
Iter: 1406 loss: 6.19091452e-07
Iter: 1407 loss: 6.18793e-07
Iter: 1408 loss: 6.18717252e-07
Iter: 1409 loss: 6.18960712e-07
Iter: 1410 loss: 6.18667627e-07
Iter: 1411 loss: 6.18587194e-07
Iter: 1412 loss: 6.1854189e-07
Iter: 1413 loss: 6.18519437e-07
Iter: 1414 loss: 6.18380113e-07
Iter: 1415 loss: 6.18597255e-07
Iter: 1416 loss: 6.18322929e-07
Iter: 1417 loss: 6.1823539e-07
Iter: 1418 loss: 6.18200204e-07
Iter: 1419 loss: 6.18160811e-07
Iter: 1420 loss: 6.18134e-07
Iter: 1421 loss: 6.18106128e-07
Iter: 1422 loss: 6.17989258e-07
Iter: 1423 loss: 6.1842934e-07
Iter: 1424 loss: 6.1794691e-07
Iter: 1425 loss: 6.17896944e-07
Iter: 1426 loss: 6.17762453e-07
Iter: 1427 loss: 6.20636115e-07
Iter: 1428 loss: 6.17780529e-07
Iter: 1429 loss: 6.1762853e-07
Iter: 1430 loss: 6.18412457e-07
Iter: 1431 loss: 6.17627052e-07
Iter: 1432 loss: 6.17504611e-07
Iter: 1433 loss: 6.17522574e-07
Iter: 1434 loss: 6.17440776e-07
Iter: 1435 loss: 6.17570151e-07
Iter: 1436 loss: 6.17365572e-07
Iter: 1437 loss: 6.17334081e-07
Iter: 1438 loss: 6.17256717e-07
Iter: 1439 loss: 6.1764672e-07
Iter: 1440 loss: 6.17233638e-07
Iter: 1441 loss: 6.17083856e-07
Iter: 1442 loss: 6.17430544e-07
Iter: 1443 loss: 6.17077603e-07
Iter: 1444 loss: 6.16980742e-07
Iter: 1445 loss: 6.17665307e-07
Iter: 1446 loss: 6.17025762e-07
Iter: 1447 loss: 6.1685455e-07
Iter: 1448 loss: 6.168342e-07
Iter: 1449 loss: 6.16819932e-07
Iter: 1450 loss: 6.16647924e-07
Iter: 1451 loss: 6.17202204e-07
Iter: 1452 loss: 6.16656735e-07
Iter: 1453 loss: 6.16529235e-07
Iter: 1454 loss: 6.16914349e-07
Iter: 1455 loss: 6.16505758e-07
Iter: 1456 loss: 6.16454372e-07
Iter: 1457 loss: 6.16867226e-07
Iter: 1458 loss: 6.16403327e-07
Iter: 1459 loss: 6.16327043e-07
Iter: 1460 loss: 6.16318516e-07
Iter: 1461 loss: 6.16285661e-07
Iter: 1462 loss: 6.16148384e-07
Iter: 1463 loss: 6.16782245e-07
Iter: 1464 loss: 6.16140483e-07
Iter: 1465 loss: 6.16043621e-07
Iter: 1466 loss: 6.16031e-07
Iter: 1467 loss: 6.15944373e-07
Iter: 1468 loss: 6.15830061e-07
Iter: 1469 loss: 6.16028501e-07
Iter: 1470 loss: 6.15755596e-07
Iter: 1471 loss: 6.15792715e-07
Iter: 1472 loss: 6.15758836e-07
Iter: 1473 loss: 6.15750309e-07
Iter: 1474 loss: 6.15732461e-07
Iter: 1475 loss: 6.15755653e-07
Iter: 1476 loss: 6.15771569e-07
Iter: 1477 loss: 6.15753436e-07
Iter: 1478 loss: 6.15778504e-07
Iter: 1479 loss: 6.15767249e-07
Iter: 1480 loss: 6.15763383e-07
Iter: 1481 loss: 6.15781e-07
Iter: 1482 loss: 6.15773502e-07
Iter: 1483 loss: 6.15757472e-07
Iter: 1484 loss: 6.15764236e-07
Iter: 1485 loss: 6.15754402e-07
Iter: 1486 loss: 6.15757585e-07
Iter: 1487 loss: 6.15770432e-07
Iter: 1488 loss: 6.15765771e-07
Iter: 1489 loss: 6.15761678e-07
Iter: 1490 loss: 6.15758267e-07
Iter: 1491 loss: 6.15755084e-07
Iter: 1492 loss: 6.15757131e-07
Iter: 1493 loss: 6.15756051e-07
Iter: 1494 loss: 6.15757244e-07
Iter: 1495 loss: 6.15757244e-07
Iter: 1496 loss: 6.15757244e-07
Iter: 1497 loss: 6.15756051e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.4
+ date
Wed Oct 21 14:22:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/300_300_300_1 --function f1 --psi 0 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090888510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f609091d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f609086b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60907c46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090813e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090813510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f609079d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60907b5d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60907b58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60907b59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090716158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090647950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090632620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60906ffd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090603c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f609071b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60906327b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f609061aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f609061a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60905e1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60905c9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60905c4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60905c98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606f0879d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606f087bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606f0876a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606f087620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606efc9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606efc9d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606ef8f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606ef88158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606f04cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60486409d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606f049488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60485e4ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60485a0ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.20872742
test_loss: 0.18890281
train_loss: 0.068045154
test_loss: 0.08276043
train_loss: 0.051019512
test_loss: 0.04934639
train_loss: 0.03440699
test_loss: 0.031075137
train_loss: 0.024313744
test_loss: 0.026351351
train_loss: 0.01606439
test_loss: 0.022647277
train_loss: 0.016208153
test_loss: 0.016326662
train_loss: 0.014925674
test_loss: 0.015660588
train_loss: 0.01502788
test_loss: 0.016618412
train_loss: 0.013141917
test_loss: 0.015140294
train_loss: 0.010812102
test_loss: 0.012925728
train_loss: 0.012105867
test_loss: 0.0133673325
train_loss: 0.01011648
test_loss: 0.012481081
train_loss: 0.0106195
test_loss: 0.010895716
train_loss: 0.009410541
test_loss: 0.009314795
train_loss: 0.0084685
test_loss: 0.009893482
train_loss: 0.008671411
test_loss: 0.009878156
train_loss: 0.008728516
test_loss: 0.009675088
train_loss: 0.00939266
test_loss: 0.01097463
train_loss: 0.011582888
test_loss: 0.010877377
train_loss: 0.011071612
test_loss: 0.01320959
train_loss: 0.008988476
test_loss: 0.008614115
train_loss: 0.009449845
test_loss: 0.009407946
train_loss: 0.009268128
test_loss: 0.010214447
train_loss: 0.009561067
test_loss: 0.009204033
train_loss: 0.010242017
test_loss: 0.012326388
train_loss: 0.007682588
test_loss: 0.008902089
train_loss: 0.0070922207
test_loss: 0.008883099
train_loss: 0.008872329
test_loss: 0.008474584
train_loss: 0.007924541
test_loss: 0.008864081
train_loss: 0.007154837
test_loss: 0.009263131
train_loss: 0.009126032
test_loss: 0.007731152
train_loss: 0.009989722
test_loss: 0.009531968
train_loss: 0.009798058
test_loss: 0.009282159
train_loss: 0.008089025
test_loss: 0.008770944
train_loss: 0.009157786
test_loss: 0.007765141
train_loss: 0.0089976275
test_loss: 0.009700893
train_loss: 0.011280595
test_loss: 0.009397688
train_loss: 0.009971358
test_loss: 0.008813958
train_loss: 0.008531626
test_loss: 0.011750375
train_loss: 0.0070583588
test_loss: 0.01001256
train_loss: 0.010181449
test_loss: 0.012202003
train_loss: 0.010196121
test_loss: 0.010221526
train_loss: 0.008012366
test_loss: 0.00822279
train_loss: 0.0075671794
test_loss: 0.0074775913
train_loss: 0.008185118
test_loss: 0.008107778
train_loss: 0.006936058
test_loss: 0.008879559
train_loss: 0.007773539
test_loss: 0.008649819
train_loss: 0.00755644
test_loss: 0.009199922
train_loss: 0.008418896
test_loss: 0.008637011
train_loss: 0.0077366163
test_loss: 0.0073035476
train_loss: 0.007851082
test_loss: 0.01220799
train_loss: 0.007378394
test_loss: 0.0069241305
train_loss: 0.007018417
test_loss: 0.007831642
train_loss: 0.007209966
test_loss: 0.008507947
train_loss: 0.008503135
test_loss: 0.008407784
train_loss: 0.007031505
test_loss: 0.008854058
train_loss: 0.006628915
test_loss: 0.0091170315
train_loss: 0.0108591365
test_loss: 0.01004231
train_loss: 0.007173881
test_loss: 0.0067829113
train_loss: 0.008821869
test_loss: 0.0075730346
train_loss: 0.008488893
test_loss: 0.009403789
train_loss: 0.008090756
test_loss: 0.009075487
train_loss: 0.0065309885
test_loss: 0.0072625685
train_loss: 0.009495073
test_loss: 0.008005113
train_loss: 0.0071104066
test_loss: 0.008565786
train_loss: 0.0060397754
test_loss: 0.006782623
train_loss: 0.0070364666
test_loss: 0.0074726166
train_loss: 0.006117938
test_loss: 0.0071345307
train_loss: 0.009758755
test_loss: 0.009565067
train_loss: 0.0077913785
test_loss: 0.009720284
train_loss: 0.008531654
test_loss: 0.008862714
train_loss: 0.010527643
test_loss: 0.010964027
train_loss: 0.008841264
test_loss: 0.009020348
train_loss: 0.0075068674
test_loss: 0.008202192
train_loss: 0.0067659807
test_loss: 0.008152663
train_loss: 0.007088872
test_loss: 0.007038209
train_loss: 0.0065797498
test_loss: 0.0074550514
train_loss: 0.007510331
test_loss: 0.008685225
train_loss: 0.0072898385
test_loss: 0.007930386
train_loss: 0.006198352
test_loss: 0.0072230394
train_loss: 0.006142411
test_loss: 0.006647858
train_loss: 0.007517608
test_loss: 0.0071263616
train_loss: 0.008666351
test_loss: 0.008053516
train_loss: 0.006646791
test_loss: 0.0077722236
train_loss: 0.006630582
test_loss: 0.006425538
train_loss: 0.0102561265
test_loss: 0.009068298
train_loss: 0.007677938
test_loss: 0.009667441
train_loss: 0.00630719
test_loss: 0.006544745
train_loss: 0.009065305
test_loss: 0.008412808
train_loss: 0.008195079
test_loss: 0.008673158
train_loss: 0.0057029137
test_loss: 0.0077170064
train_loss: 0.0074552805
test_loss: 0.0075132814
train_loss: 0.007551247
test_loss: 0.0069341664
train_loss: 0.007873169
test_loss: 0.008274232
train_loss: 0.006429212
test_loss: 0.0067346445
train_loss: 0.008331273
test_loss: 0.007291838
train_loss: 0.006365681
test_loss: 0.0063926363
train_loss: 0.007904375
test_loss: 0.009398111
train_loss: 0.008146565
test_loss: 0.008557207
train_loss: 0.0068952115
test_loss: 0.009035617
train_loss: 0.0065378314
test_loss: 0.008110817
train_loss: 0.007821106
test_loss: 0.0065513556
train_loss: 0.0068500238
test_loss: 0.0066578626
train_loss: 0.006703176
test_loss: 0.007295186
train_loss: 0.00625363
test_loss: 0.00748939
train_loss: 0.007251226
test_loss: 0.008146738
train_loss: 0.0074585127
test_loss: 0.008442246
train_loss: 0.008595383
test_loss: 0.0075154477
train_loss: 0.008115757
test_loss: 0.008123233
train_loss: 0.009228764
test_loss: 0.0073960135
train_loss: 0.010422882
test_loss: 0.008693501
train_loss: 0.0069259945
test_loss: 0.007205574
train_loss: 0.0071319686
test_loss: 0.007607532
train_loss: 0.008261762
test_loss: 0.007275671
train_loss: 0.008302253
test_loss: 0.0084925145
train_loss: 0.0076533835
test_loss: 0.009833791
train_loss: 0.0077282423
test_loss: 0.0067328117
train_loss: 0.0071923668
test_loss: 0.0068801595
train_loss: 0.0072912457
test_loss: 0.0067166034
train_loss: 0.007142771
test_loss: 0.0073474697
train_loss: 0.0075549763
test_loss: 0.00717804
train_loss: 0.0075458735
test_loss: 0.008009126
train_loss: 0.0080534965
test_loss: 0.008556047
train_loss: 0.008779319
test_loss: 0.0079969065
train_loss: 0.0057467846
test_loss: 0.0064014313
train_loss: 0.0064373715
test_loss: 0.006807694
train_loss: 0.0074630193
test_loss: 0.0076453644
train_loss: 0.008276716
test_loss: 0.007202833
train_loss: 0.0072056223
test_loss: 0.007419507
train_loss: 0.0066572176
test_loss: 0.007928493
train_loss: 0.0063646627
test_loss: 0.006636821
train_loss: 0.0077901967
test_loss: 0.006531296
train_loss: 0.0075898767
test_loss: 0.0071074823
train_loss: 0.007790772
test_loss: 0.008014868
train_loss: 0.0060235774
test_loss: 0.006490846
train_loss: 0.0105374865
test_loss: 0.010954375
train_loss: 0.0089421
test_loss: 0.0078027397
train_loss: 0.00915683
test_loss: 0.008417166
train_loss: 0.007854873
test_loss: 0.0085025765
train_loss: 0.0068173837
test_loss: 0.0065261205
train_loss: 0.008041456
test_loss: 0.008663225
train_loss: 0.0073769735
test_loss: 0.007856311
train_loss: 0.008820152
test_loss: 0.011174645
train_loss: 0.008664572
test_loss: 0.0074987
train_loss: 0.008491771
test_loss: 0.008358163
train_loss: 0.0071337027
test_loss: 0.0065082535
train_loss: 0.0073087076
test_loss: 0.0073080664
train_loss: 0.008089844
test_loss: 0.007829801
train_loss: 0.0078730555
test_loss: 0.0077403164
train_loss: 0.0070293313
test_loss: 0.0067272326
train_loss: 0.0070700273
test_loss: 0.0063628987
train_loss: 0.00785435
test_loss: 0.00878279
train_loss: 0.008119877
test_loss: 0.00849028
train_loss: 0.0068255104
test_loss: 0.008413637
train_loss: 0.008847116
test_loss: 0.008305935
train_loss: 0.0063858717
test_loss: 0.008028203
train_loss: 0.0080833435
test_loss: 0.008389083
train_loss: 0.009299779
test_loss: 0.0096630845
train_loss: 0.0076110335
test_loss: 0.0075273314
train_loss: 0.008207118
test_loss: 0.006785148
train_loss: 0.007513038
test_loss: 0.008246574
train_loss: 0.0064586126
test_loss: 0.0066353856
train_loss: 0.0060165348
test_loss: 0.0070140185
train_loss: 0.0076617347
test_loss: 0.0074469396
train_loss: 0.006693342
test_loss: 0.0066511524
train_loss: 0.006610619
test_loss: 0.0071841516
train_loss: 0.0077220756
test_loss: 0.010874885
train_loss: 0.007244843
test_loss: 0.008087229
train_loss: 0.0057359748
test_loss: 0.006891572
train_loss: 0.0081759095
test_loss: 0.0069870003
train_loss: 0.0067890054
test_loss: 0.007986112
train_loss: 0.007555412
test_loss: 0.0065123243
train_loss: 0.007984267
test_loss: 0.009954335
train_loss: 0.008297177
test_loss: 0.008968594
train_loss: 0.009216442
test_loss: 0.008656394
train_loss: 0.009050891
test_loss: 0.007920222
train_loss: 0.007831555
test_loss: 0.008905709
train_loss: 0.0064738514
test_loss: 0.006526374
train_loss: 0.0066187233
test_loss: 0.007695753
train_loss: 0.006237707
test_loss: 0.006693731
train_loss: 0.008588003
test_loss: 0.009148438
train_loss: 0.0061626295
test_loss: 0.006835959
train_loss: 0.0069089634
test_loss: 0.0075034094
train_loss: 0.011281594
test_loss: 0.009489423
train_loss: 0.006036386
test_loss: 0.0063375225
train_loss: 0.0071755974
test_loss: 0.006075463
train_loss: 0.0060531395
test_loss: 0.00658513
train_loss: 0.005622782
test_loss: 0.0060446803
train_loss: 0.0071527064
test_loss: 0.007732398
train_loss: 0.0067309383
test_loss: 0.0075264135
train_loss: 0.008455024
test_loss: 0.0085411705
train_loss: 0.0064973272
test_loss: 0.0069622155
train_loss: 0.0070117433
test_loss: 0.0060463003
train_loss: 0.006754242
test_loss: 0.0067551574
train_loss: 0.0062589594
test_loss: 0.008170876
train_loss: 0.007417298
test_loss: 0.006690497
train_loss: 0.007034218
test_loss: 0.0068995166
train_loss: 0.007518013
test_loss: 0.008251477
train_loss: 0.006725745
test_loss: 0.006553806
train_loss: 0.007582272
test_loss: 0.0077563915
train_loss: 0.0069119413
test_loss: 0.00697002
train_loss: 0.007915962
test_loss: 0.008601982
train_loss: 0.008025276
test_loss: 0.006992575
train_loss: 0.007733562
test_loss: 0.008853301
train_loss: 0.0066229994
test_loss: 0.0076031196
train_loss: 0.006569509
test_loss: 0.007059185
train_loss: 0.0068406374
test_loss: 0.0066327094
train_loss: 0.008068558
test_loss: 0.007861589
train_loss: 0.0088012805
test_loss: 0.006618355
train_loss: 0.0073958077
test_loss: 0.0077321604
train_loss: 0.008317079
test_loss: 0.0076611475
train_loss: 0.0063936124
test_loss: 0.007997449
train_loss: 0.008610602
test_loss: 0.007028836
train_loss: 0.007482725
test_loss: 0.0070781335
train_loss: 0.0059038685
test_loss: 0.007949126
train_loss: 0.006470247
test_loss: 0.0080749625
train_loss: 0.0065787053
test_loss: 0.007750476
train_loss: 0.006684088
test_loss: 0.007792245
train_loss: 0.0070909373
test_loss: 0.008074208
train_loss: 0.008419298
test_loss: 0.009696983
train_loss: 0.0056632287
test_loss: 0.006876974
train_loss: 0.009031351
test_loss: 0.007822277
train_loss: 0.0068852603
test_loss: 0.0069076186
train_loss: 0.00588044
test_loss: 0.008068695
train_loss: 0.006903019
test_loss: 0.0070435554
train_loss: 0.006601422
test_loss: 0.0070850067
train_loss: 0.007847646
test_loss: 0.007989908
train_loss: 0.006970734
test_loss: 0.006808796
train_loss: 0.007906539
test_loss: 0.008382834
train_loss: 0.0064063068
test_loss: 0.007781378
train_loss: 0.0072277985
test_loss: 0.0077609937
train_loss: 0.0065185423
test_loss: 0.007936238
train_loss: 0.00758837
test_loss: 0.0068633594
train_loss: 0.006927509
test_loss: 0.0063838684
train_loss: 0.00631133
test_loss: 0.007913471
train_loss: 0.0076213637
test_loss: 0.0077493414
train_loss: 0.0070338864
test_loss: 0.008102359
train_loss: 0.009329438
test_loss: 0.009576277
train_loss: 0.007048591
test_loss: 0.0060204035
train_loss: 0.008556387
test_loss: 0.0076601347
train_loss: 0.0072404896
test_loss: 0.009190103
train_loss: 0.007678191
test_loss: 0.007250055
train_loss: 0.007335485
test_loss: 0.006712199
train_loss: 0.007372286
test_loss: 0.006526644
train_loss: 0.006581875
test_loss: 0.0060034106
train_loss: 0.007977284
test_loss: 0.0070911893
train_loss: 0.008062888
test_loss: 0.0075691855
train_loss: 0.006521521
test_loss: 0.007267384
train_loss: 0.008370779
test_loss: 0.0067328997
train_loss: 0.0057040523
test_loss: 0.0067384057
train_loss: 0.0061733
test_loss: 0.0063020224
train_loss: 0.005312275
test_loss: 0.00473863
train_loss: 0.0073352074
test_loss: 0.006599931
train_loss: 0.007846827
test_loss: 0.007560579
train_loss: 0.0071931337
test_loss: 0.006496719
train_loss: 0.008051438
test_loss: 0.007323966
train_loss: 0.0063601844
test_loss: 0.0068160375
train_loss: 0.0065104943
test_loss: 0.008601179
train_loss: 0.0069702677
test_loss: 0.0095866
train_loss: 0.008417999
test_loss: 0.008713352
train_loss: 0.00859236
test_loss: 0.009023456
train_loss: 0.006239454
test_loss: 0.0075598746
train_loss: 0.0065661753
test_loss: 0.006622642
train_loss: 0.006088611
test_loss: 0.0077587194
train_loss: 0.006005831
test_loss: 0.0073365476
train_loss: 0.0064887116
test_loss: 0.0066512064
train_loss: 0.0067483094
test_loss: 0.0075172726
train_loss: 0.009098213
test_loss: 0.00996621
train_loss: 0.0057813693
test_loss: 0.006636787
train_loss: 0.0059623416
test_loss: 0.0055634067
train_loss: 0.0072701657
test_loss: 0.009130294
train_loss: 0.0072498205
test_loss: 0.007730228
train_loss: 0.0070437137
test_loss: 0.008341713
train_loss: 0.007519734
test_loss: 0.00698932
train_loss: 0.006080169
test_loss: 0.007348281
train_loss: 0.0051974664
test_loss: 0.005649103
train_loss: 0.0074953213
test_loss: 0.007000884
train_loss: 0.0072199553
test_loss: 0.0072241067
train_loss: 0.0074159415
test_loss: 0.0077754967
train_loss: 0.006129588
test_loss: 0.007546826
train_loss: 0.006844315
test_loss: 0.0057884273
train_loss: 0.006597747
test_loss: 0.007759213
train_loss: 0.0058037136
test_loss: 0.0059869196
train_loss: 0.0060951095
test_loss: 0.0070312824
train_loss: 0.0056309174
test_loss: 0.006464141
train_loss: 0.007407102
test_loss: 0.0062195733
train_loss: 0.005515392
test_loss: 0.0068506827
train_loss: 0.0066590942
test_loss: 0.00800936
train_loss: 0.0063605052
test_loss: 0.005927349
train_loss: 0.0065990677
test_loss: 0.0053012334
train_loss: 0.0060062874
test_loss: 0.0056138868
train_loss: 0.0069039855
test_loss: 0.007816476
train_loss: 0.006045164
test_loss: 0.006544653
train_loss: 0.005849502
test_loss: 0.0067210076
train_loss: 0.0072184918
test_loss: 0.006708661
train_loss: 0.007000356
test_loss: 0.005578829
train_loss: 0.0074603814
test_loss: 0.00794285
train_loss: 0.007557681
test_loss: 0.008299833
train_loss: 0.0061899787
test_loss: 0.005816715
train_loss: 0.0065890774
test_loss: 0.009061121
train_loss: 0.0059899553
test_loss: 0.0075360253
train_loss: 0.0060664713
test_loss: 0.007030888
train_loss: 0.0058401246
test_loss: 0.0064230175
train_loss: 0.00805316
test_loss: 0.0075641843
train_loss: 0.008075331
test_loss: 0.007464458
train_loss: 0.0081180055
test_loss: 0.008958629
train_loss: 0.008981779
test_loss: 0.007157031
train_loss: 0.0076278104
test_loss: 0.006922642
train_loss: 0.00963879
test_loss: 0.008986257
train_loss: 0.0068534496
test_loss: 0.0078124744
train_loss: 0.0057415674
test_loss: 0.0055960114
train_loss: 0.0066658673
test_loss: 0.007294874
train_loss: 0.0065649403
test_loss: 0.0069173346
train_loss: 0.0067834603
test_loss: 0.0067394436
train_loss: 0.006323036
test_loss: 0.0069367867
train_loss: 0.006655381
test_loss: 0.008833119
train_loss: 0.006277097
test_loss: 0.007115998
train_loss: 0.0074664284
test_loss: 0.0077007418
train_loss: 0.007928554
test_loss: 0.0060237767
train_loss: 0.0094894
test_loss: 0.0079863025
train_loss: 0.0069058654
test_loss: 0.0074178567
train_loss: 0.007401037
test_loss: 0.00938887
train_loss: 0.0062824525
test_loss: 0.006448144
train_loss: 0.0072041173
test_loss: 0.0077191014
train_loss: 0.006905186
test_loss: 0.0063440744
train_loss: 0.008383147
test_loss: 0.0069724414
train_loss: 0.007469603
test_loss: 0.008103307
train_loss: 0.007136371
test_loss: 0.006022502
train_loss: 0.006144631
test_loss: 0.0075010816
train_loss: 0.0056911656
test_loss: 0.0078046382
train_loss: 0.0079365745
test_loss: 0.008039293
train_loss: 0.008568669
test_loss: 0.007735611
train_loss: 0.008029622
test_loss: 0.0071672024
train_loss: 0.006763625
test_loss: 0.0067672795
train_loss: 0.0058573433
test_loss: 0.007082764
train_loss: 0.0065755406
test_loss: 0.0068377345
train_loss: 0.00596326
test_loss: 0.0058034626
train_loss: 0.008900838
test_loss: 0.00846855
train_loss: 0.006733152
test_loss: 0.006716144
train_loss: 0.0066538663
test_loss: 0.006395754
train_loss: 0.008183698
test_loss: 0.008051383
train_loss: 0.0062361737
test_loss: 0.007527544
train_loss: 0.008694903
test_loss: 0.0057344153
train_loss: 0.0060387165/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0072429795
train_loss: 0.007128897
test_loss: 0.007441837
train_loss: 0.0074600503
test_loss: 0.008160901
train_loss: 0.0051120543
test_loss: 0.0068262387
train_loss: 0.0058884015
test_loss: 0.0052689193
train_loss: 0.006531094
test_loss: 0.006669457
train_loss: 0.0065822634
test_loss: 0.005810184
train_loss: 0.008942133
test_loss: 0.007245229
train_loss: 0.007614551
test_loss: 0.008319966
train_loss: 0.0066486252
test_loss: 0.007700921
train_loss: 0.00725646
test_loss: 0.0066073104
train_loss: 0.006766948
test_loss: 0.0069215777
train_loss: 0.006544228
test_loss: 0.007128981
train_loss: 0.00689883
test_loss: 0.0060078534
train_loss: 0.0053764665
test_loss: 0.006352309
train_loss: 0.0054283678
test_loss: 0.005936882
train_loss: 0.005087239
test_loss: 0.0060640574
train_loss: 0.0057489946
test_loss: 0.005204129
train_loss: 0.008058974
test_loss: 0.0083003165
train_loss: 0.0071192887
test_loss: 0.006848555
train_loss: 0.0066938438
test_loss: 0.0067433347
train_loss: 0.0053874403
test_loss: 0.0061774943
train_loss: 0.006974621
test_loss: 0.0063945106
train_loss: 0.008258961
test_loss: 0.0079707615
train_loss: 0.0068402495
test_loss: 0.007025044
train_loss: 0.007232398
test_loss: 0.006710034
train_loss: 0.00685
test_loss: 0.007321919
train_loss: 0.0065861926
test_loss: 0.0077695157
train_loss: 0.0076235994
test_loss: 0.0063760784
train_loss: 0.007702595
test_loss: 0.0064712344
train_loss: 0.0067503965
test_loss: 0.00809836
train_loss: 0.006717662
test_loss: 0.005910815
train_loss: 0.00457916
test_loss: 0.0061876085
train_loss: 0.008954086
test_loss: 0.007435194
train_loss: 0.006798192
test_loss: 0.008635703
train_loss: 0.009775282
test_loss: 0.0077931336
train_loss: 0.0058005033
test_loss: 0.0059943344
train_loss: 0.007128296
test_loss: 0.007226123
train_loss: 0.007295292
test_loss: 0.006914349
train_loss: 0.0062261885
test_loss: 0.0076432866
train_loss: 0.008589193
test_loss: 0.007139187
train_loss: 0.0077142185
test_loss: 0.007897948
train_loss: 0.009280549
test_loss: 0.008953367
train_loss: 0.006969085
test_loss: 0.0067203697
train_loss: 0.008188476
test_loss: 0.008522287
train_loss: 0.0068190573
test_loss: 0.0060093775
train_loss: 0.007848354
test_loss: 0.0072818245
train_loss: 0.00638867
test_loss: 0.006899582
train_loss: 0.0069265803
test_loss: 0.0062979655
train_loss: 0.0064305402
test_loss: 0.0057002096
train_loss: 0.0068913815
test_loss: 0.0076004937
train_loss: 0.0051635886
test_loss: 0.007412105
train_loss: 0.0077821882
test_loss: 0.008258573
train_loss: 0.0068072146
test_loss: 0.0068506226
train_loss: 0.006311002
test_loss: 0.0073498385
train_loss: 0.005013329
test_loss: 0.0056931996
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24dec620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24e05730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24d8d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24e98378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24e980d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24d656a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24d10ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24cfd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24cfd7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24c901e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24cb52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24c50c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24c71ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24c1c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24bdae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24f8ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24bcf950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24bcff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24bcf0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24b087b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24b25620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24ac60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24b25f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24af98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24ab0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f1189bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f1189b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f118be8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f11883730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f118c7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f11848b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f117f4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f11791840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f117e9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f117ceea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f11787f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.89990923e-05
Iter: 2 loss: 0.000241211033
Iter: 3 loss: 4.09290296e-05
Iter: 4 loss: 3.08190902e-05
Iter: 5 loss: 3.06999464e-05
Iter: 6 loss: 2.76891951e-05
Iter: 7 loss: 2.65847302e-05
Iter: 8 loss: 2.49126351e-05
Iter: 9 loss: 2.11592505e-05
Iter: 10 loss: 1.95246757e-05
Iter: 11 loss: 1.76021604e-05
Iter: 12 loss: 1.38766027e-05
Iter: 13 loss: 5.68527103e-05
Iter: 14 loss: 1.38202877e-05
Iter: 15 loss: 1.25729239e-05
Iter: 16 loss: 1.4559153e-05
Iter: 17 loss: 1.19915449e-05
Iter: 18 loss: 1.06548368e-05
Iter: 19 loss: 1.29176678e-05
Iter: 20 loss: 1.00559819e-05
Iter: 21 loss: 8.75662408e-06
Iter: 22 loss: 1.32848672e-05
Iter: 23 loss: 8.414685e-06
Iter: 24 loss: 7.8747089e-06
Iter: 25 loss: 1.35273222e-05
Iter: 26 loss: 7.86043438e-06
Iter: 27 loss: 7.46944261e-06
Iter: 28 loss: 7.67807614e-06
Iter: 29 loss: 7.2123903e-06
Iter: 30 loss: 6.79185268e-06
Iter: 31 loss: 7.50474328e-06
Iter: 32 loss: 6.6024586e-06
Iter: 33 loss: 6.37205085e-06
Iter: 34 loss: 7.29769545e-06
Iter: 35 loss: 6.32035471e-06
Iter: 36 loss: 6.09023709e-06
Iter: 37 loss: 5.85930866e-06
Iter: 38 loss: 5.81249833e-06
Iter: 39 loss: 5.80857795e-06
Iter: 40 loss: 5.70431575e-06
Iter: 41 loss: 5.55462884e-06
Iter: 42 loss: 5.71424152e-06
Iter: 43 loss: 5.47154059e-06
Iter: 44 loss: 5.40066776e-06
Iter: 45 loss: 5.53448081e-06
Iter: 46 loss: 5.3705453e-06
Iter: 47 loss: 5.27533211e-06
Iter: 48 loss: 5.17228273e-06
Iter: 49 loss: 5.15639476e-06
Iter: 50 loss: 5.01934028e-06
Iter: 51 loss: 5.79788684e-06
Iter: 52 loss: 5.00046053e-06
Iter: 53 loss: 4.8695847e-06
Iter: 54 loss: 4.87450961e-06
Iter: 55 loss: 4.7659164e-06
Iter: 56 loss: 4.63799097e-06
Iter: 57 loss: 5.80681535e-06
Iter: 58 loss: 4.63239076e-06
Iter: 59 loss: 4.52957556e-06
Iter: 60 loss: 4.5662473e-06
Iter: 61 loss: 4.45789e-06
Iter: 62 loss: 4.3566356e-06
Iter: 63 loss: 4.85404507e-06
Iter: 64 loss: 4.33896184e-06
Iter: 65 loss: 4.23400115e-06
Iter: 66 loss: 4.27679106e-06
Iter: 67 loss: 4.16163948e-06
Iter: 68 loss: 4.04716184e-06
Iter: 69 loss: 5.01923751e-06
Iter: 70 loss: 4.04065759e-06
Iter: 71 loss: 3.97741223e-06
Iter: 72 loss: 4.30724594e-06
Iter: 73 loss: 3.96772066e-06
Iter: 74 loss: 3.92109405e-06
Iter: 75 loss: 3.91992171e-06
Iter: 76 loss: 3.89793604e-06
Iter: 77 loss: 3.83966744e-06
Iter: 78 loss: 4.26428e-06
Iter: 79 loss: 3.8273356e-06
Iter: 80 loss: 3.76982189e-06
Iter: 81 loss: 4.64381446e-06
Iter: 82 loss: 3.76957064e-06
Iter: 83 loss: 3.73523972e-06
Iter: 84 loss: 3.71136548e-06
Iter: 85 loss: 3.69902182e-06
Iter: 86 loss: 3.64633911e-06
Iter: 87 loss: 3.94949029e-06
Iter: 88 loss: 3.63927029e-06
Iter: 89 loss: 3.58844045e-06
Iter: 90 loss: 3.63638628e-06
Iter: 91 loss: 3.55915518e-06
Iter: 92 loss: 3.49067295e-06
Iter: 93 loss: 3.69719783e-06
Iter: 94 loss: 3.46970864e-06
Iter: 95 loss: 3.41698842e-06
Iter: 96 loss: 3.60862168e-06
Iter: 97 loss: 3.40424594e-06
Iter: 98 loss: 3.35324239e-06
Iter: 99 loss: 3.49075094e-06
Iter: 100 loss: 3.33637468e-06
Iter: 101 loss: 3.29016666e-06
Iter: 102 loss: 3.44739465e-06
Iter: 103 loss: 3.27757584e-06
Iter: 104 loss: 3.25252108e-06
Iter: 105 loss: 3.25242763e-06
Iter: 106 loss: 3.24405664e-06
Iter: 107 loss: 3.24237362e-06
Iter: 108 loss: 3.23382437e-06
Iter: 109 loss: 3.20921663e-06
Iter: 110 loss: 3.32660534e-06
Iter: 111 loss: 3.20085655e-06
Iter: 112 loss: 3.17384206e-06
Iter: 113 loss: 3.44211639e-06
Iter: 114 loss: 3.17312788e-06
Iter: 115 loss: 3.14790896e-06
Iter: 116 loss: 3.11975646e-06
Iter: 117 loss: 3.11590111e-06
Iter: 118 loss: 3.08015865e-06
Iter: 119 loss: 3.24771077e-06
Iter: 120 loss: 3.0736669e-06
Iter: 121 loss: 3.04213791e-06
Iter: 122 loss: 3.177488e-06
Iter: 123 loss: 3.03569914e-06
Iter: 124 loss: 3.00953889e-06
Iter: 125 loss: 3.08404651e-06
Iter: 126 loss: 3.00141573e-06
Iter: 127 loss: 2.98002e-06
Iter: 128 loss: 3.03689262e-06
Iter: 129 loss: 2.97289398e-06
Iter: 130 loss: 2.95049426e-06
Iter: 131 loss: 2.97234033e-06
Iter: 132 loss: 2.93783796e-06
Iter: 133 loss: 2.90648745e-06
Iter: 134 loss: 2.95400969e-06
Iter: 135 loss: 2.89151922e-06
Iter: 136 loss: 2.86540399e-06
Iter: 137 loss: 3.08980634e-06
Iter: 138 loss: 2.86389104e-06
Iter: 139 loss: 2.85892725e-06
Iter: 140 loss: 2.85412079e-06
Iter: 141 loss: 2.8453926e-06
Iter: 142 loss: 2.82820974e-06
Iter: 143 loss: 3.169411e-06
Iter: 144 loss: 2.82818223e-06
Iter: 145 loss: 2.81598818e-06
Iter: 146 loss: 2.85339729e-06
Iter: 147 loss: 2.81251346e-06
Iter: 148 loss: 2.79607139e-06
Iter: 149 loss: 2.81049188e-06
Iter: 150 loss: 2.78664857e-06
Iter: 151 loss: 2.76938204e-06
Iter: 152 loss: 2.76530955e-06
Iter: 153 loss: 2.7544138e-06
Iter: 154 loss: 2.73038313e-06
Iter: 155 loss: 2.89255013e-06
Iter: 156 loss: 2.72785883e-06
Iter: 157 loss: 2.70936857e-06
Iter: 158 loss: 2.79106962e-06
Iter: 159 loss: 2.70549845e-06
Iter: 160 loss: 2.69118436e-06
Iter: 161 loss: 2.71185854e-06
Iter: 162 loss: 2.68422536e-06
Iter: 163 loss: 2.66608595e-06
Iter: 164 loss: 2.69260249e-06
Iter: 165 loss: 2.65735116e-06
Iter: 166 loss: 2.64026244e-06
Iter: 167 loss: 2.75538218e-06
Iter: 168 loss: 2.63845595e-06
Iter: 169 loss: 2.62605249e-06
Iter: 170 loss: 2.62824096e-06
Iter: 171 loss: 2.61660625e-06
Iter: 172 loss: 2.62062258e-06
Iter: 173 loss: 2.61027662e-06
Iter: 174 loss: 2.60367187e-06
Iter: 175 loss: 2.59284434e-06
Iter: 176 loss: 2.59282751e-06
Iter: 177 loss: 2.58239743e-06
Iter: 178 loss: 2.57674833e-06
Iter: 179 loss: 2.57194915e-06
Iter: 180 loss: 2.56153089e-06
Iter: 181 loss: 2.5613349e-06
Iter: 182 loss: 2.55384566e-06
Iter: 183 loss: 2.54007978e-06
Iter: 184 loss: 2.83093345e-06
Iter: 185 loss: 2.53974304e-06
Iter: 186 loss: 2.52478958e-06
Iter: 187 loss: 2.63034303e-06
Iter: 188 loss: 2.52352675e-06
Iter: 189 loss: 2.5114191e-06
Iter: 190 loss: 2.57043075e-06
Iter: 191 loss: 2.50946664e-06
Iter: 192 loss: 2.50077892e-06
Iter: 193 loss: 2.52864834e-06
Iter: 194 loss: 2.4981307e-06
Iter: 195 loss: 2.48999436e-06
Iter: 196 loss: 2.50057883e-06
Iter: 197 loss: 2.48585025e-06
Iter: 198 loss: 2.47554863e-06
Iter: 199 loss: 2.49905111e-06
Iter: 200 loss: 2.47188837e-06
Iter: 201 loss: 2.46258014e-06
Iter: 202 loss: 2.50101539e-06
Iter: 203 loss: 2.46046238e-06
Iter: 204 loss: 2.45199817e-06
Iter: 205 loss: 2.47438811e-06
Iter: 206 loss: 2.44910188e-06
Iter: 207 loss: 2.43751788e-06
Iter: 208 loss: 2.55314762e-06
Iter: 209 loss: 2.4371509e-06
Iter: 210 loss: 2.43339696e-06
Iter: 211 loss: 2.42436909e-06
Iter: 212 loss: 2.53717053e-06
Iter: 213 loss: 2.42368696e-06
Iter: 214 loss: 2.41462294e-06
Iter: 215 loss: 2.4448118e-06
Iter: 216 loss: 2.41231101e-06
Iter: 217 loss: 2.39972269e-06
Iter: 218 loss: 2.42109081e-06
Iter: 219 loss: 2.39397423e-06
Iter: 220 loss: 2.38493863e-06
Iter: 221 loss: 2.38644816e-06
Iter: 222 loss: 2.37799145e-06
Iter: 223 loss: 2.36692495e-06
Iter: 224 loss: 2.45121123e-06
Iter: 225 loss: 2.36615369e-06
Iter: 226 loss: 2.35605944e-06
Iter: 227 loss: 2.37040604e-06
Iter: 228 loss: 2.35125663e-06
Iter: 229 loss: 2.34137042e-06
Iter: 230 loss: 2.38059e-06
Iter: 231 loss: 2.33914648e-06
Iter: 232 loss: 2.33085757e-06
Iter: 233 loss: 2.34196523e-06
Iter: 234 loss: 2.32688e-06
Iter: 235 loss: 2.31666741e-06
Iter: 236 loss: 2.3397356e-06
Iter: 237 loss: 2.31281297e-06
Iter: 238 loss: 2.30257228e-06
Iter: 239 loss: 2.33537162e-06
Iter: 240 loss: 2.29966236e-06
Iter: 241 loss: 2.30502701e-06
Iter: 242 loss: 2.29721672e-06
Iter: 243 loss: 2.29467332e-06
Iter: 244 loss: 2.28688032e-06
Iter: 245 loss: 2.3082639e-06
Iter: 246 loss: 2.28278577e-06
Iter: 247 loss: 2.27329315e-06
Iter: 248 loss: 2.29886e-06
Iter: 249 loss: 2.27053e-06
Iter: 250 loss: 2.26490897e-06
Iter: 251 loss: 2.2648419e-06
Iter: 252 loss: 2.26033876e-06
Iter: 253 loss: 2.2561544e-06
Iter: 254 loss: 2.25521899e-06
Iter: 255 loss: 2.24808036e-06
Iter: 256 loss: 2.24717655e-06
Iter: 257 loss: 2.24230052e-06
Iter: 258 loss: 2.23658981e-06
Iter: 259 loss: 2.23636494e-06
Iter: 260 loss: 2.23217148e-06
Iter: 261 loss: 2.23099028e-06
Iter: 262 loss: 2.22826475e-06
Iter: 263 loss: 2.22239714e-06
Iter: 264 loss: 2.24680275e-06
Iter: 265 loss: 2.22087579e-06
Iter: 266 loss: 2.2154436e-06
Iter: 267 loss: 2.21868777e-06
Iter: 268 loss: 2.21167738e-06
Iter: 269 loss: 2.2051895e-06
Iter: 270 loss: 2.24967926e-06
Iter: 271 loss: 2.20454694e-06
Iter: 272 loss: 2.20057927e-06
Iter: 273 loss: 2.21325263e-06
Iter: 274 loss: 2.1993169e-06
Iter: 275 loss: 2.19730646e-06
Iter: 276 loss: 2.19657181e-06
Iter: 277 loss: 2.19510366e-06
Iter: 278 loss: 2.19068806e-06
Iter: 279 loss: 2.19576464e-06
Iter: 280 loss: 2.18724335e-06
Iter: 281 loss: 2.18198875e-06
Iter: 282 loss: 2.24675159e-06
Iter: 283 loss: 2.18186e-06
Iter: 284 loss: 2.17716479e-06
Iter: 285 loss: 2.18903801e-06
Iter: 286 loss: 2.17551496e-06
Iter: 287 loss: 2.17162778e-06
Iter: 288 loss: 2.16981175e-06
Iter: 289 loss: 2.16785861e-06
Iter: 290 loss: 2.16201079e-06
Iter: 291 loss: 2.1802075e-06
Iter: 292 loss: 2.16024591e-06
Iter: 293 loss: 2.15445652e-06
Iter: 294 loss: 2.19308549e-06
Iter: 295 loss: 2.15392311e-06
Iter: 296 loss: 2.14933243e-06
Iter: 297 loss: 2.14904094e-06
Iter: 298 loss: 2.14566398e-06
Iter: 299 loss: 2.13962244e-06
Iter: 300 loss: 2.18028845e-06
Iter: 301 loss: 2.13898716e-06
Iter: 302 loss: 2.13491967e-06
Iter: 303 loss: 2.13840121e-06
Iter: 304 loss: 2.13243175e-06
Iter: 305 loss: 2.12718351e-06
Iter: 306 loss: 2.14636111e-06
Iter: 307 loss: 2.12591112e-06
Iter: 308 loss: 2.12529085e-06
Iter: 309 loss: 2.12408895e-06
Iter: 310 loss: 2.12197347e-06
Iter: 311 loss: 2.11739325e-06
Iter: 312 loss: 2.20688935e-06
Iter: 313 loss: 2.11739462e-06
Iter: 314 loss: 2.11422025e-06
Iter: 315 loss: 2.11338033e-06
Iter: 316 loss: 2.11158522e-06
Iter: 317 loss: 2.10655367e-06
Iter: 318 loss: 2.14774491e-06
Iter: 319 loss: 2.10627672e-06
Iter: 320 loss: 2.10144231e-06
Iter: 321 loss: 2.1009098e-06
Iter: 322 loss: 2.09746486e-06
Iter: 323 loss: 2.0932091e-06
Iter: 324 loss: 2.10555686e-06
Iter: 325 loss: 2.0919349e-06
Iter: 326 loss: 2.08769939e-06
Iter: 327 loss: 2.1068206e-06
Iter: 328 loss: 2.08692768e-06
Iter: 329 loss: 2.08269716e-06
Iter: 330 loss: 2.08888946e-06
Iter: 331 loss: 2.08046595e-06
Iter: 332 loss: 2.07587436e-06
Iter: 333 loss: 2.08327083e-06
Iter: 334 loss: 2.07378025e-06
Iter: 335 loss: 2.06891309e-06
Iter: 336 loss: 2.10227017e-06
Iter: 337 loss: 2.0685452e-06
Iter: 338 loss: 2.06548475e-06
Iter: 339 loss: 2.06756658e-06
Iter: 340 loss: 2.06357663e-06
Iter: 341 loss: 2.06169807e-06
Iter: 342 loss: 2.06140498e-06
Iter: 343 loss: 2.05948504e-06
Iter: 344 loss: 2.07251901e-06
Iter: 345 loss: 2.05928018e-06
Iter: 346 loss: 2.05834385e-06
Iter: 347 loss: 2.05547212e-06
Iter: 348 loss: 2.06113327e-06
Iter: 349 loss: 2.05381343e-06
Iter: 350 loss: 2.04988851e-06
Iter: 351 loss: 2.08364986e-06
Iter: 352 loss: 2.04968546e-06
Iter: 353 loss: 2.04593971e-06
Iter: 354 loss: 2.06214418e-06
Iter: 355 loss: 2.0451771e-06
Iter: 356 loss: 2.04199137e-06
Iter: 357 loss: 2.04014941e-06
Iter: 358 loss: 2.03878e-06
Iter: 359 loss: 2.03566697e-06
Iter: 360 loss: 2.04755042e-06
Iter: 361 loss: 2.03491481e-06
Iter: 362 loss: 2.03113655e-06
Iter: 363 loss: 2.04455409e-06
Iter: 364 loss: 2.03011814e-06
Iter: 365 loss: 2.02638648e-06
Iter: 366 loss: 2.02900787e-06
Iter: 367 loss: 2.02396814e-06
Iter: 368 loss: 2.02058686e-06
Iter: 369 loss: 2.04620892e-06
Iter: 370 loss: 2.02033675e-06
Iter: 371 loss: 2.0180787e-06
Iter: 372 loss: 2.02314868e-06
Iter: 373 loss: 2.0170387e-06
Iter: 374 loss: 2.01492139e-06
Iter: 375 loss: 2.02926685e-06
Iter: 376 loss: 2.01478e-06
Iter: 377 loss: 2.01302282e-06
Iter: 378 loss: 2.01302782e-06
Iter: 379 loss: 2.01183502e-06
Iter: 380 loss: 2.00898057e-06
Iter: 381 loss: 2.03365403e-06
Iter: 382 loss: 2.00836143e-06
Iter: 383 loss: 2.00551199e-06
Iter: 384 loss: 2.00965201e-06
Iter: 385 loss: 2.00419845e-06
Iter: 386 loss: 2.00170257e-06
Iter: 387 loss: 2.03556328e-06
Iter: 388 loss: 2.00171348e-06
Iter: 389 loss: 1.99938381e-06
Iter: 390 loss: 1.99652e-06
Iter: 391 loss: 1.99626379e-06
Iter: 392 loss: 1.99276315e-06
Iter: 393 loss: 1.99973965e-06
Iter: 394 loss: 1.99143096e-06
Iter: 395 loss: 1.98774387e-06
Iter: 396 loss: 1.99746637e-06
Iter: 397 loss: 1.98676048e-06
Iter: 398 loss: 1.98206317e-06
Iter: 399 loss: 2.00310365e-06
Iter: 400 loss: 1.98121734e-06
Iter: 401 loss: 1.97863073e-06
Iter: 402 loss: 1.97985401e-06
Iter: 403 loss: 1.9768022e-06
Iter: 404 loss: 1.9734382e-06
Iter: 405 loss: 1.9906538e-06
Iter: 406 loss: 1.97287e-06
Iter: 407 loss: 1.96993915e-06
Iter: 408 loss: 1.9785939e-06
Iter: 409 loss: 1.96917495e-06
Iter: 410 loss: 1.97000782e-06
Iter: 411 loss: 1.96815677e-06
Iter: 412 loss: 1.96742712e-06
Iter: 413 loss: 1.9655472e-06
Iter: 414 loss: 1.98519638e-06
Iter: 415 loss: 1.96522819e-06
Iter: 416 loss: 1.96285782e-06
Iter: 417 loss: 1.96106021e-06
Iter: 418 loss: 1.96026053e-06
Iter: 419 loss: 1.95808229e-06
Iter: 420 loss: 1.95808684e-06
Iter: 421 loss: 1.95652228e-06
Iter: 422 loss: 1.96430892e-06
Iter: 423 loss: 1.95623397e-06
Iter: 424 loss: 1.95491225e-06
Iter: 425 loss: 1.951689e-06
Iter: 426 loss: 1.99110264e-06
Iter: 427 loss: 1.95138136e-06
Iter: 428 loss: 1.94821246e-06
Iter: 429 loss: 1.9661652e-06
Iter: 430 loss: 1.94767017e-06
Iter: 431 loss: 1.94580753e-06
Iter: 432 loss: 1.97572945e-06
Iter: 433 loss: 1.94568292e-06
Iter: 434 loss: 1.94414406e-06
Iter: 435 loss: 1.9432855e-06
Iter: 436 loss: 1.94257882e-06
Iter: 437 loss: 1.94034556e-06
Iter: 438 loss: 1.94666291e-06
Iter: 439 loss: 1.93961932e-06
Iter: 440 loss: 1.93784717e-06
Iter: 441 loss: 1.95362145e-06
Iter: 442 loss: 1.93779056e-06
Iter: 443 loss: 1.93679853e-06
Iter: 444 loss: 1.95111215e-06
Iter: 445 loss: 1.936736e-06
Iter: 446 loss: 1.93549295e-06
Iter: 447 loss: 1.93494725e-06
Iter: 448 loss: 1.9342317e-06
Iter: 449 loss: 1.93327787e-06
Iter: 450 loss: 1.93260962e-06
Iter: 451 loss: 1.93221445e-06
Iter: 452 loss: 1.93068081e-06
Iter: 453 loss: 1.93218102e-06
Iter: 454 loss: 1.92981611e-06
Iter: 455 loss: 1.92835751e-06
Iter: 456 loss: 1.94972267e-06
Iter: 457 loss: 1.92829907e-06
Iter: 458 loss: 1.92727452e-06
Iter: 459 loss: 1.92565972e-06
Iter: 460 loss: 1.92560151e-06
Iter: 461 loss: 1.92326274e-06
Iter: 462 loss: 1.92325047e-06
Iter: 463 loss: 1.92157586e-06
Iter: 464 loss: 1.91887602e-06
Iter: 465 loss: 1.93064693e-06
Iter: 466 loss: 1.91838035e-06
Iter: 467 loss: 1.91662161e-06
Iter: 468 loss: 1.91661e-06
Iter: 469 loss: 1.91522622e-06
Iter: 470 loss: 1.91365507e-06
Iter: 471 loss: 1.91365916e-06
Iter: 472 loss: 1.91128129e-06
Iter: 473 loss: 1.91848358e-06
Iter: 474 loss: 1.91063782e-06
Iter: 475 loss: 1.90837477e-06
Iter: 476 loss: 1.92725565e-06
Iter: 477 loss: 1.90821856e-06
Iter: 478 loss: 1.90766286e-06
Iter: 479 loss: 1.90714e-06
Iter: 480 loss: 1.90688741e-06
Iter: 481 loss: 1.90562582e-06
Iter: 482 loss: 1.90811329e-06
Iter: 483 loss: 1.90469359e-06
Iter: 484 loss: 1.9025739e-06
Iter: 485 loss: 1.90918445e-06
Iter: 486 loss: 1.90189553e-06
Iter: 487 loss: 1.89967136e-06
Iter: 488 loss: 1.9081765e-06
Iter: 489 loss: 1.89907792e-06
Iter: 490 loss: 1.8972579e-06
Iter: 491 loss: 1.91467325e-06
Iter: 492 loss: 1.897164e-06
Iter: 493 loss: 1.89572245e-06
Iter: 494 loss: 1.89385628e-06
Iter: 495 loss: 1.89374828e-06
Iter: 496 loss: 1.89137563e-06
Iter: 497 loss: 1.89240632e-06
Iter: 498 loss: 1.88972137e-06
Iter: 499 loss: 1.8871757e-06
Iter: 500 loss: 1.90956553e-06
Iter: 501 loss: 1.88704757e-06
Iter: 502 loss: 1.88485615e-06
Iter: 503 loss: 1.89449713e-06
Iter: 504 loss: 1.88438275e-06
Iter: 505 loss: 1.88226613e-06
Iter: 506 loss: 1.88213971e-06
Iter: 507 loss: 1.8805606e-06
Iter: 508 loss: 1.87907222e-06
Iter: 509 loss: 1.90266599e-06
Iter: 510 loss: 1.87892238e-06
Iter: 511 loss: 1.87833427e-06
Iter: 512 loss: 1.87812691e-06
Iter: 513 loss: 1.87736282e-06
Iter: 514 loss: 1.87504156e-06
Iter: 515 loss: 1.89386856e-06
Iter: 516 loss: 1.87466946e-06
Iter: 517 loss: 1.87311207e-06
Iter: 518 loss: 1.87582191e-06
Iter: 519 loss: 1.87224191e-06
Iter: 520 loss: 1.87063199e-06
Iter: 521 loss: 1.88150034e-06
Iter: 522 loss: 1.8704169e-06
Iter: 523 loss: 1.86887223e-06
Iter: 524 loss: 1.8728656e-06
Iter: 525 loss: 1.8685779e-06
Iter: 526 loss: 1.86677084e-06
Iter: 527 loss: 1.86841248e-06
Iter: 528 loss: 1.86570696e-06
Iter: 529 loss: 1.86421187e-06
Iter: 530 loss: 1.86541752e-06
Iter: 531 loss: 1.86351497e-06
Iter: 532 loss: 1.86172258e-06
Iter: 533 loss: 1.86305397e-06
Iter: 534 loss: 1.86080888e-06
Iter: 535 loss: 1.8587682e-06
Iter: 536 loss: 1.87346291e-06
Iter: 537 loss: 1.85857482e-06
Iter: 538 loss: 1.85654778e-06
Iter: 539 loss: 1.85840383e-06
Iter: 540 loss: 1.85530791e-06
Iter: 541 loss: 1.85349973e-06
Iter: 542 loss: 1.85822432e-06
Iter: 543 loss: 1.85299109e-06
Iter: 544 loss: 1.85253657e-06
Iter: 545 loss: 1.85209319e-06
Iter: 546 loss: 1.85117347e-06
Iter: 547 loss: 1.85037015e-06
Iter: 548 loss: 1.85006547e-06
Iter: 549 loss: 1.84902342e-06
Iter: 550 loss: 1.84802661e-06
Iter: 551 loss: 1.84784153e-06
Iter: 552 loss: 1.84668579e-06
Iter: 553 loss: 1.85039914e-06
Iter: 554 loss: 1.84637759e-06
Iter: 555 loss: 1.84524754e-06
Iter: 556 loss: 1.85205954e-06
Iter: 557 loss: 1.84499231e-06
Iter: 558 loss: 1.84376506e-06
Iter: 559 loss: 1.84542614e-06
Iter: 560 loss: 1.84311193e-06
Iter: 561 loss: 1.84179123e-06
Iter: 562 loss: 1.8430502e-06
Iter: 563 loss: 1.84089583e-06
Iter: 564 loss: 1.83979091e-06
Iter: 565 loss: 1.84077533e-06
Iter: 566 loss: 1.83919769e-06
Iter: 567 loss: 1.83767145e-06
Iter: 568 loss: 1.84037128e-06
Iter: 569 loss: 1.83702355e-06
Iter: 570 loss: 1.83529573e-06
Iter: 571 loss: 1.85053545e-06
Iter: 572 loss: 1.83520467e-06
Iter: 573 loss: 1.83421594e-06
Iter: 574 loss: 1.83440989e-06
Iter: 575 loss: 1.83319514e-06
Iter: 576 loss: 1.83242253e-06
Iter: 577 loss: 1.83241036e-06
Iter: 578 loss: 1.83128043e-06
Iter: 579 loss: 1.8327321e-06
Iter: 580 loss: 1.83075929e-06
Iter: 581 loss: 1.82997007e-06
Iter: 582 loss: 1.82888243e-06
Iter: 583 loss: 1.82873146e-06
Iter: 584 loss: 1.82749136e-06
Iter: 585 loss: 1.82721931e-06
Iter: 586 loss: 1.82642987e-06
Iter: 587 loss: 1.82524559e-06
Iter: 588 loss: 1.84138435e-06
Iter: 589 loss: 1.82524172e-06
Iter: 590 loss: 1.82406268e-06
Iter: 591 loss: 1.82598683e-06
Iter: 592 loss: 1.82363897e-06
Iter: 593 loss: 1.82241888e-06
Iter: 594 loss: 1.82566373e-06
Iter: 595 loss: 1.82205986e-06
Iter: 596 loss: 1.82114422e-06
Iter: 597 loss: 1.82027452e-06
Iter: 598 loss: 1.82011024e-06
Iter: 599 loss: 1.81859332e-06
Iter: 600 loss: 1.82482984e-06
Iter: 601 loss: 1.81820451e-06
Iter: 602 loss: 1.81695009e-06
Iter: 603 loss: 1.82811834e-06
Iter: 604 loss: 1.81695577e-06
Iter: 605 loss: 1.81592225e-06
Iter: 606 loss: 1.81734242e-06
Iter: 607 loss: 1.81556425e-06
Iter: 608 loss: 1.81477651e-06
Iter: 609 loss: 1.82365511e-06
Iter: 610 loss: 1.81475707e-06
Iter: 611 loss: 1.81402322e-06
Iter: 612 loss: 1.82011036e-06
Iter: 613 loss: 1.81397445e-06
Iter: 614 loss: 1.81363259e-06
Iter: 615 loss: 1.81284372e-06
Iter: 616 loss: 1.82818792e-06
Iter: 617 loss: 1.81292944e-06
Iter: 618 loss: 1.81205428e-06
Iter: 619 loss: 1.81180178e-06
Iter: 620 loss: 1.8112637e-06
Iter: 621 loss: 1.81014775e-06
Iter: 622 loss: 1.81468135e-06
Iter: 623 loss: 1.80994698e-06
Iter: 624 loss: 1.80881887e-06
Iter: 625 loss: 1.82087797e-06
Iter: 626 loss: 1.80882864e-06
Iter: 627 loss: 1.80819973e-06
Iter: 628 loss: 1.80941436e-06
Iter: 629 loss: 1.80804307e-06
Iter: 630 loss: 1.80745064e-06
Iter: 631 loss: 1.80668167e-06
Iter: 632 loss: 1.80671918e-06
Iter: 633 loss: 1.80530128e-06
Iter: 634 loss: 1.80646373e-06
Iter: 635 loss: 1.80465565e-06
Iter: 636 loss: 1.80337929e-06
Iter: 637 loss: 1.81851613e-06
Iter: 638 loss: 1.80337872e-06
Iter: 639 loss: 1.80255893e-06
Iter: 640 loss: 1.80432335e-06
Iter: 641 loss: 1.80212896e-06
Iter: 642 loss: 1.80121242e-06
Iter: 643 loss: 1.80309394e-06
Iter: 644 loss: 1.80074733e-06
Iter: 645 loss: 1.79995914e-06
Iter: 646 loss: 1.79988524e-06
Iter: 647 loss: 1.79941708e-06
Iter: 648 loss: 1.79839844e-06
Iter: 649 loss: 1.81113614e-06
Iter: 650 loss: 1.79828442e-06
Iter: 651 loss: 1.79728454e-06
Iter: 652 loss: 1.79902008e-06
Iter: 653 loss: 1.7969503e-06
Iter: 654 loss: 1.79579138e-06
Iter: 655 loss: 1.79582389e-06
Iter: 656 loss: 1.7950108e-06
Iter: 657 loss: 1.79388837e-06
Iter: 658 loss: 1.79389701e-06
Iter: 659 loss: 1.79301549e-06
Iter: 660 loss: 1.79382323e-06
Iter: 661 loss: 1.79254937e-06
Iter: 662 loss: 1.79175e-06
Iter: 663 loss: 1.79276276e-06
Iter: 664 loss: 1.79131621e-06
Iter: 665 loss: 1.79026949e-06
Iter: 666 loss: 1.78917912e-06
Iter: 667 loss: 1.78891014e-06
Iter: 668 loss: 1.78754465e-06
Iter: 669 loss: 1.80811639e-06
Iter: 670 loss: 1.78758705e-06
Iter: 671 loss: 1.78658115e-06
Iter: 672 loss: 1.79259905e-06
Iter: 673 loss: 1.78656899e-06
Iter: 674 loss: 1.78573282e-06
Iter: 675 loss: 1.78537027e-06
Iter: 676 loss: 1.78498703e-06
Iter: 677 loss: 1.78536493e-06
Iter: 678 loss: 1.78448e-06
Iter: 679 loss: 1.78416428e-06
Iter: 680 loss: 1.78326684e-06
Iter: 681 loss: 1.78804805e-06
Iter: 682 loss: 1.78291862e-06
Iter: 683 loss: 1.78192909e-06
Iter: 684 loss: 1.78562323e-06
Iter: 685 loss: 1.78161213e-06
Iter: 686 loss: 1.78072082e-06
Iter: 687 loss: 1.78075697e-06
Iter: 688 loss: 1.77993115e-06
Iter: 689 loss: 1.7788733e-06
Iter: 690 loss: 1.79285405e-06
Iter: 691 loss: 1.77883851e-06
Iter: 692 loss: 1.77800734e-06
Iter: 693 loss: 1.7790386e-06
Iter: 694 loss: 1.77754919e-06
Iter: 695 loss: 1.77669369e-06
Iter: 696 loss: 1.77946595e-06
Iter: 697 loss: 1.77631716e-06
Iter: 698 loss: 1.77553454e-06
Iter: 699 loss: 1.77544405e-06
Iter: 700 loss: 1.77482866e-06
Iter: 701 loss: 1.77378411e-06
Iter: 702 loss: 1.77500033e-06
Iter: 703 loss: 1.77326979e-06
Iter: 704 loss: 1.77245113e-06
Iter: 705 loss: 1.77252332e-06
Iter: 706 loss: 1.77167715e-06
Iter: 707 loss: 1.77115601e-06
Iter: 708 loss: 1.77091886e-06
Iter: 709 loss: 1.77093523e-06
Iter: 710 loss: 1.77062327e-06
Iter: 711 loss: 1.77002039e-06
Iter: 712 loss: 1.7691649e-06
Iter: 713 loss: 1.78502466e-06
Iter: 714 loss: 1.76909896e-06
Iter: 715 loss: 1.76851051e-06
Iter: 716 loss: 1.76942876e-06
Iter: 717 loss: 1.76804497e-06
Iter: 718 loss: 1.76723029e-06
Iter: 719 loss: 1.76708807e-06
Iter: 720 loss: 1.76639753e-06
Iter: 721 loss: 1.765511e-06
Iter: 722 loss: 1.77180243e-06
Iter: 723 loss: 1.76529875e-06
Iter: 724 loss: 1.76452659e-06
Iter: 725 loss: 1.77293873e-06
Iter: 726 loss: 1.76463209e-06
Iter: 727 loss: 1.76396009e-06
Iter: 728 loss: 1.76318281e-06
Iter: 729 loss: 1.76325511e-06
Iter: 730 loss: 1.7620589e-06
Iter: 731 loss: 1.76447054e-06
Iter: 732 loss: 1.76186063e-06
Iter: 733 loss: 1.7606942e-06
Iter: 734 loss: 1.76290018e-06
Iter: 735 loss: 1.7601983e-06
Iter: 736 loss: 1.75928312e-06
Iter: 737 loss: 1.75880837e-06
Iter: 738 loss: 1.75852949e-06
Iter: 739 loss: 1.75743503e-06
Iter: 740 loss: 1.75739751e-06
Iter: 741 loss: 1.75680395e-06
Iter: 742 loss: 1.75924549e-06
Iter: 743 loss: 1.75663342e-06
Iter: 744 loss: 1.75609057e-06
Iter: 745 loss: 1.76050867e-06
Iter: 746 loss: 1.75593016e-06
Iter: 747 loss: 1.75567777e-06
Iter: 748 loss: 1.7549138e-06
Iter: 749 loss: 1.75695141e-06
Iter: 750 loss: 1.75461548e-06
Iter: 751 loss: 1.75318041e-06
Iter: 752 loss: 1.75824607e-06
Iter: 753 loss: 1.75269929e-06
Iter: 754 loss: 1.75169259e-06
Iter: 755 loss: 1.76542108e-06
Iter: 756 loss: 1.75178775e-06
Iter: 757 loss: 1.75104378e-06
Iter: 758 loss: 1.75261073e-06
Iter: 759 loss: 1.75079322e-06
Iter: 760 loss: 1.7498046e-06
Iter: 761 loss: 1.75082369e-06
Iter: 762 loss: 1.7493619e-06
Iter: 763 loss: 1.74843206e-06
Iter: 764 loss: 1.74927425e-06
Iter: 765 loss: 1.74793627e-06
Iter: 766 loss: 1.74688262e-06
Iter: 767 loss: 1.75097068e-06
Iter: 768 loss: 1.74648949e-06
Iter: 769 loss: 1.74571994e-06
Iter: 770 loss: 1.74488537e-06
Iter: 771 loss: 1.74469585e-06
Iter: 772 loss: 1.74371678e-06
Iter: 773 loss: 1.74369916e-06
Iter: 774 loss: 1.74285515e-06
Iter: 775 loss: 1.75210062e-06
Iter: 776 loss: 1.74294155e-06
Iter: 777 loss: 1.74272418e-06
Iter: 778 loss: 1.7426662e-06
Iter: 779 loss: 1.74260458e-06
Iter: 780 loss: 1.74187926e-06
Iter: 781 loss: 1.74150841e-06
Iter: 782 loss: 1.74134288e-06
Iter: 783 loss: 1.74045044e-06
Iter: 784 loss: 1.75112189e-06
Iter: 785 loss: 1.74031652e-06
Iter: 786 loss: 1.73960279e-06
Iter: 787 loss: 1.74268962e-06
Iter: 788 loss: 1.73948649e-06
Iter: 789 loss: 1.7390189e-06
Iter: 790 loss: 1.74036313e-06
Iter: 791 loss: 1.73885735e-06
Iter: 792 loss: 1.73813442e-06
Iter: 793 loss: 1.74132231e-06
Iter: 794 loss: 1.73806211e-06
Iter: 795 loss: 1.73726494e-06
Iter: 796 loss: 1.73592616e-06
Iter: 797 loss: 1.73595777e-06
Iter: 798 loss: 1.73496278e-06
Iter: 799 loss: 1.73969806e-06
Iter: 800 loss: 1.73477758e-06
Iter: 801 loss: 1.73378e-06
Iter: 802 loss: 1.73563444e-06
Iter: 803 loss: 1.73340504e-06
Iter: 804 loss: 1.73224328e-06
Iter: 805 loss: 1.73642343e-06
Iter: 806 loss: 1.73206206e-06
Iter: 807 loss: 1.73123226e-06
Iter: 808 loss: 1.73604724e-06
Iter: 809 loss: 1.73112369e-06
Iter: 810 loss: 1.73085084e-06
Iter: 811 loss: 1.73074727e-06
Iter: 812 loss: 1.73035664e-06
Iter: 813 loss: 1.73001695e-06
Iter: 814 loss: 1.73002422e-06
Iter: 815 loss: 1.72951547e-06
Iter: 816 loss: 1.72823388e-06
Iter: 817 loss: 1.73906517e-06
Iter: 818 loss: 1.7280488e-06
Iter: 819 loss: 1.7269092e-06
Iter: 820 loss: 1.73034209e-06
Iter: 821 loss: 1.72662385e-06
Iter: 822 loss: 1.72544901e-06
Iter: 823 loss: 1.73527928e-06
Iter: 824 loss: 1.72540445e-06
Iter: 825 loss: 1.72430578e-06
Iter: 826 loss: 1.72760633e-06
Iter: 827 loss: 1.72406283e-06
Iter: 828 loss: 1.72341583e-06
Iter: 829 loss: 1.73016599e-06
Iter: 830 loss: 1.72344096e-06
Iter: 831 loss: 1.72287525e-06
Iter: 832 loss: 1.72409045e-06
Iter: 833 loss: 1.72258387e-06
Iter: 834 loss: 1.72198133e-06
Iter: 835 loss: 1.72144667e-06
Iter: 836 loss: 1.7212642e-06
Iter: 837 loss: 1.72024647e-06
Iter: 838 loss: 1.72188936e-06
Iter: 839 loss: 1.71983129e-06
Iter: 840 loss: 1.7187848e-06
Iter: 841 loss: 1.72341061e-06
Iter: 842 loss: 1.71874024e-06
Iter: 843 loss: 1.71775241e-06
Iter: 844 loss: 1.72135697e-06
Iter: 845 loss: 1.71767954e-06
Iter: 846 loss: 1.71660031e-06
Iter: 847 loss: 1.72808507e-06
Iter: 848 loss: 1.71655824e-06
Iter: 849 loss: 1.71634565e-06
Iter: 850 loss: 1.71569627e-06
Iter: 851 loss: 1.72242289e-06
Iter: 852 loss: 1.71560418e-06
Iter: 853 loss: 1.71502222e-06
Iter: 854 loss: 1.71977308e-06
Iter: 855 loss: 1.71502802e-06
Iter: 856 loss: 1.71435727e-06
Iter: 857 loss: 1.71354804e-06
Iter: 858 loss: 1.71356419e-06
Iter: 859 loss: 1.71266993e-06
Iter: 860 loss: 1.71867066e-06
Iter: 861 loss: 1.71257216e-06
Iter: 862 loss: 1.71207125e-06
Iter: 863 loss: 1.71672446e-06
Iter: 864 loss: 1.71199622e-06
Iter: 865 loss: 1.71144734e-06
Iter: 866 loss: 1.71147963e-06
Iter: 867 loss: 1.71105967e-06
Iter: 868 loss: 1.71022009e-06
Iter: 869 loss: 1.71607405e-06
Iter: 870 loss: 1.71008014e-06
Iter: 871 loss: 1.70957151e-06
Iter: 872 loss: 1.70901217e-06
Iter: 873 loss: 1.70883504e-06
Iter: 874 loss: 1.70813018e-06
Iter: 875 loss: 1.71223166e-06
Iter: 876 loss: 1.70793373e-06
Iter: 877 loss: 1.70716851e-06
Iter: 878 loss: 1.70780379e-06
Iter: 879 loss: 1.70664725e-06
Iter: 880 loss: 1.70817646e-06
Iter: 881 loss: 1.70647309e-06
Iter: 882 loss: 1.70628448e-06
Iter: 883 loss: 1.70590079e-06
Iter: 884 loss: 1.70631813e-06
Iter: 885 loss: 1.70535122e-06
Iter: 886 loss: 1.70465228e-06
Iter: 887 loss: 1.705817e-06
Iter: 888 loss: 1.70432907e-06
Iter: 889 loss: 1.70354849e-06
Iter: 890 loss: 1.70738622e-06
Iter: 891 loss: 1.70345277e-06
Iter: 892 loss: 1.70285034e-06
Iter: 893 loss: 1.70377336e-06
Iter: 894 loss: 1.70258591e-06
Iter: 895 loss: 1.70211092e-06
Iter: 896 loss: 1.70114913e-06
Iter: 897 loss: 1.72280738e-06
Iter: 898 loss: 1.70116073e-06
Iter: 899 loss: 1.70019916e-06
Iter: 900 loss: 1.71048316e-06
Iter: 901 loss: 1.70017984e-06
Iter: 902 loss: 1.69957082e-06
Iter: 903 loss: 1.70366275e-06
Iter: 904 loss: 1.69940256e-06
Iter: 905 loss: 1.69875238e-06
Iter: 906 loss: 1.69929785e-06
Iter: 907 loss: 1.69847897e-06
Iter: 908 loss: 1.69768305e-06
Iter: 909 loss: 1.69878604e-06
Iter: 910 loss: 1.6972599e-06
Iter: 911 loss: 1.69666146e-06
Iter: 912 loss: 1.69658347e-06
Iter: 913 loss: 1.6962515e-06
Iter: 914 loss: 1.69564419e-06
Iter: 915 loss: 1.7022287e-06
Iter: 916 loss: 1.69549492e-06
Iter: 917 loss: 1.69512521e-06
Iter: 918 loss: 1.69516898e-06
Iter: 919 loss: 1.69497559e-06
Iter: 920 loss: 1.6944133e-06
Iter: 921 loss: 1.6996828e-06
Iter: 922 loss: 1.69430621e-06
Iter: 923 loss: 1.69382713e-06
Iter: 924 loss: 1.69263251e-06
Iter: 925 loss: 1.70868122e-06
Iter: 926 loss: 1.69263262e-06
Iter: 927 loss: 1.69189684e-06
Iter: 928 loss: 1.69173859e-06
Iter: 929 loss: 1.69112514e-06
Iter: 930 loss: 1.69167458e-06
Iter: 931 loss: 1.69068676e-06
Iter: 932 loss: 1.6897344e-06
Iter: 933 loss: 1.68892336e-06
Iter: 934 loss: 1.68868712e-06
Iter: 935 loss: 1.68877443e-06
Iter: 936 loss: 1.68792883e-06
Iter: 937 loss: 1.6875656e-06
Iter: 938 loss: 1.68764586e-06
Iter: 939 loss: 1.68719316e-06
Iter: 940 loss: 1.68658937e-06
Iter: 941 loss: 1.68852512e-06
Iter: 942 loss: 1.68642771e-06
Iter: 943 loss: 1.68567624e-06
Iter: 944 loss: 1.68604322e-06
Iter: 945 loss: 1.68514066e-06
Iter: 946 loss: 1.6843403e-06
Iter: 947 loss: 1.68716838e-06
Iter: 948 loss: 1.68410406e-06
Iter: 949 loss: 1.68365068e-06
Iter: 950 loss: 1.68906172e-06
Iter: 951 loss: 1.6836575e-06
Iter: 952 loss: 1.682934e-06
Iter: 953 loss: 1.68302267e-06
Iter: 954 loss: 1.6824805e-06
Iter: 955 loss: 1.68201097e-06
Iter: 956 loss: 1.68074644e-06
Iter: 957 loss: 1.70028011e-06
Iter: 958 loss: 1.68077281e-06
Iter: 959 loss: 1.67992255e-06
Iter: 960 loss: 1.68009035e-06
Iter: 961 loss: 1.67944506e-06
Iter: 962 loss: 1.67873941e-06
Iter: 963 loss: 1.6786579e-06
Iter: 964 loss: 1.6775532e-06
Iter: 965 loss: 1.68175961e-06
Iter: 966 loss: 1.67726671e-06
Iter: 967 loss: 1.67644271e-06
Iter: 968 loss: 1.67649046e-06
Iter: 969 loss: 1.67592884e-06
Iter: 970 loss: 1.67656481e-06
Iter: 971 loss: 1.67570488e-06
Iter: 972 loss: 1.67495e-06
Iter: 973 loss: 1.67571011e-06
Iter: 974 loss: 1.67456847e-06
Iter: 975 loss: 1.67396331e-06
Iter: 976 loss: 1.67413259e-06
Iter: 977 loss: 1.6737356e-06
Iter: 978 loss: 1.67305302e-06
Iter: 979 loss: 1.68554288e-06
Iter: 980 loss: 1.67303631e-06
Iter: 981 loss: 1.67241728e-06
Iter: 982 loss: 1.68188126e-06
Iter: 983 loss: 1.6724814e-06
Iter: 984 loss: 1.67176108e-06
Iter: 985 loss: 1.67556675e-06
Iter: 986 loss: 1.67170515e-06
Iter: 987 loss: 1.67135659e-06
Iter: 988 loss: 1.67076246e-06
Iter: 989 loss: 1.68045153e-06
Iter: 990 loss: 1.67087728e-06
Iter: 991 loss: 1.6703e-06
Iter: 992 loss: 1.66981931e-06
Iter: 993 loss: 1.66974792e-06
Iter: 994 loss: 1.66862549e-06
Iter: 995 loss: 1.67486417e-06
Iter: 996 loss: 1.66841028e-06
Iter: 997 loss: 1.6676197e-06
Iter: 998 loss: 1.66887048e-06
Iter: 999 loss: 1.66725363e-06
Iter: 1000 loss: 1.66670532e-06
Iter: 1001 loss: 1.67157737e-06
Iter: 1002 loss: 1.66662653e-06
Iter: 1003 loss: 1.66614836e-06
Iter: 1004 loss: 1.66610391e-06
Iter: 1005 loss: 1.66559039e-06
Iter: 1006 loss: 1.66499194e-06
Iter: 1007 loss: 1.67021938e-06
Iter: 1008 loss: 1.66488758e-06
Iter: 1009 loss: 1.66433324e-06
Iter: 1010 loss: 1.66515429e-06
Iter: 1011 loss: 1.66394875e-06
Iter: 1012 loss: 1.66314771e-06
Iter: 1013 loss: 1.66623988e-06
Iter: 1014 loss: 1.66292727e-06
Iter: 1015 loss: 1.66243581e-06
Iter: 1016 loss: 1.6643944e-06
Iter: 1017 loss: 1.66226209e-06
Iter: 1018 loss: 1.66169298e-06
Iter: 1019 loss: 1.66841141e-06
Iter: 1020 loss: 1.6616425e-06
Iter: 1021 loss: 1.66141456e-06
Iter: 1022 loss: 1.66076643e-06
Iter: 1023 loss: 1.67097187e-06
Iter: 1024 loss: 1.66067889e-06
Iter: 1025 loss: 1.66021505e-06
Iter: 1026 loss: 1.65985216e-06
Iter: 1027 loss: 1.65975462e-06
Iter: 1028 loss: 1.65898712e-06
Iter: 1029 loss: 1.66420841e-06
Iter: 1030 loss: 1.65904021e-06
Iter: 1031 loss: 1.65844472e-06
Iter: 1032 loss: 1.65935228e-06
Iter: 1033 loss: 1.65833274e-06
Iter: 1034 loss: 1.65783126e-06
Iter: 1035 loss: 1.65876725e-06
Iter: 1036 loss: 1.65766846e-06
Iter: 1037 loss: 1.65709127e-06
Iter: 1038 loss: 1.65663766e-06
Iter: 1039 loss: 1.65645724e-06
Iter: 1040 loss: 1.65575113e-06
Iter: 1041 loss: 1.66202767e-06
Iter: 1042 loss: 1.65581696e-06
Iter: 1043 loss: 1.65531799e-06
Iter: 1044 loss: 1.65723361e-06
Iter: 1045 loss: 1.6552292e-06
Iter: 1046 loss: 1.65485608e-06
Iter: 1047 loss: 1.65450547e-06
Iter: 1048 loss: 1.65444351e-06
Iter: 1049 loss: 1.65373615e-06
Iter: 1050 loss: 1.65484175e-06
Iter: 1051 loss: 1.65339009e-06
Iter: 1052 loss: 1.65366691e-06
Iter: 1053 loss: 1.65310951e-06
Iter: 1054 loss: 1.65285905e-06
Iter: 1055 loss: 1.65233394e-06
Iter: 1056 loss: 1.65645565e-06
Iter: 1057 loss: 1.65235144e-06
Iter: 1058 loss: 1.65184724e-06
Iter: 1059 loss: 1.65181075e-06
Iter: 1060 loss: 1.65143138e-06
Iter: 1061 loss: 1.65074334e-06
Iter: 1062 loss: 1.65139136e-06
Iter: 1063 loss: 1.65020413e-06
Iter: 1064 loss: 1.64931816e-06
Iter: 1065 loss: 1.65702181e-06
Iter: 1066 loss: 1.64935432e-06
Iter: 1067 loss: 1.64877486e-06
Iter: 1068 loss: 1.65018798e-06
Iter: 1069 loss: 1.64852975e-06
Iter: 1070 loss: 1.64788867e-06
Iter: 1071 loss: 1.64860671e-06
Iter: 1072 loss: 1.64761275e-06
Iter: 1073 loss: 1.64691789e-06
Iter: 1074 loss: 1.65026859e-06
Iter: 1075 loss: 1.64679568e-06
Iter: 1076 loss: 1.64609639e-06
Iter: 1077 loss: 1.64768198e-06
Iter: 1078 loss: 1.64587163e-06
Iter: 1079 loss: 1.64549954e-06
Iter: 1080 loss: 1.64742664e-06
Iter: 1081 loss: 1.64547816e-06
Iter: 1082 loss: 1.64493213e-06
Iter: 1083 loss: 1.64551022e-06
Iter: 1084 loss: 1.64474386e-06
Iter: 1085 loss: 1.64447647e-06
Iter: 1086 loss: 1.64431265e-06
Iter: 1087 loss: 1.64433709e-06
Iter: 1088 loss: 1.64436472e-06
Iter: 1089 loss: 1.64426285e-06
Iter: 1090 loss: 1.64435619e-06
Iter: 1091 loss: 1.64426979e-06
Iter: 1092 loss: 1.64427809e-06
Iter: 1093 loss: 1.64431185e-06
Iter: 1094 loss: 1.64439132e-06
Iter: 1095 loss: 1.6442807e-06
Iter: 1096 loss: 1.64428525e-06
Iter: 1097 loss: 1.64430253e-06
Iter: 1098 loss: 1.64436165e-06
Iter: 1099 loss: 1.64435278e-06
Iter: 1100 loss: 1.64434755e-06
Iter: 1101 loss: 1.64433277e-06
Iter: 1102 loss: 1.64431117e-06
Iter: 1103 loss: 1.64430412e-06
Iter: 1104 loss: 1.64431981e-06
Iter: 1105 loss: 1.64431412e-06
Iter: 1106 loss: 1.64431401e-06
Iter: 1107 loss: 1.64431549e-06
Iter: 1108 loss: 1.64431481e-06
Iter: 1109 loss: 1.64431412e-06
Iter: 1110 loss: 1.64431412e-06
Iter: 1111 loss: 1.64431481e-06
Iter: 1112 loss: 1.64431492e-06
Iter: 1113 loss: 1.64431412e-06
Iter: 1114 loss: 1.64385153e-06
Iter: 1115 loss: 1.64845926e-06
Iter: 1116 loss: 1.64372398e-06
Iter: 1117 loss: 1.64354697e-06
Iter: 1118 loss: 1.6428379e-06
Iter: 1119 loss: 1.64981702e-06
Iter: 1120 loss: 1.64275991e-06
Iter: 1121 loss: 1.64220728e-06
Iter: 1122 loss: 1.64564358e-06
Iter: 1123 loss: 1.6420621e-06
Iter: 1124 loss: 1.6418e-06
Iter: 1125 loss: 1.64207677e-06
Iter: 1126 loss: 1.64146536e-06
Iter: 1127 loss: 1.64083917e-06
Iter: 1128 loss: 1.64301889e-06
Iter: 1129 loss: 1.64061146e-06
Iter: 1130 loss: 1.64014966e-06
Iter: 1131 loss: 1.64516359e-06
Iter: 1132 loss: 1.64022913e-06
Iter: 1133 loss: 1.63977961e-06
Iter: 1134 loss: 1.64191658e-06
Iter: 1135 loss: 1.63974937e-06
Iter: 1136 loss: 1.63950858e-06
Iter: 1137 loss: 1.63875779e-06
Iter: 1138 loss: 1.64459584e-06
Iter: 1139 loss: 1.63863069e-06
Iter: 1140 loss: 1.63806499e-06
Iter: 1141 loss: 1.63813e-06
Iter: 1142 loss: 1.63774962e-06
Iter: 1143 loss: 1.63814582e-06
Iter: 1144 loss: 1.63771529e-06
Iter: 1145 loss: 1.63732534e-06
Iter: 1146 loss: 1.63924142e-06
Iter: 1147 loss: 1.63721097e-06
Iter: 1148 loss: 1.63702884e-06
Iter: 1149 loss: 1.63735331e-06
Iter: 1150 loss: 1.63689901e-06
Iter: 1151 loss: 1.63672507e-06
Iter: 1152 loss: 1.63660036e-06
Iter: 1153 loss: 1.63644597e-06
Iter: 1154 loss: 1.63618881e-06
Iter: 1155 loss: 1.63632524e-06
Iter: 1156 loss: 1.6359478e-06
Iter: 1157 loss: 1.63554114e-06
Iter: 1158 loss: 1.63574623e-06
Iter: 1159 loss: 1.63526136e-06
Iter: 1160 loss: 1.63454411e-06
Iter: 1161 loss: 1.63752702e-06
Iter: 1162 loss: 1.63445532e-06
Iter: 1163 loss: 1.63416803e-06
Iter: 1164 loss: 1.63719324e-06
Iter: 1165 loss: 1.63418031e-06
Iter: 1166 loss: 1.63383629e-06
Iter: 1167 loss: 1.63477671e-06
Iter: 1168 loss: 1.63368395e-06
Iter: 1169 loss: 1.63335312e-06
Iter: 1170 loss: 1.63309539e-06
Iter: 1171 loss: 1.63298e-06
Iter: 1172 loss: 1.63246125e-06
Iter: 1173 loss: 1.63314974e-06
Iter: 1174 loss: 1.63223012e-06
Iter: 1175 loss: 1.63178299e-06
Iter: 1176 loss: 1.63176844e-06
Iter: 1177 loss: 1.63151503e-06
Iter: 1178 loss: 1.63159541e-06
Iter: 1179 loss: 1.63145285e-06
Iter: 1180 loss: 1.63115874e-06
Iter: 1181 loss: 1.63565392e-06
Iter: 1182 loss: 1.63119034e-06
Iter: 1183 loss: 1.63083041e-06
Iter: 1184 loss: 1.63098719e-06
Iter: 1185 loss: 1.63055097e-06
Iter: 1186 loss: 1.63027312e-06
Iter: 1187 loss: 1.63112077e-06
Iter: 1188 loss: 1.63013317e-06
Iter: 1189 loss: 1.62982565e-06
Iter: 1190 loss: 1.62947504e-06
Iter: 1191 loss: 1.62936158e-06
Iter: 1192 loss: 1.62890342e-06
Iter: 1193 loss: 1.63247194e-06
Iter: 1194 loss: 1.62884476e-06
Iter: 1195 loss: 1.6283e-06
Iter: 1196 loss: 1.63059315e-06
Iter: 1197 loss: 1.6283085e-06
Iter: 1198 loss: 1.6280685e-06
Iter: 1199 loss: 1.62986146e-06
Iter: 1200 loss: 1.62807862e-06
Iter: 1201 loss: 1.62776894e-06
Iter: 1202 loss: 1.6275244e-06
Iter: 1203 loss: 1.62747415e-06
Iter: 1204 loss: 1.62686865e-06
Iter: 1205 loss: 1.62713536e-06
Iter: 1206 loss: 1.62651725e-06
Iter: 1207 loss: 1.62589538e-06
Iter: 1208 loss: 1.62595029e-06
Iter: 1209 loss: 1.62575748e-06
Iter: 1210 loss: 1.62576771e-06
Iter: 1211 loss: 1.62555193e-06
Iter: 1212 loss: 1.62548542e-06
Iter: 1213 loss: 1.62535525e-06
Iter: 1214 loss: 1.6251123e-06
Iter: 1215 loss: 1.62540255e-06
Iter: 1216 loss: 1.62495871e-06
Iter: 1217 loss: 1.62464971e-06
Iter: 1218 loss: 1.62441484e-06
Iter: 1219 loss: 1.62442927e-06
Iter: 1220 loss: 1.62405172e-06
Iter: 1221 loss: 1.62485696e-06
Iter: 1222 loss: 1.62393189e-06
Iter: 1223 loss: 1.62329491e-06
Iter: 1224 loss: 1.6248207e-06
Iter: 1225 loss: 1.62315882e-06
Iter: 1226 loss: 1.62267008e-06
Iter: 1227 loss: 1.62531103e-06
Iter: 1228 loss: 1.62270453e-06
Iter: 1229 loss: 1.62234073e-06
Iter: 1230 loss: 1.62333356e-06
Iter: 1231 loss: 1.62215883e-06
Iter: 1232 loss: 1.62170863e-06
Iter: 1233 loss: 1.6224767e-06
Iter: 1234 loss: 1.62153219e-06
Iter: 1235 loss: 1.62114588e-06
Iter: 1236 loss: 1.62171318e-06
Iter: 1237 loss: 1.62086394e-06
Iter: 1238 loss: 1.62057e-06
Iter: 1239 loss: 1.62112349e-06
Iter: 1240 loss: 1.62026549e-06
Iter: 1241 loss: 1.62010815e-06
Iter: 1242 loss: 1.62000583e-06
Iter: 1243 loss: 1.61981905e-06
Iter: 1244 loss: 1.62052038e-06
Iter: 1245 loss: 1.6196866e-06
Iter: 1246 loss: 1.61940966e-06
Iter: 1247 loss: 1.61931166e-06
Iter: 1248 loss: 1.61922537e-06
Iter: 1249 loss: 1.61885271e-06
Iter: 1250 loss: 1.61929211e-06
Iter: 1251 loss: 1.61870867e-06
Iter: 1252 loss: 1.61841012e-06
Iter: 1253 loss: 1.61801267e-06
Iter: 1254 loss: 1.61789444e-06
Iter: 1255 loss: 1.61750961e-06
Iter: 1256 loss: 1.62055926e-06
Iter: 1257 loss: 1.61747835e-06
Iter: 1258 loss: 1.61698733e-06
Iter: 1259 loss: 1.61858725e-06
Iter: 1260 loss: 1.61698563e-06
Iter: 1261 loss: 1.61670118e-06
Iter: 1262 loss: 1.61846492e-06
Iter: 1263 loss: 1.61662558e-06
Iter: 1264 loss: 1.61629805e-06
Iter: 1265 loss: 1.6167794e-06
Iter: 1266 loss: 1.61608193e-06
Iter: 1267 loss: 1.61584319e-06
Iter: 1268 loss: 1.61613707e-06
Iter: 1269 loss: 1.61554919e-06
Iter: 1270 loss: 1.61517369e-06
Iter: 1271 loss: 1.61496382e-06
Iter: 1272 loss: 1.61483695e-06
Iter: 1273 loss: 1.61457103e-06
Iter: 1274 loss: 1.61437902e-06
Iter: 1275 loss: 1.61418166e-06
Iter: 1276 loss: 1.61706726e-06
Iter: 1277 loss: 1.61417256e-06
Iter: 1278 loss: 1.614045e-06
Iter: 1279 loss: 1.61378762e-06
Iter: 1280 loss: 1.61385037e-06
Iter: 1281 loss: 1.61347225e-06
Iter: 1282 loss: 1.61366552e-06
Iter: 1283 loss: 1.6132567e-06
Iter: 1284 loss: 1.61288699e-06
Iter: 1285 loss: 1.61311959e-06
Iter: 1286 loss: 1.61271817e-06
Iter: 1287 loss: 1.61234561e-06
Iter: 1288 loss: 1.61285959e-06
Iter: 1289 loss: 1.6122259e-06
Iter: 1290 loss: 1.61185903e-06
Iter: 1291 loss: 1.61325841e-06
Iter: 1292 loss: 1.61178832e-06
Iter: 1293 loss: 1.6113662e-06
Iter: 1294 loss: 1.61357161e-06
Iter: 1295 loss: 1.61128946e-06
Iter: 1296 loss: 1.6110771e-06
Iter: 1297 loss: 1.61195e-06
Iter: 1298 loss: 1.61097796e-06
Iter: 1299 loss: 1.61079424e-06
Iter: 1300 loss: 1.61084699e-06
Iter: 1301 loss: 1.61061723e-06
Iter: 1302 loss: 1.6102731e-06
Iter: 1303 loss: 1.61037065e-06
Iter: 1304 loss: 1.61012053e-06
Iter: 1305 loss: 1.60960269e-06
Iter: 1306 loss: 1.61013099e-06
Iter: 1307 loss: 1.60935326e-06
Iter: 1308 loss: 1.60936702e-06
Iter: 1309 loss: 1.60918739e-06
Iter: 1310 loss: 1.60902721e-06
Iter: 1311 loss: 1.60866819e-06
Iter: 1312 loss: 1.60870411e-06
Iter: 1313 loss: 1.60852301e-06
Iter: 1314 loss: 1.60950322e-06
Iter: 1315 loss: 1.60851584e-06
Iter: 1316 loss: 1.60828176e-06
Iter: 1317 loss: 1.6079855e-06
Iter: 1318 loss: 1.60799937e-06
Iter: 1319 loss: 1.6076192e-06
Iter: 1320 loss: 1.61015532e-06
Iter: 1321 loss: 1.60767104e-06
Iter: 1322 loss: 1.6073443e-06
Iter: 1323 loss: 1.6071474e-06
Iter: 1324 loss: 1.60714058e-06
Iter: 1325 loss: 1.60677064e-06
Iter: 1326 loss: 1.60775608e-06
Iter: 1327 loss: 1.60665866e-06
Iter: 1328 loss: 1.6064605e-06
Iter: 1329 loss: 1.60976629e-06
Iter: 1330 loss: 1.60647153e-06
Iter: 1331 loss: 1.60628349e-06
Iter: 1332 loss: 1.60613149e-06
Iter: 1333 loss: 1.60604236e-06
Iter: 1334 loss: 1.60577906e-06
Iter: 1335 loss: 1.60701882e-06
Iter: 1336 loss: 1.60565423e-06
Iter: 1337 loss: 1.60552804e-06
Iter: 1338 loss: 1.60526565e-06
Iter: 1339 loss: 1.60531476e-06
Iter: 1340 loss: 1.60517834e-06
Iter: 1341 loss: 1.60502213e-06
Iter: 1342 loss: 1.60484524e-06
Iter: 1343 loss: 1.60546529e-06
Iter: 1344 loss: 1.60475167e-06
Iter: 1345 loss: 1.60467675e-06
Iter: 1346 loss: 1.60479976e-06
Iter: 1347 loss: 1.60456761e-06
Iter: 1348 loss: 1.60447405e-06
Iter: 1349 loss: 1.60453692e-06
Iter: 1350 loss: 1.60441664e-06
Iter: 1351 loss: 1.60412696e-06
Iter: 1352 loss: 1.60449076e-06
Iter: 1353 loss: 1.60399873e-06
Iter: 1354 loss: 1.60373952e-06
Iter: 1355 loss: 1.60360059e-06
Iter: 1356 loss: 1.60350328e-06
Iter: 1357 loss: 1.60310174e-06
Iter: 1358 loss: 1.60431944e-06
Iter: 1359 loss: 1.60298009e-06
Iter: 1360 loss: 1.60266882e-06
Iter: 1361 loss: 1.60453533e-06
Iter: 1362 loss: 1.60272384e-06
Iter: 1363 loss: 1.6023148e-06
Iter: 1364 loss: 1.60258492e-06
Iter: 1365 loss: 1.60212494e-06
Iter: 1366 loss: 1.60186369e-06
Iter: 1367 loss: 1.60370644e-06
Iter: 1368 loss: 1.60171408e-06
Iter: 1369 loss: 1.60147192e-06
Iter: 1370 loss: 1.60113609e-06
Iter: 1371 loss: 1.60118543e-06
Iter: 1372 loss: 1.60087507e-06
Iter: 1373 loss: 1.60083027e-06
Iter: 1374 loss: 1.60081822e-06
Iter: 1375 loss: 1.60231718e-06
Iter: 1376 loss: 1.60075774e-06
Iter: 1377 loss: 1.60063496e-06
Iter: 1378 loss: 1.60042191e-06
Iter: 1379 loss: 1.60362481e-06
Iter: 1380 loss: 1.60039872e-06
Iter: 1381 loss: 1.60019749e-06
Iter: 1382 loss: 1.60158072e-06
Iter: 1383 loss: 1.60035415e-06
Iter: 1384 loss: 1.60014235e-06
Iter: 1385 loss: 1.59984427e-06
Iter: 1386 loss: 1.60641071e-06
Iter: 1387 loss: 1.59982551e-06
Iter: 1388 loss: 1.59955982e-06
Iter: 1389 loss: 1.60002514e-06
Iter: 1390 loss: 1.59948081e-06
Iter: 1391 loss: 1.5990438e-06
Iter: 1392 loss: 1.60133459e-06
Iter: 1393 loss: 1.59900651e-06
Iter: 1394 loss: 1.59875776e-06
Iter: 1395 loss: 1.59929368e-06
Iter: 1396 loss: 1.59868659e-06
Iter: 1397 loss: 1.5986127e-06
Iter: 1398 loss: 1.59893921e-06
Iter: 1399 loss: 1.59843751e-06
Iter: 1400 loss: 1.59825822e-06
Iter: 1401 loss: 1.59859599e-06
Iter: 1402 loss: 1.5981542e-06
Iter: 1403 loss: 1.59797332e-06
Iter: 1404 loss: 1.59815193e-06
Iter: 1405 loss: 1.59791944e-06
Iter: 1406 loss: 1.59768774e-06
Iter: 1407 loss: 1.59884007e-06
Iter: 1408 loss: 1.59765159e-06
Iter: 1409 loss: 1.59746037e-06
Iter: 1410 loss: 1.59755177e-06
Iter: 1411 loss: 1.59735771e-06
Iter: 1412 loss: 1.59721696e-06
Iter: 1413 loss: 1.5997075e-06
Iter: 1414 loss: 1.59707349e-06
Iter: 1415 loss: 1.59682304e-06
Iter: 1416 loss: 1.59907734e-06
Iter: 1417 loss: 1.59682145e-06
Iter: 1418 loss: 1.59663159e-06
Iter: 1419 loss: 1.59644503e-06
Iter: 1420 loss: 1.59649471e-06
Iter: 1421 loss: 1.59604235e-06
Iter: 1422 loss: 1.59658623e-06
Iter: 1423 loss: 1.59589479e-06
Iter: 1424 loss: 1.59560705e-06
Iter: 1425 loss: 1.59753199e-06
Iter: 1426 loss: 1.59547631e-06
Iter: 1427 loss: 1.59526587e-06
Iter: 1428 loss: 1.59542992e-06
Iter: 1429 loss: 1.5951731e-06
Iter: 1430 loss: 1.5949488e-06
Iter: 1431 loss: 1.59773924e-06
Iter: 1432 loss: 1.59487104e-06
Iter: 1433 loss: 1.59467731e-06
Iter: 1434 loss: 1.59502929e-06
Iter: 1435 loss: 1.59461069e-06
Iter: 1436 loss: 1.5942386e-06
Iter: 1437 loss: 1.59612432e-06
Iter: 1438 loss: 1.59420608e-06
Iter: 1439 loss: 1.5940866e-06
Iter: 1440 loss: 1.59385809e-06
Iter: 1441 loss: 1.59395415e-06
Iter: 1442 loss: 1.59392573e-06
Iter: 1443 loss: 1.59383785e-06
Iter: 1444 loss: 1.59355568e-06
Iter: 1445 loss: 1.59341369e-06
Iter: 1446 loss: 1.59754779e-06
Iter: 1447 loss: 1.59338379e-06
Iter: 1448 loss: 1.59313311e-06
Iter: 1449 loss: 1.59477781e-06
Iter: 1450 loss: 1.59316278e-06
Iter: 1451 loss: 1.59292335e-06
Iter: 1452 loss: 1.59344302e-06
Iter: 1453 loss: 1.59286981e-06
Iter: 1454 loss: 1.59277988e-06
Iter: 1455 loss: 1.59256297e-06
Iter: 1456 loss: 1.59255126e-06
Iter: 1457 loss: 1.59222986e-06
Iter: 1458 loss: 1.59301e-06
Iter: 1459 loss: 1.59220372e-06
Iter: 1460 loss: 1.59187971e-06
Iter: 1461 loss: 1.59248918e-06
Iter: 1462 loss: 1.59183151e-06
Iter: 1463 loss: 1.59153069e-06
Iter: 1464 loss: 1.59144361e-06
Iter: 1465 loss: 1.59127626e-06
Iter: 1466 loss: 1.59086039e-06
Iter: 1467 loss: 1.59227716e-06
Iter: 1468 loss: 1.59068372e-06
Iter: 1469 loss: 1.59038723e-06
Iter: 1470 loss: 1.59173442e-06
Iter: 1471 loss: 1.59024853e-06
Iter: 1472 loss: 1.58990042e-06
Iter: 1473 loss: 1.59252863e-06
Iter: 1474 loss: 1.58985154e-06
Iter: 1475 loss: 1.58970363e-06
Iter: 1476 loss: 1.58963394e-06
Iter: 1477 loss: 1.58961348e-06
Iter: 1478 loss: 1.58924445e-06
Iter: 1479 loss: 1.58926844e-06
Iter: 1480 loss: 1.58922853e-06
Iter: 1481 loss: 1.58901139e-06
Iter: 1482 loss: 1.59118747e-06
Iter: 1483 loss: 1.58895591e-06
Iter: 1484 loss: 1.58879402e-06
Iter: 1485 loss: 1.58874332e-06
Iter: 1486 loss: 1.58859598e-06
Iter: 1487 loss: 1.58822286e-06
Iter: 1488 loss: 1.58832927e-06
Iter: 1489 loss: 1.5878511e-06
Iter: 1490 loss: 1.58835223e-06
Iter: 1491 loss: 1.58767989e-06
Iter: 1492 loss: 1.58726925e-06
Iter: 1493 loss: 1.58934415e-06
Iter: 1494 loss: 1.58717853e-06
Iter: 1495 loss: 1.58689295e-06
Iter: 1496 loss: 1.58694093e-06
Iter: 1497 loss: 1.58664477e-06
Iter: 1498 loss: 1.58636885e-06
Iter: 1499 loss: 1.58783132e-06
Iter: 1500 loss: 1.58622925e-06
Iter: 1501 loss: 1.5860146e-06
Iter: 1502 loss: 1.58614353e-06
Iter: 1503 loss: 1.58577052e-06
Iter: 1504 loss: 1.58541877e-06
Iter: 1505 loss: 1.58533055e-06
Iter: 1506 loss: 1.58509124e-06
Iter: 1507 loss: 1.58477292e-06
Iter: 1508 loss: 1.58847342e-06
Iter: 1509 loss: 1.58476848e-06
Iter: 1510 loss: 1.584649e-06
Iter: 1511 loss: 1.58456874e-06
Iter: 1512 loss: 1.58440491e-06
Iter: 1513 loss: 1.58413638e-06
Iter: 1514 loss: 1.58427247e-06
Iter: 1515 loss: 1.58405794e-06
Iter: 1516 loss: 1.58405055e-06
Iter: 1517 loss: 1.58386217e-06
Iter: 1518 loss: 1.58349235e-06
Iter: 1519 loss: 1.58396597e-06
Iter: 1520 loss: 1.58337207e-06
Iter: 1521 loss: 1.58311582e-06
Iter: 1522 loss: 1.58285525e-06
Iter: 1523 loss: 1.58276862e-06
Iter: 1524 loss: 1.58246917e-06
Iter: 1525 loss: 1.58701368e-06
Iter: 1526 loss: 1.58248918e-06
Iter: 1527 loss: 1.58221451e-06
Iter: 1528 loss: 1.58215801e-06
Iter: 1529 loss: 1.58191187e-06
Iter: 1530 loss: 1.5814835e-06
Iter: 1531 loss: 1.58231364e-06
Iter: 1532 loss: 1.58134503e-06
Iter: 1533 loss: 1.58090336e-06
Iter: 1534 loss: 1.58464195e-06
Iter: 1535 loss: 1.58086459e-06
Iter: 1536 loss: 1.58039188e-06
Iter: 1537 loss: 1.5805706e-06
Iter: 1538 loss: 1.58011869e-06
Iter: 1539 loss: 1.57985608e-06
Iter: 1540 loss: 1.58355408e-06
Iter: 1541 loss: 1.57982663e-06
Iter: 1542 loss: 1.57966235e-06
Iter: 1543 loss: 1.58001285e-06
Iter: 1544 loss: 1.57947647e-06
Iter: 1545 loss: 1.57926536e-06
Iter: 1546 loss: 1.581064e-06
Iter: 1547 loss: 1.57914519e-06
Iter: 1548 loss: 1.57906175e-06
Iter: 1549 loss: 1.57881618e-06
Iter: 1550 loss: 1.58129728e-06
Iter: 1551 loss: 1.5787042e-06
Iter: 1552 loss: 1.57857187e-06
Iter: 1553 loss: 1.57843203e-06
Iter: 1554 loss: 1.57817021e-06
Iter: 1555 loss: 1.57784916e-06
Iter: 1556 loss: 1.58497028e-06
Iter: 1557 loss: 1.57775924e-06
Iter: 1558 loss: 1.57749753e-06
Iter: 1559 loss: 1.58088119e-06
Iter: 1560 loss: 1.57737713e-06
Iter: 1561 loss: 1.57714635e-06
Iter: 1562 loss: 1.57700265e-06
Iter: 1563 loss: 1.57677755e-06
Iter: 1564 loss: 1.57640227e-06
Iter: 1565 loss: 1.57656928e-06
Iter: 1566 loss: 1.57607724e-06
Iter: 1567 loss: 1.57568775e-06
Iter: 1568 loss: 1.57775889e-06
Iter: 1569 loss: 1.5755719e-06
Iter: 1570 loss: 1.57511e-06
Iter: 1571 loss: 1.5769142e-06
Iter: 1572 loss: 1.57497209e-06
Iter: 1573 loss: 1.57447312e-06
Iter: 1574 loss: 1.57708268e-06
Iter: 1575 loss: 1.57446698e-06
Iter: 1576 loss: 1.57414365e-06
Iter: 1577 loss: 1.57631655e-06
Iter: 1578 loss: 1.57414229e-06
Iter: 1579 loss: 1.57390969e-06
Iter: 1580 loss: 1.57391696e-06
Iter: 1581 loss: 1.57375803e-06
Iter: 1582 loss: 1.57340526e-06
Iter: 1583 loss: 1.57523164e-06
Iter: 1584 loss: 1.57317152e-06
Iter: 1585 loss: 1.57278316e-06
Iter: 1586 loss: 1.57291493e-06
Iter: 1587 loss: 1.57239913e-06
Iter: 1588 loss: 1.57207387e-06
Iter: 1589 loss: 1.57201271e-06
Iter: 1590 loss: 1.57174236e-06
Iter: 1591 loss: 1.57109707e-06
Iter: 1592 loss: 1.57115358e-06
Iter: 1593 loss: 1.57067689e-06
Iter: 1594 loss: 1.57580303e-06
Iter: 1595 loss: 1.57061356e-06
Iter: 1596 loss: 1.57019065e-06
Iter: 1597 loss: 1.56994224e-06
Iter: 1598 loss: 1.56988199e-06
Iter: 1599 loss: 1.56937153e-06
Iter: 1600 loss: 1.57040984e-06
Iter: 1601 loss: 1.56922397e-06
Iter: 1602 loss: 1.56892952e-06
Iter: 1603 loss: 1.56967508e-06
Iter: 1604 loss: 1.56872454e-06
Iter: 1605 loss: 1.56826184e-06
Iter: 1606 loss: 1.57163709e-06
Iter: 1607 loss: 1.56819169e-06
Iter: 1608 loss: 1.56787155e-06
Iter: 1609 loss: 1.56861643e-06
Iter: 1610 loss: 1.5676801e-06
Iter: 1611 loss: 1.56738452e-06
Iter: 1612 loss: 1.56739702e-06
Iter: 1613 loss: 1.5670696e-06
Iter: 1614 loss: 1.56752412e-06
Iter: 1615 loss: 1.56688452e-06
Iter: 1616 loss: 1.56678834e-06
Iter: 1617 loss: 1.56636656e-06
Iter: 1618 loss: 1.56722308e-06
Iter: 1619 loss: 1.56606336e-06
Iter: 1620 loss: 1.56561873e-06
Iter: 1621 loss: 1.56691499e-06
Iter: 1622 loss: 1.56533929e-06
Iter: 1623 loss: 1.56504439e-06
Iter: 1624 loss: 1.56944543e-06
Iter: 1625 loss: 1.56511817e-06
Iter: 1626 loss: 1.56470185e-06
Iter: 1627 loss: 1.56497163e-06
Iter: 1628 loss: 1.56454689e-06
Iter: 1629 loss: 1.56402245e-06
Iter: 1630 loss: 1.56704573e-06
Iter: 1631 loss: 1.5641732e-06
Iter: 1632 loss: 1.56382612e-06
Iter: 1633 loss: 1.56393014e-06
Iter: 1634 loss: 1.5635419e-06
Iter: 1635 loss: 1.56344709e-06
Iter: 1636 loss: 1.56343867e-06
Iter: 1637 loss: 1.5632005e-06
Iter: 1638 loss: 1.56291549e-06
Iter: 1639 loss: 1.5637961e-06
Iter: 1640 loss: 1.56281487e-06
Iter: 1641 loss: 1.56246608e-06
Iter: 1642 loss: 1.56327462e-06
Iter: 1643 loss: 1.56236729e-06
Iter: 1644 loss: 1.56202941e-06
Iter: 1645 loss: 1.56301678e-06
Iter: 1646 loss: 1.56212286e-06
Iter: 1647 loss: 1.56182773e-06
Iter: 1648 loss: 1.56239923e-06
Iter: 1649 loss: 1.56184592e-06
Iter: 1650 loss: 1.56141709e-06
Iter: 1651 loss: 1.56301076e-06
Iter: 1652 loss: 1.56145438e-06
Iter: 1653 loss: 1.56121359e-06
Iter: 1654 loss: 1.56089891e-06
Iter: 1655 loss: 1.56391889e-06
Iter: 1656 loss: 1.56082251e-06
Iter: 1657 loss: 1.56040596e-06
Iter: 1658 loss: 1.56283681e-06
Iter: 1659 loss: 1.56036481e-06
Iter: 1660 loss: 1.56005524e-06
Iter: 1661 loss: 1.56078613e-06
Iter: 1662 loss: 1.55985151e-06
Iter: 1663 loss: 1.55962209e-06
Iter: 1664 loss: 1.55963471e-06
Iter: 1665 loss: 1.55933617e-06
Iter: 1666 loss: 1.56014096e-06
Iter: 1667 loss: 1.55935209e-06
Iter: 1668 loss: 1.55924022e-06
Iter: 1669 loss: 1.5589826e-06
Iter: 1670 loss: 1.55900443e-06
Iter: 1671 loss: 1.55876864e-06
Iter: 1672 loss: 1.55968439e-06
Iter: 1673 loss: 1.55865143e-06
Iter: 1674 loss: 1.55845862e-06
Iter: 1675 loss: 1.55970724e-06
Iter: 1676 loss: 1.55838018e-06
Iter: 1677 loss: 1.55830287e-06
Iter: 1678 loss: 1.55807538e-06
Iter: 1679 loss: 1.5580672e-06
Iter: 1680 loss: 1.55811699e-06
Iter: 1681 loss: 1.55799455e-06
Iter: 1682 loss: 1.55789098e-06
Iter: 1683 loss: 1.55815985e-06
Iter: 1684 loss: 1.55795453e-06
Iter: 1685 loss: 1.55781231e-06
Iter: 1686 loss: 1.5575722e-06
Iter: 1687 loss: 1.56075407e-06
Iter: 1688 loss: 1.55760881e-06
Iter: 1689 loss: 1.55736234e-06
Iter: 1690 loss: 1.55736711e-06
Iter: 1691 loss: 1.5571668e-06
Iter: 1692 loss: 1.55682869e-06
Iter: 1693 loss: 1.5600358e-06
Iter: 1694 loss: 1.5568753e-06
Iter: 1695 loss: 1.55668636e-06
Iter: 1696 loss: 1.55755811e-06
Iter: 1697 loss: 1.55653743e-06
Iter: 1698 loss: 1.55631972e-06
Iter: 1699 loss: 1.55632711e-06
Iter: 1700 loss: 1.55629914e-06
Iter: 1701 loss: 1.5560239e-06
Iter: 1702 loss: 1.55926409e-06
Iter: 1703 loss: 1.5559234e-06
Iter: 1704 loss: 1.55567466e-06
Iter: 1705 loss: 1.55880559e-06
Iter: 1706 loss: 1.55570297e-06
Iter: 1707 loss: 1.55551334e-06
Iter: 1708 loss: 1.55544853e-06
Iter: 1709 loss: 1.55535815e-06
Iter: 1710 loss: 1.55514329e-06
Iter: 1711 loss: 1.55650355e-06
Iter: 1712 loss: 1.55504745e-06
Iter: 1713 loss: 1.55482076e-06
Iter: 1714 loss: 1.55603311e-06
Iter: 1715 loss: 1.55477403e-06
Iter: 1716 loss: 1.55456928e-06
Iter: 1717 loss: 1.55459543e-06
Iter: 1718 loss: 1.55436146e-06
Iter: 1719 loss: 1.55450789e-06
Iter: 1720 loss: 1.55428779e-06
Iter: 1721 loss: 1.55423618e-06
Iter: 1722 loss: 1.55407884e-06
Iter: 1723 loss: 1.55467751e-06
Iter: 1724 loss: 1.55388852e-06
Iter: 1725 loss: 1.55377302e-06
Iter: 1726 loss: 1.55364569e-06
Iter: 1727 loss: 1.55344935e-06
Iter: 1728 loss: 1.5531914e-06
Iter: 1729 loss: 1.5534265e-06
Iter: 1730 loss: 1.55308339e-06
Iter: 1731 loss: 1.55278531e-06
Iter: 1732 loss: 1.55464e-06
Iter: 1733 loss: 1.55277917e-06
Iter: 1734 loss: 1.55243026e-06
Iter: 1735 loss: 1.55377097e-06
Iter: 1736 loss: 1.55250189e-06
Iter: 1737 loss: 1.5522487e-06
Iter: 1738 loss: 1.55282896e-06
Iter: 1739 loss: 1.55212524e-06
Iter: 1740 loss: 1.55192936e-06
Iter: 1741 loss: 1.55168652e-06
Iter: 1742 loss: 1.55157977e-06
Iter: 1743 loss: 1.551183e-06
Iter: 1744 loss: 1.55467546e-06
Iter: 1745 loss: 1.55111024e-06
Iter: 1746 loss: 1.55075236e-06
Iter: 1747 loss: 1.5523525e-06
Iter: 1748 loss: 1.55071575e-06
Iter: 1749 loss: 1.55039788e-06
Iter: 1750 loss: 1.55120517e-06
Iter: 1751 loss: 1.55050634e-06
Iter: 1752 loss: 1.55013436e-06
Iter: 1753 loss: 1.55028397e-06
Iter: 1754 loss: 1.55011912e-06
Iter: 1755 loss: 1.55036764e-06
Iter: 1756 loss: 1.54997645e-06
Iter: 1757 loss: 1.54996144e-06
Iter: 1758 loss: 1.54984355e-06
Iter: 1759 loss: 1.54979466e-06
Iter: 1760 loss: 1.5498124e-06
Iter: 1761 loss: 1.5493888e-06
Iter: 1762 loss: 1.54916415e-06
Iter: 1763 loss: 1.5490325e-06
Iter: 1764 loss: 1.54859595e-06
Iter: 1765 loss: 1.55053181e-06
Iter: 1766 loss: 1.54863471e-06
Iter: 1767 loss: 1.54829399e-06
Iter: 1768 loss: 1.55015198e-06
Iter: 1769 loss: 1.54829809e-06
Iter: 1770 loss: 1.54791462e-06
Iter: 1771 loss: 1.54900636e-06
Iter: 1772 loss: 1.5478364e-06
Iter: 1773 loss: 1.54765144e-06
Iter: 1774 loss: 1.54740133e-06
Iter: 1775 loss: 1.547284e-06
Iter: 1776 loss: 1.54691338e-06
Iter: 1777 loss: 1.55052703e-06
Iter: 1778 loss: 1.54681186e-06
Iter: 1779 loss: 1.5465821e-06
Iter: 1780 loss: 1.54670943e-06
Iter: 1781 loss: 1.54647114e-06
Iter: 1782 loss: 1.5461078e-06
Iter: 1783 loss: 1.54628265e-06
Iter: 1784 loss: 1.54597944e-06
Iter: 1785 loss: 1.54580334e-06
Iter: 1786 loss: 1.54577128e-06
Iter: 1787 loss: 1.54566806e-06
Iter: 1788 loss: 1.54665713e-06
Iter: 1789 loss: 1.5456643e-06
Iter: 1790 loss: 1.54547683e-06
Iter: 1791 loss: 1.54577765e-06
Iter: 1792 loss: 1.54517534e-06
Iter: 1793 loss: 1.54510894e-06
Iter: 1794 loss: 1.54494933e-06
Iter: 1795 loss: 1.54494683e-06
Iter: 1796 loss: 1.5447074e-06
Iter: 1797 loss: 1.54492159e-06
Iter: 1798 loss: 1.54448958e-06
Iter: 1799 loss: 1.54413965e-06
Iter: 1800 loss: 1.5443984e-06
Iter: 1801 loss: 1.54390636e-06
Iter: 1802 loss: 1.54346731e-06
Iter: 1803 loss: 1.54348572e-06
Iter: 1804 loss: 1.54319423e-06
Iter: 1805 loss: 1.54423424e-06
Iter: 1806 loss: 1.54302791e-06
Iter: 1807 loss: 1.54293514e-06
Iter: 1808 loss: 1.54278428e-06
Iter: 1809 loss: 1.54268059e-06
Iter: 1810 loss: 1.54235545e-06
Iter: 1811 loss: 1.5436151e-06
Iter: 1812 loss: 1.54221425e-06
Iter: 1813 loss: 1.54188569e-06
Iter: 1814 loss: 1.54235636e-06
Iter: 1815 loss: 1.5417495e-06
Iter: 1816 loss: 1.54137354e-06
Iter: 1817 loss: 1.54277086e-06
Iter: 1818 loss: 1.54145982e-06
Iter: 1819 loss: 1.54117879e-06
Iter: 1820 loss: 1.54294139e-06
Iter: 1821 loss: 1.54115196e-06
Iter: 1822 loss: 1.54101667e-06
Iter: 1823 loss: 1.54303621e-06
Iter: 1824 loss: 1.54109057e-06
Iter: 1825 loss: 1.54100348e-06
Iter: 1826 loss: 1.54066765e-06
Iter: 1827 loss: 1.54371969e-06
Iter: 1828 loss: 1.54069642e-06
Iter: 1829 loss: 1.54039378e-06
Iter: 1830 loss: 1.54061e-06
Iter: 1831 loss: 1.54032114e-06
Iter: 1832 loss: 1.54007887e-06
Iter: 1833 loss: 1.54047007e-06
Iter: 1834 loss: 1.53982126e-06
Iter: 1835 loss: 1.53958194e-06
Iter: 1836 loss: 1.53976907e-06
Iter: 1837 loss: 1.53941983e-06
Iter: 1838 loss: 1.53906626e-06
Iter: 1839 loss: 1.5423775e-06
Iter: 1840 loss: 1.53908354e-06
Iter: 1841 loss: 1.53874589e-06
Iter: 1842 loss: 1.53944563e-06
Iter: 1843 loss: 1.5387036e-06
Iter: 1844 loss: 1.5383248e-06
Iter: 1845 loss: 1.53832491e-06
Iter: 1846 loss: 1.53815517e-06
Iter: 1847 loss: 1.53776648e-06
Iter: 1848 loss: 1.53876465e-06
Iter: 1849 loss: 1.53766155e-06
Iter: 1850 loss: 1.53748749e-06
Iter: 1851 loss: 1.53752012e-06
Iter: 1852 loss: 1.53725978e-06
Iter: 1853 loss: 1.53697056e-06
Iter: 1854 loss: 1.53956921e-06
Iter: 1855 loss: 1.53706594e-06
Iter: 1856 loss: 1.53694396e-06
Iter: 1857 loss: 1.53681265e-06
Iter: 1858 loss: 1.53671158e-06
Iter: 1859 loss: 1.53665326e-06
Iter: 1860 loss: 1.53666747e-06
Iter: 1861 loss: 1.53659335e-06
Iter: 1862 loss: 1.53632743e-06
Iter: 1863 loss: 1.53638814e-06
Iter: 1864 loss: 1.53600206e-06
Iter: 1865 loss: 1.53667725e-06
Iter: 1866 loss: 1.5359401e-06
Iter: 1867 loss: 1.53572796e-06
Iter: 1868 loss: 1.53691474e-06
Iter: 1869 loss: 1.53567623e-06
Iter: 1870 loss: 1.53535154e-06
Iter: 1871 loss: 1.53642395e-06
Iter: 1872 loss: 1.53537121e-06
Iter: 1873 loss: 1.53516589e-06
Iter: 1874 loss: 1.53625774e-06
Iter: 1875 loss: 1.53522569e-06
Iter: 1876 loss: 1.53511e-06
Iter: 1877 loss: 1.53454107e-06
Iter: 1878 loss: 1.54045733e-06
Iter: 1879 loss: 1.53454982e-06
Iter: 1880 loss: 1.53430506e-06
Iter: 1881 loss: 1.53431085e-06
Iter: 1882 loss: 1.53409724e-06
Iter: 1883 loss: 1.53405256e-06
Iter: 1884 loss: 1.53392421e-06
Iter: 1885 loss: 1.53357018e-06
Iter: 1886 loss: 1.53322253e-06
Iter: 1887 loss: 1.53324231e-06
Iter: 1888 loss: 1.53370547e-06
Iter: 1889 loss: 1.53319047e-06
Iter: 1890 loss: 1.53307678e-06
Iter: 1891 loss: 1.53304518e-06
Iter: 1892 loss: 1.53298583e-06
Iter: 1893 loss: 1.53275755e-06
Iter: 1894 loss: 1.53282588e-06
Iter: 1895 loss: 1.53273731e-06
Iter: 1896 loss: 1.53249778e-06
Iter: 1897 loss: 1.53221993e-06
Iter: 1898 loss: 1.53221117e-06
Iter: 1899 loss: 1.53189058e-06
Iter: 1900 loss: 1.53394558e-06
Iter: 1901 loss: 1.53186306e-06
Iter: 1902 loss: 1.53157714e-06
Iter: 1903 loss: 1.53201654e-06
Iter: 1904 loss: 1.53144515e-06
Iter: 1905 loss: 1.53123494e-06
Iter: 1906 loss: 1.53230462e-06
Iter: 1907 loss: 1.53108635e-06
Iter: 1908 loss: 1.53081407e-06
Iter: 1909 loss: 1.53224232e-06
Iter: 1910 loss: 1.53071983e-06
Iter: 1911 loss: 1.5306116e-06
Iter: 1912 loss: 1.53216047e-06
Iter: 1913 loss: 1.53053861e-06
Iter: 1914 loss: 1.53045926e-06
Iter: 1915 loss: 1.53036603e-06
Iter: 1916 loss: 1.53028009e-06
Iter: 1917 loss: 1.5299845e-06
Iter: 1918 loss: 1.53098154e-06
Iter: 1919 loss: 1.52992322e-06
Iter: 1920 loss: 1.5298308e-06
Iter: 1921 loss: 1.53029919e-06
Iter: 1922 loss: 1.52965845e-06
Iter: 1923 loss: 1.52961582e-06
Iter: 1924 loss: 1.52949065e-06
Iter: 1925 loss: 1.52946609e-06
Iter: 1926 loss: 1.52933512e-06
Iter: 1927 loss: 1.53194037e-06
Iter: 1928 loss: 1.52930352e-06
Iter: 1929 loss: 1.52918335e-06
Iter: 1930 loss: 1.5291771e-06
Iter: 1931 loss: 1.52892517e-06
Iter: 1932 loss: 1.52869461e-06
Iter: 1933 loss: 1.52892085e-06
Iter: 1934 loss: 1.52861764e-06
Iter: 1935 loss: 1.52832672e-06
Iter: 1936 loss: 1.52889629e-06
Iter: 1937 loss: 1.5282186e-06
Iter: 1938 loss: 1.52797315e-06
Iter: 1939 loss: 1.52947064e-06
Iter: 1940 loss: 1.52790903e-06
Iter: 1941 loss: 1.52771645e-06
Iter: 1942 loss: 1.52916869e-06
Iter: 1943 loss: 1.52767291e-06
Iter: 1944 loss: 1.5274511e-06
Iter: 1945 loss: 1.52815278e-06
Iter: 1946 loss: 1.52738403e-06
Iter: 1947 loss: 1.5270532e-06
Iter: 1948 loss: 1.52746748e-06
Iter: 1949 loss: 1.52711573e-06
Iter: 1950 loss: 1.52678149e-06
Iter: 1951 loss: 1.52684834e-06
Iter: 1952 loss: 1.5266445e-06
Iter: 1953 loss: 1.52628e-06
Iter: 1954 loss: 1.52964571e-06
Iter: 1955 loss: 1.52631242e-06
Iter: 1956 loss: 1.52626853e-06
Iter: 1957 loss: 1.52619305e-06
Iter: 1958 loss: 1.52620282e-06
Iter: 1959 loss: 1.52624591e-06
Iter: 1960 loss: 1.52619009e-06
Iter: 1961 loss: 1.52615769e-06
Iter: 1962 loss: 1.52617e-06
Iter: 1963 loss: 1.52625421e-06
Iter: 1964 loss: 1.52619759e-06
Iter: 1965 loss: 1.52621635e-06
Iter: 1966 loss: 1.52616553e-06
Iter: 1967 loss: 1.52619873e-06
Iter: 1968 loss: 1.52618509e-06
Iter: 1969 loss: 1.52617861e-06
Iter: 1970 loss: 1.52623875e-06
Iter: 1971 loss: 1.52620032e-06
Iter: 1972 loss: 1.526186e-06
Iter: 1973 loss: 1.52619566e-06
Iter: 1974 loss: 1.52618406e-06
Iter: 1975 loss: 1.52619032e-06
Iter: 1976 loss: 1.52619509e-06
Iter: 1977 loss: 1.52619464e-06
Iter: 1978 loss: 1.52619418e-06
Iter: 1979 loss: 1.52619418e-06
Iter: 1980 loss: 1.52619418e-06
Iter: 1981 loss: 1.52619418e-06
Iter: 1982 loss: 1.52619464e-06
Iter: 1983 loss: 1.52619418e-06
Iter: 1984 loss: 1.52619464e-06
Iter: 1985 loss: 1.52559755e-06
Iter: 1986 loss: 1.53062979e-06
Iter: 1987 loss: 1.52556663e-06
Iter: 1988 loss: 1.52529253e-06
Iter: 1989 loss: 1.52510324e-06
Iter: 1990 loss: 1.52491805e-06
Iter: 1991 loss: 1.52451469e-06
Iter: 1992 loss: 1.5251976e-06
Iter: 1993 loss: 1.52431335e-06
Iter: 1994 loss: 1.52403481e-06
Iter: 1995 loss: 1.52521602e-06
Iter: 1996 loss: 1.52384894e-06
Iter: 1997 loss: 1.52348446e-06
Iter: 1998 loss: 1.52325197e-06
Iter: 1999 loss: 1.5231376e-06
Iter: 2000 loss: 1.5228859e-06
Iter: 2001 loss: 1.52313874e-06
Iter: 2002 loss: 1.5227298e-06
Iter: 2003 loss: 1.52253369e-06
Iter: 2004 loss: 1.52366829e-06
Iter: 2005 loss: 1.52233156e-06
Iter: 2006 loss: 1.52196503e-06
Iter: 2007 loss: 1.52216774e-06
Iter: 2008 loss: 1.52172822e-06
Iter: 2009 loss: 1.52142866e-06
Iter: 2010 loss: 1.52298674e-06
Iter: 2011 loss: 1.52133555e-06
Iter: 2012 loss: 1.52123425e-06
Iter: 2013 loss: 1.52239625e-06
Iter: 2014 loss: 1.52117639e-06
Iter: 2015 loss: 1.52104508e-06
Iter: 2016 loss: 1.52078508e-06
Iter: 2017 loss: 1.52079065e-06
Iter: 2018 loss: 1.52053337e-06
Iter: 2019 loss: 1.52360553e-06
Iter: 2020 loss: 1.52048483e-06
Iter: 2021 loss: 1.52025677e-06
Iter: 2022 loss: 1.52034522e-06
Iter: 2023 loss: 1.52013104e-06
Iter: 2024 loss: 1.51984295e-06
Iter: 2025 loss: 1.52049256e-06
Iter: 2026 loss: 1.51965776e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.8
+ date
Wed Oct 21 15:59:35 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/300_300_300_1 --function f1 --psi 0 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5a4e9b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5a4e46840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e73469d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e7346598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5a4e3d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5a4d71048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5a4dd60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580650730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580650598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580650510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5805dd2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5804ef9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5805078c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580570048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580576d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580576e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580558ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5805651e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5805cfae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580565048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5804430d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5804550d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc58047cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5803cd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5803cdd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5803cde18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5803cd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5802e4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5802e4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5803867b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580380268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580263f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580263730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5802f7d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5801d20d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc58016ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.08452507
test_loss: 0.08443516
train_loss: 0.042741194
test_loss: 0.040549275
train_loss: 0.023766264
test_loss: 0.022344308
train_loss: 0.013687891
test_loss: 0.012774341
train_loss: 0.008842886
test_loss: 0.009429452
train_loss: 0.0075382777
test_loss: 0.0073253484
train_loss: 0.006230848
test_loss: 0.0058036223
train_loss: 0.005321618
test_loss: 0.006907359
train_loss: 0.0056100455
test_loss: 0.0055460846
train_loss: 0.00505292
test_loss: 0.004562035
train_loss: 0.004847609
test_loss: 0.0053201937
train_loss: 0.0044560716
test_loss: 0.0042089485
train_loss: 0.0041855844
test_loss: 0.004376063
train_loss: 0.0048751985
test_loss: 0.0049506878
train_loss: 0.004115651
test_loss: 0.0047062077
train_loss: 0.004184537
test_loss: 0.0039057264
train_loss: 0.0037306484
test_loss: 0.0035374092
train_loss: 0.0043553626
test_loss: 0.0041116816
train_loss: 0.003814023
test_loss: 0.0060934373
train_loss: 0.00382631
test_loss: 0.004154601
train_loss: 0.0035850988
test_loss: 0.0039249407
train_loss: 0.0039567277
test_loss: 0.0038469597
train_loss: 0.0037426644
test_loss: 0.0034815266
train_loss: 0.003911185
test_loss: 0.0035998786
train_loss: 0.0035184436
test_loss: 0.003442887
train_loss: 0.003497784
test_loss: 0.003509353
train_loss: 0.0034172842
test_loss: 0.0035032793
train_loss: 0.0037543927
test_loss: 0.0042764335
train_loss: 0.0035710046
test_loss: 0.0033975432
train_loss: 0.0029881625
test_loss: 0.0034366287
train_loss: 0.0036396391
test_loss: 0.003060993
train_loss: 0.0031830422
test_loss: 0.0034854338
train_loss: 0.0038290913
test_loss: 0.004356102
train_loss: 0.0039343135
test_loss: 0.005452012
train_loss: 0.0041792677
test_loss: 0.0042446894
train_loss: 0.0041377004
test_loss: 0.003929287
train_loss: 0.003957173
test_loss: 0.004084968
train_loss: 0.0033931388
test_loss: 0.0035510312
train_loss: 0.0038844538
test_loss: 0.003616273
train_loss: 0.0028506352
test_loss: 0.003683773
train_loss: 0.0033307471
test_loss: 0.0035084567
train_loss: 0.003188287
test_loss: 0.003555753
train_loss: 0.0034279155
test_loss: 0.0033506134
train_loss: 0.0034186235
test_loss: 0.003707998
train_loss: 0.0036865529
test_loss: 0.0037903804
train_loss: 0.003131944
test_loss: 0.003172097
train_loss: 0.0032188087
test_loss: 0.0038509506
train_loss: 0.004243507
test_loss: 0.0042987783
train_loss: 0.0034017998
test_loss: 0.0031798477
train_loss: 0.0033607818
test_loss: 0.0034620096
train_loss: 0.0036095427
test_loss: 0.0032421655
train_loss: 0.0034349563
test_loss: 0.0032717807
train_loss: 0.003734706
test_loss: 0.0030753138
train_loss: 0.0030556142
test_loss: 0.0037373044
train_loss: 0.0027719294
test_loss: 0.003304058
train_loss: 0.0035965182
test_loss: 0.0037148728
train_loss: 0.00408367
test_loss: 0.0032784427
train_loss: 0.0033386713
test_loss: 0.0028616476
train_loss: 0.0039760564
test_loss: 0.0037324266
train_loss: 0.0032613075
test_loss: 0.003109426
train_loss: 0.0028340751
test_loss: 0.0030227625
train_loss: 0.0034393817
test_loss: 0.0036953995
train_loss: 0.00285911
test_loss: 0.0029362224
train_loss: 0.003512149
test_loss: 0.0033019416
train_loss: 0.0037413589
test_loss: 0.0032778226
train_loss: 0.003809669
test_loss: 0.0054633664
train_loss: 0.003526155
test_loss: 0.003341542
train_loss: 0.0035593174
test_loss: 0.0030573318
train_loss: 0.0037645316
test_loss: 0.0033809019
train_loss: 0.0031327177
test_loss: 0.0032070777
train_loss: 0.0031746929
test_loss: 0.0033454406
train_loss: 0.0030399454
test_loss: 0.003494038
train_loss: 0.003198822
test_loss: 0.0032912563
train_loss: 0.0031590099
test_loss: 0.0029397246
train_loss: 0.003764185
test_loss: 0.0041372296
train_loss: 0.0033516516
test_loss: 0.0028619417
train_loss: 0.0029471128
test_loss: 0.002558973
train_loss: 0.0036233906
test_loss: 0.0034191424
train_loss: 0.0032406757
test_loss: 0.0031716917
train_loss: 0.0035441464
test_loss: 0.0030385328
train_loss: 0.0037057698
test_loss: 0.0033937243
train_loss: 0.0027382397
test_loss: 0.003826509
train_loss: 0.0031375145
test_loss: 0.0037129228
train_loss: 0.002845238
test_loss: 0.0034479967
train_loss: 0.003606311
test_loss: 0.0033772218
train_loss: 0.004327141
test_loss: 0.0042068884
train_loss: 0.0029922607
test_loss: 0.0029764327
train_loss: 0.003211014
test_loss: 0.0033845818
train_loss: 0.002837438
test_loss: 0.002893888
train_loss: 0.0025293636
test_loss: 0.0028976793
train_loss: 0.0038503958
test_loss: 0.00431471
train_loss: 0.00603823
test_loss: 0.004818833
train_loss: 0.004018162
test_loss: 0.004049921
train_loss: 0.0034226242
test_loss: 0.0034997633
train_loss: 0.0027526356
test_loss: 0.0029490239
train_loss: 0.0036787752
test_loss: 0.0034597197
train_loss: 0.0032392105
test_loss: 0.0034252545
train_loss: 0.0037699554
test_loss: 0.0036410484
train_loss: 0.0030100655
test_loss: 0.003153806
train_loss: 0.002783502
test_loss: 0.00281283
train_loss: 0.0029158469
test_loss: 0.0032426768
train_loss: 0.0028854609
test_loss: 0.0030937267
train_loss: 0.002993808
test_loss: 0.0032809374
train_loss: 0.003238542
test_loss: 0.0034349405
train_loss: 0.0034842214
test_loss: 0.0030603067
train_loss: 0.003453494
test_loss: 0.003441043
train_loss: 0.0028650104
test_loss: 0.003101629
train_loss: 0.0030894922
test_loss: 0.0031817367
train_loss: 0.0041739005
test_loss: 0.0031542806
train_loss: 0.002972883
test_loss: 0.0025603718
train_loss: 0.0036032062
test_loss: 0.0029436965
train_loss: 0.003386917
test_loss: 0.0029138108
train_loss: 0.0030503804
test_loss: 0.0029925194
train_loss: 0.0039528366
test_loss: 0.0030511422
train_loss: 0.0036019236
test_loss: 0.0031698556
train_loss: 0.0042652125
test_loss: 0.005003578
train_loss: 0.0031039915
test_loss: 0.0030518174
train_loss: 0.0028373608
test_loss: 0.0028865393
train_loss: 0.0028455474
test_loss: 0.0027264033
train_loss: 0.0029141866
test_loss: 0.0029465412
train_loss: 0.0027444246
test_loss: 0.002961651
train_loss: 0.003088989
test_loss: 0.0030922492
train_loss: 0.0027519858
test_loss: 0.0031374905
train_loss: 0.0031928048
test_loss: 0.002838124
train_loss: 0.002637058
test_loss: 0.003434618
train_loss: 0.0028831726
test_loss: 0.0028821556
train_loss: 0.0027903328
test_loss: 0.0027952113
train_loss: 0.0028576748
test_loss: 0.0029145062
train_loss: 0.0031421045
test_loss: 0.0029938992
train_loss: 0.0029540933
test_loss: 0.003397649
train_loss: 0.0036678608
test_loss: 0.003422615
train_loss: 0.002766089
test_loss: 0.0031020455
train_loss: 0.0026871013
test_loss: 0.0025508401
train_loss: 0.0027302457
test_loss: 0.0026808
train_loss: 0.0027168903
test_loss: 0.0031550392
train_loss: 0.003538124
test_loss: 0.003782108
train_loss: 0.0025547473
test_loss: 0.002839375
train_loss: 0.003564859
test_loss: 0.003720334
train_loss: 0.0032495428
test_loss: 0.003331375
train_loss: 0.002913308
test_loss: 0.0033478688
train_loss: 0.0030349272
test_loss: 0.0033703737
train_loss: 0.0029100406
test_loss: 0.0026099337
train_loss: 0.0032411835
test_loss: 0.002810971
train_loss: 0.002872211
test_loss: 0.0029097826
train_loss: 0.0029032137
test_loss: 0.002710319
train_loss: 0.003363348
test_loss: 0.0034530677
train_loss: 0.003024056
test_loss: 0.0031894622
train_loss: 0.0030206547
test_loss: 0.002402669
train_loss: 0.0030070292
test_loss: 0.0028005168
train_loss: 0.00331175
test_loss: 0.00482592
train_loss: 0.004236859
test_loss: 0.0042303046
train_loss: 0.004057517
test_loss: 0.005148525
train_loss: 0.0036128252
test_loss: 0.002962852
train_loss: 0.0024765658
test_loss: 0.0028433606
train_loss: 0.0032669127
test_loss: 0.0029144203
train_loss: 0.0028996756
test_loss: 0.0029203873
train_loss: 0.0029394631
test_loss: 0.0028785835
train_loss: 0.003251946
test_loss: 0.0027611349
train_loss: 0.0024371059
test_loss: 0.0032066233
train_loss: 0.0022804309
test_loss: 0.0025646945
train_loss: 0.002824965
test_loss: 0.0034500964
train_loss: 0.0031439424
test_loss: 0.0033614272
train_loss: 0.0029015443
test_loss: 0.0031659552
train_loss: 0.0028291517
test_loss: 0.0028155118
train_loss: 0.0027322995
test_loss: 0.0029924675
train_loss: 0.0027420314
test_loss: 0.0029684703
train_loss: 0.0029101097
test_loss: 0.0038405028
train_loss: 0.0027702018
test_loss: 0.0035577228
train_loss: 0.0027404963
test_loss: 0.0031230405
train_loss: 0.0029671653
test_loss: 0.0027424288
train_loss: 0.002868209
test_loss: 0.0026599236
train_loss: 0.0026757075
test_loss: 0.002705654
train_loss: 0.0029909755
test_loss: 0.003051093
train_loss: 0.0030215518
test_loss: 0.003290884
train_loss: 0.0030399142
test_loss: 0.0033281383
train_loss: 0.00320299
test_loss: 0.0028888676
train_loss: 0.0029896067
test_loss: 0.0038217516
train_loss: 0.0029725274
test_loss: 0.0030778067
train_loss: 0.0028032542
test_loss: 0.002937099
train_loss: 0.0023758488
test_loss: 0.0026069963
train_loss: 0.0025273166
test_loss: 0.002607942
train_loss: 0.003159102
test_loss: 0.0035858967
train_loss: 0.0023175322
test_loss: 0.0028203484
train_loss: 0.0025368128
test_loss: 0.0029230828
train_loss: 0.003881476
test_loss: 0.0035832925
train_loss: 0.002920283
test_loss: 0.0031947223
train_loss: 0.0024695958
test_loss: 0.0022589448
train_loss: 0.0024197882
test_loss: 0.0028527777
train_loss: 0.0026551427
test_loss: 0.0026952664
train_loss: 0.0034722702
test_loss: 0.0029621911
train_loss: 0.0033183782
test_loss: 0.0029743027
train_loss: 0.0051465724
test_loss: 0.004726388
train_loss: 0.0035840797
test_loss: 0.003288585
train_loss: 0.0028158878
test_loss: 0.0031740575
train_loss: 0.0030173294
test_loss: 0.002882958
train_loss: 0.0027825637
test_loss: 0.0025064663
train_loss: 0.0029584747
test_loss: 0.0031092544
train_loss: 0.003030884
test_loss: 0.0029368554
train_loss: 0.0034229965
test_loss: 0.0031369603
train_loss: 0.0030058615
test_loss: 0.003408906
train_loss: 0.0026179948
test_loss: 0.002445794
train_loss: 0.00271885
test_loss: 0.002649263
train_loss: 0.0025972966
test_loss: 0.0029633786
train_loss: 0.0029063625
test_loss: 0.002729126
train_loss: 0.003273026
test_loss: 0.003316308
train_loss: 0.0024727623
test_loss: 0.003234519
train_loss: 0.0027046497
test_loss: 0.0025084743
train_loss: 0.0023960778
test_loss: 0.0025869897
train_loss: 0.0028638323
test_loss: 0.003274252
train_loss: 0.003028273
test_loss: 0.0031050623
train_loss: 0.002656296
test_loss: 0.0027762672
train_loss: 0.0028051687
test_loss: 0.0025523019
train_loss: 0.0027889814
test_loss: 0.002726892
train_loss: 0.002324146
test_loss: 0.0025556355
train_loss: 0.0034850244
test_loss: 0.002742323
train_loss: 0.0032278392
test_loss: 0.0026327888
train_loss: 0.002829197
test_loss: 0.0025525622
train_loss: 0.0027580431
test_loss: 0.002973162
train_loss: 0.002400578
test_loss: 0.002870521
train_loss: 0.003159065
test_loss: 0.00254549
train_loss: 0.0030081237
test_loss: 0.0031720044
train_loss: 0.0027167085
test_loss: 0.0030622703
train_loss: 0.003112236
test_loss: 0.0032698372
train_loss: 0.0030343325
test_loss: 0.0041128397
train_loss: 0.0032719746
test_loss: 0.005155817
train_loss: 0.003307274
test_loss: 0.0030500905
train_loss: 0.002611138
test_loss: 0.0036046163
train_loss: 0.0027566026
test_loss: 0.0027965442
train_loss: 0.0029959339
test_loss: 0.0027838058
train_loss: 0.002433658
test_loss: 0.002486698
train_loss: 0.0025676729
test_loss: 0.0031471062
train_loss: 0.002906741
test_loss: 0.0034401414
train_loss: 0.0029887182
test_loss: 0.0029231124
train_loss: 0.0030481573
test_loss: 0.003028136
train_loss: 0.0035212752
test_loss: 0.0032900113
train_loss: 0.0031384276
test_loss: 0.0030061568
train_loss: 0.0024328576
test_loss: 0.0029662964
train_loss: 0.003294739
test_loss: 0.0028551465
train_loss: 0.0037670555
test_loss: 0.0044732783
train_loss: 0.0028024719
test_loss: 0.0031074937
train_loss: 0.0024601952
test_loss: 0.0025478764
train_loss: 0.0030846917
test_loss: 0.0034119198
train_loss: 0.0029224725
test_loss: 0.0035708027
train_loss: 0.004810608
test_loss: 0.004503269
train_loss: 0.0026404958
test_loss: 0.0027709932
train_loss: 0.0024907745
test_loss: 0.0023442234
train_loss: 0.0029904034
test_loss: 0.0030618045
train_loss: 0.0032996912
test_loss: 0.0034109727
train_loss: 0.0024599188
test_loss: 0.0025673562
train_loss: 0.003256033
test_loss: 0.0034505632
train_loss: 0.003186444
test_loss: 0.0029669127
train_loss: 0.0027473047
test_loss: 0.0023534372
train_loss: 0.003013692
test_loss: 0.002956175
train_loss: 0.0024410707
test_loss: 0.0027354178
train_loss: 0.002659611
test_loss: 0.0029440948
train_loss: 0.0023567341
test_loss: 0.0026750602
train_loss: 0.0033705495
test_loss: 0.0032384458
train_loss: 0.003157788
test_loss: 0.0042411606
train_loss: 0.003381858
test_loss: 0.0033814765
train_loss: 0.0030015425
test_loss: 0.0036426308
train_loss: 0.002827919
test_loss: 0.0028148307
train_loss: 0.0026812484
test_loss: 0.0026986825
train_loss: 0.0027149012
test_loss: 0.0029992412
train_loss: 0.002739259
test_loss: 0.0032302754
train_loss: 0.0026873392
test_loss: 0.0026215455
train_loss: 0.0028632984
test_loss: 0.0025595368
train_loss: 0.0029402692
test_loss: 0.0034395757
train_loss: 0.0028152545
test_loss: 0.003119389
train_loss: 0.0024370947
test_loss: 0.0023789166
train_loss: 0.002408974
test_loss: 0.0026979141
train_loss: 0.0025745365
test_loss: 0.0026391305
train_loss: 0.0031455786
test_loss: 0.0025949434
train_loss: 0.0025956566
test_loss: 0.002889939
train_loss: 0.0034090602
test_loss: 0.0028439898
train_loss: 0.002594349
test_loss: 0.0024989294
train_loss: 0.002584768
test_loss: 0.0023210319
train_loss: 0.0025777132
test_loss: 0.002789627
train_loss: 0.0032504953
test_loss: 0.0030748665
train_loss: 0.0031190808
test_loss: 0.0028082097
train_loss: 0.002276792
test_loss: 0.0026385426
train_loss: 0.0025598311
test_loss: 0.0027748204
train_loss: 0.0027397403
test_loss: 0.0030029532
train_loss: 0.0034187487
test_loss: 0.003096954
train_loss: 0.0029314177
test_loss: 0.003162365
train_loss: 0.00251572
test_loss: 0.0028268464
train_loss: 0.0045624655
test_loss: 0.004419102
train_loss: 0.0033323523
test_loss: 0.0030616052
train_loss: 0.0027834943
test_loss: 0.002904404
train_loss: 0.0021566122
test_loss: 0.0024583922
train_loss: 0.0027486887
test_loss: 0.0024397208
train_loss: 0.0028203505
test_loss: 0.002927131
train_loss: 0.0030998853
test_loss: 0.0032054603
train_loss: 0.0030341973
test_loss: 0.00324369
train_loss: 0.002682727
test_loss: 0.002746637
train_loss: 0.0027584445
test_loss: 0.0031646932
train_loss: 0.0027795671
test_loss: 0.0025095374
train_loss: 0.0027928902
test_loss: 0.0026374194
train_loss: 0.0027300646
test_loss: 0.0030447175
train_loss: 0.0023619276
test_loss: 0.0025824555
train_loss: 0.0028952109
test_loss: 0.0026689786
train_loss: 0.0025200057
test_loss: 0.002265466
train_loss: 0.0022462276
test_loss: 0.0025295038
train_loss: 0.0030640105
test_loss: 0.0027534652
train_loss: 0.0024418528
test_loss: 0.00275537
train_loss: 0.0021709339
test_loss: 0.00223105
train_loss: 0.0030804262
test_loss: 0.0025430894
train_loss: 0.0022704017
test_loss: 0.0021442967
train_loss: 0.002213111
test_loss: 0.0026521147
train_loss: 0.0037475205
test_loss: 0.0023473385
train_loss: 0.0029936954
test_loss: 0.0031231353
train_loss: 0.002682073
test_loss: 0.0026227839
train_loss: 0.003159942
test_loss: 0.003176955
train_loss: 0.00273956
test_loss: 0.0028386335
train_loss: 0.0021789626
test_loss: 0.0027361535
train_loss: 0.0028235686
test_loss: 0.0026056254
train_loss: 0.002738581
test_loss: 0.0026809198
train_loss: 0.0023543078
test_loss: 0.0028216345
train_loss: 0.0029124161
test_loss: 0.0027608855
train_loss: 0.0025926703
test_loss: 0.0032439837
train_loss: 0.0031228678
test_loss: 0.0028993832
train_loss: 0.0027571209
test_loss: 0.0031025568
train_loss: 0.003281828
test_loss: 0.0031207525
train_loss: 0.0025335636
test_loss: 0.0029596908
train_loss: 0.0031452011
test_loss: 0.0032613869
train_loss: 0.0034380872
test_loss: 0.002588218
train_loss: 0.0043274746
test_loss: 0.004088325
train_loss: 0.0034162519
test_loss: 0.004257993
train_loss: 0.0026132572
test_loss: 0.0025685334
train_loss: 0.003345201
test_loss: 0.0037832158
train_loss: 0.0037147696
test_loss: 0.0030999738
train_loss: 0.0027059438
test_loss: 0.0027681277
train_loss: 0.0030486528
test_loss: 0.0026016268
train_loss: 0.0019923353
test_loss: 0.0023383745
train_loss: 0.002132855
test_loss: 0.002359815
train_loss: 0.0027371878
test_loss: 0.0022853173
train_loss: 0.002316924
test_loss: 0.0029418292
train_loss: 0.0030398928
test_loss: 0.002306864
train_loss: 0.0028796936
test_loss: 0.0025766934
train_loss: 0.0027323146
test_loss: 0.0028300013/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

train_loss: 0.0022770446
test_loss: 0.002557162
train_loss: 0.002434239
test_loss: 0.0024127378
train_loss: 0.0024283603
test_loss: 0.0026023213
train_loss: 0.0035124589
test_loss: 0.0031678628
train_loss: 0.0028218941
test_loss: 0.0028755006
train_loss: 0.002905932
test_loss: 0.0024352307
train_loss: 0.00237525
test_loss: 0.0023561027
train_loss: 0.002163876
test_loss: 0.002416069
train_loss: 0.0034446414
test_loss: 0.0033396916
train_loss: 0.0030583283
test_loss: 0.0027206277
train_loss: 0.002720805
test_loss: 0.0027340131
train_loss: 0.0028966514
test_loss: 0.002447911
train_loss: 0.0021912325
test_loss: 0.0023524843
train_loss: 0.0027419594
test_loss: 0.0023269493
train_loss: 0.0037861345
test_loss: 0.0037317588
train_loss: 0.0033298433
test_loss: 0.0027346073
train_loss: 0.002078266
test_loss: 0.0023754097
train_loss: 0.002475538
test_loss: 0.0025580314
train_loss: 0.0030108772
test_loss: 0.0025244404
train_loss: 0.0019655025
test_loss: 0.002400705
train_loss: 0.0029394869
test_loss: 0.0034549101
train_loss: 0.0027560391
test_loss: 0.002912144
train_loss: 0.0031413466
test_loss: 0.0033733894
train_loss: 0.002221328
test_loss: 0.002804048
train_loss: 0.002595012
test_loss: 0.0025429453
train_loss: 0.0024010951
test_loss: 0.002250401
train_loss: 0.002263416
test_loss: 0.0023082949
train_loss: 0.0021730852
test_loss: 0.0022583182
train_loss: 0.0022396394
test_loss: 0.0023083354
train_loss: 0.0025333217
test_loss: 0.0023709035
train_loss: 0.0025356724
test_loss: 0.0025796022
train_loss: 0.002804085
test_loss: 0.0026756532
train_loss: 0.0026072646
test_loss: 0.002586949
train_loss: 0.0025027276
test_loss: 0.002953629
train_loss: 0.002640729
test_loss: 0.0021138156
train_loss: 0.0027492614
test_loss: 0.00285358
train_loss: 0.002633947
test_loss: 0.003430757
train_loss: 0.0029437598
test_loss: 0.0033999365
train_loss: 0.0029148217
test_loss: 0.002347378
train_loss: 0.0029319115
test_loss: 0.002762747
train_loss: 0.002788023
test_loss: 0.0031650346
train_loss: 0.0026425913
test_loss: 0.0024567365
train_loss: 0.0029115435
test_loss: 0.0025034097
train_loss: 0.0029716967
test_loss: 0.0035163963
train_loss: 0.003298915
test_loss: 0.0029692848
train_loss: 0.0023301297
test_loss: 0.0027485758
train_loss: 0.0024834215
test_loss: 0.0026454052
train_loss: 0.002663275
test_loss: 0.0026977563
train_loss: 0.0033387677
test_loss: 0.0028410852
train_loss: 0.002333371
test_loss: 0.0025647734
train_loss: 0.0036509968
test_loss: 0.0029608908
train_loss: 0.0025409102
test_loss: 0.0026372268
train_loss: 0.0027632085
test_loss: 0.002364499
train_loss: 0.002452813
test_loss: 0.0025362105
train_loss: 0.0031739632
test_loss: 0.0027274273
train_loss: 0.0040170574
test_loss: 0.0038490382
train_loss: 0.0032284837
test_loss: 0.0025114666
train_loss: 0.002408422
test_loss: 0.0025011715
train_loss: 0.0031686923
test_loss: 0.0026920906
train_loss: 0.0033368904
test_loss: 0.0027366616
train_loss: 0.0023896683
test_loss: 0.002272545
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c7296400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c72549d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c71d79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c72f96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c719bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c719b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c715eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c7125048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c71bf598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c71bfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c70fd1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c70f4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c70f4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c7073048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c706e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c703b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c70f4e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c70a7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c7007b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c7073400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd9cda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd9830d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd9cd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd94f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd94f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd94fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd94f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd8dc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd8dcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd88b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd89c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd80cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd80c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd840d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93733640d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93732fcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 7.89715523e-06
Iter: 2 loss: 2.53828766e-05
Iter: 3 loss: 6.32508454e-06
Iter: 4 loss: 5.49849574e-06
Iter: 5 loss: 4.90394723e-06
Iter: 6 loss: 4.61843501e-06
Iter: 7 loss: 3.96236464e-06
Iter: 8 loss: 3.28641454e-06
Iter: 9 loss: 3.16156229e-06
Iter: 10 loss: 2.62821686e-06
Iter: 11 loss: 2.62716253e-06
Iter: 12 loss: 2.46858099e-06
Iter: 13 loss: 3.46108163e-06
Iter: 14 loss: 2.45036313e-06
Iter: 15 loss: 2.31603281e-06
Iter: 16 loss: 2.32731463e-06
Iter: 17 loss: 2.21192818e-06
Iter: 18 loss: 2.07887774e-06
Iter: 19 loss: 3.16828391e-06
Iter: 20 loss: 2.07052562e-06
Iter: 21 loss: 2.00343516e-06
Iter: 22 loss: 1.94322433e-06
Iter: 23 loss: 1.92655534e-06
Iter: 24 loss: 1.85451347e-06
Iter: 25 loss: 1.85256022e-06
Iter: 26 loss: 1.81600456e-06
Iter: 27 loss: 1.89088462e-06
Iter: 28 loss: 1.80126108e-06
Iter: 29 loss: 1.76352341e-06
Iter: 30 loss: 1.78267908e-06
Iter: 31 loss: 1.73835451e-06
Iter: 32 loss: 1.70076078e-06
Iter: 33 loss: 1.9354311e-06
Iter: 34 loss: 1.69632926e-06
Iter: 35 loss: 1.68679344e-06
Iter: 36 loss: 1.68250972e-06
Iter: 37 loss: 1.66566633e-06
Iter: 38 loss: 1.62202684e-06
Iter: 39 loss: 1.96124392e-06
Iter: 40 loss: 1.61359037e-06
Iter: 41 loss: 1.57371915e-06
Iter: 42 loss: 1.67256906e-06
Iter: 43 loss: 1.55961141e-06
Iter: 44 loss: 1.53068913e-06
Iter: 45 loss: 1.52411076e-06
Iter: 46 loss: 1.50534174e-06
Iter: 47 loss: 1.48542563e-06
Iter: 48 loss: 1.48257254e-06
Iter: 49 loss: 1.46807474e-06
Iter: 50 loss: 1.45134777e-06
Iter: 51 loss: 1.44934506e-06
Iter: 52 loss: 1.42464432e-06
Iter: 53 loss: 1.6778672e-06
Iter: 54 loss: 1.42394128e-06
Iter: 55 loss: 1.4121423e-06
Iter: 56 loss: 1.48354707e-06
Iter: 57 loss: 1.4106638e-06
Iter: 58 loss: 1.39819645e-06
Iter: 59 loss: 1.37202801e-06
Iter: 60 loss: 1.802935e-06
Iter: 61 loss: 1.37122038e-06
Iter: 62 loss: 1.36667e-06
Iter: 63 loss: 1.36307813e-06
Iter: 64 loss: 1.35614e-06
Iter: 65 loss: 1.35029154e-06
Iter: 66 loss: 1.34832703e-06
Iter: 67 loss: 1.34094614e-06
Iter: 68 loss: 1.34065976e-06
Iter: 69 loss: 1.33673495e-06
Iter: 70 loss: 1.33661911e-06
Iter: 71 loss: 1.33473418e-06
Iter: 72 loss: 1.32866103e-06
Iter: 73 loss: 1.33898186e-06
Iter: 74 loss: 1.32464947e-06
Iter: 75 loss: 1.31284105e-06
Iter: 76 loss: 1.32548132e-06
Iter: 77 loss: 1.30639285e-06
Iter: 78 loss: 1.29741034e-06
Iter: 79 loss: 1.37085863e-06
Iter: 80 loss: 1.29672298e-06
Iter: 81 loss: 1.28805027e-06
Iter: 82 loss: 1.29362991e-06
Iter: 83 loss: 1.28259808e-06
Iter: 84 loss: 1.27546502e-06
Iter: 85 loss: 1.3296808e-06
Iter: 86 loss: 1.27504541e-06
Iter: 87 loss: 1.26886266e-06
Iter: 88 loss: 1.27605176e-06
Iter: 89 loss: 1.26564919e-06
Iter: 90 loss: 1.25641986e-06
Iter: 91 loss: 1.29407272e-06
Iter: 92 loss: 1.25439851e-06
Iter: 93 loss: 1.25023757e-06
Iter: 94 loss: 1.25643203e-06
Iter: 95 loss: 1.24827488e-06
Iter: 96 loss: 1.24288601e-06
Iter: 97 loss: 1.25070255e-06
Iter: 98 loss: 1.24033579e-06
Iter: 99 loss: 1.23576706e-06
Iter: 100 loss: 1.29415128e-06
Iter: 101 loss: 1.23580821e-06
Iter: 102 loss: 1.23401583e-06
Iter: 103 loss: 1.23392874e-06
Iter: 104 loss: 1.23167933e-06
Iter: 105 loss: 1.22572499e-06
Iter: 106 loss: 1.28169881e-06
Iter: 107 loss: 1.22497488e-06
Iter: 108 loss: 1.22133372e-06
Iter: 109 loss: 1.24964981e-06
Iter: 110 loss: 1.22109509e-06
Iter: 111 loss: 1.21839366e-06
Iter: 112 loss: 1.21354401e-06
Iter: 113 loss: 1.21352741e-06
Iter: 114 loss: 1.2080684e-06
Iter: 115 loss: 1.26018153e-06
Iter: 116 loss: 1.2077827e-06
Iter: 117 loss: 1.20424693e-06
Iter: 118 loss: 1.20158381e-06
Iter: 119 loss: 1.20027266e-06
Iter: 120 loss: 1.19393212e-06
Iter: 121 loss: 1.23830273e-06
Iter: 122 loss: 1.193191e-06
Iter: 123 loss: 1.19035394e-06
Iter: 124 loss: 1.21712378e-06
Iter: 125 loss: 1.19025026e-06
Iter: 126 loss: 1.18717139e-06
Iter: 127 loss: 1.18474543e-06
Iter: 128 loss: 1.18373316e-06
Iter: 129 loss: 1.1802274e-06
Iter: 130 loss: 1.2062394e-06
Iter: 131 loss: 1.17990487e-06
Iter: 132 loss: 1.17748164e-06
Iter: 133 loss: 1.17965681e-06
Iter: 134 loss: 1.17606237e-06
Iter: 135 loss: 1.17325544e-06
Iter: 136 loss: 1.20538778e-06
Iter: 137 loss: 1.17317688e-06
Iter: 138 loss: 1.17114269e-06
Iter: 139 loss: 1.20326297e-06
Iter: 140 loss: 1.17113336e-06
Iter: 141 loss: 1.17039531e-06
Iter: 142 loss: 1.16843057e-06
Iter: 143 loss: 1.18318326e-06
Iter: 144 loss: 1.16814181e-06
Iter: 145 loss: 1.165188e-06
Iter: 146 loss: 1.1675321e-06
Iter: 147 loss: 1.16338924e-06
Iter: 148 loss: 1.16063916e-06
Iter: 149 loss: 1.17131481e-06
Iter: 150 loss: 1.15993248e-06
Iter: 151 loss: 1.15707724e-06
Iter: 152 loss: 1.15791704e-06
Iter: 153 loss: 1.15497539e-06
Iter: 154 loss: 1.15112596e-06
Iter: 155 loss: 1.18244805e-06
Iter: 156 loss: 1.15086414e-06
Iter: 157 loss: 1.14866157e-06
Iter: 158 loss: 1.14598629e-06
Iter: 159 loss: 1.14570958e-06
Iter: 160 loss: 1.14180853e-06
Iter: 161 loss: 1.19178958e-06
Iter: 162 loss: 1.14178829e-06
Iter: 163 loss: 1.14015529e-06
Iter: 164 loss: 1.15276714e-06
Iter: 165 loss: 1.13997635e-06
Iter: 166 loss: 1.13825968e-06
Iter: 167 loss: 1.13387807e-06
Iter: 168 loss: 1.17860509e-06
Iter: 169 loss: 1.13337012e-06
Iter: 170 loss: 1.13092597e-06
Iter: 171 loss: 1.13067961e-06
Iter: 172 loss: 1.13002739e-06
Iter: 173 loss: 1.12991074e-06
Iter: 174 loss: 1.12881958e-06
Iter: 175 loss: 1.12629323e-06
Iter: 176 loss: 1.14932413e-06
Iter: 177 loss: 1.12593762e-06
Iter: 178 loss: 1.12437931e-06
Iter: 179 loss: 1.13210717e-06
Iter: 180 loss: 1.12412351e-06
Iter: 181 loss: 1.12272551e-06
Iter: 182 loss: 1.12428859e-06
Iter: 183 loss: 1.12200064e-06
Iter: 184 loss: 1.12003318e-06
Iter: 185 loss: 1.12214252e-06
Iter: 186 loss: 1.11898021e-06
Iter: 187 loss: 1.11737813e-06
Iter: 188 loss: 1.12138389e-06
Iter: 189 loss: 1.11672239e-06
Iter: 190 loss: 1.11464396e-06
Iter: 191 loss: 1.11831696e-06
Iter: 192 loss: 1.11358679e-06
Iter: 193 loss: 1.11153724e-06
Iter: 194 loss: 1.12545445e-06
Iter: 195 loss: 1.1113375e-06
Iter: 196 loss: 1.10959172e-06
Iter: 197 loss: 1.10927886e-06
Iter: 198 loss: 1.10807798e-06
Iter: 199 loss: 1.10589951e-06
Iter: 200 loss: 1.13679346e-06
Iter: 201 loss: 1.10593453e-06
Iter: 202 loss: 1.10492624e-06
Iter: 203 loss: 1.10509836e-06
Iter: 204 loss: 1.10416545e-06
Iter: 205 loss: 1.10259816e-06
Iter: 206 loss: 1.10627661e-06
Iter: 207 loss: 1.10187898e-06
Iter: 208 loss: 1.10068299e-06
Iter: 209 loss: 1.10052201e-06
Iter: 210 loss: 1.10001838e-06
Iter: 211 loss: 1.09875691e-06
Iter: 212 loss: 1.10995961e-06
Iter: 213 loss: 1.09855944e-06
Iter: 214 loss: 1.09696293e-06
Iter: 215 loss: 1.0945846e-06
Iter: 216 loss: 1.09459256e-06
Iter: 217 loss: 1.09388952e-06
Iter: 218 loss: 1.09339771e-06
Iter: 219 loss: 1.09240261e-06
Iter: 220 loss: 1.09131179e-06
Iter: 221 loss: 1.09123107e-06
Iter: 222 loss: 1.0892719e-06
Iter: 223 loss: 1.09952043e-06
Iter: 224 loss: 1.0890227e-06
Iter: 225 loss: 1.08802328e-06
Iter: 226 loss: 1.0888383e-06
Iter: 227 loss: 1.08731797e-06
Iter: 228 loss: 1.08568906e-06
Iter: 229 loss: 1.08848099e-06
Iter: 230 loss: 1.08498864e-06
Iter: 231 loss: 1.08375116e-06
Iter: 232 loss: 1.10162318e-06
Iter: 233 loss: 1.0837116e-06
Iter: 234 loss: 1.08268137e-06
Iter: 235 loss: 1.08157792e-06
Iter: 236 loss: 1.08144206e-06
Iter: 237 loss: 1.07993549e-06
Iter: 238 loss: 1.09489145e-06
Iter: 239 loss: 1.07992616e-06
Iter: 240 loss: 1.07961569e-06
Iter: 241 loss: 1.0793899e-06
Iter: 242 loss: 1.07886581e-06
Iter: 243 loss: 1.07787014e-06
Iter: 244 loss: 1.09803887e-06
Iter: 245 loss: 1.07788765e-06
Iter: 246 loss: 1.07688925e-06
Iter: 247 loss: 1.07534811e-06
Iter: 248 loss: 1.07527717e-06
Iter: 249 loss: 1.07389042e-06
Iter: 250 loss: 1.07642836e-06
Iter: 251 loss: 1.07323274e-06
Iter: 252 loss: 1.07122491e-06
Iter: 253 loss: 1.08465906e-06
Iter: 254 loss: 1.07105848e-06
Iter: 255 loss: 1.07012318e-06
Iter: 256 loss: 1.07732444e-06
Iter: 257 loss: 1.07004803e-06
Iter: 258 loss: 1.06915877e-06
Iter: 259 loss: 1.06800985e-06
Iter: 260 loss: 1.06794835e-06
Iter: 261 loss: 1.0662759e-06
Iter: 262 loss: 1.07988672e-06
Iter: 263 loss: 1.06617404e-06
Iter: 264 loss: 1.06532184e-06
Iter: 265 loss: 1.06524078e-06
Iter: 266 loss: 1.06462119e-06
Iter: 267 loss: 1.0630431e-06
Iter: 268 loss: 1.06863013e-06
Iter: 269 loss: 1.06268294e-06
Iter: 270 loss: 1.0614857e-06
Iter: 271 loss: 1.06977723e-06
Iter: 272 loss: 1.06137873e-06
Iter: 273 loss: 1.06071559e-06
Iter: 274 loss: 1.06312348e-06
Iter: 275 loss: 1.06053358e-06
Iter: 276 loss: 1.05945617e-06
Iter: 277 loss: 1.06376524e-06
Iter: 278 loss: 1.05920185e-06
Iter: 279 loss: 1.05873642e-06
Iter: 280 loss: 1.05831509e-06
Iter: 281 loss: 1.05813535e-06
Iter: 282 loss: 1.05745744e-06
Iter: 283 loss: 1.05644563e-06
Iter: 284 loss: 1.05642823e-06
Iter: 285 loss: 1.05516904e-06
Iter: 286 loss: 1.0648223e-06
Iter: 287 loss: 1.05507581e-06
Iter: 288 loss: 1.05420315e-06
Iter: 289 loss: 1.05524782e-06
Iter: 290 loss: 1.0537866e-06
Iter: 291 loss: 1.05250388e-06
Iter: 292 loss: 1.05789979e-06
Iter: 293 loss: 1.05223091e-06
Iter: 294 loss: 1.05145728e-06
Iter: 295 loss: 1.05444008e-06
Iter: 296 loss: 1.05129277e-06
Iter: 297 loss: 1.05062463e-06
Iter: 298 loss: 1.05044069e-06
Iter: 299 loss: 1.04995115e-06
Iter: 300 loss: 1.04882588e-06
Iter: 301 loss: 1.05517756e-06
Iter: 302 loss: 1.04867888e-06
Iter: 303 loss: 1.04807225e-06
Iter: 304 loss: 1.05061383e-06
Iter: 305 loss: 1.04790365e-06
Iter: 306 loss: 1.04703747e-06
Iter: 307 loss: 1.0462561e-06
Iter: 308 loss: 1.046021e-06
Iter: 309 loss: 1.04762285e-06
Iter: 310 loss: 1.04571404e-06
Iter: 311 loss: 1.04554181e-06
Iter: 312 loss: 1.04496587e-06
Iter: 313 loss: 1.04805713e-06
Iter: 314 loss: 1.04474441e-06
Iter: 315 loss: 1.04399419e-06
Iter: 316 loss: 1.04387868e-06
Iter: 317 loss: 1.04332867e-06
Iter: 318 loss: 1.04226046e-06
Iter: 319 loss: 1.05020274e-06
Iter: 320 loss: 1.04217042e-06
Iter: 321 loss: 1.04151036e-06
Iter: 322 loss: 1.04007586e-06
Iter: 323 loss: 1.06631865e-06
Iter: 324 loss: 1.04004539e-06
Iter: 325 loss: 1.03896718e-06
Iter: 326 loss: 1.05495621e-06
Iter: 327 loss: 1.03906132e-06
Iter: 328 loss: 1.03790887e-06
Iter: 329 loss: 1.04059382e-06
Iter: 330 loss: 1.03759965e-06
Iter: 331 loss: 1.03664013e-06
Iter: 332 loss: 1.04328444e-06
Iter: 333 loss: 1.03655714e-06
Iter: 334 loss: 1.03598882e-06
Iter: 335 loss: 1.03593868e-06
Iter: 336 loss: 1.03551e-06
Iter: 337 loss: 1.03453635e-06
Iter: 338 loss: 1.0355252e-06
Iter: 339 loss: 1.0339844e-06
Iter: 340 loss: 1.03319167e-06
Iter: 341 loss: 1.03884565e-06
Iter: 342 loss: 1.03312573e-06
Iter: 343 loss: 1.0323763e-06
Iter: 344 loss: 1.03405273e-06
Iter: 345 loss: 1.0319776e-06
Iter: 346 loss: 1.03159869e-06
Iter: 347 loss: 1.03153093e-06
Iter: 348 loss: 1.03113484e-06
Iter: 349 loss: 1.03238176e-06
Iter: 350 loss: 1.03096897e-06
Iter: 351 loss: 1.03065611e-06
Iter: 352 loss: 1.03006789e-06
Iter: 353 loss: 1.03764933e-06
Iter: 354 loss: 1.03004459e-06
Iter: 355 loss: 1.02895581e-06
Iter: 356 loss: 1.02973763e-06
Iter: 357 loss: 1.02834349e-06
Iter: 358 loss: 1.02775437e-06
Iter: 359 loss: 1.03513594e-06
Iter: 360 loss: 1.0277771e-06
Iter: 361 loss: 1.02722151e-06
Iter: 362 loss: 1.02638251e-06
Iter: 363 loss: 1.02638e-06
Iter: 364 loss: 1.02548734e-06
Iter: 365 loss: 1.03387697e-06
Iter: 366 loss: 1.02547222e-06
Iter: 367 loss: 1.0247611e-06
Iter: 368 loss: 1.02334195e-06
Iter: 369 loss: 1.05465062e-06
Iter: 370 loss: 1.02330637e-06
Iter: 371 loss: 1.02236663e-06
Iter: 372 loss: 1.02235106e-06
Iter: 373 loss: 1.02141644e-06
Iter: 374 loss: 1.02160345e-06
Iter: 375 loss: 1.0208073e-06
Iter: 376 loss: 1.02012848e-06
Iter: 377 loss: 1.02016384e-06
Iter: 378 loss: 1.01964054e-06
Iter: 379 loss: 1.01917237e-06
Iter: 380 loss: 1.01907233e-06
Iter: 381 loss: 1.01873229e-06
Iter: 382 loss: 1.01855846e-06
Iter: 383 loss: 1.01813907e-06
Iter: 384 loss: 1.01770411e-06
Iter: 385 loss: 1.01768683e-06
Iter: 386 loss: 1.01721344e-06
Iter: 387 loss: 1.01646037e-06
Iter: 388 loss: 1.03124194e-06
Iter: 389 loss: 1.0164149e-06
Iter: 390 loss: 1.01549358e-06
Iter: 391 loss: 1.02293438e-06
Iter: 392 loss: 1.01539899e-06
Iter: 393 loss: 1.01487558e-06
Iter: 394 loss: 1.01785508e-06
Iter: 395 loss: 1.01490127e-06
Iter: 396 loss: 1.01440446e-06
Iter: 397 loss: 1.01351259e-06
Iter: 398 loss: 1.01349053e-06
Iter: 399 loss: 1.01263072e-06
Iter: 400 loss: 1.02483921e-06
Iter: 401 loss: 1.01267403e-06
Iter: 402 loss: 1.01212652e-06
Iter: 403 loss: 1.01117212e-06
Iter: 404 loss: 1.03064099e-06
Iter: 405 loss: 1.01111755e-06
Iter: 406 loss: 1.01013575e-06
Iter: 407 loss: 1.02504805e-06
Iter: 408 loss: 1.01013688e-06
Iter: 409 loss: 1.00947955e-06
Iter: 410 loss: 1.01036449e-06
Iter: 411 loss: 1.0092474e-06
Iter: 412 loss: 1.00841874e-06
Iter: 413 loss: 1.0102126e-06
Iter: 414 loss: 1.00801242e-06
Iter: 415 loss: 1.00737225e-06
Iter: 416 loss: 1.01020623e-06
Iter: 417 loss: 1.00722445e-06
Iter: 418 loss: 1.00683883e-06
Iter: 419 loss: 1.00677494e-06
Iter: 420 loss: 1.00644365e-06
Iter: 421 loss: 1.00557213e-06
Iter: 422 loss: 1.01209798e-06
Iter: 423 loss: 1.00527166e-06
Iter: 424 loss: 1.00450575e-06
Iter: 425 loss: 1.00734042e-06
Iter: 426 loss: 1.00427053e-06
Iter: 427 loss: 1.00361024e-06
Iter: 428 loss: 1.00370391e-06
Iter: 429 loss: 1.00311036e-06
Iter: 430 loss: 1.00234138e-06
Iter: 431 loss: 1.01049e-06
Iter: 432 loss: 1.00234547e-06
Iter: 433 loss: 1.00199168e-06
Iter: 434 loss: 1.00277794e-06
Iter: 435 loss: 1.00174259e-06
Iter: 436 loss: 1.00123793e-06
Iter: 437 loss: 1.00128159e-06
Iter: 438 loss: 1.00081036e-06
Iter: 439 loss: 1.00018633e-06
Iter: 440 loss: 1.00856482e-06
Iter: 441 loss: 1.00013813e-06
Iter: 442 loss: 9.99793656e-07
Iter: 443 loss: 9.98943e-07
Iter: 444 loss: 1.01248634e-06
Iter: 445 loss: 9.9890633e-07
Iter: 446 loss: 9.98116548e-07
Iter: 447 loss: 1.00865736e-06
Iter: 448 loss: 9.9808085e-07
Iter: 449 loss: 9.97663847e-07
Iter: 450 loss: 9.97439656e-07
Iter: 451 loss: 9.97129177e-07
Iter: 452 loss: 9.96240715e-07
Iter: 453 loss: 1.00000079e-06
Iter: 454 loss: 9.96128506e-07
Iter: 455 loss: 9.96446943e-07
Iter: 456 loss: 9.95916935e-07
Iter: 457 loss: 9.95718551e-07
Iter: 458 loss: 9.9538e-07
Iter: 459 loss: 1.00222e-06
Iter: 460 loss: 9.95309847e-07
Iter: 461 loss: 9.94804e-07
Iter: 462 loss: 9.93920139e-07
Iter: 463 loss: 9.93898766e-07
Iter: 464 loss: 9.93262347e-07
Iter: 465 loss: 1.00074749e-06
Iter: 466 loss: 9.93249841e-07
Iter: 467 loss: 9.92722562e-07
Iter: 468 loss: 9.92306809e-07
Iter: 469 loss: 9.92128207e-07
Iter: 470 loss: 9.91349907e-07
Iter: 471 loss: 9.98729774e-07
Iter: 472 loss: 9.9129818e-07
Iter: 473 loss: 9.90788521e-07
Iter: 474 loss: 9.90854915e-07
Iter: 475 loss: 9.90465651e-07
Iter: 476 loss: 9.89793307e-07
Iter: 477 loss: 9.94677748e-07
Iter: 478 loss: 9.89703381e-07
Iter: 479 loss: 9.89264e-07
Iter: 480 loss: 9.90887884e-07
Iter: 481 loss: 9.89101636e-07
Iter: 482 loss: 9.8857754e-07
Iter: 483 loss: 9.89110845e-07
Iter: 484 loss: 9.88382908e-07
Iter: 485 loss: 9.87762405e-07
Iter: 486 loss: 9.90266585e-07
Iter: 487 loss: 9.87548447e-07
Iter: 488 loss: 9.87202156e-07
Iter: 489 loss: 9.88825605e-07
Iter: 490 loss: 9.87154e-07
Iter: 491 loss: 9.86744681e-07
Iter: 492 loss: 9.90647436e-07
Iter: 493 loss: 9.86773443e-07
Iter: 494 loss: 9.86494342e-07
Iter: 495 loss: 9.85812108e-07
Iter: 496 loss: 9.9062072e-07
Iter: 497 loss: 9.85626e-07
Iter: 498 loss: 9.85035513e-07
Iter: 499 loss: 9.89475e-07
Iter: 500 loss: 9.85081e-07
Iter: 501 loss: 9.84512553e-07
Iter: 502 loss: 9.84225153e-07
Iter: 503 loss: 9.83924338e-07
Iter: 504 loss: 9.83378072e-07
Iter: 505 loss: 9.89470891e-07
Iter: 506 loss: 9.83295536e-07
Iter: 507 loss: 9.82924348e-07
Iter: 508 loss: 9.82285087e-07
Iter: 509 loss: 9.82271786e-07
Iter: 510 loss: 9.81834887e-07
Iter: 511 loss: 9.81808853e-07
Iter: 512 loss: 9.81439939e-07
Iter: 513 loss: 9.81039648e-07
Iter: 514 loss: 9.81007474e-07
Iter: 515 loss: 9.80453819e-07
Iter: 516 loss: 9.8165674e-07
Iter: 517 loss: 9.8028e-07
Iter: 518 loss: 9.79868673e-07
Iter: 519 loss: 9.79981e-07
Iter: 520 loss: 9.79544666e-07
Iter: 521 loss: 9.79038646e-07
Iter: 522 loss: 9.84494136e-07
Iter: 523 loss: 9.79021252e-07
Iter: 524 loss: 9.78825e-07
Iter: 525 loss: 9.79068773e-07
Iter: 526 loss: 9.78696107e-07
Iter: 527 loss: 9.78335834e-07
Iter: 528 loss: 9.78737717e-07
Iter: 529 loss: 9.78069579e-07
Iter: 530 loss: 9.77717718e-07
Iter: 531 loss: 9.77670425e-07
Iter: 532 loss: 9.77383252e-07
Iter: 533 loss: 9.76873935e-07
Iter: 534 loss: 9.78712478e-07
Iter: 535 loss: 9.76821e-07
Iter: 536 loss: 9.76300726e-07
Iter: 537 loss: 9.75975581e-07
Iter: 538 loss: 9.75811872e-07
Iter: 539 loss: 9.75208422e-07
Iter: 540 loss: 9.76285719e-07
Iter: 541 loss: 9.74963768e-07
Iter: 542 loss: 9.74257546e-07
Iter: 543 loss: 9.78878461e-07
Iter: 544 loss: 9.74187287e-07
Iter: 545 loss: 9.73865212e-07
Iter: 546 loss: 9.74445356e-07
Iter: 547 loss: 9.73692522e-07
Iter: 548 loss: 9.73184115e-07
Iter: 549 loss: 9.7310533e-07
Iter: 550 loss: 9.72717089e-07
Iter: 551 loss: 9.72345106e-07
Iter: 552 loss: 9.72361e-07
Iter: 553 loss: 9.72015e-07
Iter: 554 loss: 9.74263799e-07
Iter: 555 loss: 9.71937766e-07
Iter: 556 loss: 9.71720283e-07
Iter: 557 loss: 9.72460839e-07
Iter: 558 loss: 9.71620125e-07
Iter: 559 loss: 9.71344434e-07
Iter: 560 loss: 9.73324518e-07
Iter: 561 loss: 9.71378654e-07
Iter: 562 loss: 9.71259396e-07
Iter: 563 loss: 9.71031682e-07
Iter: 564 loss: 9.76181695e-07
Iter: 565 loss: 9.71022246e-07
Iter: 566 loss: 9.70675842e-07
Iter: 567 loss: 9.70930614e-07
Iter: 568 loss: 9.70440169e-07
Iter: 569 loss: 9.70003725e-07
Iter: 570 loss: 9.71956183e-07
Iter: 571 loss: 9.6986173e-07
Iter: 572 loss: 9.69439498e-07
Iter: 573 loss: 9.69294888e-07
Iter: 574 loss: 9.69078428e-07
Iter: 575 loss: 9.6837482e-07
Iter: 576 loss: 9.7091015e-07
Iter: 577 loss: 9.68216682e-07
Iter: 578 loss: 9.67623691e-07
Iter: 579 loss: 9.69208941e-07
Iter: 580 loss: 9.6749659e-07
Iter: 581 loss: 9.6692861e-07
Iter: 582 loss: 9.67588e-07
Iter: 583 loss: 9.66589823e-07
Iter: 584 loss: 9.66044809e-07
Iter: 585 loss: 9.6756e-07
Iter: 586 loss: 9.65859499e-07
Iter: 587 loss: 9.65260824e-07
Iter: 588 loss: 9.68862196e-07
Iter: 589 loss: 9.65257186e-07
Iter: 590 loss: 9.65066192e-07
Iter: 591 loss: 9.64966375e-07
Iter: 592 loss: 9.64831315e-07
Iter: 593 loss: 9.64727178e-07
Iter: 594 loss: 9.64580749e-07
Iter: 595 loss: 9.64191372e-07
Iter: 596 loss: 9.63916705e-07
Iter: 597 loss: 9.63842126e-07
Iter: 598 loss: 9.63333719e-07
Iter: 599 loss: 9.64557557e-07
Iter: 600 loss: 9.63109e-07
Iter: 601 loss: 9.62766e-07
Iter: 602 loss: 9.64278115e-07
Iter: 603 loss: 9.62747663e-07
Iter: 604 loss: 9.6233714e-07
Iter: 605 loss: 9.62579406e-07
Iter: 606 loss: 9.62178774e-07
Iter: 607 loss: 9.6179815e-07
Iter: 608 loss: 9.64020842e-07
Iter: 609 loss: 9.61845899e-07
Iter: 610 loss: 9.61616934e-07
Iter: 611 loss: 9.61532692e-07
Iter: 612 loss: 9.61360229e-07
Iter: 613 loss: 9.60939815e-07
Iter: 614 loss: 9.62646482e-07
Iter: 615 loss: 9.6084716e-07
Iter: 616 loss: 9.60588181e-07
Iter: 617 loss: 9.60884449e-07
Iter: 618 loss: 9.60421e-07
Iter: 619 loss: 9.59975523e-07
Iter: 620 loss: 9.60704256e-07
Iter: 621 loss: 9.59814543e-07
Iter: 622 loss: 9.59640829e-07
Iter: 623 loss: 9.59627e-07
Iter: 624 loss: 9.59386398e-07
Iter: 625 loss: 9.59471322e-07
Iter: 626 loss: 9.59197337e-07
Iter: 627 loss: 9.58919145e-07
Iter: 628 loss: 9.59288e-07
Iter: 629 loss: 9.58814098e-07
Iter: 630 loss: 9.58518e-07
Iter: 631 loss: 9.5863129e-07
Iter: 632 loss: 9.58338205e-07
Iter: 633 loss: 9.58077862e-07
Iter: 634 loss: 9.58641067e-07
Iter: 635 loss: 9.57957582e-07
Iter: 636 loss: 9.57688599e-07
Iter: 637 loss: 9.58203259e-07
Iter: 638 loss: 9.5751011e-07
Iter: 639 loss: 9.57196789e-07
Iter: 640 loss: 9.59175395e-07
Iter: 641 loss: 9.57223847e-07
Iter: 642 loss: 9.56942586e-07
Iter: 643 loss: 9.56945541e-07
Iter: 644 loss: 9.56842541e-07
Iter: 645 loss: 9.56447934e-07
Iter: 646 loss: 9.58260216e-07
Iter: 647 loss: 9.56357326e-07
Iter: 648 loss: 9.56168833e-07
Iter: 649 loss: 9.56764097e-07
Iter: 650 loss: 9.56073791e-07
Iter: 651 loss: 9.55739779e-07
Iter: 652 loss: 9.55345513e-07
Iter: 653 loss: 9.55261271e-07
Iter: 654 loss: 9.54960115e-07
Iter: 655 loss: 9.54946472e-07
Iter: 656 loss: 9.54645657e-07
Iter: 657 loss: 9.58461555e-07
Iter: 658 loss: 9.54680445e-07
Iter: 659 loss: 9.54449774e-07
Iter: 660 loss: 9.54133384e-07
Iter: 661 loss: 9.61071e-07
Iter: 662 loss: 9.54115876e-07
Iter: 663 loss: 9.53757308e-07
Iter: 664 loss: 9.54649522e-07
Iter: 665 loss: 9.53597464e-07
Iter: 666 loss: 9.53277208e-07
Iter: 667 loss: 9.53352298e-07
Iter: 668 loss: 9.53101392e-07
Iter: 669 loss: 9.52662162e-07
Iter: 670 loss: 9.55439873e-07
Iter: 671 loss: 9.52646815e-07
Iter: 672 loss: 9.52395567e-07
Iter: 673 loss: 9.52943935e-07
Iter: 674 loss: 9.52285404e-07
Iter: 675 loss: 9.51961397e-07
Iter: 676 loss: 9.52087248e-07
Iter: 677 loss: 9.51789502e-07
Iter: 678 loss: 9.51381651e-07
Iter: 679 loss: 9.53194331e-07
Iter: 680 loss: 9.51328502e-07
Iter: 681 loss: 9.5099881e-07
Iter: 682 loss: 9.51216e-07
Iter: 683 loss: 9.50796448e-07
Iter: 684 loss: 9.50316917e-07
Iter: 685 loss: 9.51993343e-07
Iter: 686 loss: 9.50175775e-07
Iter: 687 loss: 9.49887635e-07
Iter: 688 loss: 9.51225161e-07
Iter: 689 loss: 9.49797084e-07
Iter: 690 loss: 9.49691184e-07
Iter: 691 loss: 9.49623768e-07
Iter: 692 loss: 9.49559706e-07
Iter: 693 loss: 9.49263949e-07
Iter: 694 loss: 9.5273208e-07
Iter: 695 loss: 9.49240757e-07
Iter: 696 loss: 9.48988827e-07
Iter: 697 loss: 9.50459594e-07
Iter: 698 loss: 9.48989737e-07
Iter: 699 loss: 9.48740649e-07
Iter: 700 loss: 9.48483432e-07
Iter: 701 loss: 9.48452907e-07
Iter: 702 loss: 9.48044203e-07
Iter: 703 loss: 9.49736545e-07
Iter: 704 loss: 9.47909371e-07
Iter: 705 loss: 9.4764232e-07
Iter: 706 loss: 9.48434717e-07
Iter: 707 loss: 9.4758434e-07
Iter: 708 loss: 9.47191836e-07
Iter: 709 loss: 9.47365834e-07
Iter: 710 loss: 9.46989189e-07
Iter: 711 loss: 9.46636305e-07
Iter: 712 loss: 9.50455103e-07
Iter: 713 loss: 9.46647e-07
Iter: 714 loss: 9.4642354e-07
Iter: 715 loss: 9.46251248e-07
Iter: 716 loss: 9.4619844e-07
Iter: 717 loss: 9.45878469e-07
Iter: 718 loss: 9.4835957e-07
Iter: 719 loss: 9.45789793e-07
Iter: 720 loss: 9.4559914e-07
Iter: 721 loss: 9.45878867e-07
Iter: 722 loss: 9.4548011e-07
Iter: 723 loss: 9.45392912e-07
Iter: 724 loss: 9.45309353e-07
Iter: 725 loss: 9.45197144e-07
Iter: 726 loss: 9.45006548e-07
Iter: 727 loss: 9.44971248e-07
Iter: 728 loss: 9.44812086e-07
Iter: 729 loss: 9.4512211e-07
Iter: 730 loss: 9.44669068e-07
Iter: 731 loss: 9.44466933e-07
Iter: 732 loss: 9.44094893e-07
Iter: 733 loss: 9.44121268e-07
Iter: 734 loss: 9.43675445e-07
Iter: 735 loss: 9.47172566e-07
Iter: 736 loss: 9.4366635e-07
Iter: 737 loss: 9.43366672e-07
Iter: 738 loss: 9.4462041e-07
Iter: 739 loss: 9.433532e-07
Iter: 740 loss: 9.43030386e-07
Iter: 741 loss: 9.42980307e-07
Iter: 742 loss: 9.42843258e-07
Iter: 743 loss: 9.42433758e-07
Iter: 744 loss: 9.45790475e-07
Iter: 745 loss: 9.42445126e-07
Iter: 746 loss: 9.42160568e-07
Iter: 747 loss: 9.42269935e-07
Iter: 748 loss: 9.42016811e-07
Iter: 749 loss: 9.41671e-07
Iter: 750 loss: 9.42783458e-07
Iter: 751 loss: 9.41627206e-07
Iter: 752 loss: 9.41287794e-07
Iter: 753 loss: 9.42813415e-07
Iter: 754 loss: 9.41323151e-07
Iter: 755 loss: 9.4105576e-07
Iter: 756 loss: 9.4215045e-07
Iter: 757 loss: 9.40996e-07
Iter: 758 loss: 9.40666325e-07
Iter: 759 loss: 9.41478106e-07
Iter: 760 loss: 9.40605446e-07
Iter: 761 loss: 9.40348684e-07
Iter: 762 loss: 9.4038387e-07
Iter: 763 loss: 9.40317534e-07
Iter: 764 loss: 9.40041446e-07
Iter: 765 loss: 9.39641495e-07
Iter: 766 loss: 9.39573795e-07
Iter: 767 loss: 9.3914764e-07
Iter: 768 loss: 9.43019813e-07
Iter: 769 loss: 9.39086704e-07
Iter: 770 loss: 9.38742176e-07
Iter: 771 loss: 9.39294694e-07
Iter: 772 loss: 9.38652363e-07
Iter: 773 loss: 9.3827191e-07
Iter: 774 loss: 9.39324934e-07
Iter: 775 loss: 9.38032031e-07
Iter: 776 loss: 9.37781e-07
Iter: 777 loss: 9.38751e-07
Iter: 778 loss: 9.3767909e-07
Iter: 779 loss: 9.37345305e-07
Iter: 780 loss: 9.37764753e-07
Iter: 781 loss: 9.37154255e-07
Iter: 782 loss: 9.36735148e-07
Iter: 783 loss: 9.38244114e-07
Iter: 784 loss: 9.3652551e-07
Iter: 785 loss: 9.36250672e-07
Iter: 786 loss: 9.371e-07
Iter: 787 loss: 9.36094693e-07
Iter: 788 loss: 9.35809112e-07
Iter: 789 loss: 9.3988416e-07
Iter: 790 loss: 9.35787909e-07
Iter: 791 loss: 9.35533421e-07
Iter: 792 loss: 9.37511516e-07
Iter: 793 loss: 9.35516e-07
Iter: 794 loss: 9.35391881e-07
Iter: 795 loss: 9.35134494e-07
Iter: 796 loss: 9.35167577e-07
Iter: 797 loss: 9.34873924e-07
Iter: 798 loss: 9.34910588e-07
Iter: 799 loss: 9.34621823e-07
Iter: 800 loss: 9.34236255e-07
Iter: 801 loss: 9.35634489e-07
Iter: 802 loss: 9.34164518e-07
Iter: 803 loss: 9.33899173e-07
Iter: 804 loss: 9.34543266e-07
Iter: 805 loss: 9.3372762e-07
Iter: 806 loss: 9.33307717e-07
Iter: 807 loss: 9.34872673e-07
Iter: 808 loss: 9.33194656e-07
Iter: 809 loss: 9.32989224e-07
Iter: 810 loss: 9.33991942e-07
Iter: 811 loss: 9.32902424e-07
Iter: 812 loss: 9.32628041e-07
Iter: 813 loss: 9.32415162e-07
Iter: 814 loss: 9.32329215e-07
Iter: 815 loss: 9.3189243e-07
Iter: 816 loss: 9.35858168e-07
Iter: 817 loss: 9.318926e-07
Iter: 818 loss: 9.31607531e-07
Iter: 819 loss: 9.31568934e-07
Iter: 820 loss: 9.31376121e-07
Iter: 821 loss: 9.31005388e-07
Iter: 822 loss: 9.33709e-07
Iter: 823 loss: 9.30922511e-07
Iter: 824 loss: 9.30743056e-07
Iter: 825 loss: 9.30676151e-07
Iter: 826 loss: 9.30542456e-07
Iter: 827 loss: 9.3029405e-07
Iter: 828 loss: 9.3240584e-07
Iter: 829 loss: 9.30232e-07
Iter: 830 loss: 9.29815428e-07
Iter: 831 loss: 9.30599924e-07
Iter: 832 loss: 9.29710609e-07
Iter: 833 loss: 9.29350733e-07
Iter: 834 loss: 9.31336729e-07
Iter: 835 loss: 9.29338853e-07
Iter: 836 loss: 9.29052248e-07
Iter: 837 loss: 9.28984946e-07
Iter: 838 loss: 9.28771954e-07
Iter: 839 loss: 9.28404916e-07
Iter: 840 loss: 9.30568262e-07
Iter: 841 loss: 9.28313398e-07
Iter: 842 loss: 9.27891961e-07
Iter: 843 loss: 9.29087719e-07
Iter: 844 loss: 9.27816e-07
Iter: 845 loss: 9.27561189e-07
Iter: 846 loss: 9.27312385e-07
Iter: 847 loss: 9.27156179e-07
Iter: 848 loss: 9.26739745e-07
Iter: 849 loss: 9.31368788e-07
Iter: 850 loss: 9.26691484e-07
Iter: 851 loss: 9.26470193e-07
Iter: 852 loss: 9.26630776e-07
Iter: 853 loss: 9.26329676e-07
Iter: 854 loss: 9.25888628e-07
Iter: 855 loss: 9.26967743e-07
Iter: 856 loss: 9.25790516e-07
Iter: 857 loss: 9.25899826e-07
Iter: 858 loss: 9.25687459e-07
Iter: 859 loss: 9.25622e-07
Iter: 860 loss: 9.25345716e-07
Iter: 861 loss: 9.26584107e-07
Iter: 862 loss: 9.25301947e-07
Iter: 863 loss: 9.24992889e-07
Iter: 864 loss: 9.2568331e-07
Iter: 865 loss: 9.24867152e-07
Iter: 866 loss: 9.24597e-07
Iter: 867 loss: 9.25924155e-07
Iter: 868 loss: 9.24517e-07
Iter: 869 loss: 9.24387336e-07
Iter: 870 loss: 9.24330607e-07
Iter: 871 loss: 9.2409465e-07
Iter: 872 loss: 9.23831465e-07
Iter: 873 loss: 9.25362201e-07
Iter: 874 loss: 9.23729942e-07
Iter: 875 loss: 9.23501716e-07
Iter: 876 loss: 9.24878634e-07
Iter: 877 loss: 9.23481252e-07
Iter: 878 loss: 9.23178959e-07
Iter: 879 loss: 9.22991319e-07
Iter: 880 loss: 9.22972e-07
Iter: 881 loss: 9.22582842e-07
Iter: 882 loss: 9.25222878e-07
Iter: 883 loss: 9.22585e-07
Iter: 884 loss: 9.22331537e-07
Iter: 885 loss: 9.22605e-07
Iter: 886 loss: 9.22226775e-07
Iter: 887 loss: 9.21765491e-07
Iter: 888 loss: 9.22768947e-07
Iter: 889 loss: 9.21664878e-07
Iter: 890 loss: 9.2168159e-07
Iter: 891 loss: 9.21542323e-07
Iter: 892 loss: 9.21393394e-07
Iter: 893 loss: 9.21102867e-07
Iter: 894 loss: 9.26212863e-07
Iter: 895 loss: 9.21074616e-07
Iter: 896 loss: 9.20889306e-07
Iter: 897 loss: 9.20729235e-07
Iter: 898 loss: 9.20662444e-07
Iter: 899 loss: 9.20331672e-07
Iter: 900 loss: 9.23130415e-07
Iter: 901 loss: 9.20358048e-07
Iter: 902 loss: 9.20061723e-07
Iter: 903 loss: 9.19707475e-07
Iter: 904 loss: 9.19687068e-07
Iter: 905 loss: 9.1931372e-07
Iter: 906 loss: 9.23865343e-07
Iter: 907 loss: 9.19248e-07
Iter: 908 loss: 9.18931732e-07
Iter: 909 loss: 9.19576735e-07
Iter: 910 loss: 9.1884931e-07
Iter: 911 loss: 9.18498188e-07
Iter: 912 loss: 9.19224e-07
Iter: 913 loss: 9.18337491e-07
Iter: 914 loss: 9.1807e-07
Iter: 915 loss: 9.19023421e-07
Iter: 916 loss: 9.17971761e-07
Iter: 917 loss: 9.17641273e-07
Iter: 918 loss: 9.1790605e-07
Iter: 919 loss: 9.17489274e-07
Iter: 920 loss: 9.17155148e-07
Iter: 921 loss: 9.20132948e-07
Iter: 922 loss: 9.17078353e-07
Iter: 923 loss: 9.16900149e-07
Iter: 924 loss: 9.17322097e-07
Iter: 925 loss: 9.16796182e-07
Iter: 926 loss: 9.16548686e-07
Iter: 927 loss: 9.16564034e-07
Iter: 928 loss: 9.16423232e-07
Iter: 929 loss: 9.1618881e-07
Iter: 930 loss: 9.18076466e-07
Iter: 931 loss: 9.16090755e-07
Iter: 932 loss: 9.15768112e-07
Iter: 933 loss: 9.17238e-07
Iter: 934 loss: 9.1571269e-07
Iter: 935 loss: 9.1541358e-07
Iter: 936 loss: 9.15796818e-07
Iter: 937 loss: 9.15302735e-07
Iter: 938 loss: 9.14918701e-07
Iter: 939 loss: 9.16247131e-07
Iter: 940 loss: 9.14856059e-07
Iter: 941 loss: 9.14556495e-07
Iter: 942 loss: 9.15450528e-07
Iter: 943 loss: 9.14525629e-07
Iter: 944 loss: 9.14264263e-07
Iter: 945 loss: 9.15407497e-07
Iter: 946 loss: 9.14174e-07
Iter: 947 loss: 9.13985161e-07
Iter: 948 loss: 9.14084239e-07
Iter: 949 loss: 9.13817814e-07
Iter: 950 loss: 9.13620795e-07
Iter: 951 loss: 9.14409043e-07
Iter: 952 loss: 9.13559859e-07
Iter: 953 loss: 9.13222095e-07
Iter: 954 loss: 9.14191e-07
Iter: 955 loss: 9.13155361e-07
Iter: 956 loss: 9.12946632e-07
Iter: 957 loss: 9.14034274e-07
Iter: 958 loss: 9.12906785e-07
Iter: 959 loss: 9.12863129e-07
Iter: 960 loss: 9.12860401e-07
Iter: 961 loss: 9.12779115e-07
Iter: 962 loss: 9.12612904e-07
Iter: 963 loss: 9.13649956e-07
Iter: 964 loss: 9.12554697e-07
Iter: 965 loss: 9.12333e-07
Iter: 966 loss: 9.12535597e-07
Iter: 967 loss: 9.12210737e-07
Iter: 968 loss: 9.11938798e-07
Iter: 969 loss: 9.12796736e-07
Iter: 970 loss: 9.11854897e-07
Iter: 971 loss: 9.1162542e-07
Iter: 972 loss: 9.12654855e-07
Iter: 973 loss: 9.11550956e-07
Iter: 974 loss: 9.11281177e-07
Iter: 975 loss: 9.11549137e-07
Iter: 976 loss: 9.11123493e-07
Iter: 977 loss: 9.10848598e-07
Iter: 978 loss: 9.12686062e-07
Iter: 979 loss: 9.10894244e-07
Iter: 980 loss: 9.10664312e-07
Iter: 981 loss: 9.11253835e-07
Iter: 982 loss: 9.10680455e-07
Iter: 983 loss: 9.10438757e-07
Iter: 984 loss: 9.1048031e-07
Iter: 985 loss: 9.1032274e-07
Iter: 986 loss: 9.10093149e-07
Iter: 987 loss: 9.10567e-07
Iter: 988 loss: 9.09983441e-07
Iter: 989 loss: 9.09741857e-07
Iter: 990 loss: 9.11091831e-07
Iter: 991 loss: 9.0970758e-07
Iter: 992 loss: 9.09609639e-07
Iter: 993 loss: 9.09629421e-07
Iter: 994 loss: 9.09556775e-07
Iter: 995 loss: 9.09290634e-07
Iter: 996 loss: 9.09323148e-07
Iter: 997 loss: 9.09088385e-07
Iter: 998 loss: 9.08862e-07
Iter: 999 loss: 9.08803258e-07
Iter: 1000 loss: 9.08568154e-07
Iter: 1001 loss: 9.09489e-07
Iter: 1002 loss: 9.08477375e-07
Iter: 1003 loss: 9.08249945e-07
Iter: 1004 loss: 9.0928927e-07
Iter: 1005 loss: 9.08145125e-07
Iter: 1006 loss: 9.07835e-07
Iter: 1007 loss: 9.08628408e-07
Iter: 1008 loss: 9.07826461e-07
Iter: 1009 loss: 9.07420144e-07
Iter: 1010 loss: 9.07731305e-07
Iter: 1011 loss: 9.07205276e-07
Iter: 1012 loss: 9.06770879e-07
Iter: 1013 loss: 9.08222319e-07
Iter: 1014 loss: 9.06642697e-07
Iter: 1015 loss: 9.06276455e-07
Iter: 1016 loss: 9.07980279e-07
Iter: 1017 loss: 9.06234732e-07
Iter: 1018 loss: 9.05929255e-07
Iter: 1019 loss: 9.05905324e-07
Iter: 1020 loss: 9.05742354e-07
Iter: 1021 loss: 9.05323645e-07
Iter: 1022 loss: 9.07672188e-07
Iter: 1023 loss: 9.05281468e-07
Iter: 1024 loss: 9.05121453e-07
Iter: 1025 loss: 9.05125603e-07
Iter: 1026 loss: 9.04868e-07
Iter: 1027 loss: 9.04966555e-07
Iter: 1028 loss: 9.04736e-07
Iter: 1029 loss: 9.04573653e-07
Iter: 1030 loss: 9.04354e-07
Iter: 1031 loss: 9.04296314e-07
Iter: 1032 loss: 9.03988962e-07
Iter: 1033 loss: 9.0437004e-07
Iter: 1034 loss: 9.03822922e-07
Iter: 1035 loss: 9.03374598e-07
Iter: 1036 loss: 9.05320235e-07
Iter: 1037 loss: 9.03356636e-07
Iter: 1038 loss: 9.02963848e-07
Iter: 1039 loss: 9.03812122e-07
Iter: 1040 loss: 9.02823956e-07
Iter: 1041 loss: 9.02476927e-07
Iter: 1042 loss: 9.04255558e-07
Iter: 1043 loss: 9.02409397e-07
Iter: 1044 loss: 9.02134047e-07
Iter: 1045 loss: 9.03427804e-07
Iter: 1046 loss: 9.02065608e-07
Iter: 1047 loss: 9.01819419e-07
Iter: 1048 loss: 9.01987846e-07
Iter: 1049 loss: 9.01701469e-07
Iter: 1050 loss: 9.01319879e-07
Iter: 1051 loss: 9.01857902e-07
Iter: 1052 loss: 9.01192209e-07
Iter: 1053 loss: 9.0087326e-07
Iter: 1054 loss: 9.03866066e-07
Iter: 1055 loss: 9.0085689e-07
Iter: 1056 loss: 9.00629e-07
Iter: 1057 loss: 9.00311306e-07
Iter: 1058 loss: 9.00303348e-07
Iter: 1059 loss: 9.00048747e-07
Iter: 1060 loss: 9.00021519e-07
Iter: 1061 loss: 8.99896349e-07
Iter: 1062 loss: 8.99899476e-07
Iter: 1063 loss: 8.99763165e-07
Iter: 1064 loss: 8.99518568e-07
Iter: 1065 loss: 9.01055841e-07
Iter: 1066 loss: 8.99436884e-07
Iter: 1067 loss: 8.99087581e-07
Iter: 1068 loss: 8.99471729e-07
Iter: 1069 loss: 8.98849e-07
Iter: 1070 loss: 8.98494704e-07
Iter: 1071 loss: 8.9950538e-07
Iter: 1072 loss: 8.98366693e-07
Iter: 1073 loss: 8.98000451e-07
Iter: 1074 loss: 8.99095767e-07
Iter: 1075 loss: 8.9778689e-07
Iter: 1076 loss: 8.97470045e-07
Iter: 1077 loss: 8.98687176e-07
Iter: 1078 loss: 8.97386371e-07
Iter: 1079 loss: 8.97098744e-07
Iter: 1080 loss: 8.97845894e-07
Iter: 1081 loss: 8.96939184e-07
Iter: 1082 loss: 8.96664687e-07
Iter: 1083 loss: 8.98378858e-07
Iter: 1084 loss: 8.96606707e-07
Iter: 1085 loss: 8.9632465e-07
Iter: 1086 loss: 8.97242899e-07
Iter: 1087 loss: 8.96256665e-07
Iter: 1088 loss: 8.95943401e-07
Iter: 1089 loss: 8.9564395e-07
Iter: 1090 loss: 8.95617859e-07
Iter: 1091 loss: 8.95194603e-07
Iter: 1092 loss: 8.97622272e-07
Iter: 1093 loss: 8.95227402e-07
Iter: 1094 loss: 8.95118546e-07
Iter: 1095 loss: 8.95014352e-07
Iter: 1096 loss: 8.94915e-07
Iter: 1097 loss: 8.94511174e-07
Iter: 1098 loss: 8.992983e-07
Iter: 1099 loss: 8.94470134e-07
Iter: 1100 loss: 8.94168e-07
Iter: 1101 loss: 8.95367748e-07
Iter: 1102 loss: 8.9410014e-07
Iter: 1103 loss: 8.93827462e-07
Iter: 1104 loss: 8.93558763e-07
Iter: 1105 loss: 8.93498225e-07
Iter: 1106 loss: 8.93141419e-07
Iter: 1107 loss: 8.97100733e-07
Iter: 1108 loss: 8.93114816e-07
Iter: 1109 loss: 8.92973333e-07
Iter: 1110 loss: 8.93049446e-07
Iter: 1111 loss: 8.92709636e-07
Iter: 1112 loss: 8.92432922e-07
Iter: 1113 loss: 8.93723495e-07
Iter: 1114 loss: 8.92424055e-07
Iter: 1115 loss: 8.92202e-07
Iter: 1116 loss: 8.92497496e-07
Iter: 1117 loss: 8.92053038e-07
Iter: 1118 loss: 8.91767968e-07
Iter: 1119 loss: 8.93095773e-07
Iter: 1120 loss: 8.91735226e-07
Iter: 1121 loss: 8.91532409e-07
Iter: 1122 loss: 8.91926163e-07
Iter: 1123 loss: 8.91430147e-07
Iter: 1124 loss: 8.91157129e-07
Iter: 1125 loss: 8.92039395e-07
Iter: 1126 loss: 8.91152808e-07
Iter: 1127 loss: 8.90863816e-07
Iter: 1128 loss: 8.92229423e-07
Iter: 1129 loss: 8.90791398e-07
Iter: 1130 loss: 8.90504339e-07
Iter: 1131 loss: 8.92722596e-07
Iter: 1132 loss: 8.90485126e-07
Iter: 1133 loss: 8.90435672e-07
Iter: 1134 loss: 8.90170327e-07
Iter: 1135 loss: 8.91874663e-07
Iter: 1136 loss: 8.90093531e-07
Iter: 1137 loss: 8.89725243e-07
Iter: 1138 loss: 8.91216359e-07
Iter: 1139 loss: 8.89679143e-07
Iter: 1140 loss: 8.89354169e-07
Iter: 1141 loss: 8.90119168e-07
Iter: 1142 loss: 8.89210469e-07
Iter: 1143 loss: 8.88894419e-07
Iter: 1144 loss: 8.88835473e-07
Iter: 1145 loss: 8.88595764e-07
Iter: 1146 loss: 8.88201498e-07
Iter: 1147 loss: 8.92467085e-07
Iter: 1148 loss: 8.88191039e-07
Iter: 1149 loss: 8.87902956e-07
Iter: 1150 loss: 8.87795352e-07
Iter: 1151 loss: 8.87646593e-07
Iter: 1152 loss: 8.87266481e-07
Iter: 1153 loss: 8.89661862e-07
Iter: 1154 loss: 8.8722561e-07
Iter: 1155 loss: 8.86865621e-07
Iter: 1156 loss: 8.8789966e-07
Iter: 1157 loss: 8.86752105e-07
Iter: 1158 loss: 8.86522912e-07
Iter: 1159 loss: 8.88022896e-07
Iter: 1160 loss: 8.86455439e-07
Iter: 1161 loss: 8.86226303e-07
Iter: 1162 loss: 8.87498231e-07
Iter: 1163 loss: 8.8621141e-07
Iter: 1164 loss: 8.86067824e-07
Iter: 1165 loss: 8.88365e-07
Iter: 1166 loss: 8.8607112e-07
Iter: 1167 loss: 8.85977272e-07
Iter: 1168 loss: 8.85747681e-07
Iter: 1169 loss: 8.87614249e-07
Iter: 1170 loss: 8.85681402e-07
Iter: 1171 loss: 8.85351085e-07
Iter: 1172 loss: 8.86049406e-07
Iter: 1173 loss: 8.85216707e-07
Iter: 1174 loss: 8.85008944e-07
Iter: 1175 loss: 8.86233863e-07
Iter: 1176 loss: 8.84989277e-07
Iter: 1177 loss: 8.84673568e-07
Iter: 1178 loss: 8.84718474e-07
Iter: 1179 loss: 8.84494909e-07
Iter: 1180 loss: 8.84265887e-07
Iter: 1181 loss: 8.85809868e-07
Iter: 1182 loss: 8.84203814e-07
Iter: 1183 loss: 8.83909138e-07
Iter: 1184 loss: 8.8400094e-07
Iter: 1185 loss: 8.83714733e-07
Iter: 1186 loss: 8.83442283e-07
Iter: 1187 loss: 8.85622569e-07
Iter: 1188 loss: 8.83398968e-07
Iter: 1189 loss: 8.83157213e-07
Iter: 1190 loss: 8.83176085e-07
Iter: 1191 loss: 8.83001121e-07
Iter: 1192 loss: 8.82542565e-07
Iter: 1193 loss: 8.84528276e-07
Iter: 1194 loss: 8.82535e-07
Iter: 1195 loss: 8.82337531e-07
Iter: 1196 loss: 8.82327299e-07
Iter: 1197 loss: 8.82251697e-07
Iter: 1198 loss: 8.83852749e-07
Iter: 1199 loss: 8.82210884e-07
Iter: 1200 loss: 8.82099187e-07
Iter: 1201 loss: 8.81851804e-07
Iter: 1202 loss: 8.86452881e-07
Iter: 1203 loss: 8.8183765e-07
Iter: 1204 loss: 8.81625056e-07
Iter: 1205 loss: 8.81642e-07
Iter: 1206 loss: 8.81495623e-07
Iter: 1207 loss: 8.81274e-07
Iter: 1208 loss: 8.82290692e-07
Iter: 1209 loss: 8.81143592e-07
Iter: 1210 loss: 8.80916616e-07
Iter: 1211 loss: 8.81765686e-07
Iter: 1212 loss: 8.80822085e-07
Iter: 1213 loss: 8.80617904e-07
Iter: 1214 loss: 8.8099614e-07
Iter: 1215 loss: 8.80504217e-07
Iter: 1216 loss: 8.80301968e-07
Iter: 1217 loss: 8.8045374e-07
Iter: 1218 loss: 8.80093808e-07
Iter: 1219 loss: 8.79753543e-07
Iter: 1220 loss: 8.81761935e-07
Iter: 1221 loss: 8.79728418e-07
Iter: 1222 loss: 8.79504796e-07
Iter: 1223 loss: 8.79703407e-07
Iter: 1224 loss: 8.79363711e-07
Iter: 1225 loss: 8.79109621e-07
Iter: 1226 loss: 8.80704135e-07
Iter: 1227 loss: 8.79023787e-07
Iter: 1228 loss: 8.78793458e-07
Iter: 1229 loss: 8.79733818e-07
Iter: 1230 loss: 8.78685455e-07
Iter: 1231 loss: 8.78574838e-07
Iter: 1232 loss: 8.78559376e-07
Iter: 1233 loss: 8.78465698e-07
Iter: 1234 loss: 8.78417e-07
Iter: 1235 loss: 8.78374351e-07
Iter: 1236 loss: 8.78152093e-07
Iter: 1237 loss: 8.77976e-07
Iter: 1238 loss: 8.7796758e-07
Iter: 1239 loss: 8.77807281e-07
Iter: 1240 loss: 8.78135666e-07
Iter: 1241 loss: 8.77669663e-07
Iter: 1242 loss: 8.7753142e-07
Iter: 1243 loss: 8.78987066e-07
Iter: 1244 loss: 8.77488219e-07
Iter: 1245 loss: 8.77286652e-07
Iter: 1246 loss: 8.7739852e-07
Iter: 1247 loss: 8.7716154e-07
Iter: 1248 loss: 8.76922115e-07
Iter: 1249 loss: 8.77151365e-07
Iter: 1250 loss: 8.76783e-07
Iter: 1251 loss: 8.76496642e-07
Iter: 1252 loss: 8.77520563e-07
Iter: 1253 loss: 8.76499712e-07
Iter: 1254 loss: 8.76169111e-07
Iter: 1255 loss: 8.76682122e-07
Iter: 1256 loss: 8.76011427e-07
Iter: 1257 loss: 8.7579491e-07
Iter: 1258 loss: 8.77574848e-07
Iter: 1259 loss: 8.75744945e-07
Iter: 1260 loss: 8.75540763e-07
Iter: 1261 loss: 8.75942249e-07
Iter: 1262 loss: 8.75470107e-07
Iter: 1263 loss: 8.75318051e-07
Iter: 1264 loss: 8.75297815e-07
Iter: 1265 loss: 8.75185776e-07
Iter: 1266 loss: 8.75367959e-07
Iter: 1267 loss: 8.75179808e-07
Iter: 1268 loss: 8.75098351e-07
Iter: 1269 loss: 8.74936177e-07
Iter: 1270 loss: 8.78360765e-07
Iter: 1271 loss: 8.74897466e-07
Iter: 1272 loss: 8.74741204e-07
Iter: 1273 loss: 8.74959539e-07
Iter: 1274 loss: 8.74607849e-07
Iter: 1275 loss: 8.74412763e-07
Iter: 1276 loss: 8.75767455e-07
Iter: 1277 loss: 8.74345119e-07
Iter: 1278 loss: 8.74137e-07
Iter: 1279 loss: 8.7449348e-07
Iter: 1280 loss: 8.7412559e-07
Iter: 1281 loss: 8.73912597e-07
Iter: 1282 loss: 8.74692432e-07
Iter: 1283 loss: 8.7386843e-07
Iter: 1284 loss: 8.73725639e-07
Iter: 1285 loss: 8.73712111e-07
Iter: 1286 loss: 8.73597912e-07
Iter: 1287 loss: 8.73361273e-07
Iter: 1288 loss: 8.73819147e-07
Iter: 1289 loss: 8.73232921e-07
Iter: 1290 loss: 8.73031354e-07
Iter: 1291 loss: 8.75234377e-07
Iter: 1292 loss: 8.73036697e-07
Iter: 1293 loss: 8.72880207e-07
Iter: 1294 loss: 8.72857413e-07
Iter: 1295 loss: 8.72702685e-07
Iter: 1296 loss: 8.72730141e-07
Iter: 1297 loss: 8.72613e-07
Iter: 1298 loss: 8.72562055e-07
Iter: 1299 loss: 8.72782493e-07
Iter: 1300 loss: 8.72490205e-07
Iter: 1301 loss: 8.72462e-07
Iter: 1302 loss: 8.7234946e-07
Iter: 1303 loss: 8.74683337e-07
Iter: 1304 loss: 8.72332e-07
Iter: 1305 loss: 8.72174098e-07
Iter: 1306 loss: 8.72510896e-07
Iter: 1307 loss: 8.72094176e-07
Iter: 1308 loss: 8.71964744e-07
Iter: 1309 loss: 8.72119301e-07
Iter: 1310 loss: 8.71879479e-07
Iter: 1311 loss: 8.7169991e-07
Iter: 1312 loss: 8.72143801e-07
Iter: 1313 loss: 8.71640168e-07
Iter: 1314 loss: 8.71385055e-07
Iter: 1315 loss: 8.72144881e-07
Iter: 1316 loss: 8.71330371e-07
Iter: 1317 loss: 8.7108549e-07
Iter: 1318 loss: 8.71265797e-07
Iter: 1319 loss: 8.71006648e-07
Iter: 1320 loss: 8.70727149e-07
Iter: 1321 loss: 8.718348e-07
Iter: 1322 loss: 8.70637678e-07
Iter: 1323 loss: 8.70446797e-07
Iter: 1324 loss: 8.71011e-07
Iter: 1325 loss: 8.70387908e-07
Iter: 1326 loss: 8.70177871e-07
Iter: 1327 loss: 8.70582e-07
Iter: 1328 loss: 8.7007686e-07
Iter: 1329 loss: 8.69985683e-07
Iter: 1330 loss: 8.69983694e-07
Iter: 1331 loss: 8.69835958e-07
Iter: 1332 loss: 8.70890858e-07
Iter: 1333 loss: 8.69838e-07
Iter: 1334 loss: 8.6974e-07
Iter: 1335 loss: 8.69644396e-07
Iter: 1336 loss: 8.71169959e-07
Iter: 1337 loss: 8.69539292e-07
Iter: 1338 loss: 8.6932846e-07
Iter: 1339 loss: 8.69981761e-07
Iter: 1340 loss: 8.69258656e-07
Iter: 1341 loss: 8.69063683e-07
Iter: 1342 loss: 8.69358644e-07
Iter: 1343 loss: 8.68971881e-07
Iter: 1344 loss: 8.68805955e-07
Iter: 1345 loss: 8.69332098e-07
Iter: 1346 loss: 8.68745531e-07
Iter: 1347 loss: 8.68583697e-07
Iter: 1348 loss: 8.70010581e-07
Iter: 1349 loss: 8.6854709e-07
Iter: 1350 loss: 8.68403845e-07
Iter: 1351 loss: 8.68291465e-07
Iter: 1352 loss: 8.68228938e-07
Iter: 1353 loss: 8.67967799e-07
Iter: 1354 loss: 8.68515372e-07
Iter: 1355 loss: 8.67838423e-07
Iter: 1356 loss: 8.67617473e-07
Iter: 1357 loss: 8.67680967e-07
Iter: 1358 loss: 8.67402719e-07
Iter: 1359 loss: 8.66952519e-07
Iter: 1360 loss: 8.70295423e-07
Iter: 1361 loss: 8.66974744e-07
Iter: 1362 loss: 8.66748792e-07
Iter: 1363 loss: 8.68227e-07
Iter: 1364 loss: 8.66727646e-07
Iter: 1365 loss: 8.66612766e-07
Iter: 1366 loss: 8.66599919e-07
Iter: 1367 loss: 8.66530797e-07
Iter: 1368 loss: 8.6639659e-07
Iter: 1369 loss: 8.69175949e-07
Iter: 1370 loss: 8.66320704e-07
Iter: 1371 loss: 8.66151879e-07
Iter: 1372 loss: 8.66637947e-07
Iter: 1373 loss: 8.66176038e-07
Iter: 1374 loss: 8.66000278e-07
Iter: 1375 loss: 8.65777224e-07
Iter: 1376 loss: 8.6575028e-07
Iter: 1377 loss: 8.65467655e-07
Iter: 1378 loss: 8.67388394e-07
Iter: 1379 loss: 8.65418087e-07
Iter: 1380 loss: 8.65255117e-07
Iter: 1381 loss: 8.65673314e-07
Iter: 1382 loss: 8.65189918e-07
Iter: 1383 loss: 8.64918263e-07
Iter: 1384 loss: 8.65920242e-07
Iter: 1385 loss: 8.64859601e-07
Iter: 1386 loss: 8.64698e-07
Iter: 1387 loss: 8.65273705e-07
Iter: 1388 loss: 8.6468242e-07
Iter: 1389 loss: 8.64488584e-07
Iter: 1390 loss: 8.64416279e-07
Iter: 1391 loss: 8.6429975e-07
Iter: 1392 loss: 8.64060098e-07
Iter: 1393 loss: 8.65491643e-07
Iter: 1394 loss: 8.64073172e-07
Iter: 1395 loss: 8.63847276e-07
Iter: 1396 loss: 8.64338404e-07
Iter: 1397 loss: 8.63760533e-07
Iter: 1398 loss: 8.63747e-07
Iter: 1399 loss: 8.6370369e-07
Iter: 1400 loss: 8.63521564e-07
Iter: 1401 loss: 8.63518096e-07
Iter: 1402 loss: 8.6346455e-07
Iter: 1403 loss: 8.63386504e-07
Iter: 1404 loss: 8.63430046e-07
Iter: 1405 loss: 8.63255877e-07
Iter: 1406 loss: 8.6307989e-07
Iter: 1407 loss: 8.63151399e-07
Iter: 1408 loss: 8.62974048e-07
Iter: 1409 loss: 8.6271217e-07
Iter: 1410 loss: 8.63578464e-07
Iter: 1411 loss: 8.6265959e-07
Iter: 1412 loss: 8.62471609e-07
Iter: 1413 loss: 8.62578815e-07
Iter: 1414 loss: 8.62340471e-07
Iter: 1415 loss: 8.62118441e-07
Iter: 1416 loss: 8.63757123e-07
Iter: 1417 loss: 8.62052048e-07
Iter: 1418 loss: 8.61815579e-07
Iter: 1419 loss: 8.62688353e-07
Iter: 1420 loss: 8.61811031e-07
Iter: 1421 loss: 8.61683e-07
Iter: 1422 loss: 8.61537956e-07
Iter: 1423 loss: 8.61509534e-07
Iter: 1424 loss: 8.61271815e-07
Iter: 1425 loss: 8.62685397e-07
Iter: 1426 loss: 8.6119195e-07
Iter: 1427 loss: 8.61011927e-07
Iter: 1428 loss: 8.61007607e-07
Iter: 1429 loss: 8.6082224e-07
Iter: 1430 loss: 8.60675868e-07
Iter: 1431 loss: 8.60654e-07
Iter: 1432 loss: 8.60553314e-07
Iter: 1433 loss: 8.61067292e-07
Iter: 1434 loss: 8.6045668e-07
Iter: 1435 loss: 8.60310877e-07
Iter: 1436 loss: 8.60282967e-07
Iter: 1437 loss: 8.60253692e-07
Iter: 1438 loss: 8.60149157e-07
Iter: 1439 loss: 8.6013489e-07
Iter: 1440 loss: 8.59996476e-07
Iter: 1441 loss: 8.59816964e-07
Iter: 1442 loss: 8.60851e-07
Iter: 1443 loss: 8.59745398e-07
Iter: 1444 loss: 8.59507281e-07
Iter: 1445 loss: 8.59613237e-07
Iter: 1446 loss: 8.59445663e-07
Iter: 1447 loss: 8.59084821e-07
Iter: 1448 loss: 8.60740442e-07
Iter: 1449 loss: 8.5914121e-07
Iter: 1450 loss: 8.58902126e-07
Iter: 1451 loss: 8.59436454e-07
Iter: 1452 loss: 8.58775252e-07
Iter: 1453 loss: 8.58517467e-07
Iter: 1454 loss: 8.58881776e-07
Iter: 1455 loss: 8.58449198e-07
Iter: 1456 loss: 8.58208864e-07
Iter: 1457 loss: 8.58540886e-07
Iter: 1458 loss: 8.58110411e-07
Iter: 1459 loss: 8.57876159e-07
Iter: 1460 loss: 8.58388717e-07
Iter: 1461 loss: 8.57750308e-07
Iter: 1462 loss: 8.57600241e-07
Iter: 1463 loss: 8.57562554e-07
Iter: 1464 loss: 8.57451539e-07
Iter: 1465 loss: 8.59095394e-07
Iter: 1466 loss: 8.57446878e-07
Iter: 1467 loss: 8.57313239e-07
Iter: 1468 loss: 8.57161353e-07
Iter: 1469 loss: 8.59352554e-07
Iter: 1470 loss: 8.57140776e-07
Iter: 1471 loss: 8.5693307e-07
Iter: 1472 loss: 8.57781174e-07
Iter: 1473 loss: 8.5680648e-07
Iter: 1474 loss: 8.56699216e-07
Iter: 1475 loss: 8.57021632e-07
Iter: 1476 loss: 8.56588e-07
Iter: 1477 loss: 8.56365091e-07
Iter: 1478 loss: 8.56518568e-07
Iter: 1479 loss: 8.56233441e-07
Iter: 1480 loss: 8.56024883e-07
Iter: 1481 loss: 8.57524867e-07
Iter: 1482 loss: 8.56001918e-07
Iter: 1483 loss: 8.55877829e-07
Iter: 1484 loss: 8.56076781e-07
Iter: 1485 loss: 8.55792109e-07
Iter: 1486 loss: 8.55582584e-07
Iter: 1487 loss: 8.56188933e-07
Iter: 1488 loss: 8.55525798e-07
Iter: 1489 loss: 8.55335713e-07
Iter: 1490 loss: 8.55721169e-07
Iter: 1491 loss: 8.55279495e-07
Iter: 1492 loss: 8.55089866e-07
Iter: 1493 loss: 8.55180815e-07
Iter: 1494 loss: 8.54975383e-07
Iter: 1495 loss: 8.54767336e-07
Iter: 1496 loss: 8.55737255e-07
Iter: 1497 loss: 8.54763243e-07
Iter: 1498 loss: 8.54685084e-07
Iter: 1499 loss: 8.54624034e-07
Iter: 1500 loss: 8.54580435e-07
Iter: 1501 loss: 8.54372729e-07
Iter: 1502 loss: 8.55671885e-07
Iter: 1503 loss: 8.54316738e-07
Iter: 1504 loss: 8.5418543e-07
Iter: 1505 loss: 8.55077189e-07
Iter: 1506 loss: 8.54074415e-07
Iter: 1507 loss: 8.53865515e-07
Iter: 1508 loss: 8.54035818e-07
Iter: 1509 loss: 8.53758081e-07
Iter: 1510 loss: 8.53507231e-07
Iter: 1511 loss: 8.54422069e-07
Iter: 1512 loss: 8.53451638e-07
Iter: 1513 loss: 8.53296569e-07
Iter: 1514 loss: 8.53533152e-07
Iter: 1515 loss: 8.53109839e-07
Iter: 1516 loss: 8.52884341e-07
Iter: 1517 loss: 8.53280483e-07
Iter: 1518 loss: 8.52794699e-07
Iter: 1519 loss: 8.52537767e-07
Iter: 1520 loss: 8.54200664e-07
Iter: 1521 loss: 8.52550386e-07
Iter: 1522 loss: 8.52312837e-07
Iter: 1523 loss: 8.52209041e-07
Iter: 1524 loss: 8.5214117e-07
Iter: 1525 loss: 8.51907657e-07
Iter: 1526 loss: 8.53312372e-07
Iter: 1527 loss: 8.51815798e-07
Iter: 1528 loss: 8.51607069e-07
Iter: 1529 loss: 8.5195154e-07
Iter: 1530 loss: 8.51565801e-07
Iter: 1531 loss: 8.51498e-07
Iter: 1532 loss: 8.51471782e-07
Iter: 1533 loss: 8.51373329e-07
Iter: 1534 loss: 8.51215589e-07
Iter: 1535 loss: 8.55383746e-07
Iter: 1536 loss: 8.51179038e-07
Iter: 1537 loss: 8.50997253e-07
Iter: 1538 loss: 8.51174491e-07
Iter: 1539 loss: 8.50926881e-07
Iter: 1540 loss: 8.5067262e-07
Iter: 1541 loss: 8.50732e-07
Iter: 1542 loss: 8.50480092e-07
Iter: 1543 loss: 8.50201275e-07
Iter: 1544 loss: 8.51820062e-07
Iter: 1545 loss: 8.50122888e-07
Iter: 1546 loss: 8.49920525e-07
Iter: 1547 loss: 8.50301262e-07
Iter: 1548 loss: 8.49811499e-07
Iter: 1549 loss: 8.4956082e-07
Iter: 1550 loss: 8.50235722e-07
Iter: 1551 loss: 8.4949545e-07
Iter: 1552 loss: 8.49348851e-07
Iter: 1553 loss: 8.50496406e-07
Iter: 1554 loss: 8.49278e-07
Iter: 1555 loss: 8.49110847e-07
Iter: 1556 loss: 8.49195033e-07
Iter: 1557 loss: 8.48943671e-07
Iter: 1558 loss: 8.4873335e-07
Iter: 1559 loss: 8.4974431e-07
Iter: 1560 loss: 8.48686113e-07
Iter: 1561 loss: 8.48488753e-07
Iter: 1562 loss: 8.48631885e-07
Iter: 1563 loss: 8.48338971e-07
Iter: 1564 loss: 8.4816304e-07
Iter: 1565 loss: 8.51000323e-07
Iter: 1566 loss: 8.4812325e-07
Iter: 1567 loss: 8.47927e-07
Iter: 1568 loss: 8.49544904e-07
Iter: 1569 loss: 8.47932313e-07
Iter: 1570 loss: 8.47843523e-07
Iter: 1571 loss: 8.47747742e-07
Iter: 1572 loss: 8.49171897e-07
Iter: 1573 loss: 8.47741831e-07
Iter: 1574 loss: 8.47595857e-07
Iter: 1575 loss: 8.47990634e-07
Iter: 1576 loss: 8.47530373e-07
Iter: 1577 loss: 8.47353306e-07
Iter: 1578 loss: 8.48230968e-07
Iter: 1579 loss: 8.47317779e-07
Iter: 1580 loss: 8.47139461e-07
Iter: 1581 loss: 8.47127865e-07
Iter: 1582 loss: 8.47058061e-07
Iter: 1583 loss: 8.46839896e-07
Iter: 1584 loss: 8.48134448e-07
Iter: 1585 loss: 8.4680147e-07
Iter: 1586 loss: 8.46659702e-07
Iter: 1587 loss: 8.46657144e-07
Iter: 1588 loss: 8.46565399e-07
Iter: 1589 loss: 8.4632029e-07
Iter: 1590 loss: 8.47760418e-07
Iter: 1591 loss: 8.46278851e-07
Iter: 1592 loss: 8.4610565e-07
Iter: 1593 loss: 8.4608655e-07
Iter: 1594 loss: 8.46024818e-07
Iter: 1595 loss: 8.45856391e-07
Iter: 1596 loss: 8.4721546e-07
Iter: 1597 loss: 8.45844852e-07
Iter: 1598 loss: 8.45638795e-07
Iter: 1599 loss: 8.46132878e-07
Iter: 1600 loss: 8.45566205e-07
Iter: 1601 loss: 8.45618047e-07
Iter: 1602 loss: 8.45507316e-07
Iter: 1603 loss: 8.45436603e-07
Iter: 1604 loss: 8.45353384e-07
Iter: 1605 loss: 8.4624503e-07
Iter: 1606 loss: 8.45324678e-07
Iter: 1607 loss: 8.45245665e-07
Iter: 1608 loss: 8.45094291e-07
Iter: 1609 loss: 8.45063653e-07
Iter: 1610 loss: 8.44806891e-07
Iter: 1611 loss: 8.45724799e-07
Iter: 1612 loss: 8.4474874e-07
Iter: 1613 loss: 8.44499596e-07
Iter: 1614 loss: 8.46109629e-07
Iter: 1615 loss: 8.44423766e-07
Iter: 1616 loss: 8.44366923e-07
Iter: 1617 loss: 8.44397846e-07
Iter: 1618 loss: 8.44267447e-07
Iter: 1619 loss: 8.43994144e-07
Iter: 1620 loss: 8.44028364e-07
Iter: 1621 loss: 8.43873863e-07
Iter: 1622 loss: 8.43567136e-07
Iter: 1623 loss: 8.46204784e-07
Iter: 1624 loss: 8.43606585e-07
Iter: 1625 loss: 8.4335386e-07
Iter: 1626 loss: 8.43252906e-07
Iter: 1627 loss: 8.43178384e-07
Iter: 1628 loss: 8.4289195e-07
Iter: 1629 loss: 8.45226964e-07
Iter: 1630 loss: 8.42885868e-07
Iter: 1631 loss: 8.42660484e-07
Iter: 1632 loss: 8.42915938e-07
Iter: 1633 loss: 8.42565669e-07
Iter: 1634 loss: 8.42416398e-07
Iter: 1635 loss: 8.44039221e-07
Iter: 1636 loss: 8.42405825e-07
Iter: 1637 loss: 8.42308964e-07
Iter: 1638 loss: 8.42277927e-07
Iter: 1639 loss: 8.42178e-07
Iter: 1640 loss: 8.41890937e-07
Iter: 1641 loss: 8.45385046e-07
Iter: 1642 loss: 8.41937776e-07
Iter: 1643 loss: 8.41670101e-07
Iter: 1644 loss: 8.41946758e-07
Iter: 1645 loss: 8.41464839e-07
Iter: 1646 loss: 8.41312499e-07
Iter: 1647 loss: 8.42042368e-07
Iter: 1648 loss: 8.41339215e-07
Iter: 1649 loss: 8.41111671e-07
Iter: 1650 loss: 8.4155397e-07
Iter: 1651 loss: 8.41050166e-07
Iter: 1652 loss: 8.40833309e-07
Iter: 1653 loss: 8.42295037e-07
Iter: 1654 loss: 8.40823077e-07
Iter: 1655 loss: 8.40720588e-07
Iter: 1656 loss: 8.4069552e-07
Iter: 1657 loss: 8.40581379e-07
Iter: 1658 loss: 8.40428584e-07
Iter: 1659 loss: 8.40671305e-07
Iter: 1660 loss: 8.40344057e-07
Iter: 1661 loss: 8.40108441e-07
Iter: 1662 loss: 8.4090766e-07
Iter: 1663 loss: 8.40062341e-07
Iter: 1664 loss: 8.39872257e-07
Iter: 1665 loss: 8.40304892e-07
Iter: 1666 loss: 8.39708377e-07
Iter: 1667 loss: 8.39637778e-07
Iter: 1668 loss: 8.41206713e-07
Iter: 1669 loss: 8.39623851e-07
Iter: 1670 loss: 8.39460938e-07
Iter: 1671 loss: 8.39245558e-07
Iter: 1672 loss: 8.39266647e-07
Iter: 1673 loss: 8.39048653e-07
Iter: 1674 loss: 8.42005193e-07
Iter: 1675 loss: 8.39050699e-07
Iter: 1676 loss: 8.39010454e-07
Iter: 1677 loss: 8.38969754e-07
Iter: 1678 loss: 8.38917515e-07
Iter: 1679 loss: 8.38703784e-07
Iter: 1680 loss: 8.40226392e-07
Iter: 1681 loss: 8.38643132e-07
Iter: 1682 loss: 8.38440144e-07
Iter: 1683 loss: 8.39241295e-07
Iter: 1684 loss: 8.38394044e-07
Iter: 1685 loss: 8.38200947e-07
Iter: 1686 loss: 8.38564461e-07
Iter: 1687 loss: 8.38149674e-07
Iter: 1688 loss: 8.37969708e-07
Iter: 1689 loss: 8.38407516e-07
Iter: 1690 loss: 8.3786847e-07
Iter: 1691 loss: 8.37727498e-07
Iter: 1692 loss: 8.39076733e-07
Iter: 1693 loss: 8.37654397e-07
Iter: 1694 loss: 8.37516268e-07
Iter: 1695 loss: 8.3739053e-07
Iter: 1696 loss: 8.3738604e-07
Iter: 1697 loss: 8.37145649e-07
Iter: 1698 loss: 8.38484766e-07
Iter: 1699 loss: 8.37157359e-07
Iter: 1700 loss: 8.37017069e-07
Iter: 1701 loss: 8.37077891e-07
Iter: 1702 loss: 8.36887921e-07
Iter: 1703 loss: 8.36661457e-07
Iter: 1704 loss: 8.38030303e-07
Iter: 1705 loss: 8.36716538e-07
Iter: 1706 loss: 8.36517813e-07
Iter: 1707 loss: 8.36682659e-07
Iter: 1708 loss: 8.3639668e-07
Iter: 1709 loss: 8.36249342e-07
Iter: 1710 loss: 8.38305084e-07
Iter: 1711 loss: 8.36259744e-07
Iter: 1712 loss: 8.36089384e-07
Iter: 1713 loss: 8.37585105e-07
Iter: 1714 loss: 8.36061815e-07
Iter: 1715 loss: 8.36004062e-07
Iter: 1716 loss: 8.35871106e-07
Iter: 1717 loss: 8.37463858e-07
Iter: 1718 loss: 8.3577271e-07
Iter: 1719 loss: 8.3557785e-07
Iter: 1720 loss: 8.36237291e-07
Iter: 1721 loss: 8.35574383e-07
Iter: 1722 loss: 8.35340529e-07
Iter: 1723 loss: 8.35963306e-07
Iter: 1724 loss: 8.35317e-07
Iter: 1725 loss: 8.35156243e-07
Iter: 1726 loss: 8.35402602e-07
Iter: 1727 loss: 8.35046649e-07
Iter: 1728 loss: 8.34865261e-07
Iter: 1729 loss: 8.35710694e-07
Iter: 1730 loss: 8.3482638e-07
Iter: 1731 loss: 8.34696493e-07
Iter: 1732 loss: 8.34705133e-07
Iter: 1733 loss: 8.3455825e-07
Iter: 1734 loss: 8.34397042e-07
Iter: 1735 loss: 8.34942512e-07
Iter: 1736 loss: 8.34236e-07
Iter: 1737 loss: 8.34065645e-07
Iter: 1738 loss: 8.34769651e-07
Iter: 1739 loss: 8.33990782e-07
Iter: 1740 loss: 8.33848048e-07
Iter: 1741 loss: 8.33807917e-07
Iter: 1742 loss: 8.33695083e-07
Iter: 1743 loss: 8.33448894e-07
Iter: 1744 loss: 8.35356218e-07
Iter: 1745 loss: 8.3346913e-07
Iter: 1746 loss: 8.33433376e-07
Iter: 1747 loss: 8.33393869e-07
Iter: 1748 loss: 8.3334703e-07
Iter: 1749 loss: 8.33165359e-07
Iter: 1750 loss: 8.35992182e-07
Iter: 1751 loss: 8.33152285e-07
Iter: 1752 loss: 8.33030811e-07
Iter: 1753 loss: 8.33398246e-07
Iter: 1754 loss: 8.32983119e-07
Iter: 1755 loss: 8.32891033e-07
Iter: 1756 loss: 8.32791045e-07
Iter: 1757 loss: 8.32726641e-07
Iter: 1758 loss: 8.32528372e-07
Iter: 1759 loss: 8.33169622e-07
Iter: 1760 loss: 8.32437365e-07
Iter: 1761 loss: 8.3232851e-07
Iter: 1762 loss: 8.33023e-07
Iter: 1763 loss: 8.32266949e-07
Iter: 1764 loss: 8.32116484e-07
Iter: 1765 loss: 8.32632566e-07
Iter: 1766 loss: 8.32123419e-07
Iter: 1767 loss: 8.32019964e-07
Iter: 1768 loss: 8.32178557e-07
Iter: 1769 loss: 8.31878424e-07
Iter: 1770 loss: 8.31800662e-07
Iter: 1771 loss: 8.31926798e-07
Iter: 1772 loss: 8.31712555e-07
Iter: 1773 loss: 8.31520595e-07
Iter: 1774 loss: 8.31682769e-07
Iter: 1775 loss: 8.31460852e-07
Iter: 1776 loss: 8.31291629e-07
Iter: 1777 loss: 8.31742966e-07
Iter: 1778 loss: 8.31226089e-07
Iter: 1779 loss: 8.31034185e-07
Iter: 1780 loss: 8.31576415e-07
Iter: 1781 loss: 8.30936301e-07
Iter: 1782 loss: 8.31144462e-07
Iter: 1783 loss: 8.30868032e-07
Iter: 1784 loss: 8.30897875e-07
Iter: 1785 loss: 8.30885085e-07
Iter: 1786 loss: 8.30902763e-07
Iter: 1787 loss: 8.3093289e-07
Iter: 1788 loss: 8.30889803e-07
Iter: 1789 loss: 8.30871727e-07
Iter: 1790 loss: 8.3090822e-07
Iter: 1791 loss: 8.30876672e-07
Iter: 1792 loss: 8.30901627e-07
Iter: 1793 loss: 8.30880822e-07
Iter: 1794 loss: 8.30849785e-07
Iter: 1795 loss: 8.30862632e-07
Iter: 1796 loss: 8.30867521e-07
Iter: 1797 loss: 8.30871954e-07
Iter: 1798 loss: 8.30873205e-07
Iter: 1799 loss: 8.30867577e-07
Iter: 1800 loss: 8.30874797e-07
Iter: 1801 loss: 8.30866952e-07
Iter: 1802 loss: 8.30874342e-07
Iter: 1803 loss: 8.30873432e-07
Iter: 1804 loss: 8.30868e-07
Iter: 1805 loss: 8.30873489e-07
Iter: 1806 loss: 8.30873489e-07
Iter: 1807 loss: 8.30872466e-07
Iter: 1808 loss: 8.30872466e-07
Iter: 1809 loss: 8.30872466e-07
Iter: 1810 loss: 8.30872466e-07
Iter: 1811 loss: 8.30872466e-07
Iter: 1812 loss: 8.30868e-07
Iter: 1813 loss: 8.30868e-07
Iter: 1814 loss: 8.30868e-07
Iter: 1815 loss: 8.30868e-07
Iter: 1816 loss: 8.30872466e-07
Iter: 1817 loss: 8.30868e-07
Iter: 1818 loss: 8.30872466e-07
Iter: 1819 loss: 8.32027922e-07
Iter: 1820 loss: 8.30817726e-07
Iter: 1821 loss: 8.31018156e-07
Iter: 1822 loss: 8.30765828e-07
Iter: 1823 loss: 8.30594331e-07
Iter: 1824 loss: 8.31898888e-07
Iter: 1825 loss: 8.30502529e-07
Iter: 1826 loss: 8.304782e-07
Iter: 1827 loss: 8.30356043e-07
Iter: 1828 loss: 8.303507e-07
Iter: 1829 loss: 8.30131285e-07
Iter: 1830 loss: 8.30520207e-07
Iter: 1831 loss: 8.30087288e-07
Iter: 1832 loss: 8.29918918e-07
Iter: 1833 loss: 8.30380884e-07
Iter: 1834 loss: 8.29880264e-07
Iter: 1835 loss: 8.29678584e-07
Iter: 1836 loss: 8.29731789e-07
Iter: 1837 loss: 8.29570695e-07
Iter: 1838 loss: 8.29483952e-07
Iter: 1839 loss: 8.29443422e-07
Iter: 1840 loss: 8.29341161e-07
Iter: 1841 loss: 8.29170801e-07
Iter: 1842 loss: 8.2927005e-07
Iter: 1843 loss: 8.29093665e-07
Iter: 1844 loss: 8.28912164e-07
Iter: 1845 loss: 8.30047156e-07
Iter: 1846 loss: 8.28915e-07
Iter: 1847 loss: 8.28817122e-07
Iter: 1848 loss: 8.29146359e-07
Iter: 1849 loss: 8.28808425e-07
Iter: 1850 loss: 8.28727707e-07
Iter: 1851 loss: 8.28848e-07
Iter: 1852 loss: 8.2863005e-07
Iter: 1853 loss: 8.28567352e-07
Iter: 1854 loss: 8.28490101e-07
Iter: 1855 loss: 8.28489306e-07
Iter: 1856 loss: 8.28230782e-07
Iter: 1857 loss: 8.28092823e-07
Iter: 1858 loss: 8.27975839e-07
Iter: 1859 loss: 8.27879603e-07
Iter: 1860 loss: 8.27859196e-07
Iter: 1861 loss: 8.2771669e-07
Iter: 1862 loss: 8.28127099e-07
Iter: 1863 loss: 8.27700603e-07
Iter: 1864 loss: 8.27492386e-07
Iter: 1865 loss: 8.27662e-07
Iter: 1866 loss: 8.27359088e-07
Iter: 1867 loss: 8.27277e-07
Iter: 1868 loss: 8.2805343e-07
Iter: 1869 loss: 8.27256372e-07
Iter: 1870 loss: 8.27125177e-07
Iter: 1871 loss: 8.2715178e-07
Iter: 1872 loss: 8.27064923e-07
Iter: 1873 loss: 8.26895473e-07
Iter: 1874 loss: 8.27566055e-07
Iter: 1875 loss: 8.26803728e-07
Iter: 1876 loss: 8.26719543e-07
Iter: 1877 loss: 8.26821804e-07
Iter: 1878 loss: 8.26674295e-07
Iter: 1879 loss: 8.26515e-07
Iter: 1880 loss: 8.27877784e-07
Iter: 1881 loss: 8.26463861e-07
Iter: 1882 loss: 8.26470568e-07
Iter: 1883 loss: 8.2643669e-07
Iter: 1884 loss: 8.26351823e-07
Iter: 1885 loss: 8.26243308e-07
Iter: 1886 loss: 8.27324811e-07
Iter: 1887 loss: 8.26202836e-07
Iter: 1888 loss: 8.260497e-07
Iter: 1889 loss: 8.27470046e-07
Iter: 1890 loss: 8.26036967e-07
Iter: 1891 loss: 8.25994391e-07
Iter: 1892 loss: 8.2600576e-07
Iter: 1893 loss: 8.25915e-07
Iter: 1894 loss: 8.25742063e-07
Iter: 1895 loss: 8.26258656e-07
Iter: 1896 loss: 8.2565515e-07
Iter: 1897 loss: 8.25627467e-07
Iter: 1898 loss: 8.25649295e-07
Iter: 1899 loss: 8.25548454e-07
Iter: 1900 loss: 8.25473592e-07
Iter: 1901 loss: 8.25495874e-07
Iter: 1902 loss: 8.25344046e-07
Iter: 1903 loss: 8.25400207e-07
Iter: 1904 loss: 8.25212624e-07
Iter: 1905 loss: 8.25133327e-07
Iter: 1906 loss: 8.25108145e-07
Iter: 1907 loss: 8.25021459e-07
Iter: 1908 loss: 8.24999631e-07
Iter: 1909 loss: 8.25002871e-07
Iter: 1910 loss: 8.24834956e-07
Iter: 1911 loss: 8.24804829e-07
Iter: 1912 loss: 8.24687731e-07
Iter: 1913 loss: 8.24633901e-07
Iter: 1914 loss: 8.24568701e-07
Iter: 1915 loss: 8.24464337e-07
Iter: 1916 loss: 8.24698191e-07
Iter: 1917 loss: 8.24452172e-07
Iter: 1918 loss: 8.24374752e-07
Iter: 1919 loss: 8.24138397e-07
Iter: 1920 loss: 8.25162033e-07
Iter: 1921 loss: 8.24099118e-07
Iter: 1922 loss: 8.23863161e-07
Iter: 1923 loss: 8.25462621e-07
Iter: 1924 loss: 8.23845653e-07
Iter: 1925 loss: 8.23680637e-07
Iter: 1926 loss: 8.25314146e-07
Iter: 1927 loss: 8.23653863e-07
Iter: 1928 loss: 8.23524829e-07
Iter: 1929 loss: 8.24103836e-07
Iter: 1930 loss: 8.23555638e-07
Iter: 1931 loss: 8.23449227e-07
Iter: 1932 loss: 8.23573032e-07
Iter: 1933 loss: 8.23396931e-07
Iter: 1934 loss: 8.23262098e-07
Iter: 1935 loss: 8.23378741e-07
Iter: 1936 loss: 8.23176606e-07
Iter: 1937 loss: 8.23112089e-07
Iter: 1938 loss: 8.2355632e-07
Iter: 1939 loss: 8.23162281e-07
Iter: 1940 loss: 8.23044786e-07
Iter: 1941 loss: 8.22971174e-07
Iter: 1942 loss: 8.22940933e-07
Iter: 1943 loss: 8.2282429e-07
Iter: 1944 loss: 8.23294101e-07
Iter: 1945 loss: 8.22758125e-07
Iter: 1946 loss: 8.22666948e-07
Iter: 1947 loss: 8.22692755e-07
Iter: 1948 loss: 8.22633e-07
Iter: 1949 loss: 8.22496702e-07
Iter: 1950 loss: 8.22461516e-07
Iter: 1951 loss: 8.2234618e-07
Iter: 1952 loss: 8.22522622e-07
Iter: 1953 loss: 8.22345271e-07
Iter: 1954 loss: 8.22278821e-07
Iter: 1955 loss: 8.22082598e-07
Iter: 1956 loss: 8.22146205e-07
Iter: 1957 loss: 8.21991932e-07
Iter: 1958 loss: 8.22203674e-07
Iter: 1959 loss: 8.21962e-07
Iter: 1960 loss: 8.21759841e-07
Iter: 1961 loss: 8.22322477e-07
Iter: 1962 loss: 8.21723404e-07
Iter: 1963 loss: 8.2161614e-07
Iter: 1964 loss: 8.21895e-07
Iter: 1965 loss: 8.2158158e-07
Iter: 1966 loss: 8.21468575e-07
Iter: 1967 loss: 8.21458116e-07
Iter: 1968 loss: 8.21379103e-07
Iter: 1969 loss: 8.21376375e-07
Iter: 1970 loss: 8.21308049e-07
Iter: 1971 loss: 8.21194e-07
Iter: 1972 loss: 8.21244726e-07
Iter: 1973 loss: 8.21171739e-07
Iter: 1974 loss: 8.21006324e-07
Iter: 1975 loss: 8.21877506e-07
Iter: 1976 loss: 8.20930836e-07
Iter: 1977 loss: 8.20903324e-07
Iter: 1978 loss: 8.20920945e-07
Iter: 1979 loss: 8.20861487e-07
Iter: 1980 loss: 8.20699825e-07
Iter: 1981 loss: 8.20757805e-07
Iter: 1982 loss: 8.2058807e-07
Iter: 1983 loss: 8.20558512e-07
Iter: 1984 loss: 8.2053e-07
Iter: 1985 loss: 8.20458354e-07
Iter: 1986 loss: 8.20483e-07
Iter: 1987 loss: 8.20398e-07
Iter: 1988 loss: 8.20318178e-07
Iter: 1989 loss: 8.20260652e-07
Iter: 1990 loss: 8.20166292e-07
Iter: 1991 loss: 8.20110586e-07
Iter: 1992 loss: 8.2046688e-07
Iter: 1993 loss: 8.20074888e-07
Iter: 1994 loss: 8.19891966e-07
Iter: 1995 loss: 8.19901288e-07
Iter: 1996 loss: 8.19797492e-07
Iter: 1997 loss: 8.19604509e-07
Iter: 1998 loss: 8.20786966e-07
Iter: 1999 loss: 8.19593652e-07
Iter: 2000 loss: 8.19521915e-07
Iter: 2001 loss: 8.2064048e-07
Iter: 2002 loss: 8.19526122e-07
Iter: 2003 loss: 8.19432898e-07
Iter: 2004 loss: 8.19367301e-07
Iter: 2005 loss: 8.19351271e-07
Iter: 2006 loss: 8.19215188e-07
Iter: 2007 loss: 8.19760317e-07
Iter: 2008 loss: 8.19246111e-07
Iter: 2009 loss: 8.19187278e-07
Iter: 2010 loss: 8.19216211e-07
Iter: 2011 loss: 8.19083652e-07
Iter: 2012 loss: 8.1899492e-07
Iter: 2013 loss: 8.19025558e-07
Iter: 2014 loss: 8.18888111e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi3
+ date
Wed Oct 21 17:37:15 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/300_300_300_1 --function f1 --psi 0 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e65de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e684598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e73ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e5d7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e5d1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e674620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e674ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e5ae1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e5ae268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e5ae6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e544268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e56aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e4bcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e4ce048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e4e5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e578ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e445e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e474510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e474048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ecf4e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ecebaa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ecedd0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86eceba9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c8743a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ecf7d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ecf74b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ecf74840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c8713378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c8713048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c86b5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c866a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c860a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c8636620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c860fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ece8a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ece7ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.021129474
test_loss: 0.018521177
train_loss: 0.006700809
test_loss: 0.006398342
train_loss: 0.0037941495
test_loss: 0.0030232002
train_loss: 0.002358644
test_loss: 0.002284332
train_loss: 0.0023130241
test_loss: 0.0020607107
train_loss: 0.0022131763
test_loss: 0.0031315936
train_loss: 0.002886698
test_loss: 0.003162545
train_loss: 0.00198451
test_loss: 0.0027684001
train_loss: 0.002174793
test_loss: 0.002020517
train_loss: 0.0017280421
test_loss: 0.0018899894
train_loss: 0.0020210124
test_loss: 0.0021255054
train_loss: 0.0018337168
test_loss: 0.0018097264
train_loss: 0.0017631376
test_loss: 0.0016990654
train_loss: 0.0020722253
test_loss: 0.0018690554
train_loss: 0.0023275297
test_loss: 0.0024220876
train_loss: 0.0018989563
test_loss: 0.0018325931
train_loss: 0.0017533805
test_loss: 0.0018342382
train_loss: 0.0017319745
test_loss: 0.0018350013
train_loss: 0.002623636
test_loss: 0.0018451837
train_loss: 0.0020077894
test_loss: 0.0016715343
train_loss: 0.0019597078
test_loss: 0.0020222159
train_loss: 0.0028164247
test_loss: 0.0023677484
train_loss: 0.0018954773
test_loss: 0.0022986797
train_loss: 0.0018972855
test_loss: 0.0018784598
train_loss: 0.0020776086
test_loss: 0.0019950122
train_loss: 0.002512217
test_loss: 0.0018346596
train_loss: 0.0022241701
test_loss: 0.0020120526
train_loss: 0.0020356008
test_loss: 0.0018218786
train_loss: 0.0020169474
test_loss: 0.00225041
train_loss: 0.0017508144
test_loss: 0.0019469996
train_loss: 0.0022577066
test_loss: 0.0023491671
train_loss: 0.0021895266
test_loss: 0.00247745
train_loss: 0.001976376
test_loss: 0.0021450405
train_loss: 0.0020107643
test_loss: 0.0024825719
train_loss: 0.0018353565
test_loss: 0.0028245698
train_loss: 0.0022010836
test_loss: 0.0022737875
train_loss: 0.001995124
test_loss: 0.0018716819
train_loss: 0.002326163
test_loss: 0.002384978
train_loss: 0.0018017583
test_loss: 0.0018353201
train_loss: 0.0018519096
test_loss: 0.0016753234
train_loss: 0.0016251758
test_loss: 0.0016588295
train_loss: 0.0016164486
test_loss: 0.0018871258
train_loss: 0.001962343
test_loss: 0.0019023542
train_loss: 0.0020973538
test_loss: 0.0017814052
train_loss: 0.0018261209
test_loss: 0.0019540994
train_loss: 0.0018166138
test_loss: 0.002026662
train_loss: 0.0017242525
test_loss: 0.0017986738
train_loss: 0.0019628154
test_loss: 0.0019186742
train_loss: 0.0020187953
test_loss: 0.0017059918
train_loss: 0.0016721301
test_loss: 0.0018890171
train_loss: 0.0019327656
test_loss: 0.002133721
train_loss: 0.0019488317
test_loss: 0.0017700474
train_loss: 0.0020409771
test_loss: 0.0018341164
train_loss: 0.0019551492
test_loss: 0.0017535255
train_loss: 0.0016693167
test_loss: 0.0019435995
train_loss: 0.001767593
test_loss: 0.0022298212
train_loss: 0.0020269298
test_loss: 0.0016736557
train_loss: 0.0019419126
test_loss: 0.0020591023
train_loss: 0.0016621232
test_loss: 0.0017765346
train_loss: 0.0020071447
test_loss: 0.0020041727
train_loss: 0.002029328
test_loss: 0.0018714325
train_loss: 0.0020313791
test_loss: 0.002009335
train_loss: 0.0017616694
test_loss: 0.00260891
train_loss: 0.00255659
test_loss: 0.0022772767
train_loss: 0.0031747
test_loss: 0.003910932
train_loss: 0.0027637286
test_loss: 0.0023732416
train_loss: 0.0018943726
test_loss: 0.002266175
train_loss: 0.0020768004
test_loss: 0.0020643147
train_loss: 0.002202229
test_loss: 0.001755968
train_loss: 0.0023108039
test_loss: 0.0032293522
train_loss: 0.002191671
test_loss: 0.0032481297
train_loss: 0.0026769903
test_loss: 0.0031750365
train_loss: 0.002584875
test_loss: 0.003142315
train_loss: 0.0029558868
test_loss: 0.003002111
train_loss: 0.0019014376
test_loss: 0.0018660681
train_loss: 0.0018057512
test_loss: 0.0021538292
train_loss: 0.0021394892
test_loss: 0.0018854161
train_loss: 0.001777414
test_loss: 0.0018131494
train_loss: 0.001669548
test_loss: 0.0019128994
train_loss: 0.001622363
test_loss: 0.001595663
train_loss: 0.0018920025
test_loss: 0.0017116629
train_loss: 0.0016512536
test_loss: 0.002439839
train_loss: 0.0020334558
test_loss: 0.0021181963
train_loss: 0.001647379
test_loss: 0.0016792616
train_loss: 0.0019334671
test_loss: 0.0018653547
train_loss: 0.002399962
test_loss: 0.0022696222
train_loss: 0.002041873
test_loss: 0.0024195693
train_loss: 0.0018603551
test_loss: 0.0017105077
train_loss: 0.0020821462
test_loss: 0.0018784597
train_loss: 0.0019041016
test_loss: 0.0017013565
train_loss: 0.0015866116
test_loss: 0.0016986843
train_loss: 0.0019489683
test_loss: 0.0020174782
train_loss: 0.0017264818
test_loss: 0.001805258
train_loss: 0.0018198965
test_loss: 0.0021309957
train_loss: 0.0015507367
test_loss: 0.0017197034
train_loss: 0.0018150259
test_loss: 0.001838511
train_loss: 0.002245245
test_loss: 0.0017774529
train_loss: 0.0016223972
test_loss: 0.0017780908
train_loss: 0.0018523241
test_loss: 0.0017514259
train_loss: 0.001764163
test_loss: 0.0017971984
train_loss: 0.0015941757
test_loss: 0.001836781
train_loss: 0.0019634652
test_loss: 0.0015751056
train_loss: 0.0016274004
test_loss: 0.0017514176
train_loss: 0.0018463332
test_loss: 0.0017533714
train_loss: 0.0018450478
test_loss: 0.001897292
train_loss: 0.0018281722
test_loss: 0.002049048
train_loss: 0.0018578994
test_loss: 0.0021318924
train_loss: 0.0021621494
test_loss: 0.002096193
train_loss: 0.0019520682
test_loss: 0.0022447163
train_loss: 0.002016918
test_loss: 0.0020532683
train_loss: 0.0016725371
test_loss: 0.0024293175
train_loss: 0.001774119
test_loss: 0.0026415337
train_loss: 0.0023090069
test_loss: 0.0029016687
train_loss: 0.0019969367
test_loss: 0.0021739216
train_loss: 0.0021645874
test_loss: 0.0020060518
train_loss: 0.0017294114
test_loss: 0.0017359867
train_loss: 0.0020522468
test_loss: 0.0020128228
train_loss: 0.001750757
test_loss: 0.002051423
train_loss: 0.001969144
test_loss: 0.002268327
train_loss: 0.0020532738
test_loss: 0.0019781708
train_loss: 0.0019487161
test_loss: 0.0020336625
train_loss: 0.003142153
test_loss: 0.0028816215
train_loss: 0.0025581494
test_loss: 0.0029992503
train_loss: 0.0018234388
test_loss: 0.0025655034
train_loss: 0.00230574
test_loss: 0.0019110306
train_loss: 0.0019668972
test_loss: 0.001798116
train_loss: 0.0022611497
test_loss: 0.0021048135
train_loss: 0.00201967
test_loss: 0.0017772256
train_loss: 0.0016583173
test_loss: 0.0018891061
train_loss: 0.0024158794
test_loss: 0.0019562936
train_loss: 0.0021301596
test_loss: 0.0021349667
train_loss: 0.0033875764
test_loss: 0.0028481784
train_loss: 0.002392621
test_loss: 0.0023595688
train_loss: 0.0029490436
test_loss: 0.0020023512
train_loss: 0.0028706144
test_loss: 0.0021146676
train_loss: 0.0019055039
test_loss: 0.0021035676
train_loss: 0.0027789967
test_loss: 0.0029457244
train_loss: 0.0028297184
test_loss: 0.0023354755
train_loss: 0.0021596733
test_loss: 0.0021001326
train_loss: 0.003181175
test_loss: 0.0029290242
train_loss: 0.0018655742
test_loss: 0.0021834518
train_loss: 0.001977928
test_loss: 0.0018082621
train_loss: 0.0017962742
test_loss: 0.0018331779
train_loss: 0.0019626052
test_loss: 0.0019678366
train_loss: 0.0016253367
test_loss: 0.0016562914
train_loss: 0.0018015455
test_loss: 0.0019092602
train_loss: 0.0017332258
test_loss: 0.001873175
train_loss: 0.0018630124
test_loss: 0.001652348
train_loss: 0.0021080202
test_loss: 0.0022777168
train_loss: 0.0017250961
test_loss: 0.0018518514
train_loss: 0.0018839794
test_loss: 0.0020023745
train_loss: 0.001736228
test_loss: 0.001832638
train_loss: 0.001707329
test_loss: 0.0016956667
train_loss: 0.0019004164
test_loss: 0.001884011
train_loss: 0.0019191823
test_loss: 0.0017337472
train_loss: 0.0021875557
test_loss: 0.0018368686
train_loss: 0.001607012
test_loss: 0.0014965144
train_loss: 0.0017420237
test_loss: 0.001643097
train_loss: 0.0015962942
test_loss: 0.0016051204
train_loss: 0.0017158777
test_loss: 0.0016638156
train_loss: 0.0017133278
test_loss: 0.0018701303
train_loss: 0.0015294268
test_loss: 0.0016265096
train_loss: 0.0019365051
test_loss: 0.0019723645
train_loss: 0.0014735167
test_loss: 0.0017801924
train_loss: 0.001661818
test_loss: 0.0018804775
train_loss: 0.0017893729
test_loss: 0.0018926882
train_loss: 0.0022239122
test_loss: 0.00276539
train_loss: 0.0017777625
test_loss: 0.0020436423
train_loss: 0.0018671995
test_loss: 0.0017411876