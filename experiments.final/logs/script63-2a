+ RUN=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='0 1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1 --function f1 --psi 0 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d5006b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d50042840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4823b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d500b7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48202f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d481db620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d481dbea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4811f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4811f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4811f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48178268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480119d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4803b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480950d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480ae400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48095048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48155f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48079620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480792f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf056b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf0555378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480d10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf0555f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480d2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf04fd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d30048b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d30048730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d30066378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d30066048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf03eb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf0424268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf0496f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf03de598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf048bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf03490d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf02a9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.01446991
test_loss: 0.014563111
train_loss: 0.0061007137
test_loss: 0.0057039363
train_loss: 0.0029843245
test_loss: 0.0029687865
train_loss: 0.0022280903
test_loss: 0.0019834223
train_loss: 0.001850297
test_loss: 0.0021342314
train_loss: 0.001932413
test_loss: 0.002534642
train_loss: 0.0017468589
test_loss: 0.0018275735
train_loss: 0.0016478723
test_loss: 0.0018323604
train_loss: 0.001758661
test_loss: 0.001729004
train_loss: 0.0016869906
test_loss: 0.0016550544
train_loss: 0.001781175
test_loss: 0.002203674
train_loss: 0.001658789
test_loss: 0.0017269037
train_loss: 0.00184351
test_loss: 0.0016266324
train_loss: 0.0016099224
test_loss: 0.0017676431
train_loss: 0.0016576992
test_loss: 0.0017862144
train_loss: 0.0015958007
test_loss: 0.0018138258
train_loss: 0.0016244443
test_loss: 0.00187894
train_loss: 0.0016558589
test_loss: 0.0016680209
train_loss: 0.0015983128
test_loss: 0.0016659022
train_loss: 0.0015355722
test_loss: 0.0016872641
train_loss: 0.001711067
test_loss: 0.0016990335
train_loss: 0.0014374659
test_loss: 0.0016078943
train_loss: 0.0016087075
test_loss: 0.0015395381
train_loss: 0.0015143383
test_loss: 0.0016746816
train_loss: 0.0016346029
test_loss: 0.0017192756
train_loss: 0.0015073996
test_loss: 0.0015587525
train_loss: 0.0014686474
test_loss: 0.0016429438
train_loss: 0.0016858317
test_loss: 0.0017663288
train_loss: 0.0016277325
test_loss: 0.0016505244
train_loss: 0.0014395562
test_loss: 0.0015161858
train_loss: 0.0017073299
test_loss: 0.001642084
train_loss: 0.0015001392
test_loss: 0.0017853288
train_loss: 0.0015324022
test_loss: 0.0015694213
train_loss: 0.0017190642
test_loss: 0.002040534
train_loss: 0.0019446943
test_loss: 0.0018911173
train_loss: 0.0015931504
test_loss: 0.0017187675
train_loss: 0.0016002906
test_loss: 0.0014440895
train_loss: 0.0015964371
test_loss: 0.0017356991
train_loss: 0.0014666321
test_loss: 0.0016223755
train_loss: 0.0022661954
test_loss: 0.0016969298
train_loss: 0.002013837
test_loss: 0.0017684957
train_loss: 0.0014898067
test_loss: 0.0014933725
train_loss: 0.0014357049
test_loss: 0.0015457827
train_loss: 0.0015759708
test_loss: 0.0015848638
train_loss: 0.0015968217
test_loss: 0.0015508466
train_loss: 0.0015940558
test_loss: 0.0016074141
train_loss: 0.0014032067
test_loss: 0.0017015783
train_loss: 0.0016453989
test_loss: 0.001819515
train_loss: 0.001533316
test_loss: 0.0016596533
train_loss: 0.001744759
test_loss: 0.0015761027
train_loss: 0.0015969123
test_loss: 0.001567108
train_loss: 0.001583218
test_loss: 0.0014744926
train_loss: 0.0016172767
test_loss: 0.0014762414
train_loss: 0.0013788611
test_loss: 0.0016270675
train_loss: 0.0014433893
test_loss: 0.0016332268
train_loss: 0.0016015944
test_loss: 0.0014983468
train_loss: 0.0015836951
test_loss: 0.001656378
train_loss: 0.0019046154
test_loss: 0.0016807339
train_loss: 0.0015642921
test_loss: 0.001449934
train_loss: 0.0015236659
test_loss: 0.001625768
train_loss: 0.001438518
test_loss: 0.00159101
train_loss: 0.0017950196
test_loss: 0.0017771337
train_loss: 0.0016934378
test_loss: 0.0014408209
train_loss: 0.001451454
test_loss: 0.0014762724
train_loss: 0.0016224268
test_loss: 0.0015628005
train_loss: 0.001439381
test_loss: 0.0016408323
train_loss: 0.0014427197
test_loss: 0.0015511219
train_loss: 0.0019329096
test_loss: 0.0017298143
train_loss: 0.0014603699
test_loss: 0.0016668828
train_loss: 0.0014633517
test_loss: 0.0015038365
train_loss: 0.0016945603
test_loss: 0.0014765506
train_loss: 0.0015610064
test_loss: 0.0015900561
train_loss: 0.0014894115
test_loss: 0.0017577407
train_loss: 0.0015504151
test_loss: 0.0015772174
train_loss: 0.001785702
test_loss: 0.0015981816
train_loss: 0.0013556577
test_loss: 0.0016797732
train_loss: 0.0019596303
test_loss: 0.0016231047
train_loss: 0.0015978557
test_loss: 0.0019548875
train_loss: 0.0018089241
test_loss: 0.0017244956
train_loss: 0.0016637614
test_loss: 0.0016568156
train_loss: 0.0016396774
test_loss: 0.0016148655
train_loss: 0.0015247636
test_loss: 0.0015079884
train_loss: 0.0015650761
test_loss: 0.0013783799
train_loss: 0.0017130303
test_loss: 0.0018068748
train_loss: 0.0014303089
test_loss: 0.0014289726
train_loss: 0.0014250784
test_loss: 0.0018212582
train_loss: 0.0016418603
test_loss: 0.0018726395
train_loss: 0.0014853289
test_loss: 0.0015382114
train_loss: 0.0014621972
test_loss: 0.0015432447
train_loss: 0.0013706426
test_loss: 0.00161236
train_loss: 0.0014555722
test_loss: 0.001560037
train_loss: 0.0014903278
test_loss: 0.0014532026
train_loss: 0.0016215194
test_loss: 0.0015232835
train_loss: 0.0014487463
test_loss: 0.0014153557
train_loss: 0.001453269
test_loss: 0.001495504
train_loss: 0.001645748
test_loss: 0.0015742294
train_loss: 0.001486985
test_loss: 0.0015117292
train_loss: 0.001293146
test_loss: 0.0014536665
train_loss: 0.0012717189
test_loss: 0.0017682397
train_loss: 0.0018159614
test_loss: 0.0014019535
train_loss: 0.0015336735
test_loss: 0.0016174158
train_loss: 0.001640986
test_loss: 0.0016490876
train_loss: 0.0017285929
test_loss: 0.0015039394
train_loss: 0.0015129268
test_loss: 0.001501659
train_loss: 0.0014770599
test_loss: 0.00166554
train_loss: 0.0014768214
test_loss: 0.0015077664
train_loss: 0.0013613536
test_loss: 0.0014948576
train_loss: 0.0016425725
test_loss: 0.0014975911
train_loss: 0.0015560121
test_loss: 0.0017135025
train_loss: 0.0016344946
test_loss: 0.0017686852
train_loss: 0.0014189279
test_loss: 0.0015491813
train_loss: 0.0017162962
test_loss: 0.0013569933
train_loss: 0.0017372073
test_loss: 0.0015285186
train_loss: 0.0016153982
test_loss: 0.0020301905
train_loss: 0.0014209867
test_loss: 0.0016320463
train_loss: 0.0014127066
test_loss: 0.0014944248
train_loss: 0.0014703255
test_loss: 0.001583737
train_loss: 0.0014225662
test_loss: 0.0014884382
train_loss: 0.0012278806
test_loss: 0.0015368352
train_loss: 0.0015249306
test_loss: 0.0015929276
train_loss: 0.0014749116
test_loss: 0.001581356
train_loss: 0.0014457266
test_loss: 0.0015895407
train_loss: 0.0013781362
test_loss: 0.0016907799
train_loss: 0.0018331269
test_loss: 0.0018032367
train_loss: 0.0015140672
test_loss: 0.001694127
train_loss: 0.00144173
test_loss: 0.001309777
train_loss: 0.0013793635
test_loss: 0.0014030401
train_loss: 0.0013339465
test_loss: 0.0014826511
train_loss: 0.0014160526
test_loss: 0.0015112892
train_loss: 0.0017736193
test_loss: 0.0015836468
train_loss: 0.0014529022
test_loss: 0.0016319467
train_loss: 0.0013588046
test_loss: 0.0016108248
train_loss: 0.0016617901
test_loss: 0.0015876953
train_loss: 0.0014381143
test_loss: 0.001420244
train_loss: 0.0015028443
test_loss: 0.0014328328
train_loss: 0.0018428022
test_loss: 0.0016394155
train_loss: 0.0017813148
test_loss: 0.0014182451
train_loss: 0.0015414245
test_loss: 0.0014307202
train_loss: 0.0014657108
test_loss: 0.001655902
train_loss: 0.001583894
test_loss: 0.0016174768
train_loss: 0.0017861134
test_loss: 0.0016336904
train_loss: 0.0014783943
test_loss: 0.0015000133
train_loss: 0.0016571431
test_loss: 0.0016172334
train_loss: 0.0014215212
test_loss: 0.00190303
train_loss: 0.0014609338
test_loss: 0.0017572785
train_loss: 0.0014708242
test_loss: 0.0014933813
train_loss: 0.0016582767
test_loss: 0.0014312085
train_loss: 0.0014339054
test_loss: 0.0015002323
train_loss: 0.0016902378
test_loss: 0.0015686426
train_loss: 0.0015350088
test_loss: 0.0015101255
train_loss: 0.0013320304
test_loss: 0.001445899
train_loss: 0.0014558436
test_loss: 0.0014504981
train_loss: 0.0015751593
test_loss: 0.0017077051
train_loss: 0.0017902954
test_loss: 0.0016688957
train_loss: 0.001461013
test_loss: 0.0016848516
train_loss: 0.0015692152
test_loss: 0.0014802668
train_loss: 0.001397494
test_loss: 0.0015489759
train_loss: 0.0014356087
test_loss: 0.0017592951
train_loss: 0.00156361
test_loss: 0.001506651
train_loss: 0.0017183595
test_loss: 0.0015042296
train_loss: 0.0013784659
test_loss: 0.0014277741
train_loss: 0.0013683643
test_loss: 0.0015297084
train_loss: 0.0012907035
test_loss: 0.0014080602
train_loss: 0.0013201418
test_loss: 0.001384942
train_loss: 0.0016413741
test_loss: 0.0014957768
train_loss: 0.0013758225
test_loss: 0.0018150372
train_loss: 0.0014531198
test_loss: 0.0015570043
train_loss: 0.001414399
test_loss: 0.0014045959
train_loss: 0.0014860721
test_loss: 0.0014274438
train_loss: 0.0014137501
test_loss: 0.0015327838
train_loss: 0.001277756
test_loss: 0.0012679124
train_loss: 0.0013596637
test_loss: 0.0017831826
train_loss: 0.0015593704
test_loss: 0.0016056147
train_loss: 0.0013414015
test_loss: 0.0014384162
train_loss: 0.0015234689
test_loss: 0.0012737212
train_loss: 0.0014330641
test_loss: 0.0015948906
train_loss: 0.001573056
test_loss: 0.001617892
train_loss: 0.001447568
test_loss: 0.0014805155
train_loss: 0.0015321799
test_loss: 0.001339891
train_loss: 0.0014027252
test_loss: 0.0014441533
train_loss: 0.0015259949
test_loss: 0.0014097646
train_loss: 0.0013040403
test_loss: 0.0013511409
train_loss: 0.0016715988
test_loss: 0.0016655498
train_loss: 0.0014940179
test_loss: 0.0015317177
train_loss: 0.0016524498
test_loss: 0.0015901997
train_loss: 0.0013186787
test_loss: 0.0017084526
train_loss: 0.0013541421
test_loss: 0.0014523218
train_loss: 0.0013671687
test_loss: 0.0015558104
train_loss: 0.0015906491
test_loss: 0.0013784255
train_loss: 0.0016700743
test_loss: 0.0013970636
train_loss: 0.001479984
test_loss: 0.0014134469
train_loss: 0.0017400414
test_loss: 0.001376923
train_loss: 0.0013729646
test_loss: 0.0013080407
train_loss: 0.0014066153
test_loss: 0.0015263975
train_loss: 0.0014601744
test_loss: 0.0014722964
train_loss: 0.0012941139
test_loss: 0.00145577
train_loss: 0.0014750783
test_loss: 0.0017029109
train_loss: 0.0013822546
test_loss: 0.0013884901
train_loss: 0.0012710359
test_loss: 0.0014446508
train_loss: 0.0014942574
test_loss: 0.0014321767
train_loss: 0.0013153483
test_loss: 0.0014142843
train_loss: 0.0013886084
test_loss: 0.0017549748
train_loss: 0.0015359556
test_loss: 0.0014066724
train_loss: 0.001521497
test_loss: 0.001809925
train_loss: 0.0015140332
test_loss: 0.0014500907
train_loss: 0.0013236115
test_loss: 0.0017262682
train_loss: 0.001402145
test_loss: 0.0014286883
train_loss: 0.0012410501
test_loss: 0.0014471372
train_loss: 0.0014986605
test_loss: 0.0016517887
train_loss: 0.0015107307
test_loss: 0.0015618787
train_loss: 0.0014216696
test_loss: 0.0016765672
train_loss: 0.0014310125
test_loss: 0.001445288
train_loss: 0.0014756061
test_loss: 0.001410923
train_loss: 0.0014969462
test_loss: 0.0015486982
train_loss: 0.001447938
test_loss: 0.0016327292
train_loss: 0.0013367047
test_loss: 0.0018495074
train_loss: 0.0014273216
test_loss: 0.0014834874
train_loss: 0.0014102816
test_loss: 0.0014595977
train_loss: 0.0014819386
test_loss: 0.0016490534
train_loss: 0.0013889554
test_loss: 0.0014832243
train_loss: 0.0015212616
test_loss: 0.0016344985
train_loss: 0.0017937169
test_loss: 0.0015656346
train_loss: 0.0015375549
test_loss: 0.001715795
train_loss: 0.001305368
test_loss: 0.0015075401
train_loss: 0.0013870564
test_loss: 0.0015610573
train_loss: 0.0015742984
test_loss: 0.0014954566
train_loss: 0.0015045939
test_loss: 0.0015422287
train_loss: 0.001587027
test_loss: 0.0014604046
train_loss: 0.0013660378
test_loss: 0.0015623491
train_loss: 0.0015801725
test_loss: 0.0015435684
train_loss: 0.0014816442
test_loss: 0.0015432443
train_loss: 0.0014255899
test_loss: 0.0014864856
train_loss: 0.0014168565
test_loss: 0.0015866129
train_loss: 0.0014174237
test_loss: 0.0015370129
train_loss: 0.0013607971
test_loss: 0.0020046507
train_loss: 0.0015179964
test_loss: 0.0014220093
train_loss: 0.0013930805
test_loss: 0.0013122876
train_loss: 0.0014497729
test_loss: 0.0016451248
train_loss: 0.0015281609
test_loss: 0.0015396327
train_loss: 0.0014655908
test_loss: 0.001615487
train_loss: 0.0014467072
test_loss: 0.0014940519
train_loss: 0.0014258396
test_loss: 0.0014120623
train_loss: 0.001590949
test_loss: 0.0015562979
train_loss: 0.0013666612
test_loss: 0.001688748
train_loss: 0.0014256646
test_loss: 0.0016673609
train_loss: 0.0012837786
test_loss: 0.0014689006
train_loss: 0.0015225306
test_loss: 0.0013226365
train_loss: 0.0014435058
test_loss: 0.0016097219
train_loss: 0.001490226
test_loss: 0.0014707719
train_loss: 0.0015570254
test_loss: 0.0014763457
train_loss: 0.0014651002
test_loss: 0.0014806896
train_loss: 0.0014131137
test_loss: 0.0015715347
train_loss: 0.0013346198
test_loss: 0.0014365915
train_loss: 0.0014147961
test_loss: 0.0015735792
train_loss: 0.001558482
test_loss: 0.001689539
train_loss: 0.0015526665
test_loss: 0.0017777268
train_loss: 0.0015912428
test_loss: 0.0017169814
train_loss: 0.0015534265
test_loss: 0.0014019324
train_loss: 0.0014093252
test_loss: 0.0014856003
train_loss: 0.0012982495
test_loss: 0.0016091944
train_loss: 0.0015962397
test_loss: 0.0014576904
train_loss: 0.0014527976
test_loss: 0.0015175258
train_loss: 0.0015200137
test_loss: 0.0014382985
train_loss: 0.0016757972
test_loss: 0.0015269875
train_loss: 0.0013741413
test_loss: 0.0013454
train_loss: 0.001477249
test_loss: 0.0013973379
train_loss: 0.0013785462
test_loss: 0.0016340134
train_loss: 0.0014377218
test_loss: 0.0014360637
train_loss: 0.0013921668
test_loss: 0.0013515848
train_loss: 0.0015138204
test_loss: 0.0014131402
train_loss: 0.0013741986
test_loss: 0.0015224686
train_loss: 0.0014870095
test_loss: 0.0016747715
train_loss: 0.0014078515
test_loss: 0.0014689103
train_loss: 0.0013958886
test_loss: 0.0014381474
train_loss: 0.00137558
test_loss: 0.0015118683
train_loss: 0.0013358187
test_loss: 0.0014284201
train_loss: 0.0015065102
test_loss: 0.0013745155
train_loss: 0.0015887045
test_loss: 0.0014611683
train_loss: 0.0015606779
test_loss: 0.0015876652
train_loss: 0.0015719568
test_loss: 0.0014939316
train_loss: 0.0014262844
test_loss: 0.0014986428
train_loss: 0.0013385668
test_loss: 0.0015234804
train_loss: 0.0012120851
test_loss: 0.0012789413
train_loss: 0.0014410091
test_loss: 0.0016880564
train_loss: 0.0012948622
test_loss: 0.0017923623
train_loss: 0.0015650354
test_loss: 0.0017466852
train_loss: 0.001385052
test_loss: 0.0017402772
train_loss: 0.0014741006
test_loss: 0.0017429255
train_loss: 0.0014156213
test_loss: 0.0015291126
train_loss: 0.0016936248
test_loss: 0.0013755395
train_loss: 0.0012992825
test_loss: 0.0015683166
train_loss: 0.0013352858
test_loss: 0.0015312992
train_loss: 0.0013649424
test_loss: 0.0014869414
train_loss: 0.0013231616
test_loss: 0.0015380144
train_loss: 0.0015821371
test_loss: 0.0014820109
train_loss: 0.0012809385
test_loss: 0.0016835411
train_loss: 0.0015572383
test_loss: 0.0015560972
train_loss: 0.0016272591
test_loss: 0.0013203487
train_loss: 0.0013345317
test_loss: 0.0014885379
train_loss: 0.0014487236
test_loss: 0.0014548007
train_loss: 0.0013212552
test_loss: 0.0013025034
train_loss: 0.0014428523
test_loss: 0.0013844796
train_loss: 0.0012974569
test_loss: 0.0014775242
train_loss: 0.0014225338
test_loss: 0.0015958056
train_loss: 0.0014615837
test_loss: 0.0013792451
train_loss: 0.0013449255
test_loss: 0.0014553375
train_loss: 0.0013739971
test_loss: 0.0013114054
train_loss: 0.001374784
test_loss: 0.001417581
train_loss: 0.0013198523
test_loss: 0.0013718043
train_loss: 0.0015022773
test_loss: 0.0016950854
train_loss: 0.0019781184
test_loss: 0.001643026
train_loss: 0.0013600537
test_loss: 0.0015435726
train_loss: 0.0016295132
test_loss: 0.0015399072
train_loss: 0.001646334
test_loss: 0.0015145659
train_loss: 0.0015850936
test_loss: 0.001870616
train_loss: 0.0013257648
test_loss: 0.0017225089
train_loss: 0.0012660936
test_loss: 0.0013343855
train_loss: 0.0014383723
test_loss: 0.0015453593
train_loss: 0.0013907064
test_loss: 0.0013401497
train_loss: 0.0013601249
test_loss: 0.0014754762
train_loss: 0.001396916
test_loss: 0.0013455708
train_loss: 0.0013642213
test_loss: 0.0013115632
train_loss: 0.0015313214
test_loss: 0.0013405582
train_loss: 0.0013751647
test_loss: 0.0015248109
train_loss: 0.0013186116
test_loss: 0.0015073994
train_loss: 0.0014920167
test_loss: 0.001485834
train_loss: 0.0015281143
test_loss: 0.0014298985
train_loss: 0.0014124538
test_loss: 0.0014213847
train_loss: 0.0014022395
test_loss: 0.0014109123
train_loss: 0.0014368199
test_loss: 0.0014391667
train_loss: 0.0014725878
test_loss: 0.0015497493
train_loss: 0.0015073218
test_loss: 0.0015339466
train_loss: 0.0013636653
test_loss: 0.0013886383
train_loss: 0.001728624
test_loss: 0.0015224301
train_loss: 0.0013574834
test_loss: 0.0015514166
train_loss: 0.0014211363
test_loss: 0.0015485353
train_loss: 0.0017218156
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
test_loss: 0.0013878092
train_loss: 0.001475277
test_loss: 0.0017442113
train_loss: 0.0016256459
test_loss: 0.001633986
train_loss: 0.0012510968
test_loss: 0.0014752301
train_loss: 0.0012361784
test_loss: 0.0015759117
train_loss: 0.0013991217
test_loss: 0.001396975
train_loss: 0.0019350554
test_loss: 0.0013713478
train_loss: 0.0017251641
test_loss: 0.0014397216
train_loss: 0.001353412
test_loss: 0.0017204063
train_loss: 0.0014993977
test_loss: 0.0014968325
train_loss: 0.0015862407
test_loss: 0.0014354254
train_loss: 0.0012285576
test_loss: 0.0017759237
train_loss: 0.0015174267
test_loss: 0.0014423534
train_loss: 0.0013673684
test_loss: 0.001390212
train_loss: 0.0012749638
test_loss: 0.0014282076
train_loss: 0.0012710089
test_loss: 0.0014345777
train_loss: 0.0013052354
test_loss: 0.0016724768
train_loss: 0.0014125541
test_loss: 0.0012864624
train_loss: 0.0015152947
test_loss: 0.0015124533
train_loss: 0.0013430432
test_loss: 0.0014485155
train_loss: 0.0013324504
test_loss: 0.0013013548
train_loss: 0.0016578173
test_loss: 0.0015228215
train_loss: 0.0013796499
test_loss: 0.0018758621
train_loss: 0.0013569505
test_loss: 0.0014462544
train_loss: 0.0013229394
test_loss: 0.001368823
train_loss: 0.0016070299
test_loss: 0.0014097806
train_loss: 0.0013115944
test_loss: 0.0016082012
train_loss: 0.0014270482
test_loss: 0.0013836159
train_loss: 0.0013671007
test_loss: 0.0014289151
train_loss: 0.0013596357
test_loss: 0.001456479
train_loss: 0.0014648982
test_loss: 0.0012871297
train_loss: 0.0012566395
test_loss: 0.001533489
train_loss: 0.0014007286
test_loss: 0.0014395473
train_loss: 0.0013886758
test_loss: 0.0016520122
train_loss: 0.0014141721
test_loss: 0.0015225235
train_loss: 0.0013985448
test_loss: 0.0013241741
train_loss: 0.0014713507
test_loss: 0.0015886216
train_loss: 0.0014968038
test_loss: 0.0013955629
train_loss: 0.0014127896
test_loss: 0.0015917782
train_loss: 0.001669686
test_loss: 0.0015452713
train_loss: 0.0014676875
test_loss: 0.0014705528
train_loss: 0.0013773564
test_loss: 0.0015788996
train_loss: 0.0015142972
test_loss: 0.0015299346
train_loss: 0.0013011172
test_loss: 0.001345937
train_loss: 0.0012402565
test_loss: 0.0014224151
train_loss: 0.0013756165
test_loss: 0.0014134049
train_loss: 0.001571715
test_loss: 0.0014289744
train_loss: 0.0013906276
test_loss: 0.001414175
train_loss: 0.0014316312
test_loss: 0.0014302703
train_loss: 0.001304377
test_loss: 0.0014763335
train_loss: 0.0013441988
test_loss: 0.0015980445
train_loss: 0.0017669268
test_loss: 0.001429147
train_loss: 0.0013799511
test_loss: 0.0013702397
train_loss: 0.0014846993
test_loss: 0.0012934621
train_loss: 0.0013515005
test_loss: 0.0012363696
train_loss: 0.0012705945
test_loss: 0.0013683463
train_loss: 0.0012952298
test_loss: 0.0013100216
train_loss: 0.0013683639
test_loss: 0.0013572149
train_loss: 0.0014456001
test_loss: 0.0013699345
train_loss: 0.0013124323
test_loss: 0.0016054502
train_loss: 0.0013912058
test_loss: 0.0014344753
train_loss: 0.0012366488
test_loss: 0.0013875691
train_loss: 0.0015618025
test_loss: 0.0014748742
train_loss: 0.0015847064
test_loss: 0.0015977737
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcded400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fce0c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd819d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fce2abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd5af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd5a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd0cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd771e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fccd7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fccd7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fccae1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcc4abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcc71950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcc17268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcc71a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcbec9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcbec730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcb53ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcb69d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcbec488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbebca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe570d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbebc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe4f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe4f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe4fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe4f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbdc69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbdc6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b45057b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b4520268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b4487f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b4487730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b44d8d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b44500d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b4473f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.55788461e-06
Iter: 2 loss: 3.38037557e-06
Iter: 3 loss: 2.27885e-06
Iter: 4 loss: 1.90168623e-06
Iter: 5 loss: 1.76326103e-06
Iter: 6 loss: 1.55402813e-06
Iter: 7 loss: 1.42646286e-06
Iter: 8 loss: 1.41123576e-06
Iter: 9 loss: 1.35459095e-06
Iter: 10 loss: 1.32728883e-06
Iter: 11 loss: 1.29992065e-06
Iter: 12 loss: 1.24824055e-06
Iter: 13 loss: 2.04126036e-06
Iter: 14 loss: 1.24819985e-06
Iter: 15 loss: 1.21244136e-06
Iter: 16 loss: 1.22334723e-06
Iter: 17 loss: 1.18684898e-06
Iter: 18 loss: 1.16322008e-06
Iter: 19 loss: 1.269108e-06
Iter: 20 loss: 1.15855596e-06
Iter: 21 loss: 1.14076647e-06
Iter: 22 loss: 1.15416537e-06
Iter: 23 loss: 1.12986265e-06
Iter: 24 loss: 1.11019835e-06
Iter: 25 loss: 1.11014242e-06
Iter: 26 loss: 1.10192821e-06
Iter: 27 loss: 1.08460199e-06
Iter: 28 loss: 1.37487382e-06
Iter: 29 loss: 1.08413428e-06
Iter: 30 loss: 1.06803941e-06
Iter: 31 loss: 1.10739279e-06
Iter: 32 loss: 1.06227503e-06
Iter: 33 loss: 1.05222784e-06
Iter: 34 loss: 1.05872607e-06
Iter: 35 loss: 1.04582784e-06
Iter: 36 loss: 1.03278e-06
Iter: 37 loss: 1.07106405e-06
Iter: 38 loss: 1.02874276e-06
Iter: 39 loss: 1.02763613e-06
Iter: 40 loss: 1.02279182e-06
Iter: 41 loss: 1.01788567e-06
Iter: 42 loss: 1.00811212e-06
Iter: 43 loss: 1.19674633e-06
Iter: 44 loss: 1.00803686e-06
Iter: 45 loss: 9.98287e-07
Iter: 46 loss: 1.06929156e-06
Iter: 47 loss: 9.97500479e-07
Iter: 48 loss: 9.86287432e-07
Iter: 49 loss: 9.8971077e-07
Iter: 50 loss: 9.78229536e-07
Iter: 51 loss: 9.70720748e-07
Iter: 52 loss: 1.04609353e-06
Iter: 53 loss: 9.70449605e-07
Iter: 54 loss: 9.63264e-07
Iter: 55 loss: 9.67753635e-07
Iter: 56 loss: 9.58683131e-07
Iter: 57 loss: 9.54099278e-07
Iter: 58 loss: 9.54957727e-07
Iter: 59 loss: 9.50693277e-07
Iter: 60 loss: 9.48243724e-07
Iter: 61 loss: 9.47802e-07
Iter: 62 loss: 9.45082093e-07
Iter: 63 loss: 9.45353747e-07
Iter: 64 loss: 9.42954159e-07
Iter: 65 loss: 9.40503e-07
Iter: 66 loss: 9.42966608e-07
Iter: 67 loss: 9.3917032e-07
Iter: 68 loss: 9.36096512e-07
Iter: 69 loss: 9.37721893e-07
Iter: 70 loss: 9.34043385e-07
Iter: 71 loss: 9.29588225e-07
Iter: 72 loss: 9.36761239e-07
Iter: 73 loss: 9.27521683e-07
Iter: 74 loss: 9.24214589e-07
Iter: 75 loss: 9.24223173e-07
Iter: 76 loss: 9.20520108e-07
Iter: 77 loss: 9.28258203e-07
Iter: 78 loss: 9.19098227e-07
Iter: 79 loss: 9.17274349e-07
Iter: 80 loss: 9.15769249e-07
Iter: 81 loss: 9.15257942e-07
Iter: 82 loss: 9.12201699e-07
Iter: 83 loss: 9.51308266e-07
Iter: 84 loss: 9.12179246e-07
Iter: 85 loss: 9.10803692e-07
Iter: 86 loss: 9.100836e-07
Iter: 87 loss: 9.09432174e-07
Iter: 88 loss: 9.07177707e-07
Iter: 89 loss: 9.22794584e-07
Iter: 90 loss: 9.0697722e-07
Iter: 91 loss: 9.05090928e-07
Iter: 92 loss: 9.0153344e-07
Iter: 93 loss: 9.76295155e-07
Iter: 94 loss: 9.01504677e-07
Iter: 95 loss: 8.98078383e-07
Iter: 96 loss: 9.17606712e-07
Iter: 97 loss: 8.97591e-07
Iter: 98 loss: 8.94697e-07
Iter: 99 loss: 9.37337632e-07
Iter: 100 loss: 8.94680682e-07
Iter: 101 loss: 8.93543415e-07
Iter: 102 loss: 8.90849265e-07
Iter: 103 loss: 9.20650905e-07
Iter: 104 loss: 8.90565559e-07
Iter: 105 loss: 8.8899958e-07
Iter: 106 loss: 8.88969225e-07
Iter: 107 loss: 8.87880503e-07
Iter: 108 loss: 8.87587817e-07
Iter: 109 loss: 8.86916951e-07
Iter: 110 loss: 8.85669294e-07
Iter: 111 loss: 8.99184329e-07
Iter: 112 loss: 8.85642919e-07
Iter: 113 loss: 8.85109046e-07
Iter: 114 loss: 8.85068687e-07
Iter: 115 loss: 8.84614224e-07
Iter: 116 loss: 8.83241796e-07
Iter: 117 loss: 8.85777922e-07
Iter: 118 loss: 8.82349e-07
Iter: 119 loss: 8.8178416e-07
Iter: 120 loss: 8.8140763e-07
Iter: 121 loss: 8.80421908e-07
Iter: 122 loss: 8.79765196e-07
Iter: 123 loss: 8.7939685e-07
Iter: 124 loss: 8.78175e-07
Iter: 125 loss: 8.8497319e-07
Iter: 126 loss: 8.78003277e-07
Iter: 127 loss: 8.76744366e-07
Iter: 128 loss: 8.79324318e-07
Iter: 129 loss: 8.76249487e-07
Iter: 130 loss: 8.75310434e-07
Iter: 131 loss: 8.73980525e-07
Iter: 132 loss: 8.73951876e-07
Iter: 133 loss: 8.73367128e-07
Iter: 134 loss: 8.73066256e-07
Iter: 135 loss: 8.72184444e-07
Iter: 136 loss: 8.70418432e-07
Iter: 137 loss: 9.02885688e-07
Iter: 138 loss: 8.7038336e-07
Iter: 139 loss: 8.68725465e-07
Iter: 140 loss: 8.69348298e-07
Iter: 141 loss: 8.67552046e-07
Iter: 142 loss: 8.65892332e-07
Iter: 143 loss: 8.81092831e-07
Iter: 144 loss: 8.65786944e-07
Iter: 145 loss: 8.64818389e-07
Iter: 146 loss: 8.72926762e-07
Iter: 147 loss: 8.64763e-07
Iter: 148 loss: 8.6412615e-07
Iter: 149 loss: 8.64536844e-07
Iter: 150 loss: 8.63717673e-07
Iter: 151 loss: 8.63212449e-07
Iter: 152 loss: 8.63134915e-07
Iter: 153 loss: 8.62797833e-07
Iter: 154 loss: 8.621397e-07
Iter: 155 loss: 8.77681657e-07
Iter: 156 loss: 8.62138279e-07
Iter: 157 loss: 8.61456385e-07
Iter: 158 loss: 8.63107971e-07
Iter: 159 loss: 8.6123822e-07
Iter: 160 loss: 8.60239652e-07
Iter: 161 loss: 8.63950675e-07
Iter: 162 loss: 8.59968225e-07
Iter: 163 loss: 8.59440377e-07
Iter: 164 loss: 8.590672e-07
Iter: 165 loss: 8.5887973e-07
Iter: 166 loss: 8.57448413e-07
Iter: 167 loss: 8.59898364e-07
Iter: 168 loss: 8.56863721e-07
Iter: 169 loss: 8.55896246e-07
Iter: 170 loss: 8.55774033e-07
Iter: 171 loss: 8.55126132e-07
Iter: 172 loss: 8.54008078e-07
Iter: 173 loss: 8.66100038e-07
Iter: 174 loss: 8.5399256e-07
Iter: 175 loss: 8.52792482e-07
Iter: 176 loss: 8.54743746e-07
Iter: 177 loss: 8.52243375e-07
Iter: 178 loss: 8.51610821e-07
Iter: 179 loss: 8.50615606e-07
Iter: 180 loss: 8.50606739e-07
Iter: 181 loss: 8.49489538e-07
Iter: 182 loss: 8.5269545e-07
Iter: 183 loss: 8.49122955e-07
Iter: 184 loss: 8.48112848e-07
Iter: 185 loss: 8.56262488e-07
Iter: 186 loss: 8.48030197e-07
Iter: 187 loss: 8.47349668e-07
Iter: 188 loss: 8.50237939e-07
Iter: 189 loss: 8.47198748e-07
Iter: 190 loss: 8.46618263e-07
Iter: 191 loss: 8.55945359e-07
Iter: 192 loss: 8.46630144e-07
Iter: 193 loss: 8.4613265e-07
Iter: 194 loss: 8.45821774e-07
Iter: 195 loss: 8.45656928e-07
Iter: 196 loss: 8.45049726e-07
Iter: 197 loss: 8.44607598e-07
Iter: 198 loss: 8.44398642e-07
Iter: 199 loss: 8.4367673e-07
Iter: 200 loss: 8.4362e-07
Iter: 201 loss: 8.43250746e-07
Iter: 202 loss: 8.42464715e-07
Iter: 203 loss: 8.5726964e-07
Iter: 204 loss: 8.42453801e-07
Iter: 205 loss: 8.41706765e-07
Iter: 206 loss: 8.41706878e-07
Iter: 207 loss: 8.41135488e-07
Iter: 208 loss: 8.40259418e-07
Iter: 209 loss: 8.40248845e-07
Iter: 210 loss: 8.39491406e-07
Iter: 211 loss: 8.44182239e-07
Iter: 212 loss: 8.39384597e-07
Iter: 213 loss: 8.38573328e-07
Iter: 214 loss: 8.43321914e-07
Iter: 215 loss: 8.3847118e-07
Iter: 216 loss: 8.38004098e-07
Iter: 217 loss: 8.36914751e-07
Iter: 218 loss: 8.51928121e-07
Iter: 219 loss: 8.36833351e-07
Iter: 220 loss: 8.3572121e-07
Iter: 221 loss: 8.40432904e-07
Iter: 222 loss: 8.35482183e-07
Iter: 223 loss: 8.34520222e-07
Iter: 224 loss: 8.41150552e-07
Iter: 225 loss: 8.34421257e-07
Iter: 226 loss: 8.33635625e-07
Iter: 227 loss: 8.43692874e-07
Iter: 228 loss: 8.33625393e-07
Iter: 229 loss: 8.3323016e-07
Iter: 230 loss: 8.33016031e-07
Iter: 231 loss: 8.32848968e-07
Iter: 232 loss: 8.32352725e-07
Iter: 233 loss: 8.33742e-07
Iter: 234 loss: 8.32239436e-07
Iter: 235 loss: 8.31696184e-07
Iter: 236 loss: 8.36061133e-07
Iter: 237 loss: 8.31652301e-07
Iter: 238 loss: 8.3133591e-07
Iter: 239 loss: 8.30798683e-07
Iter: 240 loss: 8.30783222e-07
Iter: 241 loss: 8.30281692e-07
Iter: 242 loss: 8.30280896e-07
Iter: 243 loss: 8.29852809e-07
Iter: 244 loss: 8.28999362e-07
Iter: 245 loss: 8.47355409e-07
Iter: 246 loss: 8.29003341e-07
Iter: 247 loss: 8.28114253e-07
Iter: 248 loss: 8.31205966e-07
Iter: 249 loss: 8.27870906e-07
Iter: 250 loss: 8.27079248e-07
Iter: 251 loss: 8.39290578e-07
Iter: 252 loss: 8.27078281e-07
Iter: 253 loss: 8.26634391e-07
Iter: 254 loss: 8.25840459e-07
Iter: 255 loss: 8.45321495e-07
Iter: 256 loss: 8.25835798e-07
Iter: 257 loss: 8.24997301e-07
Iter: 258 loss: 8.25539e-07
Iter: 259 loss: 8.24477297e-07
Iter: 260 loss: 8.23826554e-07
Iter: 261 loss: 8.29085252e-07
Iter: 262 loss: 8.23793e-07
Iter: 263 loss: 8.23324513e-07
Iter: 264 loss: 8.23318487e-07
Iter: 265 loss: 8.2303626e-07
Iter: 266 loss: 8.22423601e-07
Iter: 267 loss: 8.28903524e-07
Iter: 268 loss: 8.2238364e-07
Iter: 269 loss: 8.21791559e-07
Iter: 270 loss: 8.2711324e-07
Iter: 271 loss: 8.21753e-07
Iter: 272 loss: 8.21271385e-07
Iter: 273 loss: 8.25322218e-07
Iter: 274 loss: 8.21245351e-07
Iter: 275 loss: 8.20999276e-07
Iter: 276 loss: 8.2109068e-07
Iter: 277 loss: 8.20814364e-07
Iter: 278 loss: 8.20515197e-07
Iter: 279 loss: 8.21900301e-07
Iter: 280 loss: 8.20493312e-07
Iter: 281 loss: 8.20151115e-07
Iter: 282 loss: 8.2075519e-07
Iter: 283 loss: 8.20058915e-07
Iter: 284 loss: 8.19761794e-07
Iter: 285 loss: 8.19194497e-07
Iter: 286 loss: 8.19214506e-07
Iter: 287 loss: 8.19013735e-07
Iter: 288 loss: 8.1891335e-07
Iter: 289 loss: 8.18578769e-07
Iter: 290 loss: 8.17841737e-07
Iter: 291 loss: 8.24795052e-07
Iter: 292 loss: 8.17692353e-07
Iter: 293 loss: 8.16774786e-07
Iter: 294 loss: 8.19630202e-07
Iter: 295 loss: 8.16564238e-07
Iter: 296 loss: 8.15775365e-07
Iter: 297 loss: 8.16136094e-07
Iter: 298 loss: 8.15231886e-07
Iter: 299 loss: 8.15809e-07
Iter: 300 loss: 8.14909185e-07
Iter: 301 loss: 8.14688462e-07
Iter: 302 loss: 8.14076884e-07
Iter: 303 loss: 8.19351271e-07
Iter: 304 loss: 8.14006171e-07
Iter: 305 loss: 8.13316547e-07
Iter: 306 loss: 8.13974225e-07
Iter: 307 loss: 8.12973326e-07
Iter: 308 loss: 8.12635676e-07
Iter: 309 loss: 8.12584403e-07
Iter: 310 loss: 8.12187864e-07
Iter: 311 loss: 8.12257326e-07
Iter: 312 loss: 8.11912173e-07
Iter: 313 loss: 8.11644554e-07
Iter: 314 loss: 8.11738801e-07
Iter: 315 loss: 8.11422296e-07
Iter: 316 loss: 8.10930715e-07
Iter: 317 loss: 8.13304837e-07
Iter: 318 loss: 8.10855795e-07
Iter: 319 loss: 8.1054111e-07
Iter: 320 loss: 8.11163375e-07
Iter: 321 loss: 8.10441861e-07
Iter: 322 loss: 8.10111828e-07
Iter: 323 loss: 8.09648839e-07
Iter: 324 loss: 8.09649123e-07
Iter: 325 loss: 8.09041694e-07
Iter: 326 loss: 8.13319843e-07
Iter: 327 loss: 8.09002074e-07
Iter: 328 loss: 8.08628556e-07
Iter: 329 loss: 8.08617187e-07
Iter: 330 loss: 8.08252e-07
Iter: 331 loss: 8.07414722e-07
Iter: 332 loss: 8.17474586e-07
Iter: 333 loss: 8.07359811e-07
Iter: 334 loss: 8.06680646e-07
Iter: 335 loss: 8.08888103e-07
Iter: 336 loss: 8.06471576e-07
Iter: 337 loss: 8.06080038e-07
Iter: 338 loss: 8.06073388e-07
Iter: 339 loss: 8.05618e-07
Iter: 340 loss: 8.05628815e-07
Iter: 341 loss: 8.05218406e-07
Iter: 342 loss: 8.04897923e-07
Iter: 343 loss: 8.04633373e-07
Iter: 344 loss: 8.04530032e-07
Iter: 345 loss: 8.04218587e-07
Iter: 346 loss: 8.04208696e-07
Iter: 347 loss: 8.03849844e-07
Iter: 348 loss: 8.03509351e-07
Iter: 349 loss: 8.03430908e-07
Iter: 350 loss: 8.03036755e-07
Iter: 351 loss: 8.03044941e-07
Iter: 352 loss: 8.0271252e-07
Iter: 353 loss: 8.02267e-07
Iter: 354 loss: 8.02260388e-07
Iter: 355 loss: 8.01921772e-07
Iter: 356 loss: 8.01351575e-07
Iter: 357 loss: 8.0139182e-07
Iter: 358 loss: 8.0097584e-07
Iter: 359 loss: 8.05510808e-07
Iter: 360 loss: 8.00984196e-07
Iter: 361 loss: 8.00749376e-07
Iter: 362 loss: 8.00898817e-07
Iter: 363 loss: 8.00574753e-07
Iter: 364 loss: 8.00330781e-07
Iter: 365 loss: 8.00328053e-07
Iter: 366 loss: 8.00118642e-07
Iter: 367 loss: 7.99859265e-07
Iter: 368 loss: 7.99876261e-07
Iter: 369 loss: 7.99472275e-07
Iter: 370 loss: 7.99037e-07
Iter: 371 loss: 7.98979499e-07
Iter: 372 loss: 7.99095915e-07
Iter: 373 loss: 7.98712335e-07
Iter: 374 loss: 7.9848428e-07
Iter: 375 loss: 7.97878101e-07
Iter: 376 loss: 8.01192527e-07
Iter: 377 loss: 7.97691655e-07
Iter: 378 loss: 7.97281359e-07
Iter: 379 loss: 7.97250777e-07
Iter: 380 loss: 7.96853158e-07
Iter: 381 loss: 7.97885605e-07
Iter: 382 loss: 7.96717814e-07
Iter: 383 loss: 7.96453094e-07
Iter: 384 loss: 7.96135623e-07
Iter: 385 loss: 7.96129768e-07
Iter: 386 loss: 7.95870733e-07
Iter: 387 loss: 7.95875394e-07
Iter: 388 loss: 7.95623066e-07
Iter: 389 loss: 7.95502501e-07
Iter: 390 loss: 7.95382277e-07
Iter: 391 loss: 7.95071571e-07
Iter: 392 loss: 7.94741254e-07
Iter: 393 loss: 7.94701236e-07
Iter: 394 loss: 7.94078233e-07
Iter: 395 loss: 7.96889367e-07
Iter: 396 loss: 7.93967558e-07
Iter: 397 loss: 7.93800609e-07
Iter: 398 loss: 7.93700963e-07
Iter: 399 loss: 7.93522872e-07
Iter: 400 loss: 7.93150207e-07
Iter: 401 loss: 7.98835572e-07
Iter: 402 loss: 7.9315555e-07
Iter: 403 loss: 7.9274821e-07
Iter: 404 loss: 7.94842094e-07
Iter: 405 loss: 7.92663855e-07
Iter: 406 loss: 7.92446713e-07
Iter: 407 loss: 7.92466949e-07
Iter: 408 loss: 7.922539e-07
Iter: 409 loss: 7.92169658e-07
Iter: 410 loss: 7.92111337e-07
Iter: 411 loss: 7.91963089e-07
Iter: 412 loss: 7.91655509e-07
Iter: 413 loss: 7.9805875e-07
Iter: 414 loss: 7.91662728e-07
Iter: 415 loss: 7.91388516e-07
Iter: 416 loss: 7.95348114e-07
Iter: 417 loss: 7.91384309e-07
Iter: 418 loss: 7.91116349e-07
Iter: 419 loss: 7.91000105e-07
Iter: 420 loss: 7.90918648e-07
Iter: 421 loss: 7.90616809e-07
Iter: 422 loss: 7.91503851e-07
Iter: 423 loss: 7.90519039e-07
Iter: 424 loss: 7.90159049e-07
Iter: 425 loss: 7.91144089e-07
Iter: 426 loss: 7.90030185e-07
Iter: 427 loss: 7.89837429e-07
Iter: 428 loss: 7.89450723e-07
Iter: 429 loss: 7.89461296e-07
Iter: 430 loss: 7.89204137e-07
Iter: 431 loss: 7.89200556e-07
Iter: 432 loss: 7.88970965e-07
Iter: 433 loss: 7.90074409e-07
Iter: 434 loss: 7.88915202e-07
Iter: 435 loss: 7.88718637e-07
Iter: 436 loss: 7.88222337e-07
Iter: 437 loss: 7.94106256e-07
Iter: 438 loss: 7.88186071e-07
Iter: 439 loss: 7.87685508e-07
Iter: 440 loss: 7.9060942e-07
Iter: 441 loss: 7.87607178e-07
Iter: 442 loss: 7.87103659e-07
Iter: 443 loss: 7.88476939e-07
Iter: 444 loss: 7.86953194e-07
Iter: 445 loss: 7.86611736e-07
Iter: 446 loss: 7.87895374e-07
Iter: 447 loss: 7.86503961e-07
Iter: 448 loss: 7.86357418e-07
Iter: 449 loss: 7.86305293e-07
Iter: 450 loss: 7.86191549e-07
Iter: 451 loss: 7.85945076e-07
Iter: 452 loss: 7.88654347e-07
Iter: 453 loss: 7.85943598e-07
Iter: 454 loss: 7.85846964e-07
Iter: 455 loss: 7.85830593e-07
Iter: 456 loss: 7.85712302e-07
Iter: 457 loss: 7.85503914e-07
Iter: 458 loss: 7.89663659e-07
Iter: 459 loss: 7.85526595e-07
Iter: 460 loss: 7.85285067e-07
Iter: 461 loss: 7.8526682e-07
Iter: 462 loss: 7.85094358e-07
Iter: 463 loss: 7.84769e-07
Iter: 464 loss: 7.853331e-07
Iter: 465 loss: 7.84622841e-07
Iter: 466 loss: 7.8410244e-07
Iter: 467 loss: 7.88177317e-07
Iter: 468 loss: 7.84103e-07
Iter: 469 loss: 7.83809526e-07
Iter: 470 loss: 7.83240353e-07
Iter: 471 loss: 7.96004088e-07
Iter: 472 loss: 7.83235407e-07
Iter: 473 loss: 7.82826874e-07
Iter: 474 loss: 7.82821076e-07
Iter: 475 loss: 7.82507414e-07
Iter: 476 loss: 7.85018074e-07
Iter: 477 loss: 7.8248587e-07
Iter: 478 loss: 7.82271457e-07
Iter: 479 loss: 7.82022653e-07
Iter: 480 loss: 7.82003042e-07
Iter: 481 loss: 7.81699214e-07
Iter: 482 loss: 7.8256943e-07
Iter: 483 loss: 7.81601159e-07
Iter: 484 loss: 7.81230881e-07
Iter: 485 loss: 7.81479912e-07
Iter: 486 loss: 7.80969572e-07
Iter: 487 loss: 7.80695245e-07
Iter: 488 loss: 7.83245468e-07
Iter: 489 loss: 7.80672735e-07
Iter: 490 loss: 7.80313144e-07
Iter: 491 loss: 7.81157041e-07
Iter: 492 loss: 7.80169785e-07
Iter: 493 loss: 7.7998061e-07
Iter: 494 loss: 7.79973902e-07
Iter: 495 loss: 7.79793e-07
Iter: 496 loss: 7.79649781e-07
Iter: 497 loss: 7.79623861e-07
Iter: 498 loss: 7.79479365e-07
Iter: 499 loss: 7.79164793e-07
Iter: 500 loss: 7.84936e-07
Iter: 501 loss: 7.7918e-07
Iter: 502 loss: 7.78955155e-07
Iter: 503 loss: 7.81127483e-07
Iter: 504 loss: 7.78937135e-07
Iter: 505 loss: 7.78712433e-07
Iter: 506 loss: 7.79240054e-07
Iter: 507 loss: 7.786374e-07
Iter: 508 loss: 7.78413153e-07
Iter: 509 loss: 7.78119841e-07
Iter: 510 loss: 7.78108188e-07
Iter: 511 loss: 7.77990067e-07
Iter: 512 loss: 7.77908554e-07
Iter: 513 loss: 7.77733646e-07
Iter: 514 loss: 7.77296805e-07
Iter: 515 loss: 7.82219161e-07
Iter: 516 loss: 7.7725764e-07
Iter: 517 loss: 7.76809e-07
Iter: 518 loss: 7.7796426e-07
Iter: 519 loss: 7.7665095e-07
Iter: 520 loss: 7.76263505e-07
Iter: 521 loss: 7.78724257e-07
Iter: 522 loss: 7.7622667e-07
Iter: 523 loss: 7.75952401e-07
Iter: 524 loss: 7.77475066e-07
Iter: 525 loss: 7.75943306e-07
Iter: 526 loss: 7.75717638e-07
Iter: 527 loss: 7.780788e-07
Iter: 528 loss: 7.75709964e-07
Iter: 529 loss: 7.7556939e-07
Iter: 530 loss: 7.75275453e-07
Iter: 531 loss: 7.79634888e-07
Iter: 532 loss: 7.75257718e-07
Iter: 533 loss: 7.75129e-07
Iter: 534 loss: 7.75074056e-07
Iter: 535 loss: 7.74921432e-07
Iter: 536 loss: 7.74606292e-07
Iter: 537 loss: 7.78774734e-07
Iter: 538 loss: 7.74597083e-07
Iter: 539 loss: 7.74259604e-07
Iter: 540 loss: 7.74810303e-07
Iter: 541 loss: 7.74102716e-07
Iter: 542 loss: 7.73833222e-07
Iter: 543 loss: 7.77567834e-07
Iter: 544 loss: 7.73829242e-07
Iter: 545 loss: 7.73578506e-07
Iter: 546 loss: 7.74343562e-07
Iter: 547 loss: 7.73483293e-07
Iter: 548 loss: 7.73345505e-07
Iter: 549 loss: 7.73238128e-07
Iter: 550 loss: 7.73177703e-07
Iter: 551 loss: 7.72893088e-07
Iter: 552 loss: 7.76071829e-07
Iter: 553 loss: 7.72902808e-07
Iter: 554 loss: 7.72764e-07
Iter: 555 loss: 7.72482508e-07
Iter: 556 loss: 7.72465967e-07
Iter: 557 loss: 7.72147303e-07
Iter: 558 loss: 7.71764917e-07
Iter: 559 loss: 7.71700627e-07
Iter: 560 loss: 7.71289933e-07
Iter: 561 loss: 7.77762807e-07
Iter: 562 loss: 7.71277769e-07
Iter: 563 loss: 7.70942222e-07
Iter: 564 loss: 7.75763169e-07
Iter: 565 loss: 7.70954841e-07
Iter: 566 loss: 7.70840472e-07
Iter: 567 loss: 7.70569045e-07
Iter: 568 loss: 7.75118792e-07
Iter: 569 loss: 7.70572342e-07
Iter: 570 loss: 7.70422218e-07
Iter: 571 loss: 7.70384759e-07
Iter: 572 loss: 7.70232646e-07
Iter: 573 loss: 7.700271e-07
Iter: 574 loss: 7.70023462e-07
Iter: 575 loss: 7.69785572e-07
Iter: 576 loss: 7.70175575e-07
Iter: 577 loss: 7.69665292e-07
Iter: 578 loss: 7.69407052e-07
Iter: 579 loss: 7.69994e-07
Iter: 580 loss: 7.69300186e-07
Iter: 581 loss: 7.69083897e-07
Iter: 582 loss: 7.72412818e-07
Iter: 583 loss: 7.69076962e-07
Iter: 584 loss: 7.68892619e-07
Iter: 585 loss: 7.68615337e-07
Iter: 586 loss: 7.6862e-07
Iter: 587 loss: 7.68301391e-07
Iter: 588 loss: 7.68350731e-07
Iter: 589 loss: 7.6804082e-07
Iter: 590 loss: 7.67912752e-07
Iter: 591 loss: 7.67845847e-07
Iter: 592 loss: 7.6765059e-07
Iter: 593 loss: 7.67406846e-07
Iter: 594 loss: 7.67396841e-07
Iter: 595 loss: 7.67126494e-07
Iter: 596 loss: 7.6723677e-07
Iter: 597 loss: 7.66953235e-07
Iter: 598 loss: 7.66631729e-07
Iter: 599 loss: 7.68530413e-07
Iter: 600 loss: 7.6660649e-07
Iter: 601 loss: 7.66341941e-07
Iter: 602 loss: 7.70209681e-07
Iter: 603 loss: 7.66335461e-07
Iter: 604 loss: 7.66202618e-07
Iter: 605 loss: 7.65908851e-07
Iter: 606 loss: 7.68910922e-07
Iter: 607 loss: 7.65886739e-07
Iter: 608 loss: 7.65817276e-07
Iter: 609 loss: 7.6572519e-07
Iter: 610 loss: 7.65609116e-07
Iter: 611 loss: 7.65444724e-07
Iter: 612 loss: 7.65426307e-07
Iter: 613 loss: 7.65223263e-07
Iter: 614 loss: 7.65796699e-07
Iter: 615 loss: 7.65169716e-07
Iter: 616 loss: 7.64980882e-07
Iter: 617 loss: 7.66623714e-07
Iter: 618 loss: 7.6495553e-07
Iter: 619 loss: 7.64823085e-07
Iter: 620 loss: 7.6478625e-07
Iter: 621 loss: 7.64729407e-07
Iter: 622 loss: 7.64474805e-07
Iter: 623 loss: 7.64445929e-07
Iter: 624 loss: 7.64270794e-07
Iter: 625 loss: 7.6394349e-07
Iter: 626 loss: 7.64518518e-07
Iter: 627 loss: 7.6383958e-07
Iter: 628 loss: 7.63622438e-07
Iter: 629 loss: 7.6359e-07
Iter: 630 loss: 7.63419678e-07
Iter: 631 loss: 7.63121875e-07
Iter: 632 loss: 7.67195786e-07
Iter: 633 loss: 7.63099195e-07
Iter: 634 loss: 7.62845332e-07
Iter: 635 loss: 7.66091944e-07
Iter: 636 loss: 7.62850448e-07
Iter: 637 loss: 7.62640411e-07
Iter: 638 loss: 7.64443939e-07
Iter: 639 loss: 7.62648881e-07
Iter: 640 loss: 7.62551736e-07
Iter: 641 loss: 7.62275818e-07
Iter: 642 loss: 7.64082927e-07
Iter: 643 loss: 7.62214142e-07
Iter: 644 loss: 7.62233924e-07
Iter: 645 loss: 7.62082095e-07
Iter: 646 loss: 7.61979e-07
Iter: 647 loss: 7.61792535e-07
Iter: 648 loss: 7.66016e-07
Iter: 649 loss: 7.61793842e-07
Iter: 650 loss: 7.61620299e-07
Iter: 651 loss: 7.61839829e-07
Iter: 652 loss: 7.61511728e-07
Iter: 653 loss: 7.61417709e-07
Iter: 654 loss: 7.61377578e-07
Iter: 655 loss: 7.61271e-07
Iter: 656 loss: 7.61043509e-07
Iter: 657 loss: 7.65215759e-07
Iter: 658 loss: 7.61053911e-07
Iter: 659 loss: 7.60838475e-07
Iter: 660 loss: 7.6157221e-07
Iter: 661 loss: 7.60768103e-07
Iter: 662 loss: 7.60491957e-07
Iter: 663 loss: 7.6088736e-07
Iter: 664 loss: 7.6041465e-07
Iter: 665 loss: 7.60274e-07
Iter: 666 loss: 7.60280216e-07
Iter: 667 loss: 7.60137e-07
Iter: 668 loss: 7.59851162e-07
Iter: 669 loss: 7.63817e-07
Iter: 670 loss: 7.5985497e-07
Iter: 671 loss: 7.59599e-07
Iter: 672 loss: 7.60107923e-07
Iter: 673 loss: 7.5948094e-07
Iter: 674 loss: 7.59381e-07
Iter: 675 loss: 7.59339912e-07
Iter: 676 loss: 7.59204227e-07
Iter: 677 loss: 7.5895e-07
Iter: 678 loss: 7.58973556e-07
Iter: 679 loss: 7.58759711e-07
Iter: 680 loss: 7.58964575e-07
Iter: 681 loss: 7.58639828e-07
Iter: 682 loss: 7.58504427e-07
Iter: 683 loss: 7.58476745e-07
Iter: 684 loss: 7.58384658e-07
Iter: 685 loss: 7.58089868e-07
Iter: 686 loss: 7.6077356e-07
Iter: 687 loss: 7.58052465e-07
Iter: 688 loss: 7.57800876e-07
Iter: 689 loss: 7.59429099e-07
Iter: 690 loss: 7.57765065e-07
Iter: 691 loss: 7.57577482e-07
Iter: 692 loss: 7.59698423e-07
Iter: 693 loss: 7.57577368e-07
Iter: 694 loss: 7.57412636e-07
Iter: 695 loss: 7.57229373e-07
Iter: 696 loss: 7.57198e-07
Iter: 697 loss: 7.56973236e-07
Iter: 698 loss: 7.58056331e-07
Iter: 699 loss: 7.56912186e-07
Iter: 700 loss: 7.56746886e-07
Iter: 701 loss: 7.58999136e-07
Iter: 702 loss: 7.56739837e-07
Iter: 703 loss: 7.5661012e-07
Iter: 704 loss: 7.56597785e-07
Iter: 705 loss: 7.56498366e-07
Iter: 706 loss: 7.56330905e-07
Iter: 707 loss: 7.56077725e-07
Iter: 708 loss: 7.56067777e-07
Iter: 709 loss: 7.5584677e-07
Iter: 710 loss: 7.56804639e-07
Iter: 711 loss: 7.55795213e-07
Iter: 712 loss: 7.5560007e-07
Iter: 713 loss: 7.55581368e-07
Iter: 714 loss: 7.55470239e-07
Iter: 715 loss: 7.55198755e-07
Iter: 716 loss: 7.59248962e-07
Iter: 717 loss: 7.55190399e-07
Iter: 718 loss: 7.54980931e-07
Iter: 719 loss: 7.5558188e-07
Iter: 720 loss: 7.54873554e-07
Iter: 721 loss: 7.54671873e-07
Iter: 722 loss: 7.54660675e-07
Iter: 723 loss: 7.54551309e-07
Iter: 724 loss: 7.54246628e-07
Iter: 725 loss: 7.5741184e-07
Iter: 726 loss: 7.54240773e-07
Iter: 727 loss: 7.53900736e-07
Iter: 728 loss: 7.55347969e-07
Iter: 729 loss: 7.53865947e-07
Iter: 730 loss: 7.53637096e-07
Iter: 731 loss: 7.55855467e-07
Iter: 732 loss: 7.53610152e-07
Iter: 733 loss: 7.53371182e-07
Iter: 734 loss: 7.53683594e-07
Iter: 735 loss: 7.53273071e-07
Iter: 736 loss: 7.53053e-07
Iter: 737 loss: 7.53500785e-07
Iter: 738 loss: 7.52986125e-07
Iter: 739 loss: 7.52767562e-07
Iter: 740 loss: 7.54108441e-07
Iter: 741 loss: 7.52767505e-07
Iter: 742 loss: 7.52535925e-07
Iter: 743 loss: 7.52649612e-07
Iter: 744 loss: 7.52449637e-07
Iter: 745 loss: 7.52260348e-07
Iter: 746 loss: 7.52046333e-07
Iter: 747 loss: 7.52021094e-07
Iter: 748 loss: 7.51884443e-07
Iter: 749 loss: 7.51838e-07
Iter: 750 loss: 7.51619496e-07
Iter: 751 loss: 7.51579933e-07
Iter: 752 loss: 7.5147193e-07
Iter: 753 loss: 7.51281277e-07
Iter: 754 loss: 7.50942263e-07
Iter: 755 loss: 7.59244699e-07
Iter: 756 loss: 7.5094465e-07
Iter: 757 loss: 7.50804475e-07
Iter: 758 loss: 7.50723075e-07
Iter: 759 loss: 7.5050923e-07
Iter: 760 loss: 7.50380082e-07
Iter: 761 loss: 7.50332561e-07
Iter: 762 loss: 7.50105926e-07
Iter: 763 loss: 7.49916921e-07
Iter: 764 loss: 7.4983808e-07
Iter: 765 loss: 7.49613832e-07
Iter: 766 loss: 7.51177936e-07
Iter: 767 loss: 7.49567789e-07
Iter: 768 loss: 7.49447565e-07
Iter: 769 loss: 7.49441938e-07
Iter: 770 loss: 7.49335868e-07
Iter: 771 loss: 7.49262426e-07
Iter: 772 loss: 7.49219396e-07
Iter: 773 loss: 7.4905563e-07
Iter: 774 loss: 7.49112417e-07
Iter: 775 loss: 7.48977243e-07
Iter: 776 loss: 7.487223e-07
Iter: 777 loss: 7.49799483e-07
Iter: 778 loss: 7.4870411e-07
Iter: 779 loss: 7.48505e-07
Iter: 780 loss: 7.49916524e-07
Iter: 781 loss: 7.48475031e-07
Iter: 782 loss: 7.48298078e-07
Iter: 783 loss: 7.47944739e-07
Iter: 784 loss: 7.5647165e-07
Iter: 785 loss: 7.47962758e-07
Iter: 786 loss: 7.47782963e-07
Iter: 787 loss: 7.47761305e-07
Iter: 788 loss: 7.47555e-07
Iter: 789 loss: 7.47625336e-07
Iter: 790 loss: 7.47406148e-07
Iter: 791 loss: 7.4726745e-07
Iter: 792 loss: 7.47025751e-07
Iter: 793 loss: 7.47013e-07
Iter: 794 loss: 7.46875e-07
Iter: 795 loss: 7.46844705e-07
Iter: 796 loss: 7.46684e-07
Iter: 797 loss: 7.4654821e-07
Iter: 798 loss: 7.46504725e-07
Iter: 799 loss: 7.46324417e-07
Iter: 800 loss: 7.46531896e-07
Iter: 801 loss: 7.46222099e-07
Iter: 802 loss: 7.46052137e-07
Iter: 803 loss: 7.48411e-07
Iter: 804 loss: 7.46037699e-07
Iter: 805 loss: 7.4593305e-07
Iter: 806 loss: 7.45983812e-07
Iter: 807 loss: 7.45889508e-07
Iter: 808 loss: 7.45756552e-07
Iter: 809 loss: 7.4576667e-07
Iter: 810 loss: 7.45663669e-07
Iter: 811 loss: 7.45511556e-07
Iter: 812 loss: 7.45528382e-07
Iter: 813 loss: 7.45408443e-07
Iter: 814 loss: 7.45700845e-07
Iter: 815 loss: 7.45383488e-07
Iter: 816 loss: 7.45231318e-07
Iter: 817 loss: 7.45020884e-07
Iter: 818 loss: 7.49936589e-07
Iter: 819 loss: 7.45026227e-07
Iter: 820 loss: 7.44941531e-07
Iter: 821 loss: 7.44875933e-07
Iter: 822 loss: 7.4473661e-07
Iter: 823 loss: 7.44497186e-07
Iter: 824 loss: 7.47929562e-07
Iter: 825 loss: 7.44473709e-07
Iter: 826 loss: 7.44149418e-07
Iter: 827 loss: 7.44534645e-07
Iter: 828 loss: 7.44010322e-07
Iter: 829 loss: 7.43811597e-07
Iter: 830 loss: 7.43805231e-07
Iter: 831 loss: 7.43605426e-07
Iter: 832 loss: 7.43929036e-07
Iter: 833 loss: 7.43566943e-07
Iter: 834 loss: 7.43422561e-07
Iter: 835 loss: 7.43200474e-07
Iter: 836 loss: 7.4319496e-07
Iter: 837 loss: 7.4305575e-07
Iter: 838 loss: 7.43044552e-07
Iter: 839 loss: 7.42889313e-07
Iter: 840 loss: 7.42974407e-07
Iter: 841 loss: 7.42783755e-07
Iter: 842 loss: 7.42604925e-07
Iter: 843 loss: 7.42416887e-07
Iter: 844 loss: 7.42399322e-07
Iter: 845 loss: 7.42225211e-07
Iter: 846 loss: 7.42236e-07
Iter: 847 loss: 7.42034899e-07
Iter: 848 loss: 7.42174507e-07
Iter: 849 loss: 7.41939118e-07
Iter: 850 loss: 7.4177683e-07
Iter: 851 loss: 7.41926e-07
Iter: 852 loss: 7.41721522e-07
Iter: 853 loss: 7.41540134e-07
Iter: 854 loss: 7.42929899e-07
Iter: 855 loss: 7.41551048e-07
Iter: 856 loss: 7.41409963e-07
Iter: 857 loss: 7.41515692e-07
Iter: 858 loss: 7.41309123e-07
Iter: 859 loss: 7.41163035e-07
Iter: 860 loss: 7.40979203e-07
Iter: 861 loss: 7.41001941e-07
Iter: 862 loss: 7.40690666e-07
Iter: 863 loss: 7.41241934e-07
Iter: 864 loss: 7.40569533e-07
Iter: 865 loss: 7.40422934e-07
Iter: 866 loss: 7.4039184e-07
Iter: 867 loss: 7.4025e-07
Iter: 868 loss: 7.39950508e-07
Iter: 869 loss: 7.42897896e-07
Iter: 870 loss: 7.39917709e-07
Iter: 871 loss: 7.39607e-07
Iter: 872 loss: 7.40366545e-07
Iter: 873 loss: 7.39483426e-07
Iter: 874 loss: 7.39238089e-07
Iter: 875 loss: 7.41004669e-07
Iter: 876 loss: 7.39220241e-07
Iter: 877 loss: 7.39071368e-07
Iter: 878 loss: 7.4115826e-07
Iter: 879 loss: 7.39065058e-07
Iter: 880 loss: 7.38896574e-07
Iter: 881 loss: 7.38820688e-07
Iter: 882 loss: 7.38748383e-07
Iter: 883 loss: 7.38598885e-07
Iter: 884 loss: 7.39157713e-07
Iter: 885 loss: 7.38556e-07
Iter: 886 loss: 7.38390668e-07
Iter: 887 loss: 7.39592167e-07
Iter: 888 loss: 7.38396466e-07
Iter: 889 loss: 7.3828528e-07
Iter: 890 loss: 7.38180802e-07
Iter: 891 loss: 7.3816085e-07
Iter: 892 loss: 7.38028575e-07
Iter: 893 loss: 7.38387598e-07
Iter: 894 loss: 7.37954792e-07
Iter: 895 loss: 7.37819846e-07
Iter: 896 loss: 7.37813139e-07
Iter: 897 loss: 7.37686435e-07
Iter: 898 loss: 7.37711105e-07
Iter: 899 loss: 7.37585708e-07
Iter: 900 loss: 7.37479581e-07
Iter: 901 loss: 7.37380788e-07
Iter: 902 loss: 7.37346113e-07
Iter: 903 loss: 7.37254254e-07
Iter: 904 loss: 7.3722731e-07
Iter: 905 loss: 7.3712954e-07
Iter: 906 loss: 7.36874e-07
Iter: 907 loss: 7.38986785e-07
Iter: 908 loss: 7.36815196e-07
Iter: 909 loss: 7.36532911e-07
Iter: 910 loss: 7.37173536e-07
Iter: 911 loss: 7.36455718e-07
Iter: 912 loss: 7.36158313e-07
Iter: 913 loss: 7.38839731e-07
Iter: 914 loss: 7.3618e-07
Iter: 915 loss: 7.3587853e-07
Iter: 916 loss: 7.36902621e-07
Iter: 917 loss: 7.35817366e-07
Iter: 918 loss: 7.35675371e-07
Iter: 919 loss: 7.35685489e-07
Iter: 920 loss: 7.35583967e-07
Iter: 921 loss: 7.35530648e-07
Iter: 922 loss: 7.35492165e-07
Iter: 923 loss: 7.35432707e-07
Iter: 924 loss: 7.35277922e-07
Iter: 925 loss: 7.36995901e-07
Iter: 926 loss: 7.3528156e-07
Iter: 927 loss: 7.35129e-07
Iter: 928 loss: 7.3606185e-07
Iter: 929 loss: 7.35120466e-07
Iter: 930 loss: 7.35036679e-07
Iter: 931 loss: 7.36162804e-07
Iter: 932 loss: 7.35040658e-07
Iter: 933 loss: 7.34909349e-07
Iter: 934 loss: 7.34668049e-07
Iter: 935 loss: 7.36050197e-07
Iter: 936 loss: 7.34589e-07
Iter: 937 loss: 7.34315961e-07
Iter: 938 loss: 7.36968218e-07
Iter: 939 loss: 7.3429635e-07
Iter: 940 loss: 7.3408728e-07
Iter: 941 loss: 7.35755066e-07
Iter: 942 loss: 7.34036e-07
Iter: 943 loss: 7.33918114e-07
Iter: 944 loss: 7.33663796e-07
Iter: 945 loss: 7.33661295e-07
Iter: 946 loss: 7.33453476e-07
Iter: 947 loss: 7.33703644e-07
Iter: 948 loss: 7.33370882e-07
Iter: 949 loss: 7.33275442e-07
Iter: 950 loss: 7.332373e-07
Iter: 951 loss: 7.33135039e-07
Iter: 952 loss: 7.33031129e-07
Iter: 953 loss: 7.33018169e-07
Iter: 954 loss: 7.32889873e-07
Iter: 955 loss: 7.33478771e-07
Iter: 956 loss: 7.32875606e-07
Iter: 957 loss: 7.32704393e-07
Iter: 958 loss: 7.33229399e-07
Iter: 959 loss: 7.32651245e-07
Iter: 960 loss: 7.32531817e-07
Iter: 961 loss: 7.32380897e-07
Iter: 962 loss: 7.32333831e-07
Iter: 963 loss: 7.32192518e-07
Iter: 964 loss: 7.32199055e-07
Iter: 965 loss: 7.32077638e-07
Iter: 966 loss: 7.3225749e-07
Iter: 967 loss: 7.31985097e-07
Iter: 968 loss: 7.31876753e-07
Iter: 969 loss: 7.31654154e-07
Iter: 970 loss: 7.31661203e-07
Iter: 971 loss: 7.31545697e-07
Iter: 972 loss: 7.31541718e-07
Iter: 973 loss: 7.31413536e-07
Iter: 974 loss: 7.31373e-07
Iter: 975 loss: 7.31288424e-07
Iter: 976 loss: 7.31115961e-07
Iter: 977 loss: 7.30959414e-07
Iter: 978 loss: 7.30923432e-07
Iter: 979 loss: 7.3069981e-07
Iter: 980 loss: 7.30891315e-07
Iter: 981 loss: 7.30557474e-07
Iter: 982 loss: 7.30416104e-07
Iter: 983 loss: 7.30420879e-07
Iter: 984 loss: 7.302325e-07
Iter: 985 loss: 7.30191687e-07
Iter: 986 loss: 7.30117563e-07
Iter: 987 loss: 7.29978069e-07
Iter: 988 loss: 7.30528825e-07
Iter: 989 loss: 7.29939813e-07
Iter: 990 loss: 7.29770363e-07
Iter: 991 loss: 7.30428042e-07
Iter: 992 loss: 7.29742169e-07
Iter: 993 loss: 7.29659064e-07
Iter: 994 loss: 7.29542194e-07
Iter: 995 loss: 7.29523435e-07
Iter: 996 loss: 7.2943908e-07
Iter: 997 loss: 7.29429246e-07
Iter: 998 loss: 7.29347335e-07
Iter: 999 loss: 7.2916896e-07
Iter: 1000 loss: 7.32148e-07
Iter: 1001 loss: 7.29184649e-07
Iter: 1002 loss: 7.28988425e-07
Iter: 1003 loss: 7.29628482e-07
Iter: 1004 loss: 7.28938232e-07
Iter: 1005 loss: 7.2878413e-07
Iter: 1006 loss: 7.3065064e-07
Iter: 1007 loss: 7.28785608e-07
Iter: 1008 loss: 7.28638383e-07
Iter: 1009 loss: 7.28410043e-07
Iter: 1010 loss: 7.28397481e-07
Iter: 1011 loss: 7.28149871e-07
Iter: 1012 loss: 7.28047667e-07
Iter: 1013 loss: 7.27873612e-07
Iter: 1014 loss: 7.2754375e-07
Iter: 1015 loss: 7.28835289e-07
Iter: 1016 loss: 7.2746252e-07
Iter: 1017 loss: 7.27245947e-07
Iter: 1018 loss: 7.27224574e-07
Iter: 1019 loss: 7.2698856e-07
Iter: 1020 loss: 7.27601e-07
Iter: 1021 loss: 7.26923645e-07
Iter: 1022 loss: 7.26796543e-07
Iter: 1023 loss: 7.26684789e-07
Iter: 1024 loss: 7.26648409e-07
Iter: 1025 loss: 7.26486064e-07
Iter: 1026 loss: 7.26465373e-07
Iter: 1027 loss: 7.26397502e-07
Iter: 1028 loss: 7.26237658e-07
Iter: 1029 loss: 7.28307839e-07
Iter: 1030 loss: 7.26241694e-07
Iter: 1031 loss: 7.26158e-07
Iter: 1032 loss: 7.26111068e-07
Iter: 1033 loss: 7.26058886e-07
Iter: 1034 loss: 7.259124e-07
Iter: 1035 loss: 7.2765e-07
Iter: 1036 loss: 7.25908933e-07
Iter: 1037 loss: 7.25711516e-07
Iter: 1038 loss: 7.2639466e-07
Iter: 1039 loss: 7.25684345e-07
Iter: 1040 loss: 7.25490281e-07
Iter: 1041 loss: 7.26599581e-07
Iter: 1042 loss: 7.25474933e-07
Iter: 1043 loss: 7.2535363e-07
Iter: 1044 loss: 7.25235054e-07
Iter: 1045 loss: 7.25191967e-07
Iter: 1046 loss: 7.25021323e-07
Iter: 1047 loss: 7.2506532e-07
Iter: 1048 loss: 7.24890299e-07
Iter: 1049 loss: 7.24665597e-07
Iter: 1050 loss: 7.25107498e-07
Iter: 1051 loss: 7.24570498e-07
Iter: 1052 loss: 7.24301344e-07
Iter: 1053 loss: 7.25093514e-07
Iter: 1054 loss: 7.24262122e-07
Iter: 1055 loss: 7.24027529e-07
Iter: 1056 loss: 7.27338943e-07
Iter: 1057 loss: 7.24038216e-07
Iter: 1058 loss: 7.23860126e-07
Iter: 1059 loss: 7.23692494e-07
Iter: 1060 loss: 7.23635708e-07
Iter: 1061 loss: 7.23422772e-07
Iter: 1062 loss: 7.26560074e-07
Iter: 1063 loss: 7.23419419e-07
Iter: 1064 loss: 7.23197502e-07
Iter: 1065 loss: 7.22963705e-07
Iter: 1066 loss: 7.22925961e-07
Iter: 1067 loss: 7.22714276e-07
Iter: 1068 loss: 7.24869835e-07
Iter: 1069 loss: 7.22708592e-07
Iter: 1070 loss: 7.22522486e-07
Iter: 1071 loss: 7.23315793e-07
Iter: 1072 loss: 7.22479115e-07
Iter: 1073 loss: 7.22390382e-07
Iter: 1074 loss: 7.22173866e-07
Iter: 1075 loss: 7.26341568e-07
Iter: 1076 loss: 7.22190748e-07
Iter: 1077 loss: 7.22101845e-07
Iter: 1078 loss: 7.22070126e-07
Iter: 1079 loss: 7.21983838e-07
Iter: 1080 loss: 7.2189323e-07
Iter: 1081 loss: 7.21842412e-07
Iter: 1082 loss: 7.21744755e-07
Iter: 1083 loss: 7.21540118e-07
Iter: 1084 loss: 7.21532047e-07
Iter: 1085 loss: 7.21315473e-07
Iter: 1086 loss: 7.21821834e-07
Iter: 1087 loss: 7.21202355e-07
Iter: 1088 loss: 7.20970206e-07
Iter: 1089 loss: 7.20982484e-07
Iter: 1090 loss: 7.20776029e-07
Iter: 1091 loss: 7.21185927e-07
Iter: 1092 loss: 7.20715377e-07
Iter: 1093 loss: 7.20517335e-07
Iter: 1094 loss: 7.20783305e-07
Iter: 1095 loss: 7.20410128e-07
Iter: 1096 loss: 7.202392e-07
Iter: 1097 loss: 7.22322341e-07
Iter: 1098 loss: 7.20216804e-07
Iter: 1099 loss: 7.20073331e-07
Iter: 1100 loss: 7.19863124e-07
Iter: 1101 loss: 7.19861873e-07
Iter: 1102 loss: 7.19681566e-07
Iter: 1103 loss: 7.19655418e-07
Iter: 1104 loss: 7.1947e-07
Iter: 1105 loss: 7.19526611e-07
Iter: 1106 loss: 7.19347042e-07
Iter: 1107 loss: 7.19216473e-07
Iter: 1108 loss: 7.1891634e-07
Iter: 1109 loss: 7.24965446e-07
Iter: 1110 loss: 7.18928163e-07
Iter: 1111 loss: 7.18857e-07
Iter: 1112 loss: 7.18749732e-07
Iter: 1113 loss: 7.18662761e-07
Iter: 1114 loss: 7.18458296e-07
Iter: 1115 loss: 7.18434535e-07
Iter: 1116 loss: 7.18302e-07
Iter: 1117 loss: 7.18581646e-07
Iter: 1118 loss: 7.18225635e-07
Iter: 1119 loss: 7.18058402e-07
Iter: 1120 loss: 7.18105071e-07
Iter: 1121 loss: 7.17955288e-07
Iter: 1122 loss: 7.17810849e-07
Iter: 1123 loss: 7.18623e-07
Iter: 1124 loss: 7.17769581e-07
Iter: 1125 loss: 7.1760212e-07
Iter: 1126 loss: 7.18309252e-07
Iter: 1127 loss: 7.17563694e-07
Iter: 1128 loss: 7.17343596e-07
Iter: 1129 loss: 7.18352567e-07
Iter: 1130 loss: 7.17304033e-07
Iter: 1131 loss: 7.17103831e-07
Iter: 1132 loss: 7.17181251e-07
Iter: 1133 loss: 7.16973375e-07
Iter: 1134 loss: 7.16737645e-07
Iter: 1135 loss: 7.18176125e-07
Iter: 1136 loss: 7.16706268e-07
Iter: 1137 loss: 7.16482191e-07
Iter: 1138 loss: 7.17772195e-07
Iter: 1139 loss: 7.16480599e-07
Iter: 1140 loss: 7.16355657e-07
Iter: 1141 loss: 7.16164777e-07
Iter: 1142 loss: 7.16151135e-07
Iter: 1143 loss: 7.15967246e-07
Iter: 1144 loss: 7.15923932e-07
Iter: 1145 loss: 7.158568e-07
Iter: 1146 loss: 7.15660917e-07
Iter: 1147 loss: 7.18454373e-07
Iter: 1148 loss: 7.15682916e-07
Iter: 1149 loss: 7.15466115e-07
Iter: 1150 loss: 7.16741738e-07
Iter: 1151 loss: 7.15442184e-07
Iter: 1152 loss: 7.15284614e-07
Iter: 1153 loss: 7.16769e-07
Iter: 1154 loss: 7.15251417e-07
Iter: 1155 loss: 7.15122269e-07
Iter: 1156 loss: 7.14821169e-07
Iter: 1157 loss: 7.18084607e-07
Iter: 1158 loss: 7.14766657e-07
Iter: 1159 loss: 7.14477551e-07
Iter: 1160 loss: 7.15722081e-07
Iter: 1161 loss: 7.14392343e-07
Iter: 1162 loss: 7.14142857e-07
Iter: 1163 loss: 7.15827582e-07
Iter: 1164 loss: 7.14119324e-07
Iter: 1165 loss: 7.13972554e-07
Iter: 1166 loss: 7.14683665e-07
Iter: 1167 loss: 7.13947e-07
Iter: 1168 loss: 7.13823169e-07
Iter: 1169 loss: 7.15135457e-07
Iter: 1170 loss: 7.13827262e-07
Iter: 1171 loss: 7.13727104e-07
Iter: 1172 loss: 7.13706584e-07
Iter: 1173 loss: 7.1365514e-07
Iter: 1174 loss: 7.13512691e-07
Iter: 1175 loss: 7.13659688e-07
Iter: 1176 loss: 7.13495069e-07
Iter: 1177 loss: 7.13336078e-07
Iter: 1178 loss: 7.15437295e-07
Iter: 1179 loss: 7.13314364e-07
Iter: 1180 loss: 7.1321017e-07
Iter: 1181 loss: 7.12995586e-07
Iter: 1182 loss: 7.16635839e-07
Iter: 1183 loss: 7.13e-07
Iter: 1184 loss: 7.12782708e-07
Iter: 1185 loss: 7.12807264e-07
Iter: 1186 loss: 7.12649921e-07
Iter: 1187 loss: 7.12365704e-07
Iter: 1188 loss: 7.16232876e-07
Iter: 1189 loss: 7.12368e-07
Iter: 1190 loss: 7.12085921e-07
Iter: 1191 loss: 7.12923793e-07
Iter: 1192 loss: 7.11968937e-07
Iter: 1193 loss: 7.11967687e-07
Iter: 1194 loss: 7.11887083e-07
Iter: 1195 loss: 7.11804e-07
Iter: 1196 loss: 7.11596158e-07
Iter: 1197 loss: 7.128736e-07
Iter: 1198 loss: 7.11551138e-07
Iter: 1199 loss: 7.11358382e-07
Iter: 1200 loss: 7.1269119e-07
Iter: 1201 loss: 7.11332461e-07
Iter: 1202 loss: 7.11149369e-07
Iter: 1203 loss: 7.11455527e-07
Iter: 1204 loss: 7.11081611e-07
Iter: 1205 loss: 7.10960705e-07
Iter: 1206 loss: 7.10962922e-07
Iter: 1207 loss: 7.10867653e-07
Iter: 1208 loss: 7.10731626e-07
Iter: 1209 loss: 7.10730319e-07
Iter: 1210 loss: 7.10564791e-07
Iter: 1211 loss: 7.111787e-07
Iter: 1212 loss: 7.10531253e-07
Iter: 1213 loss: 7.10374366e-07
Iter: 1214 loss: 7.11882535e-07
Iter: 1215 loss: 7.10388122e-07
Iter: 1216 loss: 7.10248457e-07
Iter: 1217 loss: 7.1027614e-07
Iter: 1218 loss: 7.10180927e-07
Iter: 1219 loss: 7.1005752e-07
Iter: 1220 loss: 7.10729068e-07
Iter: 1221 loss: 7.1006491e-07
Iter: 1222 loss: 7.09948552e-07
Iter: 1223 loss: 7.09792289e-07
Iter: 1224 loss: 7.09764095e-07
Iter: 1225 loss: 7.09573783e-07
Iter: 1226 loss: 7.0971123e-07
Iter: 1227 loss: 7.09468736e-07
Iter: 1228 loss: 7.09256483e-07
Iter: 1229 loss: 7.09765914e-07
Iter: 1230 loss: 7.09197479e-07
Iter: 1231 loss: 7.08936454e-07
Iter: 1232 loss: 7.11375606e-07
Iter: 1233 loss: 7.08941684e-07
Iter: 1234 loss: 7.08815946e-07
Iter: 1235 loss: 7.08602784e-07
Iter: 1236 loss: 7.08612674e-07
Iter: 1237 loss: 7.0843555e-07
Iter: 1238 loss: 7.09134611e-07
Iter: 1239 loss: 7.083745e-07
Iter: 1240 loss: 7.08267066e-07
Iter: 1241 loss: 7.08269681e-07
Iter: 1242 loss: 7.0814e-07
Iter: 1243 loss: 7.08160201e-07
Iter: 1244 loss: 7.08081416e-07
Iter: 1245 loss: 7.07948061e-07
Iter: 1246 loss: 7.08011896e-07
Iter: 1247 loss: 7.07862e-07
Iter: 1248 loss: 7.07725e-07
Iter: 1249 loss: 7.0862734e-07
Iter: 1250 loss: 7.0771938e-07
Iter: 1251 loss: 7.07593927e-07
Iter: 1252 loss: 7.08958339e-07
Iter: 1253 loss: 7.07587105e-07
Iter: 1254 loss: 7.07508e-07
Iter: 1255 loss: 7.0743755e-07
Iter: 1256 loss: 7.07427375e-07
Iter: 1257 loss: 7.07292e-07
Iter: 1258 loss: 7.08422363e-07
Iter: 1259 loss: 7.07264576e-07
Iter: 1260 loss: 7.0714043e-07
Iter: 1261 loss: 7.06925675e-07
Iter: 1262 loss: 7.10488166e-07
Iter: 1263 loss: 7.06877586e-07
Iter: 1264 loss: 7.06702167e-07
Iter: 1265 loss: 7.08721473e-07
Iter: 1266 loss: 7.06694891e-07
Iter: 1267 loss: 7.06513731e-07
Iter: 1268 loss: 7.0770318e-07
Iter: 1269 loss: 7.06516e-07
Iter: 1270 loss: 7.06390097e-07
Iter: 1271 loss: 7.06166475e-07
Iter: 1272 loss: 7.11003622e-07
Iter: 1273 loss: 7.06151354e-07
Iter: 1274 loss: 7.05974799e-07
Iter: 1275 loss: 7.06485253e-07
Iter: 1276 loss: 7.05895e-07
Iter: 1277 loss: 7.05739694e-07
Iter: 1278 loss: 7.08044411e-07
Iter: 1279 loss: 7.05728098e-07
Iter: 1280 loss: 7.05591447e-07
Iter: 1281 loss: 7.05797447e-07
Iter: 1282 loss: 7.0552926e-07
Iter: 1283 loss: 7.05403181e-07
Iter: 1284 loss: 7.05298817e-07
Iter: 1285 loss: 7.052613e-07
Iter: 1286 loss: 7.05095772e-07
Iter: 1287 loss: 7.07538106e-07
Iter: 1288 loss: 7.05095829e-07
Iter: 1289 loss: 7.04980209e-07
Iter: 1290 loss: 7.05629873e-07
Iter: 1291 loss: 7.04951958e-07
Iter: 1292 loss: 7.04819854e-07
Iter: 1293 loss: 7.04784725e-07
Iter: 1294 loss: 7.04738227e-07
Iter: 1295 loss: 7.04559284e-07
Iter: 1296 loss: 7.05871798e-07
Iter: 1297 loss: 7.04544618e-07
Iter: 1298 loss: 7.04418e-07
Iter: 1299 loss: 7.04224e-07
Iter: 1300 loss: 7.0422675e-07
Iter: 1301 loss: 7.04079753e-07
Iter: 1302 loss: 7.06125149e-07
Iter: 1303 loss: 7.04090553e-07
Iter: 1304 loss: 7.03922183e-07
Iter: 1305 loss: 7.04360332e-07
Iter: 1306 loss: 7.03876253e-07
Iter: 1307 loss: 7.03772344e-07
Iter: 1308 loss: 7.03582941e-07
Iter: 1309 loss: 7.03581634e-07
Iter: 1310 loss: 7.03456408e-07
Iter: 1311 loss: 7.03834701e-07
Iter: 1312 loss: 7.03378817e-07
Iter: 1313 loss: 7.03235457e-07
Iter: 1314 loss: 7.05057914e-07
Iter: 1315 loss: 7.03230285e-07
Iter: 1316 loss: 7.03055321e-07
Iter: 1317 loss: 7.03080104e-07
Iter: 1318 loss: 7.0295215e-07
Iter: 1319 loss: 7.02786792e-07
Iter: 1320 loss: 7.02802481e-07
Iter: 1321 loss: 7.02684702e-07
Iter: 1322 loss: 7.02558e-07
Iter: 1323 loss: 7.02536283e-07
Iter: 1324 loss: 7.02411114e-07
Iter: 1325 loss: 7.02585e-07
Iter: 1326 loss: 7.02366151e-07
Iter: 1327 loss: 7.02242858e-07
Iter: 1328 loss: 7.02562033e-07
Iter: 1329 loss: 7.02205398e-07
Iter: 1330 loss: 7.02080797e-07
Iter: 1331 loss: 7.02592388e-07
Iter: 1332 loss: 7.02069087e-07
Iter: 1333 loss: 7.01971885e-07
Iter: 1334 loss: 7.01758211e-07
Iter: 1335 loss: 7.06169203e-07
Iter: 1336 loss: 7.01760825e-07
Iter: 1337 loss: 7.01636395e-07
Iter: 1338 loss: 7.01621047e-07
Iter: 1339 loss: 7.01495594e-07
Iter: 1340 loss: 7.01458134e-07
Iter: 1341 loss: 7.01367185e-07
Iter: 1342 loss: 7.01224053e-07
Iter: 1343 loss: 7.01038459e-07
Iter: 1344 loss: 7.01027716e-07
Iter: 1345 loss: 7.00814e-07
Iter: 1346 loss: 7.02319767e-07
Iter: 1347 loss: 7.00801138e-07
Iter: 1348 loss: 7.00667044e-07
Iter: 1349 loss: 7.0066551e-07
Iter: 1350 loss: 7.00553301e-07
Iter: 1351 loss: 7.00559838e-07
Iter: 1352 loss: 7.00476221e-07
Iter: 1353 loss: 7.00377598e-07
Iter: 1354 loss: 7.0027005e-07
Iter: 1355 loss: 7.00226678e-07
Iter: 1356 loss: 7.00055125e-07
Iter: 1357 loss: 7.01792828e-07
Iter: 1358 loss: 7.00047849e-07
Iter: 1359 loss: 6.99917223e-07
Iter: 1360 loss: 7.01257193e-07
Iter: 1361 loss: 6.99908071e-07
Iter: 1362 loss: 6.99824e-07
Iter: 1363 loss: 6.99813427e-07
Iter: 1364 loss: 6.99752377e-07
Iter: 1365 loss: 6.99638463e-07
Iter: 1366 loss: 7.00024145e-07
Iter: 1367 loss: 6.9960339e-07
Iter: 1368 loss: 6.99479e-07
Iter: 1369 loss: 6.99460202e-07
Iter: 1370 loss: 6.99382667e-07
Iter: 1371 loss: 6.99226575e-07
Iter: 1372 loss: 7.00216674e-07
Iter: 1373 loss: 6.99199063e-07
Iter: 1374 loss: 6.99039902e-07
Iter: 1375 loss: 6.99213729e-07
Iter: 1376 loss: 6.98975e-07
Iter: 1377 loss: 6.98836914e-07
Iter: 1378 loss: 6.98699864e-07
Iter: 1379 loss: 6.98689178e-07
Iter: 1380 loss: 6.98468114e-07
Iter: 1381 loss: 6.99096518e-07
Iter: 1382 loss: 6.98399163e-07
Iter: 1383 loss: 6.98217946e-07
Iter: 1384 loss: 7.00085e-07
Iter: 1385 loss: 6.98219196e-07
Iter: 1386 loss: 6.98015242e-07
Iter: 1387 loss: 6.98204815e-07
Iter: 1388 loss: 6.97912924e-07
Iter: 1389 loss: 6.97790483e-07
Iter: 1390 loss: 6.9774336e-07
Iter: 1391 loss: 6.97624557e-07
Iter: 1392 loss: 6.97431e-07
Iter: 1393 loss: 6.97954363e-07
Iter: 1394 loss: 6.97358814e-07
Iter: 1395 loss: 6.97258031e-07
Iter: 1396 loss: 6.97250471e-07
Iter: 1397 loss: 6.9714639e-07
Iter: 1398 loss: 6.97462212e-07
Iter: 1399 loss: 6.97104213e-07
Iter: 1400 loss: 6.97025712e-07
Iter: 1401 loss: 6.97107794e-07
Iter: 1402 loss: 6.96975803e-07
Iter: 1403 loss: 6.96874679e-07
Iter: 1404 loss: 6.97849146e-07
Iter: 1405 loss: 6.96877862e-07
Iter: 1406 loss: 6.96773327e-07
Iter: 1407 loss: 6.96687096e-07
Iter: 1408 loss: 6.96651512e-07
Iter: 1409 loss: 6.96514064e-07
Iter: 1410 loss: 6.96854499e-07
Iter: 1411 loss: 6.96444715e-07
Iter: 1412 loss: 6.96295103e-07
Iter: 1413 loss: 6.98083795e-07
Iter: 1414 loss: 6.96301186e-07
Iter: 1415 loss: 6.96214613e-07
Iter: 1416 loss: 6.96027428e-07
Iter: 1417 loss: 6.99371924e-07
Iter: 1418 loss: 6.960218e-07
Iter: 1419 loss: 6.95804886e-07
Iter: 1420 loss: 6.95870426e-07
Iter: 1421 loss: 6.95657207e-07
Iter: 1422 loss: 6.95424433e-07
Iter: 1423 loss: 6.987392e-07
Iter: 1424 loss: 6.95423296e-07
Iter: 1425 loss: 6.95258791e-07
Iter: 1426 loss: 6.9653521e-07
Iter: 1427 loss: 6.95218e-07
Iter: 1428 loss: 6.95109179e-07
Iter: 1429 loss: 6.9485759e-07
Iter: 1430 loss: 6.99749876e-07
Iter: 1431 loss: 6.94871e-07
Iter: 1432 loss: 6.94677112e-07
Iter: 1433 loss: 6.95692108e-07
Iter: 1434 loss: 6.9463141e-07
Iter: 1435 loss: 6.9449186e-07
Iter: 1436 loss: 6.95696087e-07
Iter: 1437 loss: 6.94467417e-07
Iter: 1438 loss: 6.94368737e-07
Iter: 1439 loss: 6.94364758e-07
Iter: 1440 loss: 6.94313826e-07
Iter: 1441 loss: 6.94236576e-07
Iter: 1442 loss: 6.94237315e-07
Iter: 1443 loss: 6.94147161e-07
Iter: 1444 loss: 6.94049618e-07
Iter: 1445 loss: 6.94033758e-07
Iter: 1446 loss: 6.93927916e-07
Iter: 1447 loss: 6.93932236e-07
Iter: 1448 loss: 6.93839866e-07
Iter: 1449 loss: 6.93674224e-07
Iter: 1450 loss: 6.96109851e-07
Iter: 1451 loss: 6.93652737e-07
Iter: 1452 loss: 6.93448442e-07
Iter: 1453 loss: 6.93863512e-07
Iter: 1454 loss: 6.93371476e-07
Iter: 1455 loss: 6.93138873e-07
Iter: 1456 loss: 6.93144727e-07
Iter: 1457 loss: 6.93025413e-07
Iter: 1458 loss: 6.92835215e-07
Iter: 1459 loss: 6.92800427e-07
Iter: 1460 loss: 6.9265036e-07
Iter: 1461 loss: 6.93634206e-07
Iter: 1462 loss: 6.92623303e-07
Iter: 1463 loss: 6.92423214e-07
Iter: 1464 loss: 6.93470895e-07
Iter: 1465 loss: 6.92374556e-07
Iter: 1466 loss: 6.92289859e-07
Iter: 1467 loss: 6.92249444e-07
Iter: 1468 loss: 6.92211756e-07
Iter: 1469 loss: 6.92055664e-07
Iter: 1470 loss: 6.92003766e-07
Iter: 1471 loss: 6.91914408e-07
Iter: 1472 loss: 6.9189241e-07
Iter: 1473 loss: 6.91821e-07
Iter: 1474 loss: 6.91735181e-07
Iter: 1475 loss: 6.91659807e-07
Iter: 1476 loss: 6.91634568e-07
Iter: 1477 loss: 6.91472792e-07
Iter: 1478 loss: 6.9135217e-07
Iter: 1479 loss: 6.91323066e-07
Iter: 1480 loss: 6.91144407e-07
Iter: 1481 loss: 6.9313711e-07
Iter: 1482 loss: 6.9112366e-07
Iter: 1483 loss: 6.90926072e-07
Iter: 1484 loss: 6.91689934e-07
Iter: 1485 loss: 6.90902652e-07
Iter: 1486 loss: 6.90810339e-07
Iter: 1487 loss: 6.90646175e-07
Iter: 1488 loss: 6.90648051e-07
Iter: 1489 loss: 6.90474849e-07
Iter: 1490 loss: 6.91648e-07
Iter: 1491 loss: 6.90442107e-07
Iter: 1492 loss: 6.90318871e-07
Iter: 1493 loss: 6.91729781e-07
Iter: 1494 loss: 6.90303693e-07
Iter: 1495 loss: 6.90208822e-07
Iter: 1496 loss: 6.89969738e-07
Iter: 1497 loss: 6.92299238e-07
Iter: 1498 loss: 6.89941771e-07
Iter: 1499 loss: 6.89691092e-07
Iter: 1500 loss: 6.91978755e-07
Iter: 1501 loss: 6.8966591e-07
Iter: 1502 loss: 6.89408239e-07
Iter: 1503 loss: 6.90586944e-07
Iter: 1504 loss: 6.89383796e-07
Iter: 1505 loss: 6.89204967e-07
Iter: 1506 loss: 6.88896307e-07
Iter: 1507 loss: 6.95670508e-07
Iter: 1508 loss: 6.8890489e-07
Iter: 1509 loss: 6.88588898e-07
Iter: 1510 loss: 6.91363539e-07
Iter: 1511 loss: 6.8857662e-07
Iter: 1512 loss: 6.8854979e-07
Iter: 1513 loss: 6.88487773e-07
Iter: 1514 loss: 6.88428202e-07
Iter: 1515 loss: 6.88299451e-07
Iter: 1516 loss: 6.91269179e-07
Iter: 1517 loss: 6.8830991e-07
Iter: 1518 loss: 6.88192586e-07
Iter: 1519 loss: 6.88444629e-07
Iter: 1520 loss: 6.88146088e-07
Iter: 1521 loss: 6.88004093e-07
Iter: 1522 loss: 6.88239084e-07
Iter: 1523 loss: 6.87949694e-07
Iter: 1524 loss: 6.87818556e-07
Iter: 1525 loss: 6.89692058e-07
Iter: 1526 loss: 6.87782403e-07
Iter: 1527 loss: 6.87692136e-07
Iter: 1528 loss: 6.87523936e-07
Iter: 1529 loss: 6.91460968e-07
Iter: 1530 loss: 6.87526949e-07
Iter: 1531 loss: 6.87312593e-07
Iter: 1532 loss: 6.87571685e-07
Iter: 1533 loss: 6.87208342e-07
Iter: 1534 loss: 6.87107558e-07
Iter: 1535 loss: 6.870942e-07
Iter: 1536 loss: 6.86965e-07
Iter: 1537 loss: 6.86748763e-07
Iter: 1538 loss: 6.90687216e-07
Iter: 1539 loss: 6.86733415e-07
Iter: 1540 loss: 6.86574424e-07
Iter: 1541 loss: 6.88091e-07
Iter: 1542 loss: 6.86581302e-07
Iter: 1543 loss: 6.86396788e-07
Iter: 1544 loss: 6.86963062e-07
Iter: 1545 loss: 6.86349722e-07
Iter: 1546 loss: 6.86221426e-07
Iter: 1547 loss: 6.86102908e-07
Iter: 1548 loss: 6.86057e-07
Iter: 1549 loss: 6.85879513e-07
Iter: 1550 loss: 6.86527414e-07
Iter: 1551 loss: 6.85831594e-07
Iter: 1552 loss: 6.85582961e-07
Iter: 1553 loss: 6.88316845e-07
Iter: 1554 loss: 6.85555165e-07
Iter: 1555 loss: 6.85468251e-07
Iter: 1556 loss: 6.85252587e-07
Iter: 1557 loss: 6.88886928e-07
Iter: 1558 loss: 6.85260261e-07
Iter: 1559 loss: 6.85071768e-07
Iter: 1560 loss: 6.86322096e-07
Iter: 1561 loss: 6.85034e-07
Iter: 1562 loss: 6.84863835e-07
Iter: 1563 loss: 6.85156692e-07
Iter: 1564 loss: 6.84798351e-07
Iter: 1565 loss: 6.84654765e-07
Iter: 1566 loss: 6.85358941e-07
Iter: 1567 loss: 6.84666645e-07
Iter: 1568 loss: 6.84546819e-07
Iter: 1569 loss: 6.84524764e-07
Iter: 1570 loss: 6.84497763e-07
Iter: 1571 loss: 6.8431541e-07
Iter: 1572 loss: 6.85225e-07
Iter: 1573 loss: 6.84274e-07
Iter: 1574 loss: 6.84013457e-07
Iter: 1575 loss: 6.8456859e-07
Iter: 1576 loss: 6.8388124e-07
Iter: 1577 loss: 6.83728842e-07
Iter: 1578 loss: 6.83725773e-07
Iter: 1579 loss: 6.83557346e-07
Iter: 1580 loss: 6.83307121e-07
Iter: 1581 loss: 6.83305245e-07
Iter: 1582 loss: 6.83041776e-07
Iter: 1583 loss: 6.83967528e-07
Iter: 1584 loss: 6.82985842e-07
Iter: 1585 loss: 6.82892e-07
Iter: 1586 loss: 6.82827135e-07
Iter: 1587 loss: 6.82759037e-07
Iter: 1588 loss: 6.8264103e-07
Iter: 1589 loss: 6.82612495e-07
Iter: 1590 loss: 6.82521e-07
Iter: 1591 loss: 6.82504322e-07
Iter: 1592 loss: 6.82443e-07
Iter: 1593 loss: 6.82316227e-07
Iter: 1594 loss: 6.82325776e-07
Iter: 1595 loss: 6.82181621e-07
Iter: 1596 loss: 6.82519499e-07
Iter: 1597 loss: 6.82151267e-07
Iter: 1598 loss: 6.82019049e-07
Iter: 1599 loss: 6.82136317e-07
Iter: 1600 loss: 6.81923154e-07
Iter: 1601 loss: 6.81780875e-07
Iter: 1602 loss: 6.82459358e-07
Iter: 1603 loss: 6.8176314e-07
Iter: 1604 loss: 6.81688334e-07
Iter: 1605 loss: 6.81686117e-07
Iter: 1606 loss: 6.816216e-07
Iter: 1607 loss: 6.81473466e-07
Iter: 1608 loss: 6.84404142e-07
Iter: 1609 loss: 6.81482561e-07
Iter: 1610 loss: 6.81324082e-07
Iter: 1611 loss: 6.81554582e-07
Iter: 1612 loss: 6.81242113e-07
Iter: 1613 loss: 6.81080564e-07
Iter: 1614 loss: 6.82863345e-07
Iter: 1615 loss: 6.81085055e-07
Iter: 1616 loss: 6.80986147e-07
Iter: 1617 loss: 6.8082619e-07
Iter: 1618 loss: 6.80809876e-07
Iter: 1619 loss: 6.80586879e-07
Iter: 1620 loss: 6.80854384e-07
Iter: 1621 loss: 6.80487233e-07
Iter: 1622 loss: 6.80387757e-07
Iter: 1623 loss: 6.80330459e-07
Iter: 1624 loss: 6.80265089e-07
Iter: 1625 loss: 6.80105472e-07
Iter: 1626 loss: 6.80116273e-07
Iter: 1627 loss: 6.80003268e-07
Iter: 1628 loss: 6.80010317e-07
Iter: 1629 loss: 6.79926643e-07
Iter: 1630 loss: 6.79811308e-07
Iter: 1631 loss: 6.82619884e-07
Iter: 1632 loss: 6.79813752e-07
Iter: 1633 loss: 6.79695745e-07
Iter: 1634 loss: 6.80139578e-07
Iter: 1635 loss: 6.79648e-07
Iter: 1636 loss: 6.79564721e-07
Iter: 1637 loss: 6.7986133e-07
Iter: 1638 loss: 6.79572793e-07
Iter: 1639 loss: 6.79468201e-07
Iter: 1640 loss: 6.79342747e-07
Iter: 1641 loss: 6.79295908e-07
Iter: 1642 loss: 6.79180516e-07
Iter: 1643 loss: 6.79185177e-07
Iter: 1644 loss: 6.79065181e-07
Iter: 1645 loss: 6.794304e-07
Iter: 1646 loss: 6.79026527e-07
Iter: 1647 loss: 6.78918e-07
Iter: 1648 loss: 6.78672166e-07
Iter: 1649 loss: 6.81983579e-07
Iter: 1650 loss: 6.78686604e-07
Iter: 1651 loss: 6.78650792e-07
Iter: 1652 loss: 6.78587526e-07
Iter: 1653 loss: 6.78512606e-07
Iter: 1654 loss: 6.78333549e-07
Iter: 1655 loss: 6.78336903e-07
Iter: 1656 loss: 6.78185756e-07
Iter: 1657 loss: 6.78799836e-07
Iter: 1658 loss: 6.7814085e-07
Iter: 1659 loss: 6.7804649e-07
Iter: 1660 loss: 6.78031483e-07
Iter: 1661 loss: 6.77976232e-07
Iter: 1662 loss: 6.77728451e-07
Iter: 1663 loss: 6.80124231e-07
Iter: 1664 loss: 6.77734647e-07
Iter: 1665 loss: 6.77631874e-07
Iter: 1666 loss: 6.77609535e-07
Iter: 1667 loss: 6.77519e-07
Iter: 1668 loss: 6.77509263e-07
Iter: 1669 loss: 6.77459468e-07
Iter: 1670 loss: 6.77319292e-07
Iter: 1671 loss: 6.77430251e-07
Iter: 1672 loss: 6.77288199e-07
Iter: 1673 loss: 6.77137564e-07
Iter: 1674 loss: 6.77236926e-07
Iter: 1675 loss: 6.77051219e-07
Iter: 1676 loss: 6.76898765e-07
Iter: 1677 loss: 6.7781059e-07
Iter: 1678 loss: 6.76885634e-07
Iter: 1679 loss: 6.76780815e-07
Iter: 1680 loss: 6.77480216e-07
Iter: 1681 loss: 6.7677189e-07
Iter: 1682 loss: 6.76681111e-07
Iter: 1683 loss: 6.76792069e-07
Iter: 1684 loss: 6.76613581e-07
Iter: 1685 loss: 6.76535592e-07
Iter: 1686 loss: 6.76402806e-07
Iter: 1687 loss: 6.76372281e-07
Iter: 1688 loss: 6.76248703e-07
Iter: 1689 loss: 6.76257912e-07
Iter: 1690 loss: 6.76130583e-07
Iter: 1691 loss: 6.76136494e-07
Iter: 1692 loss: 6.7601917e-07
Iter: 1693 loss: 6.75923843e-07
Iter: 1694 loss: 6.75924866e-07
Iter: 1695 loss: 6.75843921e-07
Iter: 1696 loss: 6.75633942e-07
Iter: 1697 loss: 6.78276592e-07
Iter: 1698 loss: 6.75633885e-07
Iter: 1699 loss: 6.75477395e-07
Iter: 1700 loss: 6.76705497e-07
Iter: 1701 loss: 6.7548325e-07
Iter: 1702 loss: 6.75325225e-07
Iter: 1703 loss: 6.75943568e-07
Iter: 1704 loss: 6.75263323e-07
Iter: 1705 loss: 6.75146339e-07
Iter: 1706 loss: 6.75111664e-07
Iter: 1707 loss: 6.75052206e-07
Iter: 1708 loss: 6.74909188e-07
Iter: 1709 loss: 6.75126728e-07
Iter: 1710 loss: 6.74832336e-07
Iter: 1711 loss: 6.74689488e-07
Iter: 1712 loss: 6.75393835e-07
Iter: 1713 loss: 6.74674936e-07
Iter: 1714 loss: 6.74557441e-07
Iter: 1715 loss: 6.75106776e-07
Iter: 1716 loss: 6.74547e-07
Iter: 1717 loss: 6.7441772e-07
Iter: 1718 loss: 6.75404294e-07
Iter: 1719 loss: 6.74420448e-07
Iter: 1720 loss: 6.74353259e-07
Iter: 1721 loss: 6.74220928e-07
Iter: 1722 loss: 6.75895308e-07
Iter: 1723 loss: 6.74223656e-07
Iter: 1724 loss: 6.74060516e-07
Iter: 1725 loss: 6.75836816e-07
Iter: 1726 loss: 6.74063585e-07
Iter: 1727 loss: 6.7396553e-07
Iter: 1728 loss: 6.74178352e-07
Iter: 1729 loss: 6.73922841e-07
Iter: 1730 loss: 6.73805459e-07
Iter: 1731 loss: 6.74089449e-07
Iter: 1732 loss: 6.73760155e-07
Iter: 1733 loss: 6.73603836e-07
Iter: 1734 loss: 6.74099283e-07
Iter: 1735 loss: 6.73567797e-07
Iter: 1736 loss: 6.73463887e-07
Iter: 1737 loss: 6.73198315e-07
Iter: 1738 loss: 6.7669e-07
Iter: 1739 loss: 6.73171485e-07
Iter: 1740 loss: 6.73131069e-07
Iter: 1741 loss: 6.7305939e-07
Iter: 1742 loss: 6.7294809e-07
Iter: 1743 loss: 6.72762667e-07
Iter: 1744 loss: 6.7276676e-07
Iter: 1745 loss: 6.7260288e-07
Iter: 1746 loss: 6.7269832e-07
Iter: 1747 loss: 6.72479132e-07
Iter: 1748 loss: 6.72291947e-07
Iter: 1749 loss: 6.73851275e-07
Iter: 1750 loss: 6.72281772e-07
Iter: 1751 loss: 6.72137389e-07
Iter: 1752 loss: 6.72544502e-07
Iter: 1753 loss: 6.72120962e-07
Iter: 1754 loss: 6.71985163e-07
Iter: 1755 loss: 6.73428247e-07
Iter: 1756 loss: 6.71988232e-07
Iter: 1757 loss: 6.71899329e-07
Iter: 1758 loss: 6.71843281e-07
Iter: 1759 loss: 6.71816224e-07
Iter: 1760 loss: 6.71722205e-07
Iter: 1761 loss: 6.72124145e-07
Iter: 1762 loss: 6.71711405e-07
Iter: 1763 loss: 6.71588452e-07
Iter: 1764 loss: 6.72370902e-07
Iter: 1765 loss: 6.71602493e-07
Iter: 1766 loss: 6.71517228e-07
Iter: 1767 loss: 6.71593284e-07
Iter: 1768 loss: 6.71500743e-07
Iter: 1769 loss: 6.71372391e-07
Iter: 1770 loss: 6.7145271e-07
Iter: 1771 loss: 6.71315775e-07
Iter: 1772 loss: 6.7121448e-07
Iter: 1773 loss: 6.71212092e-07
Iter: 1774 loss: 6.71146495e-07
Iter: 1775 loss: 6.7103997e-07
Iter: 1776 loss: 6.72701503e-07
Iter: 1777 loss: 6.71039516e-07
Iter: 1778 loss: 6.70991767e-07
Iter: 1779 loss: 6.70826296e-07
Iter: 1780 loss: 6.7084909e-07
Iter: 1781 loss: 6.70698341e-07
Iter: 1782 loss: 6.70669294e-07
Iter: 1783 loss: 6.70568397e-07
Iter: 1784 loss: 6.70460622e-07
Iter: 1785 loss: 6.70466818e-07
Iter: 1786 loss: 6.70398663e-07
Iter: 1787 loss: 6.7079975e-07
Iter: 1788 loss: 6.70385361e-07
Iter: 1789 loss: 6.70303791e-07
Iter: 1790 loss: 6.70221937e-07
Iter: 1791 loss: 6.70171971e-07
Iter: 1792 loss: 6.70048962e-07
Iter: 1793 loss: 6.70185955e-07
Iter: 1794 loss: 6.69987685e-07
Iter: 1795 loss: 6.69857513e-07
Iter: 1796 loss: 6.70866314e-07
Iter: 1797 loss: 6.6985308e-07
Iter: 1798 loss: 6.69743827e-07
Iter: 1799 loss: 6.69994563e-07
Iter: 1800 loss: 6.69707561e-07
Iter: 1801 loss: 6.69632527e-07
Iter: 1802 loss: 6.70364e-07
Iter: 1803 loss: 6.69615929e-07
Iter: 1804 loss: 6.69519238e-07
Iter: 1805 loss: 6.69495648e-07
Iter: 1806 loss: 6.69436417e-07
Iter: 1807 loss: 6.69379347e-07
Iter: 1808 loss: 6.6957648e-07
Iter: 1809 loss: 6.69347799e-07
Iter: 1810 loss: 6.69262818e-07
Iter: 1811 loss: 6.69610813e-07
Iter: 1812 loss: 6.69247754e-07
Iter: 1813 loss: 6.6915652e-07
Iter: 1814 loss: 6.69002702e-07
Iter: 1815 loss: 6.71977034e-07
Iter: 1816 loss: 6.69001224e-07
Iter: 1817 loss: 6.68854796e-07
Iter: 1818 loss: 6.69190399e-07
Iter: 1819 loss: 6.68772316e-07
Iter: 1820 loss: 6.68600762e-07
Iter: 1821 loss: 6.69407882e-07
Iter: 1822 loss: 6.68593714e-07
Iter: 1823 loss: 6.68416192e-07
Iter: 1824 loss: 6.68713426e-07
Iter: 1825 loss: 6.68371285e-07
Iter: 1826 loss: 6.68232e-07
Iter: 1827 loss: 6.68252824e-07
Iter: 1828 loss: 6.68139e-07
Iter: 1829 loss: 6.6828e-07
Iter: 1830 loss: 6.68106168e-07
Iter: 1831 loss: 6.68044208e-07
Iter: 1832 loss: 6.67936e-07
Iter: 1833 loss: 6.67927907e-07
Iter: 1834 loss: 6.67884649e-07
Iter: 1835 loss: 6.67858558e-07
Iter: 1836 loss: 6.67782388e-07
Iter: 1837 loss: 6.67788527e-07
Iter: 1838 loss: 6.67723384e-07
Iter: 1839 loss: 6.67639824e-07
Iter: 1840 loss: 6.67990662e-07
Iter: 1841 loss: 6.67604866e-07
Iter: 1842 loss: 6.67490724e-07
Iter: 1843 loss: 6.67466907e-07
Iter: 1844 loss: 6.67393238e-07
Iter: 1845 loss: 6.67267159e-07
Iter: 1846 loss: 6.67759764e-07
Iter: 1847 loss: 6.67273355e-07
Iter: 1848 loss: 6.67115728e-07
Iter: 1849 loss: 6.67457812e-07
Iter: 1850 loss: 6.67064739e-07
Iter: 1851 loss: 6.66978451e-07
Iter: 1852 loss: 6.66895517e-07
Iter: 1853 loss: 6.66852941e-07
Iter: 1854 loss: 6.66728624e-07
Iter: 1855 loss: 6.67250333e-07
Iter: 1856 loss: 6.66683434e-07
Iter: 1857 loss: 6.6657725e-07
Iter: 1858 loss: 6.67523864e-07
Iter: 1859 loss: 6.6657077e-07
Iter: 1860 loss: 6.66461e-07
Iter: 1861 loss: 6.67140853e-07
Iter: 1862 loss: 6.66465212e-07
Iter: 1863 loss: 6.66381311e-07
Iter: 1864 loss: 6.6625887e-07
Iter: 1865 loss: 6.69111728e-07
Iter: 1866 loss: 6.66242499e-07
Iter: 1867 loss: 6.6607322e-07
Iter: 1868 loss: 6.66599e-07
Iter: 1869 loss: 6.65989887e-07
Iter: 1870 loss: 6.65962091e-07
Iter: 1871 loss: 6.65910932e-07
Iter: 1872 loss: 6.65857726e-07
Iter: 1873 loss: 6.65690948e-07
Iter: 1874 loss: 6.68727751e-07
Iter: 1875 loss: 6.65700213e-07
Iter: 1876 loss: 6.65580274e-07
Iter: 1877 loss: 6.65582547e-07
Iter: 1878 loss: 6.65496088e-07
Iter: 1879 loss: 6.65409175e-07
Iter: 1880 loss: 6.65381947e-07
Iter: 1881 loss: 6.65304128e-07
Iter: 1882 loss: 6.65314701e-07
Iter: 1883 loss: 6.65240634e-07
Iter: 1884 loss: 6.6518669e-07
Iter: 1885 loss: 6.6516e-07
Iter: 1886 loss: 6.65044467e-07
Iter: 1887 loss: 6.64929644e-07
Iter: 1888 loss: 6.64922197e-07
Iter: 1889 loss: 6.64706818e-07
Iter: 1890 loss: 6.65523487e-07
Iter: 1891 loss: 6.6469039e-07
Iter: 1892 loss: 6.64560389e-07
Iter: 1893 loss: 6.64541403e-07
Iter: 1894 loss: 6.64455968e-07
Iter: 1895 loss: 6.64741549e-07
Iter: 1896 loss: 6.64403501e-07
Iter: 1897 loss: 6.64337279e-07
Iter: 1898 loss: 6.64244453e-07
Iter: 1899 loss: 6.64216827e-07
Iter: 1900 loss: 6.64091601e-07
Iter: 1901 loss: 6.64788161e-07
Iter: 1902 loss: 6.64085178e-07
Iter: 1903 loss: 6.63936078e-07
Iter: 1904 loss: 6.64780544e-07
Iter: 1905 loss: 6.63910441e-07
Iter: 1906 loss: 6.63856952e-07
Iter: 1907 loss: 6.63858202e-07
Iter: 1908 loss: 6.63795845e-07
Iter: 1909 loss: 6.63692958e-07
Iter: 1910 loss: 6.64203185e-07
Iter: 1911 loss: 6.63672267e-07
Iter: 1912 loss: 6.63595642e-07
Iter: 1913 loss: 6.6366e-07
Iter: 1914 loss: 6.63569949e-07
Iter: 1915 loss: 6.63483547e-07
Iter: 1916 loss: 6.64477113e-07
Iter: 1917 loss: 6.63503045e-07
Iter: 1918 loss: 6.63434548e-07
Iter: 1919 loss: 6.63313131e-07
Iter: 1920 loss: 6.6508079e-07
Iter: 1921 loss: 6.63334049e-07
Iter: 1922 loss: 6.63213143e-07
Iter: 1923 loss: 6.63453704e-07
Iter: 1924 loss: 6.63170681e-07
Iter: 1925 loss: 6.63046535e-07
Iter: 1926 loss: 6.63879405e-07
Iter: 1927 loss: 6.63046421e-07
Iter: 1928 loss: 6.62962179e-07
Iter: 1929 loss: 6.63936476e-07
Iter: 1930 loss: 6.62943307e-07
Iter: 1931 loss: 6.62886521e-07
Iter: 1932 loss: 6.62729576e-07
Iter: 1933 loss: 6.62735204e-07
Iter: 1934 loss: 6.62558875e-07
Iter: 1935 loss: 6.62735829e-07
Iter: 1936 loss: 6.62442289e-07
Iter: 1937 loss: 6.62447746e-07
Iter: 1938 loss: 6.62375612e-07
Iter: 1939 loss: 6.62311e-07
Iter: 1940 loss: 6.62146647e-07
Iter: 1941 loss: 6.63923174e-07
Iter: 1942 loss: 6.62141076e-07
Iter: 1943 loss: 6.62056038e-07
Iter: 1944 loss: 6.62059335e-07
Iter: 1945 loss: 6.61978788e-07
Iter: 1946 loss: 6.61934678e-07
Iter: 1947 loss: 6.619e-07
Iter: 1948 loss: 6.61840943e-07
Iter: 1949 loss: 6.62558705e-07
Iter: 1950 loss: 6.61830086e-07
Iter: 1951 loss: 6.61769604e-07
Iter: 1952 loss: 6.61881927e-07
Iter: 1953 loss: 6.61731292e-07
Iter: 1954 loss: 6.61661488e-07
Iter: 1955 loss: 6.61613058e-07
Iter: 1956 loss: 6.61620732e-07
Iter: 1957 loss: 6.61550473e-07
Iter: 1958 loss: 6.61734248e-07
Iter: 1959 loss: 6.6148948e-07
Iter: 1960 loss: 6.61394324e-07
Iter: 1961 loss: 6.62252944e-07
Iter: 1962 loss: 6.61398531e-07
Iter: 1963 loss: 6.6127609e-07
Iter: 1964 loss: 6.61533818e-07
Iter: 1965 loss: 6.61255854e-07
Iter: 1966 loss: 6.61169111e-07
Iter: 1967 loss: 6.61051672e-07
Iter: 1968 loss: 6.61034164e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6
+ date
Wed Oct 21 11:11:28 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1 --function f1 --psi 0 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00059c3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00059ea950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000595c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00058f6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00058f6268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000595c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005b48158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00058201e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005820c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00058207b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057f02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00056daa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00056fcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057550d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005765510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00056fcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057cad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005755048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005696400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000567d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005687950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0000440158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005687840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0000436ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00003c36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005743bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005743840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057130d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057131e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00003ef9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000041e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00003efbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00003ef510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000026a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00002c00d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00002b6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.016922584
test_loss: 0.015464297
train_loss: 0.0063496646
test_loss: 0.0059371376
train_loss: 0.003089107
test_loss: 0.003022571
train_loss: 0.0021938388
test_loss: 0.0021872728
train_loss: 0.001975308
test_loss: 0.0027110185
train_loss: 0.0019643097
test_loss: 0.002601942
train_loss: 0.0019080758
test_loss: 0.0019133923
train_loss: 0.0021056659
test_loss: 0.0017717603
train_loss: 0.0016050718
test_loss: 0.0017009822
train_loss: 0.0016690297
test_loss: 0.0018089985
train_loss: 0.001665068
test_loss: 0.0019052639
train_loss: 0.0016067089
test_loss: 0.0016424713
train_loss: 0.002147658
test_loss: 0.0019735156
train_loss: 0.0015222677
test_loss: 0.0018868005
train_loss: 0.0015097215
test_loss: 0.0016064183
train_loss: 0.0014903806
test_loss: 0.0017969136
train_loss: 0.0016922746
test_loss: 0.001505116
train_loss: 0.0016726566
test_loss: 0.0021887969
train_loss: 0.0013760906
test_loss: 0.0016504409
train_loss: 0.001709378
test_loss: 0.0015969785
train_loss: 0.0018729742
test_loss: 0.0017861648
train_loss: 0.0015492444
test_loss: 0.001585423
train_loss: 0.0015760302
test_loss: 0.001649873
train_loss: 0.0014495514
test_loss: 0.0015191706
train_loss: 0.0017869922
test_loss: 0.0018864614
train_loss: 0.001761206
test_loss: 0.0021156517
train_loss: 0.0015662911
test_loss: 0.0017466969
train_loss: 0.0020064472
test_loss: 0.0017615614
train_loss: 0.0016644712
test_loss: 0.0016114335
train_loss: 0.0015713078
test_loss: 0.0017106709
train_loss: 0.0016948169
test_loss: 0.0016824155
train_loss: 0.0016120143
test_loss: 0.0015696279
train_loss: 0.0016563793
test_loss: 0.001630859
train_loss: 0.0014369226
test_loss: 0.0013843499
train_loss: 0.0014580477
test_loss: 0.0013461156
train_loss: 0.001629058
test_loss: 0.0018538943
train_loss: 0.0015573909
test_loss: 0.0014201169
train_loss: 0.0015552009
test_loss: 0.0014488866
train_loss: 0.00125279
test_loss: 0.0013614801
train_loss: 0.0015773998
test_loss: 0.0013969357
train_loss: 0.0014909613
test_loss: 0.0015515485
train_loss: 0.0014807436
test_loss: 0.001746128
train_loss: 0.001831281
test_loss: 0.0016882151
train_loss: 0.0015653595
test_loss: 0.001998451
train_loss: 0.0016148051
test_loss: 0.001671084
train_loss: 0.0013571302
test_loss: 0.0014476929
train_loss: 0.0014889971
test_loss: 0.0016643885
train_loss: 0.001616266
test_loss: 0.0014187436
train_loss: 0.001514863
test_loss: 0.0016089982
train_loss: 0.0014825511
test_loss: 0.0016601634
train_loss: 0.0014008313
test_loss: 0.0015862854
train_loss: 0.001678202
test_loss: 0.001606837
train_loss: 0.0014063264
test_loss: 0.0014378119
train_loss: 0.0015040135
test_loss: 0.0015010572
train_loss: 0.0014791754
test_loss: 0.0016107592
train_loss: 0.0013533931
test_loss: 0.0015067608
train_loss: 0.0016981496
test_loss: 0.0014657441
train_loss: 0.0015334378
test_loss: 0.0017094803
train_loss: 0.0018167256
test_loss: 0.0016347872
train_loss: 0.0014203442
test_loss: 0.0016187441
train_loss: 0.0015117088
test_loss: 0.0015525066
train_loss: 0.0015543187
test_loss: 0.0014492383
train_loss: 0.0014590274
test_loss: 0.0013432403
train_loss: 0.0015504617
test_loss: 0.001577946
train_loss: 0.0016386111
test_loss: 0.001490223
train_loss: 0.0015919269
test_loss: 0.0018390053
train_loss: 0.001436933
test_loss: 0.001492549
train_loss: 0.0016541091
test_loss: 0.0017509698
train_loss: 0.0016228138
test_loss: 0.0016980194
train_loss: 0.0014463514
test_loss: 0.0016705908
train_loss: 0.0012550371
test_loss: 0.0013835294
train_loss: 0.001457377
test_loss: 0.0014833568
train_loss: 0.0015047905
test_loss: 0.0015296239
train_loss: 0.0016093969
test_loss: 0.0013756385
train_loss: 0.0012608708
test_loss: 0.001434123
train_loss: 0.001690814
test_loss: 0.001616609
train_loss: 0.0013980505
test_loss: 0.0013808404
train_loss: 0.0017304267
test_loss: 0.0014900482
train_loss: 0.0016442093
test_loss: 0.0017383727
train_loss: 0.0016035688
test_loss: 0.0016672862
train_loss: 0.001699951
test_loss: 0.001395916
train_loss: 0.0014253702
test_loss: 0.0013624275
train_loss: 0.0015505585
test_loss: 0.0013897975
train_loss: 0.0017446792
test_loss: 0.0015159204
train_loss: 0.0015683649
test_loss: 0.0016961771
train_loss: 0.0016817334
test_loss: 0.0015562355
train_loss: 0.0015133785
test_loss: 0.0013989205
train_loss: 0.0017835121
test_loss: 0.0015945291
train_loss: 0.0018691765
test_loss: 0.0016857558
train_loss: 0.0015021773
test_loss: 0.001222532
train_loss: 0.0014149977
test_loss: 0.0013994314
train_loss: 0.0014292074
test_loss: 0.0017725952
train_loss: 0.0013064295
test_loss: 0.001556989
train_loss: 0.0015352789
test_loss: 0.0015697753
train_loss: 0.0016067596
test_loss: 0.0014879855
train_loss: 0.0014163388
test_loss: 0.0015228107
train_loss: 0.0014957119
test_loss: 0.0016224006
train_loss: 0.0015375295
test_loss: 0.0014367609
train_loss: 0.001469916
test_loss: 0.001765552
train_loss: 0.0017126185
test_loss: 0.0017036767
train_loss: 0.0012309423
test_loss: 0.0013365182
train_loss: 0.0013032254
test_loss: 0.0015215301
train_loss: 0.0014518165
test_loss: 0.0014942202
train_loss: 0.0023503336
test_loss: 0.0016641191
train_loss: 0.002608803
test_loss: 0.0018877266
train_loss: 0.0016163946
test_loss: 0.0018367948
train_loss: 0.0014017837
test_loss: 0.0014600441
train_loss: 0.001494664
test_loss: 0.0014045136
train_loss: 0.0013374386
test_loss: 0.0013290385
train_loss: 0.0015582292
test_loss: 0.0015251532
train_loss: 0.0018308467
test_loss: 0.001556598
train_loss: 0.0016534973
test_loss: 0.0017743967
train_loss: 0.0015802435
test_loss: 0.0017543897
train_loss: 0.001415628
test_loss: 0.0014168991
train_loss: 0.0013375932
test_loss: 0.0013343024
train_loss: 0.0015199832
test_loss: 0.0015842433
train_loss: 0.0015959642
test_loss: 0.0013427549
train_loss: 0.001436499
test_loss: 0.0014353557
train_loss: 0.0013819995
test_loss: 0.0014002947
train_loss: 0.0015829769
test_loss: 0.0015794951
train_loss: 0.0014774184
test_loss: 0.0014373057
train_loss: 0.0013282129
test_loss: 0.0017254116
train_loss: 0.0012097623
test_loss: 0.001515211
train_loss: 0.001661952
test_loss: 0.0014381306
train_loss: 0.001312728
test_loss: 0.0015822144
train_loss: 0.0013082575
test_loss: 0.0018800459
train_loss: 0.001391081
test_loss: 0.0014863592
train_loss: 0.0015928118
test_loss: 0.0015750225
train_loss: 0.0017537797
test_loss: 0.0018665208
train_loss: 0.0012924635
test_loss: 0.0014773329
train_loss: 0.001277115
test_loss: 0.0013449511
train_loss: 0.0012470348
test_loss: 0.0014733066
train_loss: 0.0015076578
test_loss: 0.0013687022
train_loss: 0.0014540675
test_loss: 0.0013542802
train_loss: 0.0016027681
test_loss: 0.0014257866
train_loss: 0.0016801774
test_loss: 0.0016586503
train_loss: 0.001751445
test_loss: 0.0015606087
train_loss: 0.0014691923
test_loss: 0.0014452144
train_loss: 0.0015131987
test_loss: 0.0015802769
train_loss: 0.001503384
test_loss: 0.0018719181
train_loss: 0.0017264546
test_loss: 0.0013557656
train_loss: 0.0014724327
test_loss: 0.0017873181
train_loss: 0.0014489254
test_loss: 0.0013847906
train_loss: 0.0014253183
test_loss: 0.0015007646
train_loss: 0.0015602747
test_loss: 0.0014432371
train_loss: 0.0015074848
test_loss: 0.0014293577
train_loss: 0.0013720643
test_loss: 0.001488991
train_loss: 0.0015257541
test_loss: 0.0014205354
train_loss: 0.0014771908
test_loss: 0.0014787721
train_loss: 0.0013509787
test_loss: 0.001591693
train_loss: 0.0014236748
test_loss: 0.0014284026
train_loss: 0.0013872355
test_loss: 0.001455978
train_loss: 0.0014261579
test_loss: 0.0015696393
train_loss: 0.0014680445
test_loss: 0.0014678705
train_loss: 0.0014584918
test_loss: 0.0012828221
train_loss: 0.0014092923
test_loss: 0.0014622549
train_loss: 0.0016629192
test_loss: 0.002077664
train_loss: 0.0012272447
test_loss: 0.0013067513
train_loss: 0.0013241839
test_loss: 0.0013261979
train_loss: 0.0013409485
test_loss: 0.001373854
train_loss: 0.0013725562
test_loss: 0.0017130424
train_loss: 0.0012811273
test_loss: 0.0013760404
train_loss: 0.0016551536
test_loss: 0.001437969
train_loss: 0.0017232173
test_loss: 0.0015198152
train_loss: 0.0011188997
test_loss: 0.0013288583
train_loss: 0.0013750118
test_loss: 0.0012872227
train_loss: 0.0013886306
test_loss: 0.0013513728
train_loss: 0.0012216235
test_loss: 0.0020118302
train_loss: 0.0014584046
test_loss: 0.0014501421
train_loss: 0.0012802893
test_loss: 0.001490633
train_loss: 0.0012149888
test_loss: 0.0015685506
train_loss: 0.001452424
test_loss: 0.0015796556
train_loss: 0.0015049299
test_loss: 0.0015773306
train_loss: 0.0013575486
test_loss: 0.0016471026
train_loss: 0.0013966772
test_loss: 0.0013467553
train_loss: 0.0015000864
test_loss: 0.0013763062
train_loss: 0.0017367135
test_loss: 0.0016560302
train_loss: 0.0016022018
test_loss: 0.0013448964
train_loss: 0.0014164273
test_loss: 0.001395188
train_loss: 0.0017147409
test_loss: 0.0014445644
train_loss: 0.0015340913
test_loss: 0.001800333
train_loss: 0.0015356683
test_loss: 0.0017019445
train_loss: 0.0012673779
test_loss: 0.0016846311
train_loss: 0.0013083556
test_loss: 0.0014498349
train_loss: 0.0013396814
test_loss: 0.0013308355
train_loss: 0.0013817691
test_loss: 0.0014955116
train_loss: 0.0014225261
test_loss: 0.0015760276
train_loss: 0.0014239026
test_loss: 0.0015069224
train_loss: 0.0015501555
test_loss: 0.001402445
train_loss: 0.0013297653
test_loss: 0.0016203086
train_loss: 0.0014156015
test_loss: 0.0015175316
train_loss: 0.0017282407
test_loss: 0.0015751172
train_loss: 0.0013798834
test_loss: 0.0014387487
train_loss: 0.00164651
test_loss: 0.0013779294
train_loss: 0.0013040913
test_loss: 0.0015441581
train_loss: 0.001321745
test_loss: 0.0016125641
train_loss: 0.0012692787
test_loss: 0.001463056
train_loss: 0.0014123239
test_loss: 0.0013847015
train_loss: 0.0014966748
test_loss: 0.0016488127
train_loss: 0.00132473
test_loss: 0.0014840197
train_loss: 0.0014807275
test_loss: 0.0014542574
train_loss: 0.0013155788
test_loss: 0.0014679137
train_loss: 0.0015213012
test_loss: 0.0015434369
train_loss: 0.0013252127
test_loss: 0.0013311154
train_loss: 0.0016238054
test_loss: 0.0014897101
train_loss: 0.0012908031
test_loss: 0.001543065
train_loss: 0.0015264873
test_loss: 0.0013498341
train_loss: 0.001633761
test_loss: 0.0015430395
train_loss: 0.0015739414
test_loss: 0.001604052
train_loss: 0.0014889279
test_loss: 0.0015858116
train_loss: 0.0012994565
test_loss: 0.0015306241
train_loss: 0.0011770177
test_loss: 0.0015719658
train_loss: 0.0015402364
test_loss: 0.001448493
train_loss: 0.0013432668
test_loss: 0.0014188776
train_loss: 0.0014614437
test_loss: 0.001577554
train_loss: 0.0016257821
test_loss: 0.0015292971
train_loss: 0.0012441145
test_loss: 0.0014539397
train_loss: 0.0012330823
test_loss: 0.001587862
train_loss: 0.0015449944
test_loss: 0.0014051842
train_loss: 0.0016765755
test_loss: 0.0013654524
train_loss: 0.0014912055
test_loss: 0.0014825221
train_loss: 0.0015913723
test_loss: 0.0015946341
train_loss: 0.0016611174
test_loss: 0.0015201087
train_loss: 0.001596886
test_loss: 0.001477695
train_loss: 0.0014049276
test_loss: 0.0017262369
train_loss: 0.0015660375
test_loss: 0.0016791009
train_loss: 0.0015475758
test_loss: 0.0014521085
train_loss: 0.0014775747
test_loss: 0.0015231619
train_loss: 0.001531827
test_loss: 0.001578575
train_loss: 0.0013302042
test_loss: 0.0012758786
train_loss: 0.0023450942
test_loss: 0.0018895536
train_loss: 0.00165673
test_loss: 0.0016972204
train_loss: 0.0016792068
test_loss: 0.0017403943
train_loss: 0.0012754467
test_loss: 0.0017439779
train_loss: 0.0014055311
test_loss: 0.001640926
train_loss: 0.001463078
test_loss: 0.0014381867
train_loss: 0.0013781944
test_loss: 0.001575314
train_loss: 0.0015353513
test_loss: 0.0015445414
train_loss: 0.001468888
test_loss: 0.001593919
train_loss: 0.0017637839
test_loss: 0.0013798762
train_loss: 0.0014539842
test_loss: 0.0015947677
train_loss: 0.001204069
test_loss: 0.0014493972
train_loss: 0.0016517015
test_loss: 0.0015245315
train_loss: 0.0012903515
test_loss: 0.0013461056
train_loss: 0.0014456187
test_loss: 0.0014642882
train_loss: 0.0013949127
test_loss: 0.0015395893
train_loss: 0.001674594
test_loss: 0.0014605705
train_loss: 0.0015067017
test_loss: 0.0014543493
train_loss: 0.001599745
test_loss: 0.0014653178
train_loss: 0.001472912
test_loss: 0.0012640479
train_loss: 0.0014644118
test_loss: 0.001668843
train_loss: 0.001297433
test_loss: 0.0014600877
train_loss: 0.0014413868
test_loss: 0.0013990005
train_loss: 0.0013880215
test_loss: 0.0013581938
train_loss: 0.001424531
test_loss: 0.0015948932
train_loss: 0.0015312403
test_loss: 0.0013536598
train_loss: 0.0012945663
test_loss: 0.0013702991
train_loss: 0.0012877707
test_loss: 0.0016350829
train_loss: 0.001673737
test_loss: 0.0014148107
train_loss: 0.0014583343
test_loss: 0.0013328424
train_loss: 0.0015257685
test_loss: 0.0016189489
train_loss: 0.0017850285
test_loss: 0.001439888
train_loss: 0.0015748064
test_loss: 0.0014320102
train_loss: 0.0014352797
test_loss: 0.0022009197
train_loss: 0.0015730768
test_loss: 0.0018778224
train_loss: 0.001388762
test_loss: 0.0017689847
train_loss: 0.0015698159
test_loss: 0.0014469388
train_loss: 0.0018500795
test_loss: 0.0019437424
train_loss: 0.0015289283
test_loss: 0.0017706858
train_loss: 0.0014095416
test_loss: 0.0016104057
train_loss: 0.0015032617
test_loss: 0.0016745807
train_loss: 0.0013683154
test_loss: 0.0015659808
train_loss: 0.0012371747
test_loss: 0.0016886488
train_loss: 0.0010801178
test_loss: 0.0014051244
train_loss: 0.0014672494
test_loss: 0.0014341665
train_loss: 0.0014049782
test_loss: 0.0013024503
train_loss: 0.0016235135
test_loss: 0.0013583913
train_loss: 0.0014002282
test_loss: 0.0014787273
train_loss: 0.0015500518
test_loss: 0.001346413
train_loss: 0.0016560353
test_loss: 0.0018179382
train_loss: 0.0014400118
test_loss: 0.0013906902
train_loss: 0.001267285
test_loss: 0.001520405
train_loss: 0.001494358
test_loss: 0.0014571564
train_loss: 0.0016935156
test_loss: 0.0013008433
train_loss: 0.0015551371
test_loss: 0.0015430052
train_loss: 0.0012946528
test_loss: 0.001511084
train_loss: 0.001231489
test_loss: 0.0013668917
train_loss: 0.0011912171
test_loss: 0.0013586886
train_loss: 0.0013569667
test_loss: 0.0015666463
train_loss: 0.0015890767
test_loss: 0.0016141132
train_loss: 0.0015018224
test_loss: 0.0016127885
train_loss: 0.0014001866
test_loss: 0.0015247817
train_loss: 0.0013752072
test_loss: 0.0014078543
train_loss: 0.0014358342
test_loss: 0.001406772
train_loss: 0.0012610734
test_loss: 0.0016445891
train_loss: 0.0012929275
test_loss: 0.0014723045
train_loss: 0.0013340383
test_loss: 0.0013695102
train_loss: 0.0015712176
test_loss: 0.0014495171
train_loss: 0.0016710183
test_loss: 0.0014492733
train_loss: 0.0013828295
test_loss: 0.0012719245
train_loss: 0.001337366
test_loss: 0.0012841054
train_loss: 0.0013890099
test_loss: 0.0014993114
train_loss: 0.0016550196
test_loss: 0.0017657408
train_loss: 0.0016594417
test_loss: 0.0018461663
train_loss: 0.0014588227
test_loss: 0.0014902338
train_loss: 0.001328009
test_loss: 0.0013774114
train_loss: 0.001249661
test_loss: 0.0015175005
train_loss: 0.0011882805
test_loss: 0.0014186902
train_loss: 0.0011757598
test_loss: 0.0015121072
train_loss: 0.0013858499
test_loss: 0.0014676728
train_loss: 0.0012283709
test_loss: 0.0013619843
train_loss: 0.0013463935
test_loss: 0.0015701469
train_loss: 0.0013195822
test_loss: 0.0012516876
train_loss: 0.0012783476
test_loss: 0.0013913248
train_loss: 0.0013592832
test_loss: 0.0011999622
train_loss: 0.0014173773
test_loss: 0.0015812849
train_loss: 0.001510446
test_loss: 0.0015270007
train_loss: 0.0014136506
test_loss: 0.0013825179
train_loss: 0.0015101341
test_loss: 0.0013817635
train_loss: 0.0012957482
test_loss: 0.0015811903
train_loss: 0.0013152575
test_loss: 0.0014296628
train_loss: 0.001495761
test_loss: 0.0016257579
train_loss: 0.0016931781
test_loss: 0.0013319232
train_loss: 0.0015897492
test_loss: 0.0017528173
train_loss: 0.0014400952
test_loss: 0.001503598
train_loss: 0.0013032975
test_loss: 0.0014513121
train_loss: 0.0013001093
test_loss: 0.001498144
train_loss: 0.0012787429
test_loss: 0.0012396291
train_loss: 0.0013602634
test_loss: 0.0015290142
train_loss: 0.0013593336
test_loss: 0.0015086288
train_loss: 0.0012473094
test_loss: 0.001448503
train_loss: 0.0012267046
test_loss: 0.0014542518
train_loss: 0.0013612706
test_loss: 0.0014592008
train_loss: 0.0017338346
test_loss: 0.0016979277
train_loss: 0.0016106345
test_loss: 0.0015233716
train_loss: 0.0014063254
test_loss: 0.0013705391
train_loss: 0.001175853
test_loss: 0.0014215696/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

train_loss: 0.001349858
test_loss: 0.0013502991
train_loss: 0.0014397176
test_loss: 0.0017013672
train_loss: 0.0016301727
test_loss: 0.001376211
train_loss: 0.0013760102
test_loss: 0.0014470856
train_loss: 0.0016027327
test_loss: 0.0014296329
train_loss: 0.0013652998
test_loss: 0.0015763638
train_loss: 0.0015627985
test_loss: 0.001286689
train_loss: 0.0013419521
test_loss: 0.0013872469
train_loss: 0.0016420328
test_loss: 0.0015574339
train_loss: 0.0012126678
test_loss: 0.0014242113
train_loss: 0.0013711075
test_loss: 0.0015151392
train_loss: 0.0013295023
test_loss: 0.0014271217
train_loss: 0.001337578
test_loss: 0.001578896
train_loss: 0.0017197067
test_loss: 0.0017009174
train_loss: 0.0015556072
test_loss: 0.0014852275
train_loss: 0.0013148505
test_loss: 0.0016218142
train_loss: 0.0012175441
test_loss: 0.001438189
train_loss: 0.0014259994
test_loss: 0.0015390974
train_loss: 0.0018140245
test_loss: 0.0014808159
train_loss: 0.0014407875
test_loss: 0.0012667786
train_loss: 0.0012273603
test_loss: 0.0014349947
train_loss: 0.0013871237
test_loss: 0.0014822838
train_loss: 0.0011827433
test_loss: 0.0013730731
train_loss: 0.0013957564
test_loss: 0.0013619129
train_loss: 0.0012280317
test_loss: 0.0013735476
train_loss: 0.001273159
test_loss: 0.0015209181
train_loss: 0.001491286
test_loss: 0.0017209142
train_loss: 0.0014648715
test_loss: 0.0015958942
train_loss: 0.0013410053
test_loss: 0.0012974691
train_loss: 0.0013341719
test_loss: 0.0015014716
train_loss: 0.0014652575
test_loss: 0.001424128
train_loss: 0.0012585017
test_loss: 0.0018846134
train_loss: 0.001580913
test_loss: 0.0013647787
train_loss: 0.0014381025
test_loss: 0.0015004886
train_loss: 0.0013167432
test_loss: 0.0015474143
train_loss: 0.0013590797
test_loss: 0.0013454137
train_loss: 0.0014641962
test_loss: 0.0015012856
train_loss: 0.0012406509
test_loss: 0.0014278528
train_loss: 0.001400263
test_loss: 0.0015334415
train_loss: 0.001547323
test_loss: 0.0014108345
train_loss: 0.0012785761
test_loss: 0.001683291
train_loss: 0.001487056
test_loss: 0.0015031291
train_loss: 0.00146004
test_loss: 0.0013781269
train_loss: 0.0013757108
test_loss: 0.0016895435
train_loss: 0.0013156289
test_loss: 0.0015654813
train_loss: 0.0014685986
test_loss: 0.0015898355
train_loss: 0.0013529727
test_loss: 0.00141687
train_loss: 0.0012826306
test_loss: 0.0014425192
train_loss: 0.001205623
test_loss: 0.0015049645
train_loss: 0.0013970996
test_loss: 0.0014691093
train_loss: 0.0014665725
test_loss: 0.0015131495
train_loss: 0.0013352314
test_loss: 0.0013118511
train_loss: 0.0013581663
test_loss: 0.0019375617
train_loss: 0.0025674752
test_loss: 0.002571135
train_loss: 0.0024057073
test_loss: 0.002015226
train_loss: 0.0019050664
test_loss: 0.0017968379
train_loss: 0.0012661112
test_loss: 0.0015596214
train_loss: 0.0014077759
test_loss: 0.0013998222
train_loss: 0.0016229561
test_loss: 0.001493405
train_loss: 0.0014516211
test_loss: 0.0016304293
train_loss: 0.001349327
test_loss: 0.0014355896
train_loss: 0.0013206949
test_loss: 0.0013440814
train_loss: 0.0013415691
test_loss: 0.0014542189
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc02a3378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc02c7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc02536a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc032a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc0209f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc0209268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc0209158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc01806a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc01809d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc0180268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc01641e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc010dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc0121a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc00e0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc0121d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc009e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc009ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc009e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefc009ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa06f0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa070ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa06a50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa070c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa06dba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0692488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0624b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0624730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0659378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0659048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa05b0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa05c5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0591f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa0520598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa057c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa050e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fefa04b0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.04875857e-06
Iter: 2 loss: 3.35923914e-06
Iter: 3 loss: 1.95891971e-06
Iter: 4 loss: 1.6219991e-06
Iter: 5 loss: 2.03794548e-06
Iter: 6 loss: 1.44654177e-06
Iter: 7 loss: 1.28987017e-06
Iter: 8 loss: 3.00532565e-06
Iter: 9 loss: 1.28655586e-06
Iter: 10 loss: 1.19389711e-06
Iter: 11 loss: 1.14961711e-06
Iter: 12 loss: 1.10464157e-06
Iter: 13 loss: 1.04875971e-06
Iter: 14 loss: 1.37738857e-06
Iter: 15 loss: 1.04147784e-06
Iter: 16 loss: 1.00935381e-06
Iter: 17 loss: 1.18029175e-06
Iter: 18 loss: 1.00451666e-06
Iter: 19 loss: 9.85606903e-07
Iter: 20 loss: 1.24373355e-06
Iter: 21 loss: 9.85559723e-07
Iter: 22 loss: 9.75490138e-07
Iter: 23 loss: 9.67629376e-07
Iter: 24 loss: 9.64467176e-07
Iter: 25 loss: 9.56505801e-07
Iter: 26 loss: 9.55882911e-07
Iter: 27 loss: 9.49360413e-07
Iter: 28 loss: 9.40651319e-07
Iter: 29 loss: 9.40162295e-07
Iter: 30 loss: 9.29916951e-07
Iter: 31 loss: 9.28597672e-07
Iter: 32 loss: 9.21252592e-07
Iter: 33 loss: 9.13004e-07
Iter: 34 loss: 9.97450684e-07
Iter: 35 loss: 9.12760697e-07
Iter: 36 loss: 9.07943e-07
Iter: 37 loss: 9.07786159e-07
Iter: 38 loss: 9.04421768e-07
Iter: 39 loss: 8.99043243e-07
Iter: 40 loss: 8.98984581e-07
Iter: 41 loss: 8.9368433e-07
Iter: 42 loss: 8.9367694e-07
Iter: 43 loss: 8.90606884e-07
Iter: 44 loss: 8.85479324e-07
Iter: 45 loss: 8.85435838e-07
Iter: 46 loss: 8.79395429e-07
Iter: 47 loss: 8.90848071e-07
Iter: 48 loss: 8.76841113e-07
Iter: 49 loss: 8.70562076e-07
Iter: 50 loss: 9.04063086e-07
Iter: 51 loss: 8.6962558e-07
Iter: 52 loss: 8.63945559e-07
Iter: 53 loss: 8.97529503e-07
Iter: 54 loss: 8.63249568e-07
Iter: 55 loss: 8.58875751e-07
Iter: 56 loss: 8.61548756e-07
Iter: 57 loss: 8.56035172e-07
Iter: 58 loss: 8.51999175e-07
Iter: 59 loss: 8.89348826e-07
Iter: 60 loss: 8.51802724e-07
Iter: 61 loss: 8.47568117e-07
Iter: 62 loss: 8.44542967e-07
Iter: 63 loss: 8.43037469e-07
Iter: 64 loss: 8.38947585e-07
Iter: 65 loss: 8.46374576e-07
Iter: 66 loss: 8.37175492e-07
Iter: 67 loss: 8.33233798e-07
Iter: 68 loss: 8.67888446e-07
Iter: 69 loss: 8.330436e-07
Iter: 70 loss: 8.29434498e-07
Iter: 71 loss: 8.60044793e-07
Iter: 72 loss: 8.29248734e-07
Iter: 73 loss: 8.28264092e-07
Iter: 74 loss: 8.29223268e-07
Iter: 75 loss: 8.27756082e-07
Iter: 76 loss: 8.26029577e-07
Iter: 77 loss: 8.25465293e-07
Iter: 78 loss: 8.24485937e-07
Iter: 79 loss: 8.2256463e-07
Iter: 80 loss: 8.24997414e-07
Iter: 81 loss: 8.21604374e-07
Iter: 82 loss: 8.19639467e-07
Iter: 83 loss: 8.22855384e-07
Iter: 84 loss: 8.18762032e-07
Iter: 85 loss: 8.16963166e-07
Iter: 86 loss: 8.40049779e-07
Iter: 87 loss: 8.16949182e-07
Iter: 88 loss: 8.15512067e-07
Iter: 89 loss: 8.17044679e-07
Iter: 90 loss: 8.14729447e-07
Iter: 91 loss: 8.12840483e-07
Iter: 92 loss: 8.13305e-07
Iter: 93 loss: 8.11499376e-07
Iter: 94 loss: 8.09694143e-07
Iter: 95 loss: 8.09685275e-07
Iter: 96 loss: 8.08574896e-07
Iter: 97 loss: 8.06167805e-07
Iter: 98 loss: 8.45636464e-07
Iter: 99 loss: 8.06102889e-07
Iter: 100 loss: 8.03696707e-07
Iter: 101 loss: 8.16736247e-07
Iter: 102 loss: 8.03332739e-07
Iter: 103 loss: 8.02628335e-07
Iter: 104 loss: 8.02247428e-07
Iter: 105 loss: 8.01368628e-07
Iter: 106 loss: 7.99191184e-07
Iter: 107 loss: 8.2202871e-07
Iter: 108 loss: 7.98944711e-07
Iter: 109 loss: 7.97269195e-07
Iter: 110 loss: 7.97217581e-07
Iter: 111 loss: 7.95919732e-07
Iter: 112 loss: 7.9417714e-07
Iter: 113 loss: 7.94086475e-07
Iter: 114 loss: 7.92336e-07
Iter: 115 loss: 7.9457925e-07
Iter: 116 loss: 7.91422281e-07
Iter: 117 loss: 7.89695036e-07
Iter: 118 loss: 8.03347575e-07
Iter: 119 loss: 7.89621708e-07
Iter: 120 loss: 7.88332841e-07
Iter: 121 loss: 7.96343045e-07
Iter: 122 loss: 7.88200566e-07
Iter: 123 loss: 7.87129181e-07
Iter: 124 loss: 7.88652414e-07
Iter: 125 loss: 7.86576209e-07
Iter: 126 loss: 7.85798306e-07
Iter: 127 loss: 7.92577225e-07
Iter: 128 loss: 7.85779434e-07
Iter: 129 loss: 7.8508134e-07
Iter: 130 loss: 7.85811835e-07
Iter: 131 loss: 7.8468554e-07
Iter: 132 loss: 7.83881717e-07
Iter: 133 loss: 7.83471705e-07
Iter: 134 loss: 7.83146902e-07
Iter: 135 loss: 7.8234109e-07
Iter: 136 loss: 7.90280865e-07
Iter: 137 loss: 7.82308121e-07
Iter: 138 loss: 7.81376457e-07
Iter: 139 loss: 7.84147232e-07
Iter: 140 loss: 7.81120093e-07
Iter: 141 loss: 7.8051761e-07
Iter: 142 loss: 7.80506184e-07
Iter: 143 loss: 7.80022106e-07
Iter: 144 loss: 7.79145296e-07
Iter: 145 loss: 7.85294731e-07
Iter: 146 loss: 7.79065772e-07
Iter: 147 loss: 7.7841969e-07
Iter: 148 loss: 7.77153218e-07
Iter: 149 loss: 8.01214924e-07
Iter: 150 loss: 7.77170385e-07
Iter: 151 loss: 7.75995204e-07
Iter: 152 loss: 7.79756078e-07
Iter: 153 loss: 7.75713147e-07
Iter: 154 loss: 7.74545128e-07
Iter: 155 loss: 7.86041255e-07
Iter: 156 loss: 7.74477257e-07
Iter: 157 loss: 7.7377581e-07
Iter: 158 loss: 7.77607738e-07
Iter: 159 loss: 7.73663714e-07
Iter: 160 loss: 7.73051738e-07
Iter: 161 loss: 7.72787075e-07
Iter: 162 loss: 7.7244772e-07
Iter: 163 loss: 7.71564032e-07
Iter: 164 loss: 7.80530456e-07
Iter: 165 loss: 7.71513612e-07
Iter: 166 loss: 7.70962e-07
Iter: 167 loss: 7.7074219e-07
Iter: 168 loss: 7.70437282e-07
Iter: 169 loss: 7.69603616e-07
Iter: 170 loss: 7.69548933e-07
Iter: 171 loss: 7.68871132e-07
Iter: 172 loss: 7.69190649e-07
Iter: 173 loss: 7.68465725e-07
Iter: 174 loss: 7.68157861e-07
Iter: 175 loss: 7.67428958e-07
Iter: 176 loss: 7.74616524e-07
Iter: 177 loss: 7.67334313e-07
Iter: 178 loss: 7.66972448e-07
Iter: 179 loss: 7.66895596e-07
Iter: 180 loss: 7.6657625e-07
Iter: 181 loss: 7.6626111e-07
Iter: 182 loss: 7.66215692e-07
Iter: 183 loss: 7.65742129e-07
Iter: 184 loss: 7.65044e-07
Iter: 185 loss: 7.65033747e-07
Iter: 186 loss: 7.64454285e-07
Iter: 187 loss: 7.6444303e-07
Iter: 188 loss: 7.63961737e-07
Iter: 189 loss: 7.6530938e-07
Iter: 190 loss: 7.63772391e-07
Iter: 191 loss: 7.63181106e-07
Iter: 192 loss: 7.63529442e-07
Iter: 193 loss: 7.62817308e-07
Iter: 194 loss: 7.62233356e-07
Iter: 195 loss: 7.65914876e-07
Iter: 196 loss: 7.62178388e-07
Iter: 197 loss: 7.61610863e-07
Iter: 198 loss: 7.62717605e-07
Iter: 199 loss: 7.6138457e-07
Iter: 200 loss: 7.60876446e-07
Iter: 201 loss: 7.60476269e-07
Iter: 202 loss: 7.60332284e-07
Iter: 203 loss: 7.59789373e-07
Iter: 204 loss: 7.68016946e-07
Iter: 205 loss: 7.5977897e-07
Iter: 206 loss: 7.59211844e-07
Iter: 207 loss: 7.61969034e-07
Iter: 208 loss: 7.59149316e-07
Iter: 209 loss: 7.58829856e-07
Iter: 210 loss: 7.58404e-07
Iter: 211 loss: 7.58388069e-07
Iter: 212 loss: 7.57856242e-07
Iter: 213 loss: 7.63792627e-07
Iter: 214 loss: 7.57802468e-07
Iter: 215 loss: 7.57416728e-07
Iter: 216 loss: 7.56458405e-07
Iter: 217 loss: 7.69220605e-07
Iter: 218 loss: 7.56420263e-07
Iter: 219 loss: 7.55489168e-07
Iter: 220 loss: 7.57733062e-07
Iter: 221 loss: 7.55158283e-07
Iter: 222 loss: 7.54232872e-07
Iter: 223 loss: 7.6465858e-07
Iter: 224 loss: 7.54237647e-07
Iter: 225 loss: 7.53668303e-07
Iter: 226 loss: 7.5857389e-07
Iter: 227 loss: 7.53590825e-07
Iter: 228 loss: 7.53299048e-07
Iter: 229 loss: 7.53121071e-07
Iter: 230 loss: 7.52995163e-07
Iter: 231 loss: 7.52479423e-07
Iter: 232 loss: 7.55626616e-07
Iter: 233 loss: 7.52420476e-07
Iter: 234 loss: 7.51973857e-07
Iter: 235 loss: 7.52918254e-07
Iter: 236 loss: 7.51830498e-07
Iter: 237 loss: 7.51435493e-07
Iter: 238 loss: 7.50784409e-07
Iter: 239 loss: 7.50788445e-07
Iter: 240 loss: 7.51077209e-07
Iter: 241 loss: 7.50502295e-07
Iter: 242 loss: 7.50257641e-07
Iter: 243 loss: 7.49726382e-07
Iter: 244 loss: 7.59023123e-07
Iter: 245 loss: 7.49705521e-07
Iter: 246 loss: 7.49243782e-07
Iter: 247 loss: 7.50997856e-07
Iter: 248 loss: 7.49146807e-07
Iter: 249 loss: 7.48607874e-07
Iter: 250 loss: 7.51245352e-07
Iter: 251 loss: 7.48542448e-07
Iter: 252 loss: 7.48165519e-07
Iter: 253 loss: 7.47138472e-07
Iter: 254 loss: 7.5516607e-07
Iter: 255 loss: 7.46951e-07
Iter: 256 loss: 7.46225169e-07
Iter: 257 loss: 7.55736664e-07
Iter: 258 loss: 7.46235401e-07
Iter: 259 loss: 7.45658838e-07
Iter: 260 loss: 7.47912736e-07
Iter: 261 loss: 7.45540433e-07
Iter: 262 loss: 7.45005082e-07
Iter: 263 loss: 7.50952665e-07
Iter: 264 loss: 7.45025318e-07
Iter: 265 loss: 7.44743943e-07
Iter: 266 loss: 7.44693807e-07
Iter: 267 loss: 7.44528336e-07
Iter: 268 loss: 7.44051704e-07
Iter: 269 loss: 7.45507e-07
Iter: 270 loss: 7.4392517e-07
Iter: 271 loss: 7.43521127e-07
Iter: 272 loss: 7.43664032e-07
Iter: 273 loss: 7.43227133e-07
Iter: 274 loss: 7.42699e-07
Iter: 275 loss: 7.43363e-07
Iter: 276 loss: 7.42447185e-07
Iter: 277 loss: 7.41897395e-07
Iter: 278 loss: 7.42735e-07
Iter: 279 loss: 7.41642964e-07
Iter: 280 loss: 7.41386202e-07
Iter: 281 loss: 7.41226359e-07
Iter: 282 loss: 7.41000179e-07
Iter: 283 loss: 7.40445842e-07
Iter: 284 loss: 7.45740181e-07
Iter: 285 loss: 7.40367796e-07
Iter: 286 loss: 7.39857683e-07
Iter: 287 loss: 7.40388771e-07
Iter: 288 loss: 7.39588927e-07
Iter: 289 loss: 7.39226493e-07
Iter: 290 loss: 7.39213078e-07
Iter: 291 loss: 7.38825634e-07
Iter: 292 loss: 7.38805625e-07
Iter: 293 loss: 7.38522e-07
Iter: 294 loss: 7.38154e-07
Iter: 295 loss: 7.37377263e-07
Iter: 296 loss: 7.48935065e-07
Iter: 297 loss: 7.37338326e-07
Iter: 298 loss: 7.36999368e-07
Iter: 299 loss: 7.3687653e-07
Iter: 300 loss: 7.36470952e-07
Iter: 301 loss: 7.38167159e-07
Iter: 302 loss: 7.36381367e-07
Iter: 303 loss: 7.36033144e-07
Iter: 304 loss: 7.35681169e-07
Iter: 305 loss: 7.35603408e-07
Iter: 306 loss: 7.35323681e-07
Iter: 307 loss: 7.35290314e-07
Iter: 308 loss: 7.35082722e-07
Iter: 309 loss: 7.34510536e-07
Iter: 310 loss: 7.37137839e-07
Iter: 311 loss: 7.34313915e-07
Iter: 312 loss: 7.33648506e-07
Iter: 313 loss: 7.36751872e-07
Iter: 314 loss: 7.33525667e-07
Iter: 315 loss: 7.3352345e-07
Iter: 316 loss: 7.332373e-07
Iter: 317 loss: 7.32991339e-07
Iter: 318 loss: 7.32662215e-07
Iter: 319 loss: 7.32695298e-07
Iter: 320 loss: 7.32262151e-07
Iter: 321 loss: 7.31857199e-07
Iter: 322 loss: 7.31757893e-07
Iter: 323 loss: 7.3113722e-07
Iter: 324 loss: 7.37013806e-07
Iter: 325 loss: 7.31085834e-07
Iter: 326 loss: 7.30792635e-07
Iter: 327 loss: 7.30782631e-07
Iter: 328 loss: 7.3055e-07
Iter: 329 loss: 7.2998148e-07
Iter: 330 loss: 7.3858206e-07
Iter: 331 loss: 7.29983697e-07
Iter: 332 loss: 7.29557712e-07
Iter: 333 loss: 7.3056151e-07
Iter: 334 loss: 7.29383657e-07
Iter: 335 loss: 7.28935106e-07
Iter: 336 loss: 7.32551939e-07
Iter: 337 loss: 7.28909072e-07
Iter: 338 loss: 7.28535724e-07
Iter: 339 loss: 7.30889496e-07
Iter: 340 loss: 7.28465523e-07
Iter: 341 loss: 7.28235875e-07
Iter: 342 loss: 7.28278678e-07
Iter: 343 loss: 7.28056364e-07
Iter: 344 loss: 7.27742872e-07
Iter: 345 loss: 7.30682359e-07
Iter: 346 loss: 7.27755435e-07
Iter: 347 loss: 7.27542329e-07
Iter: 348 loss: 7.27187626e-07
Iter: 349 loss: 7.27194731e-07
Iter: 350 loss: 7.26759538e-07
Iter: 351 loss: 7.26932512e-07
Iter: 352 loss: 7.26415578e-07
Iter: 353 loss: 7.2628211e-07
Iter: 354 loss: 7.26194969e-07
Iter: 355 loss: 7.25929794e-07
Iter: 356 loss: 7.25859707e-07
Iter: 357 loss: 7.25682412e-07
Iter: 358 loss: 7.25356927e-07
Iter: 359 loss: 7.24887968e-07
Iter: 360 loss: 7.24874667e-07
Iter: 361 loss: 7.24291624e-07
Iter: 362 loss: 7.24959307e-07
Iter: 363 loss: 7.23988e-07
Iter: 364 loss: 7.23575852e-07
Iter: 365 loss: 7.2356795e-07
Iter: 366 loss: 7.2331062e-07
Iter: 367 loss: 7.25400923e-07
Iter: 368 loss: 7.23308233e-07
Iter: 369 loss: 7.23050618e-07
Iter: 370 loss: 7.23063408e-07
Iter: 371 loss: 7.22827224e-07
Iter: 372 loss: 7.22563868e-07
Iter: 373 loss: 7.22692562e-07
Iter: 374 loss: 7.22373841e-07
Iter: 375 loss: 7.22064897e-07
Iter: 376 loss: 7.23314542e-07
Iter: 377 loss: 7.22011123e-07
Iter: 378 loss: 7.2173458e-07
Iter: 379 loss: 7.25703501e-07
Iter: 380 loss: 7.21755555e-07
Iter: 381 loss: 7.21614811e-07
Iter: 382 loss: 7.2137243e-07
Iter: 383 loss: 7.21389881e-07
Iter: 384 loss: 7.21095375e-07
Iter: 385 loss: 7.24340794e-07
Iter: 386 loss: 7.21070535e-07
Iter: 387 loss: 7.20929961e-07
Iter: 388 loss: 7.20633e-07
Iter: 389 loss: 7.20633921e-07
Iter: 390 loss: 7.20451169e-07
Iter: 391 loss: 7.20403705e-07
Iter: 392 loss: 7.20162518e-07
Iter: 393 loss: 7.1969157e-07
Iter: 394 loss: 7.29441126e-07
Iter: 395 loss: 7.19677701e-07
Iter: 396 loss: 7.19365573e-07
Iter: 397 loss: 7.18971421e-07
Iter: 398 loss: 7.18918727e-07
Iter: 399 loss: 7.1864639e-07
Iter: 400 loss: 7.18617514e-07
Iter: 401 loss: 7.18334377e-07
Iter: 402 loss: 7.186311e-07
Iter: 403 loss: 7.18209662e-07
Iter: 404 loss: 7.17921807e-07
Iter: 405 loss: 7.1812633e-07
Iter: 406 loss: 7.17709099e-07
Iter: 407 loss: 7.17431135e-07
Iter: 408 loss: 7.18866431e-07
Iter: 409 loss: 7.17408454e-07
Iter: 410 loss: 7.17232183e-07
Iter: 411 loss: 7.18865408e-07
Iter: 412 loss: 7.1720126e-07
Iter: 413 loss: 7.17002308e-07
Iter: 414 loss: 7.16871796e-07
Iter: 415 loss: 7.16820352e-07
Iter: 416 loss: 7.16572288e-07
Iter: 417 loss: 7.18636727e-07
Iter: 418 loss: 7.16539603e-07
Iter: 419 loss: 7.16286593e-07
Iter: 420 loss: 7.1632428e-07
Iter: 421 loss: 7.16098555e-07
Iter: 422 loss: 7.15822921e-07
Iter: 423 loss: 7.16404543e-07
Iter: 424 loss: 7.15739532e-07
Iter: 425 loss: 7.15541717e-07
Iter: 426 loss: 7.15523811e-07
Iter: 427 loss: 7.15417059e-07
Iter: 428 loss: 7.15095666e-07
Iter: 429 loss: 7.16541535e-07
Iter: 430 loss: 7.14981638e-07
Iter: 431 loss: 7.14617158e-07
Iter: 432 loss: 7.16035061e-07
Iter: 433 loss: 7.14524504e-07
Iter: 434 loss: 7.14167e-07
Iter: 435 loss: 7.1554075e-07
Iter: 436 loss: 7.14079306e-07
Iter: 437 loss: 7.13869724e-07
Iter: 438 loss: 7.13854149e-07
Iter: 439 loss: 7.1374086e-07
Iter: 440 loss: 7.13450731e-07
Iter: 441 loss: 7.17677153e-07
Iter: 442 loss: 7.13431803e-07
Iter: 443 loss: 7.13118254e-07
Iter: 444 loss: 7.14439466e-07
Iter: 445 loss: 7.13038389e-07
Iter: 446 loss: 7.12763324e-07
Iter: 447 loss: 7.15649435e-07
Iter: 448 loss: 7.12733765e-07
Iter: 449 loss: 7.12493261e-07
Iter: 450 loss: 7.1220262e-07
Iter: 451 loss: 7.12180281e-07
Iter: 452 loss: 7.1189163e-07
Iter: 453 loss: 7.11894472e-07
Iter: 454 loss: 7.11681537e-07
Iter: 455 loss: 7.11415112e-07
Iter: 456 loss: 7.11387315e-07
Iter: 457 loss: 7.11136863e-07
Iter: 458 loss: 7.11136295e-07
Iter: 459 loss: 7.10903237e-07
Iter: 460 loss: 7.11676194e-07
Iter: 461 loss: 7.10834456e-07
Iter: 462 loss: 7.10710196e-07
Iter: 463 loss: 7.10405e-07
Iter: 464 loss: 7.14151156e-07
Iter: 465 loss: 7.10428e-07
Iter: 466 loss: 7.10158247e-07
Iter: 467 loss: 7.12614792e-07
Iter: 468 loss: 7.10166546e-07
Iter: 469 loss: 7.0996839e-07
Iter: 470 loss: 7.10171207e-07
Iter: 471 loss: 7.09865049e-07
Iter: 472 loss: 7.09661776e-07
Iter: 473 loss: 7.12003157e-07
Iter: 474 loss: 7.09649726e-07
Iter: 475 loss: 7.09513529e-07
Iter: 476 loss: 7.09276151e-07
Iter: 477 loss: 7.11630378e-07
Iter: 478 loss: 7.09208621e-07
Iter: 479 loss: 7.08994e-07
Iter: 480 loss: 7.09017968e-07
Iter: 481 loss: 7.08733239e-07
Iter: 482 loss: 7.08868242e-07
Iter: 483 loss: 7.08614209e-07
Iter: 484 loss: 7.08387915e-07
Iter: 485 loss: 7.0840133e-07
Iter: 486 loss: 7.0820812e-07
Iter: 487 loss: 7.07883487e-07
Iter: 488 loss: 7.11583482e-07
Iter: 489 loss: 7.07876609e-07
Iter: 490 loss: 7.07699428e-07
Iter: 491 loss: 7.07503432e-07
Iter: 492 loss: 7.07461368e-07
Iter: 493 loss: 7.0724451e-07
Iter: 494 loss: 7.07219669e-07
Iter: 495 loss: 7.07048571e-07
Iter: 496 loss: 7.06764183e-07
Iter: 497 loss: 7.13302597e-07
Iter: 498 loss: 7.06751223e-07
Iter: 499 loss: 7.06491221e-07
Iter: 500 loss: 7.07042602e-07
Iter: 501 loss: 7.06424e-07
Iter: 502 loss: 7.06156584e-07
Iter: 503 loss: 7.07114111e-07
Iter: 504 loss: 7.06094681e-07
Iter: 505 loss: 7.05885e-07
Iter: 506 loss: 7.08496259e-07
Iter: 507 loss: 7.05878051e-07
Iter: 508 loss: 7.05699563e-07
Iter: 509 loss: 7.0583269e-07
Iter: 510 loss: 7.05585421e-07
Iter: 511 loss: 7.05384139e-07
Iter: 512 loss: 7.05272896e-07
Iter: 513 loss: 7.05134255e-07
Iter: 514 loss: 7.04941556e-07
Iter: 515 loss: 7.05717923e-07
Iter: 516 loss: 7.04854699e-07
Iter: 517 loss: 7.04600836e-07
Iter: 518 loss: 7.07580284e-07
Iter: 519 loss: 7.04592253e-07
Iter: 520 loss: 7.04453441e-07
Iter: 521 loss: 7.0426438e-07
Iter: 522 loss: 7.0424835e-07
Iter: 523 loss: 7.0408322e-07
Iter: 524 loss: 7.04067e-07
Iter: 525 loss: 7.03962769e-07
Iter: 526 loss: 7.04109254e-07
Iter: 527 loss: 7.03887281e-07
Iter: 528 loss: 7.03734145e-07
Iter: 529 loss: 7.04902618e-07
Iter: 530 loss: 7.03709361e-07
Iter: 531 loss: 7.03594424e-07
Iter: 532 loss: 7.03317539e-07
Iter: 533 loss: 7.05905393e-07
Iter: 534 loss: 7.03299861e-07
Iter: 535 loss: 7.02973637e-07
Iter: 536 loss: 7.03639444e-07
Iter: 537 loss: 7.02822092e-07
Iter: 538 loss: 7.02529405e-07
Iter: 539 loss: 7.03216585e-07
Iter: 540 loss: 7.02406055e-07
Iter: 541 loss: 7.02152249e-07
Iter: 542 loss: 7.02124794e-07
Iter: 543 loss: 7.01995759e-07
Iter: 544 loss: 7.01954036e-07
Iter: 545 loss: 7.01853708e-07
Iter: 546 loss: 7.0161434e-07
Iter: 547 loss: 7.01632871e-07
Iter: 548 loss: 7.01454496e-07
Iter: 549 loss: 7.01245483e-07
Iter: 550 loss: 7.01260944e-07
Iter: 551 loss: 7.01086947e-07
Iter: 552 loss: 7.01321426e-07
Iter: 553 loss: 7.00973146e-07
Iter: 554 loss: 7.00815e-07
Iter: 555 loss: 7.00873443e-07
Iter: 556 loss: 7.00714963e-07
Iter: 557 loss: 7.0053153e-07
Iter: 558 loss: 7.00536361e-07
Iter: 559 loss: 7.00464284e-07
Iter: 560 loss: 7.00514249e-07
Iter: 561 loss: 7.00386067e-07
Iter: 562 loss: 7.00193937e-07
Iter: 563 loss: 7.00163184e-07
Iter: 564 loss: 7.00050236e-07
Iter: 565 loss: 6.99917223e-07
Iter: 566 loss: 6.997426e-07
Iter: 567 loss: 6.99710142e-07
Iter: 568 loss: 6.99479756e-07
Iter: 569 loss: 7.00765554e-07
Iter: 570 loss: 6.99438203e-07
Iter: 571 loss: 6.99341115e-07
Iter: 572 loss: 6.9932139e-07
Iter: 573 loss: 6.99216343e-07
Iter: 574 loss: 6.99152451e-07
Iter: 575 loss: 6.99146312e-07
Iter: 576 loss: 6.98976123e-07
Iter: 577 loss: 6.99126872e-07
Iter: 578 loss: 6.98907684e-07
Iter: 579 loss: 6.98721749e-07
Iter: 580 loss: 6.98569693e-07
Iter: 581 loss: 6.98502163e-07
Iter: 582 loss: 6.98258305e-07
Iter: 583 loss: 6.98275812e-07
Iter: 584 loss: 6.98114491e-07
Iter: 585 loss: 6.97975338e-07
Iter: 586 loss: 6.97877681e-07
Iter: 587 loss: 6.97646328e-07
Iter: 588 loss: 6.98815825e-07
Iter: 589 loss: 6.97654286e-07
Iter: 590 loss: 6.97407131e-07
Iter: 591 loss: 6.9844657e-07
Iter: 592 loss: 6.97331643e-07
Iter: 593 loss: 6.97165603e-07
Iter: 594 loss: 6.98350163e-07
Iter: 595 loss: 6.97171e-07
Iter: 596 loss: 6.97054134e-07
Iter: 597 loss: 6.96919358e-07
Iter: 598 loss: 6.96887582e-07
Iter: 599 loss: 6.96713926e-07
Iter: 600 loss: 6.96631218e-07
Iter: 601 loss: 6.96535722e-07
Iter: 602 loss: 6.9633353e-07
Iter: 603 loss: 6.98132055e-07
Iter: 604 loss: 6.96319091e-07
Iter: 605 loss: 6.96147595e-07
Iter: 606 loss: 6.97911901e-07
Iter: 607 loss: 6.96142251e-07
Iter: 608 loss: 6.96026575e-07
Iter: 609 loss: 6.95877134e-07
Iter: 610 loss: 6.95836e-07
Iter: 611 loss: 6.95651579e-07
Iter: 612 loss: 6.95972744e-07
Iter: 613 loss: 6.95504411e-07
Iter: 614 loss: 6.95330357e-07
Iter: 615 loss: 6.95322456e-07
Iter: 616 loss: 6.95197969e-07
Iter: 617 loss: 6.95541644e-07
Iter: 618 loss: 6.95155677e-07
Iter: 619 loss: 6.94970595e-07
Iter: 620 loss: 6.94792845e-07
Iter: 621 loss: 6.94763571e-07
Iter: 622 loss: 6.94736684e-07
Iter: 623 loss: 6.94632945e-07
Iter: 624 loss: 6.94554501e-07
Iter: 625 loss: 6.94528694e-07
Iter: 626 loss: 6.94507548e-07
Iter: 627 loss: 6.94338496e-07
Iter: 628 loss: 6.94447408e-07
Iter: 629 loss: 6.94240612e-07
Iter: 630 loss: 6.9411027e-07
Iter: 631 loss: 6.9397106e-07
Iter: 632 loss: 6.93958668e-07
Iter: 633 loss: 6.93667289e-07
Iter: 634 loss: 6.94169671e-07
Iter: 635 loss: 6.93616698e-07
Iter: 636 loss: 6.93486527e-07
Iter: 637 loss: 6.93481866e-07
Iter: 638 loss: 6.93363e-07
Iter: 639 loss: 6.93172808e-07
Iter: 640 loss: 6.93163088e-07
Iter: 641 loss: 6.93010918e-07
Iter: 642 loss: 6.93240736e-07
Iter: 643 loss: 6.92918718e-07
Iter: 644 loss: 6.92698563e-07
Iter: 645 loss: 6.93244033e-07
Iter: 646 loss: 6.92639787e-07
Iter: 647 loss: 6.92423498e-07
Iter: 648 loss: 6.94256642e-07
Iter: 649 loss: 6.92435265e-07
Iter: 650 loss: 6.92266553e-07
Iter: 651 loss: 6.92521155e-07
Iter: 652 loss: 6.92179128e-07
Iter: 653 loss: 6.9203594e-07
Iter: 654 loss: 6.92534e-07
Iter: 655 loss: 6.92016101e-07
Iter: 656 loss: 6.91853074e-07
Iter: 657 loss: 6.92163042e-07
Iter: 658 loss: 6.91771845e-07
Iter: 659 loss: 6.91593584e-07
Iter: 660 loss: 6.91998252e-07
Iter: 661 loss: 6.91542652e-07
Iter: 662 loss: 6.91353e-07
Iter: 663 loss: 6.91266678e-07
Iter: 664 loss: 6.91149921e-07
Iter: 665 loss: 6.90930733e-07
Iter: 666 loss: 6.90658112e-07
Iter: 667 loss: 6.90613547e-07
Iter: 668 loss: 6.90371849e-07
Iter: 669 loss: 6.90374463e-07
Iter: 670 loss: 6.90167553e-07
Iter: 671 loss: 6.91436526e-07
Iter: 672 loss: 6.90171817e-07
Iter: 673 loss: 6.90024251e-07
Iter: 674 loss: 6.89793637e-07
Iter: 675 loss: 6.95223719e-07
Iter: 676 loss: 6.89794e-07
Iter: 677 loss: 6.89549495e-07
Iter: 678 loss: 6.90680054e-07
Iter: 679 loss: 6.89519084e-07
Iter: 680 loss: 6.89342414e-07
Iter: 681 loss: 6.89340482e-07
Iter: 682 loss: 6.89221679e-07
Iter: 683 loss: 6.8952761e-07
Iter: 684 loss: 6.89206672e-07
Iter: 685 loss: 6.89097192e-07
Iter: 686 loss: 6.8905922e-07
Iter: 687 loss: 6.89017952e-07
Iter: 688 loss: 6.88826049e-07
Iter: 689 loss: 6.90674938e-07
Iter: 690 loss: 6.8880496e-07
Iter: 691 loss: 6.88683826e-07
Iter: 692 loss: 6.88804505e-07
Iter: 693 loss: 6.8861317e-07
Iter: 694 loss: 6.88448949e-07
Iter: 695 loss: 6.88652221e-07
Iter: 696 loss: 6.88367379e-07
Iter: 697 loss: 6.88206569e-07
Iter: 698 loss: 6.87969305e-07
Iter: 699 loss: 6.87933891e-07
Iter: 700 loss: 6.87737383e-07
Iter: 701 loss: 6.8870429e-07
Iter: 702 loss: 6.87665761e-07
Iter: 703 loss: 6.87481872e-07
Iter: 704 loss: 6.87496936e-07
Iter: 705 loss: 6.87319186e-07
Iter: 706 loss: 6.87172701e-07
Iter: 707 loss: 6.87145871e-07
Iter: 708 loss: 6.86909061e-07
Iter: 709 loss: 6.86758654e-07
Iter: 710 loss: 6.86713236e-07
Iter: 711 loss: 6.86490864e-07
Iter: 712 loss: 6.8650985e-07
Iter: 713 loss: 6.86291742e-07
Iter: 714 loss: 6.86965109e-07
Iter: 715 loss: 6.86221142e-07
Iter: 716 loss: 6.86061412e-07
Iter: 717 loss: 6.86301746e-07
Iter: 718 loss: 6.85942496e-07
Iter: 719 loss: 6.8584211e-07
Iter: 720 loss: 6.85830344e-07
Iter: 721 loss: 6.8572956e-07
Iter: 722 loss: 6.85665611e-07
Iter: 723 loss: 6.85623e-07
Iter: 724 loss: 6.85449777e-07
Iter: 725 loss: 6.85870191e-07
Iter: 726 loss: 6.85376449e-07
Iter: 727 loss: 6.85221039e-07
Iter: 728 loss: 6.85310511e-07
Iter: 729 loss: 6.85099621e-07
Iter: 730 loss: 6.84941483e-07
Iter: 731 loss: 6.84907377e-07
Iter: 732 loss: 6.84807e-07
Iter: 733 loss: 6.84625661e-07
Iter: 734 loss: 6.87401496e-07
Iter: 735 loss: 6.84627537e-07
Iter: 736 loss: 6.84461156e-07
Iter: 737 loss: 6.84939721e-07
Iter: 738 loss: 6.84391864e-07
Iter: 739 loss: 6.84242877e-07
Iter: 740 loss: 6.84014708e-07
Iter: 741 loss: 6.84024712e-07
Iter: 742 loss: 6.83763687e-07
Iter: 743 loss: 6.84521808e-07
Iter: 744 loss: 6.83680241e-07
Iter: 745 loss: 6.83448718e-07
Iter: 746 loss: 6.83445307e-07
Iter: 747 loss: 6.83225949e-07
Iter: 748 loss: 6.83240842e-07
Iter: 749 loss: 6.83080259e-07
Iter: 750 loss: 6.82888697e-07
Iter: 751 loss: 6.84679662e-07
Iter: 752 loss: 6.82881648e-07
Iter: 753 loss: 6.82691962e-07
Iter: 754 loss: 6.83054054e-07
Iter: 755 loss: 6.82616474e-07
Iter: 756 loss: 6.82478685e-07
Iter: 757 loss: 6.82752614e-07
Iter: 758 loss: 6.82397342e-07
Iter: 759 loss: 6.8224216e-07
Iter: 760 loss: 6.82404448e-07
Iter: 761 loss: 6.82146606e-07
Iter: 762 loss: 6.81946858e-07
Iter: 763 loss: 6.8187984e-07
Iter: 764 loss: 6.81792415e-07
Iter: 765 loss: 6.81557822e-07
Iter: 766 loss: 6.82619486e-07
Iter: 767 loss: 6.81495521e-07
Iter: 768 loss: 6.81347274e-07
Iter: 769 loss: 6.81357506e-07
Iter: 770 loss: 6.81198856e-07
Iter: 771 loss: 6.80928451e-07
Iter: 772 loss: 6.80940957e-07
Iter: 773 loss: 6.80756273e-07
Iter: 774 loss: 6.81100175e-07
Iter: 775 loss: 6.80705512e-07
Iter: 776 loss: 6.804799e-07
Iter: 777 loss: 6.82002337e-07
Iter: 778 loss: 6.80461937e-07
Iter: 779 loss: 6.80328412e-07
Iter: 780 loss: 6.82010921e-07
Iter: 781 loss: 6.80320341e-07
Iter: 782 loss: 6.80214e-07
Iter: 783 loss: 6.80106496e-07
Iter: 784 loss: 6.8007563e-07
Iter: 785 loss: 6.79905611e-07
Iter: 786 loss: 6.79908965e-07
Iter: 787 loss: 6.7983126e-07
Iter: 788 loss: 6.79834272e-07
Iter: 789 loss: 6.79732523e-07
Iter: 790 loss: 6.79631285e-07
Iter: 791 loss: 6.79681364e-07
Iter: 792 loss: 6.79539653e-07
Iter: 793 loss: 6.79360312e-07
Iter: 794 loss: 6.79437676e-07
Iter: 795 loss: 6.79235313e-07
Iter: 796 loss: 6.78999413e-07
Iter: 797 loss: 6.79310688e-07
Iter: 798 loss: 6.78887375e-07
Iter: 799 loss: 6.78666254e-07
Iter: 800 loss: 6.81306403e-07
Iter: 801 loss: 6.78660513e-07
Iter: 802 loss: 6.78463948e-07
Iter: 803 loss: 6.79139873e-07
Iter: 804 loss: 6.78389142e-07
Iter: 805 loss: 6.78274489e-07
Iter: 806 loss: 6.77987202e-07
Iter: 807 loss: 6.82146e-07
Iter: 808 loss: 6.77968387e-07
Iter: 809 loss: 6.77733e-07
Iter: 810 loss: 6.80599328e-07
Iter: 811 loss: 6.77747948e-07
Iter: 812 loss: 6.77618345e-07
Iter: 813 loss: 6.77615731e-07
Iter: 814 loss: 6.77501e-07
Iter: 815 loss: 6.77310311e-07
Iter: 816 loss: 6.77322078e-07
Iter: 817 loss: 6.77234254e-07
Iter: 818 loss: 6.77237949e-07
Iter: 819 loss: 6.77130117e-07
Iter: 820 loss: 6.77086746e-07
Iter: 821 loss: 6.7706253e-07
Iter: 822 loss: 6.76933269e-07
Iter: 823 loss: 6.77157743e-07
Iter: 824 loss: 6.76876141e-07
Iter: 825 loss: 6.76718969e-07
Iter: 826 loss: 6.76565e-07
Iter: 827 loss: 6.76545653e-07
Iter: 828 loss: 6.76298384e-07
Iter: 829 loss: 6.77025696e-07
Iter: 830 loss: 6.76230798e-07
Iter: 831 loss: 6.7601195e-07
Iter: 832 loss: 6.77201058e-07
Iter: 833 loss: 6.75971535e-07
Iter: 834 loss: 6.75797423e-07
Iter: 835 loss: 6.77803314e-07
Iter: 836 loss: 6.75773492e-07
Iter: 837 loss: 6.7570295e-07
Iter: 838 loss: 6.75478077e-07
Iter: 839 loss: 6.79949608e-07
Iter: 840 loss: 6.75473814e-07
Iter: 841 loss: 6.75256956e-07
Iter: 842 loss: 6.75864726e-07
Iter: 843 loss: 6.7519909e-07
Iter: 844 loss: 6.75033334e-07
Iter: 845 loss: 6.77007506e-07
Iter: 846 loss: 6.75043793e-07
Iter: 847 loss: 6.74834496e-07
Iter: 848 loss: 6.74921e-07
Iter: 849 loss: 6.74671696e-07
Iter: 850 loss: 6.74561306e-07
Iter: 851 loss: 6.75877459e-07
Iter: 852 loss: 6.74574437e-07
Iter: 853 loss: 6.74469902e-07
Iter: 854 loss: 6.74656121e-07
Iter: 855 loss: 6.74400894e-07
Iter: 856 loss: 6.74277e-07
Iter: 857 loss: 6.74303294e-07
Iter: 858 loss: 6.74205637e-07
Iter: 859 loss: 6.74053e-07
Iter: 860 loss: 6.74718194e-07
Iter: 861 loss: 6.74023113e-07
Iter: 862 loss: 6.73901866e-07
Iter: 863 loss: 6.73834393e-07
Iter: 864 loss: 6.73806539e-07
Iter: 865 loss: 6.73667955e-07
Iter: 866 loss: 6.74622072e-07
Iter: 867 loss: 6.73688135e-07
Iter: 868 loss: 6.73576e-07
Iter: 869 loss: 6.74725186e-07
Iter: 870 loss: 6.73594968e-07
Iter: 871 loss: 6.73483839e-07
Iter: 872 loss: 6.7330194e-07
Iter: 873 loss: 6.73306204e-07
Iter: 874 loss: 6.73142836e-07
Iter: 875 loss: 6.73348268e-07
Iter: 876 loss: 6.73064164e-07
Iter: 877 loss: 6.72842361e-07
Iter: 878 loss: 6.73142722e-07
Iter: 879 loss: 6.72751696e-07
Iter: 880 loss: 6.72629824e-07
Iter: 881 loss: 6.72580541e-07
Iter: 882 loss: 6.7252688e-07
Iter: 883 loss: 6.7238949e-07
Iter: 884 loss: 6.7238e-07
Iter: 885 loss: 6.72215833e-07
Iter: 886 loss: 6.72235387e-07
Iter: 887 loss: 6.72132728e-07
Iter: 888 loss: 6.72039675e-07
Iter: 889 loss: 6.72021542e-07
Iter: 890 loss: 6.71890632e-07
Iter: 891 loss: 6.7301562e-07
Iter: 892 loss: 6.71875682e-07
Iter: 893 loss: 6.71788371e-07
Iter: 894 loss: 6.71659222e-07
Iter: 895 loss: 6.71617272e-07
Iter: 896 loss: 6.71461294e-07
Iter: 897 loss: 6.71620057e-07
Iter: 898 loss: 6.71391035e-07
Iter: 899 loss: 6.71262853e-07
Iter: 900 loss: 6.73094235e-07
Iter: 901 loss: 6.71269504e-07
Iter: 902 loss: 6.71149735e-07
Iter: 903 loss: 6.71473913e-07
Iter: 904 loss: 6.71111593e-07
Iter: 905 loss: 6.71017233e-07
Iter: 906 loss: 6.70950214e-07
Iter: 907 loss: 6.70899908e-07
Iter: 908 loss: 6.70750069e-07
Iter: 909 loss: 6.70852955e-07
Iter: 910 loss: 6.70664292e-07
Iter: 911 loss: 6.70488532e-07
Iter: 912 loss: 6.7100143e-07
Iter: 913 loss: 6.7044482e-07
Iter: 914 loss: 6.70272584e-07
Iter: 915 loss: 6.70283725e-07
Iter: 916 loss: 6.70175837e-07
Iter: 917 loss: 6.70151906e-07
Iter: 918 loss: 6.70076531e-07
Iter: 919 loss: 6.69988765e-07
Iter: 920 loss: 6.69980636e-07
Iter: 921 loss: 6.69948633e-07
Iter: 922 loss: 6.69767303e-07
Iter: 923 loss: 6.70677878e-07
Iter: 924 loss: 6.69716428e-07
Iter: 925 loss: 6.69587791e-07
Iter: 926 loss: 6.69569886e-07
Iter: 927 loss: 6.69464953e-07
Iter: 928 loss: 6.69352403e-07
Iter: 929 loss: 6.69329893e-07
Iter: 930 loss: 6.69167321e-07
Iter: 931 loss: 6.69547717e-07
Iter: 932 loss: 6.69089445e-07
Iter: 933 loss: 6.68920848e-07
Iter: 934 loss: 6.69373549e-07
Iter: 935 loss: 6.68857751e-07
Iter: 936 loss: 6.68723487e-07
Iter: 937 loss: 6.68834332e-07
Iter: 938 loss: 6.68570578e-07
Iter: 939 loss: 6.68436599e-07
Iter: 940 loss: 6.69317e-07
Iter: 941 loss: 6.68359803e-07
Iter: 942 loss: 6.68271e-07
Iter: 943 loss: 6.68343432e-07
Iter: 944 loss: 6.68204621e-07
Iter: 945 loss: 6.68034e-07
Iter: 946 loss: 6.68406528e-07
Iter: 947 loss: 6.6798674e-07
Iter: 948 loss: 6.67884e-07
Iter: 949 loss: 6.67888912e-07
Iter: 950 loss: 6.67805637e-07
Iter: 951 loss: 6.67765789e-07
Iter: 952 loss: 6.67713039e-07
Iter: 953 loss: 6.67592644e-07
Iter: 954 loss: 6.69233259e-07
Iter: 955 loss: 6.67600489e-07
Iter: 956 loss: 6.67536142e-07
Iter: 957 loss: 6.67345e-07
Iter: 958 loss: 6.69507358e-07
Iter: 959 loss: 6.67347479e-07
Iter: 960 loss: 6.67189738e-07
Iter: 961 loss: 6.68862526e-07
Iter: 962 loss: 6.67210315e-07
Iter: 963 loss: 6.67058657e-07
Iter: 964 loss: 6.67431e-07
Iter: 965 loss: 6.67036034e-07
Iter: 966 loss: 6.66963501e-07
Iter: 967 loss: 6.66885171e-07
Iter: 968 loss: 6.66843846e-07
Iter: 969 loss: 6.66734479e-07
Iter: 970 loss: 6.66742608e-07
Iter: 971 loss: 6.66651374e-07
Iter: 972 loss: 6.66542519e-07
Iter: 973 loss: 6.66566393e-07
Iter: 974 loss: 6.664485e-07
Iter: 975 loss: 6.6654593e-07
Iter: 976 loss: 6.66380515e-07
Iter: 977 loss: 6.661719e-07
Iter: 978 loss: 6.66213907e-07
Iter: 979 loss: 6.659875e-07
Iter: 980 loss: 6.65798666e-07
Iter: 981 loss: 6.66940252e-07
Iter: 982 loss: 6.65761604e-07
Iter: 983 loss: 6.65556399e-07
Iter: 984 loss: 6.66494543e-07
Iter: 985 loss: 6.65557366e-07
Iter: 986 loss: 6.65403491e-07
Iter: 987 loss: 6.65875859e-07
Iter: 988 loss: 6.65399853e-07
Iter: 989 loss: 6.65243533e-07
Iter: 990 loss: 6.65988182e-07
Iter: 991 loss: 6.65233301e-07
Iter: 992 loss: 6.65135644e-07
Iter: 993 loss: 6.64994957e-07
Iter: 994 loss: 6.68152836e-07
Iter: 995 loss: 6.64983e-07
Iter: 996 loss: 6.64863478e-07
Iter: 997 loss: 6.66532742e-07
Iter: 998 loss: 6.64870811e-07
Iter: 999 loss: 6.64759e-07
Iter: 1000 loss: 6.64789e-07
Iter: 1001 loss: 6.64692493e-07
Iter: 1002 loss: 6.64580966e-07
Iter: 1003 loss: 6.64773836e-07
Iter: 1004 loss: 6.64518723e-07
Iter: 1005 loss: 6.64451363e-07
Iter: 1006 loss: 6.64426238e-07
Iter: 1007 loss: 6.64349386e-07
Iter: 1008 loss: 6.64208869e-07
Iter: 1009 loss: 6.64192498e-07
Iter: 1010 loss: 6.64053289e-07
Iter: 1011 loss: 6.64033337e-07
Iter: 1012 loss: 6.63956257e-07
Iter: 1013 loss: 6.63728e-07
Iter: 1014 loss: 6.6524143e-07
Iter: 1015 loss: 6.637095e-07
Iter: 1016 loss: 6.63566311e-07
Iter: 1017 loss: 6.64462164e-07
Iter: 1018 loss: 6.63578703e-07
Iter: 1019 loss: 6.63444553e-07
Iter: 1020 loss: 6.63754747e-07
Iter: 1021 loss: 6.63377364e-07
Iter: 1022 loss: 6.63270498e-07
Iter: 1023 loss: 6.64306583e-07
Iter: 1024 loss: 6.6327209e-07
Iter: 1025 loss: 6.6320132e-07
Iter: 1026 loss: 6.63080243e-07
Iter: 1027 loss: 6.63076605e-07
Iter: 1028 loss: 6.62931143e-07
Iter: 1029 loss: 6.62946377e-07
Iter: 1030 loss: 6.62823822e-07
Iter: 1031 loss: 6.62741797e-07
Iter: 1032 loss: 6.62747425e-07
Iter: 1033 loss: 6.62650564e-07
Iter: 1034 loss: 6.6256689e-07
Iter: 1035 loss: 6.62541595e-07
Iter: 1036 loss: 6.62395337e-07
Iter: 1037 loss: 6.62784373e-07
Iter: 1038 loss: 6.62376465e-07
Iter: 1039 loss: 6.62254195e-07
Iter: 1040 loss: 6.64035099e-07
Iter: 1041 loss: 6.62252546e-07
Iter: 1042 loss: 6.62155969e-07
Iter: 1043 loss: 6.61954687e-07
Iter: 1044 loss: 6.64386448e-07
Iter: 1045 loss: 6.61928311e-07
Iter: 1046 loss: 6.61738454e-07
Iter: 1047 loss: 6.63181e-07
Iter: 1048 loss: 6.61728e-07
Iter: 1049 loss: 6.61626132e-07
Iter: 1050 loss: 6.62711727e-07
Iter: 1051 loss: 6.61623574e-07
Iter: 1052 loss: 6.61491583e-07
Iter: 1053 loss: 6.61912509e-07
Iter: 1054 loss: 6.61478339e-07
Iter: 1055 loss: 6.61397451e-07
Iter: 1056 loss: 6.61882893e-07
Iter: 1057 loss: 6.61380511e-07
Iter: 1058 loss: 6.61336401e-07
Iter: 1059 loss: 6.61460945e-07
Iter: 1060 loss: 6.61291949e-07
Iter: 1061 loss: 6.6123971e-07
Iter: 1062 loss: 6.61073955e-07
Iter: 1063 loss: 6.62878278e-07
Iter: 1064 loss: 6.61032573e-07
Iter: 1065 loss: 6.60956346e-07
Iter: 1066 loss: 6.60933893e-07
Iter: 1067 loss: 6.60826572e-07
Iter: 1068 loss: 6.6083885e-07
Iter: 1069 loss: 6.60762339e-07
Iter: 1070 loss: 6.60595106e-07
Iter: 1071 loss: 6.60462945e-07
Iter: 1072 loss: 6.60409228e-07
Iter: 1073 loss: 6.60307e-07
Iter: 1074 loss: 6.6030475e-07
Iter: 1075 loss: 6.60134219e-07
Iter: 1076 loss: 6.60040826e-07
Iter: 1077 loss: 6.60008e-07
Iter: 1078 loss: 6.5987058e-07
Iter: 1079 loss: 6.60025e-07
Iter: 1080 loss: 6.59794807e-07
Iter: 1081 loss: 6.59649686e-07
Iter: 1082 loss: 6.59836473e-07
Iter: 1083 loss: 6.59591478e-07
Iter: 1084 loss: 6.59535544e-07
Iter: 1085 loss: 6.5950934e-07
Iter: 1086 loss: 6.59447096e-07
Iter: 1087 loss: 6.5942686e-07
Iter: 1088 loss: 6.59409238e-07
Iter: 1089 loss: 6.5925866e-07
Iter: 1090 loss: 6.59472e-07
Iter: 1091 loss: 6.59252692e-07
Iter: 1092 loss: 6.59102511e-07
Iter: 1093 loss: 6.591942e-07
Iter: 1094 loss: 6.59027819e-07
Iter: 1095 loss: 6.58928286e-07
Iter: 1096 loss: 6.5872257e-07
Iter: 1097 loss: 6.63328422e-07
Iter: 1098 loss: 6.58726037e-07
Iter: 1099 loss: 6.58582508e-07
Iter: 1100 loss: 6.58535726e-07
Iter: 1101 loss: 6.58453359e-07
Iter: 1102 loss: 6.58293629e-07
Iter: 1103 loss: 6.5827561e-07
Iter: 1104 loss: 6.58104454e-07
Iter: 1105 loss: 6.58864508e-07
Iter: 1106 loss: 6.58037379e-07
Iter: 1107 loss: 6.57899363e-07
Iter: 1108 loss: 6.60153887e-07
Iter: 1109 loss: 6.57894304e-07
Iter: 1110 loss: 6.57820124e-07
Iter: 1111 loss: 6.57656528e-07
Iter: 1112 loss: 6.60299349e-07
Iter: 1113 loss: 6.57627083e-07
Iter: 1114 loss: 6.574694e-07
Iter: 1115 loss: 6.5796894e-07
Iter: 1116 loss: 6.57392093e-07
Iter: 1117 loss: 6.57221563e-07
Iter: 1118 loss: 6.58736667e-07
Iter: 1119 loss: 6.57212468e-07
Iter: 1120 loss: 6.57058706e-07
Iter: 1121 loss: 6.57923124e-07
Iter: 1122 loss: 6.57060241e-07
Iter: 1123 loss: 6.56953148e-07
Iter: 1124 loss: 6.5748111e-07
Iter: 1125 loss: 6.56944167e-07
Iter: 1126 loss: 6.56873283e-07
Iter: 1127 loss: 6.56754821e-07
Iter: 1128 loss: 6.56716793e-07
Iter: 1129 loss: 6.56598843e-07
Iter: 1130 loss: 6.56775967e-07
Iter: 1131 loss: 6.56541488e-07
Iter: 1132 loss: 6.56416887e-07
Iter: 1133 loss: 6.56626071e-07
Iter: 1134 loss: 6.56334407e-07
Iter: 1135 loss: 6.56225552e-07
Iter: 1136 loss: 6.57932219e-07
Iter: 1137 loss: 6.56217253e-07
Iter: 1138 loss: 6.56137559e-07
Iter: 1139 loss: 6.56101633e-07
Iter: 1140 loss: 6.56057409e-07
Iter: 1141 loss: 6.55976464e-07
Iter: 1142 loss: 6.56818827e-07
Iter: 1143 loss: 6.55965e-07
Iter: 1144 loss: 6.55884207e-07
Iter: 1145 loss: 6.55817189e-07
Iter: 1146 loss: 6.55773533e-07
Iter: 1147 loss: 6.55610847e-07
Iter: 1148 loss: 6.55558495e-07
Iter: 1149 loss: 6.55488407e-07
Iter: 1150 loss: 6.55271265e-07
Iter: 1151 loss: 6.5538876e-07
Iter: 1152 loss: 6.551453e-07
Iter: 1153 loss: 6.55111819e-07
Iter: 1154 loss: 6.55028771e-07
Iter: 1155 loss: 6.54903488e-07
Iter: 1156 loss: 6.54812538e-07
Iter: 1157 loss: 6.54802875e-07
Iter: 1158 loss: 6.54607e-07
Iter: 1159 loss: 6.55308611e-07
Iter: 1160 loss: 6.54549922e-07
Iter: 1161 loss: 6.54430437e-07
Iter: 1162 loss: 6.54387065e-07
Iter: 1163 loss: 6.54326698e-07
Iter: 1164 loss: 6.54126382e-07
Iter: 1165 loss: 6.54111886e-07
Iter: 1166 loss: 6.53981147e-07
Iter: 1167 loss: 6.53800271e-07
Iter: 1168 loss: 6.56490329e-07
Iter: 1169 loss: 6.53768666e-07
Iter: 1170 loss: 6.53647248e-07
Iter: 1171 loss: 6.54280427e-07
Iter: 1172 loss: 6.53651739e-07
Iter: 1173 loss: 6.53519578e-07
Iter: 1174 loss: 6.53342e-07
Iter: 1175 loss: 6.53339498e-07
Iter: 1176 loss: 6.53360303e-07
Iter: 1177 loss: 6.53243603e-07
Iter: 1178 loss: 6.53210293e-07
Iter: 1179 loss: 6.53055963e-07
Iter: 1180 loss: 6.53640029e-07
Iter: 1181 loss: 6.52964331e-07
Iter: 1182 loss: 6.52727522e-07
Iter: 1183 loss: 6.54156338e-07
Iter: 1184 loss: 6.5267875e-07
Iter: 1185 loss: 6.52552842e-07
Iter: 1186 loss: 6.53942493e-07
Iter: 1187 loss: 6.52539143e-07
Iter: 1188 loss: 6.52367646e-07
Iter: 1189 loss: 6.52769131e-07
Iter: 1190 loss: 6.523087e-07
Iter: 1191 loss: 6.52190408e-07
Iter: 1192 loss: 6.53236953e-07
Iter: 1193 loss: 6.5218444e-07
Iter: 1194 loss: 6.5209e-07
Iter: 1195 loss: 6.52031758e-07
Iter: 1196 loss: 6.52003678e-07
Iter: 1197 loss: 6.51872085e-07
Iter: 1198 loss: 6.51825758e-07
Iter: 1199 loss: 6.51735604e-07
Iter: 1200 loss: 6.51570645e-07
Iter: 1201 loss: 6.52208e-07
Iter: 1202 loss: 6.5154029e-07
Iter: 1203 loss: 6.51376808e-07
Iter: 1204 loss: 6.52021356e-07
Iter: 1205 loss: 6.5133645e-07
Iter: 1206 loss: 6.51146649e-07
Iter: 1207 loss: 6.51479809e-07
Iter: 1208 loss: 6.5104382e-07
Iter: 1209 loss: 6.50945935e-07
Iter: 1210 loss: 6.51459231e-07
Iter: 1211 loss: 6.50903075e-07
Iter: 1212 loss: 6.50713901e-07
Iter: 1213 loss: 6.51070877e-07
Iter: 1214 loss: 6.50630511e-07
Iter: 1215 loss: 6.50545303e-07
Iter: 1216 loss: 6.50453e-07
Iter: 1217 loss: 6.5044037e-07
Iter: 1218 loss: 6.50268248e-07
Iter: 1219 loss: 6.50817071e-07
Iter: 1220 loss: 6.50204242e-07
Iter: 1221 loss: 6.50195261e-07
Iter: 1222 loss: 6.50145921e-07
Iter: 1223 loss: 6.5008885e-07
Iter: 1224 loss: 6.50017228e-07
Iter: 1225 loss: 6.50013817e-07
Iter: 1226 loss: 6.49929405e-07
Iter: 1227 loss: 6.50654783e-07
Iter: 1228 loss: 6.49890808e-07
Iter: 1229 loss: 6.49828962e-07
Iter: 1230 loss: 6.49713684e-07
Iter: 1231 loss: 6.49709477e-07
Iter: 1232 loss: 6.49575952e-07
Iter: 1233 loss: 6.49871311e-07
Iter: 1234 loss: 6.49529397e-07
Iter: 1235 loss: 6.49403205e-07
Iter: 1236 loss: 6.49804633e-07
Iter: 1237 loss: 6.49354092e-07
Iter: 1238 loss: 6.49229492e-07
Iter: 1239 loss: 6.5023886e-07
Iter: 1240 loss: 6.49185893e-07
Iter: 1241 loss: 6.49097956e-07
Iter: 1242 loss: 6.49078856e-07
Iter: 1243 loss: 6.489729e-07
Iter: 1244 loss: 6.48864e-07
Iter: 1245 loss: 6.4888718e-07
Iter: 1246 loss: 6.48802768e-07
Iter: 1247 loss: 6.4870369e-07
Iter: 1248 loss: 6.48691866e-07
Iter: 1249 loss: 6.48580226e-07
Iter: 1250 loss: 6.48609785e-07
Iter: 1251 loss: 6.48489959e-07
Iter: 1252 loss: 6.4832841e-07
Iter: 1253 loss: 6.49183221e-07
Iter: 1254 loss: 6.48309594e-07
Iter: 1255 loss: 6.48168111e-07
Iter: 1256 loss: 6.4942833e-07
Iter: 1257 loss: 6.48139235e-07
Iter: 1258 loss: 6.48075684e-07
Iter: 1259 loss: 6.48278956e-07
Iter: 1260 loss: 6.48044704e-07
Iter: 1261 loss: 6.47948411e-07
Iter: 1262 loss: 6.47930278e-07
Iter: 1263 loss: 6.47889692e-07
Iter: 1264 loss: 6.47800221e-07
Iter: 1265 loss: 6.47732918e-07
Iter: 1266 loss: 6.47696197e-07
Iter: 1267 loss: 6.47553236e-07
Iter: 1268 loss: 6.48780883e-07
Iter: 1269 loss: 6.47567049e-07
Iter: 1270 loss: 6.47448928e-07
Iter: 1271 loss: 6.4768318e-07
Iter: 1272 loss: 6.4743017e-07
Iter: 1273 loss: 6.4734877e-07
Iter: 1274 loss: 6.47807383e-07
Iter: 1275 loss: 6.4732285e-07
Iter: 1276 loss: 6.47240256e-07
Iter: 1277 loss: 6.47257593e-07
Iter: 1278 loss: 6.47190063e-07
Iter: 1279 loss: 6.47064326e-07
Iter: 1280 loss: 6.47492698e-07
Iter: 1281 loss: 6.47020386e-07
Iter: 1282 loss: 6.46923183e-07
Iter: 1283 loss: 6.46857131e-07
Iter: 1284 loss: 6.46834053e-07
Iter: 1285 loss: 6.46648232e-07
Iter: 1286 loss: 6.46718718e-07
Iter: 1287 loss: 6.46537956e-07
Iter: 1288 loss: 6.46711555e-07
Iter: 1289 loss: 6.46479862e-07
Iter: 1290 loss: 6.46446892e-07
Iter: 1291 loss: 6.46318199e-07
Iter: 1292 loss: 6.48036519e-07
Iter: 1293 loss: 6.46312856e-07
Iter: 1294 loss: 6.46142666e-07
Iter: 1295 loss: 6.47096272e-07
Iter: 1296 loss: 6.46126e-07
Iter: 1297 loss: 6.46009255e-07
Iter: 1298 loss: 6.46019942e-07
Iter: 1299 loss: 6.45937575e-07
Iter: 1300 loss: 6.45824343e-07
Iter: 1301 loss: 6.45834e-07
Iter: 1302 loss: 6.45684054e-07
Iter: 1303 loss: 6.45601403e-07
Iter: 1304 loss: 6.47530783e-07
Iter: 1305 loss: 6.45596515e-07
Iter: 1306 loss: 6.45495504e-07
Iter: 1307 loss: 6.45591854e-07
Iter: 1308 loss: 6.45455884e-07
Iter: 1309 loss: 6.45338901e-07
Iter: 1310 loss: 6.45588443e-07
Iter: 1311 loss: 6.45263526e-07
Iter: 1312 loss: 6.45184798e-07
Iter: 1313 loss: 6.46213891e-07
Iter: 1314 loss: 6.45182809e-07
Iter: 1315 loss: 6.45113289e-07
Iter: 1316 loss: 6.44949125e-07
Iter: 1317 loss: 6.47235083e-07
Iter: 1318 loss: 6.44946226e-07
Iter: 1319 loss: 6.44794113e-07
Iter: 1320 loss: 6.45339298e-07
Iter: 1321 loss: 6.44766487e-07
Iter: 1322 loss: 6.4461176e-07
Iter: 1323 loss: 6.45223736e-07
Iter: 1324 loss: 6.44564466e-07
Iter: 1325 loss: 6.44452484e-07
Iter: 1326 loss: 6.44450949e-07
Iter: 1327 loss: 6.44379782e-07
Iter: 1328 loss: 6.44358238e-07
Iter: 1329 loss: 6.44366708e-07
Iter: 1330 loss: 6.44247166e-07
Iter: 1331 loss: 6.44227725e-07
Iter: 1332 loss: 6.44178442e-07
Iter: 1333 loss: 6.44037e-07
Iter: 1334 loss: 6.44014e-07
Iter: 1335 loss: 6.4393879e-07
Iter: 1336 loss: 6.43799694e-07
Iter: 1337 loss: 6.44909449e-07
Iter: 1338 loss: 6.43790486e-07
Iter: 1339 loss: 6.43676458e-07
Iter: 1340 loss: 6.44924398e-07
Iter: 1341 loss: 6.43689816e-07
Iter: 1342 loss: 6.43604267e-07
Iter: 1343 loss: 6.43628766e-07
Iter: 1344 loss: 6.43571639e-07
Iter: 1345 loss: 6.43473641e-07
Iter: 1346 loss: 6.43979604e-07
Iter: 1347 loss: 6.43459146e-07
Iter: 1348 loss: 6.43397584e-07
Iter: 1349 loss: 6.43586e-07
Iter: 1350 loss: 6.43347505e-07
Iter: 1351 loss: 6.43272415e-07
Iter: 1352 loss: 6.43239446e-07
Iter: 1353 loss: 6.43233307e-07
Iter: 1354 loss: 6.43119165e-07
Iter: 1355 loss: 6.43311523e-07
Iter: 1356 loss: 6.43082e-07
Iter: 1357 loss: 6.43088299e-07
Iter: 1358 loss: 6.43032195e-07
Iter: 1359 loss: 6.43003204e-07
Iter: 1360 loss: 6.42902478e-07
Iter: 1361 loss: 6.44158547e-07
Iter: 1362 loss: 6.42892644e-07
Iter: 1363 loss: 6.42831708e-07
Iter: 1364 loss: 6.43777355e-07
Iter: 1365 loss: 6.42810221e-07
Iter: 1366 loss: 6.42775149e-07
Iter: 1367 loss: 6.42732459e-07
Iter: 1368 loss: 6.42714099e-07
Iter: 1369 loss: 6.42640771e-07
Iter: 1370 loss: 6.42606153e-07
Iter: 1371 loss: 6.42540954e-07
Iter: 1372 loss: 6.42503778e-07
Iter: 1373 loss: 6.42492409e-07
Iter: 1374 loss: 6.42438067e-07
Iter: 1375 loss: 6.42389e-07
Iter: 1376 loss: 6.42340751e-07
Iter: 1377 loss: 6.42250825e-07
Iter: 1378 loss: 6.42554824e-07
Iter: 1379 loss: 6.42201769e-07
Iter: 1380 loss: 6.42133216e-07
Iter: 1381 loss: 6.42643954e-07
Iter: 1382 loss: 6.42135888e-07
Iter: 1383 loss: 6.4204778e-07
Iter: 1384 loss: 6.42028226e-07
Iter: 1385 loss: 6.4201231e-07
Iter: 1386 loss: 6.41876e-07
Iter: 1387 loss: 6.41995825e-07
Iter: 1388 loss: 6.41789711e-07
Iter: 1389 loss: 6.41740314e-07
Iter: 1390 loss: 6.42555733e-07
Iter: 1391 loss: 6.41716667e-07
Iter: 1392 loss: 6.41626343e-07
Iter: 1393 loss: 6.41979341e-07
Iter: 1394 loss: 6.41596159e-07
Iter: 1395 loss: 6.41560177e-07
Iter: 1396 loss: 6.41645556e-07
Iter: 1397 loss: 6.41541078e-07
Iter: 1398 loss: 6.41437339e-07
Iter: 1399 loss: 6.4154608e-07
Iter: 1400 loss: 6.41406587e-07
Iter: 1401 loss: 6.41334395e-07
Iter: 1402 loss: 6.41378392e-07
Iter: 1403 loss: 6.41283293e-07
Iter: 1404 loss: 6.41224e-07
Iter: 1405 loss: 6.41348379e-07
Iter: 1406 loss: 6.4118808e-07
Iter: 1407 loss: 6.41131862e-07
Iter: 1408 loss: 6.41141696e-07
Iter: 1409 loss: 6.41073825e-07
Iter: 1410 loss: 6.41106055e-07
Iter: 1411 loss: 6.41039094e-07
Iter: 1412 loss: 6.41004e-07
Iter: 1413 loss: 6.41191036e-07
Iter: 1414 loss: 6.40980033e-07
Iter: 1415 loss: 6.40919438e-07
Iter: 1416 loss: 6.40906137e-07
Iter: 1417 loss: 6.40863959e-07
Iter: 1418 loss: 6.40792337e-07
Iter: 1419 loss: 6.40847418e-07
Iter: 1420 loss: 6.40718895e-07
Iter: 1421 loss: 6.40598273e-07
Iter: 1422 loss: 6.40656936e-07
Iter: 1423 loss: 6.40510905e-07
Iter: 1424 loss: 6.40467533e-07
Iter: 1425 loss: 6.4042996e-07
Iter: 1426 loss: 6.40370558e-07
Iter: 1427 loss: 6.40238e-07
Iter: 1428 loss: 6.42011855e-07
Iter: 1429 loss: 6.40250164e-07
Iter: 1430 loss: 6.40144037e-07
Iter: 1431 loss: 6.40143298e-07
Iter: 1432 loss: 6.4008367e-07
Iter: 1433 loss: 6.40034159e-07
Iter: 1434 loss: 6.40013e-07
Iter: 1435 loss: 6.39955829e-07
Iter: 1436 loss: 6.39879318e-07
Iter: 1437 loss: 6.3982236e-07
Iter: 1438 loss: 6.3971936e-07
Iter: 1439 loss: 6.40570761e-07
Iter: 1440 loss: 6.39696736e-07
Iter: 1441 loss: 6.39659845e-07
Iter: 1442 loss: 6.40108453e-07
Iter: 1443 loss: 6.39665245e-07
Iter: 1444 loss: 6.39585778e-07
Iter: 1445 loss: 6.39683549e-07
Iter: 1446 loss: 6.39539792e-07
Iter: 1447 loss: 6.39487553e-07
Iter: 1448 loss: 6.39669565e-07
Iter: 1449 loss: 6.3941809e-07
Iter: 1450 loss: 6.39374548e-07
Iter: 1451 loss: 6.39364885e-07
Iter: 1452 loss: 6.39308155e-07
Iter: 1453 loss: 6.39211066e-07
Iter: 1454 loss: 6.39179518e-07
Iter: 1455 loss: 6.39116536e-07
Iter: 1456 loss: 6.39059635e-07
Iter: 1457 loss: 6.39040195e-07
Iter: 1458 loss: 6.38962433e-07
Iter: 1459 loss: 6.38952315e-07
Iter: 1460 loss: 6.38897177e-07
Iter: 1461 loss: 6.38842096e-07
Iter: 1462 loss: 6.38970619e-07
Iter: 1463 loss: 6.38800088e-07
Iter: 1464 loss: 6.38670144e-07
Iter: 1465 loss: 6.39070549e-07
Iter: 1466 loss: 6.38665426e-07
Iter: 1467 loss: 6.38584709e-07
Iter: 1468 loss: 6.38469487e-07
Iter: 1469 loss: 6.38440724e-07
Iter: 1470 loss: 6.3834392e-07
Iter: 1471 loss: 6.38355232e-07
Iter: 1472 loss: 6.38284064e-07
Iter: 1473 loss: 6.38398e-07
Iter: 1474 loss: 6.3821733e-07
Iter: 1475 loss: 6.38100232e-07
Iter: 1476 loss: 6.38325901e-07
Iter: 1477 loss: 6.38051915e-07
Iter: 1478 loss: 6.37984954e-07
Iter: 1479 loss: 6.37983703e-07
Iter: 1480 loss: 6.3795494e-07
Iter: 1481 loss: 6.3784006e-07
Iter: 1482 loss: 6.39507221e-07
Iter: 1483 loss: 6.37829601e-07
Iter: 1484 loss: 6.3770733e-07
Iter: 1485 loss: 6.38271672e-07
Iter: 1486 loss: 6.37682e-07
Iter: 1487 loss: 6.37626385e-07
Iter: 1488 loss: 6.38190954e-07
Iter: 1489 loss: 6.37628773e-07
Iter: 1490 loss: 6.37554308e-07
Iter: 1491 loss: 6.38264964e-07
Iter: 1492 loss: 6.37533503e-07
Iter: 1493 loss: 6.37539472e-07
Iter: 1494 loss: 6.37553399e-07
Iter: 1495 loss: 6.37555559e-07
Iter: 1496 loss: 6.37571873e-07
Iter: 1497 loss: 6.37553512e-07
Iter: 1498 loss: 6.37550897e-07
Iter: 1499 loss: 6.37557719e-07
Iter: 1500 loss: 6.37530206e-07
Iter: 1501 loss: 6.37549e-07
Iter: 1502 loss: 6.37532366e-07
Iter: 1503 loss: 6.37550329e-07
Iter: 1504 loss: 6.37546293e-07
Iter: 1505 loss: 6.37535265e-07
Iter: 1506 loss: 6.37536061e-07
Iter: 1507 loss: 6.37535209e-07
Iter: 1508 loss: 6.3752924e-07
Iter: 1509 loss: 6.37533503e-07
Iter: 1510 loss: 6.37535095e-07
Iter: 1511 loss: 6.37534527e-07
Iter: 1512 loss: 6.37533731e-07
Iter: 1513 loss: 6.37534299e-07
Iter: 1514 loss: 6.37533844e-07
Iter: 1515 loss: 6.37534413e-07
Iter: 1516 loss: 6.37534413e-07
Iter: 1517 loss: 6.37534413e-07
Iter: 1518 loss: 6.37533844e-07
Iter: 1519 loss: 6.37534413e-07
Iter: 1520 loss: 6.37534413e-07
Iter: 1521 loss: 6.37534413e-07
Iter: 1522 loss: 6.37534413e-07
Iter: 1523 loss: 6.37533844e-07
Iter: 1524 loss: 6.37534413e-07
Iter: 1525 loss: 6.39612665e-07
Iter: 1526 loss: 6.3746694e-07
Iter: 1527 loss: 6.37413905e-07
Iter: 1528 loss: 6.37422545e-07
Iter: 1529 loss: 6.37332732e-07
Iter: 1530 loss: 6.37415155e-07
Iter: 1531 loss: 6.37311189e-07
Iter: 1532 loss: 6.37251787e-07
Iter: 1533 loss: 6.37259745e-07
Iter: 1534 loss: 6.37208132e-07
Iter: 1535 loss: 6.37105359e-07
Iter: 1536 loss: 6.37445225e-07
Iter: 1537 loss: 6.37073128e-07
Iter: 1538 loss: 6.36965638e-07
Iter: 1539 loss: 6.37355129e-07
Iter: 1540 loss: 6.36938751e-07
Iter: 1541 loss: 6.36823643e-07
Iter: 1542 loss: 6.37293624e-07
Iter: 1543 loss: 6.36781124e-07
Iter: 1544 loss: 6.36717e-07
Iter: 1545 loss: 6.36923e-07
Iter: 1546 loss: 6.36688924e-07
Iter: 1547 loss: 6.36587174e-07
Iter: 1548 loss: 6.36745085e-07
Iter: 1549 loss: 6.36526806e-07
Iter: 1550 loss: 6.36450864e-07
Iter: 1551 loss: 6.36621678e-07
Iter: 1552 loss: 6.36381969e-07
Iter: 1553 loss: 6.36329275e-07
Iter: 1554 loss: 6.36319555e-07
Iter: 1555 loss: 6.36252878e-07
Iter: 1556 loss: 6.36247137e-07
Iter: 1557 loss: 6.36210359e-07
Iter: 1558 loss: 6.36126117e-07
Iter: 1559 loss: 6.36103778e-07
Iter: 1560 loss: 6.36086384e-07
Iter: 1561 loss: 6.35997139e-07
Iter: 1562 loss: 6.35971787e-07
Iter: 1563 loss: 6.35925346e-07
Iter: 1564 loss: 6.35799722e-07
Iter: 1565 loss: 6.36970753e-07
Iter: 1566 loss: 6.35809329e-07
Iter: 1567 loss: 6.3572918e-07
Iter: 1568 loss: 6.36007371e-07
Iter: 1569 loss: 6.35710364e-07
Iter: 1570 loss: 6.35663639e-07
Iter: 1571 loss: 6.35579738e-07
Iter: 1572 loss: 6.35560582e-07
Iter: 1573 loss: 6.35447691e-07
Iter: 1574 loss: 6.36331606e-07
Iter: 1575 loss: 6.35459401e-07
Iter: 1576 loss: 6.35362369e-07
Iter: 1577 loss: 6.3541944e-07
Iter: 1578 loss: 6.35295237e-07
Iter: 1579 loss: 6.35186439e-07
Iter: 1580 loss: 6.36172331e-07
Iter: 1581 loss: 6.3521486e-07
Iter: 1582 loss: 6.35120387e-07
Iter: 1583 loss: 6.35017955e-07
Iter: 1584 loss: 6.35005108e-07
Iter: 1585 loss: 6.34879143e-07
Iter: 1586 loss: 6.34914159e-07
Iter: 1587 loss: 6.34756702e-07
Iter: 1588 loss: 6.34590663e-07
Iter: 1589 loss: 6.34854473e-07
Iter: 1590 loss: 6.34529556e-07
Iter: 1591 loss: 6.34328444e-07
Iter: 1592 loss: 6.36875086e-07
Iter: 1593 loss: 6.34333674e-07
Iter: 1594 loss: 6.34196738e-07
Iter: 1595 loss: 6.35822289e-07
Iter: 1596 loss: 6.34207254e-07
Iter: 1597 loss: 6.34142111e-07
Iter: 1598 loss: 6.33977834e-07
Iter: 1599 loss: 6.37217227e-07
Iter: 1600 loss: 6.33985906e-07
Iter: 1601 loss: 6.33774334e-07
Iter: 1602 loss: 6.34185312e-07
Iter: 1603 loss: 6.33714649e-07
Iter: 1604 loss: 6.33561228e-07
Iter: 1605 loss: 6.33752222e-07
Iter: 1606 loss: 6.33486e-07
Iter: 1607 loss: 6.33356365e-07
Iter: 1608 loss: 6.3519667e-07
Iter: 1609 loss: 6.33346929e-07
Iter: 1610 loss: 6.33187312e-07
Iter: 1611 loss: 6.33685204e-07
Iter: 1612 loss: 6.33184357e-07
Iter: 1613 loss: 6.33117338e-07
Iter: 1614 loss: 6.33290938e-07
Iter: 1615 loss: 6.33091645e-07
Iter: 1616 loss: 6.33015759e-07
Iter: 1617 loss: 6.33387799e-07
Iter: 1618 loss: 6.32987906e-07
Iter: 1619 loss: 6.32913782e-07
Iter: 1620 loss: 6.33051059e-07
Iter: 1621 loss: 6.32901902e-07
Iter: 1622 loss: 6.32812657e-07
Iter: 1623 loss: 6.33094714e-07
Iter: 1624 loss: 6.32807e-07
Iter: 1625 loss: 6.32731826e-07
Iter: 1626 loss: 6.32591309e-07
Iter: 1627 loss: 6.32567321e-07
Iter: 1628 loss: 6.32422029e-07
Iter: 1629 loss: 6.32975684e-07
Iter: 1630 loss: 6.32384172e-07
Iter: 1631 loss: 6.322378e-07
Iter: 1632 loss: 6.32220235e-07
Iter: 1633 loss: 6.32142928e-07
Iter: 1634 loss: 6.32061415e-07
Iter: 1635 loss: 6.32051695e-07
Iter: 1636 loss: 6.31930391e-07
Iter: 1637 loss: 6.31923683e-07
Iter: 1638 loss: 6.31812441e-07
Iter: 1639 loss: 6.31647765e-07
Iter: 1640 loss: 6.32857223e-07
Iter: 1641 loss: 6.31643729e-07
Iter: 1642 loss: 6.31504577e-07
Iter: 1643 loss: 6.31450689e-07
Iter: 1644 loss: 6.31395835e-07
Iter: 1645 loss: 6.31279875e-07
Iter: 1646 loss: 6.31281694e-07
Iter: 1647 loss: 6.31157604e-07
Iter: 1648 loss: 6.31503838e-07
Iter: 1649 loss: 6.31106445e-07
Iter: 1650 loss: 6.31006856e-07
Iter: 1651 loss: 6.31129183e-07
Iter: 1652 loss: 6.30966383e-07
Iter: 1653 loss: 6.30912382e-07
Iter: 1654 loss: 6.31780381e-07
Iter: 1655 loss: 6.30878617e-07
Iter: 1656 loss: 6.30805062e-07
Iter: 1657 loss: 6.30756688e-07
Iter: 1658 loss: 6.30742e-07
Iter: 1659 loss: 6.306459e-07
Iter: 1660 loss: 6.31009243e-07
Iter: 1661 loss: 6.3061691e-07
Iter: 1662 loss: 6.30573368e-07
Iter: 1663 loss: 6.30556485e-07
Iter: 1664 loss: 6.30521299e-07
Iter: 1665 loss: 6.30436546e-07
Iter: 1666 loss: 6.32125136e-07
Iter: 1667 loss: 6.30443424e-07
Iter: 1668 loss: 6.30346392e-07
Iter: 1669 loss: 6.30906356e-07
Iter: 1670 loss: 6.30312854e-07
Iter: 1671 loss: 6.3022793e-07
Iter: 1672 loss: 6.30397722e-07
Iter: 1673 loss: 6.30179898e-07
Iter: 1674 loss: 6.30127772e-07
Iter: 1675 loss: 6.29968042e-07
Iter: 1676 loss: 6.29978558e-07
Iter: 1677 loss: 6.29817578e-07
Iter: 1678 loss: 6.30778629e-07
Iter: 1679 loss: 6.29791487e-07
Iter: 1680 loss: 6.29634656e-07
Iter: 1681 loss: 6.30300178e-07
Iter: 1682 loss: 6.29624651e-07
Iter: 1683 loss: 6.29588726e-07
Iter: 1684 loss: 6.2957281e-07
Iter: 1685 loss: 6.29510168e-07
Iter: 1686 loss: 6.29488568e-07
Iter: 1687 loss: 6.29490785e-07
Iter: 1688 loss: 6.293875e-07
Iter: 1689 loss: 6.29326451e-07
Iter: 1690 loss: 6.29337308e-07
Iter: 1691 loss: 6.29230726e-07
Iter: 1692 loss: 6.30289264e-07
Iter: 1693 loss: 6.29236922e-07
Iter: 1694 loss: 6.29148758e-07
Iter: 1695 loss: 6.29405179e-07
Iter: 1696 loss: 6.29141255e-07
Iter: 1697 loss: 6.2904e-07
Iter: 1698 loss: 6.29104477e-07
Iter: 1699 loss: 6.28983685e-07
Iter: 1700 loss: 6.28948442e-07
Iter: 1701 loss: 6.28942132e-07
Iter: 1702 loss: 6.28899102e-07
Iter: 1703 loss: 6.28760574e-07
Iter: 1704 loss: 6.29102828e-07
Iter: 1705 loss: 6.28663827e-07
Iter: 1706 loss: 6.28529449e-07
Iter: 1707 loss: 6.30086674e-07
Iter: 1708 loss: 6.28524617e-07
Iter: 1709 loss: 6.28441342e-07
Iter: 1710 loss: 6.28562134e-07
Iter: 1711 loss: 6.28384e-07
Iter: 1712 loss: 6.28292071e-07
Iter: 1713 loss: 6.28285079e-07
Iter: 1714 loss: 6.28232726e-07
Iter: 1715 loss: 6.28208568e-07
Iter: 1716 loss: 6.28148882e-07
Iter: 1717 loss: 6.2808067e-07
Iter: 1718 loss: 6.28265e-07
Iter: 1719 loss: 6.28039174e-07
Iter: 1720 loss: 6.27944e-07
Iter: 1721 loss: 6.28315e-07
Iter: 1722 loss: 6.27942256e-07
Iter: 1723 loss: 6.27845907e-07
Iter: 1724 loss: 6.2781811e-07
Iter: 1725 loss: 6.27799864e-07
Iter: 1726 loss: 6.27686632e-07
Iter: 1727 loss: 6.2824904e-07
Iter: 1728 loss: 6.27662e-07
Iter: 1729 loss: 6.27600798e-07
Iter: 1730 loss: 6.28122962e-07
Iter: 1731 loss: 6.27588804e-07
Iter: 1732 loss: 6.2749217e-07
Iter: 1733 loss: 6.27534632e-07
Iter: 1734 loss: 6.27467557e-07
Iter: 1735 loss: 6.27391387e-07
Iter: 1736 loss: 6.28699866e-07
Iter: 1737 loss: 6.27361715e-07
Iter: 1738 loss: 6.27318911e-07
Iter: 1739 loss: 6.27205964e-07
Iter: 1740 loss: 6.28994258e-07
Iter: 1741 loss: 6.27202894e-07
Iter: 1742 loss: 6.27111717e-07
Iter: 1743 loss: 6.2714787e-07
Iter: 1744 loss: 6.27032477e-07
Iter: 1745 loss: 6.26931296e-07
Iter: 1746 loss: 6.27939357e-07
Iter: 1747 loss: 6.26943915e-07
Iter: 1748 loss: 6.26841711e-07
Iter: 1749 loss: 6.27659347e-07
Iter: 1750 loss: 6.26851602e-07
Iter: 1751 loss: 6.26782594e-07
Iter: 1752 loss: 6.26598649e-07
Iter: 1753 loss: 6.29541319e-07
Iter: 1754 loss: 6.26621272e-07
Iter: 1755 loss: 6.2657756e-07
Iter: 1756 loss: 6.26544193e-07
Iter: 1757 loss: 6.26518045e-07
Iter: 1758 loss: 6.26415726e-07
Iter: 1759 loss: 6.26405267e-07
Iter: 1760 loss: 6.26298515e-07
Iter: 1761 loss: 6.2654567e-07
Iter: 1762 loss: 6.2624207e-07
Iter: 1763 loss: 6.26139297e-07
Iter: 1764 loss: 6.26531573e-07
Iter: 1765 loss: 6.26147539e-07
Iter: 1766 loss: 6.26025894e-07
Iter: 1767 loss: 6.26559086e-07
Iter: 1768 loss: 6.26030896e-07
Iter: 1769 loss: 6.25972689e-07
Iter: 1770 loss: 6.26251961e-07
Iter: 1771 loss: 6.25979681e-07
Iter: 1772 loss: 6.25910275e-07
Iter: 1773 loss: 6.26114229e-07
Iter: 1774 loss: 6.2590334e-07
Iter: 1775 loss: 6.25875259e-07
Iter: 1776 loss: 6.25728717e-07
Iter: 1777 loss: 6.26815563e-07
Iter: 1778 loss: 6.25728e-07
Iter: 1779 loss: 6.25612245e-07
Iter: 1780 loss: 6.26115252e-07
Iter: 1781 loss: 6.25610937e-07
Iter: 1782 loss: 6.25497819e-07
Iter: 1783 loss: 6.26868427e-07
Iter: 1784 loss: 6.25494067e-07
Iter: 1785 loss: 6.25418807e-07
Iter: 1786 loss: 6.25762198e-07
Iter: 1787 loss: 6.25420341e-07
Iter: 1788 loss: 6.25349116e-07
Iter: 1789 loss: 6.25271355e-07
Iter: 1790 loss: 6.25275447e-07
Iter: 1791 loss: 6.25186658e-07
Iter: 1792 loss: 6.25196662e-07
Iter: 1793 loss: 6.25144139e-07
Iter: 1794 loss: 6.25096504e-07
Iter: 1795 loss: 6.25058078e-07
Iter: 1796 loss: 6.24961217e-07
Iter: 1797 loss: 6.24868e-07
Iter: 1798 loss: 6.24848e-07
Iter: 1799 loss: 6.24737822e-07
Iter: 1800 loss: 6.26573808e-07
Iter: 1801 loss: 6.24751237e-07
Iter: 1802 loss: 6.24653239e-07
Iter: 1803 loss: 6.24912218e-07
Iter: 1804 loss: 6.24616064e-07
Iter: 1805 loss: 6.24545919e-07
Iter: 1806 loss: 6.24865265e-07
Iter: 1807 loss: 6.24499762e-07
Iter: 1808 loss: 6.24406312e-07
Iter: 1809 loss: 6.24366919e-07
Iter: 1810 loss: 6.24337304e-07
Iter: 1811 loss: 6.24198663e-07
Iter: 1812 loss: 6.2399846e-07
Iter: 1813 loss: 6.2401989e-07
Iter: 1814 loss: 6.23772792e-07
Iter: 1815 loss: 6.25221332e-07
Iter: 1816 loss: 6.23761821e-07
Iter: 1817 loss: 6.23685423e-07
Iter: 1818 loss: 6.23686e-07
Iter: 1819 loss: 6.23574806e-07
Iter: 1820 loss: 6.23684116e-07
Iter: 1821 loss: 6.2355025e-07
Iter: 1822 loss: 6.23467258e-07
Iter: 1823 loss: 6.23584e-07
Iter: 1824 loss: 6.23458732e-07
Iter: 1825 loss: 6.23347887e-07
Iter: 1826 loss: 6.23910239e-07
Iter: 1827 loss: 6.23375911e-07
Iter: 1828 loss: 6.23293772e-07
Iter: 1829 loss: 6.2328229e-07
Iter: 1830 loss: 6.23245e-07
Iter: 1831 loss: 6.2317406e-07
Iter: 1832 loss: 6.23437245e-07
Iter: 1833 loss: 6.23152289e-07
Iter: 1834 loss: 6.23104484e-07
Iter: 1835 loss: 6.23464757e-07
Iter: 1836 loss: 6.23068e-07
Iter: 1837 loss: 6.22971129e-07
Iter: 1838 loss: 6.23222263e-07
Iter: 1839 loss: 6.22935318e-07
Iter: 1840 loss: 6.22843743e-07
Iter: 1841 loss: 6.22904849e-07
Iter: 1842 loss: 6.22813445e-07
Iter: 1843 loss: 6.22723519e-07
Iter: 1844 loss: 6.22768937e-07
Iter: 1845 loss: 6.22676964e-07
Iter: 1846 loss: 6.22541791e-07
Iter: 1847 loss: 6.22354378e-07
Iter: 1848 loss: 6.22339201e-07
Iter: 1849 loss: 6.22197604e-07
Iter: 1850 loss: 6.23188384e-07
Iter: 1851 loss: 6.22193397e-07
Iter: 1852 loss: 6.22103244e-07
Iter: 1853 loss: 6.22092443e-07
Iter: 1854 loss: 6.22025823e-07
Iter: 1855 loss: 6.21948743e-07
Iter: 1856 loss: 6.21934078e-07
Iter: 1857 loss: 6.2179754e-07
Iter: 1858 loss: 6.21991489e-07
Iter: 1859 loss: 6.21738081e-07
Iter: 1860 loss: 6.21604329e-07
Iter: 1861 loss: 6.21618256e-07
Iter: 1862 loss: 6.21564823e-07
Iter: 1863 loss: 6.21478307e-07
Iter: 1864 loss: 6.214716e-07
Iter: 1865 loss: 6.21343e-07
Iter: 1866 loss: 6.22103869e-07
Iter: 1867 loss: 6.21349614e-07
Iter: 1868 loss: 6.21287768e-07
Iter: 1869 loss: 6.2204083e-07
Iter: 1870 loss: 6.21258721e-07
Iter: 1871 loss: 6.21219e-07
Iter: 1872 loss: 6.21374284e-07
Iter: 1873 loss: 6.21226832e-07
Iter: 1874 loss: 6.21157e-07
Iter: 1875 loss: 6.2104516e-07
Iter: 1876 loss: 6.21022934e-07
Iter: 1877 loss: 6.20943524e-07
Iter: 1878 loss: 6.21041181e-07
Iter: 1879 loss: 6.20896571e-07
Iter: 1880 loss: 6.20773676e-07
Iter: 1881 loss: 6.20897367e-07
Iter: 1882 loss: 6.20727064e-07
Iter: 1883 loss: 6.20573246e-07
Iter: 1884 loss: 6.20886112e-07
Iter: 1885 loss: 6.20542323e-07
Iter: 1886 loss: 6.2048673e-07
Iter: 1887 loss: 6.20447395e-07
Iter: 1888 loss: 6.20378273e-07
Iter: 1889 loss: 6.2025515e-07
Iter: 1890 loss: 6.21739787e-07
Iter: 1891 loss: 6.20247533e-07
Iter: 1892 loss: 6.20088088e-07
Iter: 1893 loss: 6.21015545e-07
Iter: 1894 loss: 6.20084336e-07
Iter: 1895 loss: 6.19931825e-07
Iter: 1896 loss: 6.20640208e-07
Iter: 1897 loss: 6.19873163e-07
Iter: 1898 loss: 6.1974481e-07
Iter: 1899 loss: 6.19787e-07
Iter: 1900 loss: 6.1966432e-07
Iter: 1901 loss: 6.19531477e-07
Iter: 1902 loss: 6.20419826e-07
Iter: 1903 loss: 6.19542e-07
Iter: 1904 loss: 6.19406592e-07
Iter: 1905 loss: 6.19857e-07
Iter: 1906 loss: 6.19383627e-07
Iter: 1907 loss: 6.19299101e-07
Iter: 1908 loss: 6.20007143e-07
Iter: 1909 loss: 6.19244304e-07
Iter: 1910 loss: 6.1921719e-07
Iter: 1911 loss: 6.19232708e-07
Iter: 1912 loss: 6.19193202e-07
Iter: 1913 loss: 6.19124876e-07
Iter: 1914 loss: 6.18966055e-07
Iter: 1915 loss: 6.18959461e-07
Iter: 1916 loss: 6.18827926e-07
Iter: 1917 loss: 6.19959678e-07
Iter: 1918 loss: 6.18829745e-07
Iter: 1919 loss: 6.18700483e-07
Iter: 1920 loss: 6.18754768e-07
Iter: 1921 loss: 6.18646823e-07
Iter: 1922 loss: 6.18519778e-07
Iter: 1923 loss: 6.18524723e-07
Iter: 1924 loss: 6.18475838e-07
Iter: 1925 loss: 6.18369882e-07
Iter: 1926 loss: 6.19697744e-07
Iter: 1927 loss: 6.1831895e-07
Iter: 1928 loss: 6.18249828e-07
Iter: 1929 loss: 6.1824511e-07
Iter: 1930 loss: 6.18181616e-07
Iter: 1931 loss: 6.1812591e-07
Iter: 1932 loss: 6.18114086e-07
Iter: 1933 loss: 6.17991304e-07
Iter: 1934 loss: 6.18347e-07
Iter: 1935 loss: 6.17971182e-07
Iter: 1936 loss: 6.1790513e-07
Iter: 1937 loss: 6.18055196e-07
Iter: 1938 loss: 6.17885803e-07
Iter: 1939 loss: 6.17777232e-07
Iter: 1940 loss: 6.1892581e-07
Iter: 1941 loss: 6.17771434e-07
Iter: 1942 loss: 6.17726187e-07
Iter: 1943 loss: 6.17659452e-07
Iter: 1944 loss: 6.17655928e-07
Iter: 1945 loss: 6.17551905e-07
Iter: 1946 loss: 6.17785531e-07
Iter: 1947 loss: 6.17543719e-07
Iter: 1948 loss: 6.17435774e-07
Iter: 1949 loss: 6.17672868e-07
Iter: 1950 loss: 6.17393084e-07
Iter: 1951 loss: 6.1725126e-07
Iter: 1952 loss: 6.17131263e-07
Iter: 1953 loss: 6.17123192e-07
Iter: 1954 loss: 6.17139449e-07
Iter: 1955 loss: 6.17059527e-07
Iter: 1956 loss: 6.1696278e-07
Iter: 1957 loss: 6.16822206e-07
Iter: 1958 loss: 6.19573939e-07
Iter: 1959 loss: 6.16840566e-07
Iter: 1960 loss: 6.1669607e-07
Iter: 1961 loss: 6.17178955e-07
Iter: 1962 loss: 6.16674242e-07
Iter: 1963 loss: 6.16616717e-07
Iter: 1964 loss: 6.16604098e-07
Iter: 1965 loss: 6.16532532e-07
Iter: 1966 loss: 6.1652031e-07
Iter: 1967 loss: 6.16497914e-07
Iter: 1968 loss: 6.16421403e-07
Iter: 1969 loss: 6.16593127e-07
Iter: 1970 loss: 6.16377235e-07
Iter: 1971 loss: 6.16315901e-07
Iter: 1972 loss: 6.16949e-07
Iter: 1973 loss: 6.16263947e-07
Iter: 1974 loss: 6.16263378e-07
Iter: 1975 loss: 6.16373e-07
Iter: 1976 loss: 6.16203693e-07
Iter: 1977 loss: 6.16121554e-07
Iter: 1978 loss: 6.16106945e-07
Iter: 1979 loss: 6.16062039e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2
+ date
Wed Oct 21 12:46:20 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/300_300_300_1 --function f1 --psi 0 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0ffa98510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0ffa5f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0ffaf9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d5522730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d556cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d556c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d554f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d54601e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d5460268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d54608c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d54b81e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b05359d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b053f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d54b67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b04fad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0d54cda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b04fa598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b0474488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b0474598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b040a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b0427620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b03680d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b0427f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b0370840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b03af268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02d1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02d1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02e32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02e3598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02618c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02621e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b03078c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b01b1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b02b4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b01b2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff0b0202f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.042416587
test_loss: 0.04139553
train_loss: 0.009857872
test_loss: 0.0097349025
train_loss: 0.0049662725
test_loss: 0.004941139
train_loss: 0.003279291
test_loss: 0.0037324051
train_loss: 0.0033179503
test_loss: 0.0029821475
train_loss: 0.0025646465
test_loss: 0.0024677187
train_loss: 0.0025650405
test_loss: 0.0026588333
train_loss: 0.0026821159
test_loss: 0.0026260447
train_loss: 0.0019910883
test_loss: 0.002125526
train_loss: 0.0025287576
test_loss: 0.0021311634
train_loss: 0.0021998337
test_loss: 0.0024981585
train_loss: 0.0020497264
test_loss: 0.0022053905
train_loss: 0.002224956
test_loss: 0.002001827
train_loss: 0.0019986124
test_loss: 0.0025447714
train_loss: 0.0019258435
test_loss: 0.002252581
train_loss: 0.0019413858
test_loss: 0.0028204299
train_loss: 0.00231431
test_loss: 0.0020137809
train_loss: 0.0022379986
test_loss: 0.0021666137
train_loss: 0.0020246883
test_loss: 0.0024672227
train_loss: 0.0019454774
test_loss: 0.0018006756
train_loss: 0.0016822859
test_loss: 0.0017636578
train_loss: 0.0022464423
test_loss: 0.0018668985
train_loss: 0.0022442539
test_loss: 0.0018224488
train_loss: 0.0021904928
test_loss: 0.0022407777
train_loss: 0.0024497379
test_loss: 0.002867468
train_loss: 0.0021002183
test_loss: 0.002209691
train_loss: 0.0018111864
test_loss: 0.001794736
train_loss: 0.002131275
test_loss: 0.0020525954
train_loss: 0.0018667819
test_loss: 0.0022992308
train_loss: 0.0018993014
test_loss: 0.0018459025
train_loss: 0.0023347056
test_loss: 0.0021350053
train_loss: 0.0016453192
test_loss: 0.001954487
train_loss: 0.002524483
test_loss: 0.00305972
train_loss: 0.003511337
test_loss: 0.0033306738
train_loss: 0.0031476808
test_loss: 0.0037367314
train_loss: 0.0023579902
test_loss: 0.0028138286
train_loss: 0.0035255696
test_loss: 0.0028400838
train_loss: 0.0027876692
test_loss: 0.002883296
train_loss: 0.0027308848
test_loss: 0.0025845212
train_loss: 0.0022093365
test_loss: 0.0019937688
train_loss: 0.002842882
test_loss: 0.0021719525
train_loss: 0.002557864
test_loss: 0.001961604
train_loss: 0.0019304573
test_loss: 0.0020076656
train_loss: 0.0024429602
test_loss: 0.0019148845
train_loss: 0.001976118
test_loss: 0.0021244634
train_loss: 0.0018249684
test_loss: 0.0024311054
train_loss: 0.0019384001
test_loss: 0.0018869451
train_loss: 0.0019590682
test_loss: 0.0021223908
train_loss: 0.0016904408
test_loss: 0.0020681554
train_loss: 0.0018385672
test_loss: 0.001619408
train_loss: 0.001882304
test_loss: 0.0016556571
train_loss: 0.0018127991
test_loss: 0.0018337858
train_loss: 0.0016711659
test_loss: 0.0015995186
train_loss: 0.0016056702
test_loss: 0.0019793184
train_loss: 0.0016807113
test_loss: 0.0016121479
train_loss: 0.0016639419
test_loss: 0.0017809843
train_loss: 0.002032394
test_loss: 0.0016948197
train_loss: 0.0020946627
test_loss: 0.001975542
train_loss: 0.0023679223
test_loss: 0.0021565207
train_loss: 0.0018711964
test_loss: 0.0020923843
train_loss: 0.002274768
test_loss: 0.0032546145
train_loss: 0.0024928818
test_loss: 0.0030991903
train_loss: 0.0025388238
test_loss: 0.0028872474
train_loss: 0.0019408856
test_loss: 0.00190508
train_loss: 0.0017086202
test_loss: 0.00172416
train_loss: 0.001539929
test_loss: 0.0016331119
train_loss: 0.001986675
test_loss: 0.001826091
train_loss: 0.0017989323
test_loss: 0.0020959587
train_loss: 0.0020630602
test_loss: 0.0020451369
train_loss: 0.0018071467
test_loss: 0.0019307403
train_loss: 0.0017687123
test_loss: 0.0018503086
train_loss: 0.0018221738
test_loss: 0.0016117974
train_loss: 0.001792416
test_loss: 0.0016202261
train_loss: 0.0021515307
test_loss: 0.0028006053
train_loss: 0.0020704714
test_loss: 0.002588455
train_loss: 0.0024512531
test_loss: 0.0027554776
train_loss: 0.00166821
test_loss: 0.0022275997
train_loss: 0.002285305
test_loss: 0.0017419494
train_loss: 0.0017354729
test_loss: 0.0018203848
train_loss: 0.0018384219
test_loss: 0.001637106
train_loss: 0.0021579266
test_loss: 0.0017753417
train_loss: 0.0018058242
test_loss: 0.001608822
train_loss: 0.0016712499
test_loss: 0.0016569749
train_loss: 0.0014993993
test_loss: 0.0016899062
train_loss: 0.0016751795
test_loss: 0.0016767209
train_loss: 0.0014455648
test_loss: 0.001631381
train_loss: 0.0016740888
test_loss: 0.0016937415
train_loss: 0.0015114147
test_loss: 0.001940931
train_loss: 0.0016413264
test_loss: 0.0019194079
train_loss: 0.0016314234
test_loss: 0.0016078468
train_loss: 0.0020148936
test_loss: 0.0016277005
train_loss: 0.0015432896
test_loss: 0.0017636399
train_loss: 0.0017367934
test_loss: 0.001964244
train_loss: 0.0019229583
test_loss: 0.0022081435
train_loss: 0.0018377504
test_loss: 0.0017186531
train_loss: 0.0015584256
test_loss: 0.0022541492
train_loss: 0.0016987226
test_loss: 0.0018897805
train_loss: 0.0017692207
test_loss: 0.0015876744
train_loss: 0.0020290324
test_loss: 0.0016579842
train_loss: 0.0019680103
test_loss: 0.0017542845
train_loss: 0.0015621508
test_loss: 0.0016407018
train_loss: 0.0017273037
test_loss: 0.0017953458
train_loss: 0.0014744096
test_loss: 0.0015936015
train_loss: 0.00190313
test_loss: 0.0016957741
train_loss: 0.0020204773
test_loss: 0.0015987777
train_loss: 0.0016995121
test_loss: 0.0018128024
train_loss: 0.0015620464
test_loss: 0.0021414468
train_loss: 0.0021142606
test_loss: 0.001917681
train_loss: 0.0018585408
test_loss: 0.0016993951
train_loss: 0.0016536074
test_loss: 0.0015911221
train_loss: 0.0020311621
test_loss: 0.0027038301
train_loss: 0.0019467036
test_loss: 0.002330311
train_loss: 0.0017962494
test_loss: 0.0018732857
train_loss: 0.0018897703
test_loss: 0.0018309602
train_loss: 0.0016266702
test_loss: 0.0019270759
train_loss: 0.0017679178
test_loss: 0.0015518677
train_loss: 0.0016396301
test_loss: 0.0017978624
train_loss: 0.0017756318
test_loss: 0.0020541584
train_loss: 0.001649586
test_loss: 0.0016750112
train_loss: 0.0019527819
test_loss: 0.0016012599
train_loss: 0.0015959012
test_loss: 0.0018556577
train_loss: 0.0022842502
test_loss: 0.0021830173
train_loss: 0.0018358813
test_loss: 0.001828471
train_loss: 0.0017152916
test_loss: 0.0019854226
train_loss: 0.001590343
test_loss: 0.0016098463
train_loss: 0.0018942216
test_loss: 0.0017149257
train_loss: 0.0016422725
test_loss: 0.0014002577
train_loss: 0.0016586324
test_loss: 0.0018785825
train_loss: 0.0022253455
test_loss: 0.0018725408
train_loss: 0.0016363865
test_loss: 0.0017698314
train_loss: 0.0021541663
test_loss: 0.0022218833
train_loss: 0.0022682333
test_loss: 0.0019263191
train_loss: 0.0017930872
test_loss: 0.0017779449
train_loss: 0.0017280998
test_loss: 0.0018828505
train_loss: 0.00207919
test_loss: 0.0019665656
train_loss: 0.001683199
test_loss: 0.001711281
train_loss: 0.0016076285
test_loss: 0.00179889
train_loss: 0.0016747594
test_loss: 0.0018168916
train_loss: 0.0022489221
test_loss: 0.0018219004
train_loss: 0.0017814899
test_loss: 0.0015374797
train_loss: 0.0018250127
test_loss: 0.0017295663
train_loss: 0.0022874193
test_loss: 0.0017969515
train_loss: 0.0016326325
test_loss: 0.0020652192
train_loss: 0.0016510999
test_loss: 0.0019866365
train_loss: 0.0020883926
test_loss: 0.0021392063
train_loss: 0.0032361876
test_loss: 0.0024249316
train_loss: 0.0028076589
test_loss: 0.0028565428
train_loss: 0.0026675418
test_loss: 0.0020266015
train_loss: 0.0018748095
test_loss: 0.0016664715
train_loss: 0.001613192
test_loss: 0.0016863288
train_loss: 0.0018933157
test_loss: 0.0017176765
train_loss: 0.0017073681
test_loss: 0.002309637
train_loss: 0.001611344
test_loss: 0.0019371123
train_loss: 0.001970003
test_loss: 0.0017136955
train_loss: 0.0016637282
test_loss: 0.0016873025
train_loss: 0.0015843962
test_loss: 0.0022258451
train_loss: 0.0020086227
test_loss: 0.0019262781
train_loss: 0.0015414294
test_loss: 0.0017540423
train_loss: 0.0016030432
test_loss: 0.0018808215
train_loss: 0.0015427447
test_loss: 0.0021475789
train_loss: 0.001812752
test_loss: 0.0019856317
train_loss: 0.0020501665
test_loss: 0.0024228077
train_loss: 0.0017230292
test_loss: 0.0017129299
train_loss: 0.0018010663
test_loss: 0.001393917
train_loss: 0.0015231608
test_loss: 0.0016031591
train_loss: 0.0017237333
test_loss: 0.0016870141
train_loss: 0.0018427407
test_loss: 0.0017763523
train_loss: 0.001707121
test_loss: 0.0023144914
train_loss: 0.0017940252
test_loss: 0.0016094762
train_loss: 0.0016109315
test_loss: 0.0020886252
train_loss: 0.0016411376
test_loss: 0.0018124237
train_loss: 0.0019333267
test_loss: 0.0019556626
train_loss: 0.0017975235
test_loss: 0.0016159329
train_loss: 0.0018503019
test_loss: 0.0017475716
train_loss: 0.0019398439
test_loss: 0.0021006984
train_loss: 0.0019830863
test_loss: 0.0015874417
train_loss: 0.001707475
test_loss: 0.0019592673
train_loss: 0.0017898488
test_loss: 0.0017107584
train_loss: 0.0019111056
test_loss: 0.0017273573
train_loss: 0.0014875765
test_loss: 0.0015465098
train_loss: 0.001835969
test_loss: 0.0021848832
train_loss: 0.0015174071
test_loss: 0.0016456885
train_loss: 0.001607694
test_loss: 0.002003151
train_loss: 0.0022411332
test_loss: 0.001574695
train_loss: 0.0016676446
test_loss: 0.0020327587
train_loss: 0.0017883439
test_loss: 0.0017311798
train_loss: 0.0017604752
test_loss: 0.0015600373
train_loss: 0.0016846218
test_loss: 0.0015226122
train_loss: 0.0016915158
test_loss: 0.0017945942
train_loss: 0.0016442479
test_loss: 0.0020207672
train_loss: 0.0019931353
test_loss: 0.0022773312
train_loss: 0.0026926906
test_loss: 0.001980295
train_loss: 0.00151742
test_loss: 0.0019190897
train_loss: 0.0017986486
test_loss: 0.0018792183
train_loss: 0.0025638822
test_loss: 0.0024571575
train_loss: 0.0017988625
test_loss: 0.0018002127
train_loss: 0.0017217057
test_loss: 0.001769533
train_loss: 0.0014243971
test_loss: 0.0015370337
train_loss: 0.0021420363
test_loss: 0.0017960343
train_loss: 0.0020291244
test_loss: 0.0018440115
train_loss: 0.0017206063
test_loss: 0.0016481085
train_loss: 0.0019494066
test_loss: 0.0017712212
train_loss: 0.0015420428
test_loss: 0.002198478
train_loss: 0.001987359
test_loss: 0.002012977
train_loss: 0.0019404148
test_loss: 0.0017896818
train_loss: 0.0015741198
test_loss: 0.0017450936
train_loss: 0.0018933095
test_loss: 0.0015688736
train_loss: 0.0018372175
test_loss: 0.0018685469
train_loss: 0.0021992442
test_loss: 0.0019560012
train_loss: 0.001779599
test_loss: 0.0017606254
train_loss: 0.0016947523
test_loss: 0.0017264847
train_loss: 0.001735568
test_loss: 0.0017616468
train_loss: 0.001682224
test_loss: 0.0018771357
train_loss: 0.0015511088
test_loss: 0.0013425712
train_loss: 0.0016881744
test_loss: 0.0016510908
train_loss: 0.0018446405
test_loss: 0.0015599336
train_loss: 0.0016445431
test_loss: 0.0016509419
train_loss: 0.0023160635
test_loss: 0.0018279812
train_loss: 0.0020026248
test_loss: 0.0018657332
train_loss: 0.0015789121
test_loss: 0.0017983424
train_loss: 0.0021070784
test_loss: 0.0021401178
train_loss: 0.0019746122
test_loss: 0.0022668268
train_loss: 0.0018765132
test_loss: 0.0018786844
train_loss: 0.0019800044
test_loss: 0.0018201419
train_loss: 0.0015015288
test_loss: 0.0014933443
train_loss: 0.0014914947
test_loss: 0.0014706483
train_loss: 0.0021939273
test_loss: 0.0018224878
train_loss: 0.0021066593
test_loss: 0.0025283182
train_loss: 0.0033423891
test_loss: 0.0026501692
train_loss: 0.0027152533
test_loss: 0.002815552
train_loss: 0.002603379
test_loss: 0.002109779
train_loss: 0.002706467
test_loss: 0.002698803
train_loss: 0.0026177242
test_loss: 0.0023334087
train_loss: 0.002599004
test_loss: 0.0024283575
train_loss: 0.0029184746
test_loss: 0.00274131
train_loss: 0.0025665509
test_loss: 0.002268018
train_loss: 0.002033954
test_loss: 0.0019314907
train_loss: 0.0016357402
test_loss: 0.001568041
train_loss: 0.0017455053
test_loss: 0.0020377832
train_loss: 0.001974842
test_loss: 0.001966537
train_loss: 0.0014623222
test_loss: 0.0021096359
train_loss: 0.0015492297
test_loss: 0.0020111115
train_loss: 0.0018896353
test_loss: 0.00185218
train_loss: 0.0014828641
test_loss: 0.002009786
train_loss: 0.0018642681
test_loss: 0.0019350682
train_loss: 0.0015392584
test_loss: 0.0015231316
train_loss: 0.0017151337
test_loss: 0.0016711652
train_loss: 0.0016291611
test_loss: 0.0015708993
train_loss: 0.0015417045
test_loss: 0.0015280126
train_loss: 0.0022749256
test_loss: 0.0017113058
train_loss: 0.0017456771
test_loss: 0.0027918345
train_loss: 0.002258473
test_loss: 0.0029745523
train_loss: 0.002263705
test_loss: 0.0025217717
train_loss: 0.0020649887
test_loss: 0.0018095337
train_loss: 0.0017527726
test_loss: 0.0022643292
train_loss: 0.0015812247
test_loss: 0.0019156131
train_loss: 0.0014904486
test_loss: 0.0016946627
train_loss: 0.0016319198
test_loss: 0.001448229
train_loss: 0.001844185
test_loss: 0.0017089456
train_loss: 0.0014647559
test_loss: 0.001854929
train_loss: 0.0016302865
test_loss: 0.0016796803
train_loss: 0.0016866399
test_loss: 0.0015797247
train_loss: 0.0017354263
test_loss: 0.0014930324
train_loss: 0.0014305932
test_loss: 0.0015768565
train_loss: 0.0016703774
test_loss: 0.0014094976
train_loss: 0.0014592662
test_loss: 0.0014212534
train_loss: 0.001728687
test_loss: 0.001636417
train_loss: 0.0016394503
test_loss: 0.001585615
train_loss: 0.002028092
test_loss: 0.001963207
train_loss: 0.0017147668
test_loss: 0.0018228355
train_loss: 0.001600907
test_loss: 0.0015807057
train_loss: 0.0017604949
test_loss: 0.0016741292
train_loss: 0.0015606701
test_loss: 0.0015693636
train_loss: 0.0018455576
test_loss: 0.0020203996
train_loss: 0.0020166906
test_loss: 0.0015183011
train_loss: 0.0016621557
test_loss: 0.0016285357
train_loss: 0.0017959625
test_loss: 0.0016377433
train_loss: 0.0015679286
test_loss: 0.0017075461
train_loss: 0.0013912593
test_loss: 0.0019233808
train_loss: 0.0018569897
test_loss: 0.0016960148
train_loss: 0.0014169803
test_loss: 0.0019337704
train_loss: 0.0018016418
test_loss: 0.001504231
train_loss: 0.0016709114
test_loss: 0.0015973654
train_loss: 0.0018798113
test_loss: 0.0020624513
train_loss: 0.0021419507
test_loss: 0.0018134452
train_loss: 0.001578904
test_loss: 0.0016081639
train_loss: 0.0015557689
test_loss: 0.0018169621
train_loss: 0.0017185452
test_loss: 0.001804663
train_loss: 0.0017948647
test_loss: 0.0017503244
train_loss: 0.0018682433
test_loss: 0.0015671909
train_loss: 0.0015899302
test_loss: 0.0018598143
train_loss: 0.0017360011
test_loss: 0.0016722707
train_loss: 0.0018938559
test_loss: 0.002098133
train_loss: 0.0014509865
test_loss: 0.0014422515
train_loss: 0.0018996892
test_loss: 0.0019311474
train_loss: 0.001508211
test_loss: 0.0016078786
train_loss: 0.0016239912
test_loss: 0.0013404964
train_loss: 0.0021609685
test_loss: 0.0015571862
train_loss: 0.0018488895
test_loss: 0.0018229483
train_loss: 0.0015386741
test_loss: 0.0016178649
train_loss: 0.0015653007
test_loss: 0.0016788129
train_loss: 0.0015952194
test_loss: 0.001689347
train_loss: 0.0019412363
test_loss: 0.0017410871
train_loss: 0.0021698491
test_loss: 0.0020270494
train_loss: 0.0021193514
test_loss: 0.0017769915
train_loss: 0.0018700949
test_loss: 0.0019114334
train_loss: 0.0020309745
test_loss: 0.0015554017
train_loss: 0.0019017162
test_loss: 0.0015795765
train_loss: 0.0021716985
test_loss: 0.0031848517
train_loss: 0.0024239025
test_loss: 0.0029914416
train_loss: 0.002330747
test_loss: 0.0022182357
train_loss: 0.001574032
test_loss: 0.0016391019
train_loss: 0.0015702824
test_loss: 0.0016732543
train_loss: 0.0016759804
test_loss: 0.0018310489
train_loss: 0.0017732417
test_loss: 0.0019100732
train_loss: 0.0016609055
test_loss: 0.0015965183
train_loss: 0.0017775988
test_loss: 0.0017048283
train_loss: 0.0016567857
test_loss: 0.0017433582
train_loss: 0.0017702953
test_loss: 0.0017357005
train_loss: 0.0014990554
test_loss: 0.0016388926
train_loss: 0.0016427597
test_loss: 0.0016435529
train_loss: 0.0018770484
test_loss: 0.0017719954
train_loss: 0.0015721954
test_loss: 0.001894413
train_loss: 0.002015405
test_loss: 0.0015027678
train_loss: 0.0014281073
test_loss: 0.0021483945
train_loss: 0.0015951224
test_loss: 0.0021121744
train_loss: 0.0021858066
test_loss: 0.0018336688
train_loss: 0.0019288564
test_loss: 0.001910683
train_loss: 0.0015607106
test_loss: 0.0016758998
train_loss: 0.0016142485
test_loss: 0.002537625
train_loss: 0.0018842551
test_loss: 0.0016486754
train_loss: 0.0015981109
test_loss: 0.0014156477
train_loss: 0.0017939374
test_loss: 0.001720991
train_loss: 0.0016633826
test_loss: 0.001358864
train_loss: 0.0016321559
test_loss: 0.0018232468
train_loss: 0.0016861092
test_loss: 0.0017893167
train_loss: 0.0015744702
test_loss: 0.0015850618
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0016896459
test_loss: 0.0017861909
train_loss: 0.0017326658
test_loss: 0.0021512476
train_loss: 0.0017774376
test_loss: 0.0015381953
train_loss: 0.0021076333
test_loss: 0.0017600888
train_loss: 0.0017391146
test_loss: 0.001492671
train_loss: 0.0016680597
test_loss: 0.0019616534
train_loss: 0.0019107346
test_loss: 0.0018579811
train_loss: 0.0017055764
test_loss: 0.0019698488
train_loss: 0.0018884059
test_loss: 0.0016008577
train_loss: 0.00219973
test_loss: 0.0018101488
train_loss: 0.0018056817
test_loss: 0.0016982694
train_loss: 0.0018181573
test_loss: 0.0014264275
train_loss: 0.0019447352
test_loss: 0.0017524667
train_loss: 0.0017145805
test_loss: 0.0018091346
train_loss: 0.0017530745
test_loss: 0.0020380658
train_loss: 0.0015640282
test_loss: 0.0017497198
train_loss: 0.001972603
test_loss: 0.0018840682
train_loss: 0.0016451849
test_loss: 0.0019779496
train_loss: 0.0018519696
test_loss: 0.0015310978
train_loss: 0.001470523
test_loss: 0.0013474916
train_loss: 0.0014531303
test_loss: 0.0014655659
train_loss: 0.0019722558
test_loss: 0.0016169827
train_loss: 0.0015551466
test_loss: 0.0016954447
train_loss: 0.0015379798
test_loss: 0.0018317521
train_loss: 0.0016312205
test_loss: 0.0017554375
train_loss: 0.0017530009
test_loss: 0.0017638106
train_loss: 0.0022684284
test_loss: 0.001740784
train_loss: 0.002200381
test_loss: 0.0017194113
train_loss: 0.0015658842
test_loss: 0.001564163
train_loss: 0.0014822453
test_loss: 0.0017933449
train_loss: 0.0015728052
test_loss: 0.0014976531
train_loss: 0.001652946
test_loss: 0.0014415368
train_loss: 0.0014704783
test_loss: 0.0017149122
train_loss: 0.0018942968
test_loss: 0.0018115971
train_loss: 0.0017146635
test_loss: 0.0019805592
train_loss: 0.00187825
test_loss: 0.0019454226
train_loss: 0.0018096899
test_loss: 0.0023496172
train_loss: 0.0015497794
test_loss: 0.002546013
train_loss: 0.0026236402
test_loss: 0.0017891711
train_loss: 0.001813586
test_loss: 0.0016461458
train_loss: 0.0016674268
test_loss: 0.0016672125
train_loss: 0.001486981
test_loss: 0.0022213743
train_loss: 0.0014753921
test_loss: 0.0016014549
train_loss: 0.0019661235
test_loss: 0.0015414532
train_loss: 0.0020347526
test_loss: 0.0015739726
train_loss: 0.0018414294
test_loss: 0.0018158897
train_loss: 0.0018493646
test_loss: 0.0016683736
train_loss: 0.001533155
test_loss: 0.0016000683
train_loss: 0.0017527041
test_loss: 0.001711548
train_loss: 0.0016850457
test_loss: 0.0019482052
train_loss: 0.0016518703
test_loss: 0.0021793768
train_loss: 0.0017433626
test_loss: 0.0015948776
train_loss: 0.0018691504
test_loss: 0.0018686998
train_loss: 0.0014951314
test_loss: 0.0019248206
train_loss: 0.0013862328
test_loss: 0.001822174
train_loss: 0.0014703576
test_loss: 0.0014195086
train_loss: 0.001576788
test_loss: 0.0017453361
train_loss: 0.0016113096
test_loss: 0.0015723238
train_loss: 0.0013901311
test_loss: 0.0018117665
train_loss: 0.0015818065
test_loss: 0.0014524597
train_loss: 0.0016293952
test_loss: 0.0019470504
train_loss: 0.0017855421
test_loss: 0.00147885
train_loss: 0.00175542
test_loss: 0.0020337384
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104607488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104621620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21045f37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104696730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21046962f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21045522f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21044fdae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21044c96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21044c92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210448af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210448a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210444dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210445bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104410158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21043bbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21043ef950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21044a99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104493268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21044937b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104303730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210431c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210432ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210431cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21042eb7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21042aa488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21042b7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21042b77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104271268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21042677b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21041ba400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f21041ee158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104191f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104141620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f210418d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104170ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2104124ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.17827345e-06
Iter: 2 loss: 1.77248021e-05
Iter: 3 loss: 3.55970133e-06
Iter: 4 loss: 2.72410352e-06
Iter: 5 loss: 3.05278468e-06
Iter: 6 loss: 2.14603369e-06
Iter: 7 loss: 1.73092508e-06
Iter: 8 loss: 4.63419519e-06
Iter: 9 loss: 1.69269413e-06
Iter: 10 loss: 1.47459446e-06
Iter: 11 loss: 1.71476722e-06
Iter: 12 loss: 1.3560383e-06
Iter: 13 loss: 1.22775543e-06
Iter: 14 loss: 2.69888e-06
Iter: 15 loss: 1.22559e-06
Iter: 16 loss: 1.17145782e-06
Iter: 17 loss: 1.42222984e-06
Iter: 18 loss: 1.16135e-06
Iter: 19 loss: 1.11586598e-06
Iter: 20 loss: 1.10981432e-06
Iter: 21 loss: 1.07758808e-06
Iter: 22 loss: 1.04641697e-06
Iter: 23 loss: 1.04605851e-06
Iter: 24 loss: 1.02550757e-06
Iter: 25 loss: 1.01184173e-06
Iter: 26 loss: 1.00405305e-06
Iter: 27 loss: 9.84174903e-07
Iter: 28 loss: 1.0958538e-06
Iter: 29 loss: 9.8141e-07
Iter: 30 loss: 9.66862444e-07
Iter: 31 loss: 9.92395144e-07
Iter: 32 loss: 9.60422199e-07
Iter: 33 loss: 9.45689294e-07
Iter: 34 loss: 9.79569677e-07
Iter: 35 loss: 9.40183781e-07
Iter: 36 loss: 9.4261128e-07
Iter: 37 loss: 9.33992283e-07
Iter: 38 loss: 9.28177769e-07
Iter: 39 loss: 9.15646183e-07
Iter: 40 loss: 1.10933638e-06
Iter: 41 loss: 9.15210251e-07
Iter: 42 loss: 9.10302049e-07
Iter: 43 loss: 9.10184326e-07
Iter: 44 loss: 9.06023274e-07
Iter: 45 loss: 9.01076476e-07
Iter: 46 loss: 9.00629516e-07
Iter: 47 loss: 8.93600145e-07
Iter: 48 loss: 9.25604752e-07
Iter: 49 loss: 8.9225216e-07
Iter: 50 loss: 8.87392105e-07
Iter: 51 loss: 9.25587642e-07
Iter: 52 loss: 8.87081512e-07
Iter: 53 loss: 8.83193479e-07
Iter: 54 loss: 8.79058e-07
Iter: 55 loss: 8.78403114e-07
Iter: 56 loss: 8.71093e-07
Iter: 57 loss: 9.11481948e-07
Iter: 58 loss: 8.70062252e-07
Iter: 59 loss: 8.65461118e-07
Iter: 60 loss: 8.83142832e-07
Iter: 61 loss: 8.64364893e-07
Iter: 62 loss: 8.59374552e-07
Iter: 63 loss: 8.57482178e-07
Iter: 64 loss: 8.54849077e-07
Iter: 65 loss: 8.48577145e-07
Iter: 66 loss: 8.74589e-07
Iter: 67 loss: 8.47250249e-07
Iter: 68 loss: 8.4197967e-07
Iter: 69 loss: 8.40741563e-07
Iter: 70 loss: 8.37392747e-07
Iter: 71 loss: 8.45942907e-07
Iter: 72 loss: 8.35872e-07
Iter: 73 loss: 8.34303e-07
Iter: 74 loss: 8.30558861e-07
Iter: 75 loss: 8.73859619e-07
Iter: 76 loss: 8.30196711e-07
Iter: 77 loss: 8.2669635e-07
Iter: 78 loss: 8.37954076e-07
Iter: 79 loss: 8.25752579e-07
Iter: 80 loss: 8.22986522e-07
Iter: 81 loss: 8.59727095e-07
Iter: 82 loss: 8.22950824e-07
Iter: 83 loss: 8.21353296e-07
Iter: 84 loss: 8.17874081e-07
Iter: 85 loss: 8.71043142e-07
Iter: 86 loss: 8.17739647e-07
Iter: 87 loss: 8.14783334e-07
Iter: 88 loss: 8.14675161e-07
Iter: 89 loss: 8.12544727e-07
Iter: 90 loss: 8.10387291e-07
Iter: 91 loss: 8.1001167e-07
Iter: 92 loss: 8.06092942e-07
Iter: 93 loss: 8.22813206e-07
Iter: 94 loss: 8.05283889e-07
Iter: 95 loss: 8.02962404e-07
Iter: 96 loss: 8.21692765e-07
Iter: 97 loss: 8.02850309e-07
Iter: 98 loss: 8.01067529e-07
Iter: 99 loss: 7.99263e-07
Iter: 100 loss: 7.98901851e-07
Iter: 101 loss: 7.95794278e-07
Iter: 102 loss: 8.14349846e-07
Iter: 103 loss: 7.95432527e-07
Iter: 104 loss: 7.93128947e-07
Iter: 105 loss: 7.90889317e-07
Iter: 106 loss: 7.90447928e-07
Iter: 107 loss: 7.88307716e-07
Iter: 108 loss: 7.8821995e-07
Iter: 109 loss: 7.86655278e-07
Iter: 110 loss: 7.866999e-07
Iter: 111 loss: 7.85993961e-07
Iter: 112 loss: 7.84023428e-07
Iter: 113 loss: 7.91552281e-07
Iter: 114 loss: 7.83190558e-07
Iter: 115 loss: 7.81425967e-07
Iter: 116 loss: 7.9938286e-07
Iter: 117 loss: 7.81376229e-07
Iter: 118 loss: 7.80081e-07
Iter: 119 loss: 7.9575409e-07
Iter: 120 loss: 7.80099072e-07
Iter: 121 loss: 7.79042e-07
Iter: 122 loss: 7.76909e-07
Iter: 123 loss: 8.1664848e-07
Iter: 124 loss: 7.76858883e-07
Iter: 125 loss: 7.74929731e-07
Iter: 126 loss: 7.84314409e-07
Iter: 127 loss: 7.74562068e-07
Iter: 128 loss: 7.72369276e-07
Iter: 129 loss: 7.83408154e-07
Iter: 130 loss: 7.72001158e-07
Iter: 131 loss: 7.70507711e-07
Iter: 132 loss: 7.70881343e-07
Iter: 133 loss: 7.69389828e-07
Iter: 134 loss: 7.67800771e-07
Iter: 135 loss: 7.72701128e-07
Iter: 136 loss: 7.67306517e-07
Iter: 137 loss: 7.66044934e-07
Iter: 138 loss: 7.76743889e-07
Iter: 139 loss: 7.66041353e-07
Iter: 140 loss: 7.651185e-07
Iter: 141 loss: 7.65589334e-07
Iter: 142 loss: 7.6445906e-07
Iter: 143 loss: 7.63474873e-07
Iter: 144 loss: 7.76385548e-07
Iter: 145 loss: 7.63506364e-07
Iter: 146 loss: 7.63074581e-07
Iter: 147 loss: 7.63046557e-07
Iter: 148 loss: 7.62688842e-07
Iter: 149 loss: 7.61794638e-07
Iter: 150 loss: 7.6957582e-07
Iter: 151 loss: 7.61604269e-07
Iter: 152 loss: 7.60407318e-07
Iter: 153 loss: 7.60938178e-07
Iter: 154 loss: 7.59581269e-07
Iter: 155 loss: 7.58438659e-07
Iter: 156 loss: 7.59796535e-07
Iter: 157 loss: 7.57876194e-07
Iter: 158 loss: 7.56132749e-07
Iter: 159 loss: 7.61871377e-07
Iter: 160 loss: 7.55655037e-07
Iter: 161 loss: 7.55474048e-07
Iter: 162 loss: 7.55093367e-07
Iter: 163 loss: 7.54627649e-07
Iter: 164 loss: 7.53358393e-07
Iter: 165 loss: 7.59463433e-07
Iter: 166 loss: 7.52891196e-07
Iter: 167 loss: 7.52222945e-07
Iter: 168 loss: 7.52097208e-07
Iter: 169 loss: 7.51318169e-07
Iter: 170 loss: 7.50966e-07
Iter: 171 loss: 7.50535605e-07
Iter: 172 loss: 7.49250148e-07
Iter: 173 loss: 7.49333253e-07
Iter: 174 loss: 7.48208606e-07
Iter: 175 loss: 7.47219474e-07
Iter: 176 loss: 7.60363832e-07
Iter: 177 loss: 7.47200488e-07
Iter: 178 loss: 7.46321803e-07
Iter: 179 loss: 7.4682066e-07
Iter: 180 loss: 7.45781e-07
Iter: 181 loss: 7.44819317e-07
Iter: 182 loss: 7.55365249e-07
Iter: 183 loss: 7.44803515e-07
Iter: 184 loss: 7.44210297e-07
Iter: 185 loss: 7.5281082e-07
Iter: 186 loss: 7.44215185e-07
Iter: 187 loss: 7.43880037e-07
Iter: 188 loss: 7.43280452e-07
Iter: 189 loss: 7.59128454e-07
Iter: 190 loss: 7.43286478e-07
Iter: 191 loss: 7.42501243e-07
Iter: 192 loss: 7.41876079e-07
Iter: 193 loss: 7.41736358e-07
Iter: 194 loss: 7.40759504e-07
Iter: 195 loss: 7.42603675e-07
Iter: 196 loss: 7.40333689e-07
Iter: 197 loss: 7.3952765e-07
Iter: 198 loss: 7.52127391e-07
Iter: 199 loss: 7.39493146e-07
Iter: 200 loss: 7.38908625e-07
Iter: 201 loss: 7.43756289e-07
Iter: 202 loss: 7.3889197e-07
Iter: 203 loss: 7.38388678e-07
Iter: 204 loss: 7.37322e-07
Iter: 205 loss: 7.61556407e-07
Iter: 206 loss: 7.37312234e-07
Iter: 207 loss: 7.36562868e-07
Iter: 208 loss: 7.42338e-07
Iter: 209 loss: 7.36484822e-07
Iter: 210 loss: 7.3559886e-07
Iter: 211 loss: 7.37356e-07
Iter: 212 loss: 7.35256549e-07
Iter: 213 loss: 7.34595915e-07
Iter: 214 loss: 7.35338745e-07
Iter: 215 loss: 7.3431903e-07
Iter: 216 loss: 7.33366278e-07
Iter: 217 loss: 7.3273452e-07
Iter: 218 loss: 7.32428191e-07
Iter: 219 loss: 7.32049443e-07
Iter: 220 loss: 7.31810303e-07
Iter: 221 loss: 7.31460545e-07
Iter: 222 loss: 7.3521619e-07
Iter: 223 loss: 7.31444e-07
Iter: 224 loss: 7.31110674e-07
Iter: 225 loss: 7.30529337e-07
Iter: 226 loss: 7.30531724e-07
Iter: 227 loss: 7.29916621e-07
Iter: 228 loss: 7.30688271e-07
Iter: 229 loss: 7.29579597e-07
Iter: 230 loss: 7.28842792e-07
Iter: 231 loss: 7.28719158e-07
Iter: 232 loss: 7.28230361e-07
Iter: 233 loss: 7.27118618e-07
Iter: 234 loss: 7.29730232e-07
Iter: 235 loss: 7.26703e-07
Iter: 236 loss: 7.25961172e-07
Iter: 237 loss: 7.34010882e-07
Iter: 238 loss: 7.25930704e-07
Iter: 239 loss: 7.25352947e-07
Iter: 240 loss: 7.30271211e-07
Iter: 241 loss: 7.25270809e-07
Iter: 242 loss: 7.2492e-07
Iter: 243 loss: 7.24177426e-07
Iter: 244 loss: 7.41120857e-07
Iter: 245 loss: 7.24193e-07
Iter: 246 loss: 7.23530547e-07
Iter: 247 loss: 7.2760588e-07
Iter: 248 loss: 7.23433e-07
Iter: 249 loss: 7.22733887e-07
Iter: 250 loss: 7.27219174e-07
Iter: 251 loss: 7.22619234e-07
Iter: 252 loss: 7.22207403e-07
Iter: 253 loss: 7.21492597e-07
Iter: 254 loss: 7.38141694e-07
Iter: 255 loss: 7.21520109e-07
Iter: 256 loss: 7.20828211e-07
Iter: 257 loss: 7.29963745e-07
Iter: 258 loss: 7.20832531e-07
Iter: 259 loss: 7.20722255e-07
Iter: 260 loss: 7.20621756e-07
Iter: 261 loss: 7.20447417e-07
Iter: 262 loss: 7.1994674e-07
Iter: 263 loss: 7.24735571e-07
Iter: 264 loss: 7.19917921e-07
Iter: 265 loss: 7.19267689e-07
Iter: 266 loss: 7.20966852e-07
Iter: 267 loss: 7.19061745e-07
Iter: 268 loss: 7.18644e-07
Iter: 269 loss: 7.18850401e-07
Iter: 270 loss: 7.18372405e-07
Iter: 271 loss: 7.17674823e-07
Iter: 272 loss: 7.18486831e-07
Iter: 273 loss: 7.17343937e-07
Iter: 274 loss: 7.16704676e-07
Iter: 275 loss: 7.20541379e-07
Iter: 276 loss: 7.16628506e-07
Iter: 277 loss: 7.16148122e-07
Iter: 278 loss: 7.15731062e-07
Iter: 279 loss: 7.15596116e-07
Iter: 280 loss: 7.15131421e-07
Iter: 281 loss: 7.15122951e-07
Iter: 282 loss: 7.14669e-07
Iter: 283 loss: 7.15402962e-07
Iter: 284 loss: 7.14431e-07
Iter: 285 loss: 7.14013e-07
Iter: 286 loss: 7.13568511e-07
Iter: 287 loss: 7.13465056e-07
Iter: 288 loss: 7.12982398e-07
Iter: 289 loss: 7.14058046e-07
Iter: 290 loss: 7.12768383e-07
Iter: 291 loss: 7.12146516e-07
Iter: 292 loss: 7.14870964e-07
Iter: 293 loss: 7.12053293e-07
Iter: 294 loss: 7.12093481e-07
Iter: 295 loss: 7.11847179e-07
Iter: 296 loss: 7.11657435e-07
Iter: 297 loss: 7.11277949e-07
Iter: 298 loss: 7.13400198e-07
Iter: 299 loss: 7.11102757e-07
Iter: 300 loss: 7.10618224e-07
Iter: 301 loss: 7.15254487e-07
Iter: 302 loss: 7.10552513e-07
Iter: 303 loss: 7.10210486e-07
Iter: 304 loss: 7.10511642e-07
Iter: 305 loss: 7.10042e-07
Iter: 306 loss: 7.09642904e-07
Iter: 307 loss: 7.11883445e-07
Iter: 308 loss: 7.09613971e-07
Iter: 309 loss: 7.093048e-07
Iter: 310 loss: 7.09608e-07
Iter: 311 loss: 7.09082883e-07
Iter: 312 loss: 7.08788207e-07
Iter: 313 loss: 7.09547919e-07
Iter: 314 loss: 7.08649168e-07
Iter: 315 loss: 7.08301513e-07
Iter: 316 loss: 7.08358471e-07
Iter: 317 loss: 7.07990921e-07
Iter: 318 loss: 7.07596314e-07
Iter: 319 loss: 7.10999643e-07
Iter: 320 loss: 7.07626214e-07
Iter: 321 loss: 7.07241952e-07
Iter: 322 loss: 7.08559867e-07
Iter: 323 loss: 7.07136678e-07
Iter: 324 loss: 7.06906349e-07
Iter: 325 loss: 7.06911351e-07
Iter: 326 loss: 7.06682954e-07
Iter: 327 loss: 7.06313756e-07
Iter: 328 loss: 7.05676428e-07
Iter: 329 loss: 7.05690184e-07
Iter: 330 loss: 7.05932564e-07
Iter: 331 loss: 7.05415459e-07
Iter: 332 loss: 7.05147897e-07
Iter: 333 loss: 7.05286311e-07
Iter: 334 loss: 7.04947183e-07
Iter: 335 loss: 7.0466092e-07
Iter: 336 loss: 7.04087597e-07
Iter: 337 loss: 7.10655115e-07
Iter: 338 loss: 7.04016e-07
Iter: 339 loss: 7.03552246e-07
Iter: 340 loss: 7.05453e-07
Iter: 341 loss: 7.03390072e-07
Iter: 342 loss: 7.02931402e-07
Iter: 343 loss: 7.06809828e-07
Iter: 344 loss: 7.02885472e-07
Iter: 345 loss: 7.02458522e-07
Iter: 346 loss: 7.05551201e-07
Iter: 347 loss: 7.02382067e-07
Iter: 348 loss: 7.02198463e-07
Iter: 349 loss: 7.01945169e-07
Iter: 350 loss: 7.01898045e-07
Iter: 351 loss: 7.01529075e-07
Iter: 352 loss: 7.04435536e-07
Iter: 353 loss: 7.01546242e-07
Iter: 354 loss: 7.01185513e-07
Iter: 355 loss: 7.01854049e-07
Iter: 356 loss: 7.01100305e-07
Iter: 357 loss: 7.00787837e-07
Iter: 358 loss: 7.01810336e-07
Iter: 359 loss: 7.00683529e-07
Iter: 360 loss: 7.00357759e-07
Iter: 361 loss: 7.0166459e-07
Iter: 362 loss: 7.00344913e-07
Iter: 363 loss: 7.00086616e-07
Iter: 364 loss: 7.0119205e-07
Iter: 365 loss: 7.00018063e-07
Iter: 366 loss: 6.99780344e-07
Iter: 367 loss: 6.99803195e-07
Iter: 368 loss: 6.99604e-07
Iter: 369 loss: 6.99528869e-07
Iter: 370 loss: 6.99479529e-07
Iter: 371 loss: 6.99338443e-07
Iter: 372 loss: 6.99010343e-07
Iter: 373 loss: 7.01363092e-07
Iter: 374 loss: 6.98950885e-07
Iter: 375 loss: 6.9870913e-07
Iter: 376 loss: 6.98869087e-07
Iter: 377 loss: 6.98549e-07
Iter: 378 loss: 6.98205326e-07
Iter: 379 loss: 6.99124428e-07
Iter: 380 loss: 6.98047188e-07
Iter: 381 loss: 6.97799749e-07
Iter: 382 loss: 7.02039131e-07
Iter: 383 loss: 6.9777218e-07
Iter: 384 loss: 6.97524229e-07
Iter: 385 loss: 6.9731e-07
Iter: 386 loss: 6.97261498e-07
Iter: 387 loss: 6.96901623e-07
Iter: 388 loss: 6.9762973e-07
Iter: 389 loss: 6.96751897e-07
Iter: 390 loss: 6.96291409e-07
Iter: 391 loss: 6.98030135e-07
Iter: 392 loss: 6.9624565e-07
Iter: 393 loss: 6.95802669e-07
Iter: 394 loss: 6.96890936e-07
Iter: 395 loss: 6.95725419e-07
Iter: 396 loss: 6.95390725e-07
Iter: 397 loss: 6.97277528e-07
Iter: 398 loss: 6.95346841e-07
Iter: 399 loss: 6.95048243e-07
Iter: 400 loss: 6.95893732e-07
Iter: 401 loss: 6.94978496e-07
Iter: 402 loss: 6.94693881e-07
Iter: 403 loss: 6.95029769e-07
Iter: 404 loss: 6.94537732e-07
Iter: 405 loss: 6.94409209e-07
Iter: 406 loss: 6.94403866e-07
Iter: 407 loss: 6.94278e-07
Iter: 408 loss: 6.94062e-07
Iter: 409 loss: 6.94056098e-07
Iter: 410 loss: 6.93841514e-07
Iter: 411 loss: 6.93564402e-07
Iter: 412 loss: 6.93500624e-07
Iter: 413 loss: 6.93236586e-07
Iter: 414 loss: 6.93450488e-07
Iter: 415 loss: 6.93061565e-07
Iter: 416 loss: 6.92609092e-07
Iter: 417 loss: 6.94191101e-07
Iter: 418 loss: 6.9249927e-07
Iter: 419 loss: 6.9215946e-07
Iter: 420 loss: 6.92166e-07
Iter: 421 loss: 6.91885e-07
Iter: 422 loss: 6.91492119e-07
Iter: 423 loss: 6.91493597e-07
Iter: 424 loss: 6.91077901e-07
Iter: 425 loss: 6.92882281e-07
Iter: 426 loss: 6.91039475e-07
Iter: 427 loss: 6.90716263e-07
Iter: 428 loss: 6.92736e-07
Iter: 429 loss: 6.90699778e-07
Iter: 430 loss: 6.90399531e-07
Iter: 431 loss: 6.90511911e-07
Iter: 432 loss: 6.90172669e-07
Iter: 433 loss: 6.8984366e-07
Iter: 434 loss: 6.92376602e-07
Iter: 435 loss: 6.89835076e-07
Iter: 436 loss: 6.89498336e-07
Iter: 437 loss: 6.89648857e-07
Iter: 438 loss: 6.89260219e-07
Iter: 439 loss: 6.8905382e-07
Iter: 440 loss: 6.89061949e-07
Iter: 441 loss: 6.88876412e-07
Iter: 442 loss: 6.89181718e-07
Iter: 443 loss: 6.88756359e-07
Iter: 444 loss: 6.88567866e-07
Iter: 445 loss: 6.88213845e-07
Iter: 446 loss: 6.95593087e-07
Iter: 447 loss: 6.88227885e-07
Iter: 448 loss: 6.87887677e-07
Iter: 449 loss: 6.89213721e-07
Iter: 450 loss: 6.87786496e-07
Iter: 451 loss: 6.87496538e-07
Iter: 452 loss: 6.87490797e-07
Iter: 453 loss: 6.87212037e-07
Iter: 454 loss: 6.87068791e-07
Iter: 455 loss: 6.8698472e-07
Iter: 456 loss: 6.86816406e-07
Iter: 457 loss: 6.86573173e-07
Iter: 458 loss: 6.86561918e-07
Iter: 459 loss: 6.86241947e-07
Iter: 460 loss: 6.86636e-07
Iter: 461 loss: 6.8605209e-07
Iter: 462 loss: 6.85710233e-07
Iter: 463 loss: 6.86653266e-07
Iter: 464 loss: 6.85591885e-07
Iter: 465 loss: 6.85197847e-07
Iter: 466 loss: 6.88033879e-07
Iter: 467 loss: 6.85190457e-07
Iter: 468 loss: 6.84950123e-07
Iter: 469 loss: 6.85607e-07
Iter: 470 loss: 6.84781298e-07
Iter: 471 loss: 6.84564156e-07
Iter: 472 loss: 6.8675763e-07
Iter: 473 loss: 6.84559382e-07
Iter: 474 loss: 6.84387658e-07
Iter: 475 loss: 6.8480972e-07
Iter: 476 loss: 6.8433252e-07
Iter: 477 loss: 6.84100314e-07
Iter: 478 loss: 6.84763506e-07
Iter: 479 loss: 6.83983899e-07
Iter: 480 loss: 6.83861515e-07
Iter: 481 loss: 6.83578605e-07
Iter: 482 loss: 6.88641194e-07
Iter: 483 loss: 6.83601286e-07
Iter: 484 loss: 6.83249425e-07
Iter: 485 loss: 6.84361453e-07
Iter: 486 loss: 6.8311806e-07
Iter: 487 loss: 6.82827192e-07
Iter: 488 loss: 6.8360373e-07
Iter: 489 loss: 6.82721407e-07
Iter: 490 loss: 6.82465384e-07
Iter: 491 loss: 6.83517612e-07
Iter: 492 loss: 6.82371933e-07
Iter: 493 loss: 6.82019902e-07
Iter: 494 loss: 6.83785743e-07
Iter: 495 loss: 6.82048835e-07
Iter: 496 loss: 6.8178e-07
Iter: 497 loss: 6.81371716e-07
Iter: 498 loss: 6.87612101e-07
Iter: 499 loss: 6.81348581e-07
Iter: 500 loss: 6.80867743e-07
Iter: 501 loss: 6.83665689e-07
Iter: 502 loss: 6.8077577e-07
Iter: 503 loss: 6.80449091e-07
Iter: 504 loss: 6.84642487e-07
Iter: 505 loss: 6.80418282e-07
Iter: 506 loss: 6.80146343e-07
Iter: 507 loss: 6.8147574e-07
Iter: 508 loss: 6.80132189e-07
Iter: 509 loss: 6.79912148e-07
Iter: 510 loss: 6.80576761e-07
Iter: 511 loss: 6.79882874e-07
Iter: 512 loss: 6.79684717e-07
Iter: 513 loss: 6.80134121e-07
Iter: 514 loss: 6.79535049e-07
Iter: 515 loss: 6.79347295e-07
Iter: 516 loss: 6.818467e-07
Iter: 517 loss: 6.79309608e-07
Iter: 518 loss: 6.79224456e-07
Iter: 519 loss: 6.78994411e-07
Iter: 520 loss: 6.81901952e-07
Iter: 521 loss: 6.78941092e-07
Iter: 522 loss: 6.78642664e-07
Iter: 523 loss: 6.78983156e-07
Iter: 524 loss: 6.785e-07
Iter: 525 loss: 6.78259767e-07
Iter: 526 loss: 6.80593757e-07
Iter: 527 loss: 6.78268123e-07
Iter: 528 loss: 6.7805496e-07
Iter: 529 loss: 6.78064339e-07
Iter: 530 loss: 6.77902904e-07
Iter: 531 loss: 6.77603282e-07
Iter: 532 loss: 6.80216488e-07
Iter: 533 loss: 6.77561275e-07
Iter: 534 loss: 6.77291382e-07
Iter: 535 loss: 6.77176104e-07
Iter: 536 loss: 6.77058836e-07
Iter: 537 loss: 6.76705213e-07
Iter: 538 loss: 6.77097148e-07
Iter: 539 loss: 6.76539912e-07
Iter: 540 loss: 6.76192428e-07
Iter: 541 loss: 6.76725e-07
Iter: 542 loss: 6.76093407e-07
Iter: 543 loss: 6.75874958e-07
Iter: 544 loss: 6.7586393e-07
Iter: 545 loss: 6.75697493e-07
Iter: 546 loss: 6.75899798e-07
Iter: 547 loss: 6.75610863e-07
Iter: 548 loss: 6.75482738e-07
Iter: 549 loss: 6.7740973e-07
Iter: 550 loss: 6.75441242e-07
Iter: 551 loss: 6.75324827e-07
Iter: 552 loss: 6.75554e-07
Iter: 553 loss: 6.75282308e-07
Iter: 554 loss: 6.75135425e-07
Iter: 555 loss: 6.74969669e-07
Iter: 556 loss: 6.74972512e-07
Iter: 557 loss: 6.74790215e-07
Iter: 558 loss: 6.74687897e-07
Iter: 559 loss: 6.74573926e-07
Iter: 560 loss: 6.74318e-07
Iter: 561 loss: 6.75505817e-07
Iter: 562 loss: 6.74284195e-07
Iter: 563 loss: 6.74005832e-07
Iter: 564 loss: 6.74025728e-07
Iter: 565 loss: 6.73789202e-07
Iter: 566 loss: 6.73464911e-07
Iter: 567 loss: 6.73498903e-07
Iter: 568 loss: 6.73282784e-07
Iter: 569 loss: 6.73815521e-07
Iter: 570 loss: 6.73196382e-07
Iter: 571 loss: 6.72941383e-07
Iter: 572 loss: 6.73160685e-07
Iter: 573 loss: 6.72779834e-07
Iter: 574 loss: 6.72556439e-07
Iter: 575 loss: 6.72504768e-07
Iter: 576 loss: 6.72367264e-07
Iter: 577 loss: 6.72033764e-07
Iter: 578 loss: 6.733041e-07
Iter: 579 loss: 6.7194668e-07
Iter: 580 loss: 6.71796215e-07
Iter: 581 loss: 6.71772796e-07
Iter: 582 loss: 6.71598912e-07
Iter: 583 loss: 6.716744e-07
Iter: 584 loss: 6.71505063e-07
Iter: 585 loss: 6.71279167e-07
Iter: 586 loss: 6.72084411e-07
Iter: 587 loss: 6.71223347e-07
Iter: 588 loss: 6.71103635e-07
Iter: 589 loss: 6.70957e-07
Iter: 590 loss: 6.70928614e-07
Iter: 591 loss: 6.70667873e-07
Iter: 592 loss: 6.71724251e-07
Iter: 593 loss: 6.70674581e-07
Iter: 594 loss: 6.70461077e-07
Iter: 595 loss: 6.70388772e-07
Iter: 596 loss: 6.70328177e-07
Iter: 597 loss: 6.70025884e-07
Iter: 598 loss: 6.70442e-07
Iter: 599 loss: 6.69954431e-07
Iter: 600 loss: 6.69706424e-07
Iter: 601 loss: 6.71543262e-07
Iter: 602 loss: 6.69661517e-07
Iter: 603 loss: 6.69530039e-07
Iter: 604 loss: 6.7083829e-07
Iter: 605 loss: 6.69486383e-07
Iter: 606 loss: 6.6935479e-07
Iter: 607 loss: 6.69325232e-07
Iter: 608 loss: 6.69266342e-07
Iter: 609 loss: 6.69069038e-07
Iter: 610 loss: 6.69768895e-07
Iter: 611 loss: 6.69023052e-07
Iter: 612 loss: 6.68847292e-07
Iter: 613 loss: 6.6877152e-07
Iter: 614 loss: 6.68684038e-07
Iter: 615 loss: 6.68552616e-07
Iter: 616 loss: 6.68525388e-07
Iter: 617 loss: 6.68422672e-07
Iter: 618 loss: 6.68625603e-07
Iter: 619 loss: 6.68384871e-07
Iter: 620 loss: 6.68190637e-07
Iter: 621 loss: 6.68146299e-07
Iter: 622 loss: 6.68043185e-07
Iter: 623 loss: 6.67821439e-07
Iter: 624 loss: 6.67803306e-07
Iter: 625 loss: 6.67638915e-07
Iter: 626 loss: 6.67386075e-07
Iter: 627 loss: 6.69672488e-07
Iter: 628 loss: 6.67353902e-07
Iter: 629 loss: 6.67088784e-07
Iter: 630 loss: 6.6686431e-07
Iter: 631 loss: 6.66806216e-07
Iter: 632 loss: 6.66595668e-07
Iter: 633 loss: 6.68102e-07
Iter: 634 loss: 6.66550477e-07
Iter: 635 loss: 6.6628553e-07
Iter: 636 loss: 6.67650795e-07
Iter: 637 loss: 6.66247615e-07
Iter: 638 loss: 6.66054575e-07
Iter: 639 loss: 6.66877213e-07
Iter: 640 loss: 6.65997618e-07
Iter: 641 loss: 6.65813445e-07
Iter: 642 loss: 6.65922698e-07
Iter: 643 loss: 6.65722609e-07
Iter: 644 loss: 6.65535595e-07
Iter: 645 loss: 6.6572386e-07
Iter: 646 loss: 6.65416678e-07
Iter: 647 loss: 6.65206642e-07
Iter: 648 loss: 6.66882897e-07
Iter: 649 loss: 6.65186292e-07
Iter: 650 loss: 6.65029802e-07
Iter: 651 loss: 6.66899837e-07
Iter: 652 loss: 6.65009225e-07
Iter: 653 loss: 6.64915433e-07
Iter: 654 loss: 6.64869958e-07
Iter: 655 loss: 6.64784295e-07
Iter: 656 loss: 6.64579034e-07
Iter: 657 loss: 6.64838467e-07
Iter: 658 loss: 6.64468132e-07
Iter: 659 loss: 6.64358595e-07
Iter: 660 loss: 6.64386334e-07
Iter: 661 loss: 6.64257414e-07
Iter: 662 loss: 6.64085064e-07
Iter: 663 loss: 6.65212724e-07
Iter: 664 loss: 6.64082222e-07
Iter: 665 loss: 6.63917035e-07
Iter: 666 loss: 6.63901233e-07
Iter: 667 loss: 6.63810283e-07
Iter: 668 loss: 6.63552669e-07
Iter: 669 loss: 6.63571e-07
Iter: 670 loss: 6.6339021e-07
Iter: 671 loss: 6.631509e-07
Iter: 672 loss: 6.66246422e-07
Iter: 673 loss: 6.63093886e-07
Iter: 674 loss: 6.6287771e-07
Iter: 675 loss: 6.63360879e-07
Iter: 676 loss: 6.62775335e-07
Iter: 677 loss: 6.6256041e-07
Iter: 678 loss: 6.62572802e-07
Iter: 679 loss: 6.62456443e-07
Iter: 680 loss: 6.6220116e-07
Iter: 681 loss: 6.63263791e-07
Iter: 682 loss: 6.62161369e-07
Iter: 683 loss: 6.61963554e-07
Iter: 684 loss: 6.64766617e-07
Iter: 685 loss: 6.61946e-07
Iter: 686 loss: 6.61759714e-07
Iter: 687 loss: 6.61833042e-07
Iter: 688 loss: 6.61690478e-07
Iter: 689 loss: 6.61481693e-07
Iter: 690 loss: 6.62010166e-07
Iter: 691 loss: 6.61366869e-07
Iter: 692 loss: 6.61165814e-07
Iter: 693 loss: 6.60975616e-07
Iter: 694 loss: 6.60944579e-07
Iter: 695 loss: 6.60772912e-07
Iter: 696 loss: 6.61693662e-07
Iter: 697 loss: 6.60709702e-07
Iter: 698 loss: 6.60486535e-07
Iter: 699 loss: 6.61615502e-07
Iter: 700 loss: 6.60500802e-07
Iter: 701 loss: 6.60278829e-07
Iter: 702 loss: 6.60508533e-07
Iter: 703 loss: 6.60218916e-07
Iter: 704 loss: 6.60044634e-07
Iter: 705 loss: 6.59822035e-07
Iter: 706 loss: 6.597744e-07
Iter: 707 loss: 6.59702835e-07
Iter: 708 loss: 6.59650368e-07
Iter: 709 loss: 6.59515536e-07
Iter: 710 loss: 6.59605234e-07
Iter: 711 loss: 6.59415718e-07
Iter: 712 loss: 6.59293391e-07
Iter: 713 loss: 6.59112402e-07
Iter: 714 loss: 6.59096941e-07
Iter: 715 loss: 6.58951535e-07
Iter: 716 loss: 6.58954946e-07
Iter: 717 loss: 6.58805675e-07
Iter: 718 loss: 6.5931323e-07
Iter: 719 loss: 6.58763838e-07
Iter: 720 loss: 6.58554313e-07
Iter: 721 loss: 6.58617296e-07
Iter: 722 loss: 6.58515489e-07
Iter: 723 loss: 6.5833666e-07
Iter: 724 loss: 6.59066359e-07
Iter: 725 loss: 6.58287718e-07
Iter: 726 loss: 6.58170222e-07
Iter: 727 loss: 6.58120257e-07
Iter: 728 loss: 6.58065119e-07
Iter: 729 loss: 6.57898e-07
Iter: 730 loss: 6.58017257e-07
Iter: 731 loss: 6.57803184e-07
Iter: 732 loss: 6.57647092e-07
Iter: 733 loss: 6.57662952e-07
Iter: 734 loss: 6.57505e-07
Iter: 735 loss: 6.57272494e-07
Iter: 736 loss: 6.62027333e-07
Iter: 737 loss: 6.57264081e-07
Iter: 738 loss: 6.5697634e-07
Iter: 739 loss: 6.58155614e-07
Iter: 740 loss: 6.56913357e-07
Iter: 741 loss: 6.56647785e-07
Iter: 742 loss: 6.59238538e-07
Iter: 743 loss: 6.56680129e-07
Iter: 744 loss: 6.56403927e-07
Iter: 745 loss: 6.56574116e-07
Iter: 746 loss: 6.56285124e-07
Iter: 747 loss: 6.56069801e-07
Iter: 748 loss: 6.55853e-07
Iter: 749 loss: 6.55808435e-07
Iter: 750 loss: 6.55876647e-07
Iter: 751 loss: 6.55618578e-07
Iter: 752 loss: 6.55510917e-07
Iter: 753 loss: 6.55364488e-07
Iter: 754 loss: 6.55342546e-07
Iter: 755 loss: 6.55189297e-07
Iter: 756 loss: 6.56401767e-07
Iter: 757 loss: 6.55184408e-07
Iter: 758 loss: 6.55035933e-07
Iter: 759 loss: 6.55066174e-07
Iter: 760 loss: 6.5493623e-07
Iter: 761 loss: 6.54796622e-07
Iter: 762 loss: 6.55003248e-07
Iter: 763 loss: 6.54696123e-07
Iter: 764 loss: 6.54567202e-07
Iter: 765 loss: 6.54711812e-07
Iter: 766 loss: 6.5445289e-07
Iter: 767 loss: 6.54297082e-07
Iter: 768 loss: 6.5585175e-07
Iter: 769 loss: 6.54301516e-07
Iter: 770 loss: 6.54169867e-07
Iter: 771 loss: 6.54201813e-07
Iter: 772 loss: 6.54056578e-07
Iter: 773 loss: 6.53847223e-07
Iter: 774 loss: 6.53879852e-07
Iter: 775 loss: 6.53655775e-07
Iter: 776 loss: 6.5357392e-07
Iter: 777 loss: 6.53550558e-07
Iter: 778 loss: 6.53397137e-07
Iter: 779 loss: 6.53153279e-07
Iter: 780 loss: 6.53171412e-07
Iter: 781 loss: 6.52924143e-07
Iter: 782 loss: 6.54422195e-07
Iter: 783 loss: 6.52868266e-07
Iter: 784 loss: 6.5279346e-07
Iter: 785 loss: 6.527614e-07
Iter: 786 loss: 6.5267875e-07
Iter: 787 loss: 6.52485596e-07
Iter: 788 loss: 6.53580287e-07
Iter: 789 loss: 6.5240647e-07
Iter: 790 loss: 6.52374922e-07
Iter: 791 loss: 6.52297558e-07
Iter: 792 loss: 6.52227641e-07
Iter: 793 loss: 6.5202812e-07
Iter: 794 loss: 6.53629e-07
Iter: 795 loss: 6.51945584e-07
Iter: 796 loss: 6.51757773e-07
Iter: 797 loss: 6.52821427e-07
Iter: 798 loss: 6.5172685e-07
Iter: 799 loss: 6.51522e-07
Iter: 800 loss: 6.51854862e-07
Iter: 801 loss: 6.51410687e-07
Iter: 802 loss: 6.51185246e-07
Iter: 803 loss: 6.51293249e-07
Iter: 804 loss: 6.51013352e-07
Iter: 805 loss: 6.50754714e-07
Iter: 806 loss: 6.53220468e-07
Iter: 807 loss: 6.50789957e-07
Iter: 808 loss: 6.50668881e-07
Iter: 809 loss: 6.50580773e-07
Iter: 810 loss: 6.50523418e-07
Iter: 811 loss: 6.50325148e-07
Iter: 812 loss: 6.51984237e-07
Iter: 813 loss: 6.50345555e-07
Iter: 814 loss: 6.50158313e-07
Iter: 815 loss: 6.50536435e-07
Iter: 816 loss: 6.50116306e-07
Iter: 817 loss: 6.50014556e-07
Iter: 818 loss: 6.50199297e-07
Iter: 819 loss: 6.49935771e-07
Iter: 820 loss: 6.49748472e-07
Iter: 821 loss: 6.51533185e-07
Iter: 822 loss: 6.4972312e-07
Iter: 823 loss: 6.49703395e-07
Iter: 824 loss: 6.49544404e-07
Iter: 825 loss: 6.51554842e-07
Iter: 826 loss: 6.49531444e-07
Iter: 827 loss: 6.49389449e-07
Iter: 828 loss: 6.49416052e-07
Iter: 829 loss: 6.4933522e-07
Iter: 830 loss: 6.492088e-07
Iter: 831 loss: 6.49220851e-07
Iter: 832 loss: 6.49066237e-07
Iter: 833 loss: 6.48943342e-07
Iter: 834 loss: 6.48908269e-07
Iter: 835 loss: 6.4866083e-07
Iter: 836 loss: 6.50549964e-07
Iter: 837 loss: 6.48691071e-07
Iter: 838 loss: 6.48541231e-07
Iter: 839 loss: 6.48682e-07
Iter: 840 loss: 6.48484161e-07
Iter: 841 loss: 6.48245759e-07
Iter: 842 loss: 6.48704372e-07
Iter: 843 loss: 6.48189427e-07
Iter: 844 loss: 6.47970751e-07
Iter: 845 loss: 6.4932226e-07
Iter: 846 loss: 6.47979391e-07
Iter: 847 loss: 6.47854e-07
Iter: 848 loss: 6.48179139e-07
Iter: 849 loss: 6.47785214e-07
Iter: 850 loss: 6.47578531e-07
Iter: 851 loss: 6.47682214e-07
Iter: 852 loss: 6.47464731e-07
Iter: 853 loss: 6.4748906e-07
Iter: 854 loss: 6.47360366e-07
Iter: 855 loss: 6.47326715e-07
Iter: 856 loss: 6.47232923e-07
Iter: 857 loss: 6.49616254e-07
Iter: 858 loss: 6.47250374e-07
Iter: 859 loss: 6.47099341e-07
Iter: 860 loss: 6.47118497e-07
Iter: 861 loss: 6.47019306e-07
Iter: 862 loss: 6.46940805e-07
Iter: 863 loss: 6.46945921e-07
Iter: 864 loss: 6.46836554e-07
Iter: 865 loss: 6.46653177e-07
Iter: 866 loss: 6.47857348e-07
Iter: 867 loss: 6.46602302e-07
Iter: 868 loss: 6.46407898e-07
Iter: 869 loss: 6.48510081e-07
Iter: 870 loss: 6.46414037e-07
Iter: 871 loss: 6.46247372e-07
Iter: 872 loss: 6.46687624e-07
Iter: 873 loss: 6.4621679e-07
Iter: 874 loss: 6.46058652e-07
Iter: 875 loss: 6.46529031e-07
Iter: 876 loss: 6.46013916e-07
Iter: 877 loss: 6.45841453e-07
Iter: 878 loss: 6.46034721e-07
Iter: 879 loss: 6.45749651e-07
Iter: 880 loss: 6.455424e-07
Iter: 881 loss: 6.4601943e-07
Iter: 882 loss: 6.45452246e-07
Iter: 883 loss: 6.45325258e-07
Iter: 884 loss: 6.46833371e-07
Iter: 885 loss: 6.45323894e-07
Iter: 886 loss: 6.45194177e-07
Iter: 887 loss: 6.45631587e-07
Iter: 888 loss: 6.4513813e-07
Iter: 889 loss: 6.45068894e-07
Iter: 890 loss: 6.4602807e-07
Iter: 891 loss: 6.45046839e-07
Iter: 892 loss: 6.44984709e-07
Iter: 893 loss: 6.44855618e-07
Iter: 894 loss: 6.46741512e-07
Iter: 895 loss: 6.44795591e-07
Iter: 896 loss: 6.44682302e-07
Iter: 897 loss: 6.45074e-07
Iter: 898 loss: 6.4462165e-07
Iter: 899 loss: 6.44493525e-07
Iter: 900 loss: 6.46142325e-07
Iter: 901 loss: 6.44522402e-07
Iter: 902 loss: 6.44440661e-07
Iter: 903 loss: 6.4432885e-07
Iter: 904 loss: 6.44265867e-07
Iter: 905 loss: 6.44123702e-07
Iter: 906 loss: 6.44059696e-07
Iter: 907 loss: 6.43989949e-07
Iter: 908 loss: 6.43814474e-07
Iter: 909 loss: 6.43814701e-07
Iter: 910 loss: 6.43714827e-07
Iter: 911 loss: 6.43622286e-07
Iter: 912 loss: 6.43592386e-07
Iter: 913 loss: 6.43435669e-07
Iter: 914 loss: 6.44456463e-07
Iter: 915 loss: 6.43460055e-07
Iter: 916 loss: 6.43372346e-07
Iter: 917 loss: 6.43962949e-07
Iter: 918 loss: 6.43305043e-07
Iter: 919 loss: 6.4323433e-07
Iter: 920 loss: 6.43479893e-07
Iter: 921 loss: 6.43219153e-07
Iter: 922 loss: 6.43137128e-07
Iter: 923 loss: 6.44057707e-07
Iter: 924 loss: 6.43148553e-07
Iter: 925 loss: 6.43085855e-07
Iter: 926 loss: 6.43084093e-07
Iter: 927 loss: 6.43048281e-07
Iter: 928 loss: 6.42969781e-07
Iter: 929 loss: 6.42771624e-07
Iter: 930 loss: 6.45209639e-07
Iter: 931 loss: 6.42783675e-07
Iter: 932 loss: 6.4263412e-07
Iter: 933 loss: 6.44202942e-07
Iter: 934 loss: 6.42659757e-07
Iter: 935 loss: 6.42556472e-07
Iter: 936 loss: 6.43410715e-07
Iter: 937 loss: 6.42558234e-07
Iter: 938 loss: 6.424616e-07
Iter: 939 loss: 6.42281179e-07
Iter: 940 loss: 6.44554461e-07
Iter: 941 loss: 6.42279588e-07
Iter: 942 loss: 6.42148507e-07
Iter: 943 loss: 6.44122906e-07
Iter: 944 loss: 6.42144641e-07
Iter: 945 loss: 6.41988095e-07
Iter: 946 loss: 6.42168857e-07
Iter: 947 loss: 6.41972861e-07
Iter: 948 loss: 6.41838483e-07
Iter: 949 loss: 6.42429541e-07
Iter: 950 loss: 6.41825693e-07
Iter: 951 loss: 6.4176e-07
Iter: 952 loss: 6.41707629e-07
Iter: 953 loss: 6.41637712e-07
Iter: 954 loss: 6.41503e-07
Iter: 955 loss: 6.41509359e-07
Iter: 956 loss: 6.41504357e-07
Iter: 957 loss: 6.4162748e-07
Iter: 958 loss: 6.41429324e-07
Iter: 959 loss: 6.41388283e-07
Iter: 960 loss: 6.4176362e-07
Iter: 961 loss: 6.4138e-07
Iter: 962 loss: 6.41267889e-07
Iter: 963 loss: 6.41071e-07
Iter: 964 loss: 6.44463171e-07
Iter: 965 loss: 6.41084739e-07
Iter: 966 loss: 6.40891926e-07
Iter: 967 loss: 6.41777604e-07
Iter: 968 loss: 6.40875783e-07
Iter: 969 loss: 6.40751296e-07
Iter: 970 loss: 6.41736278e-07
Iter: 971 loss: 6.40767098e-07
Iter: 972 loss: 6.40603218e-07
Iter: 973 loss: 6.40561211e-07
Iter: 974 loss: 6.4052665e-07
Iter: 975 loss: 6.40389544e-07
Iter: 976 loss: 6.40445762e-07
Iter: 977 loss: 6.40292456e-07
Iter: 978 loss: 6.40138808e-07
Iter: 979 loss: 6.40968437e-07
Iter: 980 loss: 6.40104531e-07
Iter: 981 loss: 6.39960206e-07
Iter: 982 loss: 6.40555186e-07
Iter: 983 loss: 6.39950031e-07
Iter: 984 loss: 6.39763812e-07
Iter: 985 loss: 6.39860332e-07
Iter: 986 loss: 6.39694804e-07
Iter: 987 loss: 6.39564e-07
Iter: 988 loss: 6.40967414e-07
Iter: 989 loss: 6.39531436e-07
Iter: 990 loss: 6.39455379e-07
Iter: 991 loss: 6.39971347e-07
Iter: 992 loss: 6.39463451e-07
Iter: 993 loss: 6.39348912e-07
Iter: 994 loss: 6.39670077e-07
Iter: 995 loss: 6.39351128e-07
Iter: 996 loss: 6.39209475e-07
Iter: 997 loss: 6.39205552e-07
Iter: 998 loss: 6.39118184e-07
Iter: 999 loss: 6.38958454e-07
Iter: 1000 loss: 6.39020641e-07
Iter: 1001 loss: 6.38879555e-07
Iter: 1002 loss: 6.38762e-07
Iter: 1003 loss: 6.39101245e-07
Iter: 1004 loss: 6.38681229e-07
Iter: 1005 loss: 6.38591587e-07
Iter: 1006 loss: 6.40283361e-07
Iter: 1007 loss: 6.38606139e-07
Iter: 1008 loss: 6.38524625e-07
Iter: 1009 loss: 6.38351707e-07
Iter: 1010 loss: 6.40379085e-07
Iter: 1011 loss: 6.38376207e-07
Iter: 1012 loss: 6.38204824e-07
Iter: 1013 loss: 6.39069413e-07
Iter: 1014 loss: 6.38182314e-07
Iter: 1015 loss: 6.38031452e-07
Iter: 1016 loss: 6.38338861e-07
Iter: 1017 loss: 6.37957555e-07
Iter: 1018 loss: 6.37792368e-07
Iter: 1019 loss: 6.39333734e-07
Iter: 1020 loss: 6.3778765e-07
Iter: 1021 loss: 6.37687094e-07
Iter: 1022 loss: 6.37736321e-07
Iter: 1023 loss: 6.37652477e-07
Iter: 1024 loss: 6.37479047e-07
Iter: 1025 loss: 6.38444078e-07
Iter: 1026 loss: 6.37463813e-07
Iter: 1027 loss: 6.3736195e-07
Iter: 1028 loss: 6.38469032e-07
Iter: 1029 loss: 6.37387075e-07
Iter: 1030 loss: 6.37324263e-07
Iter: 1031 loss: 6.37412086e-07
Iter: 1032 loss: 6.37275036e-07
Iter: 1033 loss: 6.37232574e-07
Iter: 1034 loss: 6.37105e-07
Iter: 1035 loss: 6.37108883e-07
Iter: 1036 loss: 6.37020435e-07
Iter: 1037 loss: 6.3754112e-07
Iter: 1038 loss: 6.36951711e-07
Iter: 1039 loss: 6.36888672e-07
Iter: 1040 loss: 6.37858591e-07
Iter: 1041 loss: 6.36889467e-07
Iter: 1042 loss: 6.36808863e-07
Iter: 1043 loss: 6.36669427e-07
Iter: 1044 loss: 6.3922073e-07
Iter: 1045 loss: 6.36692448e-07
Iter: 1046 loss: 6.36504694e-07
Iter: 1047 loss: 6.37053176e-07
Iter: 1048 loss: 6.36510094e-07
Iter: 1049 loss: 6.36342861e-07
Iter: 1050 loss: 6.36421589e-07
Iter: 1051 loss: 6.36242305e-07
Iter: 1052 loss: 6.36146751e-07
Iter: 1053 loss: 6.3613669e-07
Iter: 1054 loss: 6.36089737e-07
Iter: 1055 loss: 6.35897493e-07
Iter: 1056 loss: 6.35904598e-07
Iter: 1057 loss: 6.35802849e-07
Iter: 1058 loss: 6.37526171e-07
Iter: 1059 loss: 6.35804327e-07
Iter: 1060 loss: 6.35724916e-07
Iter: 1061 loss: 6.36024254e-07
Iter: 1062 loss: 6.35700758e-07
Iter: 1063 loss: 6.35589913e-07
Iter: 1064 loss: 6.35907213e-07
Iter: 1065 loss: 6.35572633e-07
Iter: 1066 loss: 6.35503625e-07
Iter: 1067 loss: 6.35330593e-07
Iter: 1068 loss: 6.35340029e-07
Iter: 1069 loss: 6.35187e-07
Iter: 1070 loss: 6.35693709e-07
Iter: 1071 loss: 6.35126412e-07
Iter: 1072 loss: 6.35012668e-07
Iter: 1073 loss: 6.36121229e-07
Iter: 1074 loss: 6.35018182e-07
Iter: 1075 loss: 6.34885509e-07
Iter: 1076 loss: 6.34993569e-07
Iter: 1077 loss: 6.34825767e-07
Iter: 1078 loss: 6.34724188e-07
Iter: 1079 loss: 6.34836908e-07
Iter: 1080 loss: 6.34703156e-07
Iter: 1081 loss: 6.34536605e-07
Iter: 1082 loss: 6.34497496e-07
Iter: 1083 loss: 6.34423714e-07
Iter: 1084 loss: 6.34296782e-07
Iter: 1085 loss: 6.35712922e-07
Iter: 1086 loss: 6.34300818e-07
Iter: 1087 loss: 6.34163e-07
Iter: 1088 loss: 6.34509e-07
Iter: 1089 loss: 6.34115395e-07
Iter: 1090 loss: 6.34008074e-07
Iter: 1091 loss: 6.3437983e-07
Iter: 1092 loss: 6.33983859e-07
Iter: 1093 loss: 6.33929403e-07
Iter: 1094 loss: 6.3522765e-07
Iter: 1095 loss: 6.33939294e-07
Iter: 1096 loss: 6.33848458e-07
Iter: 1097 loss: 6.33881e-07
Iter: 1098 loss: 6.33798095e-07
Iter: 1099 loss: 6.33687705e-07
Iter: 1100 loss: 6.33754098e-07
Iter: 1101 loss: 6.33618e-07
Iter: 1102 loss: 6.33576519e-07
Iter: 1103 loss: 6.33512286e-07
Iter: 1104 loss: 6.33441687e-07
Iter: 1105 loss: 6.3332584e-07
Iter: 1106 loss: 6.33679065e-07
Iter: 1107 loss: 6.3331504e-07
Iter: 1108 loss: 6.33138598e-07
Iter: 1109 loss: 6.34792855e-07
Iter: 1110 loss: 6.33134505e-07
Iter: 1111 loss: 6.33100399e-07
Iter: 1112 loss: 6.33035427e-07
Iter: 1113 loss: 6.33023433e-07
Iter: 1114 loss: 6.32902129e-07
Iter: 1115 loss: 6.32879789e-07
Iter: 1116 loss: 6.32753085e-07
Iter: 1117 loss: 6.32656054e-07
Iter: 1118 loss: 6.33201e-07
Iter: 1119 loss: 6.32630076e-07
Iter: 1120 loss: 6.32423735e-07
Iter: 1121 loss: 6.33378363e-07
Iter: 1122 loss: 6.32422598e-07
Iter: 1123 loss: 6.32319939e-07
Iter: 1124 loss: 6.329455e-07
Iter: 1125 loss: 6.323462e-07
Iter: 1126 loss: 6.32251442e-07
Iter: 1127 loss: 6.32469153e-07
Iter: 1128 loss: 6.32229671e-07
Iter: 1129 loss: 6.32116553e-07
Iter: 1130 loss: 6.32695901e-07
Iter: 1131 loss: 6.32105071e-07
Iter: 1132 loss: 6.32044873e-07
Iter: 1133 loss: 6.32281512e-07
Iter: 1134 loss: 6.32003605e-07
Iter: 1135 loss: 6.31935563e-07
Iter: 1136 loss: 6.31883154e-07
Iter: 1137 loss: 6.33529794e-07
Iter: 1138 loss: 6.31875537e-07
Iter: 1139 loss: 6.31765374e-07
Iter: 1140 loss: 6.3242e-07
Iter: 1141 loss: 6.31746616e-07
Iter: 1142 loss: 6.31660328e-07
Iter: 1143 loss: 6.32621436e-07
Iter: 1144 loss: 6.316792e-07
Iter: 1145 loss: 6.31587511e-07
Iter: 1146 loss: 6.31473654e-07
Iter: 1147 loss: 6.3147462e-07
Iter: 1148 loss: 6.31338366e-07
Iter: 1149 loss: 6.31723879e-07
Iter: 1150 loss: 6.31285388e-07
Iter: 1151 loss: 6.31172668e-07
Iter: 1152 loss: 6.31041758e-07
Iter: 1153 loss: 6.3102334e-07
Iter: 1154 loss: 6.30900558e-07
Iter: 1155 loss: 6.30900217e-07
Iter: 1156 loss: 6.30847353e-07
Iter: 1157 loss: 6.31121566e-07
Iter: 1158 loss: 6.30822058e-07
Iter: 1159 loss: 6.30758223e-07
Iter: 1160 loss: 6.30859802e-07
Iter: 1161 loss: 6.30696093e-07
Iter: 1162 loss: 6.30599629e-07
Iter: 1163 loss: 6.306351e-07
Iter: 1164 loss: 6.30597697e-07
Iter: 1165 loss: 6.30589625e-07
Iter: 1166 loss: 6.30571776e-07
Iter: 1167 loss: 6.30499756e-07
Iter: 1168 loss: 6.30378793e-07
Iter: 1169 loss: 6.30370096e-07
Iter: 1170 loss: 6.30305294e-07
Iter: 1171 loss: 6.30695e-07
Iter: 1172 loss: 6.3028051e-07
Iter: 1173 loss: 6.30157729e-07
Iter: 1174 loss: 6.30678869e-07
Iter: 1175 loss: 6.30127374e-07
Iter: 1176 loss: 6.29972646e-07
Iter: 1177 loss: 6.301334e-07
Iter: 1178 loss: 6.29895794e-07
Iter: 1179 loss: 6.2977324e-07
Iter: 1180 loss: 6.29974522e-07
Iter: 1181 loss: 6.29727879e-07
Iter: 1182 loss: 6.29610781e-07
Iter: 1183 loss: 6.29541e-07
Iter: 1184 loss: 6.29483395e-07
Iter: 1185 loss: 6.29356919e-07
Iter: 1186 loss: 6.30710588e-07
Iter: 1187 loss: 6.2938534e-07
Iter: 1188 loss: 6.29233853e-07
Iter: 1189 loss: 6.29526824e-07
Iter: 1190 loss: 6.29197416e-07
Iter: 1191 loss: 6.29052636e-07
Iter: 1192 loss: 6.29802457e-07
Iter: 1193 loss: 6.29066e-07
Iter: 1194 loss: 6.29005626e-07
Iter: 1195 loss: 6.28978682e-07
Iter: 1196 loss: 6.28955e-07
Iter: 1197 loss: 6.28863177e-07
Iter: 1198 loss: 6.28849421e-07
Iter: 1199 loss: 6.28695261e-07
Iter: 1200 loss: 6.29147962e-07
Iter: 1201 loss: 6.28684802e-07
Iter: 1202 loss: 6.28610337e-07
Iter: 1203 loss: 6.28535e-07
Iter: 1204 loss: 6.28527687e-07
Iter: 1205 loss: 6.28361704e-07
Iter: 1206 loss: 6.28926045e-07
Iter: 1207 loss: 6.28342946e-07
Iter: 1208 loss: 6.2828974e-07
Iter: 1209 loss: 6.29217766e-07
Iter: 1210 loss: 6.28285477e-07
Iter: 1211 loss: 6.28186626e-07
Iter: 1212 loss: 6.28091925e-07
Iter: 1213 loss: 6.28082717e-07
Iter: 1214 loss: 6.27943e-07
Iter: 1215 loss: 6.28239491e-07
Iter: 1216 loss: 6.27899283e-07
Iter: 1217 loss: 6.27772692e-07
Iter: 1218 loss: 6.27829593e-07
Iter: 1219 loss: 6.27687655e-07
Iter: 1220 loss: 6.27607506e-07
Iter: 1221 loss: 6.29274382e-07
Iter: 1222 loss: 6.27605914e-07
Iter: 1223 loss: 6.27519057e-07
Iter: 1224 loss: 6.27803843e-07
Iter: 1225 loss: 6.27478755e-07
Iter: 1226 loss: 6.27402414e-07
Iter: 1227 loss: 6.27894906e-07
Iter: 1228 loss: 6.27368308e-07
Iter: 1229 loss: 6.27234158e-07
Iter: 1230 loss: 6.27439817e-07
Iter: 1231 loss: 6.27206e-07
Iter: 1232 loss: 6.2712536e-07
Iter: 1233 loss: 6.27493e-07
Iter: 1234 loss: 6.27118538e-07
Iter: 1235 loss: 6.27057148e-07
Iter: 1236 loss: 6.26948633e-07
Iter: 1237 loss: 6.2906912e-07
Iter: 1238 loss: 6.26937833e-07
Iter: 1239 loss: 6.2681454e-07
Iter: 1240 loss: 6.274513e-07
Iter: 1241 loss: 6.26812437e-07
Iter: 1242 loss: 6.26683288e-07
Iter: 1243 loss: 6.27793554e-07
Iter: 1244 loss: 6.26708697e-07
Iter: 1245 loss: 6.26651968e-07
Iter: 1246 loss: 6.26542203e-07
Iter: 1247 loss: 6.26543851e-07
Iter: 1248 loss: 6.26410213e-07
Iter: 1249 loss: 6.26785209e-07
Iter: 1250 loss: 6.2639117e-07
Iter: 1251 loss: 6.2629465e-07
Iter: 1252 loss: 6.26319661e-07
Iter: 1253 loss: 6.2624332e-07
Iter: 1254 loss: 6.26125086e-07
Iter: 1255 loss: 6.26641281e-07
Iter: 1256 loss: 6.2608882e-07
Iter: 1257 loss: 6.2603192e-07
Iter: 1258 loss: 6.26685505e-07
Iter: 1259 loss: 6.26006795e-07
Iter: 1260 loss: 6.25913799e-07
Iter: 1261 loss: 6.2611997e-07
Iter: 1262 loss: 6.25928067e-07
Iter: 1263 loss: 6.25793064e-07
Iter: 1264 loss: 6.26540213e-07
Iter: 1265 loss: 6.25820576e-07
Iter: 1266 loss: 6.25764187e-07
Iter: 1267 loss: 6.25722123e-07
Iter: 1268 loss: 6.25680968e-07
Iter: 1269 loss: 6.25575353e-07
Iter: 1270 loss: 6.25682787e-07
Iter: 1271 loss: 6.25517828e-07
Iter: 1272 loss: 6.25418238e-07
Iter: 1273 loss: 6.25415623e-07
Iter: 1274 loss: 6.2533e-07
Iter: 1275 loss: 6.25240489e-07
Iter: 1276 loss: 6.26547603e-07
Iter: 1277 loss: 6.25251062e-07
Iter: 1278 loss: 6.25117138e-07
Iter: 1279 loss: 6.25181656e-07
Iter: 1280 loss: 6.25068083e-07
Iter: 1281 loss: 6.2495559e-07
Iter: 1282 loss: 6.25189614e-07
Iter: 1283 loss: 6.24880954e-07
Iter: 1284 loss: 6.24789379e-07
Iter: 1285 loss: 6.24884819e-07
Iter: 1286 loss: 6.24737822e-07
Iter: 1287 loss: 6.24575136e-07
Iter: 1288 loss: 6.24955874e-07
Iter: 1289 loss: 6.24514257e-07
Iter: 1290 loss: 6.24454856e-07
Iter: 1291 loss: 6.24914549e-07
Iter: 1292 loss: 6.24449171e-07
Iter: 1293 loss: 6.24304164e-07
Iter: 1294 loss: 6.24779e-07
Iter: 1295 loss: 6.24297854e-07
Iter: 1296 loss: 6.24200766e-07
Iter: 1297 loss: 6.25344853e-07
Iter: 1298 loss: 6.24212873e-07
Iter: 1299 loss: 6.24118684e-07
Iter: 1300 loss: 6.24065763e-07
Iter: 1301 loss: 6.24064796e-07
Iter: 1302 loss: 6.23969186e-07
Iter: 1303 loss: 6.24194172e-07
Iter: 1304 loss: 6.23923086e-07
Iter: 1305 loss: 6.2386863e-07
Iter: 1306 loss: 6.23949177e-07
Iter: 1307 loss: 6.23781148e-07
Iter: 1308 loss: 6.23712651e-07
Iter: 1309 loss: 6.23576e-07
Iter: 1310 loss: 6.2357492e-07
Iter: 1311 loss: 6.23486358e-07
Iter: 1312 loss: 6.2344759e-07
Iter: 1313 loss: 6.23396886e-07
Iter: 1314 loss: 6.23321057e-07
Iter: 1315 loss: 6.23330038e-07
Iter: 1316 loss: 6.23132451e-07
Iter: 1317 loss: 6.23470783e-07
Iter: 1318 loss: 6.2308419e-07
Iter: 1319 loss: 6.23051619e-07
Iter: 1320 loss: 6.230498e-07
Iter: 1321 loss: 6.22957373e-07
Iter: 1322 loss: 6.22807e-07
Iter: 1323 loss: 6.22875632e-07
Iter: 1324 loss: 6.22710957e-07
Iter: 1325 loss: 6.22550772e-07
Iter: 1326 loss: 6.22790822e-07
Iter: 1327 loss: 6.22449079e-07
Iter: 1328 loss: 6.22429354e-07
Iter: 1329 loss: 6.22370635e-07
Iter: 1330 loss: 6.2228969e-07
Iter: 1331 loss: 6.22557195e-07
Iter: 1332 loss: 6.22268431e-07
Iter: 1333 loss: 6.22232903e-07
Iter: 1334 loss: 6.22233e-07
Iter: 1335 loss: 6.22160542e-07
Iter: 1336 loss: 6.22062771e-07
Iter: 1337 loss: 6.22055495e-07
Iter: 1338 loss: 6.21979439e-07
Iter: 1339 loss: 6.21877348e-07
Iter: 1340 loss: 6.22710331e-07
Iter: 1341 loss: 6.21881895e-07
Iter: 1342 loss: 6.21793333e-07
Iter: 1343 loss: 6.2169596e-07
Iter: 1344 loss: 6.21672484e-07
Iter: 1345 loss: 6.21546292e-07
Iter: 1346 loss: 6.22867674e-07
Iter: 1347 loss: 6.21566301e-07
Iter: 1348 loss: 6.21436925e-07
Iter: 1349 loss: 6.21805498e-07
Iter: 1350 loss: 6.2138929e-07
Iter: 1351 loss: 6.21319941e-07
Iter: 1352 loss: 6.21182608e-07
Iter: 1353 loss: 6.23208621e-07
Iter: 1354 loss: 6.21180618e-07
Iter: 1355 loss: 6.21039419e-07
Iter: 1356 loss: 6.2292645e-07
Iter: 1357 loss: 6.21021741e-07
Iter: 1358 loss: 6.20940966e-07
Iter: 1359 loss: 6.20953756e-07
Iter: 1360 loss: 6.20845242e-07
Iter: 1361 loss: 6.20694436e-07
Iter: 1362 loss: 6.21453864e-07
Iter: 1363 loss: 6.20695459e-07
Iter: 1364 loss: 6.20643618e-07
Iter: 1365 loss: 6.20655101e-07
Iter: 1366 loss: 6.20570177e-07
Iter: 1367 loss: 6.20445462e-07
Iter: 1368 loss: 6.2240747e-07
Iter: 1369 loss: 6.20415278e-07
Iter: 1370 loss: 6.2033655e-07
Iter: 1371 loss: 6.21233482e-07
Iter: 1372 loss: 6.20318929e-07
Iter: 1373 loss: 6.20243441e-07
Iter: 1374 loss: 6.203108e-07
Iter: 1375 loss: 6.2019069e-07
Iter: 1376 loss: 6.20109063e-07
Iter: 1377 loss: 6.20148171e-07
Iter: 1378 loss: 6.20025673e-07
Iter: 1379 loss: 6.19944899e-07
Iter: 1380 loss: 6.19951265e-07
Iter: 1381 loss: 6.19895786e-07
Iter: 1382 loss: 6.19856735e-07
Iter: 1383 loss: 6.19832178e-07
Iter: 1384 loss: 6.1975976e-07
Iter: 1385 loss: 6.20575406e-07
Iter: 1386 loss: 6.19711216e-07
Iter: 1387 loss: 6.19642094e-07
Iter: 1388 loss: 6.1954205e-07
Iter: 1389 loss: 6.21697779e-07
Iter: 1390 loss: 6.19502714e-07
Iter: 1391 loss: 6.19392836e-07
Iter: 1392 loss: 6.20253331e-07
Iter: 1393 loss: 6.19364471e-07
Iter: 1394 loss: 6.19276591e-07
Iter: 1395 loss: 6.19377658e-07
Iter: 1396 loss: 6.19249477e-07
Iter: 1397 loss: 6.19138234e-07
Iter: 1398 loss: 6.1988942e-07
Iter: 1399 loss: 6.19135335e-07
Iter: 1400 loss: 6.19071159e-07
Iter: 1401 loss: 6.19032562e-07
Iter: 1402 loss: 6.18993511e-07
Iter: 1403 loss: 6.18879596e-07
Iter: 1404 loss: 6.18876129e-07
Iter: 1405 loss: 6.18788192e-07
Iter: 1406 loss: 6.19091452e-07
Iter: 1407 loss: 6.18793e-07
Iter: 1408 loss: 6.18717252e-07
Iter: 1409 loss: 6.18960712e-07
Iter: 1410 loss: 6.18667627e-07
Iter: 1411 loss: 6.18587194e-07
Iter: 1412 loss: 6.1854189e-07
Iter: 1413 loss: 6.18519437e-07
Iter: 1414 loss: 6.18380113e-07
Iter: 1415 loss: 6.18597255e-07
Iter: 1416 loss: 6.18322929e-07
Iter: 1417 loss: 6.1823539e-07
Iter: 1418 loss: 6.18200204e-07
Iter: 1419 loss: 6.18160811e-07
Iter: 1420 loss: 6.18134e-07
Iter: 1421 loss: 6.18106128e-07
Iter: 1422 loss: 6.17989258e-07
Iter: 1423 loss: 6.1842934e-07
Iter: 1424 loss: 6.1794691e-07
Iter: 1425 loss: 6.17896944e-07
Iter: 1426 loss: 6.17762453e-07
Iter: 1427 loss: 6.20636115e-07
Iter: 1428 loss: 6.17780529e-07
Iter: 1429 loss: 6.1762853e-07
Iter: 1430 loss: 6.18412457e-07
Iter: 1431 loss: 6.17627052e-07
Iter: 1432 loss: 6.17504611e-07
Iter: 1433 loss: 6.17522574e-07
Iter: 1434 loss: 6.17440776e-07
Iter: 1435 loss: 6.17570151e-07
Iter: 1436 loss: 6.17365572e-07
Iter: 1437 loss: 6.17334081e-07
Iter: 1438 loss: 6.17256717e-07
Iter: 1439 loss: 6.1764672e-07
Iter: 1440 loss: 6.17233638e-07
Iter: 1441 loss: 6.17083856e-07
Iter: 1442 loss: 6.17430544e-07
Iter: 1443 loss: 6.17077603e-07
Iter: 1444 loss: 6.16980742e-07
Iter: 1445 loss: 6.17665307e-07
Iter: 1446 loss: 6.17025762e-07
Iter: 1447 loss: 6.1685455e-07
Iter: 1448 loss: 6.168342e-07
Iter: 1449 loss: 6.16819932e-07
Iter: 1450 loss: 6.16647924e-07
Iter: 1451 loss: 6.17202204e-07
Iter: 1452 loss: 6.16656735e-07
Iter: 1453 loss: 6.16529235e-07
Iter: 1454 loss: 6.16914349e-07
Iter: 1455 loss: 6.16505758e-07
Iter: 1456 loss: 6.16454372e-07
Iter: 1457 loss: 6.16867226e-07
Iter: 1458 loss: 6.16403327e-07
Iter: 1459 loss: 6.16327043e-07
Iter: 1460 loss: 6.16318516e-07
Iter: 1461 loss: 6.16285661e-07
Iter: 1462 loss: 6.16148384e-07
Iter: 1463 loss: 6.16782245e-07
Iter: 1464 loss: 6.16140483e-07
Iter: 1465 loss: 6.16043621e-07
Iter: 1466 loss: 6.16031e-07
Iter: 1467 loss: 6.15944373e-07
Iter: 1468 loss: 6.15830061e-07
Iter: 1469 loss: 6.16028501e-07
Iter: 1470 loss: 6.15755596e-07
Iter: 1471 loss: 6.15792715e-07
Iter: 1472 loss: 6.15758836e-07
Iter: 1473 loss: 6.15750309e-07
Iter: 1474 loss: 6.15732461e-07
Iter: 1475 loss: 6.15755653e-07
Iter: 1476 loss: 6.15771569e-07
Iter: 1477 loss: 6.15753436e-07
Iter: 1478 loss: 6.15778504e-07
Iter: 1479 loss: 6.15767249e-07
Iter: 1480 loss: 6.15763383e-07
Iter: 1481 loss: 6.15781e-07
Iter: 1482 loss: 6.15773502e-07
Iter: 1483 loss: 6.15757472e-07
Iter: 1484 loss: 6.15764236e-07
Iter: 1485 loss: 6.15754402e-07
Iter: 1486 loss: 6.15757585e-07
Iter: 1487 loss: 6.15770432e-07
Iter: 1488 loss: 6.15765771e-07
Iter: 1489 loss: 6.15761678e-07
Iter: 1490 loss: 6.15758267e-07
Iter: 1491 loss: 6.15755084e-07
Iter: 1492 loss: 6.15757131e-07
Iter: 1493 loss: 6.15756051e-07
Iter: 1494 loss: 6.15757244e-07
Iter: 1495 loss: 6.15757244e-07
Iter: 1496 loss: 6.15757244e-07
Iter: 1497 loss: 6.15756051e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.4
+ date
Wed Oct 21 14:22:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/300_300_300_1 --function f1 --psi 0 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090888510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f609091d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f609086b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60907c46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090813e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090813510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f609079d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60907b5d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60907b58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60907b59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090716158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090647950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090632620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60906ffd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6090603c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f609071b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60906327b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f609061aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f609061a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60905e1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60905c9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60905c4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60905c98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606f0879d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606f087bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606f0876a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606f087620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606efc9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606efc9d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606ef8f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606ef88158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606f04cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60486409d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f606f049488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60485e4ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60485a0ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.20872742
test_loss: 0.18890281
train_loss: 0.068045154
test_loss: 0.08276043
train_loss: 0.051019512
test_loss: 0.04934639
train_loss: 0.03440699
test_loss: 0.031075137
train_loss: 0.024313744
test_loss: 0.026351351
train_loss: 0.01606439
test_loss: 0.022647277
train_loss: 0.016208153
test_loss: 0.016326662
train_loss: 0.014925674
test_loss: 0.015660588
train_loss: 0.01502788
test_loss: 0.016618412
train_loss: 0.013141917
test_loss: 0.015140294
train_loss: 0.010812102
test_loss: 0.012925728
train_loss: 0.012105867
test_loss: 0.0133673325
train_loss: 0.01011648
test_loss: 0.012481081
train_loss: 0.0106195
test_loss: 0.010895716
train_loss: 0.009410541
test_loss: 0.009314795
train_loss: 0.0084685
test_loss: 0.009893482
train_loss: 0.008671411
test_loss: 0.009878156
train_loss: 0.008728516
test_loss: 0.009675088
train_loss: 0.00939266
test_loss: 0.01097463
train_loss: 0.011582888
test_loss: 0.010877377
train_loss: 0.011071612
test_loss: 0.01320959
train_loss: 0.008988476
test_loss: 0.008614115
train_loss: 0.009449845
test_loss: 0.009407946
train_loss: 0.009268128
test_loss: 0.010214447
train_loss: 0.009561067
test_loss: 0.009204033
train_loss: 0.010242017
test_loss: 0.012326388
train_loss: 0.007682588
test_loss: 0.008902089
train_loss: 0.0070922207
test_loss: 0.008883099
train_loss: 0.008872329
test_loss: 0.008474584
train_loss: 0.007924541
test_loss: 0.008864081
train_loss: 0.007154837
test_loss: 0.009263131
train_loss: 0.009126032
test_loss: 0.007731152
train_loss: 0.009989722
test_loss: 0.009531968
train_loss: 0.009798058
test_loss: 0.009282159
train_loss: 0.008089025
test_loss: 0.008770944
train_loss: 0.009157786
test_loss: 0.007765141
train_loss: 0.0089976275
test_loss: 0.009700893
train_loss: 0.011280595
test_loss: 0.009397688
train_loss: 0.009971358
test_loss: 0.008813958
train_loss: 0.008531626
test_loss: 0.011750375
train_loss: 0.0070583588
test_loss: 0.01001256
train_loss: 0.010181449
test_loss: 0.012202003
train_loss: 0.010196121
test_loss: 0.010221526
train_loss: 0.008012366
test_loss: 0.00822279
train_loss: 0.0075671794
test_loss: 0.0074775913
train_loss: 0.008185118
test_loss: 0.008107778
train_loss: 0.006936058
test_loss: 0.008879559
train_loss: 0.007773539
test_loss: 0.008649819
train_loss: 0.00755644
test_loss: 0.009199922
train_loss: 0.008418896
test_loss: 0.008637011
train_loss: 0.0077366163
test_loss: 0.0073035476
train_loss: 0.007851082
test_loss: 0.01220799
train_loss: 0.007378394
test_loss: 0.0069241305
train_loss: 0.007018417
test_loss: 0.007831642
train_loss: 0.007209966
test_loss: 0.008507947
train_loss: 0.008503135
test_loss: 0.008407784
train_loss: 0.007031505
test_loss: 0.008854058
train_loss: 0.006628915
test_loss: 0.0091170315
train_loss: 0.0108591365
test_loss: 0.01004231
train_loss: 0.007173881
test_loss: 0.0067829113
train_loss: 0.008821869
test_loss: 0.0075730346
train_loss: 0.008488893
test_loss: 0.009403789
train_loss: 0.008090756
test_loss: 0.009075487
train_loss: 0.0065309885
test_loss: 0.0072625685
train_loss: 0.009495073
test_loss: 0.008005113
train_loss: 0.0071104066
test_loss: 0.008565786
train_loss: 0.0060397754
test_loss: 0.006782623
train_loss: 0.0070364666
test_loss: 0.0074726166
train_loss: 0.006117938
test_loss: 0.0071345307
train_loss: 0.009758755
test_loss: 0.009565067
train_loss: 0.0077913785
test_loss: 0.009720284
train_loss: 0.008531654
test_loss: 0.008862714
train_loss: 0.010527643
test_loss: 0.010964027
train_loss: 0.008841264
test_loss: 0.009020348
train_loss: 0.0075068674
test_loss: 0.008202192
train_loss: 0.0067659807
test_loss: 0.008152663
train_loss: 0.007088872
test_loss: 0.007038209
train_loss: 0.0065797498
test_loss: 0.0074550514
train_loss: 0.007510331
test_loss: 0.008685225
train_loss: 0.0072898385
test_loss: 0.007930386
train_loss: 0.006198352
test_loss: 0.0072230394
train_loss: 0.006142411
test_loss: 0.006647858
train_loss: 0.007517608
test_loss: 0.0071263616
train_loss: 0.008666351
test_loss: 0.008053516
train_loss: 0.006646791
test_loss: 0.0077722236
train_loss: 0.006630582
test_loss: 0.006425538
train_loss: 0.0102561265
test_loss: 0.009068298
train_loss: 0.007677938
test_loss: 0.009667441
train_loss: 0.00630719
test_loss: 0.006544745
train_loss: 0.009065305
test_loss: 0.008412808
train_loss: 0.008195079
test_loss: 0.008673158
train_loss: 0.0057029137
test_loss: 0.0077170064
train_loss: 0.0074552805
test_loss: 0.0075132814
train_loss: 0.007551247
test_loss: 0.0069341664
train_loss: 0.007873169
test_loss: 0.008274232
train_loss: 0.006429212
test_loss: 0.0067346445
train_loss: 0.008331273
test_loss: 0.007291838
train_loss: 0.006365681
test_loss: 0.0063926363
train_loss: 0.007904375
test_loss: 0.009398111
train_loss: 0.008146565
test_loss: 0.008557207
train_loss: 0.0068952115
test_loss: 0.009035617
train_loss: 0.0065378314
test_loss: 0.008110817
train_loss: 0.007821106
test_loss: 0.0065513556
train_loss: 0.0068500238
test_loss: 0.0066578626
train_loss: 0.006703176
test_loss: 0.007295186
train_loss: 0.00625363
test_loss: 0.00748939
train_loss: 0.007251226
test_loss: 0.008146738
train_loss: 0.0074585127
test_loss: 0.008442246
train_loss: 0.008595383
test_loss: 0.0075154477
train_loss: 0.008115757
test_loss: 0.008123233
train_loss: 0.009228764
test_loss: 0.0073960135
train_loss: 0.010422882
test_loss: 0.008693501
train_loss: 0.0069259945
test_loss: 0.007205574
train_loss: 0.0071319686
test_loss: 0.007607532
train_loss: 0.008261762
test_loss: 0.007275671
train_loss: 0.008302253
test_loss: 0.0084925145
train_loss: 0.0076533835
test_loss: 0.009833791
train_loss: 0.0077282423
test_loss: 0.0067328117
train_loss: 0.0071923668
test_loss: 0.0068801595
train_loss: 0.0072912457
test_loss: 0.0067166034
train_loss: 0.007142771
test_loss: 0.0073474697
train_loss: 0.0075549763
test_loss: 0.00717804
train_loss: 0.0075458735
test_loss: 0.008009126
train_loss: 0.0080534965
test_loss: 0.008556047
train_loss: 0.008779319
test_loss: 0.0079969065
train_loss: 0.0057467846
test_loss: 0.0064014313
train_loss: 0.0064373715
test_loss: 0.006807694
train_loss: 0.0074630193
test_loss: 0.0076453644
train_loss: 0.008276716
test_loss: 0.007202833
train_loss: 0.0072056223
test_loss: 0.007419507
train_loss: 0.0066572176
test_loss: 0.007928493
train_loss: 0.0063646627
test_loss: 0.006636821
train_loss: 0.0077901967
test_loss: 0.006531296
train_loss: 0.0075898767
test_loss: 0.0071074823
train_loss: 0.007790772
test_loss: 0.008014868
train_loss: 0.0060235774
test_loss: 0.006490846
train_loss: 0.0105374865
test_loss: 0.010954375
train_loss: 0.0089421
test_loss: 0.0078027397
train_loss: 0.00915683
test_loss: 0.008417166
train_loss: 0.007854873
test_loss: 0.0085025765
train_loss: 0.0068173837
test_loss: 0.0065261205
train_loss: 0.008041456
test_loss: 0.008663225
train_loss: 0.0073769735
test_loss: 0.007856311
train_loss: 0.008820152
test_loss: 0.011174645
train_loss: 0.008664572
test_loss: 0.0074987
train_loss: 0.008491771
test_loss: 0.008358163
train_loss: 0.0071337027
test_loss: 0.0065082535
train_loss: 0.0073087076
test_loss: 0.0073080664
train_loss: 0.008089844
test_loss: 0.007829801
train_loss: 0.0078730555
test_loss: 0.0077403164
train_loss: 0.0070293313
test_loss: 0.0067272326
train_loss: 0.0070700273
test_loss: 0.0063628987
train_loss: 0.00785435
test_loss: 0.00878279
train_loss: 0.008119877
test_loss: 0.00849028
train_loss: 0.0068255104
test_loss: 0.008413637
train_loss: 0.008847116
test_loss: 0.008305935
train_loss: 0.0063858717
test_loss: 0.008028203
train_loss: 0.0080833435
test_loss: 0.008389083
train_loss: 0.009299779
test_loss: 0.0096630845
train_loss: 0.0076110335
test_loss: 0.0075273314
train_loss: 0.008207118
test_loss: 0.006785148
train_loss: 0.007513038
test_loss: 0.008246574
train_loss: 0.0064586126
test_loss: 0.0066353856
train_loss: 0.0060165348
test_loss: 0.0070140185
train_loss: 0.0076617347
test_loss: 0.0074469396
train_loss: 0.006693342
test_loss: 0.0066511524
train_loss: 0.006610619
test_loss: 0.0071841516
train_loss: 0.0077220756
test_loss: 0.010874885
train_loss: 0.007244843
test_loss: 0.008087229
train_loss: 0.0057359748
test_loss: 0.006891572
train_loss: 0.0081759095
test_loss: 0.0069870003
train_loss: 0.0067890054
test_loss: 0.007986112
train_loss: 0.007555412
test_loss: 0.0065123243
train_loss: 0.007984267
test_loss: 0.009954335
train_loss: 0.008297177
test_loss: 0.008968594
train_loss: 0.009216442
test_loss: 0.008656394
train_loss: 0.009050891
test_loss: 0.007920222
train_loss: 0.007831555
test_loss: 0.008905709
train_loss: 0.0064738514
test_loss: 0.006526374
train_loss: 0.0066187233
test_loss: 0.007695753
train_loss: 0.006237707
test_loss: 0.006693731
train_loss: 0.008588003
test_loss: 0.009148438
train_loss: 0.0061626295
test_loss: 0.006835959
train_loss: 0.0069089634
test_loss: 0.0075034094
train_loss: 0.011281594
test_loss: 0.009489423
train_loss: 0.006036386
test_loss: 0.0063375225
train_loss: 0.0071755974
test_loss: 0.006075463
train_loss: 0.0060531395
test_loss: 0.00658513
train_loss: 0.005622782
test_loss: 0.0060446803
train_loss: 0.0071527064
test_loss: 0.007732398
train_loss: 0.0067309383
test_loss: 0.0075264135
train_loss: 0.008455024
test_loss: 0.0085411705
train_loss: 0.0064973272
test_loss: 0.0069622155
train_loss: 0.0070117433
test_loss: 0.0060463003
train_loss: 0.006754242
test_loss: 0.0067551574
train_loss: 0.0062589594
test_loss: 0.008170876
train_loss: 0.007417298
test_loss: 0.006690497
train_loss: 0.007034218
test_loss: 0.0068995166
train_loss: 0.007518013
test_loss: 0.008251477
train_loss: 0.006725745
test_loss: 0.006553806
train_loss: 0.007582272
test_loss: 0.0077563915
train_loss: 0.0069119413
test_loss: 0.00697002
train_loss: 0.007915962
test_loss: 0.008601982
train_loss: 0.008025276
test_loss: 0.006992575
train_loss: 0.007733562
test_loss: 0.008853301
train_loss: 0.0066229994
test_loss: 0.0076031196
train_loss: 0.006569509
test_loss: 0.007059185
train_loss: 0.0068406374
test_loss: 0.0066327094
train_loss: 0.008068558
test_loss: 0.007861589
train_loss: 0.0088012805
test_loss: 0.006618355
train_loss: 0.0073958077
test_loss: 0.0077321604
train_loss: 0.008317079
test_loss: 0.0076611475
train_loss: 0.0063936124
test_loss: 0.007997449
train_loss: 0.008610602
test_loss: 0.007028836
train_loss: 0.007482725
test_loss: 0.0070781335
train_loss: 0.0059038685
test_loss: 0.007949126
train_loss: 0.006470247
test_loss: 0.0080749625
train_loss: 0.0065787053
test_loss: 0.007750476
train_loss: 0.006684088
test_loss: 0.007792245
train_loss: 0.0070909373
test_loss: 0.008074208
train_loss: 0.008419298
test_loss: 0.009696983
train_loss: 0.0056632287
test_loss: 0.006876974
train_loss: 0.009031351
test_loss: 0.007822277
train_loss: 0.0068852603
test_loss: 0.0069076186
train_loss: 0.00588044
test_loss: 0.008068695
train_loss: 0.006903019
test_loss: 0.0070435554
train_loss: 0.006601422
test_loss: 0.0070850067
train_loss: 0.007847646
test_loss: 0.007989908
train_loss: 0.006970734
test_loss: 0.006808796
train_loss: 0.007906539
test_loss: 0.008382834
train_loss: 0.0064063068
test_loss: 0.007781378
train_loss: 0.0072277985
test_loss: 0.0077609937
train_loss: 0.0065185423
test_loss: 0.007936238
train_loss: 0.00758837
test_loss: 0.0068633594
train_loss: 0.006927509
test_loss: 0.0063838684
train_loss: 0.00631133
test_loss: 0.007913471
train_loss: 0.0076213637
test_loss: 0.0077493414
train_loss: 0.0070338864
test_loss: 0.008102359
train_loss: 0.009329438
test_loss: 0.009576277
train_loss: 0.007048591
test_loss: 0.0060204035
train_loss: 0.008556387
test_loss: 0.0076601347
train_loss: 0.0072404896
test_loss: 0.009190103
train_loss: 0.007678191
test_loss: 0.007250055
train_loss: 0.007335485
test_loss: 0.006712199
train_loss: 0.007372286
test_loss: 0.006526644
train_loss: 0.006581875
test_loss: 0.0060034106
train_loss: 0.007977284
test_loss: 0.0070911893
train_loss: 0.008062888
test_loss: 0.0075691855
train_loss: 0.006521521
test_loss: 0.007267384
train_loss: 0.008370779
test_loss: 0.0067328997
train_loss: 0.0057040523
test_loss: 0.0067384057
train_loss: 0.0061733
test_loss: 0.0063020224
train_loss: 0.005312275
test_loss: 0.00473863
train_loss: 0.0073352074
test_loss: 0.006599931
train_loss: 0.007846827
test_loss: 0.007560579
train_loss: 0.0071931337
test_loss: 0.006496719
train_loss: 0.008051438
test_loss: 0.007323966
train_loss: 0.0063601844
test_loss: 0.0068160375
train_loss: 0.0065104943
test_loss: 0.008601179
train_loss: 0.0069702677
test_loss: 0.0095866
train_loss: 0.008417999
test_loss: 0.008713352
train_loss: 0.00859236
test_loss: 0.009023456
train_loss: 0.006239454
test_loss: 0.0075598746
train_loss: 0.0065661753
test_loss: 0.006622642
train_loss: 0.006088611
test_loss: 0.0077587194
train_loss: 0.006005831
test_loss: 0.0073365476
train_loss: 0.0064887116
test_loss: 0.0066512064
train_loss: 0.0067483094
test_loss: 0.0075172726
train_loss: 0.009098213
test_loss: 0.00996621
train_loss: 0.0057813693
test_loss: 0.006636787
train_loss: 0.0059623416
test_loss: 0.0055634067
train_loss: 0.0072701657
test_loss: 0.009130294
train_loss: 0.0072498205
test_loss: 0.007730228
train_loss: 0.0070437137
test_loss: 0.008341713
train_loss: 0.007519734
test_loss: 0.00698932
train_loss: 0.006080169
test_loss: 0.007348281
train_loss: 0.0051974664
test_loss: 0.005649103
train_loss: 0.0074953213
test_loss: 0.007000884
train_loss: 0.0072199553
test_loss: 0.0072241067
train_loss: 0.0074159415
test_loss: 0.0077754967
train_loss: 0.006129588
test_loss: 0.007546826
train_loss: 0.006844315
test_loss: 0.0057884273
train_loss: 0.006597747
test_loss: 0.007759213
train_loss: 0.0058037136
test_loss: 0.0059869196
train_loss: 0.0060951095
test_loss: 0.0070312824
train_loss: 0.0056309174
test_loss: 0.006464141
train_loss: 0.007407102
test_loss: 0.0062195733
train_loss: 0.005515392
test_loss: 0.0068506827
train_loss: 0.0066590942
test_loss: 0.00800936
train_loss: 0.0063605052
test_loss: 0.005927349
train_loss: 0.0065990677
test_loss: 0.0053012334
train_loss: 0.0060062874
test_loss: 0.0056138868
train_loss: 0.0069039855
test_loss: 0.007816476
train_loss: 0.006045164
test_loss: 0.006544653
train_loss: 0.005849502
test_loss: 0.0067210076
train_loss: 0.0072184918
test_loss: 0.006708661
train_loss: 0.007000356
test_loss: 0.005578829
train_loss: 0.0074603814
test_loss: 0.00794285
train_loss: 0.007557681
test_loss: 0.008299833
train_loss: 0.0061899787
test_loss: 0.005816715
train_loss: 0.0065890774
test_loss: 0.009061121
train_loss: 0.0059899553
test_loss: 0.0075360253
train_loss: 0.0060664713
test_loss: 0.007030888
train_loss: 0.0058401246
test_loss: 0.0064230175
train_loss: 0.00805316
test_loss: 0.0075641843
train_loss: 0.008075331
test_loss: 0.007464458
train_loss: 0.0081180055
test_loss: 0.008958629
train_loss: 0.008981779
test_loss: 0.007157031
train_loss: 0.0076278104
test_loss: 0.006922642
train_loss: 0.00963879
test_loss: 0.008986257
train_loss: 0.0068534496
test_loss: 0.0078124744
train_loss: 0.0057415674
test_loss: 0.0055960114
train_loss: 0.0066658673
test_loss: 0.007294874
train_loss: 0.0065649403
test_loss: 0.0069173346
train_loss: 0.0067834603
test_loss: 0.0067394436
train_loss: 0.006323036
test_loss: 0.0069367867
train_loss: 0.006655381
test_loss: 0.008833119
train_loss: 0.006277097
test_loss: 0.007115998
train_loss: 0.0074664284
test_loss: 0.0077007418
train_loss: 0.007928554
test_loss: 0.0060237767
train_loss: 0.0094894
test_loss: 0.0079863025
train_loss: 0.0069058654
test_loss: 0.0074178567
train_loss: 0.007401037
test_loss: 0.00938887
train_loss: 0.0062824525
test_loss: 0.006448144
train_loss: 0.0072041173
test_loss: 0.0077191014
train_loss: 0.006905186
test_loss: 0.0063440744
train_loss: 0.008383147
test_loss: 0.0069724414
train_loss: 0.007469603
test_loss: 0.008103307
train_loss: 0.007136371
test_loss: 0.006022502
train_loss: 0.006144631
test_loss: 0.0075010816
train_loss: 0.0056911656
test_loss: 0.0078046382
train_loss: 0.0079365745
test_loss: 0.008039293
train_loss: 0.008568669
test_loss: 0.007735611
train_loss: 0.008029622
test_loss: 0.0071672024
train_loss: 0.006763625
test_loss: 0.0067672795
train_loss: 0.0058573433
test_loss: 0.007082764
train_loss: 0.0065755406
test_loss: 0.0068377345
train_loss: 0.00596326
test_loss: 0.0058034626
train_loss: 0.008900838
test_loss: 0.00846855
train_loss: 0.006733152
test_loss: 0.006716144
train_loss: 0.0066538663
test_loss: 0.006395754
train_loss: 0.008183698
test_loss: 0.008051383
train_loss: 0.0062361737
test_loss: 0.007527544
train_loss: 0.008694903
test_loss: 0.0057344153
train_loss: 0.0060387165/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0072429795
train_loss: 0.007128897
test_loss: 0.007441837
train_loss: 0.0074600503
test_loss: 0.008160901
train_loss: 0.0051120543
test_loss: 0.0068262387
train_loss: 0.0058884015
test_loss: 0.0052689193
train_loss: 0.006531094
test_loss: 0.006669457
train_loss: 0.0065822634
test_loss: 0.005810184
train_loss: 0.008942133
test_loss: 0.007245229
train_loss: 0.007614551
test_loss: 0.008319966
train_loss: 0.0066486252
test_loss: 0.007700921
train_loss: 0.00725646
test_loss: 0.0066073104
train_loss: 0.006766948
test_loss: 0.0069215777
train_loss: 0.006544228
test_loss: 0.007128981
train_loss: 0.00689883
test_loss: 0.0060078534
train_loss: 0.0053764665
test_loss: 0.006352309
train_loss: 0.0054283678
test_loss: 0.005936882
train_loss: 0.005087239
test_loss: 0.0060640574
train_loss: 0.0057489946
test_loss: 0.005204129
train_loss: 0.008058974
test_loss: 0.0083003165
train_loss: 0.0071192887
test_loss: 0.006848555
train_loss: 0.0066938438
test_loss: 0.0067433347
train_loss: 0.0053874403
test_loss: 0.0061774943
train_loss: 0.006974621
test_loss: 0.0063945106
train_loss: 0.008258961
test_loss: 0.0079707615
train_loss: 0.0068402495
test_loss: 0.007025044
train_loss: 0.007232398
test_loss: 0.006710034
train_loss: 0.00685
test_loss: 0.007321919
train_loss: 0.0065861926
test_loss: 0.0077695157
train_loss: 0.0076235994
test_loss: 0.0063760784
train_loss: 0.007702595
test_loss: 0.0064712344
train_loss: 0.0067503965
test_loss: 0.00809836
train_loss: 0.006717662
test_loss: 0.005910815
train_loss: 0.00457916
test_loss: 0.0061876085
train_loss: 0.008954086
test_loss: 0.007435194
train_loss: 0.006798192
test_loss: 0.008635703
train_loss: 0.009775282
test_loss: 0.0077931336
train_loss: 0.0058005033
test_loss: 0.0059943344
train_loss: 0.007128296
test_loss: 0.007226123
train_loss: 0.007295292
test_loss: 0.006914349
train_loss: 0.0062261885
test_loss: 0.0076432866
train_loss: 0.008589193
test_loss: 0.007139187
train_loss: 0.0077142185
test_loss: 0.007897948
train_loss: 0.009280549
test_loss: 0.008953367
train_loss: 0.006969085
test_loss: 0.0067203697
train_loss: 0.008188476
test_loss: 0.008522287
train_loss: 0.0068190573
test_loss: 0.0060093775
train_loss: 0.007848354
test_loss: 0.0072818245
train_loss: 0.00638867
test_loss: 0.006899582
train_loss: 0.0069265803
test_loss: 0.0062979655
train_loss: 0.0064305402
test_loss: 0.0057002096
train_loss: 0.0068913815
test_loss: 0.0076004937
train_loss: 0.0051635886
test_loss: 0.007412105
train_loss: 0.0077821882
test_loss: 0.008258573
train_loss: 0.0068072146
test_loss: 0.0068506226
train_loss: 0.006311002
test_loss: 0.0073498385
train_loss: 0.005013329
test_loss: 0.0056931996
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24dec620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24e05730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24d8d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24e98378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24e980d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24d656a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24d10ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24cfd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24cfd7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24c901e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24cb52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24c50c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24c71ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24c1c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24bdae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24f8ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24bcf950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24bcff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24bcf0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24b087b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24b25620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24ac60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24b25f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24af98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f24ab0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f1189bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f1189b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f118be8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f11883730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f118c7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f11848b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f117f4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f11791840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f117e9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f117ceea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4f11787f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.89990923e-05
Iter: 2 loss: 0.000241211033
Iter: 3 loss: 4.09290296e-05
Iter: 4 loss: 3.08190902e-05
Iter: 5 loss: 3.06999464e-05
Iter: 6 loss: 2.76891951e-05
Iter: 7 loss: 2.65847302e-05
Iter: 8 loss: 2.49126351e-05
Iter: 9 loss: 2.11592505e-05
Iter: 10 loss: 1.95246757e-05
Iter: 11 loss: 1.76021604e-05
Iter: 12 loss: 1.38766027e-05
Iter: 13 loss: 5.68527103e-05
Iter: 14 loss: 1.38202877e-05
Iter: 15 loss: 1.25729239e-05
Iter: 16 loss: 1.4559153e-05
Iter: 17 loss: 1.19915449e-05
Iter: 18 loss: 1.06548368e-05
Iter: 19 loss: 1.29176678e-05
Iter: 20 loss: 1.00559819e-05
Iter: 21 loss: 8.75662408e-06
Iter: 22 loss: 1.32848672e-05
Iter: 23 loss: 8.414685e-06
Iter: 24 loss: 7.8747089e-06
Iter: 25 loss: 1.35273222e-05
Iter: 26 loss: 7.86043438e-06
Iter: 27 loss: 7.46944261e-06
Iter: 28 loss: 7.67807614e-06
Iter: 29 loss: 7.2123903e-06
Iter: 30 loss: 6.79185268e-06
Iter: 31 loss: 7.50474328e-06
Iter: 32 loss: 6.6024586e-06
Iter: 33 loss: 6.37205085e-06
Iter: 34 loss: 7.29769545e-06
Iter: 35 loss: 6.32035471e-06
Iter: 36 loss: 6.09023709e-06
Iter: 37 loss: 5.85930866e-06
Iter: 38 loss: 5.81249833e-06
Iter: 39 loss: 5.80857795e-06
Iter: 40 loss: 5.70431575e-06
Iter: 41 loss: 5.55462884e-06
Iter: 42 loss: 5.71424152e-06
Iter: 43 loss: 5.47154059e-06
Iter: 44 loss: 5.40066776e-06
Iter: 45 loss: 5.53448081e-06
Iter: 46 loss: 5.3705453e-06
Iter: 47 loss: 5.27533211e-06
Iter: 48 loss: 5.17228273e-06
Iter: 49 loss: 5.15639476e-06
Iter: 50 loss: 5.01934028e-06
Iter: 51 loss: 5.79788684e-06
Iter: 52 loss: 5.00046053e-06
Iter: 53 loss: 4.8695847e-06
Iter: 54 loss: 4.87450961e-06
Iter: 55 loss: 4.7659164e-06
Iter: 56 loss: 4.63799097e-06
Iter: 57 loss: 5.80681535e-06
Iter: 58 loss: 4.63239076e-06
Iter: 59 loss: 4.52957556e-06
Iter: 60 loss: 4.5662473e-06
Iter: 61 loss: 4.45789e-06
Iter: 62 loss: 4.3566356e-06
Iter: 63 loss: 4.85404507e-06
Iter: 64 loss: 4.33896184e-06
Iter: 65 loss: 4.23400115e-06
Iter: 66 loss: 4.27679106e-06
Iter: 67 loss: 4.16163948e-06
Iter: 68 loss: 4.04716184e-06
Iter: 69 loss: 5.01923751e-06
Iter: 70 loss: 4.04065759e-06
Iter: 71 loss: 3.97741223e-06
Iter: 72 loss: 4.30724594e-06
Iter: 73 loss: 3.96772066e-06
Iter: 74 loss: 3.92109405e-06
Iter: 75 loss: 3.91992171e-06
Iter: 76 loss: 3.89793604e-06
Iter: 77 loss: 3.83966744e-06
Iter: 78 loss: 4.26428e-06
Iter: 79 loss: 3.8273356e-06
Iter: 80 loss: 3.76982189e-06
Iter: 81 loss: 4.64381446e-06
Iter: 82 loss: 3.76957064e-06
Iter: 83 loss: 3.73523972e-06
Iter: 84 loss: 3.71136548e-06
Iter: 85 loss: 3.69902182e-06
Iter: 86 loss: 3.64633911e-06
Iter: 87 loss: 3.94949029e-06
Iter: 88 loss: 3.63927029e-06
Iter: 89 loss: 3.58844045e-06
Iter: 90 loss: 3.63638628e-06
Iter: 91 loss: 3.55915518e-06
Iter: 92 loss: 3.49067295e-06
Iter: 93 loss: 3.69719783e-06
Iter: 94 loss: 3.46970864e-06
Iter: 95 loss: 3.41698842e-06
Iter: 96 loss: 3.60862168e-06
Iter: 97 loss: 3.40424594e-06
Iter: 98 loss: 3.35324239e-06
Iter: 99 loss: 3.49075094e-06
Iter: 100 loss: 3.33637468e-06
Iter: 101 loss: 3.29016666e-06
Iter: 102 loss: 3.44739465e-06
Iter: 103 loss: 3.27757584e-06
Iter: 104 loss: 3.25252108e-06
Iter: 105 loss: 3.25242763e-06
Iter: 106 loss: 3.24405664e-06
Iter: 107 loss: 3.24237362e-06
Iter: 108 loss: 3.23382437e-06
Iter: 109 loss: 3.20921663e-06
Iter: 110 loss: 3.32660534e-06
Iter: 111 loss: 3.20085655e-06
Iter: 112 loss: 3.17384206e-06
Iter: 113 loss: 3.44211639e-06
Iter: 114 loss: 3.17312788e-06
Iter: 115 loss: 3.14790896e-06
Iter: 116 loss: 3.11975646e-06
Iter: 117 loss: 3.11590111e-06
Iter: 118 loss: 3.08015865e-06
Iter: 119 loss: 3.24771077e-06
Iter: 120 loss: 3.0736669e-06
Iter: 121 loss: 3.04213791e-06
Iter: 122 loss: 3.177488e-06
Iter: 123 loss: 3.03569914e-06
Iter: 124 loss: 3.00953889e-06
Iter: 125 loss: 3.08404651e-06
Iter: 126 loss: 3.00141573e-06
Iter: 127 loss: 2.98002e-06
Iter: 128 loss: 3.03689262e-06
Iter: 129 loss: 2.97289398e-06
Iter: 130 loss: 2.95049426e-06
Iter: 131 loss: 2.97234033e-06
Iter: 132 loss: 2.93783796e-06
Iter: 133 loss: 2.90648745e-06
Iter: 134 loss: 2.95400969e-06
Iter: 135 loss: 2.89151922e-06
Iter: 136 loss: 2.86540399e-06
Iter: 137 loss: 3.08980634e-06
Iter: 138 loss: 2.86389104e-06
Iter: 139 loss: 2.85892725e-06
Iter: 140 loss: 2.85412079e-06
Iter: 141 loss: 2.8453926e-06
Iter: 142 loss: 2.82820974e-06
Iter: 143 loss: 3.169411e-06
Iter: 144 loss: 2.82818223e-06
Iter: 145 loss: 2.81598818e-06
Iter: 146 loss: 2.85339729e-06
Iter: 147 loss: 2.81251346e-06
Iter: 148 loss: 2.79607139e-06
Iter: 149 loss: 2.81049188e-06
Iter: 150 loss: 2.78664857e-06
Iter: 151 loss: 2.76938204e-06
Iter: 152 loss: 2.76530955e-06
Iter: 153 loss: 2.7544138e-06
Iter: 154 loss: 2.73038313e-06
Iter: 155 loss: 2.89255013e-06
Iter: 156 loss: 2.72785883e-06
Iter: 157 loss: 2.70936857e-06
Iter: 158 loss: 2.79106962e-06
Iter: 159 loss: 2.70549845e-06
Iter: 160 loss: 2.69118436e-06
Iter: 161 loss: 2.71185854e-06
Iter: 162 loss: 2.68422536e-06
Iter: 163 loss: 2.66608595e-06
Iter: 164 loss: 2.69260249e-06
Iter: 165 loss: 2.65735116e-06
Iter: 166 loss: 2.64026244e-06
Iter: 167 loss: 2.75538218e-06
Iter: 168 loss: 2.63845595e-06
Iter: 169 loss: 2.62605249e-06
Iter: 170 loss: 2.62824096e-06
Iter: 171 loss: 2.61660625e-06
Iter: 172 loss: 2.62062258e-06
Iter: 173 loss: 2.61027662e-06
Iter: 174 loss: 2.60367187e-06
Iter: 175 loss: 2.59284434e-06
Iter: 176 loss: 2.59282751e-06
Iter: 177 loss: 2.58239743e-06
Iter: 178 loss: 2.57674833e-06
Iter: 179 loss: 2.57194915e-06
Iter: 180 loss: 2.56153089e-06
Iter: 181 loss: 2.5613349e-06
Iter: 182 loss: 2.55384566e-06
Iter: 183 loss: 2.54007978e-06
Iter: 184 loss: 2.83093345e-06
Iter: 185 loss: 2.53974304e-06
Iter: 186 loss: 2.52478958e-06
Iter: 187 loss: 2.63034303e-06
Iter: 188 loss: 2.52352675e-06
Iter: 189 loss: 2.5114191e-06
Iter: 190 loss: 2.57043075e-06
Iter: 191 loss: 2.50946664e-06
Iter: 192 loss: 2.50077892e-06
Iter: 193 loss: 2.52864834e-06
Iter: 194 loss: 2.4981307e-06
Iter: 195 loss: 2.48999436e-06
Iter: 196 loss: 2.50057883e-06
Iter: 197 loss: 2.48585025e-06
Iter: 198 loss: 2.47554863e-06
Iter: 199 loss: 2.49905111e-06
Iter: 200 loss: 2.47188837e-06
Iter: 201 loss: 2.46258014e-06
Iter: 202 loss: 2.50101539e-06
Iter: 203 loss: 2.46046238e-06
Iter: 204 loss: 2.45199817e-06
Iter: 205 loss: 2.47438811e-06
Iter: 206 loss: 2.44910188e-06
Iter: 207 loss: 2.43751788e-06
Iter: 208 loss: 2.55314762e-06
Iter: 209 loss: 2.4371509e-06
Iter: 210 loss: 2.43339696e-06
Iter: 211 loss: 2.42436909e-06
Iter: 212 loss: 2.53717053e-06
Iter: 213 loss: 2.42368696e-06
Iter: 214 loss: 2.41462294e-06
Iter: 215 loss: 2.4448118e-06
Iter: 216 loss: 2.41231101e-06
Iter: 217 loss: 2.39972269e-06
Iter: 218 loss: 2.42109081e-06
Iter: 219 loss: 2.39397423e-06
Iter: 220 loss: 2.38493863e-06
Iter: 221 loss: 2.38644816e-06
Iter: 222 loss: 2.37799145e-06
Iter: 223 loss: 2.36692495e-06
Iter: 224 loss: 2.45121123e-06
Iter: 225 loss: 2.36615369e-06
Iter: 226 loss: 2.35605944e-06
Iter: 227 loss: 2.37040604e-06
Iter: 228 loss: 2.35125663e-06
Iter: 229 loss: 2.34137042e-06
Iter: 230 loss: 2.38059e-06
Iter: 231 loss: 2.33914648e-06
Iter: 232 loss: 2.33085757e-06
Iter: 233 loss: 2.34196523e-06
Iter: 234 loss: 2.32688e-06
Iter: 235 loss: 2.31666741e-06
Iter: 236 loss: 2.3397356e-06
Iter: 237 loss: 2.31281297e-06
Iter: 238 loss: 2.30257228e-06
Iter: 239 loss: 2.33537162e-06
Iter: 240 loss: 2.29966236e-06
Iter: 241 loss: 2.30502701e-06
Iter: 242 loss: 2.29721672e-06
Iter: 243 loss: 2.29467332e-06
Iter: 244 loss: 2.28688032e-06
Iter: 245 loss: 2.3082639e-06
Iter: 246 loss: 2.28278577e-06
Iter: 247 loss: 2.27329315e-06
Iter: 248 loss: 2.29886e-06
Iter: 249 loss: 2.27053e-06
Iter: 250 loss: 2.26490897e-06
Iter: 251 loss: 2.2648419e-06
Iter: 252 loss: 2.26033876e-06
Iter: 253 loss: 2.2561544e-06
Iter: 254 loss: 2.25521899e-06
Iter: 255 loss: 2.24808036e-06
Iter: 256 loss: 2.24717655e-06
Iter: 257 loss: 2.24230052e-06
Iter: 258 loss: 2.23658981e-06
Iter: 259 loss: 2.23636494e-06
Iter: 260 loss: 2.23217148e-06
Iter: 261 loss: 2.23099028e-06
Iter: 262 loss: 2.22826475e-06
Iter: 263 loss: 2.22239714e-06
Iter: 264 loss: 2.24680275e-06
Iter: 265 loss: 2.22087579e-06
Iter: 266 loss: 2.2154436e-06
Iter: 267 loss: 2.21868777e-06
Iter: 268 loss: 2.21167738e-06
Iter: 269 loss: 2.2051895e-06
Iter: 270 loss: 2.24967926e-06
Iter: 271 loss: 2.20454694e-06
Iter: 272 loss: 2.20057927e-06
Iter: 273 loss: 2.21325263e-06
Iter: 274 loss: 2.1993169e-06
Iter: 275 loss: 2.19730646e-06
Iter: 276 loss: 2.19657181e-06
Iter: 277 loss: 2.19510366e-06
Iter: 278 loss: 2.19068806e-06
Iter: 279 loss: 2.19576464e-06
Iter: 280 loss: 2.18724335e-06
Iter: 281 loss: 2.18198875e-06
Iter: 282 loss: 2.24675159e-06
Iter: 283 loss: 2.18186e-06
Iter: 284 loss: 2.17716479e-06
Iter: 285 loss: 2.18903801e-06
Iter: 286 loss: 2.17551496e-06
Iter: 287 loss: 2.17162778e-06
Iter: 288 loss: 2.16981175e-06
Iter: 289 loss: 2.16785861e-06
Iter: 290 loss: 2.16201079e-06
Iter: 291 loss: 2.1802075e-06
Iter: 292 loss: 2.16024591e-06
Iter: 293 loss: 2.15445652e-06
Iter: 294 loss: 2.19308549e-06
Iter: 295 loss: 2.15392311e-06
Iter: 296 loss: 2.14933243e-06
Iter: 297 loss: 2.14904094e-06
Iter: 298 loss: 2.14566398e-06
Iter: 299 loss: 2.13962244e-06
Iter: 300 loss: 2.18028845e-06
Iter: 301 loss: 2.13898716e-06
Iter: 302 loss: 2.13491967e-06
Iter: 303 loss: 2.13840121e-06
Iter: 304 loss: 2.13243175e-06
Iter: 305 loss: 2.12718351e-06
Iter: 306 loss: 2.14636111e-06
Iter: 307 loss: 2.12591112e-06
Iter: 308 loss: 2.12529085e-06
Iter: 309 loss: 2.12408895e-06
Iter: 310 loss: 2.12197347e-06
Iter: 311 loss: 2.11739325e-06
Iter: 312 loss: 2.20688935e-06
Iter: 313 loss: 2.11739462e-06
Iter: 314 loss: 2.11422025e-06
Iter: 315 loss: 2.11338033e-06
Iter: 316 loss: 2.11158522e-06
Iter: 317 loss: 2.10655367e-06
Iter: 318 loss: 2.14774491e-06
Iter: 319 loss: 2.10627672e-06
Iter: 320 loss: 2.10144231e-06
Iter: 321 loss: 2.1009098e-06
Iter: 322 loss: 2.09746486e-06
Iter: 323 loss: 2.0932091e-06
Iter: 324 loss: 2.10555686e-06
Iter: 325 loss: 2.0919349e-06
Iter: 326 loss: 2.08769939e-06
Iter: 327 loss: 2.1068206e-06
Iter: 328 loss: 2.08692768e-06
Iter: 329 loss: 2.08269716e-06
Iter: 330 loss: 2.08888946e-06
Iter: 331 loss: 2.08046595e-06
Iter: 332 loss: 2.07587436e-06
Iter: 333 loss: 2.08327083e-06
Iter: 334 loss: 2.07378025e-06
Iter: 335 loss: 2.06891309e-06
Iter: 336 loss: 2.10227017e-06
Iter: 337 loss: 2.0685452e-06
Iter: 338 loss: 2.06548475e-06
Iter: 339 loss: 2.06756658e-06
Iter: 340 loss: 2.06357663e-06
Iter: 341 loss: 2.06169807e-06
Iter: 342 loss: 2.06140498e-06
Iter: 343 loss: 2.05948504e-06
Iter: 344 loss: 2.07251901e-06
Iter: 345 loss: 2.05928018e-06
Iter: 346 loss: 2.05834385e-06
Iter: 347 loss: 2.05547212e-06
Iter: 348 loss: 2.06113327e-06
Iter: 349 loss: 2.05381343e-06
Iter: 350 loss: 2.04988851e-06
Iter: 351 loss: 2.08364986e-06
Iter: 352 loss: 2.04968546e-06
Iter: 353 loss: 2.04593971e-06
Iter: 354 loss: 2.06214418e-06
Iter: 355 loss: 2.0451771e-06
Iter: 356 loss: 2.04199137e-06
Iter: 357 loss: 2.04014941e-06
Iter: 358 loss: 2.03878e-06
Iter: 359 loss: 2.03566697e-06
Iter: 360 loss: 2.04755042e-06
Iter: 361 loss: 2.03491481e-06
Iter: 362 loss: 2.03113655e-06
Iter: 363 loss: 2.04455409e-06
Iter: 364 loss: 2.03011814e-06
Iter: 365 loss: 2.02638648e-06
Iter: 366 loss: 2.02900787e-06
Iter: 367 loss: 2.02396814e-06
Iter: 368 loss: 2.02058686e-06
Iter: 369 loss: 2.04620892e-06
Iter: 370 loss: 2.02033675e-06
Iter: 371 loss: 2.0180787e-06
Iter: 372 loss: 2.02314868e-06
Iter: 373 loss: 2.0170387e-06
Iter: 374 loss: 2.01492139e-06
Iter: 375 loss: 2.02926685e-06
Iter: 376 loss: 2.01478e-06
Iter: 377 loss: 2.01302282e-06
Iter: 378 loss: 2.01302782e-06
Iter: 379 loss: 2.01183502e-06
Iter: 380 loss: 2.00898057e-06
Iter: 381 loss: 2.03365403e-06
Iter: 382 loss: 2.00836143e-06
Iter: 383 loss: 2.00551199e-06
Iter: 384 loss: 2.00965201e-06
Iter: 385 loss: 2.00419845e-06
Iter: 386 loss: 2.00170257e-06
Iter: 387 loss: 2.03556328e-06
Iter: 388 loss: 2.00171348e-06
Iter: 389 loss: 1.99938381e-06
Iter: 390 loss: 1.99652e-06
Iter: 391 loss: 1.99626379e-06
Iter: 392 loss: 1.99276315e-06
Iter: 393 loss: 1.99973965e-06
Iter: 394 loss: 1.99143096e-06
Iter: 395 loss: 1.98774387e-06
Iter: 396 loss: 1.99746637e-06
Iter: 397 loss: 1.98676048e-06
Iter: 398 loss: 1.98206317e-06
Iter: 399 loss: 2.00310365e-06
Iter: 400 loss: 1.98121734e-06
Iter: 401 loss: 1.97863073e-06
Iter: 402 loss: 1.97985401e-06
Iter: 403 loss: 1.9768022e-06
Iter: 404 loss: 1.9734382e-06
Iter: 405 loss: 1.9906538e-06
Iter: 406 loss: 1.97287e-06
Iter: 407 loss: 1.96993915e-06
Iter: 408 loss: 1.9785939e-06
Iter: 409 loss: 1.96917495e-06
Iter: 410 loss: 1.97000782e-06
Iter: 411 loss: 1.96815677e-06
Iter: 412 loss: 1.96742712e-06
Iter: 413 loss: 1.9655472e-06
Iter: 414 loss: 1.98519638e-06
Iter: 415 loss: 1.96522819e-06
Iter: 416 loss: 1.96285782e-06
Iter: 417 loss: 1.96106021e-06
Iter: 418 loss: 1.96026053e-06
Iter: 419 loss: 1.95808229e-06
Iter: 420 loss: 1.95808684e-06
Iter: 421 loss: 1.95652228e-06
Iter: 422 loss: 1.96430892e-06
Iter: 423 loss: 1.95623397e-06
Iter: 424 loss: 1.95491225e-06
Iter: 425 loss: 1.951689e-06
Iter: 426 loss: 1.99110264e-06
Iter: 427 loss: 1.95138136e-06
Iter: 428 loss: 1.94821246e-06
Iter: 429 loss: 1.9661652e-06
Iter: 430 loss: 1.94767017e-06
Iter: 431 loss: 1.94580753e-06
Iter: 432 loss: 1.97572945e-06
Iter: 433 loss: 1.94568292e-06
Iter: 434 loss: 1.94414406e-06
Iter: 435 loss: 1.9432855e-06
Iter: 436 loss: 1.94257882e-06
Iter: 437 loss: 1.94034556e-06
Iter: 438 loss: 1.94666291e-06
Iter: 439 loss: 1.93961932e-06
Iter: 440 loss: 1.93784717e-06
Iter: 441 loss: 1.95362145e-06
Iter: 442 loss: 1.93779056e-06
Iter: 443 loss: 1.93679853e-06
Iter: 444 loss: 1.95111215e-06
Iter: 445 loss: 1.936736e-06
Iter: 446 loss: 1.93549295e-06
Iter: 447 loss: 1.93494725e-06
Iter: 448 loss: 1.9342317e-06
Iter: 449 loss: 1.93327787e-06
Iter: 450 loss: 1.93260962e-06
Iter: 451 loss: 1.93221445e-06
Iter: 452 loss: 1.93068081e-06
Iter: 453 loss: 1.93218102e-06
Iter: 454 loss: 1.92981611e-06
Iter: 455 loss: 1.92835751e-06
Iter: 456 loss: 1.94972267e-06
Iter: 457 loss: 1.92829907e-06
Iter: 458 loss: 1.92727452e-06
Iter: 459 loss: 1.92565972e-06
Iter: 460 loss: 1.92560151e-06
Iter: 461 loss: 1.92326274e-06
Iter: 462 loss: 1.92325047e-06
Iter: 463 loss: 1.92157586e-06
Iter: 464 loss: 1.91887602e-06
Iter: 465 loss: 1.93064693e-06
Iter: 466 loss: 1.91838035e-06
Iter: 467 loss: 1.91662161e-06
Iter: 468 loss: 1.91661e-06
Iter: 469 loss: 1.91522622e-06
Iter: 470 loss: 1.91365507e-06
Iter: 471 loss: 1.91365916e-06
Iter: 472 loss: 1.91128129e-06
Iter: 473 loss: 1.91848358e-06
Iter: 474 loss: 1.91063782e-06
Iter: 475 loss: 1.90837477e-06
Iter: 476 loss: 1.92725565e-06
Iter: 477 loss: 1.90821856e-06
Iter: 478 loss: 1.90766286e-06
Iter: 479 loss: 1.90714e-06
Iter: 480 loss: 1.90688741e-06
Iter: 481 loss: 1.90562582e-06
Iter: 482 loss: 1.90811329e-06
Iter: 483 loss: 1.90469359e-06
Iter: 484 loss: 1.9025739e-06
Iter: 485 loss: 1.90918445e-06
Iter: 486 loss: 1.90189553e-06
Iter: 487 loss: 1.89967136e-06
Iter: 488 loss: 1.9081765e-06
Iter: 489 loss: 1.89907792e-06
Iter: 490 loss: 1.8972579e-06
Iter: 491 loss: 1.91467325e-06
Iter: 492 loss: 1.897164e-06
Iter: 493 loss: 1.89572245e-06
Iter: 494 loss: 1.89385628e-06
Iter: 495 loss: 1.89374828e-06
Iter: 496 loss: 1.89137563e-06
Iter: 497 loss: 1.89240632e-06
Iter: 498 loss: 1.88972137e-06
Iter: 499 loss: 1.8871757e-06
Iter: 500 loss: 1.90956553e-06
Iter: 501 loss: 1.88704757e-06
Iter: 502 loss: 1.88485615e-06
Iter: 503 loss: 1.89449713e-06
Iter: 504 loss: 1.88438275e-06
Iter: 505 loss: 1.88226613e-06
Iter: 506 loss: 1.88213971e-06
Iter: 507 loss: 1.8805606e-06
Iter: 508 loss: 1.87907222e-06
Iter: 509 loss: 1.90266599e-06
Iter: 510 loss: 1.87892238e-06
Iter: 511 loss: 1.87833427e-06
Iter: 512 loss: 1.87812691e-06
Iter: 513 loss: 1.87736282e-06
Iter: 514 loss: 1.87504156e-06
Iter: 515 loss: 1.89386856e-06
Iter: 516 loss: 1.87466946e-06
Iter: 517 loss: 1.87311207e-06
Iter: 518 loss: 1.87582191e-06
Iter: 519 loss: 1.87224191e-06
Iter: 520 loss: 1.87063199e-06
Iter: 521 loss: 1.88150034e-06
Iter: 522 loss: 1.8704169e-06
Iter: 523 loss: 1.86887223e-06
Iter: 524 loss: 1.8728656e-06
Iter: 525 loss: 1.8685779e-06
Iter: 526 loss: 1.86677084e-06
Iter: 527 loss: 1.86841248e-06
Iter: 528 loss: 1.86570696e-06
Iter: 529 loss: 1.86421187e-06
Iter: 530 loss: 1.86541752e-06
Iter: 531 loss: 1.86351497e-06
Iter: 532 loss: 1.86172258e-06
Iter: 533 loss: 1.86305397e-06
Iter: 534 loss: 1.86080888e-06
Iter: 535 loss: 1.8587682e-06
Iter: 536 loss: 1.87346291e-06
Iter: 537 loss: 1.85857482e-06
Iter: 538 loss: 1.85654778e-06
Iter: 539 loss: 1.85840383e-06
Iter: 540 loss: 1.85530791e-06
Iter: 541 loss: 1.85349973e-06
Iter: 542 loss: 1.85822432e-06
Iter: 543 loss: 1.85299109e-06
Iter: 544 loss: 1.85253657e-06
Iter: 545 loss: 1.85209319e-06
Iter: 546 loss: 1.85117347e-06
Iter: 547 loss: 1.85037015e-06
Iter: 548 loss: 1.85006547e-06
Iter: 549 loss: 1.84902342e-06
Iter: 550 loss: 1.84802661e-06
Iter: 551 loss: 1.84784153e-06
Iter: 552 loss: 1.84668579e-06
Iter: 553 loss: 1.85039914e-06
Iter: 554 loss: 1.84637759e-06
Iter: 555 loss: 1.84524754e-06
Iter: 556 loss: 1.85205954e-06
Iter: 557 loss: 1.84499231e-06
Iter: 558 loss: 1.84376506e-06
Iter: 559 loss: 1.84542614e-06
Iter: 560 loss: 1.84311193e-06
Iter: 561 loss: 1.84179123e-06
Iter: 562 loss: 1.8430502e-06
Iter: 563 loss: 1.84089583e-06
Iter: 564 loss: 1.83979091e-06
Iter: 565 loss: 1.84077533e-06
Iter: 566 loss: 1.83919769e-06
Iter: 567 loss: 1.83767145e-06
Iter: 568 loss: 1.84037128e-06
Iter: 569 loss: 1.83702355e-06
Iter: 570 loss: 1.83529573e-06
Iter: 571 loss: 1.85053545e-06
Iter: 572 loss: 1.83520467e-06
Iter: 573 loss: 1.83421594e-06
Iter: 574 loss: 1.83440989e-06
Iter: 575 loss: 1.83319514e-06
Iter: 576 loss: 1.83242253e-06
Iter: 577 loss: 1.83241036e-06
Iter: 578 loss: 1.83128043e-06
Iter: 579 loss: 1.8327321e-06
Iter: 580 loss: 1.83075929e-06
Iter: 581 loss: 1.82997007e-06
Iter: 582 loss: 1.82888243e-06
Iter: 583 loss: 1.82873146e-06
Iter: 584 loss: 1.82749136e-06
Iter: 585 loss: 1.82721931e-06
Iter: 586 loss: 1.82642987e-06
Iter: 587 loss: 1.82524559e-06
Iter: 588 loss: 1.84138435e-06
Iter: 589 loss: 1.82524172e-06
Iter: 590 loss: 1.82406268e-06
Iter: 591 loss: 1.82598683e-06
Iter: 592 loss: 1.82363897e-06
Iter: 593 loss: 1.82241888e-06
Iter: 594 loss: 1.82566373e-06
Iter: 595 loss: 1.82205986e-06
Iter: 596 loss: 1.82114422e-06
Iter: 597 loss: 1.82027452e-06
Iter: 598 loss: 1.82011024e-06
Iter: 599 loss: 1.81859332e-06
Iter: 600 loss: 1.82482984e-06
Iter: 601 loss: 1.81820451e-06
Iter: 602 loss: 1.81695009e-06
Iter: 603 loss: 1.82811834e-06
Iter: 604 loss: 1.81695577e-06
Iter: 605 loss: 1.81592225e-06
Iter: 606 loss: 1.81734242e-06
Iter: 607 loss: 1.81556425e-06
Iter: 608 loss: 1.81477651e-06
Iter: 609 loss: 1.82365511e-06
Iter: 610 loss: 1.81475707e-06
Iter: 611 loss: 1.81402322e-06
Iter: 612 loss: 1.82011036e-06
Iter: 613 loss: 1.81397445e-06
Iter: 614 loss: 1.81363259e-06
Iter: 615 loss: 1.81284372e-06
Iter: 616 loss: 1.82818792e-06
Iter: 617 loss: 1.81292944e-06
Iter: 618 loss: 1.81205428e-06
Iter: 619 loss: 1.81180178e-06
Iter: 620 loss: 1.8112637e-06
Iter: 621 loss: 1.81014775e-06
Iter: 622 loss: 1.81468135e-06
Iter: 623 loss: 1.80994698e-06
Iter: 624 loss: 1.80881887e-06
Iter: 625 loss: 1.82087797e-06
Iter: 626 loss: 1.80882864e-06
Iter: 627 loss: 1.80819973e-06
Iter: 628 loss: 1.80941436e-06
Iter: 629 loss: 1.80804307e-06
Iter: 630 loss: 1.80745064e-06
Iter: 631 loss: 1.80668167e-06
Iter: 632 loss: 1.80671918e-06
Iter: 633 loss: 1.80530128e-06
Iter: 634 loss: 1.80646373e-06
Iter: 635 loss: 1.80465565e-06
Iter: 636 loss: 1.80337929e-06
Iter: 637 loss: 1.81851613e-06
Iter: 638 loss: 1.80337872e-06
Iter: 639 loss: 1.80255893e-06
Iter: 640 loss: 1.80432335e-06
Iter: 641 loss: 1.80212896e-06
Iter: 642 loss: 1.80121242e-06
Iter: 643 loss: 1.80309394e-06
Iter: 644 loss: 1.80074733e-06
Iter: 645 loss: 1.79995914e-06
Iter: 646 loss: 1.79988524e-06
Iter: 647 loss: 1.79941708e-06
Iter: 648 loss: 1.79839844e-06
Iter: 649 loss: 1.81113614e-06
Iter: 650 loss: 1.79828442e-06
Iter: 651 loss: 1.79728454e-06
Iter: 652 loss: 1.79902008e-06
Iter: 653 loss: 1.7969503e-06
Iter: 654 loss: 1.79579138e-06
Iter: 655 loss: 1.79582389e-06
Iter: 656 loss: 1.7950108e-06
Iter: 657 loss: 1.79388837e-06
Iter: 658 loss: 1.79389701e-06
Iter: 659 loss: 1.79301549e-06
Iter: 660 loss: 1.79382323e-06
Iter: 661 loss: 1.79254937e-06
Iter: 662 loss: 1.79175e-06
Iter: 663 loss: 1.79276276e-06
Iter: 664 loss: 1.79131621e-06
Iter: 665 loss: 1.79026949e-06
Iter: 666 loss: 1.78917912e-06
Iter: 667 loss: 1.78891014e-06
Iter: 668 loss: 1.78754465e-06
Iter: 669 loss: 1.80811639e-06
Iter: 670 loss: 1.78758705e-06
Iter: 671 loss: 1.78658115e-06
Iter: 672 loss: 1.79259905e-06
Iter: 673 loss: 1.78656899e-06
Iter: 674 loss: 1.78573282e-06
Iter: 675 loss: 1.78537027e-06
Iter: 676 loss: 1.78498703e-06
Iter: 677 loss: 1.78536493e-06
Iter: 678 loss: 1.78448e-06
Iter: 679 loss: 1.78416428e-06
Iter: 680 loss: 1.78326684e-06
Iter: 681 loss: 1.78804805e-06
Iter: 682 loss: 1.78291862e-06
Iter: 683 loss: 1.78192909e-06
Iter: 684 loss: 1.78562323e-06
Iter: 685 loss: 1.78161213e-06
Iter: 686 loss: 1.78072082e-06
Iter: 687 loss: 1.78075697e-06
Iter: 688 loss: 1.77993115e-06
Iter: 689 loss: 1.7788733e-06
Iter: 690 loss: 1.79285405e-06
Iter: 691 loss: 1.77883851e-06
Iter: 692 loss: 1.77800734e-06
Iter: 693 loss: 1.7790386e-06
Iter: 694 loss: 1.77754919e-06
Iter: 695 loss: 1.77669369e-06
Iter: 696 loss: 1.77946595e-06
Iter: 697 loss: 1.77631716e-06
Iter: 698 loss: 1.77553454e-06
Iter: 699 loss: 1.77544405e-06
Iter: 700 loss: 1.77482866e-06
Iter: 701 loss: 1.77378411e-06
Iter: 702 loss: 1.77500033e-06
Iter: 703 loss: 1.77326979e-06
Iter: 704 loss: 1.77245113e-06
Iter: 705 loss: 1.77252332e-06
Iter: 706 loss: 1.77167715e-06
Iter: 707 loss: 1.77115601e-06
Iter: 708 loss: 1.77091886e-06
Iter: 709 loss: 1.77093523e-06
Iter: 710 loss: 1.77062327e-06
Iter: 711 loss: 1.77002039e-06
Iter: 712 loss: 1.7691649e-06
Iter: 713 loss: 1.78502466e-06
Iter: 714 loss: 1.76909896e-06
Iter: 715 loss: 1.76851051e-06
Iter: 716 loss: 1.76942876e-06
Iter: 717 loss: 1.76804497e-06
Iter: 718 loss: 1.76723029e-06
Iter: 719 loss: 1.76708807e-06
Iter: 720 loss: 1.76639753e-06
Iter: 721 loss: 1.765511e-06
Iter: 722 loss: 1.77180243e-06
Iter: 723 loss: 1.76529875e-06
Iter: 724 loss: 1.76452659e-06
Iter: 725 loss: 1.77293873e-06
Iter: 726 loss: 1.76463209e-06
Iter: 727 loss: 1.76396009e-06
Iter: 728 loss: 1.76318281e-06
Iter: 729 loss: 1.76325511e-06
Iter: 730 loss: 1.7620589e-06
Iter: 731 loss: 1.76447054e-06
Iter: 732 loss: 1.76186063e-06
Iter: 733 loss: 1.7606942e-06
Iter: 734 loss: 1.76290018e-06
Iter: 735 loss: 1.7601983e-06
Iter: 736 loss: 1.75928312e-06
Iter: 737 loss: 1.75880837e-06
Iter: 738 loss: 1.75852949e-06
Iter: 739 loss: 1.75743503e-06
Iter: 740 loss: 1.75739751e-06
Iter: 741 loss: 1.75680395e-06
Iter: 742 loss: 1.75924549e-06
Iter: 743 loss: 1.75663342e-06
Iter: 744 loss: 1.75609057e-06
Iter: 745 loss: 1.76050867e-06
Iter: 746 loss: 1.75593016e-06
Iter: 747 loss: 1.75567777e-06
Iter: 748 loss: 1.7549138e-06
Iter: 749 loss: 1.75695141e-06
Iter: 750 loss: 1.75461548e-06
Iter: 751 loss: 1.75318041e-06
Iter: 752 loss: 1.75824607e-06
Iter: 753 loss: 1.75269929e-06
Iter: 754 loss: 1.75169259e-06
Iter: 755 loss: 1.76542108e-06
Iter: 756 loss: 1.75178775e-06
Iter: 757 loss: 1.75104378e-06
Iter: 758 loss: 1.75261073e-06
Iter: 759 loss: 1.75079322e-06
Iter: 760 loss: 1.7498046e-06
Iter: 761 loss: 1.75082369e-06
Iter: 762 loss: 1.7493619e-06
Iter: 763 loss: 1.74843206e-06
Iter: 764 loss: 1.74927425e-06
Iter: 765 loss: 1.74793627e-06
Iter: 766 loss: 1.74688262e-06
Iter: 767 loss: 1.75097068e-06
Iter: 768 loss: 1.74648949e-06
Iter: 769 loss: 1.74571994e-06
Iter: 770 loss: 1.74488537e-06
Iter: 771 loss: 1.74469585e-06
Iter: 772 loss: 1.74371678e-06
Iter: 773 loss: 1.74369916e-06
Iter: 774 loss: 1.74285515e-06
Iter: 775 loss: 1.75210062e-06
Iter: 776 loss: 1.74294155e-06
Iter: 777 loss: 1.74272418e-06
Iter: 778 loss: 1.7426662e-06
Iter: 779 loss: 1.74260458e-06
Iter: 780 loss: 1.74187926e-06
Iter: 781 loss: 1.74150841e-06
Iter: 782 loss: 1.74134288e-06
Iter: 783 loss: 1.74045044e-06
Iter: 784 loss: 1.75112189e-06
Iter: 785 loss: 1.74031652e-06
Iter: 786 loss: 1.73960279e-06
Iter: 787 loss: 1.74268962e-06
Iter: 788 loss: 1.73948649e-06
Iter: 789 loss: 1.7390189e-06
Iter: 790 loss: 1.74036313e-06
Iter: 791 loss: 1.73885735e-06
Iter: 792 loss: 1.73813442e-06
Iter: 793 loss: 1.74132231e-06
Iter: 794 loss: 1.73806211e-06
Iter: 795 loss: 1.73726494e-06
Iter: 796 loss: 1.73592616e-06
Iter: 797 loss: 1.73595777e-06
Iter: 798 loss: 1.73496278e-06
Iter: 799 loss: 1.73969806e-06
Iter: 800 loss: 1.73477758e-06
Iter: 801 loss: 1.73378e-06
Iter: 802 loss: 1.73563444e-06
Iter: 803 loss: 1.73340504e-06
Iter: 804 loss: 1.73224328e-06
Iter: 805 loss: 1.73642343e-06
Iter: 806 loss: 1.73206206e-06
Iter: 807 loss: 1.73123226e-06
Iter: 808 loss: 1.73604724e-06
Iter: 809 loss: 1.73112369e-06
Iter: 810 loss: 1.73085084e-06
Iter: 811 loss: 1.73074727e-06
Iter: 812 loss: 1.73035664e-06
Iter: 813 loss: 1.73001695e-06
Iter: 814 loss: 1.73002422e-06
Iter: 815 loss: 1.72951547e-06
Iter: 816 loss: 1.72823388e-06
Iter: 817 loss: 1.73906517e-06
Iter: 818 loss: 1.7280488e-06
Iter: 819 loss: 1.7269092e-06
Iter: 820 loss: 1.73034209e-06
Iter: 821 loss: 1.72662385e-06
Iter: 822 loss: 1.72544901e-06
Iter: 823 loss: 1.73527928e-06
Iter: 824 loss: 1.72540445e-06
Iter: 825 loss: 1.72430578e-06
Iter: 826 loss: 1.72760633e-06
Iter: 827 loss: 1.72406283e-06
Iter: 828 loss: 1.72341583e-06
Iter: 829 loss: 1.73016599e-06
Iter: 830 loss: 1.72344096e-06
Iter: 831 loss: 1.72287525e-06
Iter: 832 loss: 1.72409045e-06
Iter: 833 loss: 1.72258387e-06
Iter: 834 loss: 1.72198133e-06
Iter: 835 loss: 1.72144667e-06
Iter: 836 loss: 1.7212642e-06
Iter: 837 loss: 1.72024647e-06
Iter: 838 loss: 1.72188936e-06
Iter: 839 loss: 1.71983129e-06
Iter: 840 loss: 1.7187848e-06
Iter: 841 loss: 1.72341061e-06
Iter: 842 loss: 1.71874024e-06
Iter: 843 loss: 1.71775241e-06
Iter: 844 loss: 1.72135697e-06
Iter: 845 loss: 1.71767954e-06
Iter: 846 loss: 1.71660031e-06
Iter: 847 loss: 1.72808507e-06
Iter: 848 loss: 1.71655824e-06
Iter: 849 loss: 1.71634565e-06
Iter: 850 loss: 1.71569627e-06
Iter: 851 loss: 1.72242289e-06
Iter: 852 loss: 1.71560418e-06
Iter: 853 loss: 1.71502222e-06
Iter: 854 loss: 1.71977308e-06
Iter: 855 loss: 1.71502802e-06
Iter: 856 loss: 1.71435727e-06
Iter: 857 loss: 1.71354804e-06
Iter: 858 loss: 1.71356419e-06
Iter: 859 loss: 1.71266993e-06
Iter: 860 loss: 1.71867066e-06
Iter: 861 loss: 1.71257216e-06
Iter: 862 loss: 1.71207125e-06
Iter: 863 loss: 1.71672446e-06
Iter: 864 loss: 1.71199622e-06
Iter: 865 loss: 1.71144734e-06
Iter: 866 loss: 1.71147963e-06
Iter: 867 loss: 1.71105967e-06
Iter: 868 loss: 1.71022009e-06
Iter: 869 loss: 1.71607405e-06
Iter: 870 loss: 1.71008014e-06
Iter: 871 loss: 1.70957151e-06
Iter: 872 loss: 1.70901217e-06
Iter: 873 loss: 1.70883504e-06
Iter: 874 loss: 1.70813018e-06
Iter: 875 loss: 1.71223166e-06
Iter: 876 loss: 1.70793373e-06
Iter: 877 loss: 1.70716851e-06
Iter: 878 loss: 1.70780379e-06
Iter: 879 loss: 1.70664725e-06
Iter: 880 loss: 1.70817646e-06
Iter: 881 loss: 1.70647309e-06
Iter: 882 loss: 1.70628448e-06
Iter: 883 loss: 1.70590079e-06
Iter: 884 loss: 1.70631813e-06
Iter: 885 loss: 1.70535122e-06
Iter: 886 loss: 1.70465228e-06
Iter: 887 loss: 1.705817e-06
Iter: 888 loss: 1.70432907e-06
Iter: 889 loss: 1.70354849e-06
Iter: 890 loss: 1.70738622e-06
Iter: 891 loss: 1.70345277e-06
Iter: 892 loss: 1.70285034e-06
Iter: 893 loss: 1.70377336e-06
Iter: 894 loss: 1.70258591e-06
Iter: 895 loss: 1.70211092e-06
Iter: 896 loss: 1.70114913e-06
Iter: 897 loss: 1.72280738e-06
Iter: 898 loss: 1.70116073e-06
Iter: 899 loss: 1.70019916e-06
Iter: 900 loss: 1.71048316e-06
Iter: 901 loss: 1.70017984e-06
Iter: 902 loss: 1.69957082e-06
Iter: 903 loss: 1.70366275e-06
Iter: 904 loss: 1.69940256e-06
Iter: 905 loss: 1.69875238e-06
Iter: 906 loss: 1.69929785e-06
Iter: 907 loss: 1.69847897e-06
Iter: 908 loss: 1.69768305e-06
Iter: 909 loss: 1.69878604e-06
Iter: 910 loss: 1.6972599e-06
Iter: 911 loss: 1.69666146e-06
Iter: 912 loss: 1.69658347e-06
Iter: 913 loss: 1.6962515e-06
Iter: 914 loss: 1.69564419e-06
Iter: 915 loss: 1.7022287e-06
Iter: 916 loss: 1.69549492e-06
Iter: 917 loss: 1.69512521e-06
Iter: 918 loss: 1.69516898e-06
Iter: 919 loss: 1.69497559e-06
Iter: 920 loss: 1.6944133e-06
Iter: 921 loss: 1.6996828e-06
Iter: 922 loss: 1.69430621e-06
Iter: 923 loss: 1.69382713e-06
Iter: 924 loss: 1.69263251e-06
Iter: 925 loss: 1.70868122e-06
Iter: 926 loss: 1.69263262e-06
Iter: 927 loss: 1.69189684e-06
Iter: 928 loss: 1.69173859e-06
Iter: 929 loss: 1.69112514e-06
Iter: 930 loss: 1.69167458e-06
Iter: 931 loss: 1.69068676e-06
Iter: 932 loss: 1.6897344e-06
Iter: 933 loss: 1.68892336e-06
Iter: 934 loss: 1.68868712e-06
Iter: 935 loss: 1.68877443e-06
Iter: 936 loss: 1.68792883e-06
Iter: 937 loss: 1.6875656e-06
Iter: 938 loss: 1.68764586e-06
Iter: 939 loss: 1.68719316e-06
Iter: 940 loss: 1.68658937e-06
Iter: 941 loss: 1.68852512e-06
Iter: 942 loss: 1.68642771e-06
Iter: 943 loss: 1.68567624e-06
Iter: 944 loss: 1.68604322e-06
Iter: 945 loss: 1.68514066e-06
Iter: 946 loss: 1.6843403e-06
Iter: 947 loss: 1.68716838e-06
Iter: 948 loss: 1.68410406e-06
Iter: 949 loss: 1.68365068e-06
Iter: 950 loss: 1.68906172e-06
Iter: 951 loss: 1.6836575e-06
Iter: 952 loss: 1.682934e-06
Iter: 953 loss: 1.68302267e-06
Iter: 954 loss: 1.6824805e-06
Iter: 955 loss: 1.68201097e-06
Iter: 956 loss: 1.68074644e-06
Iter: 957 loss: 1.70028011e-06
Iter: 958 loss: 1.68077281e-06
Iter: 959 loss: 1.67992255e-06
Iter: 960 loss: 1.68009035e-06
Iter: 961 loss: 1.67944506e-06
Iter: 962 loss: 1.67873941e-06
Iter: 963 loss: 1.6786579e-06
Iter: 964 loss: 1.6775532e-06
Iter: 965 loss: 1.68175961e-06
Iter: 966 loss: 1.67726671e-06
Iter: 967 loss: 1.67644271e-06
Iter: 968 loss: 1.67649046e-06
Iter: 969 loss: 1.67592884e-06
Iter: 970 loss: 1.67656481e-06
Iter: 971 loss: 1.67570488e-06
Iter: 972 loss: 1.67495e-06
Iter: 973 loss: 1.67571011e-06
Iter: 974 loss: 1.67456847e-06
Iter: 975 loss: 1.67396331e-06
Iter: 976 loss: 1.67413259e-06
Iter: 977 loss: 1.6737356e-06
Iter: 978 loss: 1.67305302e-06
Iter: 979 loss: 1.68554288e-06
Iter: 980 loss: 1.67303631e-06
Iter: 981 loss: 1.67241728e-06
Iter: 982 loss: 1.68188126e-06
Iter: 983 loss: 1.6724814e-06
Iter: 984 loss: 1.67176108e-06
Iter: 985 loss: 1.67556675e-06
Iter: 986 loss: 1.67170515e-06
Iter: 987 loss: 1.67135659e-06
Iter: 988 loss: 1.67076246e-06
Iter: 989 loss: 1.68045153e-06
Iter: 990 loss: 1.67087728e-06
Iter: 991 loss: 1.6703e-06
Iter: 992 loss: 1.66981931e-06
Iter: 993 loss: 1.66974792e-06
Iter: 994 loss: 1.66862549e-06
Iter: 995 loss: 1.67486417e-06
Iter: 996 loss: 1.66841028e-06
Iter: 997 loss: 1.6676197e-06
Iter: 998 loss: 1.66887048e-06
Iter: 999 loss: 1.66725363e-06
Iter: 1000 loss: 1.66670532e-06
Iter: 1001 loss: 1.67157737e-06
Iter: 1002 loss: 1.66662653e-06
Iter: 1003 loss: 1.66614836e-06
Iter: 1004 loss: 1.66610391e-06
Iter: 1005 loss: 1.66559039e-06
Iter: 1006 loss: 1.66499194e-06
Iter: 1007 loss: 1.67021938e-06
Iter: 1008 loss: 1.66488758e-06
Iter: 1009 loss: 1.66433324e-06
Iter: 1010 loss: 1.66515429e-06
Iter: 1011 loss: 1.66394875e-06
Iter: 1012 loss: 1.66314771e-06
Iter: 1013 loss: 1.66623988e-06
Iter: 1014 loss: 1.66292727e-06
Iter: 1015 loss: 1.66243581e-06
Iter: 1016 loss: 1.6643944e-06
Iter: 1017 loss: 1.66226209e-06
Iter: 1018 loss: 1.66169298e-06
Iter: 1019 loss: 1.66841141e-06
Iter: 1020 loss: 1.6616425e-06
Iter: 1021 loss: 1.66141456e-06
Iter: 1022 loss: 1.66076643e-06
Iter: 1023 loss: 1.67097187e-06
Iter: 1024 loss: 1.66067889e-06
Iter: 1025 loss: 1.66021505e-06
Iter: 1026 loss: 1.65985216e-06
Iter: 1027 loss: 1.65975462e-06
Iter: 1028 loss: 1.65898712e-06
Iter: 1029 loss: 1.66420841e-06
Iter: 1030 loss: 1.65904021e-06
Iter: 1031 loss: 1.65844472e-06
Iter: 1032 loss: 1.65935228e-06
Iter: 1033 loss: 1.65833274e-06
Iter: 1034 loss: 1.65783126e-06
Iter: 1035 loss: 1.65876725e-06
Iter: 1036 loss: 1.65766846e-06
Iter: 1037 loss: 1.65709127e-06
Iter: 1038 loss: 1.65663766e-06
Iter: 1039 loss: 1.65645724e-06
Iter: 1040 loss: 1.65575113e-06
Iter: 1041 loss: 1.66202767e-06
Iter: 1042 loss: 1.65581696e-06
Iter: 1043 loss: 1.65531799e-06
Iter: 1044 loss: 1.65723361e-06
Iter: 1045 loss: 1.6552292e-06
Iter: 1046 loss: 1.65485608e-06
Iter: 1047 loss: 1.65450547e-06
Iter: 1048 loss: 1.65444351e-06
Iter: 1049 loss: 1.65373615e-06
Iter: 1050 loss: 1.65484175e-06
Iter: 1051 loss: 1.65339009e-06
Iter: 1052 loss: 1.65366691e-06
Iter: 1053 loss: 1.65310951e-06
Iter: 1054 loss: 1.65285905e-06
Iter: 1055 loss: 1.65233394e-06
Iter: 1056 loss: 1.65645565e-06
Iter: 1057 loss: 1.65235144e-06
Iter: 1058 loss: 1.65184724e-06
Iter: 1059 loss: 1.65181075e-06
Iter: 1060 loss: 1.65143138e-06
Iter: 1061 loss: 1.65074334e-06
Iter: 1062 loss: 1.65139136e-06
Iter: 1063 loss: 1.65020413e-06
Iter: 1064 loss: 1.64931816e-06
Iter: 1065 loss: 1.65702181e-06
Iter: 1066 loss: 1.64935432e-06
Iter: 1067 loss: 1.64877486e-06
Iter: 1068 loss: 1.65018798e-06
Iter: 1069 loss: 1.64852975e-06
Iter: 1070 loss: 1.64788867e-06
Iter: 1071 loss: 1.64860671e-06
Iter: 1072 loss: 1.64761275e-06
Iter: 1073 loss: 1.64691789e-06
Iter: 1074 loss: 1.65026859e-06
Iter: 1075 loss: 1.64679568e-06
Iter: 1076 loss: 1.64609639e-06
Iter: 1077 loss: 1.64768198e-06
Iter: 1078 loss: 1.64587163e-06
Iter: 1079 loss: 1.64549954e-06
Iter: 1080 loss: 1.64742664e-06
Iter: 1081 loss: 1.64547816e-06
Iter: 1082 loss: 1.64493213e-06
Iter: 1083 loss: 1.64551022e-06
Iter: 1084 loss: 1.64474386e-06
Iter: 1085 loss: 1.64447647e-06
Iter: 1086 loss: 1.64431265e-06
Iter: 1087 loss: 1.64433709e-06
Iter: 1088 loss: 1.64436472e-06
Iter: 1089 loss: 1.64426285e-06
Iter: 1090 loss: 1.64435619e-06
Iter: 1091 loss: 1.64426979e-06
Iter: 1092 loss: 1.64427809e-06
Iter: 1093 loss: 1.64431185e-06
Iter: 1094 loss: 1.64439132e-06
Iter: 1095 loss: 1.6442807e-06
Iter: 1096 loss: 1.64428525e-06
Iter: 1097 loss: 1.64430253e-06
Iter: 1098 loss: 1.64436165e-06
Iter: 1099 loss: 1.64435278e-06
Iter: 1100 loss: 1.64434755e-06
Iter: 1101 loss: 1.64433277e-06
Iter: 1102 loss: 1.64431117e-06
Iter: 1103 loss: 1.64430412e-06
Iter: 1104 loss: 1.64431981e-06
Iter: 1105 loss: 1.64431412e-06
Iter: 1106 loss: 1.64431401e-06
Iter: 1107 loss: 1.64431549e-06
Iter: 1108 loss: 1.64431481e-06
Iter: 1109 loss: 1.64431412e-06
Iter: 1110 loss: 1.64431412e-06
Iter: 1111 loss: 1.64431481e-06
Iter: 1112 loss: 1.64431492e-06
Iter: 1113 loss: 1.64431412e-06
Iter: 1114 loss: 1.64385153e-06
Iter: 1115 loss: 1.64845926e-06
Iter: 1116 loss: 1.64372398e-06
Iter: 1117 loss: 1.64354697e-06
Iter: 1118 loss: 1.6428379e-06
Iter: 1119 loss: 1.64981702e-06
Iter: 1120 loss: 1.64275991e-06
Iter: 1121 loss: 1.64220728e-06
Iter: 1122 loss: 1.64564358e-06
Iter: 1123 loss: 1.6420621e-06
Iter: 1124 loss: 1.6418e-06
Iter: 1125 loss: 1.64207677e-06
Iter: 1126 loss: 1.64146536e-06
Iter: 1127 loss: 1.64083917e-06
Iter: 1128 loss: 1.64301889e-06
Iter: 1129 loss: 1.64061146e-06
Iter: 1130 loss: 1.64014966e-06
Iter: 1131 loss: 1.64516359e-06
Iter: 1132 loss: 1.64022913e-06
Iter: 1133 loss: 1.63977961e-06
Iter: 1134 loss: 1.64191658e-06
Iter: 1135 loss: 1.63974937e-06
Iter: 1136 loss: 1.63950858e-06
Iter: 1137 loss: 1.63875779e-06
Iter: 1138 loss: 1.64459584e-06
Iter: 1139 loss: 1.63863069e-06
Iter: 1140 loss: 1.63806499e-06
Iter: 1141 loss: 1.63813e-06
Iter: 1142 loss: 1.63774962e-06
Iter: 1143 loss: 1.63814582e-06
Iter: 1144 loss: 1.63771529e-06
Iter: 1145 loss: 1.63732534e-06
Iter: 1146 loss: 1.63924142e-06
Iter: 1147 loss: 1.63721097e-06
Iter: 1148 loss: 1.63702884e-06
Iter: 1149 loss: 1.63735331e-06
Iter: 1150 loss: 1.63689901e-06
Iter: 1151 loss: 1.63672507e-06
Iter: 1152 loss: 1.63660036e-06
Iter: 1153 loss: 1.63644597e-06
Iter: 1154 loss: 1.63618881e-06
Iter: 1155 loss: 1.63632524e-06
Iter: 1156 loss: 1.6359478e-06
Iter: 1157 loss: 1.63554114e-06
Iter: 1158 loss: 1.63574623e-06
Iter: 1159 loss: 1.63526136e-06
Iter: 1160 loss: 1.63454411e-06
Iter: 1161 loss: 1.63752702e-06
Iter: 1162 loss: 1.63445532e-06
Iter: 1163 loss: 1.63416803e-06
Iter: 1164 loss: 1.63719324e-06
Iter: 1165 loss: 1.63418031e-06
Iter: 1166 loss: 1.63383629e-06
Iter: 1167 loss: 1.63477671e-06
Iter: 1168 loss: 1.63368395e-06
Iter: 1169 loss: 1.63335312e-06
Iter: 1170 loss: 1.63309539e-06
Iter: 1171 loss: 1.63298e-06
Iter: 1172 loss: 1.63246125e-06
Iter: 1173 loss: 1.63314974e-06
Iter: 1174 loss: 1.63223012e-06
Iter: 1175 loss: 1.63178299e-06
Iter: 1176 loss: 1.63176844e-06
Iter: 1177 loss: 1.63151503e-06
Iter: 1178 loss: 1.63159541e-06
Iter: 1179 loss: 1.63145285e-06
Iter: 1180 loss: 1.63115874e-06
Iter: 1181 loss: 1.63565392e-06
Iter: 1182 loss: 1.63119034e-06
Iter: 1183 loss: 1.63083041e-06
Iter: 1184 loss: 1.63098719e-06
Iter: 1185 loss: 1.63055097e-06
Iter: 1186 loss: 1.63027312e-06
Iter: 1187 loss: 1.63112077e-06
Iter: 1188 loss: 1.63013317e-06
Iter: 1189 loss: 1.62982565e-06
Iter: 1190 loss: 1.62947504e-06
Iter: 1191 loss: 1.62936158e-06
Iter: 1192 loss: 1.62890342e-06
Iter: 1193 loss: 1.63247194e-06
Iter: 1194 loss: 1.62884476e-06
Iter: 1195 loss: 1.6283e-06
Iter: 1196 loss: 1.63059315e-06
Iter: 1197 loss: 1.6283085e-06
Iter: 1198 loss: 1.6280685e-06
Iter: 1199 loss: 1.62986146e-06
Iter: 1200 loss: 1.62807862e-06
Iter: 1201 loss: 1.62776894e-06
Iter: 1202 loss: 1.6275244e-06
Iter: 1203 loss: 1.62747415e-06
Iter: 1204 loss: 1.62686865e-06
Iter: 1205 loss: 1.62713536e-06
Iter: 1206 loss: 1.62651725e-06
Iter: 1207 loss: 1.62589538e-06
Iter: 1208 loss: 1.62595029e-06
Iter: 1209 loss: 1.62575748e-06
Iter: 1210 loss: 1.62576771e-06
Iter: 1211 loss: 1.62555193e-06
Iter: 1212 loss: 1.62548542e-06
Iter: 1213 loss: 1.62535525e-06
Iter: 1214 loss: 1.6251123e-06
Iter: 1215 loss: 1.62540255e-06
Iter: 1216 loss: 1.62495871e-06
Iter: 1217 loss: 1.62464971e-06
Iter: 1218 loss: 1.62441484e-06
Iter: 1219 loss: 1.62442927e-06
Iter: 1220 loss: 1.62405172e-06
Iter: 1221 loss: 1.62485696e-06
Iter: 1222 loss: 1.62393189e-06
Iter: 1223 loss: 1.62329491e-06
Iter: 1224 loss: 1.6248207e-06
Iter: 1225 loss: 1.62315882e-06
Iter: 1226 loss: 1.62267008e-06
Iter: 1227 loss: 1.62531103e-06
Iter: 1228 loss: 1.62270453e-06
Iter: 1229 loss: 1.62234073e-06
Iter: 1230 loss: 1.62333356e-06
Iter: 1231 loss: 1.62215883e-06
Iter: 1232 loss: 1.62170863e-06
Iter: 1233 loss: 1.6224767e-06
Iter: 1234 loss: 1.62153219e-06
Iter: 1235 loss: 1.62114588e-06
Iter: 1236 loss: 1.62171318e-06
Iter: 1237 loss: 1.62086394e-06
Iter: 1238 loss: 1.62057e-06
Iter: 1239 loss: 1.62112349e-06
Iter: 1240 loss: 1.62026549e-06
Iter: 1241 loss: 1.62010815e-06
Iter: 1242 loss: 1.62000583e-06
Iter: 1243 loss: 1.61981905e-06
Iter: 1244 loss: 1.62052038e-06
Iter: 1245 loss: 1.6196866e-06
Iter: 1246 loss: 1.61940966e-06
Iter: 1247 loss: 1.61931166e-06
Iter: 1248 loss: 1.61922537e-06
Iter: 1249 loss: 1.61885271e-06
Iter: 1250 loss: 1.61929211e-06
Iter: 1251 loss: 1.61870867e-06
Iter: 1252 loss: 1.61841012e-06
Iter: 1253 loss: 1.61801267e-06
Iter: 1254 loss: 1.61789444e-06
Iter: 1255 loss: 1.61750961e-06
Iter: 1256 loss: 1.62055926e-06
Iter: 1257 loss: 1.61747835e-06
Iter: 1258 loss: 1.61698733e-06
Iter: 1259 loss: 1.61858725e-06
Iter: 1260 loss: 1.61698563e-06
Iter: 1261 loss: 1.61670118e-06
Iter: 1262 loss: 1.61846492e-06
Iter: 1263 loss: 1.61662558e-06
Iter: 1264 loss: 1.61629805e-06
Iter: 1265 loss: 1.6167794e-06
Iter: 1266 loss: 1.61608193e-06
Iter: 1267 loss: 1.61584319e-06
Iter: 1268 loss: 1.61613707e-06
Iter: 1269 loss: 1.61554919e-06
Iter: 1270 loss: 1.61517369e-06
Iter: 1271 loss: 1.61496382e-06
Iter: 1272 loss: 1.61483695e-06
Iter: 1273 loss: 1.61457103e-06
Iter: 1274 loss: 1.61437902e-06
Iter: 1275 loss: 1.61418166e-06
Iter: 1276 loss: 1.61706726e-06
Iter: 1277 loss: 1.61417256e-06
Iter: 1278 loss: 1.614045e-06
Iter: 1279 loss: 1.61378762e-06
Iter: 1280 loss: 1.61385037e-06
Iter: 1281 loss: 1.61347225e-06
Iter: 1282 loss: 1.61366552e-06
Iter: 1283 loss: 1.6132567e-06
Iter: 1284 loss: 1.61288699e-06
Iter: 1285 loss: 1.61311959e-06
Iter: 1286 loss: 1.61271817e-06
Iter: 1287 loss: 1.61234561e-06
Iter: 1288 loss: 1.61285959e-06
Iter: 1289 loss: 1.6122259e-06
Iter: 1290 loss: 1.61185903e-06
Iter: 1291 loss: 1.61325841e-06
Iter: 1292 loss: 1.61178832e-06
Iter: 1293 loss: 1.6113662e-06
Iter: 1294 loss: 1.61357161e-06
Iter: 1295 loss: 1.61128946e-06
Iter: 1296 loss: 1.6110771e-06
Iter: 1297 loss: 1.61195e-06
Iter: 1298 loss: 1.61097796e-06
Iter: 1299 loss: 1.61079424e-06
Iter: 1300 loss: 1.61084699e-06
Iter: 1301 loss: 1.61061723e-06
Iter: 1302 loss: 1.6102731e-06
Iter: 1303 loss: 1.61037065e-06
Iter: 1304 loss: 1.61012053e-06
Iter: 1305 loss: 1.60960269e-06
Iter: 1306 loss: 1.61013099e-06
Iter: 1307 loss: 1.60935326e-06
Iter: 1308 loss: 1.60936702e-06
Iter: 1309 loss: 1.60918739e-06
Iter: 1310 loss: 1.60902721e-06
Iter: 1311 loss: 1.60866819e-06
Iter: 1312 loss: 1.60870411e-06
Iter: 1313 loss: 1.60852301e-06
Iter: 1314 loss: 1.60950322e-06
Iter: 1315 loss: 1.60851584e-06
Iter: 1316 loss: 1.60828176e-06
Iter: 1317 loss: 1.6079855e-06
Iter: 1318 loss: 1.60799937e-06
Iter: 1319 loss: 1.6076192e-06
Iter: 1320 loss: 1.61015532e-06
Iter: 1321 loss: 1.60767104e-06
Iter: 1322 loss: 1.6073443e-06
Iter: 1323 loss: 1.6071474e-06
Iter: 1324 loss: 1.60714058e-06
Iter: 1325 loss: 1.60677064e-06
Iter: 1326 loss: 1.60775608e-06
Iter: 1327 loss: 1.60665866e-06
Iter: 1328 loss: 1.6064605e-06
Iter: 1329 loss: 1.60976629e-06
Iter: 1330 loss: 1.60647153e-06
Iter: 1331 loss: 1.60628349e-06
Iter: 1332 loss: 1.60613149e-06
Iter: 1333 loss: 1.60604236e-06
Iter: 1334 loss: 1.60577906e-06
Iter: 1335 loss: 1.60701882e-06
Iter: 1336 loss: 1.60565423e-06
Iter: 1337 loss: 1.60552804e-06
Iter: 1338 loss: 1.60526565e-06
Iter: 1339 loss: 1.60531476e-06
Iter: 1340 loss: 1.60517834e-06
Iter: 1341 loss: 1.60502213e-06
Iter: 1342 loss: 1.60484524e-06
Iter: 1343 loss: 1.60546529e-06
Iter: 1344 loss: 1.60475167e-06
Iter: 1345 loss: 1.60467675e-06
Iter: 1346 loss: 1.60479976e-06
Iter: 1347 loss: 1.60456761e-06
Iter: 1348 loss: 1.60447405e-06
Iter: 1349 loss: 1.60453692e-06
Iter: 1350 loss: 1.60441664e-06
Iter: 1351 loss: 1.60412696e-06
Iter: 1352 loss: 1.60449076e-06
Iter: 1353 loss: 1.60399873e-06
Iter: 1354 loss: 1.60373952e-06
Iter: 1355 loss: 1.60360059e-06
Iter: 1356 loss: 1.60350328e-06
Iter: 1357 loss: 1.60310174e-06
Iter: 1358 loss: 1.60431944e-06
Iter: 1359 loss: 1.60298009e-06
Iter: 1360 loss: 1.60266882e-06
Iter: 1361 loss: 1.60453533e-06
Iter: 1362 loss: 1.60272384e-06
Iter: 1363 loss: 1.6023148e-06
Iter: 1364 loss: 1.60258492e-06
Iter: 1365 loss: 1.60212494e-06
Iter: 1366 loss: 1.60186369e-06
Iter: 1367 loss: 1.60370644e-06
Iter: 1368 loss: 1.60171408e-06
Iter: 1369 loss: 1.60147192e-06
Iter: 1370 loss: 1.60113609e-06
Iter: 1371 loss: 1.60118543e-06
Iter: 1372 loss: 1.60087507e-06
Iter: 1373 loss: 1.60083027e-06
Iter: 1374 loss: 1.60081822e-06
Iter: 1375 loss: 1.60231718e-06
Iter: 1376 loss: 1.60075774e-06
Iter: 1377 loss: 1.60063496e-06
Iter: 1378 loss: 1.60042191e-06
Iter: 1379 loss: 1.60362481e-06
Iter: 1380 loss: 1.60039872e-06
Iter: 1381 loss: 1.60019749e-06
Iter: 1382 loss: 1.60158072e-06
Iter: 1383 loss: 1.60035415e-06
Iter: 1384 loss: 1.60014235e-06
Iter: 1385 loss: 1.59984427e-06
Iter: 1386 loss: 1.60641071e-06
Iter: 1387 loss: 1.59982551e-06
Iter: 1388 loss: 1.59955982e-06
Iter: 1389 loss: 1.60002514e-06
Iter: 1390 loss: 1.59948081e-06
Iter: 1391 loss: 1.5990438e-06
Iter: 1392 loss: 1.60133459e-06
Iter: 1393 loss: 1.59900651e-06
Iter: 1394 loss: 1.59875776e-06
Iter: 1395 loss: 1.59929368e-06
Iter: 1396 loss: 1.59868659e-06
Iter: 1397 loss: 1.5986127e-06
Iter: 1398 loss: 1.59893921e-06
Iter: 1399 loss: 1.59843751e-06
Iter: 1400 loss: 1.59825822e-06
Iter: 1401 loss: 1.59859599e-06
Iter: 1402 loss: 1.5981542e-06
Iter: 1403 loss: 1.59797332e-06
Iter: 1404 loss: 1.59815193e-06
Iter: 1405 loss: 1.59791944e-06
Iter: 1406 loss: 1.59768774e-06
Iter: 1407 loss: 1.59884007e-06
Iter: 1408 loss: 1.59765159e-06
Iter: 1409 loss: 1.59746037e-06
Iter: 1410 loss: 1.59755177e-06
Iter: 1411 loss: 1.59735771e-06
Iter: 1412 loss: 1.59721696e-06
Iter: 1413 loss: 1.5997075e-06
Iter: 1414 loss: 1.59707349e-06
Iter: 1415 loss: 1.59682304e-06
Iter: 1416 loss: 1.59907734e-06
Iter: 1417 loss: 1.59682145e-06
Iter: 1418 loss: 1.59663159e-06
Iter: 1419 loss: 1.59644503e-06
Iter: 1420 loss: 1.59649471e-06
Iter: 1421 loss: 1.59604235e-06
Iter: 1422 loss: 1.59658623e-06
Iter: 1423 loss: 1.59589479e-06
Iter: 1424 loss: 1.59560705e-06
Iter: 1425 loss: 1.59753199e-06
Iter: 1426 loss: 1.59547631e-06
Iter: 1427 loss: 1.59526587e-06
Iter: 1428 loss: 1.59542992e-06
Iter: 1429 loss: 1.5951731e-06
Iter: 1430 loss: 1.5949488e-06
Iter: 1431 loss: 1.59773924e-06
Iter: 1432 loss: 1.59487104e-06
Iter: 1433 loss: 1.59467731e-06
Iter: 1434 loss: 1.59502929e-06
Iter: 1435 loss: 1.59461069e-06
Iter: 1436 loss: 1.5942386e-06
Iter: 1437 loss: 1.59612432e-06
Iter: 1438 loss: 1.59420608e-06
Iter: 1439 loss: 1.5940866e-06
Iter: 1440 loss: 1.59385809e-06
Iter: 1441 loss: 1.59395415e-06
Iter: 1442 loss: 1.59392573e-06
Iter: 1443 loss: 1.59383785e-06
Iter: 1444 loss: 1.59355568e-06
Iter: 1445 loss: 1.59341369e-06
Iter: 1446 loss: 1.59754779e-06
Iter: 1447 loss: 1.59338379e-06
Iter: 1448 loss: 1.59313311e-06
Iter: 1449 loss: 1.59477781e-06
Iter: 1450 loss: 1.59316278e-06
Iter: 1451 loss: 1.59292335e-06
Iter: 1452 loss: 1.59344302e-06
Iter: 1453 loss: 1.59286981e-06
Iter: 1454 loss: 1.59277988e-06
Iter: 1455 loss: 1.59256297e-06
Iter: 1456 loss: 1.59255126e-06
Iter: 1457 loss: 1.59222986e-06
Iter: 1458 loss: 1.59301e-06
Iter: 1459 loss: 1.59220372e-06
Iter: 1460 loss: 1.59187971e-06
Iter: 1461 loss: 1.59248918e-06
Iter: 1462 loss: 1.59183151e-06
Iter: 1463 loss: 1.59153069e-06
Iter: 1464 loss: 1.59144361e-06
Iter: 1465 loss: 1.59127626e-06
Iter: 1466 loss: 1.59086039e-06
Iter: 1467 loss: 1.59227716e-06
Iter: 1468 loss: 1.59068372e-06
Iter: 1469 loss: 1.59038723e-06
Iter: 1470 loss: 1.59173442e-06
Iter: 1471 loss: 1.59024853e-06
Iter: 1472 loss: 1.58990042e-06
Iter: 1473 loss: 1.59252863e-06
Iter: 1474 loss: 1.58985154e-06
Iter: 1475 loss: 1.58970363e-06
Iter: 1476 loss: 1.58963394e-06
Iter: 1477 loss: 1.58961348e-06
Iter: 1478 loss: 1.58924445e-06
Iter: 1479 loss: 1.58926844e-06
Iter: 1480 loss: 1.58922853e-06
Iter: 1481 loss: 1.58901139e-06
Iter: 1482 loss: 1.59118747e-06
Iter: 1483 loss: 1.58895591e-06
Iter: 1484 loss: 1.58879402e-06
Iter: 1485 loss: 1.58874332e-06
Iter: 1486 loss: 1.58859598e-06
Iter: 1487 loss: 1.58822286e-06
Iter: 1488 loss: 1.58832927e-06
Iter: 1489 loss: 1.5878511e-06
Iter: 1490 loss: 1.58835223e-06
Iter: 1491 loss: 1.58767989e-06
Iter: 1492 loss: 1.58726925e-06
Iter: 1493 loss: 1.58934415e-06
Iter: 1494 loss: 1.58717853e-06
Iter: 1495 loss: 1.58689295e-06
Iter: 1496 loss: 1.58694093e-06
Iter: 1497 loss: 1.58664477e-06
Iter: 1498 loss: 1.58636885e-06
Iter: 1499 loss: 1.58783132e-06
Iter: 1500 loss: 1.58622925e-06
Iter: 1501 loss: 1.5860146e-06
Iter: 1502 loss: 1.58614353e-06
Iter: 1503 loss: 1.58577052e-06
Iter: 1504 loss: 1.58541877e-06
Iter: 1505 loss: 1.58533055e-06
Iter: 1506 loss: 1.58509124e-06
Iter: 1507 loss: 1.58477292e-06
Iter: 1508 loss: 1.58847342e-06
Iter: 1509 loss: 1.58476848e-06
Iter: 1510 loss: 1.584649e-06
Iter: 1511 loss: 1.58456874e-06
Iter: 1512 loss: 1.58440491e-06
Iter: 1513 loss: 1.58413638e-06
Iter: 1514 loss: 1.58427247e-06
Iter: 1515 loss: 1.58405794e-06
Iter: 1516 loss: 1.58405055e-06
Iter: 1517 loss: 1.58386217e-06
Iter: 1518 loss: 1.58349235e-06
Iter: 1519 loss: 1.58396597e-06
Iter: 1520 loss: 1.58337207e-06
Iter: 1521 loss: 1.58311582e-06
Iter: 1522 loss: 1.58285525e-06
Iter: 1523 loss: 1.58276862e-06
Iter: 1524 loss: 1.58246917e-06
Iter: 1525 loss: 1.58701368e-06
Iter: 1526 loss: 1.58248918e-06
Iter: 1527 loss: 1.58221451e-06
Iter: 1528 loss: 1.58215801e-06
Iter: 1529 loss: 1.58191187e-06
Iter: 1530 loss: 1.5814835e-06
Iter: 1531 loss: 1.58231364e-06
Iter: 1532 loss: 1.58134503e-06
Iter: 1533 loss: 1.58090336e-06
Iter: 1534 loss: 1.58464195e-06
Iter: 1535 loss: 1.58086459e-06
Iter: 1536 loss: 1.58039188e-06
Iter: 1537 loss: 1.5805706e-06
Iter: 1538 loss: 1.58011869e-06
Iter: 1539 loss: 1.57985608e-06
Iter: 1540 loss: 1.58355408e-06
Iter: 1541 loss: 1.57982663e-06
Iter: 1542 loss: 1.57966235e-06
Iter: 1543 loss: 1.58001285e-06
Iter: 1544 loss: 1.57947647e-06
Iter: 1545 loss: 1.57926536e-06
Iter: 1546 loss: 1.581064e-06
Iter: 1547 loss: 1.57914519e-06
Iter: 1548 loss: 1.57906175e-06
Iter: 1549 loss: 1.57881618e-06
Iter: 1550 loss: 1.58129728e-06
Iter: 1551 loss: 1.5787042e-06
Iter: 1552 loss: 1.57857187e-06
Iter: 1553 loss: 1.57843203e-06
Iter: 1554 loss: 1.57817021e-06
Iter: 1555 loss: 1.57784916e-06
Iter: 1556 loss: 1.58497028e-06
Iter: 1557 loss: 1.57775924e-06
Iter: 1558 loss: 1.57749753e-06
Iter: 1559 loss: 1.58088119e-06
Iter: 1560 loss: 1.57737713e-06
Iter: 1561 loss: 1.57714635e-06
Iter: 1562 loss: 1.57700265e-06
Iter: 1563 loss: 1.57677755e-06
Iter: 1564 loss: 1.57640227e-06
Iter: 1565 loss: 1.57656928e-06
Iter: 1566 loss: 1.57607724e-06
Iter: 1567 loss: 1.57568775e-06
Iter: 1568 loss: 1.57775889e-06
Iter: 1569 loss: 1.5755719e-06
Iter: 1570 loss: 1.57511e-06
Iter: 1571 loss: 1.5769142e-06
Iter: 1572 loss: 1.57497209e-06
Iter: 1573 loss: 1.57447312e-06
Iter: 1574 loss: 1.57708268e-06
Iter: 1575 loss: 1.57446698e-06
Iter: 1576 loss: 1.57414365e-06
Iter: 1577 loss: 1.57631655e-06
Iter: 1578 loss: 1.57414229e-06
Iter: 1579 loss: 1.57390969e-06
Iter: 1580 loss: 1.57391696e-06
Iter: 1581 loss: 1.57375803e-06
Iter: 1582 loss: 1.57340526e-06
Iter: 1583 loss: 1.57523164e-06
Iter: 1584 loss: 1.57317152e-06
Iter: 1585 loss: 1.57278316e-06
Iter: 1586 loss: 1.57291493e-06
Iter: 1587 loss: 1.57239913e-06
Iter: 1588 loss: 1.57207387e-06
Iter: 1589 loss: 1.57201271e-06
Iter: 1590 loss: 1.57174236e-06
Iter: 1591 loss: 1.57109707e-06
Iter: 1592 loss: 1.57115358e-06
Iter: 1593 loss: 1.57067689e-06
Iter: 1594 loss: 1.57580303e-06
Iter: 1595 loss: 1.57061356e-06
Iter: 1596 loss: 1.57019065e-06
Iter: 1597 loss: 1.56994224e-06
Iter: 1598 loss: 1.56988199e-06
Iter: 1599 loss: 1.56937153e-06
Iter: 1600 loss: 1.57040984e-06
Iter: 1601 loss: 1.56922397e-06
Iter: 1602 loss: 1.56892952e-06
Iter: 1603 loss: 1.56967508e-06
Iter: 1604 loss: 1.56872454e-06
Iter: 1605 loss: 1.56826184e-06
Iter: 1606 loss: 1.57163709e-06
Iter: 1607 loss: 1.56819169e-06
Iter: 1608 loss: 1.56787155e-06
Iter: 1609 loss: 1.56861643e-06
Iter: 1610 loss: 1.5676801e-06
Iter: 1611 loss: 1.56738452e-06
Iter: 1612 loss: 1.56739702e-06
Iter: 1613 loss: 1.5670696e-06
Iter: 1614 loss: 1.56752412e-06
Iter: 1615 loss: 1.56688452e-06
Iter: 1616 loss: 1.56678834e-06
Iter: 1617 loss: 1.56636656e-06
Iter: 1618 loss: 1.56722308e-06
Iter: 1619 loss: 1.56606336e-06
Iter: 1620 loss: 1.56561873e-06
Iter: 1621 loss: 1.56691499e-06
Iter: 1622 loss: 1.56533929e-06
Iter: 1623 loss: 1.56504439e-06
Iter: 1624 loss: 1.56944543e-06
Iter: 1625 loss: 1.56511817e-06
Iter: 1626 loss: 1.56470185e-06
Iter: 1627 loss: 1.56497163e-06
Iter: 1628 loss: 1.56454689e-06
Iter: 1629 loss: 1.56402245e-06
Iter: 1630 loss: 1.56704573e-06
Iter: 1631 loss: 1.5641732e-06
Iter: 1632 loss: 1.56382612e-06
Iter: 1633 loss: 1.56393014e-06
Iter: 1634 loss: 1.5635419e-06
Iter: 1635 loss: 1.56344709e-06
Iter: 1636 loss: 1.56343867e-06
Iter: 1637 loss: 1.5632005e-06
Iter: 1638 loss: 1.56291549e-06
Iter: 1639 loss: 1.5637961e-06
Iter: 1640 loss: 1.56281487e-06
Iter: 1641 loss: 1.56246608e-06
Iter: 1642 loss: 1.56327462e-06
Iter: 1643 loss: 1.56236729e-06
Iter: 1644 loss: 1.56202941e-06
Iter: 1645 loss: 1.56301678e-06
Iter: 1646 loss: 1.56212286e-06
Iter: 1647 loss: 1.56182773e-06
Iter: 1648 loss: 1.56239923e-06
Iter: 1649 loss: 1.56184592e-06
Iter: 1650 loss: 1.56141709e-06
Iter: 1651 loss: 1.56301076e-06
Iter: 1652 loss: 1.56145438e-06
Iter: 1653 loss: 1.56121359e-06
Iter: 1654 loss: 1.56089891e-06
Iter: 1655 loss: 1.56391889e-06
Iter: 1656 loss: 1.56082251e-06
Iter: 1657 loss: 1.56040596e-06
Iter: 1658 loss: 1.56283681e-06
Iter: 1659 loss: 1.56036481e-06
Iter: 1660 loss: 1.56005524e-06
Iter: 1661 loss: 1.56078613e-06
Iter: 1662 loss: 1.55985151e-06
Iter: 1663 loss: 1.55962209e-06
Iter: 1664 loss: 1.55963471e-06
Iter: 1665 loss: 1.55933617e-06
Iter: 1666 loss: 1.56014096e-06
Iter: 1667 loss: 1.55935209e-06
Iter: 1668 loss: 1.55924022e-06
Iter: 1669 loss: 1.5589826e-06
Iter: 1670 loss: 1.55900443e-06
Iter: 1671 loss: 1.55876864e-06
Iter: 1672 loss: 1.55968439e-06
Iter: 1673 loss: 1.55865143e-06
Iter: 1674 loss: 1.55845862e-06
Iter: 1675 loss: 1.55970724e-06
Iter: 1676 loss: 1.55838018e-06
Iter: 1677 loss: 1.55830287e-06
Iter: 1678 loss: 1.55807538e-06
Iter: 1679 loss: 1.5580672e-06
Iter: 1680 loss: 1.55811699e-06
Iter: 1681 loss: 1.55799455e-06
Iter: 1682 loss: 1.55789098e-06
Iter: 1683 loss: 1.55815985e-06
Iter: 1684 loss: 1.55795453e-06
Iter: 1685 loss: 1.55781231e-06
Iter: 1686 loss: 1.5575722e-06
Iter: 1687 loss: 1.56075407e-06
Iter: 1688 loss: 1.55760881e-06
Iter: 1689 loss: 1.55736234e-06
Iter: 1690 loss: 1.55736711e-06
Iter: 1691 loss: 1.5571668e-06
Iter: 1692 loss: 1.55682869e-06
Iter: 1693 loss: 1.5600358e-06
Iter: 1694 loss: 1.5568753e-06
Iter: 1695 loss: 1.55668636e-06
Iter: 1696 loss: 1.55755811e-06
Iter: 1697 loss: 1.55653743e-06
Iter: 1698 loss: 1.55631972e-06
Iter: 1699 loss: 1.55632711e-06
Iter: 1700 loss: 1.55629914e-06
Iter: 1701 loss: 1.5560239e-06
Iter: 1702 loss: 1.55926409e-06
Iter: 1703 loss: 1.5559234e-06
Iter: 1704 loss: 1.55567466e-06
Iter: 1705 loss: 1.55880559e-06
Iter: 1706 loss: 1.55570297e-06
Iter: 1707 loss: 1.55551334e-06
Iter: 1708 loss: 1.55544853e-06
Iter: 1709 loss: 1.55535815e-06
Iter: 1710 loss: 1.55514329e-06
Iter: 1711 loss: 1.55650355e-06
Iter: 1712 loss: 1.55504745e-06
Iter: 1713 loss: 1.55482076e-06
Iter: 1714 loss: 1.55603311e-06
Iter: 1715 loss: 1.55477403e-06
Iter: 1716 loss: 1.55456928e-06
Iter: 1717 loss: 1.55459543e-06
Iter: 1718 loss: 1.55436146e-06
Iter: 1719 loss: 1.55450789e-06
Iter: 1720 loss: 1.55428779e-06
Iter: 1721 loss: 1.55423618e-06
Iter: 1722 loss: 1.55407884e-06
Iter: 1723 loss: 1.55467751e-06
Iter: 1724 loss: 1.55388852e-06
Iter: 1725 loss: 1.55377302e-06
Iter: 1726 loss: 1.55364569e-06
Iter: 1727 loss: 1.55344935e-06
Iter: 1728 loss: 1.5531914e-06
Iter: 1729 loss: 1.5534265e-06
Iter: 1730 loss: 1.55308339e-06
Iter: 1731 loss: 1.55278531e-06
Iter: 1732 loss: 1.55464e-06
Iter: 1733 loss: 1.55277917e-06
Iter: 1734 loss: 1.55243026e-06
Iter: 1735 loss: 1.55377097e-06
Iter: 1736 loss: 1.55250189e-06
Iter: 1737 loss: 1.5522487e-06
Iter: 1738 loss: 1.55282896e-06
Iter: 1739 loss: 1.55212524e-06
Iter: 1740 loss: 1.55192936e-06
Iter: 1741 loss: 1.55168652e-06
Iter: 1742 loss: 1.55157977e-06
Iter: 1743 loss: 1.551183e-06
Iter: 1744 loss: 1.55467546e-06
Iter: 1745 loss: 1.55111024e-06
Iter: 1746 loss: 1.55075236e-06
Iter: 1747 loss: 1.5523525e-06
Iter: 1748 loss: 1.55071575e-06
Iter: 1749 loss: 1.55039788e-06
Iter: 1750 loss: 1.55120517e-06
Iter: 1751 loss: 1.55050634e-06
Iter: 1752 loss: 1.55013436e-06
Iter: 1753 loss: 1.55028397e-06
Iter: 1754 loss: 1.55011912e-06
Iter: 1755 loss: 1.55036764e-06
Iter: 1756 loss: 1.54997645e-06
Iter: 1757 loss: 1.54996144e-06
Iter: 1758 loss: 1.54984355e-06
Iter: 1759 loss: 1.54979466e-06
Iter: 1760 loss: 1.5498124e-06
Iter: 1761 loss: 1.5493888e-06
Iter: 1762 loss: 1.54916415e-06
Iter: 1763 loss: 1.5490325e-06
Iter: 1764 loss: 1.54859595e-06
Iter: 1765 loss: 1.55053181e-06
Iter: 1766 loss: 1.54863471e-06
Iter: 1767 loss: 1.54829399e-06
Iter: 1768 loss: 1.55015198e-06
Iter: 1769 loss: 1.54829809e-06
Iter: 1770 loss: 1.54791462e-06
Iter: 1771 loss: 1.54900636e-06
Iter: 1772 loss: 1.5478364e-06
Iter: 1773 loss: 1.54765144e-06
Iter: 1774 loss: 1.54740133e-06
Iter: 1775 loss: 1.547284e-06
Iter: 1776 loss: 1.54691338e-06
Iter: 1777 loss: 1.55052703e-06
Iter: 1778 loss: 1.54681186e-06
Iter: 1779 loss: 1.5465821e-06
Iter: 1780 loss: 1.54670943e-06
Iter: 1781 loss: 1.54647114e-06
Iter: 1782 loss: 1.5461078e-06
Iter: 1783 loss: 1.54628265e-06
Iter: 1784 loss: 1.54597944e-06
Iter: 1785 loss: 1.54580334e-06
Iter: 1786 loss: 1.54577128e-06
Iter: 1787 loss: 1.54566806e-06
Iter: 1788 loss: 1.54665713e-06
Iter: 1789 loss: 1.5456643e-06
Iter: 1790 loss: 1.54547683e-06
Iter: 1791 loss: 1.54577765e-06
Iter: 1792 loss: 1.54517534e-06
Iter: 1793 loss: 1.54510894e-06
Iter: 1794 loss: 1.54494933e-06
Iter: 1795 loss: 1.54494683e-06
Iter: 1796 loss: 1.5447074e-06
Iter: 1797 loss: 1.54492159e-06
Iter: 1798 loss: 1.54448958e-06
Iter: 1799 loss: 1.54413965e-06
Iter: 1800 loss: 1.5443984e-06
Iter: 1801 loss: 1.54390636e-06
Iter: 1802 loss: 1.54346731e-06
Iter: 1803 loss: 1.54348572e-06
Iter: 1804 loss: 1.54319423e-06
Iter: 1805 loss: 1.54423424e-06
Iter: 1806 loss: 1.54302791e-06
Iter: 1807 loss: 1.54293514e-06
Iter: 1808 loss: 1.54278428e-06
Iter: 1809 loss: 1.54268059e-06
Iter: 1810 loss: 1.54235545e-06
Iter: 1811 loss: 1.5436151e-06
Iter: 1812 loss: 1.54221425e-06
Iter: 1813 loss: 1.54188569e-06
Iter: 1814 loss: 1.54235636e-06
Iter: 1815 loss: 1.5417495e-06
Iter: 1816 loss: 1.54137354e-06
Iter: 1817 loss: 1.54277086e-06
Iter: 1818 loss: 1.54145982e-06
Iter: 1819 loss: 1.54117879e-06
Iter: 1820 loss: 1.54294139e-06
Iter: 1821 loss: 1.54115196e-06
Iter: 1822 loss: 1.54101667e-06
Iter: 1823 loss: 1.54303621e-06
Iter: 1824 loss: 1.54109057e-06
Iter: 1825 loss: 1.54100348e-06
Iter: 1826 loss: 1.54066765e-06
Iter: 1827 loss: 1.54371969e-06
Iter: 1828 loss: 1.54069642e-06
Iter: 1829 loss: 1.54039378e-06
Iter: 1830 loss: 1.54061e-06
Iter: 1831 loss: 1.54032114e-06
Iter: 1832 loss: 1.54007887e-06
Iter: 1833 loss: 1.54047007e-06
Iter: 1834 loss: 1.53982126e-06
Iter: 1835 loss: 1.53958194e-06
Iter: 1836 loss: 1.53976907e-06
Iter: 1837 loss: 1.53941983e-06
Iter: 1838 loss: 1.53906626e-06
Iter: 1839 loss: 1.5423775e-06
Iter: 1840 loss: 1.53908354e-06
Iter: 1841 loss: 1.53874589e-06
Iter: 1842 loss: 1.53944563e-06
Iter: 1843 loss: 1.5387036e-06
Iter: 1844 loss: 1.5383248e-06
Iter: 1845 loss: 1.53832491e-06
Iter: 1846 loss: 1.53815517e-06
Iter: 1847 loss: 1.53776648e-06
Iter: 1848 loss: 1.53876465e-06
Iter: 1849 loss: 1.53766155e-06
Iter: 1850 loss: 1.53748749e-06
Iter: 1851 loss: 1.53752012e-06
Iter: 1852 loss: 1.53725978e-06
Iter: 1853 loss: 1.53697056e-06
Iter: 1854 loss: 1.53956921e-06
Iter: 1855 loss: 1.53706594e-06
Iter: 1856 loss: 1.53694396e-06
Iter: 1857 loss: 1.53681265e-06
Iter: 1858 loss: 1.53671158e-06
Iter: 1859 loss: 1.53665326e-06
Iter: 1860 loss: 1.53666747e-06
Iter: 1861 loss: 1.53659335e-06
Iter: 1862 loss: 1.53632743e-06
Iter: 1863 loss: 1.53638814e-06
Iter: 1864 loss: 1.53600206e-06
Iter: 1865 loss: 1.53667725e-06
Iter: 1866 loss: 1.5359401e-06
Iter: 1867 loss: 1.53572796e-06
Iter: 1868 loss: 1.53691474e-06
Iter: 1869 loss: 1.53567623e-06
Iter: 1870 loss: 1.53535154e-06
Iter: 1871 loss: 1.53642395e-06
Iter: 1872 loss: 1.53537121e-06
Iter: 1873 loss: 1.53516589e-06
Iter: 1874 loss: 1.53625774e-06
Iter: 1875 loss: 1.53522569e-06
Iter: 1876 loss: 1.53511e-06
Iter: 1877 loss: 1.53454107e-06
Iter: 1878 loss: 1.54045733e-06
Iter: 1879 loss: 1.53454982e-06
Iter: 1880 loss: 1.53430506e-06
Iter: 1881 loss: 1.53431085e-06
Iter: 1882 loss: 1.53409724e-06
Iter: 1883 loss: 1.53405256e-06
Iter: 1884 loss: 1.53392421e-06
Iter: 1885 loss: 1.53357018e-06
Iter: 1886 loss: 1.53322253e-06
Iter: 1887 loss: 1.53324231e-06
Iter: 1888 loss: 1.53370547e-06
Iter: 1889 loss: 1.53319047e-06
Iter: 1890 loss: 1.53307678e-06
Iter: 1891 loss: 1.53304518e-06
Iter: 1892 loss: 1.53298583e-06
Iter: 1893 loss: 1.53275755e-06
Iter: 1894 loss: 1.53282588e-06
Iter: 1895 loss: 1.53273731e-06
Iter: 1896 loss: 1.53249778e-06
Iter: 1897 loss: 1.53221993e-06
Iter: 1898 loss: 1.53221117e-06
Iter: 1899 loss: 1.53189058e-06
Iter: 1900 loss: 1.53394558e-06
Iter: 1901 loss: 1.53186306e-06
Iter: 1902 loss: 1.53157714e-06
Iter: 1903 loss: 1.53201654e-06
Iter: 1904 loss: 1.53144515e-06
Iter: 1905 loss: 1.53123494e-06
Iter: 1906 loss: 1.53230462e-06
Iter: 1907 loss: 1.53108635e-06
Iter: 1908 loss: 1.53081407e-06
Iter: 1909 loss: 1.53224232e-06
Iter: 1910 loss: 1.53071983e-06
Iter: 1911 loss: 1.5306116e-06
Iter: 1912 loss: 1.53216047e-06
Iter: 1913 loss: 1.53053861e-06
Iter: 1914 loss: 1.53045926e-06
Iter: 1915 loss: 1.53036603e-06
Iter: 1916 loss: 1.53028009e-06
Iter: 1917 loss: 1.5299845e-06
Iter: 1918 loss: 1.53098154e-06
Iter: 1919 loss: 1.52992322e-06
Iter: 1920 loss: 1.5298308e-06
Iter: 1921 loss: 1.53029919e-06
Iter: 1922 loss: 1.52965845e-06
Iter: 1923 loss: 1.52961582e-06
Iter: 1924 loss: 1.52949065e-06
Iter: 1925 loss: 1.52946609e-06
Iter: 1926 loss: 1.52933512e-06
Iter: 1927 loss: 1.53194037e-06
Iter: 1928 loss: 1.52930352e-06
Iter: 1929 loss: 1.52918335e-06
Iter: 1930 loss: 1.5291771e-06
Iter: 1931 loss: 1.52892517e-06
Iter: 1932 loss: 1.52869461e-06
Iter: 1933 loss: 1.52892085e-06
Iter: 1934 loss: 1.52861764e-06
Iter: 1935 loss: 1.52832672e-06
Iter: 1936 loss: 1.52889629e-06
Iter: 1937 loss: 1.5282186e-06
Iter: 1938 loss: 1.52797315e-06
Iter: 1939 loss: 1.52947064e-06
Iter: 1940 loss: 1.52790903e-06
Iter: 1941 loss: 1.52771645e-06
Iter: 1942 loss: 1.52916869e-06
Iter: 1943 loss: 1.52767291e-06
Iter: 1944 loss: 1.5274511e-06
Iter: 1945 loss: 1.52815278e-06
Iter: 1946 loss: 1.52738403e-06
Iter: 1947 loss: 1.5270532e-06
Iter: 1948 loss: 1.52746748e-06
Iter: 1949 loss: 1.52711573e-06
Iter: 1950 loss: 1.52678149e-06
Iter: 1951 loss: 1.52684834e-06
Iter: 1952 loss: 1.5266445e-06
Iter: 1953 loss: 1.52628e-06
Iter: 1954 loss: 1.52964571e-06
Iter: 1955 loss: 1.52631242e-06
Iter: 1956 loss: 1.52626853e-06
Iter: 1957 loss: 1.52619305e-06
Iter: 1958 loss: 1.52620282e-06
Iter: 1959 loss: 1.52624591e-06
Iter: 1960 loss: 1.52619009e-06
Iter: 1961 loss: 1.52615769e-06
Iter: 1962 loss: 1.52617e-06
Iter: 1963 loss: 1.52625421e-06
Iter: 1964 loss: 1.52619759e-06
Iter: 1965 loss: 1.52621635e-06
Iter: 1966 loss: 1.52616553e-06
Iter: 1967 loss: 1.52619873e-06
Iter: 1968 loss: 1.52618509e-06
Iter: 1969 loss: 1.52617861e-06
Iter: 1970 loss: 1.52623875e-06
Iter: 1971 loss: 1.52620032e-06
Iter: 1972 loss: 1.526186e-06
Iter: 1973 loss: 1.52619566e-06
Iter: 1974 loss: 1.52618406e-06
Iter: 1975 loss: 1.52619032e-06
Iter: 1976 loss: 1.52619509e-06
Iter: 1977 loss: 1.52619464e-06
Iter: 1978 loss: 1.52619418e-06
Iter: 1979 loss: 1.52619418e-06
Iter: 1980 loss: 1.52619418e-06
Iter: 1981 loss: 1.52619418e-06
Iter: 1982 loss: 1.52619464e-06
Iter: 1983 loss: 1.52619418e-06
Iter: 1984 loss: 1.52619464e-06
Iter: 1985 loss: 1.52559755e-06
Iter: 1986 loss: 1.53062979e-06
Iter: 1987 loss: 1.52556663e-06
Iter: 1988 loss: 1.52529253e-06
Iter: 1989 loss: 1.52510324e-06
Iter: 1990 loss: 1.52491805e-06
Iter: 1991 loss: 1.52451469e-06
Iter: 1992 loss: 1.5251976e-06
Iter: 1993 loss: 1.52431335e-06
Iter: 1994 loss: 1.52403481e-06
Iter: 1995 loss: 1.52521602e-06
Iter: 1996 loss: 1.52384894e-06
Iter: 1997 loss: 1.52348446e-06
Iter: 1998 loss: 1.52325197e-06
Iter: 1999 loss: 1.5231376e-06
Iter: 2000 loss: 1.5228859e-06
Iter: 2001 loss: 1.52313874e-06
Iter: 2002 loss: 1.5227298e-06
Iter: 2003 loss: 1.52253369e-06
Iter: 2004 loss: 1.52366829e-06
Iter: 2005 loss: 1.52233156e-06
Iter: 2006 loss: 1.52196503e-06
Iter: 2007 loss: 1.52216774e-06
Iter: 2008 loss: 1.52172822e-06
Iter: 2009 loss: 1.52142866e-06
Iter: 2010 loss: 1.52298674e-06
Iter: 2011 loss: 1.52133555e-06
Iter: 2012 loss: 1.52123425e-06
Iter: 2013 loss: 1.52239625e-06
Iter: 2014 loss: 1.52117639e-06
Iter: 2015 loss: 1.52104508e-06
Iter: 2016 loss: 1.52078508e-06
Iter: 2017 loss: 1.52079065e-06
Iter: 2018 loss: 1.52053337e-06
Iter: 2019 loss: 1.52360553e-06
Iter: 2020 loss: 1.52048483e-06
Iter: 2021 loss: 1.52025677e-06
Iter: 2022 loss: 1.52034522e-06
Iter: 2023 loss: 1.52013104e-06
Iter: 2024 loss: 1.51984295e-06
Iter: 2025 loss: 1.52049256e-06
Iter: 2026 loss: 1.51965776e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.8
+ date
Wed Oct 21 15:59:35 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/300_300_300_1 --function f1 --psi 0 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5a4e9b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5a4e46840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e73469d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e7346598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5a4e3d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5a4d71048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5a4dd60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580650730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580650598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580650510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5805dd2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5804ef9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5805078c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580570048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580576d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580576e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580558ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5805651e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5805cfae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580565048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5804430d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5804550d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc58047cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5803cd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5803cdd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5803cde18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5803cd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5802e4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5802e4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5803867b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580380268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580263f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc580263730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5802f7d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5801d20d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc58016ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.08452507
test_loss: 0.08443516
train_loss: 0.042741194
test_loss: 0.040549275
train_loss: 0.023766264
test_loss: 0.022344308
train_loss: 0.013687891
test_loss: 0.012774341
train_loss: 0.008842886
test_loss: 0.009429452
train_loss: 0.0075382777
test_loss: 0.0073253484
train_loss: 0.006230848
test_loss: 0.0058036223
train_loss: 0.005321618
test_loss: 0.006907359
train_loss: 0.0056100455
test_loss: 0.0055460846
train_loss: 0.00505292
test_loss: 0.004562035
train_loss: 0.004847609
test_loss: 0.0053201937
train_loss: 0.0044560716
test_loss: 0.0042089485
train_loss: 0.0041855844
test_loss: 0.004376063
train_loss: 0.0048751985
test_loss: 0.0049506878
train_loss: 0.004115651
test_loss: 0.0047062077
train_loss: 0.004184537
test_loss: 0.0039057264
train_loss: 0.0037306484
test_loss: 0.0035374092
train_loss: 0.0043553626
test_loss: 0.0041116816
train_loss: 0.003814023
test_loss: 0.0060934373
train_loss: 0.00382631
test_loss: 0.004154601
train_loss: 0.0035850988
test_loss: 0.0039249407
train_loss: 0.0039567277
test_loss: 0.0038469597
train_loss: 0.0037426644
test_loss: 0.0034815266
train_loss: 0.003911185
test_loss: 0.0035998786
train_loss: 0.0035184436
test_loss: 0.003442887
train_loss: 0.003497784
test_loss: 0.003509353
train_loss: 0.0034172842
test_loss: 0.0035032793
train_loss: 0.0037543927
test_loss: 0.0042764335
train_loss: 0.0035710046
test_loss: 0.0033975432
train_loss: 0.0029881625
test_loss: 0.0034366287
train_loss: 0.0036396391
test_loss: 0.003060993
train_loss: 0.0031830422
test_loss: 0.0034854338
train_loss: 0.0038290913
test_loss: 0.004356102
train_loss: 0.0039343135
test_loss: 0.005452012
train_loss: 0.0041792677
test_loss: 0.0042446894
train_loss: 0.0041377004
test_loss: 0.003929287
train_loss: 0.003957173
test_loss: 0.004084968
train_loss: 0.0033931388
test_loss: 0.0035510312
train_loss: 0.0038844538
test_loss: 0.003616273
train_loss: 0.0028506352
test_loss: 0.003683773
train_loss: 0.0033307471
test_loss: 0.0035084567
train_loss: 0.003188287
test_loss: 0.003555753
train_loss: 0.0034279155
test_loss: 0.0033506134
train_loss: 0.0034186235
test_loss: 0.003707998
train_loss: 0.0036865529
test_loss: 0.0037903804
train_loss: 0.003131944
test_loss: 0.003172097
train_loss: 0.0032188087
test_loss: 0.0038509506
train_loss: 0.004243507
test_loss: 0.0042987783
train_loss: 0.0034017998
test_loss: 0.0031798477
train_loss: 0.0033607818
test_loss: 0.0034620096
train_loss: 0.0036095427
test_loss: 0.0032421655
train_loss: 0.0034349563
test_loss: 0.0032717807
train_loss: 0.003734706
test_loss: 0.0030753138
train_loss: 0.0030556142
test_loss: 0.0037373044
train_loss: 0.0027719294
test_loss: 0.003304058
train_loss: 0.0035965182
test_loss: 0.0037148728
train_loss: 0.00408367
test_loss: 0.0032784427
train_loss: 0.0033386713
test_loss: 0.0028616476
train_loss: 0.0039760564
test_loss: 0.0037324266
train_loss: 0.0032613075
test_loss: 0.003109426
train_loss: 0.0028340751
test_loss: 0.0030227625
train_loss: 0.0034393817
test_loss: 0.0036953995
train_loss: 0.00285911
test_loss: 0.0029362224
train_loss: 0.003512149
test_loss: 0.0033019416
train_loss: 0.0037413589
test_loss: 0.0032778226
train_loss: 0.003809669
test_loss: 0.0054633664
train_loss: 0.003526155
test_loss: 0.003341542
train_loss: 0.0035593174
test_loss: 0.0030573318
train_loss: 0.0037645316
test_loss: 0.0033809019
train_loss: 0.0031327177
test_loss: 0.0032070777
train_loss: 0.0031746929
test_loss: 0.0033454406
train_loss: 0.0030399454
test_loss: 0.003494038
train_loss: 0.003198822
test_loss: 0.0032912563
train_loss: 0.0031590099
test_loss: 0.0029397246
train_loss: 0.003764185
test_loss: 0.0041372296
train_loss: 0.0033516516
test_loss: 0.0028619417
train_loss: 0.0029471128
test_loss: 0.002558973
train_loss: 0.0036233906
test_loss: 0.0034191424
train_loss: 0.0032406757
test_loss: 0.0031716917
train_loss: 0.0035441464
test_loss: 0.0030385328
train_loss: 0.0037057698
test_loss: 0.0033937243
train_loss: 0.0027382397
test_loss: 0.003826509
train_loss: 0.0031375145
test_loss: 0.0037129228
train_loss: 0.002845238
test_loss: 0.0034479967
train_loss: 0.003606311
test_loss: 0.0033772218
train_loss: 0.004327141
test_loss: 0.0042068884
train_loss: 0.0029922607
test_loss: 0.0029764327
train_loss: 0.003211014
test_loss: 0.0033845818
train_loss: 0.002837438
test_loss: 0.002893888
train_loss: 0.0025293636
test_loss: 0.0028976793
train_loss: 0.0038503958
test_loss: 0.00431471
train_loss: 0.00603823
test_loss: 0.004818833
train_loss: 0.004018162
test_loss: 0.004049921
train_loss: 0.0034226242
test_loss: 0.0034997633
train_loss: 0.0027526356
test_loss: 0.0029490239
train_loss: 0.0036787752
test_loss: 0.0034597197
train_loss: 0.0032392105
test_loss: 0.0034252545
train_loss: 0.0037699554
test_loss: 0.0036410484
train_loss: 0.0030100655
test_loss: 0.003153806
train_loss: 0.002783502
test_loss: 0.00281283
train_loss: 0.0029158469
test_loss: 0.0032426768
train_loss: 0.0028854609
test_loss: 0.0030937267
train_loss: 0.002993808
test_loss: 0.0032809374
train_loss: 0.003238542
test_loss: 0.0034349405
train_loss: 0.0034842214
test_loss: 0.0030603067
train_loss: 0.003453494
test_loss: 0.003441043
train_loss: 0.0028650104
test_loss: 0.003101629
train_loss: 0.0030894922
test_loss: 0.0031817367
train_loss: 0.0041739005
test_loss: 0.0031542806
train_loss: 0.002972883
test_loss: 0.0025603718
train_loss: 0.0036032062
test_loss: 0.0029436965
train_loss: 0.003386917
test_loss: 0.0029138108
train_loss: 0.0030503804
test_loss: 0.0029925194
train_loss: 0.0039528366
test_loss: 0.0030511422
train_loss: 0.0036019236
test_loss: 0.0031698556
train_loss: 0.0042652125
test_loss: 0.005003578
train_loss: 0.0031039915
test_loss: 0.0030518174
train_loss: 0.0028373608
test_loss: 0.0028865393
train_loss: 0.0028455474
test_loss: 0.0027264033
train_loss: 0.0029141866
test_loss: 0.0029465412
train_loss: 0.0027444246
test_loss: 0.002961651
train_loss: 0.003088989
test_loss: 0.0030922492
train_loss: 0.0027519858
test_loss: 0.0031374905
train_loss: 0.0031928048
test_loss: 0.002838124
train_loss: 0.002637058
test_loss: 0.003434618
train_loss: 0.0028831726
test_loss: 0.0028821556
train_loss: 0.0027903328
test_loss: 0.0027952113
train_loss: 0.0028576748
test_loss: 0.0029145062
train_loss: 0.0031421045
test_loss: 0.0029938992
train_loss: 0.0029540933
test_loss: 0.003397649
train_loss: 0.0036678608
test_loss: 0.003422615
train_loss: 0.002766089
test_loss: 0.0031020455
train_loss: 0.0026871013
test_loss: 0.0025508401
train_loss: 0.0027302457
test_loss: 0.0026808
train_loss: 0.0027168903
test_loss: 0.0031550392
train_loss: 0.003538124
test_loss: 0.003782108
train_loss: 0.0025547473
test_loss: 0.002839375
train_loss: 0.003564859
test_loss: 0.003720334
train_loss: 0.0032495428
test_loss: 0.003331375
train_loss: 0.002913308
test_loss: 0.0033478688
train_loss: 0.0030349272
test_loss: 0.0033703737
train_loss: 0.0029100406
test_loss: 0.0026099337
train_loss: 0.0032411835
test_loss: 0.002810971
train_loss: 0.002872211
test_loss: 0.0029097826
train_loss: 0.0029032137
test_loss: 0.002710319
train_loss: 0.003363348
test_loss: 0.0034530677
train_loss: 0.003024056
test_loss: 0.0031894622
train_loss: 0.0030206547
test_loss: 0.002402669
train_loss: 0.0030070292
test_loss: 0.0028005168
train_loss: 0.00331175
test_loss: 0.00482592
train_loss: 0.004236859
test_loss: 0.0042303046
train_loss: 0.004057517
test_loss: 0.005148525
train_loss: 0.0036128252
test_loss: 0.002962852
train_loss: 0.0024765658
test_loss: 0.0028433606
train_loss: 0.0032669127
test_loss: 0.0029144203
train_loss: 0.0028996756
test_loss: 0.0029203873
train_loss: 0.0029394631
test_loss: 0.0028785835
train_loss: 0.003251946
test_loss: 0.0027611349
train_loss: 0.0024371059
test_loss: 0.0032066233
train_loss: 0.0022804309
test_loss: 0.0025646945
train_loss: 0.002824965
test_loss: 0.0034500964
train_loss: 0.0031439424
test_loss: 0.0033614272
train_loss: 0.0029015443
test_loss: 0.0031659552
train_loss: 0.0028291517
test_loss: 0.0028155118
train_loss: 0.0027322995
test_loss: 0.0029924675
train_loss: 0.0027420314
test_loss: 0.0029684703
train_loss: 0.0029101097
test_loss: 0.0038405028
train_loss: 0.0027702018
test_loss: 0.0035577228
train_loss: 0.0027404963
test_loss: 0.0031230405
train_loss: 0.0029671653
test_loss: 0.0027424288
train_loss: 0.002868209
test_loss: 0.0026599236
train_loss: 0.0026757075
test_loss: 0.002705654
train_loss: 0.0029909755
test_loss: 0.003051093
train_loss: 0.0030215518
test_loss: 0.003290884
train_loss: 0.0030399142
test_loss: 0.0033281383
train_loss: 0.00320299
test_loss: 0.0028888676
train_loss: 0.0029896067
test_loss: 0.0038217516
train_loss: 0.0029725274
test_loss: 0.0030778067
train_loss: 0.0028032542
test_loss: 0.002937099
train_loss: 0.0023758488
test_loss: 0.0026069963
train_loss: 0.0025273166
test_loss: 0.002607942
train_loss: 0.003159102
test_loss: 0.0035858967
train_loss: 0.0023175322
test_loss: 0.0028203484
train_loss: 0.0025368128
test_loss: 0.0029230828
train_loss: 0.003881476
test_loss: 0.0035832925
train_loss: 0.002920283
test_loss: 0.0031947223
train_loss: 0.0024695958
test_loss: 0.0022589448
train_loss: 0.0024197882
test_loss: 0.0028527777
train_loss: 0.0026551427
test_loss: 0.0026952664
train_loss: 0.0034722702
test_loss: 0.0029621911
train_loss: 0.0033183782
test_loss: 0.0029743027
train_loss: 0.0051465724
test_loss: 0.004726388
train_loss: 0.0035840797
test_loss: 0.003288585
train_loss: 0.0028158878
test_loss: 0.0031740575
train_loss: 0.0030173294
test_loss: 0.002882958
train_loss: 0.0027825637
test_loss: 0.0025064663
train_loss: 0.0029584747
test_loss: 0.0031092544
train_loss: 0.003030884
test_loss: 0.0029368554
train_loss: 0.0034229965
test_loss: 0.0031369603
train_loss: 0.0030058615
test_loss: 0.003408906
train_loss: 0.0026179948
test_loss: 0.002445794
train_loss: 0.00271885
test_loss: 0.002649263
train_loss: 0.0025972966
test_loss: 0.0029633786
train_loss: 0.0029063625
test_loss: 0.002729126
train_loss: 0.003273026
test_loss: 0.003316308
train_loss: 0.0024727623
test_loss: 0.003234519
train_loss: 0.0027046497
test_loss: 0.0025084743
train_loss: 0.0023960778
test_loss: 0.0025869897
train_loss: 0.0028638323
test_loss: 0.003274252
train_loss: 0.003028273
test_loss: 0.0031050623
train_loss: 0.002656296
test_loss: 0.0027762672
train_loss: 0.0028051687
test_loss: 0.0025523019
train_loss: 0.0027889814
test_loss: 0.002726892
train_loss: 0.002324146
test_loss: 0.0025556355
train_loss: 0.0034850244
test_loss: 0.002742323
train_loss: 0.0032278392
test_loss: 0.0026327888
train_loss: 0.002829197
test_loss: 0.0025525622
train_loss: 0.0027580431
test_loss: 0.002973162
train_loss: 0.002400578
test_loss: 0.002870521
train_loss: 0.003159065
test_loss: 0.00254549
train_loss: 0.0030081237
test_loss: 0.0031720044
train_loss: 0.0027167085
test_loss: 0.0030622703
train_loss: 0.003112236
test_loss: 0.0032698372
train_loss: 0.0030343325
test_loss: 0.0041128397
train_loss: 0.0032719746
test_loss: 0.005155817
train_loss: 0.003307274
test_loss: 0.0030500905
train_loss: 0.002611138
test_loss: 0.0036046163
train_loss: 0.0027566026
test_loss: 0.0027965442
train_loss: 0.0029959339
test_loss: 0.0027838058
train_loss: 0.002433658
test_loss: 0.002486698
train_loss: 0.0025676729
test_loss: 0.0031471062
train_loss: 0.002906741
test_loss: 0.0034401414
train_loss: 0.0029887182
test_loss: 0.0029231124
train_loss: 0.0030481573
test_loss: 0.003028136
train_loss: 0.0035212752
test_loss: 0.0032900113
train_loss: 0.0031384276
test_loss: 0.0030061568
train_loss: 0.0024328576
test_loss: 0.0029662964
train_loss: 0.003294739
test_loss: 0.0028551465
train_loss: 0.0037670555
test_loss: 0.0044732783
train_loss: 0.0028024719
test_loss: 0.0031074937
train_loss: 0.0024601952
test_loss: 0.0025478764
train_loss: 0.0030846917
test_loss: 0.0034119198
train_loss: 0.0029224725
test_loss: 0.0035708027
train_loss: 0.004810608
test_loss: 0.004503269
train_loss: 0.0026404958
test_loss: 0.0027709932
train_loss: 0.0024907745
test_loss: 0.0023442234
train_loss: 0.0029904034
test_loss: 0.0030618045
train_loss: 0.0032996912
test_loss: 0.0034109727
train_loss: 0.0024599188
test_loss: 0.0025673562
train_loss: 0.003256033
test_loss: 0.0034505632
train_loss: 0.003186444
test_loss: 0.0029669127
train_loss: 0.0027473047
test_loss: 0.0023534372
train_loss: 0.003013692
test_loss: 0.002956175
train_loss: 0.0024410707
test_loss: 0.0027354178
train_loss: 0.002659611
test_loss: 0.0029440948
train_loss: 0.0023567341
test_loss: 0.0026750602
train_loss: 0.0033705495
test_loss: 0.0032384458
train_loss: 0.003157788
test_loss: 0.0042411606
train_loss: 0.003381858
test_loss: 0.0033814765
train_loss: 0.0030015425
test_loss: 0.0036426308
train_loss: 0.002827919
test_loss: 0.0028148307
train_loss: 0.0026812484
test_loss: 0.0026986825
train_loss: 0.0027149012
test_loss: 0.0029992412
train_loss: 0.002739259
test_loss: 0.0032302754
train_loss: 0.0026873392
test_loss: 0.0026215455
train_loss: 0.0028632984
test_loss: 0.0025595368
train_loss: 0.0029402692
test_loss: 0.0034395757
train_loss: 0.0028152545
test_loss: 0.003119389
train_loss: 0.0024370947
test_loss: 0.0023789166
train_loss: 0.002408974
test_loss: 0.0026979141
train_loss: 0.0025745365
test_loss: 0.0026391305
train_loss: 0.0031455786
test_loss: 0.0025949434
train_loss: 0.0025956566
test_loss: 0.002889939
train_loss: 0.0034090602
test_loss: 0.0028439898
train_loss: 0.002594349
test_loss: 0.0024989294
train_loss: 0.002584768
test_loss: 0.0023210319
train_loss: 0.0025777132
test_loss: 0.002789627
train_loss: 0.0032504953
test_loss: 0.0030748665
train_loss: 0.0031190808
test_loss: 0.0028082097
train_loss: 0.002276792
test_loss: 0.0026385426
train_loss: 0.0025598311
test_loss: 0.0027748204
train_loss: 0.0027397403
test_loss: 0.0030029532
train_loss: 0.0034187487
test_loss: 0.003096954
train_loss: 0.0029314177
test_loss: 0.003162365
train_loss: 0.00251572
test_loss: 0.0028268464
train_loss: 0.0045624655
test_loss: 0.004419102
train_loss: 0.0033323523
test_loss: 0.0030616052
train_loss: 0.0027834943
test_loss: 0.002904404
train_loss: 0.0021566122
test_loss: 0.0024583922
train_loss: 0.0027486887
test_loss: 0.0024397208
train_loss: 0.0028203505
test_loss: 0.002927131
train_loss: 0.0030998853
test_loss: 0.0032054603
train_loss: 0.0030341973
test_loss: 0.00324369
train_loss: 0.002682727
test_loss: 0.002746637
train_loss: 0.0027584445
test_loss: 0.0031646932
train_loss: 0.0027795671
test_loss: 0.0025095374
train_loss: 0.0027928902
test_loss: 0.0026374194
train_loss: 0.0027300646
test_loss: 0.0030447175
train_loss: 0.0023619276
test_loss: 0.0025824555
train_loss: 0.0028952109
test_loss: 0.0026689786
train_loss: 0.0025200057
test_loss: 0.002265466
train_loss: 0.0022462276
test_loss: 0.0025295038
train_loss: 0.0030640105
test_loss: 0.0027534652
train_loss: 0.0024418528
test_loss: 0.00275537
train_loss: 0.0021709339
test_loss: 0.00223105
train_loss: 0.0030804262
test_loss: 0.0025430894
train_loss: 0.0022704017
test_loss: 0.0021442967
train_loss: 0.002213111
test_loss: 0.0026521147
train_loss: 0.0037475205
test_loss: 0.0023473385
train_loss: 0.0029936954
test_loss: 0.0031231353
train_loss: 0.002682073
test_loss: 0.0026227839
train_loss: 0.003159942
test_loss: 0.003176955
train_loss: 0.00273956
test_loss: 0.0028386335
train_loss: 0.0021789626
test_loss: 0.0027361535
train_loss: 0.0028235686
test_loss: 0.0026056254
train_loss: 0.002738581
test_loss: 0.0026809198
train_loss: 0.0023543078
test_loss: 0.0028216345
train_loss: 0.0029124161
test_loss: 0.0027608855
train_loss: 0.0025926703
test_loss: 0.0032439837
train_loss: 0.0031228678
test_loss: 0.0028993832
train_loss: 0.0027571209
test_loss: 0.0031025568
train_loss: 0.003281828
test_loss: 0.0031207525
train_loss: 0.0025335636
test_loss: 0.0029596908
train_loss: 0.0031452011
test_loss: 0.0032613869
train_loss: 0.0034380872
test_loss: 0.002588218
train_loss: 0.0043274746
test_loss: 0.004088325
train_loss: 0.0034162519
test_loss: 0.004257993
train_loss: 0.0026132572
test_loss: 0.0025685334
train_loss: 0.003345201
test_loss: 0.0037832158
train_loss: 0.0037147696
test_loss: 0.0030999738
train_loss: 0.0027059438
test_loss: 0.0027681277
train_loss: 0.0030486528
test_loss: 0.0026016268
train_loss: 0.0019923353
test_loss: 0.0023383745
train_loss: 0.002132855
test_loss: 0.002359815
train_loss: 0.0027371878
test_loss: 0.0022853173
train_loss: 0.002316924
test_loss: 0.0029418292
train_loss: 0.0030398928
test_loss: 0.002306864
train_loss: 0.0028796936
test_loss: 0.0025766934
train_loss: 0.0027323146
test_loss: 0.0028300013/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

train_loss: 0.0022770446
test_loss: 0.002557162
train_loss: 0.002434239
test_loss: 0.0024127378
train_loss: 0.0024283603
test_loss: 0.0026023213
train_loss: 0.0035124589
test_loss: 0.0031678628
train_loss: 0.0028218941
test_loss: 0.0028755006
train_loss: 0.002905932
test_loss: 0.0024352307
train_loss: 0.00237525
test_loss: 0.0023561027
train_loss: 0.002163876
test_loss: 0.002416069
train_loss: 0.0034446414
test_loss: 0.0033396916
train_loss: 0.0030583283
test_loss: 0.0027206277
train_loss: 0.002720805
test_loss: 0.0027340131
train_loss: 0.0028966514
test_loss: 0.002447911
train_loss: 0.0021912325
test_loss: 0.0023524843
train_loss: 0.0027419594
test_loss: 0.0023269493
train_loss: 0.0037861345
test_loss: 0.0037317588
train_loss: 0.0033298433
test_loss: 0.0027346073
train_loss: 0.002078266
test_loss: 0.0023754097
train_loss: 0.002475538
test_loss: 0.0025580314
train_loss: 0.0030108772
test_loss: 0.0025244404
train_loss: 0.0019655025
test_loss: 0.002400705
train_loss: 0.0029394869
test_loss: 0.0034549101
train_loss: 0.0027560391
test_loss: 0.002912144
train_loss: 0.0031413466
test_loss: 0.0033733894
train_loss: 0.002221328
test_loss: 0.002804048
train_loss: 0.002595012
test_loss: 0.0025429453
train_loss: 0.0024010951
test_loss: 0.002250401
train_loss: 0.002263416
test_loss: 0.0023082949
train_loss: 0.0021730852
test_loss: 0.0022583182
train_loss: 0.0022396394
test_loss: 0.0023083354
train_loss: 0.0025333217
test_loss: 0.0023709035
train_loss: 0.0025356724
test_loss: 0.0025796022
train_loss: 0.002804085
test_loss: 0.0026756532
train_loss: 0.0026072646
test_loss: 0.002586949
train_loss: 0.0025027276
test_loss: 0.002953629
train_loss: 0.002640729
test_loss: 0.0021138156
train_loss: 0.0027492614
test_loss: 0.00285358
train_loss: 0.002633947
test_loss: 0.003430757
train_loss: 0.0029437598
test_loss: 0.0033999365
train_loss: 0.0029148217
test_loss: 0.002347378
train_loss: 0.0029319115
test_loss: 0.002762747
train_loss: 0.002788023
test_loss: 0.0031650346
train_loss: 0.0026425913
test_loss: 0.0024567365
train_loss: 0.0029115435
test_loss: 0.0025034097
train_loss: 0.0029716967
test_loss: 0.0035163963
train_loss: 0.003298915
test_loss: 0.0029692848
train_loss: 0.0023301297
test_loss: 0.0027485758
train_loss: 0.0024834215
test_loss: 0.0026454052
train_loss: 0.002663275
test_loss: 0.0026977563
train_loss: 0.0033387677
test_loss: 0.0028410852
train_loss: 0.002333371
test_loss: 0.0025647734
train_loss: 0.0036509968
test_loss: 0.0029608908
train_loss: 0.0025409102
test_loss: 0.0026372268
train_loss: 0.0027632085
test_loss: 0.002364499
train_loss: 0.002452813
test_loss: 0.0025362105
train_loss: 0.0031739632
test_loss: 0.0027274273
train_loss: 0.0040170574
test_loss: 0.0038490382
train_loss: 0.0032284837
test_loss: 0.0025114666
train_loss: 0.002408422
test_loss: 0.0025011715
train_loss: 0.0031686923
test_loss: 0.0026920906
train_loss: 0.0033368904
test_loss: 0.0027366616
train_loss: 0.0023896683
test_loss: 0.002272545
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c7296400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c72549d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c71d79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c72f96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c719bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c719b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c715eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c7125048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c71bf598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c71bfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c70fd1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c70f4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c70f4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c7073048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c706e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c703b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c70f4e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c70a7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c7007b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93c7073400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd9cda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd9830d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd9cd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd94f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd94f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd94fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd94f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd8dc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd8dcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd88b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd89c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd80cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd80c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93bd840d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93733640d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93732fcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 7.89715523e-06
Iter: 2 loss: 2.53828766e-05
Iter: 3 loss: 6.32508454e-06
Iter: 4 loss: 5.49849574e-06
Iter: 5 loss: 4.90394723e-06
Iter: 6 loss: 4.61843501e-06
Iter: 7 loss: 3.96236464e-06
Iter: 8 loss: 3.28641454e-06
Iter: 9 loss: 3.16156229e-06
Iter: 10 loss: 2.62821686e-06
Iter: 11 loss: 2.62716253e-06
Iter: 12 loss: 2.46858099e-06
Iter: 13 loss: 3.46108163e-06
Iter: 14 loss: 2.45036313e-06
Iter: 15 loss: 2.31603281e-06
Iter: 16 loss: 2.32731463e-06
Iter: 17 loss: 2.21192818e-06
Iter: 18 loss: 2.07887774e-06
Iter: 19 loss: 3.16828391e-06
Iter: 20 loss: 2.07052562e-06
Iter: 21 loss: 2.00343516e-06
Iter: 22 loss: 1.94322433e-06
Iter: 23 loss: 1.92655534e-06
Iter: 24 loss: 1.85451347e-06
Iter: 25 loss: 1.85256022e-06
Iter: 26 loss: 1.81600456e-06
Iter: 27 loss: 1.89088462e-06
Iter: 28 loss: 1.80126108e-06
Iter: 29 loss: 1.76352341e-06
Iter: 30 loss: 1.78267908e-06
Iter: 31 loss: 1.73835451e-06
Iter: 32 loss: 1.70076078e-06
Iter: 33 loss: 1.9354311e-06
Iter: 34 loss: 1.69632926e-06
Iter: 35 loss: 1.68679344e-06
Iter: 36 loss: 1.68250972e-06
Iter: 37 loss: 1.66566633e-06
Iter: 38 loss: 1.62202684e-06
Iter: 39 loss: 1.96124392e-06
Iter: 40 loss: 1.61359037e-06
Iter: 41 loss: 1.57371915e-06
Iter: 42 loss: 1.67256906e-06
Iter: 43 loss: 1.55961141e-06
Iter: 44 loss: 1.53068913e-06
Iter: 45 loss: 1.52411076e-06
Iter: 46 loss: 1.50534174e-06
Iter: 47 loss: 1.48542563e-06
Iter: 48 loss: 1.48257254e-06
Iter: 49 loss: 1.46807474e-06
Iter: 50 loss: 1.45134777e-06
Iter: 51 loss: 1.44934506e-06
Iter: 52 loss: 1.42464432e-06
Iter: 53 loss: 1.6778672e-06
Iter: 54 loss: 1.42394128e-06
Iter: 55 loss: 1.4121423e-06
Iter: 56 loss: 1.48354707e-06
Iter: 57 loss: 1.4106638e-06
Iter: 58 loss: 1.39819645e-06
Iter: 59 loss: 1.37202801e-06
Iter: 60 loss: 1.802935e-06
Iter: 61 loss: 1.37122038e-06
Iter: 62 loss: 1.36667e-06
Iter: 63 loss: 1.36307813e-06
Iter: 64 loss: 1.35614e-06
Iter: 65 loss: 1.35029154e-06
Iter: 66 loss: 1.34832703e-06
Iter: 67 loss: 1.34094614e-06
Iter: 68 loss: 1.34065976e-06
Iter: 69 loss: 1.33673495e-06
Iter: 70 loss: 1.33661911e-06
Iter: 71 loss: 1.33473418e-06
Iter: 72 loss: 1.32866103e-06
Iter: 73 loss: 1.33898186e-06
Iter: 74 loss: 1.32464947e-06
Iter: 75 loss: 1.31284105e-06
Iter: 76 loss: 1.32548132e-06
Iter: 77 loss: 1.30639285e-06
Iter: 78 loss: 1.29741034e-06
Iter: 79 loss: 1.37085863e-06
Iter: 80 loss: 1.29672298e-06
Iter: 81 loss: 1.28805027e-06
Iter: 82 loss: 1.29362991e-06
Iter: 83 loss: 1.28259808e-06
Iter: 84 loss: 1.27546502e-06
Iter: 85 loss: 1.3296808e-06
Iter: 86 loss: 1.27504541e-06
Iter: 87 loss: 1.26886266e-06
Iter: 88 loss: 1.27605176e-06
Iter: 89 loss: 1.26564919e-06
Iter: 90 loss: 1.25641986e-06
Iter: 91 loss: 1.29407272e-06
Iter: 92 loss: 1.25439851e-06
Iter: 93 loss: 1.25023757e-06
Iter: 94 loss: 1.25643203e-06
Iter: 95 loss: 1.24827488e-06
Iter: 96 loss: 1.24288601e-06
Iter: 97 loss: 1.25070255e-06
Iter: 98 loss: 1.24033579e-06
Iter: 99 loss: 1.23576706e-06
Iter: 100 loss: 1.29415128e-06
Iter: 101 loss: 1.23580821e-06
Iter: 102 loss: 1.23401583e-06
Iter: 103 loss: 1.23392874e-06
Iter: 104 loss: 1.23167933e-06
Iter: 105 loss: 1.22572499e-06
Iter: 106 loss: 1.28169881e-06
Iter: 107 loss: 1.22497488e-06
Iter: 108 loss: 1.22133372e-06
Iter: 109 loss: 1.24964981e-06
Iter: 110 loss: 1.22109509e-06
Iter: 111 loss: 1.21839366e-06
Iter: 112 loss: 1.21354401e-06
Iter: 113 loss: 1.21352741e-06
Iter: 114 loss: 1.2080684e-06
Iter: 115 loss: 1.26018153e-06
Iter: 116 loss: 1.2077827e-06
Iter: 117 loss: 1.20424693e-06
Iter: 118 loss: 1.20158381e-06
Iter: 119 loss: 1.20027266e-06
Iter: 120 loss: 1.19393212e-06
Iter: 121 loss: 1.23830273e-06
Iter: 122 loss: 1.193191e-06
Iter: 123 loss: 1.19035394e-06
Iter: 124 loss: 1.21712378e-06
Iter: 125 loss: 1.19025026e-06
Iter: 126 loss: 1.18717139e-06
Iter: 127 loss: 1.18474543e-06
Iter: 128 loss: 1.18373316e-06
Iter: 129 loss: 1.1802274e-06
Iter: 130 loss: 1.2062394e-06
Iter: 131 loss: 1.17990487e-06
Iter: 132 loss: 1.17748164e-06
Iter: 133 loss: 1.17965681e-06
Iter: 134 loss: 1.17606237e-06
Iter: 135 loss: 1.17325544e-06
Iter: 136 loss: 1.20538778e-06
Iter: 137 loss: 1.17317688e-06
Iter: 138 loss: 1.17114269e-06
Iter: 139 loss: 1.20326297e-06
Iter: 140 loss: 1.17113336e-06
Iter: 141 loss: 1.17039531e-06
Iter: 142 loss: 1.16843057e-06
Iter: 143 loss: 1.18318326e-06
Iter: 144 loss: 1.16814181e-06
Iter: 145 loss: 1.165188e-06
Iter: 146 loss: 1.1675321e-06
Iter: 147 loss: 1.16338924e-06
Iter: 148 loss: 1.16063916e-06
Iter: 149 loss: 1.17131481e-06
Iter: 150 loss: 1.15993248e-06
Iter: 151 loss: 1.15707724e-06
Iter: 152 loss: 1.15791704e-06
Iter: 153 loss: 1.15497539e-06
Iter: 154 loss: 1.15112596e-06
Iter: 155 loss: 1.18244805e-06
Iter: 156 loss: 1.15086414e-06
Iter: 157 loss: 1.14866157e-06
Iter: 158 loss: 1.14598629e-06
Iter: 159 loss: 1.14570958e-06
Iter: 160 loss: 1.14180853e-06
Iter: 161 loss: 1.19178958e-06
Iter: 162 loss: 1.14178829e-06
Iter: 163 loss: 1.14015529e-06
Iter: 164 loss: 1.15276714e-06
Iter: 165 loss: 1.13997635e-06
Iter: 166 loss: 1.13825968e-06
Iter: 167 loss: 1.13387807e-06
Iter: 168 loss: 1.17860509e-06
Iter: 169 loss: 1.13337012e-06
Iter: 170 loss: 1.13092597e-06
Iter: 171 loss: 1.13067961e-06
Iter: 172 loss: 1.13002739e-06
Iter: 173 loss: 1.12991074e-06
Iter: 174 loss: 1.12881958e-06
Iter: 175 loss: 1.12629323e-06
Iter: 176 loss: 1.14932413e-06
Iter: 177 loss: 1.12593762e-06
Iter: 178 loss: 1.12437931e-06
Iter: 179 loss: 1.13210717e-06
Iter: 180 loss: 1.12412351e-06
Iter: 181 loss: 1.12272551e-06
Iter: 182 loss: 1.12428859e-06
Iter: 183 loss: 1.12200064e-06
Iter: 184 loss: 1.12003318e-06
Iter: 185 loss: 1.12214252e-06
Iter: 186 loss: 1.11898021e-06
Iter: 187 loss: 1.11737813e-06
Iter: 188 loss: 1.12138389e-06
Iter: 189 loss: 1.11672239e-06
Iter: 190 loss: 1.11464396e-06
Iter: 191 loss: 1.11831696e-06
Iter: 192 loss: 1.11358679e-06
Iter: 193 loss: 1.11153724e-06
Iter: 194 loss: 1.12545445e-06
Iter: 195 loss: 1.1113375e-06
Iter: 196 loss: 1.10959172e-06
Iter: 197 loss: 1.10927886e-06
Iter: 198 loss: 1.10807798e-06
Iter: 199 loss: 1.10589951e-06
Iter: 200 loss: 1.13679346e-06
Iter: 201 loss: 1.10593453e-06
Iter: 202 loss: 1.10492624e-06
Iter: 203 loss: 1.10509836e-06
Iter: 204 loss: 1.10416545e-06
Iter: 205 loss: 1.10259816e-06
Iter: 206 loss: 1.10627661e-06
Iter: 207 loss: 1.10187898e-06
Iter: 208 loss: 1.10068299e-06
Iter: 209 loss: 1.10052201e-06
Iter: 210 loss: 1.10001838e-06
Iter: 211 loss: 1.09875691e-06
Iter: 212 loss: 1.10995961e-06
Iter: 213 loss: 1.09855944e-06
Iter: 214 loss: 1.09696293e-06
Iter: 215 loss: 1.0945846e-06
Iter: 216 loss: 1.09459256e-06
Iter: 217 loss: 1.09388952e-06
Iter: 218 loss: 1.09339771e-06
Iter: 219 loss: 1.09240261e-06
Iter: 220 loss: 1.09131179e-06
Iter: 221 loss: 1.09123107e-06
Iter: 222 loss: 1.0892719e-06
Iter: 223 loss: 1.09952043e-06
Iter: 224 loss: 1.0890227e-06
Iter: 225 loss: 1.08802328e-06
Iter: 226 loss: 1.0888383e-06
Iter: 227 loss: 1.08731797e-06
Iter: 228 loss: 1.08568906e-06
Iter: 229 loss: 1.08848099e-06
Iter: 230 loss: 1.08498864e-06
Iter: 231 loss: 1.08375116e-06
Iter: 232 loss: 1.10162318e-06
Iter: 233 loss: 1.0837116e-06
Iter: 234 loss: 1.08268137e-06
Iter: 235 loss: 1.08157792e-06
Iter: 236 loss: 1.08144206e-06
Iter: 237 loss: 1.07993549e-06
Iter: 238 loss: 1.09489145e-06
Iter: 239 loss: 1.07992616e-06
Iter: 240 loss: 1.07961569e-06
Iter: 241 loss: 1.0793899e-06
Iter: 242 loss: 1.07886581e-06
Iter: 243 loss: 1.07787014e-06
Iter: 244 loss: 1.09803887e-06
Iter: 245 loss: 1.07788765e-06
Iter: 246 loss: 1.07688925e-06
Iter: 247 loss: 1.07534811e-06
Iter: 248 loss: 1.07527717e-06
Iter: 249 loss: 1.07389042e-06
Iter: 250 loss: 1.07642836e-06
Iter: 251 loss: 1.07323274e-06
Iter: 252 loss: 1.07122491e-06
Iter: 253 loss: 1.08465906e-06
Iter: 254 loss: 1.07105848e-06
Iter: 255 loss: 1.07012318e-06
Iter: 256 loss: 1.07732444e-06
Iter: 257 loss: 1.07004803e-06
Iter: 258 loss: 1.06915877e-06
Iter: 259 loss: 1.06800985e-06
Iter: 260 loss: 1.06794835e-06
Iter: 261 loss: 1.0662759e-06
Iter: 262 loss: 1.07988672e-06
Iter: 263 loss: 1.06617404e-06
Iter: 264 loss: 1.06532184e-06
Iter: 265 loss: 1.06524078e-06
Iter: 266 loss: 1.06462119e-06
Iter: 267 loss: 1.0630431e-06
Iter: 268 loss: 1.06863013e-06
Iter: 269 loss: 1.06268294e-06
Iter: 270 loss: 1.0614857e-06
Iter: 271 loss: 1.06977723e-06
Iter: 272 loss: 1.06137873e-06
Iter: 273 loss: 1.06071559e-06
Iter: 274 loss: 1.06312348e-06
Iter: 275 loss: 1.06053358e-06
Iter: 276 loss: 1.05945617e-06
Iter: 277 loss: 1.06376524e-06
Iter: 278 loss: 1.05920185e-06
Iter: 279 loss: 1.05873642e-06
Iter: 280 loss: 1.05831509e-06
Iter: 281 loss: 1.05813535e-06
Iter: 282 loss: 1.05745744e-06
Iter: 283 loss: 1.05644563e-06
Iter: 284 loss: 1.05642823e-06
Iter: 285 loss: 1.05516904e-06
Iter: 286 loss: 1.0648223e-06
Iter: 287 loss: 1.05507581e-06
Iter: 288 loss: 1.05420315e-06
Iter: 289 loss: 1.05524782e-06
Iter: 290 loss: 1.0537866e-06
Iter: 291 loss: 1.05250388e-06
Iter: 292 loss: 1.05789979e-06
Iter: 293 loss: 1.05223091e-06
Iter: 294 loss: 1.05145728e-06
Iter: 295 loss: 1.05444008e-06
Iter: 296 loss: 1.05129277e-06
Iter: 297 loss: 1.05062463e-06
Iter: 298 loss: 1.05044069e-06
Iter: 299 loss: 1.04995115e-06
Iter: 300 loss: 1.04882588e-06
Iter: 301 loss: 1.05517756e-06
Iter: 302 loss: 1.04867888e-06
Iter: 303 loss: 1.04807225e-06
Iter: 304 loss: 1.05061383e-06
Iter: 305 loss: 1.04790365e-06
Iter: 306 loss: 1.04703747e-06
Iter: 307 loss: 1.0462561e-06
Iter: 308 loss: 1.046021e-06
Iter: 309 loss: 1.04762285e-06
Iter: 310 loss: 1.04571404e-06
Iter: 311 loss: 1.04554181e-06
Iter: 312 loss: 1.04496587e-06
Iter: 313 loss: 1.04805713e-06
Iter: 314 loss: 1.04474441e-06
Iter: 315 loss: 1.04399419e-06
Iter: 316 loss: 1.04387868e-06
Iter: 317 loss: 1.04332867e-06
Iter: 318 loss: 1.04226046e-06
Iter: 319 loss: 1.05020274e-06
Iter: 320 loss: 1.04217042e-06
Iter: 321 loss: 1.04151036e-06
Iter: 322 loss: 1.04007586e-06
Iter: 323 loss: 1.06631865e-06
Iter: 324 loss: 1.04004539e-06
Iter: 325 loss: 1.03896718e-06
Iter: 326 loss: 1.05495621e-06
Iter: 327 loss: 1.03906132e-06
Iter: 328 loss: 1.03790887e-06
Iter: 329 loss: 1.04059382e-06
Iter: 330 loss: 1.03759965e-06
Iter: 331 loss: 1.03664013e-06
Iter: 332 loss: 1.04328444e-06
Iter: 333 loss: 1.03655714e-06
Iter: 334 loss: 1.03598882e-06
Iter: 335 loss: 1.03593868e-06
Iter: 336 loss: 1.03551e-06
Iter: 337 loss: 1.03453635e-06
Iter: 338 loss: 1.0355252e-06
Iter: 339 loss: 1.0339844e-06
Iter: 340 loss: 1.03319167e-06
Iter: 341 loss: 1.03884565e-06
Iter: 342 loss: 1.03312573e-06
Iter: 343 loss: 1.0323763e-06
Iter: 344 loss: 1.03405273e-06
Iter: 345 loss: 1.0319776e-06
Iter: 346 loss: 1.03159869e-06
Iter: 347 loss: 1.03153093e-06
Iter: 348 loss: 1.03113484e-06
Iter: 349 loss: 1.03238176e-06
Iter: 350 loss: 1.03096897e-06
Iter: 351 loss: 1.03065611e-06
Iter: 352 loss: 1.03006789e-06
Iter: 353 loss: 1.03764933e-06
Iter: 354 loss: 1.03004459e-06
Iter: 355 loss: 1.02895581e-06
Iter: 356 loss: 1.02973763e-06
Iter: 357 loss: 1.02834349e-06
Iter: 358 loss: 1.02775437e-06
Iter: 359 loss: 1.03513594e-06
Iter: 360 loss: 1.0277771e-06
Iter: 361 loss: 1.02722151e-06
Iter: 362 loss: 1.02638251e-06
Iter: 363 loss: 1.02638e-06
Iter: 364 loss: 1.02548734e-06
Iter: 365 loss: 1.03387697e-06
Iter: 366 loss: 1.02547222e-06
Iter: 367 loss: 1.0247611e-06
Iter: 368 loss: 1.02334195e-06
Iter: 369 loss: 1.05465062e-06
Iter: 370 loss: 1.02330637e-06
Iter: 371 loss: 1.02236663e-06
Iter: 372 loss: 1.02235106e-06
Iter: 373 loss: 1.02141644e-06
Iter: 374 loss: 1.02160345e-06
Iter: 375 loss: 1.0208073e-06
Iter: 376 loss: 1.02012848e-06
Iter: 377 loss: 1.02016384e-06
Iter: 378 loss: 1.01964054e-06
Iter: 379 loss: 1.01917237e-06
Iter: 380 loss: 1.01907233e-06
Iter: 381 loss: 1.01873229e-06
Iter: 382 loss: 1.01855846e-06
Iter: 383 loss: 1.01813907e-06
Iter: 384 loss: 1.01770411e-06
Iter: 385 loss: 1.01768683e-06
Iter: 386 loss: 1.01721344e-06
Iter: 387 loss: 1.01646037e-06
Iter: 388 loss: 1.03124194e-06
Iter: 389 loss: 1.0164149e-06
Iter: 390 loss: 1.01549358e-06
Iter: 391 loss: 1.02293438e-06
Iter: 392 loss: 1.01539899e-06
Iter: 393 loss: 1.01487558e-06
Iter: 394 loss: 1.01785508e-06
Iter: 395 loss: 1.01490127e-06
Iter: 396 loss: 1.01440446e-06
Iter: 397 loss: 1.01351259e-06
Iter: 398 loss: 1.01349053e-06
Iter: 399 loss: 1.01263072e-06
Iter: 400 loss: 1.02483921e-06
Iter: 401 loss: 1.01267403e-06
Iter: 402 loss: 1.01212652e-06
Iter: 403 loss: 1.01117212e-06
Iter: 404 loss: 1.03064099e-06
Iter: 405 loss: 1.01111755e-06
Iter: 406 loss: 1.01013575e-06
Iter: 407 loss: 1.02504805e-06
Iter: 408 loss: 1.01013688e-06
Iter: 409 loss: 1.00947955e-06
Iter: 410 loss: 1.01036449e-06
Iter: 411 loss: 1.0092474e-06
Iter: 412 loss: 1.00841874e-06
Iter: 413 loss: 1.0102126e-06
Iter: 414 loss: 1.00801242e-06
Iter: 415 loss: 1.00737225e-06
Iter: 416 loss: 1.01020623e-06
Iter: 417 loss: 1.00722445e-06
Iter: 418 loss: 1.00683883e-06
Iter: 419 loss: 1.00677494e-06
Iter: 420 loss: 1.00644365e-06
Iter: 421 loss: 1.00557213e-06
Iter: 422 loss: 1.01209798e-06
Iter: 423 loss: 1.00527166e-06
Iter: 424 loss: 1.00450575e-06
Iter: 425 loss: 1.00734042e-06
Iter: 426 loss: 1.00427053e-06
Iter: 427 loss: 1.00361024e-06
Iter: 428 loss: 1.00370391e-06
Iter: 429 loss: 1.00311036e-06
Iter: 430 loss: 1.00234138e-06
Iter: 431 loss: 1.01049e-06
Iter: 432 loss: 1.00234547e-06
Iter: 433 loss: 1.00199168e-06
Iter: 434 loss: 1.00277794e-06
Iter: 435 loss: 1.00174259e-06
Iter: 436 loss: 1.00123793e-06
Iter: 437 loss: 1.00128159e-06
Iter: 438 loss: 1.00081036e-06
Iter: 439 loss: 1.00018633e-06
Iter: 440 loss: 1.00856482e-06
Iter: 441 loss: 1.00013813e-06
Iter: 442 loss: 9.99793656e-07
Iter: 443 loss: 9.98943e-07
Iter: 444 loss: 1.01248634e-06
Iter: 445 loss: 9.9890633e-07
Iter: 446 loss: 9.98116548e-07
Iter: 447 loss: 1.00865736e-06
Iter: 448 loss: 9.9808085e-07
Iter: 449 loss: 9.97663847e-07
Iter: 450 loss: 9.97439656e-07
Iter: 451 loss: 9.97129177e-07
Iter: 452 loss: 9.96240715e-07
Iter: 453 loss: 1.00000079e-06
Iter: 454 loss: 9.96128506e-07
Iter: 455 loss: 9.96446943e-07
Iter: 456 loss: 9.95916935e-07
Iter: 457 loss: 9.95718551e-07
Iter: 458 loss: 9.9538e-07
Iter: 459 loss: 1.00222e-06
Iter: 460 loss: 9.95309847e-07
Iter: 461 loss: 9.94804e-07
Iter: 462 loss: 9.93920139e-07
Iter: 463 loss: 9.93898766e-07
Iter: 464 loss: 9.93262347e-07
Iter: 465 loss: 1.00074749e-06
Iter: 466 loss: 9.93249841e-07
Iter: 467 loss: 9.92722562e-07
Iter: 468 loss: 9.92306809e-07
Iter: 469 loss: 9.92128207e-07
Iter: 470 loss: 9.91349907e-07
Iter: 471 loss: 9.98729774e-07
Iter: 472 loss: 9.9129818e-07
Iter: 473 loss: 9.90788521e-07
Iter: 474 loss: 9.90854915e-07
Iter: 475 loss: 9.90465651e-07
Iter: 476 loss: 9.89793307e-07
Iter: 477 loss: 9.94677748e-07
Iter: 478 loss: 9.89703381e-07
Iter: 479 loss: 9.89264e-07
Iter: 480 loss: 9.90887884e-07
Iter: 481 loss: 9.89101636e-07
Iter: 482 loss: 9.8857754e-07
Iter: 483 loss: 9.89110845e-07
Iter: 484 loss: 9.88382908e-07
Iter: 485 loss: 9.87762405e-07
Iter: 486 loss: 9.90266585e-07
Iter: 487 loss: 9.87548447e-07
Iter: 488 loss: 9.87202156e-07
Iter: 489 loss: 9.88825605e-07
Iter: 490 loss: 9.87154e-07
Iter: 491 loss: 9.86744681e-07
Iter: 492 loss: 9.90647436e-07
Iter: 493 loss: 9.86773443e-07
Iter: 494 loss: 9.86494342e-07
Iter: 495 loss: 9.85812108e-07
Iter: 496 loss: 9.9062072e-07
Iter: 497 loss: 9.85626e-07
Iter: 498 loss: 9.85035513e-07
Iter: 499 loss: 9.89475e-07
Iter: 500 loss: 9.85081e-07
Iter: 501 loss: 9.84512553e-07
Iter: 502 loss: 9.84225153e-07
Iter: 503 loss: 9.83924338e-07
Iter: 504 loss: 9.83378072e-07
Iter: 505 loss: 9.89470891e-07
Iter: 506 loss: 9.83295536e-07
Iter: 507 loss: 9.82924348e-07
Iter: 508 loss: 9.82285087e-07
Iter: 509 loss: 9.82271786e-07
Iter: 510 loss: 9.81834887e-07
Iter: 511 loss: 9.81808853e-07
Iter: 512 loss: 9.81439939e-07
Iter: 513 loss: 9.81039648e-07
Iter: 514 loss: 9.81007474e-07
Iter: 515 loss: 9.80453819e-07
Iter: 516 loss: 9.8165674e-07
Iter: 517 loss: 9.8028e-07
Iter: 518 loss: 9.79868673e-07
Iter: 519 loss: 9.79981e-07
Iter: 520 loss: 9.79544666e-07
Iter: 521 loss: 9.79038646e-07
Iter: 522 loss: 9.84494136e-07
Iter: 523 loss: 9.79021252e-07
Iter: 524 loss: 9.78825e-07
Iter: 525 loss: 9.79068773e-07
Iter: 526 loss: 9.78696107e-07
Iter: 527 loss: 9.78335834e-07
Iter: 528 loss: 9.78737717e-07
Iter: 529 loss: 9.78069579e-07
Iter: 530 loss: 9.77717718e-07
Iter: 531 loss: 9.77670425e-07
Iter: 532 loss: 9.77383252e-07
Iter: 533 loss: 9.76873935e-07
Iter: 534 loss: 9.78712478e-07
Iter: 535 loss: 9.76821e-07
Iter: 536 loss: 9.76300726e-07
Iter: 537 loss: 9.75975581e-07
Iter: 538 loss: 9.75811872e-07
Iter: 539 loss: 9.75208422e-07
Iter: 540 loss: 9.76285719e-07
Iter: 541 loss: 9.74963768e-07
Iter: 542 loss: 9.74257546e-07
Iter: 543 loss: 9.78878461e-07
Iter: 544 loss: 9.74187287e-07
Iter: 545 loss: 9.73865212e-07
Iter: 546 loss: 9.74445356e-07
Iter: 547 loss: 9.73692522e-07
Iter: 548 loss: 9.73184115e-07
Iter: 549 loss: 9.7310533e-07
Iter: 550 loss: 9.72717089e-07
Iter: 551 loss: 9.72345106e-07
Iter: 552 loss: 9.72361e-07
Iter: 553 loss: 9.72015e-07
Iter: 554 loss: 9.74263799e-07
Iter: 555 loss: 9.71937766e-07
Iter: 556 loss: 9.71720283e-07
Iter: 557 loss: 9.72460839e-07
Iter: 558 loss: 9.71620125e-07
Iter: 559 loss: 9.71344434e-07
Iter: 560 loss: 9.73324518e-07
Iter: 561 loss: 9.71378654e-07
Iter: 562 loss: 9.71259396e-07
Iter: 563 loss: 9.71031682e-07
Iter: 564 loss: 9.76181695e-07
Iter: 565 loss: 9.71022246e-07
Iter: 566 loss: 9.70675842e-07
Iter: 567 loss: 9.70930614e-07
Iter: 568 loss: 9.70440169e-07
Iter: 569 loss: 9.70003725e-07
Iter: 570 loss: 9.71956183e-07
Iter: 571 loss: 9.6986173e-07
Iter: 572 loss: 9.69439498e-07
Iter: 573 loss: 9.69294888e-07
Iter: 574 loss: 9.69078428e-07
Iter: 575 loss: 9.6837482e-07
Iter: 576 loss: 9.7091015e-07
Iter: 577 loss: 9.68216682e-07
Iter: 578 loss: 9.67623691e-07
Iter: 579 loss: 9.69208941e-07
Iter: 580 loss: 9.6749659e-07
Iter: 581 loss: 9.6692861e-07
Iter: 582 loss: 9.67588e-07
Iter: 583 loss: 9.66589823e-07
Iter: 584 loss: 9.66044809e-07
Iter: 585 loss: 9.6756e-07
Iter: 586 loss: 9.65859499e-07
Iter: 587 loss: 9.65260824e-07
Iter: 588 loss: 9.68862196e-07
Iter: 589 loss: 9.65257186e-07
Iter: 590 loss: 9.65066192e-07
Iter: 591 loss: 9.64966375e-07
Iter: 592 loss: 9.64831315e-07
Iter: 593 loss: 9.64727178e-07
Iter: 594 loss: 9.64580749e-07
Iter: 595 loss: 9.64191372e-07
Iter: 596 loss: 9.63916705e-07
Iter: 597 loss: 9.63842126e-07
Iter: 598 loss: 9.63333719e-07
Iter: 599 loss: 9.64557557e-07
Iter: 600 loss: 9.63109e-07
Iter: 601 loss: 9.62766e-07
Iter: 602 loss: 9.64278115e-07
Iter: 603 loss: 9.62747663e-07
Iter: 604 loss: 9.6233714e-07
Iter: 605 loss: 9.62579406e-07
Iter: 606 loss: 9.62178774e-07
Iter: 607 loss: 9.6179815e-07
Iter: 608 loss: 9.64020842e-07
Iter: 609 loss: 9.61845899e-07
Iter: 610 loss: 9.61616934e-07
Iter: 611 loss: 9.61532692e-07
Iter: 612 loss: 9.61360229e-07
Iter: 613 loss: 9.60939815e-07
Iter: 614 loss: 9.62646482e-07
Iter: 615 loss: 9.6084716e-07
Iter: 616 loss: 9.60588181e-07
Iter: 617 loss: 9.60884449e-07
Iter: 618 loss: 9.60421e-07
Iter: 619 loss: 9.59975523e-07
Iter: 620 loss: 9.60704256e-07
Iter: 621 loss: 9.59814543e-07
Iter: 622 loss: 9.59640829e-07
Iter: 623 loss: 9.59627e-07
Iter: 624 loss: 9.59386398e-07
Iter: 625 loss: 9.59471322e-07
Iter: 626 loss: 9.59197337e-07
Iter: 627 loss: 9.58919145e-07
Iter: 628 loss: 9.59288e-07
Iter: 629 loss: 9.58814098e-07
Iter: 630 loss: 9.58518e-07
Iter: 631 loss: 9.5863129e-07
Iter: 632 loss: 9.58338205e-07
Iter: 633 loss: 9.58077862e-07
Iter: 634 loss: 9.58641067e-07
Iter: 635 loss: 9.57957582e-07
Iter: 636 loss: 9.57688599e-07
Iter: 637 loss: 9.58203259e-07
Iter: 638 loss: 9.5751011e-07
Iter: 639 loss: 9.57196789e-07
Iter: 640 loss: 9.59175395e-07
Iter: 641 loss: 9.57223847e-07
Iter: 642 loss: 9.56942586e-07
Iter: 643 loss: 9.56945541e-07
Iter: 644 loss: 9.56842541e-07
Iter: 645 loss: 9.56447934e-07
Iter: 646 loss: 9.58260216e-07
Iter: 647 loss: 9.56357326e-07
Iter: 648 loss: 9.56168833e-07
Iter: 649 loss: 9.56764097e-07
Iter: 650 loss: 9.56073791e-07
Iter: 651 loss: 9.55739779e-07
Iter: 652 loss: 9.55345513e-07
Iter: 653 loss: 9.55261271e-07
Iter: 654 loss: 9.54960115e-07
Iter: 655 loss: 9.54946472e-07
Iter: 656 loss: 9.54645657e-07
Iter: 657 loss: 9.58461555e-07
Iter: 658 loss: 9.54680445e-07
Iter: 659 loss: 9.54449774e-07
Iter: 660 loss: 9.54133384e-07
Iter: 661 loss: 9.61071e-07
Iter: 662 loss: 9.54115876e-07
Iter: 663 loss: 9.53757308e-07
Iter: 664 loss: 9.54649522e-07
Iter: 665 loss: 9.53597464e-07
Iter: 666 loss: 9.53277208e-07
Iter: 667 loss: 9.53352298e-07
Iter: 668 loss: 9.53101392e-07
Iter: 669 loss: 9.52662162e-07
Iter: 670 loss: 9.55439873e-07
Iter: 671 loss: 9.52646815e-07
Iter: 672 loss: 9.52395567e-07
Iter: 673 loss: 9.52943935e-07
Iter: 674 loss: 9.52285404e-07
Iter: 675 loss: 9.51961397e-07
Iter: 676 loss: 9.52087248e-07
Iter: 677 loss: 9.51789502e-07
Iter: 678 loss: 9.51381651e-07
Iter: 679 loss: 9.53194331e-07
Iter: 680 loss: 9.51328502e-07
Iter: 681 loss: 9.5099881e-07
Iter: 682 loss: 9.51216e-07
Iter: 683 loss: 9.50796448e-07
Iter: 684 loss: 9.50316917e-07
Iter: 685 loss: 9.51993343e-07
Iter: 686 loss: 9.50175775e-07
Iter: 687 loss: 9.49887635e-07
Iter: 688 loss: 9.51225161e-07
Iter: 689 loss: 9.49797084e-07
Iter: 690 loss: 9.49691184e-07
Iter: 691 loss: 9.49623768e-07
Iter: 692 loss: 9.49559706e-07
Iter: 693 loss: 9.49263949e-07
Iter: 694 loss: 9.5273208e-07
Iter: 695 loss: 9.49240757e-07
Iter: 696 loss: 9.48988827e-07
Iter: 697 loss: 9.50459594e-07
Iter: 698 loss: 9.48989737e-07
Iter: 699 loss: 9.48740649e-07
Iter: 700 loss: 9.48483432e-07
Iter: 701 loss: 9.48452907e-07
Iter: 702 loss: 9.48044203e-07
Iter: 703 loss: 9.49736545e-07
Iter: 704 loss: 9.47909371e-07
Iter: 705 loss: 9.4764232e-07
Iter: 706 loss: 9.48434717e-07
Iter: 707 loss: 9.4758434e-07
Iter: 708 loss: 9.47191836e-07
Iter: 709 loss: 9.47365834e-07
Iter: 710 loss: 9.46989189e-07
Iter: 711 loss: 9.46636305e-07
Iter: 712 loss: 9.50455103e-07
Iter: 713 loss: 9.46647e-07
Iter: 714 loss: 9.4642354e-07
Iter: 715 loss: 9.46251248e-07
Iter: 716 loss: 9.4619844e-07
Iter: 717 loss: 9.45878469e-07
Iter: 718 loss: 9.4835957e-07
Iter: 719 loss: 9.45789793e-07
Iter: 720 loss: 9.4559914e-07
Iter: 721 loss: 9.45878867e-07
Iter: 722 loss: 9.4548011e-07
Iter: 723 loss: 9.45392912e-07
Iter: 724 loss: 9.45309353e-07
Iter: 725 loss: 9.45197144e-07
Iter: 726 loss: 9.45006548e-07
Iter: 727 loss: 9.44971248e-07
Iter: 728 loss: 9.44812086e-07
Iter: 729 loss: 9.4512211e-07
Iter: 730 loss: 9.44669068e-07
Iter: 731 loss: 9.44466933e-07
Iter: 732 loss: 9.44094893e-07
Iter: 733 loss: 9.44121268e-07
Iter: 734 loss: 9.43675445e-07
Iter: 735 loss: 9.47172566e-07
Iter: 736 loss: 9.4366635e-07
Iter: 737 loss: 9.43366672e-07
Iter: 738 loss: 9.4462041e-07
Iter: 739 loss: 9.433532e-07
Iter: 740 loss: 9.43030386e-07
Iter: 741 loss: 9.42980307e-07
Iter: 742 loss: 9.42843258e-07
Iter: 743 loss: 9.42433758e-07
Iter: 744 loss: 9.45790475e-07
Iter: 745 loss: 9.42445126e-07
Iter: 746 loss: 9.42160568e-07
Iter: 747 loss: 9.42269935e-07
Iter: 748 loss: 9.42016811e-07
Iter: 749 loss: 9.41671e-07
Iter: 750 loss: 9.42783458e-07
Iter: 751 loss: 9.41627206e-07
Iter: 752 loss: 9.41287794e-07
Iter: 753 loss: 9.42813415e-07
Iter: 754 loss: 9.41323151e-07
Iter: 755 loss: 9.4105576e-07
Iter: 756 loss: 9.4215045e-07
Iter: 757 loss: 9.40996e-07
Iter: 758 loss: 9.40666325e-07
Iter: 759 loss: 9.41478106e-07
Iter: 760 loss: 9.40605446e-07
Iter: 761 loss: 9.40348684e-07
Iter: 762 loss: 9.4038387e-07
Iter: 763 loss: 9.40317534e-07
Iter: 764 loss: 9.40041446e-07
Iter: 765 loss: 9.39641495e-07
Iter: 766 loss: 9.39573795e-07
Iter: 767 loss: 9.3914764e-07
Iter: 768 loss: 9.43019813e-07
Iter: 769 loss: 9.39086704e-07
Iter: 770 loss: 9.38742176e-07
Iter: 771 loss: 9.39294694e-07
Iter: 772 loss: 9.38652363e-07
Iter: 773 loss: 9.3827191e-07
Iter: 774 loss: 9.39324934e-07
Iter: 775 loss: 9.38032031e-07
Iter: 776 loss: 9.37781e-07
Iter: 777 loss: 9.38751e-07
Iter: 778 loss: 9.3767909e-07
Iter: 779 loss: 9.37345305e-07
Iter: 780 loss: 9.37764753e-07
Iter: 781 loss: 9.37154255e-07
Iter: 782 loss: 9.36735148e-07
Iter: 783 loss: 9.38244114e-07
Iter: 784 loss: 9.3652551e-07
Iter: 785 loss: 9.36250672e-07
Iter: 786 loss: 9.371e-07
Iter: 787 loss: 9.36094693e-07
Iter: 788 loss: 9.35809112e-07
Iter: 789 loss: 9.3988416e-07
Iter: 790 loss: 9.35787909e-07
Iter: 791 loss: 9.35533421e-07
Iter: 792 loss: 9.37511516e-07
Iter: 793 loss: 9.35516e-07
Iter: 794 loss: 9.35391881e-07
Iter: 795 loss: 9.35134494e-07
Iter: 796 loss: 9.35167577e-07
Iter: 797 loss: 9.34873924e-07
Iter: 798 loss: 9.34910588e-07
Iter: 799 loss: 9.34621823e-07
Iter: 800 loss: 9.34236255e-07
Iter: 801 loss: 9.35634489e-07
Iter: 802 loss: 9.34164518e-07
Iter: 803 loss: 9.33899173e-07
Iter: 804 loss: 9.34543266e-07
Iter: 805 loss: 9.3372762e-07
Iter: 806 loss: 9.33307717e-07
Iter: 807 loss: 9.34872673e-07
Iter: 808 loss: 9.33194656e-07
Iter: 809 loss: 9.32989224e-07
Iter: 810 loss: 9.33991942e-07
Iter: 811 loss: 9.32902424e-07
Iter: 812 loss: 9.32628041e-07
Iter: 813 loss: 9.32415162e-07
Iter: 814 loss: 9.32329215e-07
Iter: 815 loss: 9.3189243e-07
Iter: 816 loss: 9.35858168e-07
Iter: 817 loss: 9.318926e-07
Iter: 818 loss: 9.31607531e-07
Iter: 819 loss: 9.31568934e-07
Iter: 820 loss: 9.31376121e-07
Iter: 821 loss: 9.31005388e-07
Iter: 822 loss: 9.33709e-07
Iter: 823 loss: 9.30922511e-07
Iter: 824 loss: 9.30743056e-07
Iter: 825 loss: 9.30676151e-07
Iter: 826 loss: 9.30542456e-07
Iter: 827 loss: 9.3029405e-07
Iter: 828 loss: 9.3240584e-07
Iter: 829 loss: 9.30232e-07
Iter: 830 loss: 9.29815428e-07
Iter: 831 loss: 9.30599924e-07
Iter: 832 loss: 9.29710609e-07
Iter: 833 loss: 9.29350733e-07
Iter: 834 loss: 9.31336729e-07
Iter: 835 loss: 9.29338853e-07
Iter: 836 loss: 9.29052248e-07
Iter: 837 loss: 9.28984946e-07
Iter: 838 loss: 9.28771954e-07
Iter: 839 loss: 9.28404916e-07
Iter: 840 loss: 9.30568262e-07
Iter: 841 loss: 9.28313398e-07
Iter: 842 loss: 9.27891961e-07
Iter: 843 loss: 9.29087719e-07
Iter: 844 loss: 9.27816e-07
Iter: 845 loss: 9.27561189e-07
Iter: 846 loss: 9.27312385e-07
Iter: 847 loss: 9.27156179e-07
Iter: 848 loss: 9.26739745e-07
Iter: 849 loss: 9.31368788e-07
Iter: 850 loss: 9.26691484e-07
Iter: 851 loss: 9.26470193e-07
Iter: 852 loss: 9.26630776e-07
Iter: 853 loss: 9.26329676e-07
Iter: 854 loss: 9.25888628e-07
Iter: 855 loss: 9.26967743e-07
Iter: 856 loss: 9.25790516e-07
Iter: 857 loss: 9.25899826e-07
Iter: 858 loss: 9.25687459e-07
Iter: 859 loss: 9.25622e-07
Iter: 860 loss: 9.25345716e-07
Iter: 861 loss: 9.26584107e-07
Iter: 862 loss: 9.25301947e-07
Iter: 863 loss: 9.24992889e-07
Iter: 864 loss: 9.2568331e-07
Iter: 865 loss: 9.24867152e-07
Iter: 866 loss: 9.24597e-07
Iter: 867 loss: 9.25924155e-07
Iter: 868 loss: 9.24517e-07
Iter: 869 loss: 9.24387336e-07
Iter: 870 loss: 9.24330607e-07
Iter: 871 loss: 9.2409465e-07
Iter: 872 loss: 9.23831465e-07
Iter: 873 loss: 9.25362201e-07
Iter: 874 loss: 9.23729942e-07
Iter: 875 loss: 9.23501716e-07
Iter: 876 loss: 9.24878634e-07
Iter: 877 loss: 9.23481252e-07
Iter: 878 loss: 9.23178959e-07
Iter: 879 loss: 9.22991319e-07
Iter: 880 loss: 9.22972e-07
Iter: 881 loss: 9.22582842e-07
Iter: 882 loss: 9.25222878e-07
Iter: 883 loss: 9.22585e-07
Iter: 884 loss: 9.22331537e-07
Iter: 885 loss: 9.22605e-07
Iter: 886 loss: 9.22226775e-07
Iter: 887 loss: 9.21765491e-07
Iter: 888 loss: 9.22768947e-07
Iter: 889 loss: 9.21664878e-07
Iter: 890 loss: 9.2168159e-07
Iter: 891 loss: 9.21542323e-07
Iter: 892 loss: 9.21393394e-07
Iter: 893 loss: 9.21102867e-07
Iter: 894 loss: 9.26212863e-07
Iter: 895 loss: 9.21074616e-07
Iter: 896 loss: 9.20889306e-07
Iter: 897 loss: 9.20729235e-07
Iter: 898 loss: 9.20662444e-07
Iter: 899 loss: 9.20331672e-07
Iter: 900 loss: 9.23130415e-07
Iter: 901 loss: 9.20358048e-07
Iter: 902 loss: 9.20061723e-07
Iter: 903 loss: 9.19707475e-07
Iter: 904 loss: 9.19687068e-07
Iter: 905 loss: 9.1931372e-07
Iter: 906 loss: 9.23865343e-07
Iter: 907 loss: 9.19248e-07
Iter: 908 loss: 9.18931732e-07
Iter: 909 loss: 9.19576735e-07
Iter: 910 loss: 9.1884931e-07
Iter: 911 loss: 9.18498188e-07
Iter: 912 loss: 9.19224e-07
Iter: 913 loss: 9.18337491e-07
Iter: 914 loss: 9.1807e-07
Iter: 915 loss: 9.19023421e-07
Iter: 916 loss: 9.17971761e-07
Iter: 917 loss: 9.17641273e-07
Iter: 918 loss: 9.1790605e-07
Iter: 919 loss: 9.17489274e-07
Iter: 920 loss: 9.17155148e-07
Iter: 921 loss: 9.20132948e-07
Iter: 922 loss: 9.17078353e-07
Iter: 923 loss: 9.16900149e-07
Iter: 924 loss: 9.17322097e-07
Iter: 925 loss: 9.16796182e-07
Iter: 926 loss: 9.16548686e-07
Iter: 927 loss: 9.16564034e-07
Iter: 928 loss: 9.16423232e-07
Iter: 929 loss: 9.1618881e-07
Iter: 930 loss: 9.18076466e-07
Iter: 931 loss: 9.16090755e-07
Iter: 932 loss: 9.15768112e-07
Iter: 933 loss: 9.17238e-07
Iter: 934 loss: 9.1571269e-07
Iter: 935 loss: 9.1541358e-07
Iter: 936 loss: 9.15796818e-07
Iter: 937 loss: 9.15302735e-07
Iter: 938 loss: 9.14918701e-07
Iter: 939 loss: 9.16247131e-07
Iter: 940 loss: 9.14856059e-07
Iter: 941 loss: 9.14556495e-07
Iter: 942 loss: 9.15450528e-07
Iter: 943 loss: 9.14525629e-07
Iter: 944 loss: 9.14264263e-07
Iter: 945 loss: 9.15407497e-07
Iter: 946 loss: 9.14174e-07
Iter: 947 loss: 9.13985161e-07
Iter: 948 loss: 9.14084239e-07
Iter: 949 loss: 9.13817814e-07
Iter: 950 loss: 9.13620795e-07
Iter: 951 loss: 9.14409043e-07
Iter: 952 loss: 9.13559859e-07
Iter: 953 loss: 9.13222095e-07
Iter: 954 loss: 9.14191e-07
Iter: 955 loss: 9.13155361e-07
Iter: 956 loss: 9.12946632e-07
Iter: 957 loss: 9.14034274e-07
Iter: 958 loss: 9.12906785e-07
Iter: 959 loss: 9.12863129e-07
Iter: 960 loss: 9.12860401e-07
Iter: 961 loss: 9.12779115e-07
Iter: 962 loss: 9.12612904e-07
Iter: 963 loss: 9.13649956e-07
Iter: 964 loss: 9.12554697e-07
Iter: 965 loss: 9.12333e-07
Iter: 966 loss: 9.12535597e-07
Iter: 967 loss: 9.12210737e-07
Iter: 968 loss: 9.11938798e-07
Iter: 969 loss: 9.12796736e-07
Iter: 970 loss: 9.11854897e-07
Iter: 971 loss: 9.1162542e-07
Iter: 972 loss: 9.12654855e-07
Iter: 973 loss: 9.11550956e-07
Iter: 974 loss: 9.11281177e-07
Iter: 975 loss: 9.11549137e-07
Iter: 976 loss: 9.11123493e-07
Iter: 977 loss: 9.10848598e-07
Iter: 978 loss: 9.12686062e-07
Iter: 979 loss: 9.10894244e-07
Iter: 980 loss: 9.10664312e-07
Iter: 981 loss: 9.11253835e-07
Iter: 982 loss: 9.10680455e-07
Iter: 983 loss: 9.10438757e-07
Iter: 984 loss: 9.1048031e-07
Iter: 985 loss: 9.1032274e-07
Iter: 986 loss: 9.10093149e-07
Iter: 987 loss: 9.10567e-07
Iter: 988 loss: 9.09983441e-07
Iter: 989 loss: 9.09741857e-07
Iter: 990 loss: 9.11091831e-07
Iter: 991 loss: 9.0970758e-07
Iter: 992 loss: 9.09609639e-07
Iter: 993 loss: 9.09629421e-07
Iter: 994 loss: 9.09556775e-07
Iter: 995 loss: 9.09290634e-07
Iter: 996 loss: 9.09323148e-07
Iter: 997 loss: 9.09088385e-07
Iter: 998 loss: 9.08862e-07
Iter: 999 loss: 9.08803258e-07
Iter: 1000 loss: 9.08568154e-07
Iter: 1001 loss: 9.09489e-07
Iter: 1002 loss: 9.08477375e-07
Iter: 1003 loss: 9.08249945e-07
Iter: 1004 loss: 9.0928927e-07
Iter: 1005 loss: 9.08145125e-07
Iter: 1006 loss: 9.07835e-07
Iter: 1007 loss: 9.08628408e-07
Iter: 1008 loss: 9.07826461e-07
Iter: 1009 loss: 9.07420144e-07
Iter: 1010 loss: 9.07731305e-07
Iter: 1011 loss: 9.07205276e-07
Iter: 1012 loss: 9.06770879e-07
Iter: 1013 loss: 9.08222319e-07
Iter: 1014 loss: 9.06642697e-07
Iter: 1015 loss: 9.06276455e-07
Iter: 1016 loss: 9.07980279e-07
Iter: 1017 loss: 9.06234732e-07
Iter: 1018 loss: 9.05929255e-07
Iter: 1019 loss: 9.05905324e-07
Iter: 1020 loss: 9.05742354e-07
Iter: 1021 loss: 9.05323645e-07
Iter: 1022 loss: 9.07672188e-07
Iter: 1023 loss: 9.05281468e-07
Iter: 1024 loss: 9.05121453e-07
Iter: 1025 loss: 9.05125603e-07
Iter: 1026 loss: 9.04868e-07
Iter: 1027 loss: 9.04966555e-07
Iter: 1028 loss: 9.04736e-07
Iter: 1029 loss: 9.04573653e-07
Iter: 1030 loss: 9.04354e-07
Iter: 1031 loss: 9.04296314e-07
Iter: 1032 loss: 9.03988962e-07
Iter: 1033 loss: 9.0437004e-07
Iter: 1034 loss: 9.03822922e-07
Iter: 1035 loss: 9.03374598e-07
Iter: 1036 loss: 9.05320235e-07
Iter: 1037 loss: 9.03356636e-07
Iter: 1038 loss: 9.02963848e-07
Iter: 1039 loss: 9.03812122e-07
Iter: 1040 loss: 9.02823956e-07
Iter: 1041 loss: 9.02476927e-07
Iter: 1042 loss: 9.04255558e-07
Iter: 1043 loss: 9.02409397e-07
Iter: 1044 loss: 9.02134047e-07
Iter: 1045 loss: 9.03427804e-07
Iter: 1046 loss: 9.02065608e-07
Iter: 1047 loss: 9.01819419e-07
Iter: 1048 loss: 9.01987846e-07
Iter: 1049 loss: 9.01701469e-07
Iter: 1050 loss: 9.01319879e-07
Iter: 1051 loss: 9.01857902e-07
Iter: 1052 loss: 9.01192209e-07
Iter: 1053 loss: 9.0087326e-07
Iter: 1054 loss: 9.03866066e-07
Iter: 1055 loss: 9.0085689e-07
Iter: 1056 loss: 9.00629e-07
Iter: 1057 loss: 9.00311306e-07
Iter: 1058 loss: 9.00303348e-07
Iter: 1059 loss: 9.00048747e-07
Iter: 1060 loss: 9.00021519e-07
Iter: 1061 loss: 8.99896349e-07
Iter: 1062 loss: 8.99899476e-07
Iter: 1063 loss: 8.99763165e-07
Iter: 1064 loss: 8.99518568e-07
Iter: 1065 loss: 9.01055841e-07
Iter: 1066 loss: 8.99436884e-07
Iter: 1067 loss: 8.99087581e-07
Iter: 1068 loss: 8.99471729e-07
Iter: 1069 loss: 8.98849e-07
Iter: 1070 loss: 8.98494704e-07
Iter: 1071 loss: 8.9950538e-07
Iter: 1072 loss: 8.98366693e-07
Iter: 1073 loss: 8.98000451e-07
Iter: 1074 loss: 8.99095767e-07
Iter: 1075 loss: 8.9778689e-07
Iter: 1076 loss: 8.97470045e-07
Iter: 1077 loss: 8.98687176e-07
Iter: 1078 loss: 8.97386371e-07
Iter: 1079 loss: 8.97098744e-07
Iter: 1080 loss: 8.97845894e-07
Iter: 1081 loss: 8.96939184e-07
Iter: 1082 loss: 8.96664687e-07
Iter: 1083 loss: 8.98378858e-07
Iter: 1084 loss: 8.96606707e-07
Iter: 1085 loss: 8.9632465e-07
Iter: 1086 loss: 8.97242899e-07
Iter: 1087 loss: 8.96256665e-07
Iter: 1088 loss: 8.95943401e-07
Iter: 1089 loss: 8.9564395e-07
Iter: 1090 loss: 8.95617859e-07
Iter: 1091 loss: 8.95194603e-07
Iter: 1092 loss: 8.97622272e-07
Iter: 1093 loss: 8.95227402e-07
Iter: 1094 loss: 8.95118546e-07
Iter: 1095 loss: 8.95014352e-07
Iter: 1096 loss: 8.94915e-07
Iter: 1097 loss: 8.94511174e-07
Iter: 1098 loss: 8.992983e-07
Iter: 1099 loss: 8.94470134e-07
Iter: 1100 loss: 8.94168e-07
Iter: 1101 loss: 8.95367748e-07
Iter: 1102 loss: 8.9410014e-07
Iter: 1103 loss: 8.93827462e-07
Iter: 1104 loss: 8.93558763e-07
Iter: 1105 loss: 8.93498225e-07
Iter: 1106 loss: 8.93141419e-07
Iter: 1107 loss: 8.97100733e-07
Iter: 1108 loss: 8.93114816e-07
Iter: 1109 loss: 8.92973333e-07
Iter: 1110 loss: 8.93049446e-07
Iter: 1111 loss: 8.92709636e-07
Iter: 1112 loss: 8.92432922e-07
Iter: 1113 loss: 8.93723495e-07
Iter: 1114 loss: 8.92424055e-07
Iter: 1115 loss: 8.92202e-07
Iter: 1116 loss: 8.92497496e-07
Iter: 1117 loss: 8.92053038e-07
Iter: 1118 loss: 8.91767968e-07
Iter: 1119 loss: 8.93095773e-07
Iter: 1120 loss: 8.91735226e-07
Iter: 1121 loss: 8.91532409e-07
Iter: 1122 loss: 8.91926163e-07
Iter: 1123 loss: 8.91430147e-07
Iter: 1124 loss: 8.91157129e-07
Iter: 1125 loss: 8.92039395e-07
Iter: 1126 loss: 8.91152808e-07
Iter: 1127 loss: 8.90863816e-07
Iter: 1128 loss: 8.92229423e-07
Iter: 1129 loss: 8.90791398e-07
Iter: 1130 loss: 8.90504339e-07
Iter: 1131 loss: 8.92722596e-07
Iter: 1132 loss: 8.90485126e-07
Iter: 1133 loss: 8.90435672e-07
Iter: 1134 loss: 8.90170327e-07
Iter: 1135 loss: 8.91874663e-07
Iter: 1136 loss: 8.90093531e-07
Iter: 1137 loss: 8.89725243e-07
Iter: 1138 loss: 8.91216359e-07
Iter: 1139 loss: 8.89679143e-07
Iter: 1140 loss: 8.89354169e-07
Iter: 1141 loss: 8.90119168e-07
Iter: 1142 loss: 8.89210469e-07
Iter: 1143 loss: 8.88894419e-07
Iter: 1144 loss: 8.88835473e-07
Iter: 1145 loss: 8.88595764e-07
Iter: 1146 loss: 8.88201498e-07
Iter: 1147 loss: 8.92467085e-07
Iter: 1148 loss: 8.88191039e-07
Iter: 1149 loss: 8.87902956e-07
Iter: 1150 loss: 8.87795352e-07
Iter: 1151 loss: 8.87646593e-07
Iter: 1152 loss: 8.87266481e-07
Iter: 1153 loss: 8.89661862e-07
Iter: 1154 loss: 8.8722561e-07
Iter: 1155 loss: 8.86865621e-07
Iter: 1156 loss: 8.8789966e-07
Iter: 1157 loss: 8.86752105e-07
Iter: 1158 loss: 8.86522912e-07
Iter: 1159 loss: 8.88022896e-07
Iter: 1160 loss: 8.86455439e-07
Iter: 1161 loss: 8.86226303e-07
Iter: 1162 loss: 8.87498231e-07
Iter: 1163 loss: 8.8621141e-07
Iter: 1164 loss: 8.86067824e-07
Iter: 1165 loss: 8.88365e-07
Iter: 1166 loss: 8.8607112e-07
Iter: 1167 loss: 8.85977272e-07
Iter: 1168 loss: 8.85747681e-07
Iter: 1169 loss: 8.87614249e-07
Iter: 1170 loss: 8.85681402e-07
Iter: 1171 loss: 8.85351085e-07
Iter: 1172 loss: 8.86049406e-07
Iter: 1173 loss: 8.85216707e-07
Iter: 1174 loss: 8.85008944e-07
Iter: 1175 loss: 8.86233863e-07
Iter: 1176 loss: 8.84989277e-07
Iter: 1177 loss: 8.84673568e-07
Iter: 1178 loss: 8.84718474e-07
Iter: 1179 loss: 8.84494909e-07
Iter: 1180 loss: 8.84265887e-07
Iter: 1181 loss: 8.85809868e-07
Iter: 1182 loss: 8.84203814e-07
Iter: 1183 loss: 8.83909138e-07
Iter: 1184 loss: 8.8400094e-07
Iter: 1185 loss: 8.83714733e-07
Iter: 1186 loss: 8.83442283e-07
Iter: 1187 loss: 8.85622569e-07
Iter: 1188 loss: 8.83398968e-07
Iter: 1189 loss: 8.83157213e-07
Iter: 1190 loss: 8.83176085e-07
Iter: 1191 loss: 8.83001121e-07
Iter: 1192 loss: 8.82542565e-07
Iter: 1193 loss: 8.84528276e-07
Iter: 1194 loss: 8.82535e-07
Iter: 1195 loss: 8.82337531e-07
Iter: 1196 loss: 8.82327299e-07
Iter: 1197 loss: 8.82251697e-07
Iter: 1198 loss: 8.83852749e-07
Iter: 1199 loss: 8.82210884e-07
Iter: 1200 loss: 8.82099187e-07
Iter: 1201 loss: 8.81851804e-07
Iter: 1202 loss: 8.86452881e-07
Iter: 1203 loss: 8.8183765e-07
Iter: 1204 loss: 8.81625056e-07
Iter: 1205 loss: 8.81642e-07
Iter: 1206 loss: 8.81495623e-07
Iter: 1207 loss: 8.81274e-07
Iter: 1208 loss: 8.82290692e-07
Iter: 1209 loss: 8.81143592e-07
Iter: 1210 loss: 8.80916616e-07
Iter: 1211 loss: 8.81765686e-07
Iter: 1212 loss: 8.80822085e-07
Iter: 1213 loss: 8.80617904e-07
Iter: 1214 loss: 8.8099614e-07
Iter: 1215 loss: 8.80504217e-07
Iter: 1216 loss: 8.80301968e-07
Iter: 1217 loss: 8.8045374e-07
Iter: 1218 loss: 8.80093808e-07
Iter: 1219 loss: 8.79753543e-07
Iter: 1220 loss: 8.81761935e-07
Iter: 1221 loss: 8.79728418e-07
Iter: 1222 loss: 8.79504796e-07
Iter: 1223 loss: 8.79703407e-07
Iter: 1224 loss: 8.79363711e-07
Iter: 1225 loss: 8.79109621e-07
Iter: 1226 loss: 8.80704135e-07
Iter: 1227 loss: 8.79023787e-07
Iter: 1228 loss: 8.78793458e-07
Iter: 1229 loss: 8.79733818e-07
Iter: 1230 loss: 8.78685455e-07
Iter: 1231 loss: 8.78574838e-07
Iter: 1232 loss: 8.78559376e-07
Iter: 1233 loss: 8.78465698e-07
Iter: 1234 loss: 8.78417e-07
Iter: 1235 loss: 8.78374351e-07
Iter: 1236 loss: 8.78152093e-07
Iter: 1237 loss: 8.77976e-07
Iter: 1238 loss: 8.7796758e-07
Iter: 1239 loss: 8.77807281e-07
Iter: 1240 loss: 8.78135666e-07
Iter: 1241 loss: 8.77669663e-07
Iter: 1242 loss: 8.7753142e-07
Iter: 1243 loss: 8.78987066e-07
Iter: 1244 loss: 8.77488219e-07
Iter: 1245 loss: 8.77286652e-07
Iter: 1246 loss: 8.7739852e-07
Iter: 1247 loss: 8.7716154e-07
Iter: 1248 loss: 8.76922115e-07
Iter: 1249 loss: 8.77151365e-07
Iter: 1250 loss: 8.76783e-07
Iter: 1251 loss: 8.76496642e-07
Iter: 1252 loss: 8.77520563e-07
Iter: 1253 loss: 8.76499712e-07
Iter: 1254 loss: 8.76169111e-07
Iter: 1255 loss: 8.76682122e-07
Iter: 1256 loss: 8.76011427e-07
Iter: 1257 loss: 8.7579491e-07
Iter: 1258 loss: 8.77574848e-07
Iter: 1259 loss: 8.75744945e-07
Iter: 1260 loss: 8.75540763e-07
Iter: 1261 loss: 8.75942249e-07
Iter: 1262 loss: 8.75470107e-07
Iter: 1263 loss: 8.75318051e-07
Iter: 1264 loss: 8.75297815e-07
Iter: 1265 loss: 8.75185776e-07
Iter: 1266 loss: 8.75367959e-07
Iter: 1267 loss: 8.75179808e-07
Iter: 1268 loss: 8.75098351e-07
Iter: 1269 loss: 8.74936177e-07
Iter: 1270 loss: 8.78360765e-07
Iter: 1271 loss: 8.74897466e-07
Iter: 1272 loss: 8.74741204e-07
Iter: 1273 loss: 8.74959539e-07
Iter: 1274 loss: 8.74607849e-07
Iter: 1275 loss: 8.74412763e-07
Iter: 1276 loss: 8.75767455e-07
Iter: 1277 loss: 8.74345119e-07
Iter: 1278 loss: 8.74137e-07
Iter: 1279 loss: 8.7449348e-07
Iter: 1280 loss: 8.7412559e-07
Iter: 1281 loss: 8.73912597e-07
Iter: 1282 loss: 8.74692432e-07
Iter: 1283 loss: 8.7386843e-07
Iter: 1284 loss: 8.73725639e-07
Iter: 1285 loss: 8.73712111e-07
Iter: 1286 loss: 8.73597912e-07
Iter: 1287 loss: 8.73361273e-07
Iter: 1288 loss: 8.73819147e-07
Iter: 1289 loss: 8.73232921e-07
Iter: 1290 loss: 8.73031354e-07
Iter: 1291 loss: 8.75234377e-07
Iter: 1292 loss: 8.73036697e-07
Iter: 1293 loss: 8.72880207e-07
Iter: 1294 loss: 8.72857413e-07
Iter: 1295 loss: 8.72702685e-07
Iter: 1296 loss: 8.72730141e-07
Iter: 1297 loss: 8.72613e-07
Iter: 1298 loss: 8.72562055e-07
Iter: 1299 loss: 8.72782493e-07
Iter: 1300 loss: 8.72490205e-07
Iter: 1301 loss: 8.72462e-07
Iter: 1302 loss: 8.7234946e-07
Iter: 1303 loss: 8.74683337e-07
Iter: 1304 loss: 8.72332e-07
Iter: 1305 loss: 8.72174098e-07
Iter: 1306 loss: 8.72510896e-07
Iter: 1307 loss: 8.72094176e-07
Iter: 1308 loss: 8.71964744e-07
Iter: 1309 loss: 8.72119301e-07
Iter: 1310 loss: 8.71879479e-07
Iter: 1311 loss: 8.7169991e-07
Iter: 1312 loss: 8.72143801e-07
Iter: 1313 loss: 8.71640168e-07
Iter: 1314 loss: 8.71385055e-07
Iter: 1315 loss: 8.72144881e-07
Iter: 1316 loss: 8.71330371e-07
Iter: 1317 loss: 8.7108549e-07
Iter: 1318 loss: 8.71265797e-07
Iter: 1319 loss: 8.71006648e-07
Iter: 1320 loss: 8.70727149e-07
Iter: 1321 loss: 8.718348e-07
Iter: 1322 loss: 8.70637678e-07
Iter: 1323 loss: 8.70446797e-07
Iter: 1324 loss: 8.71011e-07
Iter: 1325 loss: 8.70387908e-07
Iter: 1326 loss: 8.70177871e-07
Iter: 1327 loss: 8.70582e-07
Iter: 1328 loss: 8.7007686e-07
Iter: 1329 loss: 8.69985683e-07
Iter: 1330 loss: 8.69983694e-07
Iter: 1331 loss: 8.69835958e-07
Iter: 1332 loss: 8.70890858e-07
Iter: 1333 loss: 8.69838e-07
Iter: 1334 loss: 8.6974e-07
Iter: 1335 loss: 8.69644396e-07
Iter: 1336 loss: 8.71169959e-07
Iter: 1337 loss: 8.69539292e-07
Iter: 1338 loss: 8.6932846e-07
Iter: 1339 loss: 8.69981761e-07
Iter: 1340 loss: 8.69258656e-07
Iter: 1341 loss: 8.69063683e-07
Iter: 1342 loss: 8.69358644e-07
Iter: 1343 loss: 8.68971881e-07
Iter: 1344 loss: 8.68805955e-07
Iter: 1345 loss: 8.69332098e-07
Iter: 1346 loss: 8.68745531e-07
Iter: 1347 loss: 8.68583697e-07
Iter: 1348 loss: 8.70010581e-07
Iter: 1349 loss: 8.6854709e-07
Iter: 1350 loss: 8.68403845e-07
Iter: 1351 loss: 8.68291465e-07
Iter: 1352 loss: 8.68228938e-07
Iter: 1353 loss: 8.67967799e-07
Iter: 1354 loss: 8.68515372e-07
Iter: 1355 loss: 8.67838423e-07
Iter: 1356 loss: 8.67617473e-07
Iter: 1357 loss: 8.67680967e-07
Iter: 1358 loss: 8.67402719e-07
Iter: 1359 loss: 8.66952519e-07
Iter: 1360 loss: 8.70295423e-07
Iter: 1361 loss: 8.66974744e-07
Iter: 1362 loss: 8.66748792e-07
Iter: 1363 loss: 8.68227e-07
Iter: 1364 loss: 8.66727646e-07
Iter: 1365 loss: 8.66612766e-07
Iter: 1366 loss: 8.66599919e-07
Iter: 1367 loss: 8.66530797e-07
Iter: 1368 loss: 8.6639659e-07
Iter: 1369 loss: 8.69175949e-07
Iter: 1370 loss: 8.66320704e-07
Iter: 1371 loss: 8.66151879e-07
Iter: 1372 loss: 8.66637947e-07
Iter: 1373 loss: 8.66176038e-07
Iter: 1374 loss: 8.66000278e-07
Iter: 1375 loss: 8.65777224e-07
Iter: 1376 loss: 8.6575028e-07
Iter: 1377 loss: 8.65467655e-07
Iter: 1378 loss: 8.67388394e-07
Iter: 1379 loss: 8.65418087e-07
Iter: 1380 loss: 8.65255117e-07
Iter: 1381 loss: 8.65673314e-07
Iter: 1382 loss: 8.65189918e-07
Iter: 1383 loss: 8.64918263e-07
Iter: 1384 loss: 8.65920242e-07
Iter: 1385 loss: 8.64859601e-07
Iter: 1386 loss: 8.64698e-07
Iter: 1387 loss: 8.65273705e-07
Iter: 1388 loss: 8.6468242e-07
Iter: 1389 loss: 8.64488584e-07
Iter: 1390 loss: 8.64416279e-07
Iter: 1391 loss: 8.6429975e-07
Iter: 1392 loss: 8.64060098e-07
Iter: 1393 loss: 8.65491643e-07
Iter: 1394 loss: 8.64073172e-07
Iter: 1395 loss: 8.63847276e-07
Iter: 1396 loss: 8.64338404e-07
Iter: 1397 loss: 8.63760533e-07
Iter: 1398 loss: 8.63747e-07
Iter: 1399 loss: 8.6370369e-07
Iter: 1400 loss: 8.63521564e-07
Iter: 1401 loss: 8.63518096e-07
Iter: 1402 loss: 8.6346455e-07
Iter: 1403 loss: 8.63386504e-07
Iter: 1404 loss: 8.63430046e-07
Iter: 1405 loss: 8.63255877e-07
Iter: 1406 loss: 8.6307989e-07
Iter: 1407 loss: 8.63151399e-07
Iter: 1408 loss: 8.62974048e-07
Iter: 1409 loss: 8.6271217e-07
Iter: 1410 loss: 8.63578464e-07
Iter: 1411 loss: 8.6265959e-07
Iter: 1412 loss: 8.62471609e-07
Iter: 1413 loss: 8.62578815e-07
Iter: 1414 loss: 8.62340471e-07
Iter: 1415 loss: 8.62118441e-07
Iter: 1416 loss: 8.63757123e-07
Iter: 1417 loss: 8.62052048e-07
Iter: 1418 loss: 8.61815579e-07
Iter: 1419 loss: 8.62688353e-07
Iter: 1420 loss: 8.61811031e-07
Iter: 1421 loss: 8.61683e-07
Iter: 1422 loss: 8.61537956e-07
Iter: 1423 loss: 8.61509534e-07
Iter: 1424 loss: 8.61271815e-07
Iter: 1425 loss: 8.62685397e-07
Iter: 1426 loss: 8.6119195e-07
Iter: 1427 loss: 8.61011927e-07
Iter: 1428 loss: 8.61007607e-07
Iter: 1429 loss: 8.6082224e-07
Iter: 1430 loss: 8.60675868e-07
Iter: 1431 loss: 8.60654e-07
Iter: 1432 loss: 8.60553314e-07
Iter: 1433 loss: 8.61067292e-07
Iter: 1434 loss: 8.6045668e-07
Iter: 1435 loss: 8.60310877e-07
Iter: 1436 loss: 8.60282967e-07
Iter: 1437 loss: 8.60253692e-07
Iter: 1438 loss: 8.60149157e-07
Iter: 1439 loss: 8.6013489e-07
Iter: 1440 loss: 8.59996476e-07
Iter: 1441 loss: 8.59816964e-07
Iter: 1442 loss: 8.60851e-07
Iter: 1443 loss: 8.59745398e-07
Iter: 1444 loss: 8.59507281e-07
Iter: 1445 loss: 8.59613237e-07
Iter: 1446 loss: 8.59445663e-07
Iter: 1447 loss: 8.59084821e-07
Iter: 1448 loss: 8.60740442e-07
Iter: 1449 loss: 8.5914121e-07
Iter: 1450 loss: 8.58902126e-07
Iter: 1451 loss: 8.59436454e-07
Iter: 1452 loss: 8.58775252e-07
Iter: 1453 loss: 8.58517467e-07
Iter: 1454 loss: 8.58881776e-07
Iter: 1455 loss: 8.58449198e-07
Iter: 1456 loss: 8.58208864e-07
Iter: 1457 loss: 8.58540886e-07
Iter: 1458 loss: 8.58110411e-07
Iter: 1459 loss: 8.57876159e-07
Iter: 1460 loss: 8.58388717e-07
Iter: 1461 loss: 8.57750308e-07
Iter: 1462 loss: 8.57600241e-07
Iter: 1463 loss: 8.57562554e-07
Iter: 1464 loss: 8.57451539e-07
Iter: 1465 loss: 8.59095394e-07
Iter: 1466 loss: 8.57446878e-07
Iter: 1467 loss: 8.57313239e-07
Iter: 1468 loss: 8.57161353e-07
Iter: 1469 loss: 8.59352554e-07
Iter: 1470 loss: 8.57140776e-07
Iter: 1471 loss: 8.5693307e-07
Iter: 1472 loss: 8.57781174e-07
Iter: 1473 loss: 8.5680648e-07
Iter: 1474 loss: 8.56699216e-07
Iter: 1475 loss: 8.57021632e-07
Iter: 1476 loss: 8.56588e-07
Iter: 1477 loss: 8.56365091e-07
Iter: 1478 loss: 8.56518568e-07
Iter: 1479 loss: 8.56233441e-07
Iter: 1480 loss: 8.56024883e-07
Iter: 1481 loss: 8.57524867e-07
Iter: 1482 loss: 8.56001918e-07
Iter: 1483 loss: 8.55877829e-07
Iter: 1484 loss: 8.56076781e-07
Iter: 1485 loss: 8.55792109e-07
Iter: 1486 loss: 8.55582584e-07
Iter: 1487 loss: 8.56188933e-07
Iter: 1488 loss: 8.55525798e-07
Iter: 1489 loss: 8.55335713e-07
Iter: 1490 loss: 8.55721169e-07
Iter: 1491 loss: 8.55279495e-07
Iter: 1492 loss: 8.55089866e-07
Iter: 1493 loss: 8.55180815e-07
Iter: 1494 loss: 8.54975383e-07
Iter: 1495 loss: 8.54767336e-07
Iter: 1496 loss: 8.55737255e-07
Iter: 1497 loss: 8.54763243e-07
Iter: 1498 loss: 8.54685084e-07
Iter: 1499 loss: 8.54624034e-07
Iter: 1500 loss: 8.54580435e-07
Iter: 1501 loss: 8.54372729e-07
Iter: 1502 loss: 8.55671885e-07
Iter: 1503 loss: 8.54316738e-07
Iter: 1504 loss: 8.5418543e-07
Iter: 1505 loss: 8.55077189e-07
Iter: 1506 loss: 8.54074415e-07
Iter: 1507 loss: 8.53865515e-07
Iter: 1508 loss: 8.54035818e-07
Iter: 1509 loss: 8.53758081e-07
Iter: 1510 loss: 8.53507231e-07
Iter: 1511 loss: 8.54422069e-07
Iter: 1512 loss: 8.53451638e-07
Iter: 1513 loss: 8.53296569e-07
Iter: 1514 loss: 8.53533152e-07
Iter: 1515 loss: 8.53109839e-07
Iter: 1516 loss: 8.52884341e-07
Iter: 1517 loss: 8.53280483e-07
Iter: 1518 loss: 8.52794699e-07
Iter: 1519 loss: 8.52537767e-07
Iter: 1520 loss: 8.54200664e-07
Iter: 1521 loss: 8.52550386e-07
Iter: 1522 loss: 8.52312837e-07
Iter: 1523 loss: 8.52209041e-07
Iter: 1524 loss: 8.5214117e-07
Iter: 1525 loss: 8.51907657e-07
Iter: 1526 loss: 8.53312372e-07
Iter: 1527 loss: 8.51815798e-07
Iter: 1528 loss: 8.51607069e-07
Iter: 1529 loss: 8.5195154e-07
Iter: 1530 loss: 8.51565801e-07
Iter: 1531 loss: 8.51498e-07
Iter: 1532 loss: 8.51471782e-07
Iter: 1533 loss: 8.51373329e-07
Iter: 1534 loss: 8.51215589e-07
Iter: 1535 loss: 8.55383746e-07
Iter: 1536 loss: 8.51179038e-07
Iter: 1537 loss: 8.50997253e-07
Iter: 1538 loss: 8.51174491e-07
Iter: 1539 loss: 8.50926881e-07
Iter: 1540 loss: 8.5067262e-07
Iter: 1541 loss: 8.50732e-07
Iter: 1542 loss: 8.50480092e-07
Iter: 1543 loss: 8.50201275e-07
Iter: 1544 loss: 8.51820062e-07
Iter: 1545 loss: 8.50122888e-07
Iter: 1546 loss: 8.49920525e-07
Iter: 1547 loss: 8.50301262e-07
Iter: 1548 loss: 8.49811499e-07
Iter: 1549 loss: 8.4956082e-07
Iter: 1550 loss: 8.50235722e-07
Iter: 1551 loss: 8.4949545e-07
Iter: 1552 loss: 8.49348851e-07
Iter: 1553 loss: 8.50496406e-07
Iter: 1554 loss: 8.49278e-07
Iter: 1555 loss: 8.49110847e-07
Iter: 1556 loss: 8.49195033e-07
Iter: 1557 loss: 8.48943671e-07
Iter: 1558 loss: 8.4873335e-07
Iter: 1559 loss: 8.4974431e-07
Iter: 1560 loss: 8.48686113e-07
Iter: 1561 loss: 8.48488753e-07
Iter: 1562 loss: 8.48631885e-07
Iter: 1563 loss: 8.48338971e-07
Iter: 1564 loss: 8.4816304e-07
Iter: 1565 loss: 8.51000323e-07
Iter: 1566 loss: 8.4812325e-07
Iter: 1567 loss: 8.47927e-07
Iter: 1568 loss: 8.49544904e-07
Iter: 1569 loss: 8.47932313e-07
Iter: 1570 loss: 8.47843523e-07
Iter: 1571 loss: 8.47747742e-07
Iter: 1572 loss: 8.49171897e-07
Iter: 1573 loss: 8.47741831e-07
Iter: 1574 loss: 8.47595857e-07
Iter: 1575 loss: 8.47990634e-07
Iter: 1576 loss: 8.47530373e-07
Iter: 1577 loss: 8.47353306e-07
Iter: 1578 loss: 8.48230968e-07
Iter: 1579 loss: 8.47317779e-07
Iter: 1580 loss: 8.47139461e-07
Iter: 1581 loss: 8.47127865e-07
Iter: 1582 loss: 8.47058061e-07
Iter: 1583 loss: 8.46839896e-07
Iter: 1584 loss: 8.48134448e-07
Iter: 1585 loss: 8.4680147e-07
Iter: 1586 loss: 8.46659702e-07
Iter: 1587 loss: 8.46657144e-07
Iter: 1588 loss: 8.46565399e-07
Iter: 1589 loss: 8.4632029e-07
Iter: 1590 loss: 8.47760418e-07
Iter: 1591 loss: 8.46278851e-07
Iter: 1592 loss: 8.4610565e-07
Iter: 1593 loss: 8.4608655e-07
Iter: 1594 loss: 8.46024818e-07
Iter: 1595 loss: 8.45856391e-07
Iter: 1596 loss: 8.4721546e-07
Iter: 1597 loss: 8.45844852e-07
Iter: 1598 loss: 8.45638795e-07
Iter: 1599 loss: 8.46132878e-07
Iter: 1600 loss: 8.45566205e-07
Iter: 1601 loss: 8.45618047e-07
Iter: 1602 loss: 8.45507316e-07
Iter: 1603 loss: 8.45436603e-07
Iter: 1604 loss: 8.45353384e-07
Iter: 1605 loss: 8.4624503e-07
Iter: 1606 loss: 8.45324678e-07
Iter: 1607 loss: 8.45245665e-07
Iter: 1608 loss: 8.45094291e-07
Iter: 1609 loss: 8.45063653e-07
Iter: 1610 loss: 8.44806891e-07
Iter: 1611 loss: 8.45724799e-07
Iter: 1612 loss: 8.4474874e-07
Iter: 1613 loss: 8.44499596e-07
Iter: 1614 loss: 8.46109629e-07
Iter: 1615 loss: 8.44423766e-07
Iter: 1616 loss: 8.44366923e-07
Iter: 1617 loss: 8.44397846e-07
Iter: 1618 loss: 8.44267447e-07
Iter: 1619 loss: 8.43994144e-07
Iter: 1620 loss: 8.44028364e-07
Iter: 1621 loss: 8.43873863e-07
Iter: 1622 loss: 8.43567136e-07
Iter: 1623 loss: 8.46204784e-07
Iter: 1624 loss: 8.43606585e-07
Iter: 1625 loss: 8.4335386e-07
Iter: 1626 loss: 8.43252906e-07
Iter: 1627 loss: 8.43178384e-07
Iter: 1628 loss: 8.4289195e-07
Iter: 1629 loss: 8.45226964e-07
Iter: 1630 loss: 8.42885868e-07
Iter: 1631 loss: 8.42660484e-07
Iter: 1632 loss: 8.42915938e-07
Iter: 1633 loss: 8.42565669e-07
Iter: 1634 loss: 8.42416398e-07
Iter: 1635 loss: 8.44039221e-07
Iter: 1636 loss: 8.42405825e-07
Iter: 1637 loss: 8.42308964e-07
Iter: 1638 loss: 8.42277927e-07
Iter: 1639 loss: 8.42178e-07
Iter: 1640 loss: 8.41890937e-07
Iter: 1641 loss: 8.45385046e-07
Iter: 1642 loss: 8.41937776e-07
Iter: 1643 loss: 8.41670101e-07
Iter: 1644 loss: 8.41946758e-07
Iter: 1645 loss: 8.41464839e-07
Iter: 1646 loss: 8.41312499e-07
Iter: 1647 loss: 8.42042368e-07
Iter: 1648 loss: 8.41339215e-07
Iter: 1649 loss: 8.41111671e-07
Iter: 1650 loss: 8.4155397e-07
Iter: 1651 loss: 8.41050166e-07
Iter: 1652 loss: 8.40833309e-07
Iter: 1653 loss: 8.42295037e-07
Iter: 1654 loss: 8.40823077e-07
Iter: 1655 loss: 8.40720588e-07
Iter: 1656 loss: 8.4069552e-07
Iter: 1657 loss: 8.40581379e-07
Iter: 1658 loss: 8.40428584e-07
Iter: 1659 loss: 8.40671305e-07
Iter: 1660 loss: 8.40344057e-07
Iter: 1661 loss: 8.40108441e-07
Iter: 1662 loss: 8.4090766e-07
Iter: 1663 loss: 8.40062341e-07
Iter: 1664 loss: 8.39872257e-07
Iter: 1665 loss: 8.40304892e-07
Iter: 1666 loss: 8.39708377e-07
Iter: 1667 loss: 8.39637778e-07
Iter: 1668 loss: 8.41206713e-07
Iter: 1669 loss: 8.39623851e-07
Iter: 1670 loss: 8.39460938e-07
Iter: 1671 loss: 8.39245558e-07
Iter: 1672 loss: 8.39266647e-07
Iter: 1673 loss: 8.39048653e-07
Iter: 1674 loss: 8.42005193e-07
Iter: 1675 loss: 8.39050699e-07
Iter: 1676 loss: 8.39010454e-07
Iter: 1677 loss: 8.38969754e-07
Iter: 1678 loss: 8.38917515e-07
Iter: 1679 loss: 8.38703784e-07
Iter: 1680 loss: 8.40226392e-07
Iter: 1681 loss: 8.38643132e-07
Iter: 1682 loss: 8.38440144e-07
Iter: 1683 loss: 8.39241295e-07
Iter: 1684 loss: 8.38394044e-07
Iter: 1685 loss: 8.38200947e-07
Iter: 1686 loss: 8.38564461e-07
Iter: 1687 loss: 8.38149674e-07
Iter: 1688 loss: 8.37969708e-07
Iter: 1689 loss: 8.38407516e-07
Iter: 1690 loss: 8.3786847e-07
Iter: 1691 loss: 8.37727498e-07
Iter: 1692 loss: 8.39076733e-07
Iter: 1693 loss: 8.37654397e-07
Iter: 1694 loss: 8.37516268e-07
Iter: 1695 loss: 8.3739053e-07
Iter: 1696 loss: 8.3738604e-07
Iter: 1697 loss: 8.37145649e-07
Iter: 1698 loss: 8.38484766e-07
Iter: 1699 loss: 8.37157359e-07
Iter: 1700 loss: 8.37017069e-07
Iter: 1701 loss: 8.37077891e-07
Iter: 1702 loss: 8.36887921e-07
Iter: 1703 loss: 8.36661457e-07
Iter: 1704 loss: 8.38030303e-07
Iter: 1705 loss: 8.36716538e-07
Iter: 1706 loss: 8.36517813e-07
Iter: 1707 loss: 8.36682659e-07
Iter: 1708 loss: 8.3639668e-07
Iter: 1709 loss: 8.36249342e-07
Iter: 1710 loss: 8.38305084e-07
Iter: 1711 loss: 8.36259744e-07
Iter: 1712 loss: 8.36089384e-07
Iter: 1713 loss: 8.37585105e-07
Iter: 1714 loss: 8.36061815e-07
Iter: 1715 loss: 8.36004062e-07
Iter: 1716 loss: 8.35871106e-07
Iter: 1717 loss: 8.37463858e-07
Iter: 1718 loss: 8.3577271e-07
Iter: 1719 loss: 8.3557785e-07
Iter: 1720 loss: 8.36237291e-07
Iter: 1721 loss: 8.35574383e-07
Iter: 1722 loss: 8.35340529e-07
Iter: 1723 loss: 8.35963306e-07
Iter: 1724 loss: 8.35317e-07
Iter: 1725 loss: 8.35156243e-07
Iter: 1726 loss: 8.35402602e-07
Iter: 1727 loss: 8.35046649e-07
Iter: 1728 loss: 8.34865261e-07
Iter: 1729 loss: 8.35710694e-07
Iter: 1730 loss: 8.3482638e-07
Iter: 1731 loss: 8.34696493e-07
Iter: 1732 loss: 8.34705133e-07
Iter: 1733 loss: 8.3455825e-07
Iter: 1734 loss: 8.34397042e-07
Iter: 1735 loss: 8.34942512e-07
Iter: 1736 loss: 8.34236e-07
Iter: 1737 loss: 8.34065645e-07
Iter: 1738 loss: 8.34769651e-07
Iter: 1739 loss: 8.33990782e-07
Iter: 1740 loss: 8.33848048e-07
Iter: 1741 loss: 8.33807917e-07
Iter: 1742 loss: 8.33695083e-07
Iter: 1743 loss: 8.33448894e-07
Iter: 1744 loss: 8.35356218e-07
Iter: 1745 loss: 8.3346913e-07
Iter: 1746 loss: 8.33433376e-07
Iter: 1747 loss: 8.33393869e-07
Iter: 1748 loss: 8.3334703e-07
Iter: 1749 loss: 8.33165359e-07
Iter: 1750 loss: 8.35992182e-07
Iter: 1751 loss: 8.33152285e-07
Iter: 1752 loss: 8.33030811e-07
Iter: 1753 loss: 8.33398246e-07
Iter: 1754 loss: 8.32983119e-07
Iter: 1755 loss: 8.32891033e-07
Iter: 1756 loss: 8.32791045e-07
Iter: 1757 loss: 8.32726641e-07
Iter: 1758 loss: 8.32528372e-07
Iter: 1759 loss: 8.33169622e-07
Iter: 1760 loss: 8.32437365e-07
Iter: 1761 loss: 8.3232851e-07
Iter: 1762 loss: 8.33023e-07
Iter: 1763 loss: 8.32266949e-07
Iter: 1764 loss: 8.32116484e-07
Iter: 1765 loss: 8.32632566e-07
Iter: 1766 loss: 8.32123419e-07
Iter: 1767 loss: 8.32019964e-07
Iter: 1768 loss: 8.32178557e-07
Iter: 1769 loss: 8.31878424e-07
Iter: 1770 loss: 8.31800662e-07
Iter: 1771 loss: 8.31926798e-07
Iter: 1772 loss: 8.31712555e-07
Iter: 1773 loss: 8.31520595e-07
Iter: 1774 loss: 8.31682769e-07
Iter: 1775 loss: 8.31460852e-07
Iter: 1776 loss: 8.31291629e-07
Iter: 1777 loss: 8.31742966e-07
Iter: 1778 loss: 8.31226089e-07
Iter: 1779 loss: 8.31034185e-07
Iter: 1780 loss: 8.31576415e-07
Iter: 1781 loss: 8.30936301e-07
Iter: 1782 loss: 8.31144462e-07
Iter: 1783 loss: 8.30868032e-07
Iter: 1784 loss: 8.30897875e-07
Iter: 1785 loss: 8.30885085e-07
Iter: 1786 loss: 8.30902763e-07
Iter: 1787 loss: 8.3093289e-07
Iter: 1788 loss: 8.30889803e-07
Iter: 1789 loss: 8.30871727e-07
Iter: 1790 loss: 8.3090822e-07
Iter: 1791 loss: 8.30876672e-07
Iter: 1792 loss: 8.30901627e-07
Iter: 1793 loss: 8.30880822e-07
Iter: 1794 loss: 8.30849785e-07
Iter: 1795 loss: 8.30862632e-07
Iter: 1796 loss: 8.30867521e-07
Iter: 1797 loss: 8.30871954e-07
Iter: 1798 loss: 8.30873205e-07
Iter: 1799 loss: 8.30867577e-07
Iter: 1800 loss: 8.30874797e-07
Iter: 1801 loss: 8.30866952e-07
Iter: 1802 loss: 8.30874342e-07
Iter: 1803 loss: 8.30873432e-07
Iter: 1804 loss: 8.30868e-07
Iter: 1805 loss: 8.30873489e-07
Iter: 1806 loss: 8.30873489e-07
Iter: 1807 loss: 8.30872466e-07
Iter: 1808 loss: 8.30872466e-07
Iter: 1809 loss: 8.30872466e-07
Iter: 1810 loss: 8.30872466e-07
Iter: 1811 loss: 8.30872466e-07
Iter: 1812 loss: 8.30868e-07
Iter: 1813 loss: 8.30868e-07
Iter: 1814 loss: 8.30868e-07
Iter: 1815 loss: 8.30868e-07
Iter: 1816 loss: 8.30872466e-07
Iter: 1817 loss: 8.30868e-07
Iter: 1818 loss: 8.30872466e-07
Iter: 1819 loss: 8.32027922e-07
Iter: 1820 loss: 8.30817726e-07
Iter: 1821 loss: 8.31018156e-07
Iter: 1822 loss: 8.30765828e-07
Iter: 1823 loss: 8.30594331e-07
Iter: 1824 loss: 8.31898888e-07
Iter: 1825 loss: 8.30502529e-07
Iter: 1826 loss: 8.304782e-07
Iter: 1827 loss: 8.30356043e-07
Iter: 1828 loss: 8.303507e-07
Iter: 1829 loss: 8.30131285e-07
Iter: 1830 loss: 8.30520207e-07
Iter: 1831 loss: 8.30087288e-07
Iter: 1832 loss: 8.29918918e-07
Iter: 1833 loss: 8.30380884e-07
Iter: 1834 loss: 8.29880264e-07
Iter: 1835 loss: 8.29678584e-07
Iter: 1836 loss: 8.29731789e-07
Iter: 1837 loss: 8.29570695e-07
Iter: 1838 loss: 8.29483952e-07
Iter: 1839 loss: 8.29443422e-07
Iter: 1840 loss: 8.29341161e-07
Iter: 1841 loss: 8.29170801e-07
Iter: 1842 loss: 8.2927005e-07
Iter: 1843 loss: 8.29093665e-07
Iter: 1844 loss: 8.28912164e-07
Iter: 1845 loss: 8.30047156e-07
Iter: 1846 loss: 8.28915e-07
Iter: 1847 loss: 8.28817122e-07
Iter: 1848 loss: 8.29146359e-07
Iter: 1849 loss: 8.28808425e-07
Iter: 1850 loss: 8.28727707e-07
Iter: 1851 loss: 8.28848e-07
Iter: 1852 loss: 8.2863005e-07
Iter: 1853 loss: 8.28567352e-07
Iter: 1854 loss: 8.28490101e-07
Iter: 1855 loss: 8.28489306e-07
Iter: 1856 loss: 8.28230782e-07
Iter: 1857 loss: 8.28092823e-07
Iter: 1858 loss: 8.27975839e-07
Iter: 1859 loss: 8.27879603e-07
Iter: 1860 loss: 8.27859196e-07
Iter: 1861 loss: 8.2771669e-07
Iter: 1862 loss: 8.28127099e-07
Iter: 1863 loss: 8.27700603e-07
Iter: 1864 loss: 8.27492386e-07
Iter: 1865 loss: 8.27662e-07
Iter: 1866 loss: 8.27359088e-07
Iter: 1867 loss: 8.27277e-07
Iter: 1868 loss: 8.2805343e-07
Iter: 1869 loss: 8.27256372e-07
Iter: 1870 loss: 8.27125177e-07
Iter: 1871 loss: 8.2715178e-07
Iter: 1872 loss: 8.27064923e-07
Iter: 1873 loss: 8.26895473e-07
Iter: 1874 loss: 8.27566055e-07
Iter: 1875 loss: 8.26803728e-07
Iter: 1876 loss: 8.26719543e-07
Iter: 1877 loss: 8.26821804e-07
Iter: 1878 loss: 8.26674295e-07
Iter: 1879 loss: 8.26515e-07
Iter: 1880 loss: 8.27877784e-07
Iter: 1881 loss: 8.26463861e-07
Iter: 1882 loss: 8.26470568e-07
Iter: 1883 loss: 8.2643669e-07
Iter: 1884 loss: 8.26351823e-07
Iter: 1885 loss: 8.26243308e-07
Iter: 1886 loss: 8.27324811e-07
Iter: 1887 loss: 8.26202836e-07
Iter: 1888 loss: 8.260497e-07
Iter: 1889 loss: 8.27470046e-07
Iter: 1890 loss: 8.26036967e-07
Iter: 1891 loss: 8.25994391e-07
Iter: 1892 loss: 8.2600576e-07
Iter: 1893 loss: 8.25915e-07
Iter: 1894 loss: 8.25742063e-07
Iter: 1895 loss: 8.26258656e-07
Iter: 1896 loss: 8.2565515e-07
Iter: 1897 loss: 8.25627467e-07
Iter: 1898 loss: 8.25649295e-07
Iter: 1899 loss: 8.25548454e-07
Iter: 1900 loss: 8.25473592e-07
Iter: 1901 loss: 8.25495874e-07
Iter: 1902 loss: 8.25344046e-07
Iter: 1903 loss: 8.25400207e-07
Iter: 1904 loss: 8.25212624e-07
Iter: 1905 loss: 8.25133327e-07
Iter: 1906 loss: 8.25108145e-07
Iter: 1907 loss: 8.25021459e-07
Iter: 1908 loss: 8.24999631e-07
Iter: 1909 loss: 8.25002871e-07
Iter: 1910 loss: 8.24834956e-07
Iter: 1911 loss: 8.24804829e-07
Iter: 1912 loss: 8.24687731e-07
Iter: 1913 loss: 8.24633901e-07
Iter: 1914 loss: 8.24568701e-07
Iter: 1915 loss: 8.24464337e-07
Iter: 1916 loss: 8.24698191e-07
Iter: 1917 loss: 8.24452172e-07
Iter: 1918 loss: 8.24374752e-07
Iter: 1919 loss: 8.24138397e-07
Iter: 1920 loss: 8.25162033e-07
Iter: 1921 loss: 8.24099118e-07
Iter: 1922 loss: 8.23863161e-07
Iter: 1923 loss: 8.25462621e-07
Iter: 1924 loss: 8.23845653e-07
Iter: 1925 loss: 8.23680637e-07
Iter: 1926 loss: 8.25314146e-07
Iter: 1927 loss: 8.23653863e-07
Iter: 1928 loss: 8.23524829e-07
Iter: 1929 loss: 8.24103836e-07
Iter: 1930 loss: 8.23555638e-07
Iter: 1931 loss: 8.23449227e-07
Iter: 1932 loss: 8.23573032e-07
Iter: 1933 loss: 8.23396931e-07
Iter: 1934 loss: 8.23262098e-07
Iter: 1935 loss: 8.23378741e-07
Iter: 1936 loss: 8.23176606e-07
Iter: 1937 loss: 8.23112089e-07
Iter: 1938 loss: 8.2355632e-07
Iter: 1939 loss: 8.23162281e-07
Iter: 1940 loss: 8.23044786e-07
Iter: 1941 loss: 8.22971174e-07
Iter: 1942 loss: 8.22940933e-07
Iter: 1943 loss: 8.2282429e-07
Iter: 1944 loss: 8.23294101e-07
Iter: 1945 loss: 8.22758125e-07
Iter: 1946 loss: 8.22666948e-07
Iter: 1947 loss: 8.22692755e-07
Iter: 1948 loss: 8.22633e-07
Iter: 1949 loss: 8.22496702e-07
Iter: 1950 loss: 8.22461516e-07
Iter: 1951 loss: 8.2234618e-07
Iter: 1952 loss: 8.22522622e-07
Iter: 1953 loss: 8.22345271e-07
Iter: 1954 loss: 8.22278821e-07
Iter: 1955 loss: 8.22082598e-07
Iter: 1956 loss: 8.22146205e-07
Iter: 1957 loss: 8.21991932e-07
Iter: 1958 loss: 8.22203674e-07
Iter: 1959 loss: 8.21962e-07
Iter: 1960 loss: 8.21759841e-07
Iter: 1961 loss: 8.22322477e-07
Iter: 1962 loss: 8.21723404e-07
Iter: 1963 loss: 8.2161614e-07
Iter: 1964 loss: 8.21895e-07
Iter: 1965 loss: 8.2158158e-07
Iter: 1966 loss: 8.21468575e-07
Iter: 1967 loss: 8.21458116e-07
Iter: 1968 loss: 8.21379103e-07
Iter: 1969 loss: 8.21376375e-07
Iter: 1970 loss: 8.21308049e-07
Iter: 1971 loss: 8.21194e-07
Iter: 1972 loss: 8.21244726e-07
Iter: 1973 loss: 8.21171739e-07
Iter: 1974 loss: 8.21006324e-07
Iter: 1975 loss: 8.21877506e-07
Iter: 1976 loss: 8.20930836e-07
Iter: 1977 loss: 8.20903324e-07
Iter: 1978 loss: 8.20920945e-07
Iter: 1979 loss: 8.20861487e-07
Iter: 1980 loss: 8.20699825e-07
Iter: 1981 loss: 8.20757805e-07
Iter: 1982 loss: 8.2058807e-07
Iter: 1983 loss: 8.20558512e-07
Iter: 1984 loss: 8.2053e-07
Iter: 1985 loss: 8.20458354e-07
Iter: 1986 loss: 8.20483e-07
Iter: 1987 loss: 8.20398e-07
Iter: 1988 loss: 8.20318178e-07
Iter: 1989 loss: 8.20260652e-07
Iter: 1990 loss: 8.20166292e-07
Iter: 1991 loss: 8.20110586e-07
Iter: 1992 loss: 8.2046688e-07
Iter: 1993 loss: 8.20074888e-07
Iter: 1994 loss: 8.19891966e-07
Iter: 1995 loss: 8.19901288e-07
Iter: 1996 loss: 8.19797492e-07
Iter: 1997 loss: 8.19604509e-07
Iter: 1998 loss: 8.20786966e-07
Iter: 1999 loss: 8.19593652e-07
Iter: 2000 loss: 8.19521915e-07
Iter: 2001 loss: 8.2064048e-07
Iter: 2002 loss: 8.19526122e-07
Iter: 2003 loss: 8.19432898e-07
Iter: 2004 loss: 8.19367301e-07
Iter: 2005 loss: 8.19351271e-07
Iter: 2006 loss: 8.19215188e-07
Iter: 2007 loss: 8.19760317e-07
Iter: 2008 loss: 8.19246111e-07
Iter: 2009 loss: 8.19187278e-07
Iter: 2010 loss: 8.19216211e-07
Iter: 2011 loss: 8.19083652e-07
Iter: 2012 loss: 8.1899492e-07
Iter: 2013 loss: 8.19025558e-07
Iter: 2014 loss: 8.18888111e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi3
+ date
Wed Oct 21 17:37:15 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/300_300_300_1 --function f1 --psi 0 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e65de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e684598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e73ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e5d7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e5d1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e674620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e674ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e5ae1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e5ae268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e5ae6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e544268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e56aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e4bcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e4ce048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e4e5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e578ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e445e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e474510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f870e474048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ecf4e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ecebaa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ecedd0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86eceba9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c8743a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ecf7d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ecf74b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ecf74840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c8713378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c8713048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c86b5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c866a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c860a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c8636620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86c860fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ece8a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f86ece7ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.021129474
test_loss: 0.018521177
train_loss: 0.006700809
test_loss: 0.006398342
train_loss: 0.0037941495
test_loss: 0.0030232002
train_loss: 0.002358644
test_loss: 0.002284332
train_loss: 0.0023130241
test_loss: 0.0020607107
train_loss: 0.0022131763
test_loss: 0.0031315936
train_loss: 0.002886698
test_loss: 0.003162545
train_loss: 0.00198451
test_loss: 0.0027684001
train_loss: 0.002174793
test_loss: 0.002020517
train_loss: 0.0017280421
test_loss: 0.0018899894
train_loss: 0.0020210124
test_loss: 0.0021255054
train_loss: 0.0018337168
test_loss: 0.0018097264
train_loss: 0.0017631376
test_loss: 0.0016990654
train_loss: 0.0020722253
test_loss: 0.0018690554
train_loss: 0.0023275297
test_loss: 0.0024220876
train_loss: 0.0018989563
test_loss: 0.0018325931
train_loss: 0.0017533805
test_loss: 0.0018342382
train_loss: 0.0017319745
test_loss: 0.0018350013
train_loss: 0.002623636
test_loss: 0.0018451837
train_loss: 0.0020077894
test_loss: 0.0016715343
train_loss: 0.0019597078
test_loss: 0.0020222159
train_loss: 0.0028164247
test_loss: 0.0023677484
train_loss: 0.0018954773
test_loss: 0.0022986797
train_loss: 0.0018972855
test_loss: 0.0018784598
train_loss: 0.0020776086
test_loss: 0.0019950122
train_loss: 0.002512217
test_loss: 0.0018346596
train_loss: 0.0022241701
test_loss: 0.0020120526
train_loss: 0.0020356008
test_loss: 0.0018218786
train_loss: 0.0020169474
test_loss: 0.00225041
train_loss: 0.0017508144
test_loss: 0.0019469996
train_loss: 0.0022577066
test_loss: 0.0023491671
train_loss: 0.0021895266
test_loss: 0.00247745
train_loss: 0.001976376
test_loss: 0.0021450405
train_loss: 0.0020107643
test_loss: 0.0024825719
train_loss: 0.0018353565
test_loss: 0.0028245698
train_loss: 0.0022010836
test_loss: 0.0022737875
train_loss: 0.001995124
test_loss: 0.0018716819
train_loss: 0.002326163
test_loss: 0.002384978
train_loss: 0.0018017583
test_loss: 0.0018353201
train_loss: 0.0018519096
test_loss: 0.0016753234
train_loss: 0.0016251758
test_loss: 0.0016588295
train_loss: 0.0016164486
test_loss: 0.0018871258
train_loss: 0.001962343
test_loss: 0.0019023542
train_loss: 0.0020973538
test_loss: 0.0017814052
train_loss: 0.0018261209
test_loss: 0.0019540994
train_loss: 0.0018166138
test_loss: 0.002026662
train_loss: 0.0017242525
test_loss: 0.0017986738
train_loss: 0.0019628154
test_loss: 0.0019186742
train_loss: 0.0020187953
test_loss: 0.0017059918
train_loss: 0.0016721301
test_loss: 0.0018890171
train_loss: 0.0019327656
test_loss: 0.002133721
train_loss: 0.0019488317
test_loss: 0.0017700474
train_loss: 0.0020409771
test_loss: 0.0018341164
train_loss: 0.0019551492
test_loss: 0.0017535255
train_loss: 0.0016693167
test_loss: 0.0019435995
train_loss: 0.001767593
test_loss: 0.0022298212
train_loss: 0.0020269298
test_loss: 0.0016736557
train_loss: 0.0019419126
test_loss: 0.0020591023
train_loss: 0.0016621232
test_loss: 0.0017765346
train_loss: 0.0020071447
test_loss: 0.0020041727
train_loss: 0.002029328
test_loss: 0.0018714325
train_loss: 0.0020313791
test_loss: 0.002009335
train_loss: 0.0017616694
test_loss: 0.00260891
train_loss: 0.00255659
test_loss: 0.0022772767
train_loss: 0.0031747
test_loss: 0.003910932
train_loss: 0.0027637286
test_loss: 0.0023732416
train_loss: 0.0018943726
test_loss: 0.002266175
train_loss: 0.0020768004
test_loss: 0.0020643147
train_loss: 0.002202229
test_loss: 0.001755968
train_loss: 0.0023108039
test_loss: 0.0032293522
train_loss: 0.002191671
test_loss: 0.0032481297
train_loss: 0.0026769903
test_loss: 0.0031750365
train_loss: 0.002584875
test_loss: 0.003142315
train_loss: 0.0029558868
test_loss: 0.003002111
train_loss: 0.0019014376
test_loss: 0.0018660681
train_loss: 0.0018057512
test_loss: 0.0021538292
train_loss: 0.0021394892
test_loss: 0.0018854161
train_loss: 0.001777414
test_loss: 0.0018131494
train_loss: 0.001669548
test_loss: 0.0019128994
train_loss: 0.001622363
test_loss: 0.001595663
train_loss: 0.0018920025
test_loss: 0.0017116629
train_loss: 0.0016512536
test_loss: 0.002439839
train_loss: 0.0020334558
test_loss: 0.0021181963
train_loss: 0.001647379
test_loss: 0.0016792616
train_loss: 0.0019334671
test_loss: 0.0018653547
train_loss: 0.002399962
test_loss: 0.0022696222
train_loss: 0.002041873
test_loss: 0.0024195693
train_loss: 0.0018603551
test_loss: 0.0017105077
train_loss: 0.0020821462
test_loss: 0.0018784597
train_loss: 0.0019041016
test_loss: 0.0017013565
train_loss: 0.0015866116
test_loss: 0.0016986843
train_loss: 0.0019489683
test_loss: 0.0020174782
train_loss: 0.0017264818
test_loss: 0.001805258
train_loss: 0.0018198965
test_loss: 0.0021309957
train_loss: 0.0015507367
test_loss: 0.0017197034
train_loss: 0.0018150259
test_loss: 0.001838511
train_loss: 0.002245245
test_loss: 0.0017774529
train_loss: 0.0016223972
test_loss: 0.0017780908
train_loss: 0.0018523241
test_loss: 0.0017514259
train_loss: 0.001764163
test_loss: 0.0017971984
train_loss: 0.0015941757
test_loss: 0.001836781
train_loss: 0.0019634652
test_loss: 0.0015751056
train_loss: 0.0016274004
test_loss: 0.0017514176
train_loss: 0.0018463332
test_loss: 0.0017533714
train_loss: 0.0018450478
test_loss: 0.001897292
train_loss: 0.0018281722
test_loss: 0.002049048
train_loss: 0.0018578994
test_loss: 0.0021318924
train_loss: 0.0021621494
test_loss: 0.002096193
train_loss: 0.0019520682
test_loss: 0.0022447163
train_loss: 0.002016918
test_loss: 0.0020532683
train_loss: 0.0016725371
test_loss: 0.0024293175
train_loss: 0.001774119
test_loss: 0.0026415337
train_loss: 0.0023090069
test_loss: 0.0029016687
train_loss: 0.0019969367
test_loss: 0.0021739216
train_loss: 0.0021645874
test_loss: 0.0020060518
train_loss: 0.0017294114
test_loss: 0.0017359867
train_loss: 0.0020522468
test_loss: 0.0020128228
train_loss: 0.001750757
test_loss: 0.002051423
train_loss: 0.001969144
test_loss: 0.002268327
train_loss: 0.0020532738
test_loss: 0.0019781708
train_loss: 0.0019487161
test_loss: 0.0020336625
train_loss: 0.003142153
test_loss: 0.0028816215
train_loss: 0.0025581494
test_loss: 0.0029992503
train_loss: 0.0018234388
test_loss: 0.0025655034
train_loss: 0.00230574
test_loss: 0.0019110306
train_loss: 0.0019668972
test_loss: 0.001798116
train_loss: 0.0022611497
test_loss: 0.0021048135
train_loss: 0.00201967
test_loss: 0.0017772256
train_loss: 0.0016583173
test_loss: 0.0018891061
train_loss: 0.0024158794
test_loss: 0.0019562936
train_loss: 0.0021301596
test_loss: 0.0021349667
train_loss: 0.0033875764
test_loss: 0.0028481784
train_loss: 0.002392621
test_loss: 0.0023595688
train_loss: 0.0029490436
test_loss: 0.0020023512
train_loss: 0.0028706144
test_loss: 0.0021146676
train_loss: 0.0019055039
test_loss: 0.0021035676
train_loss: 0.0027789967
test_loss: 0.0029457244
train_loss: 0.0028297184
test_loss: 0.0023354755
train_loss: 0.0021596733
test_loss: 0.0021001326
train_loss: 0.003181175
test_loss: 0.0029290242
train_loss: 0.0018655742
test_loss: 0.0021834518
train_loss: 0.001977928
test_loss: 0.0018082621
train_loss: 0.0017962742
test_loss: 0.0018331779
train_loss: 0.0019626052
test_loss: 0.0019678366
train_loss: 0.0016253367
test_loss: 0.0016562914
train_loss: 0.0018015455
test_loss: 0.0019092602
train_loss: 0.0017332258
test_loss: 0.001873175
train_loss: 0.0018630124
test_loss: 0.001652348
train_loss: 0.0021080202
test_loss: 0.0022777168
train_loss: 0.0017250961
test_loss: 0.0018518514
train_loss: 0.0018839794
test_loss: 0.0020023745
train_loss: 0.001736228
test_loss: 0.001832638
train_loss: 0.001707329
test_loss: 0.0016956667
train_loss: 0.0019004164
test_loss: 0.001884011
train_loss: 0.0019191823
test_loss: 0.0017337472
train_loss: 0.0021875557
test_loss: 0.0018368686
train_loss: 0.001607012
test_loss: 0.0014965144
train_loss: 0.0017420237
test_loss: 0.001643097
train_loss: 0.0015962942
test_loss: 0.0016051204
train_loss: 0.0017158777
test_loss: 0.0016638156
train_loss: 0.0017133278
test_loss: 0.0018701303
train_loss: 0.0015294268
test_loss: 0.0016265096
train_loss: 0.0019365051
test_loss: 0.0019723645
train_loss: 0.0014735167
test_loss: 0.0017801924
train_loss: 0.001661818
test_loss: 0.0018804775
train_loss: 0.0017893729
test_loss: 0.0018926882
train_loss: 0.0022239122
test_loss: 0.00276539
train_loss: 0.0017777625
test_loss: 0.0020436423
train_loss: 0.0018671995
test_loss: 0.0017411876
train_loss: 0.0018727311
test_loss: 0.0023039624
train_loss: 0.0015975202
test_loss: 0.0016265396
train_loss: 0.0017204528
test_loss: 0.0016539951
train_loss: 0.0018675303
test_loss: 0.0017380938
train_loss: 0.0020464512
test_loss: 0.0017814159
train_loss: 0.0016354667
test_loss: 0.002187471
train_loss: 0.0016903379
test_loss: 0.0018706882
train_loss: 0.0017102787
test_loss: 0.0019635581
train_loss: 0.0018576812
test_loss: 0.0017340451
train_loss: 0.0021860718
test_loss: 0.0017871457
train_loss: 0.0016882309
test_loss: 0.002057662
train_loss: 0.001910544
test_loss: 0.0017126757
train_loss: 0.0021734238
test_loss: 0.0022661393
train_loss: 0.0018613185
test_loss: 0.001469308
train_loss: 0.0018244763
test_loss: 0.0019322126
train_loss: 0.0018684233
test_loss: 0.0018112997
train_loss: 0.0015720964
test_loss: 0.0020991084
train_loss: 0.0027085152
test_loss: 0.0020855917
train_loss: 0.0020645077
test_loss: 0.0017270498
train_loss: 0.0018154441
test_loss: 0.0019684786
train_loss: 0.0025289182
test_loss: 0.0018616711
train_loss: 0.0016268847
test_loss: 0.0018586232
train_loss: 0.0015936652
test_loss: 0.0018053368
train_loss: 0.0014934121
test_loss: 0.0017700404
train_loss: 0.001697974
test_loss: 0.0016844793
train_loss: 0.0021485682
test_loss: 0.0018582476
train_loss: 0.0016216224
test_loss: 0.0016174023
train_loss: 0.0018238649
test_loss: 0.0015736863
train_loss: 0.0019701447
test_loss: 0.0017744779
train_loss: 0.0024219123
test_loss: 0.0018939424
train_loss: 0.0018208786
test_loss: 0.0017432993
train_loss: 0.0020981915
test_loss: 0.0019480037
train_loss: 0.0027085936
test_loss: 0.0023727308
train_loss: 0.0025244807
test_loss: 0.0023279786
train_loss: 0.0023017174
test_loss: 0.0017921554
train_loss: 0.0024114265
test_loss: 0.0020731685
train_loss: 0.002430375
test_loss: 0.0018623901
train_loss: 0.0018106197
test_loss: 0.0019889248
train_loss: 0.0019912133
test_loss: 0.0020749504
train_loss: 0.0017329974
test_loss: 0.0019846372
train_loss: 0.0016107811
test_loss: 0.0019049514
train_loss: 0.0014826241
test_loss: 0.0015709293
train_loss: 0.0017379834
test_loss: 0.0018965892
train_loss: 0.0018290639
test_loss: 0.0019409033
train_loss: 0.00194962
test_loss: 0.002001499
train_loss: 0.002500593
test_loss: 0.0023193446
train_loss: 0.0017197813
test_loss: 0.0018534146
train_loss: 0.002117981
test_loss: 0.0022975565
train_loss: 0.0025934102
test_loss: 0.002052595
train_loss: 0.0027940506
test_loss: 0.0029271245
train_loss: 0.0025346684
test_loss: 0.0024792214
train_loss: 0.0018172595
test_loss: 0.0023307395
train_loss: 0.0020037454
test_loss: 0.0019513381
train_loss: 0.002106525
test_loss: 0.0017699369
train_loss: 0.0016973988
test_loss: 0.0021089562
train_loss: 0.0017413972
test_loss: 0.0015490842
train_loss: 0.0017168724
test_loss: 0.0016869471
train_loss: 0.0018321109
test_loss: 0.0017532879
train_loss: 0.0021623643
test_loss: 0.0018083603
train_loss: 0.0019365025
test_loss: 0.0018369782
train_loss: 0.0018546644
test_loss: 0.0018570714
train_loss: 0.0018135264
test_loss: 0.0015900926
train_loss: 0.0017714753
test_loss: 0.0015624515
train_loss: 0.001914494
test_loss: 0.002002712
train_loss: 0.0015040447
test_loss: 0.0016621049
train_loss: 0.0019636452
test_loss: 0.0018209413
train_loss: 0.0015429603
test_loss: 0.0018841022
train_loss: 0.0026508183
test_loss: 0.0021175295
train_loss: 0.0017410625
test_loss: 0.002305176
train_loss: 0.0028861552
test_loss: 0.0022152148
train_loss: 0.0027693983
test_loss: 0.0021706913
train_loss: 0.0029352102
test_loss: 0.0027216433
train_loss: 0.003611105
test_loss: 0.0025862448
train_loss: 0.0017310737
test_loss: 0.0017737789
train_loss: 0.0019073912
test_loss: 0.0021736426
train_loss: 0.0020157464
test_loss: 0.002362243
train_loss: 0.0016367832
test_loss: 0.0018733097
train_loss: 0.0017227437
test_loss: 0.0015589782
train_loss: 0.0018836522
test_loss: 0.0017045551
train_loss: 0.0017778057
test_loss: 0.001774836
train_loss: 0.0022185748
test_loss: 0.0018334225
train_loss: 0.0021883142
test_loss: 0.0020590595
train_loss: 0.0018651691
test_loss: 0.0020787634
train_loss: 0.0020250739
test_loss: 0.0018060757
train_loss: 0.0016334387
test_loss: 0.0016850034
train_loss: 0.0016147699
test_loss: 0.0014993678
train_loss: 0.0016205257
test_loss: 0.0014946796
train_loss: 0.0016957906
test_loss: 0.0017612579
train_loss: 0.0017525891
test_loss: 0.0019607556
train_loss: 0.001785465
test_loss: 0.0016947377
train_loss: 0.0017509221
test_loss: 0.0019759273
train_loss: 0.0021607832
test_loss: 0.0021176247
train_loss: 0.0017756894
test_loss: 0.0016350502
train_loss: 0.0016857113
test_loss: 0.0017216946
train_loss: 0.0014207731
test_loss: 0.0018331482
train_loss: 0.0020427294
test_loss: 0.0017829619
train_loss: 0.0017518392
test_loss: 0.0017347971
train_loss: 0.0016223956
test_loss: 0.0017900333
train_loss: 0.0015466331
test_loss: 0.0017715126
train_loss: 0.0018675638
test_loss: 0.001635533
train_loss: 0.0016510604
test_loss: 0.0020275533
train_loss: 0.0016088635
test_loss: 0.0017289016
train_loss: 0.0015231747
test_loss: 0.0017208763
train_loss: 0.0019647225
test_loss: 0.0019646697
train_loss: 0.0015786514
test_loss: 0.0017645964
train_loss: 0.002183028
test_loss: 0.0018631166
train_loss: 0.0017092655
test_loss: 0.0017824715
train_loss: 0.0014429218
test_loss: 0.0015726004
train_loss: 0.0020992286
test_loss: 0.001708858
train_loss: 0.0016273106
test_loss: 0.001624053
train_loss: 0.001955617
test_loss: 0.0016820092
train_loss: 0.0017217908
test_loss: 0.0015048637
train_loss: 0.0017675515
test_loss: 0.0019543848
train_loss: 0.00185612
test_loss: 0.0017634657
train_loss: 0.0028638272
test_loss: 0.0022176418
train_loss: 0.0017767946
test_loss: 0.0023064755
train_loss: 0.0017394053
test_loss: 0.0015553419
train_loss: 0.001701907
test_loss: 0.0018126385
train_loss: 0.0018718566
test_loss: 0.0021469523
train_loss: 0.0018862467
test_loss: 0.00166379
train_loss: 0.001489999
test_loss: 0.0016782562
train_loss: 0.0016275876
test_loss: 0.0017580375
train_loss: 0.001844948
test_loss: 0.0019628438
train_loss: 0.001519577
test_loss: 0.0016839603
train_loss: 0.0014737293
test_loss: 0.002267083
train_loss: 0.0015887595
test_loss: 0.0016213965
train_loss: 0.0017319436
test_loss: 0.0017028361
train_loss: 0.0017247277
test_loss: 0.001792675
train_loss: 0.0018703254
test_loss: 0.0015527445
train_loss: 0.0017139565
test_loss: 0.0017246554
train_loss: 0.0015809513
test_loss: 0.0017036268
train_loss: 0.0020066935
test_loss: 0.0020473043
train_loss: 0.0017117322
test_loss: 0.0021330356
train_loss: 0.0019272959
test_loss: 0.0022420872
train_loss: 0.0013623731
test_loss: 0.0014051222
train_loss: 0.0016685583
test_loss: 0.0015198173
train_loss: 0.0021772876
test_loss: 0.0018942235
train_loss: 0.0031377068
test_loss: 0.0028251342
train_loss: 0.0026709677
test_loss: 0.0019854605
train_loss: 0.0027916525
test_loss: 0.002892467
train_loss: 0.0023405761
test_loss: 0.003047366
train_loss: 0.002177735
test_loss: 0.0027907698
train_loss: 0.002637615
test_loss: 0.002712287
train_loss: 0.0020075298
test_loss: 0.002634999
train_loss: 0.0023294634
test_loss: 0.002881285
train_loss: 0.0023191995
test_loss: 0.0023653882
train_loss: 0.0019987351
test_loss: 0.0018126916
train_loss: 0.0023732954
test_loss: 0.0024632786
train_loss: 0.0016204899
test_loss: 0.001688612
train_loss: 0.0017502904
test_loss: 0.0018484559
train_loss: 0.0016235634
test_loss: 0.0015491717
train_loss: 0.0017878627
test_loss: 0.001663983
train_loss: 0.00169758
test_loss: 0.0017022198
train_loss: 0.0017884827
test_loss: 0.0017391811
train_loss: 0.0017743555
test_loss: 0.0015965754
train_loss: 0.0018354765
test_loss: 0.0017851043
train_loss: 0.002148442
test_loss: 0.001698972
train_loss: 0.001999158
test_loss: 0.002074135
train_loss: 0.0019667204
test_loss: 0.0019472655
train_loss: 0.001956274
test_loss: 0.0016521064
train_loss: 0.0014834153
test_loss: 0.0018064023
train_loss: 0.0015268459
test_loss: 0.0014838915
train_loss: 0.001738725
test_loss: 0.0026556072
train_loss: 0.0017212965
test_loss: 0.0018400676
train_loss: 0.0018631292
test_loss: 0.0018144646
train_loss: 0.0019768833
test_loss: 0.0019262837
train_loss: 0.0015955898
test_loss: 0.0018905057
train_loss: 0.0016087403
test_loss: 0.0015949092
train_loss: /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
0.0015741332
test_loss: 0.0016470853
train_loss: 0.0016137834
test_loss: 0.0018079311
train_loss: 0.0016008045
test_loss: 0.0017642388
train_loss: 0.002005857
test_loss: 0.001897959
train_loss: 0.0018167028
test_loss: 0.0018390078
train_loss: 0.0019042133
test_loss: 0.0018972537
train_loss: 0.0018687176
test_loss: 0.0015835336
train_loss: 0.0018672426
test_loss: 0.0016896087
train_loss: 0.0018301863
test_loss: 0.0017111468
train_loss: 0.0018158869
test_loss: 0.001626145
train_loss: 0.0016378032
test_loss: 0.0018910258
train_loss: 0.0018257918
test_loss: 0.0016683106
train_loss: 0.0016175548
test_loss: 0.0017814501
train_loss: 0.0017993489
test_loss: 0.0017523484
train_loss: 0.0020549488
test_loss: 0.002014261
train_loss: 0.0018215475
test_loss: 0.001655971
train_loss: 0.0016882835
test_loss: 0.0017973293
train_loss: 0.0017168088
test_loss: 0.001966328
train_loss: 0.0016693869
test_loss: 0.0019356937
train_loss: 0.0021336973
test_loss: 0.0018412077
train_loss: 0.0020277873
test_loss: 0.002187586
train_loss: 0.001761516
test_loss: 0.002515749
train_loss: 0.0017671718
test_loss: 0.0026279674
train_loss: 0.0018088177
test_loss: 0.002215396
train_loss: 0.0021104354
test_loss: 0.0016938657
train_loss: 0.0017815321
test_loss: 0.0019093675
train_loss: 0.0016571647
test_loss: 0.0020165076
train_loss: 0.001653333
test_loss: 0.0018163321
train_loss: 0.0015997926
test_loss: 0.0021033143
train_loss: 0.0021486874
test_loss: 0.0029978985
train_loss: 0.0026272354
test_loss: 0.0030646892
train_loss: 0.0024222122
test_loss: 0.0034329917
train_loss: 0.0020011712
test_loss: 0.0031612148
train_loss: 0.0019238798
test_loss: 0.0026493184
train_loss: 0.002116426
test_loss: 0.002097836
train_loss: 0.0019459913
test_loss: 0.0017711439
train_loss: 0.00254349
test_loss: 0.0022572053
train_loss: 0.0020644884
test_loss: 0.0019221009
train_loss: 0.0025746373
test_loss: 0.001998105
train_loss: 0.001856821
test_loss: 0.0017595673
train_loss: 0.0018443628
test_loss: 0.0018336832
train_loss: 0.002048396
test_loss: 0.0018372447
train_loss: 0.0019575856
test_loss: 0.001981533
train_loss: 0.0020365869
test_loss: 0.0019873448
train_loss: 0.0025353513
test_loss: 0.0020597542
train_loss: 0.0019245693
test_loss: 0.0020201067
train_loss: 0.0014407695
test_loss: 0.0016686885
train_loss: 0.0018185078
test_loss: 0.0017730504
train_loss: 0.0017010395
test_loss: 0.0015748944
train_loss: 0.0017314507
test_loss: 0.001890331
train_loss: 0.002153912
test_loss: 0.0024427532
train_loss: 0.0019021807
test_loss: 0.0019138383
train_loss: 0.0014970431
test_loss: 0.0017182248
train_loss: 0.0018225813
test_loss: 0.00189933
train_loss: 0.0019628624
test_loss: 0.0017643005
train_loss: 0.0015341156
test_loss: 0.0019267056
train_loss: 0.0017277325
test_loss: 0.0019529461
train_loss: 0.0016514503
test_loss: 0.0015035224
train_loss: 0.0019461398
test_loss: 0.0018591874
train_loss: 0.0018973649
test_loss: 0.0020683943
train_loss: 0.0019163023
test_loss: 0.0015762426
train_loss: 0.0014993073
test_loss: 0.0015043268
train_loss: 0.0016085678
test_loss: 0.0014682008
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi3/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b8db7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b9319d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b974b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b9e7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b8a2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b8a26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b857ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b819950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b819a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b819268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b8061e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea468f9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b7d1048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b7d1e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b764bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b7330d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b744620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b72d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9fda3f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea2b714378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9fda06a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9fd9f8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9fda068c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9fd9d77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9fd992488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9fd989a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9fd9897b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9fd956268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9fd94f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d820f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d81cb158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d8182f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d812a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d8175488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d8154ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d8115ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.31876709e-06
Iter: 2 loss: 2.99608928e-06
Iter: 3 loss: 2.52555219e-06
Iter: 4 loss: 2.04481057e-06
Iter: 5 loss: 3.98846851e-06
Iter: 6 loss: 1.93850974e-06
Iter: 7 loss: 1.79141796e-06
Iter: 8 loss: 1.52398729e-06
Iter: 9 loss: 7.88091529e-06
Iter: 10 loss: 1.52384871e-06
Iter: 11 loss: 1.3328372e-06
Iter: 12 loss: 1.32623268e-06
Iter: 13 loss: 1.28403371e-06
Iter: 14 loss: 1.26255031e-06
Iter: 15 loss: 1.24282315e-06
Iter: 16 loss: 1.17805075e-06
Iter: 17 loss: 1.33855519e-06
Iter: 18 loss: 1.15514149e-06
Iter: 19 loss: 1.11061445e-06
Iter: 20 loss: 1.48284312e-06
Iter: 21 loss: 1.10797669e-06
Iter: 22 loss: 1.06510777e-06
Iter: 23 loss: 1.07152346e-06
Iter: 24 loss: 1.03268667e-06
Iter: 25 loss: 1.01106752e-06
Iter: 26 loss: 1.00934085e-06
Iter: 27 loss: 9.99441681e-07
Iter: 28 loss: 9.83366135e-07
Iter: 29 loss: 9.83285531e-07
Iter: 30 loss: 9.64017204e-07
Iter: 31 loss: 1.14431418e-06
Iter: 32 loss: 9.63164666e-07
Iter: 33 loss: 9.51480501e-07
Iter: 34 loss: 9.47129422e-07
Iter: 35 loss: 9.40710379e-07
Iter: 36 loss: 9.30550868e-07
Iter: 37 loss: 9.29401608e-07
Iter: 38 loss: 9.18379783e-07
Iter: 39 loss: 9.72927069e-07
Iter: 40 loss: 9.16502643e-07
Iter: 41 loss: 9.126598e-07
Iter: 42 loss: 9.02760121e-07
Iter: 43 loss: 9.80399136e-07
Iter: 44 loss: 9.00766565e-07
Iter: 45 loss: 8.90898605e-07
Iter: 46 loss: 8.90911451e-07
Iter: 47 loss: 8.85861141e-07
Iter: 48 loss: 8.92821845e-07
Iter: 49 loss: 8.83340306e-07
Iter: 50 loss: 8.76531658e-07
Iter: 51 loss: 8.62690115e-07
Iter: 52 loss: 1.11159602e-06
Iter: 53 loss: 8.62448815e-07
Iter: 54 loss: 8.59730903e-07
Iter: 55 loss: 8.57186365e-07
Iter: 56 loss: 8.53116035e-07
Iter: 57 loss: 8.54099596e-07
Iter: 58 loss: 8.50098388e-07
Iter: 59 loss: 8.44991177e-07
Iter: 60 loss: 8.83774476e-07
Iter: 61 loss: 8.44576448e-07
Iter: 62 loss: 8.41983251e-07
Iter: 63 loss: 8.40378107e-07
Iter: 64 loss: 8.39330824e-07
Iter: 65 loss: 8.34624416e-07
Iter: 66 loss: 8.60240846e-07
Iter: 67 loss: 8.33917056e-07
Iter: 68 loss: 8.30601266e-07
Iter: 69 loss: 8.3019313e-07
Iter: 70 loss: 8.27894269e-07
Iter: 71 loss: 8.23594632e-07
Iter: 72 loss: 8.72781e-07
Iter: 73 loss: 8.23454968e-07
Iter: 74 loss: 8.21729088e-07
Iter: 75 loss: 8.21397123e-07
Iter: 76 loss: 8.20596711e-07
Iter: 77 loss: 8.17782734e-07
Iter: 78 loss: 8.13507427e-07
Iter: 79 loss: 8.13046711e-07
Iter: 80 loss: 8.12052576e-07
Iter: 81 loss: 8.10232223e-07
Iter: 82 loss: 8.07839456e-07
Iter: 83 loss: 8.08056825e-07
Iter: 84 loss: 8.05982154e-07
Iter: 85 loss: 8.02241516e-07
Iter: 86 loss: 8.06283708e-07
Iter: 87 loss: 8.00139105e-07
Iter: 88 loss: 7.9790118e-07
Iter: 89 loss: 8.01218448e-07
Iter: 90 loss: 7.96867425e-07
Iter: 91 loss: 7.93886215e-07
Iter: 92 loss: 8.0658981e-07
Iter: 93 loss: 7.93201593e-07
Iter: 94 loss: 7.91190189e-07
Iter: 95 loss: 8.06608114e-07
Iter: 96 loss: 7.9100505e-07
Iter: 97 loss: 7.89091473e-07
Iter: 98 loss: 7.87037266e-07
Iter: 99 loss: 7.86656074e-07
Iter: 100 loss: 7.84075326e-07
Iter: 101 loss: 8.12654946e-07
Iter: 102 loss: 7.84029794e-07
Iter: 103 loss: 7.82413736e-07
Iter: 104 loss: 7.82622863e-07
Iter: 105 loss: 7.81270217e-07
Iter: 106 loss: 7.7909624e-07
Iter: 107 loss: 7.9732331e-07
Iter: 108 loss: 7.79032916e-07
Iter: 109 loss: 7.78737331e-07
Iter: 110 loss: 7.78302251e-07
Iter: 111 loss: 7.77953233e-07
Iter: 112 loss: 7.76787374e-07
Iter: 113 loss: 7.76499746e-07
Iter: 114 loss: 7.75525e-07
Iter: 115 loss: 7.73905697e-07
Iter: 116 loss: 7.97354801e-07
Iter: 117 loss: 7.73867896e-07
Iter: 118 loss: 7.72797648e-07
Iter: 119 loss: 7.72376e-07
Iter: 120 loss: 7.71808686e-07
Iter: 121 loss: 7.70027384e-07
Iter: 122 loss: 7.88269404e-07
Iter: 123 loss: 7.69880046e-07
Iter: 124 loss: 7.68978452e-07
Iter: 125 loss: 7.68186283e-07
Iter: 126 loss: 7.67921392e-07
Iter: 127 loss: 7.66111043e-07
Iter: 128 loss: 7.67653e-07
Iter: 129 loss: 7.64985145e-07
Iter: 130 loss: 7.63625849e-07
Iter: 131 loss: 7.80704852e-07
Iter: 132 loss: 7.63559e-07
Iter: 133 loss: 7.62132913e-07
Iter: 134 loss: 7.61667934e-07
Iter: 135 loss: 7.60746047e-07
Iter: 136 loss: 7.59595878e-07
Iter: 137 loss: 7.72679414e-07
Iter: 138 loss: 7.59594e-07
Iter: 139 loss: 7.5855769e-07
Iter: 140 loss: 7.56946406e-07
Iter: 141 loss: 7.56937425e-07
Iter: 142 loss: 7.59251463e-07
Iter: 143 loss: 7.56449e-07
Iter: 144 loss: 7.56007353e-07
Iter: 145 loss: 7.55333531e-07
Iter: 146 loss: 7.55372e-07
Iter: 147 loss: 7.54504072e-07
Iter: 148 loss: 7.53246695e-07
Iter: 149 loss: 7.53194058e-07
Iter: 150 loss: 7.52146377e-07
Iter: 151 loss: 7.60864282e-07
Iter: 152 loss: 7.5210005e-07
Iter: 153 loss: 7.5129e-07
Iter: 154 loss: 7.51399853e-07
Iter: 155 loss: 7.5066481e-07
Iter: 156 loss: 7.50021513e-07
Iter: 157 loss: 7.59029035e-07
Iter: 158 loss: 7.50000481e-07
Iter: 159 loss: 7.49437049e-07
Iter: 160 loss: 7.48744924e-07
Iter: 161 loss: 7.48680577e-07
Iter: 162 loss: 7.47823606e-07
Iter: 163 loss: 7.47823947e-07
Iter: 164 loss: 7.47298202e-07
Iter: 165 loss: 7.46379101e-07
Iter: 166 loss: 7.68452196e-07
Iter: 167 loss: 7.46413e-07
Iter: 168 loss: 7.45061129e-07
Iter: 169 loss: 7.4832883e-07
Iter: 170 loss: 7.44554029e-07
Iter: 171 loss: 7.43651242e-07
Iter: 172 loss: 7.49856554e-07
Iter: 173 loss: 7.43538635e-07
Iter: 174 loss: 7.42610496e-07
Iter: 175 loss: 7.4469807e-07
Iter: 176 loss: 7.42241696e-07
Iter: 177 loss: 7.41633926e-07
Iter: 178 loss: 7.47011939e-07
Iter: 179 loss: 7.41633471e-07
Iter: 180 loss: 7.41250801e-07
Iter: 181 loss: 7.4121192e-07
Iter: 182 loss: 7.41000179e-07
Iter: 183 loss: 7.40176802e-07
Iter: 184 loss: 7.41300823e-07
Iter: 185 loss: 7.39582561e-07
Iter: 186 loss: 7.38881681e-07
Iter: 187 loss: 7.41605731e-07
Iter: 188 loss: 7.38647e-07
Iter: 189 loss: 7.37953826e-07
Iter: 190 loss: 7.43695637e-07
Iter: 191 loss: 7.37895903e-07
Iter: 192 loss: 7.37412904e-07
Iter: 193 loss: 7.37014147e-07
Iter: 194 loss: 7.36886e-07
Iter: 195 loss: 7.35931167e-07
Iter: 196 loss: 7.43787723e-07
Iter: 197 loss: 7.3589797e-07
Iter: 198 loss: 7.35370463e-07
Iter: 199 loss: 7.37937285e-07
Iter: 200 loss: 7.35262461e-07
Iter: 201 loss: 7.34648609e-07
Iter: 202 loss: 7.34934702e-07
Iter: 203 loss: 7.34256446e-07
Iter: 204 loss: 7.33382137e-07
Iter: 205 loss: 7.36491529e-07
Iter: 206 loss: 7.33240313e-07
Iter: 207 loss: 7.32626404e-07
Iter: 208 loss: 7.3187e-07
Iter: 209 loss: 7.31844807e-07
Iter: 210 loss: 7.30890179e-07
Iter: 211 loss: 7.4370621e-07
Iter: 212 loss: 7.30888758e-07
Iter: 213 loss: 7.30364036e-07
Iter: 214 loss: 7.3433597e-07
Iter: 215 loss: 7.30363581e-07
Iter: 216 loss: 7.29941121e-07
Iter: 217 loss: 7.33464276e-07
Iter: 218 loss: 7.29916451e-07
Iter: 219 loss: 7.29514795e-07
Iter: 220 loss: 7.28758152e-07
Iter: 221 loss: 7.45155035e-07
Iter: 222 loss: 7.28752696e-07
Iter: 223 loss: 7.2829107e-07
Iter: 224 loss: 7.27495205e-07
Iter: 225 loss: 7.2749674e-07
Iter: 226 loss: 7.26852249e-07
Iter: 227 loss: 7.26802966e-07
Iter: 228 loss: 7.26408416e-07
Iter: 229 loss: 7.25744144e-07
Iter: 230 loss: 7.25750624e-07
Iter: 231 loss: 7.25280643e-07
Iter: 232 loss: 7.25184464e-07
Iter: 233 loss: 7.24855909e-07
Iter: 234 loss: 7.26228677e-07
Iter: 235 loss: 7.24807e-07
Iter: 236 loss: 7.24481538e-07
Iter: 237 loss: 7.23958124e-07
Iter: 238 loss: 7.2398808e-07
Iter: 239 loss: 7.23493883e-07
Iter: 240 loss: 7.29290662e-07
Iter: 241 loss: 7.23517132e-07
Iter: 242 loss: 7.23198639e-07
Iter: 243 loss: 7.22589675e-07
Iter: 244 loss: 7.35726189e-07
Iter: 245 loss: 7.22564e-07
Iter: 246 loss: 7.22039601e-07
Iter: 247 loss: 7.22032269e-07
Iter: 248 loss: 7.21893457e-07
Iter: 249 loss: 7.21879246e-07
Iter: 250 loss: 7.21651077e-07
Iter: 251 loss: 7.21463323e-07
Iter: 252 loss: 7.21426659e-07
Iter: 253 loss: 7.21043364e-07
Iter: 254 loss: 7.21217646e-07
Iter: 255 loss: 7.20780577e-07
Iter: 256 loss: 7.20546836e-07
Iter: 257 loss: 7.20217486e-07
Iter: 258 loss: 7.20166895e-07
Iter: 259 loss: 7.19705213e-07
Iter: 260 loss: 7.23341486e-07
Iter: 261 loss: 7.1960244e-07
Iter: 262 loss: 7.19271839e-07
Iter: 263 loss: 7.19139905e-07
Iter: 264 loss: 7.18950389e-07
Iter: 265 loss: 7.18320848e-07
Iter: 266 loss: 7.21669721e-07
Iter: 267 loss: 7.18209378e-07
Iter: 268 loss: 7.17924138e-07
Iter: 269 loss: 7.21842071e-07
Iter: 270 loss: 7.17912428e-07
Iter: 271 loss: 7.17528508e-07
Iter: 272 loss: 7.17040678e-07
Iter: 273 loss: 7.17009186e-07
Iter: 274 loss: 7.16614522e-07
Iter: 275 loss: 7.21170068e-07
Iter: 276 loss: 7.16585816e-07
Iter: 277 loss: 7.16267095e-07
Iter: 278 loss: 7.15938427e-07
Iter: 279 loss: 7.1586976e-07
Iter: 280 loss: 7.1558e-07
Iter: 281 loss: 7.15501642e-07
Iter: 282 loss: 7.15291776e-07
Iter: 283 loss: 7.17159196e-07
Iter: 284 loss: 7.15300644e-07
Iter: 285 loss: 7.15157e-07
Iter: 286 loss: 7.14893758e-07
Iter: 287 loss: 7.19986417e-07
Iter: 288 loss: 7.1486528e-07
Iter: 289 loss: 7.14406724e-07
Iter: 290 loss: 7.14525413e-07
Iter: 291 loss: 7.14080613e-07
Iter: 292 loss: 7.13714826e-07
Iter: 293 loss: 7.15141653e-07
Iter: 294 loss: 7.1361751e-07
Iter: 295 loss: 7.13151735e-07
Iter: 296 loss: 7.13063855e-07
Iter: 297 loss: 7.12749454e-07
Iter: 298 loss: 7.12265e-07
Iter: 299 loss: 7.15586737e-07
Iter: 300 loss: 7.12265432e-07
Iter: 301 loss: 7.11756456e-07
Iter: 302 loss: 7.11947905e-07
Iter: 303 loss: 7.11443477e-07
Iter: 304 loss: 7.11290681e-07
Iter: 305 loss: 7.11208486e-07
Iter: 306 loss: 7.10985773e-07
Iter: 307 loss: 7.10556208e-07
Iter: 308 loss: 7.18406795e-07
Iter: 309 loss: 7.10552627e-07
Iter: 310 loss: 7.10180075e-07
Iter: 311 loss: 7.13466932e-07
Iter: 312 loss: 7.10176892e-07
Iter: 313 loss: 7.09954406e-07
Iter: 314 loss: 7.11030566e-07
Iter: 315 loss: 7.09890514e-07
Iter: 316 loss: 7.096736e-07
Iter: 317 loss: 7.11696714e-07
Iter: 318 loss: 7.09690767e-07
Iter: 319 loss: 7.09515916e-07
Iter: 320 loss: 7.09307585e-07
Iter: 321 loss: 7.09288e-07
Iter: 322 loss: 7.09119661e-07
Iter: 323 loss: 7.09528365e-07
Iter: 324 loss: 7.09089534e-07
Iter: 325 loss: 7.0881174e-07
Iter: 326 loss: 7.08494895e-07
Iter: 327 loss: 7.08456241e-07
Iter: 328 loss: 7.0817083e-07
Iter: 329 loss: 7.10910399e-07
Iter: 330 loss: 7.08184928e-07
Iter: 331 loss: 7.07853587e-07
Iter: 332 loss: 7.07408731e-07
Iter: 333 loss: 7.07415666e-07
Iter: 334 loss: 7.0694233e-07
Iter: 335 loss: 7.1055382e-07
Iter: 336 loss: 7.06920048e-07
Iter: 337 loss: 7.06515152e-07
Iter: 338 loss: 7.07844151e-07
Iter: 339 loss: 7.06351216e-07
Iter: 340 loss: 7.06096898e-07
Iter: 341 loss: 7.06107244e-07
Iter: 342 loss: 7.05951265e-07
Iter: 343 loss: 7.05631521e-07
Iter: 344 loss: 7.10087306e-07
Iter: 345 loss: 7.05566492e-07
Iter: 346 loss: 7.05157845e-07
Iter: 347 loss: 7.09173662e-07
Iter: 348 loss: 7.05134e-07
Iter: 349 loss: 7.04997433e-07
Iter: 350 loss: 7.04964805e-07
Iter: 351 loss: 7.04742e-07
Iter: 352 loss: 7.04606578e-07
Iter: 353 loss: 7.0450335e-07
Iter: 354 loss: 7.043202e-07
Iter: 355 loss: 7.0470594e-07
Iter: 356 loss: 7.04232207e-07
Iter: 357 loss: 7.04077138e-07
Iter: 358 loss: 7.04376362e-07
Iter: 359 loss: 7.03976866e-07
Iter: 360 loss: 7.03747e-07
Iter: 361 loss: 7.04551212e-07
Iter: 362 loss: 7.03706746e-07
Iter: 363 loss: 7.03456e-07
Iter: 364 loss: 7.03170031e-07
Iter: 365 loss: 7.0319976e-07
Iter: 366 loss: 7.02869727e-07
Iter: 367 loss: 7.02881039e-07
Iter: 368 loss: 7.02603302e-07
Iter: 369 loss: 7.0237e-07
Iter: 370 loss: 7.02322552e-07
Iter: 371 loss: 7.0194028e-07
Iter: 372 loss: 7.0652186e-07
Iter: 373 loss: 7.01962449e-07
Iter: 374 loss: 7.01672093e-07
Iter: 375 loss: 7.02692205e-07
Iter: 376 loss: 7.01634519e-07
Iter: 377 loss: 7.01359568e-07
Iter: 378 loss: 7.01160047e-07
Iter: 379 loss: 7.01051533e-07
Iter: 380 loss: 7.00838257e-07
Iter: 381 loss: 7.02208638e-07
Iter: 382 loss: 7.00824671e-07
Iter: 383 loss: 7.00700525e-07
Iter: 384 loss: 7.00633e-07
Iter: 385 loss: 7.00595251e-07
Iter: 386 loss: 7.00422277e-07
Iter: 387 loss: 7.03485512e-07
Iter: 388 loss: 7.00362193e-07
Iter: 389 loss: 7.00239184e-07
Iter: 390 loss: 7.00134592e-07
Iter: 391 loss: 7.00116232e-07
Iter: 392 loss: 6.99799e-07
Iter: 393 loss: 7.01532372e-07
Iter: 394 loss: 6.99764314e-07
Iter: 395 loss: 6.99600264e-07
Iter: 396 loss: 7.00057853e-07
Iter: 397 loss: 6.99534e-07
Iter: 398 loss: 6.99400175e-07
Iter: 399 loss: 6.99133e-07
Iter: 400 loss: 6.9908657e-07
Iter: 401 loss: 6.98795589e-07
Iter: 402 loss: 7.00787837e-07
Iter: 403 loss: 6.98793315e-07
Iter: 404 loss: 6.98503e-07
Iter: 405 loss: 6.98277063e-07
Iter: 406 loss: 6.98199528e-07
Iter: 407 loss: 6.97943165e-07
Iter: 408 loss: 6.97941118e-07
Iter: 409 loss: 6.97715052e-07
Iter: 410 loss: 6.9827729e-07
Iter: 411 loss: 6.97612506e-07
Iter: 412 loss: 6.97456e-07
Iter: 413 loss: 6.98286613e-07
Iter: 414 loss: 6.97412872e-07
Iter: 415 loss: 6.97270934e-07
Iter: 416 loss: 6.9746136e-07
Iter: 417 loss: 6.97198629e-07
Iter: 418 loss: 6.96970119e-07
Iter: 419 loss: 6.98612553e-07
Iter: 420 loss: 6.9695534e-07
Iter: 421 loss: 6.96817665e-07
Iter: 422 loss: 6.96613597e-07
Iter: 423 loss: 6.96644463e-07
Iter: 424 loss: 6.96480811e-07
Iter: 425 loss: 6.96305733e-07
Iter: 426 loss: 6.9626617e-07
Iter: 427 loss: 6.95938354e-07
Iter: 428 loss: 6.97930432e-07
Iter: 429 loss: 6.95862695e-07
Iter: 430 loss: 6.95680114e-07
Iter: 431 loss: 6.97105065e-07
Iter: 432 loss: 6.95677954e-07
Iter: 433 loss: 6.95449842e-07
Iter: 434 loss: 6.95094172e-07
Iter: 435 loss: 7.045914e-07
Iter: 436 loss: 6.95079109e-07
Iter: 437 loss: 6.94737821e-07
Iter: 438 loss: 6.98154508e-07
Iter: 439 loss: 6.94756864e-07
Iter: 440 loss: 6.94466621e-07
Iter: 441 loss: 6.94103392e-07
Iter: 442 loss: 6.94042342e-07
Iter: 443 loss: 6.93786433e-07
Iter: 444 loss: 6.93803599e-07
Iter: 445 loss: 6.93578784e-07
Iter: 446 loss: 6.93837762e-07
Iter: 447 loss: 6.93458901e-07
Iter: 448 loss: 6.93118864e-07
Iter: 449 loss: 6.9361829e-07
Iter: 450 loss: 6.92975959e-07
Iter: 451 loss: 6.92695437e-07
Iter: 452 loss: 6.94677794e-07
Iter: 453 loss: 6.92685262e-07
Iter: 454 loss: 6.92504443e-07
Iter: 455 loss: 6.92498247e-07
Iter: 456 loss: 6.92374158e-07
Iter: 457 loss: 6.92103868e-07
Iter: 458 loss: 6.94993105e-07
Iter: 459 loss: 6.92052652e-07
Iter: 460 loss: 6.91846651e-07
Iter: 461 loss: 6.92298e-07
Iter: 462 loss: 6.91746834e-07
Iter: 463 loss: 6.91438856e-07
Iter: 464 loss: 6.91473417e-07
Iter: 465 loss: 6.91249738e-07
Iter: 466 loss: 6.90933859e-07
Iter: 467 loss: 6.93563663e-07
Iter: 468 loss: 6.90961372e-07
Iter: 469 loss: 6.90649699e-07
Iter: 470 loss: 6.90513389e-07
Iter: 471 loss: 6.90394643e-07
Iter: 472 loss: 6.89951719e-07
Iter: 473 loss: 6.93292804e-07
Iter: 474 loss: 6.89974456e-07
Iter: 475 loss: 6.89726392e-07
Iter: 476 loss: 6.89342528e-07
Iter: 477 loss: 6.89325e-07
Iter: 478 loss: 6.88928537e-07
Iter: 479 loss: 6.93974869e-07
Iter: 480 loss: 6.88906653e-07
Iter: 481 loss: 6.88746582e-07
Iter: 482 loss: 6.90467118e-07
Iter: 483 loss: 6.88740442e-07
Iter: 484 loss: 6.88543139e-07
Iter: 485 loss: 6.88341288e-07
Iter: 486 loss: 6.88330431e-07
Iter: 487 loss: 6.88162174e-07
Iter: 488 loss: 6.9069813e-07
Iter: 489 loss: 6.88178943e-07
Iter: 490 loss: 6.88046043e-07
Iter: 491 loss: 6.89939384e-07
Iter: 492 loss: 6.88045645e-07
Iter: 493 loss: 6.87961801e-07
Iter: 494 loss: 6.87742954e-07
Iter: 495 loss: 6.8856383e-07
Iter: 496 loss: 6.8757447e-07
Iter: 497 loss: 6.87315946e-07
Iter: 498 loss: 6.89404033e-07
Iter: 499 loss: 6.873031e-07
Iter: 500 loss: 6.8709096e-07
Iter: 501 loss: 6.8734e-07
Iter: 502 loss: 6.87013767e-07
Iter: 503 loss: 6.86778094e-07
Iter: 504 loss: 6.88410125e-07
Iter: 505 loss: 6.86747228e-07
Iter: 506 loss: 6.86595342e-07
Iter: 507 loss: 6.86805038e-07
Iter: 508 loss: 6.86483929e-07
Iter: 509 loss: 6.8624513e-07
Iter: 510 loss: 6.86367741e-07
Iter: 511 loss: 6.86066812e-07
Iter: 512 loss: 6.85869225e-07
Iter: 513 loss: 6.86497742e-07
Iter: 514 loss: 6.85764689e-07
Iter: 515 loss: 6.85601321e-07
Iter: 516 loss: 6.8645295e-07
Iter: 517 loss: 6.85549935e-07
Iter: 518 loss: 6.85427892e-07
Iter: 519 loss: 6.86986198e-07
Iter: 520 loss: 6.85399755e-07
Iter: 521 loss: 6.85330065e-07
Iter: 522 loss: 6.85102805e-07
Iter: 523 loss: 6.88870045e-07
Iter: 524 loss: 6.85050566e-07
Iter: 525 loss: 6.84921e-07
Iter: 526 loss: 6.84929489e-07
Iter: 527 loss: 6.84809e-07
Iter: 528 loss: 6.85405155e-07
Iter: 529 loss: 6.84786755e-07
Iter: 530 loss: 6.84699614e-07
Iter: 531 loss: 6.84486395e-07
Iter: 532 loss: 6.85147256e-07
Iter: 533 loss: 6.84420911e-07
Iter: 534 loss: 6.84205872e-07
Iter: 535 loss: 6.84200359e-07
Iter: 536 loss: 6.84022325e-07
Iter: 537 loss: 6.84087411e-07
Iter: 538 loss: 6.83945586e-07
Iter: 539 loss: 6.83690587e-07
Iter: 540 loss: 6.85376733e-07
Iter: 541 loss: 6.83688e-07
Iter: 542 loss: 6.83575422e-07
Iter: 543 loss: 6.83588041e-07
Iter: 544 loss: 6.83466055e-07
Iter: 545 loss: 6.83182634e-07
Iter: 546 loss: 6.83232088e-07
Iter: 547 loss: 6.82993459e-07
Iter: 548 loss: 6.82766768e-07
Iter: 549 loss: 6.84381916e-07
Iter: 550 loss: 6.82777227e-07
Iter: 551 loss: 6.8251029e-07
Iter: 552 loss: 6.8280832e-07
Iter: 553 loss: 6.82429061e-07
Iter: 554 loss: 6.82216751e-07
Iter: 555 loss: 6.82198618e-07
Iter: 556 loss: 6.82145583e-07
Iter: 557 loss: 6.82219536e-07
Iter: 558 loss: 6.82080952e-07
Iter: 559 loss: 6.81904396e-07
Iter: 560 loss: 6.82751761e-07
Iter: 561 loss: 6.81842664e-07
Iter: 562 loss: 6.81729659e-07
Iter: 563 loss: 6.81479264e-07
Iter: 564 loss: 6.81461643e-07
Iter: 565 loss: 6.81345114e-07
Iter: 566 loss: 6.81403e-07
Iter: 567 loss: 6.81106826e-07
Iter: 568 loss: 6.80810672e-07
Iter: 569 loss: 6.81202721e-07
Iter: 570 loss: 6.80651453e-07
Iter: 571 loss: 6.80279129e-07
Iter: 572 loss: 6.80692153e-07
Iter: 573 loss: 6.80118262e-07
Iter: 574 loss: 6.79688583e-07
Iter: 575 loss: 6.83819394e-07
Iter: 576 loss: 6.79653226e-07
Iter: 577 loss: 6.79468712e-07
Iter: 578 loss: 6.804745e-07
Iter: 579 loss: 6.7939493e-07
Iter: 580 loss: 6.79107586e-07
Iter: 581 loss: 6.78745323e-07
Iter: 582 loss: 6.78712638e-07
Iter: 583 loss: 6.78429444e-07
Iter: 584 loss: 6.81441065e-07
Iter: 585 loss: 6.78484298e-07
Iter: 586 loss: 6.78164724e-07
Iter: 587 loss: 6.78976846e-07
Iter: 588 loss: 6.78147558e-07
Iter: 589 loss: 6.77928369e-07
Iter: 590 loss: 6.80978644e-07
Iter: 591 loss: 6.77930188e-07
Iter: 592 loss: 6.77863341e-07
Iter: 593 loss: 6.79383049e-07
Iter: 594 loss: 6.77837647e-07
Iter: 595 loss: 6.77769606e-07
Iter: 596 loss: 6.77564913e-07
Iter: 597 loss: 6.79342065e-07
Iter: 598 loss: 6.7753308e-07
Iter: 599 loss: 6.7727467e-07
Iter: 600 loss: 6.77813773e-07
Iter: 601 loss: 6.77161268e-07
Iter: 602 loss: 6.76915761e-07
Iter: 603 loss: 6.77066737e-07
Iter: 604 loss: 6.76761942e-07
Iter: 605 loss: 6.76362788e-07
Iter: 606 loss: 6.76327204e-07
Iter: 607 loss: 6.75974377e-07
Iter: 608 loss: 6.7562604e-07
Iter: 609 loss: 6.75650199e-07
Iter: 610 loss: 6.7533432e-07
Iter: 611 loss: 6.7533631e-07
Iter: 612 loss: 6.75105298e-07
Iter: 613 loss: 6.74626222e-07
Iter: 614 loss: 6.76685033e-07
Iter: 615 loss: 6.74519185e-07
Iter: 616 loss: 6.74238947e-07
Iter: 617 loss: 6.7419063e-07
Iter: 618 loss: 6.74032435e-07
Iter: 619 loss: 6.73635498e-07
Iter: 620 loss: 6.76450441e-07
Iter: 621 loss: 6.73612476e-07
Iter: 622 loss: 6.73355828e-07
Iter: 623 loss: 6.75005595e-07
Iter: 624 loss: 6.73355771e-07
Iter: 625 loss: 6.73186491e-07
Iter: 626 loss: 6.73857187e-07
Iter: 627 loss: 6.73104e-07
Iter: 628 loss: 6.72898636e-07
Iter: 629 loss: 6.74075295e-07
Iter: 630 loss: 6.72885e-07
Iter: 631 loss: 6.72759313e-07
Iter: 632 loss: 6.7266177e-07
Iter: 633 loss: 6.72637839e-07
Iter: 634 loss: 6.72446959e-07
Iter: 635 loss: 6.72102374e-07
Iter: 636 loss: 6.80802714e-07
Iter: 637 loss: 6.72126362e-07
Iter: 638 loss: 6.71784676e-07
Iter: 639 loss: 6.71826911e-07
Iter: 640 loss: 6.71578505e-07
Iter: 641 loss: 6.71250291e-07
Iter: 642 loss: 6.71207431e-07
Iter: 643 loss: 6.70942825e-07
Iter: 644 loss: 6.70917245e-07
Iter: 645 loss: 6.70752911e-07
Iter: 646 loss: 6.70865802e-07
Iter: 647 loss: 6.70605459e-07
Iter: 648 loss: 6.70325107e-07
Iter: 649 loss: 6.70642521e-07
Iter: 650 loss: 6.70151451e-07
Iter: 651 loss: 6.69888891e-07
Iter: 652 loss: 6.71162866e-07
Iter: 653 loss: 6.69836766e-07
Iter: 654 loss: 6.69563121e-07
Iter: 655 loss: 6.69845747e-07
Iter: 656 loss: 6.69416409e-07
Iter: 657 loss: 6.69266569e-07
Iter: 658 loss: 6.69241331e-07
Iter: 659 loss: 6.69151632e-07
Iter: 660 loss: 6.69923054e-07
Iter: 661 loss: 6.69144924e-07
Iter: 662 loss: 6.69040446e-07
Iter: 663 loss: 6.68769303e-07
Iter: 664 loss: 6.69913049e-07
Iter: 665 loss: 6.68686198e-07
Iter: 666 loss: 6.68351163e-07
Iter: 667 loss: 6.70887744e-07
Iter: 668 loss: 6.68305e-07
Iter: 669 loss: 6.68141126e-07
Iter: 670 loss: 6.6793416e-07
Iter: 671 loss: 6.67877657e-07
Iter: 672 loss: 6.67497147e-07
Iter: 673 loss: 6.71539055e-07
Iter: 674 loss: 6.67532163e-07
Iter: 675 loss: 6.67308313e-07
Iter: 676 loss: 6.67039671e-07
Iter: 677 loss: 6.67086795e-07
Iter: 678 loss: 6.66677181e-07
Iter: 679 loss: 6.70995121e-07
Iter: 680 loss: 6.66687413e-07
Iter: 681 loss: 6.66467145e-07
Iter: 682 loss: 6.67275344e-07
Iter: 683 loss: 6.66432925e-07
Iter: 684 loss: 6.66141204e-07
Iter: 685 loss: 6.65909056e-07
Iter: 686 loss: 6.65898824e-07
Iter: 687 loss: 6.65687708e-07
Iter: 688 loss: 6.68765438e-07
Iter: 689 loss: 6.65688617e-07
Iter: 690 loss: 6.6552343e-07
Iter: 691 loss: 6.65586526e-07
Iter: 692 loss: 6.65455843e-07
Iter: 693 loss: 6.65276616e-07
Iter: 694 loss: 6.6527241e-07
Iter: 695 loss: 6.65160826e-07
Iter: 696 loss: 6.65688333e-07
Iter: 697 loss: 6.65138032e-07
Iter: 698 loss: 6.65068058e-07
Iter: 699 loss: 6.64887693e-07
Iter: 700 loss: 6.66603455e-07
Iter: 701 loss: 6.64917877e-07
Iter: 702 loss: 6.64746324e-07
Iter: 703 loss: 6.65432708e-07
Iter: 704 loss: 6.64675724e-07
Iter: 705 loss: 6.64531456e-07
Iter: 706 loss: 6.64789127e-07
Iter: 707 loss: 6.64448294e-07
Iter: 708 loss: 6.64305276e-07
Iter: 709 loss: 6.64713639e-07
Iter: 710 loss: 6.64219954e-07
Iter: 711 loss: 6.6409757e-07
Iter: 712 loss: 6.64973811e-07
Iter: 713 loss: 6.64112576e-07
Iter: 714 loss: 6.63956712e-07
Iter: 715 loss: 6.63845299e-07
Iter: 716 loss: 6.63752189e-07
Iter: 717 loss: 6.63607693e-07
Iter: 718 loss: 6.66276435e-07
Iter: 719 loss: 6.63596381e-07
Iter: 720 loss: 6.63498327e-07
Iter: 721 loss: 6.63278797e-07
Iter: 722 loss: 6.6695e-07
Iter: 723 loss: 6.63283629e-07
Iter: 724 loss: 6.63101332e-07
Iter: 725 loss: 6.6308985e-07
Iter: 726 loss: 6.62994466e-07
Iter: 727 loss: 6.63164656e-07
Iter: 728 loss: 6.62938078e-07
Iter: 729 loss: 6.62799209e-07
Iter: 730 loss: 6.64445906e-07
Iter: 731 loss: 6.6278875e-07
Iter: 732 loss: 6.62726166e-07
Iter: 733 loss: 6.62950129e-07
Iter: 734 loss: 6.62699733e-07
Iter: 735 loss: 6.62683306e-07
Iter: 736 loss: 6.62554726e-07
Iter: 737 loss: 6.63629e-07
Iter: 738 loss: 6.62485661e-07
Iter: 739 loss: 6.62339687e-07
Iter: 740 loss: 6.6266864e-07
Iter: 741 loss: 6.622314e-07
Iter: 742 loss: 6.62055299e-07
Iter: 743 loss: 6.62408638e-07
Iter: 744 loss: 6.61987883e-07
Iter: 745 loss: 6.61835941e-07
Iter: 746 loss: 6.62929438e-07
Iter: 747 loss: 6.6181957e-07
Iter: 748 loss: 6.61726574e-07
Iter: 749 loss: 6.61855154e-07
Iter: 750 loss: 6.61648585e-07
Iter: 751 loss: 6.61527451e-07
Iter: 752 loss: 6.61922456e-07
Iter: 753 loss: 6.61430818e-07
Iter: 754 loss: 6.61338674e-07
Iter: 755 loss: 6.62201387e-07
Iter: 756 loss: 6.61338674e-07
Iter: 757 loss: 6.61256081e-07
Iter: 758 loss: 6.61108061e-07
Iter: 759 loss: 6.61107094e-07
Iter: 760 loss: 6.60943101e-07
Iter: 761 loss: 6.62798811e-07
Iter: 762 loss: 6.60974763e-07
Iter: 763 loss: 6.60853402e-07
Iter: 764 loss: 6.6110772e-07
Iter: 765 loss: 6.60840726e-07
Iter: 766 loss: 6.60733804e-07
Iter: 767 loss: 6.60929913e-07
Iter: 768 loss: 6.6067264e-07
Iter: 769 loss: 6.60559635e-07
Iter: 770 loss: 6.6079167e-07
Iter: 771 loss: 6.60558158e-07
Iter: 772 loss: 6.60474e-07
Iter: 773 loss: 6.60380124e-07
Iter: 774 loss: 6.60389446e-07
Iter: 775 loss: 6.60224373e-07
Iter: 776 loss: 6.60704302e-07
Iter: 777 loss: 6.60141723e-07
Iter: 778 loss: 6.60040882e-07
Iter: 779 loss: 6.59932425e-07
Iter: 780 loss: 6.59886382e-07
Iter: 781 loss: 6.5976792e-07
Iter: 782 loss: 6.59729608e-07
Iter: 783 loss: 6.59635361e-07
Iter: 784 loss: 6.59657644e-07
Iter: 785 loss: 6.59545208e-07
Iter: 786 loss: 6.59384682e-07
Iter: 787 loss: 6.59839543e-07
Iter: 788 loss: 6.59296802e-07
Iter: 789 loss: 6.59208e-07
Iter: 790 loss: 6.6002508e-07
Iter: 791 loss: 6.59210173e-07
Iter: 792 loss: 6.59062096e-07
Iter: 793 loss: 6.58937324e-07
Iter: 794 loss: 6.58914e-07
Iter: 795 loss: 6.58796e-07
Iter: 796 loss: 6.60634782e-07
Iter: 797 loss: 6.5878919e-07
Iter: 798 loss: 6.5877e-07
Iter: 799 loss: 6.58726435e-07
Iter: 800 loss: 6.58659246e-07
Iter: 801 loss: 6.5858103e-07
Iter: 802 loss: 6.58585577e-07
Iter: 803 loss: 6.58454e-07
Iter: 804 loss: 6.58873546e-07
Iter: 805 loss: 6.58414535e-07
Iter: 806 loss: 6.58296358e-07
Iter: 807 loss: 6.58502358e-07
Iter: 808 loss: 6.58238719e-07
Iter: 809 loss: 6.58157319e-07
Iter: 810 loss: 6.57932333e-07
Iter: 811 loss: 6.62154036e-07
Iter: 812 loss: 6.57892031e-07
Iter: 813 loss: 6.57677276e-07
Iter: 814 loss: 6.60787123e-07
Iter: 815 loss: 6.57658e-07
Iter: 816 loss: 6.57478836e-07
Iter: 817 loss: 6.57258511e-07
Iter: 818 loss: 6.57242481e-07
Iter: 819 loss: 6.57076441e-07
Iter: 820 loss: 6.57036594e-07
Iter: 821 loss: 6.56944621e-07
Iter: 822 loss: 6.56823772e-07
Iter: 823 loss: 6.56727707e-07
Iter: 824 loss: 6.56656198e-07
Iter: 825 loss: 6.56614134e-07
Iter: 826 loss: 6.56516477e-07
Iter: 827 loss: 6.56530858e-07
Iter: 828 loss: 6.564552e-07
Iter: 829 loss: 6.56309226e-07
Iter: 830 loss: 6.56797511e-07
Iter: 831 loss: 6.5624306e-07
Iter: 832 loss: 6.56246925e-07
Iter: 833 loss: 6.56215093e-07
Iter: 834 loss: 6.56134375e-07
Iter: 835 loss: 6.56051157e-07
Iter: 836 loss: 6.56432178e-07
Iter: 837 loss: 6.55968961e-07
Iter: 838 loss: 6.55882559e-07
Iter: 839 loss: 6.55867041e-07
Iter: 840 loss: 6.55795191e-07
Iter: 841 loss: 6.55624e-07
Iter: 842 loss: 6.59156399e-07
Iter: 843 loss: 6.55625e-07
Iter: 844 loss: 6.55396e-07
Iter: 845 loss: 6.55880626e-07
Iter: 846 loss: 6.55360623e-07
Iter: 847 loss: 6.55229712e-07
Iter: 848 loss: 6.55630913e-07
Iter: 849 loss: 6.55119777e-07
Iter: 850 loss: 6.54975793e-07
Iter: 851 loss: 6.55609e-07
Iter: 852 loss: 6.54950213e-07
Iter: 853 loss: 6.54869552e-07
Iter: 854 loss: 6.54712153e-07
Iter: 855 loss: 6.54722555e-07
Iter: 856 loss: 6.54527e-07
Iter: 857 loss: 6.5569742e-07
Iter: 858 loss: 6.54497512e-07
Iter: 859 loss: 6.54382916e-07
Iter: 860 loss: 6.54588689e-07
Iter: 861 loss: 6.54309417e-07
Iter: 862 loss: 6.54169241e-07
Iter: 863 loss: 6.54154292e-07
Iter: 864 loss: 6.54069368e-07
Iter: 865 loss: 6.53961081e-07
Iter: 866 loss: 6.54937e-07
Iter: 867 loss: 6.53940219e-07
Iter: 868 loss: 6.5387178e-07
Iter: 869 loss: 6.54051405e-07
Iter: 870 loss: 6.53780887e-07
Iter: 871 loss: 6.53688119e-07
Iter: 872 loss: 6.53738311e-07
Iter: 873 loss: 6.53625307e-07
Iter: 874 loss: 6.53517645e-07
Iter: 875 loss: 6.53833865e-07
Iter: 876 loss: 6.53478537e-07
Iter: 877 loss: 6.53301e-07
Iter: 878 loss: 6.53185225e-07
Iter: 879 loss: 6.53140432e-07
Iter: 880 loss: 6.52966889e-07
Iter: 881 loss: 6.53761106e-07
Iter: 882 loss: 6.52957283e-07
Iter: 883 loss: 6.52749691e-07
Iter: 884 loss: 6.52868209e-07
Iter: 885 loss: 6.52645952e-07
Iter: 886 loss: 6.52372307e-07
Iter: 887 loss: 6.53371785e-07
Iter: 888 loss: 6.5236577e-07
Iter: 889 loss: 6.5217057e-07
Iter: 890 loss: 6.52516121e-07
Iter: 891 loss: 6.52130325e-07
Iter: 892 loss: 6.51922335e-07
Iter: 893 loss: 6.5383233e-07
Iter: 894 loss: 6.51940468e-07
Iter: 895 loss: 6.51840935e-07
Iter: 896 loss: 6.51803589e-07
Iter: 897 loss: 6.51783807e-07
Iter: 898 loss: 6.51620212e-07
Iter: 899 loss: 6.52135668e-07
Iter: 900 loss: 6.51604921e-07
Iter: 901 loss: 6.51584799e-07
Iter: 902 loss: 6.5156047e-07
Iter: 903 loss: 6.51501068e-07
Iter: 904 loss: 6.51387268e-07
Iter: 905 loss: 6.52620827e-07
Iter: 906 loss: 6.51353105e-07
Iter: 907 loss: 6.51241237e-07
Iter: 908 loss: 6.51200367e-07
Iter: 909 loss: 6.51087589e-07
Iter: 910 loss: 6.50943889e-07
Iter: 911 loss: 6.52487415e-07
Iter: 912 loss: 6.50965944e-07
Iter: 913 loss: 6.50726236e-07
Iter: 914 loss: 6.50714355e-07
Iter: 915 loss: 6.50592597e-07
Iter: 916 loss: 6.50384266e-07
Iter: 917 loss: 6.50949403e-07
Iter: 918 loss: 6.50332368e-07
Iter: 919 loss: 6.50136712e-07
Iter: 920 loss: 6.50019217e-07
Iter: 921 loss: 6.49929689e-07
Iter: 922 loss: 6.49747676e-07
Iter: 923 loss: 6.49752167e-07
Iter: 924 loss: 6.49605852e-07
Iter: 925 loss: 6.49436174e-07
Iter: 926 loss: 6.49395247e-07
Iter: 927 loss: 6.49233698e-07
Iter: 928 loss: 6.52585e-07
Iter: 929 loss: 6.49208232e-07
Iter: 930 loss: 6.49097274e-07
Iter: 931 loss: 6.49121944e-07
Iter: 932 loss: 6.49000526e-07
Iter: 933 loss: 6.48819139e-07
Iter: 934 loss: 6.48997229e-07
Iter: 935 loss: 6.4873359e-07
Iter: 936 loss: 6.48531966e-07
Iter: 937 loss: 6.48859043e-07
Iter: 938 loss: 6.48409582e-07
Iter: 939 loss: 6.48123603e-07
Iter: 940 loss: 6.48422485e-07
Iter: 941 loss: 6.47962111e-07
Iter: 942 loss: 6.48054879e-07
Iter: 943 loss: 6.47824891e-07
Iter: 944 loss: 6.47709385e-07
Iter: 945 loss: 6.47608545e-07
Iter: 946 loss: 6.47627e-07
Iter: 947 loss: 6.47432103e-07
Iter: 948 loss: 6.47235311e-07
Iter: 949 loss: 6.47192792e-07
Iter: 950 loss: 6.4700032e-07
Iter: 951 loss: 6.49636888e-07
Iter: 952 loss: 6.47002082e-07
Iter: 953 loss: 6.46770673e-07
Iter: 954 loss: 6.46860428e-07
Iter: 955 loss: 6.46688477e-07
Iter: 956 loss: 6.46424e-07
Iter: 957 loss: 6.47155503e-07
Iter: 958 loss: 6.46424269e-07
Iter: 959 loss: 6.46240096e-07
Iter: 960 loss: 6.45855266e-07
Iter: 961 loss: 6.53534244e-07
Iter: 962 loss: 6.45920068e-07
Iter: 963 loss: 6.45627949e-07
Iter: 964 loss: 6.50053039e-07
Iter: 965 loss: 6.45612715e-07
Iter: 966 loss: 6.45342368e-07
Iter: 967 loss: 6.45826503e-07
Iter: 968 loss: 6.45256591e-07
Iter: 969 loss: 6.45036266e-07
Iter: 970 loss: 6.47817672e-07
Iter: 971 loss: 6.450515e-07
Iter: 972 loss: 6.44915133e-07
Iter: 973 loss: 6.44735223e-07
Iter: 974 loss: 6.44691852e-07
Iter: 975 loss: 6.44422585e-07
Iter: 976 loss: 6.45915179e-07
Iter: 977 loss: 6.44428155e-07
Iter: 978 loss: 6.44296108e-07
Iter: 979 loss: 6.44263309e-07
Iter: 980 loss: 6.44219938e-07
Iter: 981 loss: 6.43919748e-07
Iter: 982 loss: 6.44507736e-07
Iter: 983 loss: 6.437823e-07
Iter: 984 loss: 6.43490694e-07
Iter: 985 loss: 6.46798e-07
Iter: 986 loss: 6.43473129e-07
Iter: 987 loss: 6.43307544e-07
Iter: 988 loss: 6.43267185e-07
Iter: 989 loss: 6.43148041e-07
Iter: 990 loss: 6.42823579e-07
Iter: 991 loss: 6.46113904e-07
Iter: 992 loss: 6.42841087e-07
Iter: 993 loss: 6.42618147e-07
Iter: 994 loss: 6.42509917e-07
Iter: 995 loss: 6.42433179e-07
Iter: 996 loss: 6.42157545e-07
Iter: 997 loss: 6.42340183e-07
Iter: 998 loss: 6.4197e-07
Iter: 999 loss: 6.41743327e-07
Iter: 1000 loss: 6.43393207e-07
Iter: 1001 loss: 6.4168421e-07
Iter: 1002 loss: 6.41441147e-07
Iter: 1003 loss: 6.41389306e-07
Iter: 1004 loss: 6.41219344e-07
Iter: 1005 loss: 6.40924668e-07
Iter: 1006 loss: 6.41917e-07
Iter: 1007 loss: 6.40870041e-07
Iter: 1008 loss: 6.40511473e-07
Iter: 1009 loss: 6.41252029e-07
Iter: 1010 loss: 6.40381813e-07
Iter: 1011 loss: 6.40251073e-07
Iter: 1012 loss: 6.40247208e-07
Iter: 1013 loss: 6.40087478e-07
Iter: 1014 loss: 6.40769713e-07
Iter: 1015 loss: 6.40132157e-07
Iter: 1016 loss: 6.39957818e-07
Iter: 1017 loss: 6.39639211e-07
Iter: 1018 loss: 6.43597787e-07
Iter: 1019 loss: 6.39610107e-07
Iter: 1020 loss: 6.39422467e-07
Iter: 1021 loss: 6.39949405e-07
Iter: 1022 loss: 6.39341238e-07
Iter: 1023 loss: 6.39055e-07
Iter: 1024 loss: 6.39400582e-07
Iter: 1025 loss: 6.38962149e-07
Iter: 1026 loss: 6.38716187e-07
Iter: 1027 loss: 6.39397967e-07
Iter: 1028 loss: 6.38611e-07
Iter: 1029 loss: 6.38337e-07
Iter: 1030 loss: 6.4042672e-07
Iter: 1031 loss: 6.38318056e-07
Iter: 1032 loss: 6.38076e-07
Iter: 1033 loss: 6.38895472e-07
Iter: 1034 loss: 6.38016274e-07
Iter: 1035 loss: 6.37811866e-07
Iter: 1036 loss: 6.37484789e-07
Iter: 1037 loss: 6.43229896e-07
Iter: 1038 loss: 6.37438632e-07
Iter: 1039 loss: 6.3718835e-07
Iter: 1040 loss: 6.41273687e-07
Iter: 1041 loss: 6.37157768e-07
Iter: 1042 loss: 6.36867298e-07
Iter: 1043 loss: 6.3685593e-07
Iter: 1044 loss: 6.36653226e-07
Iter: 1045 loss: 6.36462062e-07
Iter: 1046 loss: 6.3645723e-07
Iter: 1047 loss: 6.36291134e-07
Iter: 1048 loss: 6.38513825e-07
Iter: 1049 loss: 6.36284e-07
Iter: 1050 loss: 6.36202628e-07
Iter: 1051 loss: 6.36082859e-07
Iter: 1052 loss: 6.3608104e-07
Iter: 1053 loss: 6.35907497e-07
Iter: 1054 loss: 6.35891183e-07
Iter: 1055 loss: 6.35809556e-07
Iter: 1056 loss: 6.35605431e-07
Iter: 1057 loss: 6.35855372e-07
Iter: 1058 loss: 6.35474407e-07
Iter: 1059 loss: 6.35339404e-07
Iter: 1060 loss: 6.36705e-07
Iter: 1061 loss: 6.35308311e-07
Iter: 1062 loss: 6.35207584e-07
Iter: 1063 loss: 6.35111235e-07
Iter: 1064 loss: 6.35058e-07
Iter: 1065 loss: 6.34942e-07
Iter: 1066 loss: 6.35754759e-07
Iter: 1067 loss: 6.34884145e-07
Iter: 1068 loss: 6.34735898e-07
Iter: 1069 loss: 6.35047968e-07
Iter: 1070 loss: 6.34694e-07
Iter: 1071 loss: 6.34478454e-07
Iter: 1072 loss: 6.34888579e-07
Iter: 1073 loss: 6.34375738e-07
Iter: 1074 loss: 6.34234084e-07
Iter: 1075 loss: 6.34027174e-07
Iter: 1076 loss: 6.34003754e-07
Iter: 1077 loss: 6.33703053e-07
Iter: 1078 loss: 6.371057e-07
Iter: 1079 loss: 6.33687705e-07
Iter: 1080 loss: 6.33581521e-07
Iter: 1081 loss: 6.35063373e-07
Iter: 1082 loss: 6.33605055e-07
Iter: 1083 loss: 6.33453112e-07
Iter: 1084 loss: 6.33517857e-07
Iter: 1085 loss: 6.33355398e-07
Iter: 1086 loss: 6.33300488e-07
Iter: 1087 loss: 6.33446405e-07
Iter: 1088 loss: 6.33216e-07
Iter: 1089 loss: 6.33119953e-07
Iter: 1090 loss: 6.33163324e-07
Iter: 1091 loss: 6.33055663e-07
Iter: 1092 loss: 6.3291418e-07
Iter: 1093 loss: 6.33105401e-07
Iter: 1094 loss: 6.32877686e-07
Iter: 1095 loss: 6.32741262e-07
Iter: 1096 loss: 6.33001321e-07
Iter: 1097 loss: 6.32701813e-07
Iter: 1098 loss: 6.3254879e-07
Iter: 1099 loss: 6.32988929e-07
Iter: 1100 loss: 6.32497233e-07
Iter: 1101 loss: 6.32362344e-07
Iter: 1102 loss: 6.32818512e-07
Iter: 1103 loss: 6.32319939e-07
Iter: 1104 loss: 6.32219269e-07
Iter: 1105 loss: 6.32683907e-07
Iter: 1106 loss: 6.32170327e-07
Iter: 1107 loss: 6.32060164e-07
Iter: 1108 loss: 6.3259904e-07
Iter: 1109 loss: 6.32002866e-07
Iter: 1110 loss: 6.31949e-07
Iter: 1111 loss: 6.31745706e-07
Iter: 1112 loss: 6.35010451e-07
Iter: 1113 loss: 6.31804141e-07
Iter: 1114 loss: 6.31619685e-07
Iter: 1115 loss: 6.32233082e-07
Iter: 1116 loss: 6.31554656e-07
Iter: 1117 loss: 6.31511568e-07
Iter: 1118 loss: 6.31453076e-07
Iter: 1119 loss: 6.31359853e-07
Iter: 1120 loss: 6.3132677e-07
Iter: 1121 loss: 6.31281694e-07
Iter: 1122 loss: 6.31232467e-07
Iter: 1123 loss: 6.31125886e-07
Iter: 1124 loss: 6.31129183e-07
Iter: 1125 loss: 6.30905674e-07
Iter: 1126 loss: 6.31882074e-07
Iter: 1127 loss: 6.30902377e-07
Iter: 1128 loss: 6.30836894e-07
Iter: 1129 loss: 6.3096968e-07
Iter: 1130 loss: 6.30788179e-07
Iter: 1131 loss: 6.30640841e-07
Iter: 1132 loss: 6.30645673e-07
Iter: 1133 loss: 6.30529655e-07
Iter: 1134 loss: 6.30402781e-07
Iter: 1135 loss: 6.31160844e-07
Iter: 1136 loss: 6.30395959e-07
Iter: 1137 loss: 6.30293584e-07
Iter: 1138 loss: 6.31218768e-07
Iter: 1139 loss: 6.30288696e-07
Iter: 1140 loss: 6.30118393e-07
Iter: 1141 loss: 6.30683871e-07
Iter: 1142 loss: 6.30109412e-07
Iter: 1143 loss: 6.30008572e-07
Iter: 1144 loss: 6.2999311e-07
Iter: 1145 loss: 6.2995241e-07
Iter: 1146 loss: 6.29831902e-07
Iter: 1147 loss: 6.30262093e-07
Iter: 1148 loss: 6.2974226e-07
Iter: 1149 loss: 6.29620502e-07
Iter: 1150 loss: 6.2967888e-07
Iter: 1151 loss: 6.29555245e-07
Iter: 1152 loss: 6.29522617e-07
Iter: 1153 loss: 6.2950312e-07
Iter: 1154 loss: 6.29381134e-07
Iter: 1155 loss: 6.29300587e-07
Iter: 1156 loss: 6.31864054e-07
Iter: 1157 loss: 6.29316901e-07
Iter: 1158 loss: 6.29212082e-07
Iter: 1159 loss: 6.29017563e-07
Iter: 1160 loss: 6.31851492e-07
Iter: 1161 loss: 6.29019894e-07
Iter: 1162 loss: 6.2889535e-07
Iter: 1163 loss: 6.30676482e-07
Iter: 1164 loss: 6.28901205e-07
Iter: 1165 loss: 6.28778082e-07
Iter: 1166 loss: 6.2926415e-07
Iter: 1167 loss: 6.2872067e-07
Iter: 1168 loss: 6.286445e-07
Iter: 1169 loss: 6.29035753e-07
Iter: 1170 loss: 6.28569069e-07
Iter: 1171 loss: 6.28483178e-07
Iter: 1172 loss: 6.28737666e-07
Iter: 1173 loss: 6.28501084e-07
Iter: 1174 loss: 6.28333396e-07
Iter: 1175 loss: 6.28452142e-07
Iter: 1176 loss: 6.28278656e-07
Iter: 1177 loss: 6.28153714e-07
Iter: 1178 loss: 6.2904769e-07
Iter: 1179 loss: 6.28103521e-07
Iter: 1180 loss: 6.28024054e-07
Iter: 1181 loss: 6.2801189e-07
Iter: 1182 loss: 6.27971247e-07
Iter: 1183 loss: 6.27790541e-07
Iter: 1184 loss: 6.27761e-07
Iter: 1185 loss: 6.27616657e-07
Iter: 1186 loss: 6.27567374e-07
Iter: 1187 loss: 6.2759409e-07
Iter: 1188 loss: 6.27438e-07
Iter: 1189 loss: 6.28084649e-07
Iter: 1190 loss: 6.27489953e-07
Iter: 1191 loss: 6.27435043e-07
Iter: 1192 loss: 6.27292536e-07
Iter: 1193 loss: 6.29138697e-07
Iter: 1194 loss: 6.27256213e-07
Iter: 1195 loss: 6.2711365e-07
Iter: 1196 loss: 6.27180043e-07
Iter: 1197 loss: 6.27012696e-07
Iter: 1198 loss: 6.26856945e-07
Iter: 1199 loss: 6.27975737e-07
Iter: 1200 loss: 6.26878204e-07
Iter: 1201 loss: 6.26756e-07
Iter: 1202 loss: 6.26662711e-07
Iter: 1203 loss: 6.26640599e-07
Iter: 1204 loss: 6.26464953e-07
Iter: 1205 loss: 6.28624719e-07
Iter: 1206 loss: 6.26473e-07
Iter: 1207 loss: 6.26413453e-07
Iter: 1208 loss: 6.26421524e-07
Iter: 1209 loss: 6.26363374e-07
Iter: 1210 loss: 6.26230531e-07
Iter: 1211 loss: 6.27067209e-07
Iter: 1212 loss: 6.26207e-07
Iter: 1213 loss: 6.2611764e-07
Iter: 1214 loss: 6.2632796e-07
Iter: 1215 loss: 6.26102633e-07
Iter: 1216 loss: 6.25977407e-07
Iter: 1217 loss: 6.25981784e-07
Iter: 1218 loss: 6.2590442e-07
Iter: 1219 loss: 6.2577675e-07
Iter: 1220 loss: 6.26659528e-07
Iter: 1221 loss: 6.25759583e-07
Iter: 1222 loss: 6.25657549e-07
Iter: 1223 loss: 6.25690689e-07
Iter: 1224 loss: 6.25647203e-07
Iter: 1225 loss: 6.25622818e-07
Iter: 1226 loss: 6.25560176e-07
Iter: 1227 loss: 6.25492248e-07
Iter: 1228 loss: 6.25428072e-07
Iter: 1229 loss: 6.25392e-07
Iter: 1230 loss: 6.25327971e-07
Iter: 1231 loss: 6.25388282e-07
Iter: 1232 loss: 6.25231678e-07
Iter: 1233 loss: 6.25084397e-07
Iter: 1234 loss: 6.25281871e-07
Iter: 1235 loss: 6.25029031e-07
Iter: 1236 loss: 6.2493217e-07
Iter: 1237 loss: 6.25392772e-07
Iter: 1238 loss: 6.24885104e-07
Iter: 1239 loss: 6.2476829e-07
Iter: 1240 loss: 6.2483582e-07
Iter: 1241 loss: 6.247177e-07
Iter: 1242 loss: 6.24553934e-07
Iter: 1243 loss: 6.25167104e-07
Iter: 1244 loss: 6.24557401e-07
Iter: 1245 loss: 6.24386416e-07
Iter: 1246 loss: 6.25178473e-07
Iter: 1247 loss: 6.2437249e-07
Iter: 1248 loss: 6.24283302e-07
Iter: 1249 loss: 6.24544e-07
Iter: 1250 loss: 6.2428785e-07
Iter: 1251 loss: 6.24178e-07
Iter: 1252 loss: 6.24177915e-07
Iter: 1253 loss: 6.24121185e-07
Iter: 1254 loss: 6.2407878e-07
Iter: 1255 loss: 6.24070708e-07
Iter: 1256 loss: 6.23985613e-07
Iter: 1257 loss: 6.24001245e-07
Iter: 1258 loss: 6.23998744e-07
Iter: 1259 loss: 6.23903304e-07
Iter: 1260 loss: 6.24201675e-07
Iter: 1261 loss: 6.2391257e-07
Iter: 1262 loss: 6.23814458e-07
Iter: 1263 loss: 6.24018924e-07
Iter: 1264 loss: 6.23747383e-07
Iter: 1265 loss: 6.23655694e-07
Iter: 1266 loss: 6.23911092e-07
Iter: 1267 loss: 6.23649726e-07
Iter: 1268 loss: 6.23534447e-07
Iter: 1269 loss: 6.23419567e-07
Iter: 1270 loss: 6.23422466e-07
Iter: 1271 loss: 6.23293e-07
Iter: 1272 loss: 6.24775225e-07
Iter: 1273 loss: 6.23306278e-07
Iter: 1274 loss: 6.23164283e-07
Iter: 1275 loss: 6.23161839e-07
Iter: 1276 loss: 6.23107e-07
Iter: 1277 loss: 6.22972379e-07
Iter: 1278 loss: 6.23552523e-07
Iter: 1279 loss: 6.22971072e-07
Iter: 1280 loss: 6.22850735e-07
Iter: 1281 loss: 6.23392282e-07
Iter: 1282 loss: 6.22798837e-07
Iter: 1283 loss: 6.22739549e-07
Iter: 1284 loss: 6.23222149e-07
Iter: 1285 loss: 6.22692482e-07
Iter: 1286 loss: 6.22608241e-07
Iter: 1287 loss: 6.22462039e-07
Iter: 1288 loss: 6.22461e-07
Iter: 1289 loss: 6.22438733e-07
Iter: 1290 loss: 6.22365064e-07
Iter: 1291 loss: 6.22333573e-07
Iter: 1292 loss: 6.22589596e-07
Iter: 1293 loss: 6.22281618e-07
Iter: 1294 loss: 6.22278492e-07
Iter: 1295 loss: 6.22139169e-07
Iter: 1296 loss: 6.23444862e-07
Iter: 1297 loss: 6.22147581e-07
Iter: 1298 loss: 6.21972504e-07
Iter: 1299 loss: 6.21986601e-07
Iter: 1300 loss: 6.21897755e-07
Iter: 1301 loss: 6.21782e-07
Iter: 1302 loss: 6.22457492e-07
Iter: 1303 loss: 6.21763093e-07
Iter: 1304 loss: 6.21637469e-07
Iter: 1305 loss: 6.21787137e-07
Iter: 1306 loss: 6.21575339e-07
Iter: 1307 loss: 6.21447555e-07
Iter: 1308 loss: 6.21732227e-07
Iter: 1309 loss: 6.21374738e-07
Iter: 1310 loss: 6.212681e-07
Iter: 1311 loss: 6.21257811e-07
Iter: 1312 loss: 6.21191589e-07
Iter: 1313 loss: 6.21053573e-07
Iter: 1314 loss: 6.22016159e-07
Iter: 1315 loss: 6.21006052e-07
Iter: 1316 loss: 6.20819037e-07
Iter: 1317 loss: 6.21832669e-07
Iter: 1318 loss: 6.20826711e-07
Iter: 1319 loss: 6.20691196e-07
Iter: 1320 loss: 6.21074605e-07
Iter: 1321 loss: 6.20671813e-07
Iter: 1322 loss: 6.20566084e-07
Iter: 1323 loss: 6.2056813e-07
Iter: 1324 loss: 6.20537037e-07
Iter: 1325 loss: 6.20444553e-07
Iter: 1326 loss: 6.2042551e-07
Iter: 1327 loss: 6.20358378e-07
Iter: 1328 loss: 6.20275443e-07
Iter: 1329 loss: 6.20237699e-07
Iter: 1330 loss: 6.20216213e-07
Iter: 1331 loss: 6.20295737e-07
Iter: 1332 loss: 6.2015e-07
Iter: 1333 loss: 6.2003005e-07
Iter: 1334 loss: 6.20017772e-07
Iter: 1335 loss: 6.19953767e-07
Iter: 1336 loss: 6.19824164e-07
Iter: 1337 loss: 6.20176309e-07
Iter: 1338 loss: 6.19820412e-07
Iter: 1339 loss: 6.19636523e-07
Iter: 1340 loss: 6.19928585e-07
Iter: 1341 loss: 6.19604236e-07
Iter: 1342 loss: 6.19507944e-07
Iter: 1343 loss: 6.20167e-07
Iter: 1344 loss: 6.19475202e-07
Iter: 1345 loss: 6.19385446e-07
Iter: 1346 loss: 6.19326897e-07
Iter: 1347 loss: 6.19265393e-07
Iter: 1348 loss: 6.19126922e-07
Iter: 1349 loss: 6.19153e-07
Iter: 1350 loss: 6.19106743e-07
Iter: 1351 loss: 6.19224863e-07
Iter: 1352 loss: 6.19077582e-07
Iter: 1353 loss: 6.18916658e-07
Iter: 1354 loss: 6.18905801e-07
Iter: 1355 loss: 6.18858053e-07
Iter: 1356 loss: 6.1887124e-07
Iter: 1357 loss: 6.18789e-07
Iter: 1358 loss: 6.18745503e-07
Iter: 1359 loss: 6.18701392e-07
Iter: 1360 loss: 6.1872413e-07
Iter: 1361 loss: 6.18623915e-07
Iter: 1362 loss: 6.18584181e-07
Iter: 1363 loss: 6.18499939e-07
Iter: 1364 loss: 6.184016e-07
Iter: 1365 loss: 6.19080197e-07
Iter: 1366 loss: 6.18417403e-07
Iter: 1367 loss: 6.18340266e-07
Iter: 1368 loss: 6.18259037e-07
Iter: 1369 loss: 6.18241586e-07
Iter: 1370 loss: 6.1813472e-07
Iter: 1371 loss: 6.19624871e-07
Iter: 1372 loss: 6.1815831e-07
Iter: 1373 loss: 6.18067247e-07
Iter: 1374 loss: 6.17912292e-07
Iter: 1375 loss: 6.17919795e-07
Iter: 1376 loss: 6.17805e-07
Iter: 1377 loss: 6.17828618e-07
Iter: 1378 loss: 6.17681167e-07
Iter: 1379 loss: 6.1747869e-07
Iter: 1380 loss: 6.17485853e-07
Iter: 1381 loss: 6.1742378e-07
Iter: 1382 loss: 6.17413718e-07
Iter: 1383 loss: 6.17321348e-07
Iter: 1384 loss: 6.1734363e-07
Iter: 1385 loss: 6.1726297e-07
Iter: 1386 loss: 6.17148316e-07
Iter: 1387 loss: 6.18346064e-07
Iter: 1388 loss: 6.17161277e-07
Iter: 1389 loss: 6.17028263e-07
Iter: 1390 loss: 6.17562819e-07
Iter: 1391 loss: 6.17014166e-07
Iter: 1392 loss: 6.16967498e-07
Iter: 1393 loss: 6.16906163e-07
Iter: 1394 loss: 6.16888315e-07
Iter: 1395 loss: 6.16803675e-07
Iter: 1396 loss: 6.16733132e-07
Iter: 1397 loss: 6.16698458e-07
Iter: 1398 loss: 6.16563625e-07
Iter: 1399 loss: 6.17826458e-07
Iter: 1400 loss: 6.16515422e-07
Iter: 1401 loss: 6.16454145e-07
Iter: 1402 loss: 6.16312377e-07
Iter: 1403 loss: 6.16335967e-07
Iter: 1404 loss: 6.16187606e-07
Iter: 1405 loss: 6.17774504e-07
Iter: 1406 loss: 6.16169e-07
Iter: 1407 loss: 6.16054706e-07
Iter: 1408 loss: 6.15940621e-07
Iter: 1409 loss: 6.15962e-07
Iter: 1410 loss: 6.15796921e-07
Iter: 1411 loss: 6.15770034e-07
Iter: 1412 loss: 6.15695626e-07
Iter: 1413 loss: 6.15653505e-07
Iter: 1414 loss: 6.15610759e-07
Iter: 1415 loss: 6.15497811e-07
Iter: 1416 loss: 6.16110697e-07
Iter: 1417 loss: 6.15479678e-07
Iter: 1418 loss: 6.15379577e-07
Iter: 1419 loss: 6.15954036e-07
Iter: 1420 loss: 6.15365082e-07
Iter: 1421 loss: 6.15266345e-07
Iter: 1422 loss: 6.1568096e-07
Iter: 1423 loss: 6.15336603e-07
Iter: 1424 loss: 6.1523906e-07
Iter: 1425 loss: 6.15240936e-07
Iter: 1426 loss: 6.15192448e-07
Iter: 1427 loss: 6.15093086e-07
Iter: 1428 loss: 6.15051931e-07
Iter: 1429 loss: 6.15041188e-07
Iter: 1430 loss: 6.14866053e-07
Iter: 1431 loss: 6.15472914e-07
Iter: 1432 loss: 6.14804e-07
Iter: 1433 loss: 6.14684097e-07
Iter: 1434 loss: 6.14582746e-07
Iter: 1435 loss: 6.14548355e-07
Iter: 1436 loss: 6.14438818e-07
Iter: 1437 loss: 6.14433e-07
Iter: 1438 loss: 6.14341047e-07
Iter: 1439 loss: 6.1425385e-07
Iter: 1440 loss: 6.14223e-07
Iter: 1441 loss: 6.1407377e-07
Iter: 1442 loss: 6.15251906e-07
Iter: 1443 loss: 6.14072633e-07
Iter: 1444 loss: 6.1399146e-07
Iter: 1445 loss: 6.13915176e-07
Iter: 1446 loss: 6.1384344e-07
Iter: 1447 loss: 6.13773921e-07
Iter: 1448 loss: 6.14570752e-07
Iter: 1449 loss: 6.13743055e-07
Iter: 1450 loss: 6.13628799e-07
Iter: 1451 loss: 6.14113389e-07
Iter: 1452 loss: 6.13620273e-07
Iter: 1453 loss: 6.13538e-07
Iter: 1454 loss: 6.14494184e-07
Iter: 1455 loss: 6.13579289e-07
Iter: 1456 loss: 6.13478903e-07
Iter: 1457 loss: 6.13605664e-07
Iter: 1458 loss: 6.13489135e-07
Iter: 1459 loss: 6.13418422e-07
Iter: 1460 loss: 6.13365842e-07
Iter: 1461 loss: 6.13385396e-07
Iter: 1462 loss: 6.13282168e-07
Iter: 1463 loss: 6.13222824e-07
Iter: 1464 loss: 6.13181e-07
Iter: 1465 loss: 6.13066163e-07
Iter: 1466 loss: 6.13847646e-07
Iter: 1467 loss: 6.13054397e-07
Iter: 1468 loss: 6.12963e-07
Iter: 1469 loss: 6.12830206e-07
Iter: 1470 loss: 6.12808321e-07
Iter: 1471 loss: 6.12717315e-07
Iter: 1472 loss: 6.1417029e-07
Iter: 1473 loss: 6.12693327e-07
Iter: 1474 loss: 6.12594647e-07
Iter: 1475 loss: 6.12571966e-07
Iter: 1476 loss: 6.12483632e-07
Iter: 1477 loss: 6.12396093e-07
Iter: 1478 loss: 6.13887948e-07
Iter: 1479 loss: 6.12388703e-07
Iter: 1480 loss: 6.12262852e-07
Iter: 1481 loss: 6.12279962e-07
Iter: 1482 loss: 6.12196686e-07
Iter: 1483 loss: 6.1206083e-07
Iter: 1484 loss: 6.12414851e-07
Iter: 1485 loss: 6.11993187e-07
Iter: 1486 loss: 6.11884502e-07
Iter: 1487 loss: 6.11839766e-07
Iter: 1488 loss: 6.11818393e-07
Iter: 1489 loss: 6.1186779e-07
Iter: 1490 loss: 6.11755866e-07
Iter: 1491 loss: 6.11688904e-07
Iter: 1492 loss: 6.11754842e-07
Iter: 1493 loss: 6.11676569e-07
Iter: 1494 loss: 6.11630298e-07
Iter: 1495 loss: 6.11519567e-07
Iter: 1496 loss: 6.11508483e-07
Iter: 1497 loss: 6.11397127e-07
Iter: 1498 loss: 6.11681912e-07
Iter: 1499 loss: 6.11346422e-07
Iter: 1500 loss: 6.11290488e-07
Iter: 1501 loss: 6.12480676e-07
Iter: 1502 loss: 6.11315727e-07
Iter: 1503 loss: 6.11257519e-07
Iter: 1504 loss: 6.11092787e-07
Iter: 1505 loss: 6.12276722e-07
Iter: 1506 loss: 6.11083919e-07
Iter: 1507 loss: 6.10927e-07
Iter: 1508 loss: 6.10895768e-07
Iter: 1509 loss: 6.10780148e-07
Iter: 1510 loss: 6.10785321e-07
Iter: 1511 loss: 6.10713698e-07
Iter: 1512 loss: 6.10579946e-07
Iter: 1513 loss: 6.11749726e-07
Iter: 1514 loss: 6.10542e-07
Iter: 1515 loss: 6.10403617e-07
Iter: 1516 loss: 6.10372e-07
Iter: 1517 loss: 6.10269694e-07
Iter: 1518 loss: 6.10148732e-07
Iter: 1519 loss: 6.12268821e-07
Iter: 1520 loss: 6.10139637e-07
Iter: 1521 loss: 6.10083418e-07
Iter: 1522 loss: 6.10140376e-07
Iter: 1523 loss: 6.1003044e-07
Iter: 1524 loss: 6.09950575e-07
Iter: 1525 loss: 6.10798111e-07
Iter: 1526 loss: 6.09968538e-07
Iter: 1527 loss: 6.09866106e-07
Iter: 1528 loss: 6.09800338e-07
Iter: 1529 loss: 6.09784763e-07
Iter: 1530 loss: 6.09729113e-07
Iter: 1531 loss: 6.0974503e-07
Iter: 1532 loss: 6.09667154e-07
Iter: 1533 loss: 6.0952641e-07
Iter: 1534 loss: 6.09698e-07
Iter: 1535 loss: 6.09529081e-07
Iter: 1536 loss: 6.0944808e-07
Iter: 1537 loss: 6.10136226e-07
Iter: 1538 loss: 6.09391236e-07
Iter: 1539 loss: 6.09346841e-07
Iter: 1540 loss: 6.093303e-07
Iter: 1541 loss: 6.09263736e-07
Iter: 1542 loss: 6.09179551e-07
Iter: 1543 loss: 6.09749804e-07
Iter: 1544 loss: 6.09150277e-07
Iter: 1545 loss: 6.09098777e-07
Iter: 1546 loss: 6.08940923e-07
Iter: 1547 loss: 6.08991627e-07
Iter: 1548 loss: 6.08856396e-07
Iter: 1549 loss: 6.09724452e-07
Iter: 1550 loss: 6.08824109e-07
Iter: 1551 loss: 6.08722189e-07
Iter: 1552 loss: 6.0870434e-07
Iter: 1553 loss: 6.08639084e-07
Iter: 1554 loss: 6.08534265e-07
Iter: 1555 loss: 6.09885888e-07
Iter: 1556 loss: 6.08524147e-07
Iter: 1557 loss: 6.08456048e-07
Iter: 1558 loss: 6.0882428e-07
Iter: 1559 loss: 6.08463608e-07
Iter: 1560 loss: 6.08370669e-07
Iter: 1561 loss: 6.08383e-07
Iter: 1562 loss: 6.08301434e-07
Iter: 1563 loss: 6.08240214e-07
Iter: 1564 loss: 6.08211849e-07
Iter: 1565 loss: 6.08169728e-07
Iter: 1566 loss: 6.08051664e-07
Iter: 1567 loss: 6.08177515e-07
Iter: 1568 loss: 6.08053483e-07
Iter: 1569 loss: 6.07932463e-07
Iter: 1570 loss: 6.08147388e-07
Iter: 1571 loss: 6.07871186e-07
Iter: 1572 loss: 6.07821789e-07
Iter: 1573 loss: 6.0779962e-07
Iter: 1574 loss: 6.07740446e-07
Iter: 1575 loss: 6.07655579e-07
Iter: 1576 loss: 6.0974304e-07
Iter: 1577 loss: 6.07635059e-07
Iter: 1578 loss: 6.07550533e-07
Iter: 1579 loss: 6.08293647e-07
Iter: 1580 loss: 6.07484139e-07
Iter: 1581 loss: 6.07431389e-07
Iter: 1582 loss: 6.07473396e-07
Iter: 1583 loss: 6.07354195e-07
Iter: 1584 loss: 6.07240906e-07
Iter: 1585 loss: 6.07227662e-07
Iter: 1586 loss: 6.07096695e-07
Iter: 1587 loss: 6.07003244e-07
Iter: 1588 loss: 6.06958338e-07
Iter: 1589 loss: 6.0686267e-07
Iter: 1590 loss: 6.06863694e-07
Iter: 1591 loss: 6.06829303e-07
Iter: 1592 loss: 6.0663325e-07
Iter: 1593 loss: 6.08741743e-07
Iter: 1594 loss: 6.06636263e-07
Iter: 1595 loss: 6.06563219e-07
Iter: 1596 loss: 6.06488584e-07
Iter: 1597 loss: 6.06466813e-07
Iter: 1598 loss: 6.06385356e-07
Iter: 1599 loss: 6.06272465e-07
Iter: 1600 loss: 6.06235744e-07
Iter: 1601 loss: 6.06066067e-07
Iter: 1602 loss: 6.08109531e-07
Iter: 1603 loss: 6.06084541e-07
Iter: 1604 loss: 6.05981199e-07
Iter: 1605 loss: 6.06314586e-07
Iter: 1606 loss: 6.05923788e-07
Iter: 1607 loss: 6.05823061e-07
Iter: 1608 loss: 6.06692e-07
Iter: 1609 loss: 6.05770765e-07
Iter: 1610 loss: 6.05707555e-07
Iter: 1611 loss: 6.05773948e-07
Iter: 1612 loss: 6.05679588e-07
Iter: 1613 loss: 6.05589378e-07
Iter: 1614 loss: 6.05641276e-07
Iter: 1615 loss: 6.05526225e-07
Iter: 1616 loss: 6.05455682e-07
Iter: 1617 loss: 6.06023946e-07
Iter: 1618 loss: 6.05450964e-07
Iter: 1619 loss: 6.0534785e-07
Iter: 1620 loss: 6.05370587e-07
Iter: 1621 loss: 6.05310333e-07
Iter: 1622 loss: 6.05180048e-07
Iter: 1623 loss: 6.06487902e-07
Iter: 1624 loss: 6.05192042e-07
Iter: 1625 loss: 6.0515265e-07
Iter: 1626 loss: 6.05835851e-07
Iter: 1627 loss: 6.05143782e-07
Iter: 1628 loss: 6.05114906e-07
Iter: 1629 loss: 6.05009461e-07
Iter: 1630 loss: 6.05741207e-07
Iter: 1631 loss: 6.0499525e-07
Iter: 1632 loss: 6.04846832e-07
Iter: 1633 loss: 6.04944432e-07
Iter: 1634 loss: 6.04801e-07
Iter: 1635 loss: 6.04632305e-07
Iter: 1636 loss: 6.04854108e-07
Iter: 1637 loss: 6.04641968e-07
Iter: 1638 loss: 6.04449042e-07
Iter: 1639 loss: 6.05232458e-07
Iter: 1640 loss: 6.04445859e-07
Iter: 1641 loss: 6.04351271e-07
Iter: 1642 loss: 6.05448747e-07
Iter: 1643 loss: 6.04358718e-07
Iter: 1644 loss: 6.04299146e-07
Iter: 1645 loss: 6.04176421e-07
Iter: 1646 loss: 6.04214279e-07
Iter: 1647 loss: 6.04026923e-07
Iter: 1648 loss: 6.0489765e-07
Iter: 1649 loss: 6.03999581e-07
Iter: 1650 loss: 6.03965134e-07
Iter: 1651 loss: 6.03947e-07
Iter: 1652 loss: 6.03914089e-07
Iter: 1653 loss: 6.03783633e-07
Iter: 1654 loss: 6.04190291e-07
Iter: 1655 loss: 6.03772889e-07
Iter: 1656 loss: 6.03644764e-07
Iter: 1657 loss: 6.04619913e-07
Iter: 1658 loss: 6.03665626e-07
Iter: 1659 loss: 6.03610147e-07
Iter: 1660 loss: 6.04039769e-07
Iter: 1661 loss: 6.03628905e-07
Iter: 1662 loss: 6.03583e-07
Iter: 1663 loss: 6.03460308e-07
Iter: 1664 loss: 6.03464514e-07
Iter: 1665 loss: 6.03358558e-07
Iter: 1666 loss: 6.03475712e-07
Iter: 1667 loss: 6.03308308e-07
Iter: 1668 loss: 6.03179387e-07
Iter: 1669 loss: 6.03073204e-07
Iter: 1670 loss: 6.03054559e-07
Iter: 1671 loss: 6.02942e-07
Iter: 1672 loss: 6.04296361e-07
Iter: 1673 loss: 6.02922228e-07
Iter: 1674 loss: 6.02776e-07
Iter: 1675 loss: 6.03169667e-07
Iter: 1676 loss: 6.02734417e-07
Iter: 1677 loss: 6.02597197e-07
Iter: 1678 loss: 6.03364754e-07
Iter: 1679 loss: 6.0253285e-07
Iter: 1680 loss: 6.02415355e-07
Iter: 1681 loss: 6.02445539e-07
Iter: 1682 loss: 6.02389548e-07
Iter: 1683 loss: 6.02276259e-07
Iter: 1684 loss: 6.02511932e-07
Iter: 1685 loss: 6.0223806e-07
Iter: 1686 loss: 6.02146429e-07
Iter: 1687 loss: 6.02487262e-07
Iter: 1688 loss: 6.02121872e-07
Iter: 1689 loss: 6.02001819e-07
Iter: 1690 loss: 6.02345551e-07
Iter: 1691 loss: 6.01931333e-07
Iter: 1692 loss: 6.01848342e-07
Iter: 1693 loss: 6.01846409e-07
Iter: 1694 loss: 6.01794341e-07
Iter: 1695 loss: 6.01987e-07
Iter: 1696 loss: 6.01755971e-07
Iter: 1697 loss: 6.01753641e-07
Iter: 1698 loss: 6.0167423e-07
Iter: 1699 loss: 6.01656211e-07
Iter: 1700 loss: 6.01513591e-07
Iter: 1701 loss: 6.01678835e-07
Iter: 1702 loss: 6.01506372e-07
Iter: 1703 loss: 6.01408033e-07
Iter: 1704 loss: 6.01413092e-07
Iter: 1705 loss: 6.01321574e-07
Iter: 1706 loss: 6.01193108e-07
Iter: 1707 loss: 6.02820592e-07
Iter: 1708 loss: 6.0120442e-07
Iter: 1709 loss: 6.01122792e-07
Iter: 1710 loss: 6.01712941e-07
Iter: 1711 loss: 6.01139163e-07
Iter: 1712 loss: 6.01042075e-07
Iter: 1713 loss: 6.00903604e-07
Iter: 1714 loss: 6.03534204e-07
Iter: 1715 loss: 6.00908265e-07
Iter: 1716 loss: 6.00798444e-07
Iter: 1717 loss: 6.02272507e-07
Iter: 1718 loss: 6.00774911e-07
Iter: 1719 loss: 6.00702435e-07
Iter: 1720 loss: 6.00587725e-07
Iter: 1721 loss: 6.00577e-07
Iter: 1722 loss: 6.00420492e-07
Iter: 1723 loss: 6.01430713e-07
Iter: 1724 loss: 6.00382464e-07
Iter: 1725 loss: 6.00359499e-07
Iter: 1726 loss: 6.0029106e-07
Iter: 1727 loss: 6.00249166e-07
Iter: 1728 loss: 6.00247176e-07
Iter: 1729 loss: 6.00217049e-07
Iter: 1730 loss: 6.00073292e-07
Iter: 1731 loss: 5.99993825e-07
Iter: 1732 loss: 5.99941359e-07
Iter: 1733 loss: 5.99749796e-07
Iter: 1734 loss: 6.00262069e-07
Iter: 1735 loss: 5.99759e-07
Iter: 1736 loss: 5.9965123e-07
Iter: 1737 loss: 5.99592966e-07
Iter: 1738 loss: 5.99513157e-07
Iter: 1739 loss: 5.99349619e-07
Iter: 1740 loss: 6.00241322e-07
Iter: 1741 loss: 5.993586e-07
Iter: 1742 loss: 5.99247073e-07
Iter: 1743 loss: 6.00057774e-07
Iter: 1744 loss: 5.99202e-07
Iter: 1745 loss: 5.99120256e-07
Iter: 1746 loss: 5.99917712e-07
Iter: 1747 loss: 5.99133386e-07
Iter: 1748 loss: 5.99055568e-07
Iter: 1749 loss: 5.99001851e-07
Iter: 1750 loss: 6.01076124e-07
Iter: 1751 loss: 5.99013845e-07
Iter: 1752 loss: 5.98879126e-07
Iter: 1753 loss: 5.99494456e-07
Iter: 1754 loss: 5.98828365e-07
Iter: 1755 loss: 5.98751114e-07
Iter: 1756 loss: 5.98912379e-07
Iter: 1757 loss: 5.98677047e-07
Iter: 1758 loss: 5.98567e-07
Iter: 1759 loss: 5.9872707e-07
Iter: 1760 loss: 5.98524821e-07
Iter: 1761 loss: 5.98591896e-07
Iter: 1762 loss: 5.98535e-07
Iter: 1763 loss: 5.98497763e-07
Iter: 1764 loss: 5.98455e-07
Iter: 1765 loss: 5.98525503e-07
Iter: 1766 loss: 5.98496058e-07
Iter: 1767 loss: 5.98495035e-07
Iter: 1768 loss: 5.98484e-07
Iter: 1769 loss: 5.98488896e-07
Iter: 1770 loss: 5.98513452e-07
Iter: 1771 loss: 5.98515044e-07
Iter: 1772 loss: 5.98509303e-07
Iter: 1773 loss: 5.9851925e-07
Iter: 1774 loss: 5.98513111e-07
Iter: 1775 loss: 5.98522433e-07
Iter: 1776 loss: 5.98501856e-07
Iter: 1777 loss: 5.98518625e-07
Iter: 1778 loss: 5.98517317e-07
Iter: 1779 loss: 5.98517943e-07
Iter: 1780 loss: 5.98521865e-07
Iter: 1781 loss: 5.98521808e-07
Iter: 1782 loss: 5.9852465e-07
Iter: 1783 loss: 5.98524707e-07
Iter: 1784 loss: 5.98524196e-07
Iter: 1785 loss: 5.98525276e-07
Iter: 1786 loss: 5.98525276e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/300_300_300_1
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0
+ date
Wed Oct 21 19:13:51 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1 --function f1 --psi 0 --phi 0 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c1cd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c1cf048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c11fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c11a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c222f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c17d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c170488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c0f0ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c0d5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c091400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c0a1e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c0228c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c038378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98bf87d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98bfc7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98bffd2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98bfc6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98bf4d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98bf091e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98bf24bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c1d3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98be8f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98befdb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98bedb620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c0580d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c069ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98be4e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98be43048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc983432a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc983456510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9833a4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9833619d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc983373488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc983391ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc983352950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9832f4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.003760194
test_loss: 0.0033838958
train_loss: 0.002045771
test_loss: 0.0016998034
train_loss: 0.001882595
test_loss: 0.0018523453
train_loss: 0.0016853306
test_loss: 0.0015993159
train_loss: 0.0016800937
test_loss: 0.0016026535
train_loss: 0.0014883955
test_loss: 0.0014901788
train_loss: 0.0013139767
test_loss: 0.0013508956
train_loss: 0.0014556849
test_loss: 0.001413743
train_loss: 0.0012715776
test_loss: 0.0014325437
train_loss: 0.0014117039
test_loss: 0.0016547071
train_loss: 0.0014187512
test_loss: 0.00178253
train_loss: 0.0014008495
test_loss: 0.0012423814
train_loss: 0.0012247124
test_loss: 0.0011594249
train_loss: 0.0012795815
test_loss: 0.0014776842
train_loss: 0.0011915899
test_loss: 0.0013826253
train_loss: 0.0014923451
test_loss: 0.0012739452
train_loss: 0.0013593031
test_loss: 0.0014704437
train_loss: 0.0014250306
test_loss: 0.0012891681
train_loss: 0.0015228679
test_loss: 0.0013063354
train_loss: 0.0015385287
test_loss: 0.0012237323
train_loss: 0.0014641662
test_loss: 0.0013254434
train_loss: 0.0013513787
test_loss: 0.0013271271
train_loss: 0.0013087296
test_loss: 0.0013126589
train_loss: 0.0013505481
test_loss: 0.0013116477
train_loss: 0.001207638
test_loss: 0.0013326647
train_loss: 0.0013327112
test_loss: 0.0012654457
train_loss: 0.001213344
test_loss: 0.001168182
train_loss: 0.0014055496
test_loss: 0.001336952
train_loss: 0.0013280455
test_loss: 0.0014524885
train_loss: 0.0013793997
test_loss: 0.001376338
train_loss: 0.0013716451
test_loss: 0.0012184858
train_loss: 0.0012462169
test_loss: 0.0013278812
train_loss: 0.0013059054
test_loss: 0.0013460976
train_loss: 0.0013350383
test_loss: 0.0015006395
train_loss: 0.0012023692
test_loss: 0.0014698289
train_loss: 0.0015687341
test_loss: 0.0012264894
train_loss: 0.0014151066
test_loss: 0.0012496948
train_loss: 0.0012411965
test_loss: 0.0013631034
train_loss: 0.0012938994
test_loss: 0.0014589325
train_loss: 0.001252619
test_loss: 0.0011309532
train_loss: 0.0014554712
test_loss: 0.0010368994
train_loss: 0.0013205563
test_loss: 0.0012538735
train_loss: 0.0010524761
test_loss: 0.0012680363
train_loss: 0.001253029
test_loss: 0.001373601
train_loss: 0.0016015962
test_loss: 0.00121809
train_loss: 0.001336894
test_loss: 0.0013625831
train_loss: 0.0011905866
test_loss: 0.001155957
train_loss: 0.0013541693
test_loss: 0.0012269587
train_loss: 0.0012350543
test_loss: 0.0015621341
train_loss: 0.0013142783
test_loss: 0.0013678605
train_loss: 0.0011914623
test_loss: 0.0012736394
train_loss: 0.0011346157
test_loss: 0.0011750176
train_loss: 0.0012016017
test_loss: 0.0012499363
train_loss: 0.0013768333
test_loss: 0.0013901917
train_loss: 0.0014715453
test_loss: 0.0012187358
train_loss: 0.0012225787
test_loss: 0.0012691427
train_loss: 0.0014919895
test_loss: 0.0012599519
train_loss: 0.0013063379
test_loss: 0.0010794222
train_loss: 0.0011151611
test_loss: 0.0014064
train_loss: 0.0013823251
test_loss: 0.001473049
train_loss: 0.0014132049
test_loss: 0.0014064119
train_loss: 0.0012645286
test_loss: 0.0012031425
train_loss: 0.0011430092
test_loss: 0.0013194423
train_loss: 0.0011213704
test_loss: 0.0014636447
train_loss: 0.001214107
test_loss: 0.0011791022
train_loss: 0.0016469473
test_loss: 0.0015419843
train_loss: 0.0015259963
test_loss: 0.0014426925
train_loss: 0.0019621209
test_loss: 0.0015100766
train_loss: 0.0016904243
test_loss: 0.0016152936
train_loss: 0.0018650147
test_loss: 0.001869192
train_loss: 0.001980772
test_loss: 0.0015349265
train_loss: 0.0015956343
test_loss: 0.0013980306
train_loss: 0.0016847446
test_loss: 0.0012601631
train_loss: 0.0014044036
test_loss: 0.0015497375
train_loss: 0.0012579104
test_loss: 0.0015357969
train_loss: 0.0011775296
test_loss: 0.001442458
train_loss: 0.0013950639
test_loss: 0.0014121669
train_loss: 0.001174942
test_loss: 0.0014722503
train_loss: 0.0013832266
test_loss: 0.0014030416
train_loss: 0.0013545356
test_loss: 0.0014241274
train_loss: 0.0014248678
test_loss: 0.0011829818
train_loss: 0.0012012731
test_loss: 0.0011480909
train_loss: 0.0012278374
test_loss: 0.0011695495
train_loss: 0.0013341089
test_loss: 0.0012332505
train_loss: 0.0014492215
test_loss: 0.0011064255
train_loss: 0.0013196131
test_loss: 0.0011875603
train_loss: 0.0012589573
test_loss: 0.0011729324
train_loss: 0.001682278
test_loss: 0.0011295679
train_loss: 0.0010861824
test_loss: 0.0011134072
train_loss: 0.0012485432
test_loss: 0.001311484
train_loss: 0.0012174393
test_loss: 0.0012955639
train_loss: 0.0011998718
test_loss: 0.0013576072
train_loss: 0.0012290295
test_loss: 0.0012805864
train_loss: 0.0012575493
test_loss: 0.0012809493
train_loss: 0.0011840881
test_loss: 0.0011578483
train_loss: 0.0012311249
test_loss: 0.0011439065
train_loss: 0.0012321795
test_loss: 0.001423112
train_loss: 0.0013049477
test_loss: 0.0013051287
train_loss: 0.0011244586
test_loss: 0.0011010163
train_loss: 0.001070676
test_loss: 0.0012596508
train_loss: 0.0014241373
test_loss: 0.0014688607
train_loss: 0.0010801678
test_loss: 0.0010101311
train_loss: 0.0011123485
test_loss: 0.0013334346
train_loss: 0.0011037239
test_loss: 0.0013262938
train_loss: 0.0013649231
test_loss: 0.0011694112
train_loss: 0.0012641487
test_loss: 0.0011173062
train_loss: 0.0010817104
test_loss: 0.0014446971
train_loss: 0.001213118
test_loss: 0.0014963028
train_loss: 0.001149884
test_loss: 0.0012173388
train_loss: 0.0012998751
test_loss: 0.0013232746
train_loss: 0.0012936345
test_loss: 0.0012988726
train_loss: 0.0013445943
test_loss: 0.0013120323
train_loss: 0.001136551
test_loss: 0.0012944394
train_loss: 0.0012185816
test_loss: 0.0013728978
train_loss: 0.0013322043
test_loss: 0.0011862086
train_loss: 0.0013317162
test_loss: 0.0011724136
train_loss: 0.0015402465
test_loss: 0.0011959808
train_loss: 0.0012299987
test_loss: 0.0011259863
train_loss: 0.0013096782
test_loss: 0.0012678324
train_loss: 0.0011829499
test_loss: 0.0011220062
train_loss: 0.0010934984
test_loss: 0.0012730275
train_loss: 0.0012503834
test_loss: 0.0012995433
train_loss: 0.0012423125
test_loss: 0.0010873856
train_loss: 0.0012305063
test_loss: 0.0013490254
train_loss: 0.0011260672
test_loss: 0.0011986874
train_loss: 0.0015073168
test_loss: 0.0012435456
train_loss: 0.0012941116
test_loss: 0.001179527
train_loss: 0.0013776965
test_loss: 0.0013761305
train_loss: 0.0011941819
test_loss: 0.0011573785
train_loss: 0.0012040832
test_loss: 0.0015332504
train_loss: 0.001162454
test_loss: 0.0012809712
train_loss: 0.0013649738
test_loss: 0.0012834608
train_loss: 0.00108995
test_loss: 0.0014020911
train_loss: 0.0011234159
test_loss: 0.00122476
train_loss: 0.0012028764
test_loss: 0.0013463441
train_loss: 0.0013262684
test_loss: 0.0012400685
train_loss: 0.0012908906
test_loss: 0.0011778119
train_loss: 0.0013481985
test_loss: 0.001332325
train_loss: 0.0013218012
test_loss: 0.0012067532
train_loss: 0.0013473363
test_loss: 0.0012916417
train_loss: 0.0011110817
test_loss: 0.0015175341
train_loss: 0.0015018769
test_loss: 0.0018803217
train_loss: 0.0018193325
test_loss: 0.0021911834
train_loss: 0.0017305131
test_loss: 0.0020198333
train_loss: 0.0017246624
test_loss: 0.0019339655
train_loss: 0.0017466168
test_loss: 0.0019997824
train_loss: 0.0015716031
test_loss: 0.0019788323
train_loss: 0.0014805081
test_loss: 0.0020377291
train_loss: 0.0016912132
test_loss: 0.0015908356
train_loss: 0.0015090784
test_loss: 0.001814583
train_loss: 0.0014657648
test_loss: 0.0015858219
train_loss: 0.001657879
test_loss: 0.0015823905
train_loss: 0.0012132865
test_loss: 0.001852782
train_loss: 0.0015739938
test_loss: 0.0017958182
train_loss: 0.0017700428
test_loss: 0.0016281774
train_loss: 0.0013363289
test_loss: 0.0015784615
train_loss: 0.0012988895
test_loss: 0.0012513454
train_loss: 0.0014546665
test_loss: 0.0012220904
train_loss: 0.0012160199
test_loss: 0.0013257193
train_loss: 0.001353442
test_loss: 0.0012855991
train_loss: 0.0012694108
test_loss: 0.0013193071
train_loss: 0.0013763331
test_loss: 0.0014380309
train_loss: 0.0013000772
test_loss: 0.0013919736
train_loss: 0.0013563945
test_loss: 0.0011400295
train_loss: 0.0011817524
test_loss: 0.0010963542
train_loss: 0.0011249267
test_loss: 0.0012577532
train_loss: 0.0012595875
test_loss: 0.0013866425
train_loss: 0.0011555827
test_loss: 0.0012025852
train_loss: 0.0012601009
test_loss: 0.0011695083
train_loss: 0.0011767105
test_loss: 0.0014668405
train_loss: 0.0011084786
test_loss: 0.0011071108
train_loss: 0.0011929167
test_loss: 0.0010991213
train_loss: 0.0011114597
test_loss: 0.001091749
train_loss: 0.0011285313
test_loss: 0.0011101676
train_loss: 0.0013903198
test_loss: 0.0012042216
train_loss: 0.0013007625
test_loss: 0.0012798236
train_loss: 0.00102774
test_loss: 0.0012093673
train_loss: 0.0012015107
test_loss: 0.0012221617
train_loss: 0.0012400982
test_loss: 0.001144519
train_loss: 0.001074196
test_loss: 0.0012680079
train_loss: 0.0011747385
test_loss: 0.001116526
train_loss: 0.001233038
test_loss: 0.0010845866
train_loss: 0.0014324924
test_loss: 0.0010580309
train_loss: 0.0010403667
test_loss: 0.0013490242
train_loss: 0.0011729536
test_loss: 0.0011905796
train_loss: 0.0012722552
test_loss: 0.0012741559
train_loss: 0.0010834928
test_loss: 0.0010946589
train_loss: 0.0010742715
test_loss: 0.0013504063
train_loss: 0.0011826901
test_loss: 0.0012349148
train_loss: 0.0011491836
test_loss: 0.001218941
train_loss: 0.0012300437
test_loss: 0.0011873557
train_loss: 0.0014588817
test_loss: 0.00107929
train_loss: 0.0011461215
test_loss: 0.0013790632
train_loss: 0.0013331186
test_loss: 0.0012864707
train_loss: 0.0013808523
test_loss: 0.0013320183
train_loss: 0.0013675039
test_loss: 0.001209647
train_loss: 0.0010790734
test_loss: 0.0012627101
train_loss: 0.0011415248
test_loss: 0.0012963631
train_loss: 0.0011631309
test_loss: 0.0012935636
train_loss: 0.0010367119
test_loss: 0.0010787535
train_loss: 0.0011168064
test_loss: 0.0010804216
train_loss: 0.0012398802
test_loss: 0.0012899558
train_loss: 0.0011091216
test_loss: 0.0011532817
train_loss: 0.0012696905
test_loss: 0.001093207
train_loss: 0.0011415359
test_loss: 0.0015007668
train_loss: 0.0012747259
test_loss: 0.0013175106
train_loss: 0.0011924068
test_loss: 0.0012380385
train_loss: 0.0012373389
test_loss: 0.0013042043
train_loss: 0.001110608
test_loss: 0.0011507052
train_loss: 0.00131458
test_loss: 0.0011980525
train_loss: 0.0011272314
test_loss: 0.0011800772
train_loss: 0.0013712656
test_loss: 0.0011936472
train_loss: 0.0014373516
test_loss: 0.0011670957
train_loss: 0.0014158748
test_loss: 0.0013034616
train_loss: 0.0011628994
test_loss: 0.001335781
train_loss: 0.0012562027
test_loss: 0.0013505834
train_loss: 0.0011906937
test_loss: 0.0011809258
train_loss: 0.0011224675
test_loss: 0.0013857818
train_loss: 0.0012084448
test_loss: 0.0014421254
train_loss: 0.0011524649
test_loss: 0.0014461977
train_loss: 0.0014359566
test_loss: 0.0020531253
train_loss: 0.001445042
test_loss: 0.0017462248
train_loss: 0.0012420017
test_loss: 0.0011985346
train_loss: 0.0016572163
test_loss: 0.0014247738
train_loss: 0.0019596766
test_loss: 0.0019099573
train_loss: 0.0020005398
test_loss: 0.0019473066
train_loss: 0.0017495067
test_loss: 0.0015762179
train_loss: 0.001796613
test_loss: 0.001575242
train_loss: 0.002131608
test_loss: 0.0018705268
train_loss: 0.0016611877
test_loss: 0.0014337837
train_loss: 0.0016812137
test_loss: 0.0015393951
train_loss: 0.001613912
test_loss: 0.0015636791
train_loss: 0.0016331271
test_loss: 0.001466498
train_loss: 0.001651208
test_loss: 0.0014381038
train_loss: 0.0016253908
test_loss: 0.0014596551
train_loss: 0.0017267474
test_loss: 0.0015302339
train_loss: 0.0016763081
test_loss: 0.0013785888
train_loss: 0.0015674296
test_loss: 0.0015496757
train_loss: 0.0017075918
test_loss: 0.0017387373
train_loss: 0.0011696476
test_loss: 0.0014951676
train_loss: 0.0012909113
test_loss: 0.0012251718
train_loss: 0.0012081146
test_loss: 0.0010902784
train_loss: 0.0011748408
test_loss: 0.0012472619
train_loss: 0.0014631443
test_loss: 0.0013641127
train_loss: 0.0012179561
test_loss: 0.0011655497
train_loss: 0.0010733202
test_loss: 0.001201419
train_loss: 0.0012658851
test_loss: 0.0012140897
train_loss: 0.001213236
test_loss: 0.0011718952
train_loss: 0.0011918704
test_loss: 0.0013067661
train_loss: 0.0013360344
test_loss: 0.0012150416
train_loss: 0.001255684
test_loss: 0.0011478773
train_loss: 0.001218365
test_loss: 0.001241575
train_loss: 0.0013038119
test_loss: 0.0012530399
train_loss: 0.0009673095
test_loss: 0.0012743067
train_loss: 0.0012781274
test_loss: 0.001204452
train_loss: 0.0012344361
test_loss: 0.0010573273
train_loss: 0.0012562771
test_loss: 0.0014003614
train_loss: 0.0011015633
test_loss: 0.0013241085
train_loss: 0.0011706668
test_loss: 0.0010992765
train_loss: 0.0011200142
test_loss: 0.0013372293
train_loss: 0.0012361619
test_loss: 0.0011634564
train_loss: 0.0011095345
test_loss: 0.0011956868
train_loss: 0.0011864454
test_loss: 0.0012044992
train_loss: 0.0009769782
test_loss: 0.0011300986
train_loss: 0.0012213939
test_loss: 0.0011270153
train_loss: 0.001223801
test_loss: 0.0012091274
train_loss: 0.0011196082
test_loss: 0.0015579868
train_loss: 0.0011453558
test_loss: 0.0013187763
train_loss: 0.0010710689
test_loss: 0.0012859849
train_loss: 0.0012884263
test_loss: 0.001292626
train_loss: 0.001031881
test_loss: 0.0011272266
train_loss: 0.0010772932
test_loss: 0.0011579843
train_loss: 0.0011568519
test_loss: 0.0011256173
train_loss: 0.0010293768
test_loss: 0.0014070824
train_loss: 0.0011762315
test_loss: 0.0011382959
train_loss: 0.0012631994
test_loss: 0.0012608758
train_loss: 0.0013682726
test_loss: 0.0012954844
train_loss: 0.0010743025
test_loss: 0.0013552499
train_loss: 0.0013288403
test_loss: 0.0012963206
train_loss: 0.0011819336
test_loss: 0.0011618776
train_loss: 0.0013185331
test_loss: 0.0013727306
train_loss: 0.0011078281
test_loss: 0.001201762
train_loss: 0.0012120785
test_loss: 0.0012767332
train_loss: 0.0015549902
test_loss: 0.0010881668
train_loss: 0.0011530735
test_loss: 0.0012389689
train_loss: 0.0015746988
test_loss: 0.0015948993
train_loss: 0.00181554
test_loss: 0.0014575073
train_loss: 0.0013453148
test_loss: 0.0013484801
train_loss: 0.0013560762
test_loss: 0.0016525757
train_loss: 0.0012997692
test_loss: 0.0016963739
train_loss: 0.0013584174
test_loss: 0.001578608
train_loss: 0.0018031365
test_loss: 0.0013211779
train_loss: 0.001965672
test_loss: 0.0017683976
train_loss: 0.0020942234
test_loss: 0.0019731913
train_loss: 0.0018517919
test_loss: 0.0015053033
train_loss: 0.0016545795
test_loss: 0.0014475706
train_loss: 0.0018814116
test_loss: 0.0016730838
train_loss: 0.001532237
test_loss: 0.0013928976
train_loss: 0.0018974936
test_loss: 0.0016528108
train_loss: 0.0016054768
test_loss: 0.0013740944
train_loss: 0.0013962993
test_loss: 0.001344402
train_loss: 0.0012288154
test_loss: 0.0011076402
train_loss: 0.0010077097
test_loss: 0.0013139545
train_loss: 0.0012451182
test_loss: 0.001283655
train_loss: 0.0010564784
test_loss: 0.0010385925
train_loss: 0.0011698197
test_loss: 0.0011552499
train_loss: 0.0010353515
test_loss: 0.0011384153
train_loss: 0.0010417795
test_loss: 0.0013592881
train_loss: 0.0012296089
test_loss: 0.0011464161
train_loss: 0.0010983745
test_loss: 0.0011942163
train_loss: 0.0011408424
test_loss: 0.0012687477
train_loss: 0.0011610532
test_loss: 0.0011728276
train_loss: 0.0011210272
test_loss: 0.0012661561
train_loss: 0.0010710856
test_loss: 0.0009935654
train_loss: 0.0012584132
test_loss: 0.0011754746
train_loss: 0.0012947958
test_loss: 0.0010749087
train_loss: 0.0012055745
test_loss: 0.0012187293
train_loss: 0.0013032451
test_loss: 0.0013595794
train_loss: 0.0010685453
test_loss: 0.0014250902
train_loss: 0.0011716314
test_loss: 0.0011703263
train_loss: 0.0012651468
test_loss: 0.0012233919
train_loss: 0.001388219
test_loss: 0.0010994561
train_loss: 0.0012474818
test_loss: 0.0012290066
train_loss: 0.0011047221
test_loss: 0.0012101746
train_loss: 0.0010353744
test_loss: 0.0010068938
train_loss: 0.001266305
test_loss: 0.0011922815
train_loss: 0.0013636497
test_loss: 0.0012858547
train_loss: 0.0010513007
test_loss: 0.0011084104
train_loss: 0.0011338934
test_loss: 0.0012841925
train_loss: 0.0010661545
test_loss: 0.0012915857
train_loss: 0.0012530973
test_loss: 0.0010774041
train_loss: 0.0011563438
test_loss: 0.0011538301
train_loss: 0.0011877196
test_loss: 0.001195155
train_loss: 0.001144771
test_loss: 0.001195965
train_loss: 0.0013133034
test_loss: 0.0011484327
train_loss: 0.001064782
test_loss: 0.0012169856
train_loss: 0.0012096759/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0012734998
train_loss: 0.0012537979
test_loss: 0.001365001
train_loss: 0.001327614
test_loss: 0.0012943385
train_loss: 0.001416452
test_loss: 0.0019237626
train_loss: 0.0012290461
test_loss: 0.0020997955
train_loss: 0.0012049718
test_loss: 0.0014331279
train_loss: 0.0017239986
test_loss: 0.0012720982
train_loss: 0.0020410584
test_loss: 0.0017664188
train_loss: 0.001988284
test_loss: 0.0015831445
train_loss: 0.0019612904
test_loss: 0.0018942852
train_loss: 0.0015693106
test_loss: 0.0014286125
train_loss: 0.0016525039
test_loss: 0.0015285893
train_loss: 0.0019040055
test_loss: 0.0017270499
train_loss: 0.0017951451
test_loss: 0.0015562036
train_loss: 0.0016726679
test_loss: 0.0015793246
train_loss: 0.0016245169
test_loss: 0.001355692
train_loss: 0.0017070187
test_loss: 0.0016944865
train_loss: 0.0014076753
test_loss: 0.0011768774
train_loss: 0.0012261011
test_loss: 0.0012422184
train_loss: 0.0011837755
test_loss: 0.0011608026
train_loss: 0.0011927887
test_loss: 0.0012622786
train_loss: 0.0012894021
test_loss: 0.0012631409
train_loss: 0.0010569418
test_loss: 0.0011888026
train_loss: 0.0010528918
test_loss: 0.0011536427
train_loss: 0.0011691961
test_loss: 0.0010455096
train_loss: 0.0009815887
test_loss: 0.0012278125
train_loss: 0.001073925
test_loss: 0.0011014293
train_loss: 0.0012419791
test_loss: 0.0010521584
train_loss: 0.0011216458
test_loss: 0.001097915
train_loss: 0.0017084304
test_loss: 0.0010835036
train_loss: 0.0014777795
test_loss: 0.0015466412
train_loss: 0.0013294187
test_loss: 0.0014250013
train_loss: 0.0010736982
test_loss: 0.0010258857
train_loss: 0.0012359638
test_loss: 0.001251309
train_loss: 0.0011374867
test_loss: 0.0010382438
train_loss: 0.001010829
test_loss: 0.0012656711
train_loss: 0.0011532467
test_loss: 0.00140388
train_loss: 0.0013502449
test_loss: 0.0013184845
train_loss: 0.0014170409
test_loss: 0.0011658504
train_loss: 0.00096036645
test_loss: 0.0012169231
train_loss: 0.001064267
test_loss: 0.0012155967
train_loss: 0.0011175042
test_loss: 0.0012421482
train_loss: 0.0011386506
test_loss: 0.0011510539
train_loss: 0.0013315765
test_loss: 0.0012595471
train_loss: 0.0011004552
test_loss: 0.0010688214
train_loss: 0.001156929
test_loss: 0.0011677876
train_loss: 0.0011926831
test_loss: 0.0012782445
train_loss: 0.0011960524
test_loss: 0.001217121
train_loss: 0.0012099398
test_loss: 0.001253706
train_loss: 0.0009856549
test_loss: 0.0011454123
train_loss: 0.0011189466
test_loss: 0.0010468124
train_loss: 0.0011048892
test_loss: 0.0012355802
train_loss: 0.0011510833
test_loss: 0.0014132062
train_loss: 0.0013079223
test_loss: 0.00127915
train_loss: 0.0011798448
test_loss: 0.0012474837
train_loss: 0.0012561032
test_loss: 0.0011533478
train_loss: 0.0011783394
test_loss: 0.0010365621
train_loss: 0.0011091925
test_loss: 0.0012272922
train_loss: 0.00105394
test_loss: 0.0011415601
train_loss: 0.0013601023
test_loss: 0.0012461608
train_loss: 0.0010946826
test_loss: 0.0013954894
train_loss: 0.0012915437
test_loss: 0.0012430572
train_loss: 0.0010584242
test_loss: 0.0011855135
train_loss: 0.0012556934
test_loss: 0.0013180323
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 0 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f17481a0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f174819b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f17480c9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f17481d2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f17481fef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f17481269d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1748130488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1748092ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f17480c1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f174806e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f174807ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f174802f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1730264378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1730261d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1730208840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f17302382f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f173023bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f17301fc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f17301a01e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f17301b2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f173014d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1730181158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f173010db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1730118620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f17300eb0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f17300feae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f17300b0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1730049048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1730065a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f173000d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1730024ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16e476f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16e4784488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16e478cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16e475b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16e46fa400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.45188085e-06
Iter: 2 loss: 1.85167698e-06
Iter: 3 loss: 1.31009051e-06
Iter: 4 loss: 1.10330802e-06
Iter: 5 loss: 9.44891212e-07
Iter: 6 loss: 8.78526521e-07
Iter: 7 loss: 7.66469043e-07
Iter: 8 loss: 1.20076106e-06
Iter: 9 loss: 7.40340056e-07
Iter: 10 loss: 6.31237754e-07
Iter: 11 loss: 1.67232156e-06
Iter: 12 loss: 6.27081533e-07
Iter: 13 loss: 6.04910042e-07
Iter: 14 loss: 6.50527795e-07
Iter: 15 loss: 5.960365e-07
Iter: 16 loss: 5.91705543e-07
Iter: 17 loss: 5.89378487e-07
Iter: 18 loss: 5.85591863e-07
Iter: 19 loss: 5.77109631e-07
Iter: 20 loss: 6.95133849e-07
Iter: 21 loss: 5.76634648e-07
Iter: 22 loss: 5.76413299e-07
Iter: 23 loss: 5.74898934e-07
Iter: 24 loss: 5.73090404e-07
Iter: 25 loss: 5.69401e-07
Iter: 26 loss: 6.3798791e-07
Iter: 27 loss: 5.69353119e-07
Iter: 28 loss: 5.65495384e-07
Iter: 29 loss: 5.63440835e-07
Iter: 30 loss: 5.61724846e-07
Iter: 31 loss: 5.5868486e-07
Iter: 32 loss: 5.56749e-07
Iter: 33 loss: 5.55580073e-07
Iter: 34 loss: 5.51279072e-07
Iter: 35 loss: 5.42086184e-07
Iter: 36 loss: 6.88006594e-07
Iter: 37 loss: 5.41790882e-07
Iter: 38 loss: 5.35906509e-07
Iter: 39 loss: 6.062088e-07
Iter: 40 loss: 5.35808567e-07
Iter: 41 loss: 5.32327135e-07
Iter: 42 loss: 5.3928e-07
Iter: 43 loss: 5.30924353e-07
Iter: 44 loss: 5.28392491e-07
Iter: 45 loss: 5.33329171e-07
Iter: 46 loss: 5.27362886e-07
Iter: 47 loss: 5.2557948e-07
Iter: 48 loss: 5.25285941e-07
Iter: 49 loss: 5.24238544e-07
Iter: 50 loss: 5.23085873e-07
Iter: 51 loss: 5.22913524e-07
Iter: 52 loss: 5.21887614e-07
Iter: 53 loss: 5.34866558e-07
Iter: 54 loss: 5.21886932e-07
Iter: 55 loss: 5.20567312e-07
Iter: 56 loss: 5.18819661e-07
Iter: 57 loss: 5.18751676e-07
Iter: 58 loss: 5.17748276e-07
Iter: 59 loss: 5.27982195e-07
Iter: 60 loss: 5.17690296e-07
Iter: 61 loss: 5.16951729e-07
Iter: 62 loss: 5.2359394e-07
Iter: 63 loss: 5.16923535e-07
Iter: 64 loss: 5.16328384e-07
Iter: 65 loss: 5.1482391e-07
Iter: 66 loss: 5.2588689e-07
Iter: 67 loss: 5.14492228e-07
Iter: 68 loss: 5.14943849e-07
Iter: 69 loss: 5.13821306e-07
Iter: 70 loss: 5.13307043e-07
Iter: 71 loss: 5.11676831e-07
Iter: 72 loss: 5.1571152e-07
Iter: 73 loss: 5.10761708e-07
Iter: 74 loss: 5.08388553e-07
Iter: 75 loss: 5.20508934e-07
Iter: 76 loss: 5.08000596e-07
Iter: 77 loss: 5.06899937e-07
Iter: 78 loss: 5.07019536e-07
Iter: 79 loss: 5.0603461e-07
Iter: 80 loss: 5.05293201e-07
Iter: 81 loss: 5.05274443e-07
Iter: 82 loss: 5.0471931e-07
Iter: 83 loss: 5.04636262e-07
Iter: 84 loss: 5.04255297e-07
Iter: 85 loss: 5.03908e-07
Iter: 86 loss: 5.0377173e-07
Iter: 87 loss: 5.03517811e-07
Iter: 88 loss: 5.03106207e-07
Iter: 89 loss: 5.03093588e-07
Iter: 90 loss: 5.02573698e-07
Iter: 91 loss: 5.0267397e-07
Iter: 92 loss: 5.02210924e-07
Iter: 93 loss: 5.01735258e-07
Iter: 94 loss: 5.02202624e-07
Iter: 95 loss: 5.01476279e-07
Iter: 96 loss: 5.01058139e-07
Iter: 97 loss: 5.02375087e-07
Iter: 98 loss: 5.00944111e-07
Iter: 99 loss: 5.00472709e-07
Iter: 100 loss: 5.03962497e-07
Iter: 101 loss: 5.00441558e-07
Iter: 102 loss: 4.99985617e-07
Iter: 103 loss: 4.99391e-07
Iter: 104 loss: 4.99358123e-07
Iter: 105 loss: 4.98944189e-07
Iter: 106 loss: 5.02202226e-07
Iter: 107 loss: 4.98917871e-07
Iter: 108 loss: 4.98296174e-07
Iter: 109 loss: 4.97757355e-07
Iter: 110 loss: 4.97599274e-07
Iter: 111 loss: 4.96903681e-07
Iter: 112 loss: 4.97651854e-07
Iter: 113 loss: 4.96559551e-07
Iter: 114 loss: 4.96032271e-07
Iter: 115 loss: 4.96522148e-07
Iter: 116 loss: 4.95755e-07
Iter: 117 loss: 4.95418419e-07
Iter: 118 loss: 4.97356041e-07
Iter: 119 loss: 4.9535555e-07
Iter: 120 loss: 4.95023642e-07
Iter: 121 loss: 4.97460121e-07
Iter: 122 loss: 4.94988171e-07
Iter: 123 loss: 4.9472078e-07
Iter: 124 loss: 4.98016277e-07
Iter: 125 loss: 4.94709241e-07
Iter: 126 loss: 4.94593394e-07
Iter: 127 loss: 4.94169512e-07
Iter: 128 loss: 4.94225446e-07
Iter: 129 loss: 4.93775e-07
Iter: 130 loss: 4.93980451e-07
Iter: 131 loss: 4.93442371e-07
Iter: 132 loss: 4.93264963e-07
Iter: 133 loss: 4.93075277e-07
Iter: 134 loss: 4.93042535e-07
Iter: 135 loss: 4.92765366e-07
Iter: 136 loss: 4.96268967e-07
Iter: 137 loss: 4.92784181e-07
Iter: 138 loss: 4.92583354e-07
Iter: 139 loss: 4.92227684e-07
Iter: 140 loss: 5.01076443e-07
Iter: 141 loss: 4.92232118e-07
Iter: 142 loss: 4.91938863e-07
Iter: 143 loss: 4.93852951e-07
Iter: 144 loss: 4.91904871e-07
Iter: 145 loss: 4.9157768e-07
Iter: 146 loss: 4.93023435e-07
Iter: 147 loss: 4.9152294e-07
Iter: 148 loss: 4.91246851e-07
Iter: 149 loss: 4.90706839e-07
Iter: 150 loss: 4.90711443e-07
Iter: 151 loss: 4.90281195e-07
Iter: 152 loss: 4.92531058e-07
Iter: 153 loss: 4.90192519e-07
Iter: 154 loss: 4.89828835e-07
Iter: 155 loss: 4.90415687e-07
Iter: 156 loss: 4.8965137e-07
Iter: 157 loss: 4.89838158e-07
Iter: 158 loss: 4.8959123e-07
Iter: 159 loss: 4.89502213e-07
Iter: 160 loss: 4.89317245e-07
Iter: 161 loss: 4.91840751e-07
Iter: 162 loss: 4.89310651e-07
Iter: 163 loss: 4.89168315e-07
Iter: 164 loss: 4.89606634e-07
Iter: 165 loss: 4.891e-07
Iter: 166 loss: 4.88954413e-07
Iter: 167 loss: 4.90970535e-07
Iter: 168 loss: 4.88937076e-07
Iter: 169 loss: 4.88820888e-07
Iter: 170 loss: 4.88379783e-07
Iter: 171 loss: 4.90372031e-07
Iter: 172 loss: 4.88238186e-07
Iter: 173 loss: 4.88281785e-07
Iter: 174 loss: 4.88016326e-07
Iter: 175 loss: 4.878749e-07
Iter: 176 loss: 4.8756209e-07
Iter: 177 loss: 4.9309017e-07
Iter: 178 loss: 4.87547481e-07
Iter: 179 loss: 4.87161117e-07
Iter: 180 loss: 4.87687544e-07
Iter: 181 loss: 4.86952445e-07
Iter: 182 loss: 4.8716629e-07
Iter: 183 loss: 4.86849672e-07
Iter: 184 loss: 4.86765e-07
Iter: 185 loss: 4.86564318e-07
Iter: 186 loss: 4.88833621e-07
Iter: 187 loss: 4.86527597e-07
Iter: 188 loss: 4.86297949e-07
Iter: 189 loss: 4.86947556e-07
Iter: 190 loss: 4.86233716e-07
Iter: 191 loss: 4.86047497e-07
Iter: 192 loss: 4.85750775e-07
Iter: 193 loss: 4.85723149e-07
Iter: 194 loss: 4.85935e-07
Iter: 195 loss: 4.85590363e-07
Iter: 196 loss: 4.85490773e-07
Iter: 197 loss: 4.85316377e-07
Iter: 198 loss: 4.85320925e-07
Iter: 199 loss: 4.85181772e-07
Iter: 200 loss: 4.85185637e-07
Iter: 201 loss: 4.8504e-07
Iter: 202 loss: 4.84840655e-07
Iter: 203 loss: 4.84839916e-07
Iter: 204 loss: 4.84741918e-07
Iter: 205 loss: 4.84598e-07
Iter: 206 loss: 4.88142859e-07
Iter: 207 loss: 4.84588838e-07
Iter: 208 loss: 4.84391194e-07
Iter: 209 loss: 4.87850684e-07
Iter: 210 loss: 4.84384429e-07
Iter: 211 loss: 4.84243742e-07
Iter: 212 loss: 4.83972883e-07
Iter: 213 loss: 4.8976483e-07
Iter: 214 loss: 4.83989311e-07
Iter: 215 loss: 4.83730219e-07
Iter: 216 loss: 4.84603675e-07
Iter: 217 loss: 4.83658823e-07
Iter: 218 loss: 4.83382109e-07
Iter: 219 loss: 4.83384724e-07
Iter: 220 loss: 4.8327405e-07
Iter: 221 loss: 4.83086694e-07
Iter: 222 loss: 4.86997578e-07
Iter: 223 loss: 4.83089707e-07
Iter: 224 loss: 4.82899452e-07
Iter: 225 loss: 4.83281667e-07
Iter: 226 loss: 4.82838743e-07
Iter: 227 loss: 4.82702887e-07
Iter: 228 loss: 4.83972144e-07
Iter: 229 loss: 4.82691348e-07
Iter: 230 loss: 4.82530197e-07
Iter: 231 loss: 4.83313386e-07
Iter: 232 loss: 4.82485802e-07
Iter: 233 loss: 4.82396217e-07
Iter: 234 loss: 4.82174528e-07
Iter: 235 loss: 4.86557667e-07
Iter: 236 loss: 4.82166115e-07
Iter: 237 loss: 4.81979782e-07
Iter: 238 loss: 4.81978759e-07
Iter: 239 loss: 4.81756047e-07
Iter: 240 loss: 4.81431471e-07
Iter: 241 loss: 4.81413849e-07
Iter: 242 loss: 4.81281234e-07
Iter: 243 loss: 4.81261566e-07
Iter: 244 loss: 4.81061647e-07
Iter: 245 loss: 4.80990593e-07
Iter: 246 loss: 4.80907886e-07
Iter: 247 loss: 4.8073548e-07
Iter: 248 loss: 4.80753499e-07
Iter: 249 loss: 4.80599965e-07
Iter: 250 loss: 4.80434664e-07
Iter: 251 loss: 4.81901e-07
Iter: 252 loss: 4.80433357e-07
Iter: 253 loss: 4.80257199e-07
Iter: 254 loss: 4.8037e-07
Iter: 255 loss: 4.80121685e-07
Iter: 256 loss: 4.79954224e-07
Iter: 257 loss: 4.79849916e-07
Iter: 258 loss: 4.79818141e-07
Iter: 259 loss: 4.79596e-07
Iter: 260 loss: 4.79841674e-07
Iter: 261 loss: 4.79476057e-07
Iter: 262 loss: 4.79321e-07
Iter: 263 loss: 4.80981157e-07
Iter: 264 loss: 4.79322807e-07
Iter: 265 loss: 4.79242431e-07
Iter: 266 loss: 4.79241635e-07
Iter: 267 loss: 4.79152845e-07
Iter: 268 loss: 4.79010168e-07
Iter: 269 loss: 4.81638608e-07
Iter: 270 loss: 4.79012556e-07
Iter: 271 loss: 4.78878064e-07
Iter: 272 loss: 4.78716061e-07
Iter: 273 loss: 4.78715663e-07
Iter: 274 loss: 4.78562924e-07
Iter: 275 loss: 4.78505967e-07
Iter: 276 loss: 4.78409618e-07
Iter: 277 loss: 4.7809209e-07
Iter: 278 loss: 4.7971281e-07
Iter: 279 loss: 4.77998242e-07
Iter: 280 loss: 4.77685546e-07
Iter: 281 loss: 4.8048264e-07
Iter: 282 loss: 4.77658432e-07
Iter: 283 loss: 4.77616823e-07
Iter: 284 loss: 4.77554863e-07
Iter: 285 loss: 4.7750018e-07
Iter: 286 loss: 4.77347044e-07
Iter: 287 loss: 4.78083336e-07
Iter: 288 loss: 4.77317712e-07
Iter: 289 loss: 4.77226365e-07
Iter: 290 loss: 4.78719357e-07
Iter: 291 loss: 4.77201581e-07
Iter: 292 loss: 4.77134336e-07
Iter: 293 loss: 4.78261029e-07
Iter: 294 loss: 4.77134392e-07
Iter: 295 loss: 4.77087951e-07
Iter: 296 loss: 4.76925493e-07
Iter: 297 loss: 4.7734278e-07
Iter: 298 loss: 4.7684307e-07
Iter: 299 loss: 4.76638718e-07
Iter: 300 loss: 4.78734421e-07
Iter: 301 loss: 4.76648864e-07
Iter: 302 loss: 4.76495075e-07
Iter: 303 loss: 4.78384379e-07
Iter: 304 loss: 4.76499821e-07
Iter: 305 loss: 4.7639179e-07
Iter: 306 loss: 4.76131248e-07
Iter: 307 loss: 4.7832458e-07
Iter: 308 loss: 4.7608188e-07
Iter: 309 loss: 4.75853255e-07
Iter: 310 loss: 4.76568317e-07
Iter: 311 loss: 4.75773675e-07
Iter: 312 loss: 4.75671015e-07
Iter: 313 loss: 4.75661892e-07
Iter: 314 loss: 4.75523962e-07
Iter: 315 loss: 4.75288573e-07
Iter: 316 loss: 4.75286356e-07
Iter: 317 loss: 4.75084931e-07
Iter: 318 loss: 4.74671481e-07
Iter: 319 loss: 4.83403539e-07
Iter: 320 loss: 4.74680547e-07
Iter: 321 loss: 4.74352817e-07
Iter: 322 loss: 4.74349434e-07
Iter: 323 loss: 4.74236202e-07
Iter: 324 loss: 4.74197719e-07
Iter: 325 loss: 4.74105263e-07
Iter: 326 loss: 4.73964121e-07
Iter: 327 loss: 4.73985949e-07
Iter: 328 loss: 4.73881954e-07
Iter: 329 loss: 4.74426145e-07
Iter: 330 loss: 4.73869477e-07
Iter: 331 loss: 4.73787452e-07
Iter: 332 loss: 4.74698339e-07
Iter: 333 loss: 4.73771621e-07
Iter: 334 loss: 4.73736975e-07
Iter: 335 loss: 4.73623913e-07
Iter: 336 loss: 4.75628411e-07
Iter: 337 loss: 4.7361732e-07
Iter: 338 loss: 4.73487916e-07
Iter: 339 loss: 4.73553882e-07
Iter: 340 loss: 4.73412427e-07
Iter: 341 loss: 4.73232205e-07
Iter: 342 loss: 4.75375913e-07
Iter: 343 loss: 4.73240959e-07
Iter: 344 loss: 4.73110219e-07
Iter: 345 loss: 4.72872784e-07
Iter: 346 loss: 4.76700052e-07
Iter: 347 loss: 4.72855334e-07
Iter: 348 loss: 4.72693358e-07
Iter: 349 loss: 4.72688782e-07
Iter: 350 loss: 4.72470788e-07
Iter: 351 loss: 4.72663402e-07
Iter: 352 loss: 4.72386944e-07
Iter: 353 loss: 4.72255124e-07
Iter: 354 loss: 4.7219288e-07
Iter: 355 loss: 4.72150816e-07
Iter: 356 loss: 4.71972783e-07
Iter: 357 loss: 4.72021441e-07
Iter: 358 loss: 4.71831044e-07
Iter: 359 loss: 4.71690868e-07
Iter: 360 loss: 4.7168561e-07
Iter: 361 loss: 4.71548901e-07
Iter: 362 loss: 4.72652118e-07
Iter: 363 loss: 4.71527699e-07
Iter: 364 loss: 4.71458264e-07
Iter: 365 loss: 4.71380076e-07
Iter: 366 loss: 4.7136686e-07
Iter: 367 loss: 4.71338353e-07
Iter: 368 loss: 4.71311182e-07
Iter: 369 loss: 4.71295635e-07
Iter: 370 loss: 4.7117743e-07
Iter: 371 loss: 4.72307619e-07
Iter: 372 loss: 4.71173479e-07
Iter: 373 loss: 4.71112912e-07
Iter: 374 loss: 4.72062197e-07
Iter: 375 loss: 4.71109075e-07
Iter: 376 loss: 4.71036913e-07
Iter: 377 loss: 4.71321528e-07
Iter: 378 loss: 4.71031e-07
Iter: 379 loss: 4.70963698e-07
Iter: 380 loss: 4.70868201e-07
Iter: 381 loss: 4.73061789e-07
Iter: 382 loss: 4.70858595e-07
Iter: 383 loss: 4.70762757e-07
Iter: 384 loss: 4.72520753e-07
Iter: 385 loss: 4.70755367e-07
Iter: 386 loss: 4.70656772e-07
Iter: 387 loss: 4.70473935e-07
Iter: 388 loss: 4.70480302e-07
Iter: 389 loss: 4.7025074e-07
Iter: 390 loss: 4.70105334e-07
Iter: 391 loss: 4.70038941e-07
Iter: 392 loss: 4.69778399e-07
Iter: 393 loss: 4.71558565e-07
Iter: 394 loss: 4.69772516e-07
Iter: 395 loss: 4.69626343e-07
Iter: 396 loss: 4.7105317e-07
Iter: 397 loss: 4.69615458e-07
Iter: 398 loss: 4.69500492e-07
Iter: 399 loss: 4.69493983e-07
Iter: 400 loss: 4.69450441e-07
Iter: 401 loss: 4.69298e-07
Iter: 402 loss: 4.70124803e-07
Iter: 403 loss: 4.69215564e-07
Iter: 404 loss: 4.69157158e-07
Iter: 405 loss: 4.69114184e-07
Iter: 406 loss: 4.69036394e-07
Iter: 407 loss: 4.69015902e-07
Iter: 408 loss: 4.68952351e-07
Iter: 409 loss: 4.6886862e-07
Iter: 410 loss: 4.68629196e-07
Iter: 411 loss: 4.72553779e-07
Iter: 412 loss: 4.68651507e-07
Iter: 413 loss: 4.6873069e-07
Iter: 414 loss: 4.68558596e-07
Iter: 415 loss: 4.68493681e-07
Iter: 416 loss: 4.68427e-07
Iter: 417 loss: 4.68405943e-07
Iter: 418 loss: 4.68342421e-07
Iter: 419 loss: 4.68909519e-07
Iter: 420 loss: 4.6832406e-07
Iter: 421 loss: 4.68229473e-07
Iter: 422 loss: 4.68069942e-07
Iter: 423 loss: 4.68077673e-07
Iter: 424 loss: 4.67908762e-07
Iter: 425 loss: 4.67867096e-07
Iter: 426 loss: 4.67744485e-07
Iter: 427 loss: 4.67579667e-07
Iter: 428 loss: 4.67270468e-07
Iter: 429 loss: 4.74244416e-07
Iter: 430 loss: 4.67292693e-07
Iter: 431 loss: 4.67063956e-07
Iter: 432 loss: 4.67858598e-07
Iter: 433 loss: 4.66997335e-07
Iter: 434 loss: 4.66801055e-07
Iter: 435 loss: 4.678e-07
Iter: 436 loss: 4.66789402e-07
Iter: 437 loss: 4.66705444e-07
Iter: 438 loss: 4.6686776e-07
Iter: 439 loss: 4.66656559e-07
Iter: 440 loss: 4.66628308e-07
Iter: 441 loss: 4.66612448e-07
Iter: 442 loss: 4.6656578e-07
Iter: 443 loss: 4.66434216e-07
Iter: 444 loss: 4.67921268e-07
Iter: 445 loss: 4.66456498e-07
Iter: 446 loss: 4.6636373e-07
Iter: 447 loss: 4.66398944e-07
Iter: 448 loss: 4.66306403e-07
Iter: 449 loss: 4.66163044e-07
Iter: 450 loss: 4.66158156e-07
Iter: 451 loss: 4.66067064e-07
Iter: 452 loss: 4.65912109e-07
Iter: 453 loss: 4.69295742e-07
Iter: 454 loss: 4.65932715e-07
Iter: 455 loss: 4.65904861e-07
Iter: 456 loss: 4.65825082e-07
Iter: 457 loss: 4.65784524e-07
Iter: 458 loss: 4.65664812e-07
Iter: 459 loss: 4.66342e-07
Iter: 460 loss: 4.65623231e-07
Iter: 461 loss: 4.65487744e-07
Iter: 462 loss: 4.66388883e-07
Iter: 463 loss: 4.65458584e-07
Iter: 464 loss: 4.65389803e-07
Iter: 465 loss: 4.65356749e-07
Iter: 466 loss: 4.65312922e-07
Iter: 467 loss: 4.65219443e-07
Iter: 468 loss: 4.66248025e-07
Iter: 469 loss: 4.65223764e-07
Iter: 470 loss: 4.65102346e-07
Iter: 471 loss: 4.65119427e-07
Iter: 472 loss: 4.65034361e-07
Iter: 473 loss: 4.64913256e-07
Iter: 474 loss: 4.66459198e-07
Iter: 475 loss: 4.64894185e-07
Iter: 476 loss: 4.64871789e-07
Iter: 477 loss: 4.64859454e-07
Iter: 478 loss: 4.64836745e-07
Iter: 479 loss: 4.64761285e-07
Iter: 480 loss: 4.64723513e-07
Iter: 481 loss: 4.64645922e-07
Iter: 482 loss: 4.64548719e-07
Iter: 483 loss: 4.64551448e-07
Iter: 484 loss: 4.64438529e-07
Iter: 485 loss: 4.64560628e-07
Iter: 486 loss: 4.64391121e-07
Iter: 487 loss: 4.64272603e-07
Iter: 488 loss: 4.64107785e-07
Iter: 489 loss: 4.64127254e-07
Iter: 490 loss: 4.64056654e-07
Iter: 491 loss: 4.64031302e-07
Iter: 492 loss: 4.63925062e-07
Iter: 493 loss: 4.63846362e-07
Iter: 494 loss: 4.63845339e-07
Iter: 495 loss: 4.63719374e-07
Iter: 496 loss: 4.64804458e-07
Iter: 497 loss: 4.63708432e-07
Iter: 498 loss: 4.63613588e-07
Iter: 499 loss: 4.63859692e-07
Iter: 500 loss: 4.63567574e-07
Iter: 501 loss: 4.63524827e-07
Iter: 502 loss: 4.63647837e-07
Iter: 503 loss: 4.63516471e-07
Iter: 504 loss: 4.63484412e-07
Iter: 505 loss: 4.63363364e-07
Iter: 506 loss: 4.64101674e-07
Iter: 507 loss: 4.63353899e-07
Iter: 508 loss: 4.63193516e-07
Iter: 509 loss: 4.6427121e-07
Iter: 510 loss: 4.63198944e-07
Iter: 511 loss: 4.63082756e-07
Iter: 512 loss: 4.63086167e-07
Iter: 513 loss: 4.62969695e-07
Iter: 514 loss: 4.62746499e-07
Iter: 515 loss: 4.66938161e-07
Iter: 516 loss: 4.6276088e-07
Iter: 517 loss: 4.62536889e-07
Iter: 518 loss: 4.62260175e-07
Iter: 519 loss: 4.62250227e-07
Iter: 520 loss: 4.62182982e-07
Iter: 521 loss: 4.62096864e-07
Iter: 522 loss: 4.620006e-07
Iter: 523 loss: 4.62355473e-07
Iter: 524 loss: 4.62006369e-07
Iter: 525 loss: 4.61870087e-07
Iter: 526 loss: 4.61972917e-07
Iter: 527 loss: 4.61807105e-07
Iter: 528 loss: 4.61705326e-07
Iter: 529 loss: 4.62305877e-07
Iter: 530 loss: 4.61691457e-07
Iter: 531 loss: 4.61627337e-07
Iter: 532 loss: 4.62085069e-07
Iter: 533 loss: 4.6164547e-07
Iter: 534 loss: 4.61582431e-07
Iter: 535 loss: 4.61502e-07
Iter: 536 loss: 4.62831338e-07
Iter: 537 loss: 4.61512286e-07
Iter: 538 loss: 4.61440152e-07
Iter: 539 loss: 4.61417898e-07
Iter: 540 loss: 4.61368387e-07
Iter: 541 loss: 4.61491595e-07
Iter: 542 loss: 4.61329421e-07
Iter: 543 loss: 4.61290199e-07
Iter: 544 loss: 4.6129918e-07
Iter: 545 loss: 4.61259816e-07
Iter: 546 loss: 4.61188847e-07
Iter: 547 loss: 4.61649705e-07
Iter: 548 loss: 4.611598e-07
Iter: 549 loss: 4.61134618e-07
Iter: 550 loss: 4.61019113e-07
Iter: 551 loss: 4.63035917e-07
Iter: 552 loss: 4.61003481e-07
Iter: 553 loss: 4.60861685e-07
Iter: 554 loss: 4.60789e-07
Iter: 555 loss: 4.60726767e-07
Iter: 556 loss: 4.60673078e-07
Iter: 557 loss: 4.60616263e-07
Iter: 558 loss: 4.60554475e-07
Iter: 559 loss: 4.6099629e-07
Iter: 560 loss: 4.60532902e-07
Iter: 561 loss: 4.60481715e-07
Iter: 562 loss: 4.60354272e-07
Iter: 563 loss: 4.60360411e-07
Iter: 564 loss: 4.60251698e-07
Iter: 565 loss: 4.60259145e-07
Iter: 566 loss: 4.60200425e-07
Iter: 567 loss: 4.60204262e-07
Iter: 568 loss: 4.6012471e-07
Iter: 569 loss: 4.60046294e-07
Iter: 570 loss: 4.59954435e-07
Iter: 571 loss: 4.59950968e-07
Iter: 572 loss: 4.59907028e-07
Iter: 573 loss: 4.59843307e-07
Iter: 574 loss: 4.59809144e-07
Iter: 575 loss: 4.59756137e-07
Iter: 576 loss: 4.59755398e-07
Iter: 577 loss: 4.5971592e-07
Iter: 578 loss: 4.60449314e-07
Iter: 579 loss: 4.59726607e-07
Iter: 580 loss: 4.59660782e-07
Iter: 581 loss: 4.59623948e-07
Iter: 582 loss: 4.59619855e-07
Iter: 583 loss: 4.595436e-07
Iter: 584 loss: 4.59601495e-07
Iter: 585 loss: 4.59518219e-07
Iter: 586 loss: 4.59453048e-07
Iter: 587 loss: 4.59362411e-07
Iter: 588 loss: 4.61797526e-07
Iter: 589 loss: 4.59345813e-07
Iter: 590 loss: 4.59279875e-07
Iter: 591 loss: 4.59283143e-07
Iter: 592 loss: 4.59170792e-07
Iter: 593 loss: 4.5905162e-07
Iter: 594 loss: 4.59036528e-07
Iter: 595 loss: 4.58957089e-07
Iter: 596 loss: 4.58954702e-07
Iter: 597 loss: 4.58878048e-07
Iter: 598 loss: 4.58857016e-07
Iter: 599 loss: 4.58820551e-07
Iter: 600 loss: 4.58738214e-07
Iter: 601 loss: 4.58706523e-07
Iter: 602 loss: 4.58661134e-07
Iter: 603 loss: 4.58657951e-07
Iter: 604 loss: 4.58628e-07
Iter: 605 loss: 4.5858792e-07
Iter: 606 loss: 4.58585731e-07
Iter: 607 loss: 4.58540967e-07
Iter: 608 loss: 4.58492735e-07
Iter: 609 loss: 4.58535965e-07
Iter: 610 loss: 4.58480912e-07
Iter: 611 loss: 4.58382061e-07
Iter: 612 loss: 4.58805e-07
Iter: 613 loss: 4.5838442e-07
Iter: 614 loss: 4.58373165e-07
Iter: 615 loss: 4.58315185e-07
Iter: 616 loss: 4.58312684e-07
Iter: 617 loss: 4.58233643e-07
Iter: 618 loss: 4.58163811e-07
Iter: 619 loss: 4.58147156e-07
Iter: 620 loss: 4.58071071e-07
Iter: 621 loss: 4.58903827e-07
Iter: 622 loss: 4.58061834e-07
Iter: 623 loss: 4.58007946e-07
Iter: 624 loss: 4.58091193e-07
Iter: 625 loss: 4.57963495e-07
Iter: 626 loss: 4.57872034e-07
Iter: 627 loss: 4.58442287e-07
Iter: 628 loss: 4.57814906e-07
Iter: 629 loss: 4.57788076e-07
Iter: 630 loss: 4.57710712e-07
Iter: 631 loss: 4.58305067e-07
Iter: 632 loss: 4.57664157e-07
Iter: 633 loss: 4.57569513e-07
Iter: 634 loss: 4.57554506e-07
Iter: 635 loss: 4.57496725e-07
Iter: 636 loss: 4.57490955e-07
Iter: 637 loss: 4.57440365e-07
Iter: 638 loss: 4.57348733e-07
Iter: 639 loss: 4.58207637e-07
Iter: 640 loss: 4.57387898e-07
Iter: 641 loss: 4.57313519e-07
Iter: 642 loss: 4.57334806e-07
Iter: 643 loss: 4.57279384e-07
Iter: 644 loss: 4.5720455e-07
Iter: 645 loss: 4.57329065e-07
Iter: 646 loss: 4.57174878e-07
Iter: 647 loss: 4.57170046e-07
Iter: 648 loss: 4.57166408e-07
Iter: 649 loss: 4.5712062e-07
Iter: 650 loss: 4.57084695e-07
Iter: 651 loss: 4.58071526e-07
Iter: 652 loss: 4.57059514e-07
Iter: 653 loss: 4.57031092e-07
Iter: 654 loss: 4.57083189e-07
Iter: 655 loss: 4.57013641e-07
Iter: 656 loss: 4.56909561e-07
Iter: 657 loss: 4.56872129e-07
Iter: 658 loss: 4.56822306e-07
Iter: 659 loss: 4.5683592e-07
Iter: 660 loss: 4.567909e-07
Iter: 661 loss: 4.56734455e-07
Iter: 662 loss: 4.5661e-07
Iter: 663 loss: 4.5794971e-07
Iter: 664 loss: 4.56614089e-07
Iter: 665 loss: 4.56466466e-07
Iter: 666 loss: 4.56730277e-07
Iter: 667 loss: 4.5639274e-07
Iter: 668 loss: 4.5631279e-07
Iter: 669 loss: 4.57073838e-07
Iter: 670 loss: 4.56291673e-07
Iter: 671 loss: 4.56201718e-07
Iter: 672 loss: 4.56745681e-07
Iter: 673 loss: 4.56184296e-07
Iter: 674 loss: 4.56116737e-07
Iter: 675 loss: 4.56036275e-07
Iter: 676 loss: 4.56024793e-07
Iter: 677 loss: 4.559964e-07
Iter: 678 loss: 4.55965761e-07
Iter: 679 loss: 4.55938846e-07
Iter: 680 loss: 4.55852302e-07
Iter: 681 loss: 4.57202617e-07
Iter: 682 loss: 4.55845026e-07
Iter: 683 loss: 4.55784971e-07
Iter: 684 loss: 4.55782839e-07
Iter: 685 loss: 4.55725939e-07
Iter: 686 loss: 4.55715792e-07
Iter: 687 loss: 4.55679526e-07
Iter: 688 loss: 4.55661876e-07
Iter: 689 loss: 4.5556024e-07
Iter: 690 loss: 4.55577492e-07
Iter: 691 loss: 4.55490664e-07
Iter: 692 loss: 4.55483757e-07
Iter: 693 loss: 4.55416796e-07
Iter: 694 loss: 4.55674666e-07
Iter: 695 loss: 4.55412419e-07
Iter: 696 loss: 4.5535711e-07
Iter: 697 loss: 4.55190616e-07
Iter: 698 loss: 4.56122649e-07
Iter: 699 loss: 4.55164667e-07
Iter: 700 loss: 4.54952044e-07
Iter: 701 loss: 4.55946832e-07
Iter: 702 loss: 4.54889175e-07
Iter: 703 loss: 4.54808969e-07
Iter: 704 loss: 4.54803569e-07
Iter: 705 loss: 4.54701279e-07
Iter: 706 loss: 4.55127775e-07
Iter: 707 loss: 4.54689371e-07
Iter: 708 loss: 4.54652422e-07
Iter: 709 loss: 4.54575655e-07
Iter: 710 loss: 4.5602269e-07
Iter: 711 loss: 4.54587848e-07
Iter: 712 loss: 4.54569687e-07
Iter: 713 loss: 4.54534955e-07
Iter: 714 loss: 4.54535041e-07
Iter: 715 loss: 4.54457108e-07
Iter: 716 loss: 4.54906512e-07
Iter: 717 loss: 4.54465976e-07
Iter: 718 loss: 4.54382132e-07
Iter: 719 loss: 4.5439225e-07
Iter: 720 loss: 4.54353824e-07
Iter: 721 loss: 4.54281292e-07
Iter: 722 loss: 4.54431756e-07
Iter: 723 loss: 4.54225074e-07
Iter: 724 loss: 4.54274812e-07
Iter: 725 loss: 4.54225386e-07
Iter: 726 loss: 4.54190854e-07
Iter: 727 loss: 4.54158567e-07
Iter: 728 loss: 4.54143134e-07
Iter: 729 loss: 4.54076797e-07
Iter: 730 loss: 4.54427749e-07
Iter: 731 loss: 4.54082112e-07
Iter: 732 loss: 4.54014696e-07
Iter: 733 loss: 4.53948786e-07
Iter: 734 loss: 4.53920336e-07
Iter: 735 loss: 4.53779194e-07
Iter: 736 loss: 4.54440567e-07
Iter: 737 loss: 4.53773566e-07
Iter: 738 loss: 4.53707798e-07
Iter: 739 loss: 4.53548751e-07
Iter: 740 loss: 4.5355614e-07
Iter: 741 loss: 4.53318222e-07
Iter: 742 loss: 4.54269525e-07
Iter: 743 loss: 4.53297872e-07
Iter: 744 loss: 4.53181258e-07
Iter: 745 loss: 4.53197231e-07
Iter: 746 loss: 4.53126802e-07
Iter: 747 loss: 4.5370291e-07
Iter: 748 loss: 4.53117366e-07
Iter: 749 loss: 4.53080332e-07
Iter: 750 loss: 4.53009307e-07
Iter: 751 loss: 4.54037178e-07
Iter: 752 loss: 4.52984409e-07
Iter: 753 loss: 4.52925377e-07
Iter: 754 loss: 4.53074477e-07
Iter: 755 loss: 4.5290605e-07
Iter: 756 loss: 4.52831415e-07
Iter: 757 loss: 4.52931687e-07
Iter: 758 loss: 4.52801316e-07
Iter: 759 loss: 4.52733161e-07
Iter: 760 loss: 4.53106139e-07
Iter: 761 loss: 4.52720741e-07
Iter: 762 loss: 4.52683366e-07
Iter: 763 loss: 4.52685356e-07
Iter: 764 loss: 4.52642979e-07
Iter: 765 loss: 4.52561324e-07
Iter: 766 loss: 4.53846923e-07
Iter: 767 loss: 4.52555383e-07
Iter: 768 loss: 4.52516616e-07
Iter: 769 loss: 4.52501723e-07
Iter: 770 loss: 4.52459204e-07
Iter: 771 loss: 4.52434392e-07
Iter: 772 loss: 4.52427088e-07
Iter: 773 loss: 4.52365668e-07
Iter: 774 loss: 4.52839572e-07
Iter: 775 loss: 4.5235663e-07
Iter: 776 loss: 4.52309e-07
Iter: 777 loss: 4.52553792e-07
Iter: 778 loss: 4.52320961e-07
Iter: 779 loss: 4.52248685e-07
Iter: 780 loss: 4.52231689e-07
Iter: 781 loss: 4.52190648e-07
Iter: 782 loss: 4.52105667e-07
Iter: 783 loss: 4.52254767e-07
Iter: 784 loss: 4.52056554e-07
Iter: 785 loss: 4.52058856e-07
Iter: 786 loss: 4.52057634e-07
Iter: 787 loss: 4.52044958e-07
Iter: 788 loss: 4.52005622e-07
Iter: 789 loss: 4.51953895e-07
Iter: 790 loss: 4.51944913e-07
Iter: 791 loss: 4.51898643e-07
Iter: 792 loss: 4.51980583e-07
Iter: 793 loss: 4.51885541e-07
Iter: 794 loss: 4.51853225e-07
Iter: 795 loss: 4.52107287e-07
Iter: 796 loss: 4.51850894e-07
Iter: 797 loss: 4.51822245e-07
Iter: 798 loss: 4.51893271e-07
Iter: 799 loss: 4.51757927e-07
Iter: 800 loss: 4.51728113e-07
Iter: 801 loss: 4.51684571e-07
Iter: 802 loss: 4.51692074e-07
Iter: 803 loss: 4.51596947e-07
Iter: 804 loss: 4.52386416e-07
Iter: 805 loss: 4.51608969e-07
Iter: 806 loss: 4.51544764e-07
Iter: 807 loss: 4.5144472e-07
Iter: 808 loss: 4.53548694e-07
Iter: 809 loss: 4.51440826e-07
Iter: 810 loss: 4.51387336e-07
Iter: 811 loss: 4.5137628e-07
Iter: 812 loss: 4.51308836e-07
Iter: 813 loss: 4.51383244e-07
Iter: 814 loss: 4.51281892e-07
Iter: 815 loss: 4.51233632e-07
Iter: 816 loss: 4.51200293e-07
Iter: 817 loss: 4.51164794e-07
Iter: 818 loss: 4.51083849e-07
Iter: 819 loss: 4.51326e-07
Iter: 820 loss: 4.51048209e-07
Iter: 821 loss: 4.51013136e-07
Iter: 822 loss: 4.51014841e-07
Iter: 823 loss: 4.50988409e-07
Iter: 824 loss: 4.50878559e-07
Iter: 825 loss: 4.51455264e-07
Iter: 826 loss: 4.508438e-07
Iter: 827 loss: 4.50747905e-07
Iter: 828 loss: 4.52086795e-07
Iter: 829 loss: 4.50736678e-07
Iter: 830 loss: 4.50698963e-07
Iter: 831 loss: 4.50705272e-07
Iter: 832 loss: 4.50672616e-07
Iter: 833 loss: 4.50585901e-07
Iter: 834 loss: 4.52170752e-07
Iter: 835 loss: 4.50575271e-07
Iter: 836 loss: 4.50555603e-07
Iter: 837 loss: 4.50556399e-07
Iter: 838 loss: 4.50534827e-07
Iter: 839 loss: 4.50569985e-07
Iter: 840 loss: 4.50530877e-07
Iter: 841 loss: 4.50508509e-07
Iter: 842 loss: 4.50496287e-07
Iter: 843 loss: 4.50464285e-07
Iter: 844 loss: 4.50388313e-07
Iter: 845 loss: 4.50609377e-07
Iter: 846 loss: 4.50342611e-07
Iter: 847 loss: 4.50288297e-07
Iter: 848 loss: 4.50335932e-07
Iter: 849 loss: 4.50252685e-07
Iter: 850 loss: 4.50165203e-07
Iter: 851 loss: 4.50106199e-07
Iter: 852 loss: 4.5010313e-07
Iter: 853 loss: 4.50043103e-07
Iter: 854 loss: 4.50035145e-07
Iter: 855 loss: 4.49961874e-07
Iter: 856 loss: 4.50113674e-07
Iter: 857 loss: 4.49921657e-07
Iter: 858 loss: 4.49916598e-07
Iter: 859 loss: 4.49859556e-07
Iter: 860 loss: 4.49877291e-07
Iter: 861 loss: 4.49828462e-07
Iter: 862 loss: 4.4983156e-07
Iter: 863 loss: 4.49803849e-07
Iter: 864 loss: 4.49739645e-07
Iter: 865 loss: 4.49750104e-07
Iter: 866 loss: 4.4970227e-07
Iter: 867 loss: 4.49766e-07
Iter: 868 loss: 4.49687e-07
Iter: 869 loss: 4.49614902e-07
Iter: 870 loss: 4.49813911e-07
Iter: 871 loss: 4.49612287e-07
Iter: 872 loss: 4.49542881e-07
Iter: 873 loss: 4.49554904e-07
Iter: 874 loss: 4.49519149e-07
Iter: 875 loss: 4.49467166e-07
Iter: 876 loss: 4.49466967e-07
Iter: 877 loss: 4.49446304e-07
Iter: 878 loss: 4.49393241e-07
Iter: 879 loss: 4.49407111e-07
Iter: 880 loss: 4.49369224e-07
Iter: 881 loss: 4.49566244e-07
Iter: 882 loss: 4.49341258e-07
Iter: 883 loss: 4.49311756e-07
Iter: 884 loss: 4.49358311e-07
Iter: 885 loss: 4.49301069e-07
Iter: 886 loss: 4.49228139e-07
Iter: 887 loss: 4.49965171e-07
Iter: 888 loss: 4.4922831e-07
Iter: 889 loss: 4.49196079e-07
Iter: 890 loss: 4.49156431e-07
Iter: 891 loss: 4.4913557e-07
Iter: 892 loss: 4.49079408e-07
Iter: 893 loss: 4.49086485e-07
Iter: 894 loss: 4.49030097e-07
Iter: 895 loss: 4.48969161e-07
Iter: 896 loss: 4.48976948e-07
Iter: 897 loss: 4.48907059e-07
Iter: 898 loss: 4.49310789e-07
Iter: 899 loss: 4.48911948e-07
Iter: 900 loss: 4.48886738e-07
Iter: 901 loss: 4.48889494e-07
Iter: 902 loss: 4.48854223e-07
Iter: 903 loss: 4.48823783e-07
Iter: 904 loss: 4.49183517e-07
Iter: 905 loss: 4.48831486e-07
Iter: 906 loss: 4.48782117e-07
Iter: 907 loss: 4.48783851e-07
Iter: 908 loss: 4.48762052e-07
Iter: 909 loss: 4.4894108e-07
Iter: 910 loss: 4.48757561e-07
Iter: 911 loss: 4.4872138e-07
Iter: 912 loss: 4.48668629e-07
Iter: 913 loss: 4.49016824e-07
Iter: 914 loss: 4.4865709e-07
Iter: 915 loss: 4.48528652e-07
Iter: 916 loss: 4.48915046e-07
Iter: 917 loss: 4.48513788e-07
Iter: 918 loss: 4.48462259e-07
Iter: 919 loss: 4.48462941e-07
Iter: 920 loss: 4.48404307e-07
Iter: 921 loss: 4.48373896e-07
Iter: 922 loss: 4.48352125e-07
Iter: 923 loss: 4.48297186e-07
Iter: 924 loss: 4.48639184e-07
Iter: 925 loss: 4.48282435e-07
Iter: 926 loss: 4.48226103e-07
Iter: 927 loss: 4.48597632e-07
Iter: 928 loss: 4.4825515e-07
Iter: 929 loss: 4.4818853e-07
Iter: 930 loss: 4.48163092e-07
Iter: 931 loss: 4.4817773e-07
Iter: 932 loss: 4.48096614e-07
Iter: 933 loss: 4.4842551e-07
Iter: 934 loss: 4.48089679e-07
Iter: 935 loss: 4.48032154e-07
Iter: 936 loss: 4.48192e-07
Iter: 937 loss: 4.47994637e-07
Iter: 938 loss: 4.47993528e-07
Iter: 939 loss: 4.47923327e-07
Iter: 940 loss: 4.47917614e-07
Iter: 941 loss: 4.47917955e-07
Iter: 942 loss: 4.47893541e-07
Iter: 943 loss: 4.47864181e-07
Iter: 944 loss: 4.47856678e-07
Iter: 945 loss: 4.47820867e-07
Iter: 946 loss: 4.47804638e-07
Iter: 947 loss: 4.4782621e-07
Iter: 948 loss: 4.47775562e-07
Iter: 949 loss: 4.47729832e-07
Iter: 950 loss: 4.47722442e-07
Iter: 951 loss: 4.4771096e-07
Iter: 952 loss: 4.47620579e-07
Iter: 953 loss: 4.48420337e-07
Iter: 954 loss: 4.47649853e-07
Iter: 955 loss: 4.47602076e-07
Iter: 956 loss: 4.47522666e-07
Iter: 957 loss: 4.49045785e-07
Iter: 958 loss: 4.47514822e-07
Iter: 959 loss: 4.47434701e-07
Iter: 960 loss: 4.48669624e-07
Iter: 961 loss: 4.47437628e-07
Iter: 962 loss: 4.47383684e-07
Iter: 963 loss: 4.47391415e-07
Iter: 964 loss: 4.47311578e-07
Iter: 965 loss: 4.47256639e-07
Iter: 966 loss: 4.47432114e-07
Iter: 967 loss: 4.47249022e-07
Iter: 968 loss: 4.47175324e-07
Iter: 969 loss: 4.47804581e-07
Iter: 970 loss: 4.47174955e-07
Iter: 971 loss: 4.47145339e-07
Iter: 972 loss: 4.47117e-07
Iter: 973 loss: 4.47085966e-07
Iter: 974 loss: 4.47068828e-07
Iter: 975 loss: 4.47369075e-07
Iter: 976 loss: 4.47054958e-07
Iter: 977 loss: 4.4704575e-07
Iter: 978 loss: 4.47020682e-07
Iter: 979 loss: 4.46997859e-07
Iter: 980 loss: 4.46943631e-07
Iter: 981 loss: 4.46939197e-07
Iter: 982 loss: 4.46889118e-07
Iter: 983 loss: 4.46849839e-07
Iter: 984 loss: 4.47105634e-07
Iter: 985 loss: 4.46842279e-07
Iter: 986 loss: 4.46802517e-07
Iter: 987 loss: 4.47099779e-07
Iter: 988 loss: 4.46803966e-07
Iter: 989 loss: 4.46760822e-07
Iter: 990 loss: 4.46763408e-07
Iter: 991 loss: 4.46723647e-07
Iter: 992 loss: 4.46666746e-07
Iter: 993 loss: 4.46840886e-07
Iter: 994 loss: 4.46669588e-07
Iter: 995 loss: 4.46631361e-07
Iter: 996 loss: 4.46826846e-07
Iter: 997 loss: 4.46637671e-07
Iter: 998 loss: 4.46592821e-07
Iter: 999 loss: 4.46529583e-07
Iter: 1000 loss: 4.47217673e-07
Iter: 1001 loss: 4.46514889e-07
Iter: 1002 loss: 4.4646282e-07
Iter: 1003 loss: 4.46437298e-07
Iter: 1004 loss: 4.46405636e-07
Iter: 1005 loss: 4.46436985e-07
Iter: 1006 loss: 4.4636846e-07
Iter: 1007 loss: 4.46313862e-07
Iter: 1008 loss: 4.46175221e-07
Iter: 1009 loss: 4.49063606e-07
Iter: 1010 loss: 4.46165842e-07
Iter: 1011 loss: 4.46272e-07
Iter: 1012 loss: 4.4614049e-07
Iter: 1013 loss: 4.46103286e-07
Iter: 1014 loss: 4.46038428e-07
Iter: 1015 loss: 4.4683074e-07
Iter: 1016 loss: 4.46044169e-07
Iter: 1017 loss: 4.46001366e-07
Iter: 1018 loss: 4.46333e-07
Iter: 1019 loss: 4.4598417e-07
Iter: 1020 loss: 4.45950235e-07
Iter: 1021 loss: 4.45987212e-07
Iter: 1022 loss: 4.45941879e-07
Iter: 1023 loss: 4.45903339e-07
Iter: 1024 loss: 4.46057413e-07
Iter: 1025 loss: 4.45900071e-07
Iter: 1026 loss: 4.45855932e-07
Iter: 1027 loss: 4.45777744e-07
Iter: 1028 loss: 4.45774361e-07
Iter: 1029 loss: 4.4568975e-07
Iter: 1030 loss: 4.4576359e-07
Iter: 1031 loss: 4.45643252e-07
Iter: 1032 loss: 4.45587773e-07
Iter: 1033 loss: 4.46020692e-07
Iter: 1034 loss: 4.45589421e-07
Iter: 1035 loss: 4.45523142e-07
Iter: 1036 loss: 4.45430885e-07
Iter: 1037 loss: 4.45423694e-07
Iter: 1038 loss: 4.45344483e-07
Iter: 1039 loss: 4.45848627e-07
Iter: 1040 loss: 4.45340305e-07
Iter: 1041 loss: 4.45277379e-07
Iter: 1042 loss: 4.45357898e-07
Iter: 1043 loss: 4.45242136e-07
Iter: 1044 loss: 4.45175658e-07
Iter: 1045 loss: 4.45150619e-07
Iter: 1046 loss: 4.45094372e-07
Iter: 1047 loss: 4.45019481e-07
Iter: 1048 loss: 4.4540775e-07
Iter: 1049 loss: 4.45029883e-07
Iter: 1050 loss: 4.44949222e-07
Iter: 1051 loss: 4.45493185e-07
Iter: 1052 loss: 4.44935097e-07
Iter: 1053 loss: 4.44919436e-07
Iter: 1054 loss: 4.44947375e-07
Iter: 1055 loss: 4.4492387e-07
Iter: 1056 loss: 4.44872057e-07
Iter: 1057 loss: 4.44904487e-07
Iter: 1058 loss: 4.44868192e-07
Iter: 1059 loss: 4.44824366e-07
Iter: 1060 loss: 4.44943424e-07
Iter: 1061 loss: 4.4482573e-07
Iter: 1062 loss: 4.44783069e-07
Iter: 1063 loss: 4.44940497e-07
Iter: 1064 loss: 4.44753596e-07
Iter: 1065 loss: 4.44740238e-07
Iter: 1066 loss: 4.4467825e-07
Iter: 1067 loss: 4.45096191e-07
Iter: 1068 loss: 4.44683565e-07
Iter: 1069 loss: 4.44625584e-07
Iter: 1070 loss: 4.44616717e-07
Iter: 1071 loss: 4.44555269e-07
Iter: 1072 loss: 4.44736912e-07
Iter: 1073 loss: 4.44546117e-07
Iter: 1074 loss: 4.44509141e-07
Iter: 1075 loss: 4.44454628e-07
Iter: 1076 loss: 4.46083561e-07
Iter: 1077 loss: 4.44456646e-07
Iter: 1078 loss: 4.44407789e-07
Iter: 1079 loss: 4.44409409e-07
Iter: 1080 loss: 4.44379367e-07
Iter: 1081 loss: 4.44382181e-07
Iter: 1082 loss: 4.44355209e-07
Iter: 1083 loss: 4.44327384e-07
Iter: 1084 loss: 4.44516e-07
Iter: 1085 loss: 4.44304e-07
Iter: 1086 loss: 4.44273724e-07
Iter: 1087 loss: 4.4420193e-07
Iter: 1088 loss: 4.44187776e-07
Iter: 1089 loss: 4.44185673e-07
Iter: 1090 loss: 4.4417439e-07
Iter: 1091 loss: 4.44159e-07
Iter: 1092 loss: 4.4412019e-07
Iter: 1093 loss: 4.44129284e-07
Iter: 1094 loss: 4.44095889e-07
Iter: 1095 loss: 4.4409245e-07
Iter: 1096 loss: 4.44075425e-07
Iter: 1097 loss: 4.44155631e-07
Iter: 1098 loss: 4.44055615e-07
Iter: 1099 loss: 4.44042058e-07
Iter: 1100 loss: 4.44067155e-07
Iter: 1101 loss: 4.44034129e-07
Iter: 1102 loss: 4.44003177e-07
Iter: 1103 loss: 4.44122293e-07
Iter: 1104 loss: 4.44008634e-07
Iter: 1105 loss: 4.43976603e-07
Iter: 1106 loss: 4.43954974e-07
Iter: 1107 loss: 4.43949205e-07
Iter: 1108 loss: 4.43926552e-07
Iter: 1109 loss: 4.43834097e-07
Iter: 1110 loss: 4.4526297e-07
Iter: 1111 loss: 4.43831368e-07
Iter: 1112 loss: 4.43765856e-07
Iter: 1113 loss: 4.43756676e-07
Iter: 1114 loss: 4.43713e-07
Iter: 1115 loss: 4.43727913e-07
Iter: 1116 loss: 4.43690283e-07
Iter: 1117 loss: 4.43639351e-07
Iter: 1118 loss: 4.43939541e-07
Iter: 1119 loss: 4.43629858e-07
Iter: 1120 loss: 4.4356679e-07
Iter: 1121 loss: 4.43537374e-07
Iter: 1122 loss: 4.43511567e-07
Iter: 1123 loss: 4.43462227e-07
Iter: 1124 loss: 4.43467684e-07
Iter: 1125 loss: 4.43455747e-07
Iter: 1126 loss: 4.43484112e-07
Iter: 1127 loss: 4.43444407e-07
Iter: 1128 loss: 4.43404218e-07
Iter: 1129 loss: 4.43603739e-07
Iter: 1130 loss: 4.43404844e-07
Iter: 1131 loss: 4.43396971e-07
Iter: 1132 loss: 4.43375711e-07
Iter: 1133 loss: 4.4336258e-07
Iter: 1134 loss: 4.43324353e-07
Iter: 1135 loss: 4.43295363e-07
Iter: 1136 loss: 4.4326876e-07
Iter: 1137 loss: 4.4317045e-07
Iter: 1138 loss: 4.43267226e-07
Iter: 1139 loss: 4.43122474e-07
Iter: 1140 loss: 4.43064948e-07
Iter: 1141 loss: 4.43247018e-07
Iter: 1142 loss: 4.43058553e-07
Iter: 1143 loss: 4.43031922e-07
Iter: 1144 loss: 4.43016802e-07
Iter: 1145 loss: 4.42980195e-07
Iter: 1146 loss: 4.43116363e-07
Iter: 1147 loss: 4.42956946e-07
Iter: 1148 loss: 4.42927274e-07
Iter: 1149 loss: 4.42854486e-07
Iter: 1150 loss: 4.43335836e-07
Iter: 1151 loss: 4.42828252e-07
Iter: 1152 loss: 4.42687963e-07
Iter: 1153 loss: 4.42713201e-07
Iter: 1154 loss: 4.42663691e-07
Iter: 1155 loss: 4.42737587e-07
Iter: 1156 loss: 4.42642431e-07
Iter: 1157 loss: 4.42615431e-07
Iter: 1158 loss: 4.4282595e-07
Iter: 1159 loss: 4.42612446e-07
Iter: 1160 loss: 4.42555e-07
Iter: 1161 loss: 4.42621399e-07
Iter: 1162 loss: 4.42535679e-07
Iter: 1163 loss: 4.42503847e-07
Iter: 1164 loss: 4.42644e-07
Iter: 1165 loss: 4.42498e-07
Iter: 1166 loss: 4.42449021e-07
Iter: 1167 loss: 4.42498873e-07
Iter: 1168 loss: 4.42423215e-07
Iter: 1169 loss: 4.4240295e-07
Iter: 1170 loss: 4.42311e-07
Iter: 1171 loss: 4.43421186e-07
Iter: 1172 loss: 4.42314331e-07
Iter: 1173 loss: 4.42280822e-07
Iter: 1174 loss: 4.42389535e-07
Iter: 1175 loss: 4.42259648e-07
Iter: 1176 loss: 4.42195017e-07
Iter: 1177 loss: 4.42119983e-07
Iter: 1178 loss: 4.42096166e-07
Iter: 1179 loss: 4.42038584e-07
Iter: 1180 loss: 4.43029563e-07
Iter: 1181 loss: 4.42033127e-07
Iter: 1182 loss: 4.41997713e-07
Iter: 1183 loss: 4.41985918e-07
Iter: 1184 loss: 4.41965284e-07
Iter: 1185 loss: 4.41939306e-07
Iter: 1186 loss: 4.419432e-07
Iter: 1187 loss: 4.41853615e-07
Iter: 1188 loss: 4.41856173e-07
Iter: 1189 loss: 4.4184992e-07
Iter: 1190 loss: 4.41763063e-07
Iter: 1191 loss: 4.42448965e-07
Iter: 1192 loss: 4.41800921e-07
Iter: 1193 loss: 4.4176511e-07
Iter: 1194 loss: 4.4183011e-07
Iter: 1195 loss: 4.41743509e-07
Iter: 1196 loss: 4.41705851e-07
Iter: 1197 loss: 4.41829116e-07
Iter: 1198 loss: 4.41717816e-07
Iter: 1199 loss: 4.41667908e-07
Iter: 1200 loss: 4.41715571e-07
Iter: 1201 loss: 4.41656539e-07
Iter: 1202 loss: 4.41632e-07
Iter: 1203 loss: 4.41908412e-07
Iter: 1204 loss: 4.41622262e-07
Iter: 1205 loss: 4.41604413e-07
Iter: 1206 loss: 4.41677571e-07
Iter: 1207 loss: 4.41608279e-07
Iter: 1208 loss: 4.41618226e-07
Iter: 1209 loss: 4.41618113e-07
Iter: 1210 loss: 4.41606659e-07
Iter: 1211 loss: 4.41592562e-07
Iter: 1212 loss: 4.41583524e-07
Iter: 1213 loss: 4.41593244e-07
Iter: 1214 loss: 4.41595944e-07
Iter: 1215 loss: 4.41598161e-07
Iter: 1216 loss: 4.41609188e-07
Iter: 1217 loss: 4.41597535e-07
Iter: 1218 loss: 4.41600747e-07
Iter: 1219 loss: 4.4160339e-07
Iter: 1220 loss: 4.41607483e-07
Iter: 1221 loss: 4.41607796e-07
Iter: 1222 loss: 4.41609615e-07
Iter: 1223 loss: 4.4160393e-07
Iter: 1224 loss: 4.4160916e-07
Iter: 1225 loss: 4.41607597e-07
Iter: 1226 loss: 4.4160879e-07
Iter: 1227 loss: 4.41607426e-07
Iter: 1228 loss: 4.41609131e-07
Iter: 1229 loss: 4.41607511e-07
Iter: 1230 loss: 4.41607511e-07
Iter: 1231 loss: 4.41607511e-07
Iter: 1232 loss: 4.41607511e-07
Iter: 1233 loss: 4.41609131e-07
Iter: 1234 loss: 4.41607511e-07
Iter: 1235 loss: 4.41558143e-07
Iter: 1236 loss: 4.42432366e-07
Iter: 1237 loss: 4.41544643e-07
Iter: 1238 loss: 4.41525401e-07
Iter: 1239 loss: 4.41534581e-07
Iter: 1240 loss: 4.41494223e-07
Iter: 1241 loss: 4.41479131e-07
Iter: 1242 loss: 4.41433258e-07
Iter: 1243 loss: 4.41383747e-07
Iter: 1244 loss: 4.41324062e-07
Iter: 1245 loss: 4.41299392e-07
Iter: 1246 loss: 4.41282623e-07
Iter: 1247 loss: 4.41270345e-07
Iter: 1248 loss: 4.41213416e-07
Iter: 1249 loss: 4.41253e-07
Iter: 1250 loss: 4.41190849e-07
Iter: 1251 loss: 4.41137502e-07
Iter: 1252 loss: 4.41138496e-07
Iter: 1253 loss: 4.41106e-07
Iter: 1254 loss: 4.41044733e-07
Iter: 1255 loss: 4.4104695e-07
Iter: 1256 loss: 4.41046154e-07
Iter: 1257 loss: 4.41017391e-07
Iter: 1258 loss: 4.40997837e-07
Iter: 1259 loss: 4.40976919e-07
Iter: 1260 loss: 4.41093505e-07
Iter: 1261 loss: 4.40952903e-07
Iter: 1262 loss: 4.40930819e-07
Iter: 1263 loss: 4.40968847e-07
Iter: 1264 loss: 4.40914732e-07
Iter: 1265 loss: 4.40876732e-07
Iter: 1266 loss: 4.40849135e-07
Iter: 1267 loss: 4.40831315e-07
Iter: 1268 loss: 4.40778393e-07
Iter: 1269 loss: 4.41023985e-07
Iter: 1270 loss: 4.40759379e-07
Iter: 1271 loss: 4.40729792e-07
Iter: 1272 loss: 4.40733572e-07
Iter: 1273 loss: 4.40703189e-07
Iter: 1274 loss: 4.40627417e-07
Iter: 1275 loss: 4.40638985e-07
Iter: 1276 loss: 4.40586973e-07
Iter: 1277 loss: 4.40550025e-07
Iter: 1278 loss: 4.40547495e-07
Iter: 1279 loss: 4.40439607e-07
Iter: 1280 loss: 4.40609881e-07
Iter: 1281 loss: 4.40380489e-07
Iter: 1282 loss: 4.40422809e-07
Iter: 1283 loss: 4.40356189e-07
Iter: 1284 loss: 4.40338965e-07
Iter: 1285 loss: 4.40290052e-07
Iter: 1286 loss: 4.40629833e-07
Iter: 1287 loss: 4.40293604e-07
Iter: 1288 loss: 4.40269218e-07
Iter: 1289 loss: 4.40274e-07
Iter: 1290 loss: 4.40249494e-07
Iter: 1291 loss: 4.40295082e-07
Iter: 1292 loss: 4.40230679e-07
Iter: 1293 loss: 4.40214961e-07
Iter: 1294 loss: 4.40263619e-07
Iter: 1295 loss: 4.40236022e-07
Iter: 1296 loss: 4.4023804e-07
Iter: 1297 loss: 4.40241706e-07
Iter: 1298 loss: 4.40223e-07
Iter: 1299 loss: 4.40222607e-07
Iter: 1300 loss: 4.40235453e-07
Iter: 1301 loss: 4.40241763e-07
Iter: 1302 loss: 4.40238466e-07
Iter: 1303 loss: 4.40237017e-07
Iter: 1304 loss: 4.40241536e-07
Iter: 1305 loss: 4.40235567e-07
Iter: 1306 loss: 4.40229201e-07
Iter: 1307 loss: 4.40231332e-07
Iter: 1308 loss: 4.40221754e-07
Iter: 1309 loss: 4.40230167e-07
Iter: 1310 loss: 4.40229343e-07
Iter: 1311 loss: 4.40233208e-07
Iter: 1312 loss: 4.40226188e-07
Iter: 1313 loss: 4.40228064e-07
Iter: 1314 loss: 4.40229428e-07
Iter: 1315 loss: 4.40233e-07
Iter: 1316 loss: 4.40231474e-07
Iter: 1317 loss: 4.4023102e-07
Iter: 1318 loss: 4.40231588e-07
Iter: 1319 loss: 4.40230934e-07
Iter: 1320 loss: 4.40231588e-07
Iter: 1321 loss: 4.40231588e-07
Iter: 1322 loss: 4.40230934e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4
+ date
Wed Oct 21 21:21:24 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/500_500_500_500_1 --function f1 --psi 0 --phi 0.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf70737ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf707cc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf7069f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf7075e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf706ebf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf707a0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf707a0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf7065e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf70600b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf706009d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf705c71e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf7050a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf7052a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf7065e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf704ef950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf705eae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf704ef400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf7055b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf7055b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf704617b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf7045a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf703c40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf7045a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf70362c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf70362bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf70362620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf70362510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf703b88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf703b8e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf70339f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf703361e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf703367b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf70237840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf702b6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf701c6ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf701f6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.054004863
test_loss: 0.053331073
train_loss: 0.0316386
test_loss: 0.03179666
train_loss: 0.011338393
test_loss: 0.01092879
train_loss: 0.005626294
test_loss: 0.005396626
train_loss: 0.0041093444
test_loss: 0.004196466
train_loss: 0.003519718
test_loss: 0.0033909983
train_loss: 0.0034641218
test_loss: 0.0032797626
train_loss: 0.0029133311
test_loss: 0.002721035
train_loss: 0.0022781703
test_loss: 0.002584641
train_loss: 0.0024957645
test_loss: 0.0027542608
train_loss: 0.0022719265
test_loss: 0.0023820251
train_loss: 0.0023114067
test_loss: 0.0025507582
train_loss: 0.0027872915
test_loss: 0.0022427544
train_loss: 0.001934279
test_loss: 0.0023142127
train_loss: 0.0021901638
test_loss: 0.002084927
train_loss: 0.0019861094
test_loss: 0.0018432417
train_loss: 0.0019882875
test_loss: 0.0020801518
train_loss: 0.0021567263
test_loss: 0.002055127
train_loss: 0.001998985
test_loss: 0.001940337
train_loss: 0.0017800308
test_loss: 0.0023705503
train_loss: 0.0021311163
test_loss: 0.0019803739
train_loss: 0.00213053
test_loss: 0.0020677382
train_loss: 0.0019909109
test_loss: 0.0022937523
train_loss: 0.0016395013
test_loss: 0.0018440482
train_loss: 0.0019034795
test_loss: 0.0017902565
train_loss: 0.0017570083
test_loss: 0.00207495
train_loss: 0.0018678114
test_loss: 0.0017834727
train_loss: 0.0018638314
test_loss: 0.0019838212
train_loss: 0.002113482
test_loss: 0.0018166918
train_loss: 0.0022468055
test_loss: 0.0017183151
train_loss: 0.0016706453
test_loss: 0.0016238897
train_loss: 0.0015340291
test_loss: 0.0018738197
train_loss: 0.0015646233
test_loss: 0.0015025836
train_loss: 0.001737182
test_loss: 0.0016228646
train_loss: 0.0014723247
test_loss: 0.0015721257
train_loss: 0.0015709305
test_loss: 0.0018250501
train_loss: 0.0019380914
test_loss: 0.0019134745
train_loss: 0.0019742856
test_loss: 0.0017387838
train_loss: 0.0017163467
test_loss: 0.001810897
train_loss: 0.0018018968
test_loss: 0.001979372
train_loss: 0.0019541122
test_loss: 0.002006417
train_loss: 0.0019921814
test_loss: 0.0019715829
train_loss: 0.001756208
test_loss: 0.002101512
train_loss: 0.0017347744
test_loss: 0.0017261724
train_loss: 0.0016028711
test_loss: 0.0016221288
train_loss: 0.0014557567
test_loss: 0.0014890066
train_loss: 0.0014422671
test_loss: 0.0015853172
train_loss: 0.0017291012
test_loss: 0.0017599937
train_loss: 0.0015332955
test_loss: 0.0014637201
train_loss: 0.0015467671
test_loss: 0.0015133376
train_loss: 0.0015303977
test_loss: 0.0014234685
train_loss: 0.0017854888
test_loss: 0.0017247365
train_loss: 0.0015815055
test_loss: 0.001794612
train_loss: 0.0014967702
test_loss: 0.0016268379
train_loss: 0.0015196521
test_loss: 0.0017315205
train_loss: 0.0023163192
test_loss: 0.0015749236
train_loss: 0.0017512523
test_loss: 0.002684416
train_loss: 0.0018983138
test_loss: 0.0023249113
train_loss: 0.001974748
test_loss: 0.0020564508
train_loss: 0.0017215468
test_loss: 0.0018298959
train_loss: 0.001812849
test_loss: 0.0018391472
train_loss: 0.0015411664
test_loss: 0.0017420104
train_loss: 0.0015950315
test_loss: 0.0015152015
train_loss: 0.0017095781
test_loss: 0.0016768207
train_loss: 0.0017072406
test_loss: 0.0016479826
train_loss: 0.0017468215
test_loss: 0.0017528215
train_loss: 0.0018658296
test_loss: 0.0016539408
train_loss: 0.0014715218
test_loss: 0.0014357792
train_loss: 0.0015353501
test_loss: 0.0015980168
train_loss: 0.0014139842
test_loss: 0.0016983758
train_loss: 0.0015719233
test_loss: 0.001386967
train_loss: 0.0015540782
test_loss: 0.0015498521
train_loss: 0.0015270896
test_loss: 0.0015067785
train_loss: 0.0016075231
test_loss: 0.0014854792
train_loss: 0.0012067051
test_loss: 0.0013368307
train_loss: 0.0015461772
test_loss: 0.002055762
train_loss: 0.001575269
test_loss: 0.0014754756
train_loss: 0.0014013951
test_loss: 0.0015124897
train_loss: 0.0015245923
test_loss: 0.0014897467
train_loss: 0.0014292491
test_loss: 0.0016059269
train_loss: 0.0013922954
test_loss: 0.0015327508
train_loss: 0.001347478
test_loss: 0.0014969091
train_loss: 0.0015599383
test_loss: 0.0014928906
train_loss: 0.0015752036
test_loss: 0.0014393817
train_loss: 0.0014685709
test_loss: 0.0014232512
train_loss: 0.0014855604
test_loss: 0.0012917774
train_loss: 0.0014882635
test_loss: 0.001320188
train_loss: 0.0013958062
test_loss: 0.0014766016
train_loss: 0.0014273073
test_loss: 0.0015550613
train_loss: 0.0014423111
test_loss: 0.0014747084
train_loss: 0.0016085234
test_loss: 0.0015076092
train_loss: 0.0013617735
test_loss: 0.0015403639
train_loss: 0.0013148137
test_loss: 0.0015409644
train_loss: 0.0017855479
test_loss: 0.0014466464
train_loss: 0.0015418768
test_loss: 0.0013687342
train_loss: 0.0015953354
test_loss: 0.0015373887
train_loss: 0.0015778344
test_loss: 0.001275974
train_loss: 0.0016200537
test_loss: 0.00188604
train_loss: 0.0018450372
test_loss: 0.0016675732
train_loss: 0.0017473144
test_loss: 0.0015024157
train_loss: 0.0016093948
test_loss: 0.0012732175
train_loss: 0.0014976037
test_loss: 0.0017507646
train_loss: 0.0016366098
test_loss: 0.0014277718
train_loss: 0.0016760365
test_loss: 0.0016109862
train_loss: 0.0017475525
test_loss: 0.0015758345
train_loss: 0.0021795575
test_loss: 0.0016108234
train_loss: 0.0019213182
test_loss: 0.0017689657
train_loss: 0.0015350392
test_loss: 0.0014624742
train_loss: 0.0015725767
test_loss: 0.00142075
train_loss: 0.0014343623
test_loss: 0.0014377171
train_loss: 0.0015083929
test_loss: 0.0012383616
train_loss: 0.0013403029
test_loss: 0.0013238041
train_loss: 0.001497674
test_loss: 0.0012370809
train_loss: 0.0013889095
test_loss: 0.0014134161
train_loss: 0.0013390048
test_loss: 0.0013948593
train_loss: 0.0013593446
test_loss: 0.0013548698
train_loss: 0.0014675341
test_loss: 0.0018074259
train_loss: 0.0015185509
test_loss: 0.0016206114
train_loss: 0.0015901397
test_loss: 0.0014734048
train_loss: 0.0014943321
test_loss: 0.0015945066
train_loss: 0.0012476258
test_loss: 0.0016411054
train_loss: 0.001308908
test_loss: 0.0015367363
train_loss: 0.0013363176
test_loss: 0.0015775688
train_loss: 0.0014677525
test_loss: 0.0015768359
train_loss: 0.001374641
test_loss: 0.0012258815
train_loss: 0.0016640621
test_loss: 0.0014110703
train_loss: 0.0018758113
test_loss: 0.0018188697
train_loss: 0.0017200611
test_loss: 0.0014617926
train_loss: 0.0015380292
test_loss: 0.0019700574
train_loss: 0.0019655328
test_loss: 0.0017258294
train_loss: 0.0017170203
test_loss: 0.0016935454
train_loss: 0.0013122784
test_loss: 0.001510878
train_loss: 0.0015433198
test_loss: 0.0013856346
train_loss: 0.0014826385
test_loss: 0.0012734543
train_loss: 0.0015480417
test_loss: 0.0015314677
train_loss: 0.001253505
test_loss: 0.0013783815
train_loss: 0.0014071318
test_loss: 0.0015870959
train_loss: 0.0014992899
test_loss: 0.0014162719
train_loss: 0.0016068836
test_loss: 0.0014420314
train_loss: 0.0015805113
test_loss: 0.0013793707
train_loss: 0.0015103022
test_loss: 0.0014360194
train_loss: 0.0013900832
test_loss: 0.001447729
train_loss: 0.0015126509
test_loss: 0.0014088018
train_loss: 0.0014374227
test_loss: 0.0014416601
train_loss: 0.0016583953
test_loss: 0.0015899172
train_loss: 0.0016389864
test_loss: 0.0014168418
train_loss: 0.0013399252
test_loss: 0.0015747917
train_loss: 0.0013663376
test_loss: 0.0015835762
train_loss: 0.001181381
test_loss: 0.0013052108
train_loss: 0.0011995091
test_loss: 0.0011704091
train_loss: 0.001208919
test_loss: 0.0014034396
train_loss: 0.0014108055
test_loss: 0.0014122378
train_loss: 0.001364045
test_loss: 0.0015094101
train_loss: 0.0014117114
test_loss: 0.0014124216
train_loss: 0.0012848948
test_loss: 0.001541391
train_loss: 0.0013854611
test_loss: 0.0013757191
train_loss: 0.0014187389
test_loss: 0.0015970804
train_loss: 0.0013088877
test_loss: 0.0012516204
train_loss: 0.0012490036
test_loss: 0.0014507633
train_loss: 0.0014536682
test_loss: 0.001239434
train_loss: 0.0014787011
test_loss: 0.0013456956
train_loss: 0.001290112
test_loss: 0.0014215447
train_loss: 0.0013968224
test_loss: 0.0014053736
train_loss: 0.0014245164
test_loss: 0.0013510108
train_loss: 0.0012194547
test_loss: 0.001367041
train_loss: 0.0012332423
test_loss: 0.0015831882
train_loss: 0.0013269212
test_loss: 0.0014080963
train_loss: 0.001337842
test_loss: 0.0013866284
train_loss: 0.0013366202
test_loss: 0.0013204815
train_loss: 0.001433191
test_loss: 0.0013450447
train_loss: 0.0014302335
test_loss: 0.0012619778
train_loss: 0.0013445399
test_loss: 0.0011936511
train_loss: 0.0014078855
test_loss: 0.0013612999
train_loss: 0.0013156091
test_loss: 0.0014925706
train_loss: 0.001557091
test_loss: 0.0012825548
train_loss: 0.0013731061
test_loss: 0.0014403835
train_loss: 0.0013198484
test_loss: 0.0015627638
train_loss: 0.0015277304
test_loss: 0.0014924926
train_loss: 0.0012784186
test_loss: 0.001488161
train_loss: 0.0013090199
test_loss: 0.0012463887
train_loss: 0.0013288469
test_loss: 0.0012545722
train_loss: 0.0012994264
test_loss: 0.0013237463
train_loss: 0.0014463176
test_loss: 0.001347938
train_loss: 0.0016221245
test_loss: 0.0014839786
train_loss: 0.0016605125
test_loss: 0.0014784711
train_loss: 0.0018912586
test_loss: 0.0015156631
train_loss: 0.0014639867
test_loss: 0.002002213
train_loss: 0.0016913224
test_loss: 0.0018976264
train_loss: 0.0018147714
test_loss: 0.0020270098
train_loss: 0.0016244536
test_loss: 0.0019322939
train_loss: 0.0017700009
test_loss: 0.001663335
train_loss: 0.0015066911
test_loss: 0.0018027073
train_loss: 0.0016466356
test_loss: 0.0014936008
train_loss: 0.0014889457
test_loss: 0.0015342134
train_loss: 0.001545126
test_loss: 0.0015841399
train_loss: 0.0015274904
test_loss: 0.0013390827
train_loss: 0.0012809043
test_loss: 0.0016562248
train_loss: 0.0014655145
test_loss: 0.0013899221
train_loss: 0.0012728437
test_loss: 0.0014043831
train_loss: 0.0012404512
test_loss: 0.001335088
train_loss: 0.0013046268
test_loss: 0.0011493913
train_loss: 0.001492559
test_loss: 0.0013533293
train_loss: 0.0013897003
test_loss: 0.0013521985
train_loss: 0.0013621339
test_loss: 0.0011898461
train_loss: 0.0013485583
test_loss: 0.0014384768
train_loss: 0.0014881426
test_loss: 0.0014074908
train_loss: 0.0016208391
test_loss: 0.0013908778
train_loss: 0.0012824204
test_loss: 0.0013323958
train_loss: 0.0012732648
test_loss: 0.001288641
train_loss: 0.0014213015
test_loss: 0.0013713535
train_loss: 0.0012777975
test_loss: 0.001354652
train_loss: 0.001602215
test_loss: 0.0014633511
train_loss: 0.0011652496
test_loss: 0.0012684425
train_loss: 0.001525948
test_loss: 0.0013410504
train_loss: 0.0015210331
test_loss: 0.0012568114
train_loss: 0.0013711231
test_loss: 0.0012498165
train_loss: 0.001194657
test_loss: 0.0013105124
train_loss: 0.0012237187
test_loss: 0.0013417321
train_loss: 0.0013177989
test_loss: 0.0013287117
train_loss: 0.0014234278
test_loss: 0.0014728064
train_loss: 0.001482196
test_loss: 0.0013160851
train_loss: 0.001495395
test_loss: 0.0014280733
train_loss: 0.001517212
test_loss: 0.0015506161
train_loss: 0.0015058679
test_loss: 0.0014993843
train_loss: 0.0011803928
test_loss: 0.001357682
train_loss: 0.0014394957
test_loss: 0.0014260131
train_loss: 0.0014672406
test_loss: 0.001373166
train_loss: 0.001329581
test_loss: 0.0014498917
train_loss: 0.0014497971
test_loss: 0.0012587721
train_loss: 0.0015586362
test_loss: 0.0012684789
train_loss: 0.0013414674
test_loss: 0.0012755324
train_loss: 0.0013390277
test_loss: 0.0013816477
train_loss: 0.0013339204
test_loss: 0.0013671169
train_loss: 0.0015843904
test_loss: 0.0015034286
train_loss: 0.0016308669
test_loss: 0.0016456418
train_loss: 0.001540706
test_loss: 0.0014484613
train_loss: 0.0019690557
test_loss: 0.0016643346
train_loss: 0.0012625896
test_loss: 0.0011966034
train_loss: 0.0012896941
test_loss: 0.0015060022
train_loss: 0.0015102844
test_loss: 0.0015737292
train_loss: 0.0013248058
test_loss: 0.00122875
train_loss: 0.0013355798
test_loss: 0.0015068669
train_loss: 0.0013338235
test_loss: 0.0012565218
train_loss: 0.0014679285
test_loss: 0.0012981078
train_loss: 0.0015099633
test_loss: 0.001430284
train_loss: 0.0014199477
test_loss: 0.0014673648
train_loss: 0.0014249892
test_loss: 0.0011294176
train_loss: 0.0013705534
test_loss: 0.0012096487
train_loss: 0.0013230089
test_loss: 0.0012431064
train_loss: 0.0014101611
test_loss: 0.001276344
train_loss: 0.0013979098
test_loss: 0.0014431416
train_loss: 0.0017011415
test_loss: 0.0014456668
train_loss: 0.0017065473
test_loss: 0.0012768339
train_loss: 0.0014245131
test_loss: 0.0013371326
train_loss: 0.0012937749
test_loss: 0.0014122439
train_loss: 0.0013449948
test_loss: 0.0013613658
train_loss: 0.0013163646
test_loss: 0.0014582283
train_loss: 0.0013375496
test_loss: 0.0012254061
train_loss: 0.0012953869
test_loss: 0.0015313475
train_loss: 0.0012971376
test_loss: 0.001610205
train_loss: 0.0013982232
test_loss: 0.0011976458
train_loss: 0.0013260796
test_loss: 0.0012347905
train_loss: 0.0012855574
test_loss: 0.0014341166
train_loss: 0.0018402593
test_loss: 0.00126138
train_loss: 0.0015587192
test_loss: 0.0015616313
train_loss: 0.0013757924
test_loss: 0.0013413458
train_loss: 0.0015591335
test_loss: 0.001856181
train_loss: 0.0015666122
test_loss: 0.0014617726
train_loss: 0.0018376041
test_loss: 0.0015061379
train_loss: 0.0016744088
test_loss: 0.0016097836
train_loss: 0.0016325517
test_loss: 0.0022938002
train_loss: 0.0017035188
test_loss: 0.0019999372
train_loss: 0.0017334082
test_loss: 0.0020657436
train_loss: 0.0016882541
test_loss: 0.0019156223
train_loss: 0.0013110973
test_loss: 0.0021599366
train_loss: 0.0017843577
test_loss: 0.0015648842
train_loss: 0.0013424617
test_loss: 0.0017894228
train_loss: 0.001868353
test_loss: 0.0015900146
train_loss: 0.0014176237
test_loss: 0.0016182523
train_loss: 0.0015048196
test_loss: 0.0013782937
train_loss: 0.0012215402
test_loss: 0.0012399596
train_loss: 0.0013784928
test_loss: 0.0012599685
train_loss: 0.0013352192
test_loss: 0.0011051826
train_loss: 0.0015133836
test_loss: 0.0012857551
train_loss: 0.0013377634
test_loss: 0.0015700128
train_loss: 0.0015389919
test_loss: 0.0013023397
train_loss: 0.0012316697
test_loss: 0.0012758315
train_loss: 0.0011826586
test_loss: 0.0013250553
train_loss: 0.0012211798
test_loss: 0.0012629415
train_loss: 0.0012817507
test_loss: 0.0012836416
train_loss: 0.0011987704
test_loss: 0.0012660506
train_loss: 0.0012623052
test_loss: 0.0013330926
train_loss: 0.001224704
test_loss: 0.0014778441
train_loss: 0.0014413655
test_loss: 0.0012208793
train_loss: 0.0012045767
test_loss: 0.0015170327
train_loss: 0.0016390814
test_loss: 0.0013970847
train_loss: 0.0015269741
test_loss: 0.0012439693
train_loss: 0.0012797689
test_loss: 0.0014404136
train_loss: 0.001466366
test_loss: 0.001310306
train_loss: 0.0013549847
test_loss: 0.0013978232
train_loss: 0.0011382273
test_loss: 0.0013074075
train_loss: 0.0012252285
test_loss: 0.0012948
train_loss: 0.0013227419
test_loss: 0.0013526808
train_loss: 0.0012523714
test_loss: 0.001323873
train_loss: 0.0011872494
test_loss: 0.001223443
train_loss: 0.0014256049
test_loss: 0.0013096537
train_loss: 0.001425913
test_loss: 0.0013260769
train_loss: 0.0012182196
test_loss: 0.0014868493
train_loss: 0.0014593794
test_loss: 0.0013180133
train_loss: 0.0012639053
test_loss: 0.0018855819
train_loss: 0.0017386662
test_loss: 0.0013250019
train_loss: 0.001589287
test_loss: 0.0013617299
train_loss: 0.0012676896
test_loss: 0.0012991678
train_loss: 0.0012871132
test_loss: 0.001276102
train_loss: 0.0013107278
test_loss: 0.0012567656
train_loss: 0.0011810611
test_loss: 0.00123474
train_loss: 0.0014098003
test_loss: 0.0012012349
train_loss: 0.0012737454
test_loss: 0.0013008966
train_loss: 0.0011515414
test_loss: 0.0011082649
train_loss: 0.0011987535
test_loss: 0.0014006539
train_loss: 0.0013193378
test_loss: 0.0016393462
train_loss: 0.0013117353
test_loss: 0.0014700267
train_loss: 0.0012208248
test_loss: 0.0012394666
train_loss: 0.0011501941
test_loss: 0.0011361012
train_loss: 0.0012787569
test_loss: 0.0014662299
train_loss: 0.0014243606
test_loss: 0.0011915324
train_loss: 0.0013534266
test_loss: 0.0012906797
train_loss: 0.0011785585
test_loss: 0.0014342155
train_loss: 0.0013374386
test_loss: 0.0014436748
train_loss: 0.0013766494
test_loss: 0.0015242473
train_loss: 0.0012538086
test_loss: 0.0013849402
train_loss: 0.0014139938
test_loss: 0.001324561
train_loss: 0.001136348
test_loss: 0.0012507414
train_loss: 0.0012757959
test_loss: 0.0011672173
train_loss: 0.0012705526
test_loss: 0.0013977686
train_loss: 0.0015684629
test_loss: 0.0015988076
train_loss: 0.0012692218
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0013800585
train_loss: 0.0011641694
test_loss: 0.001288959
train_loss: 0.001369532
test_loss: 0.0013070564
train_loss: 0.0014523522
test_loss: 0.0014441472
train_loss: 0.0012054429
test_loss: 0.001449819
train_loss: 0.0012293279
test_loss: 0.0012574184
train_loss: 0.0010775903
test_loss: 0.0012837977
train_loss: 0.0013533863
test_loss: 0.0013154474
train_loss: 0.0012356284
test_loss: 0.0012444162
train_loss: 0.0012036221
test_loss: 0.0016204836
train_loss: 0.0016543209
test_loss: 0.002133749
train_loss: 0.0015294997
test_loss: 0.002136595
train_loss: 0.0014845567
test_loss: 0.0021094608
train_loss: 0.0017157815
test_loss: 0.0017831482
train_loss: 0.0015058117
test_loss: 0.0013646754
train_loss: 0.0013915423
test_loss: 0.0013872795
train_loss: 0.0014402419
test_loss: 0.0013484285
train_loss: 0.0012603112
test_loss: 0.0014096189
train_loss: 0.001361714
test_loss: 0.0012188233
train_loss: 0.0013276099
test_loss: 0.0012173497
train_loss: 0.0013046282
test_loss: 0.001452401
train_loss: 0.0012567722
test_loss: 0.0013248076
train_loss: 0.0012835545
test_loss: 0.0012095756
train_loss: 0.0012140665
test_loss: 0.0013948254
train_loss: 0.0012959724
test_loss: 0.0013401638
train_loss: 0.0012050746
test_loss: 0.0013285531
train_loss: 0.0013501006
test_loss: 0.0013120805
train_loss: 0.0013911384
test_loss: 0.0015295896
train_loss: 0.0014544269
test_loss: 0.0011316912
train_loss: 0.0014867403
test_loss: 0.0013527886
train_loss: 0.0013737651
test_loss: 0.001508661
train_loss: 0.0014355122
test_loss: 0.001238822
train_loss: 0.0012855291
test_loss: 0.0011644063
train_loss: 0.0019029547
test_loss: 0.0015305494
train_loss: 0.0014947271
test_loss: 0.001243503
train_loss: 0.0014362454
test_loss: 0.0012986469
train_loss: 0.0012338583
test_loss: 0.0013459534
train_loss: 0.0014298562
test_loss: 0.0013485576
train_loss: 0.0011679346
test_loss: 0.001502143
train_loss: 0.0012695332
test_loss: 0.0012003481
train_loss: 0.0012883906
test_loss: 0.0015336813
train_loss: 0.0013300437
test_loss: 0.0012700557
train_loss: 0.0011622921
test_loss: 0.0013208376
train_loss: 0.0013868241
test_loss: 0.001467677
train_loss: 0.0011068908
test_loss: 0.0013353174
train_loss: 0.0012856667
test_loss: 0.0011870206
train_loss: 0.0014437607
test_loss: 0.0014959164
train_loss: 0.0013452413
test_loss: 0.0012244232
train_loss: 0.001382462
test_loss: 0.0011829828
train_loss: 0.0013424427
test_loss: 0.0013372691
train_loss: 0.0014417663
test_loss: 0.0015466926
train_loss: 0.0013483267
test_loss: 0.0013199676
train_loss: 0.0014090078
test_loss: 0.0012707341
train_loss: 0.0012276066
test_loss: 0.0011856302
train_loss: 0.0011699675
test_loss: 0.0012939548
train_loss: 0.0014309811
test_loss: 0.0015469942
train_loss: 0.0012154861
test_loss: 0.0012915683
train_loss: 0.0013844132
test_loss: 0.001340756
train_loss: 0.0013451776
test_loss: 0.0013948277
train_loss: 0.0013788019
test_loss: 0.0013884105
train_loss: 0.0012665166
test_loss: 0.001233174
train_loss: 0.001309896
test_loss: 0.0014848365
train_loss: 0.0012409132
test_loss: 0.0014972044
train_loss: 0.0013174586
test_loss: 0.0013394813
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe938840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe9548c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe8ef730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe96f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe877ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe877268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe84a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe9ceb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe892d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe7b1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe7b1378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe7acea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe77ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe74f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe75a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe719c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe719ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe719ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe719510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe6287b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe646620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe5de0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe646f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe5d2c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe5d2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe5d2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4afe59a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ac00c3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ac00c3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ac0054f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ac00861e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ac00867b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4abffd3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ac0025510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ac000bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4abffc2f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.65704807e-06
Iter: 2 loss: 1.77434413e-06
Iter: 3 loss: 1.66552854e-06
Iter: 4 loss: 1.33718754e-06
Iter: 5 loss: 1.4847451e-06
Iter: 6 loss: 1.11393888e-06
Iter: 7 loss: 9.72620455e-07
Iter: 8 loss: 1.31542743e-06
Iter: 9 loss: 9.22053175e-07
Iter: 10 loss: 8.26011046e-07
Iter: 11 loss: 8.22858397e-07
Iter: 12 loss: 7.94150708e-07
Iter: 13 loss: 7.83501321e-07
Iter: 14 loss: 7.67646156e-07
Iter: 15 loss: 7.48594402e-07
Iter: 16 loss: 8.30198303e-07
Iter: 17 loss: 7.4468295e-07
Iter: 18 loss: 7.2750629e-07
Iter: 19 loss: 9.43494399e-07
Iter: 20 loss: 7.27380097e-07
Iter: 21 loss: 7.20131e-07
Iter: 22 loss: 7.14184694e-07
Iter: 23 loss: 7.12052213e-07
Iter: 24 loss: 7.0293089e-07
Iter: 25 loss: 7.02902e-07
Iter: 26 loss: 6.95146525e-07
Iter: 27 loss: 6.76717661e-07
Iter: 28 loss: 8.8614064e-07
Iter: 29 loss: 6.75003093e-07
Iter: 30 loss: 6.59729039e-07
Iter: 31 loss: 7.0443059e-07
Iter: 32 loss: 6.55046392e-07
Iter: 33 loss: 6.42482121e-07
Iter: 34 loss: 6.66605331e-07
Iter: 35 loss: 6.37250821e-07
Iter: 36 loss: 6.32600461e-07
Iter: 37 loss: 6.32599e-07
Iter: 38 loss: 6.30030627e-07
Iter: 39 loss: 6.30017553e-07
Iter: 40 loss: 6.27436918e-07
Iter: 41 loss: 6.25377083e-07
Iter: 42 loss: 6.24599636e-07
Iter: 43 loss: 6.22187258e-07
Iter: 44 loss: 6.22978746e-07
Iter: 45 loss: 6.20468199e-07
Iter: 46 loss: 6.16096713e-07
Iter: 47 loss: 6.47923798e-07
Iter: 48 loss: 6.15750196e-07
Iter: 49 loss: 6.12842427e-07
Iter: 50 loss: 6.0774164e-07
Iter: 51 loss: 6.0773516e-07
Iter: 52 loss: 6.03758394e-07
Iter: 53 loss: 6.30791419e-07
Iter: 54 loss: 6.03371063e-07
Iter: 55 loss: 5.98460701e-07
Iter: 56 loss: 6.19004197e-07
Iter: 57 loss: 5.97420865e-07
Iter: 58 loss: 5.95420715e-07
Iter: 59 loss: 6.00092221e-07
Iter: 60 loss: 5.94681467e-07
Iter: 61 loss: 5.92681431e-07
Iter: 62 loss: 6.10705513e-07
Iter: 63 loss: 5.92577237e-07
Iter: 64 loss: 5.91326568e-07
Iter: 65 loss: 5.8806387e-07
Iter: 66 loss: 6.12569579e-07
Iter: 67 loss: 5.87415229e-07
Iter: 68 loss: 5.84361e-07
Iter: 69 loss: 6.11881e-07
Iter: 70 loss: 5.84228928e-07
Iter: 71 loss: 5.81863219e-07
Iter: 72 loss: 5.86292174e-07
Iter: 73 loss: 5.80866811e-07
Iter: 74 loss: 5.80273536e-07
Iter: 75 loss: 5.79622906e-07
Iter: 76 loss: 5.78864501e-07
Iter: 77 loss: 5.77558751e-07
Iter: 78 loss: 5.77545961e-07
Iter: 79 loss: 5.76044272e-07
Iter: 80 loss: 5.79528e-07
Iter: 81 loss: 5.75526826e-07
Iter: 82 loss: 5.73655598e-07
Iter: 83 loss: 5.91074e-07
Iter: 84 loss: 5.73571413e-07
Iter: 85 loss: 5.72986778e-07
Iter: 86 loss: 5.71510043e-07
Iter: 87 loss: 5.87560578e-07
Iter: 88 loss: 5.71360829e-07
Iter: 89 loss: 5.69840211e-07
Iter: 90 loss: 5.90607556e-07
Iter: 91 loss: 5.69861413e-07
Iter: 92 loss: 5.68395194e-07
Iter: 93 loss: 5.75055537e-07
Iter: 94 loss: 5.68110067e-07
Iter: 95 loss: 5.67417942e-07
Iter: 96 loss: 5.67359109e-07
Iter: 97 loss: 5.66815913e-07
Iter: 98 loss: 5.65240782e-07
Iter: 99 loss: 5.68057544e-07
Iter: 100 loss: 5.64594075e-07
Iter: 101 loss: 5.63997105e-07
Iter: 102 loss: 5.64551044e-07
Iter: 103 loss: 5.63626941e-07
Iter: 104 loss: 5.62968e-07
Iter: 105 loss: 5.62106266e-07
Iter: 106 loss: 5.6204783e-07
Iter: 107 loss: 5.61652143e-07
Iter: 108 loss: 5.61451e-07
Iter: 109 loss: 5.60834337e-07
Iter: 110 loss: 5.62071705e-07
Iter: 111 loss: 5.60635101e-07
Iter: 112 loss: 5.59998853e-07
Iter: 113 loss: 5.58537806e-07
Iter: 114 loss: 5.75346576e-07
Iter: 115 loss: 5.58418662e-07
Iter: 116 loss: 5.58473232e-07
Iter: 117 loss: 5.57651788e-07
Iter: 118 loss: 5.57090402e-07
Iter: 119 loss: 5.55337238e-07
Iter: 120 loss: 5.630435e-07
Iter: 121 loss: 5.54696442e-07
Iter: 122 loss: 5.529443e-07
Iter: 123 loss: 5.70341115e-07
Iter: 124 loss: 5.52895472e-07
Iter: 125 loss: 5.52879897e-07
Iter: 126 loss: 5.52479264e-07
Iter: 127 loss: 5.52168444e-07
Iter: 128 loss: 5.51698292e-07
Iter: 129 loss: 5.63997219e-07
Iter: 130 loss: 5.51696303e-07
Iter: 131 loss: 5.51232688e-07
Iter: 132 loss: 5.57803e-07
Iter: 133 loss: 5.51231096e-07
Iter: 134 loss: 5.50876337e-07
Iter: 135 loss: 5.50968934e-07
Iter: 136 loss: 5.50605193e-07
Iter: 137 loss: 5.50332629e-07
Iter: 138 loss: 5.49816548e-07
Iter: 139 loss: 5.61761453e-07
Iter: 140 loss: 5.49796596e-07
Iter: 141 loss: 5.49025e-07
Iter: 142 loss: 5.51794244e-07
Iter: 143 loss: 5.48870958e-07
Iter: 144 loss: 5.48223909e-07
Iter: 145 loss: 5.49478159e-07
Iter: 146 loss: 5.47922468e-07
Iter: 147 loss: 5.47329137e-07
Iter: 148 loss: 5.47338e-07
Iter: 149 loss: 5.46997342e-07
Iter: 150 loss: 5.46577098e-07
Iter: 151 loss: 5.46533386e-07
Iter: 152 loss: 5.46085857e-07
Iter: 153 loss: 5.45717739e-07
Iter: 154 loss: 5.45601779e-07
Iter: 155 loss: 5.4552811e-07
Iter: 156 loss: 5.45222179e-07
Iter: 157 loss: 5.44975819e-07
Iter: 158 loss: 5.44508794e-07
Iter: 159 loss: 5.4451732e-07
Iter: 160 loss: 5.44060072e-07
Iter: 161 loss: 5.45372245e-07
Iter: 162 loss: 5.43886131e-07
Iter: 163 loss: 5.43247779e-07
Iter: 164 loss: 5.47983859e-07
Iter: 165 loss: 5.43200144e-07
Iter: 166 loss: 5.42965438e-07
Iter: 167 loss: 5.42719249e-07
Iter: 168 loss: 5.42714588e-07
Iter: 169 loss: 5.42370515e-07
Iter: 170 loss: 5.42356133e-07
Iter: 171 loss: 5.42115e-07
Iter: 172 loss: 5.41794691e-07
Iter: 173 loss: 5.41775421e-07
Iter: 174 loss: 5.41434247e-07
Iter: 175 loss: 5.41937879e-07
Iter: 176 loss: 5.41338636e-07
Iter: 177 loss: 5.4098939e-07
Iter: 178 loss: 5.42188559e-07
Iter: 179 loss: 5.40891733e-07
Iter: 180 loss: 5.40492465e-07
Iter: 181 loss: 5.41584882e-07
Iter: 182 loss: 5.40346491e-07
Iter: 183 loss: 5.40062842e-07
Iter: 184 loss: 5.39454902e-07
Iter: 185 loss: 5.49733954e-07
Iter: 186 loss: 5.39468942e-07
Iter: 187 loss: 5.38923246e-07
Iter: 188 loss: 5.47509387e-07
Iter: 189 loss: 5.38920403e-07
Iter: 190 loss: 5.3853779e-07
Iter: 191 loss: 5.43396e-07
Iter: 192 loss: 5.38562517e-07
Iter: 193 loss: 5.38381073e-07
Iter: 194 loss: 5.37973222e-07
Iter: 195 loss: 5.41659347e-07
Iter: 196 loss: 5.37931726e-07
Iter: 197 loss: 5.37937581e-07
Iter: 198 loss: 5.37725896e-07
Iter: 199 loss: 5.3757617e-07
Iter: 200 loss: 5.37348797e-07
Iter: 201 loss: 5.37353401e-07
Iter: 202 loss: 5.37076858e-07
Iter: 203 loss: 5.37321966e-07
Iter: 204 loss: 5.36925313e-07
Iter: 205 loss: 5.36431116e-07
Iter: 206 loss: 5.37756478e-07
Iter: 207 loss: 5.36263883e-07
Iter: 208 loss: 5.36004222e-07
Iter: 209 loss: 5.35695847e-07
Iter: 210 loss: 5.35644745e-07
Iter: 211 loss: 5.35113486e-07
Iter: 212 loss: 5.36915081e-07
Iter: 213 loss: 5.34985e-07
Iter: 214 loss: 5.3449952e-07
Iter: 215 loss: 5.40711426e-07
Iter: 216 loss: 5.34500145e-07
Iter: 217 loss: 5.342946e-07
Iter: 218 loss: 5.3396775e-07
Iter: 219 loss: 5.33966499e-07
Iter: 220 loss: 5.3361407e-07
Iter: 221 loss: 5.35140373e-07
Iter: 222 loss: 5.33543528e-07
Iter: 223 loss: 5.33454283e-07
Iter: 224 loss: 5.33399486e-07
Iter: 225 loss: 5.3329552e-07
Iter: 226 loss: 5.33026764e-07
Iter: 227 loss: 5.35758659e-07
Iter: 228 loss: 5.32997774e-07
Iter: 229 loss: 5.32695253e-07
Iter: 230 loss: 5.34408855e-07
Iter: 231 loss: 5.32686045e-07
Iter: 232 loss: 5.32463787e-07
Iter: 233 loss: 5.32454692e-07
Iter: 234 loss: 5.32345439e-07
Iter: 235 loss: 5.32006766e-07
Iter: 236 loss: 5.33307e-07
Iter: 237 loss: 5.31882847e-07
Iter: 238 loss: 5.31840215e-07
Iter: 239 loss: 5.31644e-07
Iter: 240 loss: 5.31524392e-07
Iter: 241 loss: 5.3133158e-07
Iter: 242 loss: 5.31298156e-07
Iter: 243 loss: 5.31105229e-07
Iter: 244 loss: 5.30942941e-07
Iter: 245 loss: 5.30864668e-07
Iter: 246 loss: 5.30969714e-07
Iter: 247 loss: 5.30732052e-07
Iter: 248 loss: 5.30632462e-07
Iter: 249 loss: 5.30395823e-07
Iter: 250 loss: 5.34458707e-07
Iter: 251 loss: 5.30390139e-07
Iter: 252 loss: 5.30074544e-07
Iter: 253 loss: 5.29914644e-07
Iter: 254 loss: 5.29784472e-07
Iter: 255 loss: 5.29631848e-07
Iter: 256 loss: 5.29586714e-07
Iter: 257 loss: 5.29353e-07
Iter: 258 loss: 5.29309773e-07
Iter: 259 loss: 5.29131285e-07
Iter: 260 loss: 5.28902e-07
Iter: 261 loss: 5.29119859e-07
Iter: 262 loss: 5.28728606e-07
Iter: 263 loss: 5.28574105e-07
Iter: 264 loss: 5.28582575e-07
Iter: 265 loss: 5.28427904e-07
Iter: 266 loss: 5.28188252e-07
Iter: 267 loss: 5.28218948e-07
Iter: 268 loss: 5.27978841e-07
Iter: 269 loss: 5.29044939e-07
Iter: 270 loss: 5.27964403e-07
Iter: 271 loss: 5.27736461e-07
Iter: 272 loss: 5.29398164e-07
Iter: 273 loss: 5.27725433e-07
Iter: 274 loss: 5.27597649e-07
Iter: 275 loss: 5.27169846e-07
Iter: 276 loss: 5.2842671e-07
Iter: 277 loss: 5.26977487e-07
Iter: 278 loss: 5.26873805e-07
Iter: 279 loss: 5.26760914e-07
Iter: 280 loss: 5.26561905e-07
Iter: 281 loss: 5.27457303e-07
Iter: 282 loss: 5.26507506e-07
Iter: 283 loss: 5.26361077e-07
Iter: 284 loss: 5.26089195e-07
Iter: 285 loss: 5.30843295e-07
Iter: 286 loss: 5.26109261e-07
Iter: 287 loss: 5.25975111e-07
Iter: 288 loss: 5.25972951e-07
Iter: 289 loss: 5.2585807e-07
Iter: 290 loss: 5.26970268e-07
Iter: 291 loss: 5.25850851e-07
Iter: 292 loss: 5.25775647e-07
Iter: 293 loss: 5.25551741e-07
Iter: 294 loss: 5.27878115e-07
Iter: 295 loss: 5.25546284e-07
Iter: 296 loss: 5.25429073e-07
Iter: 297 loss: 5.254e-07
Iter: 298 loss: 5.25278779e-07
Iter: 299 loss: 5.25109613e-07
Iter: 300 loss: 5.25101427e-07
Iter: 301 loss: 5.24837674e-07
Iter: 302 loss: 5.24600864e-07
Iter: 303 loss: 5.24541235e-07
Iter: 304 loss: 5.24393442e-07
Iter: 305 loss: 5.24328e-07
Iter: 306 loss: 5.24157542e-07
Iter: 307 loss: 5.2395211e-07
Iter: 308 loss: 5.23934659e-07
Iter: 309 loss: 5.23699669e-07
Iter: 310 loss: 5.24112465e-07
Iter: 311 loss: 5.23623498e-07
Iter: 312 loss: 5.23501626e-07
Iter: 313 loss: 5.23490598e-07
Iter: 314 loss: 5.23403401e-07
Iter: 315 loss: 5.23293465e-07
Iter: 316 loss: 5.26349481e-07
Iter: 317 loss: 5.23285735e-07
Iter: 318 loss: 5.23056315e-07
Iter: 319 loss: 5.22866912e-07
Iter: 320 loss: 5.2280825e-07
Iter: 321 loss: 5.22789264e-07
Iter: 322 loss: 5.22679443e-07
Iter: 323 loss: 5.22567234e-07
Iter: 324 loss: 5.22306777e-07
Iter: 325 loss: 5.2230456e-07
Iter: 326 loss: 5.22043365e-07
Iter: 327 loss: 5.23028064e-07
Iter: 328 loss: 5.22012101e-07
Iter: 329 loss: 5.21748461e-07
Iter: 330 loss: 5.24511449e-07
Iter: 331 loss: 5.21746188e-07
Iter: 332 loss: 5.21615391e-07
Iter: 333 loss: 5.21436277e-07
Iter: 334 loss: 5.25831638e-07
Iter: 335 loss: 5.21411607e-07
Iter: 336 loss: 5.21223114e-07
Iter: 337 loss: 5.23179608e-07
Iter: 338 loss: 5.21181676e-07
Iter: 339 loss: 5.20977494e-07
Iter: 340 loss: 5.2200096e-07
Iter: 341 loss: 5.20920139e-07
Iter: 342 loss: 5.2083476e-07
Iter: 343 loss: 5.20723233e-07
Iter: 344 loss: 5.20683614e-07
Iter: 345 loss: 5.20535195e-07
Iter: 346 loss: 5.22980827e-07
Iter: 347 loss: 5.20528374e-07
Iter: 348 loss: 5.20367394e-07
Iter: 349 loss: 5.20377625e-07
Iter: 350 loss: 5.20219317e-07
Iter: 351 loss: 5.20092613e-07
Iter: 352 loss: 5.20003255e-07
Iter: 353 loss: 5.19945729e-07
Iter: 354 loss: 5.19739729e-07
Iter: 355 loss: 5.21527284e-07
Iter: 356 loss: 5.19738478e-07
Iter: 357 loss: 5.19561638e-07
Iter: 358 loss: 5.20637e-07
Iter: 359 loss: 5.19537252e-07
Iter: 360 loss: 5.19457672e-07
Iter: 361 loss: 5.19275e-07
Iter: 362 loss: 5.19294531e-07
Iter: 363 loss: 5.19138553e-07
Iter: 364 loss: 5.19111893e-07
Iter: 365 loss: 5.18980187e-07
Iter: 366 loss: 5.18807155e-07
Iter: 367 loss: 5.18794366e-07
Iter: 368 loss: 5.18614115e-07
Iter: 369 loss: 5.19381047e-07
Iter: 370 loss: 5.18597062e-07
Iter: 371 loss: 5.18380205e-07
Iter: 372 loss: 5.19167884e-07
Iter: 373 loss: 5.18337515e-07
Iter: 374 loss: 5.18129525e-07
Iter: 375 loss: 5.17985711e-07
Iter: 376 loss: 5.17937337e-07
Iter: 377 loss: 5.17775732e-07
Iter: 378 loss: 5.17770843e-07
Iter: 379 loss: 5.17698709e-07
Iter: 380 loss: 5.17840817e-07
Iter: 381 loss: 5.17635044e-07
Iter: 382 loss: 5.17532044e-07
Iter: 383 loss: 5.17443539e-07
Iter: 384 loss: 5.17411536e-07
Iter: 385 loss: 5.17306489e-07
Iter: 386 loss: 5.19074774e-07
Iter: 387 loss: 5.17289379e-07
Iter: 388 loss: 5.17203546e-07
Iter: 389 loss: 5.17655735e-07
Iter: 390 loss: 5.17176431e-07
Iter: 391 loss: 5.17093099e-07
Iter: 392 loss: 5.16823e-07
Iter: 393 loss: 5.19056357e-07
Iter: 394 loss: 5.16775117e-07
Iter: 395 loss: 5.16706621e-07
Iter: 396 loss: 5.16635851e-07
Iter: 397 loss: 5.16484306e-07
Iter: 398 loss: 5.16368743e-07
Iter: 399 loss: 5.16339867e-07
Iter: 400 loss: 5.1617485e-07
Iter: 401 loss: 5.16355499e-07
Iter: 402 loss: 5.16058606e-07
Iter: 403 loss: 5.15907914e-07
Iter: 404 loss: 5.15906947e-07
Iter: 405 loss: 5.15822421e-07
Iter: 406 loss: 5.15830322e-07
Iter: 407 loss: 5.15749662e-07
Iter: 408 loss: 5.156312e-07
Iter: 409 loss: 5.1579417e-07
Iter: 410 loss: 5.15601755e-07
Iter: 411 loss: 5.15495685e-07
Iter: 412 loss: 5.16170871e-07
Iter: 413 loss: 5.1548767e-07
Iter: 414 loss: 5.15366537e-07
Iter: 415 loss: 5.15225452e-07
Iter: 416 loss: 5.15215447e-07
Iter: 417 loss: 5.15009674e-07
Iter: 418 loss: 5.15753868e-07
Iter: 419 loss: 5.1493754e-07
Iter: 420 loss: 5.14763e-07
Iter: 421 loss: 5.14734211e-07
Iter: 422 loss: 5.14634962e-07
Iter: 423 loss: 5.14410203e-07
Iter: 424 loss: 5.14412136e-07
Iter: 425 loss: 5.14322e-07
Iter: 426 loss: 5.14322551e-07
Iter: 427 loss: 5.14198518e-07
Iter: 428 loss: 5.14204714e-07
Iter: 429 loss: 5.14093074e-07
Iter: 430 loss: 5.14002124e-07
Iter: 431 loss: 5.14093301e-07
Iter: 432 loss: 5.13952614e-07
Iter: 433 loss: 5.13866382e-07
Iter: 434 loss: 5.13866382e-07
Iter: 435 loss: 5.13815451e-07
Iter: 436 loss: 5.13723478e-07
Iter: 437 loss: 5.13724558e-07
Iter: 438 loss: 5.13592e-07
Iter: 439 loss: 5.13904297e-07
Iter: 440 loss: 5.13583188e-07
Iter: 441 loss: 5.13441705e-07
Iter: 442 loss: 5.14577778e-07
Iter: 443 loss: 5.1344324e-07
Iter: 444 loss: 5.1338e-07
Iter: 445 loss: 5.13217515e-07
Iter: 446 loss: 5.16052125e-07
Iter: 447 loss: 5.13244629e-07
Iter: 448 loss: 5.13006739e-07
Iter: 449 loss: 5.13122131e-07
Iter: 450 loss: 5.12854626e-07
Iter: 451 loss: 5.12880092e-07
Iter: 452 loss: 5.12721158e-07
Iter: 453 loss: 5.12656356e-07
Iter: 454 loss: 5.12532324e-07
Iter: 455 loss: 5.1570737e-07
Iter: 456 loss: 5.12531074e-07
Iter: 457 loss: 5.12398117e-07
Iter: 458 loss: 5.12756458e-07
Iter: 459 loss: 5.12310748e-07
Iter: 460 loss: 5.12137717e-07
Iter: 461 loss: 5.13864222e-07
Iter: 462 loss: 5.12116401e-07
Iter: 463 loss: 5.12039435e-07
Iter: 464 loss: 5.11901817e-07
Iter: 465 loss: 5.14780652e-07
Iter: 466 loss: 5.11895848e-07
Iter: 467 loss: 5.11718952e-07
Iter: 468 loss: 5.13369685e-07
Iter: 469 loss: 5.11699739e-07
Iter: 470 loss: 5.11510621e-07
Iter: 471 loss: 5.12263284e-07
Iter: 472 loss: 5.11470944e-07
Iter: 473 loss: 5.11374083e-07
Iter: 474 loss: 5.11562916e-07
Iter: 475 loss: 5.11325197e-07
Iter: 476 loss: 5.11250505e-07
Iter: 477 loss: 5.11970313e-07
Iter: 478 loss: 5.11229e-07
Iter: 479 loss: 5.1115137e-07
Iter: 480 loss: 5.11037115e-07
Iter: 481 loss: 5.11039332e-07
Iter: 482 loss: 5.10886537e-07
Iter: 483 loss: 5.11179962e-07
Iter: 484 loss: 5.10875623e-07
Iter: 485 loss: 5.10761936e-07
Iter: 486 loss: 5.1194553e-07
Iter: 487 loss: 5.10766654e-07
Iter: 488 loss: 5.10664506e-07
Iter: 489 loss: 5.1052632e-07
Iter: 490 loss: 5.10514099e-07
Iter: 491 loss: 5.10372331e-07
Iter: 492 loss: 5.10576115e-07
Iter: 493 loss: 5.10276209e-07
Iter: 494 loss: 5.10191626e-07
Iter: 495 loss: 5.10177529e-07
Iter: 496 loss: 5.10059e-07
Iter: 497 loss: 5.09853294e-07
Iter: 498 loss: 5.12451095e-07
Iter: 499 loss: 5.09847609e-07
Iter: 500 loss: 5.09697372e-07
Iter: 501 loss: 5.11441954e-07
Iter: 502 loss: 5.09706297e-07
Iter: 503 loss: 5.09612676e-07
Iter: 504 loss: 5.10635459e-07
Iter: 505 loss: 5.09604433e-07
Iter: 506 loss: 5.09530764e-07
Iter: 507 loss: 5.09411052e-07
Iter: 508 loss: 5.09396216e-07
Iter: 509 loss: 5.09314646e-07
Iter: 510 loss: 5.10934797e-07
Iter: 511 loss: 5.0930737e-07
Iter: 512 loss: 5.09236315e-07
Iter: 513 loss: 5.09239499e-07
Iter: 514 loss: 5.09139966e-07
Iter: 515 loss: 5.09027416e-07
Iter: 516 loss: 5.08804533e-07
Iter: 517 loss: 5.13727969e-07
Iter: 518 loss: 5.08809535e-07
Iter: 519 loss: 5.08577386e-07
Iter: 520 loss: 5.08598077e-07
Iter: 521 loss: 5.08394692e-07
Iter: 522 loss: 5.09587323e-07
Iter: 523 loss: 5.08375251e-07
Iter: 524 loss: 5.08233711e-07
Iter: 525 loss: 5.08011283e-07
Iter: 526 loss: 5.12729343e-07
Iter: 527 loss: 5.07997584e-07
Iter: 528 loss: 5.07984282e-07
Iter: 529 loss: 5.07910158e-07
Iter: 530 loss: 5.07822051e-07
Iter: 531 loss: 5.0772212e-07
Iter: 532 loss: 5.07717118e-07
Iter: 533 loss: 5.07617528e-07
Iter: 534 loss: 5.07508219e-07
Iter: 535 loss: 5.07475818e-07
Iter: 536 loss: 5.07380548e-07
Iter: 537 loss: 5.07351501e-07
Iter: 538 loss: 5.07263621e-07
Iter: 539 loss: 5.07225195e-07
Iter: 540 loss: 5.07177333e-07
Iter: 541 loss: 5.07075924e-07
Iter: 542 loss: 5.07634e-07
Iter: 543 loss: 5.07031928e-07
Iter: 544 loss: 5.06907384e-07
Iter: 545 loss: 5.07056086e-07
Iter: 546 loss: 5.06837409e-07
Iter: 547 loss: 5.06701838e-07
Iter: 548 loss: 5.06769425e-07
Iter: 549 loss: 5.06613787e-07
Iter: 550 loss: 5.06480319e-07
Iter: 551 loss: 5.06457e-07
Iter: 552 loss: 5.06393803e-07
Iter: 553 loss: 5.06368622e-07
Iter: 554 loss: 5.06324e-07
Iter: 555 loss: 5.0625556e-07
Iter: 556 loss: 5.06138349e-07
Iter: 557 loss: 5.08011681e-07
Iter: 558 loss: 5.06138349e-07
Iter: 559 loss: 5.06010906e-07
Iter: 560 loss: 5.0660276e-07
Iter: 561 loss: 5.06008234e-07
Iter: 562 loss: 5.05843843e-07
Iter: 563 loss: 5.05857429e-07
Iter: 564 loss: 5.05718049e-07
Iter: 565 loss: 5.05549338e-07
Iter: 566 loss: 5.05715434e-07
Iter: 567 loss: 5.05497042e-07
Iter: 568 loss: 5.05362436e-07
Iter: 569 loss: 5.06585138e-07
Iter: 570 loss: 5.05376136e-07
Iter: 571 loss: 5.05227661e-07
Iter: 572 loss: 5.05326398e-07
Iter: 573 loss: 5.05161211e-07
Iter: 574 loss: 5.05076571e-07
Iter: 575 loss: 5.05430535e-07
Iter: 576 loss: 5.05063213e-07
Iter: 577 loss: 5.04968398e-07
Iter: 578 loss: 5.05687581e-07
Iter: 579 loss: 5.04966067e-07
Iter: 580 loss: 5.04894672e-07
Iter: 581 loss: 5.04719878e-07
Iter: 582 loss: 5.06472929e-07
Iter: 583 loss: 5.04718685e-07
Iter: 584 loss: 5.04513139e-07
Iter: 585 loss: 5.05015805e-07
Iter: 586 loss: 5.04443392e-07
Iter: 587 loss: 5.0429469e-07
Iter: 588 loss: 5.06199683e-07
Iter: 589 loss: 5.04269224e-07
Iter: 590 loss: 5.04068396e-07
Iter: 591 loss: 5.04145078e-07
Iter: 592 loss: 5.03957153e-07
Iter: 593 loss: 5.03850572e-07
Iter: 594 loss: 5.03988588e-07
Iter: 595 loss: 5.0373967e-07
Iter: 596 loss: 5.03641104e-07
Iter: 597 loss: 5.0531952e-07
Iter: 598 loss: 5.03641616e-07
Iter: 599 loss: 5.03542481e-07
Iter: 600 loss: 5.03495471e-07
Iter: 601 loss: 5.03484e-07
Iter: 602 loss: 5.03389742e-07
Iter: 603 loss: 5.03596141e-07
Iter: 604 loss: 5.0336638e-07
Iter: 605 loss: 5.03296064e-07
Iter: 606 loss: 5.04443335e-07
Iter: 607 loss: 5.03277874e-07
Iter: 608 loss: 5.03204888e-07
Iter: 609 loss: 5.03025944e-07
Iter: 610 loss: 5.060636e-07
Iter: 611 loss: 5.03026286e-07
Iter: 612 loss: 5.02970749e-07
Iter: 613 loss: 5.02960802e-07
Iter: 614 loss: 5.02880141e-07
Iter: 615 loss: 5.02705234e-07
Iter: 616 loss: 5.02715523e-07
Iter: 617 loss: 5.02513103e-07
Iter: 618 loss: 5.02531691e-07
Iter: 619 loss: 5.02374064e-07
Iter: 620 loss: 5.02207172e-07
Iter: 621 loss: 5.03749447e-07
Iter: 622 loss: 5.02206376e-07
Iter: 623 loss: 5.02091666e-07
Iter: 624 loss: 5.02090302e-07
Iter: 625 loss: 5.02029934e-07
Iter: 626 loss: 5.01862303e-07
Iter: 627 loss: 5.04483069e-07
Iter: 628 loss: 5.01872591e-07
Iter: 629 loss: 5.01831096e-07
Iter: 630 loss: 5.01823592e-07
Iter: 631 loss: 5.01742193e-07
Iter: 632 loss: 5.01591671e-07
Iter: 633 loss: 5.01599459e-07
Iter: 634 loss: 5.01471845e-07
Iter: 635 loss: 5.01538807e-07
Iter: 636 loss: 5.01401e-07
Iter: 637 loss: 5.01252e-07
Iter: 638 loss: 5.02461717e-07
Iter: 639 loss: 5.01259819e-07
Iter: 640 loss: 5.01121917e-07
Iter: 641 loss: 5.01360262e-07
Iter: 642 loss: 5.01060867e-07
Iter: 643 loss: 5.00976853e-07
Iter: 644 loss: 5.01258342e-07
Iter: 645 loss: 5.00939223e-07
Iter: 646 loss: 5.00834233e-07
Iter: 647 loss: 5.01542104e-07
Iter: 648 loss: 5.00851456e-07
Iter: 649 loss: 5.00767499e-07
Iter: 650 loss: 5.00659e-07
Iter: 651 loss: 5.03085118e-07
Iter: 652 loss: 5.00664783e-07
Iter: 653 loss: 5.00550186e-07
Iter: 654 loss: 5.00922852e-07
Iter: 655 loss: 5.00513124e-07
Iter: 656 loss: 5.00436613e-07
Iter: 657 loss: 5.00423425e-07
Iter: 658 loss: 5.00360954e-07
Iter: 659 loss: 5.00253179e-07
Iter: 660 loss: 5.00248404e-07
Iter: 661 loss: 5.00123178e-07
Iter: 662 loss: 5.00144665e-07
Iter: 663 loss: 5.00039619e-07
Iter: 664 loss: 5.00036776e-07
Iter: 665 loss: 4.99992098e-07
Iter: 666 loss: 4.99918485e-07
Iter: 667 loss: 4.99822647e-07
Iter: 668 loss: 5.00985379e-07
Iter: 669 loss: 4.99799967e-07
Iter: 670 loss: 4.99674229e-07
Iter: 671 loss: 4.99841178e-07
Iter: 672 loss: 4.99601583e-07
Iter: 673 loss: 4.99491136e-07
Iter: 674 loss: 4.99867951e-07
Iter: 675 loss: 4.9946857e-07
Iter: 676 loss: 4.99338057e-07
Iter: 677 loss: 5.00421947e-07
Iter: 678 loss: 4.99310829e-07
Iter: 679 loss: 4.99192424e-07
Iter: 680 loss: 4.9918674e-07
Iter: 681 loss: 4.99149564e-07
Iter: 682 loss: 4.99066289e-07
Iter: 683 loss: 4.99050145e-07
Iter: 684 loss: 4.9900973e-07
Iter: 685 loss: 4.98894281e-07
Iter: 686 loss: 5.00576618e-07
Iter: 687 loss: 4.98897748e-07
Iter: 688 loss: 4.98843065e-07
Iter: 689 loss: 4.98822033e-07
Iter: 690 loss: 4.9879992e-07
Iter: 691 loss: 4.98873305e-07
Iter: 692 loss: 4.98748477e-07
Iter: 693 loss: 4.98691406e-07
Iter: 694 loss: 4.98637291e-07
Iter: 695 loss: 4.98631948e-07
Iter: 696 loss: 4.98529744e-07
Iter: 697 loss: 4.99236535e-07
Iter: 698 loss: 4.98543159e-07
Iter: 699 loss: 4.98408895e-07
Iter: 700 loss: 4.98761e-07
Iter: 701 loss: 4.98398776e-07
Iter: 702 loss: 4.98293048e-07
Iter: 703 loss: 4.98083523e-07
Iter: 704 loss: 5.00999192e-07
Iter: 705 loss: 4.9809978e-07
Iter: 706 loss: 4.97899862e-07
Iter: 707 loss: 4.99099315e-07
Iter: 708 loss: 4.9789719e-07
Iter: 709 loss: 4.97833867e-07
Iter: 710 loss: 4.97840801e-07
Iter: 711 loss: 4.97777364e-07
Iter: 712 loss: 4.97683061e-07
Iter: 713 loss: 4.97644635e-07
Iter: 714 loss: 4.97646283e-07
Iter: 715 loss: 4.97609051e-07
Iter: 716 loss: 4.97561928e-07
Iter: 717 loss: 4.97485246e-07
Iter: 718 loss: 4.97494966e-07
Iter: 719 loss: 4.97422661e-07
Iter: 720 loss: 4.97483882e-07
Iter: 721 loss: 4.97369797e-07
Iter: 722 loss: 4.97300107e-07
Iter: 723 loss: 4.9729158e-07
Iter: 724 loss: 4.97239625e-07
Iter: 725 loss: 4.97102292e-07
Iter: 726 loss: 4.98163104e-07
Iter: 727 loss: 4.97062956e-07
Iter: 728 loss: 4.96895154e-07
Iter: 729 loss: 4.97887925e-07
Iter: 730 loss: 4.96867187e-07
Iter: 731 loss: 4.96737698e-07
Iter: 732 loss: 4.98073859e-07
Iter: 733 loss: 4.96757593e-07
Iter: 734 loss: 4.96629866e-07
Iter: 735 loss: 4.97198641e-07
Iter: 736 loss: 4.96574273e-07
Iter: 737 loss: 4.96506914e-07
Iter: 738 loss: 4.9644683e-07
Iter: 739 loss: 4.96433e-07
Iter: 740 loss: 4.96311884e-07
Iter: 741 loss: 4.96450582e-07
Iter: 742 loss: 4.96276755e-07
Iter: 743 loss: 4.96200755e-07
Iter: 744 loss: 4.96186431e-07
Iter: 745 loss: 4.96135101e-07
Iter: 746 loss: 4.96148687e-07
Iter: 747 loss: 4.96095424e-07
Iter: 748 loss: 4.96038524e-07
Iter: 749 loss: 4.96058817e-07
Iter: 750 loss: 4.95960364e-07
Iter: 751 loss: 4.95907784e-07
Iter: 752 loss: 4.95908296e-07
Iter: 753 loss: 4.95887434e-07
Iter: 754 loss: 4.95882546e-07
Iter: 755 loss: 4.9584321e-07
Iter: 756 loss: 4.9577568e-07
Iter: 757 loss: 4.95683537e-07
Iter: 758 loss: 4.95668132e-07
Iter: 759 loss: 4.95611e-07
Iter: 760 loss: 4.9627937e-07
Iter: 761 loss: 4.95588267e-07
Iter: 762 loss: 4.95483732e-07
Iter: 763 loss: 4.96044947e-07
Iter: 764 loss: 4.95443828e-07
Iter: 765 loss: 4.95374e-07
Iter: 766 loss: 4.95254e-07
Iter: 767 loss: 4.95254085e-07
Iter: 768 loss: 4.95245e-07
Iter: 769 loss: 4.95207701e-07
Iter: 770 loss: 4.95184736e-07
Iter: 771 loss: 4.95067923e-07
Iter: 772 loss: 4.95953316e-07
Iter: 773 loss: 4.95040354e-07
Iter: 774 loss: 4.94951905e-07
Iter: 775 loss: 4.94949802e-07
Iter: 776 loss: 4.9487403e-07
Iter: 777 loss: 4.95431891e-07
Iter: 778 loss: 4.94911603e-07
Iter: 779 loss: 4.94871188e-07
Iter: 780 loss: 4.94804908e-07
Iter: 781 loss: 4.95514087e-07
Iter: 782 loss: 4.94780238e-07
Iter: 783 loss: 4.9467684e-07
Iter: 784 loss: 4.95232712e-07
Iter: 785 loss: 4.94688436e-07
Iter: 786 loss: 4.94619258e-07
Iter: 787 loss: 4.94616e-07
Iter: 788 loss: 4.94578444e-07
Iter: 789 loss: 4.94477149e-07
Iter: 790 loss: 4.95967186e-07
Iter: 791 loss: 4.94460437e-07
Iter: 792 loss: 4.94374603e-07
Iter: 793 loss: 4.94966343e-07
Iter: 794 loss: 4.94379321e-07
Iter: 795 loss: 4.94330038e-07
Iter: 796 loss: 4.94979e-07
Iter: 797 loss: 4.94324524e-07
Iter: 798 loss: 4.94280357e-07
Iter: 799 loss: 4.94255e-07
Iter: 800 loss: 4.95763516e-07
Iter: 801 loss: 4.94240055e-07
Iter: 802 loss: 4.94133474e-07
Iter: 803 loss: 4.94694802e-07
Iter: 804 loss: 4.94173719e-07
Iter: 805 loss: 4.94063443e-07
Iter: 806 loss: 4.94270807e-07
Iter: 807 loss: 4.94004e-07
Iter: 808 loss: 4.93993184e-07
Iter: 809 loss: 4.93869436e-07
Iter: 810 loss: 4.96062285e-07
Iter: 811 loss: 4.9388575e-07
Iter: 812 loss: 4.93793038e-07
Iter: 813 loss: 4.93781215e-07
Iter: 814 loss: 4.93708967e-07
Iter: 815 loss: 4.93646212e-07
Iter: 816 loss: 4.9363689e-07
Iter: 817 loss: 4.93577545e-07
Iter: 818 loss: 4.93468406e-07
Iter: 819 loss: 4.93464142e-07
Iter: 820 loss: 4.93361654e-07
Iter: 821 loss: 4.93376092e-07
Iter: 822 loss: 4.93328287e-07
Iter: 823 loss: 4.93408493e-07
Iter: 824 loss: 4.93286791e-07
Iter: 825 loss: 4.93231823e-07
Iter: 826 loss: 4.93111372e-07
Iter: 827 loss: 4.93111429e-07
Iter: 828 loss: 4.93024629e-07
Iter: 829 loss: 4.93863809e-07
Iter: 830 loss: 4.93018547e-07
Iter: 831 loss: 4.92862853e-07
Iter: 832 loss: 4.93043e-07
Iter: 833 loss: 4.92846084e-07
Iter: 834 loss: 4.9273433e-07
Iter: 835 loss: 4.92730464e-07
Iter: 836 loss: 4.92672598e-07
Iter: 837 loss: 4.92533786e-07
Iter: 838 loss: 4.92554477e-07
Iter: 839 loss: 4.92507297e-07
Iter: 840 loss: 4.92384686e-07
Iter: 841 loss: 4.94860103e-07
Iter: 842 loss: 4.92378e-07
Iter: 843 loss: 4.923196e-07
Iter: 844 loss: 4.9313951e-07
Iter: 845 loss: 4.92341201e-07
Iter: 846 loss: 4.9223695e-07
Iter: 847 loss: 4.92348761e-07
Iter: 848 loss: 4.92195454e-07
Iter: 849 loss: 4.92139691e-07
Iter: 850 loss: 4.92030722e-07
Iter: 851 loss: 4.91991784e-07
Iter: 852 loss: 4.91918513e-07
Iter: 853 loss: 4.93198513e-07
Iter: 854 loss: 4.9190777e-07
Iter: 855 loss: 4.91805849e-07
Iter: 856 loss: 4.91863602e-07
Iter: 857 loss: 4.91718538e-07
Iter: 858 loss: 4.91576202e-07
Iter: 859 loss: 4.91677e-07
Iter: 860 loss: 4.9145558e-07
Iter: 861 loss: 4.91311653e-07
Iter: 862 loss: 4.92543677e-07
Iter: 863 loss: 4.91317337e-07
Iter: 864 loss: 4.91221954e-07
Iter: 865 loss: 4.91602236e-07
Iter: 866 loss: 4.91177502e-07
Iter: 867 loss: 4.91116452e-07
Iter: 868 loss: 4.91068192e-07
Iter: 869 loss: 4.91045057e-07
Iter: 870 loss: 4.90989e-07
Iter: 871 loss: 4.90962407e-07
Iter: 872 loss: 4.9096451e-07
Iter: 873 loss: 4.90971445e-07
Iter: 874 loss: 4.90957291e-07
Iter: 875 loss: 4.90953482e-07
Iter: 876 loss: 4.90958e-07
Iter: 877 loss: 4.90955358e-07
Iter: 878 loss: 4.90958087e-07
Iter: 879 loss: 4.90962293e-07
Iter: 880 loss: 4.90957916e-07
Iter: 881 loss: 4.90960872e-07
Iter: 882 loss: 4.90966443e-07
Iter: 883 loss: 4.90968432e-07
Iter: 884 loss: 4.90975424e-07
Iter: 885 loss: 4.90967864e-07
Iter: 886 loss: 4.90957916e-07
Iter: 887 loss: 4.90964453e-07
Iter: 888 loss: 4.9096127e-07
Iter: 889 loss: 4.90965363e-07
Iter: 890 loss: 4.90963487e-07
Iter: 891 loss: 4.90962577e-07
Iter: 892 loss: 4.90963089e-07
Iter: 893 loss: 4.90962236e-07
Iter: 894 loss: 4.90962634e-07
Iter: 895 loss: 4.90962634e-07
Iter: 896 loss: 4.90963089e-07
Iter: 897 loss: 4.90963089e-07
Iter: 898 loss: 4.90962634e-07
Iter: 899 loss: 4.90824391e-07
Iter: 900 loss: 4.91995195e-07
Iter: 901 loss: 4.90816e-07
Iter: 902 loss: 4.9074805e-07
Iter: 903 loss: 4.90897435e-07
Iter: 904 loss: 4.90742082e-07
Iter: 905 loss: 4.90642492e-07
Iter: 906 loss: 4.91155333e-07
Iter: 907 loss: 4.90631692e-07
Iter: 908 loss: 4.90580362e-07
Iter: 909 loss: 4.90464686e-07
Iter: 910 loss: 4.91279e-07
Iter: 911 loss: 4.90442289e-07
Iter: 912 loss: 4.90299158e-07
Iter: 913 loss: 4.91576e-07
Iter: 914 loss: 4.90313937e-07
Iter: 915 loss: 4.90233333e-07
Iter: 916 loss: 4.9041671e-07
Iter: 917 loss: 4.90176092e-07
Iter: 918 loss: 4.90189905e-07
Iter: 919 loss: 4.90126808e-07
Iter: 920 loss: 4.90101229e-07
Iter: 921 loss: 4.90073774e-07
Iter: 922 loss: 4.90091111e-07
Iter: 923 loss: 4.90031e-07
Iter: 924 loss: 4.90022671e-07
Iter: 925 loss: 4.8999334e-07
Iter: 926 loss: 4.8993553e-07
Iter: 927 loss: 4.90777438e-07
Iter: 928 loss: 4.89931e-07
Iter: 929 loss: 4.89875333e-07
Iter: 930 loss: 4.89850606e-07
Iter: 931 loss: 4.8984549e-07
Iter: 932 loss: 4.89780518e-07
Iter: 933 loss: 4.89966396e-07
Iter: 934 loss: 4.89782849e-07
Iter: 935 loss: 4.8971765e-07
Iter: 936 loss: 4.89981062e-07
Iter: 937 loss: 4.89689569e-07
Iter: 938 loss: 4.89641138e-07
Iter: 939 loss: 4.89655577e-07
Iter: 940 loss: 4.89590207e-07
Iter: 941 loss: 4.89523e-07
Iter: 942 loss: 4.89473564e-07
Iter: 943 loss: 4.89424906e-07
Iter: 944 loss: 4.89332365e-07
Iter: 945 loss: 4.89509262e-07
Iter: 946 loss: 4.89283252e-07
Iter: 947 loss: 4.89158765e-07
Iter: 948 loss: 4.89402851e-07
Iter: 949 loss: 4.89128354e-07
Iter: 950 loss: 4.89055196e-07
Iter: 951 loss: 4.9019593e-07
Iter: 952 loss: 4.89043316e-07
Iter: 953 loss: 4.88961291e-07
Iter: 954 loss: 4.89377214e-07
Iter: 955 loss: 4.88971182e-07
Iter: 956 loss: 4.8889251e-07
Iter: 957 loss: 4.88824753e-07
Iter: 958 loss: 4.88828164e-07
Iter: 959 loss: 4.88756655e-07
Iter: 960 loss: 4.88758246e-07
Iter: 961 loss: 4.88693729e-07
Iter: 962 loss: 4.8881941e-07
Iter: 963 loss: 4.88698902e-07
Iter: 964 loss: 4.88660135e-07
Iter: 965 loss: 4.8871118e-07
Iter: 966 loss: 4.8861682e-07
Iter: 967 loss: 4.88583055e-07
Iter: 968 loss: 4.88637795e-07
Iter: 969 loss: 4.88536728e-07
Iter: 970 loss: 4.88446517e-07
Iter: 971 loss: 4.88773139e-07
Iter: 972 loss: 4.8844305e-07
Iter: 973 loss: 4.88382284e-07
Iter: 974 loss: 4.8837444e-07
Iter: 975 loss: 4.88353635e-07
Iter: 976 loss: 4.88248929e-07
Iter: 977 loss: 4.88175147e-07
Iter: 978 loss: 4.88145304e-07
Iter: 979 loss: 4.88043213e-07
Iter: 980 loss: 4.88779278e-07
Iter: 981 loss: 4.88028377e-07
Iter: 982 loss: 4.87936575e-07
Iter: 983 loss: 4.87915202e-07
Iter: 984 loss: 4.878288e-07
Iter: 985 loss: 4.87715511e-07
Iter: 986 loss: 4.88323849e-07
Iter: 987 loss: 4.87710793e-07
Iter: 988 loss: 4.8763161e-07
Iter: 989 loss: 4.87616092e-07
Iter: 990 loss: 4.87550324e-07
Iter: 991 loss: 4.87580905e-07
Iter: 992 loss: 4.87541627e-07
Iter: 993 loss: 4.87474836e-07
Iter: 994 loss: 4.87388888e-07
Iter: 995 loss: 4.89216745e-07
Iter: 996 loss: 4.87386956e-07
Iter: 997 loss: 4.87333182e-07
Iter: 998 loss: 4.8732295e-07
Iter: 999 loss: 4.87270484e-07
Iter: 1000 loss: 4.87398154e-07
Iter: 1001 loss: 4.87246666e-07
Iter: 1002 loss: 4.8724911e-07
Iter: 1003 loss: 4.87241323e-07
Iter: 1004 loss: 4.87225407e-07
Iter: 1005 loss: 4.87232967e-07
Iter: 1006 loss: 4.87237571e-07
Iter: 1007 loss: 4.87229272e-07
Iter: 1008 loss: 4.87211537e-07
Iter: 1009 loss: 4.87227e-07
Iter: 1010 loss: 4.87240698e-07
Iter: 1011 loss: 4.87238367e-07
Iter: 1012 loss: 4.87246496e-07
Iter: 1013 loss: 4.87247462e-07
Iter: 1014 loss: 4.87251214e-07
Iter: 1015 loss: 4.87239e-07
Iter: 1016 loss: 4.8724479e-07
Iter: 1017 loss: 4.87249849e-07
Iter: 1018 loss: 4.87244961e-07
Iter: 1019 loss: 4.87249565e-07
Iter: 1020 loss: 4.87249224e-07
Iter: 1021 loss: 4.87249622e-07
Iter: 1022 loss: 4.87247576e-07
Iter: 1023 loss: 4.87244847e-07
Iter: 1024 loss: 4.87247576e-07
Iter: 1025 loss: 4.87246893e-07
Iter: 1026 loss: 4.87247462e-07
Iter: 1027 loss: 4.87246893e-07
Iter: 1028 loss: 4.87188e-07
Iter: 1029 loss: 4.87189652e-07
Iter: 1030 loss: 4.87126727e-07
Iter: 1031 loss: 4.8705283e-07
Iter: 1032 loss: 4.871e-07
Iter: 1033 loss: 4.87027592e-07
Iter: 1034 loss: 4.86965746e-07
Iter: 1035 loss: 4.86932322e-07
Iter: 1036 loss: 4.86913052e-07
Iter: 1037 loss: 4.86839554e-07
Iter: 1038 loss: 4.88103069e-07
Iter: 1039 loss: 4.86799593e-07
Iter: 1040 loss: 4.86735075e-07
Iter: 1041 loss: 4.86927775e-07
Iter: 1042 loss: 4.86701879e-07
Iter: 1043 loss: 4.86601721e-07
Iter: 1044 loss: 4.8674184e-07
Iter: 1045 loss: 4.86572446e-07
Iter: 1046 loss: 4.8651907e-07
Iter: 1047 loss: 4.87230295e-07
Iter: 1048 loss: 4.8652862e-07
Iter: 1049 loss: 4.86476551e-07
Iter: 1050 loss: 4.86486329e-07
Iter: 1051 loss: 4.86445117e-07
Iter: 1052 loss: 4.86396971e-07
Iter: 1053 loss: 4.86401632e-07
Iter: 1054 loss: 4.86360705e-07
Iter: 1055 loss: 4.8629505e-07
Iter: 1056 loss: 4.86673855e-07
Iter: 1057 loss: 4.86300564e-07
Iter: 1058 loss: 4.86257477e-07
Iter: 1059 loss: 4.86300792e-07
Iter: 1060 loss: 4.86247643e-07
Iter: 1061 loss: 4.86154136e-07
Iter: 1062 loss: 4.86193187e-07
Iter: 1063 loss: 4.86115368e-07
Iter: 1064 loss: 4.86058923e-07
Iter: 1065 loss: 4.86285217e-07
Iter: 1066 loss: 4.86040619e-07
Iter: 1067 loss: 4.85993723e-07
Iter: 1068 loss: 4.86253043e-07
Iter: 1069 loss: 4.86011857e-07
Iter: 1070 loss: 4.85954331e-07
Iter: 1071 loss: 4.85906128e-07
Iter: 1072 loss: 4.85856617e-07
Iter: 1073 loss: 4.85797955e-07
Iter: 1074 loss: 4.85720648e-07
Iter: 1075 loss: 4.85720136e-07
Iter: 1076 loss: 4.85598719e-07
Iter: 1077 loss: 4.86116278e-07
Iter: 1078 loss: 4.8559582e-07
Iter: 1079 loss: 4.85505666e-07
Iter: 1080 loss: 4.85899818e-07
Iter: 1081 loss: 4.85509929e-07
Iter: 1082 loss: 4.85412954e-07
Iter: 1083 loss: 4.86248609e-07
Iter: 1084 loss: 4.8541915e-07
Iter: 1085 loss: 4.85380667e-07
Iter: 1086 loss: 4.85351222e-07
Iter: 1087 loss: 4.85360033e-07
Iter: 1088 loss: 4.85305918e-07
Iter: 1089 loss: 4.85625606e-07
Iter: 1090 loss: 4.85294947e-07
Iter: 1091 loss: 4.85275677e-07
Iter: 1092 loss: 4.85470082e-07
Iter: 1093 loss: 4.85265389e-07
Iter: 1094 loss: 4.85205078e-07
Iter: 1095 loss: 4.85274882e-07
Iter: 1096 loss: 4.85217129e-07
Iter: 1097 loss: 4.8520468e-07
Iter: 1098 loss: 4.85160172e-07
Iter: 1099 loss: 4.85141641e-07
Iter: 1100 loss: 4.85120211e-07
Iter: 1101 loss: 4.85329224e-07
Iter: 1102 loss: 4.85102078e-07
Iter: 1103 loss: 4.8502136e-07
Iter: 1104 loss: 4.85346959e-07
Iter: 1105 loss: 4.8501181e-07
Iter: 1106 loss: 4.84965312e-07
Iter: 1107 loss: 4.84908355e-07
Iter: 1108 loss: 4.84884254e-07
Iter: 1109 loss: 4.84811778e-07
Iter: 1110 loss: 4.85038697e-07
Iter: 1111 loss: 4.84819054e-07
Iter: 1112 loss: 4.84713894e-07
Iter: 1113 loss: 4.85381179e-07
Iter: 1114 loss: 4.84740326e-07
Iter: 1115 loss: 4.84674274e-07
Iter: 1116 loss: 4.84986799e-07
Iter: 1117 loss: 4.84666032e-07
Iter: 1118 loss: 4.84633631e-07
Iter: 1119 loss: 4.84644e-07
Iter: 1120 loss: 4.84612826e-07
Iter: 1121 loss: 4.84533416e-07
Iter: 1122 loss: 4.84603106e-07
Iter: 1123 loss: 4.84520285e-07
Iter: 1124 loss: 4.84463e-07
Iter: 1125 loss: 4.84900283e-07
Iter: 1126 loss: 4.84458837e-07
Iter: 1127 loss: 4.84421435e-07
Iter: 1128 loss: 4.8442854e-07
Iter: 1129 loss: 4.8438892e-07
Iter: 1130 loss: 4.84309339e-07
Iter: 1131 loss: 4.84450311e-07
Iter: 1132 loss: 4.84249881e-07
Iter: 1133 loss: 4.84186e-07
Iter: 1134 loss: 4.84335658e-07
Iter: 1135 loss: 4.84161774e-07
Iter: 1136 loss: 4.84124882e-07
Iter: 1137 loss: 4.84152338e-07
Iter: 1138 loss: 4.84051725e-07
Iter: 1139 loss: 4.84028249e-07
Iter: 1140 loss: 4.84046723e-07
Iter: 1141 loss: 4.83955432e-07
Iter: 1142 loss: 4.83957649e-07
Iter: 1143 loss: 4.8390649e-07
Iter: 1144 loss: 4.83828217e-07
Iter: 1145 loss: 4.8472873e-07
Iter: 1146 loss: 4.83835947e-07
Iter: 1147 loss: 4.83789563e-07
Iter: 1148 loss: 4.84283305e-07
Iter: 1149 loss: 4.83782628e-07
Iter: 1150 loss: 4.8372226e-07
Iter: 1151 loss: 4.83813722e-07
Iter: 1152 loss: 4.83711517e-07
Iter: 1153 loss: 4.83681561e-07
Iter: 1154 loss: 4.83780184e-07
Iter: 1155 loss: 4.8365763e-07
Iter: 1156 loss: 4.83645465e-07
Iter: 1157 loss: 4.83939402e-07
Iter: 1158 loss: 4.83630174e-07
Iter: 1159 loss: 4.8360431e-07
Iter: 1160 loss: 4.83648e-07
Iter: 1161 loss: 4.83588e-07
Iter: 1162 loss: 4.83563156e-07
Iter: 1163 loss: 4.83671e-07
Iter: 1164 loss: 4.83568e-07
Iter: 1165 loss: 4.83502561e-07
Iter: 1166 loss: 4.83540703e-07
Iter: 1167 loss: 4.8350563e-07
Iter: 1168 loss: 4.83482893e-07
Iter: 1169 loss: 4.83814063e-07
Iter: 1170 loss: 4.83458507e-07
Iter: 1171 loss: 4.8343918e-07
Iter: 1172 loss: 4.83430654e-07
Iter: 1173 loss: 4.83417182e-07
Iter: 1174 loss: 4.83369092e-07
Iter: 1175 loss: 4.83325e-07
Iter: 1176 loss: 4.83314352e-07
Iter: 1177 loss: 4.83230679e-07
Iter: 1178 loss: 4.83590327e-07
Iter: 1179 loss: 4.83194867e-07
Iter: 1180 loss: 4.83139445e-07
Iter: 1181 loss: 4.8313558e-07
Iter: 1182 loss: 4.83119948e-07
Iter: 1183 loss: 4.83112e-07
Iter: 1184 loss: 4.83077145e-07
Iter: 1185 loss: 4.83021836e-07
Iter: 1186 loss: 4.83258702e-07
Iter: 1187 loss: 4.82990913e-07
Iter: 1188 loss: 4.82968744e-07
Iter: 1189 loss: 4.83089536e-07
Iter: 1190 loss: 4.82967152e-07
Iter: 1191 loss: 4.82909741e-07
Iter: 1192 loss: 4.83101758e-07
Iter: 1193 loss: 4.82910366e-07
Iter: 1194 loss: 4.82896212e-07
Iter: 1195 loss: 4.82879443e-07
Iter: 1196 loss: 4.82877226e-07
Iter: 1197 loss: 4.82837038e-07
Iter: 1198 loss: 4.82876715e-07
Iter: 1199 loss: 4.82795144e-07
Iter: 1200 loss: 4.82734436e-07
Iter: 1201 loss: 4.82958626e-07
Iter: 1202 loss: 4.82716587e-07
Iter: 1203 loss: 4.82663779e-07
Iter: 1204 loss: 4.82919631e-07
Iter: 1205 loss: 4.826652e-07
Iter: 1206 loss: 4.82607334e-07
Iter: 1207 loss: 4.82529913e-07
Iter: 1208 loss: 4.82551457e-07
Iter: 1209 loss: 4.82441123e-07
Iter: 1210 loss: 4.82833684e-07
Iter: 1211 loss: 4.82434e-07
Iter: 1212 loss: 4.8237473e-07
Iter: 1213 loss: 4.83036445e-07
Iter: 1214 loss: 4.82404062e-07
Iter: 1215 loss: 4.82332666e-07
Iter: 1216 loss: 4.82431517e-07
Iter: 1217 loss: 4.82337839e-07
Iter: 1218 loss: 4.82276107e-07
Iter: 1219 loss: 4.82419296e-07
Iter: 1220 loss: 4.82265818e-07
Iter: 1221 loss: 4.82253085e-07
Iter: 1222 loss: 4.82285827e-07
Iter: 1223 loss: 4.82220571e-07
Iter: 1224 loss: 4.82196356e-07
Iter: 1225 loss: 4.82409291e-07
Iter: 1226 loss: 4.8218044e-07
Iter: 1227 loss: 4.82122118e-07
Iter: 1228 loss: 4.82128257e-07
Iter: 1229 loss: 4.82097846e-07
Iter: 1230 loss: 4.82038047e-07
Iter: 1231 loss: 4.82126097e-07
Iter: 1232 loss: 4.81995244e-07
Iter: 1233 loss: 4.81898269e-07
Iter: 1234 loss: 4.82034181e-07
Iter: 1235 loss: 4.81874849e-07
Iter: 1236 loss: 4.81810048e-07
Iter: 1237 loss: 4.8179993e-07
Iter: 1238 loss: 4.81776169e-07
Iter: 1239 loss: 4.81692041e-07
Iter: 1240 loss: 4.83416329e-07
Iter: 1241 loss: 4.81687835e-07
Iter: 1242 loss: 4.81606321e-07
Iter: 1243 loss: 4.818055e-07
Iter: 1244 loss: 4.81597397e-07
Iter: 1245 loss: 4.81539928e-07
Iter: 1246 loss: 4.81949826e-07
Iter: 1247 loss: 4.81535892e-07
Iter: 1248 loss: 4.81448581e-07
Iter: 1249 loss: 4.82006214e-07
Iter: 1250 loss: 4.81443863e-07
Iter: 1251 loss: 4.81412e-07
Iter: 1252 loss: 4.81358938e-07
Iter: 1253 loss: 4.81371274e-07
Iter: 1254 loss: 4.81299e-07
Iter: 1255 loss: 4.81952497e-07
Iter: 1256 loss: 4.81289419e-07
Iter: 1257 loss: 4.81229506e-07
Iter: 1258 loss: 4.812523e-07
Iter: 1259 loss: 4.81167604e-07
Iter: 1260 loss: 4.81106611e-07
Iter: 1261 loss: 4.81575682e-07
Iter: 1262 loss: 4.81092115e-07
Iter: 1263 loss: 4.81058521e-07
Iter: 1264 loss: 4.81116558e-07
Iter: 1265 loss: 4.8100037e-07
Iter: 1266 loss: 4.80944323e-07
Iter: 1267 loss: 4.81131224e-07
Iter: 1268 loss: 4.80931817e-07
Iter: 1269 loss: 4.80894528e-07
Iter: 1270 loss: 4.80909421e-07
Iter: 1271 loss: 4.8088458e-07
Iter: 1272 loss: 4.80832568e-07
Iter: 1273 loss: 4.8188349e-07
Iter: 1274 loss: 4.80833307e-07
Iter: 1275 loss: 4.80771575e-07
Iter: 1276 loss: 4.80803635e-07
Iter: 1277 loss: 4.80754068e-07
Iter: 1278 loss: 4.80691597e-07
Iter: 1279 loss: 4.80792437e-07
Iter: 1280 loss: 4.80644758e-07
Iter: 1281 loss: 4.80598942e-07
Iter: 1282 loss: 4.80577228e-07
Iter: 1283 loss: 4.80547e-07
Iter: 1284 loss: 4.80519532e-07
Iter: 1285 loss: 4.80465701e-07
Iter: 1286 loss: 4.80420624e-07
Iter: 1287 loss: 4.80969391e-07
Iter: 1288 loss: 4.80412837e-07
Iter: 1289 loss: 4.80360086e-07
Iter: 1290 loss: 4.80344738e-07
Iter: 1291 loss: 4.80308e-07
Iter: 1292 loss: 4.8027789e-07
Iter: 1293 loss: 4.80279937e-07
Iter: 1294 loss: 4.80243784e-07
Iter: 1295 loss: 4.8022207e-07
Iter: 1296 loss: 4.80209337e-07
Iter: 1297 loss: 4.80156586e-07
Iter: 1298 loss: 4.80279596e-07
Iter: 1299 loss: 4.80144763e-07
Iter: 1300 loss: 4.80099288e-07
Iter: 1301 loss: 4.80348262e-07
Iter: 1302 loss: 4.80124413e-07
Iter: 1303 loss: 4.80077631e-07
Iter: 1304 loss: 4.80068138e-07
Iter: 1305 loss: 4.80039148e-07
Iter: 1306 loss: 4.80014933e-07
Iter: 1307 loss: 4.79914661e-07
Iter: 1308 loss: 4.81910774e-07
Iter: 1309 loss: 4.79915911e-07
Iter: 1310 loss: 4.79814901e-07
Iter: 1311 loss: 4.80427e-07
Iter: 1312 loss: 4.79799667e-07
Iter: 1313 loss: 4.79726452e-07
Iter: 1314 loss: 4.79739697e-07
Iter: 1315 loss: 4.79683536e-07
Iter: 1316 loss: 4.79759592e-07
Iter: 1317 loss: 4.79656478e-07
Iter: 1318 loss: 4.79634309e-07
Iter: 1319 loss: 4.79665232e-07
Iter: 1320 loss: 4.79620098e-07
Iter: 1321 loss: 4.79561606e-07
Iter: 1322 loss: 4.79737139e-07
Iter: 1323 loss: 4.79554956e-07
Iter: 1324 loss: 4.79478956e-07
Iter: 1325 loss: 4.79497203e-07
Iter: 1326 loss: 4.79464518e-07
Iter: 1327 loss: 4.79405116e-07
Iter: 1328 loss: 4.79375558e-07
Iter: 1329 loss: 4.79351343e-07
Iter: 1330 loss: 4.79272e-07
Iter: 1331 loss: 4.81141342e-07
Iter: 1332 loss: 4.79270739e-07
Iter: 1333 loss: 4.79217306e-07
Iter: 1334 loss: 4.79927849e-07
Iter: 1335 loss: 4.79180812e-07
Iter: 1336 loss: 4.79134428e-07
Iter: 1337 loss: 4.79412392e-07
Iter: 1338 loss: 4.79133348e-07
Iter: 1339 loss: 4.7910271e-07
Iter: 1340 loss: 4.78996867e-07
Iter: 1341 loss: 4.79003e-07
Iter: 1342 loss: 4.78933543e-07
Iter: 1343 loss: 4.79004825e-07
Iter: 1344 loss: 4.7893127e-07
Iter: 1345 loss: 4.78838786e-07
Iter: 1346 loss: 4.78965944e-07
Iter: 1347 loss: 4.78800246e-07
Iter: 1348 loss: 4.78758693e-07
Iter: 1349 loss: 4.78758125e-07
Iter: 1350 loss: 4.78703669e-07
Iter: 1351 loss: 4.78641482e-07
Iter: 1352 loss: 4.78627157e-07
Iter: 1353 loss: 4.78562811e-07
Iter: 1354 loss: 4.79420066e-07
Iter: 1355 loss: 4.78524839e-07
Iter: 1356 loss: 4.78453046e-07
Iter: 1357 loss: 4.78405525e-07
Iter: 1358 loss: 4.78364598e-07
Iter: 1359 loss: 4.78299853e-07
Iter: 1360 loss: 4.78800416e-07
Iter: 1361 loss: 4.78306788e-07
Iter: 1362 loss: 4.78220045e-07
Iter: 1363 loss: 4.78582251e-07
Iter: 1364 loss: 4.78205948e-07
Iter: 1365 loss: 4.781391e-07
Iter: 1366 loss: 4.78100333e-07
Iter: 1367 loss: 4.78110735e-07
Iter: 1368 loss: 4.78072252e-07
Iter: 1369 loss: 4.7809e-07
Iter: 1370 loss: 4.78034e-07
Iter: 1371 loss: 4.78043376e-07
Iter: 1372 loss: 4.78026948e-07
Iter: 1373 loss: 4.77944582e-07
Iter: 1374 loss: 4.77907065e-07
Iter: 1375 loss: 4.77910589e-07
Iter: 1376 loss: 4.77819412e-07
Iter: 1377 loss: 4.77924118e-07
Iter: 1378 loss: 4.77772744e-07
Iter: 1379 loss: 4.77664628e-07
Iter: 1380 loss: 4.78543257e-07
Iter: 1381 loss: 4.77681283e-07
Iter: 1382 loss: 4.77581921e-07
Iter: 1383 loss: 4.78265918e-07
Iter: 1384 loss: 4.77569415e-07
Iter: 1385 loss: 4.77511037e-07
Iter: 1386 loss: 4.77476817e-07
Iter: 1387 loss: 4.77467e-07
Iter: 1388 loss: 4.7739104e-07
Iter: 1389 loss: 4.78451284e-07
Iter: 1390 loss: 4.77402409e-07
Iter: 1391 loss: 4.77354e-07
Iter: 1392 loss: 4.77312e-07
Iter: 1393 loss: 4.77298045e-07
Iter: 1394 loss: 4.77217213e-07
Iter: 1395 loss: 4.77151275e-07
Iter: 1396 loss: 4.77137405e-07
Iter: 1397 loss: 4.7709176e-07
Iter: 1398 loss: 4.77041169e-07
Iter: 1399 loss: 4.77028323e-07
Iter: 1400 loss: 4.77074195e-07
Iter: 1401 loss: 4.76962327e-07
Iter: 1402 loss: 4.76913669e-07
Iter: 1403 loss: 4.76872401e-07
Iter: 1404 loss: 4.76855746e-07
Iter: 1405 loss: 4.76904802e-07
Iter: 1406 loss: 4.76834e-07
Iter: 1407 loss: 4.76787562e-07
Iter: 1408 loss: 4.76750131e-07
Iter: 1409 loss: 4.77611593e-07
Iter: 1410 loss: 4.76737625e-07
Iter: 1411 loss: 4.76694908e-07
Iter: 1412 loss: 4.76740524e-07
Iter: 1413 loss: 4.76681493e-07
Iter: 1414 loss: 4.76622603e-07
Iter: 1415 loss: 4.77009e-07
Iter: 1416 loss: 4.76610694e-07
Iter: 1417 loss: 4.76540436e-07
Iter: 1418 loss: 4.7679498e-07
Iter: 1419 loss: 4.76558313e-07
Iter: 1420 loss: 4.76493966e-07
Iter: 1421 loss: 4.76450055e-07
Iter: 1422 loss: 4.76438572e-07
Iter: 1423 loss: 4.76375192e-07
Iter: 1424 loss: 4.76552941e-07
Iter: 1425 loss: 4.76343473e-07
Iter: 1426 loss: 4.76267815e-07
Iter: 1427 loss: 4.76960054e-07
Iter: 1428 loss: 4.76265086e-07
Iter: 1429 loss: 4.76226944e-07
Iter: 1430 loss: 4.76150291e-07
Iter: 1431 loss: 4.77458684e-07
Iter: 1432 loss: 4.76165326e-07
Iter: 1433 loss: 4.76084409e-07
Iter: 1434 loss: 4.76309566e-07
Iter: 1435 loss: 4.7604297e-07
Iter: 1436 loss: 4.76035154e-07
Iter: 1437 loss: 4.76013128e-07
Iter: 1438 loss: 4.76001077e-07
Iter: 1439 loss: 4.7592178e-07
Iter: 1440 loss: 4.77026958e-07
Iter: 1441 loss: 4.75922803e-07
Iter: 1442 loss: 4.75881961e-07
Iter: 1443 loss: 4.76044306e-07
Iter: 1444 loss: 4.75847912e-07
Iter: 1445 loss: 4.75777483e-07
Iter: 1446 loss: 4.76155464e-07
Iter: 1447 loss: 4.7578763e-07
Iter: 1448 loss: 4.7572027e-07
Iter: 1449 loss: 4.75614826e-07
Iter: 1450 loss: 4.773255e-07
Iter: 1451 loss: 4.7559223e-07
Iter: 1452 loss: 4.75536581e-07
Iter: 1453 loss: 4.76716309e-07
Iter: 1454 loss: 4.75535046e-07
Iter: 1455 loss: 4.75463651e-07
Iter: 1456 loss: 4.75926413e-07
Iter: 1457 loss: 4.75462969e-07
Iter: 1458 loss: 4.75403169e-07
Iter: 1459 loss: 4.75486e-07
Iter: 1460 loss: 4.75357524e-07
Iter: 1461 loss: 4.75336776e-07
Iter: 1462 loss: 4.75503214e-07
Iter: 1463 loss: 4.75339334e-07
Iter: 1464 loss: 4.75265495e-07
Iter: 1465 loss: 4.75245031e-07
Iter: 1466 loss: 4.75215302e-07
Iter: 1467 loss: 4.75177387e-07
Iter: 1468 loss: 4.75193019e-07
Iter: 1469 loss: 4.75133163e-07
Iter: 1470 loss: 4.75062649e-07
Iter: 1471 loss: 4.75207855e-07
Iter: 1472 loss: 4.75034199e-07
Iter: 1473 loss: 4.74974598e-07
Iter: 1474 loss: 4.74957659e-07
Iter: 1475 loss: 4.74950951e-07
Iter: 1476 loss: 4.7489425e-07
Iter: 1477 loss: 4.75845297e-07
Iter: 1478 loss: 4.74857501e-07
Iter: 1479 loss: 4.74820553e-07
Iter: 1480 loss: 4.74822855e-07
Iter: 1481 loss: 4.74804153e-07
Iter: 1482 loss: 4.74739068e-07
Iter: 1483 loss: 4.74730115e-07
Iter: 1484 loss: 4.74682395e-07
Iter: 1485 loss: 4.74641894e-07
Iter: 1486 loss: 4.74631179e-07
Iter: 1487 loss: 4.74550859e-07
Iter: 1488 loss: 4.75231531e-07
Iter: 1489 loss: 4.74529202e-07
Iter: 1490 loss: 4.74496375e-07
Iter: 1491 loss: 4.74790482e-07
Iter: 1492 loss: 4.74470966e-07
Iter: 1493 loss: 4.74436149e-07
Iter: 1494 loss: 4.74512632e-07
Iter: 1495 loss: 4.74409944e-07
Iter: 1496 loss: 4.74338179e-07
Iter: 1497 loss: 4.74465111e-07
Iter: 1498 loss: 4.74347246e-07
Iter: 1499 loss: 4.74271587e-07
Iter: 1500 loss: 4.74313339e-07
Iter: 1501 loss: 4.74217586e-07
Iter: 1502 loss: 4.74168445e-07
Iter: 1503 loss: 4.74181661e-07
Iter: 1504 loss: 4.7413323e-07
Iter: 1505 loss: 4.74083265e-07
Iter: 1506 loss: 4.74326669e-07
Iter: 1507 loss: 4.74062858e-07
Iter: 1508 loss: 4.73998483e-07
Iter: 1509 loss: 4.74094577e-07
Iter: 1510 loss: 4.7396864e-07
Iter: 1511 loss: 4.73925525e-07
Iter: 1512 loss: 4.73943601e-07
Iter: 1513 loss: 4.73864588e-07
Iter: 1514 loss: 4.73872888e-07
Iter: 1515 loss: 4.73830198e-07
Iter: 1516 loss: 4.73790863e-07
Iter: 1517 loss: 4.73749225e-07
Iter: 1518 loss: 4.73714437e-07
Iter: 1519 loss: 4.73633349e-07
Iter: 1520 loss: 4.74354437e-07
Iter: 1521 loss: 4.73635339e-07
Iter: 1522 loss: 4.73577472e-07
Iter: 1523 loss: 4.73489365e-07
Iter: 1524 loss: 4.7348982e-07
Iter: 1525 loss: 4.73467168e-07
Iter: 1526 loss: 4.73456168e-07
Iter: 1527 loss: 4.73411546e-07
Iter: 1528 loss: 4.73357829e-07
Iter: 1529 loss: 4.73380254e-07
Iter: 1530 loss: 4.73349274e-07
Iter: 1531 loss: 4.73340037e-07
Iter: 1532 loss: 4.73316788e-07
Iter: 1533 loss: 4.7328075e-07
Iter: 1534 loss: 4.7326651e-07
Iter: 1535 loss: 4.73247781e-07
Iter: 1536 loss: 4.73294847e-07
Iter: 1537 loss: 4.732197e-07
Iter: 1538 loss: 4.73175561e-07
Iter: 1539 loss: 4.73158082e-07
Iter: 1540 loss: 4.7315e-07
Iter: 1541 loss: 4.73074891e-07
Iter: 1542 loss: 4.73124658e-07
Iter: 1543 loss: 4.73023533e-07
Iter: 1544 loss: 4.72937614e-07
Iter: 1545 loss: 4.73260059e-07
Iter: 1546 loss: 4.72931617e-07
Iter: 1547 loss: 4.72832426e-07
Iter: 1548 loss: 4.73946471e-07
Iter: 1549 loss: 4.72852804e-07
Iter: 1550 loss: 4.72794056e-07
Iter: 1551 loss: 4.72730392e-07
Iter: 1552 loss: 4.72718625e-07
Iter: 1553 loss: 4.72703903e-07
Iter: 1554 loss: 4.72671843e-07
Iter: 1555 loss: 4.72664738e-07
Iter: 1556 loss: 4.72643023e-07
Iter: 1557 loss: 4.72601613e-07
Iter: 1558 loss: 4.72590955e-07
Iter: 1559 loss: 4.72870568e-07
Iter: 1560 loss: 4.72588198e-07
Iter: 1561 loss: 4.72564835e-07
Iter: 1562 loss: 4.72569866e-07
Iter: 1563 loss: 4.72519901e-07
Iter: 1564 loss: 4.72512681e-07
Iter: 1565 loss: 4.72625572e-07
Iter: 1566 loss: 4.72504041e-07
Iter: 1567 loss: 4.72459817e-07
Iter: 1568 loss: 4.72477154e-07
Iter: 1569 loss: 4.72406981e-07
Iter: 1570 loss: 4.72402235e-07
Iter: 1571 loss: 4.72375632e-07
Iter: 1572 loss: 4.72334108e-07
Iter: 1573 loss: 4.72266976e-07
Iter: 1574 loss: 4.72260126e-07
Iter: 1575 loss: 4.72205585e-07
Iter: 1576 loss: 4.72142659e-07
Iter: 1577 loss: 4.72614545e-07
Iter: 1578 loss: 4.72129784e-07
Iter: 1579 loss: 4.72067825e-07
Iter: 1580 loss: 4.7255304e-07
Iter: 1581 loss: 4.72078483e-07
Iter: 1582 loss: 4.72026443e-07
Iter: 1583 loss: 4.72294289e-07
Iter: 1584 loss: 4.72000124e-07
Iter: 1585 loss: 4.72002256e-07
Iter: 1586 loss: 4.72004444e-07
Iter: 1587 loss: 4.72004643e-07
Iter: 1588 loss: 4.72020076e-07
Iter: 1589 loss: 4.7199245e-07
Iter: 1590 loss: 4.72014392e-07
Iter: 1591 loss: 4.72014591e-07
Iter: 1592 loss: 4.71997026e-07
Iter: 1593 loss: 4.7201047e-07
Iter: 1594 loss: 4.71998192e-07
Iter: 1595 loss: 4.72005098e-07
Iter: 1596 loss: 4.71999442e-07
Iter: 1597 loss: 4.72009e-07
Iter: 1598 loss: 4.72003364e-07
Iter: 1599 loss: 4.71997907e-07
Iter: 1600 loss: 4.72007571e-07
Iter: 1601 loss: 4.72001886e-07
Iter: 1602 loss: 4.71997396e-07
Iter: 1603 loss: 4.71999499e-07
Iter: 1604 loss: 4.72001375e-07
Iter: 1605 loss: 4.71999158e-07
Iter: 1606 loss: 4.71999073e-07
Iter: 1607 loss: 4.71999101e-07
Iter: 1608 loss: 4.719993e-07
Iter: 1609 loss: 4.719993e-07
Iter: 1610 loss: 4.719993e-07
Iter: 1611 loss: 4.719993e-07
Iter: 1612 loss: 4.72000892e-07
Iter: 1613 loss: 4.719993e-07
Iter: 1614 loss: 4.71860233e-07
Iter: 1615 loss: 4.74051433e-07
Iter: 1616 loss: 4.71883197e-07
Iter: 1617 loss: 4.71802053e-07
Iter: 1618 loss: 4.71823768e-07
Iter: 1619 loss: 4.71787388e-07
Iter: 1620 loss: 4.71783892e-07
Iter: 1621 loss: 4.7176718e-07
Iter: 1622 loss: 4.71718238e-07
Iter: 1623 loss: 4.71696666e-07
Iter: 1624 loss: 4.7168902e-07
Iter: 1625 loss: 4.71702492e-07
Iter: 1626 loss: 4.71707864e-07
Iter: 1627 loss: 4.71719375e-07
Iter: 1628 loss: 4.71721933e-07
Iter: 1629 loss: 4.71711076e-07
Iter: 1630 loss: 4.71712781e-07
Iter: 1631 loss: 4.71711161e-07
Iter: 1632 loss: 4.71714486e-07
Iter: 1633 loss: 4.71722274e-07
Iter: 1634 loss: 4.71714202e-07
Iter: 1635 loss: 4.71710052e-07
Iter: 1636 loss: 4.7170434e-07
Iter: 1637 loss: 4.71700588e-07
Iter: 1638 loss: 4.71698797e-07
Iter: 1639 loss: 4.71696978e-07
Iter: 1640 loss: 4.71699167e-07
Iter: 1641 loss: 4.71697e-07
Iter: 1642 loss: 4.71697462e-07
Iter: 1643 loss: 4.71696382e-07
Iter: 1644 loss: 4.71696069e-07
Iter: 1645 loss: 4.71697035e-07
Iter: 1646 loss: 4.71697433e-07
Iter: 1647 loss: 4.71696978e-07
Iter: 1648 loss: 4.71697149e-07
Iter: 1649 loss: 4.71697149e-07
Iter: 1650 loss: 4.71697433e-07
Iter: 1651 loss: 4.71697433e-07
Iter: 1652 loss: 4.71697149e-07
Iter: 1653 loss: 4.71697149e-07
Iter: 1654 loss: 4.71697433e-07
Iter: 1655 loss: 4.71697433e-07
Iter: 1656 loss: 4.71697149e-07
Iter: 1657 loss: 4.71697433e-07
Iter: 1658 loss: 4.71697433e-07
Iter: 1659 loss: 4.71697149e-07
Iter: 1660 loss: 4.71697149e-07
Iter: 1661 loss: 4.71697433e-07
Iter: 1662 loss: 4.71697149e-07
Iter: 1663 loss: 4.71697149e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8
+ date
Wed Oct 21 23:30:45 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/500_500_500_500_1 --function f1 --psi 0 --phi 0.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25fae9bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25fad667b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25fae078c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25fae07510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25fada2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25fad4d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25fad4df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25faca3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25faca3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25faca3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25fac611e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25d77399d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25d773d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25fac677b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25d76f1d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25fac5bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25d76f1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25face1488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25face10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25d76807b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25d76838c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25fac220d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25d7683950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25b0512c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25b0512bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25b0512620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25b0512510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25b049a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25b049ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25b0579f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25b059a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25b059a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25b03bc620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25b042a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25b0402ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25b0361f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.015667275
test_loss: 0.015092374
train_loss: 0.0050353543
test_loss: 0.00502581
train_loss: 0.00250872
test_loss: 0.002714365
train_loss: 0.0022767643
test_loss: 0.0018130499
train_loss: 0.0018026128
test_loss: 0.0016815267
train_loss: 0.0017333545
test_loss: 0.0018245563
train_loss: 0.0015915621
test_loss: 0.0015624479
train_loss: 0.0015468616
test_loss: 0.0016885671
train_loss: 0.0019039327
test_loss: 0.0017993563
train_loss: 0.0015940242
test_loss: 0.0024131483
train_loss: 0.0017971391
test_loss: 0.0015340165
train_loss: 0.0020549756
test_loss: 0.0016460568
train_loss: 0.0020884145
test_loss: 0.0016649717
train_loss: 0.0020623903
test_loss: 0.001578634
train_loss: 0.0017587007
test_loss: 0.0016736459
train_loss: 0.0020836527
test_loss: 0.001761117
train_loss: 0.0013137073
test_loss: 0.0012397082
train_loss: 0.0017065095
test_loss: 0.0015277234
train_loss: 0.0014706214
test_loss: 0.0014165046
train_loss: 0.0013402163
test_loss: 0.0012772352
train_loss: 0.0014205112
test_loss: 0.0015973164
train_loss: 0.0016961779
test_loss: 0.0014243507
train_loss: 0.0013348494
test_loss: 0.0012605421
train_loss: 0.001269823
test_loss: 0.0014881605
train_loss: 0.0016145816
test_loss: 0.0015500862
train_loss: 0.0016276657
test_loss: 0.0015897934
train_loss: 0.0016870018
test_loss: 0.0017207246
train_loss: 0.0013914828
test_loss: 0.0016268344
train_loss: 0.001332871
test_loss: 0.0014733433
train_loss: 0.0016194229
test_loss: 0.0012875936
train_loss: 0.001544183
test_loss: 0.0014457003
train_loss: 0.0015937274
test_loss: 0.0014923462
train_loss: 0.0015688019
test_loss: 0.0012992853
train_loss: 0.0017196238
test_loss: 0.001514273
train_loss: 0.0015905435
test_loss: 0.0013759127
train_loss: 0.0014323001
test_loss: 0.001545176
train_loss: 0.0013992566
test_loss: 0.0014747835
train_loss: 0.0017769767
test_loss: 0.0014138775
train_loss: 0.0014042391
test_loss: 0.0014457101
train_loss: 0.0014258226
test_loss: 0.0014279188
train_loss: 0.0013202109
test_loss: 0.0014785893
train_loss: 0.0013999683
test_loss: 0.0013209506
train_loss: 0.0015194904
test_loss: 0.0016230822
train_loss: 0.0015744475
test_loss: 0.0013999632
train_loss: 0.0013408641
test_loss: 0.0013673898
train_loss: 0.0014594223
test_loss: 0.0013662048
train_loss: 0.0014731563
test_loss: 0.0014855732
train_loss: 0.0016475997
test_loss: 0.001421834
train_loss: 0.0014633053
test_loss: 0.0014986262
train_loss: 0.0012881131
test_loss: 0.0013116304
train_loss: 0.0015159111
test_loss: 0.0014594262
train_loss: 0.0014532497
test_loss: 0.0016611463
train_loss: 0.0015921628
test_loss: 0.0013918417
train_loss: 0.001278948
test_loss: 0.0014495957
train_loss: 0.0016451225
test_loss: 0.0013677957
train_loss: 0.0012939373
test_loss: 0.0012637336
train_loss: 0.0014989101
test_loss: 0.0017502015
train_loss: 0.0012156971
test_loss: 0.0015398712
train_loss: 0.0022105796
test_loss: 0.0017317056
train_loss: 0.0020229437
test_loss: 0.0018452505
train_loss: 0.0013229841
test_loss: 0.0014476962
train_loss: 0.0015610657
test_loss: 0.0016357086
train_loss: 0.0016396465
test_loss: 0.0013211379
train_loss: 0.0016075707
test_loss: 0.0013420673
train_loss: 0.0016019307
test_loss: 0.0016037592
train_loss: 0.0015804593
test_loss: 0.0014930561
train_loss: 0.0012528105
test_loss: 0.001497084
train_loss: 0.0016024369
test_loss: 0.0013868586
train_loss: 0.0015900267
test_loss: 0.0013394828
train_loss: 0.0013563131
test_loss: 0.0016242244
train_loss: 0.0014524857
test_loss: 0.0014359305
train_loss: 0.001261462
test_loss: 0.0013337045
train_loss: 0.0012775159
test_loss: 0.0015926618
train_loss: 0.001727388
test_loss: 0.0012609314
train_loss: 0.0013258213
test_loss: 0.001389723
train_loss: 0.0013499752
test_loss: 0.0013439311
train_loss: 0.0012993017
test_loss: 0.0016232308
train_loss: 0.0015656041
test_loss: 0.0013887335
train_loss: 0.0013496694
test_loss: 0.0015224979
train_loss: 0.0012139843
test_loss: 0.001132466
train_loss: 0.0015466162
test_loss: 0.0013531796
train_loss: 0.0013154472
test_loss: 0.0012949437
train_loss: 0.001684596
test_loss: 0.0016220262
train_loss: 0.001543629
test_loss: 0.0013632968
train_loss: 0.0012641768
test_loss: 0.0014345808
train_loss: 0.001305161
test_loss: 0.0013293674
train_loss: 0.0012549028
test_loss: 0.0013193757
train_loss: 0.0011866873
test_loss: 0.0012181168
train_loss: 0.0012976003
test_loss: 0.0014649412
train_loss: 0.0014680156
test_loss: 0.0015331692
train_loss: 0.0015759035
test_loss: 0.0018282536
train_loss: 0.0014897468
test_loss: 0.0013632583
train_loss: 0.0013157139
test_loss: 0.0014894207
train_loss: 0.0015314524
test_loss: 0.0014252401
train_loss: 0.001593035
test_loss: 0.0014249262
train_loss: 0.0014603031
test_loss: 0.0014280082
train_loss: 0.0015524584
test_loss: 0.0014931075
train_loss: 0.0015059873
test_loss: 0.00137707
train_loss: 0.001206065
test_loss: 0.0014697497
train_loss: 0.0012783265
test_loss: 0.0011128432
train_loss: 0.0013832687
test_loss: 0.0014914506
train_loss: 0.0014619001
test_loss: 0.0013748002
train_loss: 0.0010645532
test_loss: 0.0012577787
train_loss: 0.0011540549
test_loss: 0.0011391826
train_loss: 0.0012624089
test_loss: 0.0014591892
train_loss: 0.00141311
test_loss: 0.0013243672
train_loss: 0.0011118792
test_loss: 0.0012886927
train_loss: 0.0012718678
test_loss: 0.001204291
train_loss: 0.001472828
test_loss: 0.0012713532
train_loss: 0.0015198687
test_loss: 0.001422559
train_loss: 0.0012217399
test_loss: 0.0013409723
train_loss: 0.0013025719
test_loss: 0.0014251546
train_loss: 0.0012454301
test_loss: 0.0014374454
train_loss: 0.00133226
test_loss: 0.0014202758
train_loss: 0.0012901241
test_loss: 0.0014602407
train_loss: 0.0014505514
test_loss: 0.0017339882
train_loss: 0.0013086085
test_loss: 0.0013371898
train_loss: 0.0015313141
test_loss: 0.0013251853
train_loss: 0.0014367831
test_loss: 0.0013982587
train_loss: 0.0015119181
test_loss: 0.001530706
train_loss: 0.001246888
test_loss: 0.001459114
train_loss: 0.0013551981
test_loss: 0.0014541694
train_loss: 0.0014742418
test_loss: 0.0014923189
train_loss: 0.0014727295
test_loss: 0.0013456193
train_loss: 0.0013897552
test_loss: 0.0013770855
train_loss: 0.0015680871
test_loss: 0.001502335
train_loss: 0.0011469533
test_loss: 0.0017569499
train_loss: 0.0012226531
test_loss: 0.0014989941
train_loss: 0.0014318555
test_loss: 0.0014181546
train_loss: 0.0014573085
test_loss: 0.0016060569
train_loss: 0.0014673132
test_loss: 0.0018657775
train_loss: 0.0014139331
test_loss: 0.0014678478
train_loss: 0.0012495104
test_loss: 0.0014206157
train_loss: 0.0011651815
test_loss: 0.0014204419
train_loss: 0.0015220256
test_loss: 0.0012619552
train_loss: 0.0014392883
test_loss: 0.0015424897
train_loss: 0.0012533653
test_loss: 0.0012201538
train_loss: 0.0013666372
test_loss: 0.0012863948
train_loss: 0.0012277347
test_loss: 0.0014528015
train_loss: 0.0013115519
test_loss: 0.0012883624
train_loss: 0.0013192901
test_loss: 0.0014420738
train_loss: 0.0014827217
test_loss: 0.0013894003
train_loss: 0.0015085182
test_loss: 0.0014508599
train_loss: 0.0015568603
test_loss: 0.001316892
train_loss: 0.0013711461
test_loss: 0.0015987757
train_loss: 0.0013971167
test_loss: 0.0014937837
train_loss: 0.0011493939
test_loss: 0.0014161463
train_loss: 0.0015007692
test_loss: 0.0012552614
train_loss: 0.0015757408
test_loss: 0.001384028
train_loss: 0.001756253
test_loss: 0.0012992014
train_loss: 0.0014976669
test_loss: 0.0016787599
train_loss: 0.0016451692
test_loss: 0.0016180717
train_loss: 0.0022304622
test_loss: 0.0016721167
train_loss: 0.0021623385
test_loss: 0.0016295015
train_loss: 0.0014846645
test_loss: 0.0017267178
train_loss: 0.0015020961
test_loss: 0.0015968708
train_loss: 0.001525979
test_loss: 0.0015514761
train_loss: 0.0012014043
test_loss: 0.001346937
train_loss: 0.0013977146
test_loss: 0.0014816172
train_loss: 0.0012774284
test_loss: 0.0017644223
train_loss: 0.0013617355
test_loss: 0.0014854241
train_loss: 0.0013910993
test_loss: 0.0012977989
train_loss: 0.0012690607
test_loss: 0.0014752701
train_loss: 0.0013772952
test_loss: 0.0012604451
train_loss: 0.0015458094
test_loss: 0.0013656704
train_loss: 0.0014517851
test_loss: 0.0013428333
train_loss: 0.0014491952
test_loss: 0.0013781474
train_loss: 0.0012563227
test_loss: 0.00137558
train_loss: 0.0012824245
test_loss: 0.0012010889
train_loss: 0.0014372849
test_loss: 0.0013314174
train_loss: 0.0012615272
test_loss: 0.001277521
train_loss: 0.001453507
test_loss: 0.0015462455
train_loss: 0.0015004656
test_loss: 0.0016734382
train_loss: 0.0017319361
test_loss: 0.001413478
train_loss: 0.0018380596
test_loss: 0.0016403982
train_loss: 0.0013784552
test_loss: 0.0013285534
train_loss: 0.0015295697
test_loss: 0.0013901539
train_loss: 0.0014058244
test_loss: 0.0014971882
train_loss: 0.0012650645
test_loss: 0.0017426513
train_loss: 0.0016954048
test_loss: 0.0013153456
train_loss: 0.0013538578
test_loss: 0.0015300295
train_loss: 0.0017637832
test_loss: 0.0014669953
train_loss: 0.0016098167
test_loss: 0.0015222881
train_loss: 0.0014003095
test_loss: 0.0014050672
train_loss: 0.0012511841
test_loss: 0.0015419878
train_loss: 0.0015171522
test_loss: 0.0016281442
train_loss: 0.0014764704
test_loss: 0.001505039
train_loss: 0.0014621965
test_loss: 0.0012170041
train_loss: 0.0010730805
test_loss: 0.0013532034
train_loss: 0.0012517722
test_loss: 0.0012976588
train_loss: 0.0012738864
test_loss: 0.0015169765
train_loss: 0.001329965
test_loss: 0.0012023454
train_loss: 0.0013285995
test_loss: 0.0013055556
train_loss: 0.0013687104
test_loss: 0.0012577431
train_loss: 0.0012109892
test_loss: 0.0014237566
train_loss: 0.0016847355
test_loss: 0.0014808865
train_loss: 0.0013139099
test_loss: 0.0016274786
train_loss: 0.001154478
test_loss: 0.0012395709
train_loss: 0.0012357952
test_loss: 0.0012361722
train_loss: 0.0013118021
test_loss: 0.0012236622
train_loss: 0.0013867398
test_loss: 0.0014132691
train_loss: 0.0012841974
test_loss: 0.0013871487
train_loss: 0.0014134297
test_loss: 0.0014579582
train_loss: 0.0014953553
test_loss: 0.0018142138
train_loss: 0.0013091661
test_loss: 0.0013145225
train_loss: 0.0013427126
test_loss: 0.0013047339
train_loss: 0.0013374432
test_loss: 0.0013847421
train_loss: 0.0012921572
test_loss: 0.0011817395
train_loss: 0.0014119765
test_loss: 0.0016215524
train_loss: 0.0013246806
test_loss: 0.0013425559
train_loss: 0.0012186511
test_loss: 0.0013530232
train_loss: 0.001512649
test_loss: 0.0013568705
train_loss: 0.0013565649
test_loss: 0.001445606
train_loss: 0.0013286946
test_loss: 0.0013977895
train_loss: 0.001584481
test_loss: 0.0013117506
train_loss: 0.0015093319
test_loss: 0.0014534614
train_loss: 0.0015612018
test_loss: 0.0014066669
train_loss: 0.0016727536
test_loss: 0.0012174698
train_loss: 0.0014564937
test_loss: 0.0015856243
train_loss: 0.0021305499
test_loss: 0.0020145876
train_loss: 0.0019768514
test_loss: 0.0018217883
train_loss: 0.0016189013
test_loss: 0.001681812
train_loss: 0.0012867439
test_loss: 0.0017326897
train_loss: 0.0012556541
test_loss: 0.0013657628
train_loss: 0.001448797
test_loss: 0.001441752
train_loss: 0.0012132532
test_loss: 0.0014426728
train_loss: 0.0014828251
test_loss: 0.0017000115
train_loss: 0.001482892
test_loss: 0.0016110986
train_loss: 0.0013481115
test_loss: 0.0014570842
train_loss: 0.0013056005
test_loss: 0.0014994816
train_loss: 0.0013452388
test_loss: 0.0013500594
train_loss: 0.0017822061
test_loss: 0.0015905455
train_loss: 0.0012590869
test_loss: 0.0015797127
train_loss: 0.0013453922
test_loss: 0.0019482741
train_loss: 0.001988612
test_loss: 0.0017375449
train_loss: 0.0018982288
test_loss: 0.0019154146
train_loss: 0.002357363
test_loss: 0.0016038662
train_loss: 0.00219587
test_loss: 0.0019547227
train_loss: 0.0020440693
test_loss: 0.0019858335
train_loss: 0.0019584396
test_loss: 0.0015747189
train_loss: 0.001428539
test_loss: 0.001422224
train_loss: 0.0012804244
test_loss: 0.0012478189
train_loss: 0.0014242518
test_loss: 0.0013998675
train_loss: 0.0012004481
test_loss: 0.0015955577
train_loss: 0.0013705897
test_loss: 0.0013422695
train_loss: 0.0016239991
test_loss: 0.0015445533
train_loss: 0.0013474121
test_loss: 0.0014226972
train_loss: 0.0013338772
test_loss: 0.0013541427
train_loss: 0.0014742154
test_loss: 0.0014959478
train_loss: 0.0012741764
test_loss: 0.001608619
train_loss: 0.0012691692
test_loss: 0.0013801439
train_loss: 0.001125181
test_loss: 0.0012742468
train_loss: 0.0013074928
test_loss: 0.0012605153
train_loss: 0.0012017271
test_loss: 0.0012821711
train_loss: 0.0012245304
test_loss: 0.0014355638
train_loss: 0.0012549616
test_loss: 0.0012856416
train_loss: 0.0015234688
test_loss: 0.0014234564
train_loss: 0.001285132
test_loss: 0.0014737381
train_loss: 0.001261331
test_loss: 0.0018412524
train_loss: 0.0013269065
test_loss: 0.0014015661
train_loss: 0.0011905495
test_loss: 0.0012633008
train_loss: 0.0012628726
test_loss: 0.0014030666
train_loss: 0.0012464913
test_loss: 0.0012977177
train_loss: 0.0010620767
test_loss: 0.0011038288
train_loss: 0.0010402693
test_loss: 0.0012313358
train_loss: 0.0013440553
test_loss: 0.0014153019
train_loss: 0.0012874526
test_loss: 0.0012412035
train_loss: 0.001325017
test_loss: 0.0012244759
train_loss: 0.0012913637
test_loss: 0.0012526787
train_loss: 0.0014164937
test_loss: 0.0012149429
train_loss: 0.0015431441
test_loss: 0.0015310092
train_loss: 0.0012939413
test_loss: 0.0017250977
train_loss: 0.0012416029
test_loss: 0.0012642011
train_loss: 0.0012073559
test_loss: 0.0012623848
train_loss: 0.00112131
test_loss: 0.0014060867
train_loss: 0.0014235592
test_loss: 0.0013843013
train_loss: 0.0011387491
test_loss: 0.0012876347
train_loss: 0.0010440295
test_loss: 0.0012850648
train_loss: 0.0014570263
test_loss: 0.001217031
train_loss: 0.0012540767
test_loss: 0.001432312
train_loss: 0.0012840321
test_loss: 0.0013663939
train_loss: 0.0012762051
test_loss: 0.0013076215
train_loss: 0.0016591607
test_loss: 0.0012631495
train_loss: 0.0012797884
test_loss: 0.0015673942
train_loss: 0.001457999
test_loss: 0.0014874872
train_loss: 0.001571086
test_loss: 0.0013893744
train_loss: 0.001403313
test_loss: 0.0012689754
train_loss: 0.0015012289
test_loss: 0.001258434
train_loss: 0.0013797241
test_loss: 0.0012794598
train_loss: 0.0013699222
test_loss: 0.0013652703
train_loss: 0.0012152235
test_loss: 0.0013325587
train_loss: 0.0011235644
test_loss: 0.0011667581
train_loss: 0.0010854872
test_loss: 0.0013360328
train_loss: 0.0013618615
test_loss: 0.0012411871
train_loss: 0.001462711
test_loss: 0.0011597422
train_loss: 0.0012521644
test_loss: 0.0012524272
train_loss: 0.0011034579
test_loss: 0.0012148661
train_loss: 0.0011988869
test_loss: 0.0012778278
train_loss: 0.0011655991
test_loss: 0.0011891222
train_loss: 0.0012826354
test_loss: 0.0012850197
train_loss: 0.0012930362
test_loss: 0.0012347341
train_loss: 0.0012985368
test_loss: 0.001244854
train_loss: 0.0013913049
test_loss: 0.0013036656
train_loss: 0.0013662096
test_loss: 0.0013928333
train_loss: 0.0015541818
test_loss: 0.0014014615
train_loss: 0.0012649297
test_loss: 0.0012978353
train_loss: 0.0011386725
test_loss: 0.0012192151
train_loss: 0.0013423649
test_loss: 0.0013889953
train_loss: 0.0016367666
test_loss: 0.0017916468
train_loss: 0.0012388888
test_loss: 0.001277709
train_loss: 0.0014737538
test_loss: 0.001408382
train_loss: 0.0012392035
test_loss: 0.0015535983
train_loss: 0.0015234649
test_loss: 0.0012836234
train_loss: 0.0013001802
test_loss: 0.0013304361
train_loss: 0.0011731416
test_loss: 0.001479448
train_loss: 0.0017636918
test_loss: 0.0014259836
train_loss: 0.0012543884
test_loss: 0.0015193846
train_loss: 0.0011701057
test_loss: 0.0013002648
train_loss: 0.0017063061
test_loss: 0.0013738405
train_loss: 0.001577423
test_loss: 0.0016766754
train_loss: 0.0012068477
test_loss: 0.0014966413
train_loss: 0.0014614365
test_loss: 0.0013779914
train_loss: 0.001468663
test_loss: 0.0014193187
train_loss: 0.0014607174
test_loss: 0.0012017356
train_loss: 0.0012816942
test_loss: 0.0012657122
train_loss: 0.0013734906
test_loss: 0.0014401199
train_loss: 0.0013439343
test_loss: 0.0011072233
train_loss: 0.0016616622
test_loss: 0.0014599125
train_loss: 0.0013830997
test_loss: 0.0013476298
train_loss: 0.0011844505
test_loss: 0.0014077761
train_loss: 0.0013459155
test_loss: 0.001531299
train_loss: 0.001484664
test_loss: 0.0013190483
train_loss: 0.0014352861
test_loss: 0.0012378658
train_loss: 0.0013812563
test_loss: 0.001599159
train_loss: 0.0020515933
test_loss: 0.0017238111
train_loss: 0.001146582
test_loss: 0.0011292585
train_loss: 0.0014615178/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0013227048
train_loss: 0.0017747539
test_loss: 0.0014840406
train_loss: 0.0015417045
test_loss: 0.0013460553
train_loss: 0.0012631326
test_loss: 0.001418363
train_loss: 0.001112022
test_loss: 0.0011870088
train_loss: 0.0013918212
test_loss: 0.0013728247
train_loss: 0.0012189299
test_loss: 0.0014893395
train_loss: 0.0011113822
test_loss: 0.0013835266
train_loss: 0.0014302888
test_loss: 0.0012968417
train_loss: 0.0014515709
test_loss: 0.0013041594
train_loss: 0.0015518288
test_loss: 0.0013529286
train_loss: 0.0011504926
test_loss: 0.0013491549
train_loss: 0.0012385481
test_loss: 0.0010588833
train_loss: 0.001269965
test_loss: 0.0012310479
train_loss: 0.0015116988
test_loss: 0.0013315626
train_loss: 0.0018093467
test_loss: 0.0015570025
train_loss: 0.0014010435
test_loss: 0.0014334781
train_loss: 0.0013046393
test_loss: 0.001293083
train_loss: 0.0013092407
test_loss: 0.0013318607
train_loss: 0.0011799794
test_loss: 0.0012874554
train_loss: 0.0011641042
test_loss: 0.001514496
train_loss: 0.0012573786
test_loss: 0.0013421522
train_loss: 0.0016512013
test_loss: 0.0013068172
train_loss: 0.0012670885
test_loss: 0.0013973975
train_loss: 0.001268561
test_loss: 0.0011581195
train_loss: 0.0010457507
test_loss: 0.0013752582
train_loss: 0.0012203115
test_loss: 0.0014242689
train_loss: 0.0012582234
test_loss: 0.0012782246
train_loss: 0.0011933073
test_loss: 0.0012962522
train_loss: 0.001299668
test_loss: 0.0013030984
train_loss: 0.0012468491
test_loss: 0.0012655776
train_loss: 0.0013432882
test_loss: 0.0014957517
train_loss: 0.0012056414
test_loss: 0.0013713931
train_loss: 0.0013019685
test_loss: 0.0013457004
train_loss: 0.001435095
test_loss: 0.0014203389
train_loss: 0.0014706396
test_loss: 0.0016867472
train_loss: 0.001381769
test_loss: 0.0011767846
train_loss: 0.0012921845
test_loss: 0.0013492659
train_loss: 0.0013139863
test_loss: 0.0012353202
train_loss: 0.0013363753
test_loss: 0.0013285252
train_loss: 0.0014843519
test_loss: 0.0011814806
train_loss: 0.0012622967
test_loss: 0.0013870203
train_loss: 0.0012496302
test_loss: 0.0011537558
train_loss: 0.0013232032
test_loss: 0.0012675833
train_loss: 0.0012230758
test_loss: 0.0013356046
train_loss: 0.0011650058
test_loss: 0.0014794129
train_loss: 0.0011305439
test_loss: 0.0013346564
train_loss: 0.0013885809
test_loss: 0.0013073876
train_loss: 0.0010929309
test_loss: 0.0011219453
train_loss: 0.0014203341
test_loss: 0.0013948828
train_loss: 0.0011568917
test_loss: 0.0013773494
train_loss: 0.0013143222
test_loss: 0.001288813
train_loss: 0.0012775974
test_loss: 0.0012893582
train_loss: 0.0012919821
test_loss: 0.0014743423
train_loss: 0.0012567517
test_loss: 0.0012449498
train_loss: 0.001454068
test_loss: 0.0013175036
train_loss: 0.0020113343
test_loss: 0.0018355943
train_loss: 0.0015105028
test_loss: 0.0019494627
train_loss: 0.0016538711
test_loss: 0.0016145277
train_loss: 0.0013622018
test_loss: 0.0020597854
train_loss: 0.0015465876
test_loss: 0.0017846246
train_loss: 0.0014882956
test_loss: 0.0017755902
train_loss: 0.0015008801
test_loss: 0.0017624159
train_loss: 0.001779659
test_loss: 0.0015036953
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15aba08840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15aba0d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15abb0f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15aba6a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15ab9efea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15ab9ef268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15ab9ef0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15ab955048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15ab9eed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15674a1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15674a12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1567469f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15674788c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15674231e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1567425620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15674789d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1567400950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15674abea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15674007b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1540402e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15404381e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15403c1048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1540438a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15403b8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15403b8bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15403b8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15403b8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f154030b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f154030be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15402c7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15402f71e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15402f77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f154024c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1540294510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1540279ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f154022ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.37786469e-06
Iter: 2 loss: 2.4924334e-06
Iter: 3 loss: 1.71090107e-06
Iter: 4 loss: 1.4663002e-06
Iter: 5 loss: 1.4327502e-06
Iter: 6 loss: 1.26008547e-06
Iter: 7 loss: 1.11952318e-06
Iter: 8 loss: 1.11900886e-06
Iter: 9 loss: 1.01778357e-06
Iter: 10 loss: 9.17310501e-07
Iter: 11 loss: 8.96314702e-07
Iter: 12 loss: 8.24255608e-07
Iter: 13 loss: 1.82069243e-06
Iter: 14 loss: 8.24059725e-07
Iter: 15 loss: 7.89547471e-07
Iter: 16 loss: 7.89568958e-07
Iter: 17 loss: 7.78240292e-07
Iter: 18 loss: 7.66620474e-07
Iter: 19 loss: 7.64486174e-07
Iter: 20 loss: 7.45875923e-07
Iter: 21 loss: 9.36106176e-07
Iter: 22 loss: 7.45364844e-07
Iter: 23 loss: 7.3363276e-07
Iter: 24 loss: 7.27167617e-07
Iter: 25 loss: 7.21999868e-07
Iter: 26 loss: 7.09506821e-07
Iter: 27 loss: 7.17598084e-07
Iter: 28 loss: 7.01596946e-07
Iter: 29 loss: 6.86587896e-07
Iter: 30 loss: 7.01219562e-07
Iter: 31 loss: 6.78081506e-07
Iter: 32 loss: 6.66001029e-07
Iter: 33 loss: 8.49940761e-07
Iter: 34 loss: 6.65997959e-07
Iter: 35 loss: 6.59968e-07
Iter: 36 loss: 7.36962306e-07
Iter: 37 loss: 6.59918555e-07
Iter: 38 loss: 6.53818e-07
Iter: 39 loss: 6.61722197e-07
Iter: 40 loss: 6.50740162e-07
Iter: 41 loss: 6.47246452e-07
Iter: 42 loss: 6.61033653e-07
Iter: 43 loss: 6.46446097e-07
Iter: 44 loss: 6.41903512e-07
Iter: 45 loss: 6.4980361e-07
Iter: 46 loss: 6.39908421e-07
Iter: 47 loss: 6.37071878e-07
Iter: 48 loss: 6.36535106e-07
Iter: 49 loss: 6.34642333e-07
Iter: 50 loss: 6.31256398e-07
Iter: 51 loss: 6.31261969e-07
Iter: 52 loss: 6.28546331e-07
Iter: 53 loss: 6.22262121e-07
Iter: 54 loss: 7.00356622e-07
Iter: 55 loss: 6.21824427e-07
Iter: 56 loss: 6.14718147e-07
Iter: 57 loss: 6.95128278e-07
Iter: 58 loss: 6.14630437e-07
Iter: 59 loss: 6.08494304e-07
Iter: 60 loss: 6.14940859e-07
Iter: 61 loss: 6.05126502e-07
Iter: 62 loss: 6.01281272e-07
Iter: 63 loss: 5.98384759e-07
Iter: 64 loss: 5.97143128e-07
Iter: 65 loss: 5.9088643e-07
Iter: 66 loss: 6.26069323e-07
Iter: 67 loss: 5.90021841e-07
Iter: 68 loss: 5.86602e-07
Iter: 69 loss: 5.9314857e-07
Iter: 70 loss: 5.85184864e-07
Iter: 71 loss: 5.8208883e-07
Iter: 72 loss: 5.8969249e-07
Iter: 73 loss: 5.80978337e-07
Iter: 74 loss: 5.79313905e-07
Iter: 75 loss: 5.79264224e-07
Iter: 76 loss: 5.77433696e-07
Iter: 77 loss: 5.81932227e-07
Iter: 78 loss: 5.76782e-07
Iter: 79 loss: 5.75309059e-07
Iter: 80 loss: 5.75841341e-07
Iter: 81 loss: 5.74244666e-07
Iter: 82 loss: 5.72423119e-07
Iter: 83 loss: 5.93594791e-07
Iter: 84 loss: 5.72384465e-07
Iter: 85 loss: 5.71189958e-07
Iter: 86 loss: 5.68717326e-07
Iter: 87 loss: 6.15081603e-07
Iter: 88 loss: 5.68678729e-07
Iter: 89 loss: 5.67652251e-07
Iter: 90 loss: 5.67367238e-07
Iter: 91 loss: 5.66033805e-07
Iter: 92 loss: 5.63632852e-07
Iter: 93 loss: 5.63648484e-07
Iter: 94 loss: 5.62248033e-07
Iter: 95 loss: 5.62240075e-07
Iter: 96 loss: 5.60805177e-07
Iter: 97 loss: 5.60049898e-07
Iter: 98 loss: 5.59416435e-07
Iter: 99 loss: 5.57731937e-07
Iter: 100 loss: 5.59453099e-07
Iter: 101 loss: 5.56810733e-07
Iter: 102 loss: 5.54880899e-07
Iter: 103 loss: 5.55454392e-07
Iter: 104 loss: 5.53486927e-07
Iter: 105 loss: 5.50939433e-07
Iter: 106 loss: 5.65846221e-07
Iter: 107 loss: 5.50585753e-07
Iter: 108 loss: 5.48718901e-07
Iter: 109 loss: 5.50331379e-07
Iter: 110 loss: 5.47600393e-07
Iter: 111 loss: 5.463192e-07
Iter: 112 loss: 5.46267302e-07
Iter: 113 loss: 5.4511213e-07
Iter: 114 loss: 5.50325581e-07
Iter: 115 loss: 5.44859176e-07
Iter: 116 loss: 5.44346e-07
Iter: 117 loss: 5.45123726e-07
Iter: 118 loss: 5.44082e-07
Iter: 119 loss: 5.432031e-07
Iter: 120 loss: 5.43736064e-07
Iter: 121 loss: 5.42637565e-07
Iter: 122 loss: 5.41902182e-07
Iter: 123 loss: 5.43527676e-07
Iter: 124 loss: 5.41635131e-07
Iter: 125 loss: 5.41241e-07
Iter: 126 loss: 5.41232e-07
Iter: 127 loss: 5.40859219e-07
Iter: 128 loss: 5.39792211e-07
Iter: 129 loss: 5.44442628e-07
Iter: 130 loss: 5.39454618e-07
Iter: 131 loss: 5.39501457e-07
Iter: 132 loss: 5.38920801e-07
Iter: 133 loss: 5.38549841e-07
Iter: 134 loss: 5.37829635e-07
Iter: 135 loss: 5.51940445e-07
Iter: 136 loss: 5.37819801e-07
Iter: 137 loss: 5.37030587e-07
Iter: 138 loss: 5.37701681e-07
Iter: 139 loss: 5.36582093e-07
Iter: 140 loss: 5.35607683e-07
Iter: 141 loss: 5.39511063e-07
Iter: 142 loss: 5.35466597e-07
Iter: 143 loss: 5.34591493e-07
Iter: 144 loss: 5.34226331e-07
Iter: 145 loss: 5.33772e-07
Iter: 146 loss: 5.32190484e-07
Iter: 147 loss: 5.39744633e-07
Iter: 148 loss: 5.3186352e-07
Iter: 149 loss: 5.31132173e-07
Iter: 150 loss: 5.31070896e-07
Iter: 151 loss: 5.3033267e-07
Iter: 152 loss: 5.29312615e-07
Iter: 153 loss: 5.29267311e-07
Iter: 154 loss: 5.2843933e-07
Iter: 155 loss: 5.28403291e-07
Iter: 156 loss: 5.27688428e-07
Iter: 157 loss: 5.27276825e-07
Iter: 158 loss: 5.26989595e-07
Iter: 159 loss: 5.26710039e-07
Iter: 160 loss: 5.2667275e-07
Iter: 161 loss: 5.26383189e-07
Iter: 162 loss: 5.26012627e-07
Iter: 163 loss: 5.25956182e-07
Iter: 164 loss: 5.25576411e-07
Iter: 165 loss: 5.27242094e-07
Iter: 166 loss: 5.25444477e-07
Iter: 167 loss: 5.25136272e-07
Iter: 168 loss: 5.2892392e-07
Iter: 169 loss: 5.2514207e-07
Iter: 170 loss: 5.24972961e-07
Iter: 171 loss: 5.24595066e-07
Iter: 172 loss: 5.29048748e-07
Iter: 173 loss: 5.24553968e-07
Iter: 174 loss: 5.23998096e-07
Iter: 175 loss: 5.23878498e-07
Iter: 176 loss: 5.23495032e-07
Iter: 177 loss: 5.22856226e-07
Iter: 178 loss: 5.28114128e-07
Iter: 179 loss: 5.22797052e-07
Iter: 180 loss: 5.22240953e-07
Iter: 181 loss: 5.23778e-07
Iter: 182 loss: 5.22074231e-07
Iter: 183 loss: 5.21940933e-07
Iter: 184 loss: 5.2180809e-07
Iter: 185 loss: 5.21585946e-07
Iter: 186 loss: 5.21057473e-07
Iter: 187 loss: 5.27843213e-07
Iter: 188 loss: 5.21016659e-07
Iter: 189 loss: 5.20759e-07
Iter: 190 loss: 5.20739604e-07
Iter: 191 loss: 5.20445894e-07
Iter: 192 loss: 5.20215394e-07
Iter: 193 loss: 5.20144738e-07
Iter: 194 loss: 5.19837499e-07
Iter: 195 loss: 5.23216443e-07
Iter: 196 loss: 5.19838864e-07
Iter: 197 loss: 5.19510422e-07
Iter: 198 loss: 5.19529408e-07
Iter: 199 loss: 5.19273613e-07
Iter: 200 loss: 5.18889919e-07
Iter: 201 loss: 5.1880221e-07
Iter: 202 loss: 5.18522e-07
Iter: 203 loss: 5.18139132e-07
Iter: 204 loss: 5.18126797e-07
Iter: 205 loss: 5.1786418e-07
Iter: 206 loss: 5.17344574e-07
Iter: 207 loss: 5.27587474e-07
Iter: 208 loss: 5.17335536e-07
Iter: 209 loss: 5.16785917e-07
Iter: 210 loss: 5.18576144e-07
Iter: 211 loss: 5.16649266e-07
Iter: 212 loss: 5.16282739e-07
Iter: 213 loss: 5.18125887e-07
Iter: 214 loss: 5.16211344e-07
Iter: 215 loss: 5.15918657e-07
Iter: 216 loss: 5.16700879e-07
Iter: 217 loss: 5.15884039e-07
Iter: 218 loss: 5.15759211e-07
Iter: 219 loss: 5.15719933e-07
Iter: 220 loss: 5.15595275e-07
Iter: 221 loss: 5.15242107e-07
Iter: 222 loss: 5.19698119e-07
Iter: 223 loss: 5.15246825e-07
Iter: 224 loss: 5.15021327e-07
Iter: 225 loss: 5.15025079e-07
Iter: 226 loss: 5.14764452e-07
Iter: 227 loss: 5.15046e-07
Iter: 228 loss: 5.14670887e-07
Iter: 229 loss: 5.14448971e-07
Iter: 230 loss: 5.14427256e-07
Iter: 231 loss: 5.14251383e-07
Iter: 232 loss: 5.13693351e-07
Iter: 233 loss: 5.13673342e-07
Iter: 234 loss: 5.13254122e-07
Iter: 235 loss: 5.12910617e-07
Iter: 236 loss: 5.14053681e-07
Iter: 237 loss: 5.12796e-07
Iter: 238 loss: 5.12473946e-07
Iter: 239 loss: 5.15764384e-07
Iter: 240 loss: 5.12464567e-07
Iter: 241 loss: 5.12208601e-07
Iter: 242 loss: 5.11699398e-07
Iter: 243 loss: 5.19878427e-07
Iter: 244 loss: 5.11686665e-07
Iter: 245 loss: 5.11117605e-07
Iter: 246 loss: 5.13689315e-07
Iter: 247 loss: 5.1096697e-07
Iter: 248 loss: 5.10611358e-07
Iter: 249 loss: 5.11796202e-07
Iter: 250 loss: 5.10546329e-07
Iter: 251 loss: 5.10175141e-07
Iter: 252 loss: 5.11675069e-07
Iter: 253 loss: 5.10071288e-07
Iter: 254 loss: 5.09912638e-07
Iter: 255 loss: 5.09909228e-07
Iter: 256 loss: 5.09773713e-07
Iter: 257 loss: 5.09565496e-07
Iter: 258 loss: 5.09579877e-07
Iter: 259 loss: 5.09407755e-07
Iter: 260 loss: 5.09390475e-07
Iter: 261 loss: 5.09239e-07
Iter: 262 loss: 5.09009794e-07
Iter: 263 loss: 5.09006e-07
Iter: 264 loss: 5.08900087e-07
Iter: 265 loss: 5.0887769e-07
Iter: 266 loss: 5.08732285e-07
Iter: 267 loss: 5.08498886e-07
Iter: 268 loss: 5.12677445e-07
Iter: 269 loss: 5.08479502e-07
Iter: 270 loss: 5.08234734e-07
Iter: 271 loss: 5.09125471e-07
Iter: 272 loss: 5.08105586e-07
Iter: 273 loss: 5.07874347e-07
Iter: 274 loss: 5.11160124e-07
Iter: 275 loss: 5.07873e-07
Iter: 276 loss: 5.0768341e-07
Iter: 277 loss: 5.0724077e-07
Iter: 278 loss: 5.12950294e-07
Iter: 279 loss: 5.07198706e-07
Iter: 280 loss: 5.06819106e-07
Iter: 281 loss: 5.08415553e-07
Iter: 282 loss: 5.06734523e-07
Iter: 283 loss: 5.06412619e-07
Iter: 284 loss: 5.07435743e-07
Iter: 285 loss: 5.06318088e-07
Iter: 286 loss: 5.05982257e-07
Iter: 287 loss: 5.07800394e-07
Iter: 288 loss: 5.05970149e-07
Iter: 289 loss: 5.05721687e-07
Iter: 290 loss: 5.05718219e-07
Iter: 291 loss: 5.0556423e-07
Iter: 292 loss: 5.05154162e-07
Iter: 293 loss: 5.08741891e-07
Iter: 294 loss: 5.05097319e-07
Iter: 295 loss: 5.04913942e-07
Iter: 296 loss: 5.04809918e-07
Iter: 297 loss: 5.04644731e-07
Iter: 298 loss: 5.04343461e-07
Iter: 299 loss: 5.11501582e-07
Iter: 300 loss: 5.04346644e-07
Iter: 301 loss: 5.0406959e-07
Iter: 302 loss: 5.04073114e-07
Iter: 303 loss: 5.03866829e-07
Iter: 304 loss: 5.03716365e-07
Iter: 305 loss: 5.03624904e-07
Iter: 306 loss: 5.0343067e-07
Iter: 307 loss: 5.03727961e-07
Iter: 308 loss: 5.03305614e-07
Iter: 309 loss: 5.03165893e-07
Iter: 310 loss: 5.0318431e-07
Iter: 311 loss: 5.03027366e-07
Iter: 312 loss: 5.02797775e-07
Iter: 313 loss: 5.02800276e-07
Iter: 314 loss: 5.025787e-07
Iter: 315 loss: 5.0301162e-07
Iter: 316 loss: 5.02484e-07
Iter: 317 loss: 5.02255091e-07
Iter: 318 loss: 5.02482749e-07
Iter: 319 loss: 5.02109629e-07
Iter: 320 loss: 5.01912e-07
Iter: 321 loss: 5.01921818e-07
Iter: 322 loss: 5.01717864e-07
Iter: 323 loss: 5.0180023e-07
Iter: 324 loss: 5.01574107e-07
Iter: 325 loss: 5.01346904e-07
Iter: 326 loss: 5.01502768e-07
Iter: 327 loss: 5.01238446e-07
Iter: 328 loss: 5.0094e-07
Iter: 329 loss: 5.03143326e-07
Iter: 330 loss: 5.00894373e-07
Iter: 331 loss: 5.00705085e-07
Iter: 332 loss: 5.00812689e-07
Iter: 333 loss: 5.0061044e-07
Iter: 334 loss: 5.00418082e-07
Iter: 335 loss: 5.02418743e-07
Iter: 336 loss: 5.00433657e-07
Iter: 337 loss: 5.0025136e-07
Iter: 338 loss: 4.99963448e-07
Iter: 339 loss: 5.06956553e-07
Iter: 340 loss: 4.9993514e-07
Iter: 341 loss: 4.9967997e-07
Iter: 342 loss: 5.00953092e-07
Iter: 343 loss: 4.99664111e-07
Iter: 344 loss: 4.99417581e-07
Iter: 345 loss: 5.01816658e-07
Iter: 346 loss: 4.99386545e-07
Iter: 347 loss: 4.99224825e-07
Iter: 348 loss: 4.9888547e-07
Iter: 349 loss: 5.06287279e-07
Iter: 350 loss: 4.98875067e-07
Iter: 351 loss: 4.98542306e-07
Iter: 352 loss: 5.00853332e-07
Iter: 353 loss: 4.98556687e-07
Iter: 354 loss: 4.98352165e-07
Iter: 355 loss: 4.98305667e-07
Iter: 356 loss: 4.98160716e-07
Iter: 357 loss: 4.98291229e-07
Iter: 358 loss: 4.98077497e-07
Iter: 359 loss: 4.97989561e-07
Iter: 360 loss: 4.97878204e-07
Iter: 361 loss: 4.97905887e-07
Iter: 362 loss: 4.97793224e-07
Iter: 363 loss: 4.98649229e-07
Iter: 364 loss: 4.97790779e-07
Iter: 365 loss: 4.97674932e-07
Iter: 366 loss: 4.97796179e-07
Iter: 367 loss: 4.97646795e-07
Iter: 368 loss: 4.97548911e-07
Iter: 369 loss: 4.97538963e-07
Iter: 370 loss: 4.9747257e-07
Iter: 371 loss: 4.97330348e-07
Iter: 372 loss: 4.9848552e-07
Iter: 373 loss: 4.97282088e-07
Iter: 374 loss: 4.97187273e-07
Iter: 375 loss: 4.96940231e-07
Iter: 376 loss: 4.99060661e-07
Iter: 377 loss: 4.96878e-07
Iter: 378 loss: 4.96708935e-07
Iter: 379 loss: 4.9667517e-07
Iter: 380 loss: 4.96509074e-07
Iter: 381 loss: 4.96515952e-07
Iter: 382 loss: 4.96386349e-07
Iter: 383 loss: 4.96193138e-07
Iter: 384 loss: 4.95981567e-07
Iter: 385 loss: 4.95945869e-07
Iter: 386 loss: 4.95651193e-07
Iter: 387 loss: 4.97538338e-07
Iter: 388 loss: 4.9559435e-07
Iter: 389 loss: 4.95369932e-07
Iter: 390 loss: 4.95889481e-07
Iter: 391 loss: 4.95307404e-07
Iter: 392 loss: 4.9518286e-07
Iter: 393 loss: 4.95161487e-07
Iter: 394 loss: 4.95050074e-07
Iter: 395 loss: 4.94792289e-07
Iter: 396 loss: 4.94786434e-07
Iter: 397 loss: 4.94686219e-07
Iter: 398 loss: 4.94707251e-07
Iter: 399 loss: 4.94551955e-07
Iter: 400 loss: 4.94265919e-07
Iter: 401 loss: 4.99117505e-07
Iter: 402 loss: 4.94247e-07
Iter: 403 loss: 4.94139385e-07
Iter: 404 loss: 4.94151e-07
Iter: 405 loss: 4.94012227e-07
Iter: 406 loss: 4.94649498e-07
Iter: 407 loss: 4.94011942e-07
Iter: 408 loss: 4.93948846e-07
Iter: 409 loss: 4.9375609e-07
Iter: 410 loss: 4.96695861e-07
Iter: 411 loss: 4.93765697e-07
Iter: 412 loss: 4.93669631e-07
Iter: 413 loss: 4.94545702e-07
Iter: 414 loss: 4.93633252e-07
Iter: 415 loss: 4.93545258e-07
Iter: 416 loss: 4.94975268e-07
Iter: 417 loss: 4.9352991e-07
Iter: 418 loss: 4.93472612e-07
Iter: 419 loss: 4.93271386e-07
Iter: 420 loss: 4.94988114e-07
Iter: 421 loss: 4.93237508e-07
Iter: 422 loss: 4.93013545e-07
Iter: 423 loss: 4.93714253e-07
Iter: 424 loss: 4.92938227e-07
Iter: 425 loss: 4.92671234e-07
Iter: 426 loss: 4.9311268e-07
Iter: 427 loss: 4.92490244e-07
Iter: 428 loss: 4.92309198e-07
Iter: 429 loss: 4.94898245e-07
Iter: 430 loss: 4.92316929e-07
Iter: 431 loss: 4.92135882e-07
Iter: 432 loss: 4.93508139e-07
Iter: 433 loss: 4.92115248e-07
Iter: 434 loss: 4.92002641e-07
Iter: 435 loss: 4.91854394e-07
Iter: 436 loss: 4.91845071e-07
Iter: 437 loss: 4.91703304e-07
Iter: 438 loss: 4.9172013e-07
Iter: 439 loss: 4.91611047e-07
Iter: 440 loss: 4.91572564e-07
Iter: 441 loss: 4.91528226e-07
Iter: 442 loss: 4.91424089e-07
Iter: 443 loss: 4.92536799e-07
Iter: 444 loss: 4.91426817e-07
Iter: 445 loss: 4.91321e-07
Iter: 446 loss: 4.91159426e-07
Iter: 447 loss: 4.94556161e-07
Iter: 448 loss: 4.91131686e-07
Iter: 449 loss: 4.90965476e-07
Iter: 450 loss: 4.91434e-07
Iter: 451 loss: 4.90925e-07
Iter: 452 loss: 4.90762e-07
Iter: 453 loss: 4.91076094e-07
Iter: 454 loss: 4.90701893e-07
Iter: 455 loss: 4.90609182e-07
Iter: 456 loss: 4.90578827e-07
Iter: 457 loss: 4.90510388e-07
Iter: 458 loss: 4.90402272e-07
Iter: 459 loss: 4.90433138e-07
Iter: 460 loss: 4.90247544e-07
Iter: 461 loss: 4.90210482e-07
Iter: 462 loss: 4.90138575e-07
Iter: 463 loss: 4.89960826e-07
Iter: 464 loss: 4.91182675e-07
Iter: 465 loss: 4.89936497e-07
Iter: 466 loss: 4.89844354e-07
Iter: 467 loss: 4.898518e-07
Iter: 468 loss: 4.8974e-07
Iter: 469 loss: 4.89576905e-07
Iter: 470 loss: 4.93267521e-07
Iter: 471 loss: 4.8957213e-07
Iter: 472 loss: 4.89405465e-07
Iter: 473 loss: 4.89672345e-07
Iter: 474 loss: 4.8934362e-07
Iter: 475 loss: 4.89187755e-07
Iter: 476 loss: 4.89194917e-07
Iter: 477 loss: 4.89107833e-07
Iter: 478 loss: 4.89068e-07
Iter: 479 loss: 4.89015463e-07
Iter: 480 loss: 4.88905641e-07
Iter: 481 loss: 4.89069521e-07
Iter: 482 loss: 4.88856301e-07
Iter: 483 loss: 4.88754154e-07
Iter: 484 loss: 4.89949684e-07
Iter: 485 loss: 4.88757e-07
Iter: 486 loss: 4.88671617e-07
Iter: 487 loss: 4.88464536e-07
Iter: 488 loss: 4.89880222e-07
Iter: 489 loss: 4.8845078e-07
Iter: 490 loss: 4.88305602e-07
Iter: 491 loss: 4.88298951e-07
Iter: 492 loss: 4.88192939e-07
Iter: 493 loss: 4.89239255e-07
Iter: 494 loss: 4.88173782e-07
Iter: 495 loss: 4.88110913e-07
Iter: 496 loss: 4.87902412e-07
Iter: 497 loss: 4.89307638e-07
Iter: 498 loss: 4.87866942e-07
Iter: 499 loss: 4.87678108e-07
Iter: 500 loss: 4.89237e-07
Iter: 501 loss: 4.87641557e-07
Iter: 502 loss: 4.87664e-07
Iter: 503 loss: 4.875792e-07
Iter: 504 loss: 4.87529576e-07
Iter: 505 loss: 4.87441866e-07
Iter: 506 loss: 4.87762804e-07
Iter: 507 loss: 4.87381612e-07
Iter: 508 loss: 4.87200737e-07
Iter: 509 loss: 4.88401156e-07
Iter: 510 loss: 4.87185048e-07
Iter: 511 loss: 4.87085345e-07
Iter: 512 loss: 4.87107684e-07
Iter: 513 loss: 4.87014063e-07
Iter: 514 loss: 4.86825684e-07
Iter: 515 loss: 4.88031446e-07
Iter: 516 loss: 4.86739e-07
Iter: 517 loss: 4.86505769e-07
Iter: 518 loss: 4.88678324e-07
Iter: 519 loss: 4.86521856e-07
Iter: 520 loss: 4.86376507e-07
Iter: 521 loss: 4.88272633e-07
Iter: 522 loss: 4.86364456e-07
Iter: 523 loss: 4.86244062e-07
Iter: 524 loss: 4.86021349e-07
Iter: 525 loss: 4.90217e-07
Iter: 526 loss: 4.8604926e-07
Iter: 527 loss: 4.85838882e-07
Iter: 528 loss: 4.87629563e-07
Iter: 529 loss: 4.85822284e-07
Iter: 530 loss: 4.85738326e-07
Iter: 531 loss: 4.85722182e-07
Iter: 532 loss: 4.85679607e-07
Iter: 533 loss: 4.85561372e-07
Iter: 534 loss: 4.85578369e-07
Iter: 535 loss: 4.85473322e-07
Iter: 536 loss: 4.85713599e-07
Iter: 537 loss: 4.85415967e-07
Iter: 538 loss: 4.85369469e-07
Iter: 539 loss: 4.85359124e-07
Iter: 540 loss: 4.85282897e-07
Iter: 541 loss: 4.85240207e-07
Iter: 542 loss: 4.85197461e-07
Iter: 543 loss: 4.85145733e-07
Iter: 544 loss: 4.84994928e-07
Iter: 545 loss: 4.84986799e-07
Iter: 546 loss: 4.84810471e-07
Iter: 547 loss: 4.86040278e-07
Iter: 548 loss: 4.8478671e-07
Iter: 549 loss: 4.84690645e-07
Iter: 550 loss: 4.84686552e-07
Iter: 551 loss: 4.84619193e-07
Iter: 552 loss: 4.84494649e-07
Iter: 553 loss: 4.84482541e-07
Iter: 554 loss: 4.84368798e-07
Iter: 555 loss: 4.84671205e-07
Iter: 556 loss: 4.84320935e-07
Iter: 557 loss: 4.84250336e-07
Iter: 558 loss: 4.84234761e-07
Iter: 559 loss: 4.84176e-07
Iter: 560 loss: 4.8407e-07
Iter: 561 loss: 4.85551425e-07
Iter: 562 loss: 4.84078e-07
Iter: 563 loss: 4.8397294e-07
Iter: 564 loss: 4.83962594e-07
Iter: 565 loss: 4.8387119e-07
Iter: 566 loss: 4.83698784e-07
Iter: 567 loss: 4.83723738e-07
Iter: 568 loss: 4.83567305e-07
Iter: 569 loss: 4.83972542e-07
Iter: 570 loss: 4.8350671e-07
Iter: 571 loss: 4.83360907e-07
Iter: 572 loss: 4.83572421e-07
Iter: 573 loss: 4.83300482e-07
Iter: 574 loss: 4.83162751e-07
Iter: 575 loss: 4.83190604e-07
Iter: 576 loss: 4.83131657e-07
Iter: 577 loss: 4.8309056e-07
Iter: 578 loss: 4.8306805e-07
Iter: 579 loss: 4.8298989e-07
Iter: 580 loss: 4.83384838e-07
Iter: 581 loss: 4.82997223e-07
Iter: 582 loss: 4.82903602e-07
Iter: 583 loss: 4.82932478e-07
Iter: 584 loss: 4.82879841e-07
Iter: 585 loss: 4.82788607e-07
Iter: 586 loss: 4.82693849e-07
Iter: 587 loss: 4.82683163e-07
Iter: 588 loss: 4.82553332e-07
Iter: 589 loss: 4.82677819e-07
Iter: 590 loss: 4.82484666e-07
Iter: 591 loss: 4.82347389e-07
Iter: 592 loss: 4.8411664e-07
Iter: 593 loss: 4.82362225e-07
Iter: 594 loss: 4.82203518e-07
Iter: 595 loss: 4.82763426e-07
Iter: 596 loss: 4.82183339e-07
Iter: 597 loss: 4.82131895e-07
Iter: 598 loss: 4.81966424e-07
Iter: 599 loss: 4.84797567e-07
Iter: 600 loss: 4.81969039e-07
Iter: 601 loss: 4.81842051e-07
Iter: 602 loss: 4.83440431e-07
Iter: 603 loss: 4.8184171e-07
Iter: 604 loss: 4.81724385e-07
Iter: 605 loss: 4.82785e-07
Iter: 606 loss: 4.81729217e-07
Iter: 607 loss: 4.81664642e-07
Iter: 608 loss: 4.81504514e-07
Iter: 609 loss: 4.84026486e-07
Iter: 610 loss: 4.81518782e-07
Iter: 611 loss: 4.81311e-07
Iter: 612 loss: 4.81712618e-07
Iter: 613 loss: 4.81283564e-07
Iter: 614 loss: 4.81229e-07
Iter: 615 loss: 4.81198924e-07
Iter: 616 loss: 4.81127472e-07
Iter: 617 loss: 4.80961205e-07
Iter: 618 loss: 4.84083898e-07
Iter: 619 loss: 4.80925053e-07
Iter: 620 loss: 4.8078e-07
Iter: 621 loss: 4.81361099e-07
Iter: 622 loss: 4.80735764e-07
Iter: 623 loss: 4.80665562e-07
Iter: 624 loss: 4.80670565e-07
Iter: 625 loss: 4.80586777e-07
Iter: 626 loss: 4.80535277e-07
Iter: 627 loss: 4.80544543e-07
Iter: 628 loss: 4.80418066e-07
Iter: 629 loss: 4.8069694e-07
Iter: 630 loss: 4.80385097e-07
Iter: 631 loss: 4.80289032e-07
Iter: 632 loss: 4.80944209e-07
Iter: 633 loss: 4.80290623e-07
Iter: 634 loss: 4.80194558e-07
Iter: 635 loss: 4.80126573e-07
Iter: 636 loss: 4.80128392e-07
Iter: 637 loss: 4.79994696e-07
Iter: 638 loss: 4.80046083e-07
Iter: 639 loss: 4.79922164e-07
Iter: 640 loss: 4.7981581e-07
Iter: 641 loss: 4.80930964e-07
Iter: 642 loss: 4.79802281e-07
Iter: 643 loss: 4.79680693e-07
Iter: 644 loss: 4.79915457e-07
Iter: 645 loss: 4.79638857e-07
Iter: 646 loss: 4.79558821e-07
Iter: 647 loss: 4.79442178e-07
Iter: 648 loss: 4.79412392e-07
Iter: 649 loss: 4.79312689e-07
Iter: 650 loss: 4.80352412e-07
Iter: 651 loss: 4.79287849e-07
Iter: 652 loss: 4.79194e-07
Iter: 653 loss: 4.80118729e-07
Iter: 654 loss: 4.79184621e-07
Iter: 655 loss: 4.79116409e-07
Iter: 656 loss: 4.79002722e-07
Iter: 657 loss: 4.81867914e-07
Iter: 658 loss: 4.7899664e-07
Iter: 659 loss: 4.78956963e-07
Iter: 660 loss: 4.78941502e-07
Iter: 661 loss: 4.78873289e-07
Iter: 662 loss: 4.78734819e-07
Iter: 663 loss: 4.80261747e-07
Iter: 664 loss: 4.78734933e-07
Iter: 665 loss: 4.78609252e-07
Iter: 666 loss: 4.79562971e-07
Iter: 667 loss: 4.78627271e-07
Iter: 668 loss: 4.78476238e-07
Iter: 669 loss: 4.79210314e-07
Iter: 670 loss: 4.78476181e-07
Iter: 671 loss: 4.78423715e-07
Iter: 672 loss: 4.78301956e-07
Iter: 673 loss: 4.78284733e-07
Iter: 674 loss: 4.78156494e-07
Iter: 675 loss: 4.7840615e-07
Iter: 676 loss: 4.78071172e-07
Iter: 677 loss: 4.78074412e-07
Iter: 678 loss: 4.78045649e-07
Iter: 679 loss: 4.77989715e-07
Iter: 680 loss: 4.77989943e-07
Iter: 681 loss: 4.77952938e-07
Iter: 682 loss: 4.77876767e-07
Iter: 683 loss: 4.77846243e-07
Iter: 684 loss: 4.77803496e-07
Iter: 685 loss: 4.77732215e-07
Iter: 686 loss: 4.78034963e-07
Iter: 687 loss: 4.7768475e-07
Iter: 688 loss: 4.77614492e-07
Iter: 689 loss: 4.78481411e-07
Iter: 690 loss: 4.77606534e-07
Iter: 691 loss: 4.77528943e-07
Iter: 692 loss: 4.77473804e-07
Iter: 693 loss: 4.77449419e-07
Iter: 694 loss: 4.77361141e-07
Iter: 695 loss: 4.77378421e-07
Iter: 696 loss: 4.77289348e-07
Iter: 697 loss: 4.77195613e-07
Iter: 698 loss: 4.77187371e-07
Iter: 699 loss: 4.77108131e-07
Iter: 700 loss: 4.77166395e-07
Iter: 701 loss: 4.77101196e-07
Iter: 702 loss: 4.76982223e-07
Iter: 703 loss: 4.77008143e-07
Iter: 704 loss: 4.76928733e-07
Iter: 705 loss: 4.76772271e-07
Iter: 706 loss: 4.78009611e-07
Iter: 707 loss: 4.7674871e-07
Iter: 708 loss: 4.76596284e-07
Iter: 709 loss: 4.77389335e-07
Iter: 710 loss: 4.76599865e-07
Iter: 711 loss: 4.76475066e-07
Iter: 712 loss: 4.77720846e-07
Iter: 713 loss: 4.76450452e-07
Iter: 714 loss: 4.76302802e-07
Iter: 715 loss: 4.76433513e-07
Iter: 716 loss: 4.76246612e-07
Iter: 717 loss: 4.76118032e-07
Iter: 718 loss: 4.76190792e-07
Iter: 719 loss: 4.76023672e-07
Iter: 720 loss: 4.75945512e-07
Iter: 721 loss: 4.7723006e-07
Iter: 722 loss: 4.75954778e-07
Iter: 723 loss: 4.75883922e-07
Iter: 724 loss: 4.76171863e-07
Iter: 725 loss: 4.75845354e-07
Iter: 726 loss: 4.75794025e-07
Iter: 727 loss: 4.75789506e-07
Iter: 728 loss: 4.75723141e-07
Iter: 729 loss: 4.75728029e-07
Iter: 730 loss: 4.75735902e-07
Iter: 731 loss: 4.75728257e-07
Iter: 732 loss: 4.75708759e-07
Iter: 733 loss: 4.75715751e-07
Iter: 734 loss: 4.75727035e-07
Iter: 735 loss: 4.75710607e-07
Iter: 736 loss: 4.75697107e-07
Iter: 737 loss: 4.75718366e-07
Iter: 738 loss: 4.7572891e-07
Iter: 739 loss: 4.75708589e-07
Iter: 740 loss: 4.7572729e-07
Iter: 741 loss: 4.75726239e-07
Iter: 742 loss: 4.75711403e-07
Iter: 743 loss: 4.75724846e-07
Iter: 744 loss: 4.7571919e-07
Iter: 745 loss: 4.75723652e-07
Iter: 746 loss: 4.75717712e-07
Iter: 747 loss: 4.75720185e-07
Iter: 748 loss: 4.75718025e-07
Iter: 749 loss: 4.75718366e-07
Iter: 750 loss: 4.75719219e-07
Iter: 751 loss: 4.75722459e-07
Iter: 752 loss: 4.75722288e-07
Iter: 753 loss: 4.75722345e-07
Iter: 754 loss: 4.75722345e-07
Iter: 755 loss: 4.75723681e-07
Iter: 756 loss: 4.75722345e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2
+ date
Thu Oct 22 01:33:57 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/500_500_500_500_1 --function f1 --psi 0 --phi 1.2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a484e6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a484b8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a484486a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a5006a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a4842ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a4841a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a4841aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a483501e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a48350268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a483508c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a4837b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a482499d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a482688c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a482be0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a482e7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a482be048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a483a2f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a48291620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a482912f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a481f6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a48165378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a481890d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a48165f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a481b5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a48101488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a4820db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a4820d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a48213378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a48213048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a30410950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a30447268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a480c5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a303fd730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a480c7d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a3038b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a302cff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.016291335
test_loss: 0.015261427
train_loss: 0.0053271665
test_loss: 0.004914633
train_loss: 0.0024953927
test_loss: 0.0024968334
train_loss: 0.0019530149
test_loss: 0.0019468341
train_loss: 0.0018827415
test_loss: 0.0018946326
train_loss: 0.0014372927
test_loss: 0.0015974895
train_loss: 0.0015572193
test_loss: 0.0017487978
train_loss: 0.0017620807
test_loss: 0.0015091171
train_loss: 0.0018787053
test_loss: 0.0014094622
train_loss: 0.0015870593
test_loss: 0.0013763697
train_loss: 0.0016967938
test_loss: 0.0015748126
train_loss: 0.0013585563
test_loss: 0.0016009093
train_loss: 0.0016114385
test_loss: 0.0017122808
train_loss: 0.0015316987
test_loss: 0.0015740467
train_loss: 0.0012965659
test_loss: 0.001563093
train_loss: 0.0015276751
test_loss: 0.001590084
train_loss: 0.0017261384
test_loss: 0.0014600614
train_loss: 0.0014428524
test_loss: 0.0014338477
train_loss: 0.0012464612
test_loss: 0.0012835442
train_loss: 0.001604849
test_loss: 0.0014627845
train_loss: 0.0013862667
test_loss: 0.0014941957
train_loss: 0.0014000946
test_loss: 0.0016046604
train_loss: 0.0017657005
test_loss: 0.0014104183
train_loss: 0.0012583125
test_loss: 0.0014023547
train_loss: 0.0013589496
test_loss: 0.0015357877
train_loss: 0.0015917853
test_loss: 0.0014624915
train_loss: 0.0013610035
test_loss: 0.0013824408
train_loss: 0.0011852961
test_loss: 0.0014720999
train_loss: 0.0017050232
test_loss: 0.0018147375
train_loss: 0.0014175021
test_loss: 0.001710656
train_loss: 0.0015033729
test_loss: 0.0015656117
train_loss: 0.0015434308
test_loss: 0.001498219
train_loss: 0.0012830999
test_loss: 0.0015277301
train_loss: 0.0016903861
test_loss: 0.001466708
train_loss: 0.001195532
test_loss: 0.0012840711
train_loss: 0.0017181594
test_loss: 0.0014051042
train_loss: 0.0013516584
test_loss: 0.0017639897
train_loss: 0.0013629091
test_loss: 0.001411204
train_loss: 0.0015029052
test_loss: 0.0012076535
train_loss: 0.001488325
test_loss: 0.0012148246
train_loss: 0.0014099375
test_loss: 0.0015791636
train_loss: 0.0012206605
test_loss: 0.0019054865
train_loss: 0.0014458415
test_loss: 0.0012850582
train_loss: 0.0013232066
test_loss: 0.0011956833
train_loss: 0.0012700658
test_loss: 0.001376854
train_loss: 0.0012052235
test_loss: 0.0012464867
train_loss: 0.0012847222
test_loss: 0.0016447377
train_loss: 0.0015578353
test_loss: 0.0014986008
train_loss: 0.0013858965
test_loss: 0.0012820805
train_loss: 0.0014330607
test_loss: 0.0014199831
train_loss: 0.0012692579
test_loss: 0.0014265841
train_loss: 0.001274656
test_loss: 0.0013206775
train_loss: 0.0013838856
test_loss: 0.0015274943
train_loss: 0.0012475447
test_loss: 0.0014699807
train_loss: 0.00137559
test_loss: 0.0013102504
train_loss: 0.0012499085
test_loss: 0.0017188027
train_loss: 0.0016570855
test_loss: 0.0017977847
train_loss: 0.0013342415
test_loss: 0.0014655876
train_loss: 0.0014272047
test_loss: 0.0013490682
train_loss: 0.0013417245
test_loss: 0.0013056705
train_loss: 0.0012514282
test_loss: 0.0014612317
train_loss: 0.0014331673
test_loss: 0.0014283668
train_loss: 0.0014216088
test_loss: 0.0015812115
train_loss: 0.0012793666
test_loss: 0.0013394179
train_loss: 0.00135694
test_loss: 0.0012973236
train_loss: 0.0013420576
test_loss: 0.0012830623
train_loss: 0.0013303084
test_loss: 0.0013431364
train_loss: 0.0015120552
test_loss: 0.0014266638
train_loss: 0.0013349582
test_loss: 0.0012924982
train_loss: 0.0013307789
test_loss: 0.0012891111
train_loss: 0.0014761947
test_loss: 0.0012393817
train_loss: 0.0016228226
test_loss: 0.0015416705
train_loss: 0.0017335135
test_loss: 0.0015501031
train_loss: 0.0017421537
test_loss: 0.0016118749
train_loss: 0.0014777523
test_loss: 0.0013642028
train_loss: 0.0011915722
test_loss: 0.0014698415
train_loss: 0.0013968644
test_loss: 0.0015573815
train_loss: 0.0012733375
test_loss: 0.0011848687
train_loss: 0.0015984413
test_loss: 0.0013721635
train_loss: 0.0012211099
test_loss: 0.0013649125
train_loss: 0.0012007994
test_loss: 0.0014154941
train_loss: 0.0012699591
test_loss: 0.0012711496
train_loss: 0.0016119842
test_loss: 0.001325052
train_loss: 0.0013200248
test_loss: 0.0013071338
train_loss: 0.001400106
test_loss: 0.0013767988
train_loss: 0.0014136841
test_loss: 0.0015541862
train_loss: 0.0015470295
test_loss: 0.0013440046
train_loss: 0.0015543759
test_loss: 0.0014440219
train_loss: 0.0013021242
test_loss: 0.0012966135
train_loss: 0.0014010302
test_loss: 0.001497982
train_loss: 0.0012782267
test_loss: 0.0015075293
train_loss: 0.0017621019
test_loss: 0.001592215
train_loss: 0.0012669046
test_loss: 0.0014785426
train_loss: 0.0013279096
test_loss: 0.0016003533
train_loss: 0.0012814936
test_loss: 0.0015352127
train_loss: 0.0014349864
test_loss: 0.0014014681
train_loss: 0.001452079
test_loss: 0.0015180332
train_loss: 0.0014257422
test_loss: 0.0015246243
train_loss: 0.0013811053
test_loss: 0.0013899127
train_loss: 0.0013586484
test_loss: 0.0013540497
train_loss: 0.0011660289
test_loss: 0.0014237565
train_loss: 0.0015727698
test_loss: 0.0013708657
train_loss: 0.0012697008
test_loss: 0.0016255178
train_loss: 0.0013773504
test_loss: 0.0014966563
train_loss: 0.0012772426
test_loss: 0.0012020842
train_loss: 0.001566249
test_loss: 0.0012758484
train_loss: 0.0014903888
test_loss: 0.0012379626
train_loss: 0.0013731973
test_loss: 0.0013451867
train_loss: 0.0014129999
test_loss: 0.0013558593
train_loss: 0.0012679009
test_loss: 0.0014407255
train_loss: 0.0015013977
test_loss: 0.0013055777
train_loss: 0.0014164441
test_loss: 0.0013945572
train_loss: 0.0013454246
test_loss: 0.001278305
train_loss: 0.0013721691
test_loss: 0.001286187
train_loss: 0.0012587206
test_loss: 0.0016232792
train_loss: 0.001442208
test_loss: 0.0016121541
train_loss: 0.001174557
test_loss: 0.0012276452
train_loss: 0.0011277992
test_loss: 0.0012169424
train_loss: 0.0013639531
test_loss: 0.0014849069
train_loss: 0.0012657609
test_loss: 0.0017102736
train_loss: 0.0012281991
test_loss: 0.0014641089
train_loss: 0.0011975802
test_loss: 0.0014569105
train_loss: 0.0014466939
test_loss: 0.0017491494
train_loss: 0.0014173011
test_loss: 0.001322126
train_loss: 0.0013398184
test_loss: 0.0013402068
train_loss: 0.0011760779
test_loss: 0.0013563026
train_loss: 0.0011928957
test_loss: 0.0013265533
train_loss: 0.0012393444
test_loss: 0.0013535123
train_loss: 0.0015029666
test_loss: 0.0014185638
train_loss: 0.0014916193
test_loss: 0.0014525829
train_loss: 0.0014189284
test_loss: 0.0012697898
train_loss: 0.0014288018
test_loss: 0.0013987194
train_loss: 0.0012016643
test_loss: 0.0014740202
train_loss: 0.0010274714
test_loss: 0.0012039467
train_loss: 0.0014819395
test_loss: 0.0014383039
train_loss: 0.0012397947
test_loss: 0.0017327573
train_loss: 0.0017225922
test_loss: 0.0014272175
train_loss: 0.0015446242
test_loss: 0.0015800638
train_loss: 0.0012717142
test_loss: 0.0013217741
train_loss: 0.0012647455
test_loss: 0.0014594737
train_loss: 0.0013213635
test_loss: 0.0012501535
train_loss: 0.0012128277
test_loss: 0.0013804954
train_loss: 0.0012514391
test_loss: 0.0014828059
train_loss: 0.0011633827
test_loss: 0.0013427761
train_loss: 0.0011527254
test_loss: 0.0013111341
train_loss: 0.001436139
test_loss: 0.0013858599
train_loss: 0.0014629483
test_loss: 0.001363653
train_loss: 0.0010392803
test_loss: 0.0012992404
train_loss: 0.0012595757
test_loss: 0.0013216358
train_loss: 0.0011095005
test_loss: 0.0011840675
train_loss: 0.001367789
test_loss: 0.001242526
train_loss: 0.0012470993
test_loss: 0.0012321451
train_loss: 0.0013008877
test_loss: 0.0011964887
train_loss: 0.0011973719
test_loss: 0.0016121545
train_loss: 0.0015985785
test_loss: 0.0012167108
train_loss: 0.001268001
test_loss: 0.0016724251
train_loss: 0.0014138639
test_loss: 0.0014982659
train_loss: 0.0012631692
test_loss: 0.0014135164
train_loss: 0.0012799873
test_loss: 0.0011851148
train_loss: 0.0012930932
test_loss: 0.0012453938
train_loss: 0.0011505294
test_loss: 0.0012428091
train_loss: 0.0014080646
test_loss: 0.0014468071
train_loss: 0.0014039391
test_loss: 0.0014521249
train_loss: 0.0014285725
test_loss: 0.0015286241
train_loss: 0.001181773
test_loss: 0.0012891922
train_loss: 0.0012099918
test_loss: 0.0018626741
train_loss: 0.001178423
test_loss: 0.0015585464
train_loss: 0.0014219045
test_loss: 0.0013347336
train_loss: 0.0012433958
test_loss: 0.0013615517
train_loss: 0.0015481135
test_loss: 0.0017232987
train_loss: 0.0014230132
test_loss: 0.0015342049
train_loss: 0.0013460936
test_loss: 0.0014620082
train_loss: 0.0015244088
test_loss: 0.0012484775
train_loss: 0.001261932
test_loss: 0.0013952277
train_loss: 0.0014782611
test_loss: 0.0014877162
train_loss: 0.0011354454
test_loss: 0.0013537909
train_loss: 0.0012316399
test_loss: 0.0012793492
train_loss: 0.0012667567
test_loss: 0.0013555532
train_loss: 0.0012544412
test_loss: 0.0012973788
train_loss: 0.0012783508
test_loss: 0.0013917903
train_loss: 0.002198456
test_loss: 0.0019513225
train_loss: 0.0017326844
test_loss: 0.0015378983
train_loss: 0.0019627418
test_loss: 0.0017732083
train_loss: 0.0014695572
test_loss: 0.0014155536
train_loss: 0.0015782531
test_loss: 0.0013439296
train_loss: 0.0012316601
test_loss: 0.0015599678
train_loss: 0.0015483736
test_loss: 0.0013877531
train_loss: 0.0013528627
test_loss: 0.0013885658
train_loss: 0.0012556302
test_loss: 0.0013627092
train_loss: 0.001185927
test_loss: 0.0013663521
train_loss: 0.0012783431
test_loss: 0.0015180941
train_loss: 0.0013240031
test_loss: 0.0013999745
train_loss: 0.0015666636
test_loss: 0.001323685
train_loss: 0.0012113769
test_loss: 0.0013252401
train_loss: 0.0012764062
test_loss: 0.0011949681
train_loss: 0.0014163814
test_loss: 0.0012899763
train_loss: 0.0013706028
test_loss: 0.0014593407
train_loss: 0.0013321326
test_loss: 0.0013272141
train_loss: 0.0015055872
test_loss: 0.0013806592
train_loss: 0.00136409
test_loss: 0.0014056981
train_loss: 0.0013558037
test_loss: 0.0014087791
train_loss: 0.0023200375
test_loss: 0.002175279
train_loss: 0.0023222906
test_loss: 0.0022252456
train_loss: 0.002322521
test_loss: 0.001812144
train_loss: 0.0018884584
test_loss: 0.0015833116
train_loss: 0.0018000458
test_loss: 0.0014438229
train_loss: 0.0017082932
test_loss: 0.0016016081
train_loss: 0.0016209967
test_loss: 0.0012928635
train_loss: 0.0010738358
test_loss: 0.0011782916
train_loss: 0.0014600538
test_loss: 0.001492226
train_loss: 0.0013012374
test_loss: 0.0010865505
train_loss: 0.0012394325
test_loss: 0.0010989209
train_loss: 0.0011581893
test_loss: 0.001306241
train_loss: 0.0014694585
test_loss: 0.0013199793
train_loss: 0.0015468014
test_loss: 0.0013869336
train_loss: 0.0014635691
test_loss: 0.0013694549
train_loss: 0.00135697
test_loss: 0.0013846344
train_loss: 0.0011477141
test_loss: 0.0012160384
train_loss: 0.001164286
test_loss: 0.0013675456
train_loss: 0.0016684909
test_loss: 0.001366655
train_loss: 0.001208054
test_loss: 0.0013615543
train_loss: 0.0014181505
test_loss: 0.0013353353
train_loss: 0.0011437637
test_loss: 0.001357134
train_loss: 0.0013044856
test_loss: 0.0013394644
train_loss: 0.0015909582
test_loss: 0.0012688176
train_loss: 0.0013630607
test_loss: 0.0016831943
train_loss: 0.001323356
test_loss: 0.0014954712
train_loss: 0.0016055963
test_loss: 0.0014005452
train_loss: 0.0013236164
test_loss: 0.001533302
train_loss: 0.0013179219
test_loss: 0.0014715383
train_loss: 0.0015046911
test_loss: 0.001300736
train_loss: 0.0013684715
test_loss: 0.0014949322
train_loss: 0.0013324008
test_loss: 0.001274422
train_loss: 0.001317307
test_loss: 0.0014031836
train_loss: 0.0013136368
test_loss: 0.00121323
train_loss: 0.0011296821
test_loss: 0.0011203411
train_loss: 0.0011703603
test_loss: 0.0013554011
train_loss: 0.0012429985
test_loss: 0.0014295104
train_loss: 0.001424603
test_loss: 0.0012358477
train_loss: 0.0013841684
test_loss: 0.0012647472
train_loss: 0.0012208222
test_loss: 0.0012361503
train_loss: 0.0013595455
test_loss: 0.0015241171
train_loss: 0.0011519144
test_loss: 0.0011821005
train_loss: 0.0017363213
test_loss: 0.0013387394
train_loss: 0.0013160643
test_loss: 0.0012933507
train_loss: 0.0014521894
test_loss: 0.0012448234
train_loss: 0.0014172767
test_loss: 0.0014808322
train_loss: 0.0012164181
test_loss: 0.0012465707
train_loss: 0.0015813141
test_loss: 0.00130526
train_loss: 0.0011997207
test_loss: 0.0013304439
train_loss: 0.0013880467
test_loss: 0.0012792724
train_loss: 0.0012395494
test_loss: 0.0013815799
train_loss: 0.0013395152
test_loss: 0.0014695199
train_loss: 0.0012114979
test_loss: 0.0013473447
train_loss: 0.0015089026
test_loss: 0.0013722613
train_loss: 0.0014104822
test_loss: 0.0015111769
train_loss: 0.0013106733
test_loss: 0.0013854015
train_loss: 0.0017328695
test_loss: 0.0013984869
train_loss: 0.001334531
test_loss: 0.0014914535
train_loss: 0.0013669829
test_loss: 0.001338865
train_loss: 0.0014586158
test_loss: 0.0012539352
train_loss: 0.0014477458
test_loss: 0.0012557734
train_loss: 0.0012680849
test_loss: 0.0015057193
train_loss: 0.0013097513
test_loss: 0.0013272578
train_loss: 0.0014832707
test_loss: 0.0013758776
train_loss: 0.0012480641
test_loss: 0.0014461299
train_loss: 0.0015835882
test_loss: 0.0014382041
train_loss: 0.0014776278
test_loss: 0.0014613231
train_loss: 0.0017600805
test_loss: 0.0015254479
train_loss: 0.0012171068
test_loss: 0.0011601247
train_loss: 0.0011357742
test_loss: 0.0014377738
train_loss: 0.0012217641
test_loss: 0.0013884958
train_loss: 0.0010888234
test_loss: 0.0014931952
train_loss: 0.0011979823
test_loss: 0.0014530523
train_loss: 0.0011644781
test_loss: 0.001303156
train_loss: 0.0012981782
test_loss: 0.0013540745
train_loss: 0.0013913332
test_loss: 0.0013130999
train_loss: 0.0011629221
test_loss: 0.0013637821
train_loss: 0.0015575597
test_loss: 0.0012670659
train_loss: 0.0012498357
test_loss: 0.0015791334
train_loss: 0.0013130377
test_loss: 0.0013039067
train_loss: 0.0012093369
test_loss: 0.0013615672
train_loss: 0.0014066717
test_loss: 0.0013343524
train_loss: 0.0014926074
test_loss: 0.0014265855
train_loss: 0.0014049833
test_loss: 0.0012759215
train_loss: 0.0012972119
test_loss: 0.0011681407
train_loss: 0.0011654524
test_loss: 0.0012325511
train_loss: 0.001393002
test_loss: 0.0013524805
train_loss: 0.0012997489
test_loss: 0.0014246955
train_loss: 0.001551545
test_loss: 0.0017013501
train_loss: 0.0016955592
test_loss: 0.0014869656
train_loss: 0.0013434403
test_loss: 0.0016566887
train_loss: 0.0016574499
test_loss: 0.0014620173
train_loss: 0.0013265632
test_loss: 0.001688475
train_loss: 0.0011985131
test_loss: 0.0016891283
train_loss: 0.0017105483
test_loss: 0.0015556328
train_loss: 0.0010824394
test_loss: 0.0013613458
train_loss: 0.001379664
test_loss: 0.0015072046
train_loss: 0.0014710578
test_loss: 0.0011626438
train_loss: 0.0012691073
test_loss: 0.0014846519
train_loss: 0.0011149341
test_loss: 0.001094186
train_loss: 0.0012988442
test_loss: 0.0011926127
train_loss: 0.0014065709
test_loss: 0.001344525
train_loss: 0.001427534
test_loss: 0.0013798112
train_loss: 0.0012646231
test_loss: 0.0014084519
train_loss: 0.0013009465
test_loss: 0.0015169214
train_loss: 0.0011801964
test_loss: 0.0013963756
train_loss: 0.0013703471
test_loss: 0.0013667481
train_loss: 0.0011598046
test_loss: 0.0011626879
train_loss: 0.0013563838
test_loss: 0.00144286
train_loss: 0.0014317775
test_loss: 0.0015487886
train_loss: 0.0018875228
test_loss: 0.0020591612
train_loss: 0.0017968772
test_loss: 0.0021106098
train_loss: 0.0016898974
test_loss: 0.001665241
train_loss: 0.001396993
test_loss: 0.0019863623
train_loss: 0.0017186129
test_loss: 0.0017367778
train_loss: 0.0016356704
test_loss: 0.0016142932
train_loss: 0.0013713352
test_loss: 0.0017514153
train_loss: 0.0013697225
test_loss: 0.0016100378
train_loss: 0.0012446583
test_loss: 0.0015444744
train_loss: 0.0012038001
test_loss: 0.0013526562
train_loss: 0.0013722653
test_loss: 0.0014596136
train_loss: 0.001571138
test_loss: 0.001583535
train_loss: 0.0017173234
test_loss: 0.0014151142
train_loss: 0.0014798588
test_loss: 0.0012940739
train_loss: 0.0013414497
test_loss: 0.0012744946
train_loss: 0.0011968073
test_loss: 0.0014968128
train_loss: 0.0011109058
test_loss: 0.0014717242
train_loss: 0.0011355039
test_loss: 0.0012695497
train_loss: 0.0014331952
test_loss: 0.0014455125
train_loss: 0.0013616709
test_loss: 0.0012311229
train_loss: 0.0012090763
test_loss: 0.0013237435
train_loss: 0.0011783362
test_loss: 0.0012105245
train_loss: 0.0013254847
test_loss: 0.0014604953
train_loss: 0.001188823
test_loss: 0.0014308526
train_loss: 0.0012728258
test_loss: 0.0015896548
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0014506392
test_loss: 0.0013837549
train_loss: 0.0012423504
test_loss: 0.0012921887
train_loss: 0.0014081984
test_loss: 0.001389182
train_loss: 0.0012889842
test_loss: 0.001485655
train_loss: 0.0011362523
test_loss: 0.0011789033
train_loss: 0.0012054176
test_loss: 0.0013219304
train_loss: 0.0016644375
test_loss: 0.0015412075
train_loss: 0.0015101566
test_loss: 0.0014159658
train_loss: 0.0010852427
test_loss: 0.0013658324
train_loss: 0.0015970602
test_loss: 0.0014202511
train_loss: 0.0013765778
test_loss: 0.0014038499
train_loss: 0.0012386651
test_loss: 0.0012470316
train_loss: 0.0012603127
test_loss: 0.0012213171
train_loss: 0.0013046446
test_loss: 0.0011987964
train_loss: 0.0012700768
test_loss: 0.0011753882
train_loss: 0.0011992811
test_loss: 0.001125348
train_loss: 0.0012327024
test_loss: 0.0015045875
train_loss: 0.001248869
test_loss: 0.0013150304
train_loss: 0.0013298772
test_loss: 0.0013259131
train_loss: 0.0011368082
test_loss: 0.0013007708
train_loss: 0.0011925662
test_loss: 0.0011571011
train_loss: 0.0010726574
test_loss: 0.001070932
train_loss: 0.0012284801
test_loss: 0.0012491918
train_loss: 0.0012001316
test_loss: 0.0014043736
train_loss: 0.0013037634
test_loss: 0.0011496955
train_loss: 0.0014432659
test_loss: 0.0012818163
train_loss: 0.0013302389
test_loss: 0.0012705522
train_loss: 0.0015282813
test_loss: 0.0017385475
train_loss: 0.0013277543
test_loss: 0.001300073
train_loss: 0.0014278791
test_loss: 0.0013389866
train_loss: 0.0014438257
test_loss: 0.0014038653
train_loss: 0.0012887812
test_loss: 0.0012391572
train_loss: 0.0011160474
test_loss: 0.0012875766
train_loss: 0.001145148
test_loss: 0.0012174948
train_loss: 0.0010929424
test_loss: 0.0014239106
train_loss: 0.0012307256
test_loss: 0.0010636096
train_loss: 0.0012236743
test_loss: 0.0012845627
train_loss: 0.0014470944
test_loss: 0.0013000048
train_loss: 0.0014399631
test_loss: 0.0014646789
train_loss: 0.0012855147
test_loss: 0.0010991577
train_loss: 0.0013997988
test_loss: 0.00125797
train_loss: 0.0013184515
test_loss: 0.0012844566
train_loss: 0.0013882037
test_loss: 0.0015199311
train_loss: 0.0015349233
test_loss: 0.0013383762
train_loss: 0.0013957517
test_loss: 0.0014185361
train_loss: 0.0009986303
test_loss: 0.0010641721
train_loss: 0.0014123024
test_loss: 0.0015094082
train_loss: 0.0013228997
test_loss: 0.0015704382
train_loss: 0.0015776528
test_loss: 0.0012145046
train_loss: 0.0014519352
test_loss: 0.0014971675
train_loss: 0.0013067329
test_loss: 0.0014930018
train_loss: 0.0012438475
test_loss: 0.0013555273
train_loss: 0.001044681
test_loss: 0.0013034272
train_loss: 0.0012099746
test_loss: 0.001170112
train_loss: 0.0013976487
test_loss: 0.0013689067
train_loss: 0.0014927587
test_loss: 0.0012300798
train_loss: 0.001327611
test_loss: 0.0012911031
train_loss: 0.0011167455
test_loss: 0.0014215757
train_loss: 0.0012729326
test_loss: 0.0014511582
train_loss: 0.0011377886
test_loss: 0.0011410235
train_loss: 0.0011031013
test_loss: 0.0010777845
train_loss: 0.0015111711
test_loss: 0.0014149307
train_loss: 0.0013715022
test_loss: 0.0013663659
train_loss: 0.0014758351
test_loss: 0.001434961
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aecb4e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aecb6d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aecbd9730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aecab9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aeca8dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aeca8d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aeca599d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aeca929d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aeca59378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aec9cbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aec9cb378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aec9c3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aec975d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa84e3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa84e17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa84c07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa84c0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa844ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa844c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa842c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa83ef8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a804810d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1aa83ef950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a80472c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a80472bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a80472620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a80472510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a803cf8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a803cfe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a80385f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a803b31e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a803b37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a80304840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a80359510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a80337ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1a802f4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.0013266e-06
Iter: 2 loss: 1.89426362e-06
Iter: 3 loss: 1.79206177e-06
Iter: 4 loss: 1.41040891e-06
Iter: 5 loss: 2.72809439e-06
Iter: 6 loss: 1.30936132e-06
Iter: 7 loss: 1.14380452e-06
Iter: 8 loss: 2.43681416e-06
Iter: 9 loss: 1.13207739e-06
Iter: 10 loss: 1.00121167e-06
Iter: 11 loss: 1.12387886e-06
Iter: 12 loss: 9.26097528e-07
Iter: 13 loss: 8.52631615e-07
Iter: 14 loss: 1.58507873e-06
Iter: 15 loss: 8.50243339e-07
Iter: 16 loss: 7.85457701e-07
Iter: 17 loss: 8.65204e-07
Iter: 18 loss: 7.51743926e-07
Iter: 19 loss: 7.28437612e-07
Iter: 20 loss: 8.87816725e-07
Iter: 21 loss: 7.26165695e-07
Iter: 22 loss: 7.135327e-07
Iter: 23 loss: 8.20327273e-07
Iter: 24 loss: 7.1278788e-07
Iter: 25 loss: 7.0321255e-07
Iter: 26 loss: 6.93600327e-07
Iter: 27 loss: 6.91548792e-07
Iter: 28 loss: 6.78255105e-07
Iter: 29 loss: 6.81217273e-07
Iter: 30 loss: 6.68432165e-07
Iter: 31 loss: 6.55999315e-07
Iter: 32 loss: 7.37230152e-07
Iter: 33 loss: 6.54671794e-07
Iter: 34 loss: 6.43979774e-07
Iter: 35 loss: 6.845396e-07
Iter: 36 loss: 6.41394649e-07
Iter: 37 loss: 6.32475292e-07
Iter: 38 loss: 6.46943249e-07
Iter: 39 loss: 6.2835511e-07
Iter: 40 loss: 6.26345468e-07
Iter: 41 loss: 6.24779602e-07
Iter: 42 loss: 6.21145773e-07
Iter: 43 loss: 6.20082119e-07
Iter: 44 loss: 6.17890407e-07
Iter: 45 loss: 6.12915073e-07
Iter: 46 loss: 6.381905e-07
Iter: 47 loss: 6.12086183e-07
Iter: 48 loss: 6.07129493e-07
Iter: 49 loss: 6.16208808e-07
Iter: 50 loss: 6.05026e-07
Iter: 51 loss: 6.0154764e-07
Iter: 52 loss: 6.1529488e-07
Iter: 53 loss: 6.00750127e-07
Iter: 54 loss: 5.95586414e-07
Iter: 55 loss: 5.87136242e-07
Iter: 56 loss: 5.87100544e-07
Iter: 57 loss: 5.80173378e-07
Iter: 58 loss: 5.97372207e-07
Iter: 59 loss: 5.77803519e-07
Iter: 60 loss: 5.69252791e-07
Iter: 61 loss: 6.29466854e-07
Iter: 62 loss: 5.6845181e-07
Iter: 63 loss: 5.62533273e-07
Iter: 64 loss: 5.66933522e-07
Iter: 65 loss: 5.58952479e-07
Iter: 66 loss: 5.53820314e-07
Iter: 67 loss: 5.56911459e-07
Iter: 68 loss: 5.50511459e-07
Iter: 69 loss: 5.45846433e-07
Iter: 70 loss: 5.77148171e-07
Iter: 71 loss: 5.45329669e-07
Iter: 72 loss: 5.42099883e-07
Iter: 73 loss: 5.55291081e-07
Iter: 74 loss: 5.41398549e-07
Iter: 75 loss: 5.38490667e-07
Iter: 76 loss: 5.38372205e-07
Iter: 77 loss: 5.36161167e-07
Iter: 78 loss: 5.34135893e-07
Iter: 79 loss: 5.34113042e-07
Iter: 80 loss: 5.32529953e-07
Iter: 81 loss: 5.3549843e-07
Iter: 82 loss: 5.3186011e-07
Iter: 83 loss: 5.29924591e-07
Iter: 84 loss: 5.50017376e-07
Iter: 85 loss: 5.29839497e-07
Iter: 86 loss: 5.29002591e-07
Iter: 87 loss: 5.29742806e-07
Iter: 88 loss: 5.28481962e-07
Iter: 89 loss: 5.2702552e-07
Iter: 90 loss: 5.28282499e-07
Iter: 91 loss: 5.26180258e-07
Iter: 92 loss: 5.25277414e-07
Iter: 93 loss: 5.34851e-07
Iter: 94 loss: 5.25268206e-07
Iter: 95 loss: 5.24370648e-07
Iter: 96 loss: 5.23036533e-07
Iter: 97 loss: 5.22996345e-07
Iter: 98 loss: 5.21824518e-07
Iter: 99 loss: 5.24842335e-07
Iter: 100 loss: 5.21435368e-07
Iter: 101 loss: 5.20291451e-07
Iter: 102 loss: 5.3278967e-07
Iter: 103 loss: 5.20270419e-07
Iter: 104 loss: 5.19357059e-07
Iter: 105 loss: 5.18191598e-07
Iter: 106 loss: 5.1809775e-07
Iter: 107 loss: 5.16208502e-07
Iter: 108 loss: 5.16095042e-07
Iter: 109 loss: 5.1471676e-07
Iter: 110 loss: 5.12656584e-07
Iter: 111 loss: 5.13831708e-07
Iter: 112 loss: 5.11341682e-07
Iter: 113 loss: 5.09581241e-07
Iter: 114 loss: 5.09457777e-07
Iter: 115 loss: 5.08735468e-07
Iter: 116 loss: 5.08777816e-07
Iter: 117 loss: 5.0817232e-07
Iter: 118 loss: 5.08120877e-07
Iter: 119 loss: 5.07771347e-07
Iter: 120 loss: 5.07433185e-07
Iter: 121 loss: 5.06985202e-07
Iter: 122 loss: 5.06918241e-07
Iter: 123 loss: 5.06601396e-07
Iter: 124 loss: 5.066114e-07
Iter: 125 loss: 5.06394599e-07
Iter: 126 loss: 5.06271078e-07
Iter: 127 loss: 5.06183881e-07
Iter: 128 loss: 5.05894491e-07
Iter: 129 loss: 5.07219283e-07
Iter: 130 loss: 5.05823664e-07
Iter: 131 loss: 5.05471235e-07
Iter: 132 loss: 5.05147625e-07
Iter: 133 loss: 5.05072592e-07
Iter: 134 loss: 5.04552702e-07
Iter: 135 loss: 5.05940818e-07
Iter: 136 loss: 5.0434511e-07
Iter: 137 loss: 5.03795491e-07
Iter: 138 loss: 5.07639243e-07
Iter: 139 loss: 5.03729154e-07
Iter: 140 loss: 5.03299e-07
Iter: 141 loss: 5.02413457e-07
Iter: 142 loss: 5.17460421e-07
Iter: 143 loss: 5.02365253e-07
Iter: 144 loss: 5.00988904e-07
Iter: 145 loss: 5.02761395e-07
Iter: 146 loss: 5.00231124e-07
Iter: 147 loss: 4.99125122e-07
Iter: 148 loss: 5.02034823e-07
Iter: 149 loss: 4.98767122e-07
Iter: 150 loss: 4.97723136e-07
Iter: 151 loss: 5.07095251e-07
Iter: 152 loss: 4.97690394e-07
Iter: 153 loss: 4.97111614e-07
Iter: 154 loss: 4.98846e-07
Iter: 155 loss: 4.96923e-07
Iter: 156 loss: 4.9653994e-07
Iter: 157 loss: 5.02776061e-07
Iter: 158 loss: 4.96532493e-07
Iter: 159 loss: 4.96020334e-07
Iter: 160 loss: 4.95387326e-07
Iter: 161 loss: 4.95313145e-07
Iter: 162 loss: 4.94940139e-07
Iter: 163 loss: 4.94937865e-07
Iter: 164 loss: 4.94606411e-07
Iter: 165 loss: 4.94761139e-07
Iter: 166 loss: 4.94413086e-07
Iter: 167 loss: 4.94012966e-07
Iter: 168 loss: 4.94747724e-07
Iter: 169 loss: 4.93852838e-07
Iter: 170 loss: 4.93304356e-07
Iter: 171 loss: 4.94212429e-07
Iter: 172 loss: 4.93090454e-07
Iter: 173 loss: 4.92741037e-07
Iter: 174 loss: 4.92624679e-07
Iter: 175 loss: 4.9244187e-07
Iter: 176 loss: 4.92052834e-07
Iter: 177 loss: 4.94190829e-07
Iter: 178 loss: 4.92037316e-07
Iter: 179 loss: 4.91700121e-07
Iter: 180 loss: 4.92160439e-07
Iter: 181 loss: 4.91547382e-07
Iter: 182 loss: 4.91259641e-07
Iter: 183 loss: 4.91676417e-07
Iter: 184 loss: 4.91125547e-07
Iter: 185 loss: 4.90760669e-07
Iter: 186 loss: 4.93394964e-07
Iter: 187 loss: 4.90703144e-07
Iter: 188 loss: 4.90379705e-07
Iter: 189 loss: 4.92997e-07
Iter: 190 loss: 4.90353614e-07
Iter: 191 loss: 4.90109869e-07
Iter: 192 loss: 4.90071443e-07
Iter: 193 loss: 4.89876641e-07
Iter: 194 loss: 4.89675926e-07
Iter: 195 loss: 4.89676722e-07
Iter: 196 loss: 4.89466117e-07
Iter: 197 loss: 4.89066508e-07
Iter: 198 loss: 4.9885125e-07
Iter: 199 loss: 4.89044396e-07
Iter: 200 loss: 4.88727267e-07
Iter: 201 loss: 4.93022242e-07
Iter: 202 loss: 4.88712772e-07
Iter: 203 loss: 4.88471755e-07
Iter: 204 loss: 4.88861247e-07
Iter: 205 loss: 4.88304522e-07
Iter: 206 loss: 4.88031901e-07
Iter: 207 loss: 4.87793216e-07
Iter: 208 loss: 4.8769607e-07
Iter: 209 loss: 4.87413445e-07
Iter: 210 loss: 4.87378941e-07
Iter: 211 loss: 4.87176465e-07
Iter: 212 loss: 4.8670961e-07
Iter: 213 loss: 4.95104132e-07
Iter: 214 loss: 4.86664931e-07
Iter: 215 loss: 4.8628317e-07
Iter: 216 loss: 4.87195507e-07
Iter: 217 loss: 4.86138617e-07
Iter: 218 loss: 4.85638566e-07
Iter: 219 loss: 4.8754174e-07
Iter: 220 loss: 4.8551351e-07
Iter: 221 loss: 4.85201156e-07
Iter: 222 loss: 4.8610309e-07
Iter: 223 loss: 4.85115606e-07
Iter: 224 loss: 4.84919497e-07
Iter: 225 loss: 4.84888801e-07
Iter: 226 loss: 4.84717361e-07
Iter: 227 loss: 4.84558541e-07
Iter: 228 loss: 4.84491352e-07
Iter: 229 loss: 4.84402563e-07
Iter: 230 loss: 4.84400914e-07
Iter: 231 loss: 4.84301722e-07
Iter: 232 loss: 4.84223506e-07
Iter: 233 loss: 4.84213047e-07
Iter: 234 loss: 4.8402535e-07
Iter: 235 loss: 4.8434481e-07
Iter: 236 loss: 4.83974816e-07
Iter: 237 loss: 4.83816621e-07
Iter: 238 loss: 4.85655164e-07
Iter: 239 loss: 4.83819235e-07
Iter: 240 loss: 4.83722488e-07
Iter: 241 loss: 4.83481074e-07
Iter: 242 loss: 4.85063424e-07
Iter: 243 loss: 4.83401209e-07
Iter: 244 loss: 4.83262909e-07
Iter: 245 loss: 4.83235567e-07
Iter: 246 loss: 4.83068675e-07
Iter: 247 loss: 4.82731309e-07
Iter: 248 loss: 4.86204954e-07
Iter: 249 loss: 4.82670657e-07
Iter: 250 loss: 4.82290034e-07
Iter: 251 loss: 4.85297392e-07
Iter: 252 loss: 4.82245468e-07
Iter: 253 loss: 4.819276e-07
Iter: 254 loss: 4.82057317e-07
Iter: 255 loss: 4.81674931e-07
Iter: 256 loss: 4.81390089e-07
Iter: 257 loss: 4.82335849e-07
Iter: 258 loss: 4.81278619e-07
Iter: 259 loss: 4.81134293e-07
Iter: 260 loss: 4.81075688e-07
Iter: 261 loss: 4.80937729e-07
Iter: 262 loss: 4.80701033e-07
Iter: 263 loss: 4.80701146e-07
Iter: 264 loss: 4.80385552e-07
Iter: 265 loss: 4.81902873e-07
Iter: 266 loss: 4.80315407e-07
Iter: 267 loss: 4.79965479e-07
Iter: 268 loss: 4.8194704e-07
Iter: 269 loss: 4.7994024e-07
Iter: 270 loss: 4.79713208e-07
Iter: 271 loss: 4.79411767e-07
Iter: 272 loss: 4.79390792e-07
Iter: 273 loss: 4.79202868e-07
Iter: 274 loss: 4.7915762e-07
Iter: 275 loss: 4.79053426e-07
Iter: 276 loss: 4.78743743e-07
Iter: 277 loss: 4.82178564e-07
Iter: 278 loss: 4.7875335e-07
Iter: 279 loss: 4.78435e-07
Iter: 280 loss: 4.80339395e-07
Iter: 281 loss: 4.7838455e-07
Iter: 282 loss: 4.78260631e-07
Iter: 283 loss: 4.78250968e-07
Iter: 284 loss: 4.78179288e-07
Iter: 285 loss: 4.78008701e-07
Iter: 286 loss: 4.81073471e-07
Iter: 287 loss: 4.78004608e-07
Iter: 288 loss: 4.7785062e-07
Iter: 289 loss: 4.78048548e-07
Iter: 290 loss: 4.77754725e-07
Iter: 291 loss: 4.77508138e-07
Iter: 292 loss: 4.78350671e-07
Iter: 293 loss: 4.7744004e-07
Iter: 294 loss: 4.77254218e-07
Iter: 295 loss: 4.77705839e-07
Iter: 296 loss: 4.77179867e-07
Iter: 297 loss: 4.77020308e-07
Iter: 298 loss: 4.79986397e-07
Iter: 299 loss: 4.77008712e-07
Iter: 300 loss: 4.76869843e-07
Iter: 301 loss: 4.76785374e-07
Iter: 302 loss: 4.76710881e-07
Iter: 303 loss: 4.76507125e-07
Iter: 304 loss: 4.78519723e-07
Iter: 305 loss: 4.7648598e-07
Iter: 306 loss: 4.76323748e-07
Iter: 307 loss: 4.76135455e-07
Iter: 308 loss: 4.76146795e-07
Iter: 309 loss: 4.75883837e-07
Iter: 310 loss: 4.76091316e-07
Iter: 311 loss: 4.75765404e-07
Iter: 312 loss: 4.75508813e-07
Iter: 313 loss: 4.75514696e-07
Iter: 314 loss: 4.75402942e-07
Iter: 315 loss: 4.75252023e-07
Iter: 316 loss: 4.7524469e-07
Iter: 317 loss: 4.75103263e-07
Iter: 318 loss: 4.77295032e-07
Iter: 319 loss: 4.75073705e-07
Iter: 320 loss: 4.74975053e-07
Iter: 321 loss: 4.7486742e-07
Iter: 322 loss: 4.74865544e-07
Iter: 323 loss: 4.74657185e-07
Iter: 324 loss: 4.7469581e-07
Iter: 325 loss: 4.74520618e-07
Iter: 326 loss: 4.74327067e-07
Iter: 327 loss: 4.74697686e-07
Iter: 328 loss: 4.74251863e-07
Iter: 329 loss: 4.74057657e-07
Iter: 330 loss: 4.75449042e-07
Iter: 331 loss: 4.74047908e-07
Iter: 332 loss: 4.73944965e-07
Iter: 333 loss: 4.73950536e-07
Iter: 334 loss: 4.73852879e-07
Iter: 335 loss: 4.73883489e-07
Iter: 336 loss: 4.7382909e-07
Iter: 337 loss: 4.73747775e-07
Iter: 338 loss: 4.7423643e-07
Iter: 339 loss: 4.73725549e-07
Iter: 340 loss: 4.73621526e-07
Iter: 341 loss: 4.73518668e-07
Iter: 342 loss: 4.73487404e-07
Iter: 343 loss: 4.73429793e-07
Iter: 344 loss: 4.74214545e-07
Iter: 345 loss: 4.73359535e-07
Iter: 346 loss: 4.73236099e-07
Iter: 347 loss: 4.73742119e-07
Iter: 348 loss: 4.73200771e-07
Iter: 349 loss: 4.73067473e-07
Iter: 350 loss: 4.72843055e-07
Iter: 351 loss: 4.72816907e-07
Iter: 352 loss: 4.72622133e-07
Iter: 353 loss: 4.75277602e-07
Iter: 354 loss: 4.7263967e-07
Iter: 355 loss: 4.72478689e-07
Iter: 356 loss: 4.72511715e-07
Iter: 357 loss: 4.72354458e-07
Iter: 358 loss: 4.72194188e-07
Iter: 359 loss: 4.72045457e-07
Iter: 360 loss: 4.71990859e-07
Iter: 361 loss: 4.71749729e-07
Iter: 362 loss: 4.73715232e-07
Iter: 363 loss: 4.71738218e-07
Iter: 364 loss: 4.71512806e-07
Iter: 365 loss: 4.71617852e-07
Iter: 366 loss: 4.71378854e-07
Iter: 367 loss: 4.71338581e-07
Iter: 368 loss: 4.71272472e-07
Iter: 369 loss: 4.71161684e-07
Iter: 370 loss: 4.71023384e-07
Iter: 371 loss: 4.70983565e-07
Iter: 372 loss: 4.7089307e-07
Iter: 373 loss: 4.70876671e-07
Iter: 374 loss: 4.70835346e-07
Iter: 375 loss: 4.70661575e-07
Iter: 376 loss: 4.70652282e-07
Iter: 377 loss: 4.70475783e-07
Iter: 378 loss: 4.70880707e-07
Iter: 379 loss: 4.70442728e-07
Iter: 380 loss: 4.70285784e-07
Iter: 381 loss: 4.70307924e-07
Iter: 382 loss: 4.70251848e-07
Iter: 383 loss: 4.70152429e-07
Iter: 384 loss: 4.72339877e-07
Iter: 385 loss: 4.70111502e-07
Iter: 386 loss: 4.70035701e-07
Iter: 387 loss: 4.71715055e-07
Iter: 388 loss: 4.70048434e-07
Iter: 389 loss: 4.69960327e-07
Iter: 390 loss: 4.69908457e-07
Iter: 391 loss: 4.69863437e-07
Iter: 392 loss: 4.69740769e-07
Iter: 393 loss: 4.69760835e-07
Iter: 394 loss: 4.69630152e-07
Iter: 395 loss: 4.6948162e-07
Iter: 396 loss: 4.69839222e-07
Iter: 397 loss: 4.69385725e-07
Iter: 398 loss: 4.69145107e-07
Iter: 399 loss: 4.69549946e-07
Iter: 400 loss: 4.6903358e-07
Iter: 401 loss: 4.68832582e-07
Iter: 402 loss: 4.70531063e-07
Iter: 403 loss: 4.68824339e-07
Iter: 404 loss: 4.68598472e-07
Iter: 405 loss: 4.69806537e-07
Iter: 406 loss: 4.68586279e-07
Iter: 407 loss: 4.68458609e-07
Iter: 408 loss: 4.687939e-07
Iter: 409 loss: 4.68462446e-07
Iter: 410 loss: 4.68321787e-07
Iter: 411 loss: 4.68236e-07
Iter: 412 loss: 4.68193605e-07
Iter: 413 loss: 4.68044959e-07
Iter: 414 loss: 4.68604696e-07
Iter: 415 loss: 4.67998206e-07
Iter: 416 loss: 4.6785658e-07
Iter: 417 loss: 4.6944723e-07
Iter: 418 loss: 4.67868034e-07
Iter: 419 loss: 4.67824236e-07
Iter: 420 loss: 4.67732775e-07
Iter: 421 loss: 4.67719588e-07
Iter: 422 loss: 4.67649329e-07
Iter: 423 loss: 4.67805535e-07
Iter: 424 loss: 4.67613319e-07
Iter: 425 loss: 4.67498296e-07
Iter: 426 loss: 4.68146254e-07
Iter: 427 loss: 4.67461177e-07
Iter: 428 loss: 4.67382648e-07
Iter: 429 loss: 4.67412292e-07
Iter: 430 loss: 4.67353033e-07
Iter: 431 loss: 4.67205183e-07
Iter: 432 loss: 4.67040167e-07
Iter: 433 loss: 4.67057873e-07
Iter: 434 loss: 4.66772974e-07
Iter: 435 loss: 4.67534619e-07
Iter: 436 loss: 4.66682593e-07
Iter: 437 loss: 4.66460619e-07
Iter: 438 loss: 4.68755e-07
Iter: 439 loss: 4.66461643e-07
Iter: 440 loss: 4.66271359e-07
Iter: 441 loss: 4.6684778e-07
Iter: 442 loss: 4.66220285e-07
Iter: 443 loss: 4.66177653e-07
Iter: 444 loss: 4.66149743e-07
Iter: 445 loss: 4.66103415e-07
Iter: 446 loss: 4.65982851e-07
Iter: 447 loss: 4.67101557e-07
Iter: 448 loss: 4.6598646e-07
Iter: 449 loss: 4.65847734e-07
Iter: 450 loss: 4.67291898e-07
Iter: 451 loss: 4.65888718e-07
Iter: 452 loss: 4.65771365e-07
Iter: 453 loss: 4.65924472e-07
Iter: 454 loss: 4.65721882e-07
Iter: 455 loss: 4.6563639e-07
Iter: 456 loss: 4.65779351e-07
Iter: 457 loss: 4.65605694e-07
Iter: 458 loss: 4.65537397e-07
Iter: 459 loss: 4.66362337e-07
Iter: 460 loss: 4.65532395e-07
Iter: 461 loss: 4.65471715e-07
Iter: 462 loss: 4.65304282e-07
Iter: 463 loss: 4.6611143e-07
Iter: 464 loss: 4.65241641e-07
Iter: 465 loss: 4.65141397e-07
Iter: 466 loss: 4.651545e-07
Iter: 467 loss: 4.65065682e-07
Iter: 468 loss: 4.65462676e-07
Iter: 469 loss: 4.65050704e-07
Iter: 470 loss: 4.64925449e-07
Iter: 471 loss: 4.64828304e-07
Iter: 472 loss: 4.64802781e-07
Iter: 473 loss: 4.64629181e-07
Iter: 474 loss: 4.64708023e-07
Iter: 475 loss: 4.64545451e-07
Iter: 476 loss: 4.64424943e-07
Iter: 477 loss: 4.65283904e-07
Iter: 478 loss: 4.64437193e-07
Iter: 479 loss: 4.64291162e-07
Iter: 480 loss: 4.64295312e-07
Iter: 481 loss: 4.64262286e-07
Iter: 482 loss: 4.64123019e-07
Iter: 483 loss: 4.64121825e-07
Iter: 484 loss: 4.64046622e-07
Iter: 485 loss: 4.64617727e-07
Iter: 486 loss: 4.64022293e-07
Iter: 487 loss: 4.63934185e-07
Iter: 488 loss: 4.6429642e-07
Iter: 489 loss: 4.63905423e-07
Iter: 490 loss: 4.63870464e-07
Iter: 491 loss: 4.63801427e-07
Iter: 492 loss: 4.63777724e-07
Iter: 493 loss: 4.63693965e-07
Iter: 494 loss: 4.64430315e-07
Iter: 495 loss: 4.63662161e-07
Iter: 496 loss: 4.63591164e-07
Iter: 497 loss: 4.63672905e-07
Iter: 498 loss: 4.63580392e-07
Iter: 499 loss: 4.63487709e-07
Iter: 500 loss: 4.63381554e-07
Iter: 501 loss: 4.63341e-07
Iter: 502 loss: 4.63267895e-07
Iter: 503 loss: 4.6323018e-07
Iter: 504 loss: 4.63200536e-07
Iter: 505 loss: 4.63138861e-07
Iter: 506 loss: 4.63081847e-07
Iter: 507 loss: 4.62980893e-07
Iter: 508 loss: 4.63109359e-07
Iter: 509 loss: 4.62910236e-07
Iter: 510 loss: 4.62803655e-07
Iter: 511 loss: 4.62994194e-07
Iter: 512 loss: 4.627625e-07
Iter: 513 loss: 4.62752894e-07
Iter: 514 loss: 4.62710716e-07
Iter: 515 loss: 4.62653219e-07
Iter: 516 loss: 4.62566391e-07
Iter: 517 loss: 4.63564902e-07
Iter: 518 loss: 4.62537912e-07
Iter: 519 loss: 4.62489709e-07
Iter: 520 loss: 4.62635597e-07
Iter: 521 loss: 4.62463248e-07
Iter: 522 loss: 4.62357917e-07
Iter: 523 loss: 4.62499059e-07
Iter: 524 loss: 4.62299454e-07
Iter: 525 loss: 4.62215297e-07
Iter: 526 loss: 4.62222602e-07
Iter: 527 loss: 4.62172181e-07
Iter: 528 loss: 4.62037036e-07
Iter: 529 loss: 4.63531023e-07
Iter: 530 loss: 4.62045932e-07
Iter: 531 loss: 4.61889329e-07
Iter: 532 loss: 4.6258009e-07
Iter: 533 loss: 4.61860026e-07
Iter: 534 loss: 4.61752052e-07
Iter: 535 loss: 4.63271931e-07
Iter: 536 loss: 4.61726813e-07
Iter: 537 loss: 4.61665138e-07
Iter: 538 loss: 4.61680372e-07
Iter: 539 loss: 4.61605794e-07
Iter: 540 loss: 4.61513878e-07
Iter: 541 loss: 4.62107522e-07
Iter: 542 loss: 4.61486934e-07
Iter: 543 loss: 4.61360912e-07
Iter: 544 loss: 4.61290028e-07
Iter: 545 loss: 4.61284174e-07
Iter: 546 loss: 4.61166621e-07
Iter: 547 loss: 4.61282553e-07
Iter: 548 loss: 4.61095851e-07
Iter: 549 loss: 4.61038951e-07
Iter: 550 loss: 4.61028094e-07
Iter: 551 loss: 4.60961473e-07
Iter: 552 loss: 4.61034602e-07
Iter: 553 loss: 4.60913213e-07
Iter: 554 loss: 4.60848582e-07
Iter: 555 loss: 4.60720571e-07
Iter: 556 loss: 4.60700818e-07
Iter: 557 loss: 4.60623255e-07
Iter: 558 loss: 4.61312624e-07
Iter: 559 loss: 4.60604326e-07
Iter: 560 loss: 4.60517811e-07
Iter: 561 loss: 4.61280734e-07
Iter: 562 loss: 4.60526365e-07
Iter: 563 loss: 4.60464406e-07
Iter: 564 loss: 4.60386445e-07
Iter: 565 loss: 4.60386445e-07
Iter: 566 loss: 4.60278386e-07
Iter: 567 loss: 4.60340857e-07
Iter: 568 loss: 4.60255279e-07
Iter: 569 loss: 4.60148101e-07
Iter: 570 loss: 4.60148783e-07
Iter: 571 loss: 4.60113654e-07
Iter: 572 loss: 4.59979816e-07
Iter: 573 loss: 4.62070489e-07
Iter: 574 loss: 4.59963815e-07
Iter: 575 loss: 4.59855926e-07
Iter: 576 loss: 4.60768661e-07
Iter: 577 loss: 4.59882045e-07
Iter: 578 loss: 4.59763442e-07
Iter: 579 loss: 4.60469181e-07
Iter: 580 loss: 4.59751249e-07
Iter: 581 loss: 4.59692018e-07
Iter: 582 loss: 4.59569605e-07
Iter: 583 loss: 4.59578274e-07
Iter: 584 loss: 4.59415162e-07
Iter: 585 loss: 4.60161289e-07
Iter: 586 loss: 4.59442475e-07
Iter: 587 loss: 4.59267028e-07
Iter: 588 loss: 4.60531226e-07
Iter: 589 loss: 4.59287889e-07
Iter: 590 loss: 4.59238862e-07
Iter: 591 loss: 4.59091268e-07
Iter: 592 loss: 4.60206905e-07
Iter: 593 loss: 4.5909448e-07
Iter: 594 loss: 4.58924262e-07
Iter: 595 loss: 4.61160027e-07
Iter: 596 loss: 4.58924262e-07
Iter: 597 loss: 4.58838969e-07
Iter: 598 loss: 4.59510062e-07
Iter: 599 loss: 4.5882831e-07
Iter: 600 loss: 4.58772462e-07
Iter: 601 loss: 4.58615659e-07
Iter: 602 loss: 4.60144463e-07
Iter: 603 loss: 4.58606621e-07
Iter: 604 loss: 4.58446777e-07
Iter: 605 loss: 4.5973232e-07
Iter: 606 loss: 4.58429582e-07
Iter: 607 loss: 4.58332124e-07
Iter: 608 loss: 4.59549597e-07
Iter: 609 loss: 4.58325445e-07
Iter: 610 loss: 4.5826738e-07
Iter: 611 loss: 4.58248195e-07
Iter: 612 loss: 4.58227248e-07
Iter: 613 loss: 4.5811754e-07
Iter: 614 loss: 4.5795548e-07
Iter: 615 loss: 4.57990495e-07
Iter: 616 loss: 4.57920066e-07
Iter: 617 loss: 4.57884937e-07
Iter: 618 loss: 4.57787934e-07
Iter: 619 loss: 4.57731176e-07
Iter: 620 loss: 4.57723502e-07
Iter: 621 loss: 4.57630506e-07
Iter: 622 loss: 4.58641239e-07
Iter: 623 loss: 4.57610497e-07
Iter: 624 loss: 4.57556524e-07
Iter: 625 loss: 4.57735041e-07
Iter: 626 loss: 4.57546349e-07
Iter: 627 loss: 4.57447072e-07
Iter: 628 loss: 4.57334693e-07
Iter: 629 loss: 4.60073096e-07
Iter: 630 loss: 4.5733924e-07
Iter: 631 loss: 4.57325e-07
Iter: 632 loss: 4.57276968e-07
Iter: 633 loss: 4.57242606e-07
Iter: 634 loss: 4.57269493e-07
Iter: 635 loss: 4.57168511e-07
Iter: 636 loss: 4.57126134e-07
Iter: 637 loss: 4.57025664e-07
Iter: 638 loss: 4.57015062e-07
Iter: 639 loss: 4.56881253e-07
Iter: 640 loss: 4.57422772e-07
Iter: 641 loss: 4.56858032e-07
Iter: 642 loss: 4.56699e-07
Iter: 643 loss: 4.57515398e-07
Iter: 644 loss: 4.56698359e-07
Iter: 645 loss: 4.56575179e-07
Iter: 646 loss: 4.56328166e-07
Iter: 647 loss: 4.56345589e-07
Iter: 648 loss: 4.56110456e-07
Iter: 649 loss: 4.57983901e-07
Iter: 650 loss: 4.56081608e-07
Iter: 651 loss: 4.55957149e-07
Iter: 652 loss: 4.56925704e-07
Iter: 653 loss: 4.55926482e-07
Iter: 654 loss: 4.55775137e-07
Iter: 655 loss: 4.56041846e-07
Iter: 656 loss: 4.55702946e-07
Iter: 657 loss: 4.55608699e-07
Iter: 658 loss: 4.56662178e-07
Iter: 659 loss: 4.55595398e-07
Iter: 660 loss: 4.55531818e-07
Iter: 661 loss: 4.5561e-07
Iter: 662 loss: 4.55487395e-07
Iter: 663 loss: 4.55412618e-07
Iter: 664 loss: 4.55429415e-07
Iter: 665 loss: 4.55364727e-07
Iter: 666 loss: 4.55286397e-07
Iter: 667 loss: 4.5534145e-07
Iter: 668 loss: 4.55256583e-07
Iter: 669 loss: 4.55233874e-07
Iter: 670 loss: 4.55203974e-07
Iter: 671 loss: 4.5515722e-07
Iter: 672 loss: 4.55060444e-07
Iter: 673 loss: 4.55902182e-07
Iter: 674 loss: 4.55055925e-07
Iter: 675 loss: 4.54959235e-07
Iter: 676 loss: 4.55066981e-07
Iter: 677 loss: 4.54887186e-07
Iter: 678 loss: 4.54779126e-07
Iter: 679 loss: 4.54785493e-07
Iter: 680 loss: 4.54686102e-07
Iter: 681 loss: 4.54728053e-07
Iter: 682 loss: 4.54639633e-07
Iter: 683 loss: 4.54551355e-07
Iter: 684 loss: 4.54449577e-07
Iter: 685 loss: 4.54460974e-07
Iter: 686 loss: 4.54348623e-07
Iter: 687 loss: 4.54341603e-07
Iter: 688 loss: 4.54235163e-07
Iter: 689 loss: 4.54302722e-07
Iter: 690 loss: 4.54195799e-07
Iter: 691 loss: 4.54107379e-07
Iter: 692 loss: 4.54292035e-07
Iter: 693 loss: 4.54038059e-07
Iter: 694 loss: 4.53969477e-07
Iter: 695 loss: 4.55171573e-07
Iter: 696 loss: 4.53968141e-07
Iter: 697 loss: 4.53886827e-07
Iter: 698 loss: 4.53716382e-07
Iter: 699 loss: 4.55694959e-07
Iter: 700 loss: 4.53687306e-07
Iter: 701 loss: 4.53585244e-07
Iter: 702 loss: 4.53877249e-07
Iter: 703 loss: 4.53478833e-07
Iter: 704 loss: 4.53307166e-07
Iter: 705 loss: 4.54199181e-07
Iter: 706 loss: 4.53303414e-07
Iter: 707 loss: 4.53201181e-07
Iter: 708 loss: 4.54806468e-07
Iter: 709 loss: 4.53220309e-07
Iter: 710 loss: 4.53097186e-07
Iter: 711 loss: 4.52987734e-07
Iter: 712 loss: 4.52989411e-07
Iter: 713 loss: 4.52892493e-07
Iter: 714 loss: 4.5312072e-07
Iter: 715 loss: 4.5284736e-07
Iter: 716 loss: 4.52789834e-07
Iter: 717 loss: 4.53908399e-07
Iter: 718 loss: 4.52784462e-07
Iter: 719 loss: 4.52720968e-07
Iter: 720 loss: 4.52672197e-07
Iter: 721 loss: 4.5266529e-07
Iter: 722 loss: 4.52572294e-07
Iter: 723 loss: 4.52903237e-07
Iter: 724 loss: 4.52597675e-07
Iter: 725 loss: 4.52518634e-07
Iter: 726 loss: 4.52800691e-07
Iter: 727 loss: 4.52518776e-07
Iter: 728 loss: 4.52438826e-07
Iter: 729 loss: 4.52424217e-07
Iter: 730 loss: 4.52369562e-07
Iter: 731 loss: 4.52325594e-07
Iter: 732 loss: 4.53181428e-07
Iter: 733 loss: 4.52313884e-07
Iter: 734 loss: 4.52268409e-07
Iter: 735 loss: 4.52162794e-07
Iter: 736 loss: 4.535122e-07
Iter: 737 loss: 4.52146168e-07
Iter: 738 loss: 4.52002212e-07
Iter: 739 loss: 4.52456447e-07
Iter: 740 loss: 4.51959636e-07
Iter: 741 loss: 4.51843221e-07
Iter: 742 loss: 4.51969868e-07
Iter: 743 loss: 4.51766596e-07
Iter: 744 loss: 4.51674708e-07
Iter: 745 loss: 4.51648589e-07
Iter: 746 loss: 4.51606383e-07
Iter: 747 loss: 4.51515632e-07
Iter: 748 loss: 4.51502871e-07
Iter: 749 loss: 4.51379265e-07
Iter: 750 loss: 4.51700345e-07
Iter: 751 loss: 4.51331118e-07
Iter: 752 loss: 4.51198076e-07
Iter: 753 loss: 4.52064626e-07
Iter: 754 loss: 4.51218739e-07
Iter: 755 loss: 4.51121423e-07
Iter: 756 loss: 4.5110454e-07
Iter: 757 loss: 4.51054206e-07
Iter: 758 loss: 4.50975108e-07
Iter: 759 loss: 4.52139943e-07
Iter: 760 loss: 4.50983123e-07
Iter: 761 loss: 4.50882681e-07
Iter: 762 loss: 4.5090934e-07
Iter: 763 loss: 4.50856476e-07
Iter: 764 loss: 4.50792271e-07
Iter: 765 loss: 4.51247729e-07
Iter: 766 loss: 4.5075106e-07
Iter: 767 loss: 4.50687935e-07
Iter: 768 loss: 4.50854287e-07
Iter: 769 loss: 4.50656444e-07
Iter: 770 loss: 4.50593689e-07
Iter: 771 loss: 4.50479376e-07
Iter: 772 loss: 4.52601256e-07
Iter: 773 loss: 4.50467e-07
Iter: 774 loss: 4.5038388e-07
Iter: 775 loss: 4.50923352e-07
Iter: 776 loss: 4.50378707e-07
Iter: 777 loss: 4.5029725e-07
Iter: 778 loss: 4.50738128e-07
Iter: 779 loss: 4.50273774e-07
Iter: 780 loss: 4.5021585e-07
Iter: 781 loss: 4.50860227e-07
Iter: 782 loss: 4.50234182e-07
Iter: 783 loss: 4.501793e-07
Iter: 784 loss: 4.50050436e-07
Iter: 785 loss: 4.51212486e-07
Iter: 786 loss: 4.50028068e-07
Iter: 787 loss: 4.49877064e-07
Iter: 788 loss: 4.51682382e-07
Iter: 789 loss: 4.49859954e-07
Iter: 790 loss: 4.4979879e-07
Iter: 791 loss: 4.49944224e-07
Iter: 792 loss: 4.49745897e-07
Iter: 793 loss: 4.49662906e-07
Iter: 794 loss: 4.49633916e-07
Iter: 795 loss: 4.49587702e-07
Iter: 796 loss: 4.49434708e-07
Iter: 797 loss: 4.50993412e-07
Iter: 798 loss: 4.49432036e-07
Iter: 799 loss: 4.4937255e-07
Iter: 800 loss: 4.49352115e-07
Iter: 801 loss: 4.49267418e-07
Iter: 802 loss: 4.49180078e-07
Iter: 803 loss: 4.49178657e-07
Iter: 804 loss: 4.49126276e-07
Iter: 805 loss: 4.49094387e-07
Iter: 806 loss: 4.4904354e-07
Iter: 807 loss: 4.48999288e-07
Iter: 808 loss: 4.48902568e-07
Iter: 809 loss: 4.48889352e-07
Iter: 810 loss: 4.48805025e-07
Iter: 811 loss: 4.49843441e-07
Iter: 812 loss: 4.4876731e-07
Iter: 813 loss: 4.48745908e-07
Iter: 814 loss: 4.49269521e-07
Iter: 815 loss: 4.4871723e-07
Iter: 816 loss: 4.48654788e-07
Iter: 817 loss: 4.48685086e-07
Iter: 818 loss: 4.48585581e-07
Iter: 819 loss: 4.48497076e-07
Iter: 820 loss: 4.48587627e-07
Iter: 821 loss: 4.48472548e-07
Iter: 822 loss: 4.48365029e-07
Iter: 823 loss: 4.49008041e-07
Iter: 824 loss: 4.48387254e-07
Iter: 825 loss: 4.48300113e-07
Iter: 826 loss: 4.48314665e-07
Iter: 827 loss: 4.482566e-07
Iter: 828 loss: 4.48150331e-07
Iter: 829 loss: 4.48447196e-07
Iter: 830 loss: 4.48156868e-07
Iter: 831 loss: 4.4806248e-07
Iter: 832 loss: 4.48406468e-07
Iter: 833 loss: 4.48044489e-07
Iter: 834 loss: 4.48002055e-07
Iter: 835 loss: 4.48099087e-07
Iter: 836 loss: 4.47965164e-07
Iter: 837 loss: 4.47921167e-07
Iter: 838 loss: 4.48391802e-07
Iter: 839 loss: 4.4789445e-07
Iter: 840 loss: 4.47856962e-07
Iter: 841 loss: 4.47805263e-07
Iter: 842 loss: 4.47796765e-07
Iter: 843 loss: 4.47732674e-07
Iter: 844 loss: 4.47721675e-07
Iter: 845 loss: 4.477034e-07
Iter: 846 loss: 4.47613871e-07
Iter: 847 loss: 4.48304775e-07
Iter: 848 loss: 4.47587979e-07
Iter: 849 loss: 4.47534433e-07
Iter: 850 loss: 4.47784828e-07
Iter: 851 loss: 4.47519113e-07
Iter: 852 loss: 4.47424185e-07
Iter: 853 loss: 4.47349294e-07
Iter: 854 loss: 4.4732397e-07
Iter: 855 loss: 4.47231059e-07
Iter: 856 loss: 4.4849719e-07
Iter: 857 loss: 4.47204371e-07
Iter: 858 loss: 4.47104867e-07
Iter: 859 loss: 4.47325306e-07
Iter: 860 loss: 4.47082073e-07
Iter: 861 loss: 4.47057175e-07
Iter: 862 loss: 4.47213068e-07
Iter: 863 loss: 4.47028128e-07
Iter: 864 loss: 4.46950253e-07
Iter: 865 loss: 4.47066355e-07
Iter: 866 loss: 4.46915521e-07
Iter: 867 loss: 4.46825709e-07
Iter: 868 loss: 4.46908615e-07
Iter: 869 loss: 4.46771935e-07
Iter: 870 loss: 4.46704462e-07
Iter: 871 loss: 4.4672737e-07
Iter: 872 loss: 4.46651285e-07
Iter: 873 loss: 4.46581936e-07
Iter: 874 loss: 4.46590633e-07
Iter: 875 loss: 4.46546153e-07
Iter: 876 loss: 4.4664796e-07
Iter: 877 loss: 4.4650227e-07
Iter: 878 loss: 4.46402481e-07
Iter: 879 loss: 4.46635909e-07
Iter: 880 loss: 4.4638449e-07
Iter: 881 loss: 4.46334582e-07
Iter: 882 loss: 4.46333132e-07
Iter: 883 loss: 4.46312839e-07
Iter: 884 loss: 4.46298884e-07
Iter: 885 loss: 4.46261794e-07
Iter: 886 loss: 4.46219332e-07
Iter: 887 loss: 4.46234253e-07
Iter: 888 loss: 4.46164819e-07
Iter: 889 loss: 4.46113461e-07
Iter: 890 loss: 4.46987201e-07
Iter: 891 loss: 4.46094646e-07
Iter: 892 loss: 4.46051104e-07
Iter: 893 loss: 4.46029418e-07
Iter: 894 loss: 4.46025922e-07
Iter: 895 loss: 4.45969164e-07
Iter: 896 loss: 4.46640968e-07
Iter: 897 loss: 4.45974933e-07
Iter: 898 loss: 4.45895751e-07
Iter: 899 loss: 4.45863236e-07
Iter: 900 loss: 4.45863918e-07
Iter: 901 loss: 4.4578033e-07
Iter: 902 loss: 4.46070544e-07
Iter: 903 loss: 4.45801561e-07
Iter: 904 loss: 4.45735679e-07
Iter: 905 loss: 4.45745343e-07
Iter: 906 loss: 4.45723288e-07
Iter: 907 loss: 4.4559863e-07
Iter: 908 loss: 4.45677e-07
Iter: 909 loss: 4.45544174e-07
Iter: 910 loss: 4.45403145e-07
Iter: 911 loss: 4.45401383e-07
Iter: 912 loss: 4.45351219e-07
Iter: 913 loss: 4.45642058e-07
Iter: 914 loss: 4.4532e-07
Iter: 915 loss: 4.4525575e-07
Iter: 916 loss: 4.45399536e-07
Iter: 917 loss: 4.4520965e-07
Iter: 918 loss: 4.45164318e-07
Iter: 919 loss: 4.45111084e-07
Iter: 920 loss: 4.45083742e-07
Iter: 921 loss: 4.44974916e-07
Iter: 922 loss: 4.45621254e-07
Iter: 923 loss: 4.44957209e-07
Iter: 924 loss: 4.44870352e-07
Iter: 925 loss: 4.4561196e-07
Iter: 926 loss: 4.4486751e-07
Iter: 927 loss: 4.44808705e-07
Iter: 928 loss: 4.44766073e-07
Iter: 929 loss: 4.44778522e-07
Iter: 930 loss: 4.44652e-07
Iter: 931 loss: 4.45449558e-07
Iter: 932 loss: 4.44662135e-07
Iter: 933 loss: 4.44563511e-07
Iter: 934 loss: 4.4462621e-07
Iter: 935 loss: 4.44524431e-07
Iter: 936 loss: 4.44452525e-07
Iter: 937 loss: 4.44454031e-07
Iter: 938 loss: 4.44402417e-07
Iter: 939 loss: 4.44414042e-07
Iter: 940 loss: 4.44387e-07
Iter: 941 loss: 4.44330169e-07
Iter: 942 loss: 4.4425127e-07
Iter: 943 loss: 4.44250418e-07
Iter: 944 loss: 4.44206279e-07
Iter: 945 loss: 4.44197042e-07
Iter: 946 loss: 4.44158e-07
Iter: 947 loss: 4.44279266e-07
Iter: 948 loss: 4.44156655e-07
Iter: 949 loss: 4.44107485e-07
Iter: 950 loss: 4.44025432e-07
Iter: 951 loss: 4.44004456e-07
Iter: 952 loss: 4.43952729e-07
Iter: 953 loss: 4.44131e-07
Iter: 954 loss: 4.43905463e-07
Iter: 955 loss: 4.43837877e-07
Iter: 956 loss: 4.4456425e-07
Iter: 957 loss: 4.43807266e-07
Iter: 958 loss: 4.43745705e-07
Iter: 959 loss: 4.43754743e-07
Iter: 960 loss: 4.43626305e-07
Iter: 961 loss: 4.43554171e-07
Iter: 962 loss: 4.43824774e-07
Iter: 963 loss: 4.43528677e-07
Iter: 964 loss: 4.43434089e-07
Iter: 965 loss: 4.44048339e-07
Iter: 966 loss: 4.43412063e-07
Iter: 967 loss: 4.43351666e-07
Iter: 968 loss: 4.43364314e-07
Iter: 969 loss: 4.43290332e-07
Iter: 970 loss: 4.4322087e-07
Iter: 971 loss: 4.44004314e-07
Iter: 972 loss: 4.43186877e-07
Iter: 973 loss: 4.43115368e-07
Iter: 974 loss: 4.43070348e-07
Iter: 975 loss: 4.430334e-07
Iter: 976 loss: 4.4295038e-07
Iter: 977 loss: 4.4326768e-07
Iter: 978 loss: 4.42963085e-07
Iter: 979 loss: 4.42838171e-07
Iter: 980 loss: 4.43605728e-07
Iter: 981 loss: 4.42841326e-07
Iter: 982 loss: 4.42816201e-07
Iter: 983 loss: 4.42840104e-07
Iter: 984 loss: 4.42779907e-07
Iter: 985 loss: 4.42726e-07
Iter: 986 loss: 4.4267216e-07
Iter: 987 loss: 4.4264732e-07
Iter: 988 loss: 4.42609945e-07
Iter: 989 loss: 4.42947396e-07
Iter: 990 loss: 4.42568677e-07
Iter: 991 loss: 4.42516352e-07
Iter: 992 loss: 4.43263104e-07
Iter: 993 loss: 4.42533462e-07
Iter: 994 loss: 4.42477699e-07
Iter: 995 loss: 4.42489181e-07
Iter: 996 loss: 4.42432707e-07
Iter: 997 loss: 4.42416365e-07
Iter: 998 loss: 4.42573395e-07
Iter: 999 loss: 4.42394253e-07
Iter: 1000 loss: 4.42345538e-07
Iter: 1001 loss: 4.4276851e-07
Iter: 1002 loss: 4.42333516e-07
Iter: 1003 loss: 4.42295629e-07
Iter: 1004 loss: 4.42330077e-07
Iter: 1005 loss: 4.42264962e-07
Iter: 1006 loss: 4.42222927e-07
Iter: 1007 loss: 4.4224214e-07
Iter: 1008 loss: 4.4217947e-07
Iter: 1009 loss: 4.42112878e-07
Iter: 1010 loss: 4.42070643e-07
Iter: 1011 loss: 4.42050123e-07
Iter: 1012 loss: 4.41941324e-07
Iter: 1013 loss: 4.4275393e-07
Iter: 1014 loss: 4.41937971e-07
Iter: 1015 loss: 4.41859584e-07
Iter: 1016 loss: 4.42196836e-07
Iter: 1017 loss: 4.41900397e-07
Iter: 1018 loss: 4.41803365e-07
Iter: 1019 loss: 4.41766588e-07
Iter: 1020 loss: 4.4176e-07
Iter: 1021 loss: 4.41649291e-07
Iter: 1022 loss: 4.41712245e-07
Iter: 1023 loss: 4.41576361e-07
Iter: 1024 loss: 4.4150795e-07
Iter: 1025 loss: 4.42153919e-07
Iter: 1026 loss: 4.41511787e-07
Iter: 1027 loss: 4.41399123e-07
Iter: 1028 loss: 4.41869247e-07
Iter: 1029 loss: 4.41373913e-07
Iter: 1030 loss: 4.41349044e-07
Iter: 1031 loss: 4.41356832e-07
Iter: 1032 loss: 4.41310647e-07
Iter: 1033 loss: 4.4125747e-07
Iter: 1034 loss: 4.41825478e-07
Iter: 1035 loss: 4.41259942e-07
Iter: 1036 loss: 4.41222227e-07
Iter: 1037 loss: 4.41266252e-07
Iter: 1038 loss: 4.41159813e-07
Iter: 1039 loss: 4.41127753e-07
Iter: 1040 loss: 4.41421889e-07
Iter: 1041 loss: 4.41146881e-07
Iter: 1042 loss: 4.41099274e-07
Iter: 1043 loss: 4.41041806e-07
Iter: 1044 loss: 4.42823762e-07
Iter: 1045 loss: 4.4104624e-07
Iter: 1046 loss: 4.40976436e-07
Iter: 1047 loss: 4.41543648e-07
Iter: 1048 loss: 4.40998036e-07
Iter: 1049 loss: 4.40966573e-07
Iter: 1050 loss: 4.41174961e-07
Iter: 1051 loss: 4.40927266e-07
Iter: 1052 loss: 4.40861697e-07
Iter: 1053 loss: 4.40827677e-07
Iter: 1054 loss: 4.40787886e-07
Iter: 1055 loss: 4.40761653e-07
Iter: 1056 loss: 4.408802e-07
Iter: 1057 loss: 4.40734055e-07
Iter: 1058 loss: 4.40640406e-07
Iter: 1059 loss: 4.40828757e-07
Iter: 1060 loss: 4.40658027e-07
Iter: 1061 loss: 4.40588224e-07
Iter: 1062 loss: 4.41025918e-07
Iter: 1063 loss: 4.40560427e-07
Iter: 1064 loss: 4.40483802e-07
Iter: 1065 loss: 4.40474054e-07
Iter: 1066 loss: 4.40443131e-07
Iter: 1067 loss: 4.4035221e-07
Iter: 1068 loss: 4.40903108e-07
Iter: 1069 loss: 4.40360168e-07
Iter: 1070 loss: 4.40258958e-07
Iter: 1071 loss: 4.4065186e-07
Iter: 1072 loss: 4.40295366e-07
Iter: 1073 loss: 4.40223602e-07
Iter: 1074 loss: 4.40238409e-07
Iter: 1075 loss: 4.40163234e-07
Iter: 1076 loss: 4.40076377e-07
Iter: 1077 loss: 4.40367046e-07
Iter: 1078 loss: 4.40054492e-07
Iter: 1079 loss: 4.39995176e-07
Iter: 1080 loss: 4.39922388e-07
Iter: 1081 loss: 4.39924463e-07
Iter: 1082 loss: 4.39879358e-07
Iter: 1083 loss: 4.39859576e-07
Iter: 1084 loss: 4.39806968e-07
Iter: 1085 loss: 4.39842552e-07
Iter: 1086 loss: 4.39783463e-07
Iter: 1087 loss: 4.39708828e-07
Iter: 1088 loss: 4.39725795e-07
Iter: 1089 loss: 4.39690325e-07
Iter: 1090 loss: 4.39614723e-07
Iter: 1091 loss: 4.39965731e-07
Iter: 1092 loss: 4.39604889e-07
Iter: 1093 loss: 4.3956544e-07
Iter: 1094 loss: 4.39562882e-07
Iter: 1095 loss: 4.39530766e-07
Iter: 1096 loss: 4.39457779e-07
Iter: 1097 loss: 4.39443284e-07
Iter: 1098 loss: 4.39475372e-07
Iter: 1099 loss: 4.39455846e-07
Iter: 1100 loss: 4.39434132e-07
Iter: 1101 loss: 4.39436491e-07
Iter: 1102 loss: 4.3943129e-07
Iter: 1103 loss: 4.39425662e-07
Iter: 1104 loss: 4.39432171e-07
Iter: 1105 loss: 4.39463435e-07
Iter: 1106 loss: 4.39449792e-07
Iter: 1107 loss: 4.39448854e-07
Iter: 1108 loss: 4.39450105e-07
Iter: 1109 loss: 4.39450389e-07
Iter: 1110 loss: 4.39443966e-07
Iter: 1111 loss: 4.39437429e-07
Iter: 1112 loss: 4.39437656e-07
Iter: 1113 loss: 4.39438e-07
Iter: 1114 loss: 4.39437e-07
Iter: 1115 loss: 4.39440157e-07
Iter: 1116 loss: 4.39443596e-07
Iter: 1117 loss: 4.3944425e-07
Iter: 1118 loss: 4.39443681e-07
Iter: 1119 loss: 4.39443681e-07
Iter: 1120 loss: 4.39443625e-07
Iter: 1121 loss: 4.39443482e-07
Iter: 1122 loss: 4.39443511e-07
Iter: 1123 loss: 4.3944425e-07
Iter: 1124 loss: 4.39443511e-07
Iter: 1125 loss: 4.39382461e-07
Iter: 1126 loss: 4.40894e-07
Iter: 1127 loss: 4.393832e-07
Iter: 1128 loss: 4.39379789e-07
Iter: 1129 loss: 4.39319336e-07
Iter: 1130 loss: 4.39286794e-07
Iter: 1131 loss: 4.39244644e-07
Iter: 1132 loss: 4.40623296e-07
Iter: 1133 loss: 4.39215228e-07
Iter: 1134 loss: 4.39159578e-07
Iter: 1135 loss: 4.3998611e-07
Iter: 1136 loss: 4.39181946e-07
Iter: 1137 loss: 4.39125415e-07
Iter: 1138 loss: 4.39128087e-07
Iter: 1139 loss: 4.39094634e-07
Iter: 1140 loss: 4.39023324e-07
Iter: 1141 loss: 4.38941612e-07
Iter: 1142 loss: 4.41531256e-07
Iter: 1143 loss: 4.3893877e-07
Iter: 1144 loss: 4.38888492e-07
Iter: 1145 loss: 4.38864276e-07
Iter: 1146 loss: 4.38809764e-07
Iter: 1147 loss: 4.38796889e-07
Iter: 1148 loss: 4.38764545e-07
Iter: 1149 loss: 4.3868647e-07
Iter: 1150 loss: 4.38719894e-07
Iter: 1151 loss: 4.38604303e-07
Iter: 1152 loss: 4.38539018e-07
Iter: 1153 loss: 4.39058454e-07
Iter: 1154 loss: 4.38508408e-07
Iter: 1155 loss: 4.38465577e-07
Iter: 1156 loss: 4.38997e-07
Iter: 1157 loss: 4.3844932e-07
Iter: 1158 loss: 4.38387815e-07
Iter: 1159 loss: 4.38463303e-07
Iter: 1160 loss: 4.38397251e-07
Iter: 1161 loss: 4.38321706e-07
Iter: 1162 loss: 4.38322957e-07
Iter: 1163 loss: 4.3829229e-07
Iter: 1164 loss: 4.38291778e-07
Iter: 1165 loss: 4.38257928e-07
Iter: 1166 loss: 4.38244399e-07
Iter: 1167 loss: 4.38168087e-07
Iter: 1168 loss: 4.39010336e-07
Iter: 1169 loss: 4.38176869e-07
Iter: 1170 loss: 4.38177e-07
Iter: 1171 loss: 4.38407028e-07
Iter: 1172 loss: 4.38178461e-07
Iter: 1173 loss: 4.38150806e-07
Iter: 1174 loss: 4.38154956e-07
Iter: 1175 loss: 4.38162061e-07
Iter: 1176 loss: 4.38168456e-07
Iter: 1177 loss: 4.38169849e-07
Iter: 1178 loss: 4.38174595e-07
Iter: 1179 loss: 4.38163681e-07
Iter: 1180 loss: 4.38169849e-07
Iter: 1181 loss: 4.38175618e-07
Iter: 1182 loss: 4.38167575e-07
Iter: 1183 loss: 4.38168456e-07
Iter: 1184 loss: 4.38176301e-07
Iter: 1185 loss: 4.38177494e-07
Iter: 1186 loss: 4.3817812e-07
Iter: 1187 loss: 4.38179285e-07
Iter: 1188 loss: 4.38177892e-07
Iter: 1189 loss: 4.38177864e-07
Iter: 1190 loss: 4.3817829e-07
Iter: 1191 loss: 4.38178773e-07
Iter: 1192 loss: 4.38178631e-07
Iter: 1193 loss: 4.38176357e-07
Iter: 1194 loss: 4.38176357e-07
Iter: 1195 loss: 4.38178631e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6
+ date
Thu Oct 22 03:41:52 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/500_500_500_500_1 --function f1 --psi 0 --phi 1.6 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c34659a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c56b38840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c345589d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c3462e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c3462e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c3459f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c34558400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c0c0341e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c0c034268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c0c034620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c0c067268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bf01709d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3c0c067158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bf01bc0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bf0193400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bf01bc048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bf00fcae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bf00c6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bf00c6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bf0014400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bf00a5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bb05b10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bf00a5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bb05539d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bb0553d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bb0553e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bf013a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bb04d09d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bb04d0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bb05897b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bb0580268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bb03ebf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bb03eb730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bb04ffd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bb04590d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3bb032ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.022219324
test_loss: 0.019989239
train_loss: 0.0071121785
test_loss: 0.0065184697
train_loss: 0.002934982
test_loss: 0.0027800275
train_loss: 0.0021509863
test_loss: 0.0024151944
train_loss: 0.0019989044
test_loss: 0.0024378952
train_loss: 0.001824889
test_loss: 0.0019086609
train_loss: 0.0020088158
test_loss: 0.0019776004
train_loss: 0.0017800767
test_loss: 0.0017610698
train_loss: 0.001856123
test_loss: 0.002266751
train_loss: 0.0022688545
test_loss: 0.0017050165
train_loss: 0.0019055279
test_loss: 0.0018537459
train_loss: 0.0016926761
test_loss: 0.0017692938
train_loss: 0.0016036787
test_loss: 0.001491329
train_loss: 0.0016071829
test_loss: 0.0014147211
train_loss: 0.0015064294
test_loss: 0.001395014
train_loss: 0.001503515
test_loss: 0.0017194478
train_loss: 0.0012836548
test_loss: 0.0015132073
train_loss: 0.0014060154
test_loss: 0.0015988552
train_loss: 0.001475275
test_loss: 0.0015968085
train_loss: 0.001483354
test_loss: 0.0015204875
train_loss: 0.0014051597
test_loss: 0.001497847
train_loss: 0.0014671992
test_loss: 0.001387789
train_loss: 0.0017945865
test_loss: 0.0023886107
train_loss: 0.0018035015
test_loss: 0.0014806977
train_loss: 0.0018550111
test_loss: 0.0014702515
train_loss: 0.0014233921
test_loss: 0.0016449001
train_loss: 0.0014182238
test_loss: 0.0015172411
train_loss: 0.0016691454
test_loss: 0.0015081498
train_loss: 0.0016927255
test_loss: 0.0015333152
train_loss: 0.0014542046
test_loss: 0.0016123113
train_loss: 0.0015346645
test_loss: 0.0015582091
train_loss: 0.001587627
test_loss: 0.0025626277
train_loss: 0.0021999516
test_loss: 0.002198288
train_loss: 0.0018090844
test_loss: 0.0015944833
train_loss: 0.0016553109
test_loss: 0.0016242551
train_loss: 0.0018499646
test_loss: 0.0014822254
train_loss: 0.0016486506
test_loss: 0.0013818631
train_loss: 0.0015704752
test_loss: 0.0018085467
train_loss: 0.0012472599
test_loss: 0.0013294931
train_loss: 0.0012356503
test_loss: 0.0013704334
train_loss: 0.001336368
test_loss: 0.0012364554
train_loss: 0.0012190997
test_loss: 0.0014632475
train_loss: 0.0015051726
test_loss: 0.001367672
train_loss: 0.0015090852
test_loss: 0.0012438924
train_loss: 0.0015751314
test_loss: 0.0012455814
train_loss: 0.0013139594
test_loss: 0.0012573885
train_loss: 0.0013928704
test_loss: 0.001869228
train_loss: 0.0014737833
test_loss: 0.0012321363
train_loss: 0.0013820399
test_loss: 0.0011465335
train_loss: 0.0014843876
test_loss: 0.0012648582
train_loss: 0.0013213917
test_loss: 0.0016488543
train_loss: 0.0013839573
test_loss: 0.0014322707
train_loss: 0.0013127154
test_loss: 0.0015927041
train_loss: 0.0013771695
test_loss: 0.0016227945
train_loss: 0.0014066859
test_loss: 0.0014783307
train_loss: 0.0015121922
test_loss: 0.0020242138
train_loss: 0.0016239353
test_loss: 0.0014651563
train_loss: 0.0015363278
test_loss: 0.0018575779
train_loss: 0.0016383138
test_loss: 0.0014527458
train_loss: 0.0013507402
test_loss: 0.0017702897
train_loss: 0.001494251
test_loss: 0.0016361005
train_loss: 0.001413616
test_loss: 0.0016632198
train_loss: 0.0013214493
test_loss: 0.0016854997
train_loss: 0.0014827715
test_loss: 0.0018905074
train_loss: 0.001399307
test_loss: 0.0013778512
train_loss: 0.0013177535
test_loss: 0.0016620245
train_loss: 0.0014314522
test_loss: 0.0019488111
train_loss: 0.0015292936
test_loss: 0.0016368778
train_loss: 0.0011820385
test_loss: 0.001239021
train_loss: 0.0013164117
test_loss: 0.0013174294
train_loss: 0.0011620618
test_loss: 0.0015416477
train_loss: 0.0014805405
test_loss: 0.0015161092
train_loss: 0.0014002977
test_loss: 0.0013808883
train_loss: 0.0014628915
test_loss: 0.00134917
train_loss: 0.0013205623
test_loss: 0.0017145716
train_loss: 0.0016836779
test_loss: 0.00170649
train_loss: 0.0012971282
test_loss: 0.0017078231
train_loss: 0.002064803
test_loss: 0.0014401849
train_loss: 0.001369109
test_loss: 0.001754388
train_loss: 0.0013940267
test_loss: 0.0014931583
train_loss: 0.0021038018
test_loss: 0.0013415868
train_loss: 0.0014897112
test_loss: 0.001642952
train_loss: 0.0013807965
test_loss: 0.0013909739
train_loss: 0.0014420614
test_loss: 0.0013429644
train_loss: 0.0013533346
test_loss: 0.0013376832
train_loss: 0.0014744576
test_loss: 0.0012143542
train_loss: 0.0013207764
test_loss: 0.0012954326
train_loss: 0.0013709255
test_loss: 0.0016525778
train_loss: 0.0013241341
test_loss: 0.0019609465
train_loss: 0.0015104744
test_loss: 0.0016304142
train_loss: 0.0013690882
test_loss: 0.0013592963
train_loss: 0.0012933789
test_loss: 0.0013941494
train_loss: 0.0014164294
test_loss: 0.0012921472
train_loss: 0.0014954952
test_loss: 0.0014521277
train_loss: 0.0012179372
test_loss: 0.0015750331
train_loss: 0.0014821729
test_loss: 0.0016864677
train_loss: 0.0013342062
test_loss: 0.0015133702
train_loss: 0.0012998293
test_loss: 0.0016460143
train_loss: 0.0016936419
test_loss: 0.001431916
train_loss: 0.0012736861
test_loss: 0.0012259707
train_loss: 0.0013081991
test_loss: 0.0012828481
train_loss: 0.0013427059
test_loss: 0.0018858978
train_loss: 0.0015993265
test_loss: 0.0014774896
train_loss: 0.0019621009
test_loss: 0.001862303
train_loss: 0.001888768
test_loss: 0.001419002
train_loss: 0.0015689591
test_loss: 0.0016147887
train_loss: 0.0020142857
test_loss: 0.0017540525
train_loss: 0.0013179934
test_loss: 0.001347837
train_loss: 0.0012973758
test_loss: 0.001645647
train_loss: 0.0012138037
test_loss: 0.0013521033
train_loss: 0.0012794726
test_loss: 0.0012281216
train_loss: 0.0015772475
test_loss: 0.0012797079
train_loss: 0.0013127568
test_loss: 0.0015257408
train_loss: 0.0014256721
test_loss: 0.0012680708
train_loss: 0.00134299
test_loss: 0.0014483312
train_loss: 0.0014295927
test_loss: 0.0014054008
train_loss: 0.0014085083
test_loss: 0.0017631794
train_loss: 0.0015632175
test_loss: 0.0012181118
train_loss: 0.0015136816
test_loss: 0.002169635
train_loss: 0.0018667105
test_loss: 0.0015223491
train_loss: 0.0013424745
test_loss: 0.0019023246
train_loss: 0.0015307197
test_loss: 0.0020200447
train_loss: 0.0015230586
test_loss: 0.0018474458
train_loss: 0.0014234427
test_loss: 0.0013911378
train_loss: 0.0012533674
test_loss: 0.0014065892
train_loss: 0.0013784125
test_loss: 0.0014543725
train_loss: 0.0014181945
test_loss: 0.0014252226
train_loss: 0.0015512079
test_loss: 0.0012630413
train_loss: 0.0011887215
test_loss: 0.0013207608
train_loss: 0.0013542022
test_loss: 0.0014990657
train_loss: 0.001328486
test_loss: 0.0012862807
train_loss: 0.0012837795
test_loss: 0.0014383595
train_loss: 0.0013405103
test_loss: 0.0016258415
train_loss: 0.0011511429
test_loss: 0.0014263181
train_loss: 0.0011311567
test_loss: 0.0013110662
train_loss: 0.0013880497
test_loss: 0.0018083318
train_loss: 0.0012850588
test_loss: 0.0015429805
train_loss: 0.0013704656
test_loss: 0.0017942822
train_loss: 0.0011993385
test_loss: 0.0012984594
train_loss: 0.0012231194
test_loss: 0.0016144953
train_loss: 0.0014842281
test_loss: 0.0014887931
train_loss: 0.0016751038
test_loss: 0.0013480823
train_loss: 0.0013418839
test_loss: 0.0016647208
train_loss: 0.0018647241
test_loss: 0.0017339562
train_loss: 0.0016037592
test_loss: 0.0014962334
train_loss: 0.001677826
test_loss: 0.0013548647
train_loss: 0.0012724743
test_loss: 0.0014431669
train_loss: 0.0014341185
test_loss: 0.0015361188
train_loss: 0.0011795548
test_loss: 0.0012618125
train_loss: 0.0014569062
test_loss: 0.0013742874
train_loss: 0.0017308046
test_loss: 0.0014782838
train_loss: 0.0015154211
test_loss: 0.0012272082
train_loss: 0.0015462374
test_loss: 0.0014645305
train_loss: 0.0014272105
test_loss: 0.0015169072
train_loss: 0.0011554376
test_loss: 0.0012948305
train_loss: 0.0013003738
test_loss: 0.0013395234
train_loss: 0.0014258522
test_loss: 0.0013028726
train_loss: 0.0015407728
test_loss: 0.0014593607
train_loss: 0.0013971553
test_loss: 0.0013999197
train_loss: 0.0014006805
test_loss: 0.0012724138
train_loss: 0.0014191315
test_loss: 0.0012480632
train_loss: 0.0014221136
test_loss: 0.001349592
train_loss: 0.001234509
test_loss: 0.0013608342
train_loss: 0.0013480112
test_loss: 0.0011702417
train_loss: 0.0014035897
test_loss: 0.0012465368
train_loss: 0.0012953328
test_loss: 0.0013637054
train_loss: 0.001535306
test_loss: 0.0015427629
train_loss: 0.0013065601
test_loss: 0.0012912791
train_loss: 0.0016325883
test_loss: 0.0018460099
train_loss: 0.0013006948
test_loss: 0.0016003242
train_loss: 0.002120341
test_loss: 0.0017124969
train_loss: 0.0018835164
test_loss: 0.0017299695
train_loss: 0.00143322
test_loss: 0.0014920412
train_loss: 0.0013882701
test_loss: 0.0012589132
train_loss: 0.0013466616
test_loss: 0.0016437122
train_loss: 0.0019564596
test_loss: 0.0017291679
train_loss: 0.0018611416
test_loss: 0.0014669661
train_loss: 0.0014972335
test_loss: 0.001984079
train_loss: 0.0018187903
test_loss: 0.0016954603
train_loss: 0.0012121662
test_loss: 0.0013144861
train_loss: 0.0016710935
test_loss: 0.0012212142
train_loss: 0.0017153721
test_loss: 0.0015844073
train_loss: 0.0022813755
test_loss: 0.0018551018
train_loss: 0.0021346086
test_loss: 0.0020773718
train_loss: 0.0017445103
test_loss: 0.0029090147
train_loss: 0.0024548513
test_loss: 0.0019913802
train_loss: 0.0015219841
test_loss: 0.0014937099
train_loss: 0.0011935048
test_loss: 0.0014189626
train_loss: 0.0011280044
test_loss: 0.0012198389
train_loss: 0.001244107
test_loss: 0.0012684116
train_loss: 0.0013166185
test_loss: 0.0015539732
train_loss: 0.0010999112
test_loss: 0.0012881889
train_loss: 0.0013843428
test_loss: 0.0013289441
train_loss: 0.0014440872
test_loss: 0.0012445356
train_loss: 0.001226134
test_loss: 0.0011731284
train_loss: 0.00152991
test_loss: 0.0013700876
train_loss: 0.0013023458
test_loss: 0.001625224
train_loss: 0.0014572686
test_loss: 0.001325596
train_loss: 0.0013910221
test_loss: 0.0015172804
train_loss: 0.0015376417
test_loss: 0.0016612042
train_loss: 0.001346228
test_loss: 0.0014556586
train_loss: 0.0011784543
test_loss: 0.0011276889
train_loss: 0.0013552845
test_loss: 0.0012726941
train_loss: 0.0013779115
test_loss: 0.0015330787
train_loss: 0.0014137654
test_loss: 0.0014983914
train_loss: 0.0013943007
test_loss: 0.0012088242
train_loss: 0.00135774
test_loss: 0.0014476136
train_loss: 0.0014782519
test_loss: 0.0012509669
train_loss: 0.0014743626
test_loss: 0.0012927607
train_loss: 0.0016050742
test_loss: 0.0012460941
train_loss: 0.0013293744
test_loss: 0.0012715181
train_loss: 0.0017010444
test_loss: 0.0013523896
train_loss: 0.0013335877
test_loss: 0.0014394893
train_loss: 0.0014337974
test_loss: 0.0013937741
train_loss: 0.0013853528
test_loss: 0.0012899577
train_loss: 0.0012470004
test_loss: 0.0012253921
train_loss: 0.0012423616
test_loss: 0.0013655926
train_loss: 0.001531014
test_loss: 0.0012446085
train_loss: 0.0015256036
test_loss: 0.0014242989
train_loss: 0.0014747912
test_loss: 0.0013175482
train_loss: 0.0013810932
test_loss: 0.001338143
train_loss: 0.0013200343
test_loss: 0.0011976728
train_loss: 0.0012622889
test_loss: 0.0013017429
train_loss: 0.0015976796
test_loss: 0.0013699301
train_loss: 0.0014538973
test_loss: 0.0012989038
train_loss: 0.001322682
test_loss: 0.0012302236
train_loss: 0.0016074853
test_loss: 0.0014265728
train_loss: 0.0012534689
test_loss: 0.0011735191
train_loss: 0.00122292
test_loss: 0.0013806638
train_loss: 0.0013920921
test_loss: 0.0014731175
train_loss: 0.0013683366
test_loss: 0.0011834309
train_loss: 0.0012370766
test_loss: 0.0015941847
train_loss: 0.0013142851
test_loss: 0.001240528
train_loss: 0.0013751297
test_loss: 0.0014494319
train_loss: 0.0014176161
test_loss: 0.0011924536
train_loss: 0.0010720424
test_loss: 0.0012895662
train_loss: 0.0011428508
test_loss: 0.0016096309
train_loss: 0.0013864287
test_loss: 0.0014335184
train_loss: 0.0013412603
test_loss: 0.0013217045
train_loss: 0.0014652053
test_loss: 0.0013851164
train_loss: 0.0012466628
test_loss: 0.0012635811
train_loss: 0.001562051
test_loss: 0.0013982133
train_loss: 0.0012263122
test_loss: 0.001399021
train_loss: 0.0015450444
test_loss: 0.0013445449
train_loss: 0.0012629714
test_loss: 0.0012335379
train_loss: 0.0013807102
test_loss: 0.0015060644
train_loss: 0.0012356364
test_loss: 0.001179072
train_loss: 0.0012351847
test_loss: 0.0011951749
train_loss: 0.0012927664
test_loss: 0.0015418977
train_loss: 0.0012450916
test_loss: 0.0014528848
train_loss: 0.0024959096
test_loss: 0.0022436103
train_loss: 0.0015144789
test_loss: 0.0014268184
train_loss: 0.001533916
test_loss: 0.0014473998
train_loss: 0.0013155137
test_loss: 0.0011581134
train_loss: 0.0010906777
test_loss: 0.0011915902
train_loss: 0.0013354904
test_loss: 0.0015952477
train_loss: 0.0015541826
test_loss: 0.00152113
train_loss: 0.0015210744
test_loss: 0.0013152276
train_loss: 0.0012189825
test_loss: 0.0013497768
train_loss: 0.0012033633
test_loss: 0.0012178423
train_loss: 0.0017813374
test_loss: 0.0013482124
train_loss: 0.0012728437
test_loss: 0.0015854065
train_loss: 0.0016708664
test_loss: 0.0016749608
train_loss: 0.0014060473
test_loss: 0.001599157
train_loss: 0.0016292147
test_loss: 0.0015428762
train_loss: 0.0015099332
test_loss: 0.0011686425
train_loss: 0.0017796911
test_loss: 0.0014570185
train_loss: 0.0017862377
test_loss: 0.0014930597
train_loss: 0.0014230316
test_loss: 0.0013981306
train_loss: 0.0013371796
test_loss: 0.0015154186
train_loss: 0.0016730384
test_loss: 0.0014663732
train_loss: 0.0014625179
test_loss: 0.0012618975
train_loss: 0.0011999431
test_loss: 0.0012805365
train_loss: 0.0014488458
test_loss: 0.0012713915
train_loss: 0.0019463791
test_loss: 0.001359812
train_loss: 0.0021960915
test_loss: 0.0017671831
train_loss: 0.0020667997
test_loss: 0.002428833
train_loss: 0.0018529957
test_loss: 0.0018299124
train_loss: 0.0015211316
test_loss: 0.0014804144
train_loss: 0.0014415234
test_loss: 0.0012137166
train_loss: 0.0014533189
test_loss: 0.0017336268
train_loss: 0.0012282723
test_loss: 0.0016327538
train_loss: 0.0012936526
test_loss: 0.0015721325
train_loss: 0.0016267938
test_loss: 0.0013306548
train_loss: 0.0013366955
test_loss: 0.0013119739
train_loss: 0.0012095073
test_loss: 0.0012668169
train_loss: 0.0011206408
test_loss: 0.0011279156
train_loss: 0.0015219125
test_loss: 0.0011867433
train_loss: 0.0013561745
test_loss: 0.0013422876
train_loss: 0.0013783036
test_loss: 0.0013328867
train_loss: 0.0013810095
test_loss: 0.0013851034
train_loss: 0.0011623342
test_loss: 0.0012833955
train_loss: 0.001299556
test_loss: 0.0011582858
train_loss: 0.0011963166
test_loss: 0.001273764
train_loss: 0.0012567551
test_loss: 0.0011622694
train_loss: 0.0013941873
test_loss: 0.0012440241
train_loss: 0.001313878
test_loss: 0.0012756572
train_loss: 0.0015265729
test_loss: 0.0016407768
train_loss: 0.0015193159
test_loss: 0.0014737094
train_loss: 0.0013329732
test_loss: 0.0015099504
train_loss: 0.0012549476
test_loss: 0.0013819796
train_loss: 0.0012770538
test_loss: 0.0014722217
train_loss: 0.0012758512
test_loss: 0.0014057476
train_loss: 0.001197807
test_loss: 0.0014135374
train_loss: 0.0016273037
test_loss: 0.0014089867
train_loss: 0.0015504349
test_loss: 0.0014531068
train_loss: 0.0015325702
test_loss: 0.00134209
train_loss: 0.001520968
test_loss: 0.0013737783
train_loss: 0.0012339039
test_loss: 0.0013395208
train_loss: 0.0012681625
test_loss: 0.0016044462
train_loss: 0.0014795564
test_loss: 0.0015660789
train_loss: 0.0018569247
test_loss: 0.0017906363
train_loss: 0.001682403
test_loss: 0.0015124546
train_loss: 0.00192446
test_loss: 0.0014684723
train_loss: 0.0014706962
test_loss: 0.0015145727
train_loss: 0.0013505964
test_loss: 0.0013184526
train_loss: 0.0011868734
test_loss: 0.0014479796
train_loss: 0.0013993167
test_loss: 0.0013713706
train_loss: 0.0014537936
test_loss: 0.0013487358
train_loss: 0.0013424859
test_loss: 0.0013745655
train_loss: 0.0012496093
test_loss: 0.0016893747
train_loss: 0.0016739899
test_loss: 0.0014608249
train_loss: 0.0016841495
test_loss: 0.0014018151
train_loss: 0.0012855068
test_loss: 0.0013135106
train_loss: 0.0012634413
test_loss: 0.001584614
train_loss: 0.0013734982
test_loss: 0.0014844754
train_loss: 0.0013496279
test_loss: 0.0012130905
train_loss: 0.0016178234
test_loss: 0.0016242692
train_loss: 0.0017045722
test_loss: 0.0025454976
train_loss: 0.0023778183
test_loss: 0.0021309864
train_loss: 0.001966829
test_loss: 0.0024455409
train_loss: 0.0021697062
test_loss: 0.002142922
train_loss: 0.0017335668
test_loss: 0.0021726042
train_loss: 0.0018642314
test_loss: 0.0019405978
train_loss: 0.0016224197
test_loss: 0.0017276984
train_loss: 0.0012081835
test_loss: 0.0013358295
train_loss: 0.0010718484/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0011794697
train_loss: 0.0012596236
test_loss: 0.0013591595
train_loss: 0.0013381883
test_loss: 0.0012482665
train_loss: 0.0010581107
test_loss: 0.001436215
train_loss: 0.0012949263
test_loss: 0.0013510637
train_loss: 0.0012314906
test_loss: 0.0014175692
train_loss: 0.0015129764
test_loss: 0.0013786346
train_loss: 0.0013742519
test_loss: 0.0014564205
train_loss: 0.0014461136
test_loss: 0.0013963169
train_loss: 0.0016496726
test_loss: 0.0012994296
train_loss: 0.0013497791
test_loss: 0.0012347645
train_loss: 0.0013403124
test_loss: 0.0013342404
train_loss: 0.0011867012
test_loss: 0.0014054468
train_loss: 0.0012791706
test_loss: 0.0012987799
train_loss: 0.0014596976
test_loss: 0.0012468919
train_loss: 0.0015220544
test_loss: 0.0011783717
train_loss: 0.0013737411
test_loss: 0.001158739
train_loss: 0.0013088409
test_loss: 0.0014299554
train_loss: 0.001312678
test_loss: 0.001271496
train_loss: 0.0013735339
test_loss: 0.0013344291
train_loss: 0.0010890735
test_loss: 0.0015215414
train_loss: 0.0014032695
test_loss: 0.0013142461
train_loss: 0.0011810113
test_loss: 0.0011849151
train_loss: 0.0017796418
test_loss: 0.0013283188
train_loss: 0.0013603417
test_loss: 0.0014617138
train_loss: 0.0011576514
test_loss: 0.0013669416
train_loss: 0.0013246614
test_loss: 0.0016505498
train_loss: 0.0013076301
test_loss: 0.0013187039
train_loss: 0.0011249522
test_loss: 0.0011575996
train_loss: 0.0014397701
test_loss: 0.0017111774
train_loss: 0.0012042079
test_loss: 0.0013004387
train_loss: 0.0014869489
test_loss: 0.00148421
train_loss: 0.0012147481
test_loss: 0.0012663627
train_loss: 0.0012145056
test_loss: 0.0011545508
train_loss: 0.0012884547
test_loss: 0.0011109512
train_loss: 0.0013811565
test_loss: 0.0014287499
train_loss: 0.001300951
test_loss: 0.0016694568
train_loss: 0.0013017928
test_loss: 0.0015461383
train_loss: 0.0011568263
test_loss: 0.0012494216
train_loss: 0.0015244994
test_loss: 0.0012786037
train_loss: 0.001255117
test_loss: 0.0013602356
train_loss: 0.0013635763
test_loss: 0.0015369784
train_loss: 0.001748972
test_loss: 0.0016349208
train_loss: 0.0015739659
test_loss: 0.0013265638
train_loss: 0.001269518
test_loss: 0.001257211
train_loss: 0.001212775
test_loss: 0.0013535508
train_loss: 0.0015631662
test_loss: 0.0015887837
train_loss: 0.0016051837
test_loss: 0.0012572374
train_loss: 0.0011539798
test_loss: 0.0016612858
train_loss: 0.0013095303
test_loss: 0.0012574209
train_loss: 0.0012505789
test_loss: 0.0012169178
train_loss: 0.001073275
test_loss: 0.001069621
train_loss: 0.0011956008
test_loss: 0.0018008414
train_loss: 0.0013241615
test_loss: 0.001278419
train_loss: 0.0013202041
test_loss: 0.0016514005
train_loss: 0.0013286126
test_loss: 0.001498002
train_loss: 0.0012576462
test_loss: 0.0011481742
train_loss: 0.0013588353
test_loss: 0.0013103461
train_loss: 0.001318808
test_loss: 0.0014723163
train_loss: 0.0014597104
test_loss: 0.001486722
train_loss: 0.0013481636
test_loss: 0.0013784858
train_loss: 0.0012168199
test_loss: 0.0013131916
train_loss: 0.0011815862
test_loss: 0.0013052394
train_loss: 0.001512018
test_loss: 0.0015708917
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.6 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b13211e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b131967b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b131c2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b13272730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b132726a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b13153b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b131531e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b130cd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b130cd510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b13082048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b130828c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b13039d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b13067a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b130102f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b130177b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b12fe2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b12fe2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b12fe2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9b12fe26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ac8292840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ac82a8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ac82630d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ac82a89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ac827ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ac823a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9aa0168b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9aa0168730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9aa01a1378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9aa01a1048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9aa00f4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9aa0122268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9aa00bef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9aa008c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9aa00c1d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9aa00530d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9aa0063f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.6515064e-06
Iter: 2 loss: 7.6325523e-06
Iter: 3 loss: 1.78128482e-06
Iter: 4 loss: 1.4596061e-06
Iter: 5 loss: 1.84358987e-06
Iter: 6 loss: 1.28971249e-06
Iter: 7 loss: 1.07679512e-06
Iter: 8 loss: 1.34333982e-06
Iter: 9 loss: 9.66562311e-07
Iter: 10 loss: 8.33602826e-07
Iter: 11 loss: 1.73817398e-06
Iter: 12 loss: 8.20673108e-07
Iter: 13 loss: 7.33812101e-07
Iter: 14 loss: 1.32881e-06
Iter: 15 loss: 7.25411667e-07
Iter: 16 loss: 6.725557e-07
Iter: 17 loss: 6.7559472e-07
Iter: 18 loss: 6.31083196e-07
Iter: 19 loss: 6.09416247e-07
Iter: 20 loss: 7.32528406e-07
Iter: 21 loss: 6.0643913e-07
Iter: 22 loss: 5.91467483e-07
Iter: 23 loss: 7.99320674e-07
Iter: 24 loss: 5.91422122e-07
Iter: 25 loss: 5.84177712e-07
Iter: 26 loss: 5.97843155e-07
Iter: 27 loss: 5.81138465e-07
Iter: 28 loss: 5.75259605e-07
Iter: 29 loss: 5.70225097e-07
Iter: 30 loss: 5.68581072e-07
Iter: 31 loss: 5.61464446e-07
Iter: 32 loss: 6.0116804e-07
Iter: 33 loss: 5.60481112e-07
Iter: 34 loss: 5.54460087e-07
Iter: 35 loss: 5.74643877e-07
Iter: 36 loss: 5.52794404e-07
Iter: 37 loss: 5.47256036e-07
Iter: 38 loss: 5.68726364e-07
Iter: 39 loss: 5.46021738e-07
Iter: 40 loss: 5.44900502e-07
Iter: 41 loss: 5.43280692e-07
Iter: 42 loss: 5.41959082e-07
Iter: 43 loss: 5.37882e-07
Iter: 44 loss: 5.47138711e-07
Iter: 45 loss: 5.35538106e-07
Iter: 46 loss: 5.31002854e-07
Iter: 47 loss: 5.308911e-07
Iter: 48 loss: 5.28515e-07
Iter: 49 loss: 5.32894887e-07
Iter: 50 loss: 5.27533075e-07
Iter: 51 loss: 5.24734048e-07
Iter: 52 loss: 5.29709041e-07
Iter: 53 loss: 5.23471158e-07
Iter: 54 loss: 5.19719151e-07
Iter: 55 loss: 5.19952494e-07
Iter: 56 loss: 5.16788248e-07
Iter: 57 loss: 5.13277712e-07
Iter: 58 loss: 5.1153404e-07
Iter: 59 loss: 5.09864435e-07
Iter: 60 loss: 5.06900221e-07
Iter: 61 loss: 5.06359e-07
Iter: 62 loss: 5.04135301e-07
Iter: 63 loss: 5.00302576e-07
Iter: 64 loss: 5.00271e-07
Iter: 65 loss: 4.9571031e-07
Iter: 66 loss: 5.15326064e-07
Iter: 67 loss: 4.94819517e-07
Iter: 68 loss: 4.90827e-07
Iter: 69 loss: 4.90648176e-07
Iter: 70 loss: 4.87542081e-07
Iter: 71 loss: 4.85893906e-07
Iter: 72 loss: 4.85729e-07
Iter: 73 loss: 4.83913254e-07
Iter: 74 loss: 4.95659492e-07
Iter: 75 loss: 4.83732151e-07
Iter: 76 loss: 4.82297651e-07
Iter: 77 loss: 4.80449216e-07
Iter: 78 loss: 4.8034741e-07
Iter: 79 loss: 4.79361063e-07
Iter: 80 loss: 4.88991873e-07
Iter: 81 loss: 4.79325877e-07
Iter: 82 loss: 4.78260802e-07
Iter: 83 loss: 4.79632376e-07
Iter: 84 loss: 4.77733e-07
Iter: 85 loss: 4.76653327e-07
Iter: 86 loss: 4.79388063e-07
Iter: 87 loss: 4.76259316e-07
Iter: 88 loss: 4.74843e-07
Iter: 89 loss: 4.7642385e-07
Iter: 90 loss: 4.74152074e-07
Iter: 91 loss: 4.72768306e-07
Iter: 92 loss: 4.74290175e-07
Iter: 93 loss: 4.72057792e-07
Iter: 94 loss: 4.70597399e-07
Iter: 95 loss: 4.71997282e-07
Iter: 96 loss: 4.69745942e-07
Iter: 97 loss: 4.68398639e-07
Iter: 98 loss: 4.68366579e-07
Iter: 99 loss: 4.6771558e-07
Iter: 100 loss: 4.6641793e-07
Iter: 101 loss: 4.92359277e-07
Iter: 102 loss: 4.66413354e-07
Iter: 103 loss: 4.6478857e-07
Iter: 104 loss: 4.69207635e-07
Iter: 105 loss: 4.64221813e-07
Iter: 106 loss: 4.62880848e-07
Iter: 107 loss: 4.72407692e-07
Iter: 108 loss: 4.62766195e-07
Iter: 109 loss: 4.62350329e-07
Iter: 110 loss: 4.62149757e-07
Iter: 111 loss: 4.61872219e-07
Iter: 112 loss: 4.60921513e-07
Iter: 113 loss: 4.6292655e-07
Iter: 114 loss: 4.60356773e-07
Iter: 115 loss: 4.59027547e-07
Iter: 116 loss: 4.6982035e-07
Iter: 117 loss: 4.5891511e-07
Iter: 118 loss: 4.58163584e-07
Iter: 119 loss: 4.58180409e-07
Iter: 120 loss: 4.57521423e-07
Iter: 121 loss: 4.56230225e-07
Iter: 122 loss: 4.78855441e-07
Iter: 123 loss: 4.5622906e-07
Iter: 124 loss: 4.55585564e-07
Iter: 125 loss: 4.55532586e-07
Iter: 126 loss: 4.54993227e-07
Iter: 127 loss: 4.53690546e-07
Iter: 128 loss: 4.65283648e-07
Iter: 129 loss: 4.53508619e-07
Iter: 130 loss: 4.53017748e-07
Iter: 131 loss: 4.52722219e-07
Iter: 132 loss: 4.52331079e-07
Iter: 133 loss: 4.52923786e-07
Iter: 134 loss: 4.52140512e-07
Iter: 135 loss: 4.51778703e-07
Iter: 136 loss: 4.51683036e-07
Iter: 137 loss: 4.51411154e-07
Iter: 138 loss: 4.50917526e-07
Iter: 139 loss: 4.51719615e-07
Iter: 140 loss: 4.50681085e-07
Iter: 141 loss: 4.50436175e-07
Iter: 142 loss: 4.50377286e-07
Iter: 143 loss: 4.5005828e-07
Iter: 144 loss: 4.50298586e-07
Iter: 145 loss: 4.49892639e-07
Iter: 146 loss: 4.49608166e-07
Iter: 147 loss: 4.49003096e-07
Iter: 148 loss: 4.60566127e-07
Iter: 149 loss: 4.49007217e-07
Iter: 150 loss: 4.48630601e-07
Iter: 151 loss: 4.48635916e-07
Iter: 152 loss: 4.48191173e-07
Iter: 153 loss: 4.48158602e-07
Iter: 154 loss: 4.47824618e-07
Iter: 155 loss: 4.47378369e-07
Iter: 156 loss: 4.49965739e-07
Iter: 157 loss: 4.47290176e-07
Iter: 158 loss: 4.46924474e-07
Iter: 159 loss: 4.47780565e-07
Iter: 160 loss: 4.4679706e-07
Iter: 161 loss: 4.46361895e-07
Iter: 162 loss: 4.4608808e-07
Iter: 163 loss: 4.45894472e-07
Iter: 164 loss: 4.45312395e-07
Iter: 165 loss: 4.49938341e-07
Iter: 166 loss: 4.45226192e-07
Iter: 167 loss: 4.44786821e-07
Iter: 168 loss: 4.46768865e-07
Iter: 169 loss: 4.4462854e-07
Iter: 170 loss: 4.44371e-07
Iter: 171 loss: 4.43984533e-07
Iter: 172 loss: 4.43934454e-07
Iter: 173 loss: 4.43273905e-07
Iter: 174 loss: 4.45062142e-07
Iter: 175 loss: 4.43010805e-07
Iter: 176 loss: 4.43044257e-07
Iter: 177 loss: 4.42838285e-07
Iter: 178 loss: 4.42638111e-07
Iter: 179 loss: 4.42087e-07
Iter: 180 loss: 4.46335e-07
Iter: 181 loss: 4.41976567e-07
Iter: 182 loss: 4.41515681e-07
Iter: 183 loss: 4.45068395e-07
Iter: 184 loss: 4.41483053e-07
Iter: 185 loss: 4.41187552e-07
Iter: 186 loss: 4.42772603e-07
Iter: 187 loss: 4.41101065e-07
Iter: 188 loss: 4.40809089e-07
Iter: 189 loss: 4.40550906e-07
Iter: 190 loss: 4.40435315e-07
Iter: 191 loss: 4.40128247e-07
Iter: 192 loss: 4.43943691e-07
Iter: 193 loss: 4.4009829e-07
Iter: 194 loss: 4.39836469e-07
Iter: 195 loss: 4.39585563e-07
Iter: 196 loss: 4.3950871e-07
Iter: 197 loss: 4.391228e-07
Iter: 198 loss: 4.41239138e-07
Iter: 199 loss: 4.39041457e-07
Iter: 200 loss: 4.38798622e-07
Iter: 201 loss: 4.40226131e-07
Iter: 202 loss: 4.38759287e-07
Iter: 203 loss: 4.3849974e-07
Iter: 204 loss: 4.38697725e-07
Iter: 205 loss: 4.38299622e-07
Iter: 206 loss: 4.37970755e-07
Iter: 207 loss: 4.37536841e-07
Iter: 208 loss: 4.3754261e-07
Iter: 209 loss: 4.37328652e-07
Iter: 210 loss: 4.37265527e-07
Iter: 211 loss: 4.37162527e-07
Iter: 212 loss: 4.3713888e-07
Iter: 213 loss: 4.37012091e-07
Iter: 214 loss: 4.36701924e-07
Iter: 215 loss: 4.38923394e-07
Iter: 216 loss: 4.36647781e-07
Iter: 217 loss: 4.3636669e-07
Iter: 218 loss: 4.38385911e-07
Iter: 219 loss: 4.36352707e-07
Iter: 220 loss: 4.36066188e-07
Iter: 221 loss: 4.37751453e-07
Iter: 222 loss: 4.36012897e-07
Iter: 223 loss: 4.35796409e-07
Iter: 224 loss: 4.35688207e-07
Iter: 225 loss: 4.35604562e-07
Iter: 226 loss: 4.35276945e-07
Iter: 227 loss: 4.36869584e-07
Iter: 228 loss: 4.35263217e-07
Iter: 229 loss: 4.34994632e-07
Iter: 230 loss: 4.35313268e-07
Iter: 231 loss: 4.34909168e-07
Iter: 232 loss: 4.3466207e-07
Iter: 233 loss: 4.34684921e-07
Iter: 234 loss: 4.34431e-07
Iter: 235 loss: 4.34183619e-07
Iter: 236 loss: 4.34174126e-07
Iter: 237 loss: 4.34044239e-07
Iter: 238 loss: 4.34047962e-07
Iter: 239 loss: 4.33953744e-07
Iter: 240 loss: 4.33688513e-07
Iter: 241 loss: 4.33417938e-07
Iter: 242 loss: 4.33392046e-07
Iter: 243 loss: 4.33198608e-07
Iter: 244 loss: 4.33164161e-07
Iter: 245 loss: 4.32889351e-07
Iter: 246 loss: 4.33710085e-07
Iter: 247 loss: 4.32844331e-07
Iter: 248 loss: 4.32697817e-07
Iter: 249 loss: 4.32533369e-07
Iter: 250 loss: 4.32497586e-07
Iter: 251 loss: 4.32285674e-07
Iter: 252 loss: 4.32951254e-07
Iter: 253 loss: 4.3221155e-07
Iter: 254 loss: 4.31998274e-07
Iter: 255 loss: 4.34322089e-07
Iter: 256 loss: 4.32017657e-07
Iter: 257 loss: 4.31893966e-07
Iter: 258 loss: 4.31621771e-07
Iter: 259 loss: 4.34949015e-07
Iter: 260 loss: 4.31592639e-07
Iter: 261 loss: 4.31292051e-07
Iter: 262 loss: 4.31322633e-07
Iter: 263 loss: 4.31157332e-07
Iter: 264 loss: 4.31204114e-07
Iter: 265 loss: 4.31081645e-07
Iter: 266 loss: 4.30870585e-07
Iter: 267 loss: 4.30791232e-07
Iter: 268 loss: 4.30701846e-07
Iter: 269 loss: 4.30575426e-07
Iter: 270 loss: 4.30547288e-07
Iter: 271 loss: 4.30409273e-07
Iter: 272 loss: 4.30180791e-07
Iter: 273 loss: 4.33765621e-07
Iter: 274 loss: 4.30164448e-07
Iter: 275 loss: 4.29909846e-07
Iter: 276 loss: 4.31353044e-07
Iter: 277 loss: 4.29889838e-07
Iter: 278 loss: 4.297803e-07
Iter: 279 loss: 4.29769784e-07
Iter: 280 loss: 4.29687049e-07
Iter: 281 loss: 4.29533742e-07
Iter: 282 loss: 4.29548351e-07
Iter: 283 loss: 4.29415252e-07
Iter: 284 loss: 4.29327883e-07
Iter: 285 loss: 4.29288093e-07
Iter: 286 loss: 4.29188532e-07
Iter: 287 loss: 4.29160167e-07
Iter: 288 loss: 4.29078085e-07
Iter: 289 loss: 4.28983867e-07
Iter: 290 loss: 4.28983952e-07
Iter: 291 loss: 4.28767152e-07
Iter: 292 loss: 4.29175174e-07
Iter: 293 loss: 4.28693852e-07
Iter: 294 loss: 4.28467501e-07
Iter: 295 loss: 4.29205727e-07
Iter: 296 loss: 4.28415831e-07
Iter: 297 loss: 4.28265253e-07
Iter: 298 loss: 4.28171774e-07
Iter: 299 loss: 4.28079318e-07
Iter: 300 loss: 4.27783647e-07
Iter: 301 loss: 4.29070155e-07
Iter: 302 loss: 4.27745221e-07
Iter: 303 loss: 4.27542687e-07
Iter: 304 loss: 4.29016325e-07
Iter: 305 loss: 4.27503551e-07
Iter: 306 loss: 4.27309345e-07
Iter: 307 loss: 4.27495081e-07
Iter: 308 loss: 4.27234312e-07
Iter: 309 loss: 4.26980677e-07
Iter: 310 loss: 4.26947395e-07
Iter: 311 loss: 4.26824272e-07
Iter: 312 loss: 4.26651468e-07
Iter: 313 loss: 4.26591129e-07
Iter: 314 loss: 4.26510496e-07
Iter: 315 loss: 4.26384844e-07
Iter: 316 loss: 4.26364e-07
Iter: 317 loss: 4.26220709e-07
Iter: 318 loss: 4.262223e-07
Iter: 319 loss: 4.26118334e-07
Iter: 320 loss: 4.25922138e-07
Iter: 321 loss: 4.25936037e-07
Iter: 322 loss: 4.25821298e-07
Iter: 323 loss: 4.25645112e-07
Iter: 324 loss: 4.29554376e-07
Iter: 325 loss: 4.25649546e-07
Iter: 326 loss: 4.25458524e-07
Iter: 327 loss: 4.26822737e-07
Iter: 328 loss: 4.25463639e-07
Iter: 329 loss: 4.25312862e-07
Iter: 330 loss: 4.25670237e-07
Iter: 331 loss: 4.252704e-07
Iter: 332 loss: 4.25079833e-07
Iter: 333 loss: 4.25036461e-07
Iter: 334 loss: 4.24998689e-07
Iter: 335 loss: 4.24811e-07
Iter: 336 loss: 4.25559364e-07
Iter: 337 loss: 4.24785867e-07
Iter: 338 loss: 4.24623437e-07
Iter: 339 loss: 4.25213983e-07
Iter: 340 loss: 4.24572761e-07
Iter: 341 loss: 4.24493066e-07
Iter: 342 loss: 4.24855358e-07
Iter: 343 loss: 4.24424286e-07
Iter: 344 loss: 4.24331233e-07
Iter: 345 loss: 4.2439936e-07
Iter: 346 loss: 4.24271718e-07
Iter: 347 loss: 4.2404983e-07
Iter: 348 loss: 4.24520294e-07
Iter: 349 loss: 4.23950951e-07
Iter: 350 loss: 4.23771212e-07
Iter: 351 loss: 4.23689443e-07
Iter: 352 loss: 4.23610402e-07
Iter: 353 loss: 4.23442543e-07
Iter: 354 loss: 4.24675108e-07
Iter: 355 loss: 4.23400678e-07
Iter: 356 loss: 4.23233814e-07
Iter: 357 loss: 4.24432244e-07
Iter: 358 loss: 4.23194365e-07
Iter: 359 loss: 4.23074397e-07
Iter: 360 loss: 4.22839548e-07
Iter: 361 loss: 4.22841225e-07
Iter: 362 loss: 4.2266484e-07
Iter: 363 loss: 4.25215376e-07
Iter: 364 loss: 4.22646139e-07
Iter: 365 loss: 4.22490587e-07
Iter: 366 loss: 4.22655205e-07
Iter: 367 loss: 4.22417486e-07
Iter: 368 loss: 4.22264748e-07
Iter: 369 loss: 4.22896676e-07
Iter: 370 loss: 4.22226236e-07
Iter: 371 loss: 4.22096491e-07
Iter: 372 loss: 4.22407311e-07
Iter: 373 loss: 4.22060026e-07
Iter: 374 loss: 4.21874745e-07
Iter: 375 loss: 4.22157257e-07
Iter: 376 loss: 4.21834045e-07
Iter: 377 loss: 4.21695063e-07
Iter: 378 loss: 4.22005144e-07
Iter: 379 loss: 4.21649332e-07
Iter: 380 loss: 4.21558042e-07
Iter: 381 loss: 4.21578108e-07
Iter: 382 loss: 4.21444355e-07
Iter: 383 loss: 4.21301138e-07
Iter: 384 loss: 4.21330498e-07
Iter: 385 loss: 4.212107e-07
Iter: 386 loss: 4.21048355e-07
Iter: 387 loss: 4.2104449e-07
Iter: 388 loss: 4.2093194e-07
Iter: 389 loss: 4.20942058e-07
Iter: 390 loss: 4.2078625e-07
Iter: 391 loss: 4.20834169e-07
Iter: 392 loss: 4.20735887e-07
Iter: 393 loss: 4.20608927e-07
Iter: 394 loss: 4.20568227e-07
Iter: 395 loss: 4.20509195e-07
Iter: 396 loss: 4.20372942e-07
Iter: 397 loss: 4.21910926e-07
Iter: 398 loss: 4.20364586e-07
Iter: 399 loss: 4.20241918e-07
Iter: 400 loss: 4.20126753e-07
Iter: 401 loss: 4.20155885e-07
Iter: 402 loss: 4.19933031e-07
Iter: 403 loss: 4.21615709e-07
Iter: 404 loss: 4.19924419e-07
Iter: 405 loss: 4.1984373e-07
Iter: 406 loss: 4.2008179e-07
Iter: 407 loss: 4.19771339e-07
Iter: 408 loss: 4.19684568e-07
Iter: 409 loss: 4.19563492e-07
Iter: 410 loss: 4.19532654e-07
Iter: 411 loss: 4.19415414e-07
Iter: 412 loss: 4.19422292e-07
Iter: 413 loss: 4.19317303e-07
Iter: 414 loss: 4.19923765e-07
Iter: 415 loss: 4.19299539e-07
Iter: 416 loss: 4.1924136e-07
Iter: 417 loss: 4.19096807e-07
Iter: 418 loss: 4.20791253e-07
Iter: 419 loss: 4.19048035e-07
Iter: 420 loss: 4.18848316e-07
Iter: 421 loss: 4.19060825e-07
Iter: 422 loss: 4.18708481e-07
Iter: 423 loss: 4.18709106e-07
Iter: 424 loss: 4.18613951e-07
Iter: 425 loss: 4.18585387e-07
Iter: 426 loss: 4.18391778e-07
Iter: 427 loss: 4.20770903e-07
Iter: 428 loss: 4.18392176e-07
Iter: 429 loss: 4.18233583e-07
Iter: 430 loss: 4.19194919e-07
Iter: 431 loss: 4.18179468e-07
Iter: 432 loss: 4.18056288e-07
Iter: 433 loss: 4.19175194e-07
Iter: 434 loss: 4.18027952e-07
Iter: 435 loss: 4.17966788e-07
Iter: 436 loss: 4.17920717e-07
Iter: 437 loss: 4.17852959e-07
Iter: 438 loss: 4.177368e-07
Iter: 439 loss: 4.1863575e-07
Iter: 440 loss: 4.17782672e-07
Iter: 441 loss: 4.17679075e-07
Iter: 442 loss: 4.17813851e-07
Iter: 443 loss: 4.17620981e-07
Iter: 444 loss: 4.17531083e-07
Iter: 445 loss: 4.17603786e-07
Iter: 446 loss: 4.17450167e-07
Iter: 447 loss: 4.1742112e-07
Iter: 448 loss: 4.17394034e-07
Iter: 449 loss: 4.17333553e-07
Iter: 450 loss: 4.17207843e-07
Iter: 451 loss: 4.19151036e-07
Iter: 452 loss: 4.17199885e-07
Iter: 453 loss: 4.17078354e-07
Iter: 454 loss: 4.17180331e-07
Iter: 455 loss: 4.17006049e-07
Iter: 456 loss: 4.1684666e-07
Iter: 457 loss: 4.16827e-07
Iter: 458 loss: 4.16732547e-07
Iter: 459 loss: 4.16763413e-07
Iter: 460 loss: 4.16662431e-07
Iter: 461 loss: 4.1656935e-07
Iter: 462 loss: 4.16417492e-07
Iter: 463 loss: 4.18089371e-07
Iter: 464 loss: 4.16362354e-07
Iter: 465 loss: 4.16211549e-07
Iter: 466 loss: 4.17944534e-07
Iter: 467 loss: 4.16216608e-07
Iter: 468 loss: 4.16162663e-07
Iter: 469 loss: 4.16351298e-07
Iter: 470 loss: 4.16111078e-07
Iter: 471 loss: 4.15934153e-07
Iter: 472 loss: 4.15855283e-07
Iter: 473 loss: 4.15806085e-07
Iter: 474 loss: 4.15685889e-07
Iter: 475 loss: 4.17391874e-07
Iter: 476 loss: 4.15692114e-07
Iter: 477 loss: 4.1559656e-07
Iter: 478 loss: 4.15527722e-07
Iter: 479 loss: 4.15517036e-07
Iter: 480 loss: 4.15378906e-07
Iter: 481 loss: 4.17084948e-07
Iter: 482 loss: 4.15365776e-07
Iter: 483 loss: 4.15274627e-07
Iter: 484 loss: 4.15291538e-07
Iter: 485 loss: 4.15210479e-07
Iter: 486 loss: 4.15149373e-07
Iter: 487 loss: 4.17303596e-07
Iter: 488 loss: 4.151062e-07
Iter: 489 loss: 4.15063141e-07
Iter: 490 loss: 4.14966451e-07
Iter: 491 loss: 4.14947664e-07
Iter: 492 loss: 4.14811723e-07
Iter: 493 loss: 4.15897489e-07
Iter: 494 loss: 4.14817066e-07
Iter: 495 loss: 4.14709206e-07
Iter: 496 loss: 4.14715743e-07
Iter: 497 loss: 4.14651652e-07
Iter: 498 loss: 4.14519434e-07
Iter: 499 loss: 4.14511447e-07
Iter: 500 loss: 4.14498743e-07
Iter: 501 loss: 4.14449488e-07
Iter: 502 loss: 4.14436585e-07
Iter: 503 loss: 4.14358283e-07
Iter: 504 loss: 4.14599242e-07
Iter: 505 loss: 4.14346346e-07
Iter: 506 loss: 4.14200372e-07
Iter: 507 loss: 4.14426097e-07
Iter: 508 loss: 4.14181784e-07
Iter: 509 loss: 4.14070087e-07
Iter: 510 loss: 4.14133041e-07
Iter: 511 loss: 4.13990819e-07
Iter: 512 loss: 4.13915188e-07
Iter: 513 loss: 4.13903081e-07
Iter: 514 loss: 4.13875341e-07
Iter: 515 loss: 4.13727776e-07
Iter: 516 loss: 4.13739031e-07
Iter: 517 loss: 4.13639441e-07
Iter: 518 loss: 4.13625941e-07
Iter: 519 loss: 4.13533854e-07
Iter: 520 loss: 4.13595842e-07
Iter: 521 loss: 4.13502079e-07
Iter: 522 loss: 4.13440063e-07
Iter: 523 loss: 4.13276354e-07
Iter: 524 loss: 4.15323029e-07
Iter: 525 loss: 4.13282578e-07
Iter: 526 loss: 4.13153316e-07
Iter: 527 loss: 4.14202248e-07
Iter: 528 loss: 4.13105084e-07
Iter: 529 loss: 4.12989891e-07
Iter: 530 loss: 4.13127964e-07
Iter: 531 loss: 4.12938164e-07
Iter: 532 loss: 4.12778206e-07
Iter: 533 loss: 4.14530803e-07
Iter: 534 loss: 4.12765246e-07
Iter: 535 loss: 4.1271403e-07
Iter: 536 loss: 4.12683448e-07
Iter: 537 loss: 4.12671227e-07
Iter: 538 loss: 4.12533041e-07
Iter: 539 loss: 4.12558677e-07
Iter: 540 loss: 4.12469348e-07
Iter: 541 loss: 4.12284464e-07
Iter: 542 loss: 4.12757117e-07
Iter: 543 loss: 4.12248681e-07
Iter: 544 loss: 4.12121494e-07
Iter: 545 loss: 4.12691492e-07
Iter: 546 loss: 4.12129e-07
Iter: 547 loss: 4.12071273e-07
Iter: 548 loss: 4.12892547e-07
Iter: 549 loss: 4.12063059e-07
Iter: 550 loss: 4.11972024e-07
Iter: 551 loss: 4.11966141e-07
Iter: 552 loss: 4.11900828e-07
Iter: 553 loss: 4.11863e-07
Iter: 554 loss: 4.11859105e-07
Iter: 555 loss: 4.11814511e-07
Iter: 556 loss: 4.11722624e-07
Iter: 557 loss: 4.12666196e-07
Iter: 558 loss: 4.11734732e-07
Iter: 559 loss: 4.11627468e-07
Iter: 560 loss: 4.11718361e-07
Iter: 561 loss: 4.11581368e-07
Iter: 562 loss: 4.11495108e-07
Iter: 563 loss: 4.11712051e-07
Iter: 564 loss: 4.1145131e-07
Iter: 565 loss: 4.11380597e-07
Iter: 566 loss: 4.12144971e-07
Iter: 567 loss: 4.11356467e-07
Iter: 568 loss: 4.11267337e-07
Iter: 569 loss: 4.1144196e-07
Iter: 570 loss: 4.11250653e-07
Iter: 571 loss: 4.11168799e-07
Iter: 572 loss: 4.11060881e-07
Iter: 573 loss: 4.11074041e-07
Iter: 574 loss: 4.10926418e-07
Iter: 575 loss: 4.10908626e-07
Iter: 576 loss: 4.10789426e-07
Iter: 577 loss: 4.10771122e-07
Iter: 578 loss: 4.10747589e-07
Iter: 579 loss: 4.10645498e-07
Iter: 580 loss: 4.10637085e-07
Iter: 581 loss: 4.10599824e-07
Iter: 582 loss: 4.10522205e-07
Iter: 583 loss: 4.10959e-07
Iter: 584 loss: 4.10480823e-07
Iter: 585 loss: 4.10441544e-07
Iter: 586 loss: 4.11251364e-07
Iter: 587 loss: 4.1044234e-07
Iter: 588 loss: 4.10420398e-07
Iter: 589 loss: 4.10353806e-07
Iter: 590 loss: 4.10337179e-07
Iter: 591 loss: 4.10311571e-07
Iter: 592 loss: 4.10617361e-07
Iter: 593 loss: 4.10275788e-07
Iter: 594 loss: 4.10214113e-07
Iter: 595 loss: 4.10133566e-07
Iter: 596 loss: 4.10102189e-07
Iter: 597 loss: 4.10032811e-07
Iter: 598 loss: 4.10978544e-07
Iter: 599 loss: 4.10016298e-07
Iter: 600 loss: 4.09978895e-07
Iter: 601 loss: 4.10189898e-07
Iter: 602 loss: 4.09969232e-07
Iter: 603 loss: 4.09886269e-07
Iter: 604 loss: 4.099916e-07
Iter: 605 loss: 4.09832182e-07
Iter: 606 loss: 4.09787816e-07
Iter: 607 loss: 4.09827805e-07
Iter: 608 loss: 4.09771815e-07
Iter: 609 loss: 4.09678307e-07
Iter: 610 loss: 4.09701e-07
Iter: 611 loss: 4.09651705e-07
Iter: 612 loss: 4.09558822e-07
Iter: 613 loss: 4.1040235e-07
Iter: 614 loss: 4.09535517e-07
Iter: 615 loss: 4.09491633e-07
Iter: 616 loss: 4.09517469e-07
Iter: 617 loss: 4.09480776e-07
Iter: 618 loss: 4.09424473e-07
Iter: 619 loss: 4.09689505e-07
Iter: 620 loss: 4.09338782e-07
Iter: 621 loss: 4.09290266e-07
Iter: 622 loss: 4.09974035e-07
Iter: 623 loss: 4.09261474e-07
Iter: 624 loss: 4.09253744e-07
Iter: 625 loss: 4.09115813e-07
Iter: 626 loss: 4.10727438e-07
Iter: 627 loss: 4.09124652e-07
Iter: 628 loss: 4.09021084e-07
Iter: 629 loss: 4.09000393e-07
Iter: 630 loss: 4.08918908e-07
Iter: 631 loss: 4.08868175e-07
Iter: 632 loss: 4.08877554e-07
Iter: 633 loss: 4.08758581e-07
Iter: 634 loss: 4.09450479e-07
Iter: 635 loss: 4.08707052e-07
Iter: 636 loss: 4.0868818e-07
Iter: 637 loss: 4.08692699e-07
Iter: 638 loss: 4.08624885e-07
Iter: 639 loss: 4.08508868e-07
Iter: 640 loss: 4.10267774e-07
Iter: 641 loss: 4.0851188e-07
Iter: 642 loss: 4.08467372e-07
Iter: 643 loss: 4.09216028e-07
Iter: 644 loss: 4.08424341e-07
Iter: 645 loss: 4.08410642e-07
Iter: 646 loss: 4.08344931e-07
Iter: 647 loss: 4.08359767e-07
Iter: 648 loss: 4.08255403e-07
Iter: 649 loss: 4.08690028e-07
Iter: 650 loss: 4.08238e-07
Iter: 651 loss: 4.08189294e-07
Iter: 652 loss: 4.08288059e-07
Iter: 653 loss: 4.08182473e-07
Iter: 654 loss: 4.08135293e-07
Iter: 655 loss: 4.08128443e-07
Iter: 656 loss: 4.08111759e-07
Iter: 657 loss: 4.0808095e-07
Iter: 658 loss: 4.08057701e-07
Iter: 659 loss: 4.07986789e-07
Iter: 660 loss: 4.08036783e-07
Iter: 661 loss: 4.07953962e-07
Iter: 662 loss: 4.07880094e-07
Iter: 663 loss: 4.08192051e-07
Iter: 664 loss: 4.07854259e-07
Iter: 665 loss: 4.07789116e-07
Iter: 666 loss: 4.07872164e-07
Iter: 667 loss: 4.07757625e-07
Iter: 668 loss: 4.07709877e-07
Iter: 669 loss: 4.08106928e-07
Iter: 670 loss: 4.07680886e-07
Iter: 671 loss: 4.07611196e-07
Iter: 672 loss: 4.07794914e-07
Iter: 673 loss: 4.07626203e-07
Iter: 674 loss: 4.07554154e-07
Iter: 675 loss: 4.07520275e-07
Iter: 676 loss: 4.07490745e-07
Iter: 677 loss: 4.07423812e-07
Iter: 678 loss: 4.07801025e-07
Iter: 679 loss: 4.07401899e-07
Iter: 680 loss: 4.07355941e-07
Iter: 681 loss: 4.07348921e-07
Iter: 682 loss: 4.07259222e-07
Iter: 683 loss: 4.07186292e-07
Iter: 684 loss: 4.08275071e-07
Iter: 685 loss: 4.0717282e-07
Iter: 686 loss: 4.07153209e-07
Iter: 687 loss: 4.07426938e-07
Iter: 688 loss: 4.07137634e-07
Iter: 689 loss: 4.07110917e-07
Iter: 690 loss: 4.07085423e-07
Iter: 691 loss: 4.07025027e-07
Iter: 692 loss: 4.06977648e-07
Iter: 693 loss: 4.07166965e-07
Iter: 694 loss: 4.06977904e-07
Iter: 695 loss: 4.06878655e-07
Iter: 696 loss: 4.07128056e-07
Iter: 697 loss: 4.06861034e-07
Iter: 698 loss: 4.06789098e-07
Iter: 699 loss: 4.06775968e-07
Iter: 700 loss: 4.06727338e-07
Iter: 701 loss: 4.06605892e-07
Iter: 702 loss: 4.07529e-07
Iter: 703 loss: 4.06616863e-07
Iter: 704 loss: 4.06545752e-07
Iter: 705 loss: 4.06776962e-07
Iter: 706 loss: 4.06539755e-07
Iter: 707 loss: 4.06479103e-07
Iter: 708 loss: 4.06535605e-07
Iter: 709 loss: 4.06428967e-07
Iter: 710 loss: 4.06417428e-07
Iter: 711 loss: 4.06400545e-07
Iter: 712 loss: 4.06323579e-07
Iter: 713 loss: 4.06243714e-07
Iter: 714 loss: 4.07116232e-07
Iter: 715 loss: 4.06290894e-07
Iter: 716 loss: 4.06243373e-07
Iter: 717 loss: 4.06351376e-07
Iter: 718 loss: 4.06212223e-07
Iter: 719 loss: 4.06173029e-07
Iter: 720 loss: 4.06303258e-07
Iter: 721 loss: 4.06167544e-07
Iter: 722 loss: 4.06085377e-07
Iter: 723 loss: 4.06429365e-07
Iter: 724 loss: 4.06125622e-07
Iter: 725 loss: 4.06093221e-07
Iter: 726 loss: 4.06022934e-07
Iter: 727 loss: 4.06053573e-07
Iter: 728 loss: 4.05962851e-07
Iter: 729 loss: 4.06108427e-07
Iter: 730 loss: 4.05947333e-07
Iter: 731 loss: 4.05842968e-07
Iter: 732 loss: 4.06217538e-07
Iter: 733 loss: 4.05867752e-07
Iter: 734 loss: 4.05783737e-07
Iter: 735 loss: 4.05804542e-07
Iter: 736 loss: 4.05738149e-07
Iter: 737 loss: 4.05676616e-07
Iter: 738 loss: 4.06329775e-07
Iter: 739 loss: 4.05663485e-07
Iter: 740 loss: 4.05604112e-07
Iter: 741 loss: 4.05637763e-07
Iter: 742 loss: 4.0558507e-07
Iter: 743 loss: 4.0549952e-07
Iter: 744 loss: 4.05483831e-07
Iter: 745 loss: 4.05432985e-07
Iter: 746 loss: 4.05379211e-07
Iter: 747 loss: 4.058748e-07
Iter: 748 loss: 4.05341837e-07
Iter: 749 loss: 4.05298067e-07
Iter: 750 loss: 4.05456149e-07
Iter: 751 loss: 4.05274477e-07
Iter: 752 loss: 4.0528181e-07
Iter: 753 loss: 4.05234402e-07
Iter: 754 loss: 4.05255093e-07
Iter: 755 loss: 4.05276523e-07
Iter: 756 loss: 4.0524327e-07
Iter: 757 loss: 4.05262767e-07
Iter: 758 loss: 4.05263847e-07
Iter: 759 loss: 4.05260209e-07
Iter: 760 loss: 4.05260522e-07
Iter: 761 loss: 4.05292042e-07
Iter: 762 loss: 4.05268651e-07
Iter: 763 loss: 4.05281611e-07
Iter: 764 loss: 4.05272743e-07
Iter: 765 loss: 4.05269304e-07
Iter: 766 loss: 4.05266377e-07
Iter: 767 loss: 4.05288176e-07
Iter: 768 loss: 4.05275813e-07
Iter: 769 loss: 4.05272374e-07
Iter: 770 loss: 4.0527209e-07
Iter: 771 loss: 4.05273568e-07
Iter: 772 loss: 4.05274704e-07
Iter: 773 loss: 4.05274363e-07
Iter: 774 loss: 4.05275216e-07
Iter: 775 loss: 4.0527442e-07
Iter: 776 loss: 4.05275131e-07
Iter: 777 loss: 4.0527442e-07
Iter: 778 loss: 4.05204275e-07
Iter: 779 loss: 4.05563043e-07
Iter: 780 loss: 4.05189553e-07
Iter: 781 loss: 4.05140611e-07
Iter: 782 loss: 4.05150899e-07
Iter: 783 loss: 4.05134642e-07
Iter: 784 loss: 4.05067965e-07
Iter: 785 loss: 4.05237245e-07
Iter: 786 loss: 4.05046308e-07
Iter: 787 loss: 4.05002595e-07
Iter: 788 loss: 4.04987787e-07
Iter: 789 loss: 4.04950015e-07
Iter: 790 loss: 4.0490653e-07
Iter: 791 loss: 4.04953767e-07
Iter: 792 loss: 4.04875834e-07
Iter: 793 loss: 4.04818309e-07
Iter: 794 loss: 4.04939328e-07
Iter: 795 loss: 4.04773914e-07
Iter: 796 loss: 4.04738898e-07
Iter: 797 loss: 4.05232754e-07
Iter: 798 loss: 4.0472645e-07
Iter: 799 loss: 4.04655538e-07
Iter: 800 loss: 4.04643629e-07
Iter: 801 loss: 4.04615321e-07
Iter: 802 loss: 4.04592782e-07
Iter: 803 loss: 4.04504561e-07
Iter: 804 loss: 4.0449271e-07
Iter: 805 loss: 4.04410287e-07
Iter: 806 loss: 4.05196658e-07
Iter: 807 loss: 4.04384139e-07
Iter: 808 loss: 4.04366403e-07
Iter: 809 loss: 4.04317802e-07
Iter: 810 loss: 4.04303762e-07
Iter: 811 loss: 4.04197891e-07
Iter: 812 loss: 4.04757941e-07
Iter: 813 loss: 4.04198119e-07
Iter: 814 loss: 4.04111205e-07
Iter: 815 loss: 4.04218213e-07
Iter: 816 loss: 4.04082641e-07
Iter: 817 loss: 4.04017925e-07
Iter: 818 loss: 4.04760954e-07
Iter: 819 loss: 4.03999479e-07
Iter: 820 loss: 4.03958438e-07
Iter: 821 loss: 4.03998172e-07
Iter: 822 loss: 4.03952811e-07
Iter: 823 loss: 4.03895399e-07
Iter: 824 loss: 4.03971768e-07
Iter: 825 loss: 4.03862146e-07
Iter: 826 loss: 4.03767672e-07
Iter: 827 loss: 4.04317035e-07
Iter: 828 loss: 4.0375221e-07
Iter: 829 loss: 4.03749766e-07
Iter: 830 loss: 4.03724528e-07
Iter: 831 loss: 4.03688489e-07
Iter: 832 loss: 4.03599302e-07
Iter: 833 loss: 4.0399874e-07
Iter: 834 loss: 4.03618515e-07
Iter: 835 loss: 4.03539389e-07
Iter: 836 loss: 4.03768524e-07
Iter: 837 loss: 4.03494255e-07
Iter: 838 loss: 4.03460774e-07
Iter: 839 loss: 4.03420472e-07
Iter: 840 loss: 4.03419961e-07
Iter: 841 loss: 4.03377e-07
Iter: 842 loss: 4.03881756e-07
Iter: 843 loss: 4.03359365e-07
Iter: 844 loss: 4.03280637e-07
Iter: 845 loss: 4.0351523e-07
Iter: 846 loss: 4.0326e-07
Iter: 847 loss: 4.03223368e-07
Iter: 848 loss: 4.03473905e-07
Iter: 849 loss: 4.03167405e-07
Iter: 850 loss: 4.03127132e-07
Iter: 851 loss: 4.03232661e-07
Iter: 852 loss: 4.03093793e-07
Iter: 853 loss: 4.03051558e-07
Iter: 854 loss: 4.03859872e-07
Iter: 855 loss: 4.03054401e-07
Iter: 856 loss: 4.03001934e-07
Iter: 857 loss: 4.02956346e-07
Iter: 858 loss: 4.03807661e-07
Iter: 859 loss: 4.02951287e-07
Iter: 860 loss: 4.02842431e-07
Iter: 861 loss: 4.03218053e-07
Iter: 862 loss: 4.02849594e-07
Iter: 863 loss: 4.0276e-07
Iter: 864 loss: 4.03226323e-07
Iter: 865 loss: 4.02756683e-07
Iter: 866 loss: 4.02730819e-07
Iter: 867 loss: 4.0297607e-07
Iter: 868 loss: 4.02721412e-07
Iter: 869 loss: 4.02671162e-07
Iter: 870 loss: 4.02830608e-07
Iter: 871 loss: 4.02667183e-07
Iter: 872 loss: 4.02622504e-07
Iter: 873 loss: 4.02621282e-07
Iter: 874 loss: 4.0259647e-07
Iter: 875 loss: 4.02531896e-07
Iter: 876 loss: 4.02739516e-07
Iter: 877 loss: 4.02476161e-07
Iter: 878 loss: 4.02460387e-07
Iter: 879 loss: 4.02481447e-07
Iter: 880 loss: 4.02416561e-07
Iter: 881 loss: 4.02321973e-07
Iter: 882 loss: 4.02545879e-07
Iter: 883 loss: 4.02274225e-07
Iter: 884 loss: 4.02234292e-07
Iter: 885 loss: 4.02432363e-07
Iter: 886 loss: 4.02210844e-07
Iter: 887 loss: 4.02130837e-07
Iter: 888 loss: 4.02842545e-07
Iter: 889 loss: 4.02107844e-07
Iter: 890 loss: 4.02031048e-07
Iter: 891 loss: 4.02534454e-07
Iter: 892 loss: 4.02043725e-07
Iter: 893 loss: 4.02029173e-07
Iter: 894 loss: 4.01924439e-07
Iter: 895 loss: 4.03544504e-07
Iter: 896 loss: 4.01952036e-07
Iter: 897 loss: 4.01888542e-07
Iter: 898 loss: 4.02543549e-07
Iter: 899 loss: 4.01911592e-07
Iter: 900 loss: 4.01846052e-07
Iter: 901 loss: 4.02002286e-07
Iter: 902 loss: 4.01826469e-07
Iter: 903 loss: 4.01735e-07
Iter: 904 loss: 4.01923472e-07
Iter: 905 loss: 4.01739612e-07
Iter: 906 loss: 4.01693512e-07
Iter: 907 loss: 4.02016582e-07
Iter: 908 loss: 4.01660401e-07
Iter: 909 loss: 4.01644172e-07
Iter: 910 loss: 4.01627545e-07
Iter: 911 loss: 4.01614386e-07
Iter: 912 loss: 4.01535317e-07
Iter: 913 loss: 4.01613e-07
Iter: 914 loss: 4.01555042e-07
Iter: 915 loss: 4.01454429e-07
Iter: 916 loss: 4.01478189e-07
Iter: 917 loss: 4.01402275e-07
Iter: 918 loss: 4.01324286e-07
Iter: 919 loss: 4.01872455e-07
Iter: 920 loss: 4.01331363e-07
Iter: 921 loss: 4.01221257e-07
Iter: 922 loss: 4.01266334e-07
Iter: 923 loss: 4.01202414e-07
Iter: 924 loss: 4.01158275e-07
Iter: 925 loss: 4.01145343e-07
Iter: 926 loss: 4.01077301e-07
Iter: 927 loss: 4.01177061e-07
Iter: 928 loss: 4.01041e-07
Iter: 929 loss: 4.0103788e-07
Iter: 930 loss: 4.00926439e-07
Iter: 931 loss: 4.0092516e-07
Iter: 932 loss: 4.00879117e-07
Iter: 933 loss: 4.00879401e-07
Iter: 934 loss: 4.00825286e-07
Iter: 935 loss: 4.00972226e-07
Iter: 936 loss: 4.00785837e-07
Iter: 937 loss: 4.00776798e-07
Iter: 938 loss: 4.00800729e-07
Iter: 939 loss: 4.00714896e-07
Iter: 940 loss: 4.00646059e-07
Iter: 941 loss: 4.00843589e-07
Iter: 942 loss: 4.00661065e-07
Iter: 943 loss: 4.00606723e-07
Iter: 944 loss: 4.00744369e-07
Iter: 945 loss: 4.00573242e-07
Iter: 946 loss: 4.00519582e-07
Iter: 947 loss: 4.00527711e-07
Iter: 948 loss: 4.00475386e-07
Iter: 949 loss: 4.0037861e-07
Iter: 950 loss: 4.0044722e-07
Iter: 951 loss: 4.00390263e-07
Iter: 952 loss: 4.00279589e-07
Iter: 953 loss: 4.00753606e-07
Iter: 954 loss: 4.0027868e-07
Iter: 955 loss: 4.00224707e-07
Iter: 956 loss: 4.00418259e-07
Iter: 957 loss: 4.00245312e-07
Iter: 958 loss: 4.00138049e-07
Iter: 959 loss: 4.00462113e-07
Iter: 960 loss: 4.00140038e-07
Iter: 961 loss: 4.00053239e-07
Iter: 962 loss: 4.00089135e-07
Iter: 963 loss: 4.00049487e-07
Iter: 964 loss: 3.99977949e-07
Iter: 965 loss: 4.00085099e-07
Iter: 966 loss: 3.99930343e-07
Iter: 967 loss: 3.99910334e-07
Iter: 968 loss: 4.00094507e-07
Iter: 969 loss: 3.99876313e-07
Iter: 970 loss: 3.9981353e-07
Iter: 971 loss: 4.00004467e-07
Iter: 972 loss: 3.99763849e-07
Iter: 973 loss: 3.99707261e-07
Iter: 974 loss: 3.99971356e-07
Iter: 975 loss: 3.99745517e-07
Iter: 976 loss: 3.99593603e-07
Iter: 977 loss: 3.99687337e-07
Iter: 978 loss: 3.99610315e-07
Iter: 979 loss: 3.99544945e-07
Iter: 980 loss: 3.99690947e-07
Iter: 981 loss: 3.99526073e-07
Iter: 982 loss: 3.99443053e-07
Iter: 983 loss: 3.99406417e-07
Iter: 984 loss: 3.99397209e-07
Iter: 985 loss: 3.99282e-07
Iter: 986 loss: 4.00070775e-07
Iter: 987 loss: 3.99301541e-07
Iter: 988 loss: 3.99207835e-07
Iter: 989 loss: 3.99250382e-07
Iter: 990 loss: 3.99177509e-07
Iter: 991 loss: 3.99161792e-07
Iter: 992 loss: 3.99121575e-07
Iter: 993 loss: 3.99092528e-07
Iter: 994 loss: 3.99116288e-07
Iter: 995 loss: 3.99085536e-07
Iter: 996 loss: 3.99031336e-07
Iter: 997 loss: 3.99024657e-07
Iter: 998 loss: 3.98972531e-07
Iter: 999 loss: 3.98933224e-07
Iter: 1000 loss: 3.99403319e-07
Iter: 1001 loss: 3.98939932e-07
Iter: 1002 loss: 3.98883515e-07
Iter: 1003 loss: 3.98902728e-07
Iter: 1004 loss: 3.98834629e-07
Iter: 1005 loss: 3.98793418e-07
Iter: 1006 loss: 3.99081557e-07
Iter: 1007 loss: 3.9878438e-07
Iter: 1008 loss: 3.98793077e-07
Iter: 1009 loss: 3.98784721e-07
Iter: 1010 loss: 3.98771931e-07
Iter: 1011 loss: 3.98768748e-07
Iter: 1012 loss: 3.98787932e-07
Iter: 1013 loss: 3.9877969e-07
Iter: 1014 loss: 3.98789467e-07
Iter: 1015 loss: 3.98786653e-07
Iter: 1016 loss: 3.98773892e-07
Iter: 1017 loss: 3.98791258e-07
Iter: 1018 loss: 3.98779662e-07
Iter: 1019 loss: 3.98768577e-07
Iter: 1020 loss: 3.98787392e-07
Iter: 1021 loss: 3.98772841e-07
Iter: 1022 loss: 3.98779349e-07
Iter: 1023 loss: 3.98788245e-07
Iter: 1024 loss: 3.9878546e-07
Iter: 1025 loss: 3.98784323e-07
Iter: 1026 loss: 3.98786653e-07
Iter: 1027 loss: 3.98786284e-07
Iter: 1028 loss: 3.98785943e-07
Iter: 1029 loss: 3.98784465e-07
Iter: 1030 loss: 3.9878509e-07
Iter: 1031 loss: 3.98784465e-07
Iter: 1032 loss: 3.98784465e-07
Iter: 1033 loss: 3.9878509e-07
Iter: 1034 loss: 3.98700934e-07
Iter: 1035 loss: 3.99112764e-07
Iter: 1036 loss: 3.98669812e-07
Iter: 1037 loss: 3.98625218e-07
Iter: 1038 loss: 3.98659154e-07
Iter: 1039 loss: 3.9860609e-07
Iter: 1040 loss: 3.98495672e-07
Iter: 1041 loss: 3.98891018e-07
Iter: 1042 loss: 3.98479699e-07
Iter: 1043 loss: 3.98457757e-07
Iter: 1044 loss: 3.9838244e-07
Iter: 1045 loss: 3.98338329e-07
Iter: 1046 loss: 3.98288165e-07
Iter: 1047 loss: 3.98674587e-07
Iter: 1048 loss: 3.98306668e-07
Iter: 1049 loss: 3.98228451e-07
Iter: 1050 loss: 3.98283248e-07
Iter: 1051 loss: 3.9814222e-07
Iter: 1052 loss: 3.98078896e-07
Iter: 1053 loss: 3.98166719e-07
Iter: 1054 loss: 3.98058802e-07
Iter: 1055 loss: 3.97967881e-07
Iter: 1056 loss: 3.98126218e-07
Iter: 1057 loss: 3.97971633e-07
Iter: 1058 loss: 3.9784095e-07
Iter: 1059 loss: 3.98028334e-07
Iter: 1060 loss: 3.9787858e-07
Iter: 1061 loss: 3.97772908e-07
Iter: 1062 loss: 3.9827512e-07
Iter: 1063 loss: 3.97727661e-07
Iter: 1064 loss: 3.97644101e-07
Iter: 1065 loss: 3.97656919e-07
Iter: 1066 loss: 3.97615679e-07
Iter: 1067 loss: 3.97501481e-07
Iter: 1068 loss: 3.98073411e-07
Iter: 1069 loss: 3.9745052e-07
Iter: 1070 loss: 3.97371963e-07
Iter: 1071 loss: 3.97423236e-07
Iter: 1072 loss: 3.97347094e-07
Iter: 1073 loss: 3.97309122e-07
Iter: 1074 loss: 3.97680282e-07
Iter: 1075 loss: 3.9726e-07
Iter: 1076 loss: 3.97226131e-07
Iter: 1077 loss: 3.97301221e-07
Iter: 1078 loss: 3.97186227e-07
Iter: 1079 loss: 3.97040253e-07
Iter: 1080 loss: 3.97160818e-07
Iter: 1081 loss: 3.97018482e-07
Iter: 1082 loss: 3.96984177e-07
Iter: 1083 loss: 3.96988185e-07
Iter: 1084 loss: 3.96933103e-07
Iter: 1085 loss: 3.96960047e-07
Iter: 1086 loss: 3.96880324e-07
Iter: 1087 loss: 3.96871542e-07
Iter: 1088 loss: 3.96816802e-07
Iter: 1089 loss: 3.96775079e-07
Iter: 1090 loss: 3.96727899e-07
Iter: 1091 loss: 3.97339136e-07
Iter: 1092 loss: 3.96716644e-07
Iter: 1093 loss: 3.96606794e-07
Iter: 1094 loss: 3.96850965e-07
Iter: 1095 loss: 3.96637915e-07
Iter: 1096 loss: 3.96561319e-07
Iter: 1097 loss: 3.96857871e-07
Iter: 1098 loss: 3.9654833e-07
Iter: 1099 loss: 3.96472387e-07
Iter: 1100 loss: 3.96430323e-07
Iter: 1101 loss: 3.96437372e-07
Iter: 1102 loss: 3.96323145e-07
Iter: 1103 loss: 3.96766922e-07
Iter: 1104 loss: 3.9629424e-07
Iter: 1105 loss: 3.96253739e-07
Iter: 1106 loss: 3.9625283e-07
Iter: 1107 loss: 3.96209117e-07
Iter: 1108 loss: 3.96097107e-07
Iter: 1109 loss: 3.96270536e-07
Iter: 1110 loss: 3.96049472e-07
Iter: 1111 loss: 3.95970574e-07
Iter: 1112 loss: 3.9654725e-07
Iter: 1113 loss: 3.9599135e-07
Iter: 1114 loss: 3.9593553e-07
Iter: 1115 loss: 3.95951616e-07
Iter: 1116 loss: 3.95895938e-07
Iter: 1117 loss: 3.95839379e-07
Iter: 1118 loss: 3.95851117e-07
Iter: 1119 loss: 3.95790607e-07
Iter: 1120 loss: 3.95722225e-07
Iter: 1121 loss: 3.95705058e-07
Iter: 1122 loss: 3.95664443e-07
Iter: 1123 loss: 3.95641905e-07
Iter: 1124 loss: 3.95625932e-07
Iter: 1125 loss: 3.95505708e-07
Iter: 1126 loss: 3.95478821e-07
Iter: 1127 loss: 3.95448865e-07
Iter: 1128 loss: 3.95432494e-07
Iter: 1129 loss: 3.95394665e-07
Iter: 1130 loss: 3.95337793e-07
Iter: 1131 loss: 3.95220184e-07
Iter: 1132 loss: 3.95221434e-07
Iter: 1133 loss: 3.95194e-07
Iter: 1134 loss: 3.95208531e-07
Iter: 1135 loss: 3.95114569e-07
Iter: 1136 loss: 3.9526725e-07
Iter: 1137 loss: 3.95132361e-07
Iter: 1138 loss: 3.95099534e-07
Iter: 1139 loss: 3.95004804e-07
Iter: 1140 loss: 3.95030042e-07
Iter: 1141 loss: 3.94978372e-07
Iter: 1142 loss: 3.95765085e-07
Iter: 1143 loss: 3.94949922e-07
Iter: 1144 loss: 3.94922239e-07
Iter: 1145 loss: 3.94994402e-07
Iter: 1146 loss: 3.94909307e-07
Iter: 1147 loss: 3.94854311e-07
Iter: 1148 loss: 3.94884e-07
Iter: 1149 loss: 3.94830067e-07
Iter: 1150 loss: 3.94739516e-07
Iter: 1151 loss: 3.95197105e-07
Iter: 1152 loss: 3.94723713e-07
Iter: 1153 loss: 3.94660645e-07
Iter: 1154 loss: 3.94695746e-07
Iter: 1155 loss: 3.94616109e-07
Iter: 1156 loss: 3.94544855e-07
Iter: 1157 loss: 3.94543775e-07
Iter: 1158 loss: 3.94524193e-07
Iter: 1159 loss: 3.94420852e-07
Iter: 1160 loss: 3.94381516e-07
Iter: 1161 loss: 3.94337178e-07
Iter: 1162 loss: 3.94248161e-07
Iter: 1163 loss: 3.94967401e-07
Iter: 1164 loss: 3.94214283e-07
Iter: 1165 loss: 3.94163067e-07
Iter: 1166 loss: 3.94992895e-07
Iter: 1167 loss: 3.94190351e-07
Iter: 1168 loss: 3.94097299e-07
Iter: 1169 loss: 3.9404398e-07
Iter: 1170 loss: 3.93998903e-07
Iter: 1171 loss: 3.9396744e-07
Iter: 1172 loss: 3.94303385e-07
Iter: 1173 loss: 3.93959454e-07
Iter: 1174 loss: 3.93895732e-07
Iter: 1175 loss: 3.9414229e-07
Iter: 1176 loss: 3.93866259e-07
Iter: 1177 loss: 3.9384463e-07
Iter: 1178 loss: 3.93756238e-07
Iter: 1179 loss: 3.93795375e-07
Iter: 1180 loss: 3.93733671e-07
Iter: 1181 loss: 3.93722786e-07
Iter: 1182 loss: 3.93705278e-07
Iter: 1183 loss: 3.9368436e-07
Iter: 1184 loss: 3.93681802e-07
Iter: 1185 loss: 3.93598839e-07
Iter: 1186 loss: 3.93753339e-07
Iter: 1187 loss: 3.93618876e-07
Iter: 1188 loss: 3.93522356e-07
Iter: 1189 loss: 3.93516643e-07
Iter: 1190 loss: 3.93534208e-07
Iter: 1191 loss: 3.93523123e-07
Iter: 1192 loss: 3.93548731e-07
Iter: 1193 loss: 3.93540461e-07
Iter: 1194 loss: 3.93529831e-07
Iter: 1195 loss: 3.93528097e-07
Iter: 1196 loss: 3.93546486e-07
Iter: 1197 loss: 3.93545974e-07
Iter: 1198 loss: 3.93537135e-07
Iter: 1199 loss: 3.93536737e-07
Iter: 1200 loss: 3.9353381e-07
Iter: 1201 loss: 3.9351329e-07
Iter: 1202 loss: 3.93522555e-07
Iter: 1203 loss: 3.93519315e-07
Iter: 1204 loss: 3.93518405e-07
Iter: 1205 loss: 3.9351994e-07
Iter: 1206 loss: 3.93516189e-07
Iter: 1207 loss: 3.93520764e-07
Iter: 1208 loss: 3.93516785e-07
Iter: 1209 loss: 3.93520594e-07
Iter: 1210 loss: 3.93517581e-07
Iter: 1211 loss: 3.93516814e-07
Iter: 1212 loss: 3.93517581e-07
Iter: 1213 loss: 3.93516785e-07
Iter: 1214 loss: 3.93517581e-07
Iter: 1215 loss: 3.93516785e-07
Iter: 1216 loss: 3.93516785e-07
Iter: 1217 loss: 3.93516785e-07
Iter: 1218 loss: 3.93517581e-07
Iter: 1219 loss: 3.93517581e-07
Iter: 1220 loss: 3.93517581e-07
Iter: 1221 loss: 3.93517581e-07
Iter: 1222 loss: 3.93517581e-07
Iter: 1223 loss: 3.93516785e-07
Iter: 1224 loss: 3.93516785e-07
Iter: 1225 loss: 3.93517581e-07
Iter: 1226 loss: 1.02974536e-06
Iter: 1227 loss: 3.93541029e-07
Iter: 1228 loss: 3.93534918e-07
Iter: 1229 loss: 3.93522015e-07
Iter: 1230 loss: 3.935329e-07
Iter: 1231 loss: 3.9354407e-07
Iter: 1232 loss: 3.93540319e-07
Iter: 1233 loss: 3.93559105e-07
Iter: 1234 loss: 3.93540319e-07
Iter: 1235 loss: 3.93542223e-07
Iter: 1236 loss: 3.93538073e-07
Iter: 1237 loss: 3.9351778e-07
Iter: 1238 loss: 3.93527472e-07
Iter: 1239 loss: 3.93532e-07
Iter: 1240 loss: 3.93529916e-07
Iter: 1241 loss: 3.93527387e-07
Iter: 1242 loss: 3.93515563e-07
Iter: 1243 loss: 3.93523777e-07
Iter: 1244 loss: 3.93519088e-07
Iter: 1245 loss: 3.93521e-07
Iter: 1246 loss: 3.93518377e-07
Iter: 1247 loss: 3.93511755e-07
Iter: 1248 loss: 3.93514455e-07
Iter: 1249 loss: 3.93515421e-07
Iter: 1250 loss: 3.93517212e-07
Iter: 1251 loss: 3.93513687e-07
Iter: 1252 loss: 3.93514568e-07
Iter: 1253 loss: 3.93515791e-07
Iter: 1254 loss: 3.9351616e-07
Iter: 1255 loss: 3.93517155e-07
Iter: 1256 loss: 3.93517155e-07
Iter: 1257 loss: 3.93517155e-07
Iter: 1258 loss: 3.93517212e-07
Iter: 1259 loss: 3.93517212e-07
Iter: 1260 loss: 3.93517155e-07
Iter: 1261 loss: 3.93517212e-07
Iter: 1262 loss: 3.93517155e-07
Iter: 1263 loss: 3.93517212e-07
Iter: 1264 loss: 3.93517212e-07
Iter: 1265 loss: 3.93517155e-07
Iter: 1266 loss: 3.93517155e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2
+ date
Thu Oct 22 05:48:10 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/500_500_500_500_1 --function f1 --psi 0 --phi 2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbbd81439d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbbd8166840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbbd81e07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbbd80d6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbbd80d67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbbd809f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbbd81e0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbbc002f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbbc002fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbbc002f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbbd8028268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb805d6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb805e76a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb804e8048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb804f5d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb805681e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb80568e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb805e78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb804f9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb804e8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb8042b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb8044a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb8042bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb803c0a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb80486488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb804a6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb804a6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb8039c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb8039c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb80331950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb8035c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb80250620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb803ec620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb80241d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb802c50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbb802b4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.050235238
test_loss: 0.04886186
train_loss: 0.010884598
test_loss: 0.01060735
train_loss: 0.0052700625
test_loss: 0.004506449
train_loss: 0.003465233
test_loss: 0.004029825
train_loss: 0.0027652918
test_loss: 0.0027467483
train_loss: 0.0030839932
test_loss: 0.0025455158
train_loss: 0.0030823397
test_loss: 0.0022353642
train_loss: 0.0030984369
test_loss: 0.0022042915
train_loss: 0.0033824998
test_loss: 0.0028348686
train_loss: 0.003001571
test_loss: 0.0022380268
train_loss: 0.0020260606
test_loss: 0.0018976355
train_loss: 0.0022264768
test_loss: 0.0026467913
train_loss: 0.0020980297
test_loss: 0.001898952
train_loss: 0.0019713251
test_loss: 0.0019410673
train_loss: 0.0018524338
test_loss: 0.0018397046
train_loss: 0.0020319752
test_loss: 0.001896291
train_loss: 0.0018794034
test_loss: 0.0018724961
train_loss: 0.0016571996
test_loss: 0.002810398
train_loss: 0.0022062329
test_loss: 0.0018626491
train_loss: 0.0016318713
test_loss: 0.0020336162
train_loss: 0.002339267
test_loss: 0.0019574934
train_loss: 0.0020273197
test_loss: 0.0020999755
train_loss: 0.0022558337
test_loss: 0.002642575
train_loss: 0.0018851415
test_loss: 0.0021468578
train_loss: 0.0017446941
test_loss: 0.0017499274
train_loss: 0.002075382
test_loss: 0.0018778056
train_loss: 0.0018176365
test_loss: 0.0017591665
train_loss: 0.0018200155
test_loss: 0.0016479646
train_loss: 0.0016687022
test_loss: 0.0017186827
train_loss: 0.0017910891
test_loss: 0.001889909
train_loss: 0.0017570783
test_loss: 0.0020826405
train_loss: 0.0016820033
test_loss: 0.0019118276
train_loss: 0.0019231152
test_loss: 0.0027273053
train_loss: 0.0016976573
test_loss: 0.0015753311
train_loss: 0.0024276925
test_loss: 0.0017027293
train_loss: 0.0017776344
test_loss: 0.001926968
train_loss: 0.0019431557
test_loss: 0.002366981
train_loss: 0.0015375082
test_loss: 0.001644593
train_loss: 0.0017722852
test_loss: 0.0015892771
train_loss: 0.0016264018
test_loss: 0.0016822583
train_loss: 0.0020086973
test_loss: 0.002442771
train_loss: 0.0015228544
test_loss: 0.00206645
train_loss: 0.001784052
test_loss: 0.0019257964
train_loss: 0.0019191118
test_loss: 0.0018197418
train_loss: 0.001877389
test_loss: 0.0018816402
train_loss: 0.0022915856
test_loss: 0.002032997
train_loss: 0.0032028686
test_loss: 0.0020213365
train_loss: 0.0024527945
test_loss: 0.0022943234
train_loss: 0.0023257053
test_loss: 0.0019184671
train_loss: 0.0035495916
test_loss: 0.0032453781
train_loss: 0.0030024764
test_loss: 0.0022417733
train_loss: 0.0031223919
test_loss: 0.002561633
train_loss: 0.002817519
test_loss: 0.002731215
train_loss: 0.0031688616
test_loss: 0.002726955
train_loss: 0.0027079354
test_loss: 0.001978134
train_loss: 0.0029072715
test_loss: 0.0023671659
train_loss: 0.0026902258
test_loss: 0.0028176394
train_loss: 0.0015432506
test_loss: 0.0016415422
train_loss: 0.0017723349
test_loss: 0.0018675532
train_loss: 0.0019014373
test_loss: 0.0016362747
train_loss: 0.001871021
test_loss: 0.0018953437
train_loss: 0.0015304537
test_loss: 0.0016424658
train_loss: 0.0014981974
test_loss: 0.0019051802
train_loss: 0.0018310896
test_loss: 0.0018695056
train_loss: 0.0018914063
test_loss: 0.0016449377
train_loss: 0.0016230883
test_loss: 0.0017924124
train_loss: 0.0016758713
test_loss: 0.0019515541
train_loss: 0.001761492
test_loss: 0.0018065837
train_loss: 0.0019197663
test_loss: 0.0017204463
train_loss: 0.0019118435
test_loss: 0.0021610537
train_loss: 0.0018276583
test_loss: 0.0018515616
train_loss: 0.0015460894
test_loss: 0.001841151
train_loss: 0.002007624
test_loss: 0.0019341517
train_loss: 0.0020111867
test_loss: 0.0017940593
train_loss: 0.0018058175
test_loss: 0.0016534076
train_loss: 0.0019019691
test_loss: 0.001661258
train_loss: 0.0020880313
test_loss: 0.0019370028
train_loss: 0.0016246841
test_loss: 0.0016352981
train_loss: 0.0016788357
test_loss: 0.0017183955
train_loss: 0.0016073027
test_loss: 0.0017344815
train_loss: 0.0016846008
test_loss: 0.0014446657
train_loss: 0.0014242398
test_loss: 0.0014245757
train_loss: 0.0016076856
test_loss: 0.0016043426
train_loss: 0.0016408863
test_loss: 0.0020016995
train_loss: 0.0017186161
test_loss: 0.0019059213
train_loss: 0.001791779
test_loss: 0.001798619
train_loss: 0.0017170265
test_loss: 0.0015884155
train_loss: 0.0017470198
test_loss: 0.0016454294
train_loss: 0.0018364198
test_loss: 0.0019738069
train_loss: 0.0017516362
test_loss: 0.0017767317
train_loss: 0.001582175
test_loss: 0.0016874502
train_loss: 0.0017094571
test_loss: 0.0020593763
train_loss: 0.0022372243
test_loss: 0.0014488434
train_loss: 0.0032060663
test_loss: 0.002818497
train_loss: 0.00281716
test_loss: 0.0030340997
train_loss: 0.002003088
test_loss: 0.0023216554
train_loss: 0.0015832742
test_loss: 0.001815475
train_loss: 0.0014448642
test_loss: 0.0014337873
train_loss: 0.0018676694
test_loss: 0.0019367624
train_loss: 0.001674815
test_loss: 0.0016735992
train_loss: 0.0017199392
test_loss: 0.0015815652
train_loss: 0.0015086473
test_loss: 0.0019260522
train_loss: 0.0020539265
test_loss: 0.0018245849
train_loss: 0.0019044619
test_loss: 0.0017151071
train_loss: 0.0022014296
test_loss: 0.001698042
train_loss: 0.0022636403
test_loss: 0.0020300054
train_loss: 0.0015932523
test_loss: 0.0018999872
train_loss: 0.0021906658
test_loss: 0.0018525764
train_loss: 0.0016260156
test_loss: 0.0017538838
train_loss: 0.0018689062
test_loss: 0.0023085154
train_loss: 0.0016575889
test_loss: 0.0017492035
train_loss: 0.0016782389
test_loss: 0.001881931
train_loss: 0.0017620595
test_loss: 0.0014929868
train_loss: 0.0016121723
test_loss: 0.0018146321
train_loss: 0.0017936936
test_loss: 0.0016747292
train_loss: 0.0016243623
test_loss: 0.0019027727
train_loss: 0.001797585
test_loss: 0.001739693
train_loss: 0.001697141
test_loss: 0.0022990233
train_loss: 0.0025517896
test_loss: 0.0022079719
train_loss: 0.0026271276
test_loss: 0.0033765002
train_loss: 0.002356321
test_loss: 0.0032322428
train_loss: 0.0026423777
test_loss: 0.0028716396
train_loss: 0.0021046724
test_loss: 0.0027198093
train_loss: 0.0024508322
test_loss: 0.0030188195
train_loss: 0.0018831366
test_loss: 0.0025186795
train_loss: 0.0019086178
test_loss: 0.0017489999
train_loss: 0.0017628702
test_loss: 0.0017312764
train_loss: 0.0020525348
test_loss: 0.0019571723
train_loss: 0.0015095706
test_loss: 0.0018915929
train_loss: 0.0015865221
test_loss: 0.0018974713
train_loss: 0.0020185232
test_loss: 0.0017490234
train_loss: 0.0016094015
test_loss: 0.0015949255
train_loss: 0.001964748
test_loss: 0.0017927795
train_loss: 0.0018326694
test_loss: 0.0019133907
train_loss: 0.001576111
test_loss: 0.00161243
train_loss: 0.0013347646
test_loss: 0.0016713719
train_loss: 0.0015232491
test_loss: 0.0020353077
train_loss: 0.0015405407
test_loss: 0.0016731177
train_loss: 0.002098806
test_loss: 0.002092543
train_loss: 0.0015257745
test_loss: 0.0016732687
train_loss: 0.0016970948
test_loss: 0.0016923101
train_loss: 0.0018106842
test_loss: 0.001740723
train_loss: 0.0013823372
test_loss: 0.0014807879
train_loss: 0.0018014578
test_loss: 0.0016929066
train_loss: 0.0018623607
test_loss: 0.0020676423
train_loss: 0.0014890269
test_loss: 0.001574704
train_loss: 0.002292629
test_loss: 0.0016659941
train_loss: 0.0018394345
test_loss: 0.001864914
train_loss: 0.0013097032
test_loss: 0.0014312267
train_loss: 0.002193896
test_loss: 0.0019092739
train_loss: 0.0018916646
test_loss: 0.0020686528
train_loss: 0.0019800873
test_loss: 0.0017577661
train_loss: 0.001773796
test_loss: 0.0014322234
train_loss: 0.0018313003
test_loss: 0.0018441606
train_loss: 0.0022769766
test_loss: 0.0030787236
train_loss: 0.0022351353
test_loss: 0.0024104991
train_loss: 0.0026860968
test_loss: 0.003100973
train_loss: 0.0029805433
test_loss: 0.0027482165
train_loss: 0.0019887635
test_loss: 0.002903435
train_loss: 0.0017015869
test_loss: 0.0021476285
train_loss: 0.0016075999
test_loss: 0.0017367562
train_loss: 0.0016149022
test_loss: 0.0020474412
train_loss: 0.0013584829
test_loss: 0.001664688
train_loss: 0.0016434996
test_loss: 0.0018762866
train_loss: 0.0016665484
test_loss: 0.0018118165
train_loss: 0.0017871044
test_loss: 0.0020507777
train_loss: 0.0019756975
test_loss: 0.0017563326
train_loss: 0.0015000934
test_loss: 0.0015678188
train_loss: 0.0016300601
test_loss: 0.0015203399
train_loss: 0.0017359495
test_loss: 0.0016068349
train_loss: 0.0023125021
test_loss: 0.001894527
train_loss: 0.0018255839
test_loss: 0.0024006942
train_loss: 0.0022135666
test_loss: 0.0023398332
train_loss: 0.0017946269
test_loss: 0.001592869
train_loss: 0.0017481358
test_loss: 0.0017735957
train_loss: 0.00141457
test_loss: 0.001702425
train_loss: 0.0015550538
test_loss: 0.0015270283
train_loss: 0.0015588319
test_loss: 0.0015221428
train_loss: 0.0016350539
test_loss: 0.0020547172
train_loss: 0.0016777287
test_loss: 0.0015752777
train_loss: 0.0015478042
test_loss: 0.0019263293
train_loss: 0.0015680725
test_loss: 0.0018774514
train_loss: 0.0016438521
test_loss: 0.0019094164
train_loss: 0.0018432715
test_loss: 0.0015632528
train_loss: 0.0016028613
test_loss: 0.0020490005
train_loss: 0.001628818
test_loss: 0.002240644
train_loss: 0.0016278761
test_loss: 0.0018542387
train_loss: 0.0019167203
test_loss: 0.0019201298
train_loss: 0.0018570072
test_loss: 0.0018141856
train_loss: 0.0015636622
test_loss: 0.0017010015
train_loss: 0.0015405759
test_loss: 0.0018439275
train_loss: 0.0019914368
test_loss: 0.0018968397
train_loss: 0.0016390756
test_loss: 0.0014596708
train_loss: 0.0015859845
test_loss: 0.0015072279
train_loss: 0.0019993822
test_loss: 0.0017218826
train_loss: 0.0026807976
test_loss: 0.0020661606
train_loss: 0.0020745145
test_loss: 0.0028195395
train_loss: 0.0017960251
test_loss: 0.00303591
train_loss: 0.0023352532
test_loss: 0.0028238369
train_loss: 0.002805881
test_loss: 0.003353619
train_loss: 0.0021462478
test_loss: 0.002027143
train_loss: 0.001987417
test_loss: 0.0015384032
train_loss: 0.0021690093
test_loss: 0.0021661308
train_loss: 0.0012998007
test_loss: 0.0014095987
train_loss: 0.001513584
test_loss: 0.0016635048
train_loss: 0.0015973166
test_loss: 0.0015019532
train_loss: 0.0016593973
test_loss: 0.0016610586
train_loss: 0.0014452067
test_loss: 0.0016253348
train_loss: 0.001828023
test_loss: 0.0016242033
train_loss: 0.0015729001
test_loss: 0.0017836614
train_loss: 0.0017060618
test_loss: 0.001618038
train_loss: 0.0013332713
test_loss: 0.0019386373
train_loss: 0.0015988997
test_loss: 0.0017363476
train_loss: 0.0018591232
test_loss: 0.0016039198
train_loss: 0.0020618052
test_loss: 0.0016231842
train_loss: 0.0018219128
test_loss: 0.0019034898
train_loss: 0.0018774297
test_loss: 0.0015490234
train_loss: 0.0020204876
test_loss: 0.0015786772
train_loss: 0.0020059699
test_loss: 0.0014692252
train_loss: 0.0014903451
test_loss: 0.0016953055
train_loss: 0.0015569404
test_loss: 0.0014884237
train_loss: 0.0017240935
test_loss: 0.0015032899
train_loss: 0.0018558197
test_loss: 0.0017228145
train_loss: 0.0019576414
test_loss: 0.0022304377
train_loss: 0.002702011
test_loss: 0.0020941044
train_loss: 0.0027258296
test_loss: 0.0021988137
train_loss: 0.0025019613
test_loss: 0.0022336342
train_loss: 0.0024851118
test_loss: 0.0024809167
train_loss: 0.0028498438
test_loss: 0.0027480533
train_loss: 0.0017870876
test_loss: 0.0016772193
train_loss: 0.0015783387
test_loss: 0.0018176637
train_loss: 0.0016410762
test_loss: 0.0015545791
train_loss: 0.0018503711
test_loss: 0.0016990558
train_loss: 0.0015376465
test_loss: 0.0015736817
train_loss: 0.0017562811
test_loss: 0.0014929567
train_loss: 0.0016756495
test_loss: 0.0014092831
train_loss: 0.0015339127
test_loss: 0.0016001082
train_loss: 0.0016875573
test_loss: 0.0019205658
train_loss: 0.001523973
test_loss: 0.0019786058
train_loss: 0.0015219457
test_loss: 0.0015273809
train_loss: 0.0018688568
test_loss: 0.0015016892
train_loss: 0.0013343202
test_loss: 0.0019406732
train_loss: 0.0016998225
test_loss: 0.0014686037
train_loss: 0.0015224061
test_loss: 0.001961141
train_loss: 0.0018649717
test_loss: 0.0014860798
train_loss: 0.0015320028
test_loss: 0.0014788714
train_loss: 0.0019063986
test_loss: 0.0016268032
train_loss: 0.0015166923
test_loss: 0.0017243857
train_loss: 0.0019857937
test_loss: 0.0017638948
train_loss: 0.0015454682
test_loss: 0.0020115217
train_loss: 0.0014122592
test_loss: 0.0016388817
train_loss: 0.0014990754
test_loss: 0.0015451593
train_loss: 0.0015986573
test_loss: 0.0014464927
train_loss: 0.0015317001
test_loss: 0.002402515
train_loss: 0.0016953942
test_loss: 0.0016391701
train_loss: 0.0019809036
test_loss: 0.0017924856
train_loss: 0.0014394161
test_loss: 0.0015494464
train_loss: 0.0019550389
test_loss: 0.0018291218
train_loss: 0.0017791456
test_loss: 0.0016004118
train_loss: 0.0015544414
test_loss: 0.0014987168
train_loss: 0.0019296906
test_loss: 0.001525999
train_loss: 0.0015538126
test_loss: 0.0014919695
train_loss: 0.0015256412
test_loss: 0.0018881402
train_loss: 0.001471916
test_loss: 0.0016739173
train_loss: 0.0018947575
test_loss: 0.0017885487
train_loss: 0.0014261499
test_loss: 0.0017128137
train_loss: 0.0014621944
test_loss: 0.001717732
train_loss: 0.001706324
test_loss: 0.0016777474
train_loss: 0.0013295255
test_loss: 0.0016609161
train_loss: 0.0017427823
test_loss: 0.0015267051
train_loss: 0.0018458227
test_loss: 0.0014619386
train_loss: 0.0018730056
test_loss: 0.001500728
train_loss: 0.0017279632
test_loss: 0.0014193982
train_loss: 0.0017342736
test_loss: 0.0019053662
train_loss: 0.0020070889
test_loss: 0.0018416563
train_loss: 0.0020125825
test_loss: 0.0021601124
train_loss: 0.001894434
test_loss: 0.0016428453
train_loss: 0.0018180081
test_loss: 0.0015016813
train_loss: 0.0015969854
test_loss: 0.0015593967
train_loss: 0.0016317638
test_loss: 0.0016412145
train_loss: 0.0016261193
test_loss: 0.0017004512
train_loss: 0.0013264273
test_loss: 0.0014608316
train_loss: 0.0016641455
test_loss: 0.0015478317
train_loss: 0.001917691
test_loss: 0.0017605749
train_loss: 0.001657092
test_loss: 0.0018953318
train_loss: 0.0016520561
test_loss: 0.0017259609
train_loss: 0.0016829004
test_loss: 0.0016002757
train_loss: 0.0018934285
test_loss: 0.002028572
train_loss: 0.002423624
test_loss: 0.0020875544
train_loss: 0.0019864514
test_loss: 0.0016261828
train_loss: 0.0015684534
test_loss: 0.0018465403
train_loss: 0.0019282185
test_loss: 0.0018428706
train_loss: 0.001245319
test_loss: 0.0013121802
train_loss: 0.001594303
test_loss: 0.0015203734
train_loss: 0.0015216323
test_loss: 0.0015297302
train_loss: 0.0016304625
test_loss: 0.0016288974
train_loss: 0.001832987
test_loss: 0.0014649905
train_loss: 0.0016782503
test_loss: 0.0018037689
train_loss: 0.0019219675
test_loss: 0.0015558169
train_loss: 0.00172257
test_loss: 0.0016685394
train_loss: 0.0018673618
test_loss: 0.0028282658
train_loss: 0.0025746804
test_loss: 0.0031611049
train_loss: 0.002926108
test_loss: 0.0023853742
train_loss: 0.0026163894
test_loss: 0.0024896753
train_loss: 0.0020436898
test_loss: 0.0028100766
train_loss: 0.0022961458
test_loss: 0.0029591806
train_loss: 0.0024301868
test_loss: 0.0026214318
train_loss: 0.0029615418
test_loss: 0.0024060365
train_loss: 0.002267638
test_loss: 0.002520595
train_loss: 0.0017204541
test_loss: 0.0026702958
train_loss: 0.002469657
test_loss: 0.0026646778
train_loss: 0.0017367194
test_loss: 0.0017771241
train_loss: 0.0018489553
test_loss: 0.0018619653
train_loss: 0.001659238
test_loss: 0.0015620068
train_loss: 0.0016685005
test_loss: 0.0014031744
train_loss: 0.0019905772
test_loss: 0.0018223291
train_loss: 0.001701243
test_loss: 0.0015076359
train_loss: 0.0015122587
test_loss: 0.0015998889
train_loss: 0.0014839211
test_loss: 0.0014420924
train_loss: 0.0019180836
test_loss: 0.0017405078
train_loss: 0.0015816658
test_loss: 0.0017061132
train_loss: 0.0016053308
test_loss: 0.0015461404
train_loss: 0.0020377256
test_loss: 0.0017844861
train_loss: 0.0016283371
test_loss: 0.00160933
train_loss: 0.0015420666
test_loss: 0.0016907199
train_loss: 0.0013719702
test_loss: 0.0014503619
train_loss: 0.0018738714
test_loss: 0.0016901873
train_loss: 0.0017776383
test_loss: 0.0015410299
train_loss: 0.0016789676
test_loss: 0.0017053833
train_loss: 0.001542613
test_loss: 0.0016315748
train_loss: 0.0019694637
test_loss: 0.001596947
train_loss: 0.002069812
test_loss: 0.002260923
train_loss: 0.001877582
test_loss: 0.0020165248
train_loss: 0.0018194149
test_loss: 0.0015922008
train_loss: 0.0016079324
test_loss: 0.0014393276
train_loss: 0.0014992557
test_loss: 0.0015060744
train_loss: 0.0019414383
test_loss: 0.0017380995/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

train_loss: 0.0016026841
test_loss: 0.0017810608
train_loss: 0.0015561532
test_loss: 0.0015145702
train_loss: 0.0016997629
test_loss: 0.0016245992
train_loss: 0.002068162
test_loss: 0.0014573339
train_loss: 0.001422954
test_loss: 0.001455934
train_loss: 0.0017165605
test_loss: 0.0014853902
train_loss: 0.00183004
test_loss: 0.0015572459
train_loss: 0.0016840477
test_loss: 0.0014946552
train_loss: 0.0012843057
test_loss: 0.0016901885
train_loss: 0.0020834405
test_loss: 0.0019509485
train_loss: 0.0018026972
test_loss: 0.0016370543
train_loss: 0.0022858153
test_loss: 0.0015049399
train_loss: 0.0018520439
test_loss: 0.0019563076
train_loss: 0.0018342095
test_loss: 0.0017500746
train_loss: 0.0017589917
test_loss: 0.0017902757
train_loss: 0.0014686208
test_loss: 0.0013378571
train_loss: 0.0017060689
test_loss: 0.0014380227
train_loss: 0.0015066081
test_loss: 0.0015650963
train_loss: 0.0020734114
test_loss: 0.0015740569
train_loss: 0.001769946
test_loss: 0.0018273384
train_loss: 0.0018169271
test_loss: 0.0019640198
train_loss: 0.0016145726
test_loss: 0.0014544625
train_loss: 0.0015306447
test_loss: 0.0013811688
train_loss: 0.0014239396
test_loss: 0.0015550466
train_loss: 0.0015545902
test_loss: 0.0014884005
train_loss: 0.0015704263
test_loss: 0.0016884643
train_loss: 0.0016770295
test_loss: 0.0015566723
train_loss: 0.0016976321
test_loss: 0.0015082029
train_loss: 0.0015377942
test_loss: 0.0016034502
train_loss: 0.0015757547
test_loss: 0.00205893
train_loss: 0.0014605522
test_loss: 0.0016311962
train_loss: 0.001845725
test_loss: 0.0016953286
train_loss: 0.0013052648
test_loss: 0.0013159289
train_loss: 0.001503202
test_loss: 0.0020536769
train_loss: 0.0019357406
test_loss: 0.0021633976
train_loss: 0.0013522452
test_loss: 0.0016362285
train_loss: 0.001811395
test_loss: 0.0015923518
train_loss: 0.0016370226
test_loss: 0.0016279295
train_loss: 0.001999232
test_loss: 0.0018011193
train_loss: 0.0015937495
test_loss: 0.0016554563
train_loss: 0.0020266073
test_loss: 0.0018287963
train_loss: 0.0016259786
test_loss: 0.0018346934
train_loss: 0.0015490488
test_loss: 0.0015465161
train_loss: 0.0017901128
test_loss: 0.0017124665
train_loss: 0.0014604002
test_loss: 0.0016036857
train_loss: 0.0011654781
test_loss: 0.0015533089
train_loss: 0.0016378416
test_loss: 0.0015407276
train_loss: 0.001465277
test_loss: 0.0017803812
train_loss: 0.0018758778
test_loss: 0.0015038745
train_loss: 0.0018908402
test_loss: 0.0018578126
train_loss: 0.002051097
test_loss: 0.0014539423
train_loss: 0.0020970304
test_loss: 0.002108387
train_loss: 0.0018615444
test_loss: 0.0015510625
train_loss: 0.0017587135
test_loss: 0.0017280658
train_loss: 0.0015490028
test_loss: 0.0016692553
train_loss: 0.0018284176
test_loss: 0.0015381944
train_loss: 0.0015869592
test_loss: 0.0016811952
train_loss: 0.001995836
test_loss: 0.0016880065
train_loss: 0.00146645
test_loss: 0.0015100255
train_loss: 0.0020106588
test_loss: 0.0018165772
train_loss: 0.0015772845
test_loss: 0.0017042473
train_loss: 0.0014864904
test_loss: 0.0015960652
train_loss: 0.0014709025
test_loss: 0.0016006959
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfabd0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfab58a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfab4b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfac40730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfac408c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfaaf16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfaaa4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfaa64048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfab0aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfab0a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfaa451e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfa9e3c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfaa0c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfa9ab2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfa95fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfaa0cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfa9ae730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfa95cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcfa95c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcbec237b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcbec40620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcbec5d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcbec40f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcbec0b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcbebcf268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcbebd7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcbebd7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcbeb922f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcbeb92598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc982888c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc982be1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdcbeb898c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc9820e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc9825e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc9823cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc98206f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.07820426e-06
Iter: 2 loss: 2.82922883e-05
Iter: 3 loss: 2.25777285e-06
Iter: 4 loss: 1.96621522e-06
Iter: 5 loss: 1.53758128e-06
Iter: 6 loss: 1.5265307e-06
Iter: 7 loss: 1.23478458e-06
Iter: 8 loss: 3.00365878e-06
Iter: 9 loss: 1.19910601e-06
Iter: 10 loss: 9.7712541e-07
Iter: 11 loss: 1.47166963e-06
Iter: 12 loss: 8.92651485e-07
Iter: 13 loss: 8.17900855e-07
Iter: 14 loss: 1.91606432e-06
Iter: 15 loss: 8.1778569e-07
Iter: 16 loss: 7.82120537e-07
Iter: 17 loss: 7.58031149e-07
Iter: 18 loss: 7.44732e-07
Iter: 19 loss: 7.16086561e-07
Iter: 20 loss: 8.86321232e-07
Iter: 21 loss: 7.12424765e-07
Iter: 22 loss: 6.94065477e-07
Iter: 23 loss: 7.02804755e-07
Iter: 24 loss: 6.8185426e-07
Iter: 25 loss: 6.65377797e-07
Iter: 26 loss: 6.72726856e-07
Iter: 27 loss: 6.54258713e-07
Iter: 28 loss: 6.44767056e-07
Iter: 29 loss: 6.28292867e-07
Iter: 30 loss: 6.2831225e-07
Iter: 31 loss: 6.17820149e-07
Iter: 32 loss: 6.23883238e-07
Iter: 33 loss: 6.11048961e-07
Iter: 34 loss: 6.02316618e-07
Iter: 35 loss: 5.90547415e-07
Iter: 36 loss: 5.89982449e-07
Iter: 37 loss: 5.77454216e-07
Iter: 38 loss: 7.36266827e-07
Iter: 39 loss: 5.77276865e-07
Iter: 40 loss: 5.70761301e-07
Iter: 41 loss: 5.67677489e-07
Iter: 42 loss: 5.64565653e-07
Iter: 43 loss: 5.5866542e-07
Iter: 44 loss: 5.58426507e-07
Iter: 45 loss: 5.54440305e-07
Iter: 46 loss: 5.75612944e-07
Iter: 47 loss: 5.53778364e-07
Iter: 48 loss: 5.50581376e-07
Iter: 49 loss: 5.48944797e-07
Iter: 50 loss: 5.47431114e-07
Iter: 51 loss: 5.43860438e-07
Iter: 52 loss: 5.75730951e-07
Iter: 53 loss: 5.4373163e-07
Iter: 54 loss: 5.4066146e-07
Iter: 55 loss: 5.37750907e-07
Iter: 56 loss: 5.36964706e-07
Iter: 57 loss: 5.32065485e-07
Iter: 58 loss: 5.43964802e-07
Iter: 59 loss: 5.30215402e-07
Iter: 60 loss: 5.27433713e-07
Iter: 61 loss: 5.27079635e-07
Iter: 62 loss: 5.24347683e-07
Iter: 63 loss: 5.32175079e-07
Iter: 64 loss: 5.23413746e-07
Iter: 65 loss: 5.21483628e-07
Iter: 66 loss: 5.18693355e-07
Iter: 67 loss: 5.18479339e-07
Iter: 68 loss: 5.14118597e-07
Iter: 69 loss: 5.16914895e-07
Iter: 70 loss: 5.11320366e-07
Iter: 71 loss: 5.07888103e-07
Iter: 72 loss: 5.36937875e-07
Iter: 73 loss: 5.07721552e-07
Iter: 74 loss: 5.04676507e-07
Iter: 75 loss: 5.02046476e-07
Iter: 76 loss: 5.01255272e-07
Iter: 77 loss: 4.96922326e-07
Iter: 78 loss: 5.4291371e-07
Iter: 79 loss: 4.96851726e-07
Iter: 80 loss: 4.9437341e-07
Iter: 81 loss: 5.20968e-07
Iter: 82 loss: 4.94283881e-07
Iter: 83 loss: 4.92030267e-07
Iter: 84 loss: 4.89541549e-07
Iter: 85 loss: 4.89168826e-07
Iter: 86 loss: 4.86267425e-07
Iter: 87 loss: 4.94871529e-07
Iter: 88 loss: 4.85339399e-07
Iter: 89 loss: 4.8266196e-07
Iter: 90 loss: 4.97409758e-07
Iter: 91 loss: 4.82328346e-07
Iter: 92 loss: 4.80473545e-07
Iter: 93 loss: 4.9303992e-07
Iter: 94 loss: 4.80343942e-07
Iter: 95 loss: 4.79184678e-07
Iter: 96 loss: 4.9375609e-07
Iter: 97 loss: 4.79085315e-07
Iter: 98 loss: 4.77714934e-07
Iter: 99 loss: 4.76913556e-07
Iter: 100 loss: 4.76416091e-07
Iter: 101 loss: 4.75374662e-07
Iter: 102 loss: 4.7418942e-07
Iter: 103 loss: 4.74028951e-07
Iter: 104 loss: 4.72262741e-07
Iter: 105 loss: 4.88898365e-07
Iter: 106 loss: 4.72263764e-07
Iter: 107 loss: 4.71254964e-07
Iter: 108 loss: 4.73390116e-07
Iter: 109 loss: 4.70878348e-07
Iter: 110 loss: 4.69591811e-07
Iter: 111 loss: 4.69445183e-07
Iter: 112 loss: 4.68678024e-07
Iter: 113 loss: 4.67785668e-07
Iter: 114 loss: 4.67763755e-07
Iter: 115 loss: 4.66879243e-07
Iter: 116 loss: 4.68311356e-07
Iter: 117 loss: 4.66488927e-07
Iter: 118 loss: 4.65503348e-07
Iter: 119 loss: 4.65821074e-07
Iter: 120 loss: 4.64872699e-07
Iter: 121 loss: 4.63605147e-07
Iter: 122 loss: 4.64255493e-07
Iter: 123 loss: 4.62630851e-07
Iter: 124 loss: 4.61686795e-07
Iter: 125 loss: 4.72507793e-07
Iter: 126 loss: 4.61644674e-07
Iter: 127 loss: 4.60669725e-07
Iter: 128 loss: 4.65109963e-07
Iter: 129 loss: 4.60547597e-07
Iter: 130 loss: 4.59433295e-07
Iter: 131 loss: 4.65988478e-07
Iter: 132 loss: 4.59329243e-07
Iter: 133 loss: 4.58951916e-07
Iter: 134 loss: 4.58240294e-07
Iter: 135 loss: 4.762272e-07
Iter: 136 loss: 4.58188481e-07
Iter: 137 loss: 4.57117352e-07
Iter: 138 loss: 4.56109973e-07
Iter: 139 loss: 4.55818508e-07
Iter: 140 loss: 4.5478572e-07
Iter: 141 loss: 4.5475889e-07
Iter: 142 loss: 4.53873184e-07
Iter: 143 loss: 4.52565047e-07
Iter: 144 loss: 4.52507095e-07
Iter: 145 loss: 4.5084326e-07
Iter: 146 loss: 4.60588922e-07
Iter: 147 loss: 4.50717096e-07
Iter: 148 loss: 4.49620416e-07
Iter: 149 loss: 4.63299273e-07
Iter: 150 loss: 4.49605182e-07
Iter: 151 loss: 4.48731e-07
Iter: 152 loss: 4.49332902e-07
Iter: 153 loss: 4.48112587e-07
Iter: 154 loss: 4.47316467e-07
Iter: 155 loss: 4.47754871e-07
Iter: 156 loss: 4.46777875e-07
Iter: 157 loss: 4.45780131e-07
Iter: 158 loss: 4.4834411e-07
Iter: 159 loss: 4.45474683e-07
Iter: 160 loss: 4.44544838e-07
Iter: 161 loss: 4.48697421e-07
Iter: 162 loss: 4.4433645e-07
Iter: 163 loss: 4.43702e-07
Iter: 164 loss: 4.43582138e-07
Iter: 165 loss: 4.43191652e-07
Iter: 166 loss: 4.43061538e-07
Iter: 167 loss: 4.42776241e-07
Iter: 168 loss: 4.42252826e-07
Iter: 169 loss: 4.41450055e-07
Iter: 170 loss: 4.41487799e-07
Iter: 171 loss: 4.40430597e-07
Iter: 172 loss: 4.4578573e-07
Iter: 173 loss: 4.40330382e-07
Iter: 174 loss: 4.39697601e-07
Iter: 175 loss: 4.43054205e-07
Iter: 176 loss: 4.39531107e-07
Iter: 177 loss: 4.38889742e-07
Iter: 178 loss: 4.3910984e-07
Iter: 179 loss: 4.38354931e-07
Iter: 180 loss: 4.37737441e-07
Iter: 181 loss: 4.39810606e-07
Iter: 182 loss: 4.37497675e-07
Iter: 183 loss: 4.3693035e-07
Iter: 184 loss: 4.44543275e-07
Iter: 185 loss: 4.36916281e-07
Iter: 186 loss: 4.3645079e-07
Iter: 187 loss: 4.3674072e-07
Iter: 188 loss: 4.36200082e-07
Iter: 189 loss: 4.35757926e-07
Iter: 190 loss: 4.35167749e-07
Iter: 191 loss: 4.35125799e-07
Iter: 192 loss: 4.34524395e-07
Iter: 193 loss: 4.38818603e-07
Iter: 194 loss: 4.34399055e-07
Iter: 195 loss: 4.3378742e-07
Iter: 196 loss: 4.37216727e-07
Iter: 197 loss: 4.33696499e-07
Iter: 198 loss: 4.33283759e-07
Iter: 199 loss: 4.33257981e-07
Iter: 200 loss: 4.33090861e-07
Iter: 201 loss: 4.32738773e-07
Iter: 202 loss: 4.3727249e-07
Iter: 203 loss: 4.32760714e-07
Iter: 204 loss: 4.32166814e-07
Iter: 205 loss: 4.32257423e-07
Iter: 206 loss: 4.31782155e-07
Iter: 207 loss: 4.31363844e-07
Iter: 208 loss: 4.32517169e-07
Iter: 209 loss: 4.31189619e-07
Iter: 210 loss: 4.30675811e-07
Iter: 211 loss: 4.32407802e-07
Iter: 212 loss: 4.3058742e-07
Iter: 213 loss: 4.30165073e-07
Iter: 214 loss: 4.32147203e-07
Iter: 215 loss: 4.30133241e-07
Iter: 216 loss: 4.2970143e-07
Iter: 217 loss: 4.29749463e-07
Iter: 218 loss: 4.29403059e-07
Iter: 219 loss: 4.2905782e-07
Iter: 220 loss: 4.29025448e-07
Iter: 221 loss: 4.28779771e-07
Iter: 222 loss: 4.28371465e-07
Iter: 223 loss: 4.38547801e-07
Iter: 224 loss: 4.28310926e-07
Iter: 225 loss: 4.27857628e-07
Iter: 226 loss: 4.29243073e-07
Iter: 227 loss: 4.27644494e-07
Iter: 228 loss: 4.27124263e-07
Iter: 229 loss: 4.28323631e-07
Iter: 230 loss: 4.26995882e-07
Iter: 231 loss: 4.2666116e-07
Iter: 232 loss: 4.26640213e-07
Iter: 233 loss: 4.26369525e-07
Iter: 234 loss: 4.27793509e-07
Iter: 235 loss: 4.26272322e-07
Iter: 236 loss: 4.26076781e-07
Iter: 237 loss: 4.25813482e-07
Iter: 238 loss: 4.25780172e-07
Iter: 239 loss: 4.25577582e-07
Iter: 240 loss: 4.26736904e-07
Iter: 241 loss: 4.25520682e-07
Iter: 242 loss: 4.2523925e-07
Iter: 243 loss: 4.24984705e-07
Iter: 244 loss: 4.24942755e-07
Iter: 245 loss: 4.24584698e-07
Iter: 246 loss: 4.27725325e-07
Iter: 247 loss: 4.24537376e-07
Iter: 248 loss: 4.24231672e-07
Iter: 249 loss: 4.24393164e-07
Iter: 250 loss: 4.24002224e-07
Iter: 251 loss: 4.23640898e-07
Iter: 252 loss: 4.25570647e-07
Iter: 253 loss: 4.23493589e-07
Iter: 254 loss: 4.23247968e-07
Iter: 255 loss: 4.25306865e-07
Iter: 256 loss: 4.23319563e-07
Iter: 257 loss: 4.23013915e-07
Iter: 258 loss: 4.22732739e-07
Iter: 259 loss: 4.22711395e-07
Iter: 260 loss: 4.22376615e-07
Iter: 261 loss: 4.23312343e-07
Iter: 262 loss: 4.22213361e-07
Iter: 263 loss: 4.2191698e-07
Iter: 264 loss: 4.21963477e-07
Iter: 265 loss: 4.21649077e-07
Iter: 266 loss: 4.21478575e-07
Iter: 267 loss: 4.21375773e-07
Iter: 268 loss: 4.21112816e-07
Iter: 269 loss: 4.2102954e-07
Iter: 270 loss: 4.20863e-07
Iter: 271 loss: 4.20714088e-07
Iter: 272 loss: 4.20672166e-07
Iter: 273 loss: 4.20558422e-07
Iter: 274 loss: 4.20185074e-07
Iter: 275 loss: 4.20567176e-07
Iter: 276 loss: 4.20030915e-07
Iter: 277 loss: 4.19655038e-07
Iter: 278 loss: 4.21903337e-07
Iter: 279 loss: 4.19623973e-07
Iter: 280 loss: 4.19429e-07
Iter: 281 loss: 4.19346435e-07
Iter: 282 loss: 4.19218367e-07
Iter: 283 loss: 4.18911668e-07
Iter: 284 loss: 4.21616136e-07
Iter: 285 loss: 4.18848174e-07
Iter: 286 loss: 4.18638081e-07
Iter: 287 loss: 4.18627252e-07
Iter: 288 loss: 4.18489606e-07
Iter: 289 loss: 4.18116429e-07
Iter: 290 loss: 4.21003563e-07
Iter: 291 loss: 4.18182935e-07
Iter: 292 loss: 4.17962042e-07
Iter: 293 loss: 4.18069419e-07
Iter: 294 loss: 4.17827039e-07
Iter: 295 loss: 4.1751872e-07
Iter: 296 loss: 4.17051126e-07
Iter: 297 loss: 4.27848278e-07
Iter: 298 loss: 4.17038677e-07
Iter: 299 loss: 4.16560567e-07
Iter: 300 loss: 4.20102367e-07
Iter: 301 loss: 4.16446596e-07
Iter: 302 loss: 4.16011488e-07
Iter: 303 loss: 4.16515945e-07
Iter: 304 loss: 4.15841839e-07
Iter: 305 loss: 4.15877253e-07
Iter: 306 loss: 4.15639818e-07
Iter: 307 loss: 4.15481253e-07
Iter: 308 loss: 4.15132376e-07
Iter: 309 loss: 4.22739959e-07
Iter: 310 loss: 4.15125129e-07
Iter: 311 loss: 4.14783358e-07
Iter: 312 loss: 4.14892384e-07
Iter: 313 loss: 4.14503717e-07
Iter: 314 loss: 4.14234819e-07
Iter: 315 loss: 4.16989621e-07
Iter: 316 loss: 4.14235899e-07
Iter: 317 loss: 4.13936903e-07
Iter: 318 loss: 4.13938949e-07
Iter: 319 loss: 4.13707141e-07
Iter: 320 loss: 4.13447822e-07
Iter: 321 loss: 4.13680823e-07
Iter: 322 loss: 4.13277888e-07
Iter: 323 loss: 4.12999782e-07
Iter: 324 loss: 4.16845921e-07
Iter: 325 loss: 4.1299711e-07
Iter: 326 loss: 4.12793042e-07
Iter: 327 loss: 4.13810255e-07
Iter: 328 loss: 4.12729605e-07
Iter: 329 loss: 4.12548843e-07
Iter: 330 loss: 4.12373936e-07
Iter: 331 loss: 4.12308623e-07
Iter: 332 loss: 4.12042539e-07
Iter: 333 loss: 4.13289229e-07
Iter: 334 loss: 4.11948236e-07
Iter: 335 loss: 4.11727825e-07
Iter: 336 loss: 4.11443978e-07
Iter: 337 loss: 4.11416835e-07
Iter: 338 loss: 4.11054828e-07
Iter: 339 loss: 4.14716254e-07
Iter: 340 loss: 4.1108342e-07
Iter: 341 loss: 4.11038513e-07
Iter: 342 loss: 4.1092062e-07
Iter: 343 loss: 4.1078448e-07
Iter: 344 loss: 4.10565747e-07
Iter: 345 loss: 4.1367349e-07
Iter: 346 loss: 4.10483523e-07
Iter: 347 loss: 4.10277238e-07
Iter: 348 loss: 4.10711749e-07
Iter: 349 loss: 4.10142405e-07
Iter: 350 loss: 4.09858927e-07
Iter: 351 loss: 4.10948047e-07
Iter: 352 loss: 4.09762151e-07
Iter: 353 loss: 4.0953347e-07
Iter: 354 loss: 4.10786583e-07
Iter: 355 loss: 4.09500331e-07
Iter: 356 loss: 4.09319256e-07
Iter: 357 loss: 4.090017e-07
Iter: 358 loss: 4.09083555e-07
Iter: 359 loss: 4.08724532e-07
Iter: 360 loss: 4.08662032e-07
Iter: 361 loss: 4.0855457e-07
Iter: 362 loss: 4.09346171e-07
Iter: 363 loss: 4.08495794e-07
Iter: 364 loss: 4.08298575e-07
Iter: 365 loss: 4.08025443e-07
Iter: 366 loss: 4.07974596e-07
Iter: 367 loss: 4.07692482e-07
Iter: 368 loss: 4.10990111e-07
Iter: 369 loss: 4.07684865e-07
Iter: 370 loss: 4.07504444e-07
Iter: 371 loss: 4.07131495e-07
Iter: 372 loss: 4.14057979e-07
Iter: 373 loss: 4.07136895e-07
Iter: 374 loss: 4.06904178e-07
Iter: 375 loss: 4.06913102e-07
Iter: 376 loss: 4.0668391e-07
Iter: 377 loss: 4.08942327e-07
Iter: 378 loss: 4.066641e-07
Iter: 379 loss: 4.06629397e-07
Iter: 380 loss: 4.06293452e-07
Iter: 381 loss: 4.08159735e-07
Iter: 382 loss: 4.0629044e-07
Iter: 383 loss: 4.06114282e-07
Iter: 384 loss: 4.07241942e-07
Iter: 385 loss: 4.06054198e-07
Iter: 386 loss: 4.05797493e-07
Iter: 387 loss: 4.06589322e-07
Iter: 388 loss: 4.0578459e-07
Iter: 389 loss: 4.05524645e-07
Iter: 390 loss: 4.06299222e-07
Iter: 391 loss: 4.055386e-07
Iter: 392 loss: 4.05355479e-07
Iter: 393 loss: 4.05262057e-07
Iter: 394 loss: 4.05200808e-07
Iter: 395 loss: 4.05131914e-07
Iter: 396 loss: 4.05089537e-07
Iter: 397 loss: 4.04993386e-07
Iter: 398 loss: 4.04969967e-07
Iter: 399 loss: 4.04861169e-07
Iter: 400 loss: 4.04771413e-07
Iter: 401 loss: 4.04994864e-07
Iter: 402 loss: 4.04694589e-07
Iter: 403 loss: 4.04523888e-07
Iter: 404 loss: 4.05052731e-07
Iter: 405 loss: 4.04541652e-07
Iter: 406 loss: 4.04340142e-07
Iter: 407 loss: 4.04344433e-07
Iter: 408 loss: 4.04177456e-07
Iter: 409 loss: 4.04235038e-07
Iter: 410 loss: 4.04129054e-07
Iter: 411 loss: 4.04096284e-07
Iter: 412 loss: 4.03979527e-07
Iter: 413 loss: 4.06280606e-07
Iter: 414 loss: 4.03954232e-07
Iter: 415 loss: 4.03725466e-07
Iter: 416 loss: 4.03726119e-07
Iter: 417 loss: 4.0363409e-07
Iter: 418 loss: 4.03456085e-07
Iter: 419 loss: 4.04722357e-07
Iter: 420 loss: 4.03422234e-07
Iter: 421 loss: 4.03203501e-07
Iter: 422 loss: 4.04119305e-07
Iter: 423 loss: 4.03188636e-07
Iter: 424 loss: 4.03058323e-07
Iter: 425 loss: 4.02982323e-07
Iter: 426 loss: 4.02864941e-07
Iter: 427 loss: 4.02732837e-07
Iter: 428 loss: 4.04409207e-07
Iter: 429 loss: 4.02719934e-07
Iter: 430 loss: 4.02547755e-07
Iter: 431 loss: 4.04190928e-07
Iter: 432 loss: 4.02555031e-07
Iter: 433 loss: 4.02441884e-07
Iter: 434 loss: 4.02303385e-07
Iter: 435 loss: 4.05558865e-07
Iter: 436 loss: 4.02275759e-07
Iter: 437 loss: 4.0210665e-07
Iter: 438 loss: 4.03894717e-07
Iter: 439 loss: 4.02149851e-07
Iter: 440 loss: 4.01966588e-07
Iter: 441 loss: 4.0189326e-07
Iter: 442 loss: 4.01852418e-07
Iter: 443 loss: 4.01563142e-07
Iter: 444 loss: 4.03138927e-07
Iter: 445 loss: 4.01530968e-07
Iter: 446 loss: 4.01513205e-07
Iter: 447 loss: 4.01471482e-07
Iter: 448 loss: 4.013732e-07
Iter: 449 loss: 4.01263549e-07
Iter: 450 loss: 4.01387e-07
Iter: 451 loss: 4.01101687e-07
Iter: 452 loss: 4.00958129e-07
Iter: 453 loss: 4.02326151e-07
Iter: 454 loss: 4.00902792e-07
Iter: 455 loss: 4.00758495e-07
Iter: 456 loss: 4.0143874e-07
Iter: 457 loss: 4.00655324e-07
Iter: 458 loss: 4.00500738e-07
Iter: 459 loss: 4.00925046e-07
Iter: 460 loss: 4.00471862e-07
Iter: 461 loss: 4.00258273e-07
Iter: 462 loss: 4.00369288e-07
Iter: 463 loss: 4.00175509e-07
Iter: 464 loss: 4.00083138e-07
Iter: 465 loss: 4.01618053e-07
Iter: 466 loss: 4.00018507e-07
Iter: 467 loss: 3.99906668e-07
Iter: 468 loss: 4.00447846e-07
Iter: 469 loss: 3.99948817e-07
Iter: 470 loss: 3.99819413e-07
Iter: 471 loss: 3.99947083e-07
Iter: 472 loss: 3.99816372e-07
Iter: 473 loss: 3.99682961e-07
Iter: 474 loss: 3.99937562e-07
Iter: 475 loss: 3.99657921e-07
Iter: 476 loss: 3.99577175e-07
Iter: 477 loss: 3.99455871e-07
Iter: 478 loss: 3.99405508e-07
Iter: 479 loss: 3.9937342e-07
Iter: 480 loss: 3.99365661e-07
Iter: 481 loss: 3.99251235e-07
Iter: 482 loss: 3.99306714e-07
Iter: 483 loss: 3.99246687e-07
Iter: 484 loss: 3.99138457e-07
Iter: 485 loss: 3.9895798e-07
Iter: 486 loss: 4.01971931e-07
Iter: 487 loss: 3.98958775e-07
Iter: 488 loss: 3.987588e-07
Iter: 489 loss: 3.99570354e-07
Iter: 490 loss: 3.98699768e-07
Iter: 491 loss: 3.98610439e-07
Iter: 492 loss: 3.98984781e-07
Iter: 493 loss: 3.98548195e-07
Iter: 494 loss: 3.98384856e-07
Iter: 495 loss: 3.98723387e-07
Iter: 496 loss: 3.98332077e-07
Iter: 497 loss: 3.98145204e-07
Iter: 498 loss: 3.98585485e-07
Iter: 499 loss: 3.98082506e-07
Iter: 500 loss: 3.97934969e-07
Iter: 501 loss: 3.98314313e-07
Iter: 502 loss: 3.97906859e-07
Iter: 503 loss: 3.9770913e-07
Iter: 504 loss: 3.98713979e-07
Iter: 505 loss: 3.97728314e-07
Iter: 506 loss: 3.97635489e-07
Iter: 507 loss: 3.97554459e-07
Iter: 508 loss: 3.9750546e-07
Iter: 509 loss: 3.97371e-07
Iter: 510 loss: 3.97665588e-07
Iter: 511 loss: 3.97300653e-07
Iter: 512 loss: 3.97125035e-07
Iter: 513 loss: 3.97926613e-07
Iter: 514 loss: 3.97105424e-07
Iter: 515 loss: 3.97105794e-07
Iter: 516 loss: 3.9705219e-07
Iter: 517 loss: 3.96964822e-07
Iter: 518 loss: 3.96877567e-07
Iter: 519 loss: 3.97349908e-07
Iter: 520 loss: 3.96767803e-07
Iter: 521 loss: 3.96619271e-07
Iter: 522 loss: 3.97062365e-07
Iter: 523 loss: 3.96571579e-07
Iter: 524 loss: 3.96435041e-07
Iter: 525 loss: 3.96319081e-07
Iter: 526 loss: 3.96282474e-07
Iter: 527 loss: 3.95989673e-07
Iter: 528 loss: 3.9731168e-07
Iter: 529 loss: 3.95954089e-07
Iter: 530 loss: 3.95799077e-07
Iter: 531 loss: 3.96537416e-07
Iter: 532 loss: 3.95722481e-07
Iter: 533 loss: 3.95502468e-07
Iter: 534 loss: 3.95887781e-07
Iter: 535 loss: 3.95411519e-07
Iter: 536 loss: 3.95322047e-07
Iter: 537 loss: 3.95793506e-07
Iter: 538 loss: 3.95215409e-07
Iter: 539 loss: 3.95029929e-07
Iter: 540 loss: 3.96137295e-07
Iter: 541 loss: 3.94973512e-07
Iter: 542 loss: 3.94883671e-07
Iter: 543 loss: 3.94997272e-07
Iter: 544 loss: 3.94814606e-07
Iter: 545 loss: 3.94625715e-07
Iter: 546 loss: 3.94813412e-07
Iter: 547 loss: 3.94558356e-07
Iter: 548 loss: 3.94434096e-07
Iter: 549 loss: 3.94443589e-07
Iter: 550 loss: 3.94348149e-07
Iter: 551 loss: 3.94432561e-07
Iter: 552 loss: 3.94293579e-07
Iter: 553 loss: 3.94150391e-07
Iter: 554 loss: 3.93957549e-07
Iter: 555 loss: 3.939831e-07
Iter: 556 loss: 3.93775622e-07
Iter: 557 loss: 3.94306369e-07
Iter: 558 loss: 3.93723099e-07
Iter: 559 loss: 3.9357991e-07
Iter: 560 loss: 3.93646644e-07
Iter: 561 loss: 3.93433396e-07
Iter: 562 loss: 3.9326406e-07
Iter: 563 loss: 3.94192398e-07
Iter: 564 loss: 3.93260734e-07
Iter: 565 loss: 3.9296134e-07
Iter: 566 loss: 3.93375331e-07
Iter: 567 loss: 3.92896482e-07
Iter: 568 loss: 3.92715606e-07
Iter: 569 loss: 3.9455972e-07
Iter: 570 loss: 3.92677919e-07
Iter: 571 loss: 3.92610161e-07
Iter: 572 loss: 3.92403194e-07
Iter: 573 loss: 3.924022e-07
Iter: 574 loss: 3.92250371e-07
Iter: 575 loss: 3.9227524e-07
Iter: 576 loss: 3.92153538e-07
Iter: 577 loss: 3.92097519e-07
Iter: 578 loss: 3.92080807e-07
Iter: 579 loss: 3.91935146e-07
Iter: 580 loss: 3.93275172e-07
Iter: 581 loss: 3.91923408e-07
Iter: 582 loss: 3.91876171e-07
Iter: 583 loss: 3.91860226e-07
Iter: 584 loss: 3.91757055e-07
Iter: 585 loss: 3.9168259e-07
Iter: 586 loss: 3.91745516e-07
Iter: 587 loss: 3.91565692e-07
Iter: 588 loss: 3.91409628e-07
Iter: 589 loss: 3.91794515e-07
Iter: 590 loss: 3.91336869e-07
Iter: 591 loss: 3.91235119e-07
Iter: 592 loss: 3.91662866e-07
Iter: 593 loss: 3.91147125e-07
Iter: 594 loss: 3.91053561e-07
Iter: 595 loss: 3.91068653e-07
Iter: 596 loss: 3.90919752e-07
Iter: 597 loss: 3.90730634e-07
Iter: 598 loss: 3.91296737e-07
Iter: 599 loss: 3.90722761e-07
Iter: 600 loss: 3.90597791e-07
Iter: 601 loss: 3.91446747e-07
Iter: 602 loss: 3.90527646e-07
Iter: 603 loss: 3.90469097e-07
Iter: 604 loss: 3.91081983e-07
Iter: 605 loss: 3.90424361e-07
Iter: 606 loss: 3.90377977e-07
Iter: 607 loss: 3.90334833e-07
Iter: 608 loss: 3.90239506e-07
Iter: 609 loss: 3.90107971e-07
Iter: 610 loss: 3.90152024e-07
Iter: 611 loss: 3.90126559e-07
Iter: 612 loss: 3.90103793e-07
Iter: 613 loss: 3.90000366e-07
Iter: 614 loss: 3.89977629e-07
Iter: 615 loss: 3.8997868e-07
Iter: 616 loss: 3.89907768e-07
Iter: 617 loss: 3.89872497e-07
Iter: 618 loss: 3.89876902e-07
Iter: 619 loss: 3.89745082e-07
Iter: 620 loss: 3.8960971e-07
Iter: 621 loss: 3.89604395e-07
Iter: 622 loss: 3.8948275e-07
Iter: 623 loss: 3.89446484e-07
Iter: 624 loss: 3.89384553e-07
Iter: 625 loss: 3.89485479e-07
Iter: 626 loss: 3.89332683e-07
Iter: 627 loss: 3.89231673e-07
Iter: 628 loss: 3.89278313e-07
Iter: 629 loss: 3.89141633e-07
Iter: 630 loss: 3.88934609e-07
Iter: 631 loss: 3.89072625e-07
Iter: 632 loss: 3.88862929e-07
Iter: 633 loss: 3.88687e-07
Iter: 634 loss: 3.90713467e-07
Iter: 635 loss: 3.88695838e-07
Iter: 636 loss: 3.8858019e-07
Iter: 637 loss: 3.88808303e-07
Iter: 638 loss: 3.88569219e-07
Iter: 639 loss: 3.88416453e-07
Iter: 640 loss: 3.88753392e-07
Iter: 641 loss: 3.88298645e-07
Iter: 642 loss: 3.88216506e-07
Iter: 643 loss: 3.88929095e-07
Iter: 644 loss: 3.88233218e-07
Iter: 645 loss: 3.88066837e-07
Iter: 646 loss: 3.88333547e-07
Iter: 647 loss: 3.88024091e-07
Iter: 648 loss: 3.87951445e-07
Iter: 649 loss: 3.87913e-07
Iter: 650 loss: 3.8789031e-07
Iter: 651 loss: 3.87800299e-07
Iter: 652 loss: 3.87734957e-07
Iter: 653 loss: 3.87675016e-07
Iter: 654 loss: 3.87546663e-07
Iter: 655 loss: 3.87499824e-07
Iter: 656 loss: 3.8741257e-07
Iter: 657 loss: 3.87400519e-07
Iter: 658 loss: 3.87298e-07
Iter: 659 loss: 3.87229306e-07
Iter: 660 loss: 3.87192557e-07
Iter: 661 loss: 3.87065711e-07
Iter: 662 loss: 3.87863281e-07
Iter: 663 loss: 3.87094957e-07
Iter: 664 loss: 3.86949637e-07
Iter: 665 loss: 3.86959982e-07
Iter: 666 loss: 3.86852207e-07
Iter: 667 loss: 3.86787377e-07
Iter: 668 loss: 3.87883404e-07
Iter: 669 loss: 3.86755e-07
Iter: 670 loss: 3.86653255e-07
Iter: 671 loss: 3.87002672e-07
Iter: 672 loss: 3.86657405e-07
Iter: 673 loss: 3.8655412e-07
Iter: 674 loss: 3.86706972e-07
Iter: 675 loss: 3.86514984e-07
Iter: 676 loss: 3.86425739e-07
Iter: 677 loss: 3.86607439e-07
Iter: 678 loss: 3.86416019e-07
Iter: 679 loss: 3.86353378e-07
Iter: 680 loss: 3.86793033e-07
Iter: 681 loss: 3.86334449e-07
Iter: 682 loss: 3.86265299e-07
Iter: 683 loss: 3.86892111e-07
Iter: 684 loss: 3.86312706e-07
Iter: 685 loss: 3.8621539e-07
Iter: 686 loss: 3.86130779e-07
Iter: 687 loss: 3.86152777e-07
Iter: 688 loss: 3.86032212e-07
Iter: 689 loss: 3.85959197e-07
Iter: 690 loss: 3.85965393e-07
Iter: 691 loss: 3.8584534e-07
Iter: 692 loss: 3.86902627e-07
Iter: 693 loss: 3.85829651e-07
Iter: 694 loss: 3.85758881e-07
Iter: 695 loss: 3.85865405e-07
Iter: 696 loss: 3.85702066e-07
Iter: 697 loss: 3.85571781e-07
Iter: 698 loss: 3.85893088e-07
Iter: 699 loss: 3.85572946e-07
Iter: 700 loss: 3.85441894e-07
Iter: 701 loss: 3.85588351e-07
Iter: 702 loss: 3.85438767e-07
Iter: 703 loss: 3.85314195e-07
Iter: 704 loss: 3.85914745e-07
Iter: 705 loss: 3.85303395e-07
Iter: 706 loss: 3.85175554e-07
Iter: 707 loss: 3.85472418e-07
Iter: 708 loss: 3.85146336e-07
Iter: 709 loss: 3.85039698e-07
Iter: 710 loss: 3.85604437e-07
Iter: 711 loss: 3.85051862e-07
Iter: 712 loss: 3.84990415e-07
Iter: 713 loss: 3.852997e-07
Iter: 714 loss: 3.84964153e-07
Iter: 715 loss: 3.84929422e-07
Iter: 716 loss: 3.85357e-07
Iter: 717 loss: 3.84926153e-07
Iter: 718 loss: 3.84857145e-07
Iter: 719 loss: 3.85092932e-07
Iter: 720 loss: 3.84843901e-07
Iter: 721 loss: 3.84793e-07
Iter: 722 loss: 3.84733141e-07
Iter: 723 loss: 3.85133887e-07
Iter: 724 loss: 3.84693692e-07
Iter: 725 loss: 3.84542e-07
Iter: 726 loss: 3.85720341e-07
Iter: 727 loss: 3.84523076e-07
Iter: 728 loss: 3.84350585e-07
Iter: 729 loss: 3.84639748e-07
Iter: 730 loss: 3.84398021e-07
Iter: 731 loss: 3.8424173e-07
Iter: 732 loss: 3.84733539e-07
Iter: 733 loss: 3.84225046e-07
Iter: 734 loss: 3.84182897e-07
Iter: 735 loss: 3.84041982e-07
Iter: 736 loss: 3.84010377e-07
Iter: 737 loss: 3.83924601e-07
Iter: 738 loss: 3.84571649e-07
Iter: 739 loss: 3.83952852e-07
Iter: 740 loss: 3.8387526e-07
Iter: 741 loss: 3.84241645e-07
Iter: 742 loss: 3.83848544e-07
Iter: 743 loss: 3.83707118e-07
Iter: 744 loss: 3.84244657e-07
Iter: 745 loss: 3.83738666e-07
Iter: 746 loss: 3.83665054e-07
Iter: 747 loss: 3.83626968e-07
Iter: 748 loss: 3.83572569e-07
Iter: 749 loss: 3.83527066e-07
Iter: 750 loss: 3.8350089e-07
Iter: 751 loss: 3.83416335e-07
Iter: 752 loss: 3.83812846e-07
Iter: 753 loss: 3.83492363e-07
Iter: 754 loss: 3.83434923e-07
Iter: 755 loss: 3.83414772e-07
Iter: 756 loss: 3.83415653e-07
Iter: 757 loss: 3.83367592e-07
Iter: 758 loss: 3.83332576e-07
Iter: 759 loss: 3.83365688e-07
Iter: 760 loss: 3.83258964e-07
Iter: 761 loss: 3.833741e-07
Iter: 762 loss: 3.83224233e-07
Iter: 763 loss: 3.83135813e-07
Iter: 764 loss: 3.83678866e-07
Iter: 765 loss: 3.83079851e-07
Iter: 766 loss: 3.82993107e-07
Iter: 767 loss: 3.83189843e-07
Iter: 768 loss: 3.83015873e-07
Iter: 769 loss: 3.8289744e-07
Iter: 770 loss: 3.82924242e-07
Iter: 771 loss: 3.82842614e-07
Iter: 772 loss: 3.8278381e-07
Iter: 773 loss: 3.8306726e-07
Iter: 774 loss: 3.8271952e-07
Iter: 775 loss: 3.82594465e-07
Iter: 776 loss: 3.82946183e-07
Iter: 777 loss: 3.8253458e-07
Iter: 778 loss: 3.82513e-07
Iter: 779 loss: 3.83384105e-07
Iter: 780 loss: 3.82470489e-07
Iter: 781 loss: 3.82396081e-07
Iter: 782 loss: 3.82508802e-07
Iter: 783 loss: 3.82376129e-07
Iter: 784 loss: 3.82249823e-07
Iter: 785 loss: 3.82960849e-07
Iter: 786 loss: 3.82259401e-07
Iter: 787 loss: 3.82238113e-07
Iter: 788 loss: 3.82209294e-07
Iter: 789 loss: 3.82201222e-07
Iter: 790 loss: 3.82150461e-07
Iter: 791 loss: 3.82186272e-07
Iter: 792 loss: 3.82061216e-07
Iter: 793 loss: 3.8195185e-07
Iter: 794 loss: 3.82069913e-07
Iter: 795 loss: 3.81961286e-07
Iter: 796 loss: 3.81841716e-07
Iter: 797 loss: 3.8194753e-07
Iter: 798 loss: 3.8179445e-07
Iter: 799 loss: 3.8162392e-07
Iter: 800 loss: 3.82304449e-07
Iter: 801 loss: 3.81662801e-07
Iter: 802 loss: 3.81539394e-07
Iter: 803 loss: 3.81733116e-07
Iter: 804 loss: 3.81566394e-07
Iter: 805 loss: 3.81466577e-07
Iter: 806 loss: 3.81592599e-07
Iter: 807 loss: 3.81409137e-07
Iter: 808 loss: 3.81294285e-07
Iter: 809 loss: 3.8140351e-07
Iter: 810 loss: 3.81345558e-07
Iter: 811 loss: 3.81192933e-07
Iter: 812 loss: 3.81964526e-07
Iter: 813 loss: 3.81157804e-07
Iter: 814 loss: 3.81094196e-07
Iter: 815 loss: 3.81397456e-07
Iter: 816 loss: 3.81140154e-07
Iter: 817 loss: 3.81025274e-07
Iter: 818 loss: 3.81401435e-07
Iter: 819 loss: 3.81085073e-07
Iter: 820 loss: 3.81002906e-07
Iter: 821 loss: 3.81224936e-07
Iter: 822 loss: 3.80944158e-07
Iter: 823 loss: 3.80913207e-07
Iter: 824 loss: 3.80946631e-07
Iter: 825 loss: 3.8091963e-07
Iter: 826 loss: 3.80826151e-07
Iter: 827 loss: 3.80751e-07
Iter: 828 loss: 3.8073884e-07
Iter: 829 loss: 3.80672049e-07
Iter: 830 loss: 3.81011e-07
Iter: 831 loss: 3.8065474e-07
Iter: 832 loss: 3.80575869e-07
Iter: 833 loss: 3.80436234e-07
Iter: 834 loss: 3.80454082e-07
Iter: 835 loss: 3.80349803e-07
Iter: 836 loss: 3.80365805e-07
Iter: 837 loss: 3.80230631e-07
Iter: 838 loss: 3.80345568e-07
Iter: 839 loss: 3.80247e-07
Iter: 840 loss: 3.80151732e-07
Iter: 841 loss: 3.80249276e-07
Iter: 842 loss: 3.80146162e-07
Iter: 843 loss: 3.80026222e-07
Iter: 844 loss: 3.80112e-07
Iter: 845 loss: 3.7996972e-07
Iter: 846 loss: 3.79904918e-07
Iter: 847 loss: 3.81161385e-07
Iter: 848 loss: 3.7995153e-07
Iter: 849 loss: 3.79839321e-07
Iter: 850 loss: 3.80071725e-07
Iter: 851 loss: 3.79835683e-07
Iter: 852 loss: 3.79789697e-07
Iter: 853 loss: 3.79795637e-07
Iter: 854 loss: 3.79751441e-07
Iter: 855 loss: 3.79671121e-07
Iter: 856 loss: 3.79710457e-07
Iter: 857 loss: 3.79692068e-07
Iter: 858 loss: 3.79629569e-07
Iter: 859 loss: 3.79634457e-07
Iter: 860 loss: 3.79555331e-07
Iter: 861 loss: 3.79569769e-07
Iter: 862 loss: 3.79522874e-07
Iter: 863 loss: 3.79447414e-07
Iter: 864 loss: 3.7958398e-07
Iter: 865 loss: 3.79383323e-07
Iter: 866 loss: 3.7933296e-07
Iter: 867 loss: 3.79349785e-07
Iter: 868 loss: 3.7928038e-07
Iter: 869 loss: 3.79153192e-07
Iter: 870 loss: 3.7940373e-07
Iter: 871 loss: 3.79142449e-07
Iter: 872 loss: 3.79028165e-07
Iter: 873 loss: 3.79807432e-07
Iter: 874 loss: 3.79018218e-07
Iter: 875 loss: 3.78959157e-07
Iter: 876 loss: 3.78974107e-07
Iter: 877 loss: 3.78916354e-07
Iter: 878 loss: 3.78793658e-07
Iter: 879 loss: 3.78838735e-07
Iter: 880 loss: 3.78811137e-07
Iter: 881 loss: 3.78804856e-07
Iter: 882 loss: 3.78763929e-07
Iter: 883 loss: 3.78668148e-07
Iter: 884 loss: 3.78892196e-07
Iter: 885 loss: 3.78592915e-07
Iter: 886 loss: 3.78638788e-07
Iter: 887 loss: 3.7862975e-07
Iter: 888 loss: 3.78633047e-07
Iter: 889 loss: 3.78651492e-07
Iter: 890 loss: 3.78633786e-07
Iter: 891 loss: 3.78622815e-07
Iter: 892 loss: 3.78639413e-07
Iter: 893 loss: 3.78623412e-07
Iter: 894 loss: 3.78642341e-07
Iter: 895 loss: 3.78622559e-07
Iter: 896 loss: 3.78610878e-07
Iter: 897 loss: 3.78635889e-07
Iter: 898 loss: 3.78615596e-07
Iter: 899 loss: 3.78614288e-07
Iter: 900 loss: 3.78610792e-07
Iter: 901 loss: 3.78599708e-07
Iter: 902 loss: 3.78593256e-07
Iter: 903 loss: 3.78595388e-07
Iter: 904 loss: 3.78595729e-07
Iter: 905 loss: 3.78595246e-07
Iter: 906 loss: 3.78593768e-07
Iter: 907 loss: 3.78593569e-07
Iter: 908 loss: 3.78592915e-07
Iter: 909 loss: 3.78593313e-07
Iter: 910 loss: 3.78592745e-07
Iter: 911 loss: 3.78593199e-07
Iter: 912 loss: 3.78593199e-07
Iter: 913 loss: 3.78593256e-07
Iter: 914 loss: 3.78593256e-07
Iter: 915 loss: 3.78593256e-07
Iter: 916 loss: 3.78593256e-07
Iter: 917 loss: 3.78593313e-07
Iter: 918 loss: 3.78593256e-07
Iter: 919 loss: 3.78593313e-07
Iter: 920 loss: 3.78593256e-07
Iter: 921 loss: 3.78593313e-07
Iter: 922 loss: 3.78593313e-07
Iter: 923 loss: 3.78593256e-07
Iter: 924 loss: 3.78593256e-07
Iter: 925 loss: 3.78593313e-07
Iter: 926 loss: 3.78593313e-07
Iter: 927 loss: 3.78593256e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.4
+ date
Thu Oct 22 07:53:46 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/500_500_500_500_1 --function f1 --psi 0 --phi 2.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f97ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4fa32840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f9cc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f8f56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f8f5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f94b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f8f58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f8291e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f829268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f8298c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f861268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f7729d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f761730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f811048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f805400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f805d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f725d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f7422f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4f7dd950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c501488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c538a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c4d00d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c5389d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c4859d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c485d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c485e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c485840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c3a3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c3a3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c45b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c43b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c335f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c335730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c3d2d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c3150d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e4c1eff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.2360781
test_loss: 0.24747677
train_loss: 0.09619648
test_loss: 0.10246367
train_loss: 0.07887421
test_loss: 0.07734705
train_loss: 0.05984233
test_loss: 0.051024105
train_loss: 0.024800692
test_loss: 0.028799517
train_loss: 0.018395834
test_loss: 0.022214228
train_loss: 0.015657227
test_loss: 0.016323727
train_loss: 0.013827163
test_loss: 0.012764365
train_loss: 0.013485456
test_loss: 0.012551356
train_loss: 0.010215448
test_loss: 0.011081769
train_loss: 0.012074893
test_loss: 0.012692288
train_loss: 0.011755966
test_loss: 0.0123071885
train_loss: 0.013042897
test_loss: 0.014347412
train_loss: 0.012677675
test_loss: 0.010844152
train_loss: 0.008747494
test_loss: 0.009194708
train_loss: 0.010253519
test_loss: 0.011984932
train_loss: 0.00944471
test_loss: 0.009849809
train_loss: 0.009596776
test_loss: 0.0093227485
train_loss: 0.0071738428
test_loss: 0.009369986
train_loss: 0.008295045
test_loss: 0.009282572
train_loss: 0.0065061906
test_loss: 0.007822782
train_loss: 0.011852891
test_loss: 0.009864587
train_loss: 0.00992169
test_loss: 0.009244224
train_loss: 0.010813742
test_loss: 0.008638441
train_loss: 0.011272454
test_loss: 0.0086415205
train_loss: 0.0076875826
test_loss: 0.008861612
train_loss: 0.0070614014
test_loss: 0.0083004385
train_loss: 0.0078472765
test_loss: 0.008256602
train_loss: 0.010809453
test_loss: 0.008885402
train_loss: 0.0077465978
test_loss: 0.00809267
train_loss: 0.011170595
test_loss: 0.010114328
train_loss: 0.009016523
test_loss: 0.008230495
train_loss: 0.009102678
test_loss: 0.008324722
train_loss: 0.008366058
test_loss: 0.0095067015
train_loss: 0.0084214695
test_loss: 0.009361581
train_loss: 0.010294454
test_loss: 0.010020693
train_loss: 0.008215775
test_loss: 0.008044574
train_loss: 0.008362151
test_loss: 0.007252099
train_loss: 0.0102845235
test_loss: 0.010868691
train_loss: 0.008294615
test_loss: 0.008336705
train_loss: 0.00843613
test_loss: 0.011275657
train_loss: 0.009450788
test_loss: 0.0082866
train_loss: 0.008039942
test_loss: 0.008667394
train_loss: 0.008381334
test_loss: 0.008823965
train_loss: 0.009974519
test_loss: 0.012756194
train_loss: 0.009355543
test_loss: 0.009585474
train_loss: 0.0070807687
test_loss: 0.0071989424
train_loss: 0.0077393195
test_loss: 0.0079357205
train_loss: 0.007697855
test_loss: 0.008860147
train_loss: 0.011478175
test_loss: 0.013721077
train_loss: 0.008122775
test_loss: 0.010393569
train_loss: 0.008483697
test_loss: 0.009721431
train_loss: 0.014287625
test_loss: 0.012120087
train_loss: 0.008087304
test_loss: 0.010122518
train_loss: 0.009122716
test_loss: 0.011118012
train_loss: 0.0092210155
test_loss: 0.008168319
train_loss: 0.008328821
test_loss: 0.007966415
train_loss: 0.009545647
test_loss: 0.008789431
train_loss: 0.007656121
test_loss: 0.007797761
train_loss: 0.009123306
test_loss: 0.010703122
train_loss: 0.009564109
test_loss: 0.009792598
train_loss: 0.008331835
test_loss: 0.011222796
train_loss: 0.011433978
test_loss: 0.009313383
train_loss: 0.008334851
test_loss: 0.009285645
train_loss: 0.008844614
test_loss: 0.009837667
train_loss: 0.008193515
test_loss: 0.009281629
train_loss: 0.007919526
test_loss: 0.00809838
train_loss: 0.006891591
test_loss: 0.0067715165
train_loss: 0.0067465226
test_loss: 0.0074486947
train_loss: 0.010187865
test_loss: 0.012115041
train_loss: 0.010600054
test_loss: 0.009770539
train_loss: 0.0065830313
test_loss: 0.008033985
train_loss: 0.008285241
test_loss: 0.008206307
train_loss: 0.007377197
test_loss: 0.0067567336
train_loss: 0.007431402
test_loss: 0.007293414
train_loss: 0.00818217
test_loss: 0.006856809
train_loss: 0.0078016603
test_loss: 0.008334442
train_loss: 0.007870426
test_loss: 0.0073104873
train_loss: 0.0072915507
test_loss: 0.007419868
train_loss: 0.008518384
test_loss: 0.007296517
train_loss: 0.006855928
test_loss: 0.0076532047
train_loss: 0.008344835
test_loss: 0.00831473
train_loss: 0.0074566016
test_loss: 0.006769383
train_loss: 0.0075707114
test_loss: 0.009696292
train_loss: 0.0071964953
test_loss: 0.0075340234
train_loss: 0.007708133
test_loss: 0.01035351
train_loss: 0.0072550434
test_loss: 0.008678447
train_loss: 0.005535025
test_loss: 0.0072188736
train_loss: 0.009577675
test_loss: 0.0091632465
train_loss: 0.007696801
test_loss: 0.008948341
train_loss: 0.009520374
test_loss: 0.0094966255
train_loss: 0.0071023256
test_loss: 0.008289105
train_loss: 0.008379439
test_loss: 0.009502311
train_loss: 0.009618055
test_loss: 0.007881349
train_loss: 0.0058729784
test_loss: 0.007012418
train_loss: 0.007715459
test_loss: 0.009719282
train_loss: 0.008066225
test_loss: 0.008130471
train_loss: 0.010510262
test_loss: 0.009675752
train_loss: 0.008090519
test_loss: 0.0070094755
train_loss: 0.00866043
test_loss: 0.01144544
train_loss: 0.0080686845
test_loss: 0.009531507
train_loss: 0.008393139
test_loss: 0.008995135
train_loss: 0.007120078
test_loss: 0.007142075
train_loss: 0.0073184506
test_loss: 0.010339376
train_loss: 0.007388553
test_loss: 0.008643909
train_loss: 0.007111744
test_loss: 0.0075570024
train_loss: 0.0072631836
test_loss: 0.008147841
train_loss: 0.008853047
test_loss: 0.009359538
train_loss: 0.0066525512
test_loss: 0.0066561564
train_loss: 0.007588708
test_loss: 0.0076103094
train_loss: 0.00836253
test_loss: 0.0072036684
train_loss: 0.006494438
test_loss: 0.006628018
train_loss: 0.0063406737
test_loss: 0.00713376
train_loss: 0.005868132
test_loss: 0.006656404
train_loss: 0.007417831
test_loss: 0.007022719
train_loss: 0.007291142
test_loss: 0.008604808
train_loss: 0.009604431
test_loss: 0.0079820985
train_loss: 0.0065004607
test_loss: 0.0070236246
train_loss: 0.0061732004
test_loss: 0.0072986283
train_loss: 0.008486372
test_loss: 0.009629625
train_loss: 0.010167979
test_loss: 0.01036306
train_loss: 0.008248401
test_loss: 0.006877811
train_loss: 0.0067247413
test_loss: 0.0076649
train_loss: 0.0068622963
test_loss: 0.008098456
train_loss: 0.006366226
test_loss: 0.0065754326
train_loss: 0.007980791
test_loss: 0.0073180716
train_loss: 0.006977638
test_loss: 0.007776424
train_loss: 0.0076344055
test_loss: 0.0084642675
train_loss: 0.0072097364
test_loss: 0.008211673
train_loss: 0.009373814
test_loss: 0.0077452566
train_loss: 0.0068690493
test_loss: 0.007266414
train_loss: 0.008554603
test_loss: 0.007969584
train_loss: 0.008715661
test_loss: 0.0073971488
train_loss: 0.007678902
test_loss: 0.0065766587
train_loss: 0.005592364
test_loss: 0.0073243473
train_loss: 0.0073748557
test_loss: 0.007392503
train_loss: 0.006214345
test_loss: 0.0070312507
train_loss: 0.0063175634
test_loss: 0.007969739
train_loss: 0.007427994
test_loss: 0.0072946665
train_loss: 0.0072252224
test_loss: 0.007247596
train_loss: 0.007207465
test_loss: 0.006055076
train_loss: 0.007136029
test_loss: 0.008561719
train_loss: 0.0072801327
test_loss: 0.0072908984
train_loss: 0.0073391134
test_loss: 0.0069625913
train_loss: 0.0067097927
test_loss: 0.0070443186
train_loss: 0.009116767
test_loss: 0.008497853
train_loss: 0.008325538
test_loss: 0.009195348
train_loss: 0.006246561
test_loss: 0.007923362
train_loss: 0.0072922735
test_loss: 0.0076316055
train_loss: 0.007413949
test_loss: 0.008692494
train_loss: 0.007430565
test_loss: 0.0089814775
train_loss: 0.007094389
test_loss: 0.008139252
train_loss: 0.007333815
test_loss: 0.007963198
train_loss: 0.010752846
test_loss: 0.009104256
train_loss: 0.007436946
test_loss: 0.0079403715
train_loss: 0.007834986
test_loss: 0.009273295
train_loss: 0.007232967
test_loss: 0.008787084
train_loss: 0.010457463
test_loss: 0.009578544
train_loss: 0.008865243
test_loss: 0.008882621
train_loss: 0.006924986
test_loss: 0.0069920556
train_loss: 0.0068425657
test_loss: 0.007085219
train_loss: 0.0072680335
test_loss: 0.008064924
train_loss: 0.00735442
test_loss: 0.008611555
train_loss: 0.0074605164
test_loss: 0.0078035593
train_loss: 0.009338879
test_loss: 0.008398265
train_loss: 0.0072165458
test_loss: 0.008184287
train_loss: 0.008366615
test_loss: 0.007800931
train_loss: 0.00742109
test_loss: 0.007322884
train_loss: 0.007377718
test_loss: 0.007189118
train_loss: 0.007170257
test_loss: 0.007464382
train_loss: 0.0063122064
test_loss: 0.0066119516
train_loss: 0.0064357207
test_loss: 0.0067695677
train_loss: 0.007343279
test_loss: 0.0066403965
train_loss: 0.007036157
test_loss: 0.007522202
train_loss: 0.0066994675
test_loss: 0.008592489
train_loss: 0.006927219
test_loss: 0.0076618786
train_loss: 0.0064959377
test_loss: 0.0068496424
train_loss: 0.008169818
test_loss: 0.0072745187
train_loss: 0.008362874
test_loss: 0.008790301
train_loss: 0.008333161
test_loss: 0.007851346
train_loss: 0.0060118465
test_loss: 0.00678791
train_loss: 0.0072104996
test_loss: 0.0075993957
train_loss: 0.008558268
test_loss: 0.008680081
train_loss: 0.009593256
test_loss: 0.009533225
train_loss: 0.008382354
test_loss: 0.009308748
train_loss: 0.009142663
test_loss: 0.007842775
train_loss: 0.008021874
test_loss: 0.007440892
train_loss: 0.008024062
test_loss: 0.0081023155
train_loss: 0.012523221
test_loss: 0.007252336
train_loss: 0.007550625
test_loss: 0.0074768546
train_loss: 0.0074594077
test_loss: 0.0072564427
train_loss: 0.0070275962
test_loss: 0.006856139
train_loss: 0.010963231
test_loss: 0.007984756
train_loss: 0.008275193
test_loss: 0.007875416
train_loss: 0.006893182
test_loss: 0.00747133
train_loss: 0.0062000114
test_loss: 0.0070181126
train_loss: 0.006780334
test_loss: 0.007486126
train_loss: 0.006151458
test_loss: 0.0060574897
train_loss: 0.0069133705
test_loss: 0.006736579
train_loss: 0.0068306075
test_loss: 0.0070486153
train_loss: 0.0077098673
test_loss: 0.009035732
train_loss: 0.0072551724
test_loss: 0.006949706
train_loss: 0.0074799005
test_loss: 0.0093801385
train_loss: 0.007082669
test_loss: 0.00763067
train_loss: 0.0070580775
test_loss: 0.0077442685
train_loss: 0.008358625
test_loss: 0.009677439
train_loss: 0.00793677
test_loss: 0.0073136464
train_loss: 0.0077254884
test_loss: 0.008076433
train_loss: 0.0075317933
test_loss: 0.0083763255
train_loss: 0.008284394
test_loss: 0.008122116
train_loss: 0.006510025
test_loss: 0.006197162
train_loss: 0.0062548877
test_loss: 0.007259659
train_loss: 0.0076421117
test_loss: 0.006985063
train_loss: 0.0075635645
test_loss: 0.007819475
train_loss: 0.0066517396
test_loss: 0.0059036473
train_loss: 0.0064617363
test_loss: 0.007079741
train_loss: 0.007043386
test_loss: 0.006985279
train_loss: 0.0065999646
test_loss: 0.0067054294
train_loss: 0.008587915
test_loss: 0.009017488
train_loss: 0.007896881
test_loss: 0.0070770266
train_loss: 0.007471743
test_loss: 0.00795933
train_loss: 0.0060667368
test_loss: 0.007755655
train_loss: 0.007239954
test_loss: 0.006816192
train_loss: 0.0070626526
test_loss: 0.0093413815
train_loss: 0.006862658
test_loss: 0.00869161
train_loss: 0.0070505603
test_loss: 0.006732327
train_loss: 0.009254991
test_loss: 0.0085414285
train_loss: 0.007350922
test_loss: 0.007121738
train_loss: 0.008796428
test_loss: 0.010266869
train_loss: 0.0087095965
test_loss: 0.012012546
train_loss: 0.011969515
test_loss: 0.008288118
train_loss: 0.006044961
test_loss: 0.007828459
train_loss: 0.006010617
test_loss: 0.006373483
train_loss: 0.008336719
test_loss: 0.0071814787
train_loss: 0.0072614094
test_loss: 0.007948749
train_loss: 0.008262274
test_loss: 0.0073500914
train_loss: 0.0073354514
test_loss: 0.008730605
train_loss: 0.008132265
test_loss: 0.008521635
train_loss: 0.009543549
test_loss: 0.008250861
train_loss: 0.008114254
test_loss: 0.0077147116
train_loss: 0.0065735
test_loss: 0.007507592
train_loss: 0.007876076
test_loss: 0.007591961
train_loss: 0.007731512
test_loss: 0.008037688
train_loss: 0.007826618
test_loss: 0.007821662
train_loss: 0.0074359635
test_loss: 0.0073802504
train_loss: 0.007167143
test_loss: 0.007878522
train_loss: 0.0071136886
test_loss: 0.007549469
train_loss: 0.0069509726
test_loss: 0.006688511
train_loss: 0.00870476
test_loss: 0.007177922
train_loss: 0.008049483
test_loss: 0.007913125
train_loss: 0.0076702605
test_loss: 0.008222656
train_loss: 0.008313785
test_loss: 0.008386124
train_loss: 0.0077499105
test_loss: 0.007863378
train_loss: 0.006383513
test_loss: 0.0067412793
train_loss: 0.0077579087
test_loss: 0.008139537
train_loss: 0.00783569
test_loss: 0.009367501
train_loss: 0.008963724
test_loss: 0.010270917
train_loss: 0.0056938394
test_loss: 0.0059345756
train_loss: 0.009453334
test_loss: 0.008571241
train_loss: 0.0064300885
test_loss: 0.006467759
train_loss: 0.0057396838
test_loss: 0.0072073494
train_loss: 0.00914724
test_loss: 0.0074224714
train_loss: 0.008361738
test_loss: 0.007174365
train_loss: 0.0063501326
test_loss: 0.0068925037
train_loss: 0.0074111093
test_loss: 0.0068271942
train_loss: 0.009287204
test_loss: 0.010239482
train_loss: 0.007837923
test_loss: 0.007905479
train_loss: 0.0081241
test_loss: 0.0075129718
train_loss: 0.0077643003
test_loss: 0.007702804
train_loss: 0.008957069
test_loss: 0.009072356
train_loss: 0.007867892
test_loss: 0.008739633
train_loss: 0.006647948
test_loss: 0.0074356846
train_loss: 0.0085172355
test_loss: 0.006844168
train_loss: 0.0070785293
test_loss: 0.0068570646
train_loss: 0.007245893
test_loss: 0.0070691765
train_loss: 0.007977052
test_loss: 0.010668267
train_loss: 0.0098925475
test_loss: 0.009433603
train_loss: 0.0073601305
test_loss: 0.0074579357
train_loss: 0.0062315725
test_loss: 0.005843898
train_loss: 0.006915506
test_loss: 0.0067091277
train_loss: 0.0066197957
test_loss: 0.0062400615
train_loss: 0.0074027088
test_loss: 0.006943378
train_loss: 0.0069916863
test_loss: 0.007914902
train_loss: 0.0074895513
test_loss: 0.009163634
train_loss: 0.0071975533
test_loss: 0.0077234656
train_loss: 0.009218988
test_loss: 0.008557942
train_loss: 0.007781028
test_loss: 0.007937617
train_loss: 0.007098364
test_loss: 0.008361636
train_loss: 0.008376409
test_loss: 0.008395293
train_loss: 0.007660911
test_loss: 0.0098276185
train_loss: 0.008228479
test_loss: 0.008718121
train_loss: 0.006734659
test_loss: 0.007770666
train_loss: 0.008284876
test_loss: 0.0074383193
train_loss: 0.0078092515
test_loss: 0.0076069487
train_loss: 0.0073614237
test_loss: 0.007074493
train_loss: 0.0066131162
test_loss: 0.007661495
train_loss: 0.008204088
test_loss: 0.0073017287
train_loss: 0.008594449
test_loss: 0.0072503416
train_loss: 0.0065520997
test_loss: 0.0072756526
train_loss: 0.008416035
test_loss: 0.0076512736
train_loss: 0.006432844
test_loss: 0.006993121
train_loss: 0.0072176503
test_loss: 0.007360082
train_loss: 0.0071634287
test_loss: 0.006509865
train_loss: 0.006394592
test_loss: 0.0069370135
train_loss: 0.0067996206
test_loss: 0.0076616965
train_loss: 0.006411145
test_loss: 0.0072442717
train_loss: 0.0065759458
test_loss: 0.008385231
train_loss: 0.0075920313
test_loss: 0.008731841
train_loss: 0.0062607788
test_loss: 0.0068220994
train_loss: 0.0071970513
test_loss: 0.007329343
train_loss: 0.007896563
test_loss: 0.00783896
train_loss: 0.006079388
test_loss: 0.0059862123
train_loss: 0.005822558
test_loss: 0.006055931
train_loss: 0.007624304
test_loss: 0.0079025
train_loss: 0.006277653
test_loss: 0.007269221
train_loss: 0.006261036
test_loss: 0.0064550405
train_loss: 0.006233735
test_loss: 0.005875915
train_loss: 0.006191275
test_loss: 0.0072356076
train_loss: 0.0061492743
test_loss: 0.008229802
train_loss: 0.0069064936
test_loss: 0.0071820044
train_loss: 0.0064881044
test_loss: 0.006841417
train_loss: 0.007071871
test_loss: 0.0076691722
train_loss: 0.0077432743
test_loss: 0.008793371
train_loss: 0.007186329
test_loss: 0.008357351
train_loss: 0.008160861
test_loss: 0.009327482
train_loss: 0.0075168805
test_loss: 0.006756642
train_loss: 0.0074006813
test_loss: 0.0077590244
train_loss: 0.007610191
test_loss: 0.0072825598
train_loss: 0.008392047
test_loss: 0.0084193945
train_loss: 0.008915591
test_loss: 0.008924988
train_loss: 0.008072628
test_loss: 0.009580689
train_loss: 0.0071750237
test_loss: 0.0075944243
train_loss: 0.008033963
test_loss: 0.0069097066
train_loss: 0.007084351
test_loss: 0.008456535
train_loss: 0.008077551
test_loss: 0.008236816
train_loss: 0.006608987
test_loss: 0.0061751218
train_loss: 0.007737824
test_loss: 0.008474343
train_loss: 0.007854143
test_loss: 0.006749347
train_loss: 0.00800156
test_loss: 0.007574253
train_loss: 0.00812171
test_loss: 0.008327985
train_loss: 0.007602449
test_loss: 0.007755333
train_loss: 0.009333756
test_loss: 0.007947931
train_loss: 0.0056804838
test_loss: 0.0063175512
train_loss: 0.005878603
test_loss: 0.0065350146
train_loss: 0.0054788673
test_loss: 0.005741049/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

train_loss: 0.0065084375
test_loss: 0.0068267607
train_loss: 0.007135703
test_loss: 0.007824475
train_loss: 0.0064673745
test_loss: 0.006133639
train_loss: 0.006536058
test_loss: 0.00647398
train_loss: 0.006933205
test_loss: 0.0071114423
train_loss: 0.008100271
test_loss: 0.006967603
train_loss: 0.008808134
test_loss: 0.0071424
train_loss: 0.008319048
test_loss: 0.007858548
train_loss: 0.0066119386
test_loss: 0.006263993
train_loss: 0.005309132
test_loss: 0.0065852427
train_loss: 0.006318382
test_loss: 0.00705972
train_loss: 0.009147644
test_loss: 0.00808522
train_loss: 0.006454335
test_loss: 0.0077664065
train_loss: 0.0075474586
test_loss: 0.007591338
train_loss: 0.0064236615
test_loss: 0.0070654275
train_loss: 0.007551286
test_loss: 0.008218719
train_loss: 0.00591956
test_loss: 0.006313606
train_loss: 0.0066290004
test_loss: 0.007162146
train_loss: 0.0063610873
test_loss: 0.006238932
train_loss: 0.008939936
test_loss: 0.008422179
train_loss: 0.007485031
test_loss: 0.008402848
train_loss: 0.008777304
test_loss: 0.007439751
train_loss: 0.006554271
test_loss: 0.006458068
train_loss: 0.006265057
test_loss: 0.007918006
train_loss: 0.0065975687
test_loss: 0.008060405
train_loss: 0.008323772
test_loss: 0.008596671
train_loss: 0.00646995
test_loss: 0.0065173293
train_loss: 0.007958472
test_loss: 0.0070248945
train_loss: 0.007097794
test_loss: 0.008380155
train_loss: 0.0064524943
test_loss: 0.007840272
train_loss: 0.0067514097
test_loss: 0.0076603265
train_loss: 0.0066428473
test_loss: 0.006435814
train_loss: 0.0068348087
test_loss: 0.0073770606
train_loss: 0.006878455
test_loss: 0.006508388
train_loss: 0.008215904
test_loss: 0.0077764764
train_loss: 0.0074617905
test_loss: 0.006934156
train_loss: 0.006898258
test_loss: 0.0076704947
train_loss: 0.0060454127
test_loss: 0.006184776
train_loss: 0.0076956805
test_loss: 0.009304336
train_loss: 0.00818705
test_loss: 0.0071584666
train_loss: 0.00859038
test_loss: 0.008886582
train_loss: 0.007224341
test_loss: 0.008356914
train_loss: 0.007992576
test_loss: 0.009552894
train_loss: 0.006326556
test_loss: 0.008302843
train_loss: 0.0064698695
test_loss: 0.0062651173
train_loss: 0.006124857
test_loss: 0.0059968918
train_loss: 0.007225753
test_loss: 0.0068777152
train_loss: 0.0072293547
test_loss: 0.0068732435
train_loss: 0.0064510186
test_loss: 0.006766782
train_loss: 0.0065793702
test_loss: 0.0069181123
train_loss: 0.0062505724
test_loss: 0.007747222
train_loss: 0.006990121
test_loss: 0.0061712773
train_loss: 0.0076894024
test_loss: 0.006849716
train_loss: 0.0070145465
test_loss: 0.007026991
train_loss: 0.0056653884
test_loss: 0.006042564
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.4/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.4/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f25f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f2259d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f1e5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f2d1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f1b4ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f1b4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f1819d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f2d16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f122ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f0edf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f0ed378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f0a1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f0c38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f06d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f019e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f0c39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f100158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f100840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53f1007b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53ef547b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53ef798c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53ef160d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53ef79950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53ef0ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53ef0ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53ef0e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc53ef0e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc527cd98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc527cd9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc527c69f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc527c9a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc527c9a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc527bf2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc527c3e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc527babea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc527bdbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.52816234e-05
Iter: 2 loss: 0.00376619445
Iter: 3 loss: 5.20720532e-05
Iter: 4 loss: 4.63567885e-05
Iter: 5 loss: 4.58936884e-05
Iter: 6 loss: 4.16223593e-05
Iter: 7 loss: 3.64311527e-05
Iter: 8 loss: 3.87289656e-05
Iter: 9 loss: 3.29036848e-05
Iter: 10 loss: 2.74714293e-05
Iter: 11 loss: 4.34228568e-05
Iter: 12 loss: 2.57994834e-05
Iter: 13 loss: 2.19915091e-05
Iter: 14 loss: 1.98834496e-05
Iter: 15 loss: 1.82166423e-05
Iter: 16 loss: 1.45497406e-05
Iter: 17 loss: 3.07356677e-05
Iter: 18 loss: 1.38045225e-05
Iter: 19 loss: 1.17568397e-05
Iter: 20 loss: 1.79026138e-05
Iter: 21 loss: 1.11438048e-05
Iter: 22 loss: 9.71286954e-06
Iter: 23 loss: 1.41998171e-05
Iter: 24 loss: 9.29825546e-06
Iter: 25 loss: 8.19623347e-06
Iter: 26 loss: 1.69327141e-05
Iter: 27 loss: 8.12168946e-06
Iter: 28 loss: 7.47290778e-06
Iter: 29 loss: 8.86167436e-06
Iter: 30 loss: 7.21770266e-06
Iter: 31 loss: 6.55967415e-06
Iter: 32 loss: 1.06220023e-05
Iter: 33 loss: 6.48232253e-06
Iter: 34 loss: 6.04125717e-06
Iter: 35 loss: 5.98661381e-06
Iter: 36 loss: 5.67102779e-06
Iter: 37 loss: 5.54108192e-06
Iter: 38 loss: 5.5156188e-06
Iter: 39 loss: 5.31815567e-06
Iter: 40 loss: 5.68203541e-06
Iter: 41 loss: 5.23336939e-06
Iter: 42 loss: 5.09128222e-06
Iter: 43 loss: 5.25359383e-06
Iter: 44 loss: 5.01592e-06
Iter: 45 loss: 4.85817236e-06
Iter: 46 loss: 5.12112274e-06
Iter: 47 loss: 4.78643688e-06
Iter: 48 loss: 4.65776566e-06
Iter: 49 loss: 4.69768111e-06
Iter: 50 loss: 4.5666161e-06
Iter: 51 loss: 4.40714075e-06
Iter: 52 loss: 4.6027767e-06
Iter: 53 loss: 4.32405841e-06
Iter: 54 loss: 4.13462749e-06
Iter: 55 loss: 4.75423076e-06
Iter: 56 loss: 4.08230881e-06
Iter: 57 loss: 3.97493568e-06
Iter: 58 loss: 4.75901561e-06
Iter: 59 loss: 3.96614905e-06
Iter: 60 loss: 3.87311866e-06
Iter: 61 loss: 3.93187202e-06
Iter: 62 loss: 3.81411837e-06
Iter: 63 loss: 3.71639157e-06
Iter: 64 loss: 4.4843e-06
Iter: 65 loss: 3.70935368e-06
Iter: 66 loss: 3.63466e-06
Iter: 67 loss: 3.81815562e-06
Iter: 68 loss: 3.60767035e-06
Iter: 69 loss: 3.54501094e-06
Iter: 70 loss: 3.56289183e-06
Iter: 71 loss: 3.49960237e-06
Iter: 72 loss: 3.48200297e-06
Iter: 73 loss: 3.44917885e-06
Iter: 74 loss: 3.43216516e-06
Iter: 75 loss: 3.39499911e-06
Iter: 76 loss: 3.91148342e-06
Iter: 77 loss: 3.39268217e-06
Iter: 78 loss: 3.34287233e-06
Iter: 79 loss: 3.50191885e-06
Iter: 80 loss: 3.32942045e-06
Iter: 81 loss: 3.28485748e-06
Iter: 82 loss: 3.2642115e-06
Iter: 83 loss: 3.24308917e-06
Iter: 84 loss: 3.18446132e-06
Iter: 85 loss: 3.28101237e-06
Iter: 86 loss: 3.15725629e-06
Iter: 87 loss: 3.09613779e-06
Iter: 88 loss: 3.3147453e-06
Iter: 89 loss: 3.08056406e-06
Iter: 90 loss: 3.02478929e-06
Iter: 91 loss: 3.09646498e-06
Iter: 92 loss: 2.99649696e-06
Iter: 93 loss: 2.9415387e-06
Iter: 94 loss: 3.23599761e-06
Iter: 95 loss: 2.93293942e-06
Iter: 96 loss: 2.89653735e-06
Iter: 97 loss: 3.05141634e-06
Iter: 98 loss: 2.88889032e-06
Iter: 99 loss: 2.84677708e-06
Iter: 100 loss: 2.87510375e-06
Iter: 101 loss: 2.8199197e-06
Iter: 102 loss: 2.77702316e-06
Iter: 103 loss: 3.04570676e-06
Iter: 104 loss: 2.7719102e-06
Iter: 105 loss: 2.79620281e-06
Iter: 106 loss: 2.76352876e-06
Iter: 107 loss: 2.75681577e-06
Iter: 108 loss: 2.73806745e-06
Iter: 109 loss: 2.83412191e-06
Iter: 110 loss: 2.73199475e-06
Iter: 111 loss: 2.70910459e-06
Iter: 112 loss: 2.84583075e-06
Iter: 113 loss: 2.70584087e-06
Iter: 114 loss: 2.68395502e-06
Iter: 115 loss: 2.68335839e-06
Iter: 116 loss: 2.66611642e-06
Iter: 117 loss: 2.63858419e-06
Iter: 118 loss: 2.62530148e-06
Iter: 119 loss: 2.61210448e-06
Iter: 120 loss: 2.57432885e-06
Iter: 121 loss: 2.79453525e-06
Iter: 122 loss: 2.56940302e-06
Iter: 123 loss: 2.53585222e-06
Iter: 124 loss: 2.54860311e-06
Iter: 125 loss: 2.51170195e-06
Iter: 126 loss: 2.474344e-06
Iter: 127 loss: 2.5865711e-06
Iter: 128 loss: 2.46282525e-06
Iter: 129 loss: 2.42754686e-06
Iter: 130 loss: 2.6144794e-06
Iter: 131 loss: 2.4220858e-06
Iter: 132 loss: 2.38742632e-06
Iter: 133 loss: 2.49210734e-06
Iter: 134 loss: 2.37726772e-06
Iter: 135 loss: 2.35478865e-06
Iter: 136 loss: 2.49629238e-06
Iter: 137 loss: 2.35197876e-06
Iter: 138 loss: 2.34452818e-06
Iter: 139 loss: 2.34289291e-06
Iter: 140 loss: 2.3305804e-06
Iter: 141 loss: 2.30911951e-06
Iter: 142 loss: 2.30914384e-06
Iter: 143 loss: 2.29241391e-06
Iter: 144 loss: 2.3664893e-06
Iter: 145 loss: 2.28886302e-06
Iter: 146 loss: 2.27363353e-06
Iter: 147 loss: 2.32959337e-06
Iter: 148 loss: 2.27038549e-06
Iter: 149 loss: 2.25918e-06
Iter: 150 loss: 2.24096834e-06
Iter: 151 loss: 2.24069e-06
Iter: 152 loss: 2.21077676e-06
Iter: 153 loss: 2.30277215e-06
Iter: 154 loss: 2.20219522e-06
Iter: 155 loss: 2.175384e-06
Iter: 156 loss: 2.27503756e-06
Iter: 157 loss: 2.16850231e-06
Iter: 158 loss: 2.1458402e-06
Iter: 159 loss: 2.19692083e-06
Iter: 160 loss: 2.13779413e-06
Iter: 161 loss: 2.11177348e-06
Iter: 162 loss: 2.17300294e-06
Iter: 163 loss: 2.1025794e-06
Iter: 164 loss: 2.08187316e-06
Iter: 165 loss: 2.22612084e-06
Iter: 166 loss: 2.07971243e-06
Iter: 167 loss: 2.06323898e-06
Iter: 168 loss: 2.10355847e-06
Iter: 169 loss: 2.05789638e-06
Iter: 170 loss: 2.03823242e-06
Iter: 171 loss: 2.03294894e-06
Iter: 172 loss: 2.02106889e-06
Iter: 173 loss: 2.03561831e-06
Iter: 174 loss: 2.0079126e-06
Iter: 175 loss: 2.00533736e-06
Iter: 176 loss: 1.99677743e-06
Iter: 177 loss: 2.02731644e-06
Iter: 178 loss: 1.99291708e-06
Iter: 179 loss: 1.9777035e-06
Iter: 180 loss: 2.04589355e-06
Iter: 181 loss: 1.97513327e-06
Iter: 182 loss: 1.95851544e-06
Iter: 183 loss: 1.98935186e-06
Iter: 184 loss: 1.95147754e-06
Iter: 185 loss: 1.93658957e-06
Iter: 186 loss: 1.95916e-06
Iter: 187 loss: 1.92938705e-06
Iter: 188 loss: 1.91624486e-06
Iter: 189 loss: 1.9260051e-06
Iter: 190 loss: 1.90827677e-06
Iter: 191 loss: 1.88613546e-06
Iter: 192 loss: 1.93310507e-06
Iter: 193 loss: 1.87774958e-06
Iter: 194 loss: 1.8612044e-06
Iter: 195 loss: 1.91452841e-06
Iter: 196 loss: 1.85611248e-06
Iter: 197 loss: 1.83996463e-06
Iter: 198 loss: 1.89424452e-06
Iter: 199 loss: 1.83537406e-06
Iter: 200 loss: 1.8184719e-06
Iter: 201 loss: 1.84824285e-06
Iter: 202 loss: 1.81098039e-06
Iter: 203 loss: 1.79699919e-06
Iter: 204 loss: 1.9882691e-06
Iter: 205 loss: 1.79714061e-06
Iter: 206 loss: 1.79492247e-06
Iter: 207 loss: 1.79315919e-06
Iter: 208 loss: 1.78915e-06
Iter: 209 loss: 1.77780498e-06
Iter: 210 loss: 1.86517457e-06
Iter: 211 loss: 1.77531626e-06
Iter: 212 loss: 1.76509673e-06
Iter: 213 loss: 1.79863775e-06
Iter: 214 loss: 1.76191452e-06
Iter: 215 loss: 1.75120272e-06
Iter: 216 loss: 1.81455e-06
Iter: 217 loss: 1.74985325e-06
Iter: 218 loss: 1.74196362e-06
Iter: 219 loss: 1.74108231e-06
Iter: 220 loss: 1.73474723e-06
Iter: 221 loss: 1.7252e-06
Iter: 222 loss: 1.73665035e-06
Iter: 223 loss: 1.71958231e-06
Iter: 224 loss: 1.70877365e-06
Iter: 225 loss: 1.75075752e-06
Iter: 226 loss: 1.70591738e-06
Iter: 227 loss: 1.69592272e-06
Iter: 228 loss: 1.72877287e-06
Iter: 229 loss: 1.69340115e-06
Iter: 230 loss: 1.68345923e-06
Iter: 231 loss: 1.69006705e-06
Iter: 232 loss: 1.67749567e-06
Iter: 233 loss: 1.6662209e-06
Iter: 234 loss: 1.71844181e-06
Iter: 235 loss: 1.66436871e-06
Iter: 236 loss: 1.65685947e-06
Iter: 237 loss: 1.7130983e-06
Iter: 238 loss: 1.65580627e-06
Iter: 239 loss: 1.65057122e-06
Iter: 240 loss: 1.70638486e-06
Iter: 241 loss: 1.65032884e-06
Iter: 242 loss: 1.64364474e-06
Iter: 243 loss: 1.65744063e-06
Iter: 244 loss: 1.64089397e-06
Iter: 245 loss: 1.63711388e-06
Iter: 246 loss: 1.63169557e-06
Iter: 247 loss: 1.63157665e-06
Iter: 248 loss: 1.62632364e-06
Iter: 249 loss: 1.66467544e-06
Iter: 250 loss: 1.62529489e-06
Iter: 251 loss: 1.61951323e-06
Iter: 252 loss: 1.62633683e-06
Iter: 253 loss: 1.61602929e-06
Iter: 254 loss: 1.61051139e-06
Iter: 255 loss: 1.60993795e-06
Iter: 256 loss: 1.6059131e-06
Iter: 257 loss: 1.59882768e-06
Iter: 258 loss: 1.61739626e-06
Iter: 259 loss: 1.59591468e-06
Iter: 260 loss: 1.58814089e-06
Iter: 261 loss: 1.5996327e-06
Iter: 262 loss: 1.58420619e-06
Iter: 263 loss: 1.57539853e-06
Iter: 264 loss: 1.61693424e-06
Iter: 265 loss: 1.57378258e-06
Iter: 266 loss: 1.56554847e-06
Iter: 267 loss: 1.57836348e-06
Iter: 268 loss: 1.56192186e-06
Iter: 269 loss: 1.55324642e-06
Iter: 270 loss: 1.56040096e-06
Iter: 271 loss: 1.54815302e-06
Iter: 272 loss: 1.54241297e-06
Iter: 273 loss: 1.54198347e-06
Iter: 274 loss: 1.53944973e-06
Iter: 275 loss: 1.53891449e-06
Iter: 276 loss: 1.53737972e-06
Iter: 277 loss: 1.53311214e-06
Iter: 278 loss: 1.54110137e-06
Iter: 279 loss: 1.53049734e-06
Iter: 280 loss: 1.5242465e-06
Iter: 281 loss: 1.55297835e-06
Iter: 282 loss: 1.52301732e-06
Iter: 283 loss: 1.51702534e-06
Iter: 284 loss: 1.5580506e-06
Iter: 285 loss: 1.51624965e-06
Iter: 286 loss: 1.51147356e-06
Iter: 287 loss: 1.5110395e-06
Iter: 288 loss: 1.5077751e-06
Iter: 289 loss: 1.50216408e-06
Iter: 290 loss: 1.49913694e-06
Iter: 291 loss: 1.49651567e-06
Iter: 292 loss: 1.48843969e-06
Iter: 293 loss: 1.56198826e-06
Iter: 294 loss: 1.4883351e-06
Iter: 295 loss: 1.48332833e-06
Iter: 296 loss: 1.48188224e-06
Iter: 297 loss: 1.47871697e-06
Iter: 298 loss: 1.47091077e-06
Iter: 299 loss: 1.52539985e-06
Iter: 300 loss: 1.47020944e-06
Iter: 301 loss: 1.46530465e-06
Iter: 302 loss: 1.47419371e-06
Iter: 303 loss: 1.46303159e-06
Iter: 304 loss: 1.45784372e-06
Iter: 305 loss: 1.47180344e-06
Iter: 306 loss: 1.45631611e-06
Iter: 307 loss: 1.45974775e-06
Iter: 308 loss: 1.45431886e-06
Iter: 309 loss: 1.4529096e-06
Iter: 310 loss: 1.44933608e-06
Iter: 311 loss: 1.49637935e-06
Iter: 312 loss: 1.44900935e-06
Iter: 313 loss: 1.44638284e-06
Iter: 314 loss: 1.4470711e-06
Iter: 315 loss: 1.44404657e-06
Iter: 316 loss: 1.44124215e-06
Iter: 317 loss: 1.47299397e-06
Iter: 318 loss: 1.44089404e-06
Iter: 319 loss: 1.4375612e-06
Iter: 320 loss: 1.43745388e-06
Iter: 321 loss: 1.43500347e-06
Iter: 322 loss: 1.43136299e-06
Iter: 323 loss: 1.43394152e-06
Iter: 324 loss: 1.4291212e-06
Iter: 325 loss: 1.42516717e-06
Iter: 326 loss: 1.43932095e-06
Iter: 327 loss: 1.42420447e-06
Iter: 328 loss: 1.42019951e-06
Iter: 329 loss: 1.42235081e-06
Iter: 330 loss: 1.41770761e-06
Iter: 331 loss: 1.41273131e-06
Iter: 332 loss: 1.4440202e-06
Iter: 333 loss: 1.41208909e-06
Iter: 334 loss: 1.40835346e-06
Iter: 335 loss: 1.40813677e-06
Iter: 336 loss: 1.40565055e-06
Iter: 337 loss: 1.40035218e-06
Iter: 338 loss: 1.43263208e-06
Iter: 339 loss: 1.39970371e-06
Iter: 340 loss: 1.39701638e-06
Iter: 341 loss: 1.43303748e-06
Iter: 342 loss: 1.3970805e-06
Iter: 343 loss: 1.39407814e-06
Iter: 344 loss: 1.40613861e-06
Iter: 345 loss: 1.39323572e-06
Iter: 346 loss: 1.39184647e-06
Iter: 347 loss: 1.38903e-06
Iter: 348 loss: 1.43076431e-06
Iter: 349 loss: 1.38885957e-06
Iter: 350 loss: 1.38491487e-06
Iter: 351 loss: 1.38788232e-06
Iter: 352 loss: 1.38259918e-06
Iter: 353 loss: 1.37820598e-06
Iter: 354 loss: 1.42364388e-06
Iter: 355 loss: 1.37825884e-06
Iter: 356 loss: 1.37438178e-06
Iter: 357 loss: 1.37068037e-06
Iter: 358 loss: 1.37014035e-06
Iter: 359 loss: 1.36479753e-06
Iter: 360 loss: 1.37056441e-06
Iter: 361 loss: 1.36197445e-06
Iter: 362 loss: 1.35577761e-06
Iter: 363 loss: 1.38970495e-06
Iter: 364 loss: 1.35508253e-06
Iter: 365 loss: 1.35098378e-06
Iter: 366 loss: 1.35887751e-06
Iter: 367 loss: 1.34968241e-06
Iter: 368 loss: 1.34513323e-06
Iter: 369 loss: 1.36011272e-06
Iter: 370 loss: 1.34396259e-06
Iter: 371 loss: 1.34019342e-06
Iter: 372 loss: 1.34521383e-06
Iter: 373 loss: 1.338331e-06
Iter: 374 loss: 1.33417302e-06
Iter: 375 loss: 1.34612287e-06
Iter: 376 loss: 1.33295021e-06
Iter: 377 loss: 1.33771641e-06
Iter: 378 loss: 1.33176241e-06
Iter: 379 loss: 1.33080812e-06
Iter: 380 loss: 1.32829496e-06
Iter: 381 loss: 1.35525215e-06
Iter: 382 loss: 1.32799664e-06
Iter: 383 loss: 1.32624314e-06
Iter: 384 loss: 1.32376374e-06
Iter: 385 loss: 1.32381842e-06
Iter: 386 loss: 1.32136597e-06
Iter: 387 loss: 1.3214808e-06
Iter: 388 loss: 1.31936656e-06
Iter: 389 loss: 1.32255673e-06
Iter: 390 loss: 1.31821298e-06
Iter: 391 loss: 1.3155975e-06
Iter: 392 loss: 1.31539775e-06
Iter: 393 loss: 1.31352181e-06
Iter: 394 loss: 1.31085142e-06
Iter: 395 loss: 1.30927128e-06
Iter: 396 loss: 1.30823128e-06
Iter: 397 loss: 1.30459762e-06
Iter: 398 loss: 1.322137e-06
Iter: 399 loss: 1.30375747e-06
Iter: 400 loss: 1.30077342e-06
Iter: 401 loss: 1.31030311e-06
Iter: 402 loss: 1.29989121e-06
Iter: 403 loss: 1.29683167e-06
Iter: 404 loss: 1.29769683e-06
Iter: 405 loss: 1.29455634e-06
Iter: 406 loss: 1.2910225e-06
Iter: 407 loss: 1.3003455e-06
Iter: 408 loss: 1.29003206e-06
Iter: 409 loss: 1.28944885e-06
Iter: 410 loss: 1.28872512e-06
Iter: 411 loss: 1.2870114e-06
Iter: 412 loss: 1.28379406e-06
Iter: 413 loss: 1.3485726e-06
Iter: 414 loss: 1.28405873e-06
Iter: 415 loss: 1.28180659e-06
Iter: 416 loss: 1.28584293e-06
Iter: 417 loss: 1.28101146e-06
Iter: 418 loss: 1.27895237e-06
Iter: 419 loss: 1.28312877e-06
Iter: 420 loss: 1.27847579e-06
Iter: 421 loss: 1.27613657e-06
Iter: 422 loss: 1.28902025e-06
Iter: 423 loss: 1.27565977e-06
Iter: 424 loss: 1.27398221e-06
Iter: 425 loss: 1.27234466e-06
Iter: 426 loss: 1.27194289e-06
Iter: 427 loss: 1.26903296e-06
Iter: 428 loss: 1.27756698e-06
Iter: 429 loss: 1.26821465e-06
Iter: 430 loss: 1.2657448e-06
Iter: 431 loss: 1.26847203e-06
Iter: 432 loss: 1.26449572e-06
Iter: 433 loss: 1.26167924e-06
Iter: 434 loss: 1.26688599e-06
Iter: 435 loss: 1.2601937e-06
Iter: 436 loss: 1.25717679e-06
Iter: 437 loss: 1.27043086e-06
Iter: 438 loss: 1.25660335e-06
Iter: 439 loss: 1.25415477e-06
Iter: 440 loss: 1.25565202e-06
Iter: 441 loss: 1.25291012e-06
Iter: 442 loss: 1.2507503e-06
Iter: 443 loss: 1.25087047e-06
Iter: 444 loss: 1.24918324e-06
Iter: 445 loss: 1.2709952e-06
Iter: 446 loss: 1.2491671e-06
Iter: 447 loss: 1.24863755e-06
Iter: 448 loss: 1.24699568e-06
Iter: 449 loss: 1.25848055e-06
Iter: 450 loss: 1.24644589e-06
Iter: 451 loss: 1.24448582e-06
Iter: 452 loss: 1.24565076e-06
Iter: 453 loss: 1.24336293e-06
Iter: 454 loss: 1.24095618e-06
Iter: 455 loss: 1.25724603e-06
Iter: 456 loss: 1.24066582e-06
Iter: 457 loss: 1.23898735e-06
Iter: 458 loss: 1.25756844e-06
Iter: 459 loss: 1.23893597e-06
Iter: 460 loss: 1.2376122e-06
Iter: 461 loss: 1.23498546e-06
Iter: 462 loss: 1.28248769e-06
Iter: 463 loss: 1.23490224e-06
Iter: 464 loss: 1.23177847e-06
Iter: 465 loss: 1.24555697e-06
Iter: 466 loss: 1.23106906e-06
Iter: 467 loss: 1.22838958e-06
Iter: 468 loss: 1.2341817e-06
Iter: 469 loss: 1.22732922e-06
Iter: 470 loss: 1.22447864e-06
Iter: 471 loss: 1.23065047e-06
Iter: 472 loss: 1.22349559e-06
Iter: 473 loss: 1.22031565e-06
Iter: 474 loss: 1.23726795e-06
Iter: 475 loss: 1.2197338e-06
Iter: 476 loss: 1.21713083e-06
Iter: 477 loss: 1.22122606e-06
Iter: 478 loss: 1.2163066e-06
Iter: 479 loss: 1.21881646e-06
Iter: 480 loss: 1.21576409e-06
Iter: 481 loss: 1.21550556e-06
Iter: 482 loss: 1.21401035e-06
Iter: 483 loss: 1.21394669e-06
Iter: 484 loss: 1.21234325e-06
Iter: 485 loss: 1.21014637e-06
Iter: 486 loss: 1.2190842e-06
Iter: 487 loss: 1.20959726e-06
Iter: 488 loss: 1.20761e-06
Iter: 489 loss: 1.21502217e-06
Iter: 490 loss: 1.20721324e-06
Iter: 491 loss: 1.20538e-06
Iter: 492 loss: 1.21684798e-06
Iter: 493 loss: 1.20524726e-06
Iter: 494 loss: 1.2032898e-06
Iter: 495 loss: 1.20592631e-06
Iter: 496 loss: 1.20241771e-06
Iter: 497 loss: 1.20075379e-06
Iter: 498 loss: 1.20043126e-06
Iter: 499 loss: 1.19968945e-06
Iter: 500 loss: 1.19774097e-06
Iter: 501 loss: 1.20689572e-06
Iter: 502 loss: 1.19716879e-06
Iter: 503 loss: 1.19479671e-06
Iter: 504 loss: 1.19435322e-06
Iter: 505 loss: 1.19273636e-06
Iter: 506 loss: 1.18998923e-06
Iter: 507 loss: 1.19408855e-06
Iter: 508 loss: 1.18898822e-06
Iter: 509 loss: 1.18587081e-06
Iter: 510 loss: 1.19915524e-06
Iter: 511 loss: 1.18554783e-06
Iter: 512 loss: 1.18607898e-06
Iter: 513 loss: 1.18436981e-06
Iter: 514 loss: 1.18347953e-06
Iter: 515 loss: 1.18471803e-06
Iter: 516 loss: 1.18277637e-06
Iter: 517 loss: 1.18222545e-06
Iter: 518 loss: 1.18091589e-06
Iter: 519 loss: 1.19060542e-06
Iter: 520 loss: 1.18050173e-06
Iter: 521 loss: 1.17841125e-06
Iter: 522 loss: 1.18342541e-06
Iter: 523 loss: 1.17765e-06
Iter: 524 loss: 1.17544732e-06
Iter: 525 loss: 1.18157573e-06
Iter: 526 loss: 1.1746863e-06
Iter: 527 loss: 1.17319348e-06
Iter: 528 loss: 1.17694822e-06
Iter: 529 loss: 1.17287618e-06
Iter: 530 loss: 1.1711943e-06
Iter: 531 loss: 1.18662183e-06
Iter: 532 loss: 1.17131208e-06
Iter: 533 loss: 1.16987462e-06
Iter: 534 loss: 1.17220543e-06
Iter: 535 loss: 1.16915578e-06
Iter: 536 loss: 1.16797014e-06
Iter: 537 loss: 1.1661657e-06
Iter: 538 loss: 1.16592219e-06
Iter: 539 loss: 1.16347223e-06
Iter: 540 loss: 1.16751335e-06
Iter: 541 loss: 1.16233343e-06
Iter: 542 loss: 1.15978719e-06
Iter: 543 loss: 1.18542744e-06
Iter: 544 loss: 1.1597466e-06
Iter: 545 loss: 1.15840953e-06
Iter: 546 loss: 1.16631168e-06
Iter: 547 loss: 1.15817102e-06
Iter: 548 loss: 1.15752891e-06
Iter: 549 loss: 1.15717012e-06
Iter: 550 loss: 1.15656474e-06
Iter: 551 loss: 1.15537409e-06
Iter: 552 loss: 1.15547027e-06
Iter: 553 loss: 1.1546033e-06
Iter: 554 loss: 1.153536e-06
Iter: 555 loss: 1.15358739e-06
Iter: 556 loss: 1.1520649e-06
Iter: 557 loss: 1.15386558e-06
Iter: 558 loss: 1.15129592e-06
Iter: 559 loss: 1.14944828e-06
Iter: 560 loss: 1.15323303e-06
Iter: 561 loss: 1.14884369e-06
Iter: 562 loss: 1.14765635e-06
Iter: 563 loss: 1.15483886e-06
Iter: 564 loss: 1.14736872e-06
Iter: 565 loss: 1.14612953e-06
Iter: 566 loss: 1.15679018e-06
Iter: 567 loss: 1.1460927e-06
Iter: 568 loss: 1.14482486e-06
Iter: 569 loss: 1.14477416e-06
Iter: 570 loss: 1.14394561e-06
Iter: 571 loss: 1.14239015e-06
Iter: 572 loss: 1.14907584e-06
Iter: 573 loss: 1.14201544e-06
Iter: 574 loss: 1.14133059e-06
Iter: 575 loss: 1.13952308e-06
Iter: 576 loss: 1.13967985e-06
Iter: 577 loss: 1.13722899e-06
Iter: 578 loss: 1.14736918e-06
Iter: 579 loss: 1.13691624e-06
Iter: 580 loss: 1.13460328e-06
Iter: 581 loss: 1.13919134e-06
Iter: 582 loss: 1.13386523e-06
Iter: 583 loss: 1.13302622e-06
Iter: 584 loss: 1.13297051e-06
Iter: 585 loss: 1.13174201e-06
Iter: 586 loss: 1.13385943e-06
Iter: 587 loss: 1.13121087e-06
Iter: 588 loss: 1.13011527e-06
Iter: 589 loss: 1.12825012e-06
Iter: 590 loss: 1.16599585e-06
Iter: 591 loss: 1.12829025e-06
Iter: 592 loss: 1.12710268e-06
Iter: 593 loss: 1.12543785e-06
Iter: 594 loss: 1.12555688e-06
Iter: 595 loss: 1.12275961e-06
Iter: 596 loss: 1.13513227e-06
Iter: 597 loss: 1.12230805e-06
Iter: 598 loss: 1.12133853e-06
Iter: 599 loss: 1.1211074e-06
Iter: 600 loss: 1.12014254e-06
Iter: 601 loss: 1.12144858e-06
Iter: 602 loss: 1.11966438e-06
Iter: 603 loss: 1.1184278e-06
Iter: 604 loss: 1.119339e-06
Iter: 605 loss: 1.11793611e-06
Iter: 606 loss: 1.11643681e-06
Iter: 607 loss: 1.12023827e-06
Iter: 608 loss: 1.11593067e-06
Iter: 609 loss: 1.11480654e-06
Iter: 610 loss: 1.11584518e-06
Iter: 611 loss: 1.1139872e-06
Iter: 612 loss: 1.11249e-06
Iter: 613 loss: 1.11951488e-06
Iter: 614 loss: 1.11237887e-06
Iter: 615 loss: 1.1113508e-06
Iter: 616 loss: 1.11448514e-06
Iter: 617 loss: 1.11126189e-06
Iter: 618 loss: 1.11113195e-06
Iter: 619 loss: 1.11041027e-06
Iter: 620 loss: 1.11035126e-06
Iter: 621 loss: 1.10941085e-06
Iter: 622 loss: 1.11310078e-06
Iter: 623 loss: 1.10919882e-06
Iter: 624 loss: 1.10823203e-06
Iter: 625 loss: 1.10913174e-06
Iter: 626 loss: 1.10753024e-06
Iter: 627 loss: 1.10675842e-06
Iter: 628 loss: 1.10687006e-06
Iter: 629 loss: 1.10612086e-06
Iter: 630 loss: 1.10487701e-06
Iter: 631 loss: 1.10858718e-06
Iter: 632 loss: 1.10442386e-06
Iter: 633 loss: 1.10323822e-06
Iter: 634 loss: 1.11670374e-06
Iter: 635 loss: 1.10313158e-06
Iter: 636 loss: 1.10229223e-06
Iter: 637 loss: 1.10543328e-06
Iter: 638 loss: 1.10206429e-06
Iter: 639 loss: 1.10102474e-06
Iter: 640 loss: 1.10040241e-06
Iter: 641 loss: 1.1000368e-06
Iter: 642 loss: 1.09867597e-06
Iter: 643 loss: 1.10392568e-06
Iter: 644 loss: 1.09827454e-06
Iter: 645 loss: 1.09703331e-06
Iter: 646 loss: 1.10139763e-06
Iter: 647 loss: 1.09678388e-06
Iter: 648 loss: 1.09568782e-06
Iter: 649 loss: 1.09627786e-06
Iter: 650 loss: 1.09536347e-06
Iter: 651 loss: 1.0941651e-06
Iter: 652 loss: 1.09964026e-06
Iter: 653 loss: 1.09369489e-06
Iter: 654 loss: 1.09449763e-06
Iter: 655 loss: 1.09340033e-06
Iter: 656 loss: 1.09330199e-06
Iter: 657 loss: 1.09290613e-06
Iter: 658 loss: 1.0949791e-06
Iter: 659 loss: 1.09266739e-06
Iter: 660 loss: 1.09182884e-06
Iter: 661 loss: 1.09104155e-06
Iter: 662 loss: 1.09111124e-06
Iter: 663 loss: 1.09005896e-06
Iter: 664 loss: 1.09923189e-06
Iter: 665 loss: 1.08973882e-06
Iter: 666 loss: 1.08932522e-06
Iter: 667 loss: 1.09541816e-06
Iter: 668 loss: 1.08937036e-06
Iter: 669 loss: 1.08875759e-06
Iter: 670 loss: 1.08840413e-06
Iter: 671 loss: 1.08829431e-06
Iter: 672 loss: 1.08741324e-06
Iter: 673 loss: 1.08702045e-06
Iter: 674 loss: 1.08650943e-06
Iter: 675 loss: 1.08549261e-06
Iter: 676 loss: 1.0910469e-06
Iter: 677 loss: 1.08547351e-06
Iter: 678 loss: 1.08441111e-06
Iter: 679 loss: 1.08608253e-06
Iter: 680 loss: 1.08389281e-06
Iter: 681 loss: 1.08297945e-06
Iter: 682 loss: 1.08596976e-06
Iter: 683 loss: 1.08271286e-06
Iter: 684 loss: 1.08179665e-06
Iter: 685 loss: 1.0822356e-06
Iter: 686 loss: 1.0812596e-06
Iter: 687 loss: 1.08143627e-06
Iter: 688 loss: 1.0809274e-06
Iter: 689 loss: 1.08034192e-06
Iter: 690 loss: 1.08041445e-06
Iter: 691 loss: 1.08005e-06
Iter: 692 loss: 1.07967639e-06
Iter: 693 loss: 1.07927121e-06
Iter: 694 loss: 1.08345625e-06
Iter: 695 loss: 1.07882011e-06
Iter: 696 loss: 1.07797791e-06
Iter: 697 loss: 1.07707615e-06
Iter: 698 loss: 1.07678147e-06
Iter: 699 loss: 1.0761878e-06
Iter: 700 loss: 1.07593132e-06
Iter: 701 loss: 1.07507174e-06
Iter: 702 loss: 1.0754901e-06
Iter: 703 loss: 1.07468645e-06
Iter: 704 loss: 1.07424285e-06
Iter: 705 loss: 1.074083e-06
Iter: 706 loss: 1.07381595e-06
Iter: 707 loss: 1.07362882e-06
Iter: 708 loss: 1.07334949e-06
Iter: 709 loss: 1.07287804e-06
Iter: 710 loss: 1.07316839e-06
Iter: 711 loss: 1.07275218e-06
Iter: 712 loss: 1.07175595e-06
Iter: 713 loss: 1.07287019e-06
Iter: 714 loss: 1.07165079e-06
Iter: 715 loss: 1.07072708e-06
Iter: 716 loss: 1.07398057e-06
Iter: 717 loss: 1.07062351e-06
Iter: 718 loss: 1.07004348e-06
Iter: 719 loss: 1.07474477e-06
Iter: 720 loss: 1.06992388e-06
Iter: 721 loss: 1.06980417e-06
Iter: 722 loss: 1.06962398e-06
Iter: 723 loss: 1.06941752e-06
Iter: 724 loss: 1.06864604e-06
Iter: 725 loss: 1.06921561e-06
Iter: 726 loss: 1.06815241e-06
Iter: 727 loss: 1.06736707e-06
Iter: 728 loss: 1.07057826e-06
Iter: 729 loss: 1.06704181e-06
Iter: 730 loss: 1.06647781e-06
Iter: 731 loss: 1.06861671e-06
Iter: 732 loss: 1.06577158e-06
Iter: 733 loss: 1.06517473e-06
Iter: 734 loss: 1.0706492e-06
Iter: 735 loss: 1.06508e-06
Iter: 736 loss: 1.06478183e-06
Iter: 737 loss: 1.06869493e-06
Iter: 738 loss: 1.06475727e-06
Iter: 739 loss: 1.0639942e-06
Iter: 740 loss: 1.06463131e-06
Iter: 741 loss: 1.06377024e-06
Iter: 742 loss: 1.06312336e-06
Iter: 743 loss: 1.06314155e-06
Iter: 744 loss: 1.06261143e-06
Iter: 745 loss: 1.06202697e-06
Iter: 746 loss: 1.06404252e-06
Iter: 747 loss: 1.06181199e-06
Iter: 748 loss: 1.06103869e-06
Iter: 749 loss: 1.06265e-06
Iter: 750 loss: 1.0609989e-06
Iter: 751 loss: 1.06005621e-06
Iter: 752 loss: 1.06016364e-06
Iter: 753 loss: 1.05936624e-06
Iter: 754 loss: 1.05830475e-06
Iter: 755 loss: 1.0669728e-06
Iter: 756 loss: 1.05826712e-06
Iter: 757 loss: 1.05831543e-06
Iter: 758 loss: 1.05778872e-06
Iter: 759 loss: 1.05788638e-06
Iter: 760 loss: 1.05778668e-06
Iter: 761 loss: 1.05779361e-06
Iter: 762 loss: 1.05783488e-06
Iter: 763 loss: 1.0577578e-06
Iter: 764 loss: 1.05786148e-06
Iter: 765 loss: 1.05777781e-06
Iter: 766 loss: 1.0578367e-06
Iter: 767 loss: 1.05774734e-06
Iter: 768 loss: 1.05783943e-06
Iter: 769 loss: 1.05780418e-06
Iter: 770 loss: 1.05777508e-06
Iter: 771 loss: 1.05782317e-06
Iter: 772 loss: 1.05776667e-06
Iter: 773 loss: 1.05777872e-06
Iter: 774 loss: 1.05777349e-06
Iter: 775 loss: 1.05779895e-06
Iter: 776 loss: 1.05779623e-06
Iter: 777 loss: 1.05777053e-06
Iter: 778 loss: 1.05776871e-06
Iter: 779 loss: 1.05778327e-06
Iter: 780 loss: 1.05779623e-06
Iter: 781 loss: 1.05778327e-06
Iter: 782 loss: 1.05778327e-06
Iter: 783 loss: 1.05779623e-06
Iter: 784 loss: 1.05778327e-06
Iter: 785 loss: 1.05630647e-06
Iter: 786 loss: 1.06948346e-06
Iter: 787 loss: 1.05624736e-06
Iter: 788 loss: 1.05556126e-06
Iter: 789 loss: 1.05730942e-06
Iter: 790 loss: 1.05534377e-06
Iter: 791 loss: 1.05469553e-06
Iter: 792 loss: 1.05750019e-06
Iter: 793 loss: 1.0544702e-06
Iter: 794 loss: 1.05388665e-06
Iter: 795 loss: 1.05572417e-06
Iter: 796 loss: 1.05366473e-06
Iter: 797 loss: 1.05290576e-06
Iter: 798 loss: 1.05393747e-06
Iter: 799 loss: 1.05270533e-06
Iter: 800 loss: 1.05218328e-06
Iter: 801 loss: 1.05223899e-06
Iter: 802 loss: 1.05180175e-06
Iter: 803 loss: 1.05116942e-06
Iter: 804 loss: 1.05149525e-06
Iter: 805 loss: 1.05071035e-06
Iter: 806 loss: 1.04963704e-06
Iter: 807 loss: 1.04949504e-06
Iter: 808 loss: 1.04899868e-06
Iter: 809 loss: 1.04817775e-06
Iter: 810 loss: 1.05886443e-06
Iter: 811 loss: 1.0482529e-06
Iter: 812 loss: 1.04782566e-06
Iter: 813 loss: 1.04788592e-06
Iter: 814 loss: 1.04723586e-06
Iter: 815 loss: 1.04703463e-06
Iter: 816 loss: 1.04696153e-06
Iter: 817 loss: 1.04630954e-06
Iter: 818 loss: 1.04565515e-06
Iter: 819 loss: 1.06390166e-06
Iter: 820 loss: 1.04562878e-06
Iter: 821 loss: 1.04489982e-06
Iter: 822 loss: 1.04510832e-06
Iter: 823 loss: 1.04418484e-06
Iter: 824 loss: 1.04306412e-06
Iter: 825 loss: 1.04535604e-06
Iter: 826 loss: 1.04282037e-06
Iter: 827 loss: 1.04211585e-06
Iter: 828 loss: 1.04200512e-06
Iter: 829 loss: 1.0417549e-06
Iter: 830 loss: 1.04224102e-06
Iter: 831 loss: 1.04151218e-06
Iter: 832 loss: 1.04080038e-06
Iter: 833 loss: 1.04536377e-06
Iter: 834 loss: 1.04076446e-06
Iter: 835 loss: 1.03997422e-06
Iter: 836 loss: 1.0401784e-06
Iter: 837 loss: 1.03994489e-06
Iter: 838 loss: 1.03941613e-06
Iter: 839 loss: 1.04077264e-06
Iter: 840 loss: 1.03916034e-06
Iter: 841 loss: 1.03826289e-06
Iter: 842 loss: 1.03811738e-06
Iter: 843 loss: 1.03775722e-06
Iter: 844 loss: 1.03696743e-06
Iter: 845 loss: 1.0370095e-06
Iter: 846 loss: 1.03638877e-06
Iter: 847 loss: 1.04668527e-06
Iter: 848 loss: 1.0362171e-06
Iter: 849 loss: 1.03601394e-06
Iter: 850 loss: 1.03528896e-06
Iter: 851 loss: 1.03617799e-06
Iter: 852 loss: 1.03486548e-06
Iter: 853 loss: 1.03384775e-06
Iter: 854 loss: 1.04164451e-06
Iter: 855 loss: 1.03362447e-06
Iter: 856 loss: 1.03276807e-06
Iter: 857 loss: 1.03449065e-06
Iter: 858 loss: 1.03239222e-06
Iter: 859 loss: 1.03196328e-06
Iter: 860 loss: 1.03172772e-06
Iter: 861 loss: 1.03119658e-06
Iter: 862 loss: 1.03026377e-06
Iter: 863 loss: 1.03442e-06
Iter: 864 loss: 1.02995932e-06
Iter: 865 loss: 1.02911883e-06
Iter: 866 loss: 1.03739353e-06
Iter: 867 loss: 1.02911042e-06
Iter: 868 loss: 1.0286542e-06
Iter: 869 loss: 1.03496177e-06
Iter: 870 loss: 1.02849947e-06
Iter: 871 loss: 1.02813488e-06
Iter: 872 loss: 1.02759634e-06
Iter: 873 loss: 1.02758463e-06
Iter: 874 loss: 1.02657555e-06
Iter: 875 loss: 1.0299176e-06
Iter: 876 loss: 1.02629451e-06
Iter: 877 loss: 1.02542776e-06
Iter: 878 loss: 1.03576212e-06
Iter: 879 loss: 1.02547415e-06
Iter: 880 loss: 1.02514593e-06
Iter: 881 loss: 1.02517197e-06
Iter: 882 loss: 1.02478816e-06
Iter: 883 loss: 1.02422121e-06
Iter: 884 loss: 1.03265211e-06
Iter: 885 loss: 1.02425e-06
Iter: 886 loss: 1.02390209e-06
Iter: 887 loss: 1.02298895e-06
Iter: 888 loss: 1.02290119e-06
Iter: 889 loss: 1.02197805e-06
Iter: 890 loss: 1.02469562e-06
Iter: 891 loss: 1.02160129e-06
Iter: 892 loss: 1.02041099e-06
Iter: 893 loss: 1.02876152e-06
Iter: 894 loss: 1.02023864e-06
Iter: 895 loss: 1.01960154e-06
Iter: 896 loss: 1.01791932e-06
Iter: 897 loss: 1.05061076e-06
Iter: 898 loss: 1.01798059e-06
Iter: 899 loss: 1.01669264e-06
Iter: 900 loss: 1.01669696e-06
Iter: 901 loss: 1.01610419e-06
Iter: 902 loss: 1.02285526e-06
Iter: 903 loss: 1.01606543e-06
Iter: 904 loss: 1.01530122e-06
Iter: 905 loss: 1.01495777e-06
Iter: 906 loss: 1.0146224e-06
Iter: 907 loss: 1.01387081e-06
Iter: 908 loss: 1.01394403e-06
Iter: 909 loss: 1.01292676e-06
Iter: 910 loss: 1.01201499e-06
Iter: 911 loss: 1.01565388e-06
Iter: 912 loss: 1.01169985e-06
Iter: 913 loss: 1.01172623e-06
Iter: 914 loss: 1.0113107e-06
Iter: 915 loss: 1.01087267e-06
Iter: 916 loss: 1.01121782e-06
Iter: 917 loss: 1.01041576e-06
Iter: 918 loss: 1.01004912e-06
Iter: 919 loss: 1.00932266e-06
Iter: 920 loss: 1.02186516e-06
Iter: 921 loss: 1.0091502e-06
Iter: 922 loss: 1.00788202e-06
Iter: 923 loss: 1.01467504e-06
Iter: 924 loss: 1.00777072e-06
Iter: 925 loss: 1.00709713e-06
Iter: 926 loss: 1.00711452e-06
Iter: 927 loss: 1.00665147e-06
Iter: 928 loss: 1.00818193e-06
Iter: 929 loss: 1.0063228e-06
Iter: 930 loss: 1.00559419e-06
Iter: 931 loss: 1.00635327e-06
Iter: 932 loss: 1.00539773e-06
Iter: 933 loss: 1.00442708e-06
Iter: 934 loss: 1.00814088e-06
Iter: 935 loss: 1.00432385e-06
Iter: 936 loss: 1.00406874e-06
Iter: 937 loss: 1.00384159e-06
Iter: 938 loss: 1.00350462e-06
Iter: 939 loss: 1.00312513e-06
Iter: 940 loss: 1.0030725e-06
Iter: 941 loss: 1.00265731e-06
Iter: 942 loss: 1.00256443e-06
Iter: 943 loss: 1.00208831e-06
Iter: 944 loss: 1.0020326e-06
Iter: 945 loss: 1.00269222e-06
Iter: 946 loss: 1.00180193e-06
Iter: 947 loss: 1.0012983e-06
Iter: 948 loss: 1.00081195e-06
Iter: 949 loss: 1.00086856e-06
Iter: 950 loss: 1.00005661e-06
Iter: 951 loss: 1.00181546e-06
Iter: 952 loss: 9.99720442e-07
Iter: 953 loss: 9.99201234e-07
Iter: 954 loss: 1.00483123e-06
Iter: 955 loss: 9.9938768e-07
Iter: 956 loss: 9.98835e-07
Iter: 957 loss: 9.98086307e-07
Iter: 958 loss: 9.9811723e-07
Iter: 959 loss: 9.97405e-07
Iter: 960 loss: 1.00290049e-06
Iter: 961 loss: 9.97151346e-07
Iter: 962 loss: 9.96869e-07
Iter: 963 loss: 1.00047589e-06
Iter: 964 loss: 9.96655e-07
Iter: 965 loss: 9.9606234e-07
Iter: 966 loss: 9.95586902e-07
Iter: 967 loss: 9.95631808e-07
Iter: 968 loss: 9.94962306e-07
Iter: 969 loss: 1.00398302e-06
Iter: 970 loss: 9.95034497e-07
Iter: 971 loss: 9.94587822e-07
Iter: 972 loss: 9.9977251e-07
Iter: 973 loss: 9.94277457e-07
Iter: 974 loss: 9.94010293e-07
Iter: 975 loss: 9.95025857e-07
Iter: 976 loss: 9.94062248e-07
Iter: 977 loss: 9.93660478e-07
Iter: 978 loss: 9.96228778e-07
Iter: 979 loss: 9.93442882e-07
Iter: 980 loss: 9.93457e-07
Iter: 981 loss: 9.93512344e-07
Iter: 982 loss: 9.93525305e-07
Iter: 983 loss: 9.93641152e-07
Iter: 984 loss: 9.93605227e-07
Iter: 985 loss: 9.93593858e-07
Iter: 986 loss: 9.93506092e-07
Iter: 987 loss: 9.93503818e-07
Iter: 988 loss: 9.9346471e-07
Iter: 989 loss: 9.93533149e-07
Iter: 990 loss: 9.93518597e-07
Iter: 991 loss: 9.9345641e-07
Iter: 992 loss: 9.93483695e-07
Iter: 993 loss: 9.93484832e-07
Iter: 994 loss: 9.93452886e-07
Iter: 995 loss: 9.93448907e-07
Iter: 996 loss: 9.93442427e-07
Iter: 997 loss: 9.93442882e-07
Iter: 998 loss: 9.93445724e-07
Iter: 999 loss: 9.93442882e-07
Iter: 1000 loss: 9.9344436e-07
Iter: 1001 loss: 9.93444e-07
Iter: 1002 loss: 9.93442882e-07
Iter: 1003 loss: 9.93443564e-07
Iter: 1004 loss: 9.93444e-07
Iter: 1005 loss: 9.93443564e-07
Iter: 1006 loss: 9.93443791e-07
Iter: 1007 loss: 9.93443791e-07
Iter: 1008 loss: 9.93443791e-07
Iter: 1009 loss: 9.93443791e-07
Iter: 1010 loss: 9.93443791e-07
Iter: 1011 loss: 9.93444e-07
Iter: 1012 loss: 9.93444e-07
Iter: 1013 loss: 9.93444e-07
Iter: 1014 loss: 9.93443791e-07
Iter: 1015 loss: 9.93444e-07
Iter: 1016 loss: 9.93443791e-07
Iter: 1017 loss: 9.93444e-07
Iter: 1018 loss: 9.93444e-07
Iter: 1019 loss: 9.93443791e-07
Iter: 1020 loss: 9.93444e-07
Iter: 1021 loss: 1.12787643e-06
Iter: 1022 loss: 9.93576123e-07
Iter: 1023 loss: 9.93558e-07
Iter: 1024 loss: 9.93542471e-07
Iter: 1025 loss: 9.93511549e-07
Iter: 1026 loss: 9.93503477e-07
Iter: 1027 loss: 9.93577146e-07
Iter: 1028 loss: 9.93727554e-07
Iter: 1029 loss: 9.9355259e-07
Iter: 1030 loss: 9.93560661e-07
Iter: 1031 loss: 9.93557251e-07
Iter: 1032 loss: 9.93505864e-07
Iter: 1033 loss: 9.93548156e-07
Iter: 1034 loss: 9.93550429e-07
Iter: 1035 loss: 9.93502454e-07
Iter: 1036 loss: 9.9347892e-07
Iter: 1037 loss: 9.9347244e-07
Iter: 1038 loss: 9.93467893e-07
Iter: 1039 loss: 9.9344652e-07
Iter: 1040 loss: 9.9344652e-07
Iter: 1041 loss: 9.93448793e-07
Iter: 1042 loss: 9.93448907e-07
Iter: 1043 loss: 9.9344993e-07
Iter: 1044 loss: 9.93443791e-07
Iter: 1045 loss: 9.93444e-07
Iter: 1046 loss: 9.93443791e-07
Iter: 1047 loss: 9.93443791e-07
Iter: 1048 loss: 9.9344993e-07
Iter: 1049 loss: 9.93443791e-07
Iter: 1050 loss: 9.93449476e-07
Iter: 1051 loss: 9.93443791e-07
Iter: 1052 loss: 9.93449476e-07
Iter: 1053 loss: 9.93443791e-07
Iter: 1054 loss: 9.93449589e-07
Iter: 1055 loss: 9.93449589e-07
Iter: 1056 loss: 9.93449589e-07
Iter: 1057 loss: 9.93449589e-07
Iter: 1058 loss: 9.93449589e-07
Iter: 1059 loss: 9.93443791e-07
Iter: 1060 loss: 9.93443791e-07
Iter: 1061 loss: 9.93443791e-07
Iter: 1062 loss: 9.93449589e-07
Iter: 1063 loss: 9.93449589e-07
Iter: 1064 loss: 9.93443791e-07
Iter: 1065 loss: 9.93443791e-07
Iter: 1066 loss: 9.93449589e-07
Iter: 1067 loss: 9.93443791e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.8
+ date
Thu Oct 22 09:59:52 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/500_500_500_500_1 --function f1 --psi 0 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9425aca620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9425ad37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9425b718c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9425b712f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9425a86620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9425a51e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9425a1b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94259fa1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94259fa950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94259faae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94259761e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f942589b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94258a96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f942597e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9425872d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f942596fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9425872950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94259201e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9425920598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94257d97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f942580a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f942260a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f942580a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f942260c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f942258d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94225abae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94225ab620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94225bf2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94225bf598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94225308c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94225421e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94225c38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94224202f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94225127b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94223a7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9422361f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 2.2159004
test_loss: 2.1374474
train_loss: 0.60146487
test_loss: 3.491327
train_loss: 0.68841344
test_loss: 10.7876
train_loss: 2.227661
test_loss: 3.6393745
train_loss: 4.9440255
test_loss: 3.8056967
train_loss: 2.199947
test_loss: 4.278095
train_loss: 1.9986787
test_loss: 2.5101848
train_loss: 1.9937905
test_loss: 2.40606
train_loss: 1.9985973
test_loss: 6.8468523
train_loss: 1.9960567
test_loss: 2.2454805
train_loss: 0.664706
test_loss: 3.174866
train_loss: 1.9988086
test_loss: 13.113235
train_loss: 1.9984798
test_loss: 2.0087283
train_loss: 1.9971329
test_loss: 2.1687257
train_loss: 1.9992102
test_loss: 2.5919886
train_loss: 1.9963442
test_loss: 2.8915608
train_loss: 1.9988656
test_loss: 2.2420104
train_loss: 1.9938354
test_loss: 2.003625
train_loss: 1.9828321
test_loss: 1.9968435
train_loss: 1.9809502
test_loss: 1.9866852
train_loss: 1.9807644
test_loss: 1.998706
train_loss: 1.9931207
test_loss: 1.9925526
train_loss: 1.9761764
test_loss: 1.9851421
train_loss: 1.9759679
test_loss: 2.0012934
train_loss: 1.9844646
test_loss: 1.9871159
train_loss: 1.9846016
test_loss: 1.9878722
train_loss: 1.9888741
test_loss: 1.9843862
train_loss: 1.9815679
test_loss: 1.9870063
train_loss: 1.9774041
test_loss: 1.9898068
train_loss: 1.9965167
test_loss: 1.9936588
train_loss: 1.9621632
test_loss: 1.9885992
train_loss: 1.9761279
test_loss: 1.9906785
train_loss: 1.9892163
test_loss: 1.9893374
train_loss: 1.9895408
test_loss: 1.9896431
train_loss: 1.9933571
test_loss: 1.9881179
train_loss: 1.9808064
test_loss: 1.988636
train_loss: 1.9790113
test_loss: 1.9877547
train_loss: 1.9839505
test_loss: 1.9972659
train_loss: 1.9760656
test_loss: 1.9882061
train_loss: 1.9806035
test_loss: 1.9888391
train_loss: 1.9867599
test_loss: 2.0113428
train_loss: 1.9930816
test_loss: 2.0522015
train_loss: 1.9767634
test_loss: 1.9869907
train_loss: 1.9892952
test_loss: 1.9876752
train_loss: 1.9829501
test_loss: 1.9875574
train_loss: 1.994121
test_loss: 1.993932
train_loss: 1.987018
test_loss: 1.9883894
train_loss: 1.9904246
test_loss: 1.986491
train_loss: 1.9914461
test_loss: 1.9853384
train_loss: 1.997114
test_loss: 1.9897429
train_loss: 1.9912944
test_loss: 1.9857438
train_loss: 1.9892155
test_loss: 1.9849979
train_loss: 1.990919
test_loss: 1.9914199
train_loss: 1.9905031
test_loss: 1.9890445
train_loss: 1.9888302
test_loss: 1.9960976
train_loss: 1.9890411
test_loss: 2.0000603
train_loss: 1.9882396
test_loss: 1.9872147
train_loss: 1.9884121
test_loss: 1.9890914
train_loss: 1.9944783
test_loss: 1.9859184
train_loss: 1.9875313
test_loss: 1.9944491
train_loss: 1.9835846
test_loss: 1.9842132
train_loss: 1.9955102
test_loss: 1.9870478
train_loss: 1.9738678
test_loss: 2.036511
train_loss: 1.9839516
test_loss: 1.9893141
train_loss: 2.2143402
test_loss: 1.9886516
train_loss: 1.9952375
test_loss: 1.9877433
train_loss: 1.9798421
test_loss: 1.9869862
train_loss: 1.9843038
test_loss: 1.9922451
train_loss: 1.9827652
test_loss: 1.9881996
train_loss: 1.974271
test_loss: 2.0360713
train_loss: 1.9789901
test_loss: 1.9851694
train_loss: 1.980333
test_loss: 1.9863517
train_loss: 2.039454
test_loss: 1.9871083
train_loss: 2.0588992
test_loss: 1.9834744
train_loss: 1.9712052
test_loss: 1.9917284
train_loss: 1.9777963
test_loss: 1.9890919
train_loss: 1.9958049
test_loss: 1.988799
train_loss: 1.9889616
test_loss: 1.9877743
train_loss: 1.9853024
test_loss: 2.007291
train_loss: 1.9894046
test_loss: 1.9937581
train_loss: 1.9804972
test_loss: 1.9946474
train_loss: 1.9839625
test_loss: 2.0086093
train_loss: 1.9921073
test_loss: 1.9925307
train_loss: 1.9911175
test_loss: 1.9909037
train_loss: 1.9845703
test_loss: 1.9892094
train_loss: 1.9892852
test_loss: 1.9896214
train_loss: 1.9869344
test_loss: 1.9941046
train_loss: 1.9726274
test_loss: 1.9930882
train_loss: 1.9939749
test_loss: 1.9884228
train_loss: 1.987826
test_loss: 1.9889557
train_loss: 1.9965049
test_loss: 1.992901
train_loss: 1.9832361
test_loss: 1.992327
train_loss: 1.9875467
test_loss: 1.9879986
train_loss: 1.9798033
test_loss: 1.9888084
train_loss: 1.987865
test_loss: 1.9873171
train_loss: 1.986132
test_loss: 1.9911495
train_loss: 1.9932806
test_loss: 2.0350487
train_loss: 1.9939073
test_loss: 1.9865963
train_loss: 1.989169
test_loss: 1.987823
train_loss: 1.9737315
test_loss: 1.991603
train_loss: 1.9645613
test_loss: 1.9874027
train_loss: 1.9898237
test_loss: 1.9848843
train_loss: 1.9785848
test_loss: 1.9878039
train_loss: 1.9902408
test_loss: 2.0132585
train_loss: 1.9722743
test_loss: 1.9918793
train_loss: 1.9846923
test_loss: 1.9867022
train_loss: 1.9967722
test_loss: 1.9916779
train_loss: 3.0386372
test_loss: 1.9903393
train_loss: 1.9860135
test_loss: 1.9865583
train_loss: 1.9919329
test_loss: 1.9865593
train_loss: 1.9957277
test_loss: 1.9859585
train_loss: 1.984217
test_loss: 1.9853058
train_loss: 1.9921479
test_loss: 1.985687
train_loss: 1.9793786
test_loss: 1.9884977
train_loss: 1.9800117
test_loss: 1.9869554
train_loss: 1.9970996
test_loss: 1.9855219
train_loss: 1.9984553
test_loss: 1.9912952
train_loss: 1.9826193
test_loss: 1.9853001
train_loss: 1.9797642
test_loss: 1.9903626
train_loss: 1.9942918
test_loss: 1.9899086
train_loss: 1.9843031
test_loss: 1.9856329
train_loss: 2.0095165
test_loss: 1.9839097
train_loss: 1.9895251
test_loss: 1.9850457
train_loss: 1.9672923
test_loss: 1.9813288
train_loss: 1.972321
test_loss: 1.9835478
train_loss: 1.9994922
test_loss: 1.9829288
train_loss: 1.9877838
test_loss: 1.9829329
train_loss: 1.9928784
test_loss: 1.9847344
train_loss: 1.9760381
test_loss: 1.9838796
train_loss: 1.9759924
test_loss: 1.9818957
train_loss: 1.9951819
test_loss: 1.9884784
train_loss: 1.9748724
test_loss: 1.9857016
train_loss: 1.973689
test_loss: 1.9906852
train_loss: 1.9736243
test_loss: 1.9849284
train_loss: 1.9887108
test_loss: 1.985424
train_loss: 1.9910012
test_loss: 1.9829468
train_loss: 1.9987204
test_loss: 1.9853616
train_loss: 1.9859883
test_loss: 1.9917282
train_loss: 1.965673
test_loss: 1.9866551
train_loss: 1.957046
test_loss: 1.9885052
train_loss: 1.9666854
test_loss: 1.9860975
train_loss: 1.9914961
test_loss: 1.9890192
train_loss: 1.9952576
test_loss: 1.9899693
train_loss: 2.0778198
test_loss: 1.9862229
train_loss: 1.9701464
test_loss: 1.9867069
train_loss: 1.994334
test_loss: 1.9877778
train_loss: 1.9834237
test_loss: 2.0171275
train_loss: 1.9667711
test_loss: 1.9889377
train_loss: 1.9656849
test_loss: 1.9877542
train_loss: 1.9866705
test_loss: 1.9828463
train_loss: 1.991783
test_loss: 1.9856418
train_loss: 1.992245
test_loss: 1.9866376
train_loss: 1.9716642
test_loss: 1.9870012
train_loss: 1.9914746
test_loss: 1.9855616
train_loss: 1.9938352
test_loss: 1.9855607
train_loss: 1.974319
test_loss: 1.9875945
train_loss: 1.9888127
test_loss: 1.9871445
train_loss: 1.9645629
test_loss: 1.9871142
train_loss: 1.9837362
test_loss: 1.9874886
train_loss: 1.9801226
test_loss: 1.9867673
train_loss: 1.976336
test_loss: 1.992445
train_loss: 1.98086
test_loss: 1.9869778
train_loss: 1.973531
test_loss: 1.9884945
train_loss: 1.9736481
test_loss: 1.986802
train_loss: 1.9970878
test_loss: 1.9932638
train_loss: 1.9847468
test_loss: 1.9828593
train_loss: 1.9915687
test_loss: 1.9849309
train_loss: 1.9918534
test_loss: 1.9863024
train_loss: 2.0301945
test_loss: 1.9850864
train_loss: 1.9808278
test_loss: 1.9852889
train_loss: 1.998627
test_loss: 1.982427
train_loss: 1.9982121
test_loss: 1.9855487
train_loss: 1.976319
test_loss: 1.9841256
train_loss: 1.9949241
test_loss: 1.9862207
train_loss: 1.9797671
test_loss: 1.9887338
train_loss: 2.0001614
test_loss: 1.9869713
train_loss: 1.9984139
test_loss: 1.9870622
train_loss: 1.9927509
test_loss: 1.989929
train_loss: 1.9836928
test_loss: 1.9886909
train_loss: 1.9677747
test_loss: 1.988917
train_loss: 1.9926234
test_loss: 1.9844867
train_loss: 1.9766123
test_loss: 1.9879757
train_loss: 1.9819999
test_loss: 1.9849777
train_loss: 1.9895225
test_loss: 1.9838576
train_loss: 1.9791075
test_loss: 1.987081
train_loss: 1.9723438
test_loss: 1.988535
train_loss: 1.9991486
test_loss: 1.9905013
train_loss: 1.9820282
test_loss: 1.9852872
train_loss: 1.9972959
test_loss: 1.9890252
train_loss: 1.9838486
test_loss: 1.9831727
train_loss: 1.9915329
test_loss: 1.9863921
train_loss: 1.9845216
test_loss: 1.9853184
train_loss: 1.9939889
test_loss: 1.9862229
train_loss: 1.9985623
test_loss: 1.9857908
train_loss: 1.986658
test_loss: 1.9865066
train_loss: 1.978531
test_loss: 1.9867531
train_loss: 1.9898372
test_loss: 1.9900903
train_loss: 1.9970075
test_loss: 1.9868393
train_loss: 1.9822874
test_loss: 1.9855211
train_loss: 1.9712361
test_loss: 1.9906596
train_loss: 1.9880698
test_loss: 1.9896373
train_loss: 1.9829929
test_loss: 2.0136838
train_loss: 1.989345
test_loss: 1.985819
train_loss: 1.9970808
test_loss: 1.9894656
train_loss: 1.9949312
test_loss: 1.9880298
train_loss: 1.985342
test_loss: 1.9856348
train_loss: 1.9869137
test_loss: 1.9882226
train_loss: 1.9762913
test_loss: 1.9917321
train_loss: 1.9881699
test_loss: 1.987889
train_loss: 1.9941132
test_loss: 1.9847069
train_loss: 1.9716653
test_loss: 1.9901574
train_loss: 1.979505
test_loss: 1.9939537
train_loss: 1.992485
test_loss: 1.9865626
train_loss: 1.9755995
test_loss: 1.9871919
train_loss: 1.999691
test_loss: 1.9856397
train_loss: 1.99279
test_loss: 1.9863949
train_loss: 1.9695604
test_loss: 1.988463
train_loss: 1.9787351
test_loss: 1.9894692
train_loss: 1.9798155
test_loss: 1.989418
train_loss: 1.9970877
test_loss: 1.9913923
train_loss: 1.9807324
test_loss: 1.9870666
train_loss: 1.9916173
test_loss: 1.9907842
train_loss: 1.9772987
test_loss: 1.9863893
train_loss: 1.9855154
test_loss: 1.9909807
train_loss: 1.9863772
test_loss: 1.9884257
train_loss: 1.9884294
test_loss: 1.9899812
train_loss: 1.9875126
test_loss: 1.9895556
train_loss: 1.9747863
test_loss: 1.9882226
train_loss: 1.9614089
test_loss: 1.9835454
train_loss: 1.9715319
test_loss: 1.9891065
train_loss: 1.9693713
test_loss: 1.9983157
train_loss: 1.9789884
test_loss: 1.9858484
train_loss: 1.9811026
test_loss: 1.9892563
train_loss: 1.9730401
test_loss: 1.990981
train_loss: 1.9703536
test_loss: 1.9883479
train_loss: 1.9709632
test_loss: 1.9870493
train_loss: 1.9948492
test_loss: 1.9870119
train_loss: 1.9733626
test_loss: 1.9893255
train_loss: 1.981259
test_loss: 1.9885713
train_loss: 1.9802353
test_loss: 1.9881359
train_loss: 1.9635686
test_loss: 1.9873294
train_loss: 1.9885691
test_loss: 1.9957787
train_loss: 1.9722224
test_loss: 1.9872369
train_loss: 1.9756279
test_loss: 1.991211
train_loss: 1.9931079
test_loss: 1.9890453
train_loss: 1.9797214
test_loss: 1.9883082
train_loss: 1.9732208
test_loss: 1.9901252
train_loss: 1.9872131
test_loss: 1.9892278
train_loss: 1.9869066
test_loss: 1.9880904
train_loss: 1.9737632
test_loss: 1.986643
train_loss: 1.9846989
test_loss: 1.986207
train_loss: 1.9703479
test_loss: 1.9872849
train_loss: 1.9877703
test_loss: 1.9937124
train_loss: 1.9954898
test_loss: 1.982166
train_loss: 1.9994109
test_loss: 1.9885155
train_loss: 1.9920783
test_loss: 1.9887828
train_loss: 1.979127
test_loss: 1.9834751
train_loss: 1.9839542
test_loss: 1.984372
train_loss: 1.9937642
test_loss: 1.9858309
train_loss: 1.9967291
test_loss: 1.9858813
train_loss: 1.9918426
test_loss: 1.9903712
train_loss: 1.9732198
test_loss: 1.9875283
train_loss: 1.9980856
test_loss: 1.9884859
train_loss: 1.9783428
test_loss: 1.9859341
train_loss: 1.9701226
test_loss: 1.9864742
train_loss: 1.973153
test_loss: 1.9835734
train_loss: 1.9768836
test_loss: 1.9880508
train_loss: 1.9899118
test_loss: 1.995237
train_loss: 1.9775316
test_loss: 1.984469
train_loss: 1.9751025
test_loss: 1.9858855
train_loss: 1.9899197
test_loss: 1.9897704
train_loss: 1.9887247
test_loss: 1.985045
train_loss: 1.9738749
test_loss: 1.9836397
train_loss: 1.9715856
test_loss: 1.9856458
train_loss: 1.9920021
test_loss: 1.9874003
train_loss: 1.9891682
test_loss: 1.9879037
train_loss: 1.9803014
test_loss: 1.9897219
train_loss: 1.9926088
test_loss: 1.9853791
train_loss: 1.9877819
test_loss: 1.9885875
train_loss: 1.9756749
test_loss: 1.9853928
train_loss: 1.9901477
test_loss: 1.9863046
train_loss: 1.950314
test_loss: 1.9863833
train_loss: 1.9915817
test_loss: 1.9886379
train_loss: 1.9807341
test_loss: 1.9875195
train_loss: 1.9883097
test_loss: 1.9859426
train_loss: 1.9827865
test_loss: 1.9878522
train_loss: 1.9746251
test_loss: 1.9829111
train_loss: 1.9896505
test_loss: 1.987062
train_loss: 1.9931514
test_loss: 1.9854133
train_loss: 1.9911828
test_loss: 1.9846259
train_loss: 1.9624944
test_loss: 1.9868695
train_loss: 3.8655963
test_loss: 2.0489411
train_loss: 1.9985819
test_loss: 3.043558
train_loss: 0.2989992
test_loss: 1.6046207
train_loss: 1.9977683
test_loss: 1.901849
train_loss: 1.9958304
test_loss: 2.0177898
train_loss: 1.9962037
test_loss: 1.9920709
train_loss: 2.0227861
test_loss: 1.9931831
train_loss: 1.9967421
test_loss: 1.9943837
train_loss: 1.9982327
test_loss: 1.9934627
train_loss: 1.9740762
test_loss: 1.9923228
train_loss: 1.9847853
test_loss: 1.9920821
train_loss: 1.97412
test_loss: 1.9965271
train_loss: 1.9811652
test_loss: 1.9944853
train_loss: 1.9829859
test_loss: 1.9919584
train_loss: 1.9795413
test_loss: 1.9905471
train_loss: 1.9826664
test_loss: 1.9900917
train_loss: 1.9916615
test_loss: 1.9899856
train_loss: 1.9839106
test_loss: 1.9997771
train_loss: 1.972231
test_loss: 1.9911337
train_loss: 1.9849663
test_loss: 1.9933197
train_loss: 1.9897361
test_loss: 1.9911528
train_loss: 1.9823562
test_loss: 1.990915
train_loss: 1.9821632
test_loss: 1.9922038
train_loss: 1.9861759
test_loss: 1.9915249
train_loss: 1.9915247
test_loss: 1.9972049
train_loss: 2.0821204
test_loss: 1.9904631
train_loss: 1.9803158
test_loss: 1.9899879
train_loss: 1.9837286
test_loss: 1.9905902
train_loss: 1.9858696
test_loss: 1.9990587
train_loss: 1.9911969
test_loss: 1.9910945
train_loss: 1.9852674
test_loss: 2.001016
train_loss: 1.9853867
test_loss: 1.9894785
train_loss: 1.9646676
test_loss: 1.9894314
train_loss: 1.9939947
test_loss: 1.9916219
train_loss: 1.9839555
test_loss: 1.9913288
train_loss: 1.9948701
test_loss: 1.9864063
train_loss: 1.9907086
test_loss: 1.9902027
train_loss: 1.9939188
test_loss: 2.0997086
train_loss: 1.9961631
test_loss: 1.9980689
train_loss: 1.9922062
test_loss: 2.0108175
train_loss: 1.9982969
test_loss: 2.0066173
train_loss: 1.9965417
test_loss: 1.9987626
train_loss: 1.9956069
test_loss: 2.005632
train_loss: 1.9880537
test_loss: 2.0176466
train_loss: 1.9963198
test_loss: 1.9984313
train_loss: 1.9862124
test_loss: 2.040297
train_loss: 1.9963605
test_loss: 2.0233567
train_loss: 1.9850907
test_loss: 2.1154673
train_loss: 1.996137
test_loss: 2.0005927
train_loss: 1.9921197
test_loss: 2.0240602
train_loss: 1.9881268
test_loss: 2.0003161
train_loss: 1.9902169
test_loss: 2.0009599
train_loss: 1.9900901
test_loss: 1.9919136
train_loss: 2.4647212
test_loss: 2.0171232
train_loss: 1.9918545
test_loss: 1.9967598
train_loss: 1.992703
test_loss: 1.9965942
train_loss: 1.9939646
test_loss: 2.0642633
train_loss: 2.001484
test_loss: 1.9932388
train_loss: 1.9820932
test_loss: 2.0061731
train_loss: 1.9981802
test_loss: 2.0085247
train_loss: 1.9967217
test_loss: 2.020461
train_loss: 1.9988177
test_loss: 2.0120623
train_loss: 1.9928629
test_loss: 2.0215347
train_loss: 1.9949892
test_loss: 1.9956574
train_loss: 2.0348012
test_loss: 1.9934983
train_loss: 1.9887847
test_loss: 1.9980316
train_loss: 1.980639
test_loss: 2.009523
train_loss: 1.9759703
test_loss: 2.0018268
train_loss: 1.986315
test_loss: 1.9921596
train_loss: 1.9972913
test_loss: 1.997982
train_loss: 1.9842647
test_loss: 1.9923729
train_loss: 1.9875832
test_loss: 2.4465446
train_loss: 1.981664
test_loss: 2.0103343
train_loss: 1.9814165
test_loss: 1.9928782
train_loss: 1.9926336
test_loss: 1.994112
train_loss: 1.9922493
test_loss: 2.2984164
train_loss: 1.9938507
test_loss: 1.9954172
train_loss: 1.9868835
test_loss: 2.000838
train_loss: 1.9948032
test_loss: 2.0102017
train_loss: 1.979022
test_loss: 2.003298
train_loss: 1.9935696
test_loss: 2.0034945
train_loss: 1.9952497
test_loss: 1.9971527
train_loss: 2.0205781
test_loss: 2.0107634
train_loss: 1.982878
test_loss: 2.0362139
train_loss: 1.9749033
test_loss: 2.003671
train_loss: 1.9773979
test_loss: 2.0120592
train_loss: 1.9845742
test_loss: 2.070066
train_loss: 1.9931766
test_loss: 1.9958509
train_loss: 1.9869245
test_loss: 1.9953895
train_loss: 1.993103
test_loss: 1.9940847
train_loss: 1.9904739
test_loss: 1.9998232
train_loss: 1.9902887
test_loss: 1.9937061
train_loss: 1.9813546
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 2.028142
train_loss: 1.9875338
test_loss: 1.9992931
train_loss: 1.9910299
test_loss: 1.993012
train_loss: 1.9763018
test_loss: 1.9978654
train_loss: 1.9901541
test_loss: 1.9983436
train_loss: 1.9881183
test_loss: 2.067289
train_loss: 1.9814419
test_loss: 2.0279071
train_loss: 1.9821807
test_loss: 2.0227606
train_loss: 1.9915428
test_loss: 1.9968029
train_loss: 1.9793756
test_loss: 1.9987391
train_loss: 1.9894735
test_loss: 1.9982427
train_loss: 1.9920077
test_loss: 2.0037532
train_loss: 1.987567
test_loss: 2.013089
train_loss: 1.981746
test_loss: 1.9947511
train_loss: 1.9836195
test_loss: 2.0249774
train_loss: 1.9978838
test_loss: 1.9949574
train_loss: 1.9945436
test_loss: 1.9922677
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi2.8/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e959598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e98b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e9da730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18ea78730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18ea78840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18ea780d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18ea781e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e8a86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e8a8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e85c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e85c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e826d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e83aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e7eb2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e7ef7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e7b6620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e7b6bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e7b6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e7b66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e6d5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e6f36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e6870d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e6f3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe18e6c2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe183cbb488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe183c62b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe183c62730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe183c90378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe183c90048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe183c02950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe183c11268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe183bb4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe183b65598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe183bb5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe183b450d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe183ad8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 562.295471
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Traceback (most recent call last):
  File "biholoNN_train.py", line 169, in <module>
    results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=train_func, initial_position=init_params, max_iterations=max_epochs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 287, in minimize
    parallel_iterations=parallel_iterations)[0]
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 574, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2499, in while_loop_v2
    return_same_structure=True)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2735, in while_loop
    loop_vars = body(*loop_vars)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 260, in _body
    max_line_search_iterations)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 156, in line_search_step
    max_iterations=max_iterations)  # No search needed for these.
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 259, in hager_zhang
    threshold_use_approximate_wolfe_condition)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 609, in _prepare_args
    val_initial = value_and_gradients_function(initial_step_size)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 251, in _restricted_func
    objective_value, gradient = value_and_gradients_function(pt)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 814, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
	 [[Sum_1/_34]]
  (1) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_f_16876]

Function call stack:
f -> f

++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi3
+ date
Thu Oct 22 11:57:59 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/500_500_500_500_1 --function f1 --psi 0 --phi 3 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896fc5cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896fbcd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896fb977b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896fb449d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896fb28f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896fc19620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896fc19ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896faf8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896faf8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896faf8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896fa652f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896f9e7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896f9e70d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896f9d70d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896f9ef510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896f9d7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896f9b1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896f9ae510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896f9b1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f894e486400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896f961620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896f94d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f896f961f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f894e3f1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f894e386488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f894e39db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f894e39d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f894e3b4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f894e3b4048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8928297950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8928236268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f89281c7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f89282dc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f89281d3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f89282260d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f892821df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 1.9957166
test_loss: 1.9959389
train_loss: 1.9991801
test_loss: 2.029613
train_loss: 1.9978178
test_loss: 1.9950681
train_loss: 1.9879355
test_loss: 2.0030081
train_loss: 1.9894607
test_loss: 2.0015674
train_loss: 1.9889939
test_loss: 1.9899904
train_loss: 1.9951059
test_loss: 2.119324
train_loss: 1.9974413
test_loss: 1.9911808
train_loss: 1.9986755
test_loss: 2.0631151
train_loss: 1.9766203
test_loss: 1.9913737
train_loss: 1.9973313
test_loss: 1.9985555
train_loss: 1.9957318
test_loss: 1.9970602
train_loss: 1.978873
test_loss: 1.9924661
train_loss: 1.9947276
test_loss: 1.9899436
train_loss: 1.9985409
test_loss: 1.9900758
train_loss: 1.9714657
test_loss: 1.9908683
train_loss: 1.9945537
test_loss: 1.9917529
train_loss: 1.9987925
test_loss: 1.9921361
train_loss: 1.9976764
test_loss: 2.5017178
train_loss: 1.9924667
test_loss: 2.1965468
train_loss: 2.0342534
test_loss: 2.1218607
train_loss: 2.0640693
test_loss: 2.0850558
train_loss: 2.0008419
test_loss: 2.8061512
train_loss: 1.9983753
test_loss: 2.0078795
train_loss: 1.9996822
test_loss: 2.023546
train_loss: 1.9936163
test_loss: 2.0069091
train_loss: 1.9982975
test_loss: 2.0492032
train_loss: 2.0766027
test_loss: 2.9337652
train_loss: 2.0265932
test_loss: 2.3494525
train_loss: 2.7834568
test_loss: 2.212174
train_loss: 2.2851882
test_loss: 2.728462
train_loss: 2.0780332
test_loss: 2.1138425
train_loss: 1.9981401
test_loss: 2.2333584
train_loss: 1.9963334
test_loss: 1.9954094
train_loss: 1.9992813
test_loss: 1.9960169
train_loss: 1.99833
test_loss: 1.9947491
train_loss: 1.9848984
test_loss: 1.9950321
train_loss: 1.9961306
test_loss: 1.9951476
train_loss: 1.991054
test_loss: 1.9953878
train_loss: 1.9963131
test_loss: 1.9951977
train_loss: 1.9860849
test_loss: 1.9956297
train_loss: 1.9888532
test_loss: 1.9948369
train_loss: 1.9876287
test_loss: 1.9960335
train_loss: 1.9972245
test_loss: 1.9941542
train_loss: 1.9962962
test_loss: 1.9952992
train_loss: 1.9879969
test_loss: 1.9950181
train_loss: 1.9897866
test_loss: 1.9937485
train_loss: 1.9829214
test_loss: 1.9948831
train_loss: 1.9913839
test_loss: 1.993736
train_loss: 1.9890345
test_loss: 1.9949889
train_loss: 1.9843576
test_loss: 1.99517
train_loss: 1.9972008
test_loss: 1.9957916
train_loss: 1.9886293
test_loss: 1.9950995
train_loss: 1.9936218
test_loss: 1.995444
train_loss: 1.9929981
test_loss: 1.9950076
train_loss: 1.9952801
test_loss: 1.9946549
train_loss: 1.9852962
test_loss: 1.9947854
train_loss: 1.9921062
test_loss: 1.9958888
train_loss: 1.995648
test_loss: 1.9954754
train_loss: 1.988218
test_loss: 1.9943384
train_loss: 1.9943972
test_loss: 1.9951288
train_loss: 1.9908476
test_loss: 1.9918269
train_loss: 1.9930329
test_loss: 1.9947152
train_loss: 1.9935932
test_loss: 1.9949625
train_loss: 1.9946812
test_loss: 1.9948075
train_loss: 1.9910969
test_loss: 1.9955635
train_loss: 1.994005
test_loss: 1.9951603
train_loss: 1.9799362
test_loss: 1.9954139
train_loss: 1.9938821
test_loss: 1.9956894
train_loss: 1.9908278
test_loss: 1.9940909
train_loss: 1.9933958
test_loss: 1.9958962
train_loss: 1.9883448
test_loss: 1.9956919
train_loss: 1.9890733
test_loss: 1.9958693
train_loss: 1.9943911
test_loss: 1.9958773
train_loss: 1.9967837
test_loss: 1.9952323
train_loss: 1.9920442
test_loss: 1.9950715
train_loss: 1.988981
test_loss: 1.9954896
train_loss: 1.9942046
test_loss: 1.9943745
train_loss: 1.9912913
test_loss: 1.9947617
train_loss: 1.9851583
test_loss: 1.9962003
train_loss: 1.9966356
test_loss: 1.9951173
train_loss: 1.9922872
test_loss: 1.9945353
train_loss: 1.9866992
test_loss: 1.9953729
train_loss: 1.9888175
test_loss: 1.9958564
train_loss: 1.9955252
test_loss: 1.9954853
train_loss: 1.9922523
test_loss: 1.9947617
train_loss: 1.9792569
test_loss: 1.9947523
train_loss: 1.9941727
test_loss: 1.9950192
train_loss: 1.9876264
test_loss: 1.9952122
train_loss: 1.9939008
test_loss: 1.9965373
train_loss: 1.9915326
test_loss: 1.9956028
train_loss: 1.994103
test_loss: 1.9946533
train_loss: 1.9940189
test_loss: 1.9958415
train_loss: 1.992773
test_loss: 1.9952419
train_loss: 1.9928358
test_loss: 1.9958159
train_loss: 1.9944111
test_loss: 1.9952383
train_loss: 1.9954257
test_loss: 1.9959761
train_loss: 1.9754658
test_loss: 1.9956422
train_loss: 1.991509
test_loss: 1.9954903
train_loss: 1.9879055
test_loss: 1.9951283
train_loss: 1.995155
test_loss: 1.9965514
train_loss: 1.9870112
test_loss: 1.995175
train_loss: 1.9937979
test_loss: 1.9947131
train_loss: 1.9840126
test_loss: 1.9952694
train_loss: 1.9938604
test_loss: 1.9940561
train_loss: 1.9900733
test_loss: 1.9954717
train_loss: 1.9925476
test_loss: 1.995159
train_loss: 1.9914389
test_loss: 1.9961665
train_loss: 1.992995
test_loss: 1.9948469
train_loss: 1.987193
test_loss: 1.9967841
train_loss: 1.9865321
test_loss: 1.9942774
train_loss: 1.9902738
test_loss: 1.9959861
train_loss: 1.9885702
test_loss: 1.9957025
train_loss: 1.9918805
test_loss: 1.9950796
train_loss: 1.9833367
test_loss: 1.9947523
train_loss: 1.9944916
test_loss: 1.9956853
train_loss: 1.9849421
test_loss: 1.9960669
train_loss: 1.9772797
test_loss: 1.9961765
train_loss: 1.9946165
test_loss: 1.9945349
train_loss: 1.9929297
test_loss: 1.9946913
train_loss: 1.9936166
test_loss: 1.9953675
train_loss: 1.9901478
test_loss: 1.9963247
train_loss: 1.9910648
test_loss: 1.995157
train_loss: 1.9897001
test_loss: 1.9949031
train_loss: 1.9878997
test_loss: 1.9936818
train_loss: 1.9882859
test_loss: 1.9951673
train_loss: 1.991034
test_loss: 1.9957675
train_loss: 1.9891682
test_loss: 1.9945025
train_loss: 1.991734
test_loss: 1.9960402
train_loss: 1.9920962
test_loss: 1.9954153
train_loss: 1.9851344
test_loss: 1.9962445
train_loss: 1.9891822
test_loss: 1.9952377
train_loss: 1.9890401
test_loss: 1.9952563
train_loss: 1.9868987
test_loss: 1.9952697
train_loss: 1.9888597
test_loss: 1.9954245
train_loss: 1.9870899
test_loss: 1.9946232
train_loss: 1.9785964
test_loss: 1.9951476
train_loss: 1.9768397
test_loss: 1.9955183
train_loss: 1.9933597
test_loss: 1.9956665
train_loss: 1.986447
test_loss: 1.9955868
train_loss: 1.9849017
test_loss: 1.9953421
train_loss: 1.9917158
test_loss: 1.9949546
train_loss: 1.9877068
test_loss: 1.9944317
train_loss: 1.9803187
test_loss: 1.9958675
train_loss: 1.9803903
test_loss: 1.9948783
train_loss: 1.9810492
test_loss: 1.9961464
train_loss: 1.9846662
test_loss: 1.9961853
train_loss: 1.9849653
test_loss: 1.9947623
train_loss: 1.9882895
test_loss: 1.9956043
train_loss: 1.9917129
test_loss: 1.9952602
train_loss: 1.9883623
test_loss: 1.9951693
train_loss: 1.9882115
test_loss: 1.9958642
train_loss: 1.9765801
test_loss: 1.9946768
train_loss: 1.986021
test_loss: 1.9960866
train_loss: 1.9872763
test_loss: 1.995181
train_loss: 1.9853249
test_loss: 1.9959242
train_loss: 1.987109
test_loss: 1.9930358
train_loss: 1.9865927
test_loss: 1.9957404
train_loss: 1.9905819
test_loss: 1.9942741
train_loss: 1.994307
test_loss: 1.9957604
train_loss: 1.9878138
test_loss: 1.9952917
train_loss: 1.983711
test_loss: 1.994836
train_loss: 1.9833791
test_loss: 1.9958897
train_loss: 1.9945678
test_loss: 1.9954576
train_loss: 1.9915732
test_loss: 1.9948
train_loss: 1.99372
test_loss: 1.9945406
train_loss: 1.989495
test_loss: 1.9935414
train_loss: 1.9937958
test_loss: 1.9949309
train_loss: 1.9803115
test_loss: 1.9944963
train_loss: 1.990584
test_loss: 1.9948921
train_loss: 1.984343
test_loss: 1.9943587
train_loss: 1.9887094
test_loss: 1.9957275
train_loss: 1.9915944
test_loss: 1.9943687
train_loss: 1.9882255
test_loss: 1.9951524
train_loss: 1.9816592
test_loss: 1.9942079
train_loss: 1.9900804
test_loss: 1.9946817
train_loss: 1.989261
test_loss: 1.995035
train_loss: 1.9874306
test_loss: 1.9957572
train_loss: 1.9865547
test_loss: 1.9954531
train_loss: 1.9895166
test_loss: 1.9960057
train_loss: 1.9843
test_loss: 1.9947302
train_loss: 1.986902
test_loss: 1.9942654
train_loss: 1.9887111
test_loss: 1.9953121
train_loss: 1.984685
test_loss: 1.9952047
train_loss: 1.979751
test_loss: 1.9949398
train_loss: 1.9897978
test_loss: 1.994383
train_loss: 1.9900341
test_loss: 1.9952043
train_loss: 1.9799763
test_loss: 1.9946797
train_loss: 1.9917402
test_loss: 1.9958513
train_loss: 1.9861838
test_loss: 1.9956945
train_loss: 1.9878521
test_loss: 1.9955431
train_loss: 1.9906033
test_loss: 1.9952644
train_loss: 1.9816732
test_loss: 1.9954648
train_loss: 1.9898834
test_loss: 1.9953754
train_loss: 1.9902267
test_loss: 1.9948487
train_loss: 1.9864788
test_loss: 1.9954859
train_loss: 1.9926392
test_loss: 1.9956056
train_loss: 1.9834648
test_loss: 1.9953701
train_loss: 1.9903429
test_loss: 1.9953748
train_loss: 1.9905739
test_loss: 1.9964954
train_loss: 1.9920341
test_loss: 1.9940776
train_loss: 1.9891714
test_loss: 1.9944252
train_loss: 1.9828124
test_loss: 1.9963399
train_loss: 1.9898427
test_loss: 1.9948173
train_loss: 1.9898067
test_loss: 1.9955595
train_loss: 1.9908428
test_loss: 1.9956862
train_loss: 1.9888692
test_loss: 1.994597
train_loss: 1.9825047
test_loss: 1.9944806
train_loss: 1.9842649
test_loss: 1.9960005
train_loss: 1.9877388
test_loss: 1.9947511
train_loss: 1.9912903
test_loss: 1.9945203
train_loss: 1.980642
test_loss: 1.995199
train_loss: 1.9895238
test_loss: 1.9959217
train_loss: 1.9896672
test_loss: 1.9960963
train_loss: 1.9933108
test_loss: 1.9957213
train_loss: 1.977635
test_loss: 1.9953321
train_loss: 1.9886243
test_loss: 1.9944211
train_loss: 1.9873357
test_loss: 1.9956763
train_loss: 1.9912416
test_loss: 1.9943349
train_loss: 1.9842286
test_loss: 1.9952121
train_loss: 1.9918823
test_loss: 1.9941033
train_loss: 1.9887112
test_loss: 1.9959803
train_loss: 1.9916828
test_loss: 1.9962173
train_loss: 1.9864304
test_loss: 1.9958284
train_loss: 1.9786301
test_loss: 1.9961753
train_loss: 1.9815059
test_loss: 1.9959283
train_loss: 1.990743
test_loss: 1.9955928
train_loss: 1.9908253
test_loss: 1.9941306
train_loss: 1.9839929
test_loss: 1.9950004
train_loss: 1.9845549
test_loss: 1.9952294
train_loss: 1.9923942
test_loss: 1.9939866
train_loss: 1.9926965
test_loss: 1.9951987
train_loss: 1.9894276
test_loss: 1.9960014
train_loss: 1.9803473
test_loss: 1.9961791
train_loss: 1.9900742
test_loss: 1.994513
train_loss: 1.9862363
test_loss: 1.9953098
train_loss: 1.9869304
test_loss: 1.9952612
train_loss: 1.9912388
test_loss: 1.9957538
train_loss: 1.9863056
test_loss: 1.9952097
train_loss: 1.9890293
test_loss: 1.9957509
train_loss: 1.9880718
test_loss: 1.9950497
train_loss: 1.9827659
test_loss: 1.9939663
train_loss: 1.9828635
test_loss: 1.9962056
train_loss: 1.9856312
test_loss: 1.9956778
train_loss: 1.9910718
test_loss: 1.9967687
train_loss: 1.9915497
test_loss: 1.9954613
train_loss: 1.9914523
test_loss: 1.9945457
train_loss: 1.9860916
test_loss: 1.9952512
train_loss: 1.9916971
test_loss: 1.9946513
train_loss: 1.9885564
test_loss: 1.9943602
train_loss: 1.9853888
test_loss: 1.9953759
train_loss: 1.9901006
test_loss: 1.9942149
train_loss: 1.99122
test_loss: 1.9948275
train_loss: 1.9874536
test_loss: 1.9956735
train_loss: 1.9865472
test_loss: 1.9952056
train_loss: 1.9909544
test_loss: 1.9947927
train_loss: 1.9762071
test_loss: 1.994721
train_loss: 1.9838274
test_loss: 1.9954157
train_loss: 1.9894392
test_loss: 1.9946854
train_loss: 1.9847331
test_loss: 1.9953954
train_loss: 1.9722255
test_loss: 1.993829
train_loss: 1.9917766
test_loss: 1.994977
train_loss: 1.9874041
test_loss: 1.9955723
train_loss: 1.9895188
test_loss: 1.9953456
train_loss: 1.9864922
test_loss: 1.9960371
train_loss: 1.9836906
test_loss: 1.9953737
train_loss: 1.9874126
test_loss: 1.9949578
train_loss: 1.9872886
test_loss: 1.9948782
train_loss: 1.9838067
test_loss: 1.994657
train_loss: 1.9805808
test_loss: 1.9941437
train_loss: 1.9868888
test_loss: 1.9943238
train_loss: 1.988673
test_loss: 1.9941176
train_loss: 1.98317
test_loss: 1.994276
train_loss: 1.9858799
test_loss: 1.9949841
train_loss: 1.9804406
test_loss: 1.9938279
train_loss: 1.9836874
test_loss: 1.9932698
train_loss: 1.9891876
test_loss: 1.9953648
train_loss: 1.9865594
test_loss: 1.9954474
train_loss: 1.986023
test_loss: 1.9947942
train_loss: 1.984922
test_loss: 1.99519
train_loss: 1.9944372
test_loss: 1.9949039
train_loss: 1.9881291
test_loss: 1.9947795
train_loss: 1.9862455
test_loss: 1.9946011
train_loss: 1.9769769
test_loss: 1.993572
train_loss: 1.990373
test_loss: 1.9933809
train_loss: 1.9842874
test_loss: 1.9937897
train_loss: 1.9820325
test_loss: 1.9943854
train_loss: 1.9879138
test_loss: 1.9947888
train_loss: 1.9904463
test_loss: 1.9955218
train_loss: 1.9906397
test_loss: 1.9924157
train_loss: 1.975802
test_loss: 1.9945878
train_loss: 1.9804807
test_loss: 1.9958575
train_loss: 1.9729681
test_loss: 1.9952282
train_loss: 1.9873183
test_loss: 1.9941769
train_loss: 1.9852853
test_loss: 1.9945368
train_loss: 1.9840987
test_loss: 1.9939044
train_loss: 1.980422
test_loss: 1.9948496
train_loss: 1.9851505
test_loss: 1.99475
train_loss: 1.9898776
test_loss: 1.9944274
train_loss: 1.9798684
test_loss: 1.9943432
train_loss: 1.9870698
test_loss: 1.9951954
train_loss: 1.9813366
test_loss: 1.9950737
train_loss: 1.9830289
test_loss: 1.9944519
train_loss: 1.982034
test_loss: 1.9948465
train_loss: 1.9838767
test_loss: 1.9946556
train_loss: 1.9856358
test_loss: 1.9949185
train_loss: 1.9794215
test_loss: 1.9944403
train_loss: 1.9835658
test_loss: 1.9948677
train_loss: 1.9862894
test_loss: 1.9951328
train_loss: 1.9876235
test_loss: 1.9953369
train_loss: 1.9887663
test_loss: 1.9951402
train_loss: 1.9855099
test_loss: 1.9949906
train_loss: 1.9874567
test_loss: 1.9947757
train_loss: 1.9828393
test_loss: 1.9953743
train_loss: 1.9791393
test_loss: 1.9945025
train_loss: 1.9859395
test_loss: 1.9931111
train_loss: 1.9880178
test_loss: 1.9929266
train_loss: 1.9861774
test_loss: 1.9955853
train_loss: 1.9818015
test_loss: 1.994531
train_loss: 1.9859073
test_loss: 1.9945687
train_loss: 1.9877225
test_loss: 1.9948602
train_loss: 1.9876406
test_loss: 1.9947978
train_loss: 1.987382
test_loss: 1.9932833
train_loss: 1.9860425
test_loss: 1.9950261
train_loss: 1.9695694
test_loss: 1.9944626
train_loss: 1.9849701
test_loss: 1.995531
train_loss: 1.9886589
test_loss: 1.9927225
train_loss: 1.9847955
test_loss: 1.994436
train_loss: 1.9883248
test_loss: 1.9947957
train_loss: 1.9852328
test_loss: 1.9947885
train_loss: 1.9786487
test_loss: 1.9951917
train_loss: 1.9867026
test_loss: 1.9950278
train_loss: 1.9883499
test_loss: 1.9944459
train_loss: 1.9873706
test_loss: 1.9944232
train_loss: 1.9869883
test_loss: 1.9951693
train_loss: 1.9821949
test_loss: 1.9931597
train_loss: 1.9849617
test_loss: 1.9947809
train_loss: 1.9769924
test_loss: 1.9945124
train_loss: 1.9862225
test_loss: 1.994111
train_loss: 1.9841697
test_loss: 1.9940802
train_loss: 1.9856608
test_loss: 1.9948343
train_loss: 1.983467
test_loss: 1.9943848
train_loss: 1.9770064
test_loss: 1.9946021
train_loss: 1.9849113
test_loss: 1.9945341
train_loss: 1.9829012
test_loss: 1.9950075
train_loss: 1.9787575
test_loss: 1.9944571
train_loss: 1.9855592
test_loss: 1.9940085
train_loss: 1.9850119
test_loss: 1.9936615
train_loss: 1.9882389
test_loss: 1.9926083
train_loss: 1.9861586
test_loss: 1.9940785
train_loss: 1.9792145
test_loss: 1.9944204
train_loss: 1.981637
test_loss: 1.9956536
train_loss: 1.9783581
test_loss: 1.9940187
train_loss: 1.9856819
test_loss: 1.993909
train_loss: 1.9809629
test_loss: 1.9942763
train_loss: 1.9789565
test_loss: 1.9942478
train_loss: 1.9743171
test_loss: 1.993639
train_loss: 1.9824324
test_loss: 1.9943833
train_loss: 1.9887975
test_loss: 1.9952935
train_loss: 1.9819294
test_loss: 1.9945947
train_loss: 1.9899341
test_loss: 1.9951646
train_loss: 1.9867342
test_loss: 1.9955969
train_loss: 1.9836494
test_loss: 1.9948452
train_loss: 1.9831133
test_loss: 1.9960494
train_loss: 1.9841018
test_loss: 1.9946734
train_loss: 1.97543
test_loss: 1.9943312
train_loss: 1.9782279
test_loss: 1.9937067
train_loss: 1.9901155
test_loss: 1.9945228
train_loss: 1.98687
test_loss: 1.9943804
train_loss: 1.9867325
test_loss: 1.9938732
train_loss: 1.9816806
test_loss: 1.9944657
train_loss: 1.9813353
test_loss: 1.9935547
train_loss: 1.9749081
test_loss: 1.9939102
train_loss: 1.9864051
test_loss: 1.9937807
train_loss: 1.9877962
test_loss: 1.9943641
train_loss: 1.9833825
test_loss: 1.9934303
train_loss: 1.9896836
test_loss: 1.9947897
train_loss: 1.9829602
test_loss: 1.993865
train_loss: 1.9824158
test_loss: 1.9946767
train_loss: 1.9830422
test_loss: 1.9949199
train_loss: 1.9818144
test_loss: 1.9938862
train_loss: 1.975064
test_loss: 1.9952813
train_loss: 1.9885681
test_loss: 1.9941281
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 1.9797354
test_loss: 1.9952717
train_loss: 1.9810857
test_loss: 1.9941363
train_loss: 1.9751316
test_loss: 1.9944655
train_loss: 1.9885874
test_loss: 1.9948142
train_loss: 1.9839714
test_loss: 1.9947257
train_loss: 1.9772364
test_loss: 1.9941909
train_loss: 1.9804789
test_loss: 1.9939072
train_loss: 1.9805608
test_loss: 1.9956335
train_loss: 1.9868766
test_loss: 1.9953326
train_loss: 1.983662
test_loss: 1.9923303
train_loss: 1.9836416
test_loss: 1.994241
train_loss: 1.9810387
test_loss: 1.9945519
train_loss: 1.9798715
test_loss: 1.9956108
train_loss: 1.9843948
test_loss: 1.994232
train_loss: 1.9837567
test_loss: 1.9940678
train_loss: 1.9834507
test_loss: 1.9946055
train_loss: 1.9831214
test_loss: 1.9942846
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi3/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 3 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi3/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76702d4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7670370730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76703167b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76703c0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7670286f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7670286268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7670286158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76701de048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7670288ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7670288ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76701c71e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7670167bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76701868c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7670144268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7670186a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7670109f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76701092f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76700d4ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76700ce730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76700dc0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f767003a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f767004b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f767003af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76500bfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7650070488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7650077b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7650077840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7650042378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7650042048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e47e6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e47a8268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e475c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e46ff730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e4748d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e46db0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e46e1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2263.94507
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Traceback (most recent call last):
  File "biholoNN_train.py", line 169, in <module>
    results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=train_func, initial_position=init_params, max_iterations=max_epochs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 287, in minimize
    parallel_iterations=parallel_iterations)[0]
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 574, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2499, in while_loop_v2
    return_same_structure=True)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2735, in while_loop
    loop_vars = body(*loop_vars)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 260, in _body
    max_line_search_iterations)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 156, in line_search_step
    max_iterations=max_iterations)  # No search needed for these.
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 259, in hager_zhang
    threshold_use_approximate_wolfe_condition)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 609, in _prepare_args
    val_initial = value_and_gradients_function(initial_step_size)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 251, in _restricted_func
    objective_value, gradient = value_and_gradients_function(pt)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 814, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
	 [[Sum_1/_34]]
  (1) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_f_16876]

Function call stack:
f -> f

++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/500_500_500_500_1
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi1_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi1_phi0
+ date
Thu Oct 22 13:57:07 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 1 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3728296620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f37282bc400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3728318268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f37283b3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f37282e92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f37282e97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f37282e9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3728306e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3728153510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3728154378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3728306b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f37281d7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f37282051e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f372822eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f372822ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3728205950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3728238840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f372807fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3728016950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3728016ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3727fd10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3706b27a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3706ad4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3706af0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3706af6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3706af6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3706a43a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3706a437b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f37280f3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f37280f3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f37280f8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f36e0159ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f36e0159d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3706a998c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f36e008d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f36e009eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.0042443476
test_loss: 0.003472891
train_loss: 0.0024718961
test_loss: 0.002454281
train_loss: 0.0018402934
test_loss: 0.0019193012
train_loss: 0.0018327198
test_loss: 0.001793501
train_loss: 0.0016474565
test_loss: 0.0018343453
train_loss: 0.0017754743
test_loss: 0.0017184749
train_loss: 0.0016187567
test_loss: 0.0017495507
train_loss: 0.0017827883
test_loss: 0.0017232454
train_loss: 0.0016164008
test_loss: 0.0017738742
train_loss: 0.0016942228
test_loss: 0.0019176815
train_loss: 0.0018445331
test_loss: 0.0016128751
train_loss: 0.0017998057
test_loss: 0.0019332962
train_loss: 0.0016709378
test_loss: 0.0016845181
train_loss: 0.0016238393
test_loss: 0.0016488015
train_loss: 0.0017231985
test_loss: 0.0017106592
train_loss: 0.0016424158
test_loss: 0.0017776854
train_loss: 0.0017255314
test_loss: 0.001931882
train_loss: 0.0019673451
test_loss: 0.0018938279
train_loss: 0.0017196427
test_loss: 0.0017296443
train_loss: 0.0017455914
test_loss: 0.0016577363
train_loss: 0.0017226373
test_loss: 0.0018346739
train_loss: 0.0017500102
test_loss: 0.0018936334
train_loss: 0.0017567559
test_loss: 0.0017490457
train_loss: 0.0016716046
test_loss: 0.0015572704
train_loss: 0.001719594
test_loss: 0.0016609266
train_loss: 0.0016846934
test_loss: 0.0018894514
train_loss: 0.0017037713
test_loss: 0.0017036528
train_loss: 0.001816684
test_loss: 0.0017061218
train_loss: 0.001671525
test_loss: 0.001677816
train_loss: 0.0017956542
test_loss: 0.001641327
train_loss: 0.0016250698
test_loss: 0.0016109984
train_loss: 0.0016200959
test_loss: 0.0016235729
train_loss: 0.0017345203
test_loss: 0.0016649244
train_loss: 0.0017114738
test_loss: 0.0017141921
train_loss: 0.0014783535
test_loss: 0.0015848246
train_loss: 0.001551006
test_loss: 0.0016234899
train_loss: 0.0016879607
test_loss: 0.0016909698
train_loss: 0.0016910783
test_loss: 0.0015347545
train_loss: 0.0017408618
test_loss: 0.0016806999
train_loss: 0.0017221288
test_loss: 0.0016774182
train_loss: 0.0015818891
test_loss: 0.0017009127
train_loss: 0.0017818699
test_loss: 0.0019099185
train_loss: 0.0015621595
test_loss: 0.0016867451
train_loss: 0.0016190703
test_loss: 0.0016079864
train_loss: 0.0018538572
test_loss: 0.0020470219
train_loss: 0.0016897921
test_loss: 0.0016984856
train_loss: 0.0017611205
test_loss: 0.0017241482
train_loss: 0.0016692581
test_loss: 0.0016829422
train_loss: 0.001501332
test_loss: 0.0015447248
train_loss: 0.0015466854
test_loss: 0.0016254443
train_loss: 0.0016465242
test_loss: 0.0017590355
train_loss: 0.0015373221
test_loss: 0.0017281042
train_loss: 0.0017868637
test_loss: 0.0016702579
train_loss: 0.0014996347
test_loss: 0.0016964152
train_loss: 0.0016289947
test_loss: 0.0015686266
train_loss: 0.0017227302
test_loss: 0.0016402878
train_loss: 0.001669623
test_loss: 0.0015914984
train_loss: 0.0015829911
test_loss: 0.0016464646
train_loss: 0.0016802188
test_loss: 0.0017634764
train_loss: 0.0015636282
test_loss: 0.0016260457
train_loss: 0.0016581875
test_loss: 0.0017196996
train_loss: 0.0015346757
test_loss: 0.0016062822
train_loss: 0.0018053718
test_loss: 0.0019511881
train_loss: 0.0017338537
test_loss: 0.0016527079
train_loss: 0.0016518943
test_loss: 0.0017847352
train_loss: 0.0017231889
test_loss: 0.0018249564
train_loss: 0.0019431208
test_loss: 0.0015596419
train_loss: 0.0015454281
test_loss: 0.0015787246
train_loss: 0.0017401623
test_loss: 0.0018438358
train_loss: 0.0017456879
test_loss: 0.0019273598
train_loss: 0.0017097015
test_loss: 0.0016163167
train_loss: 0.0017112823
test_loss: 0.0017080439
train_loss: 0.0014200879
test_loss: 0.0016380248
train_loss: 0.0017199344
test_loss: 0.0017098512
train_loss: 0.0018018899
test_loss: 0.0016943488
train_loss: 0.0016530447
test_loss: 0.001649376
train_loss: 0.0016505016
test_loss: 0.0018978701
train_loss: 0.0016333921
test_loss: 0.0017201221
train_loss: 0.0014988163
test_loss: 0.0017139425
train_loss: 0.0016395422
test_loss: 0.0017658246
train_loss: 0.0017672529
test_loss: 0.0016689055
train_loss: 0.0016068087
test_loss: 0.0017094823
train_loss: 0.0014802306
test_loss: 0.0018122667
train_loss: 0.0016992554
test_loss: 0.0015830337
train_loss: 0.0014919692
test_loss: 0.0016075887
train_loss: 0.0015957961
test_loss: 0.0016918099
train_loss: 0.0016795467
test_loss: 0.0017022567
train_loss: 0.0015930679
test_loss: 0.0017229732
train_loss: 0.0014880622
test_loss: 0.0016706113
train_loss: 0.0017135502
test_loss: 0.0016507681
train_loss: 0.0017051445
test_loss: 0.0017851609
train_loss: 0.0016865018
test_loss: 0.0017123722
train_loss: 0.001536316
test_loss: 0.001639298
train_loss: 0.0016546168
test_loss: 0.0015396441
train_loss: 0.0016043707
test_loss: 0.0016380326
train_loss: 0.0015549882
test_loss: 0.0017008806
train_loss: 0.0019211627
test_loss: 0.0016148699
train_loss: 0.0016804163
test_loss: 0.0017527246
train_loss: 0.0015807921
test_loss: 0.0016452987
train_loss: 0.0016701256
test_loss: 0.0016153397
train_loss: 0.0014755179
test_loss: 0.0016604271
train_loss: 0.0014802701
test_loss: 0.0016402433
train_loss: 0.0015604494
test_loss: 0.0017253044
train_loss: 0.0016451952
test_loss: 0.0015978916
train_loss: 0.0015945064
test_loss: 0.0017216227
train_loss: 0.0017641359
test_loss: 0.0016866405
train_loss: 0.001612477
test_loss: 0.0017585495
train_loss: 0.0016268871
test_loss: 0.001673256
train_loss: 0.0018933392
test_loss: 0.0018121761
train_loss: 0.0016367421
test_loss: 0.0017269773
train_loss: 0.0015156157
test_loss: 0.0018249679
train_loss: 0.0016064552
test_loss: 0.0018285672
train_loss: 0.0014905683
test_loss: 0.0016383779
train_loss: 0.0015235123
test_loss: 0.0015779379
train_loss: 0.0016178228
test_loss: 0.0016339746
train_loss: 0.0015332347
test_loss: 0.0016101633
train_loss: 0.0018566499
test_loss: 0.001825426
train_loss: 0.0015039471
test_loss: 0.0016020284
train_loss: 0.0014608275
test_loss: 0.0018784626
train_loss: 0.0017552979
test_loss: 0.0018462397
train_loss: 0.0015790934
test_loss: 0.001706754
train_loss: 0.0013839221
test_loss: 0.001506261
train_loss: 0.001494079
test_loss: 0.0017638658
train_loss: 0.0016898839
test_loss: 0.0016012558
train_loss: 0.0015676451
test_loss: 0.0016366885
train_loss: 0.0016060655
test_loss: 0.00175895
train_loss: 0.0016049223
test_loss: 0.0016707054
train_loss: 0.0015553788
test_loss: 0.0016035718
train_loss: 0.0014475015
test_loss: 0.0016006669
train_loss: 0.0014680759
test_loss: 0.001551967
train_loss: 0.0016377292
test_loss: 0.0016097001
train_loss: 0.0015686437
test_loss: 0.0017899018
train_loss: 0.0017209915
test_loss: 0.0018612294
train_loss: 0.0014991718
test_loss: 0.0016581651
train_loss: 0.0019300185
test_loss: 0.0017173198
train_loss: 0.0016527954
test_loss: 0.0016743413
train_loss: 0.0016598012
test_loss: 0.0019128079
train_loss: 0.0017209413
test_loss: 0.0017055537
train_loss: 0.0015491451
test_loss: 0.0015974863
train_loss: 0.0013900555
test_loss: 0.0016513662
train_loss: 0.0016013941
test_loss: 0.0016282725
train_loss: 0.0015261861
test_loss: 0.0014878944
train_loss: 0.0015077437
test_loss: 0.0015132283
train_loss: 0.0016537509
test_loss: 0.0016932195
train_loss: 0.0014211144
test_loss: 0.00164407
train_loss: 0.0015781946
test_loss: 0.0016996457
train_loss: 0.0015206321
test_loss: 0.001614097
train_loss: 0.0018069658
test_loss: 0.001851934
train_loss: 0.0016339478
test_loss: 0.0016851632
train_loss: 0.0015013912
test_loss: 0.0018022737
train_loss: 0.0016327213
test_loss: 0.0015726999
train_loss: 0.001630316
test_loss: 0.0016165548
train_loss: 0.0014588082
test_loss: 0.001653761
train_loss: 0.0016346212
test_loss: 0.001726047
train_loss: 0.0015826019
test_loss: 0.0017195808
train_loss: 0.0014873502
test_loss: 0.0016559241
train_loss: 0.0017258921
test_loss: 0.00169957
train_loss: 0.0015748234
test_loss: 0.0016362227
train_loss: 0.0016547515
test_loss: 0.0015891854
train_loss: 0.0014894537
test_loss: 0.0016393455
train_loss: 0.0016778003
test_loss: 0.0016685362
train_loss: 0.0017154706
test_loss: 0.0020817341
train_loss: 0.001787889
test_loss: 0.0017745045
train_loss: 0.0015251389
test_loss: 0.0017884453
train_loss: 0.0015010945
test_loss: 0.0017494845
train_loss: 0.0017050725
test_loss: 0.001690654
train_loss: 0.0015320716
test_loss: 0.0017148124
train_loss: 0.0018228776
test_loss: 0.0019344265