+ RUN=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='0 1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1 --function f1 --psi 0 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d5006b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d50042840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4823b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d500b7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48202f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d481db620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d481dbea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4811f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4811f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4811f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48178268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480119d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4803b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480950d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480ae400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48095048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48155f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d48079620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480792f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf056b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf0555378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480d10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf0555f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d480d2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf04fd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d30048b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d30048730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d30066378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d30066048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf03eb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf0424268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf0496f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf03de598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf048bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf03490d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9cf02a9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.01446991
test_loss: 0.014563111
train_loss: 0.0061007137
test_loss: 0.0057039363
train_loss: 0.0029843245
test_loss: 0.0029687865
train_loss: 0.0022280903
test_loss: 0.0019834223
train_loss: 0.001850297
test_loss: 0.0021342314
train_loss: 0.001932413
test_loss: 0.002534642
train_loss: 0.0017468589
test_loss: 0.0018275735
train_loss: 0.0016478723
test_loss: 0.0018323604
train_loss: 0.001758661
test_loss: 0.001729004
train_loss: 0.0016869906
test_loss: 0.0016550544
train_loss: 0.001781175
test_loss: 0.002203674
train_loss: 0.001658789
test_loss: 0.0017269037
train_loss: 0.00184351
test_loss: 0.0016266324
train_loss: 0.0016099224
test_loss: 0.0017676431
train_loss: 0.0016576992
test_loss: 0.0017862144
train_loss: 0.0015958007
test_loss: 0.0018138258
train_loss: 0.0016244443
test_loss: 0.00187894
train_loss: 0.0016558589
test_loss: 0.0016680209
train_loss: 0.0015983128
test_loss: 0.0016659022
train_loss: 0.0015355722
test_loss: 0.0016872641
train_loss: 0.001711067
test_loss: 0.0016990335
train_loss: 0.0014374659
test_loss: 0.0016078943
train_loss: 0.0016087075
test_loss: 0.0015395381
train_loss: 0.0015143383
test_loss: 0.0016746816
train_loss: 0.0016346029
test_loss: 0.0017192756
train_loss: 0.0015073996
test_loss: 0.0015587525
train_loss: 0.0014686474
test_loss: 0.0016429438
train_loss: 0.0016858317
test_loss: 0.0017663288
train_loss: 0.0016277325
test_loss: 0.0016505244
train_loss: 0.0014395562
test_loss: 0.0015161858
train_loss: 0.0017073299
test_loss: 0.001642084
train_loss: 0.0015001392
test_loss: 0.0017853288
train_loss: 0.0015324022
test_loss: 0.0015694213
train_loss: 0.0017190642
test_loss: 0.002040534
train_loss: 0.0019446943
test_loss: 0.0018911173
train_loss: 0.0015931504
test_loss: 0.0017187675
train_loss: 0.0016002906
test_loss: 0.0014440895
train_loss: 0.0015964371
test_loss: 0.0017356991
train_loss: 0.0014666321
test_loss: 0.0016223755
train_loss: 0.0022661954
test_loss: 0.0016969298
train_loss: 0.002013837
test_loss: 0.0017684957
train_loss: 0.0014898067
test_loss: 0.0014933725
train_loss: 0.0014357049
test_loss: 0.0015457827
train_loss: 0.0015759708
test_loss: 0.0015848638
train_loss: 0.0015968217
test_loss: 0.0015508466
train_loss: 0.0015940558
test_loss: 0.0016074141
train_loss: 0.0014032067
test_loss: 0.0017015783
train_loss: 0.0016453989
test_loss: 0.001819515
train_loss: 0.001533316
test_loss: 0.0016596533
train_loss: 0.001744759
test_loss: 0.0015761027
train_loss: 0.0015969123
test_loss: 0.001567108
train_loss: 0.001583218
test_loss: 0.0014744926
train_loss: 0.0016172767
test_loss: 0.0014762414
train_loss: 0.0013788611
test_loss: 0.0016270675
train_loss: 0.0014433893
test_loss: 0.0016332268
train_loss: 0.0016015944
test_loss: 0.0014983468
train_loss: 0.0015836951
test_loss: 0.001656378
train_loss: 0.0019046154
test_loss: 0.0016807339
train_loss: 0.0015642921
test_loss: 0.001449934
train_loss: 0.0015236659
test_loss: 0.001625768
train_loss: 0.001438518
test_loss: 0.00159101
train_loss: 0.0017950196
test_loss: 0.0017771337
train_loss: 0.0016934378
test_loss: 0.0014408209
train_loss: 0.001451454
test_loss: 0.0014762724
train_loss: 0.0016224268
test_loss: 0.0015628005
train_loss: 0.001439381
test_loss: 0.0016408323
train_loss: 0.0014427197
test_loss: 0.0015511219
train_loss: 0.0019329096
test_loss: 0.0017298143
train_loss: 0.0014603699
test_loss: 0.0016668828
train_loss: 0.0014633517
test_loss: 0.0015038365
train_loss: 0.0016945603
test_loss: 0.0014765506
train_loss: 0.0015610064
test_loss: 0.0015900561
train_loss: 0.0014894115
test_loss: 0.0017577407
train_loss: 0.0015504151
test_loss: 0.0015772174
train_loss: 0.001785702
test_loss: 0.0015981816
train_loss: 0.0013556577
test_loss: 0.0016797732
train_loss: 0.0019596303
test_loss: 0.0016231047
train_loss: 0.0015978557
test_loss: 0.0019548875
train_loss: 0.0018089241
test_loss: 0.0017244956
train_loss: 0.0016637614
test_loss: 0.0016568156
train_loss: 0.0016396774
test_loss: 0.0016148655
train_loss: 0.0015247636
test_loss: 0.0015079884
train_loss: 0.0015650761
test_loss: 0.0013783799
train_loss: 0.0017130303
test_loss: 0.0018068748
train_loss: 0.0014303089
test_loss: 0.0014289726
train_loss: 0.0014250784
test_loss: 0.0018212582
train_loss: 0.0016418603
test_loss: 0.0018726395
train_loss: 0.0014853289
test_loss: 0.0015382114
train_loss: 0.0014621972
test_loss: 0.0015432447
train_loss: 0.0013706426
test_loss: 0.00161236
train_loss: 0.0014555722
test_loss: 0.001560037
train_loss: 0.0014903278
test_loss: 0.0014532026
train_loss: 0.0016215194
test_loss: 0.0015232835
train_loss: 0.0014487463
test_loss: 0.0014153557
train_loss: 0.001453269
test_loss: 0.001495504
train_loss: 0.001645748
test_loss: 0.0015742294
train_loss: 0.001486985
test_loss: 0.0015117292
train_loss: 0.001293146
test_loss: 0.0014536665
train_loss: 0.0012717189
test_loss: 0.0017682397
train_loss: 0.0018159614
test_loss: 0.0014019535
train_loss: 0.0015336735
test_loss: 0.0016174158
train_loss: 0.001640986
test_loss: 0.0016490876
train_loss: 0.0017285929
test_loss: 0.0015039394
train_loss: 0.0015129268
test_loss: 0.001501659
train_loss: 0.0014770599
test_loss: 0.00166554
train_loss: 0.0014768214
test_loss: 0.0015077664
train_loss: 0.0013613536
test_loss: 0.0014948576
train_loss: 0.0016425725
test_loss: 0.0014975911
train_loss: 0.0015560121
test_loss: 0.0017135025
train_loss: 0.0016344946
test_loss: 0.0017686852
train_loss: 0.0014189279
test_loss: 0.0015491813
train_loss: 0.0017162962
test_loss: 0.0013569933
train_loss: 0.0017372073
test_loss: 0.0015285186
train_loss: 0.0016153982
test_loss: 0.0020301905
train_loss: 0.0014209867
test_loss: 0.0016320463
train_loss: 0.0014127066
test_loss: 0.0014944248
train_loss: 0.0014703255
test_loss: 0.001583737
train_loss: 0.0014225662
test_loss: 0.0014884382
train_loss: 0.0012278806
test_loss: 0.0015368352
train_loss: 0.0015249306
test_loss: 0.0015929276
train_loss: 0.0014749116
test_loss: 0.001581356
train_loss: 0.0014457266
test_loss: 0.0015895407
train_loss: 0.0013781362
test_loss: 0.0016907799
train_loss: 0.0018331269
test_loss: 0.0018032367
train_loss: 0.0015140672
test_loss: 0.001694127
train_loss: 0.00144173
test_loss: 0.001309777
train_loss: 0.0013793635
test_loss: 0.0014030401
train_loss: 0.0013339465
test_loss: 0.0014826511
train_loss: 0.0014160526
test_loss: 0.0015112892
train_loss: 0.0017736193
test_loss: 0.0015836468
train_loss: 0.0014529022
test_loss: 0.0016319467
train_loss: 0.0013588046
test_loss: 0.0016108248
train_loss: 0.0016617901
test_loss: 0.0015876953
train_loss: 0.0014381143
test_loss: 0.001420244
train_loss: 0.0015028443
test_loss: 0.0014328328
train_loss: 0.0018428022
test_loss: 0.0016394155
train_loss: 0.0017813148
test_loss: 0.0014182451
train_loss: 0.0015414245
test_loss: 0.0014307202
train_loss: 0.0014657108
test_loss: 0.001655902
train_loss: 0.001583894
test_loss: 0.0016174768
train_loss: 0.0017861134
test_loss: 0.0016336904
train_loss: 0.0014783943
test_loss: 0.0015000133
train_loss: 0.0016571431
test_loss: 0.0016172334
train_loss: 0.0014215212
test_loss: 0.00190303
train_loss: 0.0014609338
test_loss: 0.0017572785
train_loss: 0.0014708242
test_loss: 0.0014933813
train_loss: 0.0016582767
test_loss: 0.0014312085
train_loss: 0.0014339054
test_loss: 0.0015002323
train_loss: 0.0016902378
test_loss: 0.0015686426
train_loss: 0.0015350088
test_loss: 0.0015101255
train_loss: 0.0013320304
test_loss: 0.001445899
train_loss: 0.0014558436
test_loss: 0.0014504981
train_loss: 0.0015751593
test_loss: 0.0017077051
train_loss: 0.0017902954
test_loss: 0.0016688957
train_loss: 0.001461013
test_loss: 0.0016848516
train_loss: 0.0015692152
test_loss: 0.0014802668
train_loss: 0.001397494
test_loss: 0.0015489759
train_loss: 0.0014356087
test_loss: 0.0017592951
train_loss: 0.00156361
test_loss: 0.001506651
train_loss: 0.0017183595
test_loss: 0.0015042296
train_loss: 0.0013784659
test_loss: 0.0014277741
train_loss: 0.0013683643
test_loss: 0.0015297084
train_loss: 0.0012907035
test_loss: 0.0014080602
train_loss: 0.0013201418
test_loss: 0.001384942
train_loss: 0.0016413741
test_loss: 0.0014957768
train_loss: 0.0013758225
test_loss: 0.0018150372
train_loss: 0.0014531198
test_loss: 0.0015570043
train_loss: 0.001414399
test_loss: 0.0014045959
train_loss: 0.0014860721
test_loss: 0.0014274438
train_loss: 0.0014137501
test_loss: 0.0015327838
train_loss: 0.001277756
test_loss: 0.0012679124
train_loss: 0.0013596637
test_loss: 0.0017831826
train_loss: 0.0015593704
test_loss: 0.0016056147
train_loss: 0.0013414015
test_loss: 0.0014384162
train_loss: 0.0015234689
test_loss: 0.0012737212
train_loss: 0.0014330641
test_loss: 0.0015948906
train_loss: 0.001573056
test_loss: 0.001617892
train_loss: 0.001447568
test_loss: 0.0014805155
train_loss: 0.0015321799
test_loss: 0.001339891
train_loss: 0.0014027252
test_loss: 0.0014441533
train_loss: 0.0015259949
test_loss: 0.0014097646
train_loss: 0.0013040403
test_loss: 0.0013511409
train_loss: 0.0016715988
test_loss: 0.0016655498
train_loss: 0.0014940179
test_loss: 0.0015317177
train_loss: 0.0016524498
test_loss: 0.0015901997
train_loss: 0.0013186787
test_loss: 0.0017084526
train_loss: 0.0013541421
test_loss: 0.0014523218
train_loss: 0.0013671687
test_loss: 0.0015558104
train_loss: 0.0015906491
test_loss: 0.0013784255
train_loss: 0.0016700743
test_loss: 0.0013970636
train_loss: 0.001479984
test_loss: 0.0014134469
train_loss: 0.0017400414
test_loss: 0.001376923
train_loss: 0.0013729646
test_loss: 0.0013080407
train_loss: 0.0014066153
test_loss: 0.0015263975
train_loss: 0.0014601744
test_loss: 0.0014722964
train_loss: 0.0012941139
test_loss: 0.00145577
train_loss: 0.0014750783
test_loss: 0.0017029109
train_loss: 0.0013822546
test_loss: 0.0013884901
train_loss: 0.0012710359
test_loss: 0.0014446508
train_loss: 0.0014942574
test_loss: 0.0014321767
train_loss: 0.0013153483
test_loss: 0.0014142843
train_loss: 0.0013886084
test_loss: 0.0017549748
train_loss: 0.0015359556
test_loss: 0.0014066724
train_loss: 0.001521497
test_loss: 0.001809925
train_loss: 0.0015140332
test_loss: 0.0014500907
train_loss: 0.0013236115
test_loss: 0.0017262682
train_loss: 0.001402145
test_loss: 0.0014286883
train_loss: 0.0012410501
test_loss: 0.0014471372
train_loss: 0.0014986605
test_loss: 0.0016517887
train_loss: 0.0015107307
test_loss: 0.0015618787
train_loss: 0.0014216696
test_loss: 0.0016765672
train_loss: 0.0014310125
test_loss: 0.001445288
train_loss: 0.0014756061
test_loss: 0.001410923
train_loss: 0.0014969462
test_loss: 0.0015486982
train_loss: 0.001447938
test_loss: 0.0016327292
train_loss: 0.0013367047
test_loss: 0.0018495074
train_loss: 0.0014273216
test_loss: 0.0014834874
train_loss: 0.0014102816
test_loss: 0.0014595977
train_loss: 0.0014819386
test_loss: 0.0016490534
train_loss: 0.0013889554
test_loss: 0.0014832243
train_loss: 0.0015212616
test_loss: 0.0016344985
train_loss: 0.0017937169
test_loss: 0.0015656346
train_loss: 0.0015375549
test_loss: 0.001715795
train_loss: 0.001305368
test_loss: 0.0015075401
train_loss: 0.0013870564
test_loss: 0.0015610573
train_loss: 0.0015742984
test_loss: 0.0014954566
train_loss: 0.0015045939
test_loss: 0.0015422287
train_loss: 0.001587027
test_loss: 0.0014604046
train_loss: 0.0013660378
test_loss: 0.0015623491
train_loss: 0.0015801725
test_loss: 0.0015435684
train_loss: 0.0014816442
test_loss: 0.0015432443
train_loss: 0.0014255899
test_loss: 0.0014864856
train_loss: 0.0014168565
test_loss: 0.0015866129
train_loss: 0.0014174237
test_loss: 0.0015370129
train_loss: 0.0013607971
test_loss: 0.0020046507
train_loss: 0.0015179964
test_loss: 0.0014220093
train_loss: 0.0013930805
test_loss: 0.0013122876
train_loss: 0.0014497729
test_loss: 0.0016451248
train_loss: 0.0015281609
test_loss: 0.0015396327
train_loss: 0.0014655908
test_loss: 0.001615487
train_loss: 0.0014467072
test_loss: 0.0014940519
train_loss: 0.0014258396
test_loss: 0.0014120623
train_loss: 0.001590949
test_loss: 0.0015562979
train_loss: 0.0013666612
test_loss: 0.001688748
train_loss: 0.0014256646
test_loss: 0.0016673609
train_loss: 0.0012837786
test_loss: 0.0014689006
train_loss: 0.0015225306
test_loss: 0.0013226365
train_loss: 0.0014435058
test_loss: 0.0016097219
train_loss: 0.001490226
test_loss: 0.0014707719
train_loss: 0.0015570254
test_loss: 0.0014763457
train_loss: 0.0014651002
test_loss: 0.0014806896
train_loss: 0.0014131137
test_loss: 0.0015715347
train_loss: 0.0013346198
test_loss: 0.0014365915
train_loss: 0.0014147961
test_loss: 0.0015735792
train_loss: 0.001558482
test_loss: 0.001689539
train_loss: 0.0015526665
test_loss: 0.0017777268
train_loss: 0.0015912428
test_loss: 0.0017169814
train_loss: 0.0015534265
test_loss: 0.0014019324
train_loss: 0.0014093252
test_loss: 0.0014856003
train_loss: 0.0012982495
test_loss: 0.0016091944
train_loss: 0.0015962397
test_loss: 0.0014576904
train_loss: 0.0014527976
test_loss: 0.0015175258
train_loss: 0.0015200137
test_loss: 0.0014382985
train_loss: 0.0016757972
test_loss: 0.0015269875
train_loss: 0.0013741413
test_loss: 0.0013454
train_loss: 0.001477249
test_loss: 0.0013973379
train_loss: 0.0013785462
test_loss: 0.0016340134
train_loss: 0.0014377218
test_loss: 0.0014360637
train_loss: 0.0013921668
test_loss: 0.0013515848
train_loss: 0.0015138204
test_loss: 0.0014131402
train_loss: 0.0013741986
test_loss: 0.0015224686
train_loss: 0.0014870095
test_loss: 0.0016747715
train_loss: 0.0014078515
test_loss: 0.0014689103
train_loss: 0.0013958886
test_loss: 0.0014381474
train_loss: 0.00137558
test_loss: 0.0015118683
train_loss: 0.0013358187
test_loss: 0.0014284201
train_loss: 0.0015065102
test_loss: 0.0013745155
train_loss: 0.0015887045
test_loss: 0.0014611683
train_loss: 0.0015606779
test_loss: 0.0015876652
train_loss: 0.0015719568
test_loss: 0.0014939316
train_loss: 0.0014262844
test_loss: 0.0014986428
train_loss: 0.0013385668
test_loss: 0.0015234804
train_loss: 0.0012120851
test_loss: 0.0012789413
train_loss: 0.0014410091
test_loss: 0.0016880564
train_loss: 0.0012948622
test_loss: 0.0017923623
train_loss: 0.0015650354
test_loss: 0.0017466852
train_loss: 0.001385052
test_loss: 0.0017402772
train_loss: 0.0014741006
test_loss: 0.0017429255
train_loss: 0.0014156213
test_loss: 0.0015291126
train_loss: 0.0016936248
test_loss: 0.0013755395
train_loss: 0.0012992825
test_loss: 0.0015683166
train_loss: 0.0013352858
test_loss: 0.0015312992
train_loss: 0.0013649424
test_loss: 0.0014869414
train_loss: 0.0013231616
test_loss: 0.0015380144
train_loss: 0.0015821371
test_loss: 0.0014820109
train_loss: 0.0012809385
test_loss: 0.0016835411
train_loss: 0.0015572383
test_loss: 0.0015560972
train_loss: 0.0016272591
test_loss: 0.0013203487
train_loss: 0.0013345317
test_loss: 0.0014885379
train_loss: 0.0014487236
test_loss: 0.0014548007
train_loss: 0.0013212552
test_loss: 0.0013025034
train_loss: 0.0014428523
test_loss: 0.0013844796
train_loss: 0.0012974569
test_loss: 0.0014775242
train_loss: 0.0014225338
test_loss: 0.0015958056
train_loss: 0.0014615837
test_loss: 0.0013792451
train_loss: 0.0013449255
test_loss: 0.0014553375
train_loss: 0.0013739971
test_loss: 0.0013114054
train_loss: 0.001374784
test_loss: 0.001417581
train_loss: 0.0013198523
test_loss: 0.0013718043
train_loss: 0.0015022773
test_loss: 0.0016950854
train_loss: 0.0019781184
test_loss: 0.001643026
train_loss: 0.0013600537
test_loss: 0.0015435726
train_loss: 0.0016295132
test_loss: 0.0015399072
train_loss: 0.001646334
test_loss: 0.0015145659
train_loss: 0.0015850936
test_loss: 0.001870616
train_loss: 0.0013257648
test_loss: 0.0017225089
train_loss: 0.0012660936
test_loss: 0.0013343855
train_loss: 0.0014383723
test_loss: 0.0015453593
train_loss: 0.0013907064
test_loss: 0.0013401497
train_loss: 0.0013601249
test_loss: 0.0014754762
train_loss: 0.001396916
test_loss: 0.0013455708
train_loss: 0.0013642213
test_loss: 0.0013115632
train_loss: 0.0015313214
test_loss: 0.0013405582
train_loss: 0.0013751647
test_loss: 0.0015248109
train_loss: 0.0013186116
test_loss: 0.0015073994
train_loss: 0.0014920167
test_loss: 0.001485834
train_loss: 0.0015281143
test_loss: 0.0014298985
train_loss: 0.0014124538
test_loss: 0.0014213847
train_loss: 0.0014022395
test_loss: 0.0014109123
train_loss: 0.0014368199
test_loss: 0.0014391667
train_loss: 0.0014725878
test_loss: 0.0015497493
train_loss: 0.0015073218
test_loss: 0.0015339466
train_loss: 0.0013636653
test_loss: 0.0013886383
train_loss: 0.001728624
test_loss: 0.0015224301
train_loss: 0.0013574834
test_loss: 0.0015514166
train_loss: 0.0014211363
test_loss: 0.0015485353
train_loss: 0.0017218156
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
test_loss: 0.0013878092
train_loss: 0.001475277
test_loss: 0.0017442113
train_loss: 0.0016256459
test_loss: 0.001633986
train_loss: 0.0012510968
test_loss: 0.0014752301
train_loss: 0.0012361784
test_loss: 0.0015759117
train_loss: 0.0013991217
test_loss: 0.001396975
train_loss: 0.0019350554
test_loss: 0.0013713478
train_loss: 0.0017251641
test_loss: 0.0014397216
train_loss: 0.001353412
test_loss: 0.0017204063
train_loss: 0.0014993977
test_loss: 0.0014968325
train_loss: 0.0015862407
test_loss: 0.0014354254
train_loss: 0.0012285576
test_loss: 0.0017759237
train_loss: 0.0015174267
test_loss: 0.0014423534
train_loss: 0.0013673684
test_loss: 0.001390212
train_loss: 0.0012749638
test_loss: 0.0014282076
train_loss: 0.0012710089
test_loss: 0.0014345777
train_loss: 0.0013052354
test_loss: 0.0016724768
train_loss: 0.0014125541
test_loss: 0.0012864624
train_loss: 0.0015152947
test_loss: 0.0015124533
train_loss: 0.0013430432
test_loss: 0.0014485155
train_loss: 0.0013324504
test_loss: 0.0013013548
train_loss: 0.0016578173
test_loss: 0.0015228215
train_loss: 0.0013796499
test_loss: 0.0018758621
train_loss: 0.0013569505
test_loss: 0.0014462544
train_loss: 0.0013229394
test_loss: 0.001368823
train_loss: 0.0016070299
test_loss: 0.0014097806
train_loss: 0.0013115944
test_loss: 0.0016082012
train_loss: 0.0014270482
test_loss: 0.0013836159
train_loss: 0.0013671007
test_loss: 0.0014289151
train_loss: 0.0013596357
test_loss: 0.001456479
train_loss: 0.0014648982
test_loss: 0.0012871297
train_loss: 0.0012566395
test_loss: 0.001533489
train_loss: 0.0014007286
test_loss: 0.0014395473
train_loss: 0.0013886758
test_loss: 0.0016520122
train_loss: 0.0014141721
test_loss: 0.0015225235
train_loss: 0.0013985448
test_loss: 0.0013241741
train_loss: 0.0014713507
test_loss: 0.0015886216
train_loss: 0.0014968038
test_loss: 0.0013955629
train_loss: 0.0014127896
test_loss: 0.0015917782
train_loss: 0.001669686
test_loss: 0.0015452713
train_loss: 0.0014676875
test_loss: 0.0014705528
train_loss: 0.0013773564
test_loss: 0.0015788996
train_loss: 0.0015142972
test_loss: 0.0015299346
train_loss: 0.0013011172
test_loss: 0.001345937
train_loss: 0.0012402565
test_loss: 0.0014224151
train_loss: 0.0013756165
test_loss: 0.0014134049
train_loss: 0.001571715
test_loss: 0.0014289744
train_loss: 0.0013906276
test_loss: 0.001414175
train_loss: 0.0014316312
test_loss: 0.0014302703
train_loss: 0.001304377
test_loss: 0.0014763335
train_loss: 0.0013441988
test_loss: 0.0015980445
train_loss: 0.0017669268
test_loss: 0.001429147
train_loss: 0.0013799511
test_loss: 0.0013702397
train_loss: 0.0014846993
test_loss: 0.0012934621
train_loss: 0.0013515005
test_loss: 0.0012363696
train_loss: 0.0012705945
test_loss: 0.0013683463
train_loss: 0.0012952298
test_loss: 0.0013100216
train_loss: 0.0013683639
test_loss: 0.0013572149
train_loss: 0.0014456001
test_loss: 0.0013699345
train_loss: 0.0013124323
test_loss: 0.0016054502
train_loss: 0.0013912058
test_loss: 0.0014344753
train_loss: 0.0012366488
test_loss: 0.0013875691
train_loss: 0.0015618025
test_loss: 0.0014748742
train_loss: 0.0015847064
test_loss: 0.0015977737
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcded400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fce0c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd819d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fce2abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd5af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd5a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd0cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcd771e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fccd7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fccd7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fccae1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcc4abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcc71950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcc17268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcc71a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcbec9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcbec730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcb53ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcb69d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7fcbec488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbebca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe570d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbebc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe4f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe4f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe4fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbe4f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbdc69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7dbdc6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b45057b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b4520268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b4487f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b4487730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b44d8d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b44500d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7b4473f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.55788461e-06
Iter: 2 loss: 3.38037557e-06
Iter: 3 loss: 2.27885e-06
Iter: 4 loss: 1.90168623e-06
Iter: 5 loss: 1.76326103e-06
Iter: 6 loss: 1.55402813e-06
Iter: 7 loss: 1.42646286e-06
Iter: 8 loss: 1.41123576e-06
Iter: 9 loss: 1.35459095e-06
Iter: 10 loss: 1.32728883e-06
Iter: 11 loss: 1.29992065e-06
Iter: 12 loss: 1.24824055e-06
Iter: 13 loss: 2.04126036e-06
Iter: 14 loss: 1.24819985e-06
Iter: 15 loss: 1.21244136e-06
Iter: 16 loss: 1.22334723e-06
Iter: 17 loss: 1.18684898e-06
Iter: 18 loss: 1.16322008e-06
Iter: 19 loss: 1.269108e-06
Iter: 20 loss: 1.15855596e-06
Iter: 21 loss: 1.14076647e-06
Iter: 22 loss: 1.15416537e-06
Iter: 23 loss: 1.12986265e-06
Iter: 24 loss: 1.11019835e-06
Iter: 25 loss: 1.11014242e-06
Iter: 26 loss: 1.10192821e-06
Iter: 27 loss: 1.08460199e-06
Iter: 28 loss: 1.37487382e-06
Iter: 29 loss: 1.08413428e-06
Iter: 30 loss: 1.06803941e-06
Iter: 31 loss: 1.10739279e-06
Iter: 32 loss: 1.06227503e-06
Iter: 33 loss: 1.05222784e-06
Iter: 34 loss: 1.05872607e-06
Iter: 35 loss: 1.04582784e-06
Iter: 36 loss: 1.03278e-06
Iter: 37 loss: 1.07106405e-06
Iter: 38 loss: 1.02874276e-06
Iter: 39 loss: 1.02763613e-06
Iter: 40 loss: 1.02279182e-06
Iter: 41 loss: 1.01788567e-06
Iter: 42 loss: 1.00811212e-06
Iter: 43 loss: 1.19674633e-06
Iter: 44 loss: 1.00803686e-06
Iter: 45 loss: 9.98287e-07
Iter: 46 loss: 1.06929156e-06
Iter: 47 loss: 9.97500479e-07
Iter: 48 loss: 9.86287432e-07
Iter: 49 loss: 9.8971077e-07
Iter: 50 loss: 9.78229536e-07
Iter: 51 loss: 9.70720748e-07
Iter: 52 loss: 1.04609353e-06
Iter: 53 loss: 9.70449605e-07
Iter: 54 loss: 9.63264e-07
Iter: 55 loss: 9.67753635e-07
Iter: 56 loss: 9.58683131e-07
Iter: 57 loss: 9.54099278e-07
Iter: 58 loss: 9.54957727e-07
Iter: 59 loss: 9.50693277e-07
Iter: 60 loss: 9.48243724e-07
Iter: 61 loss: 9.47802e-07
Iter: 62 loss: 9.45082093e-07
Iter: 63 loss: 9.45353747e-07
Iter: 64 loss: 9.42954159e-07
Iter: 65 loss: 9.40503e-07
Iter: 66 loss: 9.42966608e-07
Iter: 67 loss: 9.3917032e-07
Iter: 68 loss: 9.36096512e-07
Iter: 69 loss: 9.37721893e-07
Iter: 70 loss: 9.34043385e-07
Iter: 71 loss: 9.29588225e-07
Iter: 72 loss: 9.36761239e-07
Iter: 73 loss: 9.27521683e-07
Iter: 74 loss: 9.24214589e-07
Iter: 75 loss: 9.24223173e-07
Iter: 76 loss: 9.20520108e-07
Iter: 77 loss: 9.28258203e-07
Iter: 78 loss: 9.19098227e-07
Iter: 79 loss: 9.17274349e-07
Iter: 80 loss: 9.15769249e-07
Iter: 81 loss: 9.15257942e-07
Iter: 82 loss: 9.12201699e-07
Iter: 83 loss: 9.51308266e-07
Iter: 84 loss: 9.12179246e-07
Iter: 85 loss: 9.10803692e-07
Iter: 86 loss: 9.100836e-07
Iter: 87 loss: 9.09432174e-07
Iter: 88 loss: 9.07177707e-07
Iter: 89 loss: 9.22794584e-07
Iter: 90 loss: 9.0697722e-07
Iter: 91 loss: 9.05090928e-07
Iter: 92 loss: 9.0153344e-07
Iter: 93 loss: 9.76295155e-07
Iter: 94 loss: 9.01504677e-07
Iter: 95 loss: 8.98078383e-07
Iter: 96 loss: 9.17606712e-07
Iter: 97 loss: 8.97591e-07
Iter: 98 loss: 8.94697e-07
Iter: 99 loss: 9.37337632e-07
Iter: 100 loss: 8.94680682e-07
Iter: 101 loss: 8.93543415e-07
Iter: 102 loss: 8.90849265e-07
Iter: 103 loss: 9.20650905e-07
Iter: 104 loss: 8.90565559e-07
Iter: 105 loss: 8.8899958e-07
Iter: 106 loss: 8.88969225e-07
Iter: 107 loss: 8.87880503e-07
Iter: 108 loss: 8.87587817e-07
Iter: 109 loss: 8.86916951e-07
Iter: 110 loss: 8.85669294e-07
Iter: 111 loss: 8.99184329e-07
Iter: 112 loss: 8.85642919e-07
Iter: 113 loss: 8.85109046e-07
Iter: 114 loss: 8.85068687e-07
Iter: 115 loss: 8.84614224e-07
Iter: 116 loss: 8.83241796e-07
Iter: 117 loss: 8.85777922e-07
Iter: 118 loss: 8.82349e-07
Iter: 119 loss: 8.8178416e-07
Iter: 120 loss: 8.8140763e-07
Iter: 121 loss: 8.80421908e-07
Iter: 122 loss: 8.79765196e-07
Iter: 123 loss: 8.7939685e-07
Iter: 124 loss: 8.78175e-07
Iter: 125 loss: 8.8497319e-07
Iter: 126 loss: 8.78003277e-07
Iter: 127 loss: 8.76744366e-07
Iter: 128 loss: 8.79324318e-07
Iter: 129 loss: 8.76249487e-07
Iter: 130 loss: 8.75310434e-07
Iter: 131 loss: 8.73980525e-07
Iter: 132 loss: 8.73951876e-07
Iter: 133 loss: 8.73367128e-07
Iter: 134 loss: 8.73066256e-07
Iter: 135 loss: 8.72184444e-07
Iter: 136 loss: 8.70418432e-07
Iter: 137 loss: 9.02885688e-07
Iter: 138 loss: 8.7038336e-07
Iter: 139 loss: 8.68725465e-07
Iter: 140 loss: 8.69348298e-07
Iter: 141 loss: 8.67552046e-07
Iter: 142 loss: 8.65892332e-07
Iter: 143 loss: 8.81092831e-07
Iter: 144 loss: 8.65786944e-07
Iter: 145 loss: 8.64818389e-07
Iter: 146 loss: 8.72926762e-07
Iter: 147 loss: 8.64763e-07
Iter: 148 loss: 8.6412615e-07
Iter: 149 loss: 8.64536844e-07
Iter: 150 loss: 8.63717673e-07
Iter: 151 loss: 8.63212449e-07
Iter: 152 loss: 8.63134915e-07
Iter: 153 loss: 8.62797833e-07
Iter: 154 loss: 8.621397e-07
Iter: 155 loss: 8.77681657e-07
Iter: 156 loss: 8.62138279e-07
Iter: 157 loss: 8.61456385e-07
Iter: 158 loss: 8.63107971e-07
Iter: 159 loss: 8.6123822e-07
Iter: 160 loss: 8.60239652e-07
Iter: 161 loss: 8.63950675e-07
Iter: 162 loss: 8.59968225e-07
Iter: 163 loss: 8.59440377e-07
Iter: 164 loss: 8.590672e-07
Iter: 165 loss: 8.5887973e-07
Iter: 166 loss: 8.57448413e-07
Iter: 167 loss: 8.59898364e-07
Iter: 168 loss: 8.56863721e-07
Iter: 169 loss: 8.55896246e-07
Iter: 170 loss: 8.55774033e-07
Iter: 171 loss: 8.55126132e-07
Iter: 172 loss: 8.54008078e-07
Iter: 173 loss: 8.66100038e-07
Iter: 174 loss: 8.5399256e-07
Iter: 175 loss: 8.52792482e-07
Iter: 176 loss: 8.54743746e-07
Iter: 177 loss: 8.52243375e-07
Iter: 178 loss: 8.51610821e-07
Iter: 179 loss: 8.50615606e-07
Iter: 180 loss: 8.50606739e-07
Iter: 181 loss: 8.49489538e-07
Iter: 182 loss: 8.5269545e-07
Iter: 183 loss: 8.49122955e-07
Iter: 184 loss: 8.48112848e-07
Iter: 185 loss: 8.56262488e-07
Iter: 186 loss: 8.48030197e-07
Iter: 187 loss: 8.47349668e-07
Iter: 188 loss: 8.50237939e-07
Iter: 189 loss: 8.47198748e-07
Iter: 190 loss: 8.46618263e-07
Iter: 191 loss: 8.55945359e-07
Iter: 192 loss: 8.46630144e-07
Iter: 193 loss: 8.4613265e-07
Iter: 194 loss: 8.45821774e-07
Iter: 195 loss: 8.45656928e-07
Iter: 196 loss: 8.45049726e-07
Iter: 197 loss: 8.44607598e-07
Iter: 198 loss: 8.44398642e-07
Iter: 199 loss: 8.4367673e-07
Iter: 200 loss: 8.4362e-07
Iter: 201 loss: 8.43250746e-07
Iter: 202 loss: 8.42464715e-07
Iter: 203 loss: 8.5726964e-07
Iter: 204 loss: 8.42453801e-07
Iter: 205 loss: 8.41706765e-07
Iter: 206 loss: 8.41706878e-07
Iter: 207 loss: 8.41135488e-07
Iter: 208 loss: 8.40259418e-07
Iter: 209 loss: 8.40248845e-07
Iter: 210 loss: 8.39491406e-07
Iter: 211 loss: 8.44182239e-07
Iter: 212 loss: 8.39384597e-07
Iter: 213 loss: 8.38573328e-07
Iter: 214 loss: 8.43321914e-07
Iter: 215 loss: 8.3847118e-07
Iter: 216 loss: 8.38004098e-07
Iter: 217 loss: 8.36914751e-07
Iter: 218 loss: 8.51928121e-07
Iter: 219 loss: 8.36833351e-07
Iter: 220 loss: 8.3572121e-07
Iter: 221 loss: 8.40432904e-07
Iter: 222 loss: 8.35482183e-07
Iter: 223 loss: 8.34520222e-07
Iter: 224 loss: 8.41150552e-07
Iter: 225 loss: 8.34421257e-07
Iter: 226 loss: 8.33635625e-07
Iter: 227 loss: 8.43692874e-07
Iter: 228 loss: 8.33625393e-07
Iter: 229 loss: 8.3323016e-07
Iter: 230 loss: 8.33016031e-07
Iter: 231 loss: 8.32848968e-07
Iter: 232 loss: 8.32352725e-07
Iter: 233 loss: 8.33742e-07
Iter: 234 loss: 8.32239436e-07
Iter: 235 loss: 8.31696184e-07
Iter: 236 loss: 8.36061133e-07
Iter: 237 loss: 8.31652301e-07
Iter: 238 loss: 8.3133591e-07
Iter: 239 loss: 8.30798683e-07
Iter: 240 loss: 8.30783222e-07
Iter: 241 loss: 8.30281692e-07
Iter: 242 loss: 8.30280896e-07
Iter: 243 loss: 8.29852809e-07
Iter: 244 loss: 8.28999362e-07
Iter: 245 loss: 8.47355409e-07
Iter: 246 loss: 8.29003341e-07
Iter: 247 loss: 8.28114253e-07
Iter: 248 loss: 8.31205966e-07
Iter: 249 loss: 8.27870906e-07
Iter: 250 loss: 8.27079248e-07
Iter: 251 loss: 8.39290578e-07
Iter: 252 loss: 8.27078281e-07
Iter: 253 loss: 8.26634391e-07
Iter: 254 loss: 8.25840459e-07
Iter: 255 loss: 8.45321495e-07
Iter: 256 loss: 8.25835798e-07
Iter: 257 loss: 8.24997301e-07
Iter: 258 loss: 8.25539e-07
Iter: 259 loss: 8.24477297e-07
Iter: 260 loss: 8.23826554e-07
Iter: 261 loss: 8.29085252e-07
Iter: 262 loss: 8.23793e-07
Iter: 263 loss: 8.23324513e-07
Iter: 264 loss: 8.23318487e-07
Iter: 265 loss: 8.2303626e-07
Iter: 266 loss: 8.22423601e-07
Iter: 267 loss: 8.28903524e-07
Iter: 268 loss: 8.2238364e-07
Iter: 269 loss: 8.21791559e-07
Iter: 270 loss: 8.2711324e-07
Iter: 271 loss: 8.21753e-07
Iter: 272 loss: 8.21271385e-07
Iter: 273 loss: 8.25322218e-07
Iter: 274 loss: 8.21245351e-07
Iter: 275 loss: 8.20999276e-07
Iter: 276 loss: 8.2109068e-07
Iter: 277 loss: 8.20814364e-07
Iter: 278 loss: 8.20515197e-07
Iter: 279 loss: 8.21900301e-07
Iter: 280 loss: 8.20493312e-07
Iter: 281 loss: 8.20151115e-07
Iter: 282 loss: 8.2075519e-07
Iter: 283 loss: 8.20058915e-07
Iter: 284 loss: 8.19761794e-07
Iter: 285 loss: 8.19194497e-07
Iter: 286 loss: 8.19214506e-07
Iter: 287 loss: 8.19013735e-07
Iter: 288 loss: 8.1891335e-07
Iter: 289 loss: 8.18578769e-07
Iter: 290 loss: 8.17841737e-07
Iter: 291 loss: 8.24795052e-07
Iter: 292 loss: 8.17692353e-07
Iter: 293 loss: 8.16774786e-07
Iter: 294 loss: 8.19630202e-07
Iter: 295 loss: 8.16564238e-07
Iter: 296 loss: 8.15775365e-07
Iter: 297 loss: 8.16136094e-07
Iter: 298 loss: 8.15231886e-07
Iter: 299 loss: 8.15809e-07
Iter: 300 loss: 8.14909185e-07
Iter: 301 loss: 8.14688462e-07
Iter: 302 loss: 8.14076884e-07
Iter: 303 loss: 8.19351271e-07
Iter: 304 loss: 8.14006171e-07
Iter: 305 loss: 8.13316547e-07
Iter: 306 loss: 8.13974225e-07
Iter: 307 loss: 8.12973326e-07
Iter: 308 loss: 8.12635676e-07
Iter: 309 loss: 8.12584403e-07
Iter: 310 loss: 8.12187864e-07
Iter: 311 loss: 8.12257326e-07
Iter: 312 loss: 8.11912173e-07
Iter: 313 loss: 8.11644554e-07
Iter: 314 loss: 8.11738801e-07
Iter: 315 loss: 8.11422296e-07
Iter: 316 loss: 8.10930715e-07
Iter: 317 loss: 8.13304837e-07
Iter: 318 loss: 8.10855795e-07
Iter: 319 loss: 8.1054111e-07
Iter: 320 loss: 8.11163375e-07
Iter: 321 loss: 8.10441861e-07
Iter: 322 loss: 8.10111828e-07
Iter: 323 loss: 8.09648839e-07
Iter: 324 loss: 8.09649123e-07
Iter: 325 loss: 8.09041694e-07
Iter: 326 loss: 8.13319843e-07
Iter: 327 loss: 8.09002074e-07
Iter: 328 loss: 8.08628556e-07
Iter: 329 loss: 8.08617187e-07
Iter: 330 loss: 8.08252e-07
Iter: 331 loss: 8.07414722e-07
Iter: 332 loss: 8.17474586e-07
Iter: 333 loss: 8.07359811e-07
Iter: 334 loss: 8.06680646e-07
Iter: 335 loss: 8.08888103e-07
Iter: 336 loss: 8.06471576e-07
Iter: 337 loss: 8.06080038e-07
Iter: 338 loss: 8.06073388e-07
Iter: 339 loss: 8.05618e-07
Iter: 340 loss: 8.05628815e-07
Iter: 341 loss: 8.05218406e-07
Iter: 342 loss: 8.04897923e-07
Iter: 343 loss: 8.04633373e-07
Iter: 344 loss: 8.04530032e-07
Iter: 345 loss: 8.04218587e-07
Iter: 346 loss: 8.04208696e-07
Iter: 347 loss: 8.03849844e-07
Iter: 348 loss: 8.03509351e-07
Iter: 349 loss: 8.03430908e-07
Iter: 350 loss: 8.03036755e-07
Iter: 351 loss: 8.03044941e-07
Iter: 352 loss: 8.0271252e-07
Iter: 353 loss: 8.02267e-07
Iter: 354 loss: 8.02260388e-07
Iter: 355 loss: 8.01921772e-07
Iter: 356 loss: 8.01351575e-07
Iter: 357 loss: 8.0139182e-07
Iter: 358 loss: 8.0097584e-07
Iter: 359 loss: 8.05510808e-07
Iter: 360 loss: 8.00984196e-07
Iter: 361 loss: 8.00749376e-07
Iter: 362 loss: 8.00898817e-07
Iter: 363 loss: 8.00574753e-07
Iter: 364 loss: 8.00330781e-07
Iter: 365 loss: 8.00328053e-07
Iter: 366 loss: 8.00118642e-07
Iter: 367 loss: 7.99859265e-07
Iter: 368 loss: 7.99876261e-07
Iter: 369 loss: 7.99472275e-07
Iter: 370 loss: 7.99037e-07
Iter: 371 loss: 7.98979499e-07
Iter: 372 loss: 7.99095915e-07
Iter: 373 loss: 7.98712335e-07
Iter: 374 loss: 7.9848428e-07
Iter: 375 loss: 7.97878101e-07
Iter: 376 loss: 8.01192527e-07
Iter: 377 loss: 7.97691655e-07
Iter: 378 loss: 7.97281359e-07
Iter: 379 loss: 7.97250777e-07
Iter: 380 loss: 7.96853158e-07
Iter: 381 loss: 7.97885605e-07
Iter: 382 loss: 7.96717814e-07
Iter: 383 loss: 7.96453094e-07
Iter: 384 loss: 7.96135623e-07
Iter: 385 loss: 7.96129768e-07
Iter: 386 loss: 7.95870733e-07
Iter: 387 loss: 7.95875394e-07
Iter: 388 loss: 7.95623066e-07
Iter: 389 loss: 7.95502501e-07
Iter: 390 loss: 7.95382277e-07
Iter: 391 loss: 7.95071571e-07
Iter: 392 loss: 7.94741254e-07
Iter: 393 loss: 7.94701236e-07
Iter: 394 loss: 7.94078233e-07
Iter: 395 loss: 7.96889367e-07
Iter: 396 loss: 7.93967558e-07
Iter: 397 loss: 7.93800609e-07
Iter: 398 loss: 7.93700963e-07
Iter: 399 loss: 7.93522872e-07
Iter: 400 loss: 7.93150207e-07
Iter: 401 loss: 7.98835572e-07
Iter: 402 loss: 7.9315555e-07
Iter: 403 loss: 7.9274821e-07
Iter: 404 loss: 7.94842094e-07
Iter: 405 loss: 7.92663855e-07
Iter: 406 loss: 7.92446713e-07
Iter: 407 loss: 7.92466949e-07
Iter: 408 loss: 7.922539e-07
Iter: 409 loss: 7.92169658e-07
Iter: 410 loss: 7.92111337e-07
Iter: 411 loss: 7.91963089e-07
Iter: 412 loss: 7.91655509e-07
Iter: 413 loss: 7.9805875e-07
Iter: 414 loss: 7.91662728e-07
Iter: 415 loss: 7.91388516e-07
Iter: 416 loss: 7.95348114e-07
Iter: 417 loss: 7.91384309e-07
Iter: 418 loss: 7.91116349e-07
Iter: 419 loss: 7.91000105e-07
Iter: 420 loss: 7.90918648e-07
Iter: 421 loss: 7.90616809e-07
Iter: 422 loss: 7.91503851e-07
Iter: 423 loss: 7.90519039e-07
Iter: 424 loss: 7.90159049e-07
Iter: 425 loss: 7.91144089e-07
Iter: 426 loss: 7.90030185e-07
Iter: 427 loss: 7.89837429e-07
Iter: 428 loss: 7.89450723e-07
Iter: 429 loss: 7.89461296e-07
Iter: 430 loss: 7.89204137e-07
Iter: 431 loss: 7.89200556e-07
Iter: 432 loss: 7.88970965e-07
Iter: 433 loss: 7.90074409e-07
Iter: 434 loss: 7.88915202e-07
Iter: 435 loss: 7.88718637e-07
Iter: 436 loss: 7.88222337e-07
Iter: 437 loss: 7.94106256e-07
Iter: 438 loss: 7.88186071e-07
Iter: 439 loss: 7.87685508e-07
Iter: 440 loss: 7.9060942e-07
Iter: 441 loss: 7.87607178e-07
Iter: 442 loss: 7.87103659e-07
Iter: 443 loss: 7.88476939e-07
Iter: 444 loss: 7.86953194e-07
Iter: 445 loss: 7.86611736e-07
Iter: 446 loss: 7.87895374e-07
Iter: 447 loss: 7.86503961e-07
Iter: 448 loss: 7.86357418e-07
Iter: 449 loss: 7.86305293e-07
Iter: 450 loss: 7.86191549e-07
Iter: 451 loss: 7.85945076e-07
Iter: 452 loss: 7.88654347e-07
Iter: 453 loss: 7.85943598e-07
Iter: 454 loss: 7.85846964e-07
Iter: 455 loss: 7.85830593e-07
Iter: 456 loss: 7.85712302e-07
Iter: 457 loss: 7.85503914e-07
Iter: 458 loss: 7.89663659e-07
Iter: 459 loss: 7.85526595e-07
Iter: 460 loss: 7.85285067e-07
Iter: 461 loss: 7.8526682e-07
Iter: 462 loss: 7.85094358e-07
Iter: 463 loss: 7.84769e-07
Iter: 464 loss: 7.853331e-07
Iter: 465 loss: 7.84622841e-07
Iter: 466 loss: 7.8410244e-07
Iter: 467 loss: 7.88177317e-07
Iter: 468 loss: 7.84103e-07
Iter: 469 loss: 7.83809526e-07
Iter: 470 loss: 7.83240353e-07
Iter: 471 loss: 7.96004088e-07
Iter: 472 loss: 7.83235407e-07
Iter: 473 loss: 7.82826874e-07
Iter: 474 loss: 7.82821076e-07
Iter: 475 loss: 7.82507414e-07
Iter: 476 loss: 7.85018074e-07
Iter: 477 loss: 7.8248587e-07
Iter: 478 loss: 7.82271457e-07
Iter: 479 loss: 7.82022653e-07
Iter: 480 loss: 7.82003042e-07
Iter: 481 loss: 7.81699214e-07
Iter: 482 loss: 7.8256943e-07
Iter: 483 loss: 7.81601159e-07
Iter: 484 loss: 7.81230881e-07
Iter: 485 loss: 7.81479912e-07
Iter: 486 loss: 7.80969572e-07
Iter: 487 loss: 7.80695245e-07
Iter: 488 loss: 7.83245468e-07
Iter: 489 loss: 7.80672735e-07
Iter: 490 loss: 7.80313144e-07
Iter: 491 loss: 7.81157041e-07
Iter: 492 loss: 7.80169785e-07
Iter: 493 loss: 7.7998061e-07
Iter: 494 loss: 7.79973902e-07
Iter: 495 loss: 7.79793e-07
Iter: 496 loss: 7.79649781e-07
Iter: 497 loss: 7.79623861e-07
Iter: 498 loss: 7.79479365e-07
Iter: 499 loss: 7.79164793e-07
Iter: 500 loss: 7.84936e-07
Iter: 501 loss: 7.7918e-07
Iter: 502 loss: 7.78955155e-07
Iter: 503 loss: 7.81127483e-07
Iter: 504 loss: 7.78937135e-07
Iter: 505 loss: 7.78712433e-07
Iter: 506 loss: 7.79240054e-07
Iter: 507 loss: 7.786374e-07
Iter: 508 loss: 7.78413153e-07
Iter: 509 loss: 7.78119841e-07
Iter: 510 loss: 7.78108188e-07
Iter: 511 loss: 7.77990067e-07
Iter: 512 loss: 7.77908554e-07
Iter: 513 loss: 7.77733646e-07
Iter: 514 loss: 7.77296805e-07
Iter: 515 loss: 7.82219161e-07
Iter: 516 loss: 7.7725764e-07
Iter: 517 loss: 7.76809e-07
Iter: 518 loss: 7.7796426e-07
Iter: 519 loss: 7.7665095e-07
Iter: 520 loss: 7.76263505e-07
Iter: 521 loss: 7.78724257e-07
Iter: 522 loss: 7.7622667e-07
Iter: 523 loss: 7.75952401e-07
Iter: 524 loss: 7.77475066e-07
Iter: 525 loss: 7.75943306e-07
Iter: 526 loss: 7.75717638e-07
Iter: 527 loss: 7.780788e-07
Iter: 528 loss: 7.75709964e-07
Iter: 529 loss: 7.7556939e-07
Iter: 530 loss: 7.75275453e-07
Iter: 531 loss: 7.79634888e-07
Iter: 532 loss: 7.75257718e-07
Iter: 533 loss: 7.75129e-07
Iter: 534 loss: 7.75074056e-07
Iter: 535 loss: 7.74921432e-07
Iter: 536 loss: 7.74606292e-07
Iter: 537 loss: 7.78774734e-07
Iter: 538 loss: 7.74597083e-07
Iter: 539 loss: 7.74259604e-07
Iter: 540 loss: 7.74810303e-07
Iter: 541 loss: 7.74102716e-07
Iter: 542 loss: 7.73833222e-07
Iter: 543 loss: 7.77567834e-07
Iter: 544 loss: 7.73829242e-07
Iter: 545 loss: 7.73578506e-07
Iter: 546 loss: 7.74343562e-07
Iter: 547 loss: 7.73483293e-07
Iter: 548 loss: 7.73345505e-07
Iter: 549 loss: 7.73238128e-07
Iter: 550 loss: 7.73177703e-07
Iter: 551 loss: 7.72893088e-07
Iter: 552 loss: 7.76071829e-07
Iter: 553 loss: 7.72902808e-07
Iter: 554 loss: 7.72764e-07
Iter: 555 loss: 7.72482508e-07
Iter: 556 loss: 7.72465967e-07
Iter: 557 loss: 7.72147303e-07
Iter: 558 loss: 7.71764917e-07
Iter: 559 loss: 7.71700627e-07
Iter: 560 loss: 7.71289933e-07
Iter: 561 loss: 7.77762807e-07
Iter: 562 loss: 7.71277769e-07
Iter: 563 loss: 7.70942222e-07
Iter: 564 loss: 7.75763169e-07
Iter: 565 loss: 7.70954841e-07
Iter: 566 loss: 7.70840472e-07
Iter: 567 loss: 7.70569045e-07
Iter: 568 loss: 7.75118792e-07
Iter: 569 loss: 7.70572342e-07
Iter: 570 loss: 7.70422218e-07
Iter: 571 loss: 7.70384759e-07
Iter: 572 loss: 7.70232646e-07
Iter: 573 loss: 7.700271e-07
Iter: 574 loss: 7.70023462e-07
Iter: 575 loss: 7.69785572e-07
Iter: 576 loss: 7.70175575e-07
Iter: 577 loss: 7.69665292e-07
Iter: 578 loss: 7.69407052e-07
Iter: 579 loss: 7.69994e-07
Iter: 580 loss: 7.69300186e-07
Iter: 581 loss: 7.69083897e-07
Iter: 582 loss: 7.72412818e-07
Iter: 583 loss: 7.69076962e-07
Iter: 584 loss: 7.68892619e-07
Iter: 585 loss: 7.68615337e-07
Iter: 586 loss: 7.6862e-07
Iter: 587 loss: 7.68301391e-07
Iter: 588 loss: 7.68350731e-07
Iter: 589 loss: 7.6804082e-07
Iter: 590 loss: 7.67912752e-07
Iter: 591 loss: 7.67845847e-07
Iter: 592 loss: 7.6765059e-07
Iter: 593 loss: 7.67406846e-07
Iter: 594 loss: 7.67396841e-07
Iter: 595 loss: 7.67126494e-07
Iter: 596 loss: 7.6723677e-07
Iter: 597 loss: 7.66953235e-07
Iter: 598 loss: 7.66631729e-07
Iter: 599 loss: 7.68530413e-07
Iter: 600 loss: 7.6660649e-07
Iter: 601 loss: 7.66341941e-07
Iter: 602 loss: 7.70209681e-07
Iter: 603 loss: 7.66335461e-07
Iter: 604 loss: 7.66202618e-07
Iter: 605 loss: 7.65908851e-07
Iter: 606 loss: 7.68910922e-07
Iter: 607 loss: 7.65886739e-07
Iter: 608 loss: 7.65817276e-07
Iter: 609 loss: 7.6572519e-07
Iter: 610 loss: 7.65609116e-07
Iter: 611 loss: 7.65444724e-07
Iter: 612 loss: 7.65426307e-07
Iter: 613 loss: 7.65223263e-07
Iter: 614 loss: 7.65796699e-07
Iter: 615 loss: 7.65169716e-07
Iter: 616 loss: 7.64980882e-07
Iter: 617 loss: 7.66623714e-07
Iter: 618 loss: 7.6495553e-07
Iter: 619 loss: 7.64823085e-07
Iter: 620 loss: 7.6478625e-07
Iter: 621 loss: 7.64729407e-07
Iter: 622 loss: 7.64474805e-07
Iter: 623 loss: 7.64445929e-07
Iter: 624 loss: 7.64270794e-07
Iter: 625 loss: 7.6394349e-07
Iter: 626 loss: 7.64518518e-07
Iter: 627 loss: 7.6383958e-07
Iter: 628 loss: 7.63622438e-07
Iter: 629 loss: 7.6359e-07
Iter: 630 loss: 7.63419678e-07
Iter: 631 loss: 7.63121875e-07
Iter: 632 loss: 7.67195786e-07
Iter: 633 loss: 7.63099195e-07
Iter: 634 loss: 7.62845332e-07
Iter: 635 loss: 7.66091944e-07
Iter: 636 loss: 7.62850448e-07
Iter: 637 loss: 7.62640411e-07
Iter: 638 loss: 7.64443939e-07
Iter: 639 loss: 7.62648881e-07
Iter: 640 loss: 7.62551736e-07
Iter: 641 loss: 7.62275818e-07
Iter: 642 loss: 7.64082927e-07
Iter: 643 loss: 7.62214142e-07
Iter: 644 loss: 7.62233924e-07
Iter: 645 loss: 7.62082095e-07
Iter: 646 loss: 7.61979e-07
Iter: 647 loss: 7.61792535e-07
Iter: 648 loss: 7.66016e-07
Iter: 649 loss: 7.61793842e-07
Iter: 650 loss: 7.61620299e-07
Iter: 651 loss: 7.61839829e-07
Iter: 652 loss: 7.61511728e-07
Iter: 653 loss: 7.61417709e-07
Iter: 654 loss: 7.61377578e-07
Iter: 655 loss: 7.61271e-07
Iter: 656 loss: 7.61043509e-07
Iter: 657 loss: 7.65215759e-07
Iter: 658 loss: 7.61053911e-07
Iter: 659 loss: 7.60838475e-07
Iter: 660 loss: 7.6157221e-07
Iter: 661 loss: 7.60768103e-07
Iter: 662 loss: 7.60491957e-07
Iter: 663 loss: 7.6088736e-07
Iter: 664 loss: 7.6041465e-07
Iter: 665 loss: 7.60274e-07
Iter: 666 loss: 7.60280216e-07
Iter: 667 loss: 7.60137e-07
Iter: 668 loss: 7.59851162e-07
Iter: 669 loss: 7.63817e-07
Iter: 670 loss: 7.5985497e-07
Iter: 671 loss: 7.59599e-07
Iter: 672 loss: 7.60107923e-07
Iter: 673 loss: 7.5948094e-07
Iter: 674 loss: 7.59381e-07
Iter: 675 loss: 7.59339912e-07
Iter: 676 loss: 7.59204227e-07
Iter: 677 loss: 7.5895e-07
Iter: 678 loss: 7.58973556e-07
Iter: 679 loss: 7.58759711e-07
Iter: 680 loss: 7.58964575e-07
Iter: 681 loss: 7.58639828e-07
Iter: 682 loss: 7.58504427e-07
Iter: 683 loss: 7.58476745e-07
Iter: 684 loss: 7.58384658e-07
Iter: 685 loss: 7.58089868e-07
Iter: 686 loss: 7.6077356e-07
Iter: 687 loss: 7.58052465e-07
Iter: 688 loss: 7.57800876e-07
Iter: 689 loss: 7.59429099e-07
Iter: 690 loss: 7.57765065e-07
Iter: 691 loss: 7.57577482e-07
Iter: 692 loss: 7.59698423e-07
Iter: 693 loss: 7.57577368e-07
Iter: 694 loss: 7.57412636e-07
Iter: 695 loss: 7.57229373e-07
Iter: 696 loss: 7.57198e-07
Iter: 697 loss: 7.56973236e-07
Iter: 698 loss: 7.58056331e-07
Iter: 699 loss: 7.56912186e-07
Iter: 700 loss: 7.56746886e-07
Iter: 701 loss: 7.58999136e-07
Iter: 702 loss: 7.56739837e-07
Iter: 703 loss: 7.5661012e-07
Iter: 704 loss: 7.56597785e-07
Iter: 705 loss: 7.56498366e-07
Iter: 706 loss: 7.56330905e-07
Iter: 707 loss: 7.56077725e-07
Iter: 708 loss: 7.56067777e-07
Iter: 709 loss: 7.5584677e-07
Iter: 710 loss: 7.56804639e-07
Iter: 711 loss: 7.55795213e-07
Iter: 712 loss: 7.5560007e-07
Iter: 713 loss: 7.55581368e-07
Iter: 714 loss: 7.55470239e-07
Iter: 715 loss: 7.55198755e-07
Iter: 716 loss: 7.59248962e-07
Iter: 717 loss: 7.55190399e-07
Iter: 718 loss: 7.54980931e-07
Iter: 719 loss: 7.5558188e-07
Iter: 720 loss: 7.54873554e-07
Iter: 721 loss: 7.54671873e-07
Iter: 722 loss: 7.54660675e-07
Iter: 723 loss: 7.54551309e-07
Iter: 724 loss: 7.54246628e-07
Iter: 725 loss: 7.5741184e-07
Iter: 726 loss: 7.54240773e-07
Iter: 727 loss: 7.53900736e-07
Iter: 728 loss: 7.55347969e-07
Iter: 729 loss: 7.53865947e-07
Iter: 730 loss: 7.53637096e-07
Iter: 731 loss: 7.55855467e-07
Iter: 732 loss: 7.53610152e-07
Iter: 733 loss: 7.53371182e-07
Iter: 734 loss: 7.53683594e-07
Iter: 735 loss: 7.53273071e-07
Iter: 736 loss: 7.53053e-07
Iter: 737 loss: 7.53500785e-07
Iter: 738 loss: 7.52986125e-07
Iter: 739 loss: 7.52767562e-07
Iter: 740 loss: 7.54108441e-07
Iter: 741 loss: 7.52767505e-07
Iter: 742 loss: 7.52535925e-07
Iter: 743 loss: 7.52649612e-07
Iter: 744 loss: 7.52449637e-07
Iter: 745 loss: 7.52260348e-07
Iter: 746 loss: 7.52046333e-07
Iter: 747 loss: 7.52021094e-07
Iter: 748 loss: 7.51884443e-07
Iter: 749 loss: 7.51838e-07
Iter: 750 loss: 7.51619496e-07
Iter: 751 loss: 7.51579933e-07
Iter: 752 loss: 7.5147193e-07
Iter: 753 loss: 7.51281277e-07
Iter: 754 loss: 7.50942263e-07
Iter: 755 loss: 7.59244699e-07
Iter: 756 loss: 7.5094465e-07
Iter: 757 loss: 7.50804475e-07
Iter: 758 loss: 7.50723075e-07
Iter: 759 loss: 7.5050923e-07
Iter: 760 loss: 7.50380082e-07
Iter: 761 loss: 7.50332561e-07
Iter: 762 loss: 7.50105926e-07
Iter: 763 loss: 7.49916921e-07
Iter: 764 loss: 7.4983808e-07
Iter: 765 loss: 7.49613832e-07
Iter: 766 loss: 7.51177936e-07
Iter: 767 loss: 7.49567789e-07
Iter: 768 loss: 7.49447565e-07
Iter: 769 loss: 7.49441938e-07
Iter: 770 loss: 7.49335868e-07
Iter: 771 loss: 7.49262426e-07
Iter: 772 loss: 7.49219396e-07
Iter: 773 loss: 7.4905563e-07
Iter: 774 loss: 7.49112417e-07
Iter: 775 loss: 7.48977243e-07
Iter: 776 loss: 7.487223e-07
Iter: 777 loss: 7.49799483e-07
Iter: 778 loss: 7.4870411e-07
Iter: 779 loss: 7.48505e-07
Iter: 780 loss: 7.49916524e-07
Iter: 781 loss: 7.48475031e-07
Iter: 782 loss: 7.48298078e-07
Iter: 783 loss: 7.47944739e-07
Iter: 784 loss: 7.5647165e-07
Iter: 785 loss: 7.47962758e-07
Iter: 786 loss: 7.47782963e-07
Iter: 787 loss: 7.47761305e-07
Iter: 788 loss: 7.47555e-07
Iter: 789 loss: 7.47625336e-07
Iter: 790 loss: 7.47406148e-07
Iter: 791 loss: 7.4726745e-07
Iter: 792 loss: 7.47025751e-07
Iter: 793 loss: 7.47013e-07
Iter: 794 loss: 7.46875e-07
Iter: 795 loss: 7.46844705e-07
Iter: 796 loss: 7.46684e-07
Iter: 797 loss: 7.4654821e-07
Iter: 798 loss: 7.46504725e-07
Iter: 799 loss: 7.46324417e-07
Iter: 800 loss: 7.46531896e-07
Iter: 801 loss: 7.46222099e-07
Iter: 802 loss: 7.46052137e-07
Iter: 803 loss: 7.48411e-07
Iter: 804 loss: 7.46037699e-07
Iter: 805 loss: 7.4593305e-07
Iter: 806 loss: 7.45983812e-07
Iter: 807 loss: 7.45889508e-07
Iter: 808 loss: 7.45756552e-07
Iter: 809 loss: 7.4576667e-07
Iter: 810 loss: 7.45663669e-07
Iter: 811 loss: 7.45511556e-07
Iter: 812 loss: 7.45528382e-07
Iter: 813 loss: 7.45408443e-07
Iter: 814 loss: 7.45700845e-07
Iter: 815 loss: 7.45383488e-07
Iter: 816 loss: 7.45231318e-07
Iter: 817 loss: 7.45020884e-07
Iter: 818 loss: 7.49936589e-07
Iter: 819 loss: 7.45026227e-07
Iter: 820 loss: 7.44941531e-07
Iter: 821 loss: 7.44875933e-07
Iter: 822 loss: 7.4473661e-07
Iter: 823 loss: 7.44497186e-07
Iter: 824 loss: 7.47929562e-07
Iter: 825 loss: 7.44473709e-07
Iter: 826 loss: 7.44149418e-07
Iter: 827 loss: 7.44534645e-07
Iter: 828 loss: 7.44010322e-07
Iter: 829 loss: 7.43811597e-07
Iter: 830 loss: 7.43805231e-07
Iter: 831 loss: 7.43605426e-07
Iter: 832 loss: 7.43929036e-07
Iter: 833 loss: 7.43566943e-07
Iter: 834 loss: 7.43422561e-07
Iter: 835 loss: 7.43200474e-07
Iter: 836 loss: 7.4319496e-07
Iter: 837 loss: 7.4305575e-07
Iter: 838 loss: 7.43044552e-07
Iter: 839 loss: 7.42889313e-07
Iter: 840 loss: 7.42974407e-07
Iter: 841 loss: 7.42783755e-07
Iter: 842 loss: 7.42604925e-07
Iter: 843 loss: 7.42416887e-07
Iter: 844 loss: 7.42399322e-07
Iter: 845 loss: 7.42225211e-07
Iter: 846 loss: 7.42236e-07
Iter: 847 loss: 7.42034899e-07
Iter: 848 loss: 7.42174507e-07
Iter: 849 loss: 7.41939118e-07
Iter: 850 loss: 7.4177683e-07
Iter: 851 loss: 7.41926e-07
Iter: 852 loss: 7.41721522e-07
Iter: 853 loss: 7.41540134e-07
Iter: 854 loss: 7.42929899e-07
Iter: 855 loss: 7.41551048e-07
Iter: 856 loss: 7.41409963e-07
Iter: 857 loss: 7.41515692e-07
Iter: 858 loss: 7.41309123e-07
Iter: 859 loss: 7.41163035e-07
Iter: 860 loss: 7.40979203e-07
Iter: 861 loss: 7.41001941e-07
Iter: 862 loss: 7.40690666e-07
Iter: 863 loss: 7.41241934e-07
Iter: 864 loss: 7.40569533e-07
Iter: 865 loss: 7.40422934e-07
Iter: 866 loss: 7.4039184e-07
Iter: 867 loss: 7.4025e-07
Iter: 868 loss: 7.39950508e-07
Iter: 869 loss: 7.42897896e-07
Iter: 870 loss: 7.39917709e-07
Iter: 871 loss: 7.39607e-07
Iter: 872 loss: 7.40366545e-07
Iter: 873 loss: 7.39483426e-07
Iter: 874 loss: 7.39238089e-07
Iter: 875 loss: 7.41004669e-07
Iter: 876 loss: 7.39220241e-07
Iter: 877 loss: 7.39071368e-07
Iter: 878 loss: 7.4115826e-07
Iter: 879 loss: 7.39065058e-07
Iter: 880 loss: 7.38896574e-07
Iter: 881 loss: 7.38820688e-07
Iter: 882 loss: 7.38748383e-07
Iter: 883 loss: 7.38598885e-07
Iter: 884 loss: 7.39157713e-07
Iter: 885 loss: 7.38556e-07
Iter: 886 loss: 7.38390668e-07
Iter: 887 loss: 7.39592167e-07
Iter: 888 loss: 7.38396466e-07
Iter: 889 loss: 7.3828528e-07
Iter: 890 loss: 7.38180802e-07
Iter: 891 loss: 7.3816085e-07
Iter: 892 loss: 7.38028575e-07
Iter: 893 loss: 7.38387598e-07
Iter: 894 loss: 7.37954792e-07
Iter: 895 loss: 7.37819846e-07
Iter: 896 loss: 7.37813139e-07
Iter: 897 loss: 7.37686435e-07
Iter: 898 loss: 7.37711105e-07
Iter: 899 loss: 7.37585708e-07
Iter: 900 loss: 7.37479581e-07
Iter: 901 loss: 7.37380788e-07
Iter: 902 loss: 7.37346113e-07
Iter: 903 loss: 7.37254254e-07
Iter: 904 loss: 7.3722731e-07
Iter: 905 loss: 7.3712954e-07
Iter: 906 loss: 7.36874e-07
Iter: 907 loss: 7.38986785e-07
Iter: 908 loss: 7.36815196e-07
Iter: 909 loss: 7.36532911e-07
Iter: 910 loss: 7.37173536e-07
Iter: 911 loss: 7.36455718e-07
Iter: 912 loss: 7.36158313e-07
Iter: 913 loss: 7.38839731e-07
Iter: 914 loss: 7.3618e-07
Iter: 915 loss: 7.3587853e-07
Iter: 916 loss: 7.36902621e-07
Iter: 917 loss: 7.35817366e-07
Iter: 918 loss: 7.35675371e-07
Iter: 919 loss: 7.35685489e-07
Iter: 920 loss: 7.35583967e-07
Iter: 921 loss: 7.35530648e-07
Iter: 922 loss: 7.35492165e-07
Iter: 923 loss: 7.35432707e-07
Iter: 924 loss: 7.35277922e-07
Iter: 925 loss: 7.36995901e-07
Iter: 926 loss: 7.3528156e-07
Iter: 927 loss: 7.35129e-07
Iter: 928 loss: 7.3606185e-07
Iter: 929 loss: 7.35120466e-07
Iter: 930 loss: 7.35036679e-07
Iter: 931 loss: 7.36162804e-07
Iter: 932 loss: 7.35040658e-07
Iter: 933 loss: 7.34909349e-07
Iter: 934 loss: 7.34668049e-07
Iter: 935 loss: 7.36050197e-07
Iter: 936 loss: 7.34589e-07
Iter: 937 loss: 7.34315961e-07
Iter: 938 loss: 7.36968218e-07
Iter: 939 loss: 7.3429635e-07
Iter: 940 loss: 7.3408728e-07
Iter: 941 loss: 7.35755066e-07
Iter: 942 loss: 7.34036e-07
Iter: 943 loss: 7.33918114e-07
Iter: 944 loss: 7.33663796e-07
Iter: 945 loss: 7.33661295e-07
Iter: 946 loss: 7.33453476e-07
Iter: 947 loss: 7.33703644e-07
Iter: 948 loss: 7.33370882e-07
Iter: 949 loss: 7.33275442e-07
Iter: 950 loss: 7.332373e-07
Iter: 951 loss: 7.33135039e-07
Iter: 952 loss: 7.33031129e-07
Iter: 953 loss: 7.33018169e-07
Iter: 954 loss: 7.32889873e-07
Iter: 955 loss: 7.33478771e-07
Iter: 956 loss: 7.32875606e-07
Iter: 957 loss: 7.32704393e-07
Iter: 958 loss: 7.33229399e-07
Iter: 959 loss: 7.32651245e-07
Iter: 960 loss: 7.32531817e-07
Iter: 961 loss: 7.32380897e-07
Iter: 962 loss: 7.32333831e-07
Iter: 963 loss: 7.32192518e-07
Iter: 964 loss: 7.32199055e-07
Iter: 965 loss: 7.32077638e-07
Iter: 966 loss: 7.3225749e-07
Iter: 967 loss: 7.31985097e-07
Iter: 968 loss: 7.31876753e-07
Iter: 969 loss: 7.31654154e-07
Iter: 970 loss: 7.31661203e-07
Iter: 971 loss: 7.31545697e-07
Iter: 972 loss: 7.31541718e-07
Iter: 973 loss: 7.31413536e-07
Iter: 974 loss: 7.31373e-07
Iter: 975 loss: 7.31288424e-07
Iter: 976 loss: 7.31115961e-07
Iter: 977 loss: 7.30959414e-07
Iter: 978 loss: 7.30923432e-07
Iter: 979 loss: 7.3069981e-07
Iter: 980 loss: 7.30891315e-07
Iter: 981 loss: 7.30557474e-07
Iter: 982 loss: 7.30416104e-07
Iter: 983 loss: 7.30420879e-07
Iter: 984 loss: 7.302325e-07
Iter: 985 loss: 7.30191687e-07
Iter: 986 loss: 7.30117563e-07
Iter: 987 loss: 7.29978069e-07
Iter: 988 loss: 7.30528825e-07
Iter: 989 loss: 7.29939813e-07
Iter: 990 loss: 7.29770363e-07
Iter: 991 loss: 7.30428042e-07
Iter: 992 loss: 7.29742169e-07
Iter: 993 loss: 7.29659064e-07
Iter: 994 loss: 7.29542194e-07
Iter: 995 loss: 7.29523435e-07
Iter: 996 loss: 7.2943908e-07
Iter: 997 loss: 7.29429246e-07
Iter: 998 loss: 7.29347335e-07
Iter: 999 loss: 7.2916896e-07
Iter: 1000 loss: 7.32148e-07
Iter: 1001 loss: 7.29184649e-07
Iter: 1002 loss: 7.28988425e-07
Iter: 1003 loss: 7.29628482e-07
Iter: 1004 loss: 7.28938232e-07
Iter: 1005 loss: 7.2878413e-07
Iter: 1006 loss: 7.3065064e-07
Iter: 1007 loss: 7.28785608e-07
Iter: 1008 loss: 7.28638383e-07
Iter: 1009 loss: 7.28410043e-07
Iter: 1010 loss: 7.28397481e-07
Iter: 1011 loss: 7.28149871e-07
Iter: 1012 loss: 7.28047667e-07
Iter: 1013 loss: 7.27873612e-07
Iter: 1014 loss: 7.2754375e-07
Iter: 1015 loss: 7.28835289e-07
Iter: 1016 loss: 7.2746252e-07
Iter: 1017 loss: 7.27245947e-07
Iter: 1018 loss: 7.27224574e-07
Iter: 1019 loss: 7.2698856e-07
Iter: 1020 loss: 7.27601e-07
Iter: 1021 loss: 7.26923645e-07
Iter: 1022 loss: 7.26796543e-07
Iter: 1023 loss: 7.26684789e-07
Iter: 1024 loss: 7.26648409e-07
Iter: 1025 loss: 7.26486064e-07
Iter: 1026 loss: 7.26465373e-07
Iter: 1027 loss: 7.26397502e-07
Iter: 1028 loss: 7.26237658e-07
Iter: 1029 loss: 7.28307839e-07
Iter: 1030 loss: 7.26241694e-07
Iter: 1031 loss: 7.26158e-07
Iter: 1032 loss: 7.26111068e-07
Iter: 1033 loss: 7.26058886e-07
Iter: 1034 loss: 7.259124e-07
Iter: 1035 loss: 7.2765e-07
Iter: 1036 loss: 7.25908933e-07
Iter: 1037 loss: 7.25711516e-07
Iter: 1038 loss: 7.2639466e-07
Iter: 1039 loss: 7.25684345e-07
Iter: 1040 loss: 7.25490281e-07
Iter: 1041 loss: 7.26599581e-07
Iter: 1042 loss: 7.25474933e-07
Iter: 1043 loss: 7.2535363e-07
Iter: 1044 loss: 7.25235054e-07
Iter: 1045 loss: 7.25191967e-07
Iter: 1046 loss: 7.25021323e-07
Iter: 1047 loss: 7.2506532e-07
Iter: 1048 loss: 7.24890299e-07
Iter: 1049 loss: 7.24665597e-07
Iter: 1050 loss: 7.25107498e-07
Iter: 1051 loss: 7.24570498e-07
Iter: 1052 loss: 7.24301344e-07
Iter: 1053 loss: 7.25093514e-07
Iter: 1054 loss: 7.24262122e-07
Iter: 1055 loss: 7.24027529e-07
Iter: 1056 loss: 7.27338943e-07
Iter: 1057 loss: 7.24038216e-07
Iter: 1058 loss: 7.23860126e-07
Iter: 1059 loss: 7.23692494e-07
Iter: 1060 loss: 7.23635708e-07
Iter: 1061 loss: 7.23422772e-07
Iter: 1062 loss: 7.26560074e-07
Iter: 1063 loss: 7.23419419e-07
Iter: 1064 loss: 7.23197502e-07
Iter: 1065 loss: 7.22963705e-07
Iter: 1066 loss: 7.22925961e-07
Iter: 1067 loss: 7.22714276e-07
Iter: 1068 loss: 7.24869835e-07
Iter: 1069 loss: 7.22708592e-07
Iter: 1070 loss: 7.22522486e-07
Iter: 1071 loss: 7.23315793e-07
Iter: 1072 loss: 7.22479115e-07
Iter: 1073 loss: 7.22390382e-07
Iter: 1074 loss: 7.22173866e-07
Iter: 1075 loss: 7.26341568e-07
Iter: 1076 loss: 7.22190748e-07
Iter: 1077 loss: 7.22101845e-07
Iter: 1078 loss: 7.22070126e-07
Iter: 1079 loss: 7.21983838e-07
Iter: 1080 loss: 7.2189323e-07
Iter: 1081 loss: 7.21842412e-07
Iter: 1082 loss: 7.21744755e-07
Iter: 1083 loss: 7.21540118e-07
Iter: 1084 loss: 7.21532047e-07
Iter: 1085 loss: 7.21315473e-07
Iter: 1086 loss: 7.21821834e-07
Iter: 1087 loss: 7.21202355e-07
Iter: 1088 loss: 7.20970206e-07
Iter: 1089 loss: 7.20982484e-07
Iter: 1090 loss: 7.20776029e-07
Iter: 1091 loss: 7.21185927e-07
Iter: 1092 loss: 7.20715377e-07
Iter: 1093 loss: 7.20517335e-07
Iter: 1094 loss: 7.20783305e-07
Iter: 1095 loss: 7.20410128e-07
Iter: 1096 loss: 7.202392e-07
Iter: 1097 loss: 7.22322341e-07
Iter: 1098 loss: 7.20216804e-07
Iter: 1099 loss: 7.20073331e-07
Iter: 1100 loss: 7.19863124e-07
Iter: 1101 loss: 7.19861873e-07
Iter: 1102 loss: 7.19681566e-07
Iter: 1103 loss: 7.19655418e-07
Iter: 1104 loss: 7.1947e-07
Iter: 1105 loss: 7.19526611e-07
Iter: 1106 loss: 7.19347042e-07
Iter: 1107 loss: 7.19216473e-07
Iter: 1108 loss: 7.1891634e-07
Iter: 1109 loss: 7.24965446e-07
Iter: 1110 loss: 7.18928163e-07
Iter: 1111 loss: 7.18857e-07
Iter: 1112 loss: 7.18749732e-07
Iter: 1113 loss: 7.18662761e-07
Iter: 1114 loss: 7.18458296e-07
Iter: 1115 loss: 7.18434535e-07
Iter: 1116 loss: 7.18302e-07
Iter: 1117 loss: 7.18581646e-07
Iter: 1118 loss: 7.18225635e-07
Iter: 1119 loss: 7.18058402e-07
Iter: 1120 loss: 7.18105071e-07
Iter: 1121 loss: 7.17955288e-07
Iter: 1122 loss: 7.17810849e-07
Iter: 1123 loss: 7.18623e-07
Iter: 1124 loss: 7.17769581e-07
Iter: 1125 loss: 7.1760212e-07
Iter: 1126 loss: 7.18309252e-07
Iter: 1127 loss: 7.17563694e-07
Iter: 1128 loss: 7.17343596e-07
Iter: 1129 loss: 7.18352567e-07
Iter: 1130 loss: 7.17304033e-07
Iter: 1131 loss: 7.17103831e-07
Iter: 1132 loss: 7.17181251e-07
Iter: 1133 loss: 7.16973375e-07
Iter: 1134 loss: 7.16737645e-07
Iter: 1135 loss: 7.18176125e-07
Iter: 1136 loss: 7.16706268e-07
Iter: 1137 loss: 7.16482191e-07
Iter: 1138 loss: 7.17772195e-07
Iter: 1139 loss: 7.16480599e-07
Iter: 1140 loss: 7.16355657e-07
Iter: 1141 loss: 7.16164777e-07
Iter: 1142 loss: 7.16151135e-07
Iter: 1143 loss: 7.15967246e-07
Iter: 1144 loss: 7.15923932e-07
Iter: 1145 loss: 7.158568e-07
Iter: 1146 loss: 7.15660917e-07
Iter: 1147 loss: 7.18454373e-07
Iter: 1148 loss: 7.15682916e-07
Iter: 1149 loss: 7.15466115e-07
Iter: 1150 loss: 7.16741738e-07
Iter: 1151 loss: 7.15442184e-07
Iter: 1152 loss: 7.15284614e-07
Iter: 1153 loss: 7.16769e-07
Iter: 1154 loss: 7.15251417e-07
Iter: 1155 loss: 7.15122269e-07
Iter: 1156 loss: 7.14821169e-07
Iter: 1157 loss: 7.18084607e-07
Iter: 1158 loss: 7.14766657e-07
Iter: 1159 loss: 7.14477551e-07
Iter: 1160 loss: 7.15722081e-07
Iter: 1161 loss: 7.14392343e-07
Iter: 1162 loss: 7.14142857e-07
Iter: 1163 loss: 7.15827582e-07
Iter: 1164 loss: 7.14119324e-07
Iter: 1165 loss: 7.13972554e-07
Iter: 1166 loss: 7.14683665e-07
Iter: 1167 loss: 7.13947e-07
Iter: 1168 loss: 7.13823169e-07
Iter: 1169 loss: 7.15135457e-07
Iter: 1170 loss: 7.13827262e-07
Iter: 1171 loss: 7.13727104e-07
Iter: 1172 loss: 7.13706584e-07
Iter: 1173 loss: 7.1365514e-07
Iter: 1174 loss: 7.13512691e-07
Iter: 1175 loss: 7.13659688e-07
Iter: 1176 loss: 7.13495069e-07
Iter: 1177 loss: 7.13336078e-07
Iter: 1178 loss: 7.15437295e-07
Iter: 1179 loss: 7.13314364e-07
Iter: 1180 loss: 7.1321017e-07
Iter: 1181 loss: 7.12995586e-07
Iter: 1182 loss: 7.16635839e-07
Iter: 1183 loss: 7.13e-07
Iter: 1184 loss: 7.12782708e-07
Iter: 1185 loss: 7.12807264e-07
Iter: 1186 loss: 7.12649921e-07
Iter: 1187 loss: 7.12365704e-07
Iter: 1188 loss: 7.16232876e-07
Iter: 1189 loss: 7.12368e-07
Iter: 1190 loss: 7.12085921e-07
Iter: 1191 loss: 7.12923793e-07
Iter: 1192 loss: 7.11968937e-07
Iter: 1193 loss: 7.11967687e-07
Iter: 1194 loss: 7.11887083e-07
Iter: 1195 loss: 7.11804e-07
Iter: 1196 loss: 7.11596158e-07
Iter: 1197 loss: 7.128736e-07
Iter: 1198 loss: 7.11551138e-07
Iter: 1199 loss: 7.11358382e-07
Iter: 1200 loss: 7.1269119e-07
Iter: 1201 loss: 7.11332461e-07
Iter: 1202 loss: 7.11149369e-07
Iter: 1203 loss: 7.11455527e-07
Iter: 1204 loss: 7.11081611e-07
Iter: 1205 loss: 7.10960705e-07
Iter: 1206 loss: 7.10962922e-07
Iter: 1207 loss: 7.10867653e-07
Iter: 1208 loss: 7.10731626e-07
Iter: 1209 loss: 7.10730319e-07
Iter: 1210 loss: 7.10564791e-07
Iter: 1211 loss: 7.111787e-07
Iter: 1212 loss: 7.10531253e-07
Iter: 1213 loss: 7.10374366e-07
Iter: 1214 loss: 7.11882535e-07
Iter: 1215 loss: 7.10388122e-07
Iter: 1216 loss: 7.10248457e-07
Iter: 1217 loss: 7.1027614e-07
Iter: 1218 loss: 7.10180927e-07
Iter: 1219 loss: 7.1005752e-07
Iter: 1220 loss: 7.10729068e-07
Iter: 1221 loss: 7.1006491e-07
Iter: 1222 loss: 7.09948552e-07
Iter: 1223 loss: 7.09792289e-07
Iter: 1224 loss: 7.09764095e-07
Iter: 1225 loss: 7.09573783e-07
Iter: 1226 loss: 7.0971123e-07
Iter: 1227 loss: 7.09468736e-07
Iter: 1228 loss: 7.09256483e-07
Iter: 1229 loss: 7.09765914e-07
Iter: 1230 loss: 7.09197479e-07
Iter: 1231 loss: 7.08936454e-07
Iter: 1232 loss: 7.11375606e-07
Iter: 1233 loss: 7.08941684e-07
Iter: 1234 loss: 7.08815946e-07
Iter: 1235 loss: 7.08602784e-07
Iter: 1236 loss: 7.08612674e-07
Iter: 1237 loss: 7.0843555e-07
Iter: 1238 loss: 7.09134611e-07
Iter: 1239 loss: 7.083745e-07
Iter: 1240 loss: 7.08267066e-07
Iter: 1241 loss: 7.08269681e-07
Iter: 1242 loss: 7.0814e-07
Iter: 1243 loss: 7.08160201e-07
Iter: 1244 loss: 7.08081416e-07
Iter: 1245 loss: 7.07948061e-07
Iter: 1246 loss: 7.08011896e-07
Iter: 1247 loss: 7.07862e-07
Iter: 1248 loss: 7.07725e-07
Iter: 1249 loss: 7.0862734e-07
Iter: 1250 loss: 7.0771938e-07
Iter: 1251 loss: 7.07593927e-07
Iter: 1252 loss: 7.08958339e-07
Iter: 1253 loss: 7.07587105e-07
Iter: 1254 loss: 7.07508e-07
Iter: 1255 loss: 7.0743755e-07
Iter: 1256 loss: 7.07427375e-07
Iter: 1257 loss: 7.07292e-07
Iter: 1258 loss: 7.08422363e-07
Iter: 1259 loss: 7.07264576e-07
Iter: 1260 loss: 7.0714043e-07
Iter: 1261 loss: 7.06925675e-07
Iter: 1262 loss: 7.10488166e-07
Iter: 1263 loss: 7.06877586e-07
Iter: 1264 loss: 7.06702167e-07
Iter: 1265 loss: 7.08721473e-07
Iter: 1266 loss: 7.06694891e-07
Iter: 1267 loss: 7.06513731e-07
Iter: 1268 loss: 7.0770318e-07
Iter: 1269 loss: 7.06516e-07
Iter: 1270 loss: 7.06390097e-07
Iter: 1271 loss: 7.06166475e-07
Iter: 1272 loss: 7.11003622e-07
Iter: 1273 loss: 7.06151354e-07
Iter: 1274 loss: 7.05974799e-07
Iter: 1275 loss: 7.06485253e-07
Iter: 1276 loss: 7.05895e-07
Iter: 1277 loss: 7.05739694e-07
Iter: 1278 loss: 7.08044411e-07
Iter: 1279 loss: 7.05728098e-07
Iter: 1280 loss: 7.05591447e-07
Iter: 1281 loss: 7.05797447e-07
Iter: 1282 loss: 7.0552926e-07
Iter: 1283 loss: 7.05403181e-07
Iter: 1284 loss: 7.05298817e-07
Iter: 1285 loss: 7.052613e-07
Iter: 1286 loss: 7.05095772e-07
Iter: 1287 loss: 7.07538106e-07
Iter: 1288 loss: 7.05095829e-07
Iter: 1289 loss: 7.04980209e-07
Iter: 1290 loss: 7.05629873e-07
Iter: 1291 loss: 7.04951958e-07
Iter: 1292 loss: 7.04819854e-07
Iter: 1293 loss: 7.04784725e-07
Iter: 1294 loss: 7.04738227e-07
Iter: 1295 loss: 7.04559284e-07
Iter: 1296 loss: 7.05871798e-07
Iter: 1297 loss: 7.04544618e-07
Iter: 1298 loss: 7.04418e-07
Iter: 1299 loss: 7.04224e-07
Iter: 1300 loss: 7.0422675e-07
Iter: 1301 loss: 7.04079753e-07
Iter: 1302 loss: 7.06125149e-07
Iter: 1303 loss: 7.04090553e-07
Iter: 1304 loss: 7.03922183e-07
Iter: 1305 loss: 7.04360332e-07
Iter: 1306 loss: 7.03876253e-07
Iter: 1307 loss: 7.03772344e-07
Iter: 1308 loss: 7.03582941e-07
Iter: 1309 loss: 7.03581634e-07
Iter: 1310 loss: 7.03456408e-07
Iter: 1311 loss: 7.03834701e-07
Iter: 1312 loss: 7.03378817e-07
Iter: 1313 loss: 7.03235457e-07
Iter: 1314 loss: 7.05057914e-07
Iter: 1315 loss: 7.03230285e-07
Iter: 1316 loss: 7.03055321e-07
Iter: 1317 loss: 7.03080104e-07
Iter: 1318 loss: 7.0295215e-07
Iter: 1319 loss: 7.02786792e-07
Iter: 1320 loss: 7.02802481e-07
Iter: 1321 loss: 7.02684702e-07
Iter: 1322 loss: 7.02558e-07
Iter: 1323 loss: 7.02536283e-07
Iter: 1324 loss: 7.02411114e-07
Iter: 1325 loss: 7.02585e-07
Iter: 1326 loss: 7.02366151e-07
Iter: 1327 loss: 7.02242858e-07
Iter: 1328 loss: 7.02562033e-07
Iter: 1329 loss: 7.02205398e-07
Iter: 1330 loss: 7.02080797e-07
Iter: 1331 loss: 7.02592388e-07
Iter: 1332 loss: 7.02069087e-07
Iter: 1333 loss: 7.01971885e-07
Iter: 1334 loss: 7.01758211e-07
Iter: 1335 loss: 7.06169203e-07
Iter: 1336 loss: 7.01760825e-07
Iter: 1337 loss: 7.01636395e-07
Iter: 1338 loss: 7.01621047e-07
Iter: 1339 loss: 7.01495594e-07
Iter: 1340 loss: 7.01458134e-07
Iter: 1341 loss: 7.01367185e-07
Iter: 1342 loss: 7.01224053e-07
Iter: 1343 loss: 7.01038459e-07
Iter: 1344 loss: 7.01027716e-07
Iter: 1345 loss: 7.00814e-07
Iter: 1346 loss: 7.02319767e-07
Iter: 1347 loss: 7.00801138e-07
Iter: 1348 loss: 7.00667044e-07
Iter: 1349 loss: 7.0066551e-07
Iter: 1350 loss: 7.00553301e-07
Iter: 1351 loss: 7.00559838e-07
Iter: 1352 loss: 7.00476221e-07
Iter: 1353 loss: 7.00377598e-07
Iter: 1354 loss: 7.0027005e-07
Iter: 1355 loss: 7.00226678e-07
Iter: 1356 loss: 7.00055125e-07
Iter: 1357 loss: 7.01792828e-07
Iter: 1358 loss: 7.00047849e-07
Iter: 1359 loss: 6.99917223e-07
Iter: 1360 loss: 7.01257193e-07
Iter: 1361 loss: 6.99908071e-07
Iter: 1362 loss: 6.99824e-07
Iter: 1363 loss: 6.99813427e-07
Iter: 1364 loss: 6.99752377e-07
Iter: 1365 loss: 6.99638463e-07
Iter: 1366 loss: 7.00024145e-07
Iter: 1367 loss: 6.9960339e-07
Iter: 1368 loss: 6.99479e-07
Iter: 1369 loss: 6.99460202e-07
Iter: 1370 loss: 6.99382667e-07
Iter: 1371 loss: 6.99226575e-07
Iter: 1372 loss: 7.00216674e-07
Iter: 1373 loss: 6.99199063e-07
Iter: 1374 loss: 6.99039902e-07
Iter: 1375 loss: 6.99213729e-07
Iter: 1376 loss: 6.98975e-07
Iter: 1377 loss: 6.98836914e-07
Iter: 1378 loss: 6.98699864e-07
Iter: 1379 loss: 6.98689178e-07
Iter: 1380 loss: 6.98468114e-07
Iter: 1381 loss: 6.99096518e-07
Iter: 1382 loss: 6.98399163e-07
Iter: 1383 loss: 6.98217946e-07
Iter: 1384 loss: 7.00085e-07
Iter: 1385 loss: 6.98219196e-07
Iter: 1386 loss: 6.98015242e-07
Iter: 1387 loss: 6.98204815e-07
Iter: 1388 loss: 6.97912924e-07
Iter: 1389 loss: 6.97790483e-07
Iter: 1390 loss: 6.9774336e-07
Iter: 1391 loss: 6.97624557e-07
Iter: 1392 loss: 6.97431e-07
Iter: 1393 loss: 6.97954363e-07
Iter: 1394 loss: 6.97358814e-07
Iter: 1395 loss: 6.97258031e-07
Iter: 1396 loss: 6.97250471e-07
Iter: 1397 loss: 6.9714639e-07
Iter: 1398 loss: 6.97462212e-07
Iter: 1399 loss: 6.97104213e-07
Iter: 1400 loss: 6.97025712e-07
Iter: 1401 loss: 6.97107794e-07
Iter: 1402 loss: 6.96975803e-07
Iter: 1403 loss: 6.96874679e-07
Iter: 1404 loss: 6.97849146e-07
Iter: 1405 loss: 6.96877862e-07
Iter: 1406 loss: 6.96773327e-07
Iter: 1407 loss: 6.96687096e-07
Iter: 1408 loss: 6.96651512e-07
Iter: 1409 loss: 6.96514064e-07
Iter: 1410 loss: 6.96854499e-07
Iter: 1411 loss: 6.96444715e-07
Iter: 1412 loss: 6.96295103e-07
Iter: 1413 loss: 6.98083795e-07
Iter: 1414 loss: 6.96301186e-07
Iter: 1415 loss: 6.96214613e-07
Iter: 1416 loss: 6.96027428e-07
Iter: 1417 loss: 6.99371924e-07
Iter: 1418 loss: 6.960218e-07
Iter: 1419 loss: 6.95804886e-07
Iter: 1420 loss: 6.95870426e-07
Iter: 1421 loss: 6.95657207e-07
Iter: 1422 loss: 6.95424433e-07
Iter: 1423 loss: 6.987392e-07
Iter: 1424 loss: 6.95423296e-07
Iter: 1425 loss: 6.95258791e-07
Iter: 1426 loss: 6.9653521e-07
Iter: 1427 loss: 6.95218e-07
Iter: 1428 loss: 6.95109179e-07
Iter: 1429 loss: 6.9485759e-07
Iter: 1430 loss: 6.99749876e-07
Iter: 1431 loss: 6.94871e-07
Iter: 1432 loss: 6.94677112e-07
Iter: 1433 loss: 6.95692108e-07
Iter: 1434 loss: 6.9463141e-07
Iter: 1435 loss: 6.9449186e-07
Iter: 1436 loss: 6.95696087e-07
Iter: 1437 loss: 6.94467417e-07
Iter: 1438 loss: 6.94368737e-07
Iter: 1439 loss: 6.94364758e-07
Iter: 1440 loss: 6.94313826e-07
Iter: 1441 loss: 6.94236576e-07
Iter: 1442 loss: 6.94237315e-07
Iter: 1443 loss: 6.94147161e-07
Iter: 1444 loss: 6.94049618e-07
Iter: 1445 loss: 6.94033758e-07
Iter: 1446 loss: 6.93927916e-07
Iter: 1447 loss: 6.93932236e-07
Iter: 1448 loss: 6.93839866e-07
Iter: 1449 loss: 6.93674224e-07
Iter: 1450 loss: 6.96109851e-07
Iter: 1451 loss: 6.93652737e-07
Iter: 1452 loss: 6.93448442e-07
Iter: 1453 loss: 6.93863512e-07
Iter: 1454 loss: 6.93371476e-07
Iter: 1455 loss: 6.93138873e-07
Iter: 1456 loss: 6.93144727e-07
Iter: 1457 loss: 6.93025413e-07
Iter: 1458 loss: 6.92835215e-07
Iter: 1459 loss: 6.92800427e-07
Iter: 1460 loss: 6.9265036e-07
Iter: 1461 loss: 6.93634206e-07
Iter: 1462 loss: 6.92623303e-07
Iter: 1463 loss: 6.92423214e-07
Iter: 1464 loss: 6.93470895e-07
Iter: 1465 loss: 6.92374556e-07
Iter: 1466 loss: 6.92289859e-07
Iter: 1467 loss: 6.92249444e-07
Iter: 1468 loss: 6.92211756e-07
Iter: 1469 loss: 6.92055664e-07
Iter: 1470 loss: 6.92003766e-07
Iter: 1471 loss: 6.91914408e-07
Iter: 1472 loss: 6.9189241e-07
Iter: 1473 loss: 6.91821e-07
Iter: 1474 loss: 6.91735181e-07
Iter: 1475 loss: 6.91659807e-07
Iter: 1476 loss: 6.91634568e-07
Iter: 1477 loss: 6.91472792e-07
Iter: 1478 loss: 6.9135217e-07
Iter: 1479 loss: 6.91323066e-07
Iter: 1480 loss: 6.91144407e-07
Iter: 1481 loss: 6.9313711e-07
Iter: 1482 loss: 6.9112366e-07
Iter: 1483 loss: 6.90926072e-07
Iter: 1484 loss: 6.91689934e-07
Iter: 1485 loss: 6.90902652e-07
Iter: 1486 loss: 6.90810339e-07
Iter: 1487 loss: 6.90646175e-07
Iter: 1488 loss: 6.90648051e-07
Iter: 1489 loss: 6.90474849e-07
Iter: 1490 loss: 6.91648e-07
Iter: 1491 loss: 6.90442107e-07
Iter: 1492 loss: 6.90318871e-07
Iter: 1493 loss: 6.91729781e-07
Iter: 1494 loss: 6.90303693e-07
Iter: 1495 loss: 6.90208822e-07
Iter: 1496 loss: 6.89969738e-07
Iter: 1497 loss: 6.92299238e-07
Iter: 1498 loss: 6.89941771e-07
Iter: 1499 loss: 6.89691092e-07
Iter: 1500 loss: 6.91978755e-07
Iter: 1501 loss: 6.8966591e-07
Iter: 1502 loss: 6.89408239e-07
Iter: 1503 loss: 6.90586944e-07
Iter: 1504 loss: 6.89383796e-07
Iter: 1505 loss: 6.89204967e-07
Iter: 1506 loss: 6.88896307e-07
Iter: 1507 loss: 6.95670508e-07
Iter: 1508 loss: 6.8890489e-07
Iter: 1509 loss: 6.88588898e-07
Iter: 1510 loss: 6.91363539e-07
Iter: 1511 loss: 6.8857662e-07
Iter: 1512 loss: 6.8854979e-07
Iter: 1513 loss: 6.88487773e-07
Iter: 1514 loss: 6.88428202e-07
Iter: 1515 loss: 6.88299451e-07
Iter: 1516 loss: 6.91269179e-07
Iter: 1517 loss: 6.8830991e-07
Iter: 1518 loss: 6.88192586e-07
Iter: 1519 loss: 6.88444629e-07
Iter: 1520 loss: 6.88146088e-07
Iter: 1521 loss: 6.88004093e-07
Iter: 1522 loss: 6.88239084e-07
Iter: 1523 loss: 6.87949694e-07
Iter: 1524 loss: 6.87818556e-07
Iter: 1525 loss: 6.89692058e-07
Iter: 1526 loss: 6.87782403e-07
Iter: 1527 loss: 6.87692136e-07
Iter: 1528 loss: 6.87523936e-07
Iter: 1529 loss: 6.91460968e-07
Iter: 1530 loss: 6.87526949e-07
Iter: 1531 loss: 6.87312593e-07
Iter: 1532 loss: 6.87571685e-07
Iter: 1533 loss: 6.87208342e-07
Iter: 1534 loss: 6.87107558e-07
Iter: 1535 loss: 6.870942e-07
Iter: 1536 loss: 6.86965e-07
Iter: 1537 loss: 6.86748763e-07
Iter: 1538 loss: 6.90687216e-07
Iter: 1539 loss: 6.86733415e-07
Iter: 1540 loss: 6.86574424e-07
Iter: 1541 loss: 6.88091e-07
Iter: 1542 loss: 6.86581302e-07
Iter: 1543 loss: 6.86396788e-07
Iter: 1544 loss: 6.86963062e-07
Iter: 1545 loss: 6.86349722e-07
Iter: 1546 loss: 6.86221426e-07
Iter: 1547 loss: 6.86102908e-07
Iter: 1548 loss: 6.86057e-07
Iter: 1549 loss: 6.85879513e-07
Iter: 1550 loss: 6.86527414e-07
Iter: 1551 loss: 6.85831594e-07
Iter: 1552 loss: 6.85582961e-07
Iter: 1553 loss: 6.88316845e-07
Iter: 1554 loss: 6.85555165e-07
Iter: 1555 loss: 6.85468251e-07
Iter: 1556 loss: 6.85252587e-07
Iter: 1557 loss: 6.88886928e-07
Iter: 1558 loss: 6.85260261e-07
Iter: 1559 loss: 6.85071768e-07
Iter: 1560 loss: 6.86322096e-07
Iter: 1561 loss: 6.85034e-07
Iter: 1562 loss: 6.84863835e-07
Iter: 1563 loss: 6.85156692e-07
Iter: 1564 loss: 6.84798351e-07
Iter: 1565 loss: 6.84654765e-07
Iter: 1566 loss: 6.85358941e-07
Iter: 1567 loss: 6.84666645e-07
Iter: 1568 loss: 6.84546819e-07
Iter: 1569 loss: 6.84524764e-07
Iter: 1570 loss: 6.84497763e-07
Iter: 1571 loss: 6.8431541e-07
Iter: 1572 loss: 6.85225e-07
Iter: 1573 loss: 6.84274e-07
Iter: 1574 loss: 6.84013457e-07
Iter: 1575 loss: 6.8456859e-07
Iter: 1576 loss: 6.8388124e-07
Iter: 1577 loss: 6.83728842e-07
Iter: 1578 loss: 6.83725773e-07
Iter: 1579 loss: 6.83557346e-07
Iter: 1580 loss: 6.83307121e-07
Iter: 1581 loss: 6.83305245e-07
Iter: 1582 loss: 6.83041776e-07
Iter: 1583 loss: 6.83967528e-07
Iter: 1584 loss: 6.82985842e-07
Iter: 1585 loss: 6.82892e-07
Iter: 1586 loss: 6.82827135e-07
Iter: 1587 loss: 6.82759037e-07
Iter: 1588 loss: 6.8264103e-07
Iter: 1589 loss: 6.82612495e-07
Iter: 1590 loss: 6.82521e-07
Iter: 1591 loss: 6.82504322e-07
Iter: 1592 loss: 6.82443e-07
Iter: 1593 loss: 6.82316227e-07
Iter: 1594 loss: 6.82325776e-07
Iter: 1595 loss: 6.82181621e-07
Iter: 1596 loss: 6.82519499e-07
Iter: 1597 loss: 6.82151267e-07
Iter: 1598 loss: 6.82019049e-07
Iter: 1599 loss: 6.82136317e-07
Iter: 1600 loss: 6.81923154e-07
Iter: 1601 loss: 6.81780875e-07
Iter: 1602 loss: 6.82459358e-07
Iter: 1603 loss: 6.8176314e-07
Iter: 1604 loss: 6.81688334e-07
Iter: 1605 loss: 6.81686117e-07
Iter: 1606 loss: 6.816216e-07
Iter: 1607 loss: 6.81473466e-07
Iter: 1608 loss: 6.84404142e-07
Iter: 1609 loss: 6.81482561e-07
Iter: 1610 loss: 6.81324082e-07
Iter: 1611 loss: 6.81554582e-07
Iter: 1612 loss: 6.81242113e-07
Iter: 1613 loss: 6.81080564e-07
Iter: 1614 loss: 6.82863345e-07
Iter: 1615 loss: 6.81085055e-07
Iter: 1616 loss: 6.80986147e-07
Iter: 1617 loss: 6.8082619e-07
Iter: 1618 loss: 6.80809876e-07
Iter: 1619 loss: 6.80586879e-07
Iter: 1620 loss: 6.80854384e-07
Iter: 1621 loss: 6.80487233e-07
Iter: 1622 loss: 6.80387757e-07
Iter: 1623 loss: 6.80330459e-07
Iter: 1624 loss: 6.80265089e-07
Iter: 1625 loss: 6.80105472e-07
Iter: 1626 loss: 6.80116273e-07
Iter: 1627 loss: 6.80003268e-07
Iter: 1628 loss: 6.80010317e-07
Iter: 1629 loss: 6.79926643e-07
Iter: 1630 loss: 6.79811308e-07
Iter: 1631 loss: 6.82619884e-07
Iter: 1632 loss: 6.79813752e-07
Iter: 1633 loss: 6.79695745e-07
Iter: 1634 loss: 6.80139578e-07
Iter: 1635 loss: 6.79648e-07
Iter: 1636 loss: 6.79564721e-07
Iter: 1637 loss: 6.7986133e-07
Iter: 1638 loss: 6.79572793e-07
Iter: 1639 loss: 6.79468201e-07
Iter: 1640 loss: 6.79342747e-07
Iter: 1641 loss: 6.79295908e-07
Iter: 1642 loss: 6.79180516e-07
Iter: 1643 loss: 6.79185177e-07
Iter: 1644 loss: 6.79065181e-07
Iter: 1645 loss: 6.794304e-07
Iter: 1646 loss: 6.79026527e-07
Iter: 1647 loss: 6.78918e-07
Iter: 1648 loss: 6.78672166e-07
Iter: 1649 loss: 6.81983579e-07
Iter: 1650 loss: 6.78686604e-07
Iter: 1651 loss: 6.78650792e-07
Iter: 1652 loss: 6.78587526e-07
Iter: 1653 loss: 6.78512606e-07
Iter: 1654 loss: 6.78333549e-07
Iter: 1655 loss: 6.78336903e-07
Iter: 1656 loss: 6.78185756e-07
Iter: 1657 loss: 6.78799836e-07
Iter: 1658 loss: 6.7814085e-07
Iter: 1659 loss: 6.7804649e-07
Iter: 1660 loss: 6.78031483e-07
Iter: 1661 loss: 6.77976232e-07
Iter: 1662 loss: 6.77728451e-07
Iter: 1663 loss: 6.80124231e-07
Iter: 1664 loss: 6.77734647e-07
Iter: 1665 loss: 6.77631874e-07
Iter: 1666 loss: 6.77609535e-07
Iter: 1667 loss: 6.77519e-07
Iter: 1668 loss: 6.77509263e-07
Iter: 1669 loss: 6.77459468e-07
Iter: 1670 loss: 6.77319292e-07
Iter: 1671 loss: 6.77430251e-07
Iter: 1672 loss: 6.77288199e-07
Iter: 1673 loss: 6.77137564e-07
Iter: 1674 loss: 6.77236926e-07
Iter: 1675 loss: 6.77051219e-07
Iter: 1676 loss: 6.76898765e-07
Iter: 1677 loss: 6.7781059e-07
Iter: 1678 loss: 6.76885634e-07
Iter: 1679 loss: 6.76780815e-07
Iter: 1680 loss: 6.77480216e-07
Iter: 1681 loss: 6.7677189e-07
Iter: 1682 loss: 6.76681111e-07
Iter: 1683 loss: 6.76792069e-07
Iter: 1684 loss: 6.76613581e-07
Iter: 1685 loss: 6.76535592e-07
Iter: 1686 loss: 6.76402806e-07
Iter: 1687 loss: 6.76372281e-07
Iter: 1688 loss: 6.76248703e-07
Iter: 1689 loss: 6.76257912e-07
Iter: 1690 loss: 6.76130583e-07
Iter: 1691 loss: 6.76136494e-07
Iter: 1692 loss: 6.7601917e-07
Iter: 1693 loss: 6.75923843e-07
Iter: 1694 loss: 6.75924866e-07
Iter: 1695 loss: 6.75843921e-07
Iter: 1696 loss: 6.75633942e-07
Iter: 1697 loss: 6.78276592e-07
Iter: 1698 loss: 6.75633885e-07
Iter: 1699 loss: 6.75477395e-07
Iter: 1700 loss: 6.76705497e-07
Iter: 1701 loss: 6.7548325e-07
Iter: 1702 loss: 6.75325225e-07
Iter: 1703 loss: 6.75943568e-07
Iter: 1704 loss: 6.75263323e-07
Iter: 1705 loss: 6.75146339e-07
Iter: 1706 loss: 6.75111664e-07
Iter: 1707 loss: 6.75052206e-07
Iter: 1708 loss: 6.74909188e-07
Iter: 1709 loss: 6.75126728e-07
Iter: 1710 loss: 6.74832336e-07
Iter: 1711 loss: 6.74689488e-07
Iter: 1712 loss: 6.75393835e-07
Iter: 1713 loss: 6.74674936e-07
Iter: 1714 loss: 6.74557441e-07
Iter: 1715 loss: 6.75106776e-07
Iter: 1716 loss: 6.74547e-07
Iter: 1717 loss: 6.7441772e-07
Iter: 1718 loss: 6.75404294e-07
Iter: 1719 loss: 6.74420448e-07
Iter: 1720 loss: 6.74353259e-07
Iter: 1721 loss: 6.74220928e-07
Iter: 1722 loss: 6.75895308e-07
Iter: 1723 loss: 6.74223656e-07
Iter: 1724 loss: 6.74060516e-07
Iter: 1725 loss: 6.75836816e-07
Iter: 1726 loss: 6.74063585e-07
Iter: 1727 loss: 6.7396553e-07
Iter: 1728 loss: 6.74178352e-07
Iter: 1729 loss: 6.73922841e-07
Iter: 1730 loss: 6.73805459e-07
Iter: 1731 loss: 6.74089449e-07
Iter: 1732 loss: 6.73760155e-07
Iter: 1733 loss: 6.73603836e-07
Iter: 1734 loss: 6.74099283e-07
Iter: 1735 loss: 6.73567797e-07
Iter: 1736 loss: 6.73463887e-07
Iter: 1737 loss: 6.73198315e-07
Iter: 1738 loss: 6.7669e-07
Iter: 1739 loss: 6.73171485e-07
Iter: 1740 loss: 6.73131069e-07
Iter: 1741 loss: 6.7305939e-07
Iter: 1742 loss: 6.7294809e-07
Iter: 1743 loss: 6.72762667e-07
Iter: 1744 loss: 6.7276676e-07
Iter: 1745 loss: 6.7260288e-07
Iter: 1746 loss: 6.7269832e-07
Iter: 1747 loss: 6.72479132e-07
Iter: 1748 loss: 6.72291947e-07
Iter: 1749 loss: 6.73851275e-07
Iter: 1750 loss: 6.72281772e-07
Iter: 1751 loss: 6.72137389e-07
Iter: 1752 loss: 6.72544502e-07
Iter: 1753 loss: 6.72120962e-07
Iter: 1754 loss: 6.71985163e-07
Iter: 1755 loss: 6.73428247e-07
Iter: 1756 loss: 6.71988232e-07
Iter: 1757 loss: 6.71899329e-07
Iter: 1758 loss: 6.71843281e-07
Iter: 1759 loss: 6.71816224e-07
Iter: 1760 loss: 6.71722205e-07
Iter: 1761 loss: 6.72124145e-07
Iter: 1762 loss: 6.71711405e-07
Iter: 1763 loss: 6.71588452e-07
Iter: 1764 loss: 6.72370902e-07
Iter: 1765 loss: 6.71602493e-07
Iter: 1766 loss: 6.71517228e-07
Iter: 1767 loss: 6.71593284e-07
Iter: 1768 loss: 6.71500743e-07
Iter: 1769 loss: 6.71372391e-07
Iter: 1770 loss: 6.7145271e-07
Iter: 1771 loss: 6.71315775e-07
Iter: 1772 loss: 6.7121448e-07
Iter: 1773 loss: 6.71212092e-07
Iter: 1774 loss: 6.71146495e-07
Iter: 1775 loss: 6.7103997e-07
Iter: 1776 loss: 6.72701503e-07
Iter: 1777 loss: 6.71039516e-07
Iter: 1778 loss: 6.70991767e-07
Iter: 1779 loss: 6.70826296e-07
Iter: 1780 loss: 6.7084909e-07
Iter: 1781 loss: 6.70698341e-07
Iter: 1782 loss: 6.70669294e-07
Iter: 1783 loss: 6.70568397e-07
Iter: 1784 loss: 6.70460622e-07
Iter: 1785 loss: 6.70466818e-07
Iter: 1786 loss: 6.70398663e-07
Iter: 1787 loss: 6.7079975e-07
Iter: 1788 loss: 6.70385361e-07
Iter: 1789 loss: 6.70303791e-07
Iter: 1790 loss: 6.70221937e-07
Iter: 1791 loss: 6.70171971e-07
Iter: 1792 loss: 6.70048962e-07
Iter: 1793 loss: 6.70185955e-07
Iter: 1794 loss: 6.69987685e-07
Iter: 1795 loss: 6.69857513e-07
Iter: 1796 loss: 6.70866314e-07
Iter: 1797 loss: 6.6985308e-07
Iter: 1798 loss: 6.69743827e-07
Iter: 1799 loss: 6.69994563e-07
Iter: 1800 loss: 6.69707561e-07
Iter: 1801 loss: 6.69632527e-07
Iter: 1802 loss: 6.70364e-07
Iter: 1803 loss: 6.69615929e-07
Iter: 1804 loss: 6.69519238e-07
Iter: 1805 loss: 6.69495648e-07
Iter: 1806 loss: 6.69436417e-07
Iter: 1807 loss: 6.69379347e-07
Iter: 1808 loss: 6.6957648e-07
Iter: 1809 loss: 6.69347799e-07
Iter: 1810 loss: 6.69262818e-07
Iter: 1811 loss: 6.69610813e-07
Iter: 1812 loss: 6.69247754e-07
Iter: 1813 loss: 6.6915652e-07
Iter: 1814 loss: 6.69002702e-07
Iter: 1815 loss: 6.71977034e-07
Iter: 1816 loss: 6.69001224e-07
Iter: 1817 loss: 6.68854796e-07
Iter: 1818 loss: 6.69190399e-07
Iter: 1819 loss: 6.68772316e-07
Iter: 1820 loss: 6.68600762e-07
Iter: 1821 loss: 6.69407882e-07
Iter: 1822 loss: 6.68593714e-07
Iter: 1823 loss: 6.68416192e-07
Iter: 1824 loss: 6.68713426e-07
Iter: 1825 loss: 6.68371285e-07
Iter: 1826 loss: 6.68232e-07
Iter: 1827 loss: 6.68252824e-07
Iter: 1828 loss: 6.68139e-07
Iter: 1829 loss: 6.6828e-07
Iter: 1830 loss: 6.68106168e-07
Iter: 1831 loss: 6.68044208e-07
Iter: 1832 loss: 6.67936e-07
Iter: 1833 loss: 6.67927907e-07
Iter: 1834 loss: 6.67884649e-07
Iter: 1835 loss: 6.67858558e-07
Iter: 1836 loss: 6.67782388e-07
Iter: 1837 loss: 6.67788527e-07
Iter: 1838 loss: 6.67723384e-07
Iter: 1839 loss: 6.67639824e-07
Iter: 1840 loss: 6.67990662e-07
Iter: 1841 loss: 6.67604866e-07
Iter: 1842 loss: 6.67490724e-07
Iter: 1843 loss: 6.67466907e-07
Iter: 1844 loss: 6.67393238e-07
Iter: 1845 loss: 6.67267159e-07
Iter: 1846 loss: 6.67759764e-07
Iter: 1847 loss: 6.67273355e-07
Iter: 1848 loss: 6.67115728e-07
Iter: 1849 loss: 6.67457812e-07
Iter: 1850 loss: 6.67064739e-07
Iter: 1851 loss: 6.66978451e-07
Iter: 1852 loss: 6.66895517e-07
Iter: 1853 loss: 6.66852941e-07
Iter: 1854 loss: 6.66728624e-07
Iter: 1855 loss: 6.67250333e-07
Iter: 1856 loss: 6.66683434e-07
Iter: 1857 loss: 6.6657725e-07
Iter: 1858 loss: 6.67523864e-07
Iter: 1859 loss: 6.6657077e-07
Iter: 1860 loss: 6.66461e-07
Iter: 1861 loss: 6.67140853e-07
Iter: 1862 loss: 6.66465212e-07
Iter: 1863 loss: 6.66381311e-07
Iter: 1864 loss: 6.6625887e-07
Iter: 1865 loss: 6.69111728e-07
Iter: 1866 loss: 6.66242499e-07
Iter: 1867 loss: 6.6607322e-07
Iter: 1868 loss: 6.66599e-07
Iter: 1869 loss: 6.65989887e-07
Iter: 1870 loss: 6.65962091e-07
Iter: 1871 loss: 6.65910932e-07
Iter: 1872 loss: 6.65857726e-07
Iter: 1873 loss: 6.65690948e-07
Iter: 1874 loss: 6.68727751e-07
Iter: 1875 loss: 6.65700213e-07
Iter: 1876 loss: 6.65580274e-07
Iter: 1877 loss: 6.65582547e-07
Iter: 1878 loss: 6.65496088e-07
Iter: 1879 loss: 6.65409175e-07
Iter: 1880 loss: 6.65381947e-07
Iter: 1881 loss: 6.65304128e-07
Iter: 1882 loss: 6.65314701e-07
Iter: 1883 loss: 6.65240634e-07
Iter: 1884 loss: 6.6518669e-07
Iter: 1885 loss: 6.6516e-07
Iter: 1886 loss: 6.65044467e-07
Iter: 1887 loss: 6.64929644e-07
Iter: 1888 loss: 6.64922197e-07
Iter: 1889 loss: 6.64706818e-07
Iter: 1890 loss: 6.65523487e-07
Iter: 1891 loss: 6.6469039e-07
Iter: 1892 loss: 6.64560389e-07
Iter: 1893 loss: 6.64541403e-07
Iter: 1894 loss: 6.64455968e-07
Iter: 1895 loss: 6.64741549e-07
Iter: 1896 loss: 6.64403501e-07
Iter: 1897 loss: 6.64337279e-07
Iter: 1898 loss: 6.64244453e-07
Iter: 1899 loss: 6.64216827e-07
Iter: 1900 loss: 6.64091601e-07
Iter: 1901 loss: 6.64788161e-07
Iter: 1902 loss: 6.64085178e-07
Iter: 1903 loss: 6.63936078e-07
Iter: 1904 loss: 6.64780544e-07
Iter: 1905 loss: 6.63910441e-07
Iter: 1906 loss: 6.63856952e-07
Iter: 1907 loss: 6.63858202e-07
Iter: 1908 loss: 6.63795845e-07
Iter: 1909 loss: 6.63692958e-07
Iter: 1910 loss: 6.64203185e-07
Iter: 1911 loss: 6.63672267e-07
Iter: 1912 loss: 6.63595642e-07
Iter: 1913 loss: 6.6366e-07
Iter: 1914 loss: 6.63569949e-07
Iter: 1915 loss: 6.63483547e-07
Iter: 1916 loss: 6.64477113e-07
Iter: 1917 loss: 6.63503045e-07
Iter: 1918 loss: 6.63434548e-07
Iter: 1919 loss: 6.63313131e-07
Iter: 1920 loss: 6.6508079e-07
Iter: 1921 loss: 6.63334049e-07
Iter: 1922 loss: 6.63213143e-07
Iter: 1923 loss: 6.63453704e-07
Iter: 1924 loss: 6.63170681e-07
Iter: 1925 loss: 6.63046535e-07
Iter: 1926 loss: 6.63879405e-07
Iter: 1927 loss: 6.63046421e-07
Iter: 1928 loss: 6.62962179e-07
Iter: 1929 loss: 6.63936476e-07
Iter: 1930 loss: 6.62943307e-07
Iter: 1931 loss: 6.62886521e-07
Iter: 1932 loss: 6.62729576e-07
Iter: 1933 loss: 6.62735204e-07
Iter: 1934 loss: 6.62558875e-07
Iter: 1935 loss: 6.62735829e-07
Iter: 1936 loss: 6.62442289e-07
Iter: 1937 loss: 6.62447746e-07
Iter: 1938 loss: 6.62375612e-07
Iter: 1939 loss: 6.62311e-07
Iter: 1940 loss: 6.62146647e-07
Iter: 1941 loss: 6.63923174e-07
Iter: 1942 loss: 6.62141076e-07
Iter: 1943 loss: 6.62056038e-07
Iter: 1944 loss: 6.62059335e-07
Iter: 1945 loss: 6.61978788e-07
Iter: 1946 loss: 6.61934678e-07
Iter: 1947 loss: 6.619e-07
Iter: 1948 loss: 6.61840943e-07
Iter: 1949 loss: 6.62558705e-07
Iter: 1950 loss: 6.61830086e-07
Iter: 1951 loss: 6.61769604e-07
Iter: 1952 loss: 6.61881927e-07
Iter: 1953 loss: 6.61731292e-07
Iter: 1954 loss: 6.61661488e-07
Iter: 1955 loss: 6.61613058e-07
Iter: 1956 loss: 6.61620732e-07
Iter: 1957 loss: 6.61550473e-07
Iter: 1958 loss: 6.61734248e-07
Iter: 1959 loss: 6.6148948e-07
Iter: 1960 loss: 6.61394324e-07
Iter: 1961 loss: 6.62252944e-07
Iter: 1962 loss: 6.61398531e-07
Iter: 1963 loss: 6.6127609e-07
Iter: 1964 loss: 6.61533818e-07
Iter: 1965 loss: 6.61255854e-07
Iter: 1966 loss: 6.61169111e-07
Iter: 1967 loss: 6.61051672e-07
Iter: 1968 loss: 6.61034164e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.6
+ date
Wed Oct 21 11:11:28 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1 --function f1 --psi 0 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00059c3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00059ea950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000595c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00058f6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00058f6268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000595c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005b48158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00058201e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005820c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00058207b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057f02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00056daa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00056fcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057550d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005765510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00056fcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057cad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005755048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005696400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000567d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005687950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0000440158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005687840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0000436ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00003c36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005743bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0005743840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057130d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00057131e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00003ef9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000041e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00003efbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00003ef510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f000026a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00002c00d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00002b6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
