+ RUN=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ LAYERS=300_100_100_100_1
+ case $RUN in
+ PSI='-2 -1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output120
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output121
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi0
+ date
Sat Nov  7 20:23:51 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_100_100_100_1 --function f1 --psi -2 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1eaf7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1eae82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1eb6fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1ebecc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1eb281e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1eb288c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1eb28ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1eb28488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1ead2e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1ea94048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1e99f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1e96da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1e96d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1e986a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1e986e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1e9869d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8afb430400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b1e986598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8afb430bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8afb39dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8afb39df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8ad42f9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8afb49e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8afb4baf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8afb485378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8afb3f88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8afb3f8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8ad42a7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8ad42a7158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8ad41f3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8ad41b0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8ad4238048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8ad41b0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8ad412ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8ad40da400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8ad40ecea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.006445974
test_loss: 0.006583903
train_loss: 0.0052240947
test_loss: 0.005185754
train_loss: 0.004804647
test_loss: 0.0049643507
train_loss: 0.0046409504
test_loss: 0.0047438927
train_loss: 0.0045718625
test_loss: 0.004648838
train_loss: 0.004639615
test_loss: 0.004473514
train_loss: 0.0045032613
test_loss: 0.0043244995
train_loss: 0.0048793987
test_loss: 0.004482557
train_loss: 0.0042961356
test_loss: 0.004253227
train_loss: 0.004013522
test_loss: 0.0041488674
train_loss: 0.0041667907
test_loss: 0.0044130716
train_loss: 0.0043200245
test_loss: 0.0043542287
train_loss: 0.004083612
test_loss: 0.004233347
train_loss: 0.0041654455
test_loss: 0.0042026984
train_loss: 0.0040853275
test_loss: 0.004358681
train_loss: 0.0040386906
test_loss: 0.004190699
train_loss: 0.0042970255
test_loss: 0.0042053633
train_loss: 0.0041749612
test_loss: 0.0041066543
train_loss: 0.004488313
test_loss: 0.0041510793
train_loss: 0.0041911313
test_loss: 0.00413669
train_loss: 0.0043543833
test_loss: 0.004332317
train_loss: 0.0040398324
test_loss: 0.0041178768
train_loss: 0.004002816
test_loss: 0.004387715
train_loss: 0.003933836
test_loss: 0.0039889826
train_loss: 0.0042532953
test_loss: 0.004070138
train_loss: 0.0040420685
test_loss: 0.0042386367
train_loss: 0.0037912885
test_loss: 0.003943133
train_loss: 0.0041551935
test_loss: 0.004277767
train_loss: 0.004370979
test_loss: 0.0043683276
train_loss: 0.0038862284
test_loss: 0.0039091003
train_loss: 0.0039676945
test_loss: 0.0040536304
train_loss: 0.0039213886
test_loss: 0.004203795
train_loss: 0.0039339336
test_loss: 0.003942171
train_loss: 0.0038022865
test_loss: 0.0040820767
train_loss: 0.0040396727
test_loss: 0.0042378586
train_loss: 0.0040193824
test_loss: 0.004160261
train_loss: 0.0039332462
test_loss: 0.0040016016
train_loss: 0.0037107714
test_loss: 0.0038352464
train_loss: 0.0038624597
test_loss: 0.0042974385
train_loss: 0.0038204
test_loss: 0.004103712
train_loss: 0.003786413
test_loss: 0.0041452604
train_loss: 0.0038283232
test_loss: 0.0038672606
train_loss: 0.003928165
test_loss: 0.0039248485
train_loss: 0.00400389
test_loss: 0.0040325243
train_loss: 0.0036923732
test_loss: 0.003943933
train_loss: 0.0037425244
test_loss: 0.0038458337
train_loss: 0.0036727288
test_loss: 0.0039109103
train_loss: 0.0036452073
test_loss: 0.003924917
train_loss: 0.0035693569
test_loss: 0.0038171043
train_loss: 0.0037118278
test_loss: 0.0041305576
train_loss: 0.0038448973
test_loss: 0.0038908038
train_loss: 0.0035764524
test_loss: 0.0038616175
train_loss: 0.003787724
test_loss: 0.0038836647
train_loss: 0.0035414665
test_loss: 0.0038435473
train_loss: 0.003635931
test_loss: 0.0038468458
train_loss: 0.0036799363
test_loss: 0.003931107
train_loss: 0.003537625
test_loss: 0.0038755047
train_loss: 0.003454843
test_loss: 0.0040195836
train_loss: 0.0035520871
test_loss: 0.0038887411
train_loss: 0.0036029583
test_loss: 0.0038820817
train_loss: 0.0035155476
test_loss: 0.0038696795
train_loss: 0.0034509758
test_loss: 0.0037696254
train_loss: 0.0034457247
test_loss: 0.0036685134
train_loss: 0.0036751067
test_loss: 0.0038906368
train_loss: 0.0034275658
test_loss: 0.0038146009
train_loss: 0.0033244612
test_loss: 0.0037215587
train_loss: 0.004382428
test_loss: 0.0040633543
train_loss: 0.0035789404
test_loss: 0.0040815314
train_loss: 0.0033578011
test_loss: 0.0036726133
train_loss: 0.0033416604
test_loss: 0.0038997773
train_loss: 0.0034251662
test_loss: 0.0036964861
train_loss: 0.0033787414
test_loss: 0.0038479292
train_loss: 0.003411158
test_loss: 0.0036328118
train_loss: 0.0033737763
test_loss: 0.0037317493
train_loss: 0.0034384269
test_loss: 0.003663338
train_loss: 0.0032194625
test_loss: 0.003640304
train_loss: 0.0034141312
test_loss: 0.0039310446
train_loss: 0.0031311533
test_loss: 0.0036748445
train_loss: 0.0033015748
test_loss: 0.0035982498
train_loss: 0.0033732483
test_loss: 0.0037272829
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi0/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi0/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1186839268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1186910e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1186910ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1186895268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f118689a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f118689a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f118689a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f118681cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11867c9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11867c91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1186793620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11867429d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1186742730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11866c8d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11866c8e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11866edd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11866c8d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11866c8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11489fff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11489ffe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11489c5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f114897c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1148924598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f114893df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11241e16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11241f7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11241f7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11241b5c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11241b5e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f112417f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11241768c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1124122c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f112417ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11240abc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f11240e5d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f112407f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.94282366e-05
Iter: 2 loss: 3.51244889e-05
Iter: 3 loss: 1.7602053e-05
Iter: 4 loss: 1.71891879e-05
Iter: 5 loss: 1.66764621e-05
Iter: 6 loss: 1.66329773e-05
Iter: 7 loss: 1.61965472e-05
Iter: 8 loss: 1.61965345e-05
Iter: 9 loss: 1.58394159e-05
Iter: 10 loss: 1.5197942e-05
Iter: 11 loss: 3.09656607e-05
Iter: 12 loss: 1.51978766e-05
Iter: 13 loss: 1.48290537e-05
Iter: 14 loss: 1.48268309e-05
Iter: 15 loss: 1.45620525e-05
Iter: 16 loss: 1.57884388e-05
Iter: 17 loss: 1.45126523e-05
Iter: 18 loss: 1.42686913e-05
Iter: 19 loss: 1.38904725e-05
Iter: 20 loss: 1.38854139e-05
Iter: 21 loss: 1.34331822e-05
Iter: 22 loss: 1.30430963e-05
Iter: 23 loss: 1.29210548e-05
Iter: 24 loss: 1.25328788e-05
Iter: 25 loss: 1.24031158e-05
Iter: 26 loss: 1.21207213e-05
Iter: 27 loss: 1.17502314e-05
Iter: 28 loss: 1.17272757e-05
Iter: 29 loss: 1.13842361e-05
Iter: 30 loss: 1.10892543e-05
Iter: 31 loss: 1.09959146e-05
Iter: 32 loss: 1.07036631e-05
Iter: 33 loss: 1.07030819e-05
Iter: 34 loss: 1.04764877e-05
Iter: 35 loss: 1.05153795e-05
Iter: 36 loss: 1.03063494e-05
Iter: 37 loss: 1.00748748e-05
Iter: 38 loss: 1.09283937e-05
Iter: 39 loss: 1.00175e-05
Iter: 40 loss: 9.82540223e-06
Iter: 41 loss: 9.82187e-06
Iter: 42 loss: 9.64086394e-06
Iter: 43 loss: 9.53217113e-06
Iter: 44 loss: 9.45738429e-06
Iter: 45 loss: 9.36524611e-06
Iter: 46 loss: 9.35802746e-06
Iter: 47 loss: 9.26951725e-06
Iter: 48 loss: 9.10102062e-06
Iter: 49 loss: 1.27293442e-05
Iter: 50 loss: 9.1003094e-06
Iter: 51 loss: 8.92709068e-06
Iter: 52 loss: 1.00002762e-05
Iter: 53 loss: 8.90682895e-06
Iter: 54 loss: 8.84721e-06
Iter: 55 loss: 8.8459019e-06
Iter: 56 loss: 8.78971e-06
Iter: 57 loss: 8.67574181e-06
Iter: 58 loss: 1.07574797e-05
Iter: 59 loss: 8.67392737e-06
Iter: 60 loss: 8.56531369e-06
Iter: 61 loss: 8.81359847e-06
Iter: 62 loss: 8.52432913e-06
Iter: 63 loss: 8.46233706e-06
Iter: 64 loss: 8.45987779e-06
Iter: 65 loss: 8.38391497e-06
Iter: 66 loss: 8.23125447e-06
Iter: 67 loss: 1.11018162e-05
Iter: 68 loss: 8.2294e-06
Iter: 69 loss: 8.10800793e-06
Iter: 70 loss: 8.1528724e-06
Iter: 71 loss: 8.02326758e-06
Iter: 72 loss: 7.90470222e-06
Iter: 73 loss: 9.14355132e-06
Iter: 74 loss: 7.90143895e-06
Iter: 75 loss: 7.85625525e-06
Iter: 76 loss: 8.4593521e-06
Iter: 77 loss: 7.85606699e-06
Iter: 78 loss: 7.79449692e-06
Iter: 79 loss: 7.72599196e-06
Iter: 80 loss: 7.71649e-06
Iter: 81 loss: 7.68681366e-06
Iter: 82 loss: 7.6825163e-06
Iter: 83 loss: 7.655638e-06
Iter: 84 loss: 7.64596825e-06
Iter: 85 loss: 7.63097887e-06
Iter: 86 loss: 7.59476552e-06
Iter: 87 loss: 7.54520534e-06
Iter: 88 loss: 7.54277289e-06
Iter: 89 loss: 7.49811352e-06
Iter: 90 loss: 7.49455648e-06
Iter: 91 loss: 7.46254545e-06
Iter: 92 loss: 7.50928211e-06
Iter: 93 loss: 7.44690442e-06
Iter: 94 loss: 7.41643726e-06
Iter: 95 loss: 7.36195625e-06
Iter: 96 loss: 8.71044267e-06
Iter: 97 loss: 7.36196034e-06
Iter: 98 loss: 7.35466256e-06
Iter: 99 loss: 7.33737579e-06
Iter: 100 loss: 7.31282398e-06
Iter: 101 loss: 7.27378e-06
Iter: 102 loss: 7.27348424e-06
Iter: 103 loss: 7.21955485e-06
Iter: 104 loss: 7.15347915e-06
Iter: 105 loss: 7.14757334e-06
Iter: 106 loss: 7.08064044e-06
Iter: 107 loss: 7.23134144e-06
Iter: 108 loss: 7.0554197e-06
Iter: 109 loss: 7.00789e-06
Iter: 110 loss: 7.00670125e-06
Iter: 111 loss: 6.98457097e-06
Iter: 112 loss: 6.98310532e-06
Iter: 113 loss: 6.97172572e-06
Iter: 114 loss: 6.9440689e-06
Iter: 115 loss: 7.23559242e-06
Iter: 116 loss: 6.94104892e-06
Iter: 117 loss: 6.91522109e-06
Iter: 118 loss: 6.91436207e-06
Iter: 119 loss: 6.90538855e-06
Iter: 120 loss: 6.88331966e-06
Iter: 121 loss: 7.10110044e-06
Iter: 122 loss: 6.8805557e-06
Iter: 123 loss: 6.8545678e-06
Iter: 124 loss: 7.17844887e-06
Iter: 125 loss: 6.8543859e-06
Iter: 126 loss: 6.82867812e-06
Iter: 127 loss: 6.84719544e-06
Iter: 128 loss: 6.81275196e-06
Iter: 129 loss: 6.78858123e-06
Iter: 130 loss: 6.79643153e-06
Iter: 131 loss: 6.77142725e-06
Iter: 132 loss: 6.73606428e-06
Iter: 133 loss: 6.72232409e-06
Iter: 134 loss: 6.70320196e-06
Iter: 135 loss: 6.7210658e-06
Iter: 136 loss: 6.68953544e-06
Iter: 137 loss: 6.67795484e-06
Iter: 138 loss: 6.65434209e-06
Iter: 139 loss: 7.07269328e-06
Iter: 140 loss: 6.65400148e-06
Iter: 141 loss: 6.63355468e-06
Iter: 142 loss: 6.68288976e-06
Iter: 143 loss: 6.6259845e-06
Iter: 144 loss: 6.60789192e-06
Iter: 145 loss: 6.61295917e-06
Iter: 146 loss: 6.59485158e-06
Iter: 147 loss: 6.59160423e-06
Iter: 148 loss: 6.58229146e-06
Iter: 149 loss: 6.57413329e-06
Iter: 150 loss: 6.55392614e-06
Iter: 151 loss: 6.75700812e-06
Iter: 152 loss: 6.5514273e-06
Iter: 153 loss: 6.52783e-06
Iter: 154 loss: 6.83056533e-06
Iter: 155 loss: 6.52768176e-06
Iter: 156 loss: 6.50667926e-06
Iter: 157 loss: 6.47010756e-06
Iter: 158 loss: 6.4700921e-06
Iter: 159 loss: 6.45869386e-06
Iter: 160 loss: 6.45744512e-06
Iter: 161 loss: 6.44242709e-06
Iter: 162 loss: 6.4164833e-06
Iter: 163 loss: 6.41656243e-06
Iter: 164 loss: 6.40060171e-06
Iter: 165 loss: 6.54169617e-06
Iter: 166 loss: 6.3995617e-06
Iter: 167 loss: 6.38922211e-06
Iter: 168 loss: 6.39069367e-06
Iter: 169 loss: 6.38134952e-06
Iter: 170 loss: 6.36580307e-06
Iter: 171 loss: 6.41119914e-06
Iter: 172 loss: 6.36096229e-06
Iter: 173 loss: 6.35283095e-06
Iter: 174 loss: 6.35153447e-06
Iter: 175 loss: 6.34597745e-06
Iter: 176 loss: 6.32762976e-06
Iter: 177 loss: 6.32901447e-06
Iter: 178 loss: 6.30884824e-06
Iter: 179 loss: 6.27954114e-06
Iter: 180 loss: 6.69909105e-06
Iter: 181 loss: 6.27945974e-06
Iter: 182 loss: 6.27276268e-06
Iter: 183 loss: 6.27104691e-06
Iter: 184 loss: 6.26165638e-06
Iter: 185 loss: 6.24212362e-06
Iter: 186 loss: 6.58341e-06
Iter: 187 loss: 6.24174072e-06
Iter: 188 loss: 6.23467895e-06
Iter: 189 loss: 6.23303958e-06
Iter: 190 loss: 6.22540165e-06
Iter: 191 loss: 6.21736763e-06
Iter: 192 loss: 6.21597428e-06
Iter: 193 loss: 6.20499941e-06
Iter: 194 loss: 6.21269555e-06
Iter: 195 loss: 6.19803541e-06
Iter: 196 loss: 6.18584636e-06
Iter: 197 loss: 6.18581635e-06
Iter: 198 loss: 6.18013746e-06
Iter: 199 loss: 6.16529178e-06
Iter: 200 loss: 6.27494319e-06
Iter: 201 loss: 6.16218222e-06
Iter: 202 loss: 6.14551027e-06
Iter: 203 loss: 6.32254614e-06
Iter: 204 loss: 6.14503051e-06
Iter: 205 loss: 6.1347946e-06
Iter: 206 loss: 6.13672182e-06
Iter: 207 loss: 6.12701842e-06
Iter: 208 loss: 6.1173705e-06
Iter: 209 loss: 6.11725227e-06
Iter: 210 loss: 6.10863799e-06
Iter: 211 loss: 6.13463453e-06
Iter: 212 loss: 6.10603729e-06
Iter: 213 loss: 6.10022698e-06
Iter: 214 loss: 6.08514847e-06
Iter: 215 loss: 6.20120773e-06
Iter: 216 loss: 6.08217897e-06
Iter: 217 loss: 6.07049515e-06
Iter: 218 loss: 6.06988806e-06
Iter: 219 loss: 6.06033655e-06
Iter: 220 loss: 6.1519163e-06
Iter: 221 loss: 6.06005142e-06
Iter: 222 loss: 6.0543066e-06
Iter: 223 loss: 6.04056549e-06
Iter: 224 loss: 6.18563354e-06
Iter: 225 loss: 6.03897479e-06
Iter: 226 loss: 6.02561431e-06
Iter: 227 loss: 6.02517503e-06
Iter: 228 loss: 6.02056207e-06
Iter: 229 loss: 6.01306147e-06
Iter: 230 loss: 6.01300098e-06
Iter: 231 loss: 6.00745807e-06
Iter: 232 loss: 6.00745807e-06
Iter: 233 loss: 6.0016373e-06
Iter: 234 loss: 5.99678151e-06
Iter: 235 loss: 5.99528266e-06
Iter: 236 loss: 5.98871247e-06
Iter: 237 loss: 5.97772078e-06
Iter: 238 loss: 5.97778171e-06
Iter: 239 loss: 5.96231075e-06
Iter: 240 loss: 6.09968492e-06
Iter: 241 loss: 5.96160498e-06
Iter: 242 loss: 5.95450183e-06
Iter: 243 loss: 6.02672208e-06
Iter: 244 loss: 5.95421079e-06
Iter: 245 loss: 5.94563062e-06
Iter: 246 loss: 5.94127459e-06
Iter: 247 loss: 5.93736968e-06
Iter: 248 loss: 5.92528841e-06
Iter: 249 loss: 5.92742572e-06
Iter: 250 loss: 5.91652133e-06
Iter: 251 loss: 5.9135823e-06
Iter: 252 loss: 5.91219123e-06
Iter: 253 loss: 5.90781974e-06
Iter: 254 loss: 5.90621585e-06
Iter: 255 loss: 5.90373656e-06
Iter: 256 loss: 5.89841693e-06
Iter: 257 loss: 5.908817e-06
Iter: 258 loss: 5.89608453e-06
Iter: 259 loss: 5.88936427e-06
Iter: 260 loss: 5.92820652e-06
Iter: 261 loss: 5.88849525e-06
Iter: 262 loss: 5.88477542e-06
Iter: 263 loss: 5.8752039e-06
Iter: 264 loss: 5.94245648e-06
Iter: 265 loss: 5.87302202e-06
Iter: 266 loss: 5.86516217e-06
Iter: 267 loss: 5.86397073e-06
Iter: 268 loss: 5.85743601e-06
Iter: 269 loss: 5.84849067e-06
Iter: 270 loss: 5.84814643e-06
Iter: 271 loss: 5.83835299e-06
Iter: 272 loss: 5.83313113e-06
Iter: 273 loss: 5.82863686e-06
Iter: 274 loss: 5.8167534e-06
Iter: 275 loss: 5.81679433e-06
Iter: 276 loss: 5.81079894e-06
Iter: 277 loss: 5.87312843e-06
Iter: 278 loss: 5.81066433e-06
Iter: 279 loss: 5.80466121e-06
Iter: 280 loss: 5.80634969e-06
Iter: 281 loss: 5.8003111e-06
Iter: 282 loss: 5.79624339e-06
Iter: 283 loss: 5.79762491e-06
Iter: 284 loss: 5.79334164e-06
Iter: 285 loss: 5.78879917e-06
Iter: 286 loss: 5.78884101e-06
Iter: 287 loss: 5.78400068e-06
Iter: 288 loss: 5.77365608e-06
Iter: 289 loss: 5.93906498e-06
Iter: 290 loss: 5.77325363e-06
Iter: 291 loss: 5.76835328e-06
Iter: 292 loss: 5.76769571e-06
Iter: 293 loss: 5.76267712e-06
Iter: 294 loss: 5.75356626e-06
Iter: 295 loss: 5.7536181e-06
Iter: 296 loss: 5.74285787e-06
Iter: 297 loss: 5.7565012e-06
Iter: 298 loss: 5.73723628e-06
Iter: 299 loss: 5.73180887e-06
Iter: 300 loss: 5.73007537e-06
Iter: 301 loss: 5.72726731e-06
Iter: 302 loss: 5.72003864e-06
Iter: 303 loss: 5.77679612e-06
Iter: 304 loss: 5.71865894e-06
Iter: 305 loss: 5.71120381e-06
Iter: 306 loss: 5.77120136e-06
Iter: 307 loss: 5.71070041e-06
Iter: 308 loss: 5.70689281e-06
Iter: 309 loss: 5.73414763e-06
Iter: 310 loss: 5.70655448e-06
Iter: 311 loss: 5.70222164e-06
Iter: 312 loss: 5.71468217e-06
Iter: 313 loss: 5.70082966e-06
Iter: 314 loss: 5.69549366e-06
Iter: 315 loss: 5.69201848e-06
Iter: 316 loss: 5.689918e-06
Iter: 317 loss: 5.68442374e-06
Iter: 318 loss: 5.69164285e-06
Iter: 319 loss: 5.68135147e-06
Iter: 320 loss: 5.67156e-06
Iter: 321 loss: 5.72018e-06
Iter: 322 loss: 5.67005e-06
Iter: 323 loss: 5.66439348e-06
Iter: 324 loss: 5.66705512e-06
Iter: 325 loss: 5.66062408e-06
Iter: 326 loss: 5.65542905e-06
Iter: 327 loss: 5.73742727e-06
Iter: 328 loss: 5.65550818e-06
Iter: 329 loss: 5.65211e-06
Iter: 330 loss: 5.64321908e-06
Iter: 331 loss: 5.71673172e-06
Iter: 332 loss: 5.64166839e-06
Iter: 333 loss: 5.64119546e-06
Iter: 334 loss: 5.6383883e-06
Iter: 335 loss: 5.63561434e-06
Iter: 336 loss: 5.63960293e-06
Iter: 337 loss: 5.6342451e-06
Iter: 338 loss: 5.63150479e-06
Iter: 339 loss: 5.62364494e-06
Iter: 340 loss: 5.65744676e-06
Iter: 341 loss: 5.62054856e-06
Iter: 342 loss: 5.61287925e-06
Iter: 343 loss: 5.61275647e-06
Iter: 344 loss: 5.60683566e-06
Iter: 345 loss: 5.64799348e-06
Iter: 346 loss: 5.60631634e-06
Iter: 347 loss: 5.60005083e-06
Iter: 348 loss: 5.60878834e-06
Iter: 349 loss: 5.59676255e-06
Iter: 350 loss: 5.59182081e-06
Iter: 351 loss: 5.59734508e-06
Iter: 352 loss: 5.58910642e-06
Iter: 353 loss: 5.58506554e-06
Iter: 354 loss: 5.63326921e-06
Iter: 355 loss: 5.58505781e-06
Iter: 356 loss: 5.58024749e-06
Iter: 357 loss: 5.57375461e-06
Iter: 358 loss: 5.57335125e-06
Iter: 359 loss: 5.57152543e-06
Iter: 360 loss: 5.57119802e-06
Iter: 361 loss: 5.56907571e-06
Iter: 362 loss: 5.56692e-06
Iter: 363 loss: 5.56667146e-06
Iter: 364 loss: 5.56258783e-06
Iter: 365 loss: 5.55450879e-06
Iter: 366 loss: 5.70596603e-06
Iter: 367 loss: 5.55440511e-06
Iter: 368 loss: 5.55866836e-06
Iter: 369 loss: 5.55097e-06
Iter: 370 loss: 5.54907638e-06
Iter: 371 loss: 5.54423195e-06
Iter: 372 loss: 5.59099135e-06
Iter: 373 loss: 5.54369308e-06
Iter: 374 loss: 5.53780046e-06
Iter: 375 loss: 5.54809185e-06
Iter: 376 loss: 5.53533664e-06
Iter: 377 loss: 5.53104428e-06
Iter: 378 loss: 5.5850187e-06
Iter: 379 loss: 5.5309888e-06
Iter: 380 loss: 5.52744859e-06
Iter: 381 loss: 5.5511955e-06
Iter: 382 loss: 5.52703113e-06
Iter: 383 loss: 5.52461916e-06
Iter: 384 loss: 5.52405436e-06
Iter: 385 loss: 5.52241636e-06
Iter: 386 loss: 5.51933954e-06
Iter: 387 loss: 5.5228893e-06
Iter: 388 loss: 5.5175924e-06
Iter: 389 loss: 5.51243329e-06
Iter: 390 loss: 5.54048e-06
Iter: 391 loss: 5.51171297e-06
Iter: 392 loss: 5.50807499e-06
Iter: 393 loss: 5.50459663e-06
Iter: 394 loss: 5.50383129e-06
Iter: 395 loss: 5.50116692e-06
Iter: 396 loss: 5.50077903e-06
Iter: 397 loss: 5.49840161e-06
Iter: 398 loss: 5.49198e-06
Iter: 399 loss: 5.52238544e-06
Iter: 400 loss: 5.4897514e-06
Iter: 401 loss: 5.48769412e-06
Iter: 402 loss: 5.48638809e-06
Iter: 403 loss: 5.48360276e-06
Iter: 404 loss: 5.49221477e-06
Iter: 405 loss: 5.48266325e-06
Iter: 406 loss: 5.48050639e-06
Iter: 407 loss: 5.47600439e-06
Iter: 408 loss: 5.55458428e-06
Iter: 409 loss: 5.47584204e-06
Iter: 410 loss: 5.47253e-06
Iter: 411 loss: 5.51093854e-06
Iter: 412 loss: 5.47241507e-06
Iter: 413 loss: 5.46937099e-06
Iter: 414 loss: 5.4833481e-06
Iter: 415 loss: 5.46871161e-06
Iter: 416 loss: 5.46491265e-06
Iter: 417 loss: 5.46490264e-06
Iter: 418 loss: 5.46202773e-06
Iter: 419 loss: 5.45820239e-06
Iter: 420 loss: 5.46940146e-06
Iter: 421 loss: 5.45703642e-06
Iter: 422 loss: 5.45399598e-06
Iter: 423 loss: 5.49824381e-06
Iter: 424 loss: 5.45400781e-06
Iter: 425 loss: 5.4508605e-06
Iter: 426 loss: 5.44488148e-06
Iter: 427 loss: 5.5590267e-06
Iter: 428 loss: 5.44477734e-06
Iter: 429 loss: 5.44256045e-06
Iter: 430 loss: 5.44227669e-06
Iter: 431 loss: 5.4397824e-06
Iter: 432 loss: 5.43693113e-06
Iter: 433 loss: 5.43674605e-06
Iter: 434 loss: 5.43305487e-06
Iter: 435 loss: 5.43453916e-06
Iter: 436 loss: 5.43059923e-06
Iter: 437 loss: 5.42925591e-06
Iter: 438 loss: 5.42837597e-06
Iter: 439 loss: 5.42663747e-06
Iter: 440 loss: 5.42203816e-06
Iter: 441 loss: 5.45086277e-06
Iter: 442 loss: 5.42077669e-06
Iter: 443 loss: 5.4155953e-06
Iter: 444 loss: 5.44401519e-06
Iter: 445 loss: 5.41482859e-06
Iter: 446 loss: 5.4113043e-06
Iter: 447 loss: 5.41120198e-06
Iter: 448 loss: 5.40794e-06
Iter: 449 loss: 5.409187e-06
Iter: 450 loss: 5.40568817e-06
Iter: 451 loss: 5.40259316e-06
Iter: 452 loss: 5.41161216e-06
Iter: 453 loss: 5.40166957e-06
Iter: 454 loss: 5.39958819e-06
Iter: 455 loss: 5.41617919e-06
Iter: 456 loss: 5.39933581e-06
Iter: 457 loss: 5.39671419e-06
Iter: 458 loss: 5.39546863e-06
Iter: 459 loss: 5.39411121e-06
Iter: 460 loss: 5.39156372e-06
Iter: 461 loss: 5.4003076e-06
Iter: 462 loss: 5.39089615e-06
Iter: 463 loss: 5.38781796e-06
Iter: 464 loss: 5.39933217e-06
Iter: 465 loss: 5.38709537e-06
Iter: 466 loss: 5.38453241e-06
Iter: 467 loss: 5.38002132e-06
Iter: 468 loss: 5.38006361e-06
Iter: 469 loss: 5.37835695e-06
Iter: 470 loss: 5.37791948e-06
Iter: 471 loss: 5.37538699e-06
Iter: 472 loss: 5.36954667e-06
Iter: 473 loss: 5.44397108e-06
Iter: 474 loss: 5.36911421e-06
Iter: 475 loss: 5.36419066e-06
Iter: 476 loss: 5.39370376e-06
Iter: 477 loss: 5.3636868e-06
Iter: 478 loss: 5.36107473e-06
Iter: 479 loss: 5.38427048e-06
Iter: 480 loss: 5.36095376e-06
Iter: 481 loss: 5.35809795e-06
Iter: 482 loss: 5.36526568e-06
Iter: 483 loss: 5.35710114e-06
Iter: 484 loss: 5.35503204e-06
Iter: 485 loss: 5.35647678e-06
Iter: 486 loss: 5.35376466e-06
Iter: 487 loss: 5.35118306e-06
Iter: 488 loss: 5.35694744e-06
Iter: 489 loss: 5.35025e-06
Iter: 490 loss: 5.34664377e-06
Iter: 491 loss: 5.36561402e-06
Iter: 492 loss: 5.34610899e-06
Iter: 493 loss: 5.34406081e-06
Iter: 494 loss: 5.34152696e-06
Iter: 495 loss: 5.34143237e-06
Iter: 496 loss: 5.33787897e-06
Iter: 497 loss: 5.38270569e-06
Iter: 498 loss: 5.33779e-06
Iter: 499 loss: 5.33534967e-06
Iter: 500 loss: 5.33157254e-06
Iter: 501 loss: 5.33158345e-06
Iter: 502 loss: 5.3287049e-06
Iter: 503 loss: 5.35003e-06
Iter: 504 loss: 5.32845843e-06
Iter: 505 loss: 5.32495233e-06
Iter: 506 loss: 5.33239199e-06
Iter: 507 loss: 5.32361491e-06
Iter: 508 loss: 5.32133527e-06
Iter: 509 loss: 5.32099784e-06
Iter: 510 loss: 5.31949581e-06
Iter: 511 loss: 5.3167937e-06
Iter: 512 loss: 5.32030026e-06
Iter: 513 loss: 5.31532169e-06
Iter: 514 loss: 5.31105798e-06
Iter: 515 loss: 5.34285164e-06
Iter: 516 loss: 5.3105523e-06
Iter: 517 loss: 5.30797342e-06
Iter: 518 loss: 5.30740908e-06
Iter: 519 loss: 5.3056724e-06
Iter: 520 loss: 5.30227408e-06
Iter: 521 loss: 5.30950456e-06
Iter: 522 loss: 5.30092802e-06
Iter: 523 loss: 5.29746239e-06
Iter: 524 loss: 5.34818082e-06
Iter: 525 loss: 5.29749695e-06
Iter: 526 loss: 5.29542558e-06
Iter: 527 loss: 5.29301178e-06
Iter: 528 loss: 5.29277349e-06
Iter: 529 loss: 5.29034605e-06
Iter: 530 loss: 5.3256681e-06
Iter: 531 loss: 5.29037243e-06
Iter: 532 loss: 5.28813825e-06
Iter: 533 loss: 5.28460623e-06
Iter: 534 loss: 5.28470491e-06
Iter: 535 loss: 5.2817627e-06
Iter: 536 loss: 5.28884721e-06
Iter: 537 loss: 5.28064675e-06
Iter: 538 loss: 5.27771317e-06
Iter: 539 loss: 5.32175545e-06
Iter: 540 loss: 5.27779594e-06
Iter: 541 loss: 5.27574139e-06
Iter: 542 loss: 5.27173779e-06
Iter: 543 loss: 5.33985121e-06
Iter: 544 loss: 5.27163e-06
Iter: 545 loss: 5.26717258e-06
Iter: 546 loss: 5.2683863e-06
Iter: 547 loss: 5.26384065e-06
Iter: 548 loss: 5.26449094e-06
Iter: 549 loss: 5.26121767e-06
Iter: 550 loss: 5.2601e-06
Iter: 551 loss: 5.25843552e-06
Iter: 552 loss: 5.25832911e-06
Iter: 553 loss: 5.2559185e-06
Iter: 554 loss: 5.25557607e-06
Iter: 555 loss: 5.25388805e-06
Iter: 556 loss: 5.25158794e-06
Iter: 557 loss: 5.25152927e-06
Iter: 558 loss: 5.24992856e-06
Iter: 559 loss: 5.24840561e-06
Iter: 560 loss: 5.24802726e-06
Iter: 561 loss: 5.24569805e-06
Iter: 562 loss: 5.25412133e-06
Iter: 563 loss: 5.24511324e-06
Iter: 564 loss: 5.24183724e-06
Iter: 565 loss: 5.24432971e-06
Iter: 566 loss: 5.23985364e-06
Iter: 567 loss: 5.23725612e-06
Iter: 568 loss: 5.23232347e-06
Iter: 569 loss: 5.3455683e-06
Iter: 570 loss: 5.23227163e-06
Iter: 571 loss: 5.22891469e-06
Iter: 572 loss: 5.22807659e-06
Iter: 573 loss: 5.22512801e-06
Iter: 574 loss: 5.22204391e-06
Iter: 575 loss: 5.22150413e-06
Iter: 576 loss: 5.21814945e-06
Iter: 577 loss: 5.21800757e-06
Iter: 578 loss: 5.21557376e-06
Iter: 579 loss: 5.21652419e-06
Iter: 580 loss: 5.21412039e-06
Iter: 581 loss: 5.2129044e-06
Iter: 582 loss: 5.21067159e-06
Iter: 583 loss: 5.21077e-06
Iter: 584 loss: 5.20806179e-06
Iter: 585 loss: 5.21394077e-06
Iter: 586 loss: 5.20710728e-06
Iter: 587 loss: 5.20531e-06
Iter: 588 loss: 5.20517551e-06
Iter: 589 loss: 5.20329377e-06
Iter: 590 loss: 5.19985451e-06
Iter: 591 loss: 5.28328837e-06
Iter: 592 loss: 5.19993773e-06
Iter: 593 loss: 5.19605283e-06
Iter: 594 loss: 5.21081e-06
Iter: 595 loss: 5.19500054e-06
Iter: 596 loss: 5.19155037e-06
Iter: 597 loss: 5.22418077e-06
Iter: 598 loss: 5.19133846e-06
Iter: 599 loss: 5.18946763e-06
Iter: 600 loss: 5.18443812e-06
Iter: 601 loss: 5.21742186e-06
Iter: 602 loss: 5.18313209e-06
Iter: 603 loss: 5.18375828e-06
Iter: 604 loss: 5.18067191e-06
Iter: 605 loss: 5.17875833e-06
Iter: 606 loss: 5.17867784e-06
Iter: 607 loss: 5.17727403e-06
Iter: 608 loss: 5.17546187e-06
Iter: 609 loss: 5.17267381e-06
Iter: 610 loss: 5.17261e-06
Iter: 611 loss: 5.17068747e-06
Iter: 612 loss: 5.17055378e-06
Iter: 613 loss: 5.16824275e-06
Iter: 614 loss: 5.16769524e-06
Iter: 615 loss: 5.16601085e-06
Iter: 616 loss: 5.16328782e-06
Iter: 617 loss: 5.16688033e-06
Iter: 618 loss: 5.16193131e-06
Iter: 619 loss: 5.15945158e-06
Iter: 620 loss: 5.18942034e-06
Iter: 621 loss: 5.15934e-06
Iter: 622 loss: 5.15678448e-06
Iter: 623 loss: 5.15699912e-06
Iter: 624 loss: 5.15474312e-06
Iter: 625 loss: 5.15185957e-06
Iter: 626 loss: 5.15582724e-06
Iter: 627 loss: 5.1502966e-06
Iter: 628 loss: 5.14811836e-06
Iter: 629 loss: 5.14806607e-06
Iter: 630 loss: 5.14653357e-06
Iter: 631 loss: 5.14279418e-06
Iter: 632 loss: 5.18085108e-06
Iter: 633 loss: 5.14238673e-06
Iter: 634 loss: 5.13997657e-06
Iter: 635 loss: 5.13998521e-06
Iter: 636 loss: 5.13760506e-06
Iter: 637 loss: 5.14291605e-06
Iter: 638 loss: 5.13684972e-06
Iter: 639 loss: 5.13436726e-06
Iter: 640 loss: 5.12865108e-06
Iter: 641 loss: 5.18981597e-06
Iter: 642 loss: 5.12818178e-06
Iter: 643 loss: 5.12532188e-06
Iter: 644 loss: 5.12494353e-06
Iter: 645 loss: 5.12198858e-06
Iter: 646 loss: 5.14033536e-06
Iter: 647 loss: 5.12167389e-06
Iter: 648 loss: 5.11977851e-06
Iter: 649 loss: 5.1172683e-06
Iter: 650 loss: 5.11712369e-06
Iter: 651 loss: 5.11446478e-06
Iter: 652 loss: 5.14396561e-06
Iter: 653 loss: 5.114548e-06
Iter: 654 loss: 5.11202234e-06
Iter: 655 loss: 5.11944381e-06
Iter: 656 loss: 5.1113e-06
Iter: 657 loss: 5.1091497e-06
Iter: 658 loss: 5.1093707e-06
Iter: 659 loss: 5.10763857e-06
Iter: 660 loss: 5.10550672e-06
Iter: 661 loss: 5.12922679e-06
Iter: 662 loss: 5.10549035e-06
Iter: 663 loss: 5.10336849e-06
Iter: 664 loss: 5.0987237e-06
Iter: 665 loss: 5.16401906e-06
Iter: 666 loss: 5.0983117e-06
Iter: 667 loss: 5.09424717e-06
Iter: 668 loss: 5.12009228e-06
Iter: 669 loss: 5.09378788e-06
Iter: 670 loss: 5.09051324e-06
Iter: 671 loss: 5.13845771e-06
Iter: 672 loss: 5.09055189e-06
Iter: 673 loss: 5.08849143e-06
Iter: 674 loss: 5.08439325e-06
Iter: 675 loss: 5.16545879e-06
Iter: 676 loss: 5.08441917e-06
Iter: 677 loss: 5.08179437e-06
Iter: 678 loss: 5.09076062e-06
Iter: 679 loss: 5.08093626e-06
Iter: 680 loss: 5.07928416e-06
Iter: 681 loss: 5.07908089e-06
Iter: 682 loss: 5.07777531e-06
Iter: 683 loss: 5.07564528e-06
Iter: 684 loss: 5.07567484e-06
Iter: 685 loss: 5.07327195e-06
Iter: 686 loss: 5.08004223e-06
Iter: 687 loss: 5.07275763e-06
Iter: 688 loss: 5.06967444e-06
Iter: 689 loss: 5.08476705e-06
Iter: 690 loss: 5.06915239e-06
Iter: 691 loss: 5.06661536e-06
Iter: 692 loss: 5.06674041e-06
Iter: 693 loss: 5.06472861e-06
Iter: 694 loss: 5.06231936e-06
Iter: 695 loss: 5.0804374e-06
Iter: 696 loss: 5.06209199e-06
Iter: 697 loss: 5.05931666e-06
Iter: 698 loss: 5.05715525e-06
Iter: 699 loss: 5.05627486e-06
Iter: 700 loss: 5.05300341e-06
Iter: 701 loss: 5.05824482e-06
Iter: 702 loss: 5.0515564e-06
Iter: 703 loss: 5.05005937e-06
Iter: 704 loss: 5.04978652e-06
Iter: 705 loss: 5.04837863e-06
Iter: 706 loss: 5.04627678e-06
Iter: 707 loss: 5.04617674e-06
Iter: 708 loss: 5.04436684e-06
Iter: 709 loss: 5.04236141e-06
Iter: 710 loss: 5.0419867e-06
Iter: 711 loss: 5.03975571e-06
Iter: 712 loss: 5.03960837e-06
Iter: 713 loss: 5.03735646e-06
Iter: 714 loss: 5.03694264e-06
Iter: 715 loss: 5.03534875e-06
Iter: 716 loss: 5.03276078e-06
Iter: 717 loss: 5.03039882e-06
Iter: 718 loss: 5.02968396e-06
Iter: 719 loss: 5.02611147e-06
Iter: 720 loss: 5.02597231e-06
Iter: 721 loss: 5.02390731e-06
Iter: 722 loss: 5.0248027e-06
Iter: 723 loss: 5.02247576e-06
Iter: 724 loss: 5.02032e-06
Iter: 725 loss: 5.02460443e-06
Iter: 726 loss: 5.01948398e-06
Iter: 727 loss: 5.01654e-06
Iter: 728 loss: 5.02660214e-06
Iter: 729 loss: 5.01574596e-06
Iter: 730 loss: 5.01436944e-06
Iter: 731 loss: 5.01456179e-06
Iter: 732 loss: 5.013193e-06
Iter: 733 loss: 5.01182149e-06
Iter: 734 loss: 5.02729836e-06
Iter: 735 loss: 5.01188606e-06
Iter: 736 loss: 5.01025806e-06
Iter: 737 loss: 5.00893429e-06
Iter: 738 loss: 5.00851365e-06
Iter: 739 loss: 5.00621854e-06
Iter: 740 loss: 5.0024546e-06
Iter: 741 loss: 5.09440588e-06
Iter: 742 loss: 5.00241094e-06
Iter: 743 loss: 5.00024e-06
Iter: 744 loss: 4.99982161e-06
Iter: 745 loss: 4.99749694e-06
Iter: 746 loss: 5.00394708e-06
Iter: 747 loss: 4.99662383e-06
Iter: 748 loss: 4.99484304e-06
Iter: 749 loss: 4.9919372e-06
Iter: 750 loss: 4.99194812e-06
Iter: 751 loss: 4.99072758e-06
Iter: 752 loss: 4.9898922e-06
Iter: 753 loss: 4.98890131e-06
Iter: 754 loss: 4.98839927e-06
Iter: 755 loss: 4.98777626e-06
Iter: 756 loss: 4.98618738e-06
Iter: 757 loss: 4.98817099e-06
Iter: 758 loss: 4.98531426e-06
Iter: 759 loss: 4.98347208e-06
Iter: 760 loss: 4.99896078e-06
Iter: 761 loss: 4.98332201e-06
Iter: 762 loss: 4.98218105e-06
Iter: 763 loss: 4.98047712e-06
Iter: 764 loss: 4.98031841e-06
Iter: 765 loss: 4.97786823e-06
Iter: 766 loss: 4.98563622e-06
Iter: 767 loss: 4.97715246e-06
Iter: 768 loss: 4.97391193e-06
Iter: 769 loss: 4.99107136e-06
Iter: 770 loss: 4.97347673e-06
Iter: 771 loss: 4.97196288e-06
Iter: 772 loss: 4.96895973e-06
Iter: 773 loss: 5.02606781e-06
Iter: 774 loss: 4.96891153e-06
Iter: 775 loss: 4.96604844e-06
Iter: 776 loss: 4.99723637e-06
Iter: 777 loss: 4.96593793e-06
Iter: 778 loss: 4.96402208e-06
Iter: 779 loss: 4.99353882e-06
Iter: 780 loss: 4.96412122e-06
Iter: 781 loss: 4.96293251e-06
Iter: 782 loss: 4.96064786e-06
Iter: 783 loss: 4.99952557e-06
Iter: 784 loss: 4.96058328e-06
Iter: 785 loss: 4.96005941e-06
Iter: 786 loss: 4.95949826e-06
Iter: 787 loss: 4.95861468e-06
Iter: 788 loss: 4.95776248e-06
Iter: 789 loss: 4.95754557e-06
Iter: 790 loss: 4.95568156e-06
Iter: 791 loss: 4.95615268e-06
Iter: 792 loss: 4.9544e-06
Iter: 793 loss: 4.95220957e-06
Iter: 794 loss: 4.98014924e-06
Iter: 795 loss: 4.95222321e-06
Iter: 796 loss: 4.95081213e-06
Iter: 797 loss: 4.94889036e-06
Iter: 798 loss: 4.9487353e-06
Iter: 799 loss: 4.94608412e-06
Iter: 800 loss: 4.95145e-06
Iter: 801 loss: 4.94493361e-06
Iter: 802 loss: 4.94283813e-06
Iter: 803 loss: 4.94289816e-06
Iter: 804 loss: 4.94188043e-06
Iter: 805 loss: 4.93938296e-06
Iter: 806 loss: 4.96045095e-06
Iter: 807 loss: 4.93908283e-06
Iter: 808 loss: 4.93641937e-06
Iter: 809 loss: 4.95406675e-06
Iter: 810 loss: 4.93613152e-06
Iter: 811 loss: 4.93534935e-06
Iter: 812 loss: 4.93506468e-06
Iter: 813 loss: 4.93416974e-06
Iter: 814 loss: 4.9318478e-06
Iter: 815 loss: 4.94813776e-06
Iter: 816 loss: 4.93134849e-06
Iter: 817 loss: 4.92978324e-06
Iter: 818 loss: 4.92970503e-06
Iter: 819 loss: 4.92782056e-06
Iter: 820 loss: 4.92847312e-06
Iter: 821 loss: 4.92655545e-06
Iter: 822 loss: 4.92441177e-06
Iter: 823 loss: 4.92537856e-06
Iter: 824 loss: 4.92295294e-06
Iter: 825 loss: 4.92104573e-06
Iter: 826 loss: 4.92107574e-06
Iter: 827 loss: 4.91958235e-06
Iter: 828 loss: 4.91811443e-06
Iter: 829 loss: 4.91786295e-06
Iter: 830 loss: 4.91578703e-06
Iter: 831 loss: 4.92096115e-06
Iter: 832 loss: 4.91489482e-06
Iter: 833 loss: 4.91406e-06
Iter: 834 loss: 4.91377614e-06
Iter: 835 loss: 4.91311675e-06
Iter: 836 loss: 4.91111223e-06
Iter: 837 loss: 4.9197597e-06
Iter: 838 loss: 4.91018181e-06
Iter: 839 loss: 4.90734419e-06
Iter: 840 loss: 4.92081926e-06
Iter: 841 loss: 4.90686489e-06
Iter: 842 loss: 4.90573257e-06
Iter: 843 loss: 4.90545426e-06
Iter: 844 loss: 4.90407274e-06
Iter: 845 loss: 4.90077173e-06
Iter: 846 loss: 4.93238213e-06
Iter: 847 loss: 4.9002565e-06
Iter: 848 loss: 4.89792546e-06
Iter: 849 loss: 4.89793956e-06
Iter: 850 loss: 4.89571175e-06
Iter: 851 loss: 4.90513321e-06
Iter: 852 loss: 4.89528e-06
Iter: 853 loss: 4.8937568e-06
Iter: 854 loss: 4.89325248e-06
Iter: 855 loss: 4.89229205e-06
Iter: 856 loss: 4.89090553e-06
Iter: 857 loss: 4.89090598e-06
Iter: 858 loss: 4.8898e-06
Iter: 859 loss: 4.88953947e-06
Iter: 860 loss: 4.88863543e-06
Iter: 861 loss: 4.8872339e-06
Iter: 862 loss: 4.88727e-06
Iter: 863 loss: 4.8859938e-06
Iter: 864 loss: 4.88495698e-06
Iter: 865 loss: 4.88477326e-06
Iter: 866 loss: 4.88365686e-06
Iter: 867 loss: 4.88088244e-06
Iter: 868 loss: 4.90840557e-06
Iter: 869 loss: 4.88057412e-06
Iter: 870 loss: 4.87744092e-06
Iter: 871 loss: 4.88504611e-06
Iter: 872 loss: 4.87615307e-06
Iter: 873 loss: 4.87471516e-06
Iter: 874 loss: 4.8745942e-06
Iter: 875 loss: 4.87280477e-06
Iter: 876 loss: 4.87101488e-06
Iter: 877 loss: 4.87056332e-06
Iter: 878 loss: 4.86887711e-06
Iter: 879 loss: 4.88025125e-06
Iter: 880 loss: 4.86871886e-06
Iter: 881 loss: 4.86761564e-06
Iter: 882 loss: 4.88251317e-06
Iter: 883 loss: 4.86753834e-06
Iter: 884 loss: 4.86675162e-06
Iter: 885 loss: 4.86550471e-06
Iter: 886 loss: 4.89749618e-06
Iter: 887 loss: 4.86543649e-06
Iter: 888 loss: 4.86408408e-06
Iter: 889 loss: 4.88185924e-06
Iter: 890 loss: 4.86420686e-06
Iter: 891 loss: 4.86297358e-06
Iter: 892 loss: 4.86408408e-06
Iter: 893 loss: 4.86224098e-06
Iter: 894 loss: 4.86082e-06
Iter: 895 loss: 4.85861392e-06
Iter: 896 loss: 4.85855e-06
Iter: 897 loss: 4.85713099e-06
Iter: 898 loss: 4.85698092e-06
Iter: 899 loss: 4.85522924e-06
Iter: 900 loss: 4.85305281e-06
Iter: 901 loss: 4.85293276e-06
Iter: 902 loss: 4.8504603e-06
Iter: 903 loss: 4.85316104e-06
Iter: 904 loss: 4.8492393e-06
Iter: 905 loss: 4.84753491e-06
Iter: 906 loss: 4.86561657e-06
Iter: 907 loss: 4.84754082e-06
Iter: 908 loss: 4.84564589e-06
Iter: 909 loss: 4.85170585e-06
Iter: 910 loss: 4.84520569e-06
Iter: 911 loss: 4.84443444e-06
Iter: 912 loss: 4.8447414e-06
Iter: 913 loss: 4.8438942e-06
Iter: 914 loss: 4.84274096e-06
Iter: 915 loss: 4.85317742e-06
Iter: 916 loss: 4.84274278e-06
Iter: 917 loss: 4.84177326e-06
Iter: 918 loss: 4.83962822e-06
Iter: 919 loss: 4.8732868e-06
Iter: 920 loss: 4.83958274e-06
Iter: 921 loss: 4.83775966e-06
Iter: 922 loss: 4.86521549e-06
Iter: 923 loss: 4.83788881e-06
Iter: 924 loss: 4.83634949e-06
Iter: 925 loss: 4.83776375e-06
Iter: 926 loss: 4.83533677e-06
Iter: 927 loss: 4.83323e-06
Iter: 928 loss: 4.83131498e-06
Iter: 929 loss: 4.83079793e-06
Iter: 930 loss: 4.82898668e-06
Iter: 931 loss: 4.82905398e-06
Iter: 932 loss: 4.82716951e-06
Iter: 933 loss: 4.82742053e-06
Iter: 934 loss: 4.82573796e-06
Iter: 935 loss: 4.82399219e-06
Iter: 936 loss: 4.82383894e-06
Iter: 937 loss: 4.82239648e-06
Iter: 938 loss: 4.82054793e-06
Iter: 939 loss: 4.83009808e-06
Iter: 940 loss: 4.82019823e-06
Iter: 941 loss: 4.81842608e-06
Iter: 942 loss: 4.83963231e-06
Iter: 943 loss: 4.81841289e-06
Iter: 944 loss: 4.81749157e-06
Iter: 945 loss: 4.81581537e-06
Iter: 946 loss: 4.84706834e-06
Iter: 947 loss: 4.81579e-06
Iter: 948 loss: 4.81402e-06
Iter: 949 loss: 4.8139982e-06
Iter: 950 loss: 4.8124557e-06
Iter: 951 loss: 4.81094548e-06
Iter: 952 loss: 4.81063216e-06
Iter: 953 loss: 4.80919789e-06
Iter: 954 loss: 4.82140149e-06
Iter: 955 loss: 4.8091797e-06
Iter: 956 loss: 4.80777499e-06
Iter: 957 loss: 4.81043e-06
Iter: 958 loss: 4.80707422e-06
Iter: 959 loss: 4.80538802e-06
Iter: 960 loss: 4.8050124e-06
Iter: 961 loss: 4.80404788e-06
Iter: 962 loss: 4.80263407e-06
Iter: 963 loss: 4.81202323e-06
Iter: 964 loss: 4.80242716e-06
Iter: 965 loss: 4.80057588e-06
Iter: 966 loss: 4.8049078e-06
Iter: 967 loss: 4.79985283e-06
Iter: 968 loss: 4.79832488e-06
Iter: 969 loss: 4.79595928e-06
Iter: 970 loss: 4.79592836e-06
Iter: 971 loss: 4.7932e-06
Iter: 972 loss: 4.80272456e-06
Iter: 973 loss: 4.79255323e-06
Iter: 974 loss: 4.79099435e-06
Iter: 975 loss: 4.79077062e-06
Iter: 976 loss: 4.78985385e-06
Iter: 977 loss: 4.78797938e-06
Iter: 978 loss: 4.82479027e-06
Iter: 979 loss: 4.7879812e-06
Iter: 980 loss: 4.78669153e-06
Iter: 981 loss: 4.78659604e-06
Iter: 982 loss: 4.7853182e-06
Iter: 983 loss: 4.78457605e-06
Iter: 984 loss: 4.78404263e-06
Iter: 985 loss: 4.78287529e-06
Iter: 986 loss: 4.78613401e-06
Iter: 987 loss: 4.7823828e-06
Iter: 988 loss: 4.78071934e-06
Iter: 989 loss: 4.78647053e-06
Iter: 990 loss: 4.78032916e-06
Iter: 991 loss: 4.77846697e-06
Iter: 992 loss: 4.7775643e-06
Iter: 993 loss: 4.77678714e-06
Iter: 994 loss: 4.77461253e-06
Iter: 995 loss: 4.77969024e-06
Iter: 996 loss: 4.77386266e-06
Iter: 997 loss: 4.77107e-06
Iter: 998 loss: 4.79348273e-06
Iter: 999 loss: 4.77103549e-06
Iter: 1000 loss: 4.76921832e-06
Iter: 1001 loss: 4.76686e-06
Iter: 1002 loss: 4.76672085e-06
Iter: 1003 loss: 4.76451532e-06
Iter: 1004 loss: 4.76724472e-06
Iter: 1005 loss: 4.7632484e-06
Iter: 1006 loss: 4.76276546e-06
Iter: 1007 loss: 4.76192781e-06
Iter: 1008 loss: 4.76108926e-06
Iter: 1009 loss: 4.75977049e-06
Iter: 1010 loss: 4.75966772e-06
Iter: 1011 loss: 4.75871275e-06
Iter: 1012 loss: 4.7742933e-06
Iter: 1013 loss: 4.75867137e-06
Iter: 1014 loss: 4.75770867e-06
Iter: 1015 loss: 4.75715933e-06
Iter: 1016 loss: 4.75674642e-06
Iter: 1017 loss: 4.75532033e-06
Iter: 1018 loss: 4.75443903e-06
Iter: 1019 loss: 4.75388106e-06
Iter: 1020 loss: 4.75114848e-06
Iter: 1021 loss: 4.76571677e-06
Iter: 1022 loss: 4.75069191e-06
Iter: 1023 loss: 4.74833541e-06
Iter: 1024 loss: 4.75136312e-06
Iter: 1025 loss: 4.74710123e-06
Iter: 1026 loss: 4.74511035e-06
Iter: 1027 loss: 4.7460444e-06
Iter: 1028 loss: 4.74378521e-06
Iter: 1029 loss: 4.74181661e-06
Iter: 1030 loss: 4.7418207e-06
Iter: 1031 loss: 4.74031549e-06
Iter: 1032 loss: 4.73846467e-06
Iter: 1033 loss: 4.73826549e-06
Iter: 1034 loss: 4.73648697e-06
Iter: 1035 loss: 4.73617774e-06
Iter: 1036 loss: 4.73491718e-06
Iter: 1037 loss: 4.73485943e-06
Iter: 1038 loss: 4.73364162e-06
Iter: 1039 loss: 4.73267755e-06
Iter: 1040 loss: 4.73163891e-06
Iter: 1041 loss: 4.73153341e-06
Iter: 1042 loss: 4.72998636e-06
Iter: 1043 loss: 4.736964e-06
Iter: 1044 loss: 4.72977445e-06
Iter: 1045 loss: 4.72784268e-06
Iter: 1046 loss: 4.72904594e-06
Iter: 1047 loss: 4.7267049e-06
Iter: 1048 loss: 4.72484408e-06
Iter: 1049 loss: 4.72430565e-06
Iter: 1050 loss: 4.72335e-06
Iter: 1051 loss: 4.72095689e-06
Iter: 1052 loss: 4.75178877e-06
Iter: 1053 loss: 4.72090687e-06
Iter: 1054 loss: 4.71920157e-06
Iter: 1055 loss: 4.72002193e-06
Iter: 1056 loss: 4.7180647e-06
Iter: 1057 loss: 4.71625208e-06
Iter: 1058 loss: 4.71599833e-06
Iter: 1059 loss: 4.71470685e-06
Iter: 1060 loss: 4.7127387e-06
Iter: 1061 loss: 4.71280964e-06
Iter: 1062 loss: 4.71103249e-06
Iter: 1063 loss: 4.71072872e-06
Iter: 1064 loss: 4.70960322e-06
Iter: 1065 loss: 4.70774648e-06
Iter: 1066 loss: 4.70392933e-06
Iter: 1067 loss: 4.76148261e-06
Iter: 1068 loss: 4.70378291e-06
Iter: 1069 loss: 4.70318264e-06
Iter: 1070 loss: 4.70176474e-06
Iter: 1071 loss: 4.69997167e-06
Iter: 1072 loss: 4.70282339e-06
Iter: 1073 loss: 4.6989253e-06
Iter: 1074 loss: 4.69735915e-06
Iter: 1075 loss: 4.69834958e-06
Iter: 1076 loss: 4.69644874e-06
Iter: 1077 loss: 4.69407132e-06
Iter: 1078 loss: 4.71115163e-06
Iter: 1079 loss: 4.69388669e-06
Iter: 1080 loss: 4.69292081e-06
Iter: 1081 loss: 4.69147653e-06
Iter: 1082 loss: 4.69143697e-06
Iter: 1083 loss: 4.68985627e-06
Iter: 1084 loss: 4.71258863e-06
Iter: 1085 loss: 4.6897967e-06
Iter: 1086 loss: 4.68858934e-06
Iter: 1087 loss: 4.68867711e-06
Iter: 1088 loss: 4.68764392e-06
Iter: 1089 loss: 4.68595363e-06
Iter: 1090 loss: 4.68529288e-06
Iter: 1091 loss: 4.6844184e-06
Iter: 1092 loss: 4.68212829e-06
Iter: 1093 loss: 4.70583973e-06
Iter: 1094 loss: 4.68203552e-06
Iter: 1095 loss: 4.67991731e-06
Iter: 1096 loss: 4.68289181e-06
Iter: 1097 loss: 4.67878453e-06
Iter: 1098 loss: 4.67698374e-06
Iter: 1099 loss: 4.67338123e-06
Iter: 1100 loss: 4.74114586e-06
Iter: 1101 loss: 4.67324071e-06
Iter: 1102 loss: 4.67202e-06
Iter: 1103 loss: 4.67139444e-06
Iter: 1104 loss: 4.66981146e-06
Iter: 1105 loss: 4.67565178e-06
Iter: 1106 loss: 4.66940674e-06
Iter: 1107 loss: 4.66811707e-06
Iter: 1108 loss: 4.66745769e-06
Iter: 1109 loss: 4.66682741e-06
Iter: 1110 loss: 4.66536312e-06
Iter: 1111 loss: 4.66523306e-06
Iter: 1112 loss: 4.66463825e-06
Iter: 1113 loss: 4.66281926e-06
Iter: 1114 loss: 4.67963264e-06
Iter: 1115 loss: 4.66258689e-06
Iter: 1116 loss: 4.66070105e-06
Iter: 1117 loss: 4.68729468e-06
Iter: 1118 loss: 4.66069559e-06
Iter: 1119 loss: 4.65903e-06
Iter: 1120 loss: 4.6592304e-06
Iter: 1121 loss: 4.65766e-06
Iter: 1122 loss: 4.65532457e-06
Iter: 1123 loss: 4.65552148e-06
Iter: 1124 loss: 4.65352787e-06
Iter: 1125 loss: 4.65054291e-06
Iter: 1126 loss: 4.66967e-06
Iter: 1127 loss: 4.65024e-06
Iter: 1128 loss: 4.64750246e-06
Iter: 1129 loss: 4.6628661e-06
Iter: 1130 loss: 4.64724963e-06
Iter: 1131 loss: 4.64578443e-06
Iter: 1132 loss: 4.6429268e-06
Iter: 1133 loss: 4.69403039e-06
Iter: 1134 loss: 4.6427881e-06
Iter: 1135 loss: 4.64053392e-06
Iter: 1136 loss: 4.67092241e-06
Iter: 1137 loss: 4.64053574e-06
Iter: 1138 loss: 4.63898641e-06
Iter: 1139 loss: 4.66238e-06
Iter: 1140 loss: 4.63900733e-06
Iter: 1141 loss: 4.63803144e-06
Iter: 1142 loss: 4.63627066e-06
Iter: 1143 loss: 4.63632477e-06
Iter: 1144 loss: 4.6350392e-06
Iter: 1145 loss: 4.6349378e-06
Iter: 1146 loss: 4.63404194e-06
Iter: 1147 loss: 4.63156357e-06
Iter: 1148 loss: 4.64553796e-06
Iter: 1149 loss: 4.63104425e-06
Iter: 1150 loss: 4.62799e-06
Iter: 1151 loss: 4.66039683e-06
Iter: 1152 loss: 4.62797743e-06
Iter: 1153 loss: 4.62532626e-06
Iter: 1154 loss: 4.63355764e-06
Iter: 1155 loss: 4.62456774e-06
Iter: 1156 loss: 4.62286243e-06
Iter: 1157 loss: 4.62483968e-06
Iter: 1158 loss: 4.62194703e-06
Iter: 1159 loss: 4.61988338e-06
Iter: 1160 loss: 4.62565276e-06
Iter: 1161 loss: 4.61929585e-06
Iter: 1162 loss: 4.61758736e-06
Iter: 1163 loss: 4.63935658e-06
Iter: 1164 loss: 4.6175619e-06
Iter: 1165 loss: 4.61660511e-06
Iter: 1166 loss: 4.61455966e-06
Iter: 1167 loss: 4.64047707e-06
Iter: 1168 loss: 4.61444051e-06
Iter: 1169 loss: 4.612265e-06
Iter: 1170 loss: 4.62464641e-06
Iter: 1171 loss: 4.6119726e-06
Iter: 1172 loss: 4.61088166e-06
Iter: 1173 loss: 4.61078344e-06
Iter: 1174 loss: 4.60967203e-06
Iter: 1175 loss: 4.60722e-06
Iter: 1176 loss: 4.6393925e-06
Iter: 1177 loss: 4.60709725e-06
Iter: 1178 loss: 4.60591809e-06
Iter: 1179 loss: 4.60559295e-06
Iter: 1180 loss: 4.60439242e-06
Iter: 1181 loss: 4.60138881e-06
Iter: 1182 loss: 4.62564913e-06
Iter: 1183 loss: 4.60078081e-06
Iter: 1184 loss: 4.59798412e-06
Iter: 1185 loss: 4.63276638e-06
Iter: 1186 loss: 4.59796411e-06
Iter: 1187 loss: 4.59594685e-06
Iter: 1188 loss: 4.61002628e-06
Iter: 1189 loss: 4.59576813e-06
Iter: 1190 loss: 4.59454213e-06
Iter: 1191 loss: 4.59323974e-06
Iter: 1192 loss: 4.59313e-06
Iter: 1193 loss: 4.59082094e-06
Iter: 1194 loss: 4.60056526e-06
Iter: 1195 loss: 4.59025887e-06
Iter: 1196 loss: 4.58883369e-06
Iter: 1197 loss: 4.58883778e-06
Iter: 1198 loss: 4.58792874e-06
Iter: 1199 loss: 4.58547174e-06
Iter: 1200 loss: 4.60281717e-06
Iter: 1201 loss: 4.58496561e-06
Iter: 1202 loss: 4.58168688e-06
Iter: 1203 loss: 4.58938894e-06
Iter: 1204 loss: 4.58064e-06
Iter: 1205 loss: 4.57933493e-06
Iter: 1206 loss: 4.57860415e-06
Iter: 1207 loss: 4.57687e-06
Iter: 1208 loss: 4.57435544e-06
Iter: 1209 loss: 4.57437181e-06
Iter: 1210 loss: 4.573365e-06
Iter: 1211 loss: 4.57310125e-06
Iter: 1212 loss: 4.57196893e-06
Iter: 1213 loss: 4.56932321e-06
Iter: 1214 loss: 4.60485899e-06
Iter: 1215 loss: 4.5691977e-06
Iter: 1216 loss: 4.56722728e-06
Iter: 1217 loss: 4.58699651e-06
Iter: 1218 loss: 4.56719044e-06
Iter: 1219 loss: 4.56577527e-06
Iter: 1220 loss: 4.57716396e-06
Iter: 1221 loss: 4.56569796e-06
Iter: 1222 loss: 4.56464204e-06
Iter: 1223 loss: 4.56246607e-06
Iter: 1224 loss: 4.59959074e-06
Iter: 1225 loss: 4.56244061e-06
Iter: 1226 loss: 4.55977e-06
Iter: 1227 loss: 4.58032764e-06
Iter: 1228 loss: 4.55948611e-06
Iter: 1229 loss: 4.55791042e-06
Iter: 1230 loss: 4.57369197e-06
Iter: 1231 loss: 4.55799909e-06
Iter: 1232 loss: 4.556573e-06
Iter: 1233 loss: 4.55357576e-06
Iter: 1234 loss: 4.59795956e-06
Iter: 1235 loss: 4.55341706e-06
Iter: 1236 loss: 4.55051077e-06
Iter: 1237 loss: 4.55962481e-06
Iter: 1238 loss: 4.5496563e-06
Iter: 1239 loss: 4.54798646e-06
Iter: 1240 loss: 4.54802284e-06
Iter: 1241 loss: 4.54607925e-06
Iter: 1242 loss: 4.54641486e-06
Iter: 1243 loss: 4.54467499e-06
Iter: 1244 loss: 4.5436318e-06
Iter: 1245 loss: 4.5575307e-06
Iter: 1246 loss: 4.54364363e-06
Iter: 1247 loss: 4.54237579e-06
Iter: 1248 loss: 4.5405277e-06
Iter: 1249 loss: 4.54062592e-06
Iter: 1250 loss: 4.53875964e-06
Iter: 1251 loss: 4.54266592e-06
Iter: 1252 loss: 4.53813072e-06
Iter: 1253 loss: 4.53613211e-06
Iter: 1254 loss: 4.55678401e-06
Iter: 1255 loss: 4.53610028e-06
Iter: 1256 loss: 4.53454049e-06
Iter: 1257 loss: 4.53202938e-06
Iter: 1258 loss: 4.53200118e-06
Iter: 1259 loss: 4.53012717e-06
Iter: 1260 loss: 4.53008124e-06
Iter: 1261 loss: 4.52910672e-06
Iter: 1262 loss: 4.53518669e-06
Iter: 1263 loss: 4.52895574e-06
Iter: 1264 loss: 4.52775294e-06
Iter: 1265 loss: 4.52622498e-06
Iter: 1266 loss: 4.52616041e-06
Iter: 1267 loss: 4.52466929e-06
Iter: 1268 loss: 4.52648237e-06
Iter: 1269 loss: 4.52380846e-06
Iter: 1270 loss: 4.52223412e-06
Iter: 1271 loss: 4.53197526e-06
Iter: 1272 loss: 4.52205677e-06
Iter: 1273 loss: 4.52001768e-06
Iter: 1274 loss: 4.52907261e-06
Iter: 1275 loss: 4.51982214e-06
Iter: 1276 loss: 4.51882488e-06
Iter: 1277 loss: 4.51951837e-06
Iter: 1278 loss: 4.51816322e-06
Iter: 1279 loss: 4.51614051e-06
Iter: 1280 loss: 4.51659389e-06
Iter: 1281 loss: 4.51461801e-06
Iter: 1282 loss: 4.51281267e-06
Iter: 1283 loss: 4.51305095e-06
Iter: 1284 loss: 4.51144206e-06
Iter: 1285 loss: 4.51007691e-06
Iter: 1286 loss: 4.50993184e-06
Iter: 1287 loss: 4.50882044e-06
Iter: 1288 loss: 4.50759489e-06
Iter: 1289 loss: 4.50729e-06
Iter: 1290 loss: 4.50614561e-06
Iter: 1291 loss: 4.51373717e-06
Iter: 1292 loss: 4.5059187e-06
Iter: 1293 loss: 4.50468542e-06
Iter: 1294 loss: 4.5101342e-06
Iter: 1295 loss: 4.50445e-06
Iter: 1296 loss: 4.50299376e-06
Iter: 1297 loss: 4.50430525e-06
Iter: 1298 loss: 4.5021834e-06
Iter: 1299 loss: 4.50099515e-06
Iter: 1300 loss: 4.4997214e-06
Iter: 1301 loss: 4.49959498e-06
Iter: 1302 loss: 4.49722847e-06
Iter: 1303 loss: 4.5022648e-06
Iter: 1304 loss: 4.4963449e-06
Iter: 1305 loss: 4.49426125e-06
Iter: 1306 loss: 4.49412528e-06
Iter: 1307 loss: 4.49323943e-06
Iter: 1308 loss: 4.49196887e-06
Iter: 1309 loss: 4.49189974e-06
Iter: 1310 loss: 4.48953142e-06
Iter: 1311 loss: 4.50263178e-06
Iter: 1312 loss: 4.48920218e-06
Iter: 1313 loss: 4.48807168e-06
Iter: 1314 loss: 4.48689798e-06
Iter: 1315 loss: 4.48660739e-06
Iter: 1316 loss: 4.48563424e-06
Iter: 1317 loss: 4.48554692e-06
Iter: 1318 loss: 4.48473338e-06
Iter: 1319 loss: 4.48375295e-06
Iter: 1320 loss: 4.48357241e-06
Iter: 1321 loss: 4.48225956e-06
Iter: 1322 loss: 4.485094e-06
Iter: 1323 loss: 4.48180526e-06
Iter: 1324 loss: 4.48039737e-06
Iter: 1325 loss: 4.48673e-06
Iter: 1326 loss: 4.48006176e-06
Iter: 1327 loss: 4.47848424e-06
Iter: 1328 loss: 4.48295441e-06
Iter: 1329 loss: 4.47797356e-06
Iter: 1330 loss: 4.47675939e-06
Iter: 1331 loss: 4.47525372e-06
Iter: 1332 loss: 4.47500679e-06
Iter: 1333 loss: 4.47258e-06
Iter: 1334 loss: 4.47649381e-06
Iter: 1335 loss: 4.47137108e-06
Iter: 1336 loss: 4.47126695e-06
Iter: 1337 loss: 4.47019374e-06
Iter: 1338 loss: 4.46944796e-06
Iter: 1339 loss: 4.46766853e-06
Iter: 1340 loss: 4.48542914e-06
Iter: 1341 loss: 4.4675453e-06
Iter: 1342 loss: 4.4657636e-06
Iter: 1343 loss: 4.46561535e-06
Iter: 1344 loss: 4.46481681e-06
Iter: 1345 loss: 4.46333752e-06
Iter: 1346 loss: 4.46336e-06
Iter: 1347 loss: 4.46214426e-06
Iter: 1348 loss: 4.47550156e-06
Iter: 1349 loss: 4.46203921e-06
Iter: 1350 loss: 4.46073682e-06
Iter: 1351 loss: 4.45961086e-06
Iter: 1352 loss: 4.45926071e-06
Iter: 1353 loss: 4.45752357e-06
Iter: 1354 loss: 4.46031572e-06
Iter: 1355 loss: 4.45678688e-06
Iter: 1356 loss: 4.45489331e-06
Iter: 1357 loss: 4.47147977e-06
Iter: 1358 loss: 4.45473825e-06
Iter: 1359 loss: 4.45338719e-06
Iter: 1360 loss: 4.45911064e-06
Iter: 1361 loss: 4.45303522e-06
Iter: 1362 loss: 4.45187425e-06
Iter: 1363 loss: 4.45040405e-06
Iter: 1364 loss: 4.45031492e-06
Iter: 1365 loss: 4.44825855e-06
Iter: 1366 loss: 4.45217484e-06
Iter: 1367 loss: 4.44739817e-06
Iter: 1368 loss: 4.44720172e-06
Iter: 1369 loss: 4.44659418e-06
Iter: 1370 loss: 4.44569969e-06
Iter: 1371 loss: 4.44364423e-06
Iter: 1372 loss: 4.46214744e-06
Iter: 1373 loss: 4.44334455e-06
Iter: 1374 loss: 4.44250645e-06
Iter: 1375 loss: 4.44193347e-06
Iter: 1376 loss: 4.44108946e-06
Iter: 1377 loss: 4.43910176e-06
Iter: 1378 loss: 4.46538706e-06
Iter: 1379 loss: 4.43894442e-06
Iter: 1380 loss: 4.43686486e-06
Iter: 1381 loss: 4.44882971e-06
Iter: 1382 loss: 4.43662884e-06
Iter: 1383 loss: 4.43448062e-06
Iter: 1384 loss: 4.44898888e-06
Iter: 1385 loss: 4.43430554e-06
Iter: 1386 loss: 4.43339559e-06
Iter: 1387 loss: 4.43281897e-06
Iter: 1388 loss: 4.43251338e-06
Iter: 1389 loss: 4.43135878e-06
Iter: 1390 loss: 4.44207262e-06
Iter: 1391 loss: 4.43125828e-06
Iter: 1392 loss: 4.43014096e-06
Iter: 1393 loss: 4.43647104e-06
Iter: 1394 loss: 4.42995815e-06
Iter: 1395 loss: 4.42918645e-06
Iter: 1396 loss: 4.42806322e-06
Iter: 1397 loss: 4.42794271e-06
Iter: 1398 loss: 4.42628971e-06
Iter: 1399 loss: 4.42649889e-06
Iter: 1400 loss: 4.42493e-06
Iter: 1401 loss: 4.42349119e-06
Iter: 1402 loss: 4.42352666e-06
Iter: 1403 loss: 4.42152532e-06
Iter: 1404 loss: 4.41995235e-06
Iter: 1405 loss: 4.4194876e-06
Iter: 1406 loss: 4.41863904e-06
Iter: 1407 loss: 4.41844077e-06
Iter: 1408 loss: 4.41761449e-06
Iter: 1409 loss: 4.41606335e-06
Iter: 1410 loss: 4.41608154e-06
Iter: 1411 loss: 4.41483644e-06
Iter: 1412 loss: 4.41949578e-06
Iter: 1413 loss: 4.41443308e-06
Iter: 1414 loss: 4.41352358e-06
Iter: 1415 loss: 4.41345537e-06
Iter: 1416 loss: 4.41296061e-06
Iter: 1417 loss: 4.41152679e-06
Iter: 1418 loss: 4.42306191e-06
Iter: 1419 loss: 4.41120301e-06
Iter: 1420 loss: 4.40946951e-06
Iter: 1421 loss: 4.42544024e-06
Iter: 1422 loss: 4.40934537e-06
Iter: 1423 loss: 4.4081e-06
Iter: 1424 loss: 4.41429211e-06
Iter: 1425 loss: 4.40789745e-06
Iter: 1426 loss: 4.40657641e-06
Iter: 1427 loss: 4.40534768e-06
Iter: 1428 loss: 4.40501663e-06
Iter: 1429 loss: 4.4026433e-06
Iter: 1430 loss: 4.4010003e-06
Iter: 1431 loss: 4.40011263e-06
Iter: 1432 loss: 4.39789801e-06
Iter: 1433 loss: 4.39791211e-06
Iter: 1434 loss: 4.39610039e-06
Iter: 1435 loss: 4.41099e-06
Iter: 1436 loss: 4.39596351e-06
Iter: 1437 loss: 4.39526502e-06
Iter: 1438 loss: 4.39684709e-06
Iter: 1439 loss: 4.39510768e-06
Iter: 1440 loss: 4.39409769e-06
Iter: 1441 loss: 4.39475e-06
Iter: 1442 loss: 4.39351243e-06
Iter: 1443 loss: 4.39258e-06
Iter: 1444 loss: 4.39113091e-06
Iter: 1445 loss: 4.39111045e-06
Iter: 1446 loss: 4.39067571e-06
Iter: 1447 loss: 4.39026553e-06
Iter: 1448 loss: 4.38957886e-06
Iter: 1449 loss: 4.38792176e-06
Iter: 1450 loss: 4.39997802e-06
Iter: 1451 loss: 4.38745974e-06
Iter: 1452 loss: 4.38523239e-06
Iter: 1453 loss: 4.40258e-06
Iter: 1454 loss: 4.38506959e-06
Iter: 1455 loss: 4.38348252e-06
Iter: 1456 loss: 4.39110136e-06
Iter: 1457 loss: 4.38317693e-06
Iter: 1458 loss: 4.38147345e-06
Iter: 1459 loss: 4.38200232e-06
Iter: 1460 loss: 4.38026154e-06
Iter: 1461 loss: 4.37821063e-06
Iter: 1462 loss: 4.3798791e-06
Iter: 1463 loss: 4.37700146e-06
Iter: 1464 loss: 4.37547442e-06
Iter: 1465 loss: 4.38151892e-06
Iter: 1466 loss: 4.37513154e-06
Iter: 1467 loss: 4.37379185e-06
Iter: 1468 loss: 4.393788e-06
Iter: 1469 loss: 4.37378549e-06
Iter: 1470 loss: 4.37313429e-06
Iter: 1471 loss: 4.37257404e-06
Iter: 1472 loss: 4.37229573e-06
Iter: 1473 loss: 4.37108883e-06
Iter: 1474 loss: 4.38383449e-06
Iter: 1475 loss: 4.37103154e-06
Iter: 1476 loss: 4.37027893e-06
Iter: 1477 loss: 4.36832761e-06
Iter: 1478 loss: 4.38588177e-06
Iter: 1479 loss: 4.36807295e-06
Iter: 1480 loss: 4.3672303e-06
Iter: 1481 loss: 4.36694336e-06
Iter: 1482 loss: 4.36578921e-06
Iter: 1483 loss: 4.36376695e-06
Iter: 1484 loss: 4.41424163e-06
Iter: 1485 loss: 4.36384744e-06
Iter: 1486 loss: 4.36196569e-06
Iter: 1487 loss: 4.37776089e-06
Iter: 1488 loss: 4.36181972e-06
Iter: 1489 loss: 4.36067876e-06
Iter: 1490 loss: 4.36494565e-06
Iter: 1491 loss: 4.36039409e-06
Iter: 1492 loss: 4.35896527e-06
Iter: 1493 loss: 4.36090613e-06
Iter: 1494 loss: 4.35819584e-06
Iter: 1495 loss: 4.35671518e-06
Iter: 1496 loss: 4.35843322e-06
Iter: 1497 loss: 4.35580296e-06
Iter: 1498 loss: 4.35458105e-06
Iter: 1499 loss: 4.3547343e-06
Iter: 1500 loss: 4.35354832e-06
Iter: 1501 loss: 4.35210131e-06
Iter: 1502 loss: 4.3520572e-06
Iter: 1503 loss: 4.35085e-06
Iter: 1504 loss: 4.34919457e-06
Iter: 1505 loss: 4.34912045e-06
Iter: 1506 loss: 4.34827462e-06
Iter: 1507 loss: 4.3480677e-06
Iter: 1508 loss: 4.3471191e-06
Iter: 1509 loss: 4.3444611e-06
Iter: 1510 loss: 4.3624832e-06
Iter: 1511 loss: 4.34385584e-06
Iter: 1512 loss: 4.34254116e-06
Iter: 1513 loss: 4.34245158e-06
Iter: 1514 loss: 4.34091e-06
Iter: 1515 loss: 4.34345293e-06
Iter: 1516 loss: 4.34030699e-06
Iter: 1517 loss: 4.33900823e-06
Iter: 1518 loss: 4.33859805e-06
Iter: 1519 loss: 4.3379473e-06
Iter: 1520 loss: 4.33635296e-06
Iter: 1521 loss: 4.35878474e-06
Iter: 1522 loss: 4.33634523e-06
Iter: 1523 loss: 4.33511605e-06
Iter: 1524 loss: 4.33474725e-06
Iter: 1525 loss: 4.33403193e-06
Iter: 1526 loss: 4.33215746e-06
Iter: 1527 loss: 4.33687728e-06
Iter: 1528 loss: 4.33153264e-06
Iter: 1529 loss: 4.32971956e-06
Iter: 1530 loss: 4.32791603e-06
Iter: 1531 loss: 4.32757815e-06
Iter: 1532 loss: 4.32568595e-06
Iter: 1533 loss: 4.32549268e-06
Iter: 1534 loss: 4.32361276e-06
Iter: 1535 loss: 4.32513752e-06
Iter: 1536 loss: 4.32246179e-06
Iter: 1537 loss: 4.32135585e-06
Iter: 1538 loss: 4.3333439e-06
Iter: 1539 loss: 4.32130491e-06
Iter: 1540 loss: 4.32021125e-06
Iter: 1541 loss: 4.31801709e-06
Iter: 1542 loss: 4.31789886e-06
Iter: 1543 loss: 4.3166192e-06
Iter: 1544 loss: 4.32347861e-06
Iter: 1545 loss: 4.31644e-06
Iter: 1546 loss: 4.31490298e-06
Iter: 1547 loss: 4.32107026e-06
Iter: 1548 loss: 4.31463104e-06
Iter: 1549 loss: 4.31328317e-06
Iter: 1550 loss: 4.31132548e-06
Iter: 1551 loss: 4.31139915e-06
Iter: 1552 loss: 4.30962e-06
Iter: 1553 loss: 4.30959881e-06
Iter: 1554 loss: 4.30820728e-06
Iter: 1555 loss: 4.30696218e-06
Iter: 1556 loss: 4.30650698e-06
Iter: 1557 loss: 4.30436103e-06
Iter: 1558 loss: 4.31582e-06
Iter: 1559 loss: 4.30408181e-06
Iter: 1560 loss: 4.30246519e-06
Iter: 1561 loss: 4.30146974e-06
Iter: 1562 loss: 4.3007085e-06
Iter: 1563 loss: 4.29949478e-06
Iter: 1564 loss: 4.29930742e-06
Iter: 1565 loss: 4.29820784e-06
Iter: 1566 loss: 4.30181217e-06
Iter: 1567 loss: 4.29790907e-06
Iter: 1568 loss: 4.29719148e-06
Iter: 1569 loss: 4.29714692e-06
Iter: 1570 loss: 4.29659849e-06
Iter: 1571 loss: 4.29479769e-06
Iter: 1572 loss: 4.29558168e-06
Iter: 1573 loss: 4.29361353e-06
Iter: 1574 loss: 4.29230113e-06
Iter: 1575 loss: 4.29113061e-06
Iter: 1576 loss: 4.29087368e-06
Iter: 1577 loss: 4.28889416e-06
Iter: 1578 loss: 4.28892372e-06
Iter: 1579 loss: 4.28734893e-06
Iter: 1580 loss: 4.28445173e-06
Iter: 1581 loss: 4.34865569e-06
Iter: 1582 loss: 4.28452586e-06
Iter: 1583 loss: 4.28299518e-06
Iter: 1584 loss: 4.28281965e-06
Iter: 1585 loss: 4.28140265e-06
Iter: 1586 loss: 4.28125441e-06
Iter: 1587 loss: 4.28019894e-06
Iter: 1588 loss: 4.27869327e-06
Iter: 1589 loss: 4.28521207e-06
Iter: 1590 loss: 4.27846226e-06
Iter: 1591 loss: 4.27705208e-06
Iter: 1592 loss: 4.27687837e-06
Iter: 1593 loss: 4.27583109e-06
Iter: 1594 loss: 4.27427494e-06
Iter: 1595 loss: 4.281158e-06
Iter: 1596 loss: 4.27398e-06
Iter: 1597 loss: 4.27244186e-06
Iter: 1598 loss: 4.29483498e-06
Iter: 1599 loss: 4.27247687e-06
Iter: 1600 loss: 4.27143368e-06
Iter: 1601 loss: 4.26927e-06
Iter: 1602 loss: 4.2936158e-06
Iter: 1603 loss: 4.26894258e-06
Iter: 1604 loss: 4.26687666e-06
Iter: 1605 loss: 4.2667e-06
Iter: 1606 loss: 4.26594306e-06
Iter: 1607 loss: 4.26357e-06
Iter: 1608 loss: 4.27182749e-06
Iter: 1609 loss: 4.26255883e-06
Iter: 1610 loss: 4.26255701e-06
Iter: 1611 loss: 4.26120187e-06
Iter: 1612 loss: 4.26012502e-06
Iter: 1613 loss: 4.25967892e-06
Iter: 1614 loss: 4.25899634e-06
Iter: 1615 loss: 4.25791177e-06
Iter: 1616 loss: 4.26171073e-06
Iter: 1617 loss: 4.25754706e-06
Iter: 1618 loss: 4.25634971e-06
Iter: 1619 loss: 4.25964436e-06
Iter: 1620 loss: 4.25593134e-06
Iter: 1621 loss: 4.25478947e-06
Iter: 1622 loss: 4.25562848e-06
Iter: 1623 loss: 4.25407188e-06
Iter: 1624 loss: 4.25307735e-06
Iter: 1625 loss: 4.25152211e-06
Iter: 1626 loss: 4.2515012e-06
Iter: 1627 loss: 4.24933478e-06
Iter: 1628 loss: 4.24877771e-06
Iter: 1629 loss: 4.24745667e-06
Iter: 1630 loss: 4.24505151e-06
Iter: 1631 loss: 4.2715219e-06
Iter: 1632 loss: 4.24501377e-06
Iter: 1633 loss: 4.24319751e-06
Iter: 1634 loss: 4.26646329e-06
Iter: 1635 loss: 4.2431675e-06
Iter: 1636 loss: 4.24245e-06
Iter: 1637 loss: 4.24274549e-06
Iter: 1638 loss: 4.24190966e-06
Iter: 1639 loss: 4.24049722e-06
Iter: 1640 loss: 4.24431937e-06
Iter: 1641 loss: 4.24004747e-06
Iter: 1642 loss: 4.23915753e-06
Iter: 1643 loss: 4.23786469e-06
Iter: 1644 loss: 4.23780193e-06
Iter: 1645 loss: 4.23668871e-06
Iter: 1646 loss: 4.2511806e-06
Iter: 1647 loss: 4.23672736e-06
Iter: 1648 loss: 4.23534311e-06
Iter: 1649 loss: 4.235284e-06
Iter: 1650 loss: 4.23421079e-06
Iter: 1651 loss: 4.23317306e-06
Iter: 1652 loss: 4.23386382e-06
Iter: 1653 loss: 4.2324782e-06
Iter: 1654 loss: 4.23157553e-06
Iter: 1655 loss: 4.23153233e-06
Iter: 1656 loss: 4.23088113e-06
Iter: 1657 loss: 4.22941275e-06
Iter: 1658 loss: 4.25753751e-06
Iter: 1659 loss: 4.22936046e-06
Iter: 1660 loss: 4.22844369e-06
Iter: 1661 loss: 4.24226755e-06
Iter: 1662 loss: 4.22846642e-06
Iter: 1663 loss: 4.22760149e-06
Iter: 1664 loss: 4.22773383e-06
Iter: 1665 loss: 4.22696303e-06
Iter: 1666 loss: 4.22583253e-06
Iter: 1667 loss: 4.23308666e-06
Iter: 1668 loss: 4.22564972e-06
Iter: 1669 loss: 4.22460607e-06
Iter: 1670 loss: 4.2239576e-06
Iter: 1671 loss: 4.22347784e-06
Iter: 1672 loss: 4.22195899e-06
Iter: 1673 loss: 4.22199446e-06
Iter: 1674 loss: 4.22112271e-06
Iter: 1675 loss: 4.21892446e-06
Iter: 1676 loss: 4.24017162e-06
Iter: 1677 loss: 4.21855839e-06
Iter: 1678 loss: 4.21595269e-06
Iter: 1679 loss: 4.22506082e-06
Iter: 1680 loss: 4.21530422e-06
Iter: 1681 loss: 4.21257027e-06
Iter: 1682 loss: 4.24436894e-06
Iter: 1683 loss: 4.21251298e-06
Iter: 1684 loss: 4.21151844e-06
Iter: 1685 loss: 4.20938431e-06
Iter: 1686 loss: 4.24782957e-06
Iter: 1687 loss: 4.2094216e-06
Iter: 1688 loss: 4.20842844e-06
Iter: 1689 loss: 4.20823289e-06
Iter: 1690 loss: 4.207046e-06
Iter: 1691 loss: 4.20620154e-06
Iter: 1692 loss: 4.20584365e-06
Iter: 1693 loss: 4.20456672e-06
Iter: 1694 loss: 4.20751894e-06
Iter: 1695 loss: 4.20410333e-06
Iter: 1696 loss: 4.20251445e-06
Iter: 1697 loss: 4.20805554e-06
Iter: 1698 loss: 4.20217884e-06
Iter: 1699 loss: 4.20090464e-06
Iter: 1700 loss: 4.20976721e-06
Iter: 1701 loss: 4.20074639e-06
Iter: 1702 loss: 4.199469e-06
Iter: 1703 loss: 4.19891649e-06
Iter: 1704 loss: 4.19834578e-06
Iter: 1705 loss: 4.19726075e-06
Iter: 1706 loss: 4.19726621e-06
Iter: 1707 loss: 4.19637945e-06
Iter: 1708 loss: 4.19466778e-06
Iter: 1709 loss: 4.22692801e-06
Iter: 1710 loss: 4.19466323e-06
Iter: 1711 loss: 4.19309436e-06
Iter: 1712 loss: 4.19845128e-06
Iter: 1713 loss: 4.19269782e-06
Iter: 1714 loss: 4.19215621e-06
Iter: 1715 loss: 4.19190656e-06
Iter: 1716 loss: 4.19152229e-06
Iter: 1717 loss: 4.19019307e-06
Iter: 1718 loss: 4.19566095e-06
Iter: 1719 loss: 4.18973468e-06
Iter: 1720 loss: 4.18854552e-06
Iter: 1721 loss: 4.18847594e-06
Iter: 1722 loss: 4.18718719e-06
Iter: 1723 loss: 4.18877289e-06
Iter: 1724 loss: 4.18648278e-06
Iter: 1725 loss: 4.1851481e-06
Iter: 1726 loss: 4.18310537e-06
Iter: 1727 loss: 4.18314494e-06
Iter: 1728 loss: 4.18073e-06
Iter: 1729 loss: 4.21200821e-06
Iter: 1730 loss: 4.18070431e-06
Iter: 1731 loss: 4.17923275e-06
Iter: 1732 loss: 4.18667105e-06
Iter: 1733 loss: 4.1790463e-06
Iter: 1734 loss: 4.17729279e-06
Iter: 1735 loss: 4.17769e-06
Iter: 1736 loss: 4.17625415e-06
Iter: 1737 loss: 4.17488081e-06
Iter: 1738 loss: 4.194746e-06
Iter: 1739 loss: 4.17491083e-06
Iter: 1740 loss: 4.17369301e-06
Iter: 1741 loss: 4.17313368e-06
Iter: 1742 loss: 4.1726089e-06
Iter: 1743 loss: 4.17166075e-06
Iter: 1744 loss: 4.17108e-06
Iter: 1745 loss: 4.170794e-06
Iter: 1746 loss: 4.17016872e-06
Iter: 1747 loss: 4.17000774e-06
Iter: 1748 loss: 4.1692856e-06
Iter: 1749 loss: 4.16764215e-06
Iter: 1750 loss: 4.1802341e-06
Iter: 1751 loss: 4.16725925e-06
Iter: 1752 loss: 4.16570947e-06
Iter: 1753 loss: 4.18590298e-06
Iter: 1754 loss: 4.16568491e-06
Iter: 1755 loss: 4.16437797e-06
Iter: 1756 loss: 4.16836701e-06
Iter: 1757 loss: 4.16397597e-06
Iter: 1758 loss: 4.16276089e-06
Iter: 1759 loss: 4.16033481e-06
Iter: 1760 loss: 4.21186678e-06
Iter: 1761 loss: 4.16028161e-06
Iter: 1762 loss: 4.15823934e-06
Iter: 1763 loss: 4.15816612e-06
Iter: 1764 loss: 4.15701e-06
Iter: 1765 loss: 4.15960312e-06
Iter: 1766 loss: 4.15656314e-06
Iter: 1767 loss: 4.15504564e-06
Iter: 1768 loss: 4.15860268e-06
Iter: 1769 loss: 4.1544763e-06
Iter: 1770 loss: 4.15347222e-06
Iter: 1771 loss: 4.16186e-06
Iter: 1772 loss: 4.15334489e-06
Iter: 1773 loss: 4.15231261e-06
Iter: 1774 loss: 4.15367913e-06
Iter: 1775 loss: 4.15170871e-06
Iter: 1776 loss: 4.15099885e-06
Iter: 1777 loss: 4.14931947e-06
Iter: 1778 loss: 4.17572028e-06
Iter: 1779 loss: 4.14924807e-06
Iter: 1780 loss: 4.14861461e-06
Iter: 1781 loss: 4.14825081e-06
Iter: 1782 loss: 4.14720489e-06
Iter: 1783 loss: 4.14565602e-06
Iter: 1784 loss: 4.14563783e-06
Iter: 1785 loss: 4.1442222e-06
Iter: 1786 loss: 4.15289469e-06
Iter: 1787 loss: 4.1440544e-06
Iter: 1788 loss: 4.14280566e-06
Iter: 1789 loss: 4.15210161e-06
Iter: 1790 loss: 4.14271926e-06
Iter: 1791 loss: 4.14182796e-06
Iter: 1792 loss: 4.14042552e-06
Iter: 1793 loss: 4.14038e-06
Iter: 1794 loss: 4.13930866e-06
Iter: 1795 loss: 4.1392741e-06
Iter: 1796 loss: 4.13853e-06
Iter: 1797 loss: 4.13934504e-06
Iter: 1798 loss: 4.13801945e-06
Iter: 1799 loss: 4.13684938e-06
Iter: 1800 loss: 4.1395283e-06
Iter: 1801 loss: 4.13648331e-06
Iter: 1802 loss: 4.13534735e-06
Iter: 1803 loss: 4.14091073e-06
Iter: 1804 loss: 4.13507e-06
Iter: 1805 loss: 4.13377711e-06
Iter: 1806 loss: 4.13741873e-06
Iter: 1807 loss: 4.1334979e-06
Iter: 1808 loss: 4.13254656e-06
Iter: 1809 loss: 4.13032285e-06
Iter: 1810 loss: 4.15145905e-06
Iter: 1811 loss: 4.12997724e-06
Iter: 1812 loss: 4.12918644e-06
Iter: 1813 loss: 4.12871759e-06
Iter: 1814 loss: 4.12737791e-06
Iter: 1815 loss: 4.12740883e-06
Iter: 1816 loss: 4.12633335e-06
Iter: 1817 loss: 4.12525833e-06
Iter: 1818 loss: 4.12682857e-06
Iter: 1819 loss: 4.12463942e-06
Iter: 1820 loss: 4.12355121e-06
Iter: 1821 loss: 4.14028636e-06
Iter: 1822 loss: 4.12355e-06
Iter: 1823 loss: 4.12275676e-06
Iter: 1824 loss: 4.12127292e-06
Iter: 1825 loss: 4.15072691e-06
Iter: 1826 loss: 4.1212e-06
Iter: 1827 loss: 4.11995643e-06
Iter: 1828 loss: 4.13444468e-06
Iter: 1829 loss: 4.11996e-06
Iter: 1830 loss: 4.11909514e-06
Iter: 1831 loss: 4.11906694e-06
Iter: 1832 loss: 4.11821e-06
Iter: 1833 loss: 4.11677365e-06
Iter: 1834 loss: 4.12311192e-06
Iter: 1835 loss: 4.1164767e-06
Iter: 1836 loss: 4.1150879e-06
Iter: 1837 loss: 4.11897099e-06
Iter: 1838 loss: 4.11458632e-06
Iter: 1839 loss: 4.11301971e-06
Iter: 1840 loss: 4.11778274e-06
Iter: 1841 loss: 4.11240717e-06
Iter: 1842 loss: 4.11104793e-06
Iter: 1843 loss: 4.1083631e-06
Iter: 1844 loss: 4.15619706e-06
Iter: 1845 loss: 4.10825078e-06
Iter: 1846 loss: 4.10725e-06
Iter: 1847 loss: 4.10701068e-06
Iter: 1848 loss: 4.10577513e-06
Iter: 1849 loss: 4.10754637e-06
Iter: 1850 loss: 4.10512803e-06
Iter: 1851 loss: 4.10412895e-06
Iter: 1852 loss: 4.10365374e-06
Iter: 1853 loss: 4.1030662e-06
Iter: 1854 loss: 4.10200391e-06
Iter: 1855 loss: 4.10201392e-06
Iter: 1856 loss: 4.10114853e-06
Iter: 1857 loss: 4.09938184e-06
Iter: 1858 loss: 4.13432917e-06
Iter: 1859 loss: 4.0994255e-06
Iter: 1860 loss: 4.09801623e-06
Iter: 1861 loss: 4.11325891e-06
Iter: 1862 loss: 4.09790664e-06
Iter: 1863 loss: 4.09647964e-06
Iter: 1864 loss: 4.09710356e-06
Iter: 1865 loss: 4.09557379e-06
Iter: 1866 loss: 4.09362747e-06
Iter: 1867 loss: 4.10140774e-06
Iter: 1868 loss: 4.09323275e-06
Iter: 1869 loss: 4.09151562e-06
Iter: 1870 loss: 4.09706081e-06
Iter: 1871 loss: 4.09105724e-06
Iter: 1872 loss: 4.08927735e-06
Iter: 1873 loss: 4.09988752e-06
Iter: 1874 loss: 4.0891141e-06
Iter: 1875 loss: 4.08801725e-06
Iter: 1876 loss: 4.08587948e-06
Iter: 1877 loss: 4.13076441e-06
Iter: 1878 loss: 4.08585311e-06
Iter: 1879 loss: 4.08442247e-06
Iter: 1880 loss: 4.10278972e-06
Iter: 1881 loss: 4.0844493e-06
Iter: 1882 loss: 4.08300411e-06
Iter: 1883 loss: 4.08758478e-06
Iter: 1884 loss: 4.08256892e-06
Iter: 1885 loss: 4.08135656e-06
Iter: 1886 loss: 4.07950074e-06
Iter: 1887 loss: 4.07949437e-06
Iter: 1888 loss: 4.07863081e-06
Iter: 1889 loss: 4.07824882e-06
Iter: 1890 loss: 4.0773275e-06
Iter: 1891 loss: 4.0752293e-06
Iter: 1892 loss: 4.12056488e-06
Iter: 1893 loss: 4.07525476e-06
Iter: 1894 loss: 4.07324933e-06
Iter: 1895 loss: 4.08545975e-06
Iter: 1896 loss: 4.07302605e-06
Iter: 1897 loss: 4.0711343e-06
Iter: 1898 loss: 4.07421612e-06
Iter: 1899 loss: 4.07029893e-06
Iter: 1900 loss: 4.06836625e-06
Iter: 1901 loss: 4.07788639e-06
Iter: 1902 loss: 4.06805884e-06
Iter: 1903 loss: 4.06642084e-06
Iter: 1904 loss: 4.06947311e-06
Iter: 1905 loss: 4.06571144e-06
Iter: 1906 loss: 4.0640266e-06
Iter: 1907 loss: 4.07981679e-06
Iter: 1908 loss: 4.06391564e-06
Iter: 1909 loss: 4.06276195e-06
Iter: 1910 loss: 4.06030631e-06
Iter: 1911 loss: 4.10377925e-06
Iter: 1912 loss: 4.06033087e-06
Iter: 1913 loss: 4.05840456e-06
Iter: 1914 loss: 4.07165817e-06
Iter: 1915 loss: 4.05819173e-06
Iter: 1916 loss: 4.05630135e-06
Iter: 1917 loss: 4.06784602e-06
Iter: 1918 loss: 4.056119e-06
Iter: 1919 loss: 4.05478568e-06
Iter: 1920 loss: 4.05314495e-06
Iter: 1921 loss: 4.05297624e-06
Iter: 1922 loss: 4.05192441e-06
Iter: 1923 loss: 4.05182163e-06
Iter: 1924 loss: 4.05061928e-06
Iter: 1925 loss: 4.04913681e-06
Iter: 1926 loss: 4.04902e-06
Iter: 1927 loss: 4.04735147e-06
Iter: 1928 loss: 4.0557934e-06
Iter: 1929 loss: 4.04709135e-06
Iter: 1930 loss: 4.04581442e-06
Iter: 1931 loss: 4.04883895e-06
Iter: 1932 loss: 4.04520688e-06
Iter: 1933 loss: 4.04387538e-06
Iter: 1934 loss: 4.04919774e-06
Iter: 1935 loss: 4.04352977e-06
Iter: 1936 loss: 4.04208367e-06
Iter: 1937 loss: 4.04412685e-06
Iter: 1938 loss: 4.04140928e-06
Iter: 1939 loss: 4.0398595e-06
Iter: 1940 loss: 4.05644187e-06
Iter: 1941 loss: 4.03987087e-06
Iter: 1942 loss: 4.03872491e-06
Iter: 1943 loss: 4.03668082e-06
Iter: 1944 loss: 4.08445067e-06
Iter: 1945 loss: 4.03663716e-06
Iter: 1946 loss: 4.03489139e-06
Iter: 1947 loss: 4.04281127e-06
Iter: 1948 loss: 4.03455351e-06
Iter: 1949 loss: 4.03278409e-06
Iter: 1950 loss: 4.04849106e-06
Iter: 1951 loss: 4.03278136e-06
Iter: 1952 loss: 4.03165905e-06
Iter: 1953 loss: 4.03008744e-06
Iter: 1954 loss: 4.02999649e-06
Iter: 1955 loss: 4.02873229e-06
Iter: 1956 loss: 4.04719322e-06
Iter: 1957 loss: 4.02874775e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi0.4
+ date
Sat Nov  7 21:06:06 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/300_100_100_100_1 --function f1 --psi -2 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff761801048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7618dcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff781ddd158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7618587b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff761858bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff761858e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7201f28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7201b0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7201b0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff720181b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff73c0c0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff73c0ce840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff73c0ceb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7201036a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff720123950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff720123730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7201037b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff720103840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7200baa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff720086f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff6c07da950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff72002e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff73c0526a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff73c06a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff73c06a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff6c072a048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff6c06ef8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff6c0762510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff6c07620d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff6c0774840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff6c07a87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff6c061c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff6c061cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff6c0638510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff6c0664bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff6c06649d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.007400913
test_loss: 0.007882469
train_loss: 0.005102786
test_loss: 0.005439374
train_loss: 0.0044234134
test_loss: 0.0048747826
train_loss: 0.004791922
test_loss: 0.0051208003
train_loss: 0.004554355
test_loss: 0.004910094
train_loss: 0.0041654296
test_loss: 0.0044926256
train_loss: 0.00407051
test_loss: 0.004640933
train_loss: 0.0040169405
test_loss: 0.0043919515
train_loss: 0.0042197155
test_loss: 0.0045742774
train_loss: 0.0040589287
test_loss: 0.0044820462
train_loss: 0.0041952096
test_loss: 0.0045189997
train_loss: 0.0039364113
test_loss: 0.004705839
train_loss: 0.0040172674
test_loss: 0.0042393594
train_loss: 0.003720128
test_loss: 0.0040532458
train_loss: 0.0040158364
test_loss: 0.0042391773
train_loss: 0.004163975
test_loss: 0.0042466265
train_loss: 0.0035965065
test_loss: 0.0041252505
train_loss: 0.003877795
test_loss: 0.0042564594
train_loss: 0.0037408054
test_loss: 0.0041329255
train_loss: 0.0036869864
test_loss: 0.0041394117
train_loss: 0.003662464
test_loss: 0.0041459897
train_loss: 0.0037384166
test_loss: 0.0041351803
train_loss: 0.0036452059
test_loss: 0.0042523555
train_loss: 0.0036083995
test_loss: 0.004004835
train_loss: 0.0036673613
test_loss: 0.0040671597
train_loss: 0.003580443
test_loss: 0.004073049
train_loss: 0.0038383007
test_loss: 0.003981771
train_loss: 0.003517751
test_loss: 0.0041647474
train_loss: 0.004213307
test_loss: 0.00406477
train_loss: 0.0036174892
test_loss: 0.004119005
train_loss: 0.0035372793
test_loss: 0.003948357
train_loss: 0.0037910545
test_loss: 0.004117811
train_loss: 0.0036443176
test_loss: 0.0039740102
train_loss: 0.0035760214
test_loss: 0.0040204953
train_loss: 0.003504531
test_loss: 0.0038858065
train_loss: 0.003506093
test_loss: 0.003869422
train_loss: 0.0033853706
test_loss: 0.003859686
train_loss: 0.0033790674
test_loss: 0.0037589318
train_loss: 0.0034234994
test_loss: 0.0039363145
train_loss: 0.0035104402
test_loss: 0.0039349147
train_loss: 0.0033368368
test_loss: 0.0038991324
train_loss: 0.0032973846
test_loss: 0.0038875446
train_loss: 0.0034983705
test_loss: 0.004298375
train_loss: 0.0032346337
test_loss: 0.003851786
train_loss: 0.0033944703
test_loss: 0.0036947788
train_loss: 0.0035711264
test_loss: 0.0039325473
train_loss: 0.003246965
test_loss: 0.0037292046
train_loss: 0.0034766789
test_loss: 0.003805313
train_loss: 0.0035444612
test_loss: 0.0038784652
train_loss: 0.0038903076
test_loss: 0.003981043
train_loss: 0.0032572073
test_loss: 0.0037960557
train_loss: 0.003365052
test_loss: 0.0038310562
train_loss: 0.0032615517
test_loss: 0.0037301541
train_loss: 0.0034763857
test_loss: 0.0038049126
train_loss: 0.0031276077
test_loss: 0.003785319
train_loss: 0.0031600194
test_loss: 0.0036960323
train_loss: 0.003291517
test_loss: 0.0037812248
train_loss: 0.0033399225
test_loss: 0.0038957598
train_loss: 0.0031504654
test_loss: 0.0037484076
train_loss: 0.0031219495
test_loss: 0.003744917
train_loss: 0.0034150449
test_loss: 0.0037967179
train_loss: 0.0031021936
test_loss: 0.0036548981
train_loss: 0.003499425
test_loss: 0.0037110054
train_loss: 0.003277456
test_loss: 0.0037549862
train_loss: 0.0031692628
test_loss: 0.003642971
train_loss: 0.003332343
test_loss: 0.0036779498
train_loss: 0.0031314115
test_loss: 0.0037292698
train_loss: 0.0032035185
test_loss: 0.003681928
train_loss: 0.0030489524
test_loss: 0.0035638832
train_loss: 0.003142321
test_loss: 0.0036938868
train_loss: 0.0031774791
test_loss: 0.003669701
train_loss: 0.0030510556
test_loss: 0.0037768935
train_loss: 0.0030889222
test_loss: 0.0037777626
train_loss: 0.0030996695
test_loss: 0.003635392
train_loss: 0.0032131956
test_loss: 0.0036399376
train_loss: 0.003035601
test_loss: 0.00379377
train_loss: 0.0030088136
test_loss: 0.0036812734
train_loss: 0.00328613
test_loss: 0.0037326321
train_loss: 0.0030844072
test_loss: 0.0035343633
train_loss: 0.0031811984
test_loss: 0.0038183648
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi0.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi0.4/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef882ad268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef881f8d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef8830cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef88249ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef8825f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef8825fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef8819af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef8813a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef88151400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef8813af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef88399e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef880d7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef880d4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef8806bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef8803ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef880468c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef8800a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef8800a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef87fe4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef87fe41e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef87fe4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fd7f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fd408c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fd4cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fd47598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fd758c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fcd4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fcfa840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fcd4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fcb6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fc56598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fc79840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fc79950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fc2e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fbde6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fef7fb96730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.77585971e-05
Iter: 2 loss: 1.8124385e-05
Iter: 3 loss: 1.44186943e-05
Iter: 4 loss: 1.36451981e-05
Iter: 5 loss: 2.30572041e-05
Iter: 6 loss: 1.36362687e-05
Iter: 7 loss: 1.32972973e-05
Iter: 8 loss: 1.28688735e-05
Iter: 9 loss: 1.28356151e-05
Iter: 10 loss: 1.22870824e-05
Iter: 11 loss: 1.61680382e-05
Iter: 12 loss: 1.22379479e-05
Iter: 13 loss: 1.16701976e-05
Iter: 14 loss: 1.32352034e-05
Iter: 15 loss: 1.14857739e-05
Iter: 16 loss: 1.12751804e-05
Iter: 17 loss: 1.20240893e-05
Iter: 18 loss: 1.12210892e-05
Iter: 19 loss: 1.09340372e-05
Iter: 20 loss: 1.12237103e-05
Iter: 21 loss: 1.07731466e-05
Iter: 22 loss: 1.0492573e-05
Iter: 23 loss: 1.00998286e-05
Iter: 24 loss: 1.00839989e-05
Iter: 25 loss: 9.75247167e-06
Iter: 26 loss: 9.75152e-06
Iter: 27 loss: 9.36381366e-06
Iter: 28 loss: 9.3393628e-06
Iter: 29 loss: 9.0458052e-06
Iter: 30 loss: 8.78091669e-06
Iter: 31 loss: 9.39585516e-06
Iter: 32 loss: 8.68277311e-06
Iter: 33 loss: 8.51966433e-06
Iter: 34 loss: 8.49438402e-06
Iter: 35 loss: 8.38114829e-06
Iter: 36 loss: 8.17520413e-06
Iter: 37 loss: 9.90735134e-06
Iter: 38 loss: 8.16329703e-06
Iter: 39 loss: 7.99808186e-06
Iter: 40 loss: 8.33345257e-06
Iter: 41 loss: 7.93142e-06
Iter: 42 loss: 7.7291661e-06
Iter: 43 loss: 1.00659181e-05
Iter: 44 loss: 7.72610838e-06
Iter: 45 loss: 7.62908167e-06
Iter: 46 loss: 7.47307558e-06
Iter: 47 loss: 7.47195736e-06
Iter: 48 loss: 7.33561865e-06
Iter: 49 loss: 7.33013076e-06
Iter: 50 loss: 7.2239709e-06
Iter: 51 loss: 7.14684484e-06
Iter: 52 loss: 7.11046596e-06
Iter: 53 loss: 7.03574096e-06
Iter: 54 loss: 7.0353085e-06
Iter: 55 loss: 6.97362429e-06
Iter: 56 loss: 7.00025566e-06
Iter: 57 loss: 6.9311327e-06
Iter: 58 loss: 6.85531e-06
Iter: 59 loss: 6.86280418e-06
Iter: 60 loss: 6.79687309e-06
Iter: 61 loss: 6.75074125e-06
Iter: 62 loss: 7.37448454e-06
Iter: 63 loss: 6.7506362e-06
Iter: 64 loss: 6.69605197e-06
Iter: 65 loss: 6.67130598e-06
Iter: 66 loss: 6.64388517e-06
Iter: 67 loss: 6.56325346e-06
Iter: 68 loss: 6.4823721e-06
Iter: 69 loss: 6.46611e-06
Iter: 70 loss: 6.32911e-06
Iter: 71 loss: 6.37251696e-06
Iter: 72 loss: 6.23149708e-06
Iter: 73 loss: 6.12080521e-06
Iter: 74 loss: 6.12063832e-06
Iter: 75 loss: 6.06771e-06
Iter: 76 loss: 6.67689392e-06
Iter: 77 loss: 6.06691447e-06
Iter: 78 loss: 6.00921339e-06
Iter: 79 loss: 6.09502331e-06
Iter: 80 loss: 5.98136239e-06
Iter: 81 loss: 5.9543645e-06
Iter: 82 loss: 6.07577749e-06
Iter: 83 loss: 5.94911944e-06
Iter: 84 loss: 5.92430479e-06
Iter: 85 loss: 6.12665826e-06
Iter: 86 loss: 5.92282186e-06
Iter: 87 loss: 5.90718219e-06
Iter: 88 loss: 5.86782699e-06
Iter: 89 loss: 6.21901381e-06
Iter: 90 loss: 5.86182e-06
Iter: 91 loss: 5.82028133e-06
Iter: 92 loss: 5.81929862e-06
Iter: 93 loss: 5.7995594e-06
Iter: 94 loss: 5.74976684e-06
Iter: 95 loss: 6.19630327e-06
Iter: 96 loss: 5.74202204e-06
Iter: 97 loss: 5.67726102e-06
Iter: 98 loss: 6.21784966e-06
Iter: 99 loss: 5.67348343e-06
Iter: 100 loss: 5.62667e-06
Iter: 101 loss: 6.09228255e-06
Iter: 102 loss: 5.6251547e-06
Iter: 103 loss: 5.60129138e-06
Iter: 104 loss: 5.59162163e-06
Iter: 105 loss: 5.57884186e-06
Iter: 106 loss: 5.5443461e-06
Iter: 107 loss: 5.51483572e-06
Iter: 108 loss: 5.50548975e-06
Iter: 109 loss: 5.47622358e-06
Iter: 110 loss: 5.73337866e-06
Iter: 111 loss: 5.47466334e-06
Iter: 112 loss: 5.44571049e-06
Iter: 113 loss: 5.48541448e-06
Iter: 114 loss: 5.43128499e-06
Iter: 115 loss: 5.40254405e-06
Iter: 116 loss: 5.40211659e-06
Iter: 117 loss: 5.38992799e-06
Iter: 118 loss: 5.36632433e-06
Iter: 119 loss: 5.85199359e-06
Iter: 120 loss: 5.36617517e-06
Iter: 121 loss: 5.33726961e-06
Iter: 122 loss: 5.71264536e-06
Iter: 123 loss: 5.33696857e-06
Iter: 124 loss: 5.31411115e-06
Iter: 125 loss: 5.26475378e-06
Iter: 126 loss: 6.02849286e-06
Iter: 127 loss: 5.26298481e-06
Iter: 128 loss: 5.27936572e-06
Iter: 129 loss: 5.24818415e-06
Iter: 130 loss: 5.23986e-06
Iter: 131 loss: 5.22168875e-06
Iter: 132 loss: 5.49574543e-06
Iter: 133 loss: 5.22098344e-06
Iter: 134 loss: 5.20084359e-06
Iter: 135 loss: 5.305797e-06
Iter: 136 loss: 5.19762125e-06
Iter: 137 loss: 5.17781518e-06
Iter: 138 loss: 5.29006093e-06
Iter: 139 loss: 5.17509761e-06
Iter: 140 loss: 5.16671935e-06
Iter: 141 loss: 5.15253214e-06
Iter: 142 loss: 5.15243119e-06
Iter: 143 loss: 5.12729457e-06
Iter: 144 loss: 5.14019166e-06
Iter: 145 loss: 5.11045891e-06
Iter: 146 loss: 5.0835697e-06
Iter: 147 loss: 5.25058431e-06
Iter: 148 loss: 5.08037647e-06
Iter: 149 loss: 5.05770686e-06
Iter: 150 loss: 5.01795876e-06
Iter: 151 loss: 5.01802151e-06
Iter: 152 loss: 5.06853758e-06
Iter: 153 loss: 5.00437636e-06
Iter: 154 loss: 4.99630369e-06
Iter: 155 loss: 4.98591e-06
Iter: 156 loss: 4.98516874e-06
Iter: 157 loss: 4.97552355e-06
Iter: 158 loss: 5.0962135e-06
Iter: 159 loss: 4.97548444e-06
Iter: 160 loss: 4.96501207e-06
Iter: 161 loss: 4.95419681e-06
Iter: 162 loss: 4.95223412e-06
Iter: 163 loss: 4.94333199e-06
Iter: 164 loss: 5.00625265e-06
Iter: 165 loss: 4.94263895e-06
Iter: 166 loss: 4.93217522e-06
Iter: 167 loss: 4.95010772e-06
Iter: 168 loss: 4.92755908e-06
Iter: 169 loss: 4.91762421e-06
Iter: 170 loss: 4.89856393e-06
Iter: 171 loss: 5.30229272e-06
Iter: 172 loss: 4.89857212e-06
Iter: 173 loss: 4.89634203e-06
Iter: 174 loss: 4.88857859e-06
Iter: 175 loss: 4.88062733e-06
Iter: 176 loss: 4.85742657e-06
Iter: 177 loss: 4.94187771e-06
Iter: 178 loss: 4.84734301e-06
Iter: 179 loss: 4.82428277e-06
Iter: 180 loss: 5.05790285e-06
Iter: 181 loss: 4.82350106e-06
Iter: 182 loss: 4.80841481e-06
Iter: 183 loss: 4.81604548e-06
Iter: 184 loss: 4.79850678e-06
Iter: 185 loss: 4.78789252e-06
Iter: 186 loss: 4.78768106e-06
Iter: 187 loss: 4.78068569e-06
Iter: 188 loss: 4.87385523e-06
Iter: 189 loss: 4.78060701e-06
Iter: 190 loss: 4.77571757e-06
Iter: 191 loss: 4.76545665e-06
Iter: 192 loss: 4.93478456e-06
Iter: 193 loss: 4.76517789e-06
Iter: 194 loss: 4.75925117e-06
Iter: 195 loss: 4.75817251e-06
Iter: 196 loss: 4.75233719e-06
Iter: 197 loss: 4.73725868e-06
Iter: 198 loss: 4.85124747e-06
Iter: 199 loss: 4.73425825e-06
Iter: 200 loss: 4.71850399e-06
Iter: 201 loss: 4.9562359e-06
Iter: 202 loss: 4.71850672e-06
Iter: 203 loss: 4.70789746e-06
Iter: 204 loss: 4.77069261e-06
Iter: 205 loss: 4.7065023e-06
Iter: 206 loss: 4.69954284e-06
Iter: 207 loss: 4.69003226e-06
Iter: 208 loss: 4.68959115e-06
Iter: 209 loss: 4.67755945e-06
Iter: 210 loss: 4.72313923e-06
Iter: 211 loss: 4.67470409e-06
Iter: 212 loss: 4.66629217e-06
Iter: 213 loss: 4.66614483e-06
Iter: 214 loss: 4.66308848e-06
Iter: 215 loss: 4.65621588e-06
Iter: 216 loss: 4.75615798e-06
Iter: 217 loss: 4.65585299e-06
Iter: 218 loss: 4.64606364e-06
Iter: 219 loss: 4.63932793e-06
Iter: 220 loss: 4.63566539e-06
Iter: 221 loss: 4.62151456e-06
Iter: 222 loss: 4.69119595e-06
Iter: 223 loss: 4.61906848e-06
Iter: 224 loss: 4.60740875e-06
Iter: 225 loss: 4.60731053e-06
Iter: 226 loss: 4.59991043e-06
Iter: 227 loss: 4.59170542e-06
Iter: 228 loss: 4.59052762e-06
Iter: 229 loss: 4.58745126e-06
Iter: 230 loss: 4.5857978e-06
Iter: 231 loss: 4.58196791e-06
Iter: 232 loss: 4.57164697e-06
Iter: 233 loss: 4.63122797e-06
Iter: 234 loss: 4.56875432e-06
Iter: 235 loss: 4.56881753e-06
Iter: 236 loss: 4.56412954e-06
Iter: 237 loss: 4.56054158e-06
Iter: 238 loss: 4.55572263e-06
Iter: 239 loss: 4.55541976e-06
Iter: 240 loss: 4.54815654e-06
Iter: 241 loss: 4.54989186e-06
Iter: 242 loss: 4.54293604e-06
Iter: 243 loss: 4.53569282e-06
Iter: 244 loss: 4.6111054e-06
Iter: 245 loss: 4.53557277e-06
Iter: 246 loss: 4.52923723e-06
Iter: 247 loss: 4.54558676e-06
Iter: 248 loss: 4.52703625e-06
Iter: 249 loss: 4.52068025e-06
Iter: 250 loss: 4.50706102e-06
Iter: 251 loss: 4.72372903e-06
Iter: 252 loss: 4.50650441e-06
Iter: 253 loss: 4.49477875e-06
Iter: 254 loss: 4.60513638e-06
Iter: 255 loss: 4.49424624e-06
Iter: 256 loss: 4.48611218e-06
Iter: 257 loss: 4.48751598e-06
Iter: 258 loss: 4.4800654e-06
Iter: 259 loss: 4.47956108e-06
Iter: 260 loss: 4.47480534e-06
Iter: 261 loss: 4.47168e-06
Iter: 262 loss: 4.46548438e-06
Iter: 263 loss: 4.5862671e-06
Iter: 264 loss: 4.46538343e-06
Iter: 265 loss: 4.46006106e-06
Iter: 266 loss: 4.46010472e-06
Iter: 267 loss: 4.45501882e-06
Iter: 268 loss: 4.44480384e-06
Iter: 269 loss: 4.62443313e-06
Iter: 270 loss: 4.44450507e-06
Iter: 271 loss: 4.43619729e-06
Iter: 272 loss: 4.46490594e-06
Iter: 273 loss: 4.43400904e-06
Iter: 274 loss: 4.42733108e-06
Iter: 275 loss: 4.42728106e-06
Iter: 276 loss: 4.42284818e-06
Iter: 277 loss: 4.41509201e-06
Iter: 278 loss: 4.41507837e-06
Iter: 279 loss: 4.40999247e-06
Iter: 280 loss: 4.4604767e-06
Iter: 281 loss: 4.40994882e-06
Iter: 282 loss: 4.40425811e-06
Iter: 283 loss: 4.41901466e-06
Iter: 284 loss: 4.40241229e-06
Iter: 285 loss: 4.39822725e-06
Iter: 286 loss: 4.39844825e-06
Iter: 287 loss: 4.39490941e-06
Iter: 288 loss: 4.38869847e-06
Iter: 289 loss: 4.38524057e-06
Iter: 290 loss: 4.38245524e-06
Iter: 291 loss: 4.37229846e-06
Iter: 292 loss: 4.42129294e-06
Iter: 293 loss: 4.37055087e-06
Iter: 294 loss: 4.36431083e-06
Iter: 295 loss: 4.44568786e-06
Iter: 296 loss: 4.36420805e-06
Iter: 297 loss: 4.35651236e-06
Iter: 298 loss: 4.34721278e-06
Iter: 299 loss: 4.34620142e-06
Iter: 300 loss: 4.34318508e-06
Iter: 301 loss: 4.34246977e-06
Iter: 302 loss: 4.33932883e-06
Iter: 303 loss: 4.33897912e-06
Iter: 304 loss: 4.33690275e-06
Iter: 305 loss: 4.3337368e-06
Iter: 306 loss: 4.32808929e-06
Iter: 307 loss: 4.32815614e-06
Iter: 308 loss: 4.323761e-06
Iter: 309 loss: 4.32309389e-06
Iter: 310 loss: 4.32033e-06
Iter: 311 loss: 4.31647641e-06
Iter: 312 loss: 4.31626086e-06
Iter: 313 loss: 4.31220133e-06
Iter: 314 loss: 4.32563866e-06
Iter: 315 loss: 4.31104581e-06
Iter: 316 loss: 4.30461569e-06
Iter: 317 loss: 4.31599165e-06
Iter: 318 loss: 4.30180262e-06
Iter: 319 loss: 4.29748525e-06
Iter: 320 loss: 4.29344436e-06
Iter: 321 loss: 4.29243073e-06
Iter: 322 loss: 4.28411113e-06
Iter: 323 loss: 4.3082282e-06
Iter: 324 loss: 4.28156318e-06
Iter: 325 loss: 4.2761194e-06
Iter: 326 loss: 4.3312e-06
Iter: 327 loss: 4.27601117e-06
Iter: 328 loss: 4.27362784e-06
Iter: 329 loss: 4.27358509e-06
Iter: 330 loss: 4.27096e-06
Iter: 331 loss: 4.26379302e-06
Iter: 332 loss: 4.30565797e-06
Iter: 333 loss: 4.26173301e-06
Iter: 334 loss: 4.26000679e-06
Iter: 335 loss: 4.25736516e-06
Iter: 336 loss: 4.25485314e-06
Iter: 337 loss: 4.2507e-06
Iter: 338 loss: 4.25069902e-06
Iter: 339 loss: 4.24576592e-06
Iter: 340 loss: 4.24870905e-06
Iter: 341 loss: 4.24252721e-06
Iter: 342 loss: 4.23536767e-06
Iter: 343 loss: 4.31778699e-06
Iter: 344 loss: 4.23528218e-06
Iter: 345 loss: 4.23253459e-06
Iter: 346 loss: 4.22940411e-06
Iter: 347 loss: 4.22900303e-06
Iter: 348 loss: 4.22640824e-06
Iter: 349 loss: 4.22636913e-06
Iter: 350 loss: 4.22352196e-06
Iter: 351 loss: 4.22123321e-06
Iter: 352 loss: 4.22044e-06
Iter: 353 loss: 4.21730419e-06
Iter: 354 loss: 4.21683853e-06
Iter: 355 loss: 4.21478171e-06
Iter: 356 loss: 4.20974129e-06
Iter: 357 loss: 4.22184e-06
Iter: 358 loss: 4.20796323e-06
Iter: 359 loss: 4.20365541e-06
Iter: 360 loss: 4.25811277e-06
Iter: 361 loss: 4.2037359e-06
Iter: 362 loss: 4.19983e-06
Iter: 363 loss: 4.21282493e-06
Iter: 364 loss: 4.19878552e-06
Iter: 365 loss: 4.19522485e-06
Iter: 366 loss: 4.19294065e-06
Iter: 367 loss: 4.19152639e-06
Iter: 368 loss: 4.18757691e-06
Iter: 369 loss: 4.18760055e-06
Iter: 370 loss: 4.18567379e-06
Iter: 371 loss: 4.18100717e-06
Iter: 372 loss: 4.22943822e-06
Iter: 373 loss: 4.18039645e-06
Iter: 374 loss: 4.17694628e-06
Iter: 375 loss: 4.17671754e-06
Iter: 376 loss: 4.17355659e-06
Iter: 377 loss: 4.17679257e-06
Iter: 378 loss: 4.17162482e-06
Iter: 379 loss: 4.16983085e-06
Iter: 380 loss: 4.16795683e-06
Iter: 381 loss: 4.16752209e-06
Iter: 382 loss: 4.16413059e-06
Iter: 383 loss: 4.21416598e-06
Iter: 384 loss: 4.16417697e-06
Iter: 385 loss: 4.16207058e-06
Iter: 386 loss: 4.15859495e-06
Iter: 387 loss: 4.15868226e-06
Iter: 388 loss: 4.15437398e-06
Iter: 389 loss: 4.15194336e-06
Iter: 390 loss: 4.15001841e-06
Iter: 391 loss: 4.14501392e-06
Iter: 392 loss: 4.21572304e-06
Iter: 393 loss: 4.14502392e-06
Iter: 394 loss: 4.14169972e-06
Iter: 395 loss: 4.17066622e-06
Iter: 396 loss: 4.14158239e-06
Iter: 397 loss: 4.138039e-06
Iter: 398 loss: 4.14090209e-06
Iter: 399 loss: 4.13587804e-06
Iter: 400 loss: 4.13305224e-06
Iter: 401 loss: 4.1478952e-06
Iter: 402 loss: 4.13254838e-06
Iter: 403 loss: 4.13013368e-06
Iter: 404 loss: 4.14126316e-06
Iter: 405 loss: 4.12972713e-06
Iter: 406 loss: 4.12773898e-06
Iter: 407 loss: 4.12312647e-06
Iter: 408 loss: 4.17893534e-06
Iter: 409 loss: 4.12269492e-06
Iter: 410 loss: 4.11973451e-06
Iter: 411 loss: 4.11920428e-06
Iter: 412 loss: 4.11660039e-06
Iter: 413 loss: 4.11311839e-06
Iter: 414 loss: 4.11296833e-06
Iter: 415 loss: 4.10935536e-06
Iter: 416 loss: 4.11829751e-06
Iter: 417 loss: 4.10813436e-06
Iter: 418 loss: 4.10236271e-06
Iter: 419 loss: 4.11778e-06
Iter: 420 loss: 4.10049597e-06
Iter: 421 loss: 4.09810173e-06
Iter: 422 loss: 4.0976056e-06
Iter: 423 loss: 4.09591348e-06
Iter: 424 loss: 4.09210861e-06
Iter: 425 loss: 4.09248332e-06
Iter: 426 loss: 4.089306e-06
Iter: 427 loss: 4.08676442e-06
Iter: 428 loss: 4.08671303e-06
Iter: 429 loss: 4.08464757e-06
Iter: 430 loss: 4.09714767e-06
Iter: 431 loss: 4.08433243e-06
Iter: 432 loss: 4.08199867e-06
Iter: 433 loss: 4.07743755e-06
Iter: 434 loss: 4.16214243e-06
Iter: 435 loss: 4.07743391e-06
Iter: 436 loss: 4.07278549e-06
Iter: 437 loss: 4.14346869e-06
Iter: 438 loss: 4.0727582e-06
Iter: 439 loss: 4.0693858e-06
Iter: 440 loss: 4.07423522e-06
Iter: 441 loss: 4.06773961e-06
Iter: 442 loss: 4.06454183e-06
Iter: 443 loss: 4.06075105e-06
Iter: 444 loss: 4.06040226e-06
Iter: 445 loss: 4.0530349e-06
Iter: 446 loss: 4.10968278e-06
Iter: 447 loss: 4.05244873e-06
Iter: 448 loss: 4.04996717e-06
Iter: 449 loss: 4.05113587e-06
Iter: 450 loss: 4.04814818e-06
Iter: 451 loss: 4.04631737e-06
Iter: 452 loss: 4.04632738e-06
Iter: 453 loss: 4.04421189e-06
Iter: 454 loss: 4.03969807e-06
Iter: 455 loss: 4.10500888e-06
Iter: 456 loss: 4.03940521e-06
Iter: 457 loss: 4.03591275e-06
Iter: 458 loss: 4.05049423e-06
Iter: 459 loss: 4.03521517e-06
Iter: 460 loss: 4.03174e-06
Iter: 461 loss: 4.02721571e-06
Iter: 462 loss: 4.02690966e-06
Iter: 463 loss: 4.02277601e-06
Iter: 464 loss: 4.02230944e-06
Iter: 465 loss: 4.01908073e-06
Iter: 466 loss: 4.0476184e-06
Iter: 467 loss: 4.0188861e-06
Iter: 468 loss: 4.01735224e-06
Iter: 469 loss: 4.01474881e-06
Iter: 470 loss: 4.01474972e-06
Iter: 471 loss: 4.01113448e-06
Iter: 472 loss: 4.04590719e-06
Iter: 473 loss: 4.01099032e-06
Iter: 474 loss: 4.00897807e-06
Iter: 475 loss: 4.01079842e-06
Iter: 476 loss: 4.00767e-06
Iter: 477 loss: 4.0058103e-06
Iter: 478 loss: 4.01286388e-06
Iter: 479 loss: 4.00543831e-06
Iter: 480 loss: 4.00274e-06
Iter: 481 loss: 4.00460885e-06
Iter: 482 loss: 4.00109229e-06
Iter: 483 loss: 3.9989809e-06
Iter: 484 loss: 3.99875125e-06
Iter: 485 loss: 3.99713372e-06
Iter: 486 loss: 3.99397868e-06
Iter: 487 loss: 4.0269324e-06
Iter: 488 loss: 3.99376677e-06
Iter: 489 loss: 3.99071814e-06
Iter: 490 loss: 3.98417387e-06
Iter: 491 loss: 4.09006589e-06
Iter: 492 loss: 3.98388738e-06
Iter: 493 loss: 3.97870781e-06
Iter: 494 loss: 3.98815064e-06
Iter: 495 loss: 3.976465e-06
Iter: 496 loss: 3.97117492e-06
Iter: 497 loss: 4.00783256e-06
Iter: 498 loss: 3.97069152e-06
Iter: 499 loss: 3.96945325e-06
Iter: 500 loss: 3.96905125e-06
Iter: 501 loss: 3.96743962e-06
Iter: 502 loss: 3.96663836e-06
Iter: 503 loss: 3.96587302e-06
Iter: 504 loss: 3.96397036e-06
Iter: 505 loss: 3.97310077e-06
Iter: 506 loss: 3.96369e-06
Iter: 507 loss: 3.96189762e-06
Iter: 508 loss: 3.97011809e-06
Iter: 509 loss: 3.96163614e-06
Iter: 510 loss: 3.96021e-06
Iter: 511 loss: 3.95776533e-06
Iter: 512 loss: 3.95779534e-06
Iter: 513 loss: 3.95462121e-06
Iter: 514 loss: 3.9843153e-06
Iter: 515 loss: 3.95445431e-06
Iter: 516 loss: 3.95178586e-06
Iter: 517 loss: 3.95142342e-06
Iter: 518 loss: 3.94944345e-06
Iter: 519 loss: 3.94636845e-06
Iter: 520 loss: 3.94991366e-06
Iter: 521 loss: 3.94466224e-06
Iter: 522 loss: 3.94076869e-06
Iter: 523 loss: 3.98271231e-06
Iter: 524 loss: 3.94074e-06
Iter: 525 loss: 3.93849041e-06
Iter: 526 loss: 3.93661048e-06
Iter: 527 loss: 3.93605387e-06
Iter: 528 loss: 3.93372102e-06
Iter: 529 loss: 3.93213668e-06
Iter: 530 loss: 3.93121e-06
Iter: 531 loss: 3.92801758e-06
Iter: 532 loss: 3.96649466e-06
Iter: 533 loss: 3.92794709e-06
Iter: 534 loss: 3.9267652e-06
Iter: 535 loss: 3.92655875e-06
Iter: 536 loss: 3.92544962e-06
Iter: 537 loss: 3.92219226e-06
Iter: 538 loss: 3.94423796e-06
Iter: 539 loss: 3.92143374e-06
Iter: 540 loss: 3.91925823e-06
Iter: 541 loss: 3.91894127e-06
Iter: 542 loss: 3.91707272e-06
Iter: 543 loss: 3.91677304e-06
Iter: 544 loss: 3.91539743e-06
Iter: 545 loss: 3.91263802e-06
Iter: 546 loss: 3.9156962e-06
Iter: 547 loss: 3.91102731e-06
Iter: 548 loss: 3.90748619e-06
Iter: 549 loss: 3.93214e-06
Iter: 550 loss: 3.90709465e-06
Iter: 551 loss: 3.90518107e-06
Iter: 552 loss: 3.90685e-06
Iter: 553 loss: 3.90401055e-06
Iter: 554 loss: 3.90250352e-06
Iter: 555 loss: 3.91338108e-06
Iter: 556 loss: 3.90229525e-06
Iter: 557 loss: 3.90036712e-06
Iter: 558 loss: 3.89893466e-06
Iter: 559 loss: 3.89830257e-06
Iter: 560 loss: 3.89596107e-06
Iter: 561 loss: 3.90066452e-06
Iter: 562 loss: 3.89518073e-06
Iter: 563 loss: 3.89312117e-06
Iter: 564 loss: 3.88980243e-06
Iter: 565 loss: 3.88977833e-06
Iter: 566 loss: 3.88711487e-06
Iter: 567 loss: 3.88680928e-06
Iter: 568 loss: 3.8841813e-06
Iter: 569 loss: 3.89783872e-06
Iter: 570 loss: 3.8837552e-06
Iter: 571 loss: 3.88255921e-06
Iter: 572 loss: 3.88139551e-06
Iter: 573 loss: 3.88118406e-06
Iter: 574 loss: 3.87839282e-06
Iter: 575 loss: 3.89076422e-06
Iter: 576 loss: 3.87780801e-06
Iter: 577 loss: 3.8758194e-06
Iter: 578 loss: 3.87870887e-06
Iter: 579 loss: 3.87481487e-06
Iter: 580 loss: 3.87350246e-06
Iter: 581 loss: 3.89298111e-06
Iter: 582 loss: 3.8734147e-06
Iter: 583 loss: 3.8723183e-06
Iter: 584 loss: 3.8702633e-06
Iter: 585 loss: 3.91922413e-06
Iter: 586 loss: 3.87029422e-06
Iter: 587 loss: 3.86748798e-06
Iter: 588 loss: 3.87279852e-06
Iter: 589 loss: 3.8662929e-06
Iter: 590 loss: 3.86472766e-06
Iter: 591 loss: 3.86453939e-06
Iter: 592 loss: 3.86333477e-06
Iter: 593 loss: 3.85971362e-06
Iter: 594 loss: 3.87185855e-06
Iter: 595 loss: 3.8579783e-06
Iter: 596 loss: 3.85289786e-06
Iter: 597 loss: 3.89589559e-06
Iter: 598 loss: 3.85266912e-06
Iter: 599 loss: 3.8501953e-06
Iter: 600 loss: 3.85192152e-06
Iter: 601 loss: 3.84885925e-06
Iter: 602 loss: 3.84752457e-06
Iter: 603 loss: 3.84697159e-06
Iter: 604 loss: 3.84578743e-06
Iter: 605 loss: 3.84401255e-06
Iter: 606 loss: 3.8439639e-06
Iter: 607 loss: 3.84254417e-06
Iter: 608 loss: 3.84258556e-06
Iter: 609 loss: 3.84124769e-06
Iter: 610 loss: 3.83881297e-06
Iter: 611 loss: 3.8925391e-06
Iter: 612 loss: 3.83883344e-06
Iter: 613 loss: 3.83693032e-06
Iter: 614 loss: 3.83694896e-06
Iter: 615 loss: 3.8351227e-06
Iter: 616 loss: 3.83520273e-06
Iter: 617 loss: 3.83370025e-06
Iter: 618 loss: 3.83113638e-06
Iter: 619 loss: 3.83030601e-06
Iter: 620 loss: 3.82875533e-06
Iter: 621 loss: 3.82628605e-06
Iter: 622 loss: 3.82627331e-06
Iter: 623 loss: 3.82403596e-06
Iter: 624 loss: 3.82707185e-06
Iter: 625 loss: 3.82290091e-06
Iter: 626 loss: 3.82078633e-06
Iter: 627 loss: 3.81920063e-06
Iter: 628 loss: 3.8184221e-06
Iter: 629 loss: 3.81605787e-06
Iter: 630 loss: 3.82253756e-06
Iter: 631 loss: 3.81523932e-06
Iter: 632 loss: 3.81244263e-06
Iter: 633 loss: 3.81777591e-06
Iter: 634 loss: 3.81124983e-06
Iter: 635 loss: 3.80966117e-06
Iter: 636 loss: 3.80932261e-06
Iter: 637 loss: 3.80841971e-06
Iter: 638 loss: 3.80698657e-06
Iter: 639 loss: 3.83984479e-06
Iter: 640 loss: 3.80700749e-06
Iter: 641 loss: 3.80500501e-06
Iter: 642 loss: 3.8191065e-06
Iter: 643 loss: 3.80483357e-06
Iter: 644 loss: 3.80286656e-06
Iter: 645 loss: 3.79989388e-06
Iter: 646 loss: 3.79985909e-06
Iter: 647 loss: 3.79934818e-06
Iter: 648 loss: 3.7984978e-06
Iter: 649 loss: 3.79766198e-06
Iter: 650 loss: 3.79563e-06
Iter: 651 loss: 3.82594226e-06
Iter: 652 loss: 3.79556468e-06
Iter: 653 loss: 3.79293169e-06
Iter: 654 loss: 3.80549454e-06
Iter: 655 loss: 3.79243852e-06
Iter: 656 loss: 3.79069616e-06
Iter: 657 loss: 3.81348627e-06
Iter: 658 loss: 3.7906334e-06
Iter: 659 loss: 3.78948289e-06
Iter: 660 loss: 3.78828236e-06
Iter: 661 loss: 3.78799245e-06
Iter: 662 loss: 3.785638e-06
Iter: 663 loss: 3.78231516e-06
Iter: 664 loss: 3.78211075e-06
Iter: 665 loss: 3.77810966e-06
Iter: 666 loss: 3.79783114e-06
Iter: 667 loss: 3.77740457e-06
Iter: 668 loss: 3.77486e-06
Iter: 669 loss: 3.77487e-06
Iter: 670 loss: 3.7722225e-06
Iter: 671 loss: 3.77523156e-06
Iter: 672 loss: 3.77080119e-06
Iter: 673 loss: 3.76968592e-06
Iter: 674 loss: 3.77758488e-06
Iter: 675 loss: 3.76963158e-06
Iter: 676 loss: 3.7685013e-06
Iter: 677 loss: 3.76938624e-06
Iter: 678 loss: 3.76777962e-06
Iter: 679 loss: 3.76642902e-06
Iter: 680 loss: 3.76452726e-06
Iter: 681 loss: 3.76442e-06
Iter: 682 loss: 3.76252819e-06
Iter: 683 loss: 3.7623447e-06
Iter: 684 loss: 3.7616378e-06
Iter: 685 loss: 3.76061917e-06
Iter: 686 loss: 3.76049411e-06
Iter: 687 loss: 3.75879699e-06
Iter: 688 loss: 3.7642692e-06
Iter: 689 loss: 3.75825607e-06
Iter: 690 loss: 3.75584978e-06
Iter: 691 loss: 3.75691866e-06
Iter: 692 loss: 3.75422428e-06
Iter: 693 loss: 3.7517716e-06
Iter: 694 loss: 3.75806326e-06
Iter: 695 loss: 3.75086643e-06
Iter: 696 loss: 3.7488303e-06
Iter: 697 loss: 3.74809406e-06
Iter: 698 loss: 3.74695765e-06
Iter: 699 loss: 3.7449538e-06
Iter: 700 loss: 3.76203e-06
Iter: 701 loss: 3.74476849e-06
Iter: 702 loss: 3.74339879e-06
Iter: 703 loss: 3.74340425e-06
Iter: 704 loss: 3.74211049e-06
Iter: 705 loss: 3.73932198e-06
Iter: 706 loss: 3.77707238e-06
Iter: 707 loss: 3.73914258e-06
Iter: 708 loss: 3.73860416e-06
Iter: 709 loss: 3.73797252e-06
Iter: 710 loss: 3.73696685e-06
Iter: 711 loss: 3.73459534e-06
Iter: 712 loss: 3.7596551e-06
Iter: 713 loss: 3.73432886e-06
Iter: 714 loss: 3.73191915e-06
Iter: 715 loss: 3.7616278e-06
Iter: 716 loss: 3.73188232e-06
Iter: 717 loss: 3.72950512e-06
Iter: 718 loss: 3.73210264e-06
Iter: 719 loss: 3.72819159e-06
Iter: 720 loss: 3.72692284e-06
Iter: 721 loss: 3.73097237e-06
Iter: 722 loss: 3.726562e-06
Iter: 723 loss: 3.7249315e-06
Iter: 724 loss: 3.73117359e-06
Iter: 725 loss: 3.72463592e-06
Iter: 726 loss: 3.72320801e-06
Iter: 727 loss: 3.72284626e-06
Iter: 728 loss: 3.72190379e-06
Iter: 729 loss: 3.72032082e-06
Iter: 730 loss: 3.72467866e-06
Iter: 731 loss: 3.71984629e-06
Iter: 732 loss: 3.71815827e-06
Iter: 733 loss: 3.716e-06
Iter: 734 loss: 3.71584611e-06
Iter: 735 loss: 3.71404576e-06
Iter: 736 loss: 3.71393094e-06
Iter: 737 loss: 3.71203851e-06
Iter: 738 loss: 3.71836313e-06
Iter: 739 loss: 3.7115251e-06
Iter: 740 loss: 3.71019337e-06
Iter: 741 loss: 3.70802172e-06
Iter: 742 loss: 3.7080008e-06
Iter: 743 loss: 3.70670705e-06
Iter: 744 loss: 3.70628504e-06
Iter: 745 loss: 3.70557382e-06
Iter: 746 loss: 3.70383032e-06
Iter: 747 loss: 3.72454224e-06
Iter: 748 loss: 3.70370026e-06
Iter: 749 loss: 3.70253497e-06
Iter: 750 loss: 3.70233442e-06
Iter: 751 loss: 3.70141242e-06
Iter: 752 loss: 3.70008e-06
Iter: 753 loss: 3.70005205e-06
Iter: 754 loss: 3.69863938e-06
Iter: 755 loss: 3.7123209e-06
Iter: 756 loss: 3.69868076e-06
Iter: 757 loss: 3.69715144e-06
Iter: 758 loss: 3.69588e-06
Iter: 759 loss: 3.695543e-06
Iter: 760 loss: 3.69355485e-06
Iter: 761 loss: 3.70064276e-06
Iter: 762 loss: 3.69319514e-06
Iter: 763 loss: 3.69127338e-06
Iter: 764 loss: 3.68860833e-06
Iter: 765 loss: 3.68847532e-06
Iter: 766 loss: 3.68490737e-06
Iter: 767 loss: 3.70967246e-06
Iter: 768 loss: 3.68456017e-06
Iter: 769 loss: 3.683572e-06
Iter: 770 loss: 3.68357e-06
Iter: 771 loss: 3.68213705e-06
Iter: 772 loss: 3.68139217e-06
Iter: 773 loss: 3.68085102e-06
Iter: 774 loss: 3.67914822e-06
Iter: 775 loss: 3.68233032e-06
Iter: 776 loss: 3.67848156e-06
Iter: 777 loss: 3.6770964e-06
Iter: 778 loss: 3.67712573e-06
Iter: 779 loss: 3.67645816e-06
Iter: 780 loss: 3.67462894e-06
Iter: 781 loss: 3.68760584e-06
Iter: 782 loss: 3.67420012e-06
Iter: 783 loss: 3.67237362e-06
Iter: 784 loss: 3.67238545e-06
Iter: 785 loss: 3.67111898e-06
Iter: 786 loss: 3.66948257e-06
Iter: 787 loss: 3.66930362e-06
Iter: 788 loss: 3.66751328e-06
Iter: 789 loss: 3.68684118e-06
Iter: 790 loss: 3.66747236e-06
Iter: 791 loss: 3.66556719e-06
Iter: 792 loss: 3.66332415e-06
Iter: 793 loss: 3.662979e-06
Iter: 794 loss: 3.66101881e-06
Iter: 795 loss: 3.67358e-06
Iter: 796 loss: 3.66089444e-06
Iter: 797 loss: 3.65928599e-06
Iter: 798 loss: 3.66019231e-06
Iter: 799 loss: 3.65832352e-06
Iter: 800 loss: 3.65669507e-06
Iter: 801 loss: 3.6612555e-06
Iter: 802 loss: 3.65616688e-06
Iter: 803 loss: 3.65459937e-06
Iter: 804 loss: 3.67011216e-06
Iter: 805 loss: 3.65450364e-06
Iter: 806 loss: 3.65257665e-06
Iter: 807 loss: 3.65153255e-06
Iter: 808 loss: 3.65061442e-06
Iter: 809 loss: 3.6491615e-06
Iter: 810 loss: 3.65258438e-06
Iter: 811 loss: 3.64866446e-06
Iter: 812 loss: 3.6465e-06
Iter: 813 loss: 3.65620372e-06
Iter: 814 loss: 3.64610514e-06
Iter: 815 loss: 3.6445208e-06
Iter: 816 loss: 3.64273774e-06
Iter: 817 loss: 3.64253e-06
Iter: 818 loss: 3.64130869e-06
Iter: 819 loss: 3.64102652e-06
Iter: 820 loss: 3.63982622e-06
Iter: 821 loss: 3.63763957e-06
Iter: 822 loss: 3.69052987e-06
Iter: 823 loss: 3.63762229e-06
Iter: 824 loss: 3.63654431e-06
Iter: 825 loss: 3.63650406e-06
Iter: 826 loss: 3.63549498e-06
Iter: 827 loss: 3.63569552e-06
Iter: 828 loss: 3.63470713e-06
Iter: 829 loss: 3.63351705e-06
Iter: 830 loss: 3.63147365e-06
Iter: 831 loss: 3.6806332e-06
Iter: 832 loss: 3.63145728e-06
Iter: 833 loss: 3.62879314e-06
Iter: 834 loss: 3.64848097e-06
Iter: 835 loss: 3.62867536e-06
Iter: 836 loss: 3.62646e-06
Iter: 837 loss: 3.62685523e-06
Iter: 838 loss: 3.62484866e-06
Iter: 839 loss: 3.62287074e-06
Iter: 840 loss: 3.62283481e-06
Iter: 841 loss: 3.62102355e-06
Iter: 842 loss: 3.62048058e-06
Iter: 843 loss: 3.61950333e-06
Iter: 844 loss: 3.61834645e-06
Iter: 845 loss: 3.62480705e-06
Iter: 846 loss: 3.61820184e-06
Iter: 847 loss: 3.61671027e-06
Iter: 848 loss: 3.6182505e-06
Iter: 849 loss: 3.6159804e-06
Iter: 850 loss: 3.61482512e-06
Iter: 851 loss: 3.61566663e-06
Iter: 852 loss: 3.61410639e-06
Iter: 853 loss: 3.613123e-06
Iter: 854 loss: 3.6131446e-06
Iter: 855 loss: 3.61240313e-06
Iter: 856 loss: 3.6101294e-06
Iter: 857 loss: 3.61856792e-06
Iter: 858 loss: 3.60916965e-06
Iter: 859 loss: 3.60667536e-06
Iter: 860 loss: 3.63794834e-06
Iter: 861 loss: 3.60669173e-06
Iter: 862 loss: 3.60492459e-06
Iter: 863 loss: 3.62843844e-06
Iter: 864 loss: 3.60491322e-06
Iter: 865 loss: 3.60367267e-06
Iter: 866 loss: 3.60067361e-06
Iter: 867 loss: 3.63092613e-06
Iter: 868 loss: 3.60032641e-06
Iter: 869 loss: 3.59781711e-06
Iter: 870 loss: 3.62010292e-06
Iter: 871 loss: 3.59769319e-06
Iter: 872 loss: 3.59597789e-06
Iter: 873 loss: 3.5992266e-06
Iter: 874 loss: 3.59528713e-06
Iter: 875 loss: 3.59378396e-06
Iter: 876 loss: 3.6151423e-06
Iter: 877 loss: 3.59382602e-06
Iter: 878 loss: 3.59251271e-06
Iter: 879 loss: 3.59835826e-06
Iter: 880 loss: 3.59224919e-06
Iter: 881 loss: 3.59126602e-06
Iter: 882 loss: 3.58884927e-06
Iter: 883 loss: 3.61691286e-06
Iter: 884 loss: 3.58863235e-06
Iter: 885 loss: 3.58680131e-06
Iter: 886 loss: 3.58679335e-06
Iter: 887 loss: 3.58509919e-06
Iter: 888 loss: 3.58998022e-06
Iter: 889 loss: 3.58466082e-06
Iter: 890 loss: 3.58363968e-06
Iter: 891 loss: 3.58175066e-06
Iter: 892 loss: 3.58182319e-06
Iter: 893 loss: 3.58114949e-06
Iter: 894 loss: 3.58067837e-06
Iter: 895 loss: 3.57995191e-06
Iter: 896 loss: 3.57832255e-06
Iter: 897 loss: 3.61245839e-06
Iter: 898 loss: 3.57831414e-06
Iter: 899 loss: 3.57697354e-06
Iter: 900 loss: 3.59352543e-06
Iter: 901 loss: 3.57699196e-06
Iter: 902 loss: 3.57558656e-06
Iter: 903 loss: 3.57693489e-06
Iter: 904 loss: 3.57486169e-06
Iter: 905 loss: 3.57395925e-06
Iter: 906 loss: 3.57184399e-06
Iter: 907 loss: 3.60236072e-06
Iter: 908 loss: 3.57176305e-06
Iter: 909 loss: 3.56879718e-06
Iter: 910 loss: 3.58222906e-06
Iter: 911 loss: 3.56826467e-06
Iter: 912 loss: 3.56614555e-06
Iter: 913 loss: 3.57684326e-06
Iter: 914 loss: 3.56585701e-06
Iter: 915 loss: 3.56439523e-06
Iter: 916 loss: 3.56441205e-06
Iter: 917 loss: 3.56310829e-06
Iter: 918 loss: 3.56252826e-06
Iter: 919 loss: 3.56194619e-06
Iter: 920 loss: 3.56072132e-06
Iter: 921 loss: 3.56311853e-06
Iter: 922 loss: 3.56024543e-06
Iter: 923 loss: 3.5589851e-06
Iter: 924 loss: 3.56300598e-06
Iter: 925 loss: 3.55854286e-06
Iter: 926 loss: 3.55685029e-06
Iter: 927 loss: 3.56282135e-06
Iter: 928 loss: 3.55633347e-06
Iter: 929 loss: 3.55539e-06
Iter: 930 loss: 3.55416023e-06
Iter: 931 loss: 3.55399266e-06
Iter: 932 loss: 3.55246311e-06
Iter: 933 loss: 3.57578961e-06
Iter: 934 loss: 3.55246516e-06
Iter: 935 loss: 3.55090106e-06
Iter: 936 loss: 3.54886492e-06
Iter: 937 loss: 3.54876988e-06
Iter: 938 loss: 3.54747181e-06
Iter: 939 loss: 3.56725059e-06
Iter: 940 loss: 3.5473895e-06
Iter: 941 loss: 3.54603344e-06
Iter: 942 loss: 3.54708163e-06
Iter: 943 loss: 3.54519329e-06
Iter: 944 loss: 3.54388021e-06
Iter: 945 loss: 3.54310691e-06
Iter: 946 loss: 3.54248823e-06
Iter: 947 loss: 3.54090912e-06
Iter: 948 loss: 3.54215126e-06
Iter: 949 loss: 3.53998144e-06
Iter: 950 loss: 3.53850214e-06
Iter: 951 loss: 3.53846e-06
Iter: 952 loss: 3.53710357e-06
Iter: 953 loss: 3.53875203e-06
Iter: 954 loss: 3.53626683e-06
Iter: 955 loss: 3.53522842e-06
Iter: 956 loss: 3.53344512e-06
Iter: 957 loss: 3.57765475e-06
Iter: 958 loss: 3.53338874e-06
Iter: 959 loss: 3.53109363e-06
Iter: 960 loss: 3.55118527e-06
Iter: 961 loss: 3.53087489e-06
Iter: 962 loss: 3.52952702e-06
Iter: 963 loss: 3.52956363e-06
Iter: 964 loss: 3.52885422e-06
Iter: 965 loss: 3.52787129e-06
Iter: 966 loss: 3.52778125e-06
Iter: 967 loss: 3.52626193e-06
Iter: 968 loss: 3.52752136e-06
Iter: 969 loss: 3.52525058e-06
Iter: 970 loss: 3.52440475e-06
Iter: 971 loss: 3.52417283e-06
Iter: 972 loss: 3.52375127e-06
Iter: 973 loss: 3.52241568e-06
Iter: 974 loss: 3.53108317e-06
Iter: 975 loss: 3.52195252e-06
Iter: 976 loss: 3.52006896e-06
Iter: 977 loss: 3.53317409e-06
Iter: 978 loss: 3.51981362e-06
Iter: 979 loss: 3.51760445e-06
Iter: 980 loss: 3.52517441e-06
Iter: 981 loss: 3.51702e-06
Iter: 982 loss: 3.51581025e-06
Iter: 983 loss: 3.51313383e-06
Iter: 984 loss: 3.55305383e-06
Iter: 985 loss: 3.51296558e-06
Iter: 986 loss: 3.50958749e-06
Iter: 987 loss: 3.52777738e-06
Iter: 988 loss: 3.50913069e-06
Iter: 989 loss: 3.50870346e-06
Iter: 990 loss: 3.50803566e-06
Iter: 991 loss: 3.5070052e-06
Iter: 992 loss: 3.50478808e-06
Iter: 993 loss: 3.5407802e-06
Iter: 994 loss: 3.50474602e-06
Iter: 995 loss: 3.5031635e-06
Iter: 996 loss: 3.5121875e-06
Iter: 997 loss: 3.50295886e-06
Iter: 998 loss: 3.50216487e-06
Iter: 999 loss: 3.50217283e-06
Iter: 1000 loss: 3.5015189e-06
Iter: 1001 loss: 3.50010373e-06
Iter: 1002 loss: 3.52526331e-06
Iter: 1003 loss: 3.50009532e-06
Iter: 1004 loss: 3.49809261e-06
Iter: 1005 loss: 3.50083064e-06
Iter: 1006 loss: 3.49713605e-06
Iter: 1007 loss: 3.49559468e-06
Iter: 1008 loss: 3.51030258e-06
Iter: 1009 loss: 3.49558763e-06
Iter: 1010 loss: 3.49418087e-06
Iter: 1011 loss: 3.49800757e-06
Iter: 1012 loss: 3.49366906e-06
Iter: 1013 loss: 3.49222273e-06
Iter: 1014 loss: 3.48938374e-06
Iter: 1015 loss: 3.54701842e-06
Iter: 1016 loss: 3.4893967e-06
Iter: 1017 loss: 3.48892399e-06
Iter: 1018 loss: 3.4882878e-06
Iter: 1019 loss: 3.48727735e-06
Iter: 1020 loss: 3.48633193e-06
Iter: 1021 loss: 3.48595768e-06
Iter: 1022 loss: 3.48434696e-06
Iter: 1023 loss: 3.48502203e-06
Iter: 1024 loss: 3.48329036e-06
Iter: 1025 loss: 3.4824759e-06
Iter: 1026 loss: 3.48240519e-06
Iter: 1027 loss: 3.48131289e-06
Iter: 1028 loss: 3.47955097e-06
Iter: 1029 loss: 3.47951482e-06
Iter: 1030 loss: 3.47756077e-06
Iter: 1031 loss: 3.47816604e-06
Iter: 1032 loss: 3.47619925e-06
Iter: 1033 loss: 3.47457535e-06
Iter: 1034 loss: 3.47454079e-06
Iter: 1035 loss: 3.47288324e-06
Iter: 1036 loss: 3.4734453e-06
Iter: 1037 loss: 3.47152536e-06
Iter: 1038 loss: 3.47010382e-06
Iter: 1039 loss: 3.47382615e-06
Iter: 1040 loss: 3.46964771e-06
Iter: 1041 loss: 3.46839943e-06
Iter: 1042 loss: 3.47049081e-06
Iter: 1043 loss: 3.46779029e-06
Iter: 1044 loss: 3.46662705e-06
Iter: 1045 loss: 3.4817e-06
Iter: 1046 loss: 3.4666341e-06
Iter: 1047 loss: 3.46561296e-06
Iter: 1048 loss: 3.4650318e-06
Iter: 1049 loss: 3.46457682e-06
Iter: 1050 loss: 3.46326328e-06
Iter: 1051 loss: 3.4629395e-06
Iter: 1052 loss: 3.4621969e-06
Iter: 1053 loss: 3.46052e-06
Iter: 1054 loss: 3.46048796e-06
Iter: 1055 loss: 3.45941498e-06
Iter: 1056 loss: 3.45750914e-06
Iter: 1057 loss: 3.45756348e-06
Iter: 1058 loss: 3.45562876e-06
Iter: 1059 loss: 3.46052184e-06
Iter: 1060 loss: 3.45499143e-06
Iter: 1061 loss: 3.45274429e-06
Iter: 1062 loss: 3.47861487e-06
Iter: 1063 loss: 3.45267631e-06
Iter: 1064 loss: 3.45178705e-06
Iter: 1065 loss: 3.44995965e-06
Iter: 1066 loss: 3.49054e-06
Iter: 1067 loss: 3.45000581e-06
Iter: 1068 loss: 3.44828595e-06
Iter: 1069 loss: 3.45322087e-06
Iter: 1070 loss: 3.44769182e-06
Iter: 1071 loss: 3.44757359e-06
Iter: 1072 loss: 3.44697128e-06
Iter: 1073 loss: 3.44653336e-06
Iter: 1074 loss: 3.44523323e-06
Iter: 1075 loss: 3.45238414e-06
Iter: 1076 loss: 3.44489808e-06
Iter: 1077 loss: 3.44271257e-06
Iter: 1078 loss: 3.4473278e-06
Iter: 1079 loss: 3.44192699e-06
Iter: 1080 loss: 3.44028945e-06
Iter: 1081 loss: 3.44037085e-06
Iter: 1082 loss: 3.43914508e-06
Iter: 1083 loss: 3.44201044e-06
Iter: 1084 loss: 3.43870715e-06
Iter: 1085 loss: 3.43737565e-06
Iter: 1086 loss: 3.43485317e-06
Iter: 1087 loss: 3.48435015e-06
Iter: 1088 loss: 3.43482748e-06
Iter: 1089 loss: 3.43438978e-06
Iter: 1090 loss: 3.43357897e-06
Iter: 1091 loss: 3.43264719e-06
Iter: 1092 loss: 3.43291276e-06
Iter: 1093 loss: 3.43209035e-06
Iter: 1094 loss: 3.43114584e-06
Iter: 1095 loss: 3.42941394e-06
Iter: 1096 loss: 3.46849174e-06
Iter: 1097 loss: 3.42945941e-06
Iter: 1098 loss: 3.42928479e-06
Iter: 1099 loss: 3.428328e-06
Iter: 1100 loss: 3.42779185e-06
Iter: 1101 loss: 3.42645262e-06
Iter: 1102 loss: 3.44702175e-06
Iter: 1103 loss: 3.42644239e-06
Iter: 1104 loss: 3.424851e-06
Iter: 1105 loss: 3.42526346e-06
Iter: 1106 loss: 3.42372823e-06
Iter: 1107 loss: 3.42187968e-06
Iter: 1108 loss: 3.42174189e-06
Iter: 1109 loss: 3.42099133e-06
Iter: 1110 loss: 3.41944519e-06
Iter: 1111 loss: 3.45391959e-06
Iter: 1112 loss: 3.41941654e-06
Iter: 1113 loss: 3.41735881e-06
Iter: 1114 loss: 3.42224666e-06
Iter: 1115 loss: 3.41668465e-06
Iter: 1116 loss: 3.41617624e-06
Iter: 1117 loss: 3.4157697e-06
Iter: 1118 loss: 3.41522582e-06
Iter: 1119 loss: 3.41459668e-06
Iter: 1120 loss: 3.41444365e-06
Iter: 1121 loss: 3.41325585e-06
Iter: 1122 loss: 3.41191799e-06
Iter: 1123 loss: 3.41177065e-06
Iter: 1124 loss: 3.41084842e-06
Iter: 1125 loss: 3.41056e-06
Iter: 1126 loss: 3.40970973e-06
Iter: 1127 loss: 3.40850784e-06
Iter: 1128 loss: 3.40847e-06
Iter: 1129 loss: 3.4068089e-06
Iter: 1130 loss: 3.40574229e-06
Iter: 1131 loss: 3.40501265e-06
Iter: 1132 loss: 3.40402198e-06
Iter: 1133 loss: 3.40339329e-06
Iter: 1134 loss: 3.40272368e-06
Iter: 1135 loss: 3.40089196e-06
Iter: 1136 loss: 3.41718919e-06
Iter: 1137 loss: 3.40054544e-06
Iter: 1138 loss: 3.3984993e-06
Iter: 1139 loss: 3.41111172e-06
Iter: 1140 loss: 3.39823282e-06
Iter: 1141 loss: 3.39684766e-06
Iter: 1142 loss: 3.39684834e-06
Iter: 1143 loss: 3.39634926e-06
Iter: 1144 loss: 3.3951e-06
Iter: 1145 loss: 3.40933116e-06
Iter: 1146 loss: 3.39492158e-06
Iter: 1147 loss: 3.39317558e-06
Iter: 1148 loss: 3.39473149e-06
Iter: 1149 loss: 3.39209032e-06
Iter: 1150 loss: 3.39034318e-06
Iter: 1151 loss: 3.39037456e-06
Iter: 1152 loss: 3.38917425e-06
Iter: 1153 loss: 3.38835935e-06
Iter: 1154 loss: 3.38792643e-06
Iter: 1155 loss: 3.38612017e-06
Iter: 1156 loss: 3.38762652e-06
Iter: 1157 loss: 3.38490759e-06
Iter: 1158 loss: 3.38369182e-06
Iter: 1159 loss: 3.38359246e-06
Iter: 1160 loss: 3.38273094e-06
Iter: 1161 loss: 3.38168502e-06
Iter: 1162 loss: 3.3815902e-06
Iter: 1163 loss: 3.38012842e-06
Iter: 1164 loss: 3.38187579e-06
Iter: 1165 loss: 3.3793649e-06
Iter: 1166 loss: 3.37789561e-06
Iter: 1167 loss: 3.37788561e-06
Iter: 1168 loss: 3.37725987e-06
Iter: 1169 loss: 3.3760125e-06
Iter: 1170 loss: 3.39256326e-06
Iter: 1171 loss: 3.37601045e-06
Iter: 1172 loss: 3.37422034e-06
Iter: 1173 loss: 3.37853567e-06
Iter: 1174 loss: 3.37354913e-06
Iter: 1175 loss: 3.37100346e-06
Iter: 1176 loss: 3.38285486e-06
Iter: 1177 loss: 3.37055826e-06
Iter: 1178 loss: 3.3693259e-06
Iter: 1179 loss: 3.36805556e-06
Iter: 1180 loss: 3.36780704e-06
Iter: 1181 loss: 3.36615176e-06
Iter: 1182 loss: 3.38016207e-06
Iter: 1183 loss: 3.3660358e-06
Iter: 1184 loss: 3.36455378e-06
Iter: 1185 loss: 3.37514166e-06
Iter: 1186 loss: 3.36447488e-06
Iter: 1187 loss: 3.36365974e-06
Iter: 1188 loss: 3.36302401e-06
Iter: 1189 loss: 3.3627648e-06
Iter: 1190 loss: 3.36137373e-06
Iter: 1191 loss: 3.36666108e-06
Iter: 1192 loss: 3.36116204e-06
Iter: 1193 loss: 3.35974687e-06
Iter: 1194 loss: 3.36918129e-06
Iter: 1195 loss: 3.35961954e-06
Iter: 1196 loss: 3.35873e-06
Iter: 1197 loss: 3.35716481e-06
Iter: 1198 loss: 3.35711479e-06
Iter: 1199 loss: 3.3553838e-06
Iter: 1200 loss: 3.37130768e-06
Iter: 1201 loss: 3.3553606e-06
Iter: 1202 loss: 3.35342315e-06
Iter: 1203 loss: 3.35673758e-06
Iter: 1204 loss: 3.35253571e-06
Iter: 1205 loss: 3.35134609e-06
Iter: 1206 loss: 3.34979632e-06
Iter: 1207 loss: 3.3496749e-06
Iter: 1208 loss: 3.34844935e-06
Iter: 1209 loss: 3.34838933e-06
Iter: 1210 loss: 3.34715196e-06
Iter: 1211 loss: 3.34877359e-06
Iter: 1212 loss: 3.34659899e-06
Iter: 1213 loss: 3.34576771e-06
Iter: 1214 loss: 3.34412834e-06
Iter: 1215 loss: 3.37859683e-06
Iter: 1216 loss: 3.34415381e-06
Iter: 1217 loss: 3.34299057e-06
Iter: 1218 loss: 3.34277911e-06
Iter: 1219 loss: 3.34187507e-06
Iter: 1220 loss: 3.34245283e-06
Iter: 1221 loss: 3.3412814e-06
Iter: 1222 loss: 3.34024753e-06
Iter: 1223 loss: 3.33933781e-06
Iter: 1224 loss: 3.33901494e-06
Iter: 1225 loss: 3.33703656e-06
Iter: 1226 loss: 3.35129903e-06
Iter: 1227 loss: 3.33679168e-06
Iter: 1228 loss: 3.33519915e-06
Iter: 1229 loss: 3.33890375e-06
Iter: 1230 loss: 3.33461458e-06
Iter: 1231 loss: 3.33335265e-06
Iter: 1232 loss: 3.33326079e-06
Iter: 1233 loss: 3.33236494e-06
Iter: 1234 loss: 3.33096614e-06
Iter: 1235 loss: 3.33097341e-06
Iter: 1236 loss: 3.32995705e-06
Iter: 1237 loss: 3.32812533e-06
Iter: 1238 loss: 3.32816e-06
Iter: 1239 loss: 3.32679565e-06
Iter: 1240 loss: 3.32979289e-06
Iter: 1241 loss: 3.32620721e-06
Iter: 1242 loss: 3.32511763e-06
Iter: 1243 loss: 3.32508739e-06
Iter: 1244 loss: 3.3244296e-06
Iter: 1245 loss: 3.32351283e-06
Iter: 1246 loss: 3.32346758e-06
Iter: 1247 loss: 3.32229956e-06
Iter: 1248 loss: 3.32306263e-06
Iter: 1249 loss: 3.32154605e-06
Iter: 1250 loss: 3.31938645e-06
Iter: 1251 loss: 3.32673449e-06
Iter: 1252 loss: 3.31885167e-06
Iter: 1253 loss: 3.31719389e-06
Iter: 1254 loss: 3.32023592e-06
Iter: 1255 loss: 3.31655269e-06
Iter: 1256 loss: 3.3155261e-06
Iter: 1257 loss: 3.32386458e-06
Iter: 1258 loss: 3.31536398e-06
Iter: 1259 loss: 3.31422484e-06
Iter: 1260 loss: 3.3141107e-06
Iter: 1261 loss: 3.31330466e-06
Iter: 1262 loss: 3.31158708e-06
Iter: 1263 loss: 3.31429123e-06
Iter: 1264 loss: 3.31074966e-06
Iter: 1265 loss: 3.31003866e-06
Iter: 1266 loss: 3.31004e-06
Iter: 1267 loss: 3.30927401e-06
Iter: 1268 loss: 3.30852572e-06
Iter: 1269 loss: 3.30836156e-06
Iter: 1270 loss: 3.30722014e-06
Iter: 1271 loss: 3.30612511e-06
Iter: 1272 loss: 3.30574267e-06
Iter: 1273 loss: 3.30518105e-06
Iter: 1274 loss: 3.3049987e-06
Iter: 1275 loss: 3.3040767e-06
Iter: 1276 loss: 3.302604e-06
Iter: 1277 loss: 3.30256626e-06
Iter: 1278 loss: 3.30074431e-06
Iter: 1279 loss: 3.30251851e-06
Iter: 1280 loss: 3.29965178e-06
Iter: 1281 loss: 3.29828526e-06
Iter: 1282 loss: 3.29825934e-06
Iter: 1283 loss: 3.29721661e-06
Iter: 1284 loss: 3.29597833e-06
Iter: 1285 loss: 3.29581576e-06
Iter: 1286 loss: 3.29454178e-06
Iter: 1287 loss: 3.30648527e-06
Iter: 1288 loss: 3.29438694e-06
Iter: 1289 loss: 3.29349541e-06
Iter: 1290 loss: 3.2990215e-06
Iter: 1291 loss: 3.29336899e-06
Iter: 1292 loss: 3.29252771e-06
Iter: 1293 loss: 3.29173304e-06
Iter: 1294 loss: 3.29160957e-06
Iter: 1295 loss: 3.29018712e-06
Iter: 1296 loss: 3.29584987e-06
Iter: 1297 loss: 3.28990154e-06
Iter: 1298 loss: 3.28872784e-06
Iter: 1299 loss: 3.30380067e-06
Iter: 1300 loss: 3.2887051e-06
Iter: 1301 loss: 3.28806459e-06
Iter: 1302 loss: 3.28616852e-06
Iter: 1303 loss: 3.29771092e-06
Iter: 1304 loss: 3.28568012e-06
Iter: 1305 loss: 3.28360329e-06
Iter: 1306 loss: 3.30577768e-06
Iter: 1307 loss: 3.283468e-06
Iter: 1308 loss: 3.28194028e-06
Iter: 1309 loss: 3.29987643e-06
Iter: 1310 loss: 3.2818989e-06
Iter: 1311 loss: 3.28099213e-06
Iter: 1312 loss: 3.27918701e-06
Iter: 1313 loss: 3.308533e-06
Iter: 1314 loss: 3.27913699e-06
Iter: 1315 loss: 3.27837483e-06
Iter: 1316 loss: 3.27814723e-06
Iter: 1317 loss: 3.2773255e-06
Iter: 1318 loss: 3.27806629e-06
Iter: 1319 loss: 3.27681187e-06
Iter: 1320 loss: 3.27606017e-06
Iter: 1321 loss: 3.27536986e-06
Iter: 1322 loss: 3.27510952e-06
Iter: 1323 loss: 3.27364501e-06
Iter: 1324 loss: 3.28644182e-06
Iter: 1325 loss: 3.27362363e-06
Iter: 1326 loss: 3.27247267e-06
Iter: 1327 loss: 3.27289445e-06
Iter: 1328 loss: 3.2717046e-06
Iter: 1329 loss: 3.27031421e-06
Iter: 1330 loss: 3.27063162e-06
Iter: 1331 loss: 3.26927466e-06
Iter: 1332 loss: 3.26769054e-06
Iter: 1333 loss: 3.29368822e-06
Iter: 1334 loss: 3.26763438e-06
Iter: 1335 loss: 3.2663504e-06
Iter: 1336 loss: 3.26539703e-06
Iter: 1337 loss: 3.2650305e-06
Iter: 1338 loss: 3.26375402e-06
Iter: 1339 loss: 3.26364534e-06
Iter: 1340 loss: 3.26273766e-06
Iter: 1341 loss: 3.26142163e-06
Iter: 1342 loss: 3.26139889e-06
Iter: 1343 loss: 3.26052054e-06
Iter: 1344 loss: 3.25965334e-06
Iter: 1345 loss: 3.2595608e-06
Iter: 1346 loss: 3.25851238e-06
Iter: 1347 loss: 3.26125155e-06
Iter: 1348 loss: 3.2581579e-06
Iter: 1349 loss: 3.25665451e-06
Iter: 1350 loss: 3.2595608e-06
Iter: 1351 loss: 3.25596056e-06
Iter: 1352 loss: 3.25451401e-06
Iter: 1353 loss: 3.25510928e-06
Iter: 1354 loss: 3.25349129e-06
Iter: 1355 loss: 3.25214637e-06
Iter: 1356 loss: 3.26978466e-06
Iter: 1357 loss: 3.25209976e-06
Iter: 1358 loss: 3.25089195e-06
Iter: 1359 loss: 3.24988423e-06
Iter: 1360 loss: 3.24955704e-06
Iter: 1361 loss: 3.24759799e-06
Iter: 1362 loss: 3.25300016e-06
Iter: 1363 loss: 3.24704547e-06
Iter: 1364 loss: 3.24596408e-06
Iter: 1365 loss: 3.24591792e-06
Iter: 1366 loss: 3.2448595e-06
Iter: 1367 loss: 3.24422012e-06
Iter: 1368 loss: 3.24379607e-06
Iter: 1369 loss: 3.24258258e-06
Iter: 1370 loss: 3.24323582e-06
Iter: 1371 loss: 3.24179973e-06
Iter: 1372 loss: 3.24084294e-06
Iter: 1373 loss: 3.2408459e-06
Iter: 1374 loss: 3.23972631e-06
Iter: 1375 loss: 3.23836184e-06
Iter: 1376 loss: 3.23827862e-06
Iter: 1377 loss: 3.23672043e-06
Iter: 1378 loss: 3.23926588e-06
Iter: 1379 loss: 3.23594782e-06
Iter: 1380 loss: 3.23428571e-06
Iter: 1381 loss: 3.25726091e-06
Iter: 1382 loss: 3.23425047e-06
Iter: 1383 loss: 3.23298696e-06
Iter: 1384 loss: 3.23074914e-06
Iter: 1385 loss: 3.23067275e-06
Iter: 1386 loss: 3.22948472e-06
Iter: 1387 loss: 3.2294115e-06
Iter: 1388 loss: 3.22835785e-06
Iter: 1389 loss: 3.22993446e-06
Iter: 1390 loss: 3.22785809e-06
Iter: 1391 loss: 3.22669439e-06
Iter: 1392 loss: 3.22615415e-06
Iter: 1393 loss: 3.22565211e-06
Iter: 1394 loss: 3.22448705e-06
Iter: 1395 loss: 3.24302073e-06
Iter: 1396 loss: 3.2244659e-06
Iter: 1397 loss: 3.22356027e-06
Iter: 1398 loss: 3.22770529e-06
Iter: 1399 loss: 3.22344113e-06
Iter: 1400 loss: 3.22269693e-06
Iter: 1401 loss: 3.22092319e-06
Iter: 1402 loss: 3.23611107e-06
Iter: 1403 loss: 3.22060328e-06
Iter: 1404 loss: 3.21862899e-06
Iter: 1405 loss: 3.24006623e-06
Iter: 1406 loss: 3.21860307e-06
Iter: 1407 loss: 3.21689595e-06
Iter: 1408 loss: 3.2305984e-06
Iter: 1409 loss: 3.21677862e-06
Iter: 1410 loss: 3.21580251e-06
Iter: 1411 loss: 3.2132275e-06
Iter: 1412 loss: 3.24052144e-06
Iter: 1413 loss: 3.21291418e-06
Iter: 1414 loss: 3.21268135e-06
Iter: 1415 loss: 3.21181369e-06
Iter: 1416 loss: 3.21079779e-06
Iter: 1417 loss: 3.21047037e-06
Iter: 1418 loss: 3.20990875e-06
Iter: 1419 loss: 3.2087928e-06
Iter: 1420 loss: 3.2098485e-06
Iter: 1421 loss: 3.20815639e-06
Iter: 1422 loss: 3.20661684e-06
Iter: 1423 loss: 3.22256301e-06
Iter: 1424 loss: 3.20656363e-06
Iter: 1425 loss: 3.20579852e-06
Iter: 1426 loss: 3.20555318e-06
Iter: 1427 loss: 3.20506e-06
Iter: 1428 loss: 3.20387267e-06
Iter: 1429 loss: 3.20603613e-06
Iter: 1430 loss: 3.20339632e-06
Iter: 1431 loss: 3.20188155e-06
Iter: 1432 loss: 3.21286916e-06
Iter: 1433 loss: 3.20178196e-06
Iter: 1434 loss: 3.20062532e-06
Iter: 1435 loss: 3.19938454e-06
Iter: 1436 loss: 3.19919445e-06
Iter: 1437 loss: 3.19759147e-06
Iter: 1438 loss: 3.19912647e-06
Iter: 1439 loss: 3.19672881e-06
Iter: 1440 loss: 3.1956431e-06
Iter: 1441 loss: 3.19547598e-06
Iter: 1442 loss: 3.1946845e-06
Iter: 1443 loss: 3.19334822e-06
Iter: 1444 loss: 3.19340074e-06
Iter: 1445 loss: 3.19214405e-06
Iter: 1446 loss: 3.19894343e-06
Iter: 1447 loss: 3.19185324e-06
Iter: 1448 loss: 3.1905895e-06
Iter: 1449 loss: 3.19823448e-06
Iter: 1450 loss: 3.19034575e-06
Iter: 1451 loss: 3.18954699e-06
Iter: 1452 loss: 3.18872958e-06
Iter: 1453 loss: 3.18857496e-06
Iter: 1454 loss: 3.18727098e-06
Iter: 1455 loss: 3.20245294e-06
Iter: 1456 loss: 3.18728e-06
Iter: 1457 loss: 3.1861473e-06
Iter: 1458 loss: 3.18561911e-06
Iter: 1459 loss: 3.18508933e-06
Iter: 1460 loss: 3.18385537e-06
Iter: 1461 loss: 3.18768411e-06
Iter: 1462 loss: 3.18346315e-06
Iter: 1463 loss: 3.18218235e-06
Iter: 1464 loss: 3.19268884e-06
Iter: 1465 loss: 3.18206548e-06
Iter: 1466 loss: 3.18079719e-06
Iter: 1467 loss: 3.17998547e-06
Iter: 1468 loss: 3.17945774e-06
Iter: 1469 loss: 3.17834565e-06
Iter: 1470 loss: 3.18169941e-06
Iter: 1471 loss: 3.17799913e-06
Iter: 1472 loss: 3.17719469e-06
Iter: 1473 loss: 3.18780894e-06
Iter: 1474 loss: 3.17721197e-06
Iter: 1475 loss: 3.17630293e-06
Iter: 1476 loss: 3.17541708e-06
Iter: 1477 loss: 3.17517765e-06
Iter: 1478 loss: 3.17420063e-06
Iter: 1479 loss: 3.17493095e-06
Iter: 1480 loss: 3.17351828e-06
Iter: 1481 loss: 3.17211561e-06
Iter: 1482 loss: 3.19011838e-06
Iter: 1483 loss: 3.17210424e-06
Iter: 1484 loss: 3.17112426e-06
Iter: 1485 loss: 3.16956539e-06
Iter: 1486 loss: 3.16949263e-06
Iter: 1487 loss: 3.16839964e-06
Iter: 1488 loss: 3.1683619e-06
Iter: 1489 loss: 3.1672605e-06
Iter: 1490 loss: 3.16636306e-06
Iter: 1491 loss: 3.16598471e-06
Iter: 1492 loss: 3.16423757e-06
Iter: 1493 loss: 3.1668028e-06
Iter: 1494 loss: 3.16322439e-06
Iter: 1495 loss: 3.16252135e-06
Iter: 1496 loss: 3.16232695e-06
Iter: 1497 loss: 3.16167143e-06
Iter: 1498 loss: 3.16162868e-06
Iter: 1499 loss: 3.16110481e-06
Iter: 1500 loss: 3.16018668e-06
Iter: 1501 loss: 3.15904e-06
Iter: 1502 loss: 3.15896386e-06
Iter: 1503 loss: 3.15774196e-06
Iter: 1504 loss: 3.17408376e-06
Iter: 1505 loss: 3.1576742e-06
Iter: 1506 loss: 3.15651187e-06
Iter: 1507 loss: 3.15899479e-06
Iter: 1508 loss: 3.15594843e-06
Iter: 1509 loss: 3.15476791e-06
Iter: 1510 loss: 3.15272609e-06
Iter: 1511 loss: 3.1527311e-06
Iter: 1512 loss: 3.15159218e-06
Iter: 1513 loss: 3.15134776e-06
Iter: 1514 loss: 3.14992531e-06
Iter: 1515 loss: 3.14795125e-06
Iter: 1516 loss: 3.14788554e-06
Iter: 1517 loss: 3.14636509e-06
Iter: 1518 loss: 3.16500928e-06
Iter: 1519 loss: 3.14640715e-06
Iter: 1520 loss: 3.14530553e-06
Iter: 1521 loss: 3.15115949e-06
Iter: 1522 loss: 3.14515819e-06
Iter: 1523 loss: 3.14439421e-06
Iter: 1524 loss: 3.14306249e-06
Iter: 1525 loss: 3.14310773e-06
Iter: 1526 loss: 3.1420127e-06
Iter: 1527 loss: 3.14202e-06
Iter: 1528 loss: 3.14114868e-06
Iter: 1529 loss: 3.14409795e-06
Iter: 1530 loss: 3.14100589e-06
Iter: 1531 loss: 3.14034855e-06
Iter: 1532 loss: 3.13913142e-06
Iter: 1533 loss: 3.16310229e-06
Iter: 1534 loss: 3.13904638e-06
Iter: 1535 loss: 3.13718647e-06
Iter: 1536 loss: 3.14100294e-06
Iter: 1537 loss: 3.13637111e-06
Iter: 1538 loss: 3.13501982e-06
Iter: 1539 loss: 3.1349382e-06
Iter: 1540 loss: 3.13401097e-06
Iter: 1541 loss: 3.13193732e-06
Iter: 1542 loss: 3.15585703e-06
Iter: 1543 loss: 3.13175155e-06
Iter: 1544 loss: 3.13024884e-06
Iter: 1545 loss: 3.13022792e-06
Iter: 1546 loss: 3.12905286e-06
Iter: 1547 loss: 3.13601913e-06
Iter: 1548 loss: 3.12890324e-06
Iter: 1549 loss: 3.12823272e-06
Iter: 1550 loss: 3.12713246e-06
Iter: 1551 loss: 3.15356829e-06
Iter: 1552 loss: 3.12714565e-06
Iter: 1553 loss: 3.12568818e-06
Iter: 1554 loss: 3.14382305e-06
Iter: 1555 loss: 3.12568454e-06
Iter: 1556 loss: 3.12482621e-06
Iter: 1557 loss: 3.12463044e-06
Iter: 1558 loss: 3.12403199e-06
Iter: 1559 loss: 3.12298062e-06
Iter: 1560 loss: 3.12731208e-06
Iter: 1561 loss: 3.12277234e-06
Iter: 1562 loss: 3.12158727e-06
Iter: 1563 loss: 3.12493194e-06
Iter: 1564 loss: 3.12115822e-06
Iter: 1565 loss: 3.11983104e-06
Iter: 1566 loss: 3.11926669e-06
Iter: 1567 loss: 3.11873237e-06
Iter: 1568 loss: 3.11699046e-06
Iter: 1569 loss: 3.11864073e-06
Iter: 1570 loss: 3.11603026e-06
Iter: 1571 loss: 3.11469239e-06
Iter: 1572 loss: 3.11465169e-06
Iter: 1573 loss: 3.1136351e-06
Iter: 1574 loss: 3.11315353e-06
Iter: 1575 loss: 3.11258282e-06
Iter: 1576 loss: 3.11151553e-06
Iter: 1577 loss: 3.11161466e-06
Iter: 1578 loss: 3.11073154e-06
Iter: 1579 loss: 3.10930182e-06
Iter: 1580 loss: 3.12995053e-06
Iter: 1581 loss: 3.10935957e-06
Iter: 1582 loss: 3.10849236e-06
Iter: 1583 loss: 3.10727546e-06
Iter: 1584 loss: 3.10716223e-06
Iter: 1585 loss: 3.10599e-06
Iter: 1586 loss: 3.1236043e-06
Iter: 1587 loss: 3.10602036e-06
Iter: 1588 loss: 3.10473115e-06
Iter: 1589 loss: 3.10389919e-06
Iter: 1590 loss: 3.10354517e-06
Iter: 1591 loss: 3.10219548e-06
Iter: 1592 loss: 3.1104687e-06
Iter: 1593 loss: 3.10204e-06
Iter: 1594 loss: 3.10097744e-06
Iter: 1595 loss: 3.10437395e-06
Iter: 1596 loss: 3.10061591e-06
Iter: 1597 loss: 3.09939742e-06
Iter: 1598 loss: 3.09946336e-06
Iter: 1599 loss: 3.09837651e-06
Iter: 1600 loss: 3.09724192e-06
Iter: 1601 loss: 3.10134737e-06
Iter: 1602 loss: 3.09695361e-06
Iter: 1603 loss: 3.0960714e-06
Iter: 1604 loss: 3.10173209e-06
Iter: 1605 loss: 3.09597e-06
Iter: 1606 loss: 3.09482857e-06
Iter: 1607 loss: 3.09476673e-06
Iter: 1608 loss: 3.09393567e-06
Iter: 1609 loss: 3.09281768e-06
Iter: 1610 loss: 3.09408097e-06
Iter: 1611 loss: 3.09224333e-06
Iter: 1612 loss: 3.09136203e-06
Iter: 1613 loss: 3.09136362e-06
Iter: 1614 loss: 3.09041661e-06
Iter: 1615 loss: 3.08820154e-06
Iter: 1616 loss: 3.11272652e-06
Iter: 1617 loss: 3.08794e-06
Iter: 1618 loss: 3.08665972e-06
Iter: 1619 loss: 3.08653534e-06
Iter: 1620 loss: 3.08531685e-06
Iter: 1621 loss: 3.0876929e-06
Iter: 1622 loss: 3.08486415e-06
Iter: 1623 loss: 3.08395511e-06
Iter: 1624 loss: 3.08563858e-06
Iter: 1625 loss: 3.08360131e-06
Iter: 1626 loss: 3.0827764e-06
Iter: 1627 loss: 3.0918834e-06
Iter: 1628 loss: 3.08272274e-06
Iter: 1629 loss: 3.08210133e-06
Iter: 1630 loss: 3.08213384e-06
Iter: 1631 loss: 3.0815936e-06
Iter: 1632 loss: 3.0808e-06
Iter: 1633 loss: 3.08027393e-06
Iter: 1634 loss: 3.08001495e-06
Iter: 1635 loss: 3.0788683e-06
Iter: 1636 loss: 3.09246252e-06
Iter: 1637 loss: 3.07884784e-06
Iter: 1638 loss: 3.07792925e-06
Iter: 1639 loss: 3.08180597e-06
Iter: 1640 loss: 3.07770642e-06
Iter: 1641 loss: 3.07699361e-06
Iter: 1642 loss: 3.0753381e-06
Iter: 1643 loss: 3.09617826e-06
Iter: 1644 loss: 3.07518167e-06
Iter: 1645 loss: 3.07430719e-06
Iter: 1646 loss: 3.07414393e-06
Iter: 1647 loss: 3.07307027e-06
Iter: 1648 loss: 3.07253435e-06
Iter: 1649 loss: 3.07204232e-06
Iter: 1650 loss: 3.07082178e-06
Iter: 1651 loss: 3.07398e-06
Iter: 1652 loss: 3.07044797e-06
Iter: 1653 loss: 3.06933271e-06
Iter: 1654 loss: 3.0841627e-06
Iter: 1655 loss: 3.06934089e-06
Iter: 1656 loss: 3.06887478e-06
Iter: 1657 loss: 3.06826701e-06
Iter: 1658 loss: 3.06819243e-06
Iter: 1659 loss: 3.0673e-06
Iter: 1660 loss: 3.07617893e-06
Iter: 1661 loss: 3.06733477e-06
Iter: 1662 loss: 3.06661923e-06
Iter: 1663 loss: 3.06674588e-06
Iter: 1664 loss: 3.06603488e-06
Iter: 1665 loss: 3.06523725e-06
Iter: 1666 loss: 3.06529819e-06
Iter: 1667 loss: 3.06451943e-06
Iter: 1668 loss: 3.06326e-06
Iter: 1669 loss: 3.06722814e-06
Iter: 1670 loss: 3.0629e-06
Iter: 1671 loss: 3.06165975e-06
Iter: 1672 loss: 3.07692653e-06
Iter: 1673 loss: 3.06166908e-06
Iter: 1674 loss: 3.06098241e-06
Iter: 1675 loss: 3.06000061e-06
Iter: 1676 loss: 3.06001061e-06
Iter: 1677 loss: 3.05874914e-06
Iter: 1678 loss: 3.06373158e-06
Iter: 1679 loss: 3.05850881e-06
Iter: 1680 loss: 3.05708e-06
Iter: 1681 loss: 3.06523862e-06
Iter: 1682 loss: 3.05690128e-06
Iter: 1683 loss: 3.05632329e-06
Iter: 1684 loss: 3.0561946e-06
Iter: 1685 loss: 3.05576759e-06
Iter: 1686 loss: 3.05497088e-06
Iter: 1687 loss: 3.0639776e-06
Iter: 1688 loss: 3.05494382e-06
Iter: 1689 loss: 3.05424783e-06
Iter: 1690 loss: 3.0533181e-06
Iter: 1691 loss: 3.05329445e-06
Iter: 1692 loss: 3.05240837e-06
Iter: 1693 loss: 3.05239928e-06
Iter: 1694 loss: 3.05173558e-06
Iter: 1695 loss: 3.05111644e-06
Iter: 1696 loss: 3.05092226e-06
Iter: 1697 loss: 3.04959076e-06
Iter: 1698 loss: 3.04963214e-06
Iter: 1699 loss: 3.04855234e-06
Iter: 1700 loss: 3.04694527e-06
Iter: 1701 loss: 3.05610365e-06
Iter: 1702 loss: 3.04680407e-06
Iter: 1703 loss: 3.04584e-06
Iter: 1704 loss: 3.04582454e-06
Iter: 1705 loss: 3.04527066e-06
Iter: 1706 loss: 3.04420018e-06
Iter: 1707 loss: 3.04418654e-06
Iter: 1708 loss: 3.043275e-06
Iter: 1709 loss: 3.04751347e-06
Iter: 1710 loss: 3.04311561e-06
Iter: 1711 loss: 3.04215882e-06
Iter: 1712 loss: 3.04961964e-06
Iter: 1713 loss: 3.04219384e-06
Iter: 1714 loss: 3.04156947e-06
Iter: 1715 loss: 3.04061223e-06
Iter: 1716 loss: 3.04067657e-06
Iter: 1717 loss: 3.03996376e-06
Iter: 1718 loss: 3.03994648e-06
Iter: 1719 loss: 3.03924412e-06
Iter: 1720 loss: 3.03794445e-06
Iter: 1721 loss: 3.06333277e-06
Iter: 1722 loss: 3.03783509e-06
Iter: 1723 loss: 3.03668025e-06
Iter: 1724 loss: 3.05321714e-06
Iter: 1725 loss: 3.0366889e-06
Iter: 1726 loss: 3.0355443e-06
Iter: 1727 loss: 3.03773322e-06
Iter: 1728 loss: 3.03506704e-06
Iter: 1729 loss: 3.03406932e-06
Iter: 1730 loss: 3.03306228e-06
Iter: 1731 loss: 3.03289221e-06
Iter: 1732 loss: 3.03125444e-06
Iter: 1733 loss: 3.03819411e-06
Iter: 1734 loss: 3.03087745e-06
Iter: 1735 loss: 3.03024103e-06
Iter: 1736 loss: 3.03013894e-06
Iter: 1737 loss: 3.0295796e-06
Iter: 1738 loss: 3.02881563e-06
Iter: 1739 loss: 3.02874719e-06
Iter: 1740 loss: 3.02780836e-06
Iter: 1741 loss: 3.02923354e-06
Iter: 1742 loss: 3.02730632e-06
Iter: 1743 loss: 3.02662102e-06
Iter: 1744 loss: 3.02656144e-06
Iter: 1745 loss: 3.02601711e-06
Iter: 1746 loss: 3.0247968e-06
Iter: 1747 loss: 3.04034234e-06
Iter: 1748 loss: 3.02464105e-06
Iter: 1749 loss: 3.02331864e-06
Iter: 1750 loss: 3.03937622e-06
Iter: 1751 loss: 3.02331114e-06
Iter: 1752 loss: 3.02200169e-06
Iter: 1753 loss: 3.0232145e-06
Iter: 1754 loss: 3.02126773e-06
Iter: 1755 loss: 3.02026137e-06
Iter: 1756 loss: 3.02395529e-06
Iter: 1757 loss: 3.02004491e-06
Iter: 1758 loss: 3.01878845e-06
Iter: 1759 loss: 3.02230046e-06
Iter: 1760 loss: 3.01841487e-06
Iter: 1761 loss: 3.01743171e-06
Iter: 1762 loss: 3.01767204e-06
Iter: 1763 loss: 3.01678097e-06
Iter: 1764 loss: 3.01577757e-06
Iter: 1765 loss: 3.01824e-06
Iter: 1766 loss: 3.01538239e-06
Iter: 1767 loss: 3.01463524e-06
Iter: 1768 loss: 3.02431863e-06
Iter: 1769 loss: 3.01457658e-06
Iter: 1770 loss: 3.01385535e-06
Iter: 1771 loss: 3.01427917e-06
Iter: 1772 loss: 3.01337286e-06
Iter: 1773 loss: 3.01262753e-06
Iter: 1774 loss: 3.01173668e-06
Iter: 1775 loss: 3.01169666e-06
Iter: 1776 loss: 3.01065302e-06
Iter: 1777 loss: 3.01066621e-06
Iter: 1778 loss: 3.00976967e-06
Iter: 1779 loss: 3.00920965e-06
Iter: 1780 loss: 3.00889928e-06
Iter: 1781 loss: 3.00790111e-06
Iter: 1782 loss: 3.01066189e-06
Iter: 1783 loss: 3.00756665e-06
Iter: 1784 loss: 3.00612828e-06
Iter: 1785 loss: 3.01144792e-06
Iter: 1786 loss: 3.00584566e-06
Iter: 1787 loss: 3.00514512e-06
Iter: 1788 loss: 3.00752663e-06
Iter: 1789 loss: 3.00502325e-06
Iter: 1790 loss: 3.00428655e-06
Iter: 1791 loss: 3.0079857e-06
Iter: 1792 loss: 3.00424108e-06
Iter: 1793 loss: 3.00366946e-06
Iter: 1794 loss: 3.00264492e-06
Iter: 1795 loss: 3.02607123e-06
Iter: 1796 loss: 3.00269312e-06
Iter: 1797 loss: 3.00154556e-06
Iter: 1798 loss: 3.00728016e-06
Iter: 1799 loss: 3.00133297e-06
Iter: 1800 loss: 3.00039437e-06
Iter: 1801 loss: 3.00691772e-06
Iter: 1802 loss: 3.00035435e-06
Iter: 1803 loss: 2.9993696e-06
Iter: 1804 loss: 2.99979956e-06
Iter: 1805 loss: 2.99868361e-06
Iter: 1806 loss: 2.99758108e-06
Iter: 1807 loss: 2.99750718e-06
Iter: 1808 loss: 2.99658586e-06
Iter: 1809 loss: 2.99587623e-06
Iter: 1810 loss: 2.99576459e-06
Iter: 1811 loss: 2.99495332e-06
Iter: 1812 loss: 2.99453268e-06
Iter: 1813 loss: 2.99417411e-06
Iter: 1814 loss: 2.99329577e-06
Iter: 1815 loss: 2.99727617e-06
Iter: 1816 loss: 2.99313342e-06
Iter: 1817 loss: 2.99244584e-06
Iter: 1818 loss: 2.99938392e-06
Iter: 1819 loss: 2.99241901e-06
Iter: 1820 loss: 2.99205317e-06
Iter: 1821 loss: 2.9914313e-06
Iter: 1822 loss: 2.99136195e-06
Iter: 1823 loss: 2.99050248e-06
Iter: 1824 loss: 2.99974363e-06
Iter: 1825 loss: 2.99053772e-06
Iter: 1826 loss: 2.98995269e-06
Iter: 1827 loss: 2.98901932e-06
Iter: 1828 loss: 2.98902637e-06
Iter: 1829 loss: 2.98772761e-06
Iter: 1830 loss: 2.98962277e-06
Iter: 1831 loss: 2.98721079e-06
Iter: 1832 loss: 2.98613327e-06
Iter: 1833 loss: 3.00007378e-06
Iter: 1834 loss: 2.98605801e-06
Iter: 1835 loss: 2.98510986e-06
Iter: 1836 loss: 2.98903456e-06
Iter: 1837 loss: 2.98491932e-06
Iter: 1838 loss: 2.98420446e-06
Iter: 1839 loss: 2.98318719e-06
Iter: 1840 loss: 2.98313716e-06
Iter: 1841 loss: 2.98248233e-06
Iter: 1842 loss: 2.98251371e-06
Iter: 1843 loss: 2.98178384e-06
Iter: 1844 loss: 2.98293617e-06
Iter: 1845 loss: 2.98160012e-06
Iter: 1846 loss: 2.98097302e-06
Iter: 1847 loss: 2.98037662e-06
Iter: 1848 loss: 2.9802959e-06
Iter: 1849 loss: 2.97964584e-06
Iter: 1850 loss: 2.97953284e-06
Iter: 1851 loss: 2.97918018e-06
Iter: 1852 loss: 2.9784967e-06
Iter: 1853 loss: 2.97852966e-06
Iter: 1854 loss: 2.97763017e-06
Iter: 1855 loss: 2.98393888e-06
Iter: 1856 loss: 2.97755969e-06
Iter: 1857 loss: 2.97672e-06
Iter: 1858 loss: 2.9755729e-06
Iter: 1859 loss: 2.97552492e-06
Iter: 1860 loss: 2.97434372e-06
Iter: 1861 loss: 2.98043642e-06
Iter: 1862 loss: 2.97417705e-06
Iter: 1863 loss: 2.97327324e-06
Iter: 1864 loss: 2.97661882e-06
Iter: 1865 loss: 2.97303905e-06
Iter: 1866 loss: 2.97219503e-06
Iter: 1867 loss: 2.98039322e-06
Iter: 1868 loss: 2.97220595e-06
Iter: 1869 loss: 2.97171027e-06
Iter: 1870 loss: 2.9712478e-06
Iter: 1871 loss: 2.97112092e-06
Iter: 1872 loss: 2.97035399e-06
Iter: 1873 loss: 2.97332917e-06
Iter: 1874 loss: 2.97025963e-06
Iter: 1875 loss: 2.96937151e-06
Iter: 1876 loss: 2.97287806e-06
Iter: 1877 loss: 2.96920643e-06
Iter: 1878 loss: 2.96868711e-06
Iter: 1879 loss: 2.96789176e-06
Iter: 1880 loss: 2.96784719e-06
Iter: 1881 loss: 2.96694543e-06
Iter: 1882 loss: 2.98022269e-06
Iter: 1883 loss: 2.96693679e-06
Iter: 1884 loss: 2.96609187e-06
Iter: 1885 loss: 2.96571579e-06
Iter: 1886 loss: 2.96527719e-06
Iter: 1887 loss: 2.96452981e-06
Iter: 1888 loss: 2.96455528e-06
Iter: 1889 loss: 2.96400503e-06
Iter: 1890 loss: 2.96293888e-06
Iter: 1891 loss: 2.9859907e-06
Iter: 1892 loss: 2.96294616e-06
Iter: 1893 loss: 2.96196094e-06
Iter: 1894 loss: 2.96687222e-06
Iter: 1895 loss: 2.96173948e-06
Iter: 1896 loss: 2.96098801e-06
Iter: 1897 loss: 2.96690791e-06
Iter: 1898 loss: 2.96097733e-06
Iter: 1899 loss: 2.96034727e-06
Iter: 1900 loss: 2.96300777e-06
Iter: 1901 loss: 2.9601e-06
Iter: 1902 loss: 2.95949098e-06
Iter: 1903 loss: 2.95903601e-06
Iter: 1904 loss: 2.95884183e-06
Iter: 1905 loss: 2.95807422e-06
Iter: 1906 loss: 2.96070584e-06
Iter: 1907 loss: 2.95790983e-06
Iter: 1908 loss: 2.95690506e-06
Iter: 1909 loss: 2.96274538e-06
Iter: 1910 loss: 2.95683503e-06
Iter: 1911 loss: 2.95605105e-06
Iter: 1912 loss: 2.95495374e-06
Iter: 1913 loss: 2.95490076e-06
Iter: 1914 loss: 2.95424229e-06
Iter: 1915 loss: 2.95420909e-06
Iter: 1916 loss: 2.95347036e-06
Iter: 1917 loss: 2.95285599e-06
Iter: 1918 loss: 2.9527273e-06
Iter: 1919 loss: 2.9519872e-06
Iter: 1920 loss: 2.96409462e-06
Iter: 1921 loss: 2.95199129e-06
Iter: 1922 loss: 2.95137852e-06
Iter: 1923 loss: 2.95093605e-06
Iter: 1924 loss: 2.95075824e-06
Iter: 1925 loss: 2.95012865e-06
Iter: 1926 loss: 2.95022437e-06
Iter: 1927 loss: 2.94961274e-06
Iter: 1928 loss: 2.94863958e-06
Iter: 1929 loss: 2.9531584e-06
Iter: 1930 loss: 2.94845313e-06
Iter: 1931 loss: 2.94756455e-06
Iter: 1932 loss: 2.95652126e-06
Iter: 1933 loss: 2.94758502e-06
Iter: 1934 loss: 2.94689403e-06
Iter: 1935 loss: 2.94618e-06
Iter: 1936 loss: 2.94619349e-06
Iter: 1937 loss: 2.94497204e-06
Iter: 1938 loss: 2.94585607e-06
Iter: 1939 loss: 2.94422784e-06
Iter: 1940 loss: 2.94328379e-06
Iter: 1941 loss: 2.94324082e-06
Iter: 1942 loss: 2.94258666e-06
Iter: 1943 loss: 2.94115875e-06
Iter: 1944 loss: 2.96995e-06
Iter: 1945 loss: 2.94114261e-06
Iter: 1946 loss: 2.94008692e-06
Iter: 1947 loss: 2.94012739e-06
Iter: 1948 loss: 2.93918629e-06
Iter: 1949 loss: 2.94100164e-06
Iter: 1950 loss: 2.93876724e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi0.8
+ date
Sat Nov  7 21:45:17 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/300_100_100_100_1 --function f1 --psi -2 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc744541048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc744586d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7445f2158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc74460ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7446341e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc744634598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc744634048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7443ce048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7443ce488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc74442bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc74435f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc74434cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc74437a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc74437a048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc744473a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc74429f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc74429f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7442abd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc744304950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc74425cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc744285840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc744225620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7397dd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7397dd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7398116a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc739811f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc73978f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7441d8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7441d8048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7441d4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7397c4268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7397106a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc739710378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc739710268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc73971ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7396b7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.004325806
test_loss: 0.0047328565
train_loss: 0.0042335377
test_loss: 0.0045373873
train_loss: 0.0038090814
test_loss: 0.0046134493
train_loss: 0.0038080253
test_loss: 0.0042712768
train_loss: 0.0035979494
test_loss: 0.00418937
train_loss: 0.0036860113
test_loss: 0.0042338767
train_loss: 0.0036488236
test_loss: 0.004262572
train_loss: 0.0035175653
test_loss: 0.004256324
train_loss: 0.0037591502
test_loss: 0.0043150294
train_loss: 0.0037861108
test_loss: 0.0043227808
train_loss: 0.0037725654
test_loss: 0.004472474
train_loss: 0.0036556597
test_loss: 0.0041779927
train_loss: 0.0035160584
test_loss: 0.0041705235
train_loss: 0.0038602985
test_loss: 0.0043331063
train_loss: 0.003701224
test_loss: 0.004261431
train_loss: 0.0035468908
test_loss: 0.0041856198
train_loss: 0.0036782648
test_loss: 0.0042670625
train_loss: 0.0034886152
test_loss: 0.004244697
train_loss: 0.003469696
test_loss: 0.004405392
train_loss: 0.0035263593
test_loss: 0.0041891183
train_loss: 0.0036560823
test_loss: 0.004137425
train_loss: 0.0034976725
test_loss: 0.0041164528
train_loss: 0.0035126365
test_loss: 0.004112985
train_loss: 0.003455094
test_loss: 0.0042136954
train_loss: 0.0034352443
test_loss: 0.0043085
train_loss: 0.0035050781
test_loss: 0.0041563776
train_loss: 0.0034959232
test_loss: 0.0043615624
train_loss: 0.003639446
test_loss: 0.004237667
train_loss: 0.0034668713
test_loss: 0.004150754
train_loss: 0.003560954
test_loss: 0.0041920883
train_loss: 0.0034210058
test_loss: 0.004100102
train_loss: 0.003517887
test_loss: 0.0041765072
train_loss: 0.0036004453
test_loss: 0.004293518
train_loss: 0.0032688924
test_loss: 0.0040220413
train_loss: 0.0034681198
test_loss: 0.0041287458
train_loss: 0.0032558432
test_loss: 0.003967867
train_loss: 0.0035391706
test_loss: 0.004141045
train_loss: 0.003370347
test_loss: 0.0041153827
train_loss: 0.0033198586
test_loss: 0.0041511566
train_loss: 0.0033538528
test_loss: 0.004039967
train_loss: 0.003914604
test_loss: 0.004144762
train_loss: 0.0032424005
test_loss: 0.0041880296
train_loss: 0.0033935285
test_loss: 0.0040460657
train_loss: 0.0034129634
test_loss: 0.004176964
train_loss: 0.0034643358
test_loss: 0.004119248
train_loss: 0.0031014562
test_loss: 0.003874813
train_loss: 0.00333953
test_loss: 0.0040021506
train_loss: 0.0032421867
test_loss: 0.003973882
train_loss: 0.003241368
test_loss: 0.004005683
train_loss: 0.003412943
test_loss: 0.0040450455
train_loss: 0.0033661355
test_loss: 0.0040933937
train_loss: 0.0033626186
test_loss: 0.0039777644
train_loss: 0.003242735
test_loss: 0.003987932
train_loss: 0.0033165691
test_loss: 0.003908796
train_loss: 0.0032065194
test_loss: 0.0038525937
train_loss: 0.0032793584
test_loss: 0.00384797
train_loss: 0.0032259491
test_loss: 0.003970136
train_loss: 0.0033376184
test_loss: 0.003972078
train_loss: 0.003115709
test_loss: 0.004008133
train_loss: 0.0032012067
test_loss: 0.003972833
train_loss: 0.0035482151
test_loss: 0.0040975045
train_loss: 0.0032826704
test_loss: 0.0038959908
train_loss: 0.0032348256
test_loss: 0.004147621
train_loss: 0.0032682628
test_loss: 0.0038876543
train_loss: 0.0031011323
test_loss: 0.0038705282
train_loss: 0.0032116529
test_loss: 0.003968342
train_loss: 0.0032908518
test_loss: 0.0038166188
train_loss: 0.003122846
test_loss: 0.003986535
train_loss: 0.0032434894
test_loss: 0.004079417
train_loss: 0.0030287388
test_loss: 0.0037744176
train_loss: 0.0034218843
test_loss: 0.0040269224
train_loss: 0.0033107223
test_loss: 0.0038788202
train_loss: 0.0029999416
test_loss: 0.0037880961
train_loss: 0.0032056374
test_loss: 0.0038329717
train_loss: 0.0033245021
test_loss: 0.004104924
train_loss: 0.0033600284
test_loss: 0.0038927055
train_loss: 0.0032311385
test_loss: 0.0039206375
train_loss: 0.003282573
test_loss: 0.0038545737
train_loss: 0.0032036398
test_loss: 0.003917065
train_loss: 0.0030434849
test_loss: 0.0038659177
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi0.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi0.8/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0dd6f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0de048c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0de44b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0ddae510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0ddaea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0dd1d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0dcf48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0dc9d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0dc9d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0dc9d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0dbff9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0dc327b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0dc32950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0dbe0400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0dbe00d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0dbae8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0db37598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0db37f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0db20950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0db378c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0dae7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0da898c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0da37510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0da66598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0da62620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0da0bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0d9c4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0d9f12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5b0d9f1378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5afd6af620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5afd641950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5afd674158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5afd674c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5afd629620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5afd5d9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5afd57e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.64056983e-05
Iter: 2 loss: 1.98075613e-05
Iter: 3 loss: 1.45237045e-05
Iter: 4 loss: 1.38381729e-05
Iter: 5 loss: 1.53365108e-05
Iter: 6 loss: 1.35744785e-05
Iter: 7 loss: 1.31374154e-05
Iter: 8 loss: 1.33011108e-05
Iter: 9 loss: 1.28327101e-05
Iter: 10 loss: 1.22324946e-05
Iter: 11 loss: 1.55494126e-05
Iter: 12 loss: 1.21461908e-05
Iter: 13 loss: 1.16421706e-05
Iter: 14 loss: 1.17976324e-05
Iter: 15 loss: 1.12826547e-05
Iter: 16 loss: 1.09972079e-05
Iter: 17 loss: 1.09794237e-05
Iter: 18 loss: 1.07540018e-05
Iter: 19 loss: 1.04723304e-05
Iter: 20 loss: 1.04492492e-05
Iter: 21 loss: 1.00540474e-05
Iter: 22 loss: 1.03075963e-05
Iter: 23 loss: 9.80344612e-06
Iter: 24 loss: 9.2203e-06
Iter: 25 loss: 1.39034973e-05
Iter: 26 loss: 9.18223213e-06
Iter: 27 loss: 8.92807566e-06
Iter: 28 loss: 8.73842691e-06
Iter: 29 loss: 8.65417132e-06
Iter: 30 loss: 8.35203718e-06
Iter: 31 loss: 8.54313475e-06
Iter: 32 loss: 8.15934254e-06
Iter: 33 loss: 7.95247615e-06
Iter: 34 loss: 1.08403601e-05
Iter: 35 loss: 7.95188316e-06
Iter: 36 loss: 7.81998e-06
Iter: 37 loss: 7.74564069e-06
Iter: 38 loss: 7.68848349e-06
Iter: 39 loss: 7.5074513e-06
Iter: 40 loss: 7.50249092e-06
Iter: 41 loss: 7.41675376e-06
Iter: 42 loss: 7.33002798e-06
Iter: 43 loss: 7.31317186e-06
Iter: 44 loss: 7.15385249e-06
Iter: 45 loss: 8.05716809e-06
Iter: 46 loss: 7.1318691e-06
Iter: 47 loss: 6.97896394e-06
Iter: 48 loss: 7.09320102e-06
Iter: 49 loss: 6.88513319e-06
Iter: 50 loss: 6.77041044e-06
Iter: 51 loss: 7.97063e-06
Iter: 52 loss: 6.76731634e-06
Iter: 53 loss: 6.66595042e-06
Iter: 54 loss: 6.7591418e-06
Iter: 55 loss: 6.60733667e-06
Iter: 56 loss: 6.49635331e-06
Iter: 57 loss: 6.49012782e-06
Iter: 58 loss: 6.40557e-06
Iter: 59 loss: 6.30818e-06
Iter: 60 loss: 6.30814475e-06
Iter: 61 loss: 6.23496589e-06
Iter: 62 loss: 6.39466634e-06
Iter: 63 loss: 6.20682249e-06
Iter: 64 loss: 6.15787394e-06
Iter: 65 loss: 6.06764524e-06
Iter: 66 loss: 8.14361738e-06
Iter: 67 loss: 6.06763206e-06
Iter: 68 loss: 5.95045913e-06
Iter: 69 loss: 6.2759691e-06
Iter: 70 loss: 5.91280877e-06
Iter: 71 loss: 5.82476696e-06
Iter: 72 loss: 6.85396571e-06
Iter: 73 loss: 5.82333814e-06
Iter: 74 loss: 5.7637576e-06
Iter: 75 loss: 6.64901972e-06
Iter: 76 loss: 5.76365073e-06
Iter: 77 loss: 5.72183853e-06
Iter: 78 loss: 5.64918446e-06
Iter: 79 loss: 5.64923039e-06
Iter: 80 loss: 5.61475554e-06
Iter: 81 loss: 5.60888384e-06
Iter: 82 loss: 5.57533622e-06
Iter: 83 loss: 5.52101119e-06
Iter: 84 loss: 5.52059601e-06
Iter: 85 loss: 5.48610433e-06
Iter: 86 loss: 5.4842003e-06
Iter: 87 loss: 5.45925286e-06
Iter: 88 loss: 5.49819151e-06
Iter: 89 loss: 5.44769e-06
Iter: 90 loss: 5.42585076e-06
Iter: 91 loss: 5.40924975e-06
Iter: 92 loss: 5.40209749e-06
Iter: 93 loss: 5.36197922e-06
Iter: 94 loss: 5.55097813e-06
Iter: 95 loss: 5.35465824e-06
Iter: 96 loss: 5.30952457e-06
Iter: 97 loss: 5.41495547e-06
Iter: 98 loss: 5.29278714e-06
Iter: 99 loss: 5.25923042e-06
Iter: 100 loss: 5.23567678e-06
Iter: 101 loss: 5.22385517e-06
Iter: 102 loss: 5.16401633e-06
Iter: 103 loss: 5.22005303e-06
Iter: 104 loss: 5.12977113e-06
Iter: 105 loss: 5.08734229e-06
Iter: 106 loss: 5.70339716e-06
Iter: 107 loss: 5.0873341e-06
Iter: 108 loss: 5.04338868e-06
Iter: 109 loss: 5.24457846e-06
Iter: 110 loss: 5.03518413e-06
Iter: 111 loss: 5.01198929e-06
Iter: 112 loss: 5.01886e-06
Iter: 113 loss: 4.9954856e-06
Iter: 114 loss: 4.97785868e-06
Iter: 115 loss: 4.97756673e-06
Iter: 116 loss: 4.96479606e-06
Iter: 117 loss: 4.94533197e-06
Iter: 118 loss: 4.94495362e-06
Iter: 119 loss: 4.92489107e-06
Iter: 120 loss: 5.21171933e-06
Iter: 121 loss: 4.9248024e-06
Iter: 122 loss: 4.90690945e-06
Iter: 123 loss: 4.88580736e-06
Iter: 124 loss: 4.88347177e-06
Iter: 125 loss: 4.8611214e-06
Iter: 126 loss: 4.94917185e-06
Iter: 127 loss: 4.85593046e-06
Iter: 128 loss: 4.83238273e-06
Iter: 129 loss: 4.92112667e-06
Iter: 130 loss: 4.82672567e-06
Iter: 131 loss: 4.79939627e-06
Iter: 132 loss: 4.82956602e-06
Iter: 133 loss: 4.78437596e-06
Iter: 134 loss: 4.75940215e-06
Iter: 135 loss: 4.76056675e-06
Iter: 136 loss: 4.73950286e-06
Iter: 137 loss: 4.7116e-06
Iter: 138 loss: 4.82180667e-06
Iter: 139 loss: 4.70514351e-06
Iter: 140 loss: 4.68840244e-06
Iter: 141 loss: 4.68841608e-06
Iter: 142 loss: 4.6731443e-06
Iter: 143 loss: 4.72709053e-06
Iter: 144 loss: 4.66926303e-06
Iter: 145 loss: 4.6611267e-06
Iter: 146 loss: 4.64723689e-06
Iter: 147 loss: 4.64714867e-06
Iter: 148 loss: 4.62843673e-06
Iter: 149 loss: 4.86064073e-06
Iter: 150 loss: 4.62819571e-06
Iter: 151 loss: 4.61781e-06
Iter: 152 loss: 4.6063451e-06
Iter: 153 loss: 4.60470164e-06
Iter: 154 loss: 4.58567547e-06
Iter: 155 loss: 4.77624235e-06
Iter: 156 loss: 4.58518207e-06
Iter: 157 loss: 4.57256556e-06
Iter: 158 loss: 4.55951204e-06
Iter: 159 loss: 4.55713462e-06
Iter: 160 loss: 4.53881967e-06
Iter: 161 loss: 4.58335353e-06
Iter: 162 loss: 4.53214398e-06
Iter: 163 loss: 4.51119786e-06
Iter: 164 loss: 4.69211727e-06
Iter: 165 loss: 4.51007054e-06
Iter: 166 loss: 4.49571917e-06
Iter: 167 loss: 4.5055076e-06
Iter: 168 loss: 4.48680385e-06
Iter: 169 loss: 4.47295588e-06
Iter: 170 loss: 4.48862602e-06
Iter: 171 loss: 4.46545073e-06
Iter: 172 loss: 4.45046226e-06
Iter: 173 loss: 4.46930335e-06
Iter: 174 loss: 4.44272428e-06
Iter: 175 loss: 4.44280931e-06
Iter: 176 loss: 4.43612316e-06
Iter: 177 loss: 4.43112e-06
Iter: 178 loss: 4.42038572e-06
Iter: 179 loss: 4.59448302e-06
Iter: 180 loss: 4.4200774e-06
Iter: 181 loss: 4.40559916e-06
Iter: 182 loss: 4.42518558e-06
Iter: 183 loss: 4.39843689e-06
Iter: 184 loss: 4.38192365e-06
Iter: 185 loss: 4.59396233e-06
Iter: 186 loss: 4.3818045e-06
Iter: 187 loss: 4.37501512e-06
Iter: 188 loss: 4.36714708e-06
Iter: 189 loss: 4.36616301e-06
Iter: 190 loss: 4.34737e-06
Iter: 191 loss: 4.38502502e-06
Iter: 192 loss: 4.33978494e-06
Iter: 193 loss: 4.32849356e-06
Iter: 194 loss: 4.33842661e-06
Iter: 195 loss: 4.32188881e-06
Iter: 196 loss: 4.31219632e-06
Iter: 197 loss: 4.40084568e-06
Iter: 198 loss: 4.31178296e-06
Iter: 199 loss: 4.30284445e-06
Iter: 200 loss: 4.32575371e-06
Iter: 201 loss: 4.29978491e-06
Iter: 202 loss: 4.29146712e-06
Iter: 203 loss: 4.29065403e-06
Iter: 204 loss: 4.2847023e-06
Iter: 205 loss: 4.27548457e-06
Iter: 206 loss: 4.31270746e-06
Iter: 207 loss: 4.27334271e-06
Iter: 208 loss: 4.26436054e-06
Iter: 209 loss: 4.26264296e-06
Iter: 210 loss: 4.25654616e-06
Iter: 211 loss: 4.24255586e-06
Iter: 212 loss: 4.44717671e-06
Iter: 213 loss: 4.24254722e-06
Iter: 214 loss: 4.2351121e-06
Iter: 215 loss: 4.22463927e-06
Iter: 216 loss: 4.22429912e-06
Iter: 217 loss: 4.21744517e-06
Iter: 218 loss: 4.21735513e-06
Iter: 219 loss: 4.20998913e-06
Iter: 220 loss: 4.20054903e-06
Iter: 221 loss: 4.19993921e-06
Iter: 222 loss: 4.1910971e-06
Iter: 223 loss: 4.31591889e-06
Iter: 224 loss: 4.19102298e-06
Iter: 225 loss: 4.18477111e-06
Iter: 226 loss: 4.20395463e-06
Iter: 227 loss: 4.18285663e-06
Iter: 228 loss: 4.17883e-06
Iter: 229 loss: 4.16761031e-06
Iter: 230 loss: 4.23503434e-06
Iter: 231 loss: 4.16458806e-06
Iter: 232 loss: 4.15458e-06
Iter: 233 loss: 4.15337718e-06
Iter: 234 loss: 4.14715305e-06
Iter: 235 loss: 4.14395163e-06
Iter: 236 loss: 4.14102669e-06
Iter: 237 loss: 4.13052248e-06
Iter: 238 loss: 4.13086582e-06
Iter: 239 loss: 4.12219379e-06
Iter: 240 loss: 4.10830944e-06
Iter: 241 loss: 4.14776696e-06
Iter: 242 loss: 4.1037697e-06
Iter: 243 loss: 4.09702534e-06
Iter: 244 loss: 4.09665699e-06
Iter: 245 loss: 4.09009681e-06
Iter: 246 loss: 4.10056691e-06
Iter: 247 loss: 4.0870159e-06
Iter: 248 loss: 4.08170035e-06
Iter: 249 loss: 4.07524794e-06
Iter: 250 loss: 4.07467451e-06
Iter: 251 loss: 4.07164589e-06
Iter: 252 loss: 4.0697746e-06
Iter: 253 loss: 4.06711524e-06
Iter: 254 loss: 4.06280833e-06
Iter: 255 loss: 4.06276e-06
Iter: 256 loss: 4.05893888e-06
Iter: 257 loss: 4.05887795e-06
Iter: 258 loss: 4.05594392e-06
Iter: 259 loss: 4.04837283e-06
Iter: 260 loss: 4.11291512e-06
Iter: 261 loss: 4.04707635e-06
Iter: 262 loss: 4.0378718e-06
Iter: 263 loss: 4.08533197e-06
Iter: 264 loss: 4.03634749e-06
Iter: 265 loss: 4.02844671e-06
Iter: 266 loss: 4.11836481e-06
Iter: 267 loss: 4.02837713e-06
Iter: 268 loss: 4.02238766e-06
Iter: 269 loss: 4.0162231e-06
Iter: 270 loss: 4.01514944e-06
Iter: 271 loss: 4.00848057e-06
Iter: 272 loss: 4.03810236e-06
Iter: 273 loss: 4.00704812e-06
Iter: 274 loss: 3.99916598e-06
Iter: 275 loss: 4.00438375e-06
Iter: 276 loss: 3.99409782e-06
Iter: 277 loss: 3.99058899e-06
Iter: 278 loss: 3.98903194e-06
Iter: 279 loss: 3.98598149e-06
Iter: 280 loss: 3.98142038e-06
Iter: 281 loss: 3.98137036e-06
Iter: 282 loss: 3.97664098e-06
Iter: 283 loss: 3.99672626e-06
Iter: 284 loss: 3.97567055e-06
Iter: 285 loss: 3.96936093e-06
Iter: 286 loss: 3.98291195e-06
Iter: 287 loss: 3.96700761e-06
Iter: 288 loss: 3.9628726e-06
Iter: 289 loss: 3.97340409e-06
Iter: 290 loss: 3.961517e-06
Iter: 291 loss: 3.95652114e-06
Iter: 292 loss: 3.97323583e-06
Iter: 293 loss: 3.95519783e-06
Iter: 294 loss: 3.95059533e-06
Iter: 295 loss: 3.94143535e-06
Iter: 296 loss: 4.11991141e-06
Iter: 297 loss: 3.94134531e-06
Iter: 298 loss: 3.93606661e-06
Iter: 299 loss: 3.93574373e-06
Iter: 300 loss: 3.93057508e-06
Iter: 301 loss: 3.94053723e-06
Iter: 302 loss: 3.92837592e-06
Iter: 303 loss: 3.92475613e-06
Iter: 304 loss: 3.92068e-06
Iter: 305 loss: 3.92009451e-06
Iter: 306 loss: 3.91314461e-06
Iter: 307 loss: 3.94810831e-06
Iter: 308 loss: 3.91188814e-06
Iter: 309 loss: 3.90708828e-06
Iter: 310 loss: 3.96051519e-06
Iter: 311 loss: 3.90692912e-06
Iter: 312 loss: 3.90218156e-06
Iter: 313 loss: 3.9078227e-06
Iter: 314 loss: 3.89961951e-06
Iter: 315 loss: 3.89515071e-06
Iter: 316 loss: 3.89789056e-06
Iter: 317 loss: 3.89230445e-06
Iter: 318 loss: 3.88936041e-06
Iter: 319 loss: 3.88930312e-06
Iter: 320 loss: 3.88639683e-06
Iter: 321 loss: 3.88027138e-06
Iter: 322 loss: 3.98204793e-06
Iter: 323 loss: 3.88000808e-06
Iter: 324 loss: 3.87600812e-06
Iter: 325 loss: 3.87583304e-06
Iter: 326 loss: 3.87225464e-06
Iter: 327 loss: 3.87093314e-06
Iter: 328 loss: 3.86896045e-06
Iter: 329 loss: 3.86565353e-06
Iter: 330 loss: 3.86408919e-06
Iter: 331 loss: 3.86244665e-06
Iter: 332 loss: 3.85728617e-06
Iter: 333 loss: 3.9176939e-06
Iter: 334 loss: 3.8572507e-06
Iter: 335 loss: 3.85240082e-06
Iter: 336 loss: 3.85186695e-06
Iter: 337 loss: 3.84822806e-06
Iter: 338 loss: 3.84327632e-06
Iter: 339 loss: 3.84015948e-06
Iter: 340 loss: 3.83823772e-06
Iter: 341 loss: 3.83288716e-06
Iter: 342 loss: 3.83287761e-06
Iter: 343 loss: 3.82871576e-06
Iter: 344 loss: 3.85472322e-06
Iter: 345 loss: 3.82819417e-06
Iter: 346 loss: 3.82445842e-06
Iter: 347 loss: 3.82332382e-06
Iter: 348 loss: 3.82106055e-06
Iter: 349 loss: 3.81783957e-06
Iter: 350 loss: 3.83492261e-06
Iter: 351 loss: 3.81720929e-06
Iter: 352 loss: 3.81358768e-06
Iter: 353 loss: 3.82758071e-06
Iter: 354 loss: 3.81269274e-06
Iter: 355 loss: 3.8097e-06
Iter: 356 loss: 3.8064386e-06
Iter: 357 loss: 3.80589063e-06
Iter: 358 loss: 3.80175879e-06
Iter: 359 loss: 3.80177926e-06
Iter: 360 loss: 3.79935341e-06
Iter: 361 loss: 3.79551602e-06
Iter: 362 loss: 3.79551966e-06
Iter: 363 loss: 3.7900154e-06
Iter: 364 loss: 3.78831737e-06
Iter: 365 loss: 3.78498498e-06
Iter: 366 loss: 3.778571e-06
Iter: 367 loss: 3.77833749e-06
Iter: 368 loss: 3.77532842e-06
Iter: 369 loss: 3.7708603e-06
Iter: 370 loss: 3.77076276e-06
Iter: 371 loss: 3.76593766e-06
Iter: 372 loss: 3.78224831e-06
Iter: 373 loss: 3.76470803e-06
Iter: 374 loss: 3.76080061e-06
Iter: 375 loss: 3.80592155e-06
Iter: 376 loss: 3.76066646e-06
Iter: 377 loss: 3.7567786e-06
Iter: 378 loss: 3.76449407e-06
Iter: 379 loss: 3.75520131e-06
Iter: 380 loss: 3.75259629e-06
Iter: 381 loss: 3.75529225e-06
Iter: 382 loss: 3.75111517e-06
Iter: 383 loss: 3.74785827e-06
Iter: 384 loss: 3.7616519e-06
Iter: 385 loss: 3.74723277e-06
Iter: 386 loss: 3.74310662e-06
Iter: 387 loss: 3.74583396e-06
Iter: 388 loss: 3.74047613e-06
Iter: 389 loss: 3.73796502e-06
Iter: 390 loss: 3.75361424e-06
Iter: 391 loss: 3.7376426e-06
Iter: 392 loss: 3.73465946e-06
Iter: 393 loss: 3.73507714e-06
Iter: 394 loss: 3.73238436e-06
Iter: 395 loss: 3.72851605e-06
Iter: 396 loss: 3.72431168e-06
Iter: 397 loss: 3.72369936e-06
Iter: 398 loss: 3.72221712e-06
Iter: 399 loss: 3.72127147e-06
Iter: 400 loss: 3.71895749e-06
Iter: 401 loss: 3.71715146e-06
Iter: 402 loss: 3.71658e-06
Iter: 403 loss: 3.71263945e-06
Iter: 404 loss: 3.71170654e-06
Iter: 405 loss: 3.70932685e-06
Iter: 406 loss: 3.70529119e-06
Iter: 407 loss: 3.7239829e-06
Iter: 408 loss: 3.70454154e-06
Iter: 409 loss: 3.70071484e-06
Iter: 410 loss: 3.7534428e-06
Iter: 411 loss: 3.70075122e-06
Iter: 412 loss: 3.69840359e-06
Iter: 413 loss: 3.69595591e-06
Iter: 414 loss: 3.69553118e-06
Iter: 415 loss: 3.69182453e-06
Iter: 416 loss: 3.70581893e-06
Iter: 417 loss: 3.69093573e-06
Iter: 418 loss: 3.68754195e-06
Iter: 419 loss: 3.71969554e-06
Iter: 420 loss: 3.68739848e-06
Iter: 421 loss: 3.68491965e-06
Iter: 422 loss: 3.68076212e-06
Iter: 423 loss: 3.68074052e-06
Iter: 424 loss: 3.67692292e-06
Iter: 425 loss: 3.67693474e-06
Iter: 426 loss: 3.67420444e-06
Iter: 427 loss: 3.67395091e-06
Iter: 428 loss: 3.67209054e-06
Iter: 429 loss: 3.66930476e-06
Iter: 430 loss: 3.66896347e-06
Iter: 431 loss: 3.66693666e-06
Iter: 432 loss: 3.66505765e-06
Iter: 433 loss: 3.66474023e-06
Iter: 434 loss: 3.66293352e-06
Iter: 435 loss: 3.65974938e-06
Iter: 436 loss: 3.6597653e-06
Iter: 437 loss: 3.65631104e-06
Iter: 438 loss: 3.65622964e-06
Iter: 439 loss: 3.65344022e-06
Iter: 440 loss: 3.65136339e-06
Iter: 441 loss: 3.65089454e-06
Iter: 442 loss: 3.64848802e-06
Iter: 443 loss: 3.6518245e-06
Iter: 444 loss: 3.64730658e-06
Iter: 445 loss: 3.64469406e-06
Iter: 446 loss: 3.63973595e-06
Iter: 447 loss: 3.74784827e-06
Iter: 448 loss: 3.63964682e-06
Iter: 449 loss: 3.63712115e-06
Iter: 450 loss: 3.63632353e-06
Iter: 451 loss: 3.63431582e-06
Iter: 452 loss: 3.63791446e-06
Iter: 453 loss: 3.63345316e-06
Iter: 454 loss: 3.63157e-06
Iter: 455 loss: 3.63113395e-06
Iter: 456 loss: 3.6299225e-06
Iter: 457 loss: 3.62727678e-06
Iter: 458 loss: 3.65380515e-06
Iter: 459 loss: 3.62712353e-06
Iter: 460 loss: 3.62554556e-06
Iter: 461 loss: 3.62228366e-06
Iter: 462 loss: 3.68534347e-06
Iter: 463 loss: 3.62232049e-06
Iter: 464 loss: 3.61875573e-06
Iter: 465 loss: 3.64059588e-06
Iter: 466 loss: 3.6182314e-06
Iter: 467 loss: 3.61530056e-06
Iter: 468 loss: 3.64689822e-06
Iter: 469 loss: 3.61530601e-06
Iter: 470 loss: 3.61330967e-06
Iter: 471 loss: 3.60962235e-06
Iter: 472 loss: 3.68873702e-06
Iter: 473 loss: 3.60968556e-06
Iter: 474 loss: 3.60508238e-06
Iter: 475 loss: 3.61036473e-06
Iter: 476 loss: 3.60257718e-06
Iter: 477 loss: 3.60257422e-06
Iter: 478 loss: 3.60038462e-06
Iter: 479 loss: 3.59868386e-06
Iter: 480 loss: 3.59515298e-06
Iter: 481 loss: 3.65710957e-06
Iter: 482 loss: 3.59509522e-06
Iter: 483 loss: 3.5924686e-06
Iter: 484 loss: 3.61571142e-06
Iter: 485 loss: 3.59243131e-06
Iter: 486 loss: 3.58981697e-06
Iter: 487 loss: 3.59935575e-06
Iter: 488 loss: 3.58923444e-06
Iter: 489 loss: 3.58659508e-06
Iter: 490 loss: 3.58581315e-06
Iter: 491 loss: 3.58424586e-06
Iter: 492 loss: 3.58166471e-06
Iter: 493 loss: 3.61075945e-06
Iter: 494 loss: 3.58162083e-06
Iter: 495 loss: 3.57920726e-06
Iter: 496 loss: 3.57873841e-06
Iter: 497 loss: 3.57713043e-06
Iter: 498 loss: 3.57402951e-06
Iter: 499 loss: 3.57612976e-06
Iter: 500 loss: 3.5720991e-06
Iter: 501 loss: 3.56956753e-06
Iter: 502 loss: 3.59308615e-06
Iter: 503 loss: 3.56945134e-06
Iter: 504 loss: 3.56664123e-06
Iter: 505 loss: 3.57201793e-06
Iter: 506 loss: 3.5655e-06
Iter: 507 loss: 3.56356941e-06
Iter: 508 loss: 3.56233363e-06
Iter: 509 loss: 3.561624e-06
Iter: 510 loss: 3.55889824e-06
Iter: 511 loss: 3.57258841e-06
Iter: 512 loss: 3.55844304e-06
Iter: 513 loss: 3.55606e-06
Iter: 514 loss: 3.58732359e-06
Iter: 515 loss: 3.555994e-06
Iter: 516 loss: 3.55474549e-06
Iter: 517 loss: 3.5519829e-06
Iter: 518 loss: 3.59122168e-06
Iter: 519 loss: 3.55182237e-06
Iter: 520 loss: 3.54840313e-06
Iter: 521 loss: 3.57562226e-06
Iter: 522 loss: 3.54820645e-06
Iter: 523 loss: 3.54444069e-06
Iter: 524 loss: 3.55112707e-06
Iter: 525 loss: 3.5429282e-06
Iter: 526 loss: 3.54096278e-06
Iter: 527 loss: 3.55000179e-06
Iter: 528 loss: 3.54053373e-06
Iter: 529 loss: 3.53854853e-06
Iter: 530 loss: 3.54132544e-06
Iter: 531 loss: 3.53756559e-06
Iter: 532 loss: 3.53485666e-06
Iter: 533 loss: 3.53496898e-06
Iter: 534 loss: 3.53263931e-06
Iter: 535 loss: 3.53008272e-06
Iter: 536 loss: 3.53539758e-06
Iter: 537 loss: 3.52910502e-06
Iter: 538 loss: 3.52740312e-06
Iter: 539 loss: 3.52739e-06
Iter: 540 loss: 3.5260166e-06
Iter: 541 loss: 3.52384814e-06
Iter: 542 loss: 3.52385314e-06
Iter: 543 loss: 3.52100255e-06
Iter: 544 loss: 3.52286884e-06
Iter: 545 loss: 3.51913741e-06
Iter: 546 loss: 3.51738913e-06
Iter: 547 loss: 3.51724884e-06
Iter: 548 loss: 3.51489166e-06
Iter: 549 loss: 3.51063863e-06
Iter: 550 loss: 3.51065682e-06
Iter: 551 loss: 3.50735695e-06
Iter: 552 loss: 3.52376105e-06
Iter: 553 loss: 3.50669234e-06
Iter: 554 loss: 3.50490063e-06
Iter: 555 loss: 3.50480445e-06
Iter: 556 loss: 3.50352116e-06
Iter: 557 loss: 3.50105893e-06
Iter: 558 loss: 3.55360385e-06
Iter: 559 loss: 3.50102937e-06
Iter: 560 loss: 3.49838206e-06
Iter: 561 loss: 3.52452071e-06
Iter: 562 loss: 3.49826769e-06
Iter: 563 loss: 3.49622837e-06
Iter: 564 loss: 3.50270466e-06
Iter: 565 loss: 3.49566653e-06
Iter: 566 loss: 3.49421066e-06
Iter: 567 loss: 3.49293555e-06
Iter: 568 loss: 3.49251286e-06
Iter: 569 loss: 3.48971935e-06
Iter: 570 loss: 3.49512584e-06
Iter: 571 loss: 3.48856656e-06
Iter: 572 loss: 3.48630374e-06
Iter: 573 loss: 3.48630692e-06
Iter: 574 loss: 3.48484673e-06
Iter: 575 loss: 3.48091703e-06
Iter: 576 loss: 3.50162213e-06
Iter: 577 loss: 3.47964396e-06
Iter: 578 loss: 3.47449873e-06
Iter: 579 loss: 3.51183871e-06
Iter: 580 loss: 3.47407399e-06
Iter: 581 loss: 3.4736679e-06
Iter: 582 loss: 3.47255968e-06
Iter: 583 loss: 3.4713687e-06
Iter: 584 loss: 3.46974116e-06
Iter: 585 loss: 3.46970432e-06
Iter: 586 loss: 3.46795582e-06
Iter: 587 loss: 3.46765296e-06
Iter: 588 loss: 3.46638626e-06
Iter: 589 loss: 3.46356023e-06
Iter: 590 loss: 3.50415962e-06
Iter: 591 loss: 3.46355569e-06
Iter: 592 loss: 3.46250658e-06
Iter: 593 loss: 3.46129968e-06
Iter: 594 loss: 3.46108141e-06
Iter: 595 loss: 3.45934814e-06
Iter: 596 loss: 3.47735295e-06
Iter: 597 loss: 3.4593138e-06
Iter: 598 loss: 3.45780563e-06
Iter: 599 loss: 3.45503622e-06
Iter: 600 loss: 3.51232552e-06
Iter: 601 loss: 3.45505305e-06
Iter: 602 loss: 3.45140074e-06
Iter: 603 loss: 3.46363504e-06
Iter: 604 loss: 3.4504219e-06
Iter: 605 loss: 3.44766e-06
Iter: 606 loss: 3.46970069e-06
Iter: 607 loss: 3.44754244e-06
Iter: 608 loss: 3.44449927e-06
Iter: 609 loss: 3.44873115e-06
Iter: 610 loss: 3.44306454e-06
Iter: 611 loss: 3.44044975e-06
Iter: 612 loss: 3.44373802e-06
Iter: 613 loss: 3.43918714e-06
Iter: 614 loss: 3.4371435e-06
Iter: 615 loss: 3.44189516e-06
Iter: 616 loss: 3.43642523e-06
Iter: 617 loss: 3.43448528e-06
Iter: 618 loss: 3.43447391e-06
Iter: 619 loss: 3.43326587e-06
Iter: 620 loss: 3.43086322e-06
Iter: 621 loss: 3.47129708e-06
Iter: 622 loss: 3.43087299e-06
Iter: 623 loss: 3.42892645e-06
Iter: 624 loss: 3.45834246e-06
Iter: 625 loss: 3.42888052e-06
Iter: 626 loss: 3.42699468e-06
Iter: 627 loss: 3.43063448e-06
Iter: 628 loss: 3.42626481e-06
Iter: 629 loss: 3.42487624e-06
Iter: 630 loss: 3.42466365e-06
Iter: 631 loss: 3.42364e-06
Iter: 632 loss: 3.4216032e-06
Iter: 633 loss: 3.43944475e-06
Iter: 634 loss: 3.42147268e-06
Iter: 635 loss: 3.41963391e-06
Iter: 636 loss: 3.41570922e-06
Iter: 637 loss: 3.48160211e-06
Iter: 638 loss: 3.41566215e-06
Iter: 639 loss: 3.41296982e-06
Iter: 640 loss: 3.45201852e-06
Iter: 641 loss: 3.41302348e-06
Iter: 642 loss: 3.41137616e-06
Iter: 643 loss: 3.42143085e-06
Iter: 644 loss: 3.41116606e-06
Iter: 645 loss: 3.40974111e-06
Iter: 646 loss: 3.4153e-06
Iter: 647 loss: 3.40935821e-06
Iter: 648 loss: 3.40820202e-06
Iter: 649 loss: 3.40601605e-06
Iter: 650 loss: 3.45845433e-06
Iter: 651 loss: 3.40602378e-06
Iter: 652 loss: 3.40429665e-06
Iter: 653 loss: 3.40425458e-06
Iter: 654 loss: 3.40259135e-06
Iter: 655 loss: 3.41042551e-06
Iter: 656 loss: 3.40227325e-06
Iter: 657 loss: 3.40109e-06
Iter: 658 loss: 3.39894859e-06
Iter: 659 loss: 3.44305772e-06
Iter: 660 loss: 3.39903568e-06
Iter: 661 loss: 3.39685243e-06
Iter: 662 loss: 3.42031649e-06
Iter: 663 loss: 3.3967392e-06
Iter: 664 loss: 3.39404482e-06
Iter: 665 loss: 3.39444318e-06
Iter: 666 loss: 3.39201779e-06
Iter: 667 loss: 3.38968744e-06
Iter: 668 loss: 3.39370945e-06
Iter: 669 loss: 3.38878249e-06
Iter: 670 loss: 3.38626046e-06
Iter: 671 loss: 3.41300938e-06
Iter: 672 loss: 3.38630912e-06
Iter: 673 loss: 3.38506493e-06
Iter: 674 loss: 3.38321706e-06
Iter: 675 loss: 3.38318796e-06
Iter: 676 loss: 3.38084237e-06
Iter: 677 loss: 3.39167491e-06
Iter: 678 loss: 3.38045743e-06
Iter: 679 loss: 3.37918937e-06
Iter: 680 loss: 3.37920073e-06
Iter: 681 loss: 3.37801748e-06
Iter: 682 loss: 3.37656593e-06
Iter: 683 loss: 3.37638903e-06
Iter: 684 loss: 3.37388519e-06
Iter: 685 loss: 3.3769802e-06
Iter: 686 loss: 3.37262327e-06
Iter: 687 loss: 3.3703343e-06
Iter: 688 loss: 3.38270638e-06
Iter: 689 loss: 3.37009533e-06
Iter: 690 loss: 3.36728021e-06
Iter: 691 loss: 3.37582969e-06
Iter: 692 loss: 3.36645e-06
Iter: 693 loss: 3.36484914e-06
Iter: 694 loss: 3.36296625e-06
Iter: 695 loss: 3.3628462e-06
Iter: 696 loss: 3.36130984e-06
Iter: 697 loss: 3.36118728e-06
Iter: 698 loss: 3.35965478e-06
Iter: 699 loss: 3.35907544e-06
Iter: 700 loss: 3.35806703e-06
Iter: 701 loss: 3.35656409e-06
Iter: 702 loss: 3.36139624e-06
Iter: 703 loss: 3.35614027e-06
Iter: 704 loss: 3.35503637e-06
Iter: 705 loss: 3.36931294e-06
Iter: 706 loss: 3.35498839e-06
Iter: 707 loss: 3.35402865e-06
Iter: 708 loss: 3.35126151e-06
Iter: 709 loss: 3.36218181e-06
Iter: 710 loss: 3.35015784e-06
Iter: 711 loss: 3.34706738e-06
Iter: 712 loss: 3.37877964e-06
Iter: 713 loss: 3.34695e-06
Iter: 714 loss: 3.34497736e-06
Iter: 715 loss: 3.35919412e-06
Iter: 716 loss: 3.34482183e-06
Iter: 717 loss: 3.34307879e-06
Iter: 718 loss: 3.35323557e-06
Iter: 719 loss: 3.34281663e-06
Iter: 720 loss: 3.34108745e-06
Iter: 721 loss: 3.33856929e-06
Iter: 722 loss: 3.33848311e-06
Iter: 723 loss: 3.33624234e-06
Iter: 724 loss: 3.3604972e-06
Iter: 725 loss: 3.33617845e-06
Iter: 726 loss: 3.33483945e-06
Iter: 727 loss: 3.33594539e-06
Iter: 728 loss: 3.33410071e-06
Iter: 729 loss: 3.33251205e-06
Iter: 730 loss: 3.33252751e-06
Iter: 731 loss: 3.33186631e-06
Iter: 732 loss: 3.33029675e-06
Iter: 733 loss: 3.3506376e-06
Iter: 734 loss: 3.33017715e-06
Iter: 735 loss: 3.32827267e-06
Iter: 736 loss: 3.33472963e-06
Iter: 737 loss: 3.32769332e-06
Iter: 738 loss: 3.32555e-06
Iter: 739 loss: 3.34059769e-06
Iter: 740 loss: 3.32531408e-06
Iter: 741 loss: 3.32391528e-06
Iter: 742 loss: 3.32607965e-06
Iter: 743 loss: 3.32327045e-06
Iter: 744 loss: 3.32181753e-06
Iter: 745 loss: 3.32679974e-06
Iter: 746 loss: 3.3213214e-06
Iter: 747 loss: 3.31989554e-06
Iter: 748 loss: 3.31652427e-06
Iter: 749 loss: 3.35829031e-06
Iter: 750 loss: 3.31633396e-06
Iter: 751 loss: 3.31447154e-06
Iter: 752 loss: 3.3145061e-06
Iter: 753 loss: 3.3130907e-06
Iter: 754 loss: 3.31397314e-06
Iter: 755 loss: 3.3122376e-06
Iter: 756 loss: 3.31073079e-06
Iter: 757 loss: 3.31073625e-06
Iter: 758 loss: 3.30967123e-06
Iter: 759 loss: 3.30812713e-06
Iter: 760 loss: 3.30802959e-06
Iter: 761 loss: 3.30614148e-06
Iter: 762 loss: 3.31325782e-06
Iter: 763 loss: 3.30573334e-06
Iter: 764 loss: 3.3035808e-06
Iter: 765 loss: 3.30378862e-06
Iter: 766 loss: 3.30204966e-06
Iter: 767 loss: 3.30140165e-06
Iter: 768 loss: 3.30086664e-06
Iter: 769 loss: 3.29974182e-06
Iter: 770 loss: 3.298519e-06
Iter: 771 loss: 3.29834711e-06
Iter: 772 loss: 3.29681279e-06
Iter: 773 loss: 3.30555872e-06
Iter: 774 loss: 3.29667182e-06
Iter: 775 loss: 3.29510453e-06
Iter: 776 loss: 3.29733643e-06
Iter: 777 loss: 3.29436239e-06
Iter: 778 loss: 3.29316481e-06
Iter: 779 loss: 3.29658769e-06
Iter: 780 loss: 3.29281306e-06
Iter: 781 loss: 3.29168665e-06
Iter: 782 loss: 3.29738668e-06
Iter: 783 loss: 3.29156819e-06
Iter: 784 loss: 3.29064096e-06
Iter: 785 loss: 3.28858823e-06
Iter: 786 loss: 3.3208521e-06
Iter: 787 loss: 3.28857959e-06
Iter: 788 loss: 3.28645228e-06
Iter: 789 loss: 3.29282057e-06
Iter: 790 loss: 3.28584133e-06
Iter: 791 loss: 3.28346414e-06
Iter: 792 loss: 3.29211116e-06
Iter: 793 loss: 3.28287024e-06
Iter: 794 loss: 3.28142141e-06
Iter: 795 loss: 3.29848285e-06
Iter: 796 loss: 3.28143324e-06
Iter: 797 loss: 3.279873e-06
Iter: 798 loss: 3.28149872e-06
Iter: 799 loss: 3.27902785e-06
Iter: 800 loss: 3.27724979e-06
Iter: 801 loss: 3.2790183e-06
Iter: 802 loss: 3.27624775e-06
Iter: 803 loss: 3.27455291e-06
Iter: 804 loss: 3.27949056e-06
Iter: 805 loss: 3.27407042e-06
Iter: 806 loss: 3.27295311e-06
Iter: 807 loss: 3.27297448e-06
Iter: 808 loss: 3.2717935e-06
Iter: 809 loss: 3.26974441e-06
Iter: 810 loss: 3.26967574e-06
Iter: 811 loss: 3.2681653e-06
Iter: 812 loss: 3.27548742e-06
Iter: 813 loss: 3.26784289e-06
Iter: 814 loss: 3.26644067e-06
Iter: 815 loss: 3.27698945e-06
Iter: 816 loss: 3.26630743e-06
Iter: 817 loss: 3.26489771e-06
Iter: 818 loss: 3.26383042e-06
Iter: 819 loss: 3.26345344e-06
Iter: 820 loss: 3.26229701e-06
Iter: 821 loss: 3.26226836e-06
Iter: 822 loss: 3.26125428e-06
Iter: 823 loss: 3.25982728e-06
Iter: 824 loss: 3.25981136e-06
Iter: 825 loss: 3.25833344e-06
Iter: 826 loss: 3.25798464e-06
Iter: 827 loss: 3.25706242e-06
Iter: 828 loss: 3.2547232e-06
Iter: 829 loss: 3.26417012e-06
Iter: 830 loss: 3.2542448e-06
Iter: 831 loss: 3.25226824e-06
Iter: 832 loss: 3.25952897e-06
Iter: 833 loss: 3.25173e-06
Iter: 834 loss: 3.2501423e-06
Iter: 835 loss: 3.27231032e-06
Iter: 836 loss: 3.25012411e-06
Iter: 837 loss: 3.24889561e-06
Iter: 838 loss: 3.24931852e-06
Iter: 839 loss: 3.24805205e-06
Iter: 840 loss: 3.24644816e-06
Iter: 841 loss: 3.24636221e-06
Iter: 842 loss: 3.24517623e-06
Iter: 843 loss: 3.2433727e-06
Iter: 844 loss: 3.25152178e-06
Iter: 845 loss: 3.24306257e-06
Iter: 846 loss: 3.24222287e-06
Iter: 847 loss: 3.24201119e-06
Iter: 848 loss: 3.24138e-06
Iter: 849 loss: 3.2400369e-06
Iter: 850 loss: 3.26538066e-06
Iter: 851 loss: 3.24006305e-06
Iter: 852 loss: 3.23865925e-06
Iter: 853 loss: 3.24721123e-06
Iter: 854 loss: 3.23856125e-06
Iter: 855 loss: 3.23700328e-06
Iter: 856 loss: 3.23936752e-06
Iter: 857 loss: 3.23618292e-06
Iter: 858 loss: 3.23487984e-06
Iter: 859 loss: 3.23441236e-06
Iter: 860 loss: 3.23363884e-06
Iter: 861 loss: 3.23187828e-06
Iter: 862 loss: 3.23197264e-06
Iter: 863 loss: 3.23100949e-06
Iter: 864 loss: 3.22935966e-06
Iter: 865 loss: 3.26540203e-06
Iter: 866 loss: 3.22931e-06
Iter: 867 loss: 3.22701226e-06
Iter: 868 loss: 3.22695109e-06
Iter: 869 loss: 3.22517781e-06
Iter: 870 loss: 3.22392111e-06
Iter: 871 loss: 3.22380174e-06
Iter: 872 loss: 3.22240385e-06
Iter: 873 loss: 3.22604365e-06
Iter: 874 loss: 3.22184951e-06
Iter: 875 loss: 3.22091842e-06
Iter: 876 loss: 3.220998e-06
Iter: 877 loss: 3.22019059e-06
Iter: 878 loss: 3.21902462e-06
Iter: 879 loss: 3.22335222e-06
Iter: 880 loss: 3.21859761e-06
Iter: 881 loss: 3.21767061e-06
Iter: 882 loss: 3.22824326e-06
Iter: 883 loss: 3.21758102e-06
Iter: 884 loss: 3.21634639e-06
Iter: 885 loss: 3.21508014e-06
Iter: 886 loss: 3.21486414e-06
Iter: 887 loss: 3.21316929e-06
Iter: 888 loss: 3.2163457e-06
Iter: 889 loss: 3.21238076e-06
Iter: 890 loss: 3.21114703e-06
Iter: 891 loss: 3.22529036e-06
Iter: 892 loss: 3.21113703e-06
Iter: 893 loss: 3.20962067e-06
Iter: 894 loss: 3.208419e-06
Iter: 895 loss: 3.20795402e-06
Iter: 896 loss: 3.20627714e-06
Iter: 897 loss: 3.21687935e-06
Iter: 898 loss: 3.20608115e-06
Iter: 899 loss: 3.20419304e-06
Iter: 900 loss: 3.21020048e-06
Iter: 901 loss: 3.20366235e-06
Iter: 902 loss: 3.20269282e-06
Iter: 903 loss: 3.20115964e-06
Iter: 904 loss: 3.20118488e-06
Iter: 905 loss: 3.19912397e-06
Iter: 906 loss: 3.20842901e-06
Iter: 907 loss: 3.19867422e-06
Iter: 908 loss: 3.19751507e-06
Iter: 909 loss: 3.20840036e-06
Iter: 910 loss: 3.19739274e-06
Iter: 911 loss: 3.19582637e-06
Iter: 912 loss: 3.19541869e-06
Iter: 913 loss: 3.19438823e-06
Iter: 914 loss: 3.19242895e-06
Iter: 915 loss: 3.19330297e-06
Iter: 916 loss: 3.19118362e-06
Iter: 917 loss: 3.19009223e-06
Iter: 918 loss: 3.18991829e-06
Iter: 919 loss: 3.18867274e-06
Iter: 920 loss: 3.18814341e-06
Iter: 921 loss: 3.18746834e-06
Iter: 922 loss: 3.18557704e-06
Iter: 923 loss: 3.18731668e-06
Iter: 924 loss: 3.18451771e-06
Iter: 925 loss: 3.18316052e-06
Iter: 926 loss: 3.19458377e-06
Iter: 927 loss: 3.18311049e-06
Iter: 928 loss: 3.18181037e-06
Iter: 929 loss: 3.18707248e-06
Iter: 930 loss: 3.18148227e-06
Iter: 931 loss: 3.1806951e-06
Iter: 932 loss: 3.1816985e-06
Iter: 933 loss: 3.18018328e-06
Iter: 934 loss: 3.17949957e-06
Iter: 935 loss: 3.19032142e-06
Iter: 936 loss: 3.17947342e-06
Iter: 937 loss: 3.17896092e-06
Iter: 938 loss: 3.17731519e-06
Iter: 939 loss: 3.18684397e-06
Iter: 940 loss: 3.17694366e-06
Iter: 941 loss: 3.17494664e-06
Iter: 942 loss: 3.18028515e-06
Iter: 943 loss: 3.17429863e-06
Iter: 944 loss: 3.17280319e-06
Iter: 945 loss: 3.19630226e-06
Iter: 946 loss: 3.17280455e-06
Iter: 947 loss: 3.17111744e-06
Iter: 948 loss: 3.17036961e-06
Iter: 949 loss: 3.16946353e-06
Iter: 950 loss: 3.16765909e-06
Iter: 951 loss: 3.17106446e-06
Iter: 952 loss: 3.16683e-06
Iter: 953 loss: 3.16602222e-06
Iter: 954 loss: 3.16591058e-06
Iter: 955 loss: 3.16494948e-06
Iter: 956 loss: 3.1638931e-06
Iter: 957 loss: 3.16377918e-06
Iter: 958 loss: 3.1624586e-06
Iter: 959 loss: 3.16203818e-06
Iter: 960 loss: 3.16127966e-06
Iter: 961 loss: 3.16102251e-06
Iter: 962 loss: 3.16040428e-06
Iter: 963 loss: 3.15980242e-06
Iter: 964 loss: 3.15860598e-06
Iter: 965 loss: 3.18434286e-06
Iter: 966 loss: 3.15862098e-06
Iter: 967 loss: 3.15697457e-06
Iter: 968 loss: 3.16565183e-06
Iter: 969 loss: 3.15679745e-06
Iter: 970 loss: 3.15525904e-06
Iter: 971 loss: 3.16210026e-06
Iter: 972 loss: 3.15497414e-06
Iter: 973 loss: 3.15393936e-06
Iter: 974 loss: 3.15245416e-06
Iter: 975 loss: 3.15242664e-06
Iter: 976 loss: 3.1504685e-06
Iter: 977 loss: 3.15726334e-06
Iter: 978 loss: 3.1499344e-06
Iter: 979 loss: 3.14881026e-06
Iter: 980 loss: 3.16045225e-06
Iter: 981 loss: 3.14878207e-06
Iter: 982 loss: 3.14763656e-06
Iter: 983 loss: 3.1521e-06
Iter: 984 loss: 3.1473935e-06
Iter: 985 loss: 3.14645854e-06
Iter: 986 loss: 3.14491854e-06
Iter: 987 loss: 3.18214461e-06
Iter: 988 loss: 3.14487397e-06
Iter: 989 loss: 3.14442696e-06
Iter: 990 loss: 3.14409294e-06
Iter: 991 loss: 3.14319823e-06
Iter: 992 loss: 3.14195518e-06
Iter: 993 loss: 3.14188787e-06
Iter: 994 loss: 3.14042518e-06
Iter: 995 loss: 3.14030467e-06
Iter: 996 loss: 3.13927239e-06
Iter: 997 loss: 3.13741748e-06
Iter: 998 loss: 3.15037551e-06
Iter: 999 loss: 3.13722126e-06
Iter: 1000 loss: 3.13525152e-06
Iter: 1001 loss: 3.14578847e-06
Iter: 1002 loss: 3.13499777e-06
Iter: 1003 loss: 3.13409191e-06
Iter: 1004 loss: 3.13414921e-06
Iter: 1005 loss: 3.13336113e-06
Iter: 1006 loss: 3.13197233e-06
Iter: 1007 loss: 3.1423142e-06
Iter: 1008 loss: 3.13184319e-06
Iter: 1009 loss: 3.13097394e-06
Iter: 1010 loss: 3.13012083e-06
Iter: 1011 loss: 3.12989846e-06
Iter: 1012 loss: 3.12847123e-06
Iter: 1013 loss: 3.13063083e-06
Iter: 1014 loss: 3.12783482e-06
Iter: 1015 loss: 3.12642578e-06
Iter: 1016 loss: 3.13678402e-06
Iter: 1017 loss: 3.12628117e-06
Iter: 1018 loss: 3.12525663e-06
Iter: 1019 loss: 3.13196506e-06
Iter: 1020 loss: 3.12515158e-06
Iter: 1021 loss: 3.12402199e-06
Iter: 1022 loss: 3.12286829e-06
Iter: 1023 loss: 3.12263501e-06
Iter: 1024 loss: 3.12146722e-06
Iter: 1025 loss: 3.13430837e-06
Iter: 1026 loss: 3.12141924e-06
Iter: 1027 loss: 3.11995564e-06
Iter: 1028 loss: 3.11968142e-06
Iter: 1029 loss: 3.11872054e-06
Iter: 1030 loss: 3.11729491e-06
Iter: 1031 loss: 3.11738677e-06
Iter: 1032 loss: 3.11624535e-06
Iter: 1033 loss: 3.11517761e-06
Iter: 1034 loss: 3.11512485e-06
Iter: 1035 loss: 3.11410372e-06
Iter: 1036 loss: 3.11500548e-06
Iter: 1037 loss: 3.11341273e-06
Iter: 1038 loss: 3.11238273e-06
Iter: 1039 loss: 3.11350232e-06
Iter: 1040 loss: 3.11180111e-06
Iter: 1041 loss: 3.11073018e-06
Iter: 1042 loss: 3.12333532e-06
Iter: 1043 loss: 3.11067583e-06
Iter: 1044 loss: 3.11008716e-06
Iter: 1045 loss: 3.10825681e-06
Iter: 1046 loss: 3.11355689e-06
Iter: 1047 loss: 3.10720702e-06
Iter: 1048 loss: 3.10533414e-06
Iter: 1049 loss: 3.10530481e-06
Iter: 1050 loss: 3.10380779e-06
Iter: 1051 loss: 3.11786152e-06
Iter: 1052 loss: 3.10384212e-06
Iter: 1053 loss: 3.10224414e-06
Iter: 1054 loss: 3.10179803e-06
Iter: 1055 loss: 3.10097948e-06
Iter: 1056 loss: 3.09909933e-06
Iter: 1057 loss: 3.10360701e-06
Iter: 1058 loss: 3.09847519e-06
Iter: 1059 loss: 3.09741677e-06
Iter: 1060 loss: 3.09737743e-06
Iter: 1061 loss: 3.09655638e-06
Iter: 1062 loss: 3.09564666e-06
Iter: 1063 loss: 3.09551388e-06
Iter: 1064 loss: 3.09412e-06
Iter: 1065 loss: 3.09353732e-06
Iter: 1066 loss: 3.09288157e-06
Iter: 1067 loss: 3.09142115e-06
Iter: 1068 loss: 3.11129952e-06
Iter: 1069 loss: 3.09143661e-06
Iter: 1070 loss: 3.0899057e-06
Iter: 1071 loss: 3.09113329e-06
Iter: 1072 loss: 3.08897461e-06
Iter: 1073 loss: 3.08771723e-06
Iter: 1074 loss: 3.08956828e-06
Iter: 1075 loss: 3.08717154e-06
Iter: 1076 loss: 3.08575954e-06
Iter: 1077 loss: 3.09720531e-06
Iter: 1078 loss: 3.08559811e-06
Iter: 1079 loss: 3.08466429e-06
Iter: 1080 loss: 3.08257881e-06
Iter: 1081 loss: 3.10921087e-06
Iter: 1082 loss: 3.08245581e-06
Iter: 1083 loss: 3.08019276e-06
Iter: 1084 loss: 3.09353732e-06
Iter: 1085 loss: 3.07987511e-06
Iter: 1086 loss: 3.07879509e-06
Iter: 1087 loss: 3.07880964e-06
Iter: 1088 loss: 3.07766231e-06
Iter: 1089 loss: 3.07660571e-06
Iter: 1090 loss: 3.07630603e-06
Iter: 1091 loss: 3.07502137e-06
Iter: 1092 loss: 3.08084304e-06
Iter: 1093 loss: 3.07483106e-06
Iter: 1094 loss: 3.073583e-06
Iter: 1095 loss: 3.08463973e-06
Iter: 1096 loss: 3.07353434e-06
Iter: 1097 loss: 3.07263599e-06
Iter: 1098 loss: 3.07043547e-06
Iter: 1099 loss: 3.08586414e-06
Iter: 1100 loss: 3.06986703e-06
Iter: 1101 loss: 3.06752668e-06
Iter: 1102 loss: 3.1011341e-06
Iter: 1103 loss: 3.06754669e-06
Iter: 1104 loss: 3.06646939e-06
Iter: 1105 loss: 3.06646643e-06
Iter: 1106 loss: 3.06569882e-06
Iter: 1107 loss: 3.06447919e-06
Iter: 1108 loss: 3.06452102e-06
Iter: 1109 loss: 3.06274433e-06
Iter: 1110 loss: 3.06803418e-06
Iter: 1111 loss: 3.06221455e-06
Iter: 1112 loss: 3.0607855e-06
Iter: 1113 loss: 3.07856271e-06
Iter: 1114 loss: 3.0607323e-06
Iter: 1115 loss: 3.06002403e-06
Iter: 1116 loss: 3.05854314e-06
Iter: 1117 loss: 3.08326639e-06
Iter: 1118 loss: 3.05849426e-06
Iter: 1119 loss: 3.0567644e-06
Iter: 1120 loss: 3.06350421e-06
Iter: 1121 loss: 3.05634239e-06
Iter: 1122 loss: 3.05518506e-06
Iter: 1123 loss: 3.05511685e-06
Iter: 1124 loss: 3.0538863e-06
Iter: 1125 loss: 3.05323397e-06
Iter: 1126 loss: 3.05276671e-06
Iter: 1127 loss: 3.05102e-06
Iter: 1128 loss: 3.05385379e-06
Iter: 1129 loss: 3.05025651e-06
Iter: 1130 loss: 3.04790274e-06
Iter: 1131 loss: 3.06876154e-06
Iter: 1132 loss: 3.04791297e-06
Iter: 1133 loss: 3.04704508e-06
Iter: 1134 loss: 3.04488458e-06
Iter: 1135 loss: 3.06688298e-06
Iter: 1136 loss: 3.04467949e-06
Iter: 1137 loss: 3.04265814e-06
Iter: 1138 loss: 3.06257562e-06
Iter: 1139 loss: 3.04257219e-06
Iter: 1140 loss: 3.04068544e-06
Iter: 1141 loss: 3.0582446e-06
Iter: 1142 loss: 3.04062178e-06
Iter: 1143 loss: 3.03981824e-06
Iter: 1144 loss: 3.03871138e-06
Iter: 1145 loss: 3.03862635e-06
Iter: 1146 loss: 3.03707702e-06
Iter: 1147 loss: 3.05289723e-06
Iter: 1148 loss: 3.0370752e-06
Iter: 1149 loss: 3.03566594e-06
Iter: 1150 loss: 3.03560341e-06
Iter: 1151 loss: 3.03462184e-06
Iter: 1152 loss: 3.0331687e-06
Iter: 1153 loss: 3.03322349e-06
Iter: 1154 loss: 3.03202887e-06
Iter: 1155 loss: 3.02967419e-06
Iter: 1156 loss: 3.03759498e-06
Iter: 1157 loss: 3.0289948e-06
Iter: 1158 loss: 3.02776152e-06
Iter: 1159 loss: 3.02773651e-06
Iter: 1160 loss: 3.02679473e-06
Iter: 1161 loss: 3.02550552e-06
Iter: 1162 loss: 3.02538092e-06
Iter: 1163 loss: 3.02420403e-06
Iter: 1164 loss: 3.03711658e-06
Iter: 1165 loss: 3.02419426e-06
Iter: 1166 loss: 3.02284707e-06
Iter: 1167 loss: 3.02325452e-06
Iter: 1168 loss: 3.02179797e-06
Iter: 1169 loss: 3.0207234e-06
Iter: 1170 loss: 3.02024182e-06
Iter: 1171 loss: 3.01968794e-06
Iter: 1172 loss: 3.01857722e-06
Iter: 1173 loss: 3.01856653e-06
Iter: 1174 loss: 3.01732393e-06
Iter: 1175 loss: 3.01750288e-06
Iter: 1176 loss: 3.01639807e-06
Iter: 1177 loss: 3.01513819e-06
Iter: 1178 loss: 3.01488353e-06
Iter: 1179 loss: 3.01408068e-06
Iter: 1180 loss: 3.0123017e-06
Iter: 1181 loss: 3.0348865e-06
Iter: 1182 loss: 3.01236764e-06
Iter: 1183 loss: 3.01116734e-06
Iter: 1184 loss: 3.00879083e-06
Iter: 1185 loss: 3.04782043e-06
Iter: 1186 loss: 3.00874126e-06
Iter: 1187 loss: 3.00649572e-06
Iter: 1188 loss: 3.02187163e-06
Iter: 1189 loss: 3.00632655e-06
Iter: 1190 loss: 3.0051242e-06
Iter: 1191 loss: 3.00510692e-06
Iter: 1192 loss: 3.00399734e-06
Iter: 1193 loss: 3.00479905e-06
Iter: 1194 loss: 3.00338797e-06
Iter: 1195 loss: 3.00229021e-06
Iter: 1196 loss: 3.00439297e-06
Iter: 1197 loss: 3.00183228e-06
Iter: 1198 loss: 3.00066858e-06
Iter: 1199 loss: 3.01025875e-06
Iter: 1200 loss: 3.0005433e-06
Iter: 1201 loss: 2.99955082e-06
Iter: 1202 loss: 2.99846693e-06
Iter: 1203 loss: 2.99835574e-06
Iter: 1204 loss: 2.9968096e-06
Iter: 1205 loss: 2.99692647e-06
Iter: 1206 loss: 2.99568569e-06
Iter: 1207 loss: 2.99484259e-06
Iter: 1208 loss: 2.99453313e-06
Iter: 1209 loss: 2.99347562e-06
Iter: 1210 loss: 2.99117164e-06
Iter: 1211 loss: 3.02894387e-06
Iter: 1212 loss: 2.99118051e-06
Iter: 1213 loss: 2.98956297e-06
Iter: 1214 loss: 3.00651027e-06
Iter: 1215 loss: 2.98952205e-06
Iter: 1216 loss: 2.98817758e-06
Iter: 1217 loss: 2.99359317e-06
Iter: 1218 loss: 2.98779196e-06
Iter: 1219 loss: 2.9865746e-06
Iter: 1220 loss: 2.98566283e-06
Iter: 1221 loss: 2.98527902e-06
Iter: 1222 loss: 2.98394116e-06
Iter: 1223 loss: 2.9860139e-06
Iter: 1224 loss: 2.98334e-06
Iter: 1225 loss: 2.98233567e-06
Iter: 1226 loss: 2.98221858e-06
Iter: 1227 loss: 2.98138684e-06
Iter: 1228 loss: 2.98024361e-06
Iter: 1229 loss: 2.98016607e-06
Iter: 1230 loss: 2.97918541e-06
Iter: 1231 loss: 2.97920292e-06
Iter: 1232 loss: 2.97826796e-06
Iter: 1233 loss: 2.97705924e-06
Iter: 1234 loss: 2.97697943e-06
Iter: 1235 loss: 2.97540646e-06
Iter: 1236 loss: 2.97716178e-06
Iter: 1237 loss: 2.97462088e-06
Iter: 1238 loss: 2.97309316e-06
Iter: 1239 loss: 2.98312921e-06
Iter: 1240 loss: 2.9728817e-06
Iter: 1241 loss: 2.97137626e-06
Iter: 1242 loss: 2.97960742e-06
Iter: 1243 loss: 2.97107454e-06
Iter: 1244 loss: 2.96988514e-06
Iter: 1245 loss: 2.96955932e-06
Iter: 1246 loss: 2.9688058e-06
Iter: 1247 loss: 2.96763687e-06
Iter: 1248 loss: 2.97315319e-06
Iter: 1249 loss: 2.96749022e-06
Iter: 1250 loss: 2.96603548e-06
Iter: 1251 loss: 2.96934786e-06
Iter: 1252 loss: 2.96555754e-06
Iter: 1253 loss: 2.9644234e-06
Iter: 1254 loss: 2.96256303e-06
Iter: 1255 loss: 2.96258531e-06
Iter: 1256 loss: 2.96091412e-06
Iter: 1257 loss: 2.97880479e-06
Iter: 1258 loss: 2.96081544e-06
Iter: 1259 loss: 2.95935047e-06
Iter: 1260 loss: 2.97062684e-06
Iter: 1261 loss: 2.95931341e-06
Iter: 1262 loss: 2.95823975e-06
Iter: 1263 loss: 2.95752488e-06
Iter: 1264 loss: 2.95708196e-06
Iter: 1265 loss: 2.95616746e-06
Iter: 1266 loss: 2.95615e-06
Iter: 1267 loss: 2.95538121e-06
Iter: 1268 loss: 2.95354721e-06
Iter: 1269 loss: 2.96657709e-06
Iter: 1270 loss: 2.9531443e-06
Iter: 1271 loss: 2.9511209e-06
Iter: 1272 loss: 2.96546864e-06
Iter: 1273 loss: 2.95100426e-06
Iter: 1274 loss: 2.94999427e-06
Iter: 1275 loss: 2.94997244e-06
Iter: 1276 loss: 2.94929214e-06
Iter: 1277 loss: 2.94825531e-06
Iter: 1278 loss: 2.94815368e-06
Iter: 1279 loss: 2.94655365e-06
Iter: 1280 loss: 2.94695133e-06
Iter: 1281 loss: 2.94542951e-06
Iter: 1282 loss: 2.94415349e-06
Iter: 1283 loss: 2.94410484e-06
Iter: 1284 loss: 2.94315578e-06
Iter: 1285 loss: 2.94240704e-06
Iter: 1286 loss: 2.94213783e-06
Iter: 1287 loss: 2.94066035e-06
Iter: 1288 loss: 2.94043502e-06
Iter: 1289 loss: 2.93931953e-06
Iter: 1290 loss: 2.93835524e-06
Iter: 1291 loss: 2.93832136e-06
Iter: 1292 loss: 2.9372145e-06
Iter: 1293 loss: 2.93764629e-06
Iter: 1294 loss: 2.93642074e-06
Iter: 1295 loss: 2.93556036e-06
Iter: 1296 loss: 2.94076426e-06
Iter: 1297 loss: 2.93546418e-06
Iter: 1298 loss: 2.93446374e-06
Iter: 1299 loss: 2.93460653e-06
Iter: 1300 loss: 2.93377593e-06
Iter: 1301 loss: 2.93277753e-06
Iter: 1302 loss: 2.93176049e-06
Iter: 1303 loss: 2.93154676e-06
Iter: 1304 loss: 2.93008043e-06
Iter: 1305 loss: 2.94664233e-06
Iter: 1306 loss: 2.93000858e-06
Iter: 1307 loss: 2.92843652e-06
Iter: 1308 loss: 2.93154358e-06
Iter: 1309 loss: 2.92773211e-06
Iter: 1310 loss: 2.926668e-06
Iter: 1311 loss: 2.92682398e-06
Iter: 1312 loss: 2.92578557e-06
Iter: 1313 loss: 2.92437062e-06
Iter: 1314 loss: 2.93606695e-06
Iter: 1315 loss: 2.92433242e-06
Iter: 1316 loss: 2.92307323e-06
Iter: 1317 loss: 2.92467212e-06
Iter: 1318 loss: 2.92236905e-06
Iter: 1319 loss: 2.92117966e-06
Iter: 1320 loss: 2.92269897e-06
Iter: 1321 loss: 2.92045752e-06
Iter: 1322 loss: 2.91946299e-06
Iter: 1323 loss: 2.91944571e-06
Iter: 1324 loss: 2.91858578e-06
Iter: 1325 loss: 2.91699098e-06
Iter: 1326 loss: 2.94033566e-06
Iter: 1327 loss: 2.91698302e-06
Iter: 1328 loss: 2.91627521e-06
Iter: 1329 loss: 2.91568199e-06
Iter: 1330 loss: 2.91545734e-06
Iter: 1331 loss: 2.91396441e-06
Iter: 1332 loss: 2.92295226e-06
Iter: 1333 loss: 2.91382116e-06
Iter: 1334 loss: 2.91284618e-06
Iter: 1335 loss: 2.91075094e-06
Iter: 1336 loss: 2.94617303e-06
Iter: 1337 loss: 2.91073343e-06
Iter: 1338 loss: 2.90857088e-06
Iter: 1339 loss: 2.92482036e-06
Iter: 1340 loss: 2.90839398e-06
Iter: 1341 loss: 2.90701746e-06
Iter: 1342 loss: 2.92914137e-06
Iter: 1343 loss: 2.90699404e-06
Iter: 1344 loss: 2.90621233e-06
Iter: 1345 loss: 2.90469575e-06
Iter: 1346 loss: 2.93509538e-06
Iter: 1347 loss: 2.90472121e-06
Iter: 1348 loss: 2.90352455e-06
Iter: 1349 loss: 2.90354865e-06
Iter: 1350 loss: 2.90262187e-06
Iter: 1351 loss: 2.90554203e-06
Iter: 1352 loss: 2.90243906e-06
Iter: 1353 loss: 2.90161211e-06
Iter: 1354 loss: 2.90102798e-06
Iter: 1355 loss: 2.90071398e-06
Iter: 1356 loss: 2.89934792e-06
Iter: 1357 loss: 2.90039088e-06
Iter: 1358 loss: 2.89849686e-06
Iter: 1359 loss: 2.89767831e-06
Iter: 1360 loss: 2.89751233e-06
Iter: 1361 loss: 2.89666787e-06
Iter: 1362 loss: 2.89537707e-06
Iter: 1363 loss: 2.89528271e-06
Iter: 1364 loss: 2.8940924e-06
Iter: 1365 loss: 2.91339848e-06
Iter: 1366 loss: 2.89405489e-06
Iter: 1367 loss: 2.89296395e-06
Iter: 1368 loss: 2.89222908e-06
Iter: 1369 loss: 2.89179525e-06
Iter: 1370 loss: 2.89041554e-06
Iter: 1371 loss: 2.88934871e-06
Iter: 1372 loss: 2.88892033e-06
Iter: 1373 loss: 2.88878891e-06
Iter: 1374 loss: 2.88786669e-06
Iter: 1375 loss: 2.88727711e-06
Iter: 1376 loss: 2.88639785e-06
Iter: 1377 loss: 2.88634283e-06
Iter: 1378 loss: 2.88533329e-06
Iter: 1379 loss: 2.88608021e-06
Iter: 1380 loss: 2.88466072e-06
Iter: 1381 loss: 2.88349588e-06
Iter: 1382 loss: 2.89927198e-06
Iter: 1383 loss: 2.88348247e-06
Iter: 1384 loss: 2.88254796e-06
Iter: 1385 loss: 2.88130877e-06
Iter: 1386 loss: 2.88129422e-06
Iter: 1387 loss: 2.87961984e-06
Iter: 1388 loss: 2.88411456e-06
Iter: 1389 loss: 2.87901821e-06
Iter: 1390 loss: 2.87747912e-06
Iter: 1391 loss: 2.88222282e-06
Iter: 1392 loss: 2.87692956e-06
Iter: 1393 loss: 2.8756815e-06
Iter: 1394 loss: 2.87563535e-06
Iter: 1395 loss: 2.8749073e-06
Iter: 1396 loss: 2.87367766e-06
Iter: 1397 loss: 2.87369085e-06
Iter: 1398 loss: 2.87220382e-06
Iter: 1399 loss: 2.89296804e-06
Iter: 1400 loss: 2.87223793e-06
Iter: 1401 loss: 2.87141052e-06
Iter: 1402 loss: 2.87017542e-06
Iter: 1403 loss: 2.87014609e-06
Iter: 1404 loss: 2.86891645e-06
Iter: 1405 loss: 2.87561852e-06
Iter: 1406 loss: 2.86872228e-06
Iter: 1407 loss: 2.86770364e-06
Iter: 1408 loss: 2.88103911e-06
Iter: 1409 loss: 2.86773275e-06
Iter: 1410 loss: 2.867067e-06
Iter: 1411 loss: 2.8656782e-06
Iter: 1412 loss: 2.88342244e-06
Iter: 1413 loss: 2.86556747e-06
Iter: 1414 loss: 2.86423756e-06
Iter: 1415 loss: 2.8759107e-06
Iter: 1416 loss: 2.86405066e-06
Iter: 1417 loss: 2.86230897e-06
Iter: 1418 loss: 2.86270188e-06
Iter: 1419 loss: 2.86099498e-06
Iter: 1420 loss: 2.8597e-06
Iter: 1421 loss: 2.86385784e-06
Iter: 1422 loss: 2.85928809e-06
Iter: 1423 loss: 2.85783176e-06
Iter: 1424 loss: 2.85925898e-06
Iter: 1425 loss: 2.85702868e-06
Iter: 1426 loss: 2.85608508e-06
Iter: 1427 loss: 2.85603073e-06
Iter: 1428 loss: 2.85526198e-06
Iter: 1429 loss: 2.85477859e-06
Iter: 1430 loss: 2.85440092e-06
Iter: 1431 loss: 2.85353735e-06
Iter: 1432 loss: 2.8621273e-06
Iter: 1433 loss: 2.85353462e-06
Iter: 1434 loss: 2.8525883e-06
Iter: 1435 loss: 2.85133547e-06
Iter: 1436 loss: 2.85134684e-06
Iter: 1437 loss: 2.84991665e-06
Iter: 1438 loss: 2.85515716e-06
Iter: 1439 loss: 2.84950102e-06
Iter: 1440 loss: 2.84874113e-06
Iter: 1441 loss: 2.8486902e-06
Iter: 1442 loss: 2.84801649e-06
Iter: 1443 loss: 2.84597718e-06
Iter: 1444 loss: 2.86402633e-06
Iter: 1445 loss: 2.84578027e-06
Iter: 1446 loss: 2.84381713e-06
Iter: 1447 loss: 2.8576917e-06
Iter: 1448 loss: 2.84368571e-06
Iter: 1449 loss: 2.84202542e-06
Iter: 1450 loss: 2.85793908e-06
Iter: 1451 loss: 2.84205953e-06
Iter: 1452 loss: 2.84109819e-06
Iter: 1453 loss: 2.8403565e-06
Iter: 1454 loss: 2.84002385e-06
Iter: 1455 loss: 2.83869372e-06
Iter: 1456 loss: 2.84292219e-06
Iter: 1457 loss: 2.83831582e-06
Iter: 1458 loss: 2.83746795e-06
Iter: 1459 loss: 2.85155807e-06
Iter: 1460 loss: 2.83746158e-06
Iter: 1461 loss: 2.8366062e-06
Iter: 1462 loss: 2.8368413e-06
Iter: 1463 loss: 2.83601435e-06
Iter: 1464 loss: 2.83487975e-06
Iter: 1465 loss: 2.83658187e-06
Iter: 1466 loss: 2.83421014e-06
Iter: 1467 loss: 2.83304189e-06
Iter: 1468 loss: 2.84466455e-06
Iter: 1469 loss: 2.83294457e-06
Iter: 1470 loss: 2.83225199e-06
Iter: 1471 loss: 2.83048689e-06
Iter: 1472 loss: 2.84916041e-06
Iter: 1473 loss: 2.83037116e-06
Iter: 1474 loss: 2.82905376e-06
Iter: 1475 loss: 2.82892142e-06
Iter: 1476 loss: 2.82761766e-06
Iter: 1477 loss: 2.82816973e-06
Iter: 1478 loss: 2.82655219e-06
Iter: 1479 loss: 2.8256884e-06
Iter: 1480 loss: 2.82557585e-06
Iter: 1481 loss: 2.82491192e-06
Iter: 1482 loss: 2.8237946e-06
Iter: 1483 loss: 2.84102794e-06
Iter: 1484 loss: 2.82380597e-06
Iter: 1485 loss: 2.8228535e-06
Iter: 1486 loss: 2.8231666e-06
Iter: 1487 loss: 2.82217707e-06
Iter: 1488 loss: 2.82133169e-06
Iter: 1489 loss: 2.82055839e-06
Iter: 1490 loss: 2.82032852e-06
Iter: 1491 loss: 2.81892267e-06
Iter: 1492 loss: 2.83517966e-06
Iter: 1493 loss: 2.81889515e-06
Iter: 1494 loss: 2.81758139e-06
Iter: 1495 loss: 2.82018959e-06
Iter: 1496 loss: 2.81706593e-06
Iter: 1497 loss: 2.81571511e-06
Iter: 1498 loss: 2.8192178e-06
Iter: 1499 loss: 2.81528014e-06
Iter: 1500 loss: 2.81407574e-06
Iter: 1501 loss: 2.82122437e-06
Iter: 1502 loss: 2.81392909e-06
Iter: 1503 loss: 2.81282928e-06
Iter: 1504 loss: 2.81079e-06
Iter: 1505 loss: 2.8108102e-06
Iter: 1506 loss: 2.80942368e-06
Iter: 1507 loss: 2.82800556e-06
Iter: 1508 loss: 2.80938775e-06
Iter: 1509 loss: 2.80831705e-06
Iter: 1510 loss: 2.81624193e-06
Iter: 1511 loss: 2.80819427e-06
Iter: 1512 loss: 2.80753102e-06
Iter: 1513 loss: 2.8061645e-06
Iter: 1514 loss: 2.83340432e-06
Iter: 1515 loss: 2.80608879e-06
Iter: 1516 loss: 2.80501445e-06
Iter: 1517 loss: 2.81899975e-06
Iter: 1518 loss: 2.80495078e-06
Iter: 1519 loss: 2.80381914e-06
Iter: 1520 loss: 2.80665631e-06
Iter: 1521 loss: 2.80343602e-06
Iter: 1522 loss: 2.80255472e-06
Iter: 1523 loss: 2.80092536e-06
Iter: 1524 loss: 2.80093082e-06
Iter: 1525 loss: 2.79925371e-06
Iter: 1526 loss: 2.81699477e-06
Iter: 1527 loss: 2.79925575e-06
Iter: 1528 loss: 2.79796132e-06
Iter: 1529 loss: 2.80692439e-06
Iter: 1530 loss: 2.79786332e-06
Iter: 1531 loss: 2.79669234e-06
Iter: 1532 loss: 2.79630467e-06
Iter: 1533 loss: 2.7956653e-06
Iter: 1534 loss: 2.79434744e-06
Iter: 1535 loss: 2.81030589e-06
Iter: 1536 loss: 2.79433516e-06
Iter: 1537 loss: 2.79343953e-06
Iter: 1538 loss: 2.79379469e-06
Iter: 1539 loss: 2.79293977e-06
Iter: 1540 loss: 2.79182814e-06
Iter: 1541 loss: 2.79152823e-06
Iter: 1542 loss: 2.79075948e-06
Iter: 1543 loss: 2.79010965e-06
Iter: 1544 loss: 2.78995822e-06
Iter: 1545 loss: 2.78929087e-06
Iter: 1546 loss: 2.78792413e-06
Iter: 1547 loss: 2.81700068e-06
Iter: 1548 loss: 2.78798143e-06
Iter: 1549 loss: 2.78667108e-06
Iter: 1550 loss: 2.78756806e-06
Iter: 1551 loss: 2.78589096e-06
Iter: 1552 loss: 2.78442303e-06
Iter: 1553 loss: 2.78444736e-06
Iter: 1554 loss: 2.78356447e-06
Iter: 1555 loss: 2.78283892e-06
Iter: 1556 loss: 2.78250809e-06
Iter: 1557 loss: 2.78126367e-06
Iter: 1558 loss: 2.78093921e-06
Iter: 1559 loss: 2.78014e-06
Iter: 1560 loss: 2.77967411e-06
Iter: 1561 loss: 2.77921754e-06
Iter: 1562 loss: 2.77850086e-06
Iter: 1563 loss: 2.77772824e-06
Iter: 1564 loss: 2.77757181e-06
Iter: 1565 loss: 2.77665413e-06
Iter: 1566 loss: 2.78695e-06
Iter: 1567 loss: 2.7765991e-06
Iter: 1568 loss: 2.77590925e-06
Iter: 1569 loss: 2.77612185e-06
Iter: 1570 loss: 2.77537492e-06
Iter: 1571 loss: 2.77428262e-06
Iter: 1572 loss: 2.77405888e-06
Iter: 1573 loss: 2.77331674e-06
Iter: 1574 loss: 2.77246431e-06
Iter: 1575 loss: 2.78780408e-06
Iter: 1576 loss: 2.77243657e-06
Iter: 1577 loss: 2.77126105e-06
Iter: 1578 loss: 2.77040544e-06
Iter: 1579 loss: 2.7700903e-06
Iter: 1580 loss: 2.7686192e-06
Iter: 1581 loss: 2.76900801e-06
Iter: 1582 loss: 2.76755145e-06
Iter: 1583 loss: 2.76607852e-06
Iter: 1584 loss: 2.78375046e-06
Iter: 1585 loss: 2.76601372e-06
Iter: 1586 loss: 2.76444894e-06
Iter: 1587 loss: 2.76689934e-06
Iter: 1588 loss: 2.76380479e-06
Iter: 1589 loss: 2.76273045e-06
Iter: 1590 loss: 2.76193509e-06
Iter: 1591 loss: 2.76157334e-06
Iter: 1592 loss: 2.76052515e-06
Iter: 1593 loss: 2.76054652e-06
Iter: 1594 loss: 2.75931779e-06
Iter: 1595 loss: 2.76074024e-06
Iter: 1596 loss: 2.75877073e-06
Iter: 1597 loss: 2.7578144e-06
Iter: 1598 loss: 2.76143237e-06
Iter: 1599 loss: 2.75763864e-06
Iter: 1600 loss: 2.75668026e-06
Iter: 1601 loss: 2.76012861e-06
Iter: 1602 loss: 2.75645743e-06
Iter: 1603 loss: 2.75553543e-06
Iter: 1604 loss: 2.75444245e-06
Iter: 1605 loss: 2.75430102e-06
Iter: 1606 loss: 2.75299521e-06
Iter: 1607 loss: 2.76474088e-06
Iter: 1608 loss: 2.75286925e-06
Iter: 1609 loss: 2.75167213e-06
Iter: 1610 loss: 2.75807542e-06
Iter: 1611 loss: 2.75148341e-06
Iter: 1612 loss: 2.75043658e-06
Iter: 1613 loss: 2.74894478e-06
Iter: 1614 loss: 2.74896774e-06
Iter: 1615 loss: 2.74740205e-06
Iter: 1616 loss: 2.75378852e-06
Iter: 1617 loss: 2.74706417e-06
Iter: 1618 loss: 2.74615354e-06
Iter: 1619 loss: 2.74611602e-06
Iter: 1620 loss: 2.74555623e-06
Iter: 1621 loss: 2.74401782e-06
Iter: 1622 loss: 2.76106698e-06
Iter: 1623 loss: 2.74393278e-06
Iter: 1624 loss: 2.74223908e-06
Iter: 1625 loss: 2.75006937e-06
Iter: 1626 loss: 2.74190779e-06
Iter: 1627 loss: 2.74106105e-06
Iter: 1628 loss: 2.74096124e-06
Iter: 1629 loss: 2.74020294e-06
Iter: 1630 loss: 2.73849037e-06
Iter: 1631 loss: 2.76831611e-06
Iter: 1632 loss: 2.73845694e-06
Iter: 1633 loss: 2.73743626e-06
Iter: 1634 loss: 2.73725027e-06
Iter: 1635 loss: 2.73649266e-06
Iter: 1636 loss: 2.73549631e-06
Iter: 1637 loss: 2.73539445e-06
Iter: 1638 loss: 2.73407295e-06
Iter: 1639 loss: 2.73719434e-06
Iter: 1640 loss: 2.73359137e-06
Iter: 1641 loss: 2.73238379e-06
Iter: 1642 loss: 2.74795457e-06
Iter: 1643 loss: 2.73237265e-06
Iter: 1644 loss: 2.73127739e-06
Iter: 1645 loss: 2.73127398e-06
Iter: 1646 loss: 2.73052638e-06
Iter: 1647 loss: 2.72946909e-06
Iter: 1648 loss: 2.72898728e-06
Iter: 1649 loss: 2.72846137e-06
Iter: 1650 loss: 2.72714806e-06
Iter: 1651 loss: 2.7446074e-06
Iter: 1652 loss: 2.72714669e-06
Iter: 1653 loss: 2.72588045e-06
Iter: 1654 loss: 2.72780608e-06
Iter: 1655 loss: 2.72526404e-06
Iter: 1656 loss: 2.72421812e-06
Iter: 1657 loss: 2.72235093e-06
Iter: 1658 loss: 2.72238412e-06
Iter: 1659 loss: 2.72121133e-06
Iter: 1660 loss: 2.72107923e-06
Iter: 1661 loss: 2.71972567e-06
Iter: 1662 loss: 2.72124339e-06
Iter: 1663 loss: 2.71901672e-06
Iter: 1664 loss: 2.71810131e-06
Iter: 1665 loss: 2.72211582e-06
Iter: 1666 loss: 2.71794283e-06
Iter: 1667 loss: 2.71687395e-06
Iter: 1668 loss: 2.71783324e-06
Iter: 1669 loss: 2.71621252e-06
Iter: 1670 loss: 2.71512e-06
Iter: 1671 loss: 2.71540489e-06
Iter: 1672 loss: 2.7143326e-06
Iter: 1673 loss: 2.7133633e-06
Iter: 1674 loss: 2.71339763e-06
Iter: 1675 loss: 2.71245699e-06
Iter: 1676 loss: 2.71209e-06
Iter: 1677 loss: 2.71159115e-06
Iter: 1678 loss: 2.71051795e-06
Iter: 1679 loss: 2.71244676e-06
Iter: 1680 loss: 2.70992905e-06
Iter: 1681 loss: 2.70877354e-06
Iter: 1682 loss: 2.71006593e-06
Iter: 1683 loss: 2.70816645e-06
Iter: 1684 loss: 2.70686814e-06
Iter: 1685 loss: 2.72464104e-06
Iter: 1686 loss: 2.70684632e-06
Iter: 1687 loss: 2.70596638e-06
Iter: 1688 loss: 2.7044041e-06
Iter: 1689 loss: 2.70438545e-06
Iter: 1690 loss: 2.70322266e-06
Iter: 1691 loss: 2.7068304e-06
Iter: 1692 loss: 2.70281816e-06
Iter: 1693 loss: 2.70159717e-06
Iter: 1694 loss: 2.72035413e-06
Iter: 1695 loss: 2.70153896e-06
Iter: 1696 loss: 2.70067017e-06
Iter: 1697 loss: 2.70017881e-06
Iter: 1698 loss: 2.6997418e-06
Iter: 1699 loss: 2.6989344e-06
Iter: 1700 loss: 2.71078989e-06
Iter: 1701 loss: 2.69889233e-06
Iter: 1702 loss: 2.69815564e-06
Iter: 1703 loss: 2.69672501e-06
Iter: 1704 loss: 2.72770649e-06
Iter: 1705 loss: 2.69667316e-06
Iter: 1706 loss: 2.6953735e-06
Iter: 1707 loss: 2.70213582e-06
Iter: 1708 loss: 2.69518387e-06
Iter: 1709 loss: 2.69375118e-06
Iter: 1710 loss: 2.70332202e-06
Iter: 1711 loss: 2.69363045e-06
Iter: 1712 loss: 2.6928833e-06
Iter: 1713 loss: 2.6919447e-06
Iter: 1714 loss: 2.69181851e-06
Iter: 1715 loss: 2.69035581e-06
Iter: 1716 loss: 2.69424845e-06
Iter: 1717 loss: 2.68971257e-06
Iter: 1718 loss: 2.68896883e-06
Iter: 1719 loss: 2.68892973e-06
Iter: 1720 loss: 2.68830399e-06
Iter: 1721 loss: 2.68816348e-06
Iter: 1722 loss: 2.68764097e-06
Iter: 1723 loss: 2.68690064e-06
Iter: 1724 loss: 2.68687427e-06
Iter: 1725 loss: 2.68628673e-06
Iter: 1726 loss: 2.68555459e-06
Iter: 1727 loss: 2.6951061e-06
Iter: 1728 loss: 2.68545023e-06
Iter: 1729 loss: 2.68450685e-06
Iter: 1730 loss: 2.68557e-06
Iter: 1731 loss: 2.68401982e-06
Iter: 1732 loss: 2.68318604e-06
Iter: 1733 loss: 2.68359599e-06
Iter: 1734 loss: 2.68260374e-06
Iter: 1735 loss: 2.68119e-06
Iter: 1736 loss: 2.68833173e-06
Iter: 1737 loss: 2.68099438e-06
Iter: 1738 loss: 2.68014242e-06
Iter: 1739 loss: 2.67850055e-06
Iter: 1740 loss: 2.70847613e-06
Iter: 1741 loss: 2.67839823e-06
Iter: 1742 loss: 2.6776479e-06
Iter: 1743 loss: 2.67724363e-06
Iter: 1744 loss: 2.67656355e-06
Iter: 1745 loss: 2.67555015e-06
Iter: 1746 loss: 2.67548489e-06
Iter: 1747 loss: 2.67432915e-06
Iter: 1748 loss: 2.67628934e-06
Iter: 1749 loss: 2.67370888e-06
Iter: 1750 loss: 2.67270434e-06
Iter: 1751 loss: 2.68249141e-06
Iter: 1752 loss: 2.67262567e-06
Iter: 1753 loss: 2.67187988e-06
Iter: 1754 loss: 2.67609175e-06
Iter: 1755 loss: 2.67175665e-06
Iter: 1756 loss: 2.67097289e-06
Iter: 1757 loss: 2.67018049e-06
Iter: 1758 loss: 2.67008386e-06
Iter: 1759 loss: 2.66874258e-06
Iter: 1760 loss: 2.67110045e-06
Iter: 1761 loss: 2.66822644e-06
Iter: 1762 loss: 2.66709867e-06
Iter: 1763 loss: 2.66709958e-06
Iter: 1764 loss: 2.66642951e-06
Iter: 1765 loss: 2.665382e-06
Iter: 1766 loss: 2.66537245e-06
Iter: 1767 loss: 2.66412781e-06
Iter: 1768 loss: 2.67905352e-06
Iter: 1769 loss: 2.66415282e-06
Iter: 1770 loss: 2.66333655e-06
Iter: 1771 loss: 2.66230131e-06
Iter: 1772 loss: 2.66227175e-06
Iter: 1773 loss: 2.66143388e-06
Iter: 1774 loss: 2.6614141e-06
Iter: 1775 loss: 2.66056304e-06
Iter: 1776 loss: 2.66193251e-06
Iter: 1777 loss: 2.66022857e-06
Iter: 1778 loss: 2.65956214e-06
Iter: 1779 loss: 2.65874337e-06
Iter: 1780 loss: 2.65868084e-06
Iter: 1781 loss: 2.65767358e-06
Iter: 1782 loss: 2.66470761e-06
Iter: 1783 loss: 2.6576547e-06
Iter: 1784 loss: 2.65651897e-06
Iter: 1785 loss: 2.66040911e-06
Iter: 1786 loss: 2.65635367e-06
Iter: 1787 loss: 2.65510198e-06
Iter: 1788 loss: 2.65547396e-06
Iter: 1789 loss: 2.65417657e-06
Iter: 1790 loss: 2.65287986e-06
Iter: 1791 loss: 2.65452718e-06
Iter: 1792 loss: 2.65223184e-06
Iter: 1793 loss: 2.65118933e-06
Iter: 1794 loss: 2.6511716e-06
Iter: 1795 loss: 2.65023573e-06
Iter: 1796 loss: 2.64929599e-06
Iter: 1797 loss: 2.64911478e-06
Iter: 1798 loss: 2.64802111e-06
Iter: 1799 loss: 2.65674498e-06
Iter: 1800 loss: 2.64799974e-06
Iter: 1801 loss: 2.64694904e-06
Iter: 1802 loss: 2.64796245e-06
Iter: 1803 loss: 2.64629875e-06
Iter: 1804 loss: 2.64548476e-06
Iter: 1805 loss: 2.64599316e-06
Iter: 1806 loss: 2.64500545e-06
Iter: 1807 loss: 2.64378673e-06
Iter: 1808 loss: 2.65577864e-06
Iter: 1809 loss: 2.64374785e-06
Iter: 1810 loss: 2.64313599e-06
Iter: 1811 loss: 2.64213713e-06
Iter: 1812 loss: 2.64214304e-06
Iter: 1813 loss: 2.64066966e-06
Iter: 1814 loss: 2.6421344e-06
Iter: 1815 loss: 2.63986e-06
Iter: 1816 loss: 2.63858465e-06
Iter: 1817 loss: 2.63852e-06
Iter: 1818 loss: 2.63741595e-06
Iter: 1819 loss: 2.63894322e-06
Iter: 1820 loss: 2.63681159e-06
Iter: 1821 loss: 2.63584025e-06
Iter: 1822 loss: 2.63699258e-06
Iter: 1823 loss: 2.63529137e-06
Iter: 1824 loss: 2.63419133e-06
Iter: 1825 loss: 2.64007167e-06
Iter: 1826 loss: 2.63406946e-06
Iter: 1827 loss: 2.63305219e-06
Iter: 1828 loss: 2.63925676e-06
Iter: 1829 loss: 2.6329883e-06
Iter: 1830 loss: 2.63227389e-06
Iter: 1831 loss: 2.63121979e-06
Iter: 1832 loss: 2.63124866e-06
Iter: 1833 loss: 2.62982462e-06
Iter: 1834 loss: 2.64636969e-06
Iter: 1835 loss: 2.62983463e-06
Iter: 1836 loss: 2.62912044e-06
Iter: 1837 loss: 2.62833464e-06
Iter: 1838 loss: 2.62817139e-06
Iter: 1839 loss: 2.62715866e-06
Iter: 1840 loss: 2.64001164e-06
Iter: 1841 loss: 2.62714684e-06
Iter: 1842 loss: 2.62605636e-06
Iter: 1843 loss: 2.62494837e-06
Iter: 1844 loss: 2.6247576e-06
Iter: 1845 loss: 2.62343838e-06
Iter: 1846 loss: 2.62456865e-06
Iter: 1847 loss: 2.62263552e-06
Iter: 1848 loss: 2.62148205e-06
Iter: 1849 loss: 2.63227594e-06
Iter: 1850 loss: 2.62145863e-06
Iter: 1851 loss: 2.62030426e-06
Iter: 1852 loss: 2.62563526e-06
Iter: 1853 loss: 2.62004869e-06
Iter: 1854 loss: 2.6191417e-06
Iter: 1855 loss: 2.61987816e-06
Iter: 1856 loss: 2.61861578e-06
Iter: 1857 loss: 2.617684e-06
Iter: 1858 loss: 2.61858395e-06
Iter: 1859 loss: 2.6171474e-06
Iter: 1860 loss: 2.61601167e-06
Iter: 1861 loss: 2.63277661e-06
Iter: 1862 loss: 2.61596938e-06
Iter: 1863 loss: 2.61526657e-06
Iter: 1864 loss: 2.6146231e-06
Iter: 1865 loss: 2.61441664e-06
Iter: 1866 loss: 2.61338096e-06
Iter: 1867 loss: 2.6191924e-06
Iter: 1868 loss: 2.6133207e-06
Iter: 1869 loss: 2.61215246e-06
Iter: 1870 loss: 2.61194418e-06
Iter: 1871 loss: 2.61121568e-06
Iter: 1872 loss: 2.61008586e-06
Iter: 1873 loss: 2.61519244e-06
Iter: 1874 loss: 2.60983234e-06
Iter: 1875 loss: 2.60851357e-06
Iter: 1876 loss: 2.61284754e-06
Iter: 1877 loss: 2.60817637e-06
Iter: 1878 loss: 2.60727575e-06
Iter: 1879 loss: 2.60594652e-06
Iter: 1880 loss: 2.60594152e-06
Iter: 1881 loss: 2.60441584e-06
Iter: 1882 loss: 2.6116154e-06
Iter: 1883 loss: 2.60411616e-06
Iter: 1884 loss: 2.60340948e-06
Iter: 1885 loss: 2.60338e-06
Iter: 1886 loss: 2.60249863e-06
Iter: 1887 loss: 2.60143065e-06
Iter: 1888 loss: 2.60139927e-06
Iter: 1889 loss: 2.60021852e-06
Iter: 1890 loss: 2.6053815e-06
Iter: 1891 loss: 2.59996568e-06
Iter: 1892 loss: 2.59900435e-06
Iter: 1893 loss: 2.60618731e-06
Iter: 1894 loss: 2.59887747e-06
Iter: 1895 loss: 2.59790409e-06
Iter: 1896 loss: 2.5988561e-06
Iter: 1897 loss: 2.59729291e-06
Iter: 1898 loss: 2.59616399e-06
Iter: 1899 loss: 2.59586068e-06
Iter: 1900 loss: 2.59516128e-06
Iter: 1901 loss: 2.59377748e-06
Iter: 1902 loss: 2.61441619e-06
Iter: 1903 loss: 2.59376702e-06
Iter: 1904 loss: 2.59301851e-06
Iter: 1905 loss: 2.59214767e-06
Iter: 1906 loss: 2.59210242e-06
Iter: 1907 loss: 2.59091985e-06
Iter: 1908 loss: 2.60842762e-06
Iter: 1909 loss: 2.59094031e-06
Iter: 1910 loss: 2.59011813e-06
Iter: 1911 loss: 2.58970431e-06
Iter: 1912 loss: 2.58931186e-06
Iter: 1913 loss: 2.58847513e-06
Iter: 1914 loss: 2.58777936e-06
Iter: 1915 loss: 2.5875413e-06
Iter: 1916 loss: 2.58609271e-06
Iter: 1917 loss: 2.59195713e-06
Iter: 1918 loss: 2.58579848e-06
Iter: 1919 loss: 2.58473915e-06
Iter: 1920 loss: 2.58466207e-06
Iter: 1921 loss: 2.58397858e-06
Iter: 1922 loss: 2.5827278e-06
Iter: 1923 loss: 2.58271098e-06
Iter: 1924 loss: 2.58115824e-06
Iter: 1925 loss: 2.58409318e-06
Iter: 1926 loss: 2.58053342e-06
Iter: 1927 loss: 2.58036425e-06
Iter: 1928 loss: 2.57985448e-06
Iter: 1929 loss: 2.57936563e-06
Iter: 1930 loss: 2.57828333e-06
Iter: 1931 loss: 2.59965418e-06
Iter: 1932 loss: 2.57834336e-06
Iter: 1933 loss: 2.57732836e-06
Iter: 1934 loss: 2.58526143e-06
Iter: 1935 loss: 2.5773229e-06
Iter: 1936 loss: 2.57656848e-06
Iter: 1937 loss: 2.58048158e-06
Iter: 1938 loss: 2.57645206e-06
Iter: 1939 loss: 2.57584657e-06
Iter: 1940 loss: 2.57547094e-06
Iter: 1941 loss: 2.57529314e-06
Iter: 1942 loss: 2.57455531e-06
Iter: 1943 loss: 2.58592058e-06
Iter: 1944 loss: 2.57456213e-06
Iter: 1945 loss: 2.57403917e-06
Iter: 1946 loss: 2.57296824e-06
Iter: 1947 loss: 2.57296438e-06
Iter: 1948 loss: 2.57188663e-06
Iter: 1949 loss: 2.57323472e-06
Iter: 1950 loss: 2.57132115e-06
Iter: 1951 loss: 2.56991052e-06
Iter: 1952 loss: 2.56968588e-06
Iter: 1953 loss: 2.56866224e-06
Iter: 1954 loss: 2.56906083e-06
Iter: 1955 loss: 2.56803219e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi1.2
+ date
Sat Nov  7 22:24:28 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/300_100_100_100_1 --function f1 --psi -2 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bde1200d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bde1ffea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bde1ffbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb8424ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb8423268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bde13c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bde0c1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb8351bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb83516a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb8376d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb83fe9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb83f4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb83ce510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b90298d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b902988c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb8321730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb8321048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b901d0d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b9020b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b901d4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b90136730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b90258bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b90174048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b901907b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b90190bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b900842f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b901166a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b70446510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b70446158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b70445b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b900539d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b900c8048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b900c89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b900c3048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b703d7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b70384268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.004419552
test_loss: 0.00518928
train_loss: 0.004105057
test_loss: 0.0048532435
train_loss: 0.004021576
test_loss: 0.004688209
train_loss: 0.0036505086
test_loss: 0.00477708
train_loss: 0.003885605
test_loss: 0.0046080295
train_loss: 0.0037462881
test_loss: 0.0046573333
train_loss: 0.0036729267
test_loss: 0.0046924143
train_loss: 0.0036461363
test_loss: 0.0045134695
train_loss: 0.003981114
test_loss: 0.0046814694
train_loss: 0.0037377703
test_loss: 0.004732873
train_loss: 0.003765095
test_loss: 0.004640467
train_loss: 0.0039978796
test_loss: 0.004559669
train_loss: 0.0037002189
test_loss: 0.004615215
train_loss: 0.0038201637
test_loss: 0.0047733034
train_loss: 0.0036325916
test_loss: 0.0046838233
train_loss: 0.0037293334
test_loss: 0.0047471407
train_loss: 0.0035734628
test_loss: 0.004454941
train_loss: 0.0034744218
test_loss: 0.0045640115
train_loss: 0.0038134665
test_loss: 0.0047210674
train_loss: 0.0037209685
test_loss: 0.004643889
train_loss: 0.0036706282
test_loss: 0.0044214395
train_loss: 0.003476535
test_loss: 0.0045528323
train_loss: 0.0034949882
test_loss: 0.0045267297
train_loss: 0.0035884436
test_loss: 0.004488852
train_loss: 0.003755126
test_loss: 0.0045252964
train_loss: 0.003554275
test_loss: 0.004402126
train_loss: 0.0036419027
test_loss: 0.004366709
train_loss: 0.00346877
test_loss: 0.004496365
train_loss: 0.0036536918
test_loss: 0.0045709116
train_loss: 0.0035276776
test_loss: 0.0043299333
train_loss: 0.0032168182
test_loss: 0.004337434
train_loss: 0.0036760853
test_loss: 0.00473155
train_loss: 0.0035425706
test_loss: 0.004319232
train_loss: 0.0034492656
test_loss: 0.0043564765
train_loss: 0.0033323318
test_loss: 0.0043101553
train_loss: 0.0036783644
test_loss: 0.004408464
train_loss: 0.0035750843
test_loss: 0.0044275443
train_loss: 0.0036921562
test_loss: 0.0046392507
train_loss: 0.0035551717
test_loss: 0.0045350175
train_loss: 0.00347176
test_loss: 0.00432461
train_loss: 0.0032700126
test_loss: 0.004381495
train_loss: 0.003815372
test_loss: 0.004571834
train_loss: 0.003651559
test_loss: 0.004427234
train_loss: 0.003394777
test_loss: 0.0042403066
train_loss: 0.0035870997
test_loss: 0.004431913
train_loss: 0.0035378046
test_loss: 0.00437649
train_loss: 0.0033245988
test_loss: 0.0043320535
train_loss: 0.0033659064
test_loss: 0.004416278
train_loss: 0.0034372592
test_loss: 0.0043969234
train_loss: 0.0034432292
test_loss: 0.0044742497
train_loss: 0.0032296753
test_loss: 0.0043103276
train_loss: 0.0035987424
test_loss: 0.0046126237
train_loss: 0.0033785454
test_loss: 0.0042841886
train_loss: 0.0036038675
test_loss: 0.0044485973
train_loss: 0.0032852557
test_loss: 0.0043156263
train_loss: 0.0032891077
test_loss: 0.0043476056
train_loss: 0.0035708058
test_loss: 0.0044916393
train_loss: 0.0034766132
test_loss: 0.0044672154
train_loss: 0.0035197688
test_loss: 0.0043216376
train_loss: 0.003199012
test_loss: 0.004268479
train_loss: 0.0035098824
test_loss: 0.004382307
train_loss: 0.0033704732
test_loss: 0.004357788
train_loss: 0.0032280562
test_loss: 0.0041973377
train_loss: 0.003245141
test_loss: 0.004242193
train_loss: 0.0035421532
test_loss: 0.0042611044
train_loss: 0.0036061546
test_loss: 0.004659502
train_loss: 0.0034045535
test_loss: 0.0043467684
train_loss: 0.0033274144
test_loss: 0.0042627314
train_loss: 0.0033363472
test_loss: 0.0044665816
train_loss: 0.0033818502
test_loss: 0.004254826
train_loss: 0.0034465326
test_loss: 0.0041884026
train_loss: 0.003404426
test_loss: 0.0043670107
train_loss: 0.0033339015
test_loss: 0.004399334
train_loss: 0.0035581975
test_loss: 0.0043666856
train_loss: 0.0034860838
test_loss: 0.0044108178
train_loss: 0.0036894565
test_loss: 0.0042354367
train_loss: 0.003388386
test_loss: 0.0042484268
train_loss: 0.0034938436
test_loss: 0.0043300367
train_loss: 0.0033164902
test_loss: 0.0042403215
train_loss: 0.0031718754
test_loss: 0.0043299417
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi1.2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi1.2/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77925991e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f779253cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77925c9400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77925c9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77925332f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77924a0620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7792474f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7792474ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77924270d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77923e4e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7792427950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77923aa0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f779239a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f779239a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7792306950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7792343e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f779233a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f779233abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77922b49d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77922b4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f779233aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7780061378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f77800888c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7780088510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f778003e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7780041a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777ffab048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777ffc0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777ff4d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777ff6e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777ff15598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777ff4a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777ff4aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777fefdb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777fefdae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f777fe66730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.03043601e-05
Iter: 2 loss: 2.13928288e-05
Iter: 3 loss: 1.76234e-05
Iter: 4 loss: 1.66110494e-05
Iter: 5 loss: 1.98952785e-05
Iter: 6 loss: 1.63260047e-05
Iter: 7 loss: 1.56334208e-05
Iter: 8 loss: 1.71883021e-05
Iter: 9 loss: 1.53712026e-05
Iter: 10 loss: 1.45759068e-05
Iter: 11 loss: 1.67807957e-05
Iter: 12 loss: 1.4320226e-05
Iter: 13 loss: 1.36861881e-05
Iter: 14 loss: 1.51034492e-05
Iter: 15 loss: 1.34441516e-05
Iter: 16 loss: 1.2621188e-05
Iter: 17 loss: 1.51645199e-05
Iter: 18 loss: 1.23790851e-05
Iter: 19 loss: 1.194783e-05
Iter: 20 loss: 1.25094757e-05
Iter: 21 loss: 1.17289765e-05
Iter: 22 loss: 1.1256242e-05
Iter: 23 loss: 1.13565111e-05
Iter: 24 loss: 1.0906233e-05
Iter: 25 loss: 1.03548027e-05
Iter: 26 loss: 1.86317484e-05
Iter: 27 loss: 1.03544726e-05
Iter: 28 loss: 1.00655816e-05
Iter: 29 loss: 9.74680188e-06
Iter: 30 loss: 9.70091241e-06
Iter: 31 loss: 9.34108175e-06
Iter: 32 loss: 1.08189306e-05
Iter: 33 loss: 9.26257053e-06
Iter: 34 loss: 8.96314305e-06
Iter: 35 loss: 1.0014347e-05
Iter: 36 loss: 8.88555587e-06
Iter: 37 loss: 8.62310844e-06
Iter: 38 loss: 8.80394418e-06
Iter: 39 loss: 8.45900922e-06
Iter: 40 loss: 8.26135783e-06
Iter: 41 loss: 8.24843846e-06
Iter: 42 loss: 8.02765771e-06
Iter: 43 loss: 8.03409421e-06
Iter: 44 loss: 7.85297289e-06
Iter: 45 loss: 7.63214666e-06
Iter: 46 loss: 9.42098541e-06
Iter: 47 loss: 7.61778665e-06
Iter: 48 loss: 7.45341276e-06
Iter: 49 loss: 8.00842099e-06
Iter: 50 loss: 7.40920268e-06
Iter: 51 loss: 7.30675038e-06
Iter: 52 loss: 7.79777201e-06
Iter: 53 loss: 7.28873965e-06
Iter: 54 loss: 7.17521198e-06
Iter: 55 loss: 7.19378295e-06
Iter: 56 loss: 7.08961852e-06
Iter: 57 loss: 6.98062786e-06
Iter: 58 loss: 6.94966548e-06
Iter: 59 loss: 6.88325235e-06
Iter: 60 loss: 6.74460534e-06
Iter: 61 loss: 8.24203562e-06
Iter: 62 loss: 6.7412675e-06
Iter: 63 loss: 6.64966456e-06
Iter: 64 loss: 7.08681273e-06
Iter: 65 loss: 6.63303263e-06
Iter: 66 loss: 6.55926442e-06
Iter: 67 loss: 6.53650659e-06
Iter: 68 loss: 6.49291e-06
Iter: 69 loss: 6.39660539e-06
Iter: 70 loss: 6.60120304e-06
Iter: 71 loss: 6.35868309e-06
Iter: 72 loss: 6.24562381e-06
Iter: 73 loss: 6.31917783e-06
Iter: 74 loss: 6.1738665e-06
Iter: 75 loss: 6.04172237e-06
Iter: 76 loss: 6.1550445e-06
Iter: 77 loss: 5.96364498e-06
Iter: 78 loss: 5.87838895e-06
Iter: 79 loss: 5.87365093e-06
Iter: 80 loss: 5.80388041e-06
Iter: 81 loss: 6.4672895e-06
Iter: 82 loss: 5.80116466e-06
Iter: 83 loss: 5.74486694e-06
Iter: 84 loss: 5.68498581e-06
Iter: 85 loss: 5.675171e-06
Iter: 86 loss: 5.64757647e-06
Iter: 87 loss: 5.63767389e-06
Iter: 88 loss: 5.60960234e-06
Iter: 89 loss: 5.54802909e-06
Iter: 90 loss: 6.45463388e-06
Iter: 91 loss: 5.54525877e-06
Iter: 92 loss: 5.50889854e-06
Iter: 93 loss: 5.50454752e-06
Iter: 94 loss: 5.4830316e-06
Iter: 95 loss: 5.43392616e-06
Iter: 96 loss: 6.07193033e-06
Iter: 97 loss: 5.43083479e-06
Iter: 98 loss: 5.38452332e-06
Iter: 99 loss: 5.74633896e-06
Iter: 100 loss: 5.38132599e-06
Iter: 101 loss: 5.34178e-06
Iter: 102 loss: 5.57227395e-06
Iter: 103 loss: 5.33654566e-06
Iter: 104 loss: 5.29839326e-06
Iter: 105 loss: 5.30918624e-06
Iter: 106 loss: 5.27081602e-06
Iter: 107 loss: 5.22167375e-06
Iter: 108 loss: 5.20913909e-06
Iter: 109 loss: 5.17822309e-06
Iter: 110 loss: 5.13113082e-06
Iter: 111 loss: 5.13106352e-06
Iter: 112 loss: 5.10261907e-06
Iter: 113 loss: 5.04431682e-06
Iter: 114 loss: 6.09887229e-06
Iter: 115 loss: 5.04335458e-06
Iter: 116 loss: 5.06397e-06
Iter: 117 loss: 5.01600516e-06
Iter: 118 loss: 4.99762064e-06
Iter: 119 loss: 4.98796635e-06
Iter: 120 loss: 4.9795608e-06
Iter: 121 loss: 4.95567292e-06
Iter: 122 loss: 4.96334542e-06
Iter: 123 loss: 4.93872221e-06
Iter: 124 loss: 4.9133123e-06
Iter: 125 loss: 4.91311675e-06
Iter: 126 loss: 4.90065122e-06
Iter: 127 loss: 4.87634679e-06
Iter: 128 loss: 5.3527292e-06
Iter: 129 loss: 4.87605575e-06
Iter: 130 loss: 4.85213695e-06
Iter: 131 loss: 4.85210285e-06
Iter: 132 loss: 4.836264e-06
Iter: 133 loss: 4.79963455e-06
Iter: 134 loss: 5.26386566e-06
Iter: 135 loss: 4.79722621e-06
Iter: 136 loss: 4.76076275e-06
Iter: 137 loss: 4.99567523e-06
Iter: 138 loss: 4.75675643e-06
Iter: 139 loss: 4.7331514e-06
Iter: 140 loss: 5.04736454e-06
Iter: 141 loss: 4.73309592e-06
Iter: 142 loss: 4.71384556e-06
Iter: 143 loss: 4.73574619e-06
Iter: 144 loss: 4.70338455e-06
Iter: 145 loss: 4.68590133e-06
Iter: 146 loss: 4.65851372e-06
Iter: 147 loss: 4.65817584e-06
Iter: 148 loss: 4.63432161e-06
Iter: 149 loss: 4.9303203e-06
Iter: 150 loss: 4.63401329e-06
Iter: 151 loss: 4.61113314e-06
Iter: 152 loss: 4.59064177e-06
Iter: 153 loss: 4.58483464e-06
Iter: 154 loss: 4.59113107e-06
Iter: 155 loss: 4.57403257e-06
Iter: 156 loss: 4.56242242e-06
Iter: 157 loss: 4.5511506e-06
Iter: 158 loss: 4.5485549e-06
Iter: 159 loss: 4.53276562e-06
Iter: 160 loss: 4.59559169e-06
Iter: 161 loss: 4.52901577e-06
Iter: 162 loss: 4.50839343e-06
Iter: 163 loss: 4.5384345e-06
Iter: 164 loss: 4.49821755e-06
Iter: 165 loss: 4.48187e-06
Iter: 166 loss: 4.50106108e-06
Iter: 167 loss: 4.47326329e-06
Iter: 168 loss: 4.46175727e-06
Iter: 169 loss: 4.56892622e-06
Iter: 170 loss: 4.46129297e-06
Iter: 171 loss: 4.44882699e-06
Iter: 172 loss: 4.4392e-06
Iter: 173 loss: 4.43518911e-06
Iter: 174 loss: 4.41767952e-06
Iter: 175 loss: 4.43229874e-06
Iter: 176 loss: 4.40728036e-06
Iter: 177 loss: 4.39051837e-06
Iter: 178 loss: 4.5599395e-06
Iter: 179 loss: 4.3899754e-06
Iter: 180 loss: 4.37912558e-06
Iter: 181 loss: 4.36842038e-06
Iter: 182 loss: 4.36610708e-06
Iter: 183 loss: 4.34788581e-06
Iter: 184 loss: 4.49545678e-06
Iter: 185 loss: 4.34672711e-06
Iter: 186 loss: 4.3327309e-06
Iter: 187 loss: 4.44548095e-06
Iter: 188 loss: 4.33177502e-06
Iter: 189 loss: 4.31943226e-06
Iter: 190 loss: 4.30987529e-06
Iter: 191 loss: 4.3059058e-06
Iter: 192 loss: 4.28737621e-06
Iter: 193 loss: 4.29194552e-06
Iter: 194 loss: 4.27378745e-06
Iter: 195 loss: 4.27861778e-06
Iter: 196 loss: 4.26689257e-06
Iter: 197 loss: 4.25991129e-06
Iter: 198 loss: 4.24561858e-06
Iter: 199 loss: 4.50120206e-06
Iter: 200 loss: 4.24542577e-06
Iter: 201 loss: 4.23563051e-06
Iter: 202 loss: 4.37932522e-06
Iter: 203 loss: 4.23558504e-06
Iter: 204 loss: 4.22662924e-06
Iter: 205 loss: 4.24192876e-06
Iter: 206 loss: 4.22257563e-06
Iter: 207 loss: 4.21396089e-06
Iter: 208 loss: 4.19953858e-06
Iter: 209 loss: 4.19955722e-06
Iter: 210 loss: 4.19502e-06
Iter: 211 loss: 4.19177377e-06
Iter: 212 loss: 4.18595755e-06
Iter: 213 loss: 4.17423917e-06
Iter: 214 loss: 4.40151098e-06
Iter: 215 loss: 4.17407773e-06
Iter: 216 loss: 4.16128387e-06
Iter: 217 loss: 4.20767719e-06
Iter: 218 loss: 4.15800196e-06
Iter: 219 loss: 4.14690567e-06
Iter: 220 loss: 4.16737112e-06
Iter: 221 loss: 4.14214082e-06
Iter: 222 loss: 4.13315183e-06
Iter: 223 loss: 4.20902825e-06
Iter: 224 loss: 4.13251837e-06
Iter: 225 loss: 4.122272e-06
Iter: 226 loss: 4.14152964e-06
Iter: 227 loss: 4.1178273e-06
Iter: 228 loss: 4.1098e-06
Iter: 229 loss: 4.13673843e-06
Iter: 230 loss: 4.10758821e-06
Iter: 231 loss: 4.10047369e-06
Iter: 232 loss: 4.10315897e-06
Iter: 233 loss: 4.09549057e-06
Iter: 234 loss: 4.08976211e-06
Iter: 235 loss: 4.08930055e-06
Iter: 236 loss: 4.08478536e-06
Iter: 237 loss: 4.07731659e-06
Iter: 238 loss: 4.07731386e-06
Iter: 239 loss: 4.06985419e-06
Iter: 240 loss: 4.13265843e-06
Iter: 241 loss: 4.06943582e-06
Iter: 242 loss: 4.06301342e-06
Iter: 243 loss: 4.08773576e-06
Iter: 244 loss: 4.06142408e-06
Iter: 245 loss: 4.05658693e-06
Iter: 246 loss: 4.04457114e-06
Iter: 247 loss: 4.15372051e-06
Iter: 248 loss: 4.04269031e-06
Iter: 249 loss: 4.03701597e-06
Iter: 250 loss: 4.03557078e-06
Iter: 251 loss: 4.02827209e-06
Iter: 252 loss: 4.02515707e-06
Iter: 253 loss: 4.02121168e-06
Iter: 254 loss: 4.01287571e-06
Iter: 255 loss: 4.02435217e-06
Iter: 256 loss: 4.00879344e-06
Iter: 257 loss: 3.99795636e-06
Iter: 258 loss: 4.00344379e-06
Iter: 259 loss: 3.9907e-06
Iter: 260 loss: 3.98242582e-06
Iter: 261 loss: 4.02435126e-06
Iter: 262 loss: 3.98116117e-06
Iter: 263 loss: 3.97280473e-06
Iter: 264 loss: 4.02979e-06
Iter: 265 loss: 3.97197391e-06
Iter: 266 loss: 3.96376254e-06
Iter: 267 loss: 3.98634256e-06
Iter: 268 loss: 3.9611341e-06
Iter: 269 loss: 3.95592906e-06
Iter: 270 loss: 3.96599444e-06
Iter: 271 loss: 3.95390589e-06
Iter: 272 loss: 3.95007646e-06
Iter: 273 loss: 3.950061e-06
Iter: 274 loss: 3.94700146e-06
Iter: 275 loss: 3.93881965e-06
Iter: 276 loss: 3.98837528e-06
Iter: 277 loss: 3.93649771e-06
Iter: 278 loss: 3.932656e-06
Iter: 279 loss: 3.9314009e-06
Iter: 280 loss: 3.92665879e-06
Iter: 281 loss: 3.92535912e-06
Iter: 282 loss: 3.92252923e-06
Iter: 283 loss: 3.91622052e-06
Iter: 284 loss: 3.91908634e-06
Iter: 285 loss: 3.91193134e-06
Iter: 286 loss: 3.90425066e-06
Iter: 287 loss: 3.92299171e-06
Iter: 288 loss: 3.90150262e-06
Iter: 289 loss: 3.89287061e-06
Iter: 290 loss: 3.95542793e-06
Iter: 291 loss: 3.892108e-06
Iter: 292 loss: 3.88594708e-06
Iter: 293 loss: 3.88843682e-06
Iter: 294 loss: 3.8817675e-06
Iter: 295 loss: 3.87497948e-06
Iter: 296 loss: 3.87394948e-06
Iter: 297 loss: 3.86929241e-06
Iter: 298 loss: 3.86059946e-06
Iter: 299 loss: 3.89533761e-06
Iter: 300 loss: 3.85862586e-06
Iter: 301 loss: 3.85253543e-06
Iter: 302 loss: 3.92711e-06
Iter: 303 loss: 3.85249768e-06
Iter: 304 loss: 3.8471685e-06
Iter: 305 loss: 3.86266311e-06
Iter: 306 loss: 3.84564e-06
Iter: 307 loss: 3.83980205e-06
Iter: 308 loss: 3.84457417e-06
Iter: 309 loss: 3.83638462e-06
Iter: 310 loss: 3.83205042e-06
Iter: 311 loss: 3.83186944e-06
Iter: 312 loss: 3.82931285e-06
Iter: 313 loss: 3.82174312e-06
Iter: 314 loss: 3.84426767e-06
Iter: 315 loss: 3.81785185e-06
Iter: 316 loss: 3.82268945e-06
Iter: 317 loss: 3.81462405e-06
Iter: 318 loss: 3.81195878e-06
Iter: 319 loss: 3.80549409e-06
Iter: 320 loss: 3.87264208e-06
Iter: 321 loss: 3.80473e-06
Iter: 322 loss: 3.79933681e-06
Iter: 323 loss: 3.83245879e-06
Iter: 324 loss: 3.79869243e-06
Iter: 325 loss: 3.79364883e-06
Iter: 326 loss: 3.80596225e-06
Iter: 327 loss: 3.79183848e-06
Iter: 328 loss: 3.78594791e-06
Iter: 329 loss: 3.8244807e-06
Iter: 330 loss: 3.78531786e-06
Iter: 331 loss: 3.78237792e-06
Iter: 332 loss: 3.77988681e-06
Iter: 333 loss: 3.77909146e-06
Iter: 334 loss: 3.77270112e-06
Iter: 335 loss: 3.77501829e-06
Iter: 336 loss: 3.76836351e-06
Iter: 337 loss: 3.76067283e-06
Iter: 338 loss: 3.7846246e-06
Iter: 339 loss: 3.75856553e-06
Iter: 340 loss: 3.75337572e-06
Iter: 341 loss: 3.7840091e-06
Iter: 342 loss: 3.75276204e-06
Iter: 343 loss: 3.74807905e-06
Iter: 344 loss: 3.78939876e-06
Iter: 345 loss: 3.74778097e-06
Iter: 346 loss: 3.7437826e-06
Iter: 347 loss: 3.75104537e-06
Iter: 348 loss: 3.74181218e-06
Iter: 349 loss: 3.73877492e-06
Iter: 350 loss: 3.77079959e-06
Iter: 351 loss: 3.73863554e-06
Iter: 352 loss: 3.73674538e-06
Iter: 353 loss: 3.731672e-06
Iter: 354 loss: 3.76846265e-06
Iter: 355 loss: 3.73064177e-06
Iter: 356 loss: 3.7254224e-06
Iter: 357 loss: 3.72524346e-06
Iter: 358 loss: 3.72096792e-06
Iter: 359 loss: 3.72147861e-06
Iter: 360 loss: 3.71774195e-06
Iter: 361 loss: 3.71360761e-06
Iter: 362 loss: 3.7113648e-06
Iter: 363 loss: 3.70952375e-06
Iter: 364 loss: 3.70351427e-06
Iter: 365 loss: 3.70721477e-06
Iter: 366 loss: 3.69962549e-06
Iter: 367 loss: 3.69552617e-06
Iter: 368 loss: 3.69489953e-06
Iter: 369 loss: 3.69098188e-06
Iter: 370 loss: 3.69307827e-06
Iter: 371 loss: 3.68834253e-06
Iter: 372 loss: 3.68458586e-06
Iter: 373 loss: 3.69393206e-06
Iter: 374 loss: 3.68328051e-06
Iter: 375 loss: 3.67982898e-06
Iter: 376 loss: 3.67888879e-06
Iter: 377 loss: 3.6768156e-06
Iter: 378 loss: 3.67150187e-06
Iter: 379 loss: 3.70989619e-06
Iter: 380 loss: 3.67105145e-06
Iter: 381 loss: 3.66832433e-06
Iter: 382 loss: 3.66824679e-06
Iter: 383 loss: 3.66555582e-06
Iter: 384 loss: 3.66062136e-06
Iter: 385 loss: 3.77662082e-06
Iter: 386 loss: 3.66052404e-06
Iter: 387 loss: 3.65664982e-06
Iter: 388 loss: 3.71453552e-06
Iter: 389 loss: 3.65659616e-06
Iter: 390 loss: 3.6531037e-06
Iter: 391 loss: 3.64784319e-06
Iter: 392 loss: 3.64770403e-06
Iter: 393 loss: 3.64460311e-06
Iter: 394 loss: 3.6440756e-06
Iter: 395 loss: 3.64162679e-06
Iter: 396 loss: 3.63853e-06
Iter: 397 loss: 3.63838649e-06
Iter: 398 loss: 3.6346205e-06
Iter: 399 loss: 3.63814638e-06
Iter: 400 loss: 3.63247796e-06
Iter: 401 loss: 3.627978e-06
Iter: 402 loss: 3.63029721e-06
Iter: 403 loss: 3.62507103e-06
Iter: 404 loss: 3.62096443e-06
Iter: 405 loss: 3.68433189e-06
Iter: 406 loss: 3.62096216e-06
Iter: 407 loss: 3.61698039e-06
Iter: 408 loss: 3.62879905e-06
Iter: 409 loss: 3.61579464e-06
Iter: 410 loss: 3.61275806e-06
Iter: 411 loss: 3.61042248e-06
Iter: 412 loss: 3.6094907e-06
Iter: 413 loss: 3.6041763e-06
Iter: 414 loss: 3.61682442e-06
Iter: 415 loss: 3.6022625e-06
Iter: 416 loss: 3.59808337e-06
Iter: 417 loss: 3.60443687e-06
Iter: 418 loss: 3.59606975e-06
Iter: 419 loss: 3.59196019e-06
Iter: 420 loss: 3.65649453e-06
Iter: 421 loss: 3.5918697e-06
Iter: 422 loss: 3.58905936e-06
Iter: 423 loss: 3.61641855e-06
Iter: 424 loss: 3.58890361e-06
Iter: 425 loss: 3.58699685e-06
Iter: 426 loss: 3.58308557e-06
Iter: 427 loss: 3.64919833e-06
Iter: 428 loss: 3.58295051e-06
Iter: 429 loss: 3.57815179e-06
Iter: 430 loss: 3.61828916e-06
Iter: 431 loss: 3.57788372e-06
Iter: 432 loss: 3.5742e-06
Iter: 433 loss: 3.58244961e-06
Iter: 434 loss: 3.57268618e-06
Iter: 435 loss: 3.56944656e-06
Iter: 436 loss: 3.58497118e-06
Iter: 437 loss: 3.56877831e-06
Iter: 438 loss: 3.5651949e-06
Iter: 439 loss: 3.56381088e-06
Iter: 440 loss: 3.56185069e-06
Iter: 441 loss: 3.55857651e-06
Iter: 442 loss: 3.558873e-06
Iter: 443 loss: 3.55616817e-06
Iter: 444 loss: 3.55205043e-06
Iter: 445 loss: 3.58012835e-06
Iter: 446 loss: 3.55159341e-06
Iter: 447 loss: 3.54781741e-06
Iter: 448 loss: 3.57219233e-06
Iter: 449 loss: 3.5474e-06
Iter: 450 loss: 3.54424878e-06
Iter: 451 loss: 3.54401982e-06
Iter: 452 loss: 3.54166923e-06
Iter: 453 loss: 3.53884434e-06
Iter: 454 loss: 3.53797168e-06
Iter: 455 loss: 3.53625387e-06
Iter: 456 loss: 3.53193309e-06
Iter: 457 loss: 3.5405692e-06
Iter: 458 loss: 3.53003429e-06
Iter: 459 loss: 3.52667348e-06
Iter: 460 loss: 3.5264361e-06
Iter: 461 loss: 3.52350617e-06
Iter: 462 loss: 3.53548103e-06
Iter: 463 loss: 3.52283746e-06
Iter: 464 loss: 3.52112988e-06
Iter: 465 loss: 3.51676135e-06
Iter: 466 loss: 3.55209204e-06
Iter: 467 loss: 3.51602102e-06
Iter: 468 loss: 3.51281528e-06
Iter: 469 loss: 3.51236531e-06
Iter: 470 loss: 3.50985965e-06
Iter: 471 loss: 3.50916025e-06
Iter: 472 loss: 3.5076132e-06
Iter: 473 loss: 3.50388109e-06
Iter: 474 loss: 3.51805693e-06
Iter: 475 loss: 3.50289247e-06
Iter: 476 loss: 3.49970674e-06
Iter: 477 loss: 3.50715982e-06
Iter: 478 loss: 3.49847141e-06
Iter: 479 loss: 3.49595985e-06
Iter: 480 loss: 3.49467837e-06
Iter: 481 loss: 3.49341167e-06
Iter: 482 loss: 3.4899308e-06
Iter: 483 loss: 3.51066819e-06
Iter: 484 loss: 3.48944968e-06
Iter: 485 loss: 3.48613548e-06
Iter: 486 loss: 3.48641242e-06
Iter: 487 loss: 3.483608e-06
Iter: 488 loss: 3.47985701e-06
Iter: 489 loss: 3.479822e-06
Iter: 490 loss: 3.47740911e-06
Iter: 491 loss: 3.47450555e-06
Iter: 492 loss: 3.47424202e-06
Iter: 493 loss: 3.4714667e-06
Iter: 494 loss: 3.47314631e-06
Iter: 495 loss: 3.469788e-06
Iter: 496 loss: 3.46560432e-06
Iter: 497 loss: 3.49332299e-06
Iter: 498 loss: 3.4652162e-06
Iter: 499 loss: 3.460789e-06
Iter: 500 loss: 3.48686353e-06
Iter: 501 loss: 3.46016327e-06
Iter: 502 loss: 3.45859e-06
Iter: 503 loss: 3.45667286e-06
Iter: 504 loss: 3.45643e-06
Iter: 505 loss: 3.4541581e-06
Iter: 506 loss: 3.47896957e-06
Iter: 507 loss: 3.4540742e-06
Iter: 508 loss: 3.45143303e-06
Iter: 509 loss: 3.45003514e-06
Iter: 510 loss: 3.44886894e-06
Iter: 511 loss: 3.4459681e-06
Iter: 512 loss: 3.4673817e-06
Iter: 513 loss: 3.44569457e-06
Iter: 514 loss: 3.44337082e-06
Iter: 515 loss: 3.44767454e-06
Iter: 516 loss: 3.44228692e-06
Iter: 517 loss: 3.43984038e-06
Iter: 518 loss: 3.43684155e-06
Iter: 519 loss: 3.436518e-06
Iter: 520 loss: 3.43273564e-06
Iter: 521 loss: 3.45327021e-06
Iter: 522 loss: 3.4321688e-06
Iter: 523 loss: 3.42844146e-06
Iter: 524 loss: 3.43591455e-06
Iter: 525 loss: 3.42706176e-06
Iter: 526 loss: 3.42360454e-06
Iter: 527 loss: 3.43554484e-06
Iter: 528 loss: 3.42271392e-06
Iter: 529 loss: 3.41807731e-06
Iter: 530 loss: 3.43312081e-06
Iter: 531 loss: 3.41678151e-06
Iter: 532 loss: 3.41398299e-06
Iter: 533 loss: 3.42114618e-06
Iter: 534 loss: 3.41297982e-06
Iter: 535 loss: 3.41053146e-06
Iter: 536 loss: 3.40813153e-06
Iter: 537 loss: 3.40758106e-06
Iter: 538 loss: 3.40291217e-06
Iter: 539 loss: 3.40664519e-06
Iter: 540 loss: 3.40024e-06
Iter: 541 loss: 3.40463771e-06
Iter: 542 loss: 3.39840813e-06
Iter: 543 loss: 3.39742019e-06
Iter: 544 loss: 3.39543158e-06
Iter: 545 loss: 3.43637976e-06
Iter: 546 loss: 3.39536518e-06
Iter: 547 loss: 3.39270559e-06
Iter: 548 loss: 3.39695202e-06
Iter: 549 loss: 3.39142298e-06
Iter: 550 loss: 3.38795326e-06
Iter: 551 loss: 3.40906172e-06
Iter: 552 loss: 3.38750579e-06
Iter: 553 loss: 3.38556083e-06
Iter: 554 loss: 3.38798759e-06
Iter: 555 loss: 3.38456152e-06
Iter: 556 loss: 3.38177983e-06
Iter: 557 loss: 3.38679274e-06
Iter: 558 loss: 3.38054315e-06
Iter: 559 loss: 3.37854317e-06
Iter: 560 loss: 3.3768406e-06
Iter: 561 loss: 3.37631786e-06
Iter: 562 loss: 3.37291885e-06
Iter: 563 loss: 3.38985251e-06
Iter: 564 loss: 3.37244865e-06
Iter: 565 loss: 3.36939343e-06
Iter: 566 loss: 3.37630581e-06
Iter: 567 loss: 3.36833091e-06
Iter: 568 loss: 3.36547487e-06
Iter: 569 loss: 3.3980125e-06
Iter: 570 loss: 3.36546736e-06
Iter: 571 loss: 3.36361018e-06
Iter: 572 loss: 3.36361882e-06
Iter: 573 loss: 3.36215362e-06
Iter: 574 loss: 3.35913523e-06
Iter: 575 loss: 3.35585059e-06
Iter: 576 loss: 3.35544269e-06
Iter: 577 loss: 3.35169921e-06
Iter: 578 loss: 3.37786855e-06
Iter: 579 loss: 3.35135519e-06
Iter: 580 loss: 3.34804304e-06
Iter: 581 loss: 3.35308459e-06
Iter: 582 loss: 3.34653168e-06
Iter: 583 loss: 3.34508377e-06
Iter: 584 loss: 3.34452238e-06
Iter: 585 loss: 3.34280912e-06
Iter: 586 loss: 3.34172546e-06
Iter: 587 loss: 3.34095512e-06
Iter: 588 loss: 3.3391284e-06
Iter: 589 loss: 3.34667857e-06
Iter: 590 loss: 3.3387555e-06
Iter: 591 loss: 3.33640901e-06
Iter: 592 loss: 3.33989919e-06
Iter: 593 loss: 3.33540834e-06
Iter: 594 loss: 3.33362641e-06
Iter: 595 loss: 3.34386391e-06
Iter: 596 loss: 3.33345292e-06
Iter: 597 loss: 3.33174603e-06
Iter: 598 loss: 3.32997274e-06
Iter: 599 loss: 3.32955346e-06
Iter: 600 loss: 3.32641639e-06
Iter: 601 loss: 3.32719173e-06
Iter: 602 loss: 3.32391733e-06
Iter: 603 loss: 3.32094373e-06
Iter: 604 loss: 3.34338893e-06
Iter: 605 loss: 3.32067043e-06
Iter: 606 loss: 3.31787828e-06
Iter: 607 loss: 3.33783464e-06
Iter: 608 loss: 3.31756564e-06
Iter: 609 loss: 3.31624642e-06
Iter: 610 loss: 3.31533693e-06
Iter: 611 loss: 3.31478236e-06
Iter: 612 loss: 3.31217075e-06
Iter: 613 loss: 3.31618503e-06
Iter: 614 loss: 3.31095657e-06
Iter: 615 loss: 3.30830767e-06
Iter: 616 loss: 3.30875309e-06
Iter: 617 loss: 3.30629882e-06
Iter: 618 loss: 3.30311832e-06
Iter: 619 loss: 3.31860429e-06
Iter: 620 loss: 3.30260809e-06
Iter: 621 loss: 3.30070043e-06
Iter: 622 loss: 3.3005872e-06
Iter: 623 loss: 3.29893192e-06
Iter: 624 loss: 3.29823547e-06
Iter: 625 loss: 3.29723935e-06
Iter: 626 loss: 3.29539262e-06
Iter: 627 loss: 3.29635623e-06
Iter: 628 loss: 3.29418572e-06
Iter: 629 loss: 3.29093814e-06
Iter: 630 loss: 3.30849957e-06
Iter: 631 loss: 3.2904436e-06
Iter: 632 loss: 3.28899068e-06
Iter: 633 loss: 3.2907019e-06
Iter: 634 loss: 3.28831334e-06
Iter: 635 loss: 3.28581405e-06
Iter: 636 loss: 3.28627311e-06
Iter: 637 loss: 3.28414853e-06
Iter: 638 loss: 3.28201759e-06
Iter: 639 loss: 3.28690771e-06
Iter: 640 loss: 3.28122132e-06
Iter: 641 loss: 3.27908811e-06
Iter: 642 loss: 3.28547867e-06
Iter: 643 loss: 3.27825364e-06
Iter: 644 loss: 3.27614953e-06
Iter: 645 loss: 3.29813861e-06
Iter: 646 loss: 3.27610087e-06
Iter: 647 loss: 3.27487e-06
Iter: 648 loss: 3.27237331e-06
Iter: 649 loss: 3.31529031e-06
Iter: 650 loss: 3.2723035e-06
Iter: 651 loss: 3.26912186e-06
Iter: 652 loss: 3.27621319e-06
Iter: 653 loss: 3.26797363e-06
Iter: 654 loss: 3.26471513e-06
Iter: 655 loss: 3.29298723e-06
Iter: 656 loss: 3.26456961e-06
Iter: 657 loss: 3.26236045e-06
Iter: 658 loss: 3.26024815e-06
Iter: 659 loss: 3.25980227e-06
Iter: 660 loss: 3.2588955e-06
Iter: 661 loss: 3.25782707e-06
Iter: 662 loss: 3.2565863e-06
Iter: 663 loss: 3.25818337e-06
Iter: 664 loss: 3.25572546e-06
Iter: 665 loss: 3.25469932e-06
Iter: 666 loss: 3.25287783e-06
Iter: 667 loss: 3.25288875e-06
Iter: 668 loss: 3.24943176e-06
Iter: 669 loss: 3.2666162e-06
Iter: 670 loss: 3.24883922e-06
Iter: 671 loss: 3.24738266e-06
Iter: 672 loss: 3.24663165e-06
Iter: 673 loss: 3.24587199e-06
Iter: 674 loss: 3.2434441e-06
Iter: 675 loss: 3.26148779e-06
Iter: 676 loss: 3.24313692e-06
Iter: 677 loss: 3.2410619e-06
Iter: 678 loss: 3.23817449e-06
Iter: 679 loss: 3.23803806e-06
Iter: 680 loss: 3.23471204e-06
Iter: 681 loss: 3.24672487e-06
Iter: 682 loss: 3.23382096e-06
Iter: 683 loss: 3.23144877e-06
Iter: 684 loss: 3.24364373e-06
Iter: 685 loss: 3.2311541e-06
Iter: 686 loss: 3.22859887e-06
Iter: 687 loss: 3.24313601e-06
Iter: 688 loss: 3.22825122e-06
Iter: 689 loss: 3.2264536e-06
Iter: 690 loss: 3.22531673e-06
Iter: 691 loss: 3.22456822e-06
Iter: 692 loss: 3.22214851e-06
Iter: 693 loss: 3.22932374e-06
Iter: 694 loss: 3.22144047e-06
Iter: 695 loss: 3.21934726e-06
Iter: 696 loss: 3.22520918e-06
Iter: 697 loss: 3.21854441e-06
Iter: 698 loss: 3.21603579e-06
Iter: 699 loss: 3.24690018e-06
Iter: 700 loss: 3.21598236e-06
Iter: 701 loss: 3.21486596e-06
Iter: 702 loss: 3.21261109e-06
Iter: 703 loss: 3.2552598e-06
Iter: 704 loss: 3.2126627e-06
Iter: 705 loss: 3.21059588e-06
Iter: 706 loss: 3.24323e-06
Iter: 707 loss: 3.21058769e-06
Iter: 708 loss: 3.20874778e-06
Iter: 709 loss: 3.21064772e-06
Iter: 710 loss: 3.2077005e-06
Iter: 711 loss: 3.20606e-06
Iter: 712 loss: 3.20425693e-06
Iter: 713 loss: 3.20386357e-06
Iter: 714 loss: 3.20164281e-06
Iter: 715 loss: 3.20172785e-06
Iter: 716 loss: 3.20020672e-06
Iter: 717 loss: 3.19816149e-06
Iter: 718 loss: 3.19801848e-06
Iter: 719 loss: 3.19524952e-06
Iter: 720 loss: 3.21025641e-06
Iter: 721 loss: 3.19484e-06
Iter: 722 loss: 3.19283163e-06
Iter: 723 loss: 3.1976615e-06
Iter: 724 loss: 3.19218384e-06
Iter: 725 loss: 3.19006267e-06
Iter: 726 loss: 3.20666095e-06
Iter: 727 loss: 3.18999e-06
Iter: 728 loss: 3.18818684e-06
Iter: 729 loss: 3.1860302e-06
Iter: 730 loss: 3.18586285e-06
Iter: 731 loss: 3.18318803e-06
Iter: 732 loss: 3.2012008e-06
Iter: 733 loss: 3.18299976e-06
Iter: 734 loss: 3.18173124e-06
Iter: 735 loss: 3.18152138e-06
Iter: 736 loss: 3.18076718e-06
Iter: 737 loss: 3.17817376e-06
Iter: 738 loss: 3.18531261e-06
Iter: 739 loss: 3.17672198e-06
Iter: 740 loss: 3.176978e-06
Iter: 741 loss: 3.17547415e-06
Iter: 742 loss: 3.17435024e-06
Iter: 743 loss: 3.17396598e-06
Iter: 744 loss: 3.17329e-06
Iter: 745 loss: 3.17167337e-06
Iter: 746 loss: 3.16959176e-06
Iter: 747 loss: 3.1694367e-06
Iter: 748 loss: 3.16755541e-06
Iter: 749 loss: 3.1675163e-06
Iter: 750 loss: 3.16582054e-06
Iter: 751 loss: 3.16288242e-06
Iter: 752 loss: 3.16280966e-06
Iter: 753 loss: 3.16027445e-06
Iter: 754 loss: 3.17609829e-06
Iter: 755 loss: 3.15998113e-06
Iter: 756 loss: 3.15770421e-06
Iter: 757 loss: 3.16642695e-06
Iter: 758 loss: 3.15710486e-06
Iter: 759 loss: 3.15490252e-06
Iter: 760 loss: 3.16831893e-06
Iter: 761 loss: 3.15454963e-06
Iter: 762 loss: 3.15298439e-06
Iter: 763 loss: 3.15261968e-06
Iter: 764 loss: 3.15163948e-06
Iter: 765 loss: 3.14966769e-06
Iter: 766 loss: 3.16619798e-06
Iter: 767 loss: 3.14955378e-06
Iter: 768 loss: 3.14757381e-06
Iter: 769 loss: 3.15400098e-06
Iter: 770 loss: 3.14701447e-06
Iter: 771 loss: 3.14545241e-06
Iter: 772 loss: 3.14310046e-06
Iter: 773 loss: 3.1430377e-06
Iter: 774 loss: 3.14204362e-06
Iter: 775 loss: 3.14179806e-06
Iter: 776 loss: 3.1405084e-06
Iter: 777 loss: 3.13904866e-06
Iter: 778 loss: 3.13887654e-06
Iter: 779 loss: 3.13709256e-06
Iter: 780 loss: 3.14040426e-06
Iter: 781 loss: 3.13633836e-06
Iter: 782 loss: 3.13459532e-06
Iter: 783 loss: 3.14660792e-06
Iter: 784 loss: 3.13442956e-06
Iter: 785 loss: 3.13267083e-06
Iter: 786 loss: 3.13512373e-06
Iter: 787 loss: 3.13196597e-06
Iter: 788 loss: 3.13052215e-06
Iter: 789 loss: 3.12846e-06
Iter: 790 loss: 3.12848056e-06
Iter: 791 loss: 3.12590828e-06
Iter: 792 loss: 3.15369516e-06
Iter: 793 loss: 3.12598627e-06
Iter: 794 loss: 3.12357156e-06
Iter: 795 loss: 3.13214559e-06
Iter: 796 loss: 3.12292809e-06
Iter: 797 loss: 3.12127304e-06
Iter: 798 loss: 3.12177781e-06
Iter: 799 loss: 3.12018415e-06
Iter: 800 loss: 3.11847816e-06
Iter: 801 loss: 3.11849044e-06
Iter: 802 loss: 3.11707254e-06
Iter: 803 loss: 3.11653093e-06
Iter: 804 loss: 3.11566328e-06
Iter: 805 loss: 3.11448366e-06
Iter: 806 loss: 3.11472104e-06
Iter: 807 loss: 3.11352187e-06
Iter: 808 loss: 3.1116333e-06
Iter: 809 loss: 3.12323755e-06
Iter: 810 loss: 3.11140957e-06
Iter: 811 loss: 3.10933683e-06
Iter: 812 loss: 3.11318126e-06
Iter: 813 loss: 3.10843643e-06
Iter: 814 loss: 3.10675864e-06
Iter: 815 loss: 3.10346377e-06
Iter: 816 loss: 3.17486888e-06
Iter: 817 loss: 3.10345058e-06
Iter: 818 loss: 3.10239557e-06
Iter: 819 loss: 3.10189012e-06
Iter: 820 loss: 3.10038467e-06
Iter: 821 loss: 3.10117048e-06
Iter: 822 loss: 3.09945153e-06
Iter: 823 loss: 3.09775919e-06
Iter: 824 loss: 3.09569714e-06
Iter: 825 loss: 3.09552388e-06
Iter: 826 loss: 3.09260463e-06
Iter: 827 loss: 3.11496842e-06
Iter: 828 loss: 3.0925039e-06
Iter: 829 loss: 3.0909921e-06
Iter: 830 loss: 3.0909514e-06
Iter: 831 loss: 3.08992912e-06
Iter: 832 loss: 3.08722906e-06
Iter: 833 loss: 3.10795713e-06
Iter: 834 loss: 3.08665813e-06
Iter: 835 loss: 3.08608742e-06
Iter: 836 loss: 3.08487915e-06
Iter: 837 loss: 3.08391645e-06
Iter: 838 loss: 3.08321205e-06
Iter: 839 loss: 3.08291828e-06
Iter: 840 loss: 3.08159451e-06
Iter: 841 loss: 3.07945447e-06
Iter: 842 loss: 3.07942855e-06
Iter: 843 loss: 3.07780715e-06
Iter: 844 loss: 3.07753453e-06
Iter: 845 loss: 3.07666141e-06
Iter: 846 loss: 3.07555888e-06
Iter: 847 loss: 3.07552227e-06
Iter: 848 loss: 3.07395158e-06
Iter: 849 loss: 3.07694791e-06
Iter: 850 loss: 3.07334039e-06
Iter: 851 loss: 3.07174264e-06
Iter: 852 loss: 3.07818027e-06
Iter: 853 loss: 3.07136133e-06
Iter: 854 loss: 3.06966263e-06
Iter: 855 loss: 3.07258347e-06
Iter: 856 loss: 3.06885795e-06
Iter: 857 loss: 3.06731636e-06
Iter: 858 loss: 3.06867378e-06
Iter: 859 loss: 3.06631659e-06
Iter: 860 loss: 3.06431821e-06
Iter: 861 loss: 3.07290497e-06
Iter: 862 loss: 3.06392303e-06
Iter: 863 loss: 3.06231368e-06
Iter: 864 loss: 3.0800818e-06
Iter: 865 loss: 3.06231777e-06
Iter: 866 loss: 3.06122297e-06
Iter: 867 loss: 3.05958656e-06
Iter: 868 loss: 3.0595379e-06
Iter: 869 loss: 3.05799767e-06
Iter: 870 loss: 3.05784579e-06
Iter: 871 loss: 3.05683443e-06
Iter: 872 loss: 3.05445747e-06
Iter: 873 loss: 3.0856113e-06
Iter: 874 loss: 3.05432241e-06
Iter: 875 loss: 3.05229105e-06
Iter: 876 loss: 3.07106257e-06
Iter: 877 loss: 3.05221783e-06
Iter: 878 loss: 3.05049502e-06
Iter: 879 loss: 3.06317565e-06
Iter: 880 loss: 3.050332e-06
Iter: 881 loss: 3.04920331e-06
Iter: 882 loss: 3.04676678e-06
Iter: 883 loss: 3.08559333e-06
Iter: 884 loss: 3.04661808e-06
Iter: 885 loss: 3.04427294e-06
Iter: 886 loss: 3.06210109e-06
Iter: 887 loss: 3.0440483e-06
Iter: 888 loss: 3.0424726e-06
Iter: 889 loss: 3.06356537e-06
Iter: 890 loss: 3.04245168e-06
Iter: 891 loss: 3.04153946e-06
Iter: 892 loss: 3.04076275e-06
Iter: 893 loss: 3.04049945e-06
Iter: 894 loss: 3.03850129e-06
Iter: 895 loss: 3.03825209e-06
Iter: 896 loss: 3.03687102e-06
Iter: 897 loss: 3.03480829e-06
Iter: 898 loss: 3.0348117e-06
Iter: 899 loss: 3.03310571e-06
Iter: 900 loss: 3.03626643e-06
Iter: 901 loss: 3.03234401e-06
Iter: 902 loss: 3.03074239e-06
Iter: 903 loss: 3.03603065e-06
Iter: 904 loss: 3.03024126e-06
Iter: 905 loss: 3.02875219e-06
Iter: 906 loss: 3.04142441e-06
Iter: 907 loss: 3.02860872e-06
Iter: 908 loss: 3.02756553e-06
Iter: 909 loss: 3.02525541e-06
Iter: 910 loss: 3.05912772e-06
Iter: 911 loss: 3.02509193e-06
Iter: 912 loss: 3.02394324e-06
Iter: 913 loss: 3.02384251e-06
Iter: 914 loss: 3.0224669e-06
Iter: 915 loss: 3.02187141e-06
Iter: 916 loss: 3.02111243e-06
Iter: 917 loss: 3.01979981e-06
Iter: 918 loss: 3.02225862e-06
Iter: 919 loss: 3.01920591e-06
Iter: 920 loss: 3.01774799e-06
Iter: 921 loss: 3.01898581e-06
Iter: 922 loss: 3.01690443e-06
Iter: 923 loss: 3.0153e-06
Iter: 924 loss: 3.03665911e-06
Iter: 925 loss: 3.01534851e-06
Iter: 926 loss: 3.01390651e-06
Iter: 927 loss: 3.01113323e-06
Iter: 928 loss: 3.06079187e-06
Iter: 929 loss: 3.01101e-06
Iter: 930 loss: 3.00862371e-06
Iter: 931 loss: 3.03006618e-06
Iter: 932 loss: 3.00858119e-06
Iter: 933 loss: 3.00695569e-06
Iter: 934 loss: 3.02273338e-06
Iter: 935 loss: 3.00694023e-06
Iter: 936 loss: 3.00554598e-06
Iter: 937 loss: 3.00751435e-06
Iter: 938 loss: 3.00492138e-06
Iter: 939 loss: 3.00381953e-06
Iter: 940 loss: 3.01223417e-06
Iter: 941 loss: 3.00377815e-06
Iter: 942 loss: 3.00256443e-06
Iter: 943 loss: 3.00208058e-06
Iter: 944 loss: 3.00145211e-06
Iter: 945 loss: 2.99999e-06
Iter: 946 loss: 3.00253146e-06
Iter: 947 loss: 2.99946214e-06
Iter: 948 loss: 2.9982707e-06
Iter: 949 loss: 3.0072847e-06
Iter: 950 loss: 2.99815474e-06
Iter: 951 loss: 2.9967141e-06
Iter: 952 loss: 2.99711769e-06
Iter: 953 loss: 2.99572753e-06
Iter: 954 loss: 2.99448948e-06
Iter: 955 loss: 2.99293333e-06
Iter: 956 loss: 2.99279941e-06
Iter: 957 loss: 2.99046e-06
Iter: 958 loss: 3.00153238e-06
Iter: 959 loss: 2.99005569e-06
Iter: 960 loss: 2.98816713e-06
Iter: 961 loss: 3.01320802e-06
Iter: 962 loss: 2.9880739e-06
Iter: 963 loss: 2.98689156e-06
Iter: 964 loss: 2.98669579e-06
Iter: 965 loss: 2.98573877e-06
Iter: 966 loss: 2.98422174e-06
Iter: 967 loss: 2.98396117e-06
Iter: 968 loss: 2.98277428e-06
Iter: 969 loss: 2.98163241e-06
Iter: 970 loss: 2.98147916e-06
Iter: 971 loss: 2.98004716e-06
Iter: 972 loss: 2.97814472e-06
Iter: 973 loss: 2.97798761e-06
Iter: 974 loss: 2.97666293e-06
Iter: 975 loss: 2.97646238e-06
Iter: 976 loss: 2.97548149e-06
Iter: 977 loss: 2.97454562e-06
Iter: 978 loss: 2.97438328e-06
Iter: 979 loss: 2.97272072e-06
Iter: 980 loss: 2.97229099e-06
Iter: 981 loss: 2.97136489e-06
Iter: 982 loss: 2.97000497e-06
Iter: 983 loss: 2.96990038e-06
Iter: 984 loss: 2.9688008e-06
Iter: 985 loss: 2.9675864e-06
Iter: 986 loss: 2.96746589e-06
Iter: 987 loss: 2.96566259e-06
Iter: 988 loss: 2.96865619e-06
Iter: 989 loss: 2.96497387e-06
Iter: 990 loss: 2.96332428e-06
Iter: 991 loss: 2.96373514e-06
Iter: 992 loss: 2.9621774e-06
Iter: 993 loss: 2.96068492e-06
Iter: 994 loss: 2.96061239e-06
Iter: 995 loss: 2.95939481e-06
Iter: 996 loss: 2.95763311e-06
Iter: 997 loss: 2.95758377e-06
Iter: 998 loss: 2.95566747e-06
Iter: 999 loss: 2.96450571e-06
Iter: 1000 loss: 2.95525842e-06
Iter: 1001 loss: 2.95375708e-06
Iter: 1002 loss: 2.97059114e-06
Iter: 1003 loss: 2.95373661e-06
Iter: 1004 loss: 2.95231371e-06
Iter: 1005 loss: 2.9515104e-06
Iter: 1006 loss: 2.95093196e-06
Iter: 1007 loss: 2.9498151e-06
Iter: 1008 loss: 2.94973415e-06
Iter: 1009 loss: 2.94914753e-06
Iter: 1010 loss: 2.94760503e-06
Iter: 1011 loss: 2.95673044e-06
Iter: 1012 loss: 2.94730626e-06
Iter: 1013 loss: 2.94565393e-06
Iter: 1014 loss: 2.96938492e-06
Iter: 1015 loss: 2.94564961e-06
Iter: 1016 loss: 2.94451888e-06
Iter: 1017 loss: 2.95338577e-06
Iter: 1018 loss: 2.94446863e-06
Iter: 1019 loss: 2.94359461e-06
Iter: 1020 loss: 2.94159963e-06
Iter: 1021 loss: 2.96165331e-06
Iter: 1022 loss: 2.94131678e-06
Iter: 1023 loss: 2.93869471e-06
Iter: 1024 loss: 2.95122663e-06
Iter: 1025 loss: 2.93827429e-06
Iter: 1026 loss: 2.93622134e-06
Iter: 1027 loss: 2.94224697e-06
Iter: 1028 loss: 2.93560606e-06
Iter: 1029 loss: 2.93413586e-06
Iter: 1030 loss: 2.95597511e-06
Iter: 1031 loss: 2.93412359e-06
Iter: 1032 loss: 2.93286803e-06
Iter: 1033 loss: 2.93002086e-06
Iter: 1034 loss: 2.97851716e-06
Iter: 1035 loss: 2.92996106e-06
Iter: 1036 loss: 2.92811774e-06
Iter: 1037 loss: 2.95581322e-06
Iter: 1038 loss: 2.92813411e-06
Iter: 1039 loss: 2.92662025e-06
Iter: 1040 loss: 2.93535163e-06
Iter: 1041 loss: 2.92642972e-06
Iter: 1042 loss: 2.92515529e-06
Iter: 1043 loss: 2.92761388e-06
Iter: 1044 loss: 2.92463847e-06
Iter: 1045 loss: 2.92334153e-06
Iter: 1046 loss: 2.93142239e-06
Iter: 1047 loss: 2.92318964e-06
Iter: 1048 loss: 2.92237678e-06
Iter: 1049 loss: 2.92063373e-06
Iter: 1050 loss: 2.95258906e-06
Iter: 1051 loss: 2.92066443e-06
Iter: 1052 loss: 2.91894594e-06
Iter: 1053 loss: 2.93513267e-06
Iter: 1054 loss: 2.91892093e-06
Iter: 1055 loss: 2.91682863e-06
Iter: 1056 loss: 2.91671563e-06
Iter: 1057 loss: 2.91510059e-06
Iter: 1058 loss: 2.91333436e-06
Iter: 1059 loss: 2.91660945e-06
Iter: 1060 loss: 2.91259312e-06
Iter: 1061 loss: 2.91087918e-06
Iter: 1062 loss: 2.91479319e-06
Iter: 1063 loss: 2.91021252e-06
Iter: 1064 loss: 2.9085063e-06
Iter: 1065 loss: 2.91518108e-06
Iter: 1066 loss: 2.90823846e-06
Iter: 1067 loss: 2.90668254e-06
Iter: 1068 loss: 2.91833067e-06
Iter: 1069 loss: 2.90655044e-06
Iter: 1070 loss: 2.90548905e-06
Iter: 1071 loss: 2.90637536e-06
Iter: 1072 loss: 2.9049022e-06
Iter: 1073 loss: 2.90357752e-06
Iter: 1074 loss: 2.9039752e-06
Iter: 1075 loss: 2.90268099e-06
Iter: 1076 loss: 2.9017383e-06
Iter: 1077 loss: 2.90163098e-06
Iter: 1078 loss: 2.90086746e-06
Iter: 1079 loss: 2.90000753e-06
Iter: 1080 loss: 2.89981381e-06
Iter: 1081 loss: 2.89817399e-06
Iter: 1082 loss: 2.90517141e-06
Iter: 1083 loss: 2.89776813e-06
Iter: 1084 loss: 2.89672334e-06
Iter: 1085 loss: 2.89562286e-06
Iter: 1086 loss: 2.89542572e-06
Iter: 1087 loss: 2.89364107e-06
Iter: 1088 loss: 2.90613571e-06
Iter: 1089 loss: 2.89351556e-06
Iter: 1090 loss: 2.89158561e-06
Iter: 1091 loss: 2.89800278e-06
Iter: 1092 loss: 2.8910174e-06
Iter: 1093 loss: 2.89024729e-06
Iter: 1094 loss: 2.88840693e-06
Iter: 1095 loss: 2.91128799e-06
Iter: 1096 loss: 2.88834235e-06
Iter: 1097 loss: 2.88616138e-06
Iter: 1098 loss: 2.90479375e-06
Iter: 1099 loss: 2.88606543e-06
Iter: 1100 loss: 2.88434853e-06
Iter: 1101 loss: 2.88831438e-06
Iter: 1102 loss: 2.88366118e-06
Iter: 1103 loss: 2.88218303e-06
Iter: 1104 loss: 2.89720356e-06
Iter: 1105 loss: 2.8820923e-06
Iter: 1106 loss: 2.88071988e-06
Iter: 1107 loss: 2.87957e-06
Iter: 1108 loss: 2.87921625e-06
Iter: 1109 loss: 2.8773984e-06
Iter: 1110 loss: 2.88849174e-06
Iter: 1111 loss: 2.87714875e-06
Iter: 1112 loss: 2.87585408e-06
Iter: 1113 loss: 2.89091872e-06
Iter: 1114 loss: 2.87584589e-06
Iter: 1115 loss: 2.8747e-06
Iter: 1116 loss: 2.87363537e-06
Iter: 1117 loss: 2.87334433e-06
Iter: 1118 loss: 2.87205808e-06
Iter: 1119 loss: 2.87204625e-06
Iter: 1120 loss: 2.8712941e-06
Iter: 1121 loss: 2.86918907e-06
Iter: 1122 loss: 2.88151296e-06
Iter: 1123 loss: 2.86863e-06
Iter: 1124 loss: 2.86900604e-06
Iter: 1125 loss: 2.86765589e-06
Iter: 1126 loss: 2.8669765e-06
Iter: 1127 loss: 2.86685918e-06
Iter: 1128 loss: 2.86638442e-06
Iter: 1129 loss: 2.86530576e-06
Iter: 1130 loss: 2.86355203e-06
Iter: 1131 loss: 2.90681646e-06
Iter: 1132 loss: 2.86355248e-06
Iter: 1133 loss: 2.86145814e-06
Iter: 1134 loss: 2.86624572e-06
Iter: 1135 loss: 2.86063869e-06
Iter: 1136 loss: 2.85873853e-06
Iter: 1137 loss: 2.85869919e-06
Iter: 1138 loss: 2.85734086e-06
Iter: 1139 loss: 2.86096383e-06
Iter: 1140 loss: 2.85685019e-06
Iter: 1141 loss: 2.85541205e-06
Iter: 1142 loss: 2.85465603e-06
Iter: 1143 loss: 2.8538866e-06
Iter: 1144 loss: 2.85214037e-06
Iter: 1145 loss: 2.86751e-06
Iter: 1146 loss: 2.85196506e-06
Iter: 1147 loss: 2.85056649e-06
Iter: 1148 loss: 2.86350928e-06
Iter: 1149 loss: 2.85049168e-06
Iter: 1150 loss: 2.84946168e-06
Iter: 1151 loss: 2.85e-06
Iter: 1152 loss: 2.84884686e-06
Iter: 1153 loss: 2.84789394e-06
Iter: 1154 loss: 2.85478154e-06
Iter: 1155 loss: 2.84783209e-06
Iter: 1156 loss: 2.84689827e-06
Iter: 1157 loss: 2.84501493e-06
Iter: 1158 loss: 2.87304965e-06
Iter: 1159 loss: 2.84488851e-06
Iter: 1160 loss: 2.84386624e-06
Iter: 1161 loss: 2.84369366e-06
Iter: 1162 loss: 2.8425934e-06
Iter: 1163 loss: 2.84343878e-06
Iter: 1164 loss: 2.84194175e-06
Iter: 1165 loss: 2.84053976e-06
Iter: 1166 loss: 2.83886834e-06
Iter: 1167 loss: 2.83869986e-06
Iter: 1168 loss: 2.83652798e-06
Iter: 1169 loss: 2.83916916e-06
Iter: 1170 loss: 2.83544546e-06
Iter: 1171 loss: 2.83318468e-06
Iter: 1172 loss: 2.84101861e-06
Iter: 1173 loss: 2.83262443e-06
Iter: 1174 loss: 2.83116196e-06
Iter: 1175 loss: 2.83113923e-06
Iter: 1176 loss: 2.83007375e-06
Iter: 1177 loss: 2.83462077e-06
Iter: 1178 loss: 2.82984547e-06
Iter: 1179 loss: 2.82879864e-06
Iter: 1180 loss: 2.82686278e-06
Iter: 1181 loss: 2.86742443e-06
Iter: 1182 loss: 2.82684186e-06
Iter: 1183 loss: 2.82762721e-06
Iter: 1184 loss: 2.82609449e-06
Iter: 1185 loss: 2.82562951e-06
Iter: 1186 loss: 2.8245156e-06
Iter: 1187 loss: 2.83884219e-06
Iter: 1188 loss: 2.82438168e-06
Iter: 1189 loss: 2.82309929e-06
Iter: 1190 loss: 2.83473219e-06
Iter: 1191 loss: 2.82309247e-06
Iter: 1192 loss: 2.8219913e-06
Iter: 1193 loss: 2.82067163e-06
Iter: 1194 loss: 2.82057499e-06
Iter: 1195 loss: 2.81886582e-06
Iter: 1196 loss: 2.83428881e-06
Iter: 1197 loss: 2.81877828e-06
Iter: 1198 loss: 2.81764915e-06
Iter: 1199 loss: 2.82544852e-06
Iter: 1200 loss: 2.8175125e-06
Iter: 1201 loss: 2.81666416e-06
Iter: 1202 loss: 2.81466714e-06
Iter: 1203 loss: 2.84256748e-06
Iter: 1204 loss: 2.81455755e-06
Iter: 1205 loss: 2.81262e-06
Iter: 1206 loss: 2.82794144e-06
Iter: 1207 loss: 2.81241955e-06
Iter: 1208 loss: 2.81104349e-06
Iter: 1209 loss: 2.81069742e-06
Iter: 1210 loss: 2.80971926e-06
Iter: 1211 loss: 2.80818358e-06
Iter: 1212 loss: 2.8204081e-06
Iter: 1213 loss: 2.80813742e-06
Iter: 1214 loss: 2.80650534e-06
Iter: 1215 loss: 2.81301868e-06
Iter: 1216 loss: 2.80615495e-06
Iter: 1217 loss: 2.80496056e-06
Iter: 1218 loss: 2.80753e-06
Iter: 1219 loss: 2.80448808e-06
Iter: 1220 loss: 2.80352697e-06
Iter: 1221 loss: 2.81346752e-06
Iter: 1222 loss: 2.80353106e-06
Iter: 1223 loss: 2.80246149e-06
Iter: 1224 loss: 2.80123436e-06
Iter: 1225 loss: 2.8011325e-06
Iter: 1226 loss: 2.79988853e-06
Iter: 1227 loss: 2.80690438e-06
Iter: 1228 loss: 2.79975529e-06
Iter: 1229 loss: 2.79844153e-06
Iter: 1230 loss: 2.80047493e-06
Iter: 1231 loss: 2.79772826e-06
Iter: 1232 loss: 2.79677397e-06
Iter: 1233 loss: 2.79851429e-06
Iter: 1234 loss: 2.79628512e-06
Iter: 1235 loss: 2.79500978e-06
Iter: 1236 loss: 2.80150107e-06
Iter: 1237 loss: 2.79466462e-06
Iter: 1238 loss: 2.79360574e-06
Iter: 1239 loss: 2.79297865e-06
Iter: 1240 loss: 2.79253868e-06
Iter: 1241 loss: 2.79098549e-06
Iter: 1242 loss: 2.79329061e-06
Iter: 1243 loss: 2.79026381e-06
Iter: 1244 loss: 2.78869766e-06
Iter: 1245 loss: 2.79510687e-06
Iter: 1246 loss: 2.78823472e-06
Iter: 1247 loss: 2.78681455e-06
Iter: 1248 loss: 2.78618427e-06
Iter: 1249 loss: 2.78544985e-06
Iter: 1250 loss: 2.78420976e-06
Iter: 1251 loss: 2.78405969e-06
Iter: 1252 loss: 2.78284597e-06
Iter: 1253 loss: 2.78240896e-06
Iter: 1254 loss: 2.78172706e-06
Iter: 1255 loss: 2.7804424e-06
Iter: 1256 loss: 2.79493224e-06
Iter: 1257 loss: 2.78039852e-06
Iter: 1258 loss: 2.77913432e-06
Iter: 1259 loss: 2.78099787e-06
Iter: 1260 loss: 2.7785461e-06
Iter: 1261 loss: 2.77758863e-06
Iter: 1262 loss: 2.77671393e-06
Iter: 1263 loss: 2.77640561e-06
Iter: 1264 loss: 2.77529853e-06
Iter: 1265 loss: 2.77533263e-06
Iter: 1266 loss: 2.77436152e-06
Iter: 1267 loss: 2.77356685e-06
Iter: 1268 loss: 2.77336926e-06
Iter: 1269 loss: 2.7724584e-06
Iter: 1270 loss: 2.77246704e-06
Iter: 1271 loss: 2.77186928e-06
Iter: 1272 loss: 2.77039499e-06
Iter: 1273 loss: 2.78687207e-06
Iter: 1274 loss: 2.77019353e-06
Iter: 1275 loss: 2.76860419e-06
Iter: 1276 loss: 2.77767867e-06
Iter: 1277 loss: 2.76836022e-06
Iter: 1278 loss: 2.76696824e-06
Iter: 1279 loss: 2.76741594e-06
Iter: 1280 loss: 2.76595438e-06
Iter: 1281 loss: 2.76425112e-06
Iter: 1282 loss: 2.77485333e-06
Iter: 1283 loss: 2.76406581e-06
Iter: 1284 loss: 2.76258743e-06
Iter: 1285 loss: 2.76231776e-06
Iter: 1286 loss: 2.76124592e-06
Iter: 1287 loss: 2.76032551e-06
Iter: 1288 loss: 2.76013884e-06
Iter: 1289 loss: 2.75904358e-06
Iter: 1290 loss: 2.75780735e-06
Iter: 1291 loss: 2.75767911e-06
Iter: 1292 loss: 2.75712841e-06
Iter: 1293 loss: 2.75685829e-06
Iter: 1294 loss: 2.75627599e-06
Iter: 1295 loss: 2.75543948e-06
Iter: 1296 loss: 2.75532716e-06
Iter: 1297 loss: 2.75425487e-06
Iter: 1298 loss: 2.75364664e-06
Iter: 1299 loss: 2.75313278e-06
Iter: 1300 loss: 2.75167122e-06
Iter: 1301 loss: 2.75901198e-06
Iter: 1302 loss: 2.75147454e-06
Iter: 1303 loss: 2.74998683e-06
Iter: 1304 loss: 2.7637891e-06
Iter: 1305 loss: 2.74999252e-06
Iter: 1306 loss: 2.74906779e-06
Iter: 1307 loss: 2.74834656e-06
Iter: 1308 loss: 2.7480387e-06
Iter: 1309 loss: 2.74670469e-06
Iter: 1310 loss: 2.75846583e-06
Iter: 1311 loss: 2.74669878e-06
Iter: 1312 loss: 2.74549666e-06
Iter: 1313 loss: 2.74464014e-06
Iter: 1314 loss: 2.74427589e-06
Iter: 1315 loss: 2.74282547e-06
Iter: 1316 loss: 2.74532385e-06
Iter: 1317 loss: 2.74210038e-06
Iter: 1318 loss: 2.74073568e-06
Iter: 1319 loss: 2.74727972e-06
Iter: 1320 loss: 2.7405265e-06
Iter: 1321 loss: 2.7390829e-06
Iter: 1322 loss: 2.74081685e-06
Iter: 1323 loss: 2.73833416e-06
Iter: 1324 loss: 2.73715523e-06
Iter: 1325 loss: 2.73717933e-06
Iter: 1326 loss: 2.73626119e-06
Iter: 1327 loss: 2.73642399e-06
Iter: 1328 loss: 2.73557362e-06
Iter: 1329 loss: 2.7343267e-06
Iter: 1330 loss: 2.74271133e-06
Iter: 1331 loss: 2.73414093e-06
Iter: 1332 loss: 2.73315709e-06
Iter: 1333 loss: 2.73195974e-06
Iter: 1334 loss: 2.73179899e-06
Iter: 1335 loss: 2.73008891e-06
Iter: 1336 loss: 2.72979946e-06
Iter: 1337 loss: 2.72871171e-06
Iter: 1338 loss: 2.72737861e-06
Iter: 1339 loss: 2.7273602e-06
Iter: 1340 loss: 2.7258618e-06
Iter: 1341 loss: 2.72691386e-06
Iter: 1342 loss: 2.72507464e-06
Iter: 1343 loss: 2.72419948e-06
Iter: 1344 loss: 2.72714942e-06
Iter: 1345 loss: 2.72401894e-06
Iter: 1346 loss: 2.72288662e-06
Iter: 1347 loss: 2.72275201e-06
Iter: 1348 loss: 2.72198e-06
Iter: 1349 loss: 2.72059765e-06
Iter: 1350 loss: 2.72425632e-06
Iter: 1351 loss: 2.72015041e-06
Iter: 1352 loss: 2.71904014e-06
Iter: 1353 loss: 2.71745694e-06
Iter: 1354 loss: 2.71739646e-06
Iter: 1355 loss: 2.71564613e-06
Iter: 1356 loss: 2.73897899e-06
Iter: 1357 loss: 2.71556e-06
Iter: 1358 loss: 2.71402905e-06
Iter: 1359 loss: 2.72070247e-06
Iter: 1360 loss: 2.71370936e-06
Iter: 1361 loss: 2.71233603e-06
Iter: 1362 loss: 2.72374723e-06
Iter: 1363 loss: 2.7122278e-06
Iter: 1364 loss: 2.71130284e-06
Iter: 1365 loss: 2.7129513e-06
Iter: 1366 loss: 2.71079398e-06
Iter: 1367 loss: 2.70953569e-06
Iter: 1368 loss: 2.70969235e-06
Iter: 1369 loss: 2.70848341e-06
Iter: 1370 loss: 2.70738929e-06
Iter: 1371 loss: 2.70728538e-06
Iter: 1372 loss: 2.70656119e-06
Iter: 1373 loss: 2.7049673e-06
Iter: 1374 loss: 2.71594081e-06
Iter: 1375 loss: 2.70483611e-06
Iter: 1376 loss: 2.70365649e-06
Iter: 1377 loss: 2.71532417e-06
Iter: 1378 loss: 2.70368105e-06
Iter: 1379 loss: 2.70287455e-06
Iter: 1380 loss: 2.70157784e-06
Iter: 1381 loss: 2.73169e-06
Iter: 1382 loss: 2.7015667e-06
Iter: 1383 loss: 2.70063083e-06
Iter: 1384 loss: 2.70057762e-06
Iter: 1385 loss: 2.69979773e-06
Iter: 1386 loss: 2.69869952e-06
Iter: 1387 loss: 2.69863472e-06
Iter: 1388 loss: 2.69703969e-06
Iter: 1389 loss: 2.69823477e-06
Iter: 1390 loss: 2.69604607e-06
Iter: 1391 loss: 2.69424891e-06
Iter: 1392 loss: 2.69777092e-06
Iter: 1393 loss: 2.6934656e-06
Iter: 1394 loss: 2.6923733e-06
Iter: 1395 loss: 2.69230918e-06
Iter: 1396 loss: 2.69123e-06
Iter: 1397 loss: 2.69442353e-06
Iter: 1398 loss: 2.69099291e-06
Iter: 1399 loss: 2.68999e-06
Iter: 1400 loss: 2.69361499e-06
Iter: 1401 loss: 2.68970348e-06
Iter: 1402 loss: 2.68888971e-06
Iter: 1403 loss: 2.69055249e-06
Iter: 1404 loss: 2.68854546e-06
Iter: 1405 loss: 2.68766871e-06
Iter: 1406 loss: 2.68592203e-06
Iter: 1407 loss: 2.72386751e-06
Iter: 1408 loss: 2.68592953e-06
Iter: 1409 loss: 2.68452732e-06
Iter: 1410 loss: 2.7039755e-06
Iter: 1411 loss: 2.6843968e-06
Iter: 1412 loss: 2.68348549e-06
Iter: 1413 loss: 2.69494512e-06
Iter: 1414 loss: 2.68344411e-06
Iter: 1415 loss: 2.68258964e-06
Iter: 1416 loss: 2.68101985e-06
Iter: 1417 loss: 2.71064232e-06
Iter: 1418 loss: 2.68094891e-06
Iter: 1419 loss: 2.67957262e-06
Iter: 1420 loss: 2.69371822e-06
Iter: 1421 loss: 2.67958512e-06
Iter: 1422 loss: 2.67832638e-06
Iter: 1423 loss: 2.68192503e-06
Iter: 1424 loss: 2.67791393e-06
Iter: 1425 loss: 2.67702922e-06
Iter: 1426 loss: 2.67586688e-06
Iter: 1427 loss: 2.67581709e-06
Iter: 1428 loss: 2.67447535e-06
Iter: 1429 loss: 2.68753047e-06
Iter: 1430 loss: 2.6743985e-06
Iter: 1431 loss: 2.67337441e-06
Iter: 1432 loss: 2.67233372e-06
Iter: 1433 loss: 2.6721541e-06
Iter: 1434 loss: 2.67200539e-06
Iter: 1435 loss: 2.67127712e-06
Iter: 1436 loss: 2.67062296e-06
Iter: 1437 loss: 2.66981783e-06
Iter: 1438 loss: 2.66978873e-06
Iter: 1439 loss: 2.66868915e-06
Iter: 1440 loss: 2.6763837e-06
Iter: 1441 loss: 2.66859524e-06
Iter: 1442 loss: 2.66779512e-06
Iter: 1443 loss: 2.66689221e-06
Iter: 1444 loss: 2.66683082e-06
Iter: 1445 loss: 2.66535403e-06
Iter: 1446 loss: 2.6726882e-06
Iter: 1447 loss: 2.66516417e-06
Iter: 1448 loss: 2.66393226e-06
Iter: 1449 loss: 2.67371479e-06
Iter: 1450 loss: 2.66380766e-06
Iter: 1451 loss: 2.66279676e-06
Iter: 1452 loss: 2.6614448e-06
Iter: 1453 loss: 2.66129064e-06
Iter: 1454 loss: 2.6600851e-06
Iter: 1455 loss: 2.67053247e-06
Iter: 1456 loss: 2.66004827e-06
Iter: 1457 loss: 2.65907397e-06
Iter: 1458 loss: 2.66373308e-06
Iter: 1459 loss: 2.65888639e-06
Iter: 1460 loss: 2.65781864e-06
Iter: 1461 loss: 2.65607559e-06
Iter: 1462 loss: 2.65605172e-06
Iter: 1463 loss: 2.6542757e-06
Iter: 1464 loss: 2.65768631e-06
Iter: 1465 loss: 2.65373069e-06
Iter: 1466 loss: 2.65187919e-06
Iter: 1467 loss: 2.66141046e-06
Iter: 1468 loss: 2.65168069e-06
Iter: 1469 loss: 2.65048425e-06
Iter: 1470 loss: 2.66198231e-06
Iter: 1471 loss: 2.65052222e-06
Iter: 1472 loss: 2.64918049e-06
Iter: 1473 loss: 2.65090898e-06
Iter: 1474 loss: 2.64852429e-06
Iter: 1475 loss: 2.64766049e-06
Iter: 1476 loss: 2.65087328e-06
Iter: 1477 loss: 2.64749e-06
Iter: 1478 loss: 2.64655864e-06
Iter: 1479 loss: 2.64698861e-06
Iter: 1480 loss: 2.64593746e-06
Iter: 1481 loss: 2.6449577e-06
Iter: 1482 loss: 2.64595656e-06
Iter: 1483 loss: 2.64438427e-06
Iter: 1484 loss: 2.6433222e-06
Iter: 1485 loss: 2.65566678e-06
Iter: 1486 loss: 2.64339042e-06
Iter: 1487 loss: 2.64223e-06
Iter: 1488 loss: 2.64092523e-06
Iter: 1489 loss: 2.64080677e-06
Iter: 1490 loss: 2.63964466e-06
Iter: 1491 loss: 2.64646e-06
Iter: 1492 loss: 2.63948823e-06
Iter: 1493 loss: 2.63822631e-06
Iter: 1494 loss: 2.64060145e-06
Iter: 1495 loss: 2.63774041e-06
Iter: 1496 loss: 2.63628635e-06
Iter: 1497 loss: 2.6402613e-06
Iter: 1498 loss: 2.63569291e-06
Iter: 1499 loss: 2.63456832e-06
Iter: 1500 loss: 2.63475522e-06
Iter: 1501 loss: 2.63378365e-06
Iter: 1502 loss: 2.63242669e-06
Iter: 1503 loss: 2.63285756e-06
Iter: 1504 loss: 2.63142647e-06
Iter: 1505 loss: 2.63107108e-06
Iter: 1506 loss: 2.63067477e-06
Iter: 1507 loss: 2.62990034e-06
Iter: 1508 loss: 2.63077072e-06
Iter: 1509 loss: 2.62951016e-06
Iter: 1510 loss: 2.62864842e-06
Iter: 1511 loss: 2.62773938e-06
Iter: 1512 loss: 2.62761591e-06
Iter: 1513 loss: 2.62635899e-06
Iter: 1514 loss: 2.64383493e-06
Iter: 1515 loss: 2.62632898e-06
Iter: 1516 loss: 2.62542835e-06
Iter: 1517 loss: 2.6235025e-06
Iter: 1518 loss: 2.65194785e-06
Iter: 1519 loss: 2.62344838e-06
Iter: 1520 loss: 2.62322601e-06
Iter: 1521 loss: 2.62241201e-06
Iter: 1522 loss: 2.62161666e-06
Iter: 1523 loss: 2.62004778e-06
Iter: 1524 loss: 2.64720802e-06
Iter: 1525 loss: 2.62001e-06
Iter: 1526 loss: 2.61852642e-06
Iter: 1527 loss: 2.62445815e-06
Iter: 1528 loss: 2.61826699e-06
Iter: 1529 loss: 2.61697824e-06
Iter: 1530 loss: 2.62887511e-06
Iter: 1531 loss: 2.61691525e-06
Iter: 1532 loss: 2.61593141e-06
Iter: 1533 loss: 2.6170992e-06
Iter: 1534 loss: 2.61543823e-06
Iter: 1535 loss: 2.61453397e-06
Iter: 1536 loss: 2.61432433e-06
Iter: 1537 loss: 2.61374134e-06
Iter: 1538 loss: 2.61246987e-06
Iter: 1539 loss: 2.62067829e-06
Iter: 1540 loss: 2.61225182e-06
Iter: 1541 loss: 2.61148534e-06
Iter: 1542 loss: 2.61148034e-06
Iter: 1543 loss: 2.61064906e-06
Iter: 1544 loss: 2.60891306e-06
Iter: 1545 loss: 2.63289894e-06
Iter: 1546 loss: 2.60882916e-06
Iter: 1547 loss: 2.60751199e-06
Iter: 1548 loss: 2.62363255e-06
Iter: 1549 loss: 2.60752449e-06
Iter: 1550 loss: 2.60652723e-06
Iter: 1551 loss: 2.61004311e-06
Iter: 1552 loss: 2.60623824e-06
Iter: 1553 loss: 2.60538854e-06
Iter: 1554 loss: 2.60460092e-06
Iter: 1555 loss: 2.60425168e-06
Iter: 1556 loss: 2.6032194e-06
Iter: 1557 loss: 2.60316938e-06
Iter: 1558 loss: 2.60248612e-06
Iter: 1559 loss: 2.60101615e-06
Iter: 1560 loss: 2.63265269e-06
Iter: 1561 loss: 2.60105026e-06
Iter: 1562 loss: 2.59969602e-06
Iter: 1563 loss: 2.6016578e-06
Iter: 1564 loss: 2.59910894e-06
Iter: 1565 loss: 2.59777903e-06
Iter: 1566 loss: 2.59774151e-06
Iter: 1567 loss: 2.59696071e-06
Iter: 1568 loss: 2.59735839e-06
Iter: 1569 loss: 2.59632e-06
Iter: 1570 loss: 2.59536341e-06
Iter: 1571 loss: 2.59433455e-06
Iter: 1572 loss: 2.59415219e-06
Iter: 1573 loss: 2.59337799e-06
Iter: 1574 loss: 2.59323951e-06
Iter: 1575 loss: 2.59233184e-06
Iter: 1576 loss: 2.5934969e-06
Iter: 1577 loss: 2.59184617e-06
Iter: 1578 loss: 2.59095827e-06
Iter: 1579 loss: 2.59089757e-06
Iter: 1580 loss: 2.59019134e-06
Iter: 1581 loss: 2.58892942e-06
Iter: 1582 loss: 2.59400895e-06
Iter: 1583 loss: 2.58866658e-06
Iter: 1584 loss: 2.58743376e-06
Iter: 1585 loss: 2.59086164e-06
Iter: 1586 loss: 2.58701675e-06
Iter: 1587 loss: 2.58597629e-06
Iter: 1588 loss: 2.58761838e-06
Iter: 1589 loss: 2.58538876e-06
Iter: 1590 loss: 2.5839031e-06
Iter: 1591 loss: 2.59259787e-06
Iter: 1592 loss: 2.58366049e-06
Iter: 1593 loss: 2.5828474e-06
Iter: 1594 loss: 2.58207365e-06
Iter: 1595 loss: 2.58186537e-06
Iter: 1596 loss: 2.58056548e-06
Iter: 1597 loss: 2.58431646e-06
Iter: 1598 loss: 2.58017917e-06
Iter: 1599 loss: 2.57936335e-06
Iter: 1600 loss: 2.59085914e-06
Iter: 1601 loss: 2.57932425e-06
Iter: 1602 loss: 2.57845636e-06
Iter: 1603 loss: 2.57695115e-06
Iter: 1604 loss: 2.5769059e-06
Iter: 1605 loss: 2.5754066e-06
Iter: 1606 loss: 2.58131e-06
Iter: 1607 loss: 2.57501597e-06
Iter: 1608 loss: 2.57414604e-06
Iter: 1609 loss: 2.57410443e-06
Iter: 1610 loss: 2.57325473e-06
Iter: 1611 loss: 2.57199849e-06
Iter: 1612 loss: 2.57196893e-06
Iter: 1613 loss: 2.57061856e-06
Iter: 1614 loss: 2.571918e-06
Iter: 1615 loss: 2.5698937e-06
Iter: 1616 loss: 2.56823546e-06
Iter: 1617 loss: 2.5854672e-06
Iter: 1618 loss: 2.56823387e-06
Iter: 1619 loss: 2.56713747e-06
Iter: 1620 loss: 2.567102e-06
Iter: 1621 loss: 2.56633825e-06
Iter: 1622 loss: 2.56529029e-06
Iter: 1623 loss: 2.56526482e-06
Iter: 1624 loss: 2.56458588e-06
Iter: 1625 loss: 2.5646641e-06
Iter: 1626 loss: 2.56409839e-06
Iter: 1627 loss: 2.56325757e-06
Iter: 1628 loss: 2.56310045e-06
Iter: 1629 loss: 2.56252497e-06
Iter: 1630 loss: 2.56143494e-06
Iter: 1631 loss: 2.56652766e-06
Iter: 1632 loss: 2.56124804e-06
Iter: 1633 loss: 2.56031626e-06
Iter: 1634 loss: 2.56875705e-06
Iter: 1635 loss: 2.56021167e-06
Iter: 1636 loss: 2.55941723e-06
Iter: 1637 loss: 2.55834857e-06
Iter: 1638 loss: 2.5582342e-06
Iter: 1639 loss: 2.55708846e-06
Iter: 1640 loss: 2.56241401e-06
Iter: 1641 loss: 2.55687223e-06
Iter: 1642 loss: 2.55548548e-06
Iter: 1643 loss: 2.5660654e-06
Iter: 1644 loss: 2.55539589e-06
Iter: 1645 loss: 2.55460054e-06
Iter: 1646 loss: 2.55285499e-06
Iter: 1647 loss: 2.57917804e-06
Iter: 1648 loss: 2.55281111e-06
Iter: 1649 loss: 2.55180566e-06
Iter: 1650 loss: 2.55176928e-06
Iter: 1651 loss: 2.55088617e-06
Iter: 1652 loss: 2.55103146e-06
Iter: 1653 loss: 2.55025e-06
Iter: 1654 loss: 2.54909764e-06
Iter: 1655 loss: 2.55309851e-06
Iter: 1656 loss: 2.5488348e-06
Iter: 1657 loss: 2.54761676e-06
Iter: 1658 loss: 2.55509121e-06
Iter: 1659 loss: 2.54750239e-06
Iter: 1660 loss: 2.54677479e-06
Iter: 1661 loss: 2.54556744e-06
Iter: 1662 loss: 2.54554584e-06
Iter: 1663 loss: 2.54426232e-06
Iter: 1664 loss: 2.5521108e-06
Iter: 1665 loss: 2.54405e-06
Iter: 1666 loss: 2.54283373e-06
Iter: 1667 loss: 2.5478555e-06
Iter: 1668 loss: 2.54254905e-06
Iter: 1669 loss: 2.54121824e-06
Iter: 1670 loss: 2.54602674e-06
Iter: 1671 loss: 2.54081806e-06
Iter: 1672 loss: 2.54004453e-06
Iter: 1673 loss: 2.53869757e-06
Iter: 1674 loss: 2.53866347e-06
Iter: 1675 loss: 2.53830467e-06
Iter: 1676 loss: 2.53779353e-06
Iter: 1677 loss: 2.53705366e-06
Iter: 1678 loss: 2.53642315e-06
Iter: 1679 loss: 2.5362956e-06
Iter: 1680 loss: 2.53518579e-06
Iter: 1681 loss: 2.53446706e-06
Iter: 1682 loss: 2.53409189e-06
Iter: 1683 loss: 2.53296253e-06
Iter: 1684 loss: 2.53299277e-06
Iter: 1685 loss: 2.53197186e-06
Iter: 1686 loss: 2.53267353e-06
Iter: 1687 loss: 2.53133703e-06
Iter: 1688 loss: 2.53060489e-06
Iter: 1689 loss: 2.53966414e-06
Iter: 1690 loss: 2.53060239e-06
Iter: 1691 loss: 2.52986774e-06
Iter: 1692 loss: 2.52885434e-06
Iter: 1693 loss: 2.5287718e-06
Iter: 1694 loss: 2.52762948e-06
Iter: 1695 loss: 2.52933205e-06
Iter: 1696 loss: 2.52717359e-06
Iter: 1697 loss: 2.52600285e-06
Iter: 1698 loss: 2.52809559e-06
Iter: 1699 loss: 2.52529867e-06
Iter: 1700 loss: 2.52427526e-06
Iter: 1701 loss: 2.52428367e-06
Iter: 1702 loss: 2.52337759e-06
Iter: 1703 loss: 2.52246377e-06
Iter: 1704 loss: 2.52227983e-06
Iter: 1705 loss: 2.52113614e-06
Iter: 1706 loss: 2.53067606e-06
Iter: 1707 loss: 2.52104837e-06
Iter: 1708 loss: 2.52004179e-06
Iter: 1709 loss: 2.5262807e-06
Iter: 1710 loss: 2.51999268e-06
Iter: 1711 loss: 2.51913661e-06
Iter: 1712 loss: 2.51794609e-06
Iter: 1713 loss: 2.51791334e-06
Iter: 1714 loss: 2.51650727e-06
Iter: 1715 loss: 2.51549454e-06
Iter: 1716 loss: 2.51503548e-06
Iter: 1717 loss: 2.51533311e-06
Iter: 1718 loss: 2.5143e-06
Iter: 1719 loss: 2.51376423e-06
Iter: 1720 loss: 2.51313713e-06
Iter: 1721 loss: 2.51310894e-06
Iter: 1722 loss: 2.51204574e-06
Iter: 1723 loss: 2.51807342e-06
Iter: 1724 loss: 2.51183701e-06
Iter: 1725 loss: 2.5109066e-06
Iter: 1726 loss: 2.51051915e-06
Iter: 1727 loss: 2.50998346e-06
Iter: 1728 loss: 2.50896846e-06
Iter: 1729 loss: 2.50899575e-06
Iter: 1730 loss: 2.50804487e-06
Iter: 1731 loss: 2.50679841e-06
Iter: 1732 loss: 2.51416122e-06
Iter: 1733 loss: 2.50651897e-06
Iter: 1734 loss: 2.50498624e-06
Iter: 1735 loss: 2.51250094e-06
Iter: 1736 loss: 2.50476319e-06
Iter: 1737 loss: 2.50366156e-06
Iter: 1738 loss: 2.50360085e-06
Iter: 1739 loss: 2.5027407e-06
Iter: 1740 loss: 2.50166613e-06
Iter: 1741 loss: 2.51894289e-06
Iter: 1742 loss: 2.50167113e-06
Iter: 1743 loss: 2.5005902e-06
Iter: 1744 loss: 2.50138442e-06
Iter: 1745 loss: 2.49996538e-06
Iter: 1746 loss: 2.49914842e-06
Iter: 1747 loss: 2.49940672e-06
Iter: 1748 loss: 2.49857135e-06
Iter: 1749 loss: 2.49735581e-06
Iter: 1750 loss: 2.49732739e-06
Iter: 1751 loss: 2.49634786e-06
Iter: 1752 loss: 2.49560867e-06
Iter: 1753 loss: 2.49542336e-06
Iter: 1754 loss: 2.49469417e-06
Iter: 1755 loss: 2.49427308e-06
Iter: 1756 loss: 2.493902e-06
Iter: 1757 loss: 2.49289087e-06
Iter: 1758 loss: 2.50145536e-06
Iter: 1759 loss: 2.49284585e-06
Iter: 1760 loss: 2.49191294e-06
Iter: 1761 loss: 2.49063123e-06
Iter: 1762 loss: 2.49057211e-06
Iter: 1763 loss: 2.4891724e-06
Iter: 1764 loss: 2.4932292e-06
Iter: 1765 loss: 2.48860488e-06
Iter: 1766 loss: 2.48729066e-06
Iter: 1767 loss: 2.4899573e-06
Iter: 1768 loss: 2.48669448e-06
Iter: 1769 loss: 2.48559149e-06
Iter: 1770 loss: 2.48560355e-06
Iter: 1771 loss: 2.48477e-06
Iter: 1772 loss: 2.48356673e-06
Iter: 1773 loss: 2.48356332e-06
Iter: 1774 loss: 2.48305241e-06
Iter: 1775 loss: 2.48276388e-06
Iter: 1776 loss: 2.48218976e-06
Iter: 1777 loss: 2.48167657e-06
Iter: 1778 loss: 2.48153674e-06
Iter: 1779 loss: 2.48064316e-06
Iter: 1780 loss: 2.47968819e-06
Iter: 1781 loss: 2.47958474e-06
Iter: 1782 loss: 2.47824528e-06
Iter: 1783 loss: 2.48730794e-06
Iter: 1784 loss: 2.47818366e-06
Iter: 1785 loss: 2.47698517e-06
Iter: 1786 loss: 2.48552169e-06
Iter: 1787 loss: 2.4769206e-06
Iter: 1788 loss: 2.4759247e-06
Iter: 1789 loss: 2.47828257e-06
Iter: 1790 loss: 2.47556318e-06
Iter: 1791 loss: 2.47477465e-06
Iter: 1792 loss: 2.47740627e-06
Iter: 1793 loss: 2.4745575e-06
Iter: 1794 loss: 2.47354956e-06
Iter: 1795 loss: 2.47332537e-06
Iter: 1796 loss: 2.47269372e-06
Iter: 1797 loss: 2.47141702e-06
Iter: 1798 loss: 2.46939226e-06
Iter: 1799 loss: 2.46927766e-06
Iter: 1800 loss: 2.46776381e-06
Iter: 1801 loss: 2.46774698e-06
Iter: 1802 loss: 2.46679247e-06
Iter: 1803 loss: 2.47390085e-06
Iter: 1804 loss: 2.46670379e-06
Iter: 1805 loss: 2.46568538e-06
Iter: 1806 loss: 2.46638979e-06
Iter: 1807 loss: 2.46505374e-06
Iter: 1808 loss: 2.46448462e-06
Iter: 1809 loss: 2.46449986e-06
Iter: 1810 loss: 2.46391483e-06
Iter: 1811 loss: 2.46252785e-06
Iter: 1812 loss: 2.48157221e-06
Iter: 1813 loss: 2.46240097e-06
Iter: 1814 loss: 2.46106083e-06
Iter: 1815 loss: 2.46166519e-06
Iter: 1816 loss: 2.46012974e-06
Iter: 1817 loss: 2.45828e-06
Iter: 1818 loss: 2.46479522e-06
Iter: 1819 loss: 2.45775482e-06
Iter: 1820 loss: 2.4564938e-06
Iter: 1821 loss: 2.46097443e-06
Iter: 1822 loss: 2.45611477e-06
Iter: 1823 loss: 2.45559659e-06
Iter: 1824 loss: 2.4553874e-06
Iter: 1825 loss: 2.45476349e-06
Iter: 1826 loss: 2.45371166e-06
Iter: 1827 loss: 2.45373053e-06
Iter: 1828 loss: 2.45254637e-06
Iter: 1829 loss: 2.45744468e-06
Iter: 1830 loss: 2.45226875e-06
Iter: 1831 loss: 2.45136289e-06
Iter: 1832 loss: 2.45806e-06
Iter: 1833 loss: 2.45121873e-06
Iter: 1834 loss: 2.45046704e-06
Iter: 1835 loss: 2.44934836e-06
Iter: 1836 loss: 2.44927355e-06
Iter: 1837 loss: 2.44799094e-06
Iter: 1838 loss: 2.45226124e-06
Iter: 1839 loss: 2.44764078e-06
Iter: 1840 loss: 2.44663852e-06
Iter: 1841 loss: 2.44665148e-06
Iter: 1842 loss: 2.44579e-06
Iter: 1843 loss: 2.44581793e-06
Iter: 1844 loss: 2.44520425e-06
Iter: 1845 loss: 2.4439355e-06
Iter: 1846 loss: 2.44895136e-06
Iter: 1847 loss: 2.44370631e-06
Iter: 1848 loss: 2.44267176e-06
Iter: 1849 loss: 2.44210651e-06
Iter: 1850 loss: 2.44166449e-06
Iter: 1851 loss: 2.44056423e-06
Iter: 1852 loss: 2.44052944e-06
Iter: 1853 loss: 2.4396868e-06
Iter: 1854 loss: 2.43816794e-06
Iter: 1855 loss: 2.44945022e-06
Iter: 1856 loss: 2.43799423e-06
Iter: 1857 loss: 2.43692193e-06
Iter: 1858 loss: 2.44239504e-06
Iter: 1859 loss: 2.43678687e-06
Iter: 1860 loss: 2.43638897e-06
Iter: 1861 loss: 2.43636259e-06
Iter: 1862 loss: 2.43583872e-06
Iter: 1863 loss: 2.43452951e-06
Iter: 1864 loss: 2.44304078e-06
Iter: 1865 loss: 2.43424961e-06
Iter: 1866 loss: 2.4328674e-06
Iter: 1867 loss: 2.44230932e-06
Iter: 1868 loss: 2.43277827e-06
Iter: 1869 loss: 2.43167051e-06
Iter: 1870 loss: 2.43872296e-06
Iter: 1871 loss: 2.43157569e-06
Iter: 1872 loss: 2.43042405e-06
Iter: 1873 loss: 2.43042541e-06
Iter: 1874 loss: 2.42955912e-06
Iter: 1875 loss: 2.42843953e-06
Iter: 1876 loss: 2.42946885e-06
Iter: 1877 loss: 2.42775104e-06
Iter: 1878 loss: 2.42686656e-06
Iter: 1879 loss: 2.42678311e-06
Iter: 1880 loss: 2.42613919e-06
Iter: 1881 loss: 2.42580586e-06
Iter: 1882 loss: 2.42547e-06
Iter: 1883 loss: 2.4247e-06
Iter: 1884 loss: 2.43048271e-06
Iter: 1885 loss: 2.42465603e-06
Iter: 1886 loss: 2.42386272e-06
Iter: 1887 loss: 2.42277315e-06
Iter: 1888 loss: 2.42271e-06
Iter: 1889 loss: 2.42163742e-06
Iter: 1890 loss: 2.42359e-06
Iter: 1891 loss: 2.42110241e-06
Iter: 1892 loss: 2.42007e-06
Iter: 1893 loss: 2.42908891e-06
Iter: 1894 loss: 2.41995144e-06
Iter: 1895 loss: 2.41891439e-06
Iter: 1896 loss: 2.42432179e-06
Iter: 1897 loss: 2.41872885e-06
Iter: 1898 loss: 2.41782254e-06
Iter: 1899 loss: 2.4189153e-06
Iter: 1900 loss: 2.41732823e-06
Iter: 1901 loss: 2.41654357e-06
Iter: 1902 loss: 2.41581711e-06
Iter: 1903 loss: 2.41559474e-06
Iter: 1904 loss: 2.41447333e-06
Iter: 1905 loss: 2.43146542e-06
Iter: 1906 loss: 2.41446924e-06
Iter: 1907 loss: 2.41355588e-06
Iter: 1908 loss: 2.41378939e-06
Iter: 1909 loss: 2.41298017e-06
Iter: 1910 loss: 2.41195335e-06
Iter: 1911 loss: 2.41370753e-06
Iter: 1912 loss: 2.41154521e-06
Iter: 1913 loss: 2.4106771e-06
Iter: 1914 loss: 2.41069392e-06
Iter: 1915 loss: 2.41009593e-06
Iter: 1916 loss: 2.40908548e-06
Iter: 1917 loss: 2.40905638e-06
Iter: 1918 loss: 2.408309e-06
Iter: 1919 loss: 2.40828058e-06
Iter: 1920 loss: 2.40767804e-06
Iter: 1921 loss: 2.4061917e-06
Iter: 1922 loss: 2.42387478e-06
Iter: 1923 loss: 2.40604072e-06
Iter: 1924 loss: 2.40463214e-06
Iter: 1925 loss: 2.41176576e-06
Iter: 1926 loss: 2.40433565e-06
Iter: 1927 loss: 2.40314193e-06
Iter: 1928 loss: 2.40981672e-06
Iter: 1929 loss: 2.40291683e-06
Iter: 1930 loss: 2.40183772e-06
Iter: 1931 loss: 2.41546263e-06
Iter: 1932 loss: 2.40186387e-06
Iter: 1933 loss: 2.40133272e-06
Iter: 1934 loss: 2.39988731e-06
Iter: 1935 loss: 2.41476164e-06
Iter: 1936 loss: 2.39976703e-06
Iter: 1937 loss: 2.39832389e-06
Iter: 1938 loss: 2.41277166e-06
Iter: 1939 loss: 2.39832821e-06
Iter: 1940 loss: 2.39734027e-06
Iter: 1941 loss: 2.40080249e-06
Iter: 1942 loss: 2.39703741e-06
Iter: 1943 loss: 2.39593e-06
Iter: 1944 loss: 2.39937776e-06
Iter: 1945 loss: 2.39563178e-06
Iter: 1946 loss: 2.39489145e-06
Iter: 1947 loss: 2.39597057e-06
Iter: 1948 loss: 2.39448332e-06
Iter: 1949 loss: 2.39366773e-06
Iter: 1950 loss: 2.40495865e-06
Iter: 1951 loss: 2.39366409e-06
Iter: 1952 loss: 2.39317637e-06
Iter: 1953 loss: 2.39229166e-06
Iter: 1954 loss: 2.39228166e-06
Iter: 1955 loss: 2.39112569e-06
Iter: 1956 loss: 2.39837186e-06
Iter: 1957 loss: 2.39112774e-06
Iter: 1958 loss: 2.39004112e-06
Iter: 1959 loss: 2.39067094e-06
Iter: 1960 loss: 2.38938924e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi1.6
+ date
Sat Nov  7 23:02:47 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/300_100_100_100_1 --function f1 --psi -2 --phi 1.6 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0f01401e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0f01c8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0f0182730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0f00ded08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0f00f46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0c6385d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0c63dbc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0c62fb7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0c630a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0c630aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0f007d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0f007bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0f0055950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0f0055f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a036aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0f0055730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a02f1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a02f1378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0c62f17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0c62caf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a0243950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a02ba378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a0273268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a0271a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a029f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a0197bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a0136620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a01e7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a01e7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a01d9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a01fc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a011c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a011cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a012a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a00c5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0a00c5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0063529867
test_loss: 0.0069916383
train_loss: 0.0054114014
test_loss: 0.006343697
train_loss: 0.005109752
test_loss: 0.005896222
train_loss: 0.005023239
test_loss: 0.005829964
train_loss: 0.0045315553
test_loss: 0.0056931493
train_loss: 0.0046906658
test_loss: 0.0056070643
train_loss: 0.00453417
test_loss: 0.0057625314
train_loss: 0.0045668543
test_loss: 0.00565905
train_loss: 0.004931928
test_loss: 0.005827256
train_loss: 0.0047287545
test_loss: 0.0057774503
train_loss: 0.0045088287
test_loss: 0.005824819
train_loss: 0.004211121
test_loss: 0.0055803726
train_loss: 0.004496744
test_loss: 0.005684829
train_loss: 0.004064978
test_loss: 0.0055433535
train_loss: 0.004405981
test_loss: 0.0055429786
train_loss: 0.004218847
test_loss: 0.0055368617
train_loss: 0.004797063
test_loss: 0.005876108
train_loss: 0.0044945353
test_loss: 0.0056018815
train_loss: 0.004052431
test_loss: 0.005538506
train_loss: 0.004267839
test_loss: 0.0056023365
train_loss: 0.004431589
test_loss: 0.0055583604
train_loss: 0.004309932
test_loss: 0.0056057335
train_loss: 0.0042888606
test_loss: 0.0055847294
train_loss: 0.0042028893
test_loss: 0.0055472204
train_loss: 0.0045231692
test_loss: 0.0056765634
train_loss: 0.0046643442
test_loss: 0.005972166
train_loss: 0.004038777
test_loss: 0.0054623447
train_loss: 0.0043910313
test_loss: 0.005508632
train_loss: 0.0040511847
test_loss: 0.005491507
train_loss: 0.0041777696
test_loss: 0.0053313416
train_loss: 0.004106728
test_loss: 0.005484484
train_loss: 0.0041259113
test_loss: 0.005379032
train_loss: 0.0047506653
test_loss: 0.0058417832
train_loss: 0.0041310024
test_loss: 0.0053939084
train_loss: 0.0039049543
test_loss: 0.005442749
train_loss: 0.003957606
test_loss: 0.00530087
train_loss: 0.003949509
test_loss: 0.0052970094
train_loss: 0.0039485004
test_loss: 0.005603137
train_loss: 0.004160584
test_loss: 0.005397502
train_loss: 0.0040882183
test_loss: 0.00530082
train_loss: 0.0042407005
test_loss: 0.0054601603
train_loss: 0.004355592
test_loss: 0.005773928
train_loss: 0.004245894
test_loss: 0.005584457
train_loss: 0.004008189
test_loss: 0.0053739105
train_loss: 0.0039834743
test_loss: 0.005248964
train_loss: 0.004044053
test_loss: 0.005352696
train_loss: 0.004078263
test_loss: 0.0054687522
train_loss: 0.003880537
test_loss: 0.0053354464
train_loss: 0.0042247633
test_loss: 0.0053474144
train_loss: 0.0039014905
test_loss: 0.0053493525
train_loss: 0.004360082
test_loss: 0.0057051945
train_loss: 0.003757751
test_loss: 0.00542022
train_loss: 0.003939897
test_loss: 0.0052983686
train_loss: 0.0041315053
test_loss: 0.005379437
train_loss: 0.0040390454
test_loss: 0.005425767
train_loss: 0.0042918087
test_loss: 0.0055230744
train_loss: 0.004204517
test_loss: 0.005500366
train_loss: 0.0038810633
test_loss: 0.0052721156
train_loss: 0.0037501443
test_loss: 0.005196016
train_loss: 0.004225254
test_loss: 0.005477419
train_loss: 0.0039599505
test_loss: 0.005215136
train_loss: 0.0039637266
test_loss: 0.005321121
train_loss: 0.004148554
test_loss: 0.005310727
train_loss: 0.0038844617
test_loss: 0.0052324785
train_loss: 0.0040858206
test_loss: 0.0052437317
train_loss: 0.0038880515
test_loss: 0.0054138834
train_loss: 0.0038767133
test_loss: 0.0051662615
train_loss: 0.0039714826
test_loss: 0.0053119976
train_loss: 0.0040860646
test_loss: 0.0053042304
train_loss: 0.004015316
test_loss: 0.0052156667
train_loss: 0.0036732922
test_loss: 0.0052811303
train_loss: 0.0039741416
test_loss: 0.0052259327
train_loss: 0.0037416024
test_loss: 0.0054974393
train_loss: 0.003894215
test_loss: 0.0055606808
train_loss: 0.0038958006
test_loss: 0.0051922034
train_loss: 0.004004794
test_loss: 0.005391674
train_loss: 0.0039914693
test_loss: 0.0052827317
train_loss: 0.0037689866
test_loss: 0.005332229
train_loss: 0.0035191637
test_loss: 0.005306596
train_loss: 0.0036267757
test_loss: 0.0052963737
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi1.6/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.6 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi1.6/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6bfbcd268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6bfc2a048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6bfc2af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa698307e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6983201e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa698320b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa69825e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa69820b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6982251e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6981cdea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa698225ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa69819ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa69819a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa69819a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa69812ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6980f2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa698121620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6980ce7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa698094840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6980ce9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa698040840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa68075fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa68079b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa680759510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa680759b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa680705620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6806b48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6806c9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6806c9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa68068fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa680632488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6806870d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6806509d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6805fd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6805fd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6805d0b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.28158e-05
Iter: 2 loss: 4.72897555e-05
Iter: 3 loss: 2.83565278e-05
Iter: 4 loss: 2.66955722e-05
Iter: 5 loss: 2.81453285e-05
Iter: 6 loss: 2.57218344e-05
Iter: 7 loss: 2.39228757e-05
Iter: 8 loss: 2.78017942e-05
Iter: 9 loss: 2.32332131e-05
Iter: 10 loss: 2.13844469e-05
Iter: 11 loss: 2.17482047e-05
Iter: 12 loss: 2.0012867e-05
Iter: 13 loss: 1.83110205e-05
Iter: 14 loss: 2.67904361e-05
Iter: 15 loss: 1.80241368e-05
Iter: 16 loss: 1.66652135e-05
Iter: 17 loss: 2.03813561e-05
Iter: 18 loss: 1.62242195e-05
Iter: 19 loss: 1.55788312e-05
Iter: 20 loss: 1.63761069e-05
Iter: 21 loss: 1.52416824e-05
Iter: 22 loss: 1.43867819e-05
Iter: 23 loss: 1.79276885e-05
Iter: 24 loss: 1.42015006e-05
Iter: 25 loss: 1.35409455e-05
Iter: 26 loss: 1.5916512e-05
Iter: 27 loss: 1.33736221e-05
Iter: 28 loss: 1.2821255e-05
Iter: 29 loss: 1.22987649e-05
Iter: 30 loss: 1.21709e-05
Iter: 31 loss: 1.16622432e-05
Iter: 32 loss: 1.16608044e-05
Iter: 33 loss: 1.13310944e-05
Iter: 34 loss: 1.10008959e-05
Iter: 35 loss: 1.09338944e-05
Iter: 36 loss: 1.05545769e-05
Iter: 37 loss: 1.05535373e-05
Iter: 38 loss: 1.02278245e-05
Iter: 39 loss: 1.41049086e-05
Iter: 40 loss: 1.0223559e-05
Iter: 41 loss: 1.00702819e-05
Iter: 42 loss: 1.00836314e-05
Iter: 43 loss: 9.95158916e-06
Iter: 44 loss: 9.72831549e-06
Iter: 45 loss: 1.01671048e-05
Iter: 46 loss: 9.63634375e-06
Iter: 47 loss: 9.40754126e-06
Iter: 48 loss: 9.35929256e-06
Iter: 49 loss: 9.2092414e-06
Iter: 50 loss: 8.97662176e-06
Iter: 51 loss: 1.26480172e-05
Iter: 52 loss: 8.97663e-06
Iter: 53 loss: 8.84997644e-06
Iter: 54 loss: 8.66233313e-06
Iter: 55 loss: 8.65781e-06
Iter: 56 loss: 8.37048174e-06
Iter: 57 loss: 9.83338941e-06
Iter: 58 loss: 8.32358819e-06
Iter: 59 loss: 8.13877796e-06
Iter: 60 loss: 9.80653567e-06
Iter: 61 loss: 8.1301514e-06
Iter: 62 loss: 8.01506485e-06
Iter: 63 loss: 7.9947522e-06
Iter: 64 loss: 7.91647471e-06
Iter: 65 loss: 7.76211527e-06
Iter: 66 loss: 7.83637643e-06
Iter: 67 loss: 7.6581664e-06
Iter: 68 loss: 7.50704839e-06
Iter: 69 loss: 7.50693061e-06
Iter: 70 loss: 7.46155e-06
Iter: 71 loss: 7.46035903e-06
Iter: 72 loss: 7.40662745e-06
Iter: 73 loss: 7.27648239e-06
Iter: 74 loss: 8.65179663e-06
Iter: 75 loss: 7.26192229e-06
Iter: 76 loss: 7.16789418e-06
Iter: 77 loss: 7.1652712e-06
Iter: 78 loss: 7.10489167e-06
Iter: 79 loss: 7.06930177e-06
Iter: 80 loss: 7.0441738e-06
Iter: 81 loss: 6.95108838e-06
Iter: 82 loss: 7.12132305e-06
Iter: 83 loss: 6.91089735e-06
Iter: 84 loss: 6.82676364e-06
Iter: 85 loss: 7.52418873e-06
Iter: 86 loss: 6.82169502e-06
Iter: 87 loss: 6.75838783e-06
Iter: 88 loss: 6.66714914e-06
Iter: 89 loss: 6.66428105e-06
Iter: 90 loss: 6.58393492e-06
Iter: 91 loss: 6.58366025e-06
Iter: 92 loss: 6.51522805e-06
Iter: 93 loss: 6.51564e-06
Iter: 94 loss: 6.46076342e-06
Iter: 95 loss: 6.38276e-06
Iter: 96 loss: 6.82219434e-06
Iter: 97 loss: 6.37182438e-06
Iter: 98 loss: 6.32305273e-06
Iter: 99 loss: 6.25959592e-06
Iter: 100 loss: 6.25543862e-06
Iter: 101 loss: 6.20690753e-06
Iter: 102 loss: 6.19887305e-06
Iter: 103 loss: 6.16389207e-06
Iter: 104 loss: 6.58813633e-06
Iter: 105 loss: 6.16366106e-06
Iter: 106 loss: 6.14069495e-06
Iter: 107 loss: 6.09727658e-06
Iter: 108 loss: 7.03651813e-06
Iter: 109 loss: 6.09696963e-06
Iter: 110 loss: 6.04266734e-06
Iter: 111 loss: 6.47121306e-06
Iter: 112 loss: 6.03887065e-06
Iter: 113 loss: 6.00184194e-06
Iter: 114 loss: 5.98094e-06
Iter: 115 loss: 5.96488e-06
Iter: 116 loss: 5.91479829e-06
Iter: 117 loss: 6.12151598e-06
Iter: 118 loss: 5.90410309e-06
Iter: 119 loss: 5.86606529e-06
Iter: 120 loss: 6.18540435e-06
Iter: 121 loss: 5.86394071e-06
Iter: 122 loss: 5.83786186e-06
Iter: 123 loss: 5.78432537e-06
Iter: 124 loss: 6.72510305e-06
Iter: 125 loss: 5.78322033e-06
Iter: 126 loss: 5.7350162e-06
Iter: 127 loss: 5.73444322e-06
Iter: 128 loss: 5.7010634e-06
Iter: 129 loss: 5.73182251e-06
Iter: 130 loss: 5.681738e-06
Iter: 131 loss: 5.64063612e-06
Iter: 132 loss: 5.69610347e-06
Iter: 133 loss: 5.62023251e-06
Iter: 134 loss: 5.57988687e-06
Iter: 135 loss: 5.59323416e-06
Iter: 136 loss: 5.55127826e-06
Iter: 137 loss: 5.55758243e-06
Iter: 138 loss: 5.53021619e-06
Iter: 139 loss: 5.51423727e-06
Iter: 140 loss: 5.51101402e-06
Iter: 141 loss: 5.50024561e-06
Iter: 142 loss: 5.47787204e-06
Iter: 143 loss: 5.4758857e-06
Iter: 144 loss: 5.45915373e-06
Iter: 145 loss: 5.41863346e-06
Iter: 146 loss: 5.55329825e-06
Iter: 147 loss: 5.40757901e-06
Iter: 148 loss: 5.38243785e-06
Iter: 149 loss: 5.38189624e-06
Iter: 150 loss: 5.36206699e-06
Iter: 151 loss: 5.32820377e-06
Iter: 152 loss: 5.54013423e-06
Iter: 153 loss: 5.32414833e-06
Iter: 154 loss: 5.29795761e-06
Iter: 155 loss: 5.37947562e-06
Iter: 156 loss: 5.29027966e-06
Iter: 157 loss: 5.26447047e-06
Iter: 158 loss: 5.2529731e-06
Iter: 159 loss: 5.23973449e-06
Iter: 160 loss: 5.21428865e-06
Iter: 161 loss: 5.58979264e-06
Iter: 162 loss: 5.21427501e-06
Iter: 163 loss: 5.19049536e-06
Iter: 164 loss: 5.18321622e-06
Iter: 165 loss: 5.16900218e-06
Iter: 166 loss: 5.14589e-06
Iter: 167 loss: 5.24942971e-06
Iter: 168 loss: 5.14132716e-06
Iter: 169 loss: 5.11948292e-06
Iter: 170 loss: 5.16498494e-06
Iter: 171 loss: 5.11077087e-06
Iter: 172 loss: 5.08907397e-06
Iter: 173 loss: 5.08899211e-06
Iter: 174 loss: 5.07753793e-06
Iter: 175 loss: 5.0622557e-06
Iter: 176 loss: 5.06137803e-06
Iter: 177 loss: 5.04394029e-06
Iter: 178 loss: 5.22484061e-06
Iter: 179 loss: 5.04353193e-06
Iter: 180 loss: 5.03071624e-06
Iter: 181 loss: 5.02498551e-06
Iter: 182 loss: 5.0186527e-06
Iter: 183 loss: 4.99859652e-06
Iter: 184 loss: 4.99595262e-06
Iter: 185 loss: 4.98172676e-06
Iter: 186 loss: 4.96350913e-06
Iter: 187 loss: 4.96323173e-06
Iter: 188 loss: 4.95223912e-06
Iter: 189 loss: 4.95440236e-06
Iter: 190 loss: 4.9442242e-06
Iter: 191 loss: 4.92619074e-06
Iter: 192 loss: 4.919807e-06
Iter: 193 loss: 4.90983e-06
Iter: 194 loss: 4.89771173e-06
Iter: 195 loss: 4.89688728e-06
Iter: 196 loss: 4.88538171e-06
Iter: 197 loss: 4.86131921e-06
Iter: 198 loss: 5.27341854e-06
Iter: 199 loss: 4.86081262e-06
Iter: 200 loss: 4.83898657e-06
Iter: 201 loss: 5.03960655e-06
Iter: 202 loss: 4.83797885e-06
Iter: 203 loss: 4.82397581e-06
Iter: 204 loss: 5.02723651e-06
Iter: 205 loss: 4.82383848e-06
Iter: 206 loss: 4.80898689e-06
Iter: 207 loss: 4.80202743e-06
Iter: 208 loss: 4.79446589e-06
Iter: 209 loss: 4.78347647e-06
Iter: 210 loss: 4.84393695e-06
Iter: 211 loss: 4.78176571e-06
Iter: 212 loss: 4.76851937e-06
Iter: 213 loss: 4.76533205e-06
Iter: 214 loss: 4.75698698e-06
Iter: 215 loss: 4.74092e-06
Iter: 216 loss: 4.78767151e-06
Iter: 217 loss: 4.73593354e-06
Iter: 218 loss: 4.72209e-06
Iter: 219 loss: 4.73894943e-06
Iter: 220 loss: 4.71487556e-06
Iter: 221 loss: 4.69996212e-06
Iter: 222 loss: 4.84777593e-06
Iter: 223 loss: 4.69938641e-06
Iter: 224 loss: 4.69035422e-06
Iter: 225 loss: 4.67809241e-06
Iter: 226 loss: 4.67746941e-06
Iter: 227 loss: 4.65611538e-06
Iter: 228 loss: 4.73071259e-06
Iter: 229 loss: 4.65053608e-06
Iter: 230 loss: 4.63811648e-06
Iter: 231 loss: 4.82590531e-06
Iter: 232 loss: 4.6380992e-06
Iter: 233 loss: 4.62922299e-06
Iter: 234 loss: 4.61371837e-06
Iter: 235 loss: 4.61380068e-06
Iter: 236 loss: 4.60119054e-06
Iter: 237 loss: 4.60114461e-06
Iter: 238 loss: 4.59136163e-06
Iter: 239 loss: 4.69015367e-06
Iter: 240 loss: 4.59104103e-06
Iter: 241 loss: 4.58542354e-06
Iter: 242 loss: 4.57474289e-06
Iter: 243 loss: 4.80796643e-06
Iter: 244 loss: 4.57454644e-06
Iter: 245 loss: 4.56602083e-06
Iter: 246 loss: 4.56588623e-06
Iter: 247 loss: 4.55902864e-06
Iter: 248 loss: 4.54636938e-06
Iter: 249 loss: 4.84124803e-06
Iter: 250 loss: 4.54638803e-06
Iter: 251 loss: 4.53414805e-06
Iter: 252 loss: 4.6341429e-06
Iter: 253 loss: 4.53345683e-06
Iter: 254 loss: 4.524818e-06
Iter: 255 loss: 4.53837083e-06
Iter: 256 loss: 4.52087033e-06
Iter: 257 loss: 4.50872449e-06
Iter: 258 loss: 4.53928533e-06
Iter: 259 loss: 4.50443e-06
Iter: 260 loss: 4.4966323e-06
Iter: 261 loss: 4.5062161e-06
Iter: 262 loss: 4.49255094e-06
Iter: 263 loss: 4.4807357e-06
Iter: 264 loss: 4.49732761e-06
Iter: 265 loss: 4.47481125e-06
Iter: 266 loss: 4.46279273e-06
Iter: 267 loss: 4.56991074e-06
Iter: 268 loss: 4.46212107e-06
Iter: 269 loss: 4.45481055e-06
Iter: 270 loss: 4.45151363e-06
Iter: 271 loss: 4.4477e-06
Iter: 272 loss: 4.44452871e-06
Iter: 273 loss: 4.44243278e-06
Iter: 274 loss: 4.43715362e-06
Iter: 275 loss: 4.42322153e-06
Iter: 276 loss: 4.53578696e-06
Iter: 277 loss: 4.42066585e-06
Iter: 278 loss: 4.4090184e-06
Iter: 279 loss: 4.58433533e-06
Iter: 280 loss: 4.4090034e-06
Iter: 281 loss: 4.40127542e-06
Iter: 282 loss: 4.44166e-06
Iter: 283 loss: 4.40010399e-06
Iter: 284 loss: 4.39463474e-06
Iter: 285 loss: 4.3838163e-06
Iter: 286 loss: 4.5955112e-06
Iter: 287 loss: 4.38364259e-06
Iter: 288 loss: 4.37150493e-06
Iter: 289 loss: 4.46646663e-06
Iter: 290 loss: 4.37062818e-06
Iter: 291 loss: 4.36007304e-06
Iter: 292 loss: 4.39172891e-06
Iter: 293 loss: 4.35678066e-06
Iter: 294 loss: 4.34750746e-06
Iter: 295 loss: 4.3778532e-06
Iter: 296 loss: 4.34486401e-06
Iter: 297 loss: 4.33671812e-06
Iter: 298 loss: 4.33761033e-06
Iter: 299 loss: 4.33045e-06
Iter: 300 loss: 4.32194884e-06
Iter: 301 loss: 4.44918442e-06
Iter: 302 loss: 4.32191155e-06
Iter: 303 loss: 4.3163468e-06
Iter: 304 loss: 4.31928947e-06
Iter: 305 loss: 4.31276931e-06
Iter: 306 loss: 4.30479304e-06
Iter: 307 loss: 4.31970693e-06
Iter: 308 loss: 4.30160935e-06
Iter: 309 loss: 4.29289958e-06
Iter: 310 loss: 4.41602788e-06
Iter: 311 loss: 4.29288866e-06
Iter: 312 loss: 4.2895449e-06
Iter: 313 loss: 4.28132216e-06
Iter: 314 loss: 4.37448944e-06
Iter: 315 loss: 4.28042495e-06
Iter: 316 loss: 4.27239229e-06
Iter: 317 loss: 4.27229952e-06
Iter: 318 loss: 4.26663428e-06
Iter: 319 loss: 4.26624638e-06
Iter: 320 loss: 4.2620668e-06
Iter: 321 loss: 4.25416874e-06
Iter: 322 loss: 4.2455622e-06
Iter: 323 loss: 4.2442166e-06
Iter: 324 loss: 4.23898655e-06
Iter: 325 loss: 4.2378324e-06
Iter: 326 loss: 4.23269603e-06
Iter: 327 loss: 4.22840913e-06
Iter: 328 loss: 4.2269603e-06
Iter: 329 loss: 4.21830828e-06
Iter: 330 loss: 4.25473945e-06
Iter: 331 loss: 4.21655568e-06
Iter: 332 loss: 4.20985816e-06
Iter: 333 loss: 4.21339564e-06
Iter: 334 loss: 4.20562947e-06
Iter: 335 loss: 4.19565731e-06
Iter: 336 loss: 4.26326687e-06
Iter: 337 loss: 4.19471326e-06
Iter: 338 loss: 4.18837226e-06
Iter: 339 loss: 4.19633034e-06
Iter: 340 loss: 4.18505851e-06
Iter: 341 loss: 4.18007676e-06
Iter: 342 loss: 4.17985711e-06
Iter: 343 loss: 4.17569618e-06
Iter: 344 loss: 4.17051388e-06
Iter: 345 loss: 4.17013598e-06
Iter: 346 loss: 4.16488911e-06
Iter: 347 loss: 4.17693218e-06
Iter: 348 loss: 4.16288367e-06
Iter: 349 loss: 4.15429349e-06
Iter: 350 loss: 4.16133798e-06
Iter: 351 loss: 4.14921806e-06
Iter: 352 loss: 4.14310034e-06
Iter: 353 loss: 4.16649436e-06
Iter: 354 loss: 4.14161241e-06
Iter: 355 loss: 4.1366493e-06
Iter: 356 loss: 4.13335511e-06
Iter: 357 loss: 4.13143334e-06
Iter: 358 loss: 4.12214195e-06
Iter: 359 loss: 4.19822209e-06
Iter: 360 loss: 4.12164945e-06
Iter: 361 loss: 4.11618203e-06
Iter: 362 loss: 4.11964493e-06
Iter: 363 loss: 4.11262045e-06
Iter: 364 loss: 4.1057674e-06
Iter: 365 loss: 4.1252697e-06
Iter: 366 loss: 4.1035496e-06
Iter: 367 loss: 4.09765471e-06
Iter: 368 loss: 4.12709505e-06
Iter: 369 loss: 4.09668974e-06
Iter: 370 loss: 4.09062113e-06
Iter: 371 loss: 4.10194434e-06
Iter: 372 loss: 4.08793494e-06
Iter: 373 loss: 4.08486358e-06
Iter: 374 loss: 4.08477717e-06
Iter: 375 loss: 4.08151391e-06
Iter: 376 loss: 4.07986863e-06
Iter: 377 loss: 4.07847892e-06
Iter: 378 loss: 4.07370862e-06
Iter: 379 loss: 4.07205744e-06
Iter: 380 loss: 4.06948129e-06
Iter: 381 loss: 4.06569097e-06
Iter: 382 loss: 4.06545041e-06
Iter: 383 loss: 4.06236e-06
Iter: 384 loss: 4.05579067e-06
Iter: 385 loss: 4.15812383e-06
Iter: 386 loss: 4.05557785e-06
Iter: 387 loss: 4.04843058e-06
Iter: 388 loss: 4.07686457e-06
Iter: 389 loss: 4.04691036e-06
Iter: 390 loss: 4.04029743e-06
Iter: 391 loss: 4.07617381e-06
Iter: 392 loss: 4.03951253e-06
Iter: 393 loss: 4.03385047e-06
Iter: 394 loss: 4.05526771e-06
Iter: 395 loss: 4.03254489e-06
Iter: 396 loss: 4.0275072e-06
Iter: 397 loss: 4.02313071e-06
Iter: 398 loss: 4.02169826e-06
Iter: 399 loss: 4.0160312e-06
Iter: 400 loss: 4.10060875e-06
Iter: 401 loss: 4.01602301e-06
Iter: 402 loss: 4.01193165e-06
Iter: 403 loss: 4.0206769e-06
Iter: 404 loss: 4.01046464e-06
Iter: 405 loss: 4.00558292e-06
Iter: 406 loss: 4.01633179e-06
Iter: 407 loss: 4.00367117e-06
Iter: 408 loss: 4.00008867e-06
Iter: 409 loss: 4.00007411e-06
Iter: 410 loss: 3.99784813e-06
Iter: 411 loss: 3.99217424e-06
Iter: 412 loss: 4.04628418e-06
Iter: 413 loss: 3.99152214e-06
Iter: 414 loss: 3.98569227e-06
Iter: 415 loss: 4.04385764e-06
Iter: 416 loss: 3.98552629e-06
Iter: 417 loss: 3.98035172e-06
Iter: 418 loss: 3.99515739e-06
Iter: 419 loss: 3.97888925e-06
Iter: 420 loss: 3.97449821e-06
Iter: 421 loss: 3.96902e-06
Iter: 422 loss: 3.96856e-06
Iter: 423 loss: 3.96392e-06
Iter: 424 loss: 4.01201396e-06
Iter: 425 loss: 3.96383803e-06
Iter: 426 loss: 3.95941242e-06
Iter: 427 loss: 3.96956329e-06
Iter: 428 loss: 3.95761253e-06
Iter: 429 loss: 3.9533e-06
Iter: 430 loss: 3.96536325e-06
Iter: 431 loss: 3.95185907e-06
Iter: 432 loss: 3.94725e-06
Iter: 433 loss: 3.94330664e-06
Iter: 434 loss: 3.94216e-06
Iter: 435 loss: 3.93862592e-06
Iter: 436 loss: 3.93816799e-06
Iter: 437 loss: 3.9355e-06
Iter: 438 loss: 3.93525806e-06
Iter: 439 loss: 3.93338087e-06
Iter: 440 loss: 3.92951824e-06
Iter: 441 loss: 3.96675523e-06
Iter: 442 loss: 3.92930906e-06
Iter: 443 loss: 3.92565e-06
Iter: 444 loss: 3.92542097e-06
Iter: 445 loss: 3.92266247e-06
Iter: 446 loss: 3.91916228e-06
Iter: 447 loss: 3.91936919e-06
Iter: 448 loss: 3.91634603e-06
Iter: 449 loss: 3.91173035e-06
Iter: 450 loss: 3.96632095e-06
Iter: 451 loss: 3.91153844e-06
Iter: 452 loss: 3.90878904e-06
Iter: 453 loss: 3.9069896e-06
Iter: 454 loss: 3.90601144e-06
Iter: 455 loss: 3.90146215e-06
Iter: 456 loss: 3.89958041e-06
Iter: 457 loss: 3.89710931e-06
Iter: 458 loss: 3.89233946e-06
Iter: 459 loss: 3.89227125e-06
Iter: 460 loss: 3.88800527e-06
Iter: 461 loss: 3.88594e-06
Iter: 462 loss: 3.88368335e-06
Iter: 463 loss: 3.87783803e-06
Iter: 464 loss: 3.91129925e-06
Iter: 465 loss: 3.87703949e-06
Iter: 466 loss: 3.87332966e-06
Iter: 467 loss: 3.87851878e-06
Iter: 468 loss: 3.87144382e-06
Iter: 469 loss: 3.86633974e-06
Iter: 470 loss: 3.89359502e-06
Iter: 471 loss: 3.86561942e-06
Iter: 472 loss: 3.86256397e-06
Iter: 473 loss: 3.89179149e-06
Iter: 474 loss: 3.86252714e-06
Iter: 475 loss: 3.85918383e-06
Iter: 476 loss: 3.86013562e-06
Iter: 477 loss: 3.85693875e-06
Iter: 478 loss: 3.85327e-06
Iter: 479 loss: 3.85372641e-06
Iter: 480 loss: 3.85042949e-06
Iter: 481 loss: 3.84753184e-06
Iter: 482 loss: 3.89057277e-06
Iter: 483 loss: 3.84760096e-06
Iter: 484 loss: 3.84479381e-06
Iter: 485 loss: 3.84218492e-06
Iter: 486 loss: 3.84159557e-06
Iter: 487 loss: 3.83732458e-06
Iter: 488 loss: 3.83844645e-06
Iter: 489 loss: 3.83415227e-06
Iter: 490 loss: 3.82870439e-06
Iter: 491 loss: 3.86435204e-06
Iter: 492 loss: 3.8280532e-06
Iter: 493 loss: 3.82463895e-06
Iter: 494 loss: 3.85492785e-06
Iter: 495 loss: 3.82446069e-06
Iter: 496 loss: 3.82174039e-06
Iter: 497 loss: 3.81912287e-06
Iter: 498 loss: 3.81855e-06
Iter: 499 loss: 3.81475274e-06
Iter: 500 loss: 3.84235318e-06
Iter: 501 loss: 3.81441282e-06
Iter: 502 loss: 3.81110476e-06
Iter: 503 loss: 3.81550217e-06
Iter: 504 loss: 3.80947131e-06
Iter: 505 loss: 3.80509118e-06
Iter: 506 loss: 3.83216684e-06
Iter: 507 loss: 3.80448046e-06
Iter: 508 loss: 3.8020562e-06
Iter: 509 loss: 3.84038685e-06
Iter: 510 loss: 3.80207848e-06
Iter: 511 loss: 3.80068718e-06
Iter: 512 loss: 3.79744e-06
Iter: 513 loss: 3.83424958e-06
Iter: 514 loss: 3.79706398e-06
Iter: 515 loss: 3.79269704e-06
Iter: 516 loss: 3.81305426e-06
Iter: 517 loss: 3.79188555e-06
Iter: 518 loss: 3.78796153e-06
Iter: 519 loss: 3.81221321e-06
Iter: 520 loss: 3.78733398e-06
Iter: 521 loss: 3.78473032e-06
Iter: 522 loss: 3.78028335e-06
Iter: 523 loss: 3.78026402e-06
Iter: 524 loss: 3.77628749e-06
Iter: 525 loss: 3.81359769e-06
Iter: 526 loss: 3.77625338e-06
Iter: 527 loss: 3.77292963e-06
Iter: 528 loss: 3.78150912e-06
Iter: 529 loss: 3.7716909e-06
Iter: 530 loss: 3.76829485e-06
Iter: 531 loss: 3.78313507e-06
Iter: 532 loss: 3.76762637e-06
Iter: 533 loss: 3.7652203e-06
Iter: 534 loss: 3.76220055e-06
Iter: 535 loss: 3.76197431e-06
Iter: 536 loss: 3.75626769e-06
Iter: 537 loss: 3.78653544e-06
Iter: 538 loss: 3.75546529e-06
Iter: 539 loss: 3.75251398e-06
Iter: 540 loss: 3.75254035e-06
Iter: 541 loss: 3.75066702e-06
Iter: 542 loss: 3.75875948e-06
Iter: 543 loss: 3.75010541e-06
Iter: 544 loss: 3.74770116e-06
Iter: 545 loss: 3.7438308e-06
Iter: 546 loss: 3.74379943e-06
Iter: 547 loss: 3.74063484e-06
Iter: 548 loss: 3.76116827e-06
Iter: 549 loss: 3.7402715e-06
Iter: 550 loss: 3.7373145e-06
Iter: 551 loss: 3.7525449e-06
Iter: 552 loss: 3.7369407e-06
Iter: 553 loss: 3.73447278e-06
Iter: 554 loss: 3.73362218e-06
Iter: 555 loss: 3.7321463e-06
Iter: 556 loss: 3.72904356e-06
Iter: 557 loss: 3.72761519e-06
Iter: 558 loss: 3.72601244e-06
Iter: 559 loss: 3.72197519e-06
Iter: 560 loss: 3.78247728e-06
Iter: 561 loss: 3.72196564e-06
Iter: 562 loss: 3.71922215e-06
Iter: 563 loss: 3.72376257e-06
Iter: 564 loss: 3.71795318e-06
Iter: 565 loss: 3.71434567e-06
Iter: 566 loss: 3.71304827e-06
Iter: 567 loss: 3.71114061e-06
Iter: 568 loss: 3.70734688e-06
Iter: 569 loss: 3.7389409e-06
Iter: 570 loss: 3.70705584e-06
Iter: 571 loss: 3.70351768e-06
Iter: 572 loss: 3.71243414e-06
Iter: 573 loss: 3.70230919e-06
Iter: 574 loss: 3.69929217e-06
Iter: 575 loss: 3.74247247e-06
Iter: 576 loss: 3.69937356e-06
Iter: 577 loss: 3.6973272e-06
Iter: 578 loss: 3.69929285e-06
Iter: 579 loss: 3.69608074e-06
Iter: 580 loss: 3.69386339e-06
Iter: 581 loss: 3.69047279e-06
Iter: 582 loss: 3.69043732e-06
Iter: 583 loss: 3.688486e-06
Iter: 584 loss: 3.68805036e-06
Iter: 585 loss: 3.6863521e-06
Iter: 586 loss: 3.68413e-06
Iter: 587 loss: 3.68391557e-06
Iter: 588 loss: 3.68049632e-06
Iter: 589 loss: 3.68361657e-06
Iter: 590 loss: 3.67855159e-06
Iter: 591 loss: 3.67550797e-06
Iter: 592 loss: 3.69302779e-06
Iter: 593 loss: 3.67504981e-06
Iter: 594 loss: 3.67188318e-06
Iter: 595 loss: 3.68264023e-06
Iter: 596 loss: 3.67109305e-06
Iter: 597 loss: 3.66811651e-06
Iter: 598 loss: 3.67146413e-06
Iter: 599 loss: 3.66673908e-06
Iter: 600 loss: 3.66330278e-06
Iter: 601 loss: 3.66648737e-06
Iter: 602 loss: 3.66128097e-06
Iter: 603 loss: 3.65852839e-06
Iter: 604 loss: 3.70015755e-06
Iter: 605 loss: 3.65850292e-06
Iter: 606 loss: 3.65612186e-06
Iter: 607 loss: 3.66428435e-06
Iter: 608 loss: 3.65568258e-06
Iter: 609 loss: 3.65278675e-06
Iter: 610 loss: 3.65679443e-06
Iter: 611 loss: 3.65138021e-06
Iter: 612 loss: 3.64880134e-06
Iter: 613 loss: 3.65113124e-06
Iter: 614 loss: 3.64730136e-06
Iter: 615 loss: 3.64475136e-06
Iter: 616 loss: 3.65171286e-06
Iter: 617 loss: 3.64404423e-06
Iter: 618 loss: 3.64080643e-06
Iter: 619 loss: 3.64974244e-06
Iter: 620 loss: 3.63962408e-06
Iter: 621 loss: 3.63685513e-06
Iter: 622 loss: 3.63387358e-06
Iter: 623 loss: 3.6333231e-06
Iter: 624 loss: 3.62979608e-06
Iter: 625 loss: 3.66560698e-06
Iter: 626 loss: 3.62964761e-06
Iter: 627 loss: 3.62708738e-06
Iter: 628 loss: 3.63333061e-06
Iter: 629 loss: 3.62619312e-06
Iter: 630 loss: 3.62237506e-06
Iter: 631 loss: 3.62175842e-06
Iter: 632 loss: 3.61924367e-06
Iter: 633 loss: 3.61570642e-06
Iter: 634 loss: 3.63669983e-06
Iter: 635 loss: 3.61536831e-06
Iter: 636 loss: 3.61239609e-06
Iter: 637 loss: 3.61581328e-06
Iter: 638 loss: 3.61073694e-06
Iter: 639 loss: 3.60821923e-06
Iter: 640 loss: 3.60812101e-06
Iter: 641 loss: 3.60635886e-06
Iter: 642 loss: 3.61297498e-06
Iter: 643 loss: 3.60593231e-06
Iter: 644 loss: 3.60403146e-06
Iter: 645 loss: 3.60123067e-06
Iter: 646 loss: 3.60119975e-06
Iter: 647 loss: 3.59844148e-06
Iter: 648 loss: 3.61860293e-06
Iter: 649 loss: 3.59823048e-06
Iter: 650 loss: 3.59602654e-06
Iter: 651 loss: 3.60651188e-06
Iter: 652 loss: 3.59564365e-06
Iter: 653 loss: 3.59329306e-06
Iter: 654 loss: 3.58951365e-06
Iter: 655 loss: 3.58952343e-06
Iter: 656 loss: 3.58585885e-06
Iter: 657 loss: 3.59606611e-06
Iter: 658 loss: 3.58472334e-06
Iter: 659 loss: 3.58085094e-06
Iter: 660 loss: 3.60548756e-06
Iter: 661 loss: 3.58037346e-06
Iter: 662 loss: 3.57773547e-06
Iter: 663 loss: 3.59730552e-06
Iter: 664 loss: 3.57758699e-06
Iter: 665 loss: 3.57568797e-06
Iter: 666 loss: 3.57206045e-06
Iter: 667 loss: 3.65293818e-06
Iter: 668 loss: 3.57205045e-06
Iter: 669 loss: 3.56902069e-06
Iter: 670 loss: 3.56908504e-06
Iter: 671 loss: 3.56707324e-06
Iter: 672 loss: 3.57702197e-06
Iter: 673 loss: 3.56661781e-06
Iter: 674 loss: 3.56436158e-06
Iter: 675 loss: 3.57202134e-06
Iter: 676 loss: 3.56363171e-06
Iter: 677 loss: 3.5616124e-06
Iter: 678 loss: 3.56582063e-06
Iter: 679 loss: 3.56079022e-06
Iter: 680 loss: 3.55924453e-06
Iter: 681 loss: 3.55772318e-06
Iter: 682 loss: 3.5574908e-06
Iter: 683 loss: 3.55435941e-06
Iter: 684 loss: 3.57465274e-06
Iter: 685 loss: 3.554042e-06
Iter: 686 loss: 3.55149223e-06
Iter: 687 loss: 3.55501516e-06
Iter: 688 loss: 3.55026555e-06
Iter: 689 loss: 3.54793951e-06
Iter: 690 loss: 3.54525946e-06
Iter: 691 loss: 3.54488657e-06
Iter: 692 loss: 3.54187159e-06
Iter: 693 loss: 3.57674139e-06
Iter: 694 loss: 3.54181748e-06
Iter: 695 loss: 3.53905352e-06
Iter: 696 loss: 3.54698136e-06
Iter: 697 loss: 3.53828409e-06
Iter: 698 loss: 3.53557471e-06
Iter: 699 loss: 3.53906603e-06
Iter: 700 loss: 3.53410678e-06
Iter: 701 loss: 3.53119071e-06
Iter: 702 loss: 3.52973984e-06
Iter: 703 loss: 3.52828829e-06
Iter: 704 loss: 3.52562165e-06
Iter: 705 loss: 3.52547568e-06
Iter: 706 loss: 3.52372194e-06
Iter: 707 loss: 3.54026088e-06
Iter: 708 loss: 3.52373399e-06
Iter: 709 loss: 3.52230904e-06
Iter: 710 loss: 3.52152119e-06
Iter: 711 loss: 3.52093957e-06
Iter: 712 loss: 3.51879862e-06
Iter: 713 loss: 3.52047596e-06
Iter: 714 loss: 3.51744643e-06
Iter: 715 loss: 3.51516428e-06
Iter: 716 loss: 3.52515053e-06
Iter: 717 loss: 3.51477092e-06
Iter: 718 loss: 3.51209337e-06
Iter: 719 loss: 3.51781409e-06
Iter: 720 loss: 3.5110595e-06
Iter: 721 loss: 3.50915889e-06
Iter: 722 loss: 3.51019344e-06
Iter: 723 loss: 3.50780942e-06
Iter: 724 loss: 3.50529103e-06
Iter: 725 loss: 3.50640084e-06
Iter: 726 loss: 3.50347864e-06
Iter: 727 loss: 3.50097253e-06
Iter: 728 loss: 3.53512223e-06
Iter: 729 loss: 3.50088499e-06
Iter: 730 loss: 3.49853076e-06
Iter: 731 loss: 3.49856668e-06
Iter: 732 loss: 3.4966838e-06
Iter: 733 loss: 3.4940374e-06
Iter: 734 loss: 3.50311757e-06
Iter: 735 loss: 3.49333754e-06
Iter: 736 loss: 3.4906775e-06
Iter: 737 loss: 3.49064862e-06
Iter: 738 loss: 3.48851586e-06
Iter: 739 loss: 3.48655158e-06
Iter: 740 loss: 3.48609387e-06
Iter: 741 loss: 3.48451772e-06
Iter: 742 loss: 3.48675349e-06
Iter: 743 loss: 3.48378171e-06
Iter: 744 loss: 3.48190338e-06
Iter: 745 loss: 3.4801642e-06
Iter: 746 loss: 3.47976538e-06
Iter: 747 loss: 3.47715718e-06
Iter: 748 loss: 3.49527613e-06
Iter: 749 loss: 3.47691866e-06
Iter: 750 loss: 3.47486207e-06
Iter: 751 loss: 3.48428671e-06
Iter: 752 loss: 3.4745176e-06
Iter: 753 loss: 3.47249488e-06
Iter: 754 loss: 3.47000241e-06
Iter: 755 loss: 3.46966e-06
Iter: 756 loss: 3.46656202e-06
Iter: 757 loss: 3.47194964e-06
Iter: 758 loss: 3.4652337e-06
Iter: 759 loss: 3.46113984e-06
Iter: 760 loss: 3.47338937e-06
Iter: 761 loss: 3.45993453e-06
Iter: 762 loss: 3.45730905e-06
Iter: 763 loss: 3.45733952e-06
Iter: 764 loss: 3.45561716e-06
Iter: 765 loss: 3.45240278e-06
Iter: 766 loss: 3.52028019e-06
Iter: 767 loss: 3.45238391e-06
Iter: 768 loss: 3.44952832e-06
Iter: 769 loss: 3.48754111e-06
Iter: 770 loss: 3.44952173e-06
Iter: 771 loss: 3.44801e-06
Iter: 772 loss: 3.46344905e-06
Iter: 773 loss: 3.44785872e-06
Iter: 774 loss: 3.4461309e-06
Iter: 775 loss: 3.44528507e-06
Iter: 776 loss: 3.44434829e-06
Iter: 777 loss: 3.44186242e-06
Iter: 778 loss: 3.44993032e-06
Iter: 779 loss: 3.44118121e-06
Iter: 780 loss: 3.43912234e-06
Iter: 781 loss: 3.4397865e-06
Iter: 782 loss: 3.43772e-06
Iter: 783 loss: 3.43530837e-06
Iter: 784 loss: 3.45729154e-06
Iter: 785 loss: 3.43519423e-06
Iter: 786 loss: 3.43338024e-06
Iter: 787 loss: 3.43403667e-06
Iter: 788 loss: 3.43210127e-06
Iter: 789 loss: 3.429255e-06
Iter: 790 loss: 3.42669637e-06
Iter: 791 loss: 3.42613816e-06
Iter: 792 loss: 3.42323551e-06
Iter: 793 loss: 3.45602939e-06
Iter: 794 loss: 3.42310977e-06
Iter: 795 loss: 3.42072576e-06
Iter: 796 loss: 3.4274658e-06
Iter: 797 loss: 3.41994087e-06
Iter: 798 loss: 3.4171951e-06
Iter: 799 loss: 3.42426893e-06
Iter: 800 loss: 3.41615441e-06
Iter: 801 loss: 3.41411328e-06
Iter: 802 loss: 3.41468e-06
Iter: 803 loss: 3.41269174e-06
Iter: 804 loss: 3.41030773e-06
Iter: 805 loss: 3.43161878e-06
Iter: 806 loss: 3.41015698e-06
Iter: 807 loss: 3.4079967e-06
Iter: 808 loss: 3.42258159e-06
Iter: 809 loss: 3.40780389e-06
Iter: 810 loss: 3.40591259e-06
Iter: 811 loss: 3.40547967e-06
Iter: 812 loss: 3.40422503e-06
Iter: 813 loss: 3.40235329e-06
Iter: 814 loss: 3.41069904e-06
Iter: 815 loss: 3.40184124e-06
Iter: 816 loss: 3.40030829e-06
Iter: 817 loss: 3.40193969e-06
Iter: 818 loss: 3.39933194e-06
Iter: 819 loss: 3.39670169e-06
Iter: 820 loss: 3.40404154e-06
Iter: 821 loss: 3.3959659e-06
Iter: 822 loss: 3.39387134e-06
Iter: 823 loss: 3.39412736e-06
Iter: 824 loss: 3.39236931e-06
Iter: 825 loss: 3.38909376e-06
Iter: 826 loss: 3.38922564e-06
Iter: 827 loss: 3.38658492e-06
Iter: 828 loss: 3.38382893e-06
Iter: 829 loss: 3.42283715e-06
Iter: 830 loss: 3.38387485e-06
Iter: 831 loss: 3.38126529e-06
Iter: 832 loss: 3.38552e-06
Iter: 833 loss: 3.38019981e-06
Iter: 834 loss: 3.37781512e-06
Iter: 835 loss: 3.38239829e-06
Iter: 836 loss: 3.37670599e-06
Iter: 837 loss: 3.37423216e-06
Iter: 838 loss: 3.37526558e-06
Iter: 839 loss: 3.37247752e-06
Iter: 840 loss: 3.37207894e-06
Iter: 841 loss: 3.37098163e-06
Iter: 842 loss: 3.36999324e-06
Iter: 843 loss: 3.36926405e-06
Iter: 844 loss: 3.36897483e-06
Iter: 845 loss: 3.36720655e-06
Iter: 846 loss: 3.36651829e-06
Iter: 847 loss: 3.36556172e-06
Iter: 848 loss: 3.36297057e-06
Iter: 849 loss: 3.37704796e-06
Iter: 850 loss: 3.36262383e-06
Iter: 851 loss: 3.36059816e-06
Iter: 852 loss: 3.37134406e-06
Iter: 853 loss: 3.36032963e-06
Iter: 854 loss: 3.35830828e-06
Iter: 855 loss: 3.35527147e-06
Iter: 856 loss: 3.35531831e-06
Iter: 857 loss: 3.35239929e-06
Iter: 858 loss: 3.37196775e-06
Iter: 859 loss: 3.35215964e-06
Iter: 860 loss: 3.34969536e-06
Iter: 861 loss: 3.35001846e-06
Iter: 862 loss: 3.34787819e-06
Iter: 863 loss: 3.34604533e-06
Iter: 864 loss: 3.34583456e-06
Iter: 865 loss: 3.34441802e-06
Iter: 866 loss: 3.3417673e-06
Iter: 867 loss: 3.39812163e-06
Iter: 868 loss: 3.3416793e-06
Iter: 869 loss: 3.33847606e-06
Iter: 870 loss: 3.35728737e-06
Iter: 871 loss: 3.33803723e-06
Iter: 872 loss: 3.33719368e-06
Iter: 873 loss: 3.3368708e-06
Iter: 874 loss: 3.33571302e-06
Iter: 875 loss: 3.33457137e-06
Iter: 876 loss: 3.33431285e-06
Iter: 877 loss: 3.33243702e-06
Iter: 878 loss: 3.33684693e-06
Iter: 879 loss: 3.33177923e-06
Iter: 880 loss: 3.32976697e-06
Iter: 881 loss: 3.3315273e-06
Iter: 882 loss: 3.32856644e-06
Iter: 883 loss: 3.32641639e-06
Iter: 884 loss: 3.34772631e-06
Iter: 885 loss: 3.32629497e-06
Iter: 886 loss: 3.32474883e-06
Iter: 887 loss: 3.32758782e-06
Iter: 888 loss: 3.32429386e-06
Iter: 889 loss: 3.32281343e-06
Iter: 890 loss: 3.32015179e-06
Iter: 891 loss: 3.37879396e-06
Iter: 892 loss: 3.32021909e-06
Iter: 893 loss: 3.31715955e-06
Iter: 894 loss: 3.3456115e-06
Iter: 895 loss: 3.31702813e-06
Iter: 896 loss: 3.3148674e-06
Iter: 897 loss: 3.31963815e-06
Iter: 898 loss: 3.31395404e-06
Iter: 899 loss: 3.31101637e-06
Iter: 900 loss: 3.32007721e-06
Iter: 901 loss: 3.31012961e-06
Iter: 902 loss: 3.30778403e-06
Iter: 903 loss: 3.30687567e-06
Iter: 904 loss: 3.3056167e-06
Iter: 905 loss: 3.30360422e-06
Iter: 906 loss: 3.30345961e-06
Iter: 907 loss: 3.30175885e-06
Iter: 908 loss: 3.30975649e-06
Iter: 909 loss: 3.30134435e-06
Iter: 910 loss: 3.29983777e-06
Iter: 911 loss: 3.29794e-06
Iter: 912 loss: 3.29780892e-06
Iter: 913 loss: 3.2956109e-06
Iter: 914 loss: 3.31414844e-06
Iter: 915 loss: 3.29550289e-06
Iter: 916 loss: 3.29418663e-06
Iter: 917 loss: 3.29699014e-06
Iter: 918 loss: 3.29365821e-06
Iter: 919 loss: 3.29180557e-06
Iter: 920 loss: 3.29461227e-06
Iter: 921 loss: 3.29081e-06
Iter: 922 loss: 3.28908322e-06
Iter: 923 loss: 3.29007889e-06
Iter: 924 loss: 3.28789361e-06
Iter: 925 loss: 3.28562169e-06
Iter: 926 loss: 3.2866792e-06
Iter: 927 loss: 3.283968e-06
Iter: 928 loss: 3.28166379e-06
Iter: 929 loss: 3.30087869e-06
Iter: 930 loss: 3.28156898e-06
Iter: 931 loss: 3.27951375e-06
Iter: 932 loss: 3.28486294e-06
Iter: 933 loss: 3.27872704e-06
Iter: 934 loss: 3.27662224e-06
Iter: 935 loss: 3.28223064e-06
Iter: 936 loss: 3.27586963e-06
Iter: 937 loss: 3.27403404e-06
Iter: 938 loss: 3.27192242e-06
Iter: 939 loss: 3.27168254e-06
Iter: 940 loss: 3.27260432e-06
Iter: 941 loss: 3.2703017e-06
Iter: 942 loss: 3.26956615e-06
Iter: 943 loss: 3.2681919e-06
Iter: 944 loss: 3.2681678e-06
Iter: 945 loss: 3.26635904e-06
Iter: 946 loss: 3.26806321e-06
Iter: 947 loss: 3.26528811e-06
Iter: 948 loss: 3.26316604e-06
Iter: 949 loss: 3.27420889e-06
Iter: 950 loss: 3.26288205e-06
Iter: 951 loss: 3.26134295e-06
Iter: 952 loss: 3.26841337e-06
Iter: 953 loss: 3.2609305e-06
Iter: 954 loss: 3.25907195e-06
Iter: 955 loss: 3.25707902e-06
Iter: 956 loss: 3.25676729e-06
Iter: 957 loss: 3.25464953e-06
Iter: 958 loss: 3.2686994e-06
Iter: 959 loss: 3.25440942e-06
Iter: 960 loss: 3.25271071e-06
Iter: 961 loss: 3.25091014e-06
Iter: 962 loss: 3.25048541e-06
Iter: 963 loss: 3.24879488e-06
Iter: 964 loss: 3.248648e-06
Iter: 965 loss: 3.2473522e-06
Iter: 966 loss: 3.24820212e-06
Iter: 967 loss: 3.24652092e-06
Iter: 968 loss: 3.24494545e-06
Iter: 969 loss: 3.24507573e-06
Iter: 970 loss: 3.24352777e-06
Iter: 971 loss: 3.24196571e-06
Iter: 972 loss: 3.25995279e-06
Iter: 973 loss: 3.24183839e-06
Iter: 974 loss: 3.23987069e-06
Iter: 975 loss: 3.24109646e-06
Iter: 976 loss: 3.23842823e-06
Iter: 977 loss: 3.23713311e-06
Iter: 978 loss: 3.24153393e-06
Iter: 979 loss: 3.23686777e-06
Iter: 980 loss: 3.23543554e-06
Iter: 981 loss: 3.23429e-06
Iter: 982 loss: 3.23366544e-06
Iter: 983 loss: 3.23125664e-06
Iter: 984 loss: 3.24931261e-06
Iter: 985 loss: 3.23104678e-06
Iter: 986 loss: 3.22917685e-06
Iter: 987 loss: 3.23598988e-06
Iter: 988 loss: 3.22884034e-06
Iter: 989 loss: 3.22719234e-06
Iter: 990 loss: 3.22394726e-06
Iter: 991 loss: 3.29234899e-06
Iter: 992 loss: 3.22400501e-06
Iter: 993 loss: 3.22134247e-06
Iter: 994 loss: 3.25791e-06
Iter: 995 loss: 3.22140022e-06
Iter: 996 loss: 3.2196167e-06
Iter: 997 loss: 3.22041956e-06
Iter: 998 loss: 3.21842299e-06
Iter: 999 loss: 3.21620655e-06
Iter: 1000 loss: 3.24088842e-06
Iter: 1001 loss: 3.21612674e-06
Iter: 1002 loss: 3.21484458e-06
Iter: 1003 loss: 3.21315724e-06
Iter: 1004 loss: 3.21306061e-06
Iter: 1005 loss: 3.21052426e-06
Iter: 1006 loss: 3.2243006e-06
Iter: 1007 loss: 3.21014841e-06
Iter: 1008 loss: 3.20912432e-06
Iter: 1009 loss: 3.20885283e-06
Iter: 1010 loss: 3.20820163e-06
Iter: 1011 loss: 3.20620256e-06
Iter: 1012 loss: 3.22360825e-06
Iter: 1013 loss: 3.20591653e-06
Iter: 1014 loss: 3.20427944e-06
Iter: 1015 loss: 3.2286589e-06
Iter: 1016 loss: 3.20427353e-06
Iter: 1017 loss: 3.20284175e-06
Iter: 1018 loss: 3.2017424e-06
Iter: 1019 loss: 3.2011892e-06
Iter: 1020 loss: 3.19875744e-06
Iter: 1021 loss: 3.21740299e-06
Iter: 1022 loss: 3.19852052e-06
Iter: 1023 loss: 3.19701326e-06
Iter: 1024 loss: 3.19930518e-06
Iter: 1025 loss: 3.19626088e-06
Iter: 1026 loss: 3.19480137e-06
Iter: 1027 loss: 3.19319042e-06
Iter: 1028 loss: 3.1928796e-06
Iter: 1029 loss: 3.19043261e-06
Iter: 1030 loss: 3.20398067e-06
Iter: 1031 loss: 3.19006813e-06
Iter: 1032 loss: 3.18761954e-06
Iter: 1033 loss: 3.19235392e-06
Iter: 1034 loss: 3.18654111e-06
Iter: 1035 loss: 3.18373259e-06
Iter: 1036 loss: 3.19781066e-06
Iter: 1037 loss: 3.18315961e-06
Iter: 1038 loss: 3.18123e-06
Iter: 1039 loss: 3.18129605e-06
Iter: 1040 loss: 3.17969648e-06
Iter: 1041 loss: 3.17917738e-06
Iter: 1042 loss: 3.1785903e-06
Iter: 1043 loss: 3.17747117e-06
Iter: 1044 loss: 3.17545073e-06
Iter: 1045 loss: 3.17546119e-06
Iter: 1046 loss: 3.17324202e-06
Iter: 1047 loss: 3.17625791e-06
Iter: 1048 loss: 3.17193326e-06
Iter: 1049 loss: 3.17044078e-06
Iter: 1050 loss: 3.17040553e-06
Iter: 1051 loss: 3.16936871e-06
Iter: 1052 loss: 3.16922569e-06
Iter: 1053 loss: 3.16835781e-06
Iter: 1054 loss: 3.16662613e-06
Iter: 1055 loss: 3.17501645e-06
Iter: 1056 loss: 3.16626392e-06
Iter: 1057 loss: 3.16501837e-06
Iter: 1058 loss: 3.16406022e-06
Iter: 1059 loss: 3.16370324e-06
Iter: 1060 loss: 3.1614436e-06
Iter: 1061 loss: 3.16579735e-06
Iter: 1062 loss: 3.16057276e-06
Iter: 1063 loss: 3.15823218e-06
Iter: 1064 loss: 3.16621163e-06
Iter: 1065 loss: 3.15774332e-06
Iter: 1066 loss: 3.15529451e-06
Iter: 1067 loss: 3.16812066e-06
Iter: 1068 loss: 3.15509055e-06
Iter: 1069 loss: 3.15342913e-06
Iter: 1070 loss: 3.15814123e-06
Iter: 1071 loss: 3.15293823e-06
Iter: 1072 loss: 3.15150032e-06
Iter: 1073 loss: 3.15122816e-06
Iter: 1074 loss: 3.15018224e-06
Iter: 1075 loss: 3.14861154e-06
Iter: 1076 loss: 3.14851741e-06
Iter: 1077 loss: 3.14778e-06
Iter: 1078 loss: 3.14568979e-06
Iter: 1079 loss: 3.15808052e-06
Iter: 1080 loss: 3.14515478e-06
Iter: 1081 loss: 3.14289355e-06
Iter: 1082 loss: 3.17254489e-06
Iter: 1083 loss: 3.1428076e-06
Iter: 1084 loss: 3.14163572e-06
Iter: 1085 loss: 3.14863564e-06
Iter: 1086 loss: 3.14142494e-06
Iter: 1087 loss: 3.14025738e-06
Iter: 1088 loss: 3.14013323e-06
Iter: 1089 loss: 3.13930764e-06
Iter: 1090 loss: 3.13687178e-06
Iter: 1091 loss: 3.13761552e-06
Iter: 1092 loss: 3.13523515e-06
Iter: 1093 loss: 3.13332794e-06
Iter: 1094 loss: 3.13608143e-06
Iter: 1095 loss: 3.13254759e-06
Iter: 1096 loss: 3.13007558e-06
Iter: 1097 loss: 3.14137742e-06
Iter: 1098 loss: 3.12978409e-06
Iter: 1099 loss: 3.12831889e-06
Iter: 1100 loss: 3.13838791e-06
Iter: 1101 loss: 3.12806242e-06
Iter: 1102 loss: 3.12656084e-06
Iter: 1103 loss: 3.12648262e-06
Iter: 1104 loss: 3.12547718e-06
Iter: 1105 loss: 3.12332645e-06
Iter: 1106 loss: 3.12838915e-06
Iter: 1107 loss: 3.12263819e-06
Iter: 1108 loss: 3.12164593e-06
Iter: 1109 loss: 3.12155225e-06
Iter: 1110 loss: 3.12040061e-06
Iter: 1111 loss: 3.11840267e-06
Iter: 1112 loss: 3.11840654e-06
Iter: 1113 loss: 3.11708709e-06
Iter: 1114 loss: 3.11974145e-06
Iter: 1115 loss: 3.11656891e-06
Iter: 1116 loss: 3.11450958e-06
Iter: 1117 loss: 3.11877398e-06
Iter: 1118 loss: 3.11364556e-06
Iter: 1119 loss: 3.11188069e-06
Iter: 1120 loss: 3.12664156e-06
Iter: 1121 loss: 3.11177223e-06
Iter: 1122 loss: 3.11072199e-06
Iter: 1123 loss: 3.11391022e-06
Iter: 1124 loss: 3.1103732e-06
Iter: 1125 loss: 3.1092809e-06
Iter: 1126 loss: 3.10692735e-06
Iter: 1127 loss: 3.15695593e-06
Iter: 1128 loss: 3.10693486e-06
Iter: 1129 loss: 3.10528139e-06
Iter: 1130 loss: 3.12200132e-06
Iter: 1131 loss: 3.10529254e-06
Iter: 1132 loss: 3.10365476e-06
Iter: 1133 loss: 3.10409382e-06
Iter: 1134 loss: 3.10240785e-06
Iter: 1135 loss: 3.10011501e-06
Iter: 1136 loss: 3.1150787e-06
Iter: 1137 loss: 3.09989719e-06
Iter: 1138 loss: 3.09811367e-06
Iter: 1139 loss: 3.09878715e-06
Iter: 1140 loss: 3.09682855e-06
Iter: 1141 loss: 3.0952e-06
Iter: 1142 loss: 3.10892528e-06
Iter: 1143 loss: 3.09507e-06
Iter: 1144 loss: 3.09345364e-06
Iter: 1145 loss: 3.10657333e-06
Iter: 1146 loss: 3.09334791e-06
Iter: 1147 loss: 3.09244683e-06
Iter: 1148 loss: 3.09012034e-06
Iter: 1149 loss: 3.11545818e-06
Iter: 1150 loss: 3.08998278e-06
Iter: 1151 loss: 3.08840663e-06
Iter: 1152 loss: 3.08842391e-06
Iter: 1153 loss: 3.08697167e-06
Iter: 1154 loss: 3.08850349e-06
Iter: 1155 loss: 3.08623521e-06
Iter: 1156 loss: 3.08481276e-06
Iter: 1157 loss: 3.09402776e-06
Iter: 1158 loss: 3.08462904e-06
Iter: 1159 loss: 3.08348376e-06
Iter: 1160 loss: 3.08356925e-06
Iter: 1161 loss: 3.08264453e-06
Iter: 1162 loss: 3.08084941e-06
Iter: 1163 loss: 3.08100925e-06
Iter: 1164 loss: 3.07964092e-06
Iter: 1165 loss: 3.0775609e-06
Iter: 1166 loss: 3.08421522e-06
Iter: 1167 loss: 3.07708456e-06
Iter: 1168 loss: 3.07523806e-06
Iter: 1169 loss: 3.09163943e-06
Iter: 1170 loss: 3.07512801e-06
Iter: 1171 loss: 3.07375285e-06
Iter: 1172 loss: 3.0748206e-06
Iter: 1173 loss: 3.07302025e-06
Iter: 1174 loss: 3.07113078e-06
Iter: 1175 loss: 3.0728047e-06
Iter: 1176 loss: 3.0699839e-06
Iter: 1177 loss: 3.06974061e-06
Iter: 1178 loss: 3.06920401e-06
Iter: 1179 loss: 3.0684032e-06
Iter: 1180 loss: 3.06641914e-06
Iter: 1181 loss: 3.08434846e-06
Iter: 1182 loss: 3.06617403e-06
Iter: 1183 loss: 3.06427819e-06
Iter: 1184 loss: 3.07334312e-06
Iter: 1185 loss: 3.06399079e-06
Iter: 1186 loss: 3.06266747e-06
Iter: 1187 loss: 3.07340247e-06
Iter: 1188 loss: 3.06258471e-06
Iter: 1189 loss: 3.06106222e-06
Iter: 1190 loss: 3.05941876e-06
Iter: 1191 loss: 3.05921048e-06
Iter: 1192 loss: 3.0575402e-06
Iter: 1193 loss: 3.05752155e-06
Iter: 1194 loss: 3.05667072e-06
Iter: 1195 loss: 3.05516051e-06
Iter: 1196 loss: 3.05519e-06
Iter: 1197 loss: 3.05294634e-06
Iter: 1198 loss: 3.05706158e-06
Iter: 1199 loss: 3.05198591e-06
Iter: 1200 loss: 3.05033268e-06
Iter: 1201 loss: 3.05874164e-06
Iter: 1202 loss: 3.05004824e-06
Iter: 1203 loss: 3.04803393e-06
Iter: 1204 loss: 3.05294225e-06
Iter: 1205 loss: 3.04726859e-06
Iter: 1206 loss: 3.04600781e-06
Iter: 1207 loss: 3.05004096e-06
Iter: 1208 loss: 3.04559308e-06
Iter: 1209 loss: 3.04397781e-06
Iter: 1210 loss: 3.04554487e-06
Iter: 1211 loss: 3.04310674e-06
Iter: 1212 loss: 3.04165724e-06
Iter: 1213 loss: 3.04161358e-06
Iter: 1214 loss: 3.04098739e-06
Iter: 1215 loss: 3.03910792e-06
Iter: 1216 loss: 3.04913897e-06
Iter: 1217 loss: 3.03864181e-06
Iter: 1218 loss: 3.03673028e-06
Iter: 1219 loss: 3.05798153e-06
Iter: 1220 loss: 3.03663455e-06
Iter: 1221 loss: 3.03544539e-06
Iter: 1222 loss: 3.04833861e-06
Iter: 1223 loss: 3.03538354e-06
Iter: 1224 loss: 3.03437537e-06
Iter: 1225 loss: 3.03339129e-06
Iter: 1226 loss: 3.03312527e-06
Iter: 1227 loss: 3.03146317e-06
Iter: 1228 loss: 3.0445658e-06
Iter: 1229 loss: 3.03136903e-06
Iter: 1230 loss: 3.03012098e-06
Iter: 1231 loss: 3.02823469e-06
Iter: 1232 loss: 3.0281451e-06
Iter: 1233 loss: 3.02651347e-06
Iter: 1234 loss: 3.04849937e-06
Iter: 1235 loss: 3.02646e-06
Iter: 1236 loss: 3.02538342e-06
Iter: 1237 loss: 3.0253741e-06
Iter: 1238 loss: 3.02461785e-06
Iter: 1239 loss: 3.02244143e-06
Iter: 1240 loss: 3.03018487e-06
Iter: 1241 loss: 3.02196622e-06
Iter: 1242 loss: 3.02053127e-06
Iter: 1243 loss: 3.02265971e-06
Iter: 1244 loss: 3.01989621e-06
Iter: 1245 loss: 3.01872387e-06
Iter: 1246 loss: 3.03140177e-06
Iter: 1247 loss: 3.01865521e-06
Iter: 1248 loss: 3.01756245e-06
Iter: 1249 loss: 3.02283297e-06
Iter: 1250 loss: 3.01731461e-06
Iter: 1251 loss: 3.01650698e-06
Iter: 1252 loss: 3.01491582e-06
Iter: 1253 loss: 3.04153536e-06
Iter: 1254 loss: 3.01483351e-06
Iter: 1255 loss: 3.01358432e-06
Iter: 1256 loss: 3.02421859e-06
Iter: 1257 loss: 3.01355e-06
Iter: 1258 loss: 3.01228056e-06
Iter: 1259 loss: 3.01631303e-06
Iter: 1260 loss: 3.01185332e-06
Iter: 1261 loss: 3.01045111e-06
Iter: 1262 loss: 3.01213822e-06
Iter: 1263 loss: 3.00975535e-06
Iter: 1264 loss: 3.00851525e-06
Iter: 1265 loss: 3.01571663e-06
Iter: 1266 loss: 3.00830652e-06
Iter: 1267 loss: 3.00714419e-06
Iter: 1268 loss: 3.00515603e-06
Iter: 1269 loss: 3.00510851e-06
Iter: 1270 loss: 3.00336251e-06
Iter: 1271 loss: 3.01453451e-06
Iter: 1272 loss: 3.00313832e-06
Iter: 1273 loss: 3.00167494e-06
Iter: 1274 loss: 3.00826605e-06
Iter: 1275 loss: 3.00125726e-06
Iter: 1276 loss: 2.99980888e-06
Iter: 1277 loss: 3.00832903e-06
Iter: 1278 loss: 2.99976296e-06
Iter: 1279 loss: 2.99852218e-06
Iter: 1280 loss: 2.99767362e-06
Iter: 1281 loss: 2.99717931e-06
Iter: 1282 loss: 2.99687827e-06
Iter: 1283 loss: 2.99656267e-06
Iter: 1284 loss: 2.99587214e-06
Iter: 1285 loss: 2.99614862e-06
Iter: 1286 loss: 2.99546e-06
Iter: 1287 loss: 2.99464136e-06
Iter: 1288 loss: 2.99300268e-06
Iter: 1289 loss: 3.02199351e-06
Iter: 1290 loss: 2.99289832e-06
Iter: 1291 loss: 2.99167118e-06
Iter: 1292 loss: 3.01057594e-06
Iter: 1293 loss: 2.99164549e-06
Iter: 1294 loss: 2.99033945e-06
Iter: 1295 loss: 2.99417707e-06
Iter: 1296 loss: 2.99007843e-06
Iter: 1297 loss: 2.98877194e-06
Iter: 1298 loss: 2.98929945e-06
Iter: 1299 loss: 2.98790656e-06
Iter: 1300 loss: 2.98652958e-06
Iter: 1301 loss: 2.99479143e-06
Iter: 1302 loss: 2.98632813e-06
Iter: 1303 loss: 2.98515533e-06
Iter: 1304 loss: 2.98436271e-06
Iter: 1305 loss: 2.98390569e-06
Iter: 1306 loss: 2.98251689e-06
Iter: 1307 loss: 2.98334635e-06
Iter: 1308 loss: 2.98164196e-06
Iter: 1309 loss: 2.98023338e-06
Iter: 1310 loss: 3.00299803e-06
Iter: 1311 loss: 2.98024634e-06
Iter: 1312 loss: 2.97927454e-06
Iter: 1313 loss: 2.98272403e-06
Iter: 1314 loss: 2.97893439e-06
Iter: 1315 loss: 2.97792985e-06
Iter: 1316 loss: 2.97823021e-06
Iter: 1317 loss: 2.9771345e-06
Iter: 1318 loss: 2.9764492e-06
Iter: 1319 loss: 2.97640736e-06
Iter: 1320 loss: 2.97573979e-06
Iter: 1321 loss: 2.97426914e-06
Iter: 1322 loss: 2.99663e-06
Iter: 1323 loss: 2.97420956e-06
Iter: 1324 loss: 2.97264933e-06
Iter: 1325 loss: 2.97630027e-06
Iter: 1326 loss: 2.97205634e-06
Iter: 1327 loss: 2.97106499e-06
Iter: 1328 loss: 2.98098394e-06
Iter: 1329 loss: 2.97095039e-06
Iter: 1330 loss: 2.96993949e-06
Iter: 1331 loss: 2.97284328e-06
Iter: 1332 loss: 2.96951589e-06
Iter: 1333 loss: 2.96860617e-06
Iter: 1334 loss: 2.96830603e-06
Iter: 1335 loss: 2.96770122e-06
Iter: 1336 loss: 2.96605867e-06
Iter: 1337 loss: 2.97173483e-06
Iter: 1338 loss: 2.96562234e-06
Iter: 1339 loss: 2.96442158e-06
Iter: 1340 loss: 2.96916232e-06
Iter: 1341 loss: 2.96422331e-06
Iter: 1342 loss: 2.96320923e-06
Iter: 1343 loss: 2.96098233e-06
Iter: 1344 loss: 2.98381792e-06
Iter: 1345 loss: 2.96074495e-06
Iter: 1346 loss: 2.95995096e-06
Iter: 1347 loss: 2.95934251e-06
Iter: 1348 loss: 2.95813834e-06
Iter: 1349 loss: 2.958908e-06
Iter: 1350 loss: 2.95740392e-06
Iter: 1351 loss: 2.95623204e-06
Iter: 1352 loss: 2.96651888e-06
Iter: 1353 loss: 2.95613199e-06
Iter: 1354 loss: 2.95535165e-06
Iter: 1355 loss: 2.96420558e-06
Iter: 1356 loss: 2.95528662e-06
Iter: 1357 loss: 2.95486689e-06
Iter: 1358 loss: 2.95336235e-06
Iter: 1359 loss: 2.96215603e-06
Iter: 1360 loss: 2.95302561e-06
Iter: 1361 loss: 2.95135851e-06
Iter: 1362 loss: 2.96244843e-06
Iter: 1363 loss: 2.95118889e-06
Iter: 1364 loss: 2.95006885e-06
Iter: 1365 loss: 2.96126063e-06
Iter: 1366 loss: 2.94999654e-06
Iter: 1367 loss: 2.94889878e-06
Iter: 1368 loss: 2.95027166e-06
Iter: 1369 loss: 2.9482394e-06
Iter: 1370 loss: 2.94739425e-06
Iter: 1371 loss: 2.94843971e-06
Iter: 1372 loss: 2.946922e-06
Iter: 1373 loss: 2.94552729e-06
Iter: 1374 loss: 2.94611596e-06
Iter: 1375 loss: 2.94460801e-06
Iter: 1376 loss: 2.94300071e-06
Iter: 1377 loss: 2.94756228e-06
Iter: 1378 loss: 2.94241318e-06
Iter: 1379 loss: 2.94078973e-06
Iter: 1380 loss: 2.9396856e-06
Iter: 1381 loss: 2.9390776e-06
Iter: 1382 loss: 2.93814651e-06
Iter: 1383 loss: 2.93789685e-06
Iter: 1384 loss: 2.93683502e-06
Iter: 1385 loss: 2.93644098e-06
Iter: 1386 loss: 2.93584299e-06
Iter: 1387 loss: 2.93509083e-06
Iter: 1388 loss: 2.93500034e-06
Iter: 1389 loss: 2.93433e-06
Iter: 1390 loss: 2.9348671e-06
Iter: 1391 loss: 2.93393032e-06
Iter: 1392 loss: 2.93333028e-06
Iter: 1393 loss: 2.93171979e-06
Iter: 1394 loss: 2.95281279e-06
Iter: 1395 loss: 2.9317257e-06
Iter: 1396 loss: 2.93035328e-06
Iter: 1397 loss: 2.94624351e-06
Iter: 1398 loss: 2.9303269e-06
Iter: 1399 loss: 2.92933123e-06
Iter: 1400 loss: 2.93642142e-06
Iter: 1401 loss: 2.92927962e-06
Iter: 1402 loss: 2.92817026e-06
Iter: 1403 loss: 2.92770846e-06
Iter: 1404 loss: 2.92726122e-06
Iter: 1405 loss: 2.9257053e-06
Iter: 1406 loss: 2.92581558e-06
Iter: 1407 loss: 2.92452523e-06
Iter: 1408 loss: 2.92306368e-06
Iter: 1409 loss: 2.9230473e-06
Iter: 1410 loss: 2.92231903e-06
Iter: 1411 loss: 2.92095046e-06
Iter: 1412 loss: 2.92094455e-06
Iter: 1413 loss: 2.91905485e-06
Iter: 1414 loss: 2.92826326e-06
Iter: 1415 loss: 2.91878337e-06
Iter: 1416 loss: 2.91747665e-06
Iter: 1417 loss: 2.9207722e-06
Iter: 1418 loss: 2.91699371e-06
Iter: 1419 loss: 2.91530978e-06
Iter: 1420 loss: 2.92378536e-06
Iter: 1421 loss: 2.91503738e-06
Iter: 1422 loss: 2.91411652e-06
Iter: 1423 loss: 2.92642653e-06
Iter: 1424 loss: 2.91410697e-06
Iter: 1425 loss: 2.91323477e-06
Iter: 1426 loss: 2.9122707e-06
Iter: 1427 loss: 2.91214883e-06
Iter: 1428 loss: 2.91076367e-06
Iter: 1429 loss: 2.91116157e-06
Iter: 1430 loss: 2.90992898e-06
Iter: 1431 loss: 2.90840671e-06
Iter: 1432 loss: 2.91192805e-06
Iter: 1433 loss: 2.90786511e-06
Iter: 1434 loss: 2.90657817e-06
Iter: 1435 loss: 2.90664866e-06
Iter: 1436 loss: 2.90584853e-06
Iter: 1437 loss: 2.90571279e-06
Iter: 1438 loss: 2.90516687e-06
Iter: 1439 loss: 2.90381536e-06
Iter: 1440 loss: 2.90329922e-06
Iter: 1441 loss: 2.90266257e-06
Iter: 1442 loss: 2.90150729e-06
Iter: 1443 loss: 2.90149228e-06
Iter: 1444 loss: 2.90062371e-06
Iter: 1445 loss: 2.89902982e-06
Iter: 1446 loss: 2.92644495e-06
Iter: 1447 loss: 2.89894933e-06
Iter: 1448 loss: 2.89714944e-06
Iter: 1449 loss: 2.91541051e-06
Iter: 1450 loss: 2.89712852e-06
Iter: 1451 loss: 2.895948e-06
Iter: 1452 loss: 2.89606487e-06
Iter: 1453 loss: 2.89500304e-06
Iter: 1454 loss: 2.89318473e-06
Iter: 1455 loss: 2.91288643e-06
Iter: 1456 loss: 2.89317063e-06
Iter: 1457 loss: 2.89239597e-06
Iter: 1458 loss: 2.90256048e-06
Iter: 1459 loss: 2.89234504e-06
Iter: 1460 loss: 2.89159584e-06
Iter: 1461 loss: 2.88987758e-06
Iter: 1462 loss: 2.91056404e-06
Iter: 1463 loss: 2.88971523e-06
Iter: 1464 loss: 2.88794627e-06
Iter: 1465 loss: 2.89277409e-06
Iter: 1466 loss: 2.88722913e-06
Iter: 1467 loss: 2.88565798e-06
Iter: 1468 loss: 2.89030231e-06
Iter: 1469 loss: 2.88509341e-06
Iter: 1470 loss: 2.88429055e-06
Iter: 1471 loss: 2.88418551e-06
Iter: 1472 loss: 2.88344563e-06
Iter: 1473 loss: 2.88220826e-06
Iter: 1474 loss: 2.88220963e-06
Iter: 1475 loss: 2.88098954e-06
Iter: 1476 loss: 2.8893437e-06
Iter: 1477 loss: 2.88101819e-06
Iter: 1478 loss: 2.88015713e-06
Iter: 1479 loss: 2.88139154e-06
Iter: 1480 loss: 2.87983721e-06
Iter: 1481 loss: 2.87855823e-06
Iter: 1482 loss: 2.87810508e-06
Iter: 1483 loss: 2.8773693e-06
Iter: 1484 loss: 2.87592593e-06
Iter: 1485 loss: 2.88094543e-06
Iter: 1486 loss: 2.87546391e-06
Iter: 1487 loss: 2.87415969e-06
Iter: 1488 loss: 2.87907051e-06
Iter: 1489 loss: 2.87378634e-06
Iter: 1490 loss: 2.87263583e-06
Iter: 1491 loss: 2.88286401e-06
Iter: 1492 loss: 2.87256557e-06
Iter: 1493 loss: 2.87142052e-06
Iter: 1494 loss: 2.87431976e-06
Iter: 1495 loss: 2.87103558e-06
Iter: 1496 loss: 2.86974068e-06
Iter: 1497 loss: 2.87270404e-06
Iter: 1498 loss: 2.86914201e-06
Iter: 1499 loss: 2.8683221e-06
Iter: 1500 loss: 2.86692284e-06
Iter: 1501 loss: 2.89995705e-06
Iter: 1502 loss: 2.86696013e-06
Iter: 1503 loss: 2.86493696e-06
Iter: 1504 loss: 2.87330613e-06
Iter: 1505 loss: 2.86452382e-06
Iter: 1506 loss: 2.86362842e-06
Iter: 1507 loss: 2.86363024e-06
Iter: 1508 loss: 2.86261775e-06
Iter: 1509 loss: 2.86114869e-06
Iter: 1510 loss: 2.8611978e-06
Iter: 1511 loss: 2.85975329e-06
Iter: 1512 loss: 2.86556906e-06
Iter: 1513 loss: 2.85946976e-06
Iter: 1514 loss: 2.8580821e-06
Iter: 1515 loss: 2.86172053e-06
Iter: 1516 loss: 2.85760257e-06
Iter: 1517 loss: 2.85635201e-06
Iter: 1518 loss: 2.86378372e-06
Iter: 1519 loss: 2.85625401e-06
Iter: 1520 loss: 2.85522447e-06
Iter: 1521 loss: 2.85381361e-06
Iter: 1522 loss: 2.85375268e-06
Iter: 1523 loss: 2.85226042e-06
Iter: 1524 loss: 2.86802469e-06
Iter: 1525 loss: 2.8521913e-06
Iter: 1526 loss: 2.85086685e-06
Iter: 1527 loss: 2.8552e-06
Iter: 1528 loss: 2.85059468e-06
Iter: 1529 loss: 2.84926318e-06
Iter: 1530 loss: 2.85962051e-06
Iter: 1531 loss: 2.84913904e-06
Iter: 1532 loss: 2.84820339e-06
Iter: 1533 loss: 2.85178567e-06
Iter: 1534 loss: 2.847936e-06
Iter: 1535 loss: 2.84718544e-06
Iter: 1536 loss: 2.84540261e-06
Iter: 1537 loss: 2.86975455e-06
Iter: 1538 loss: 2.84529824e-06
Iter: 1539 loss: 2.84359658e-06
Iter: 1540 loss: 2.85231499e-06
Iter: 1541 loss: 2.84327916e-06
Iter: 1542 loss: 2.84164071e-06
Iter: 1543 loss: 2.84552198e-06
Iter: 1544 loss: 2.84108296e-06
Iter: 1545 loss: 2.84031762e-06
Iter: 1546 loss: 2.84025373e-06
Iter: 1547 loss: 2.83947838e-06
Iter: 1548 loss: 2.83866962e-06
Iter: 1549 loss: 2.83853274e-06
Iter: 1550 loss: 2.83742656e-06
Iter: 1551 loss: 2.84000021e-06
Iter: 1552 loss: 2.83707914e-06
Iter: 1553 loss: 2.83572581e-06
Iter: 1554 loss: 2.83933514e-06
Iter: 1555 loss: 2.83538611e-06
Iter: 1556 loss: 2.83405e-06
Iter: 1557 loss: 2.83737063e-06
Iter: 1558 loss: 2.83358258e-06
Iter: 1559 loss: 2.83244935e-06
Iter: 1560 loss: 2.83202439e-06
Iter: 1561 loss: 2.83144209e-06
Iter: 1562 loss: 2.82952942e-06
Iter: 1563 loss: 2.83834243e-06
Iter: 1564 loss: 2.8292734e-06
Iter: 1565 loss: 2.82856968e-06
Iter: 1566 loss: 2.82854762e-06
Iter: 1567 loss: 2.82782e-06
Iter: 1568 loss: 2.82748988e-06
Iter: 1569 loss: 2.82714404e-06
Iter: 1570 loss: 2.82612655e-06
Iter: 1571 loss: 2.8308848e-06
Iter: 1572 loss: 2.8259276e-06
Iter: 1573 loss: 2.82522387e-06
Iter: 1574 loss: 2.8242921e-06
Iter: 1575 loss: 2.82425412e-06
Iter: 1576 loss: 2.82279552e-06
Iter: 1577 loss: 2.82623864e-06
Iter: 1578 loss: 2.8222637e-06
Iter: 1579 loss: 2.82086194e-06
Iter: 1580 loss: 2.82478e-06
Iter: 1581 loss: 2.82042856e-06
Iter: 1582 loss: 2.81915754e-06
Iter: 1583 loss: 2.83888335e-06
Iter: 1584 loss: 2.81918346e-06
Iter: 1585 loss: 2.81858593e-06
Iter: 1586 loss: 2.81807388e-06
Iter: 1587 loss: 2.81789971e-06
Iter: 1588 loss: 2.81667872e-06
Iter: 1589 loss: 2.81840357e-06
Iter: 1590 loss: 2.81606776e-06
Iter: 1591 loss: 2.81497523e-06
Iter: 1592 loss: 2.82525934e-06
Iter: 1593 loss: 2.81488292e-06
Iter: 1594 loss: 2.81410826e-06
Iter: 1595 loss: 2.81288021e-06
Iter: 1596 loss: 2.81290409e-06
Iter: 1597 loss: 2.8111117e-06
Iter: 1598 loss: 2.81663733e-06
Iter: 1599 loss: 2.81058101e-06
Iter: 1600 loss: 2.8089712e-06
Iter: 1601 loss: 2.82338624e-06
Iter: 1602 loss: 2.80883978e-06
Iter: 1603 loss: 2.80753466e-06
Iter: 1604 loss: 2.82085239e-06
Iter: 1605 loss: 2.80753216e-06
Iter: 1606 loss: 2.80685936e-06
Iter: 1607 loss: 2.80654285e-06
Iter: 1608 loss: 2.80629979e-06
Iter: 1609 loss: 2.80516952e-06
Iter: 1610 loss: 2.80522499e-06
Iter: 1611 loss: 2.80414633e-06
Iter: 1612 loss: 2.80307313e-06
Iter: 1613 loss: 2.80575841e-06
Iter: 1614 loss: 2.80263066e-06
Iter: 1615 loss: 2.80122367e-06
Iter: 1616 loss: 2.80125369e-06
Iter: 1617 loss: 2.80012205e-06
Iter: 1618 loss: 2.79961796e-06
Iter: 1619 loss: 2.79923279e-06
Iter: 1620 loss: 2.79854839e-06
Iter: 1621 loss: 2.79751362e-06
Iter: 1622 loss: 2.79742858e-06
Iter: 1623 loss: 2.79611413e-06
Iter: 1624 loss: 2.79950564e-06
Iter: 1625 loss: 2.79571441e-06
Iter: 1626 loss: 2.79459482e-06
Iter: 1627 loss: 2.80150834e-06
Iter: 1628 loss: 2.79448113e-06
Iter: 1629 loss: 2.79332198e-06
Iter: 1630 loss: 2.79253982e-06
Iter: 1631 loss: 2.79216306e-06
Iter: 1632 loss: 2.79070423e-06
Iter: 1633 loss: 2.7982212e-06
Iter: 1634 loss: 2.79052961e-06
Iter: 1635 loss: 2.78930793e-06
Iter: 1636 loss: 2.79099186e-06
Iter: 1637 loss: 2.7886349e-06
Iter: 1638 loss: 2.78769971e-06
Iter: 1639 loss: 2.7953788e-06
Iter: 1640 loss: 2.78766902e-06
Iter: 1641 loss: 2.78647826e-06
Iter: 1642 loss: 2.79035248e-06
Iter: 1643 loss: 2.7860774e-06
Iter: 1644 loss: 2.78516313e-06
Iter: 1645 loss: 2.78766447e-06
Iter: 1646 loss: 2.78483026e-06
Iter: 1647 loss: 2.784137e-06
Iter: 1648 loss: 2.78425705e-06
Iter: 1649 loss: 2.78358402e-06
Iter: 1650 loss: 2.78237985e-06
Iter: 1651 loss: 2.78175185e-06
Iter: 1652 loss: 2.78122889e-06
Iter: 1653 loss: 2.77952108e-06
Iter: 1654 loss: 2.7810438e-06
Iter: 1655 loss: 2.77856634e-06
Iter: 1656 loss: 2.77764047e-06
Iter: 1657 loss: 2.77740946e-06
Iter: 1658 loss: 2.77646768e-06
Iter: 1659 loss: 2.77821573e-06
Iter: 1660 loss: 2.77609433e-06
Iter: 1661 loss: 2.77497838e-06
Iter: 1662 loss: 2.77524e-06
Iter: 1663 loss: 2.77415484e-06
Iter: 1664 loss: 2.7730066e-06
Iter: 1665 loss: 2.77880326e-06
Iter: 1666 loss: 2.77276058e-06
Iter: 1667 loss: 2.77143477e-06
Iter: 1668 loss: 2.77113259e-06
Iter: 1669 loss: 2.77032041e-06
Iter: 1670 loss: 2.76896753e-06
Iter: 1671 loss: 2.77664503e-06
Iter: 1672 loss: 2.76889841e-06
Iter: 1673 loss: 2.76766332e-06
Iter: 1674 loss: 2.76801347e-06
Iter: 1675 loss: 2.7667852e-06
Iter: 1676 loss: 2.76622222e-06
Iter: 1677 loss: 2.76585024e-06
Iter: 1678 loss: 2.76517881e-06
Iter: 1679 loss: 2.76409969e-06
Iter: 1680 loss: 2.79305596e-06
Iter: 1681 loss: 2.76408946e-06
Iter: 1682 loss: 2.76252331e-06
Iter: 1683 loss: 2.7676731e-06
Iter: 1684 loss: 2.76207265e-06
Iter: 1685 loss: 2.76090759e-06
Iter: 1686 loss: 2.7630058e-06
Iter: 1687 loss: 2.76040259e-06
Iter: 1688 loss: 2.75932462e-06
Iter: 1689 loss: 2.75896127e-06
Iter: 1690 loss: 2.75826915e-06
Iter: 1691 loss: 2.75696902e-06
Iter: 1692 loss: 2.76500259e-06
Iter: 1693 loss: 2.7567603e-06
Iter: 1694 loss: 2.7556257e-06
Iter: 1695 loss: 2.76744345e-06
Iter: 1696 loss: 2.75564366e-06
Iter: 1697 loss: 2.75461616e-06
Iter: 1698 loss: 2.75589446e-06
Iter: 1699 loss: 2.75410252e-06
Iter: 1700 loss: 2.75311754e-06
Iter: 1701 loss: 2.75244065e-06
Iter: 1702 loss: 2.75213642e-06
Iter: 1703 loss: 2.75034154e-06
Iter: 1704 loss: 2.76232822e-06
Iter: 1705 loss: 2.75015509e-06
Iter: 1706 loss: 2.74917466e-06
Iter: 1707 loss: 2.7483793e-06
Iter: 1708 loss: 2.7480919e-06
Iter: 1709 loss: 2.74583181e-06
Iter: 1710 loss: 2.74952617e-06
Iter: 1711 loss: 2.74479726e-06
Iter: 1712 loss: 2.74468948e-06
Iter: 1713 loss: 2.7441356e-06
Iter: 1714 loss: 2.74355466e-06
Iter: 1715 loss: 2.74290846e-06
Iter: 1716 loss: 2.74266176e-06
Iter: 1717 loss: 2.74169e-06
Iter: 1718 loss: 2.74381023e-06
Iter: 1719 loss: 2.74125819e-06
Iter: 1720 loss: 2.74010699e-06
Iter: 1721 loss: 2.74002741e-06
Iter: 1722 loss: 2.7390231e-06
Iter: 1723 loss: 2.73752039e-06
Iter: 1724 loss: 2.74323929e-06
Iter: 1725 loss: 2.73720161e-06
Iter: 1726 loss: 2.73611886e-06
Iter: 1727 loss: 2.73458318e-06
Iter: 1728 loss: 2.73445175e-06
Iter: 1729 loss: 2.73384171e-06
Iter: 1730 loss: 2.73338833e-06
Iter: 1731 loss: 2.73238e-06
Iter: 1732 loss: 2.73394676e-06
Iter: 1733 loss: 2.73195519e-06
Iter: 1734 loss: 2.73095065e-06
Iter: 1735 loss: 2.73044134e-06
Iter: 1736 loss: 2.72990928e-06
Iter: 1737 loss: 2.72861575e-06
Iter: 1738 loss: 2.7409651e-06
Iter: 1739 loss: 2.72858983e-06
Iter: 1740 loss: 2.72753437e-06
Iter: 1741 loss: 2.72712714e-06
Iter: 1742 loss: 2.72657212e-06
Iter: 1743 loss: 2.72515172e-06
Iter: 1744 loss: 2.73109777e-06
Iter: 1745 loss: 2.72492207e-06
Iter: 1746 loss: 2.72370676e-06
Iter: 1747 loss: 2.72846682e-06
Iter: 1748 loss: 2.72343186e-06
Iter: 1749 loss: 2.72213e-06
Iter: 1750 loss: 2.73215528e-06
Iter: 1751 loss: 2.72199668e-06
Iter: 1752 loss: 2.72132547e-06
Iter: 1753 loss: 2.72093621e-06
Iter: 1754 loss: 2.72070065e-06
Iter: 1755 loss: 2.71929525e-06
Iter: 1756 loss: 2.71911e-06
Iter: 1757 loss: 2.71815748e-06
Iter: 1758 loss: 2.71633098e-06
Iter: 1759 loss: 2.72513648e-06
Iter: 1760 loss: 2.71604767e-06
Iter: 1761 loss: 2.71499812e-06
Iter: 1762 loss: 2.71343197e-06
Iter: 1763 loss: 2.71336148e-06
Iter: 1764 loss: 2.71131694e-06
Iter: 1765 loss: 2.72677721e-06
Iter: 1766 loss: 2.71118734e-06
Iter: 1767 loss: 2.70955866e-06
Iter: 1768 loss: 2.71497402e-06
Iter: 1769 loss: 2.70908731e-06
Iter: 1770 loss: 2.7079443e-06
Iter: 1771 loss: 2.72469697e-06
Iter: 1772 loss: 2.70793271e-06
Iter: 1773 loss: 2.70674263e-06
Iter: 1774 loss: 2.70678379e-06
Iter: 1775 loss: 2.70583951e-06
Iter: 1776 loss: 2.70482656e-06
Iter: 1777 loss: 2.7141291e-06
Iter: 1778 loss: 2.70474175e-06
Iter: 1779 loss: 2.70394435e-06
Iter: 1780 loss: 2.70281089e-06
Iter: 1781 loss: 2.70269857e-06
Iter: 1782 loss: 2.70114128e-06
Iter: 1783 loss: 2.71237354e-06
Iter: 1784 loss: 2.70103055e-06
Iter: 1785 loss: 2.70026294e-06
Iter: 1786 loss: 2.7002261e-06
Iter: 1787 loss: 2.69954398e-06
Iter: 1788 loss: 2.69798511e-06
Iter: 1789 loss: 2.7229562e-06
Iter: 1790 loss: 2.69796374e-06
Iter: 1791 loss: 2.69692282e-06
Iter: 1792 loss: 2.70445321e-06
Iter: 1793 loss: 2.69680231e-06
Iter: 1794 loss: 2.69586599e-06
Iter: 1795 loss: 2.69721272e-06
Iter: 1796 loss: 2.6953378e-06
Iter: 1797 loss: 2.69409657e-06
Iter: 1798 loss: 2.69462771e-06
Iter: 1799 loss: 2.69314069e-06
Iter: 1800 loss: 2.69166026e-06
Iter: 1801 loss: 2.69176826e-06
Iter: 1802 loss: 2.69040106e-06
Iter: 1803 loss: 2.68858389e-06
Iter: 1804 loss: 2.70316423e-06
Iter: 1805 loss: 2.68835515e-06
Iter: 1806 loss: 2.68696499e-06
Iter: 1807 loss: 2.68878784e-06
Iter: 1808 loss: 2.68610665e-06
Iter: 1809 loss: 2.68464646e-06
Iter: 1810 loss: 2.68463555e-06
Iter: 1811 loss: 2.68382473e-06
Iter: 1812 loss: 2.68356189e-06
Iter: 1813 loss: 2.6830437e-06
Iter: 1814 loss: 2.68167719e-06
Iter: 1815 loss: 2.68461508e-06
Iter: 1816 loss: 2.68106487e-06
Iter: 1817 loss: 2.68003305e-06
Iter: 1818 loss: 2.68349618e-06
Iter: 1819 loss: 2.67968858e-06
Iter: 1820 loss: 2.67869473e-06
Iter: 1821 loss: 2.68794611e-06
Iter: 1822 loss: 2.67873747e-06
Iter: 1823 loss: 2.67768633e-06
Iter: 1824 loss: 2.67679843e-06
Iter: 1825 loss: 2.67658447e-06
Iter: 1826 loss: 2.67545238e-06
Iter: 1827 loss: 2.67636233e-06
Iter: 1828 loss: 2.67489804e-06
Iter: 1829 loss: 2.67368546e-06
Iter: 1830 loss: 2.68029726e-06
Iter: 1831 loss: 2.67348514e-06
Iter: 1832 loss: 2.67228688e-06
Iter: 1833 loss: 2.67433893e-06
Iter: 1834 loss: 2.67172186e-06
Iter: 1835 loss: 2.67042606e-06
Iter: 1836 loss: 2.67006e-06
Iter: 1837 loss: 2.66929851e-06
Iter: 1838 loss: 2.66763186e-06
Iter: 1839 loss: 2.67075939e-06
Iter: 1840 loss: 2.66705229e-06
Iter: 1841 loss: 2.66494544e-06
Iter: 1842 loss: 2.67388782e-06
Iter: 1843 loss: 2.66460802e-06
Iter: 1844 loss: 2.66327061e-06
Iter: 1845 loss: 2.66331381e-06
Iter: 1846 loss: 2.66229836e-06
Iter: 1847 loss: 2.66067696e-06
Iter: 1848 loss: 2.66061556e-06
Iter: 1849 loss: 2.65920949e-06
Iter: 1850 loss: 2.67439464e-06
Iter: 1851 loss: 2.65910035e-06
Iter: 1852 loss: 2.65789595e-06
Iter: 1853 loss: 2.65735707e-06
Iter: 1854 loss: 2.65673134e-06
Iter: 1855 loss: 2.65567905e-06
Iter: 1856 loss: 2.65554331e-06
Iter: 1857 loss: 2.65452718e-06
Iter: 1858 loss: 2.65564472e-06
Iter: 1859 loss: 2.65402014e-06
Iter: 1860 loss: 2.6531211e-06
Iter: 1861 loss: 2.65193239e-06
Iter: 1862 loss: 2.65180552e-06
Iter: 1863 loss: 2.6504963e-06
Iter: 1864 loss: 2.66359098e-06
Iter: 1865 loss: 2.65046469e-06
Iter: 1866 loss: 2.64924665e-06
Iter: 1867 loss: 2.64985624e-06
Iter: 1868 loss: 2.64847517e-06
Iter: 1869 loss: 2.64675327e-06
Iter: 1870 loss: 2.64989376e-06
Iter: 1871 loss: 2.64591563e-06
Iter: 1872 loss: 2.64462e-06
Iter: 1873 loss: 2.64272012e-06
Iter: 1874 loss: 2.64268215e-06
Iter: 1875 loss: 2.64027335e-06
Iter: 1876 loss: 2.64029904e-06
Iter: 1877 loss: 2.63913876e-06
Iter: 1878 loss: 2.65497806e-06
Iter: 1879 loss: 2.63920492e-06
Iter: 1880 loss: 2.63825041e-06
Iter: 1881 loss: 2.63660104e-06
Iter: 1882 loss: 2.67782798e-06
Iter: 1883 loss: 2.6365808e-06
Iter: 1884 loss: 2.63509651e-06
Iter: 1885 loss: 2.6548712e-06
Iter: 1886 loss: 2.6350915e-06
Iter: 1887 loss: 2.63394122e-06
Iter: 1888 loss: 2.63337415e-06
Iter: 1889 loss: 2.63286529e-06
Iter: 1890 loss: 2.63209631e-06
Iter: 1891 loss: 2.63190032e-06
Iter: 1892 loss: 2.63110451e-06
Iter: 1893 loss: 2.62984349e-06
Iter: 1894 loss: 2.62983349e-06
Iter: 1895 loss: 2.62845242e-06
Iter: 1896 loss: 2.62997378e-06
Iter: 1897 loss: 2.62766389e-06
Iter: 1898 loss: 2.62646086e-06
Iter: 1899 loss: 2.63696529e-06
Iter: 1900 loss: 2.6264504e-06
Iter: 1901 loss: 2.62525919e-06
Iter: 1902 loss: 2.62507547e-06
Iter: 1903 loss: 2.62425806e-06
Iter: 1904 loss: 2.62255116e-06
Iter: 1905 loss: 2.62805497e-06
Iter: 1906 loss: 2.62198819e-06
Iter: 1907 loss: 2.62086633e-06
Iter: 1908 loss: 2.62103981e-06
Iter: 1909 loss: 2.61990226e-06
Iter: 1910 loss: 2.61826699e-06
Iter: 1911 loss: 2.6328737e-06
Iter: 1912 loss: 2.61826426e-06
Iter: 1913 loss: 2.61687319e-06
Iter: 1914 loss: 2.62594926e-06
Iter: 1915 loss: 2.61676541e-06
Iter: 1916 loss: 2.61575133e-06
Iter: 1917 loss: 2.61512287e-06
Iter: 1918 loss: 2.61472951e-06
Iter: 1919 loss: 2.61350669e-06
Iter: 1920 loss: 2.62258709e-06
Iter: 1921 loss: 2.6134262e-06
Iter: 1922 loss: 2.61216701e-06
Iter: 1923 loss: 2.61151e-06
Iter: 1924 loss: 2.6109019e-06
Iter: 1925 loss: 2.60990601e-06
Iter: 1926 loss: 2.60970046e-06
Iter: 1927 loss: 2.60915135e-06
Iter: 1928 loss: 2.60785782e-06
Iter: 1929 loss: 2.62137e-06
Iter: 1930 loss: 2.6076882e-06
Iter: 1931 loss: 2.60577099e-06
Iter: 1932 loss: 2.60719366e-06
Iter: 1933 loss: 2.60462298e-06
Iter: 1934 loss: 2.60301317e-06
Iter: 1935 loss: 2.60304159e-06
Iter: 1936 loss: 2.60189017e-06
Iter: 1937 loss: 2.60077923e-06
Iter: 1938 loss: 2.60053093e-06
Iter: 1939 loss: 2.59855551e-06
Iter: 1940 loss: 2.60503452e-06
Iter: 1941 loss: 2.59804165e-06
Iter: 1942 loss: 2.59632498e-06
Iter: 1943 loss: 2.59701596e-06
Iter: 1944 loss: 2.59523677e-06
Iter: 1945 loss: 2.594031e-06
Iter: 1946 loss: 2.59395847e-06
Iter: 1947 loss: 2.59280978e-06
Iter: 1948 loss: 2.59382023e-06
Iter: 1949 loss: 2.59230364e-06
Iter: 1950 loss: 2.59099806e-06
Iter: 1951 loss: 2.59079206e-06
Iter: 1952 loss: 2.59001308e-06
Iter: 1953 loss: 2.58870023e-06
Iter: 1954 loss: 2.60509796e-06
Iter: 1955 loss: 2.58864202e-06
Iter: 1956 loss: 2.58755426e-06
Iter: 1957 loss: 2.58887167e-06
Iter: 1958 loss: 2.58699038e-06
Iter: 1959 loss: 2.58526597e-06
Iter: 1960 loss: 2.59089643e-06
Iter: 1961 loss: 2.58471573e-06
Iter: 1962 loss: 2.58391901e-06
Iter: 1963 loss: 2.58294631e-06
Iter: 1964 loss: 2.58279715e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi2
+ date
Sat Nov  7 23:45:29 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/300_100_100_100_1 --function f1 --psi -2 --phi 2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd37668e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3767b9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3766bd048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3766e8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3766d5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3766d5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd37659f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd37659fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd37662d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd376670d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd37666c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3370700d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd337070bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3765582f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd376558840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd376558730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd337033620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd337033f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd337027bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd376541ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd336fe4c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd310702620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3106fd730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd336fab9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd336fab8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd310685158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd310628488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd31062a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd31062a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3106cabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3105e7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3105ce6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3105cee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3105c6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3105c66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3105c6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.01085217
test_loss: 0.013619502
train_loss: 1.9968494
test_loss: 1.9956816
train_loss: 0.49029177
test_loss: 0.4936858
train_loss: 0.49330315
test_loss: 0.4948967
train_loss: 0.49925724
test_loss: 0.4942969
train_loss: 0.5010188
test_loss: 0.49390405
train_loss: 0.483077
test_loss: 0.49316227
train_loss: 0.50069183
test_loss: 0.49248734
train_loss: 0.48056677
test_loss: 0.49162033
train_loss: 0.5027883
test_loss: 0.4908099
train_loss: 0.48779666
test_loss: 0.4899154
train_loss: 0.47120366
test_loss: 0.48889837
train_loss: 0.487187
test_loss: 0.48783213
train_loss: 0.48325068
test_loss: 0.48661786
train_loss: 0.48821276
test_loss: 0.4854271
train_loss: 0.4773049
test_loss: 0.4841878
train_loss: 0.4829116
test_loss: 0.48272997
train_loss: 0.4649492
test_loss: 0.48130313
train_loss: 0.470557
test_loss: 0.47977674
train_loss: 0.4690858
test_loss: 0.47813722
train_loss: 0.47576964
test_loss: 0.47637576
train_loss: 0.4634899
test_loss: 0.47465298
train_loss: 0.47396195
test_loss: 0.47269943
train_loss: 0.46698606
test_loss: 0.470585
train_loss: 0.4612801
test_loss: 0.4682203
train_loss: 0.45769995
test_loss: 0.46484658
train_loss: 0.45024022
test_loss: 0.45513788
train_loss: 0.44029236
test_loss: 0.4275757
train_loss: 0.38282794
test_loss: 0.39568257
train_loss: 0.36908185
test_loss: 0.36949357
train_loss: 0.34083068
test_loss: 0.34815294
train_loss: 0.32449216
test_loss: 0.33017352
train_loss: 0.31189013
test_loss: 0.3148101
train_loss: 0.30553144
test_loss: 0.30126163
train_loss: 0.28145093
test_loss: 0.28905958
train_loss: 0.263726
test_loss: 0.2778228
train_loss: 0.27423888
test_loss: 0.26733866
train_loss: 0.2620003
test_loss: 0.25730002
train_loss: 0.24507846
test_loss: 0.24741964
train_loss: 0.23916498
test_loss: 0.23776169
train_loss: 0.21952528
test_loss: 0.22815724
train_loss: 0.22170654
test_loss: 0.21855958
train_loss: 0.1997611
test_loss: 0.20889942
train_loss: 0.2005537
test_loss: 0.19970426
train_loss: 0.18355593
test_loss: 0.19051093
train_loss: 0.17105831
test_loss: 0.18153556
train_loss: 0.17335002
test_loss: 0.17262398
train_loss: 0.162303
test_loss: 0.16389798
train_loss: 0.15542528
test_loss: 0.15519845
train_loss: 0.15714708
test_loss: 0.14681122
train_loss: 0.13556334
test_loss: 0.13868365
train_loss: 0.13162419
test_loss: 0.13113591
train_loss: 0.13185395
test_loss: 0.12384026
train_loss: 0.11458324
test_loss: 0.11689922
train_loss: 0.10518441
test_loss: 0.11018811
train_loss: 0.099461116
test_loss: 0.10376502
train_loss: 0.09124565
test_loss: 0.097663745
train_loss: 0.096288785
test_loss: 0.09211009
train_loss: 0.07878128
test_loss: 0.08688893
train_loss: 0.086460166
test_loss: 0.08216816
train_loss: 0.07193588
test_loss: 0.07785204
train_loss: 0.07416342
test_loss: 0.07407211
train_loss: 0.068633795
test_loss: 0.07079821
train_loss: 0.061631456
test_loss: 0.06791776
train_loss: 0.06968706
test_loss: 0.065492935
train_loss: 0.060373403
test_loss: 0.06333631
train_loss: 0.05756464
test_loss: 0.061495345
train_loss: 0.06303136
test_loss: 0.059865598
train_loss: 0.053307533
test_loss: 0.05846295
train_loss: 0.057311397
test_loss: 0.057224512
train_loss: 0.0547689
test_loss: 0.056181848
train_loss: 0.047613356
test_loss: 0.054972973
train_loss: 0.047704943
test_loss: 0.05406304
train_loss: 0.05210297
test_loss: 0.053188324
train_loss: 0.04585569
test_loss: 0.05219131
train_loss: 0.04642342
test_loss: 0.051339287
train_loss: 0.047485784
test_loss: 0.0506227
train_loss: 0.055713538
test_loss: 0.049739666
train_loss: 0.044834748
test_loss: 0.048912592
train_loss: 0.04323715
test_loss: 0.04836129
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi2/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4c15268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4c66c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4c66a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4b5cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4bbf9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4bbfc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4b038c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4aa77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4ac9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4a6bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4a5e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4a1ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c49d1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c49d1d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4991950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c49918c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c49c1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4fd2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4f787b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4fa1ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4f366a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4ee2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4f1d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4ecb620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4f1d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4ed62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4e2e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4e5d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4e5d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4e0ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4da97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4dda620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4ddad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4d8b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4d8b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76b4d8b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.006786271
Iter: 2 loss: 0.00645361
Iter: 3 loss: 0.00638815574
Iter: 4 loss: 0.00611037947
Iter: 5 loss: 0.00863159075
Iter: 6 loss: 0.00609714445
Iter: 7 loss: 0.00584297068
Iter: 8 loss: 0.00613036845
Iter: 9 loss: 0.00567966
Iter: 10 loss: 0.00534773385
Iter: 11 loss: 0.00705963885
Iter: 12 loss: 0.00527928676
Iter: 13 loss: 0.00495087262
Iter: 14 loss: 0.00632943213
Iter: 15 loss: 0.00489056576
Iter: 16 loss: 0.0045875432
Iter: 17 loss: 0.0147077227
Iter: 18 loss: 0.00458659884
Iter: 19 loss: 0.00438663969
Iter: 20 loss: 0.00521622458
Iter: 21 loss: 0.00434358
Iter: 22 loss: 0.00412662514
Iter: 23 loss: 0.00466997316
Iter: 24 loss: 0.00402437802
Iter: 25 loss: 0.00375705305
Iter: 26 loss: 0.00512381
Iter: 27 loss: 0.00370755745
Iter: 28 loss: 0.00351019436
Iter: 29 loss: 0.00375811872
Iter: 30 loss: 0.00340854377
Iter: 31 loss: 0.00325317332
Iter: 32 loss: 0.00397704914
Iter: 33 loss: 0.00322348159
Iter: 34 loss: 0.00309545547
Iter: 35 loss: 0.0032272269
Iter: 36 loss: 0.00302448194
Iter: 37 loss: 0.00290611433
Iter: 38 loss: 0.00366385351
Iter: 39 loss: 0.00288436
Iter: 40 loss: 0.00272927806
Iter: 41 loss: 0.0045153359
Iter: 42 loss: 0.00272768969
Iter: 43 loss: 0.00264093559
Iter: 44 loss: 0.00276187295
Iter: 45 loss: 0.00259834644
Iter: 46 loss: 0.00250299042
Iter: 47 loss: 0.00391236506
Iter: 48 loss: 0.00250272127
Iter: 49 loss: 0.00243514916
Iter: 50 loss: 0.00250281161
Iter: 51 loss: 0.0023982008
Iter: 52 loss: 0.00228926959
Iter: 53 loss: 0.00252084457
Iter: 54 loss: 0.00224443688
Iter: 55 loss: 0.00217769179
Iter: 56 loss: 0.00216405652
Iter: 57 loss: 0.00210620114
Iter: 58 loss: 0.00268543931
Iter: 59 loss: 0.00210299459
Iter: 60 loss: 0.00205505174
Iter: 61 loss: 0.00202933559
Iter: 62 loss: 0.00200800784
Iter: 63 loss: 0.00193267362
Iter: 64 loss: 0.0020814403
Iter: 65 loss: 0.00190008164
Iter: 66 loss: 0.00184556877
Iter: 67 loss: 0.00266241
Iter: 68 loss: 0.00184552092
Iter: 69 loss: 0.00180353294
Iter: 70 loss: 0.00195312616
Iter: 71 loss: 0.00179297093
Iter: 72 loss: 0.00174998352
Iter: 73 loss: 0.00185632345
Iter: 74 loss: 0.00173366454
Iter: 75 loss: 0.00168054411
Iter: 76 loss: 0.00202021562
Iter: 77 loss: 0.0016750053
Iter: 78 loss: 0.00162542332
Iter: 79 loss: 0.00182738272
Iter: 80 loss: 0.00161464571
Iter: 81 loss: 0.00158385106
Iter: 82 loss: 0.00163625251
Iter: 83 loss: 0.00156984339
Iter: 84 loss: 0.00153536338
Iter: 85 loss: 0.00152164127
Iter: 86 loss: 0.00150343892
Iter: 87 loss: 0.00145962159
Iter: 88 loss: 0.00187234441
Iter: 89 loss: 0.00145784742
Iter: 90 loss: 0.00142707652
Iter: 91 loss: 0.00150767621
Iter: 92 loss: 0.00141665386
Iter: 93 loss: 0.00139450724
Iter: 94 loss: 0.00173437479
Iter: 95 loss: 0.00139430386
Iter: 96 loss: 0.00136975967
Iter: 97 loss: 0.00136410887
Iter: 98 loss: 0.00134841632
Iter: 99 loss: 0.00132124312
Iter: 100 loss: 0.00136791472
Iter: 101 loss: 0.00130913954
Iter: 102 loss: 0.00127554033
Iter: 103 loss: 0.00135270145
Iter: 104 loss: 0.001263162
Iter: 105 loss: 0.00123841071
Iter: 106 loss: 0.00138287013
Iter: 107 loss: 0.00123514538
Iter: 108 loss: 0.00120993657
Iter: 109 loss: 0.00122332713
Iter: 110 loss: 0.00119326706
Iter: 111 loss: 0.00116719294
Iter: 112 loss: 0.0012513604
Iter: 113 loss: 0.00116028532
Iter: 114 loss: 0.00114022638
Iter: 115 loss: 0.00113909366
Iter: 116 loss: 0.00112662523
Iter: 117 loss: 0.00111430907
Iter: 118 loss: 0.00111160718
Iter: 119 loss: 0.00109306106
Iter: 120 loss: 0.00111710676
Iter: 121 loss: 0.00108368252
Iter: 122 loss: 0.00106058898
Iter: 123 loss: 0.00114147854
Iter: 124 loss: 0.00105454982
Iter: 125 loss: 0.00103346212
Iter: 126 loss: 0.0011406272
Iter: 127 loss: 0.00103011681
Iter: 128 loss: 0.0010148501
Iter: 129 loss: 0.00116509967
Iter: 130 loss: 0.00101403787
Iter: 131 loss: 0.000999009353
Iter: 132 loss: 0.00098813267
Iter: 133 loss: 0.000982973375
Iter: 134 loss: 0.000964917475
Iter: 135 loss: 0.000976363546
Iter: 136 loss: 0.000953447481
Iter: 137 loss: 0.000931703195
Iter: 138 loss: 0.000993160764
Iter: 139 loss: 0.000924908614
Iter: 140 loss: 0.000905326393
Iter: 141 loss: 0.000911976444
Iter: 142 loss: 0.00089146581
Iter: 143 loss: 0.000874019694
Iter: 144 loss: 0.0010699291
Iter: 145 loss: 0.000873695943
Iter: 146 loss: 0.000860019471
Iter: 147 loss: 0.000959502067
Iter: 148 loss: 0.000858871965
Iter: 149 loss: 0.000845791888
Iter: 150 loss: 0.000931863149
Iter: 151 loss: 0.000844440772
Iter: 152 loss: 0.000833337195
Iter: 153 loss: 0.000829691766
Iter: 154 loss: 0.000823261857
Iter: 155 loss: 0.000812257582
Iter: 156 loss: 0.000804276322
Iter: 157 loss: 0.000800474314
Iter: 158 loss: 0.000784839212
Iter: 159 loss: 0.000830655568
Iter: 160 loss: 0.00078009034
Iter: 161 loss: 0.000764067285
Iter: 162 loss: 0.000968988228
Iter: 163 loss: 0.000763846561
Iter: 164 loss: 0.000752624881
Iter: 165 loss: 0.000815407839
Iter: 166 loss: 0.00075119111
Iter: 167 loss: 0.000743378885
Iter: 168 loss: 0.000733456633
Iter: 169 loss: 0.00073272
Iter: 170 loss: 0.000719777774
Iter: 171 loss: 0.000738348288
Iter: 172 loss: 0.000713497458
Iter: 173 loss: 0.000699771103
Iter: 174 loss: 0.000712888315
Iter: 175 loss: 0.000691903115
Iter: 176 loss: 0.000680497498
Iter: 177 loss: 0.000832402846
Iter: 178 loss: 0.000680437195
Iter: 179 loss: 0.000670626934
Iter: 180 loss: 0.000677931763
Iter: 181 loss: 0.00066460378
Iter: 182 loss: 0.000654534146
Iter: 183 loss: 0.000762553187
Iter: 184 loss: 0.000654333
Iter: 185 loss: 0.000646442873
Iter: 186 loss: 0.000730583095
Iter: 187 loss: 0.000646263943
Iter: 188 loss: 0.000641602674
Iter: 189 loss: 0.000635326374
Iter: 190 loss: 0.000634990691
Iter: 191 loss: 0.000625254703
Iter: 192 loss: 0.000649716
Iter: 193 loss: 0.000621878
Iter: 194 loss: 0.000614684366
Iter: 195 loss: 0.000656123739
Iter: 196 loss: 0.00061372295
Iter: 197 loss: 0.00060761848
Iter: 198 loss: 0.000622600317
Iter: 199 loss: 0.000605596
Iter: 200 loss: 0.000597887789
Iter: 201 loss: 0.000635497621
Iter: 202 loss: 0.000596483587
Iter: 203 loss: 0.000592598808
Iter: 204 loss: 0.000584978668
Iter: 205 loss: 0.000733605935
Iter: 206 loss: 0.000584911
Iter: 207 loss: 0.000575494429
Iter: 208 loss: 0.000610529562
Iter: 209 loss: 0.000573169091
Iter: 210 loss: 0.000563867565
Iter: 211 loss: 0.000601841311
Iter: 212 loss: 0.000561873778
Iter: 213 loss: 0.000554196304
Iter: 214 loss: 0.000570818083
Iter: 215 loss: 0.000551213161
Iter: 216 loss: 0.000544029404
Iter: 217 loss: 0.000540892768
Iter: 218 loss: 0.000537220272
Iter: 219 loss: 0.000533876417
Iter: 220 loss: 0.000532428443
Iter: 221 loss: 0.000526880147
Iter: 222 loss: 0.000537508342
Iter: 223 loss: 0.000524557312
Iter: 224 loss: 0.000521303155
Iter: 225 loss: 0.000518171
Iter: 226 loss: 0.000517452601
Iter: 227 loss: 0.000512601691
Iter: 228 loss: 0.000511231832
Iter: 229 loss: 0.000508279772
Iter: 230 loss: 0.000500776921
Iter: 231 loss: 0.000538516673
Iter: 232 loss: 0.000499532092
Iter: 233 loss: 0.000494923093
Iter: 234 loss: 0.000545029761
Iter: 235 loss: 0.000494792883
Iter: 236 loss: 0.000491662126
Iter: 237 loss: 0.000494315173
Iter: 238 loss: 0.000489810307
Iter: 239 loss: 0.000486308243
Iter: 240 loss: 0.000483528769
Iter: 241 loss: 0.000482456
Iter: 242 loss: 0.000476854126
Iter: 243 loss: 0.000512435392
Iter: 244 loss: 0.000476182846
Iter: 245 loss: 0.000471114821
Iter: 246 loss: 0.000472559
Iter: 247 loss: 0.000467478676
Iter: 248 loss: 0.000460167357
Iter: 249 loss: 0.000483832555
Iter: 250 loss: 0.000458095747
Iter: 251 loss: 0.000452130422
Iter: 252 loss: 0.000461269519
Iter: 253 loss: 0.000449330983
Iter: 254 loss: 0.000443665427
Iter: 255 loss: 0.000512768864
Iter: 256 loss: 0.000443597586
Iter: 257 loss: 0.000440565753
Iter: 258 loss: 0.00044056421
Iter: 259 loss: 0.000437557406
Iter: 260 loss: 0.00043508425
Iter: 261 loss: 0.000434215239
Iter: 262 loss: 0.000430876506
Iter: 263 loss: 0.000429000182
Iter: 264 loss: 0.000427551975
Iter: 265 loss: 0.000421789184
Iter: 266 loss: 0.000454862544
Iter: 267 loss: 0.000421005534
Iter: 268 loss: 0.00041705108
Iter: 269 loss: 0.000431695604
Iter: 270 loss: 0.000416072027
Iter: 271 loss: 0.000412990514
Iter: 272 loss: 0.000419819844
Iter: 273 loss: 0.000411793473
Iter: 274 loss: 0.000407347456
Iter: 275 loss: 0.000419414777
Iter: 276 loss: 0.000405904691
Iter: 277 loss: 0.000402670586
Iter: 278 loss: 0.000414116425
Iter: 279 loss: 0.000401841127
Iter: 280 loss: 0.000399131328
Iter: 281 loss: 0.000393823953
Iter: 282 loss: 0.00050158205
Iter: 283 loss: 0.000393782597
Iter: 284 loss: 0.000389376422
Iter: 285 loss: 0.000389334629
Iter: 286 loss: 0.000385930121
Iter: 287 loss: 0.000385261897
Iter: 288 loss: 0.000382983184
Iter: 289 loss: 0.000378096
Iter: 290 loss: 0.000389683933
Iter: 291 loss: 0.000376331824
Iter: 292 loss: 0.000373822579
Iter: 293 loss: 0.00037369749
Iter: 294 loss: 0.000370928203
Iter: 295 loss: 0.000378557597
Iter: 296 loss: 0.000370022957
Iter: 297 loss: 0.000367067783
Iter: 298 loss: 0.000362785882
Iter: 299 loss: 0.000362659106
Iter: 300 loss: 0.000359234517
Iter: 301 loss: 0.000372107723
Iter: 302 loss: 0.000358416321
Iter: 303 loss: 0.000355226279
Iter: 304 loss: 0.000368962297
Iter: 305 loss: 0.000354563759
Iter: 306 loss: 0.000351136492
Iter: 307 loss: 0.000359498139
Iter: 308 loss: 0.000349910755
Iter: 309 loss: 0.000347206369
Iter: 310 loss: 0.000351979543
Iter: 311 loss: 0.000346008397
Iter: 312 loss: 0.000342364627
Iter: 313 loss: 0.000363650091
Iter: 314 loss: 0.000341907755
Iter: 315 loss: 0.00034009447
Iter: 316 loss: 0.000341867446
Iter: 317 loss: 0.000339066668
Iter: 318 loss: 0.000336664089
Iter: 319 loss: 0.000333083561
Iter: 320 loss: 0.000333003292
Iter: 321 loss: 0.000329848786
Iter: 322 loss: 0.000329847215
Iter: 323 loss: 0.000327611197
Iter: 324 loss: 0.000329181406
Iter: 325 loss: 0.000326204521
Iter: 326 loss: 0.000323033484
Iter: 327 loss: 0.000332593627
Iter: 328 loss: 0.000322089239
Iter: 329 loss: 0.000319260609
Iter: 330 loss: 0.000323227374
Iter: 331 loss: 0.000317861675
Iter: 332 loss: 0.000316997961
Iter: 333 loss: 0.000316105812
Iter: 334 loss: 0.0003147643
Iter: 335 loss: 0.000312593649
Iter: 336 loss: 0.000312581367
Iter: 337 loss: 0.000309923082
Iter: 338 loss: 0.000309514318
Iter: 339 loss: 0.000307668466
Iter: 340 loss: 0.000305006804
Iter: 341 loss: 0.000309795549
Iter: 342 loss: 0.000303855399
Iter: 343 loss: 0.000300961838
Iter: 344 loss: 0.000320602849
Iter: 345 loss: 0.000300676533
Iter: 346 loss: 0.000298008963
Iter: 347 loss: 0.000306322647
Iter: 348 loss: 0.000297227583
Iter: 349 loss: 0.000295435573
Iter: 350 loss: 0.00029463775
Iter: 351 loss: 0.000293733028
Iter: 352 loss: 0.000290998374
Iter: 353 loss: 0.000302426633
Iter: 354 loss: 0.000290427241
Iter: 355 loss: 0.000287875067
Iter: 356 loss: 0.000315437384
Iter: 357 loss: 0.000287809264
Iter: 358 loss: 0.000286058756
Iter: 359 loss: 0.00028755833
Iter: 360 loss: 0.000285022485
Iter: 361 loss: 0.000283164671
Iter: 362 loss: 0.000284159
Iter: 363 loss: 0.000281938701
Iter: 364 loss: 0.000279536733
Iter: 365 loss: 0.000284116104
Iter: 366 loss: 0.000278536754
Iter: 367 loss: 0.000277747109
Iter: 368 loss: 0.000277236511
Iter: 369 loss: 0.000276155421
Iter: 370 loss: 0.00027670135
Iter: 371 loss: 0.000275432598
Iter: 372 loss: 0.000274225429
Iter: 373 loss: 0.000271278841
Iter: 374 loss: 0.000301449269
Iter: 375 loss: 0.000270931021
Iter: 376 loss: 0.000268656528
Iter: 377 loss: 0.000303673907
Iter: 378 loss: 0.000268655713
Iter: 379 loss: 0.00026657444
Iter: 380 loss: 0.000269338285
Iter: 381 loss: 0.000265525247
Iter: 382 loss: 0.00026367855
Iter: 383 loss: 0.000270766381
Iter: 384 loss: 0.000263242953
Iter: 385 loss: 0.000261764217
Iter: 386 loss: 0.000274977
Iter: 387 loss: 0.000261691021
Iter: 388 loss: 0.000260438101
Iter: 389 loss: 0.000257957086
Iter: 390 loss: 0.000305737136
Iter: 391 loss: 0.000257929496
Iter: 392 loss: 0.000255234889
Iter: 393 loss: 0.000274163729
Iter: 394 loss: 0.000254983432
Iter: 395 loss: 0.000253531965
Iter: 396 loss: 0.000253523409
Iter: 397 loss: 0.000252343249
Iter: 398 loss: 0.00025079865
Iter: 399 loss: 0.000250701
Iter: 400 loss: 0.000249031058
Iter: 401 loss: 0.000254242244
Iter: 402 loss: 0.00024854613
Iter: 403 loss: 0.000247978809
Iter: 404 loss: 0.000247618358
Iter: 405 loss: 0.00024713279
Iter: 406 loss: 0.000245832081
Iter: 407 loss: 0.00025470421
Iter: 408 loss: 0.000245528499
Iter: 409 loss: 0.000243482078
Iter: 410 loss: 0.000245103205
Iter: 411 loss: 0.000242254813
Iter: 412 loss: 0.000240759226
Iter: 413 loss: 0.00025212107
Iter: 414 loss: 0.000240641224
Iter: 415 loss: 0.000238918263
Iter: 416 loss: 0.000238444132
Iter: 417 loss: 0.00023738743
Iter: 418 loss: 0.000235799685
Iter: 419 loss: 0.000249217148
Iter: 420 loss: 0.000235707295
Iter: 421 loss: 0.000234413965
Iter: 422 loss: 0.000240827125
Iter: 423 loss: 0.000234193605
Iter: 424 loss: 0.000232919963
Iter: 425 loss: 0.000233434504
Iter: 426 loss: 0.00023204273
Iter: 427 loss: 0.000230689679
Iter: 428 loss: 0.000231062935
Iter: 429 loss: 0.000229709651
Iter: 430 loss: 0.000228308039
Iter: 431 loss: 0.000228264078
Iter: 432 loss: 0.000227538636
Iter: 433 loss: 0.000227571873
Iter: 434 loss: 0.000226968317
Iter: 435 loss: 0.000226151707
Iter: 436 loss: 0.000235548869
Iter: 437 loss: 0.000226141841
Iter: 438 loss: 0.000225344571
Iter: 439 loss: 0.000225383279
Iter: 440 loss: 0.000224721327
Iter: 441 loss: 0.000223892857
Iter: 442 loss: 0.000222486444
Iter: 443 loss: 0.000222483926
Iter: 444 loss: 0.000220588816
Iter: 445 loss: 0.000226034768
Iter: 446 loss: 0.000219998765
Iter: 447 loss: 0.000218683708
Iter: 448 loss: 0.000228829478
Iter: 449 loss: 0.000218586298
Iter: 450 loss: 0.000217180117
Iter: 451 loss: 0.000218671077
Iter: 452 loss: 0.000216407716
Iter: 453 loss: 0.000215080116
Iter: 454 loss: 0.000222611648
Iter: 455 loss: 0.000214898711
Iter: 456 loss: 0.000213839143
Iter: 457 loss: 0.000217019697
Iter: 458 loss: 0.000213517022
Iter: 459 loss: 0.000212211933
Iter: 460 loss: 0.000213267544
Iter: 461 loss: 0.000211432329
Iter: 462 loss: 0.000210118538
Iter: 463 loss: 0.000215200969
Iter: 464 loss: 0.000209810707
Iter: 465 loss: 0.000208633399
Iter: 466 loss: 0.000211496168
Iter: 467 loss: 0.00020821736
Iter: 468 loss: 0.000207215693
Iter: 469 loss: 0.000207205318
Iter: 470 loss: 0.000206522949
Iter: 471 loss: 0.000207809528
Iter: 472 loss: 0.000206235069
Iter: 473 loss: 0.000205521064
Iter: 474 loss: 0.000204535696
Iter: 475 loss: 0.000204491866
Iter: 476 loss: 0.000203317293
Iter: 477 loss: 0.000203414878
Iter: 478 loss: 0.000202406896
Iter: 479 loss: 0.000200842784
Iter: 480 loss: 0.000205354299
Iter: 481 loss: 0.000200351569
Iter: 482 loss: 0.000199085509
Iter: 483 loss: 0.000202635012
Iter: 484 loss: 0.000198673442
Iter: 485 loss: 0.000196987647
Iter: 486 loss: 0.000205538119
Iter: 487 loss: 0.000196712703
Iter: 488 loss: 0.000195835921
Iter: 489 loss: 0.000196652167
Iter: 490 loss: 0.000195332861
Iter: 491 loss: 0.000193958869
Iter: 492 loss: 0.000199315487
Iter: 493 loss: 0.000193640124
Iter: 494 loss: 0.000192545238
Iter: 495 loss: 0.000195070868
Iter: 496 loss: 0.000192142426
Iter: 497 loss: 0.000191262079
Iter: 498 loss: 0.000197405025
Iter: 499 loss: 0.000191179875
Iter: 500 loss: 0.000190520077
Iter: 501 loss: 0.000193770335
Iter: 502 loss: 0.000190410748
Iter: 503 loss: 0.000189563158
Iter: 504 loss: 0.000191128216
Iter: 505 loss: 0.000189198094
Iter: 506 loss: 0.000188412319
Iter: 507 loss: 0.000189248938
Iter: 508 loss: 0.000187979924
Iter: 509 loss: 0.000187275364
Iter: 510 loss: 0.000186178688
Iter: 511 loss: 0.000186163379
Iter: 512 loss: 0.00018489355
Iter: 513 loss: 0.000191305531
Iter: 514 loss: 0.000184682634
Iter: 515 loss: 0.000183637036
Iter: 516 loss: 0.000184707358
Iter: 517 loss: 0.000183056094
Iter: 518 loss: 0.00018182912
Iter: 519 loss: 0.000190301042
Iter: 520 loss: 0.0001817101
Iter: 521 loss: 0.000180575531
Iter: 522 loss: 0.000183780168
Iter: 523 loss: 0.000180214643
Iter: 524 loss: 0.000179457202
Iter: 525 loss: 0.000182680466
Iter: 526 loss: 0.000179299299
Iter: 527 loss: 0.000178404734
Iter: 528 loss: 0.000179593888
Iter: 529 loss: 0.000177950991
Iter: 530 loss: 0.000177168433
Iter: 531 loss: 0.000180834235
Iter: 532 loss: 0.000177024398
Iter: 533 loss: 0.000176404705
Iter: 534 loss: 0.000179665061
Iter: 535 loss: 0.000176304238
Iter: 536 loss: 0.000175658119
Iter: 537 loss: 0.000180134099
Iter: 538 loss: 0.000175597786
Iter: 539 loss: 0.00017504803
Iter: 540 loss: 0.000174767483
Iter: 541 loss: 0.000174511049
Iter: 542 loss: 0.000173912209
Iter: 543 loss: 0.00017473218
Iter: 544 loss: 0.000173614069
Iter: 545 loss: 0.000172841581
Iter: 546 loss: 0.000172012922
Iter: 547 loss: 0.000171880252
Iter: 548 loss: 0.000170871484
Iter: 549 loss: 0.000174827132
Iter: 550 loss: 0.000170640735
Iter: 551 loss: 0.000169593637
Iter: 552 loss: 0.000170677449
Iter: 553 loss: 0.000169013365
Iter: 554 loss: 0.000168191269
Iter: 555 loss: 0.000168182858
Iter: 556 loss: 0.000167450402
Iter: 557 loss: 0.000166803104
Iter: 558 loss: 0.000166614773
Iter: 559 loss: 0.000165881385
Iter: 560 loss: 0.000165878679
Iter: 561 loss: 0.000165301739
Iter: 562 loss: 0.000165262143
Iter: 563 loss: 0.000164827041
Iter: 564 loss: 0.000164130208
Iter: 565 loss: 0.000169611492
Iter: 566 loss: 0.000164083758
Iter: 567 loss: 0.000163568417
Iter: 568 loss: 0.000170773172
Iter: 569 loss: 0.000163567252
Iter: 570 loss: 0.000163179007
Iter: 571 loss: 0.000163525314
Iter: 572 loss: 0.000162952987
Iter: 573 loss: 0.00016256317
Iter: 574 loss: 0.00016200266
Iter: 575 loss: 0.000161984819
Iter: 576 loss: 0.000161140488
Iter: 577 loss: 0.000163399061
Iter: 578 loss: 0.00016086144
Iter: 579 loss: 0.000160052703
Iter: 580 loss: 0.000159633812
Iter: 581 loss: 0.00015926038
Iter: 582 loss: 0.00015822165
Iter: 583 loss: 0.000164041659
Iter: 584 loss: 0.000158076393
Iter: 585 loss: 0.000157228147
Iter: 586 loss: 0.00016059418
Iter: 587 loss: 0.000157038579
Iter: 588 loss: 0.000156278576
Iter: 589 loss: 0.000162549026
Iter: 590 loss: 0.000156231836
Iter: 591 loss: 0.000155633927
Iter: 592 loss: 0.000155585425
Iter: 593 loss: 0.000155141213
Iter: 594 loss: 0.000154243578
Iter: 595 loss: 0.000159616058
Iter: 596 loss: 0.000154135734
Iter: 597 loss: 0.000153368979
Iter: 598 loss: 0.000154203852
Iter: 599 loss: 0.00015294936
Iter: 600 loss: 0.000152578679
Iter: 601 loss: 0.000152553432
Iter: 602 loss: 0.000152115448
Iter: 603 loss: 0.000152616412
Iter: 604 loss: 0.000151881279
Iter: 605 loss: 0.000151485263
Iter: 606 loss: 0.000151822416
Iter: 607 loss: 0.00015125073
Iter: 608 loss: 0.000150835243
Iter: 609 loss: 0.000150592619
Iter: 610 loss: 0.000150418564
Iter: 611 loss: 0.000149722749
Iter: 612 loss: 0.000151265675
Iter: 613 loss: 0.000149456901
Iter: 614 loss: 0.000148764593
Iter: 615 loss: 0.00014995021
Iter: 616 loss: 0.00014845439
Iter: 617 loss: 0.000147684565
Iter: 618 loss: 0.000150169566
Iter: 619 loss: 0.000147467261
Iter: 620 loss: 0.000146710838
Iter: 621 loss: 0.000147368177
Iter: 622 loss: 0.000146265535
Iter: 623 loss: 0.000145441765
Iter: 624 loss: 0.000153826477
Iter: 625 loss: 0.000145418875
Iter: 626 loss: 0.000144877849
Iter: 627 loss: 0.000146384438
Iter: 628 loss: 0.000144704944
Iter: 629 loss: 0.000144023943
Iter: 630 loss: 0.000144358783
Iter: 631 loss: 0.00014356969
Iter: 632 loss: 0.000142988545
Iter: 633 loss: 0.000149746
Iter: 634 loss: 0.000142977209
Iter: 635 loss: 0.000142565172
Iter: 636 loss: 0.000145595943
Iter: 637 loss: 0.000142530786
Iter: 638 loss: 0.000142013756
Iter: 639 loss: 0.00014201022
Iter: 640 loss: 0.000141599536
Iter: 641 loss: 0.000141228607
Iter: 642 loss: 0.000141134806
Iter: 643 loss: 0.000140901073
Iter: 644 loss: 0.000140423625
Iter: 645 loss: 0.000143969664
Iter: 646 loss: 0.000140385266
Iter: 647 loss: 0.000139974116
Iter: 648 loss: 0.000139285607
Iter: 649 loss: 0.000139284981
Iter: 650 loss: 0.000138527801
Iter: 651 loss: 0.00014159354
Iter: 652 loss: 0.000138360498
Iter: 653 loss: 0.000137550334
Iter: 654 loss: 0.000138717282
Iter: 655 loss: 0.000137157011
Iter: 656 loss: 0.000136545874
Iter: 657 loss: 0.000142723395
Iter: 658 loss: 0.000136527757
Iter: 659 loss: 0.000135975308
Iter: 660 loss: 0.000136120216
Iter: 661 loss: 0.000135573646
Iter: 662 loss: 0.000135001901
Iter: 663 loss: 0.000139969197
Iter: 664 loss: 0.000134970847
Iter: 665 loss: 0.000134461356
Iter: 666 loss: 0.000134611182
Iter: 667 loss: 0.000134093774
Iter: 668 loss: 0.000133694528
Iter: 669 loss: 0.000133683949
Iter: 670 loss: 0.000133443507
Iter: 671 loss: 0.000133699883
Iter: 672 loss: 0.000133310532
Iter: 673 loss: 0.000132987741
Iter: 674 loss: 0.000132811852
Iter: 675 loss: 0.000132668385
Iter: 676 loss: 0.00013227071
Iter: 677 loss: 0.000133082794
Iter: 678 loss: 0.000132111541
Iter: 679 loss: 0.000131582798
Iter: 680 loss: 0.000131876615
Iter: 681 loss: 0.00013123898
Iter: 682 loss: 0.000130678716
Iter: 683 loss: 0.000134059257
Iter: 684 loss: 0.000130610511
Iter: 685 loss: 0.000130094544
Iter: 686 loss: 0.000129937514
Iter: 687 loss: 0.000129631662
Iter: 688 loss: 0.000128997839
Iter: 689 loss: 0.000131516746
Iter: 690 loss: 0.000128854372
Iter: 691 loss: 0.000128253858
Iter: 692 loss: 0.000129539229
Iter: 693 loss: 0.000128018044
Iter: 694 loss: 0.000127323277
Iter: 695 loss: 0.000128653934
Iter: 696 loss: 0.000127031846
Iter: 697 loss: 0.000126480954
Iter: 698 loss: 0.000127771316
Iter: 699 loss: 0.000126276165
Iter: 700 loss: 0.000125796607
Iter: 701 loss: 0.000125797
Iter: 702 loss: 0.000125483086
Iter: 703 loss: 0.000129688924
Iter: 704 loss: 0.00012548166
Iter: 705 loss: 0.000125220409
Iter: 706 loss: 0.00012534007
Iter: 707 loss: 0.000125043414
Iter: 708 loss: 0.000124823535
Iter: 709 loss: 0.000124422193
Iter: 710 loss: 0.000133982743
Iter: 711 loss: 0.000124422426
Iter: 712 loss: 0.000123916805
Iter: 713 loss: 0.00012839488
Iter: 714 loss: 0.000123891528
Iter: 715 loss: 0.000123578619
Iter: 716 loss: 0.000123403675
Iter: 717 loss: 0.000123268343
Iter: 718 loss: 0.000122770536
Iter: 719 loss: 0.000126281244
Iter: 720 loss: 0.00012272739
Iter: 721 loss: 0.000122343627
Iter: 722 loss: 0.000122319572
Iter: 723 loss: 0.000122028425
Iter: 724 loss: 0.000121523779
Iter: 725 loss: 0.00012211276
Iter: 726 loss: 0.000121255667
Iter: 727 loss: 0.000120614983
Iter: 728 loss: 0.00012298912
Iter: 729 loss: 0.000120456418
Iter: 730 loss: 0.000119952718
Iter: 731 loss: 0.000121989353
Iter: 732 loss: 0.000119840006
Iter: 733 loss: 0.000119319309
Iter: 734 loss: 0.000120332013
Iter: 735 loss: 0.000119103046
Iter: 736 loss: 0.000118779033
Iter: 737 loss: 0.000118769443
Iter: 738 loss: 0.000118426804
Iter: 739 loss: 0.000119469289
Iter: 740 loss: 0.000118325122
Iter: 741 loss: 0.000118045864
Iter: 742 loss: 0.000117863521
Iter: 743 loss: 0.000117756259
Iter: 744 loss: 0.000117361953
Iter: 745 loss: 0.000118258635
Iter: 746 loss: 0.000117214513
Iter: 747 loss: 0.000116828771
Iter: 748 loss: 0.000117076437
Iter: 749 loss: 0.000116584073
Iter: 750 loss: 0.000116161391
Iter: 751 loss: 0.000119275952
Iter: 752 loss: 0.000116125462
Iter: 753 loss: 0.000115737843
Iter: 754 loss: 0.0001160147
Iter: 755 loss: 0.000115496012
Iter: 756 loss: 0.000115095492
Iter: 757 loss: 0.000115757459
Iter: 758 loss: 0.000114912051
Iter: 759 loss: 0.000114513743
Iter: 760 loss: 0.000115791736
Iter: 761 loss: 0.000114400827
Iter: 762 loss: 0.000113976268
Iter: 763 loss: 0.000116058509
Iter: 764 loss: 0.000113901762
Iter: 765 loss: 0.000113554474
Iter: 766 loss: 0.000113472815
Iter: 767 loss: 0.000113250368
Iter: 768 loss: 0.000112802329
Iter: 769 loss: 0.000113210896
Iter: 770 loss: 0.000112543552
Iter: 771 loss: 0.00011282802
Iter: 772 loss: 0.000112318186
Iter: 773 loss: 0.000112150374
Iter: 774 loss: 0.000111747526
Iter: 775 loss: 0.000116224473
Iter: 776 loss: 0.000111707188
Iter: 777 loss: 0.000111358429
Iter: 778 loss: 0.000113411734
Iter: 779 loss: 0.000111312955
Iter: 780 loss: 0.000111006237
Iter: 781 loss: 0.000110835477
Iter: 782 loss: 0.000110702378
Iter: 783 loss: 0.000110290726
Iter: 784 loss: 0.000112710477
Iter: 785 loss: 0.000110237961
Iter: 786 loss: 0.000109971406
Iter: 787 loss: 0.00011194282
Iter: 788 loss: 0.000109949317
Iter: 789 loss: 0.00010971263
Iter: 790 loss: 0.000109563698
Iter: 791 loss: 0.000109469642
Iter: 792 loss: 0.000109045388
Iter: 793 loss: 0.000109431727
Iter: 794 loss: 0.000108799592
Iter: 795 loss: 0.000108411026
Iter: 796 loss: 0.000111342946
Iter: 797 loss: 0.000108381311
Iter: 798 loss: 0.00010800414
Iter: 799 loss: 0.00010801998
Iter: 800 loss: 0.000107708562
Iter: 801 loss: 0.000107300606
Iter: 802 loss: 0.00011036489
Iter: 803 loss: 0.000107269079
Iter: 804 loss: 0.000106950269
Iter: 805 loss: 0.000107563996
Iter: 806 loss: 0.000106817781
Iter: 807 loss: 0.000106636813
Iter: 808 loss: 0.000106630978
Iter: 809 loss: 0.000106400505
Iter: 810 loss: 0.000106180385
Iter: 811 loss: 0.000106128675
Iter: 812 loss: 0.000105910105
Iter: 813 loss: 0.000105505504
Iter: 814 loss: 0.000114752271
Iter: 815 loss: 0.000105504398
Iter: 816 loss: 0.000105044368
Iter: 817 loss: 0.000108874432
Iter: 818 loss: 0.000105017061
Iter: 819 loss: 0.00010471305
Iter: 820 loss: 0.000108027
Iter: 821 loss: 0.000104706327
Iter: 822 loss: 0.000104512459
Iter: 823 loss: 0.000104295767
Iter: 824 loss: 0.000104265127
Iter: 825 loss: 0.000103956852
Iter: 826 loss: 0.00010512898
Iter: 827 loss: 0.000103882929
Iter: 828 loss: 0.000103567654
Iter: 829 loss: 0.000104542312
Iter: 830 loss: 0.000103473772
Iter: 831 loss: 0.000103139479
Iter: 832 loss: 0.000104329716
Iter: 833 loss: 0.000103053637
Iter: 834 loss: 0.000102739905
Iter: 835 loss: 0.000103306797
Iter: 836 loss: 0.000102600883
Iter: 837 loss: 0.000102334423
Iter: 838 loss: 0.000102830832
Iter: 839 loss: 0.000102220787
Iter: 840 loss: 0.000101870151
Iter: 841 loss: 0.000102570426
Iter: 842 loss: 0.000101727361
Iter: 843 loss: 0.000101742953
Iter: 844 loss: 0.000101566839
Iter: 845 loss: 0.000101462399
Iter: 846 loss: 0.000101198166
Iter: 847 loss: 0.000103561761
Iter: 848 loss: 0.000101155849
Iter: 849 loss: 0.000100837882
Iter: 850 loss: 0.000101103331
Iter: 851 loss: 0.000100648627
Iter: 852 loss: 0.000100246136
Iter: 853 loss: 0.000101200261
Iter: 854 loss: 0.000100098478
Iter: 855 loss: 9.97891329e-05
Iter: 856 loss: 0.000100223842
Iter: 857 loss: 9.9637e-05
Iter: 858 loss: 9.92407586e-05
Iter: 859 loss: 9.94614456e-05
Iter: 860 loss: 9.89805194e-05
Iter: 861 loss: 9.87841559e-05
Iter: 862 loss: 9.87378517e-05
Iter: 863 loss: 9.8499826e-05
Iter: 864 loss: 9.81529374e-05
Iter: 865 loss: 9.81427584e-05
Iter: 866 loss: 9.77132077e-05
Iter: 867 loss: 0.000100584861
Iter: 868 loss: 9.76694646e-05
Iter: 869 loss: 9.7424163e-05
Iter: 870 loss: 9.83530481e-05
Iter: 871 loss: 9.73651331e-05
Iter: 872 loss: 9.71078262e-05
Iter: 873 loss: 9.76347292e-05
Iter: 874 loss: 9.70048204e-05
Iter: 875 loss: 9.67046944e-05
Iter: 876 loss: 9.72040871e-05
Iter: 877 loss: 9.65693e-05
Iter: 878 loss: 9.63299099e-05
Iter: 879 loss: 9.72806738e-05
Iter: 880 loss: 9.6276257e-05
Iter: 881 loss: 9.61143232e-05
Iter: 882 loss: 9.6112919e-05
Iter: 883 loss: 9.59235331e-05
Iter: 884 loss: 9.57498778e-05
Iter: 885 loss: 9.57036173e-05
Iter: 886 loss: 9.5476862e-05
Iter: 887 loss: 9.54987918e-05
Iter: 888 loss: 9.53005074e-05
Iter: 889 loss: 9.50434e-05
Iter: 890 loss: 9.5799478e-05
Iter: 891 loss: 9.49638197e-05
Iter: 892 loss: 9.46695072e-05
Iter: 893 loss: 9.44222265e-05
Iter: 894 loss: 9.43384584e-05
Iter: 895 loss: 9.39399761e-05
Iter: 896 loss: 9.51574912e-05
Iter: 897 loss: 9.38209851e-05
Iter: 898 loss: 9.35521e-05
Iter: 899 loss: 9.35439675e-05
Iter: 900 loss: 9.33237e-05
Iter: 901 loss: 9.35113421e-05
Iter: 902 loss: 9.31948161e-05
Iter: 903 loss: 9.29398957e-05
Iter: 904 loss: 9.30223105e-05
Iter: 905 loss: 9.27587243e-05
Iter: 906 loss: 9.24538472e-05
Iter: 907 loss: 9.41062754e-05
Iter: 908 loss: 9.24086053e-05
Iter: 909 loss: 9.21358369e-05
Iter: 910 loss: 9.32683e-05
Iter: 911 loss: 9.20774328e-05
Iter: 912 loss: 9.18287478e-05
Iter: 913 loss: 9.20977909e-05
Iter: 914 loss: 9.16920544e-05
Iter: 915 loss: 9.13768745e-05
Iter: 916 loss: 9.20870516e-05
Iter: 917 loss: 9.12579781e-05
Iter: 918 loss: 9.1011083e-05
Iter: 919 loss: 9.19011072e-05
Iter: 920 loss: 9.09484152e-05
Iter: 921 loss: 9.07939e-05
Iter: 922 loss: 9.0782094e-05
Iter: 923 loss: 9.06274363e-05
Iter: 924 loss: 9.04805784e-05
Iter: 925 loss: 9.04461485e-05
Iter: 926 loss: 9.02779866e-05
Iter: 927 loss: 9.01504391e-05
Iter: 928 loss: 9.00957093e-05
Iter: 929 loss: 8.97912105e-05
Iter: 930 loss: 9.00083251e-05
Iter: 931 loss: 8.96022539e-05
Iter: 932 loss: 8.92740718e-05
Iter: 933 loss: 9.11316893e-05
Iter: 934 loss: 8.92275857e-05
Iter: 935 loss: 8.90007141e-05
Iter: 936 loss: 8.95669727e-05
Iter: 937 loss: 8.89215735e-05
Iter: 938 loss: 8.86576381e-05
Iter: 939 loss: 9.06766654e-05
Iter: 940 loss: 8.86378111e-05
Iter: 941 loss: 8.84320543e-05
Iter: 942 loss: 8.81888554e-05
Iter: 943 loss: 8.81618762e-05
Iter: 944 loss: 8.78445353e-05
Iter: 945 loss: 8.86778435e-05
Iter: 946 loss: 8.77363855e-05
Iter: 947 loss: 8.7432054e-05
Iter: 948 loss: 9.15920828e-05
Iter: 949 loss: 8.7430526e-05
Iter: 950 loss: 8.72538658e-05
Iter: 951 loss: 8.70687363e-05
Iter: 952 loss: 8.70365693e-05
Iter: 953 loss: 8.68809e-05
Iter: 954 loss: 8.68751667e-05
Iter: 955 loss: 8.67048293e-05
Iter: 956 loss: 8.74350662e-05
Iter: 957 loss: 8.66687551e-05
Iter: 958 loss: 8.65448383e-05
Iter: 959 loss: 8.63744281e-05
Iter: 960 loss: 8.63667083e-05
Iter: 961 loss: 8.61608278e-05
Iter: 962 loss: 8.61594308e-05
Iter: 963 loss: 8.59967695e-05
Iter: 964 loss: 8.57182e-05
Iter: 965 loss: 8.61871522e-05
Iter: 966 loss: 8.5593e-05
Iter: 967 loss: 8.52576704e-05
Iter: 968 loss: 8.67537165e-05
Iter: 969 loss: 8.51918958e-05
Iter: 970 loss: 8.49380885e-05
Iter: 971 loss: 8.62874876e-05
Iter: 972 loss: 8.49000644e-05
Iter: 973 loss: 8.47185e-05
Iter: 974 loss: 8.56446422e-05
Iter: 975 loss: 8.46884068e-05
Iter: 976 loss: 8.44844471e-05
Iter: 977 loss: 8.4541869e-05
Iter: 978 loss: 8.43368e-05
Iter: 979 loss: 8.41165311e-05
Iter: 980 loss: 8.43521e-05
Iter: 981 loss: 8.39946806e-05
Iter: 982 loss: 8.37305e-05
Iter: 983 loss: 8.48525669e-05
Iter: 984 loss: 8.36748077e-05
Iter: 985 loss: 8.34722159e-05
Iter: 986 loss: 8.54217651e-05
Iter: 987 loss: 8.34644597e-05
Iter: 988 loss: 8.3328312e-05
Iter: 989 loss: 8.37753614e-05
Iter: 990 loss: 8.32890291e-05
Iter: 991 loss: 8.31320503e-05
Iter: 992 loss: 8.44756869e-05
Iter: 993 loss: 8.31231e-05
Iter: 994 loss: 8.30368444e-05
Iter: 995 loss: 8.27943368e-05
Iter: 996 loss: 8.40569119e-05
Iter: 997 loss: 8.27176409e-05
Iter: 998 loss: 8.2449551e-05
Iter: 999 loss: 8.38731139e-05
Iter: 1000 loss: 8.24077288e-05
Iter: 1001 loss: 8.21812864e-05
Iter: 1002 loss: 8.25953757e-05
Iter: 1003 loss: 8.20848436e-05
Iter: 1004 loss: 8.18118715e-05
Iter: 1005 loss: 8.34898092e-05
Iter: 1006 loss: 8.1779981e-05
Iter: 1007 loss: 8.16072934e-05
Iter: 1008 loss: 8.13987208e-05
Iter: 1009 loss: 8.13783699e-05
Iter: 1010 loss: 8.11445643e-05
Iter: 1011 loss: 8.4239975e-05
Iter: 1012 loss: 8.11432619e-05
Iter: 1013 loss: 8.09463e-05
Iter: 1014 loss: 8.1964e-05
Iter: 1015 loss: 8.09156045e-05
Iter: 1016 loss: 8.07663891e-05
Iter: 1017 loss: 8.05454474e-05
Iter: 1018 loss: 8.05400123e-05
Iter: 1019 loss: 8.03088042e-05
Iter: 1020 loss: 8.28623379e-05
Iter: 1021 loss: 8.0304133e-05
Iter: 1022 loss: 8.01449569e-05
Iter: 1023 loss: 8.08844561e-05
Iter: 1024 loss: 8.01150745e-05
Iter: 1025 loss: 8.00067064e-05
Iter: 1026 loss: 8.00066045e-05
Iter: 1027 loss: 7.98999245e-05
Iter: 1028 loss: 7.97632238e-05
Iter: 1029 loss: 7.97536341e-05
Iter: 1030 loss: 7.95811939e-05
Iter: 1031 loss: 7.9766105e-05
Iter: 1032 loss: 7.9486359e-05
Iter: 1033 loss: 7.93291838e-05
Iter: 1034 loss: 7.90654449e-05
Iter: 1035 loss: 7.90648483e-05
Iter: 1036 loss: 7.87832396e-05
Iter: 1037 loss: 8.16927495e-05
Iter: 1038 loss: 7.87751487e-05
Iter: 1039 loss: 7.85869779e-05
Iter: 1040 loss: 7.92262144e-05
Iter: 1041 loss: 7.85348e-05
Iter: 1042 loss: 7.83189171e-05
Iter: 1043 loss: 7.90140184e-05
Iter: 1044 loss: 7.82586721e-05
Iter: 1045 loss: 7.80562259e-05
Iter: 1046 loss: 7.8409e-05
Iter: 1047 loss: 7.79659895e-05
Iter: 1048 loss: 7.77938258e-05
Iter: 1049 loss: 7.82942807e-05
Iter: 1050 loss: 7.77403038e-05
Iter: 1051 loss: 7.74997e-05
Iter: 1052 loss: 7.82482e-05
Iter: 1053 loss: 7.74293439e-05
Iter: 1054 loss: 7.72818894e-05
Iter: 1055 loss: 7.75170774e-05
Iter: 1056 loss: 7.72141211e-05
Iter: 1057 loss: 7.70661718e-05
Iter: 1058 loss: 7.76975503e-05
Iter: 1059 loss: 7.70348051e-05
Iter: 1060 loss: 7.68829e-05
Iter: 1061 loss: 7.89669e-05
Iter: 1062 loss: 7.68827e-05
Iter: 1063 loss: 7.68108875e-05
Iter: 1064 loss: 7.66663e-05
Iter: 1065 loss: 7.93987565e-05
Iter: 1066 loss: 7.66636294e-05
Iter: 1067 loss: 7.65047e-05
Iter: 1068 loss: 7.71798514e-05
Iter: 1069 loss: 7.64719734e-05
Iter: 1070 loss: 7.63184e-05
Iter: 1071 loss: 7.62064301e-05
Iter: 1072 loss: 7.61548727e-05
Iter: 1073 loss: 7.59464747e-05
Iter: 1074 loss: 7.70039187e-05
Iter: 1075 loss: 7.59123213e-05
Iter: 1076 loss: 7.57376547e-05
Iter: 1077 loss: 7.6168566e-05
Iter: 1078 loss: 7.56746231e-05
Iter: 1079 loss: 7.5467e-05
Iter: 1080 loss: 7.57002126e-05
Iter: 1081 loss: 7.53538043e-05
Iter: 1082 loss: 7.51601619e-05
Iter: 1083 loss: 7.55311921e-05
Iter: 1084 loss: 7.50777835e-05
Iter: 1085 loss: 7.48784e-05
Iter: 1086 loss: 7.63527496e-05
Iter: 1087 loss: 7.48614402e-05
Iter: 1088 loss: 7.46776714e-05
Iter: 1089 loss: 7.53263885e-05
Iter: 1090 loss: 7.46305668e-05
Iter: 1091 loss: 7.44853169e-05
Iter: 1092 loss: 7.46637452e-05
Iter: 1093 loss: 7.44112403e-05
Iter: 1094 loss: 7.42509947e-05
Iter: 1095 loss: 7.62179116e-05
Iter: 1096 loss: 7.42488191e-05
Iter: 1097 loss: 7.41643526e-05
Iter: 1098 loss: 7.41647091e-05
Iter: 1099 loss: 7.41073964e-05
Iter: 1100 loss: 7.39538445e-05
Iter: 1101 loss: 7.49120081e-05
Iter: 1102 loss: 7.391299e-05
Iter: 1103 loss: 7.37178343e-05
Iter: 1104 loss: 7.37997761e-05
Iter: 1105 loss: 7.35833673e-05
Iter: 1106 loss: 7.33937559e-05
Iter: 1107 loss: 7.5350923e-05
Iter: 1108 loss: 7.33886118e-05
Iter: 1109 loss: 7.32153421e-05
Iter: 1110 loss: 7.37450755e-05
Iter: 1111 loss: 7.3164163e-05
Iter: 1112 loss: 7.3015457e-05
Iter: 1113 loss: 7.30156171e-05
Iter: 1114 loss: 7.2896124e-05
Iter: 1115 loss: 7.27020088e-05
Iter: 1116 loss: 7.31900509e-05
Iter: 1117 loss: 7.26335129e-05
Iter: 1118 loss: 7.24208076e-05
Iter: 1119 loss: 7.31639593e-05
Iter: 1120 loss: 7.23652047e-05
Iter: 1121 loss: 7.21657125e-05
Iter: 1122 loss: 7.26726576e-05
Iter: 1123 loss: 7.20954e-05
Iter: 1124 loss: 7.19243326e-05
Iter: 1125 loss: 7.22963232e-05
Iter: 1126 loss: 7.18577794e-05
Iter: 1127 loss: 7.16742215e-05
Iter: 1128 loss: 7.27452643e-05
Iter: 1129 loss: 7.16513241e-05
Iter: 1130 loss: 7.15763163e-05
Iter: 1131 loss: 7.15631468e-05
Iter: 1132 loss: 7.14842317e-05
Iter: 1133 loss: 7.14995113e-05
Iter: 1134 loss: 7.14255148e-05
Iter: 1135 loss: 7.13257177e-05
Iter: 1136 loss: 7.12694091e-05
Iter: 1137 loss: 7.12256e-05
Iter: 1138 loss: 7.11018656e-05
Iter: 1139 loss: 7.09258893e-05
Iter: 1140 loss: 7.0918788e-05
Iter: 1141 loss: 7.08015e-05
Iter: 1142 loss: 7.07889631e-05
Iter: 1143 loss: 7.06694e-05
Iter: 1144 loss: 7.06121937e-05
Iter: 1145 loss: 7.05539424e-05
Iter: 1146 loss: 7.03884507e-05
Iter: 1147 loss: 7.08039734e-05
Iter: 1148 loss: 7.03310288e-05
Iter: 1149 loss: 7.01459503e-05
Iter: 1150 loss: 7.11997563e-05
Iter: 1151 loss: 7.01208628e-05
Iter: 1152 loss: 6.99854645e-05
Iter: 1153 loss: 6.98919903e-05
Iter: 1154 loss: 6.98431759e-05
Iter: 1155 loss: 6.96299248e-05
Iter: 1156 loss: 7.05908606e-05
Iter: 1157 loss: 6.95888884e-05
Iter: 1158 loss: 6.94440532e-05
Iter: 1159 loss: 7.02903344e-05
Iter: 1160 loss: 6.94251648e-05
Iter: 1161 loss: 6.92750327e-05
Iter: 1162 loss: 6.92372e-05
Iter: 1163 loss: 6.91420209e-05
Iter: 1164 loss: 6.92234535e-05
Iter: 1165 loss: 6.90541929e-05
Iter: 1166 loss: 6.90046872e-05
Iter: 1167 loss: 6.88935688e-05
Iter: 1168 loss: 7.03700498e-05
Iter: 1169 loss: 6.88858199e-05
Iter: 1170 loss: 6.87739957e-05
Iter: 1171 loss: 6.9043148e-05
Iter: 1172 loss: 6.8733847e-05
Iter: 1173 loss: 6.86000567e-05
Iter: 1174 loss: 6.88037253e-05
Iter: 1175 loss: 6.85376945e-05
Iter: 1176 loss: 6.84121187e-05
Iter: 1177 loss: 6.85625e-05
Iter: 1178 loss: 6.83462713e-05
Iter: 1179 loss: 6.8216541e-05
Iter: 1180 loss: 6.82794489e-05
Iter: 1181 loss: 6.81280799e-05
Iter: 1182 loss: 6.79683e-05
Iter: 1183 loss: 6.90651359e-05
Iter: 1184 loss: 6.79533114e-05
Iter: 1185 loss: 6.78280412e-05
Iter: 1186 loss: 6.86307176e-05
Iter: 1187 loss: 6.781501e-05
Iter: 1188 loss: 6.76828786e-05
Iter: 1189 loss: 6.75600386e-05
Iter: 1190 loss: 6.75294723e-05
Iter: 1191 loss: 6.73719042e-05
Iter: 1192 loss: 6.85083505e-05
Iter: 1193 loss: 6.73590112e-05
Iter: 1194 loss: 6.72143287e-05
Iter: 1195 loss: 6.75464835e-05
Iter: 1196 loss: 6.71605e-05
Iter: 1197 loss: 6.70104055e-05
Iter: 1198 loss: 6.73034083e-05
Iter: 1199 loss: 6.69486471e-05
Iter: 1200 loss: 6.69857254e-05
Iter: 1201 loss: 6.68895e-05
Iter: 1202 loss: 6.68539215e-05
Iter: 1203 loss: 6.67459462e-05
Iter: 1204 loss: 6.70155932e-05
Iter: 1205 loss: 6.66863052e-05
Iter: 1206 loss: 6.65521948e-05
Iter: 1207 loss: 6.70438385e-05
Iter: 1208 loss: 6.65197076e-05
Iter: 1209 loss: 6.63995888e-05
Iter: 1210 loss: 6.72097667e-05
Iter: 1211 loss: 6.63885221e-05
Iter: 1212 loss: 6.62926614e-05
Iter: 1213 loss: 6.64270192e-05
Iter: 1214 loss: 6.62462699e-05
Iter: 1215 loss: 6.61104423e-05
Iter: 1216 loss: 6.6079745e-05
Iter: 1217 loss: 6.59926372e-05
Iter: 1218 loss: 6.58552162e-05
Iter: 1219 loss: 6.62278e-05
Iter: 1220 loss: 6.58096542e-05
Iter: 1221 loss: 6.56646444e-05
Iter: 1222 loss: 6.60439837e-05
Iter: 1223 loss: 6.5614935e-05
Iter: 1224 loss: 6.54605828e-05
Iter: 1225 loss: 6.58187346e-05
Iter: 1226 loss: 6.54027099e-05
Iter: 1227 loss: 6.52608942e-05
Iter: 1228 loss: 6.5526081e-05
Iter: 1229 loss: 6.52014205e-05
Iter: 1230 loss: 6.50338916e-05
Iter: 1231 loss: 6.66501583e-05
Iter: 1232 loss: 6.50274233e-05
Iter: 1233 loss: 6.49507565e-05
Iter: 1234 loss: 6.50546863e-05
Iter: 1235 loss: 6.49115173e-05
Iter: 1236 loss: 6.4809421e-05
Iter: 1237 loss: 6.60274673e-05
Iter: 1238 loss: 6.48086279e-05
Iter: 1239 loss: 6.4750915e-05
Iter: 1240 loss: 6.46366607e-05
Iter: 1241 loss: 6.69519577e-05
Iter: 1242 loss: 6.46371482e-05
Iter: 1243 loss: 6.44905158e-05
Iter: 1244 loss: 6.45959e-05
Iter: 1245 loss: 6.44003594e-05
Iter: 1246 loss: 6.42604573e-05
Iter: 1247 loss: 6.44454412e-05
Iter: 1248 loss: 6.41895313e-05
Iter: 1249 loss: 6.40636717e-05
Iter: 1250 loss: 6.5586828e-05
Iter: 1251 loss: 6.40626749e-05
Iter: 1252 loss: 6.39663631e-05
Iter: 1253 loss: 6.42466548e-05
Iter: 1254 loss: 6.39367e-05
Iter: 1255 loss: 6.38477213e-05
Iter: 1256 loss: 6.40051148e-05
Iter: 1257 loss: 6.38092e-05
Iter: 1258 loss: 6.36942859e-05
Iter: 1259 loss: 6.36066e-05
Iter: 1260 loss: 6.35696633e-05
Iter: 1261 loss: 6.34088501e-05
Iter: 1262 loss: 6.39932332e-05
Iter: 1263 loss: 6.33680247e-05
Iter: 1264 loss: 6.32144875e-05
Iter: 1265 loss: 6.38280617e-05
Iter: 1266 loss: 6.31801595e-05
Iter: 1267 loss: 6.30432041e-05
Iter: 1268 loss: 6.32349766e-05
Iter: 1269 loss: 6.29756178e-05
Iter: 1270 loss: 6.30226132e-05
Iter: 1271 loss: 6.29176211e-05
Iter: 1272 loss: 6.28717535e-05
Iter: 1273 loss: 6.28175621e-05
Iter: 1274 loss: 6.2811494e-05
Iter: 1275 loss: 6.27251429e-05
Iter: 1276 loss: 6.26550827e-05
Iter: 1277 loss: 6.26300825e-05
Iter: 1278 loss: 6.24909881e-05
Iter: 1279 loss: 6.26101901e-05
Iter: 1280 loss: 6.24095264e-05
Iter: 1281 loss: 6.22704101e-05
Iter: 1282 loss: 6.26503752e-05
Iter: 1283 loss: 6.22243097e-05
Iter: 1284 loss: 6.21061772e-05
Iter: 1285 loss: 6.3190324e-05
Iter: 1286 loss: 6.21017389e-05
Iter: 1287 loss: 6.20034261e-05
Iter: 1288 loss: 6.22751904e-05
Iter: 1289 loss: 6.1971914e-05
Iter: 1290 loss: 6.18872436e-05
Iter: 1291 loss: 6.21247455e-05
Iter: 1292 loss: 6.186065e-05
Iter: 1293 loss: 6.17587648e-05
Iter: 1294 loss: 6.17490296e-05
Iter: 1295 loss: 6.16731559e-05
Iter: 1296 loss: 6.15342069e-05
Iter: 1297 loss: 6.21120635e-05
Iter: 1298 loss: 6.15033641e-05
Iter: 1299 loss: 6.13851589e-05
Iter: 1300 loss: 6.16482284e-05
Iter: 1301 loss: 6.1339204e-05
Iter: 1302 loss: 6.12081785e-05
Iter: 1303 loss: 6.14116943e-05
Iter: 1304 loss: 6.11477953e-05
Iter: 1305 loss: 6.11623254e-05
Iter: 1306 loss: 6.11093419e-05
Iter: 1307 loss: 6.10618881e-05
Iter: 1308 loss: 6.09469898e-05
Iter: 1309 loss: 6.21907675e-05
Iter: 1310 loss: 6.09356539e-05
Iter: 1311 loss: 6.08282753e-05
Iter: 1312 loss: 6.09153249e-05
Iter: 1313 loss: 6.0763894e-05
Iter: 1314 loss: 6.06514222e-05
Iter: 1315 loss: 6.12963486e-05
Iter: 1316 loss: 6.06354e-05
Iter: 1317 loss: 6.05333917e-05
Iter: 1318 loss: 6.08971532e-05
Iter: 1319 loss: 6.05075948e-05
Iter: 1320 loss: 6.03972148e-05
Iter: 1321 loss: 6.03103217e-05
Iter: 1322 loss: 6.02777909e-05
Iter: 1323 loss: 6.01503416e-05
Iter: 1324 loss: 6.0871047e-05
Iter: 1325 loss: 6.01330357e-05
Iter: 1326 loss: 6.00207422e-05
Iter: 1327 loss: 6.04984889e-05
Iter: 1328 loss: 5.99970881e-05
Iter: 1329 loss: 5.98774168e-05
Iter: 1330 loss: 6.01503e-05
Iter: 1331 loss: 5.98321967e-05
Iter: 1332 loss: 5.97311082e-05
Iter: 1333 loss: 6.02721702e-05
Iter: 1334 loss: 5.97166145e-05
Iter: 1335 loss: 5.96138052e-05
Iter: 1336 loss: 5.96298705e-05
Iter: 1337 loss: 5.95375022e-05
Iter: 1338 loss: 5.94475569e-05
Iter: 1339 loss: 6.00994026e-05
Iter: 1340 loss: 5.94400117e-05
Iter: 1341 loss: 5.93681907e-05
Iter: 1342 loss: 5.97074904e-05
Iter: 1343 loss: 5.93549739e-05
Iter: 1344 loss: 5.92581055e-05
Iter: 1345 loss: 5.95973033e-05
Iter: 1346 loss: 5.92333672e-05
Iter: 1347 loss: 5.91782955e-05
Iter: 1348 loss: 5.90329946e-05
Iter: 1349 loss: 6.00942221e-05
Iter: 1350 loss: 5.90020572e-05
Iter: 1351 loss: 5.8877984e-05
Iter: 1352 loss: 5.95227648e-05
Iter: 1353 loss: 5.8858197e-05
Iter: 1354 loss: 5.87310242e-05
Iter: 1355 loss: 5.89286319e-05
Iter: 1356 loss: 5.86713359e-05
Iter: 1357 loss: 5.85635717e-05
Iter: 1358 loss: 5.94682133e-05
Iter: 1359 loss: 5.85569287e-05
Iter: 1360 loss: 5.8443773e-05
Iter: 1361 loss: 5.86568312e-05
Iter: 1362 loss: 5.83958135e-05
Iter: 1363 loss: 5.8296504e-05
Iter: 1364 loss: 5.84595909e-05
Iter: 1365 loss: 5.82507928e-05
Iter: 1366 loss: 5.81415588e-05
Iter: 1367 loss: 5.82929788e-05
Iter: 1368 loss: 5.80874657e-05
Iter: 1369 loss: 5.79890548e-05
Iter: 1370 loss: 5.91601565e-05
Iter: 1371 loss: 5.79875959e-05
Iter: 1372 loss: 5.78977488e-05
Iter: 1373 loss: 5.793327e-05
Iter: 1374 loss: 5.78351646e-05
Iter: 1375 loss: 5.77434e-05
Iter: 1376 loss: 5.80864835e-05
Iter: 1377 loss: 5.77213577e-05
Iter: 1378 loss: 5.76894126e-05
Iter: 1379 loss: 5.76734637e-05
Iter: 1380 loss: 5.76379825e-05
Iter: 1381 loss: 5.75552694e-05
Iter: 1382 loss: 5.84582704e-05
Iter: 1383 loss: 5.75455124e-05
Iter: 1384 loss: 5.74574515e-05
Iter: 1385 loss: 5.75569647e-05
Iter: 1386 loss: 5.74098813e-05
Iter: 1387 loss: 5.73117904e-05
Iter: 1388 loss: 5.73907892e-05
Iter: 1389 loss: 5.72524114e-05
Iter: 1390 loss: 5.7142508e-05
Iter: 1391 loss: 5.81004315e-05
Iter: 1392 loss: 5.71363344e-05
Iter: 1393 loss: 5.70332923e-05
Iter: 1394 loss: 5.70978e-05
Iter: 1395 loss: 5.69683434e-05
Iter: 1396 loss: 5.68614232e-05
Iter: 1397 loss: 5.68725191e-05
Iter: 1398 loss: 5.67791612e-05
Iter: 1399 loss: 5.66638628e-05
Iter: 1400 loss: 5.79695843e-05
Iter: 1401 loss: 5.66620656e-05
Iter: 1402 loss: 5.65795126e-05
Iter: 1403 loss: 5.71670207e-05
Iter: 1404 loss: 5.65719456e-05
Iter: 1405 loss: 5.64959191e-05
Iter: 1406 loss: 5.6406876e-05
Iter: 1407 loss: 5.63969406e-05
Iter: 1408 loss: 5.62993446e-05
Iter: 1409 loss: 5.73112156e-05
Iter: 1410 loss: 5.62971181e-05
Iter: 1411 loss: 5.62251043e-05
Iter: 1412 loss: 5.69193144e-05
Iter: 1413 loss: 5.62228161e-05
Iter: 1414 loss: 5.6154393e-05
Iter: 1415 loss: 5.64119182e-05
Iter: 1416 loss: 5.6137178e-05
Iter: 1417 loss: 5.60823028e-05
Iter: 1418 loss: 5.60036933e-05
Iter: 1419 loss: 5.60006592e-05
Iter: 1420 loss: 5.591429e-05
Iter: 1421 loss: 5.59569744e-05
Iter: 1422 loss: 5.58562497e-05
Iter: 1423 loss: 5.57519415e-05
Iter: 1424 loss: 5.60414301e-05
Iter: 1425 loss: 5.571773e-05
Iter: 1426 loss: 5.55972583e-05
Iter: 1427 loss: 5.58881729e-05
Iter: 1428 loss: 5.55539118e-05
Iter: 1429 loss: 5.54629041e-05
Iter: 1430 loss: 5.57325693e-05
Iter: 1431 loss: 5.54347644e-05
Iter: 1432 loss: 5.53240607e-05
Iter: 1433 loss: 5.55174702e-05
Iter: 1434 loss: 5.52752535e-05
Iter: 1435 loss: 5.51852572e-05
Iter: 1436 loss: 5.6243669e-05
Iter: 1437 loss: 5.51835074e-05
Iter: 1438 loss: 5.51143421e-05
Iter: 1439 loss: 5.50117256e-05
Iter: 1440 loss: 5.50083787e-05
Iter: 1441 loss: 5.48768367e-05
Iter: 1442 loss: 5.53564241e-05
Iter: 1443 loss: 5.48438802e-05
Iter: 1444 loss: 5.47826712e-05
Iter: 1445 loss: 5.47752788e-05
Iter: 1446 loss: 5.47198433e-05
Iter: 1447 loss: 5.50632976e-05
Iter: 1448 loss: 5.47141208e-05
Iter: 1449 loss: 5.46539777e-05
Iter: 1450 loss: 5.46009032e-05
Iter: 1451 loss: 5.45853036e-05
Iter: 1452 loss: 5.45259572e-05
Iter: 1453 loss: 5.44919276e-05
Iter: 1454 loss: 5.44659924e-05
Iter: 1455 loss: 5.43834503e-05
Iter: 1456 loss: 5.4827975e-05
Iter: 1457 loss: 5.43725146e-05
Iter: 1458 loss: 5.42804846e-05
Iter: 1459 loss: 5.43873066e-05
Iter: 1460 loss: 5.42321504e-05
Iter: 1461 loss: 5.41556365e-05
Iter: 1462 loss: 5.43860951e-05
Iter: 1463 loss: 5.413299e-05
Iter: 1464 loss: 5.40457659e-05
Iter: 1465 loss: 5.41098489e-05
Iter: 1466 loss: 5.39909961e-05
Iter: 1467 loss: 5.38902023e-05
Iter: 1468 loss: 5.40934125e-05
Iter: 1469 loss: 5.38494787e-05
Iter: 1470 loss: 5.37342312e-05
Iter: 1471 loss: 5.40801484e-05
Iter: 1472 loss: 5.37006563e-05
Iter: 1473 loss: 5.35869694e-05
Iter: 1474 loss: 5.3914031e-05
Iter: 1475 loss: 5.35526124e-05
Iter: 1476 loss: 5.34447099e-05
Iter: 1477 loss: 5.40980909e-05
Iter: 1478 loss: 5.34310966e-05
Iter: 1479 loss: 5.33540951e-05
Iter: 1480 loss: 5.37521846e-05
Iter: 1481 loss: 5.33414641e-05
Iter: 1482 loss: 5.32841768e-05
Iter: 1483 loss: 5.32827471e-05
Iter: 1484 loss: 5.32542545e-05
Iter: 1485 loss: 5.31730184e-05
Iter: 1486 loss: 5.35644795e-05
Iter: 1487 loss: 5.31449041e-05
Iter: 1488 loss: 5.30589605e-05
Iter: 1489 loss: 5.37642118e-05
Iter: 1490 loss: 5.30541147e-05
Iter: 1491 loss: 5.29675744e-05
Iter: 1492 loss: 5.30962097e-05
Iter: 1493 loss: 5.29263707e-05
Iter: 1494 loss: 5.28397431e-05
Iter: 1495 loss: 5.28333476e-05
Iter: 1496 loss: 5.27679e-05
Iter: 1497 loss: 5.26674594e-05
Iter: 1498 loss: 5.3119511e-05
Iter: 1499 loss: 5.26483345e-05
Iter: 1500 loss: 5.2563264e-05
Iter: 1501 loss: 5.30054676e-05
Iter: 1502 loss: 5.255008e-05
Iter: 1503 loss: 5.24678544e-05
Iter: 1504 loss: 5.27515331e-05
Iter: 1505 loss: 5.24458083e-05
Iter: 1506 loss: 5.23803756e-05
Iter: 1507 loss: 5.2552321e-05
Iter: 1508 loss: 5.23576455e-05
Iter: 1509 loss: 5.22787e-05
Iter: 1510 loss: 5.22650662e-05
Iter: 1511 loss: 5.22119954e-05
Iter: 1512 loss: 5.21139045e-05
Iter: 1513 loss: 5.2316871e-05
Iter: 1514 loss: 5.20746908e-05
Iter: 1515 loss: 5.21060865e-05
Iter: 1516 loss: 5.20398389e-05
Iter: 1517 loss: 5.20011708e-05
Iter: 1518 loss: 5.19238238e-05
Iter: 1519 loss: 5.33395105e-05
Iter: 1520 loss: 5.19224341e-05
Iter: 1521 loss: 5.18488305e-05
Iter: 1522 loss: 5.200124e-05
Iter: 1523 loss: 5.18190027e-05
Iter: 1524 loss: 5.17531371e-05
Iter: 1525 loss: 5.1930765e-05
Iter: 1526 loss: 5.17318331e-05
Iter: 1527 loss: 5.16578657e-05
Iter: 1528 loss: 5.17116387e-05
Iter: 1529 loss: 5.16119653e-05
Iter: 1530 loss: 5.15219181e-05
Iter: 1531 loss: 5.22776936e-05
Iter: 1532 loss: 5.15166175e-05
Iter: 1533 loss: 5.14559724e-05
Iter: 1534 loss: 5.1457002e-05
Iter: 1535 loss: 5.14068888e-05
Iter: 1536 loss: 5.13192281e-05
Iter: 1537 loss: 5.13320629e-05
Iter: 1538 loss: 5.12528459e-05
Iter: 1539 loss: 5.11460021e-05
Iter: 1540 loss: 5.16477376e-05
Iter: 1541 loss: 5.1126277e-05
Iter: 1542 loss: 5.10418686e-05
Iter: 1543 loss: 5.14739804e-05
Iter: 1544 loss: 5.10296159e-05
Iter: 1545 loss: 5.09434722e-05
Iter: 1546 loss: 5.1089035e-05
Iter: 1547 loss: 5.09057027e-05
Iter: 1548 loss: 5.08362e-05
Iter: 1549 loss: 5.15507418e-05
Iter: 1550 loss: 5.0834391e-05
Iter: 1551 loss: 5.07817786e-05
Iter: 1552 loss: 5.12316619e-05
Iter: 1553 loss: 5.07780569e-05
Iter: 1554 loss: 5.0716073e-05
Iter: 1555 loss: 5.06881188e-05
Iter: 1556 loss: 5.06565084e-05
Iter: 1557 loss: 5.06014949e-05
Iter: 1558 loss: 5.05595235e-05
Iter: 1559 loss: 5.05414428e-05
Iter: 1560 loss: 5.04553245e-05
Iter: 1561 loss: 5.04996024e-05
Iter: 1562 loss: 5.03973388e-05
Iter: 1563 loss: 5.03094925e-05
Iter: 1564 loss: 5.16232831e-05
Iter: 1565 loss: 5.03091978e-05
Iter: 1566 loss: 5.02483308e-05
Iter: 1567 loss: 5.0229246e-05
Iter: 1568 loss: 5.01937102e-05
Iter: 1569 loss: 5.01063478e-05
Iter: 1570 loss: 5.04120471e-05
Iter: 1571 loss: 5.00838214e-05
Iter: 1572 loss: 4.99865782e-05
Iter: 1573 loss: 5.05260832e-05
Iter: 1574 loss: 4.99724774e-05
Iter: 1575 loss: 4.99047237e-05
Iter: 1576 loss: 4.98848567e-05
Iter: 1577 loss: 4.98441404e-05
Iter: 1578 loss: 4.97513e-05
Iter: 1579 loss: 4.997422e-05
Iter: 1580 loss: 4.97176588e-05
Iter: 1581 loss: 4.96258399e-05
Iter: 1582 loss: 4.99843154e-05
Iter: 1583 loss: 4.96045213e-05
Iter: 1584 loss: 4.9518545e-05
Iter: 1585 loss: 4.97230649e-05
Iter: 1586 loss: 4.94878404e-05
Iter: 1587 loss: 4.94646156e-05
Iter: 1588 loss: 4.94387859e-05
Iter: 1589 loss: 4.94036321e-05
Iter: 1590 loss: 4.93442967e-05
Iter: 1591 loss: 4.93439366e-05
Iter: 1592 loss: 4.92794934e-05
Iter: 1593 loss: 4.93158368e-05
Iter: 1594 loss: 4.92381587e-05
Iter: 1595 loss: 4.91506507e-05
Iter: 1596 loss: 4.93965053e-05
Iter: 1597 loss: 4.91219835e-05
Iter: 1598 loss: 4.90506209e-05
Iter: 1599 loss: 4.9045062e-05
Iter: 1600 loss: 4.89909471e-05
Iter: 1601 loss: 4.89099839e-05
Iter: 1602 loss: 4.94134947e-05
Iter: 1603 loss: 4.88999285e-05
Iter: 1604 loss: 4.88191072e-05
Iter: 1605 loss: 4.90231614e-05
Iter: 1606 loss: 4.87913712e-05
Iter: 1607 loss: 4.86949357e-05
Iter: 1608 loss: 4.88108781e-05
Iter: 1609 loss: 4.86439239e-05
Iter: 1610 loss: 4.8561029e-05
Iter: 1611 loss: 4.86215e-05
Iter: 1612 loss: 4.8510803e-05
Iter: 1613 loss: 4.84311167e-05
Iter: 1614 loss: 4.84311131e-05
Iter: 1615 loss: 4.83641343e-05
Iter: 1616 loss: 4.84378252e-05
Iter: 1617 loss: 4.83268159e-05
Iter: 1618 loss: 4.82600735e-05
Iter: 1619 loss: 4.84126795e-05
Iter: 1620 loss: 4.82350406e-05
Iter: 1621 loss: 4.82010219e-05
Iter: 1622 loss: 4.81910356e-05
Iter: 1623 loss: 4.81608295e-05
Iter: 1624 loss: 4.80796662e-05
Iter: 1625 loss: 4.86359204e-05
Iter: 1626 loss: 4.80619419e-05
Iter: 1627 loss: 4.79813534e-05
Iter: 1628 loss: 4.82733521e-05
Iter: 1629 loss: 4.79613955e-05
Iter: 1630 loss: 4.7880807e-05
Iter: 1631 loss: 4.78727161e-05
Iter: 1632 loss: 4.78126967e-05
Iter: 1633 loss: 4.77250542e-05
Iter: 1634 loss: 4.88540391e-05
Iter: 1635 loss: 4.77239919e-05
Iter: 1636 loss: 4.76531422e-05
Iter: 1637 loss: 4.7785943e-05
Iter: 1638 loss: 4.76225978e-05
Iter: 1639 loss: 4.75421257e-05
Iter: 1640 loss: 4.76094647e-05
Iter: 1641 loss: 4.74937333e-05
Iter: 1642 loss: 4.74106273e-05
Iter: 1643 loss: 4.76876594e-05
Iter: 1644 loss: 4.73885084e-05
Iter: 1645 loss: 4.73173641e-05
Iter: 1646 loss: 4.77954345e-05
Iter: 1647 loss: 4.73105974e-05
Iter: 1648 loss: 4.72479114e-05
Iter: 1649 loss: 4.72155225e-05
Iter: 1650 loss: 4.7186e-05
Iter: 1651 loss: 4.71016465e-05
Iter: 1652 loss: 4.73533073e-05
Iter: 1653 loss: 4.70759041e-05
Iter: 1654 loss: 4.70349696e-05
Iter: 1655 loss: 4.70291707e-05
Iter: 1656 loss: 4.69731713e-05
Iter: 1657 loss: 4.69886727e-05
Iter: 1658 loss: 4.69319602e-05
Iter: 1659 loss: 4.68862345e-05
Iter: 1660 loss: 4.7002577e-05
Iter: 1661 loss: 4.68702638e-05
Iter: 1662 loss: 4.68255239e-05
Iter: 1663 loss: 4.67572027e-05
Iter: 1664 loss: 4.67559694e-05
Iter: 1665 loss: 4.66660385e-05
Iter: 1666 loss: 4.71244712e-05
Iter: 1667 loss: 4.66518431e-05
Iter: 1668 loss: 4.65823032e-05
Iter: 1669 loss: 4.66665588e-05
Iter: 1670 loss: 4.65448975e-05
Iter: 1671 loss: 4.64612858e-05
Iter: 1672 loss: 4.65533049e-05
Iter: 1673 loss: 4.64160657e-05
Iter: 1674 loss: 4.6340203e-05
Iter: 1675 loss: 4.72820138e-05
Iter: 1676 loss: 4.63394172e-05
Iter: 1677 loss: 4.62788303e-05
Iter: 1678 loss: 4.64069381e-05
Iter: 1679 loss: 4.62549215e-05
Iter: 1680 loss: 4.61896707e-05
Iter: 1681 loss: 4.62505377e-05
Iter: 1682 loss: 4.6152596e-05
Iter: 1683 loss: 4.60835217e-05
Iter: 1684 loss: 4.64256518e-05
Iter: 1685 loss: 4.60719639e-05
Iter: 1686 loss: 4.60097654e-05
Iter: 1687 loss: 4.61285126e-05
Iter: 1688 loss: 4.5983943e-05
Iter: 1689 loss: 4.59274925e-05
Iter: 1690 loss: 4.65264784e-05
Iter: 1691 loss: 4.5926452e-05
Iter: 1692 loss: 4.58647046e-05
Iter: 1693 loss: 4.60764859e-05
Iter: 1694 loss: 4.58488357e-05
Iter: 1695 loss: 4.58209397e-05
Iter: 1696 loss: 4.5751196e-05
Iter: 1697 loss: 4.63640354e-05
Iter: 1698 loss: 4.57397873e-05
Iter: 1699 loss: 4.56687158e-05
Iter: 1700 loss: 4.6163259e-05
Iter: 1701 loss: 4.56619709e-05
Iter: 1702 loss: 4.55991976e-05
Iter: 1703 loss: 4.59914554e-05
Iter: 1704 loss: 4.55917907e-05
Iter: 1705 loss: 4.55379923e-05
Iter: 1706 loss: 4.55499248e-05
Iter: 1707 loss: 4.54976034e-05
Iter: 1708 loss: 4.54318251e-05
Iter: 1709 loss: 4.54985202e-05
Iter: 1710 loss: 4.5393972e-05
Iter: 1711 loss: 4.53083267e-05
Iter: 1712 loss: 4.56567213e-05
Iter: 1713 loss: 4.5288929e-05
Iter: 1714 loss: 4.52195163e-05
Iter: 1715 loss: 4.52604036e-05
Iter: 1716 loss: 4.5174791e-05
Iter: 1717 loss: 4.50876e-05
Iter: 1718 loss: 4.53338143e-05
Iter: 1719 loss: 4.50601547e-05
Iter: 1720 loss: 4.49819636e-05
Iter: 1721 loss: 4.59252042e-05
Iter: 1722 loss: 4.49807849e-05
Iter: 1723 loss: 4.49301806e-05
Iter: 1724 loss: 4.50466832e-05
Iter: 1725 loss: 4.49116269e-05
Iter: 1726 loss: 4.48702558e-05
Iter: 1727 loss: 4.48694482e-05
Iter: 1728 loss: 4.48400206e-05
Iter: 1729 loss: 4.47808634e-05
Iter: 1730 loss: 4.58569921e-05
Iter: 1731 loss: 4.47801431e-05
Iter: 1732 loss: 4.47275306e-05
Iter: 1733 loss: 4.49563304e-05
Iter: 1734 loss: 4.47180937e-05
Iter: 1735 loss: 4.46701233e-05
Iter: 1736 loss: 4.46157355e-05
Iter: 1737 loss: 4.46091581e-05
Iter: 1738 loss: 4.45250553e-05
Iter: 1739 loss: 4.49308354e-05
Iter: 1740 loss: 4.45103578e-05
Iter: 1741 loss: 4.44420402e-05
Iter: 1742 loss: 4.45687838e-05
Iter: 1743 loss: 4.44123179e-05
Iter: 1744 loss: 4.43396057e-05
Iter: 1745 loss: 4.51777851e-05
Iter: 1746 loss: 4.43389072e-05
Iter: 1747 loss: 4.42916935e-05
Iter: 1748 loss: 4.42495511e-05
Iter: 1749 loss: 4.42384771e-05
Iter: 1750 loss: 4.41759221e-05
Iter: 1751 loss: 4.45738333e-05
Iter: 1752 loss: 4.41693519e-05
Iter: 1753 loss: 4.41116827e-05
Iter: 1754 loss: 4.41257653e-05
Iter: 1755 loss: 4.40690237e-05
Iter: 1756 loss: 4.40007789e-05
Iter: 1757 loss: 4.41171505e-05
Iter: 1758 loss: 4.39700161e-05
Iter: 1759 loss: 4.39593787e-05
Iter: 1760 loss: 4.39322794e-05
Iter: 1761 loss: 4.38942807e-05
Iter: 1762 loss: 4.38224961e-05
Iter: 1763 loss: 4.54579713e-05
Iter: 1764 loss: 4.38221978e-05
Iter: 1765 loss: 4.37589151e-05
Iter: 1766 loss: 4.40872827e-05
Iter: 1767 loss: 4.3748445e-05
Iter: 1768 loss: 4.36999035e-05
Iter: 1769 loss: 4.36605842e-05
Iter: 1770 loss: 4.36449263e-05
Iter: 1771 loss: 4.35715265e-05
Iter: 1772 loss: 4.37225426e-05
Iter: 1773 loss: 4.35417278e-05
Iter: 1774 loss: 4.34706e-05
Iter: 1775 loss: 4.38815769e-05
Iter: 1776 loss: 4.3462238e-05
Iter: 1777 loss: 4.33953464e-05
Iter: 1778 loss: 4.3623746e-05
Iter: 1779 loss: 4.33781534e-05
Iter: 1780 loss: 4.33100213e-05
Iter: 1781 loss: 4.34547037e-05
Iter: 1782 loss: 4.32835477e-05
Iter: 1783 loss: 4.32292145e-05
Iter: 1784 loss: 4.36371411e-05
Iter: 1785 loss: 4.32249071e-05
Iter: 1786 loss: 4.31853186e-05
Iter: 1787 loss: 4.31704248e-05
Iter: 1788 loss: 4.31479712e-05
Iter: 1789 loss: 4.30838118e-05
Iter: 1790 loss: 4.31944536e-05
Iter: 1791 loss: 4.30554428e-05
Iter: 1792 loss: 4.29931861e-05
Iter: 1793 loss: 4.32484558e-05
Iter: 1794 loss: 4.29799366e-05
Iter: 1795 loss: 4.2944037e-05
Iter: 1796 loss: 4.2937234e-05
Iter: 1797 loss: 4.29145111e-05
Iter: 1798 loss: 4.2862077e-05
Iter: 1799 loss: 4.35344118e-05
Iter: 1800 loss: 4.28584062e-05
Iter: 1801 loss: 4.28017374e-05
Iter: 1802 loss: 4.2848078e-05
Iter: 1803 loss: 4.27676e-05
Iter: 1804 loss: 4.27058403e-05
Iter: 1805 loss: 4.32249326e-05
Iter: 1806 loss: 4.27019477e-05
Iter: 1807 loss: 4.26526167e-05
Iter: 1808 loss: 4.26904553e-05
Iter: 1809 loss: 4.26223087e-05
Iter: 1810 loss: 4.25655053e-05
Iter: 1811 loss: 4.25688704e-05
Iter: 1812 loss: 4.25199978e-05
Iter: 1813 loss: 4.24480677e-05
Iter: 1814 loss: 4.27035338e-05
Iter: 1815 loss: 4.2429383e-05
Iter: 1816 loss: 4.23718084e-05
Iter: 1817 loss: 4.23718229e-05
Iter: 1818 loss: 4.23342499e-05
Iter: 1819 loss: 4.22870471e-05
Iter: 1820 loss: 4.22835656e-05
Iter: 1821 loss: 4.22270459e-05
Iter: 1822 loss: 4.25054968e-05
Iter: 1823 loss: 4.22162957e-05
Iter: 1824 loss: 4.21542572e-05
Iter: 1825 loss: 4.22967641e-05
Iter: 1826 loss: 4.21315272e-05
Iter: 1827 loss: 4.20926808e-05
Iter: 1828 loss: 4.20928882e-05
Iter: 1829 loss: 4.20511751e-05
Iter: 1830 loss: 4.21340737e-05
Iter: 1831 loss: 4.20339857e-05
Iter: 1832 loss: 4.20003453e-05
Iter: 1833 loss: 4.19250973e-05
Iter: 1834 loss: 4.28878448e-05
Iter: 1835 loss: 4.19199459e-05
Iter: 1836 loss: 4.18556556e-05
Iter: 1837 loss: 4.22638186e-05
Iter: 1838 loss: 4.1848205e-05
Iter: 1839 loss: 4.17902629e-05
Iter: 1840 loss: 4.19628486e-05
Iter: 1841 loss: 4.17731644e-05
Iter: 1842 loss: 4.17159899e-05
Iter: 1843 loss: 4.20180804e-05
Iter: 1844 loss: 4.17070405e-05
Iter: 1845 loss: 4.16622206e-05
Iter: 1846 loss: 4.1682124e-05
Iter: 1847 loss: 4.16314288e-05
Iter: 1848 loss: 4.15622089e-05
Iter: 1849 loss: 4.16050643e-05
Iter: 1850 loss: 4.15173054e-05
Iter: 1851 loss: 4.14449169e-05
Iter: 1852 loss: 4.18205236e-05
Iter: 1853 loss: 4.14338574e-05
Iter: 1854 loss: 4.13910166e-05
Iter: 1855 loss: 4.20136712e-05
Iter: 1856 loss: 4.13909438e-05
Iter: 1857 loss: 4.13519738e-05
Iter: 1858 loss: 4.12864465e-05
Iter: 1859 loss: 4.12866648e-05
Iter: 1860 loss: 4.12189802e-05
Iter: 1861 loss: 4.16100229e-05
Iter: 1862 loss: 4.12104564e-05
Iter: 1863 loss: 4.12163645e-05
Iter: 1864 loss: 4.11882575e-05
Iter: 1865 loss: 4.1167903e-05
Iter: 1866 loss: 4.11050787e-05
Iter: 1867 loss: 4.12687659e-05
Iter: 1868 loss: 4.10708781e-05
Iter: 1869 loss: 4.10095381e-05
Iter: 1870 loss: 4.14420138e-05
Iter: 1871 loss: 4.10037319e-05
Iter: 1872 loss: 4.0945517e-05
Iter: 1873 loss: 4.10156026e-05
Iter: 1874 loss: 4.09154745e-05
Iter: 1875 loss: 4.08552223e-05
Iter: 1876 loss: 4.09111599e-05
Iter: 1877 loss: 4.08214109e-05
Iter: 1878 loss: 4.07518601e-05
Iter: 1879 loss: 4.12302543e-05
Iter: 1880 loss: 4.07454208e-05
Iter: 1881 loss: 4.06964646e-05
Iter: 1882 loss: 4.0784591e-05
Iter: 1883 loss: 4.06758772e-05
Iter: 1884 loss: 4.06197796e-05
Iter: 1885 loss: 4.0904255e-05
Iter: 1886 loss: 4.06109175e-05
Iter: 1887 loss: 4.05618739e-05
Iter: 1888 loss: 4.07064872e-05
Iter: 1889 loss: 4.05472965e-05
Iter: 1890 loss: 4.05035389e-05
Iter: 1891 loss: 4.05137325e-05
Iter: 1892 loss: 4.0471954e-05
Iter: 1893 loss: 4.04166858e-05
Iter: 1894 loss: 4.0776984e-05
Iter: 1895 loss: 4.04113052e-05
Iter: 1896 loss: 4.0363353e-05
Iter: 1897 loss: 4.05959072e-05
Iter: 1898 loss: 4.03555969e-05
Iter: 1899 loss: 4.03252925e-05
Iter: 1900 loss: 4.03247868e-05
Iter: 1901 loss: 4.03014856e-05
Iter: 1902 loss: 4.0245468e-05
Iter: 1903 loss: 4.0881343e-05
Iter: 1904 loss: 4.02399237e-05
Iter: 1905 loss: 4.01863617e-05
Iter: 1906 loss: 4.01468969e-05
Iter: 1907 loss: 4.0129e-05
Iter: 1908 loss: 4.00478821e-05
Iter: 1909 loss: 4.03609447e-05
Iter: 1910 loss: 4.00291901e-05
Iter: 1911 loss: 3.9958144e-05
Iter: 1912 loss: 4.02836085e-05
Iter: 1913 loss: 3.99453129e-05
Iter: 1914 loss: 3.9890052e-05
Iter: 1915 loss: 4.03245067e-05
Iter: 1916 loss: 3.98855846e-05
Iter: 1917 loss: 3.98339071e-05
Iter: 1918 loss: 3.98781849e-05
Iter: 1919 loss: 3.98039083e-05
Iter: 1920 loss: 3.97466029e-05
Iter: 1921 loss: 3.99601522e-05
Iter: 1922 loss: 3.97332697e-05
Iter: 1923 loss: 3.96759933e-05
Iter: 1924 loss: 3.98746306e-05
Iter: 1925 loss: 3.96612595e-05
Iter: 1926 loss: 3.96079668e-05
Iter: 1927 loss: 3.96591204e-05
Iter: 1928 loss: 3.95770257e-05
Iter: 1929 loss: 3.95183233e-05
Iter: 1930 loss: 3.98559e-05
Iter: 1931 loss: 3.95113857e-05
Iter: 1932 loss: 3.94862072e-05
Iter: 1933 loss: 3.94827948e-05
Iter: 1934 loss: 3.94578456e-05
Iter: 1935 loss: 3.94517301e-05
Iter: 1936 loss: 3.94368035e-05
Iter: 1937 loss: 3.94063827e-05
Iter: 1938 loss: 3.9392602e-05
Iter: 1939 loss: 3.93771879e-05
Iter: 1940 loss: 3.93257578e-05
Iter: 1941 loss: 3.93299852e-05
Iter: 1942 loss: 3.92862712e-05
Iter: 1943 loss: 3.92370275e-05
Iter: 1944 loss: 3.93380105e-05
Iter: 1945 loss: 3.92168877e-05
Iter: 1946 loss: 3.91499634e-05
Iter: 1947 loss: 3.9245213e-05
Iter: 1948 loss: 3.91172689e-05
Iter: 1949 loss: 3.90502209e-05
Iter: 1950 loss: 3.9274757e-05
Iter: 1951 loss: 3.90315072e-05
Iter: 1952 loss: 3.89679117e-05
Iter: 1953 loss: 3.90811e-05
Iter: 1954 loss: 3.89407141e-05
Iter: 1955 loss: 3.88790322e-05
Iter: 1956 loss: 3.94070594e-05
Iter: 1957 loss: 3.8875769e-05
Iter: 1958 loss: 3.88244953e-05
Iter: 1959 loss: 3.90239147e-05
Iter: 1960 loss: 3.88128319e-05
Iter: 1961 loss: 3.87616237e-05
Iter: 1962 loss: 3.87901237e-05
Iter: 1963 loss: 3.87289911e-05
Iter: 1964 loss: 3.86848951e-05
Iter: 1965 loss: 3.86838219e-05
Iter: 1966 loss: 3.86535285e-05
Iter: 1967 loss: 3.89807246e-05
Iter: 1968 loss: 3.86533102e-05
Iter: 1969 loss: 3.86337088e-05
Iter: 1970 loss: 3.8580496e-05
Iter: 1971 loss: 3.89092602e-05
Iter: 1972 loss: 3.8567021e-05
Iter: 1973 loss: 3.85001913e-05
Iter: 1974 loss: 3.89560373e-05
Iter: 1975 loss: 3.8493381e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi2.4
+ date
Sun Nov  8 00:42:02 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2/300_100_100_100_1 --function f1 --psi -2 --phi 2.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa08df1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa09fba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa08ff730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa0920d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa092b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa092ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa08500d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa08729d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa0872730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa07ea9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa0718950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa071e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa071e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa06eb620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa06eb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa07746a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa07867b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9bcc26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9bc89950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa0786598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafa07b4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9bd16510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9bbaf620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9bbda6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9bbda620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9bc13620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9bb39598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9bc65840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9bc65378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9bc3e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9bb71598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9ba887b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9ba88a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9ba83730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9ba83840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf9ba839d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.05100238
test_loss: 0.056135483
train_loss: 0.031129224
test_loss: 0.035140425
train_loss: 0.02620966
test_loss: 0.029296553
train_loss: 0.020013822
test_loss: 0.026647627
train_loss: 0.01821534
test_loss: 0.022129992
train_loss: 0.016754642
test_loss: 0.020114813
train_loss: 0.018966697
test_loss: 0.020147571
train_loss: 0.016643822
test_loss: 0.019976206
train_loss: 0.016962152
test_loss: 0.017883642
train_loss: 0.017165048
test_loss: 0.018571205
train_loss: 0.014193802
test_loss: 0.01886996
train_loss: 0.015812106
test_loss: 0.019151742
train_loss: 0.013705894
test_loss: 0.01632581
train_loss: 0.012485875
test_loss: 0.015985915
train_loss: 0.014573376
test_loss: 0.01670944
train_loss: 0.012492585
test_loss: 0.015645351
train_loss: 0.012184973
test_loss: 0.016104318
train_loss: 0.013673403
test_loss: 0.016589757
train_loss: 0.012501151
test_loss: 0.01679069
train_loss: 0.012587994
test_loss: 0.015854781
train_loss: 0.014107514
test_loss: 0.015869772
train_loss: 0.011526722
test_loss: 0.01606678
train_loss: 0.012160312
test_loss: 0.015721908
train_loss: 0.011441699
test_loss: 0.015108543
train_loss: 0.011822748
test_loss: 0.015305974
train_loss: 0.012742315
test_loss: 0.01561778
train_loss: 0.011069484
test_loss: 0.015418959
train_loss: 0.012034114
test_loss: 0.014610556
train_loss: 0.013846282
test_loss: 0.01498714
train_loss: 0.01103663
test_loss: 0.014212072
train_loss: 0.013052924
test_loss: 0.014362293
train_loss: 0.012809695
test_loss: 0.014331909
train_loss: 0.009822051
test_loss: 0.013867704
train_loss: 0.010590905
test_loss: 0.013851772
train_loss: 0.011603855
test_loss: 0.01513772
train_loss: 0.011304064
test_loss: 0.014596664
train_loss: 0.0113117
test_loss: 0.014421493
train_loss: 0.011223778
test_loss: 0.014093673
train_loss: 0.011800358
test_loss: 0.01407142
train_loss: 0.010959096
test_loss: 0.014064099
train_loss: 0.010675201
test_loss: 0.014282586
train_loss: 0.0101093715
test_loss: 0.014333552
train_loss: 0.0127770705
test_loss: 0.014190116
train_loss: 0.0099132005
test_loss: 0.013820742
train_loss: 0.010032073
test_loss: 0.014301814
train_loss: 0.009541788
test_loss: 0.015317154
train_loss: 0.01255774
test_loss: 0.014650492
train_loss: 0.0110900495
test_loss: 0.014521738
train_loss: 0.010358691
test_loss: 0.0133807035
train_loss: 0.009302273
test_loss: 0.01318648
train_loss: 0.010407384
test_loss: 0.013186064
train_loss: 0.010603869
test_loss: 0.013306549
train_loss: 0.01000312
test_loss: 0.01379754
train_loss: 0.009411279
test_loss: 0.013777249
train_loss: 0.0095367
test_loss: 0.014046303
train_loss: 0.010266204
test_loss: 0.0135751255
train_loss: 0.010770543
test_loss: 0.014489638
train_loss: 0.009746478
test_loss: 0.012793186
train_loss: 0.009694018
test_loss: 0.013913251
train_loss: 0.009596471
test_loss: 0.013538363
train_loss: 0.009987289
test_loss: 0.013696071
train_loss: 0.010770546
test_loss: 0.012689826
train_loss: 0.009457375
test_loss: 0.012547941
train_loss: 0.008876994
test_loss: 0.013605643
train_loss: 0.00971891
test_loss: 0.012541319
train_loss: 0.009424956
test_loss: 0.013415147
train_loss: 0.010137691
test_loss: 0.013748456
train_loss: 0.008423973
test_loss: 0.012717955
train_loss: 0.010665565
test_loss: 0.012971349
train_loss: 0.010043996
test_loss: 0.012854091
train_loss: 0.009361706
test_loss: 0.013881095
train_loss: 0.009223439
test_loss: 0.013073492
train_loss: 0.009062645
test_loss: 0.013874715
train_loss: 0.008442947
test_loss: 0.012553127
train_loss: 0.009082745
test_loss: 0.012878045
train_loss: 0.009393
test_loss: 0.014119859
train_loss: 0.0096835485
test_loss: 0.012829212
train_loss: 0.009422842
test_loss: 0.012355654
train_loss: 0.008294325
test_loss: 0.012803585
train_loss: 0.009724903
test_loss: 0.012316151
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi2.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi2.4/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff63ec891e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff63ed587b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff63ecc8bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff63ec45730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff63ec45a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff63ec45d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff600ef96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff600ea67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff600e901e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff600e5ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff600e1a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff600ea6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff600e160d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc6d0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc6bdb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc63c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc63cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc665378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc630840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc5d5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc5d59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc583b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc5b3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc5676a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc567158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc50b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc4cb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc4ed268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc486158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc4ad950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc44a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc4677b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc467b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc429840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc4298c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5dc3d5ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000140117656
Iter: 2 loss: 0.00147721684
Iter: 3 loss: 0.000125153034
Iter: 4 loss: 0.000118348849
Iter: 5 loss: 0.000111596266
Iter: 6 loss: 0.000110172667
Iter: 7 loss: 0.000104036728
Iter: 8 loss: 0.000120040757
Iter: 9 loss: 0.00010198298
Iter: 10 loss: 9.80350378e-05
Iter: 11 loss: 9.26824578e-05
Iter: 12 loss: 9.24137421e-05
Iter: 13 loss: 8.43307134e-05
Iter: 14 loss: 8.92845565e-05
Iter: 15 loss: 7.91484781e-05
Iter: 16 loss: 7.14134658e-05
Iter: 17 loss: 0.00010987241
Iter: 18 loss: 7.01045647e-05
Iter: 19 loss: 6.50688235e-05
Iter: 20 loss: 8.00028938e-05
Iter: 21 loss: 6.35080069e-05
Iter: 22 loss: 5.93051591e-05
Iter: 23 loss: 6.5702654e-05
Iter: 24 loss: 5.73131583e-05
Iter: 25 loss: 5.37026899e-05
Iter: 26 loss: 7.76757079e-05
Iter: 27 loss: 5.33326202e-05
Iter: 28 loss: 5.1176281e-05
Iter: 29 loss: 5.56138693e-05
Iter: 30 loss: 5.03127885e-05
Iter: 31 loss: 4.78762959e-05
Iter: 32 loss: 5.18418e-05
Iter: 33 loss: 4.67576247e-05
Iter: 34 loss: 4.48914252e-05
Iter: 35 loss: 6.07917464e-05
Iter: 36 loss: 4.47861385e-05
Iter: 37 loss: 4.32818415e-05
Iter: 38 loss: 4.40323856e-05
Iter: 39 loss: 4.2275351e-05
Iter: 40 loss: 4.33540736e-05
Iter: 41 loss: 4.18959462e-05
Iter: 42 loss: 4.13894741e-05
Iter: 43 loss: 4.03719641e-05
Iter: 44 loss: 5.9367514e-05
Iter: 45 loss: 4.03576232e-05
Iter: 46 loss: 3.95547177e-05
Iter: 47 loss: 4.11687433e-05
Iter: 48 loss: 3.92253205e-05
Iter: 49 loss: 3.83765946e-05
Iter: 50 loss: 4.11467081e-05
Iter: 51 loss: 3.81369064e-05
Iter: 52 loss: 3.75311174e-05
Iter: 53 loss: 3.67564498e-05
Iter: 54 loss: 3.67013054e-05
Iter: 55 loss: 3.59092883e-05
Iter: 56 loss: 3.59012047e-05
Iter: 57 loss: 3.54671647e-05
Iter: 58 loss: 3.55507655e-05
Iter: 59 loss: 3.51443232e-05
Iter: 60 loss: 3.4332159e-05
Iter: 61 loss: 3.38447026e-05
Iter: 62 loss: 3.35104e-05
Iter: 63 loss: 3.29315371e-05
Iter: 64 loss: 3.29140639e-05
Iter: 65 loss: 3.24771681e-05
Iter: 66 loss: 3.17168378e-05
Iter: 67 loss: 3.17177582e-05
Iter: 68 loss: 3.10385331e-05
Iter: 69 loss: 3.1033378e-05
Iter: 70 loss: 3.06854708e-05
Iter: 71 loss: 3.03866454e-05
Iter: 72 loss: 3.02918852e-05
Iter: 73 loss: 3.07260743e-05
Iter: 74 loss: 3.00830543e-05
Iter: 75 loss: 2.99105195e-05
Iter: 76 loss: 2.95278987e-05
Iter: 77 loss: 3.51271519e-05
Iter: 78 loss: 2.95102982e-05
Iter: 79 loss: 2.92574241e-05
Iter: 80 loss: 2.94713536e-05
Iter: 81 loss: 2.91074393e-05
Iter: 82 loss: 2.87194016e-05
Iter: 83 loss: 3.2829812e-05
Iter: 84 loss: 2.87106268e-05
Iter: 85 loss: 2.83957e-05
Iter: 86 loss: 2.8272385e-05
Iter: 87 loss: 2.81028661e-05
Iter: 88 loss: 2.77277e-05
Iter: 89 loss: 2.98645937e-05
Iter: 90 loss: 2.767767e-05
Iter: 91 loss: 2.73809364e-05
Iter: 92 loss: 2.77771505e-05
Iter: 93 loss: 2.72321558e-05
Iter: 94 loss: 2.69671546e-05
Iter: 95 loss: 2.69790871e-05
Iter: 96 loss: 2.67573796e-05
Iter: 97 loss: 2.64142818e-05
Iter: 98 loss: 2.84197613e-05
Iter: 99 loss: 2.6369933e-05
Iter: 100 loss: 2.61031255e-05
Iter: 101 loss: 2.65026629e-05
Iter: 102 loss: 2.5974754e-05
Iter: 103 loss: 2.56376807e-05
Iter: 104 loss: 2.62861504e-05
Iter: 105 loss: 2.549493e-05
Iter: 106 loss: 2.53372637e-05
Iter: 107 loss: 2.53120561e-05
Iter: 108 loss: 2.50962476e-05
Iter: 109 loss: 2.57782449e-05
Iter: 110 loss: 2.50350276e-05
Iter: 111 loss: 2.49404493e-05
Iter: 112 loss: 2.47650169e-05
Iter: 113 loss: 2.87802977e-05
Iter: 114 loss: 2.47649932e-05
Iter: 115 loss: 2.45925294e-05
Iter: 116 loss: 2.48510205e-05
Iter: 117 loss: 2.45095325e-05
Iter: 118 loss: 2.43108589e-05
Iter: 119 loss: 2.66186144e-05
Iter: 120 loss: 2.4308978e-05
Iter: 121 loss: 2.41911657e-05
Iter: 122 loss: 2.42128917e-05
Iter: 123 loss: 2.41030903e-05
Iter: 124 loss: 2.39618421e-05
Iter: 125 loss: 2.41582e-05
Iter: 126 loss: 2.3890545e-05
Iter: 127 loss: 2.37062595e-05
Iter: 128 loss: 2.42215574e-05
Iter: 129 loss: 2.36466949e-05
Iter: 130 loss: 2.34549589e-05
Iter: 131 loss: 2.3892253e-05
Iter: 132 loss: 2.33836872e-05
Iter: 133 loss: 2.32654093e-05
Iter: 134 loss: 2.35560983e-05
Iter: 135 loss: 2.32225721e-05
Iter: 136 loss: 2.30549958e-05
Iter: 137 loss: 2.29870493e-05
Iter: 138 loss: 2.28966564e-05
Iter: 139 loss: 2.28077588e-05
Iter: 140 loss: 2.27929013e-05
Iter: 141 loss: 2.27758828e-05
Iter: 142 loss: 2.27531818e-05
Iter: 143 loss: 2.27296841e-05
Iter: 144 loss: 2.26510092e-05
Iter: 145 loss: 2.26785833e-05
Iter: 146 loss: 2.25767326e-05
Iter: 147 loss: 2.24530449e-05
Iter: 148 loss: 2.31806298e-05
Iter: 149 loss: 2.24370051e-05
Iter: 150 loss: 2.23386287e-05
Iter: 151 loss: 2.26494121e-05
Iter: 152 loss: 2.23107163e-05
Iter: 153 loss: 2.21827686e-05
Iter: 154 loss: 2.23951647e-05
Iter: 155 loss: 2.21248683e-05
Iter: 156 loss: 2.20132752e-05
Iter: 157 loss: 2.2186865e-05
Iter: 158 loss: 2.19601316e-05
Iter: 159 loss: 2.18616515e-05
Iter: 160 loss: 2.20792936e-05
Iter: 161 loss: 2.18239e-05
Iter: 162 loss: 2.17089255e-05
Iter: 163 loss: 2.22865219e-05
Iter: 164 loss: 2.16893823e-05
Iter: 165 loss: 2.1604068e-05
Iter: 166 loss: 2.15497348e-05
Iter: 167 loss: 2.15144901e-05
Iter: 168 loss: 2.14108477e-05
Iter: 169 loss: 2.23473889e-05
Iter: 170 loss: 2.14052816e-05
Iter: 171 loss: 2.13278399e-05
Iter: 172 loss: 2.13303538e-05
Iter: 173 loss: 2.12661362e-05
Iter: 174 loss: 2.12922132e-05
Iter: 175 loss: 2.12248942e-05
Iter: 176 loss: 2.11733986e-05
Iter: 177 loss: 2.10658745e-05
Iter: 178 loss: 2.2844597e-05
Iter: 179 loss: 2.10640228e-05
Iter: 180 loss: 2.10110629e-05
Iter: 181 loss: 2.10405324e-05
Iter: 182 loss: 2.09776954e-05
Iter: 183 loss: 2.08935235e-05
Iter: 184 loss: 2.09761783e-05
Iter: 185 loss: 2.08458205e-05
Iter: 186 loss: 2.0770698e-05
Iter: 187 loss: 2.19159592e-05
Iter: 188 loss: 2.07701651e-05
Iter: 189 loss: 2.07232479e-05
Iter: 190 loss: 2.06970981e-05
Iter: 191 loss: 2.06762015e-05
Iter: 192 loss: 2.05932956e-05
Iter: 193 loss: 2.06052355e-05
Iter: 194 loss: 2.05312717e-05
Iter: 195 loss: 2.04501375e-05
Iter: 196 loss: 2.13991843e-05
Iter: 197 loss: 2.04501448e-05
Iter: 198 loss: 2.03935851e-05
Iter: 199 loss: 2.04726475e-05
Iter: 200 loss: 2.03667041e-05
Iter: 201 loss: 2.02998508e-05
Iter: 202 loss: 2.0346677e-05
Iter: 203 loss: 2.02575102e-05
Iter: 204 loss: 2.01742332e-05
Iter: 205 loss: 2.03488908e-05
Iter: 206 loss: 2.01432522e-05
Iter: 207 loss: 2.01038856e-05
Iter: 208 loss: 2.01003731e-05
Iter: 209 loss: 2.00595186e-05
Iter: 210 loss: 2.03998898e-05
Iter: 211 loss: 2.00558679e-05
Iter: 212 loss: 2.00407976e-05
Iter: 213 loss: 1.9992036e-05
Iter: 214 loss: 2.00224422e-05
Iter: 215 loss: 1.99477709e-05
Iter: 216 loss: 1.98740236e-05
Iter: 217 loss: 2.07265693e-05
Iter: 218 loss: 1.98717953e-05
Iter: 219 loss: 1.98324251e-05
Iter: 220 loss: 2.02926476e-05
Iter: 221 loss: 1.98309717e-05
Iter: 222 loss: 1.97943737e-05
Iter: 223 loss: 1.9775749e-05
Iter: 224 loss: 1.97572826e-05
Iter: 225 loss: 1.96983401e-05
Iter: 226 loss: 1.98015241e-05
Iter: 227 loss: 1.96724286e-05
Iter: 228 loss: 1.96202382e-05
Iter: 229 loss: 1.98329981e-05
Iter: 230 loss: 1.96084438e-05
Iter: 231 loss: 1.95677749e-05
Iter: 232 loss: 1.9684343e-05
Iter: 233 loss: 1.95550328e-05
Iter: 234 loss: 1.95004104e-05
Iter: 235 loss: 1.95214598e-05
Iter: 236 loss: 1.94624299e-05
Iter: 237 loss: 1.94125132e-05
Iter: 238 loss: 1.96144974e-05
Iter: 239 loss: 1.94017448e-05
Iter: 240 loss: 1.93487376e-05
Iter: 241 loss: 1.94248878e-05
Iter: 242 loss: 1.9323732e-05
Iter: 243 loss: 1.93774758e-05
Iter: 244 loss: 1.93035357e-05
Iter: 245 loss: 1.92925945e-05
Iter: 246 loss: 1.92570333e-05
Iter: 247 loss: 1.93077649e-05
Iter: 248 loss: 1.92330263e-05
Iter: 249 loss: 1.91883937e-05
Iter: 250 loss: 1.91822764e-05
Iter: 251 loss: 1.91512772e-05
Iter: 252 loss: 1.90982428e-05
Iter: 253 loss: 1.98775015e-05
Iter: 254 loss: 1.9097939e-05
Iter: 255 loss: 1.90564606e-05
Iter: 256 loss: 1.92408861e-05
Iter: 257 loss: 1.90483461e-05
Iter: 258 loss: 1.90155879e-05
Iter: 259 loss: 1.90108403e-05
Iter: 260 loss: 1.89881175e-05
Iter: 261 loss: 1.89434832e-05
Iter: 262 loss: 1.9038991e-05
Iter: 263 loss: 1.89253878e-05
Iter: 264 loss: 1.88767954e-05
Iter: 265 loss: 1.89959283e-05
Iter: 266 loss: 1.8858791e-05
Iter: 267 loss: 1.88165159e-05
Iter: 268 loss: 1.91517538e-05
Iter: 269 loss: 1.88140748e-05
Iter: 270 loss: 1.87874648e-05
Iter: 271 loss: 1.87797104e-05
Iter: 272 loss: 1.87630703e-05
Iter: 273 loss: 1.87120331e-05
Iter: 274 loss: 1.87890837e-05
Iter: 275 loss: 1.86875786e-05
Iter: 276 loss: 1.87542792e-05
Iter: 277 loss: 1.86782363e-05
Iter: 278 loss: 1.86689358e-05
Iter: 279 loss: 1.86423604e-05
Iter: 280 loss: 1.88023514e-05
Iter: 281 loss: 1.86354446e-05
Iter: 282 loss: 1.86033976e-05
Iter: 283 loss: 1.85582758e-05
Iter: 284 loss: 1.85568097e-05
Iter: 285 loss: 1.85143745e-05
Iter: 286 loss: 1.90373794e-05
Iter: 287 loss: 1.85133849e-05
Iter: 288 loss: 1.84781165e-05
Iter: 289 loss: 1.85897e-05
Iter: 290 loss: 1.84681849e-05
Iter: 291 loss: 1.84223227e-05
Iter: 292 loss: 1.85079407e-05
Iter: 293 loss: 1.84025557e-05
Iter: 294 loss: 1.83738157e-05
Iter: 295 loss: 1.84095024e-05
Iter: 296 loss: 1.83587108e-05
Iter: 297 loss: 1.83294578e-05
Iter: 298 loss: 1.84270666e-05
Iter: 299 loss: 1.83211214e-05
Iter: 300 loss: 1.82856456e-05
Iter: 301 loss: 1.83415104e-05
Iter: 302 loss: 1.82685399e-05
Iter: 303 loss: 1.82242875e-05
Iter: 304 loss: 1.83653046e-05
Iter: 305 loss: 1.82118602e-05
Iter: 306 loss: 1.8178398e-05
Iter: 307 loss: 1.82505755e-05
Iter: 308 loss: 1.81653413e-05
Iter: 309 loss: 1.81320502e-05
Iter: 310 loss: 1.82827462e-05
Iter: 311 loss: 1.8126655e-05
Iter: 312 loss: 1.81120686e-05
Iter: 313 loss: 1.81058967e-05
Iter: 314 loss: 1.8097744e-05
Iter: 315 loss: 1.80701572e-05
Iter: 316 loss: 1.81190971e-05
Iter: 317 loss: 1.80522366e-05
Iter: 318 loss: 1.80128263e-05
Iter: 319 loss: 1.80594e-05
Iter: 320 loss: 1.79929502e-05
Iter: 321 loss: 1.79492599e-05
Iter: 322 loss: 1.79775761e-05
Iter: 323 loss: 1.79213421e-05
Iter: 324 loss: 1.79219896e-05
Iter: 325 loss: 1.78986702e-05
Iter: 326 loss: 1.7884202e-05
Iter: 327 loss: 1.78655228e-05
Iter: 328 loss: 1.7864073e-05
Iter: 329 loss: 1.78302889e-05
Iter: 330 loss: 1.78093505e-05
Iter: 331 loss: 1.77959537e-05
Iter: 332 loss: 1.77673901e-05
Iter: 333 loss: 1.81253054e-05
Iter: 334 loss: 1.77667625e-05
Iter: 335 loss: 1.77400107e-05
Iter: 336 loss: 1.77810871e-05
Iter: 337 loss: 1.77280617e-05
Iter: 338 loss: 1.76940321e-05
Iter: 339 loss: 1.77759066e-05
Iter: 340 loss: 1.76821686e-05
Iter: 341 loss: 1.7656881e-05
Iter: 342 loss: 1.77143847e-05
Iter: 343 loss: 1.76476533e-05
Iter: 344 loss: 1.76638569e-05
Iter: 345 loss: 1.76384237e-05
Iter: 346 loss: 1.76315862e-05
Iter: 347 loss: 1.76110971e-05
Iter: 348 loss: 1.77584188e-05
Iter: 349 loss: 1.76071371e-05
Iter: 350 loss: 1.75845926e-05
Iter: 351 loss: 1.7567e-05
Iter: 352 loss: 1.75612149e-05
Iter: 353 loss: 1.75277637e-05
Iter: 354 loss: 1.75762743e-05
Iter: 355 loss: 1.75111709e-05
Iter: 356 loss: 1.74786401e-05
Iter: 357 loss: 1.77867259e-05
Iter: 358 loss: 1.74775341e-05
Iter: 359 loss: 1.74498091e-05
Iter: 360 loss: 1.76315334e-05
Iter: 361 loss: 1.74473171e-05
Iter: 362 loss: 1.74245088e-05
Iter: 363 loss: 1.74420384e-05
Iter: 364 loss: 1.74107845e-05
Iter: 365 loss: 1.73931548e-05
Iter: 366 loss: 1.73838234e-05
Iter: 367 loss: 1.73756234e-05
Iter: 368 loss: 1.73405897e-05
Iter: 369 loss: 1.74107936e-05
Iter: 370 loss: 1.73273074e-05
Iter: 371 loss: 1.72953296e-05
Iter: 372 loss: 1.76202811e-05
Iter: 373 loss: 1.72942273e-05
Iter: 374 loss: 1.72751643e-05
Iter: 375 loss: 1.7356746e-05
Iter: 376 loss: 1.72708496e-05
Iter: 377 loss: 1.72574473e-05
Iter: 378 loss: 1.73306144e-05
Iter: 379 loss: 1.72556738e-05
Iter: 380 loss: 1.72330256e-05
Iter: 381 loss: 1.72394066e-05
Iter: 382 loss: 1.72164328e-05
Iter: 383 loss: 1.72049768e-05
Iter: 384 loss: 1.71889333e-05
Iter: 385 loss: 1.71885422e-05
Iter: 386 loss: 1.71660067e-05
Iter: 387 loss: 1.71562278e-05
Iter: 388 loss: 1.7144419e-05
Iter: 389 loss: 1.71144529e-05
Iter: 390 loss: 1.7387536e-05
Iter: 391 loss: 1.71127922e-05
Iter: 392 loss: 1.70964613e-05
Iter: 393 loss: 1.71548327e-05
Iter: 394 loss: 1.70920503e-05
Iter: 395 loss: 1.70713683e-05
Iter: 396 loss: 1.71593565e-05
Iter: 397 loss: 1.70672465e-05
Iter: 398 loss: 1.70489111e-05
Iter: 399 loss: 1.70352432e-05
Iter: 400 loss: 1.70287803e-05
Iter: 401 loss: 1.69970808e-05
Iter: 402 loss: 1.70131425e-05
Iter: 403 loss: 1.6976e-05
Iter: 404 loss: 1.69508621e-05
Iter: 405 loss: 1.71895772e-05
Iter: 406 loss: 1.69510367e-05
Iter: 407 loss: 1.69285049e-05
Iter: 408 loss: 1.69906161e-05
Iter: 409 loss: 1.69223276e-05
Iter: 410 loss: 1.68985607e-05
Iter: 411 loss: 1.70030544e-05
Iter: 412 loss: 1.68954284e-05
Iter: 413 loss: 1.68924271e-05
Iter: 414 loss: 1.68843653e-05
Iter: 415 loss: 1.68807856e-05
Iter: 416 loss: 1.68689221e-05
Iter: 417 loss: 1.68499719e-05
Iter: 418 loss: 1.68482438e-05
Iter: 419 loss: 1.68202314e-05
Iter: 420 loss: 1.69850427e-05
Iter: 421 loss: 1.68163187e-05
Iter: 422 loss: 1.67938379e-05
Iter: 423 loss: 1.6882308e-05
Iter: 424 loss: 1.67882554e-05
Iter: 425 loss: 1.6769689e-05
Iter: 426 loss: 1.67814596e-05
Iter: 427 loss: 1.67583385e-05
Iter: 428 loss: 1.67337694e-05
Iter: 429 loss: 1.70091298e-05
Iter: 430 loss: 1.67334692e-05
Iter: 431 loss: 1.671672e-05
Iter: 432 loss: 1.67633134e-05
Iter: 433 loss: 1.67114304e-05
Iter: 434 loss: 1.66985574e-05
Iter: 435 loss: 1.66758946e-05
Iter: 436 loss: 1.66755854e-05
Iter: 437 loss: 1.66448808e-05
Iter: 438 loss: 1.67811268e-05
Iter: 439 loss: 1.66388018e-05
Iter: 440 loss: 1.66168738e-05
Iter: 441 loss: 1.67173293e-05
Iter: 442 loss: 1.66122609e-05
Iter: 443 loss: 1.65947786e-05
Iter: 444 loss: 1.67475646e-05
Iter: 445 loss: 1.65930123e-05
Iter: 446 loss: 1.65900128e-05
Iter: 447 loss: 1.65866477e-05
Iter: 448 loss: 1.65806177e-05
Iter: 449 loss: 1.65649035e-05
Iter: 450 loss: 1.66608916e-05
Iter: 451 loss: 1.65607562e-05
Iter: 452 loss: 1.65449892e-05
Iter: 453 loss: 1.65463571e-05
Iter: 454 loss: 1.65333222e-05
Iter: 455 loss: 1.65148867e-05
Iter: 456 loss: 1.65694328e-05
Iter: 457 loss: 1.65086e-05
Iter: 458 loss: 1.64874273e-05
Iter: 459 loss: 1.64921366e-05
Iter: 460 loss: 1.64714129e-05
Iter: 461 loss: 1.64549638e-05
Iter: 462 loss: 1.64539124e-05
Iter: 463 loss: 1.64431895e-05
Iter: 464 loss: 1.64693029e-05
Iter: 465 loss: 1.6439868e-05
Iter: 466 loss: 1.64266694e-05
Iter: 467 loss: 1.64099074e-05
Iter: 468 loss: 1.64074372e-05
Iter: 469 loss: 1.63846416e-05
Iter: 470 loss: 1.64856647e-05
Iter: 471 loss: 1.6379201e-05
Iter: 472 loss: 1.63632358e-05
Iter: 473 loss: 1.63788445e-05
Iter: 474 loss: 1.63534569e-05
Iter: 475 loss: 1.6331076e-05
Iter: 476 loss: 1.64424964e-05
Iter: 477 loss: 1.6326987e-05
Iter: 478 loss: 1.63308323e-05
Iter: 479 loss: 1.63189688e-05
Iter: 480 loss: 1.63128225e-05
Iter: 481 loss: 1.63037967e-05
Iter: 482 loss: 1.63027435e-05
Iter: 483 loss: 1.62932156e-05
Iter: 484 loss: 1.62729357e-05
Iter: 485 loss: 1.65941728e-05
Iter: 486 loss: 1.62730321e-05
Iter: 487 loss: 1.62512224e-05
Iter: 488 loss: 1.63313598e-05
Iter: 489 loss: 1.62470151e-05
Iter: 490 loss: 1.62268425e-05
Iter: 491 loss: 1.63023542e-05
Iter: 492 loss: 1.62216602e-05
Iter: 493 loss: 1.62068281e-05
Iter: 494 loss: 1.62746055e-05
Iter: 495 loss: 1.62038905e-05
Iter: 496 loss: 1.61884636e-05
Iter: 497 loss: 1.62666729e-05
Iter: 498 loss: 1.61866774e-05
Iter: 499 loss: 1.61715943e-05
Iter: 500 loss: 1.61842654e-05
Iter: 501 loss: 1.61635071e-05
Iter: 502 loss: 1.61503831e-05
Iter: 503 loss: 1.61766e-05
Iter: 504 loss: 1.61437129e-05
Iter: 505 loss: 1.6129e-05
Iter: 506 loss: 1.61290282e-05
Iter: 507 loss: 1.61173175e-05
Iter: 508 loss: 1.60922755e-05
Iter: 509 loss: 1.61559983e-05
Iter: 510 loss: 1.60835061e-05
Iter: 511 loss: 1.60951058e-05
Iter: 512 loss: 1.60775053e-05
Iter: 513 loss: 1.6072172e-05
Iter: 514 loss: 1.60768795e-05
Iter: 515 loss: 1.60687359e-05
Iter: 516 loss: 1.60625332e-05
Iter: 517 loss: 1.60459967e-05
Iter: 518 loss: 1.61260832e-05
Iter: 519 loss: 1.60403797e-05
Iter: 520 loss: 1.60184391e-05
Iter: 521 loss: 1.61081261e-05
Iter: 522 loss: 1.60130166e-05
Iter: 523 loss: 1.59958363e-05
Iter: 524 loss: 1.6025e-05
Iter: 525 loss: 1.59877109e-05
Iter: 526 loss: 1.5967431e-05
Iter: 527 loss: 1.60283489e-05
Iter: 528 loss: 1.59611573e-05
Iter: 529 loss: 1.59491428e-05
Iter: 530 loss: 1.59491356e-05
Iter: 531 loss: 1.59371448e-05
Iter: 532 loss: 1.59290503e-05
Iter: 533 loss: 1.59252231e-05
Iter: 534 loss: 1.59075644e-05
Iter: 535 loss: 1.59531301e-05
Iter: 536 loss: 1.59024275e-05
Iter: 537 loss: 1.58849525e-05
Iter: 538 loss: 1.58839066e-05
Iter: 539 loss: 1.58706389e-05
Iter: 540 loss: 1.58495823e-05
Iter: 541 loss: 1.59582269e-05
Iter: 542 loss: 1.58459734e-05
Iter: 543 loss: 1.58351468e-05
Iter: 544 loss: 1.58346011e-05
Iter: 545 loss: 1.58224975e-05
Iter: 546 loss: 1.58904149e-05
Iter: 547 loss: 1.58201801e-05
Iter: 548 loss: 1.58124858e-05
Iter: 549 loss: 1.57972809e-05
Iter: 550 loss: 1.60648106e-05
Iter: 551 loss: 1.5796104e-05
Iter: 552 loss: 1.57836985e-05
Iter: 553 loss: 1.58135899e-05
Iter: 554 loss: 1.57787072e-05
Iter: 555 loss: 1.5764037e-05
Iter: 556 loss: 1.57496725e-05
Iter: 557 loss: 1.57467621e-05
Iter: 558 loss: 1.57252543e-05
Iter: 559 loss: 1.59707633e-05
Iter: 560 loss: 1.57247378e-05
Iter: 561 loss: 1.57123941e-05
Iter: 562 loss: 1.57386421e-05
Iter: 563 loss: 1.57078903e-05
Iter: 564 loss: 1.56884525e-05
Iter: 565 loss: 1.57361865e-05
Iter: 566 loss: 1.56821152e-05
Iter: 567 loss: 1.56629794e-05
Iter: 568 loss: 1.57015329e-05
Iter: 569 loss: 1.56557198e-05
Iter: 570 loss: 1.5642745e-05
Iter: 571 loss: 1.5663727e-05
Iter: 572 loss: 1.56359947e-05
Iter: 573 loss: 1.56202295e-05
Iter: 574 loss: 1.56397255e-05
Iter: 575 loss: 1.56121805e-05
Iter: 576 loss: 1.55936723e-05
Iter: 577 loss: 1.56629067e-05
Iter: 578 loss: 1.55894668e-05
Iter: 579 loss: 1.55964517e-05
Iter: 580 loss: 1.55820053e-05
Iter: 581 loss: 1.55784455e-05
Iter: 582 loss: 1.55674697e-05
Iter: 583 loss: 1.56172e-05
Iter: 584 loss: 1.55630387e-05
Iter: 585 loss: 1.55513899e-05
Iter: 586 loss: 1.55935195e-05
Iter: 587 loss: 1.55482958e-05
Iter: 588 loss: 1.55350208e-05
Iter: 589 loss: 1.55156049e-05
Iter: 590 loss: 1.55145735e-05
Iter: 591 loss: 1.54956488e-05
Iter: 592 loss: 1.56652241e-05
Iter: 593 loss: 1.54958143e-05
Iter: 594 loss: 1.5480502e-05
Iter: 595 loss: 1.5481528e-05
Iter: 596 loss: 1.54690024e-05
Iter: 597 loss: 1.54516529e-05
Iter: 598 loss: 1.56150472e-05
Iter: 599 loss: 1.54508016e-05
Iter: 600 loss: 1.54366753e-05
Iter: 601 loss: 1.55791495e-05
Iter: 602 loss: 1.54367353e-05
Iter: 603 loss: 1.54284862e-05
Iter: 604 loss: 1.54141653e-05
Iter: 605 loss: 1.57328141e-05
Iter: 606 loss: 1.54142599e-05
Iter: 607 loss: 1.53931032e-05
Iter: 608 loss: 1.54445261e-05
Iter: 609 loss: 1.53857654e-05
Iter: 610 loss: 1.53746259e-05
Iter: 611 loss: 1.55282869e-05
Iter: 612 loss: 1.53742549e-05
Iter: 613 loss: 1.53697074e-05
Iter: 614 loss: 1.53694564e-05
Iter: 615 loss: 1.53632736e-05
Iter: 616 loss: 1.53470901e-05
Iter: 617 loss: 1.55112575e-05
Iter: 618 loss: 1.53446472e-05
Iter: 619 loss: 1.53346537e-05
Iter: 620 loss: 1.53514229e-05
Iter: 621 loss: 1.53301135e-05
Iter: 622 loss: 1.53178225e-05
Iter: 623 loss: 1.53378023e-05
Iter: 624 loss: 1.53123692e-05
Iter: 625 loss: 1.52981902e-05
Iter: 626 loss: 1.53115634e-05
Iter: 627 loss: 1.52908633e-05
Iter: 628 loss: 1.52753328e-05
Iter: 629 loss: 1.53048913e-05
Iter: 630 loss: 1.5269432e-05
Iter: 631 loss: 1.52528291e-05
Iter: 632 loss: 1.53235869e-05
Iter: 633 loss: 1.5250348e-05
Iter: 634 loss: 1.52373323e-05
Iter: 635 loss: 1.53480942e-05
Iter: 636 loss: 1.52372095e-05
Iter: 637 loss: 1.52246485e-05
Iter: 638 loss: 1.52316352e-05
Iter: 639 loss: 1.52169359e-05
Iter: 640 loss: 1.52048706e-05
Iter: 641 loss: 1.5237657e-05
Iter: 642 loss: 1.52016237e-05
Iter: 643 loss: 1.51912964e-05
Iter: 644 loss: 1.51789536e-05
Iter: 645 loss: 1.51779568e-05
Iter: 646 loss: 1.51822478e-05
Iter: 647 loss: 1.51704508e-05
Iter: 648 loss: 1.51625936e-05
Iter: 649 loss: 1.51746353e-05
Iter: 650 loss: 1.51592731e-05
Iter: 651 loss: 1.5154802e-05
Iter: 652 loss: 1.51445156e-05
Iter: 653 loss: 1.52894045e-05
Iter: 654 loss: 1.5143326e-05
Iter: 655 loss: 1.51309187e-05
Iter: 656 loss: 1.51601816e-05
Iter: 657 loss: 1.51262866e-05
Iter: 658 loss: 1.51134209e-05
Iter: 659 loss: 1.51915719e-05
Iter: 660 loss: 1.51119457e-05
Iter: 661 loss: 1.51033037e-05
Iter: 662 loss: 1.50969554e-05
Iter: 663 loss: 1.509294e-05
Iter: 664 loss: 1.50765527e-05
Iter: 665 loss: 1.51170925e-05
Iter: 666 loss: 1.50701671e-05
Iter: 667 loss: 1.50556843e-05
Iter: 668 loss: 1.5120665e-05
Iter: 669 loss: 1.50530987e-05
Iter: 670 loss: 1.50416417e-05
Iter: 671 loss: 1.52043103e-05
Iter: 672 loss: 1.50417727e-05
Iter: 673 loss: 1.50336618e-05
Iter: 674 loss: 1.50231226e-05
Iter: 675 loss: 1.50217766e-05
Iter: 676 loss: 1.50053875e-05
Iter: 677 loss: 1.50167389e-05
Iter: 678 loss: 1.49949055e-05
Iter: 679 loss: 1.49816515e-05
Iter: 680 loss: 1.50991045e-05
Iter: 681 loss: 1.49806838e-05
Iter: 682 loss: 1.4979958e-05
Iter: 683 loss: 1.49752213e-05
Iter: 684 loss: 1.49723464e-05
Iter: 685 loss: 1.49638054e-05
Iter: 686 loss: 1.49714469e-05
Iter: 687 loss: 1.49561538e-05
Iter: 688 loss: 1.49383013e-05
Iter: 689 loss: 1.49617244e-05
Iter: 690 loss: 1.49295474e-05
Iter: 691 loss: 1.49165207e-05
Iter: 692 loss: 1.51020249e-05
Iter: 693 loss: 1.49168272e-05
Iter: 694 loss: 1.49069101e-05
Iter: 695 loss: 1.49023672e-05
Iter: 696 loss: 1.48984109e-05
Iter: 697 loss: 1.48813051e-05
Iter: 698 loss: 1.49398311e-05
Iter: 699 loss: 1.48771951e-05
Iter: 700 loss: 1.48661738e-05
Iter: 701 loss: 1.48657909e-05
Iter: 702 loss: 1.48578856e-05
Iter: 703 loss: 1.48428599e-05
Iter: 704 loss: 1.49980297e-05
Iter: 705 loss: 1.48420186e-05
Iter: 706 loss: 1.48313666e-05
Iter: 707 loss: 1.49373482e-05
Iter: 708 loss: 1.4830759e-05
Iter: 709 loss: 1.48241979e-05
Iter: 710 loss: 1.48179606e-05
Iter: 711 loss: 1.48167446e-05
Iter: 712 loss: 1.48037325e-05
Iter: 713 loss: 1.47973442e-05
Iter: 714 loss: 1.47916098e-05
Iter: 715 loss: 1.48123017e-05
Iter: 716 loss: 1.47871033e-05
Iter: 717 loss: 1.47814735e-05
Iter: 718 loss: 1.47701776e-05
Iter: 719 loss: 1.49495372e-05
Iter: 720 loss: 1.47698947e-05
Iter: 721 loss: 1.47620931e-05
Iter: 722 loss: 1.47534283e-05
Iter: 723 loss: 1.47524224e-05
Iter: 724 loss: 1.47402152e-05
Iter: 725 loss: 1.48275585e-05
Iter: 726 loss: 1.47392557e-05
Iter: 727 loss: 1.47269957e-05
Iter: 728 loss: 1.47368928e-05
Iter: 729 loss: 1.47206993e-05
Iter: 730 loss: 1.47096198e-05
Iter: 731 loss: 1.47736691e-05
Iter: 732 loss: 1.47081391e-05
Iter: 733 loss: 1.46975717e-05
Iter: 734 loss: 1.46983875e-05
Iter: 735 loss: 1.46883476e-05
Iter: 736 loss: 1.46734019e-05
Iter: 737 loss: 1.47301744e-05
Iter: 738 loss: 1.46693792e-05
Iter: 739 loss: 1.46595921e-05
Iter: 740 loss: 1.47586979e-05
Iter: 741 loss: 1.46596531e-05
Iter: 742 loss: 1.46491875e-05
Iter: 743 loss: 1.46452621e-05
Iter: 744 loss: 1.4639636e-05
Iter: 745 loss: 1.46260854e-05
Iter: 746 loss: 1.46583961e-05
Iter: 747 loss: 1.46219882e-05
Iter: 748 loss: 1.46145394e-05
Iter: 749 loss: 1.46878283e-05
Iter: 750 loss: 1.46143393e-05
Iter: 751 loss: 1.46040102e-05
Iter: 752 loss: 1.46288303e-05
Iter: 753 loss: 1.4600555e-05
Iter: 754 loss: 1.45946524e-05
Iter: 755 loss: 1.45807544e-05
Iter: 756 loss: 1.47241681e-05
Iter: 757 loss: 1.45791309e-05
Iter: 758 loss: 1.45645627e-05
Iter: 759 loss: 1.46036245e-05
Iter: 760 loss: 1.45604827e-05
Iter: 761 loss: 1.45474123e-05
Iter: 762 loss: 1.4607358e-05
Iter: 763 loss: 1.45455251e-05
Iter: 764 loss: 1.45324793e-05
Iter: 765 loss: 1.45675485e-05
Iter: 766 loss: 1.45283566e-05
Iter: 767 loss: 1.45154572e-05
Iter: 768 loss: 1.45210406e-05
Iter: 769 loss: 1.45066233e-05
Iter: 770 loss: 1.44941005e-05
Iter: 771 loss: 1.45889244e-05
Iter: 772 loss: 1.44930673e-05
Iter: 773 loss: 1.44821488e-05
Iter: 774 loss: 1.44888199e-05
Iter: 775 loss: 1.44756741e-05
Iter: 776 loss: 1.44624482e-05
Iter: 777 loss: 1.46043303e-05
Iter: 778 loss: 1.44619908e-05
Iter: 779 loss: 1.4454039e-05
Iter: 780 loss: 1.44696769e-05
Iter: 781 loss: 1.44505593e-05
Iter: 782 loss: 1.44418727e-05
Iter: 783 loss: 1.44263413e-05
Iter: 784 loss: 1.47637593e-05
Iter: 785 loss: 1.44258956e-05
Iter: 786 loss: 1.44883197e-05
Iter: 787 loss: 1.44219848e-05
Iter: 788 loss: 1.44204914e-05
Iter: 789 loss: 1.44148144e-05
Iter: 790 loss: 1.44274381e-05
Iter: 791 loss: 1.44111091e-05
Iter: 792 loss: 1.44019778e-05
Iter: 793 loss: 1.43958587e-05
Iter: 794 loss: 1.43924763e-05
Iter: 795 loss: 1.43771222e-05
Iter: 796 loss: 1.44895603e-05
Iter: 797 loss: 1.43757361e-05
Iter: 798 loss: 1.43622474e-05
Iter: 799 loss: 1.44234882e-05
Iter: 800 loss: 1.43595125e-05
Iter: 801 loss: 1.43482775e-05
Iter: 802 loss: 1.43946554e-05
Iter: 803 loss: 1.43456236e-05
Iter: 804 loss: 1.43377465e-05
Iter: 805 loss: 1.43367179e-05
Iter: 806 loss: 1.43299758e-05
Iter: 807 loss: 1.43174857e-05
Iter: 808 loss: 1.43489087e-05
Iter: 809 loss: 1.43135958e-05
Iter: 810 loss: 1.43028101e-05
Iter: 811 loss: 1.43632415e-05
Iter: 812 loss: 1.43014713e-05
Iter: 813 loss: 1.42927674e-05
Iter: 814 loss: 1.4368381e-05
Iter: 815 loss: 1.42928402e-05
Iter: 816 loss: 1.42849667e-05
Iter: 817 loss: 1.42840718e-05
Iter: 818 loss: 1.42787794e-05
Iter: 819 loss: 1.42697181e-05
Iter: 820 loss: 1.42945682e-05
Iter: 821 loss: 1.42665986e-05
Iter: 822 loss: 1.42701701e-05
Iter: 823 loss: 1.42636563e-05
Iter: 824 loss: 1.42615154e-05
Iter: 825 loss: 1.42535964e-05
Iter: 826 loss: 1.42524732e-05
Iter: 827 loss: 1.42461813e-05
Iter: 828 loss: 1.42355166e-05
Iter: 829 loss: 1.43002653e-05
Iter: 830 loss: 1.42347026e-05
Iter: 831 loss: 1.42264016e-05
Iter: 832 loss: 1.42208228e-05
Iter: 833 loss: 1.4217132e-05
Iter: 834 loss: 1.42054578e-05
Iter: 835 loss: 1.43340249e-05
Iter: 836 loss: 1.42054023e-05
Iter: 837 loss: 1.41968276e-05
Iter: 838 loss: 1.41971968e-05
Iter: 839 loss: 1.41902856e-05
Iter: 840 loss: 1.41782457e-05
Iter: 841 loss: 1.42576446e-05
Iter: 842 loss: 1.41776991e-05
Iter: 843 loss: 1.41680475e-05
Iter: 844 loss: 1.41858618e-05
Iter: 845 loss: 1.41645323e-05
Iter: 846 loss: 1.4154476e-05
Iter: 847 loss: 1.41692508e-05
Iter: 848 loss: 1.41496203e-05
Iter: 849 loss: 1.41380142e-05
Iter: 850 loss: 1.42252647e-05
Iter: 851 loss: 1.41369355e-05
Iter: 852 loss: 1.41291821e-05
Iter: 853 loss: 1.41374194e-05
Iter: 854 loss: 1.41244618e-05
Iter: 855 loss: 1.4116893e-05
Iter: 856 loss: 1.41580267e-05
Iter: 857 loss: 1.41156952e-05
Iter: 858 loss: 1.41101036e-05
Iter: 859 loss: 1.41100454e-05
Iter: 860 loss: 1.41073288e-05
Iter: 861 loss: 1.41018645e-05
Iter: 862 loss: 1.41213204e-05
Iter: 863 loss: 1.40984193e-05
Iter: 864 loss: 1.40904658e-05
Iter: 865 loss: 1.40957018e-05
Iter: 866 loss: 1.40853635e-05
Iter: 867 loss: 1.40745642e-05
Iter: 868 loss: 1.41150686e-05
Iter: 869 loss: 1.40721422e-05
Iter: 870 loss: 1.40615557e-05
Iter: 871 loss: 1.40741795e-05
Iter: 872 loss: 1.40556667e-05
Iter: 873 loss: 1.40457141e-05
Iter: 874 loss: 1.41101409e-05
Iter: 875 loss: 1.4044449e-05
Iter: 876 loss: 1.40343927e-05
Iter: 877 loss: 1.40373149e-05
Iter: 878 loss: 1.40277516e-05
Iter: 879 loss: 1.40165812e-05
Iter: 880 loss: 1.41852379e-05
Iter: 881 loss: 1.40172024e-05
Iter: 882 loss: 1.40107277e-05
Iter: 883 loss: 1.40149223e-05
Iter: 884 loss: 1.40064694e-05
Iter: 885 loss: 1.39983695e-05
Iter: 886 loss: 1.40462853e-05
Iter: 887 loss: 1.39970298e-05
Iter: 888 loss: 1.39886488e-05
Iter: 889 loss: 1.39865706e-05
Iter: 890 loss: 1.39807307e-05
Iter: 891 loss: 1.39913254e-05
Iter: 892 loss: 1.39788826e-05
Iter: 893 loss: 1.39770191e-05
Iter: 894 loss: 1.39708336e-05
Iter: 895 loss: 1.39719095e-05
Iter: 896 loss: 1.39652266e-05
Iter: 897 loss: 1.39542744e-05
Iter: 898 loss: 1.39710819e-05
Iter: 899 loss: 1.39488839e-05
Iter: 900 loss: 1.39394469e-05
Iter: 901 loss: 1.39927724e-05
Iter: 902 loss: 1.39384811e-05
Iter: 903 loss: 1.39303866e-05
Iter: 904 loss: 1.39378844e-05
Iter: 905 loss: 1.39250624e-05
Iter: 906 loss: 1.39157492e-05
Iter: 907 loss: 1.39282693e-05
Iter: 908 loss: 1.39104959e-05
Iter: 909 loss: 1.38982105e-05
Iter: 910 loss: 1.39204358e-05
Iter: 911 loss: 1.3892547e-05
Iter: 912 loss: 1.38820678e-05
Iter: 913 loss: 1.39548192e-05
Iter: 914 loss: 1.38815758e-05
Iter: 915 loss: 1.38718651e-05
Iter: 916 loss: 1.39205276e-05
Iter: 917 loss: 1.38702835e-05
Iter: 918 loss: 1.38645082e-05
Iter: 919 loss: 1.39104513e-05
Iter: 920 loss: 1.38634578e-05
Iter: 921 loss: 1.38589439e-05
Iter: 922 loss: 1.38683918e-05
Iter: 923 loss: 1.38565683e-05
Iter: 924 loss: 1.38514206e-05
Iter: 925 loss: 1.38600954e-05
Iter: 926 loss: 1.38481946e-05
Iter: 927 loss: 1.38443038e-05
Iter: 928 loss: 1.38436972e-05
Iter: 929 loss: 1.38414016e-05
Iter: 930 loss: 1.38350542e-05
Iter: 931 loss: 1.38599662e-05
Iter: 932 loss: 1.38325395e-05
Iter: 933 loss: 1.38253254e-05
Iter: 934 loss: 1.38280911e-05
Iter: 935 loss: 1.38206451e-05
Iter: 936 loss: 1.38102087e-05
Iter: 937 loss: 1.38143714e-05
Iter: 938 loss: 1.38034811e-05
Iter: 939 loss: 1.37943507e-05
Iter: 940 loss: 1.3860461e-05
Iter: 941 loss: 1.37931111e-05
Iter: 942 loss: 1.37834304e-05
Iter: 943 loss: 1.37903698e-05
Iter: 944 loss: 1.37778716e-05
Iter: 945 loss: 1.37697416e-05
Iter: 946 loss: 1.38747182e-05
Iter: 947 loss: 1.37689349e-05
Iter: 948 loss: 1.37626712e-05
Iter: 949 loss: 1.37614788e-05
Iter: 950 loss: 1.37571342e-05
Iter: 951 loss: 1.374781e-05
Iter: 952 loss: 1.38248961e-05
Iter: 953 loss: 1.37480056e-05
Iter: 954 loss: 1.37397992e-05
Iter: 955 loss: 1.37676116e-05
Iter: 956 loss: 1.37373845e-05
Iter: 957 loss: 1.37305415e-05
Iter: 958 loss: 1.37509487e-05
Iter: 959 loss: 1.37286588e-05
Iter: 960 loss: 1.37267753e-05
Iter: 961 loss: 1.37255629e-05
Iter: 962 loss: 1.37225434e-05
Iter: 963 loss: 1.37160641e-05
Iter: 964 loss: 1.38191463e-05
Iter: 965 loss: 1.37162642e-05
Iter: 966 loss: 1.37104271e-05
Iter: 967 loss: 1.37011311e-05
Iter: 968 loss: 1.37007091e-05
Iter: 969 loss: 1.36896042e-05
Iter: 970 loss: 1.37394563e-05
Iter: 971 loss: 1.36879271e-05
Iter: 972 loss: 1.36796598e-05
Iter: 973 loss: 1.36838107e-05
Iter: 974 loss: 1.36738427e-05
Iter: 975 loss: 1.36613517e-05
Iter: 976 loss: 1.37098114e-05
Iter: 977 loss: 1.36588924e-05
Iter: 978 loss: 1.36511871e-05
Iter: 979 loss: 1.36594517e-05
Iter: 980 loss: 1.36467197e-05
Iter: 981 loss: 1.36362005e-05
Iter: 982 loss: 1.36640192e-05
Iter: 983 loss: 1.36317612e-05
Iter: 984 loss: 1.36221506e-05
Iter: 985 loss: 1.36720864e-05
Iter: 986 loss: 1.36208164e-05
Iter: 987 loss: 1.3611605e-05
Iter: 988 loss: 1.36584795e-05
Iter: 989 loss: 1.36096796e-05
Iter: 990 loss: 1.36013496e-05
Iter: 991 loss: 1.36458557e-05
Iter: 992 loss: 1.36007275e-05
Iter: 993 loss: 1.35953032e-05
Iter: 994 loss: 1.36377294e-05
Iter: 995 loss: 1.35945793e-05
Iter: 996 loss: 1.35887676e-05
Iter: 997 loss: 1.36143963e-05
Iter: 998 loss: 1.3587196e-05
Iter: 999 loss: 1.3584031e-05
Iter: 1000 loss: 1.35789141e-05
Iter: 1001 loss: 1.36820718e-05
Iter: 1002 loss: 1.35785212e-05
Iter: 1003 loss: 1.35709579e-05
Iter: 1004 loss: 1.35713381e-05
Iter: 1005 loss: 1.35653645e-05
Iter: 1006 loss: 1.35565606e-05
Iter: 1007 loss: 1.35875016e-05
Iter: 1008 loss: 1.35533319e-05
Iter: 1009 loss: 1.35461678e-05
Iter: 1010 loss: 1.35733799e-05
Iter: 1011 loss: 1.35437558e-05
Iter: 1012 loss: 1.35362898e-05
Iter: 1013 loss: 1.35351484e-05
Iter: 1014 loss: 1.35296268e-05
Iter: 1015 loss: 1.35167465e-05
Iter: 1016 loss: 1.35567252e-05
Iter: 1017 loss: 1.35130631e-05
Iter: 1018 loss: 1.35047385e-05
Iter: 1019 loss: 1.35468599e-05
Iter: 1020 loss: 1.35029441e-05
Iter: 1021 loss: 1.34939082e-05
Iter: 1022 loss: 1.34979109e-05
Iter: 1023 loss: 1.34880456e-05
Iter: 1024 loss: 1.34795537e-05
Iter: 1025 loss: 1.3556918e-05
Iter: 1026 loss: 1.34790425e-05
Iter: 1027 loss: 1.34705024e-05
Iter: 1028 loss: 1.35076034e-05
Iter: 1029 loss: 1.34689126e-05
Iter: 1030 loss: 1.34642323e-05
Iter: 1031 loss: 1.34633456e-05
Iter: 1032 loss: 1.34583388e-05
Iter: 1033 loss: 1.34509874e-05
Iter: 1034 loss: 1.34510501e-05
Iter: 1035 loss: 1.34464026e-05
Iter: 1036 loss: 1.34438633e-05
Iter: 1037 loss: 1.34423226e-05
Iter: 1038 loss: 1.34349148e-05
Iter: 1039 loss: 1.34471575e-05
Iter: 1040 loss: 1.3430832e-05
Iter: 1041 loss: 1.34206384e-05
Iter: 1042 loss: 1.34322281e-05
Iter: 1043 loss: 1.34154916e-05
Iter: 1044 loss: 1.34067132e-05
Iter: 1045 loss: 1.34260254e-05
Iter: 1046 loss: 1.34038364e-05
Iter: 1047 loss: 1.3394143e-05
Iter: 1048 loss: 1.34076126e-05
Iter: 1049 loss: 1.33905924e-05
Iter: 1050 loss: 1.33786798e-05
Iter: 1051 loss: 1.34297361e-05
Iter: 1052 loss: 1.3376708e-05
Iter: 1053 loss: 1.33661115e-05
Iter: 1054 loss: 1.3397299e-05
Iter: 1055 loss: 1.33631984e-05
Iter: 1056 loss: 1.33544163e-05
Iter: 1057 loss: 1.33500635e-05
Iter: 1058 loss: 1.33470976e-05
Iter: 1059 loss: 1.33372687e-05
Iter: 1060 loss: 1.33374542e-05
Iter: 1061 loss: 1.33338817e-05
Iter: 1062 loss: 1.33338444e-05
Iter: 1063 loss: 1.33297763e-05
Iter: 1064 loss: 1.3345667e-05
Iter: 1065 loss: 1.33292742e-05
Iter: 1066 loss: 1.33250005e-05
Iter: 1067 loss: 1.33143594e-05
Iter: 1068 loss: 1.34161073e-05
Iter: 1069 loss: 1.33132144e-05
Iter: 1070 loss: 1.33041758e-05
Iter: 1071 loss: 1.33253207e-05
Iter: 1072 loss: 1.33003432e-05
Iter: 1073 loss: 1.32921195e-05
Iter: 1074 loss: 1.32991772e-05
Iter: 1075 loss: 1.32871937e-05
Iter: 1076 loss: 1.32777859e-05
Iter: 1077 loss: 1.33469875e-05
Iter: 1078 loss: 1.32773348e-05
Iter: 1079 loss: 1.32688983e-05
Iter: 1080 loss: 1.32985624e-05
Iter: 1081 loss: 1.3267002e-05
Iter: 1082 loss: 1.32606674e-05
Iter: 1083 loss: 1.32518926e-05
Iter: 1084 loss: 1.32515561e-05
Iter: 1085 loss: 1.32402984e-05
Iter: 1086 loss: 1.33284575e-05
Iter: 1087 loss: 1.32395471e-05
Iter: 1088 loss: 1.32316563e-05
Iter: 1089 loss: 1.32415207e-05
Iter: 1090 loss: 1.32284549e-05
Iter: 1091 loss: 1.32176101e-05
Iter: 1092 loss: 1.32480718e-05
Iter: 1093 loss: 1.32149617e-05
Iter: 1094 loss: 1.32049281e-05
Iter: 1095 loss: 1.32416044e-05
Iter: 1096 loss: 1.32021542e-05
Iter: 1097 loss: 1.32074647e-05
Iter: 1098 loss: 1.31986581e-05
Iter: 1099 loss: 1.31966362e-05
Iter: 1100 loss: 1.31934994e-05
Iter: 1101 loss: 1.31932466e-05
Iter: 1102 loss: 1.31892411e-05
Iter: 1103 loss: 1.3189986e-05
Iter: 1104 loss: 1.31854922e-05
Iter: 1105 loss: 1.31807119e-05
Iter: 1106 loss: 1.31757315e-05
Iter: 1107 loss: 1.3174038e-05
Iter: 1108 loss: 1.31674024e-05
Iter: 1109 loss: 1.31961115e-05
Iter: 1110 loss: 1.31652705e-05
Iter: 1111 loss: 1.31576389e-05
Iter: 1112 loss: 1.31531542e-05
Iter: 1113 loss: 1.31502884e-05
Iter: 1114 loss: 1.31379666e-05
Iter: 1115 loss: 1.32288405e-05
Iter: 1116 loss: 1.31362449e-05
Iter: 1117 loss: 1.31303404e-05
Iter: 1118 loss: 1.31723755e-05
Iter: 1119 loss: 1.3129641e-05
Iter: 1120 loss: 1.31225352e-05
Iter: 1121 loss: 1.31177767e-05
Iter: 1122 loss: 1.3115432e-05
Iter: 1123 loss: 1.31064753e-05
Iter: 1124 loss: 1.31592396e-05
Iter: 1125 loss: 1.31056713e-05
Iter: 1126 loss: 1.30990629e-05
Iter: 1127 loss: 1.3098721e-05
Iter: 1128 loss: 1.30936969e-05
Iter: 1129 loss: 1.30838307e-05
Iter: 1130 loss: 1.31447159e-05
Iter: 1131 loss: 1.30821454e-05
Iter: 1132 loss: 1.30856e-05
Iter: 1133 loss: 1.30786184e-05
Iter: 1134 loss: 1.30774342e-05
Iter: 1135 loss: 1.30734561e-05
Iter: 1136 loss: 1.30777244e-05
Iter: 1137 loss: 1.30705366e-05
Iter: 1138 loss: 1.30655089e-05
Iter: 1139 loss: 1.31442885e-05
Iter: 1140 loss: 1.30652024e-05
Iter: 1141 loss: 1.30606431e-05
Iter: 1142 loss: 1.3052123e-05
Iter: 1143 loss: 1.3171586e-05
Iter: 1144 loss: 1.30511198e-05
Iter: 1145 loss: 1.30416865e-05
Iter: 1146 loss: 1.30830249e-05
Iter: 1147 loss: 1.30394401e-05
Iter: 1148 loss: 1.30307008e-05
Iter: 1149 loss: 1.30735116e-05
Iter: 1150 loss: 1.30292719e-05
Iter: 1151 loss: 1.30218614e-05
Iter: 1152 loss: 1.30367098e-05
Iter: 1153 loss: 1.3019152e-05
Iter: 1154 loss: 1.30119533e-05
Iter: 1155 loss: 1.30255285e-05
Iter: 1156 loss: 1.30088956e-05
Iter: 1157 loss: 1.30010367e-05
Iter: 1158 loss: 1.30329081e-05
Iter: 1159 loss: 1.29991258e-05
Iter: 1160 loss: 1.29911577e-05
Iter: 1161 loss: 1.29960363e-05
Iter: 1162 loss: 1.29859418e-05
Iter: 1163 loss: 1.29766613e-05
Iter: 1164 loss: 1.30337385e-05
Iter: 1165 loss: 1.29756354e-05
Iter: 1166 loss: 1.29703021e-05
Iter: 1167 loss: 1.29693653e-05
Iter: 1168 loss: 1.29661976e-05
Iter: 1169 loss: 1.29752916e-05
Iter: 1170 loss: 1.29635755e-05
Iter: 1171 loss: 1.29618275e-05
Iter: 1172 loss: 1.29569171e-05
Iter: 1173 loss: 1.29638502e-05
Iter: 1174 loss: 1.29532509e-05
Iter: 1175 loss: 1.29466853e-05
Iter: 1176 loss: 1.29489799e-05
Iter: 1177 loss: 1.29418349e-05
Iter: 1178 loss: 1.29348755e-05
Iter: 1179 loss: 1.29347673e-05
Iter: 1180 loss: 1.29318923e-05
Iter: 1181 loss: 1.29268265e-05
Iter: 1182 loss: 1.29263171e-05
Iter: 1183 loss: 1.29197197e-05
Iter: 1184 loss: 1.29267773e-05
Iter: 1185 loss: 1.29160726e-05
Iter: 1186 loss: 1.29070795e-05
Iter: 1187 loss: 1.29035307e-05
Iter: 1188 loss: 1.28984711e-05
Iter: 1189 loss: 1.28895408e-05
Iter: 1190 loss: 1.28893134e-05
Iter: 1191 loss: 1.28838119e-05
Iter: 1192 loss: 1.28876673e-05
Iter: 1193 loss: 1.28806287e-05
Iter: 1194 loss: 1.28715601e-05
Iter: 1195 loss: 1.28948577e-05
Iter: 1196 loss: 1.28692627e-05
Iter: 1197 loss: 1.28616039e-05
Iter: 1198 loss: 1.28905904e-05
Iter: 1199 loss: 1.28594984e-05
Iter: 1200 loss: 1.28568718e-05
Iter: 1201 loss: 1.28560314e-05
Iter: 1202 loss: 1.2851815e-05
Iter: 1203 loss: 1.28524316e-05
Iter: 1204 loss: 1.28485317e-05
Iter: 1205 loss: 1.28451829e-05
Iter: 1206 loss: 1.28371721e-05
Iter: 1207 loss: 1.29240252e-05
Iter: 1208 loss: 1.28363881e-05
Iter: 1209 loss: 1.28298125e-05
Iter: 1210 loss: 1.29149303e-05
Iter: 1211 loss: 1.28300035e-05
Iter: 1212 loss: 1.28248275e-05
Iter: 1213 loss: 1.2845424e-05
Iter: 1214 loss: 1.28229731e-05
Iter: 1215 loss: 1.28180982e-05
Iter: 1216 loss: 1.28076135e-05
Iter: 1217 loss: 1.29723339e-05
Iter: 1218 loss: 1.28078245e-05
Iter: 1219 loss: 1.28000365e-05
Iter: 1220 loss: 1.28000011e-05
Iter: 1221 loss: 1.2793661e-05
Iter: 1222 loss: 1.27907133e-05
Iter: 1223 loss: 1.27879957e-05
Iter: 1224 loss: 1.27794101e-05
Iter: 1225 loss: 1.2821547e-05
Iter: 1226 loss: 1.27770145e-05
Iter: 1227 loss: 1.27705589e-05
Iter: 1228 loss: 1.28367847e-05
Iter: 1229 loss: 1.27698113e-05
Iter: 1230 loss: 1.27653429e-05
Iter: 1231 loss: 1.27767262e-05
Iter: 1232 loss: 1.27635567e-05
Iter: 1233 loss: 1.27595704e-05
Iter: 1234 loss: 1.27734647e-05
Iter: 1235 loss: 1.27578896e-05
Iter: 1236 loss: 1.27512803e-05
Iter: 1237 loss: 1.28025722e-05
Iter: 1238 loss: 1.27517196e-05
Iter: 1239 loss: 1.27493367e-05
Iter: 1240 loss: 1.27427884e-05
Iter: 1241 loss: 1.28083102e-05
Iter: 1242 loss: 1.27424573e-05
Iter: 1243 loss: 1.27364237e-05
Iter: 1244 loss: 1.27476578e-05
Iter: 1245 loss: 1.27342555e-05
Iter: 1246 loss: 1.27275434e-05
Iter: 1247 loss: 1.27693729e-05
Iter: 1248 loss: 1.27269886e-05
Iter: 1249 loss: 1.27213489e-05
Iter: 1250 loss: 1.27248913e-05
Iter: 1251 loss: 1.27181393e-05
Iter: 1252 loss: 1.27119365e-05
Iter: 1253 loss: 1.27107814e-05
Iter: 1254 loss: 1.27078019e-05
Iter: 1255 loss: 1.26983596e-05
Iter: 1256 loss: 1.27246431e-05
Iter: 1257 loss: 1.26958103e-05
Iter: 1258 loss: 1.26872865e-05
Iter: 1259 loss: 1.27235644e-05
Iter: 1260 loss: 1.26855575e-05
Iter: 1261 loss: 1.26802379e-05
Iter: 1262 loss: 1.26878249e-05
Iter: 1263 loss: 1.26771756e-05
Iter: 1264 loss: 1.26694622e-05
Iter: 1265 loss: 1.27149578e-05
Iter: 1266 loss: 1.26682871e-05
Iter: 1267 loss: 1.26619834e-05
Iter: 1268 loss: 1.26912673e-05
Iter: 1269 loss: 1.26611922e-05
Iter: 1270 loss: 1.26620871e-05
Iter: 1271 loss: 1.26585e-05
Iter: 1272 loss: 1.26575706e-05
Iter: 1273 loss: 1.26541227e-05
Iter: 1274 loss: 1.26522864e-05
Iter: 1275 loss: 1.26494779e-05
Iter: 1276 loss: 1.26415362e-05
Iter: 1277 loss: 1.26746199e-05
Iter: 1278 loss: 1.263985e-05
Iter: 1279 loss: 1.26334235e-05
Iter: 1280 loss: 1.26773411e-05
Iter: 1281 loss: 1.26331524e-05
Iter: 1282 loss: 1.26279101e-05
Iter: 1283 loss: 1.26423129e-05
Iter: 1284 loss: 1.26267332e-05
Iter: 1285 loss: 1.26220148e-05
Iter: 1286 loss: 1.26179521e-05
Iter: 1287 loss: 1.26162467e-05
Iter: 1288 loss: 1.26082095e-05
Iter: 1289 loss: 1.26213872e-05
Iter: 1290 loss: 1.26046198e-05
Iter: 1291 loss: 1.25986871e-05
Iter: 1292 loss: 1.26260493e-05
Iter: 1293 loss: 1.25970164e-05
Iter: 1294 loss: 1.25895513e-05
Iter: 1295 loss: 1.25908336e-05
Iter: 1296 loss: 1.25842116e-05
Iter: 1297 loss: 1.25762799e-05
Iter: 1298 loss: 1.26499963e-05
Iter: 1299 loss: 1.25763663e-05
Iter: 1300 loss: 1.25704373e-05
Iter: 1301 loss: 1.26057621e-05
Iter: 1302 loss: 1.25699853e-05
Iter: 1303 loss: 1.25696897e-05
Iter: 1304 loss: 1.25684019e-05
Iter: 1305 loss: 1.25662627e-05
Iter: 1306 loss: 1.25604256e-05
Iter: 1307 loss: 1.25674151e-05
Iter: 1308 loss: 1.25550641e-05
Iter: 1309 loss: 1.25479082e-05
Iter: 1310 loss: 1.25732713e-05
Iter: 1311 loss: 1.25463184e-05
Iter: 1312 loss: 1.25400329e-05
Iter: 1313 loss: 1.25705756e-05
Iter: 1314 loss: 1.25390561e-05
Iter: 1315 loss: 1.25334527e-05
Iter: 1316 loss: 1.25531751e-05
Iter: 1317 loss: 1.25322695e-05
Iter: 1318 loss: 1.2526576e-05
Iter: 1319 loss: 1.25273255e-05
Iter: 1320 loss: 1.25229635e-05
Iter: 1321 loss: 1.25170409e-05
Iter: 1322 loss: 1.25312854e-05
Iter: 1323 loss: 1.25144506e-05
Iter: 1324 loss: 1.25084161e-05
Iter: 1325 loss: 1.25058323e-05
Iter: 1326 loss: 1.25019078e-05
Iter: 1327 loss: 1.24930957e-05
Iter: 1328 loss: 1.25678844e-05
Iter: 1329 loss: 1.2492631e-05
Iter: 1330 loss: 1.24873e-05
Iter: 1331 loss: 1.25019369e-05
Iter: 1332 loss: 1.24852195e-05
Iter: 1333 loss: 1.24791222e-05
Iter: 1334 loss: 1.25002225e-05
Iter: 1335 loss: 1.24772714e-05
Iter: 1336 loss: 1.24746775e-05
Iter: 1337 loss: 1.24734133e-05
Iter: 1338 loss: 1.24693652e-05
Iter: 1339 loss: 1.24650369e-05
Iter: 1340 loss: 1.24641601e-05
Iter: 1341 loss: 1.24611979e-05
Iter: 1342 loss: 1.24581165e-05
Iter: 1343 loss: 1.2458122e-05
Iter: 1344 loss: 1.24529688e-05
Iter: 1345 loss: 1.24648332e-05
Iter: 1346 loss: 1.24509215e-05
Iter: 1347 loss: 1.24448206e-05
Iter: 1348 loss: 1.24724847e-05
Iter: 1349 loss: 1.2442908e-05
Iter: 1350 loss: 1.24378148e-05
Iter: 1351 loss: 1.24474209e-05
Iter: 1352 loss: 1.24356429e-05
Iter: 1353 loss: 1.24302833e-05
Iter: 1354 loss: 1.24294666e-05
Iter: 1355 loss: 1.24258149e-05
Iter: 1356 loss: 1.24180206e-05
Iter: 1357 loss: 1.24355493e-05
Iter: 1358 loss: 1.24152266e-05
Iter: 1359 loss: 1.24078961e-05
Iter: 1360 loss: 1.24412036e-05
Iter: 1361 loss: 1.240647e-05
Iter: 1362 loss: 1.24000007e-05
Iter: 1363 loss: 1.24063099e-05
Iter: 1364 loss: 1.23960426e-05
Iter: 1365 loss: 1.23870459e-05
Iter: 1366 loss: 1.24183571e-05
Iter: 1367 loss: 1.23849568e-05
Iter: 1368 loss: 1.23829177e-05
Iter: 1369 loss: 1.23808495e-05
Iter: 1370 loss: 1.23776081e-05
Iter: 1371 loss: 1.23930067e-05
Iter: 1372 loss: 1.23765158e-05
Iter: 1373 loss: 1.23750151e-05
Iter: 1374 loss: 1.23701675e-05
Iter: 1375 loss: 1.23717537e-05
Iter: 1376 loss: 1.23653508e-05
Iter: 1377 loss: 1.23553473e-05
Iter: 1378 loss: 1.24013914e-05
Iter: 1379 loss: 1.23535574e-05
Iter: 1380 loss: 1.23490336e-05
Iter: 1381 loss: 1.2348517e-05
Iter: 1382 loss: 1.23449045e-05
Iter: 1383 loss: 1.23436e-05
Iter: 1384 loss: 1.23408445e-05
Iter: 1385 loss: 1.23360132e-05
Iter: 1386 loss: 1.23422951e-05
Iter: 1387 loss: 1.23339141e-05
Iter: 1388 loss: 1.23265345e-05
Iter: 1389 loss: 1.23372738e-05
Iter: 1390 loss: 1.23230157e-05
Iter: 1391 loss: 1.23172622e-05
Iter: 1392 loss: 1.2330067e-05
Iter: 1393 loss: 1.23151876e-05
Iter: 1394 loss: 1.23070877e-05
Iter: 1395 loss: 1.23248992e-05
Iter: 1396 loss: 1.23038153e-05
Iter: 1397 loss: 1.22979909e-05
Iter: 1398 loss: 1.23287646e-05
Iter: 1399 loss: 1.22970187e-05
Iter: 1400 loss: 1.22925467e-05
Iter: 1401 loss: 1.23415139e-05
Iter: 1402 loss: 1.22918063e-05
Iter: 1403 loss: 1.22875254e-05
Iter: 1404 loss: 1.2321364e-05
Iter: 1405 loss: 1.2286775e-05
Iter: 1406 loss: 1.22840456e-05
Iter: 1407 loss: 1.22767487e-05
Iter: 1408 loss: 1.23510799e-05
Iter: 1409 loss: 1.22757874e-05
Iter: 1410 loss: 1.22692891e-05
Iter: 1411 loss: 1.22735264e-05
Iter: 1412 loss: 1.22652955e-05
Iter: 1413 loss: 1.22589427e-05
Iter: 1414 loss: 1.23276923e-05
Iter: 1415 loss: 1.22592164e-05
Iter: 1416 loss: 1.22535284e-05
Iter: 1417 loss: 1.22602114e-05
Iter: 1418 loss: 1.2250337e-05
Iter: 1419 loss: 1.22435358e-05
Iter: 1420 loss: 1.22522652e-05
Iter: 1421 loss: 1.22390684e-05
Iter: 1422 loss: 1.22335123e-05
Iter: 1423 loss: 1.22519705e-05
Iter: 1424 loss: 1.22317942e-05
Iter: 1425 loss: 1.22254642e-05
Iter: 1426 loss: 1.22270567e-05
Iter: 1427 loss: 1.22210076e-05
Iter: 1428 loss: 1.22121328e-05
Iter: 1429 loss: 1.22551965e-05
Iter: 1430 loss: 1.22110332e-05
Iter: 1431 loss: 1.22051079e-05
Iter: 1432 loss: 1.222234e-05
Iter: 1433 loss: 1.22031597e-05
Iter: 1434 loss: 1.21977537e-05
Iter: 1435 loss: 1.22482234e-05
Iter: 1436 loss: 1.21978555e-05
Iter: 1437 loss: 1.21952116e-05
Iter: 1438 loss: 1.21944477e-05
Iter: 1439 loss: 1.21933299e-05
Iter: 1440 loss: 1.21883058e-05
Iter: 1441 loss: 1.22293295e-05
Iter: 1442 loss: 1.21879766e-05
Iter: 1443 loss: 1.21829944e-05
Iter: 1444 loss: 1.21869743e-05
Iter: 1445 loss: 1.21808389e-05
Iter: 1446 loss: 1.21757548e-05
Iter: 1447 loss: 1.21739404e-05
Iter: 1448 loss: 1.21708035e-05
Iter: 1449 loss: 1.21650146e-05
Iter: 1450 loss: 1.21651465e-05
Iter: 1451 loss: 1.21602397e-05
Iter: 1452 loss: 1.21594067e-05
Iter: 1453 loss: 1.21565135e-05
Iter: 1454 loss: 1.21506619e-05
Iter: 1455 loss: 1.21555859e-05
Iter: 1456 loss: 1.21478115e-05
Iter: 1457 loss: 1.21409576e-05
Iter: 1458 loss: 1.21617941e-05
Iter: 1459 loss: 1.21384483e-05
Iter: 1460 loss: 1.21325256e-05
Iter: 1461 loss: 1.21553421e-05
Iter: 1462 loss: 1.21308512e-05
Iter: 1463 loss: 1.21254316e-05
Iter: 1464 loss: 1.21240782e-05
Iter: 1465 loss: 1.21198163e-05
Iter: 1466 loss: 1.21124449e-05
Iter: 1467 loss: 1.22159618e-05
Iter: 1468 loss: 1.21120811e-05
Iter: 1469 loss: 1.21119883e-05
Iter: 1470 loss: 1.21097273e-05
Iter: 1471 loss: 1.21082885e-05
Iter: 1472 loss: 1.21042494e-05
Iter: 1473 loss: 1.21610046e-05
Iter: 1474 loss: 1.21042576e-05
Iter: 1475 loss: 1.2100978e-05
Iter: 1476 loss: 1.20957302e-05
Iter: 1477 loss: 1.2095511e-05
Iter: 1478 loss: 1.20880377e-05
Iter: 1479 loss: 1.2106444e-05
Iter: 1480 loss: 1.20858667e-05
Iter: 1481 loss: 1.20811619e-05
Iter: 1482 loss: 1.20812847e-05
Iter: 1483 loss: 1.20765708e-05
Iter: 1484 loss: 1.20806017e-05
Iter: 1485 loss: 1.20742488e-05
Iter: 1486 loss: 1.20696568e-05
Iter: 1487 loss: 1.20691311e-05
Iter: 1488 loss: 1.20652476e-05
Iter: 1489 loss: 1.2059214e-05
Iter: 1490 loss: 1.2081694e-05
Iter: 1491 loss: 1.20577952e-05
Iter: 1492 loss: 1.20518989e-05
Iter: 1493 loss: 1.20676987e-05
Iter: 1494 loss: 1.20494515e-05
Iter: 1495 loss: 1.20437699e-05
Iter: 1496 loss: 1.204203e-05
Iter: 1497 loss: 1.20376299e-05
Iter: 1498 loss: 1.2030303e-05
Iter: 1499 loss: 1.21190878e-05
Iter: 1500 loss: 1.20299555e-05
Iter: 1501 loss: 1.20318327e-05
Iter: 1502 loss: 1.20277691e-05
Iter: 1503 loss: 1.2026806e-05
Iter: 1504 loss: 1.20217892e-05
Iter: 1505 loss: 1.20442728e-05
Iter: 1506 loss: 1.20194145e-05
Iter: 1507 loss: 1.20125378e-05
Iter: 1508 loss: 1.2023047e-05
Iter: 1509 loss: 1.20087507e-05
Iter: 1510 loss: 1.20034838e-05
Iter: 1511 loss: 1.20222467e-05
Iter: 1512 loss: 1.20013792e-05
Iter: 1513 loss: 1.1997101e-05
Iter: 1514 loss: 1.19982287e-05
Iter: 1515 loss: 1.19931738e-05
Iter: 1516 loss: 1.19871274e-05
Iter: 1517 loss: 1.20688637e-05
Iter: 1518 loss: 1.19865545e-05
Iter: 1519 loss: 1.19818742e-05
Iter: 1520 loss: 1.19809365e-05
Iter: 1521 loss: 1.1977344e-05
Iter: 1522 loss: 1.19726374e-05
Iter: 1523 loss: 1.19775923e-05
Iter: 1524 loss: 1.1969978e-05
Iter: 1525 loss: 1.1962522e-05
Iter: 1526 loss: 1.19711749e-05
Iter: 1527 loss: 1.19586039e-05
Iter: 1528 loss: 1.19508986e-05
Iter: 1529 loss: 1.19984188e-05
Iter: 1530 loss: 1.19499164e-05
Iter: 1531 loss: 1.1943348e-05
Iter: 1532 loss: 1.19557399e-05
Iter: 1533 loss: 1.19413326e-05
Iter: 1534 loss: 1.19377864e-05
Iter: 1535 loss: 1.19369233e-05
Iter: 1536 loss: 1.19333672e-05
Iter: 1537 loss: 1.19478682e-05
Iter: 1538 loss: 1.19327105e-05
Iter: 1539 loss: 1.19306014e-05
Iter: 1540 loss: 1.19276183e-05
Iter: 1541 loss: 1.19738897e-05
Iter: 1542 loss: 1.19269716e-05
Iter: 1543 loss: 1.19223441e-05
Iter: 1544 loss: 1.19292736e-05
Iter: 1545 loss: 1.19194146e-05
Iter: 1546 loss: 1.19138185e-05
Iter: 1547 loss: 1.19175784e-05
Iter: 1548 loss: 1.19101896e-05
Iter: 1549 loss: 1.19055894e-05
Iter: 1550 loss: 1.19782526e-05
Iter: 1551 loss: 1.19045508e-05
Iter: 1552 loss: 1.19013712e-05
Iter: 1553 loss: 1.1922757e-05
Iter: 1554 loss: 1.19006636e-05
Iter: 1555 loss: 1.18974067e-05
Iter: 1556 loss: 1.18906401e-05
Iter: 1557 loss: 1.19802944e-05
Iter: 1558 loss: 1.18901189e-05
Iter: 1559 loss: 1.18826374e-05
Iter: 1560 loss: 1.1960512e-05
Iter: 1561 loss: 1.18827083e-05
Iter: 1562 loss: 1.18786274e-05
Iter: 1563 loss: 1.18837288e-05
Iter: 1564 loss: 1.18769394e-05
Iter: 1565 loss: 1.18714852e-05
Iter: 1566 loss: 1.18727712e-05
Iter: 1567 loss: 1.18670041e-05
Iter: 1568 loss: 1.18622211e-05
Iter: 1569 loss: 1.18620492e-05
Iter: 1570 loss: 1.18590606e-05
Iter: 1571 loss: 1.18594699e-05
Iter: 1572 loss: 1.18577345e-05
Iter: 1573 loss: 1.18529888e-05
Iter: 1574 loss: 1.1862252e-05
Iter: 1575 loss: 1.18501412e-05
Iter: 1576 loss: 1.18440184e-05
Iter: 1577 loss: 1.1883958e-05
Iter: 1578 loss: 1.18432927e-05
Iter: 1579 loss: 1.18386406e-05
Iter: 1580 loss: 1.18490225e-05
Iter: 1581 loss: 1.18370581e-05
Iter: 1582 loss: 1.18319585e-05
Iter: 1583 loss: 1.18396183e-05
Iter: 1584 loss: 1.18300759e-05
Iter: 1585 loss: 1.18244616e-05
Iter: 1586 loss: 1.18677672e-05
Iter: 1587 loss: 1.18242315e-05
Iter: 1588 loss: 1.18187127e-05
Iter: 1589 loss: 1.18285943e-05
Iter: 1590 loss: 1.18166236e-05
Iter: 1591 loss: 1.18126336e-05
Iter: 1592 loss: 1.18158187e-05
Iter: 1593 loss: 1.18104672e-05
Iter: 1594 loss: 1.18047683e-05
Iter: 1595 loss: 1.18016578e-05
Iter: 1596 loss: 1.17989093e-05
Iter: 1597 loss: 1.17906438e-05
Iter: 1598 loss: 1.18351099e-05
Iter: 1599 loss: 1.1790371e-05
Iter: 1600 loss: 1.178394e-05
Iter: 1601 loss: 1.17966392e-05
Iter: 1602 loss: 1.17807285e-05
Iter: 1603 loss: 1.17796253e-05
Iter: 1604 loss: 1.177781e-05
Iter: 1605 loss: 1.17736217e-05
Iter: 1606 loss: 1.17726813e-05
Iter: 1607 loss: 1.17704585e-05
Iter: 1608 loss: 1.17678355e-05
Iter: 1609 loss: 1.17634827e-05
Iter: 1610 loss: 1.18547841e-05
Iter: 1611 loss: 1.1763681e-05
Iter: 1612 loss: 1.17586478e-05
Iter: 1613 loss: 1.17557593e-05
Iter: 1614 loss: 1.17531863e-05
Iter: 1615 loss: 1.17476611e-05
Iter: 1616 loss: 1.1748074e-05
Iter: 1617 loss: 1.17445261e-05
Iter: 1618 loss: 1.1762213e-05
Iter: 1619 loss: 1.17439031e-05
Iter: 1620 loss: 1.17408235e-05
Iter: 1621 loss: 1.17464651e-05
Iter: 1622 loss: 1.17389809e-05
Iter: 1623 loss: 1.17349591e-05
Iter: 1624 loss: 1.17408263e-05
Iter: 1625 loss: 1.17330837e-05
Iter: 1626 loss: 1.17289028e-05
Iter: 1627 loss: 1.17370901e-05
Iter: 1628 loss: 1.17268428e-05
Iter: 1629 loss: 1.17224536e-05
Iter: 1630 loss: 1.17269938e-05
Iter: 1631 loss: 1.17203817e-05
Iter: 1632 loss: 1.17142754e-05
Iter: 1633 loss: 1.17142545e-05
Iter: 1634 loss: 1.17093095e-05
Iter: 1635 loss: 1.17030668e-05
Iter: 1636 loss: 1.17923519e-05
Iter: 1637 loss: 1.17030313e-05
Iter: 1638 loss: 1.17005511e-05
Iter: 1639 loss: 1.17001337e-05
Iter: 1640 loss: 1.16989177e-05
Iter: 1641 loss: 1.1695387e-05
Iter: 1642 loss: 1.16953925e-05
Iter: 1643 loss: 1.16923038e-05
Iter: 1644 loss: 1.16848732e-05
Iter: 1645 loss: 1.17033414e-05
Iter: 1646 loss: 1.1682986e-05
Iter: 1647 loss: 1.16775736e-05
Iter: 1648 loss: 1.17148247e-05
Iter: 1649 loss: 1.16772753e-05
Iter: 1650 loss: 1.16726824e-05
Iter: 1651 loss: 1.16762412e-05
Iter: 1652 loss: 1.16695055e-05
Iter: 1653 loss: 1.16633746e-05
Iter: 1654 loss: 1.17183163e-05
Iter: 1655 loss: 1.16631272e-05
Iter: 1656 loss: 1.16592601e-05
Iter: 1657 loss: 1.16745414e-05
Iter: 1658 loss: 1.16587316e-05
Iter: 1659 loss: 1.1654929e-05
Iter: 1660 loss: 1.16550427e-05
Iter: 1661 loss: 1.16524598e-05
Iter: 1662 loss: 1.16469892e-05
Iter: 1663 loss: 1.16508909e-05
Iter: 1664 loss: 1.16436677e-05
Iter: 1665 loss: 1.16372157e-05
Iter: 1666 loss: 1.1653844e-05
Iter: 1667 loss: 1.16346364e-05
Iter: 1668 loss: 1.16292449e-05
Iter: 1669 loss: 1.16609699e-05
Iter: 1670 loss: 1.16286301e-05
Iter: 1671 loss: 1.16310339e-05
Iter: 1672 loss: 1.16267738e-05
Iter: 1673 loss: 1.16258207e-05
Iter: 1674 loss: 1.16212977e-05
Iter: 1675 loss: 1.16194587e-05
Iter: 1676 loss: 1.16164192e-05
Iter: 1677 loss: 1.16103347e-05
Iter: 1678 loss: 1.16411484e-05
Iter: 1679 loss: 1.16095189e-05
Iter: 1680 loss: 1.16040737e-05
Iter: 1681 loss: 1.1608161e-05
Iter: 1682 loss: 1.16013543e-05
Iter: 1683 loss: 1.15946223e-05
Iter: 1684 loss: 1.16252413e-05
Iter: 1685 loss: 1.15932671e-05
Iter: 1686 loss: 1.15893727e-05
Iter: 1687 loss: 1.16196952e-05
Iter: 1688 loss: 1.15891798e-05
Iter: 1689 loss: 1.15858811e-05
Iter: 1690 loss: 1.15951316e-05
Iter: 1691 loss: 1.158532e-05
Iter: 1692 loss: 1.15804123e-05
Iter: 1693 loss: 1.15792909e-05
Iter: 1694 loss: 1.15766034e-05
Iter: 1695 loss: 1.1571612e-05
Iter: 1696 loss: 1.1605066e-05
Iter: 1697 loss: 1.15704515e-05
Iter: 1698 loss: 1.15672974e-05
Iter: 1699 loss: 1.15690855e-05
Iter: 1700 loss: 1.15645471e-05
Iter: 1701 loss: 1.15598414e-05
Iter: 1702 loss: 1.15688981e-05
Iter: 1703 loss: 1.15576468e-05
Iter: 1704 loss: 1.15619387e-05
Iter: 1705 loss: 1.15563416e-05
Iter: 1706 loss: 1.15547009e-05
Iter: 1707 loss: 1.15507264e-05
Iter: 1708 loss: 1.15769817e-05
Iter: 1709 loss: 1.15489529e-05
Iter: 1710 loss: 1.15454859e-05
Iter: 1711 loss: 1.15435341e-05
Iter: 1712 loss: 1.15415751e-05
Iter: 1713 loss: 1.15345356e-05
Iter: 1714 loss: 1.15427865e-05
Iter: 1715 loss: 1.15308731e-05
Iter: 1716 loss: 1.15244211e-05
Iter: 1717 loss: 1.15866405e-05
Iter: 1718 loss: 1.15242938e-05
Iter: 1719 loss: 1.1519287e-05
Iter: 1720 loss: 1.15448056e-05
Iter: 1721 loss: 1.15184966e-05
Iter: 1722 loss: 1.1512815e-05
Iter: 1723 loss: 1.15210796e-05
Iter: 1724 loss: 1.15105067e-05
Iter: 1725 loss: 1.15051598e-05
Iter: 1726 loss: 1.15329203e-05
Iter: 1727 loss: 1.15043204e-05
Iter: 1728 loss: 1.15006878e-05
Iter: 1729 loss: 1.15059929e-05
Iter: 1730 loss: 1.14989452e-05
Iter: 1731 loss: 1.1493642e-05
Iter: 1732 loss: 1.14892946e-05
Iter: 1733 loss: 1.14881841e-05
Iter: 1734 loss: 1.14795102e-05
Iter: 1735 loss: 1.15198691e-05
Iter: 1736 loss: 1.1477845e-05
Iter: 1737 loss: 1.1476538e-05
Iter: 1738 loss: 1.1475272e-05
Iter: 1739 loss: 1.14715476e-05
Iter: 1740 loss: 1.14713157e-05
Iter: 1741 loss: 1.14688428e-05
Iter: 1742 loss: 1.14660015e-05
Iter: 1743 loss: 1.14596523e-05
Iter: 1744 loss: 1.15039493e-05
Iter: 1745 loss: 1.14578233e-05
Iter: 1746 loss: 1.14513887e-05
Iter: 1747 loss: 1.15024786e-05
Iter: 1748 loss: 1.14513914e-05
Iter: 1749 loss: 1.14460636e-05
Iter: 1750 loss: 1.14524992e-05
Iter: 1751 loss: 1.14433869e-05
Iter: 1752 loss: 1.1437206e-05
Iter: 1753 loss: 1.14730374e-05
Iter: 1754 loss: 1.14363575e-05
Iter: 1755 loss: 1.14314171e-05
Iter: 1756 loss: 1.14478307e-05
Iter: 1757 loss: 1.14304339e-05
Iter: 1758 loss: 1.14258573e-05
Iter: 1759 loss: 1.14422864e-05
Iter: 1760 loss: 1.14245458e-05
Iter: 1761 loss: 1.14206978e-05
Iter: 1762 loss: 1.14292743e-05
Iter: 1763 loss: 1.14179766e-05
Iter: 1764 loss: 1.14136637e-05
Iter: 1765 loss: 1.14155955e-05
Iter: 1766 loss: 1.14100358e-05
Iter: 1767 loss: 1.14051636e-05
Iter: 1768 loss: 1.1437176e-05
Iter: 1769 loss: 1.14046052e-05
Iter: 1770 loss: 1.14009154e-05
Iter: 1771 loss: 1.14204968e-05
Iter: 1772 loss: 1.14005716e-05
Iter: 1773 loss: 1.13961769e-05
Iter: 1774 loss: 1.14264694e-05
Iter: 1775 loss: 1.13965743e-05
Iter: 1776 loss: 1.13941132e-05
Iter: 1777 loss: 1.1389041e-05
Iter: 1778 loss: 1.14223594e-05
Iter: 1779 loss: 1.13878195e-05
Iter: 1780 loss: 1.13827455e-05
Iter: 1781 loss: 1.1386539e-05
Iter: 1782 loss: 1.13794522e-05
Iter: 1783 loss: 1.13726983e-05
Iter: 1784 loss: 1.13910501e-05
Iter: 1785 loss: 1.13701462e-05
Iter: 1786 loss: 1.13652204e-05
Iter: 1787 loss: 1.14419472e-05
Iter: 1788 loss: 1.13653223e-05
Iter: 1789 loss: 1.13614178e-05
Iter: 1790 loss: 1.13748829e-05
Iter: 1791 loss: 1.13601463e-05
Iter: 1792 loss: 1.13559472e-05
Iter: 1793 loss: 1.13578317e-05
Iter: 1794 loss: 1.13530132e-05
Iter: 1795 loss: 1.13470696e-05
Iter: 1796 loss: 1.13801871e-05
Iter: 1797 loss: 1.13465958e-05
Iter: 1798 loss: 1.1341619e-05
Iter: 1799 loss: 1.13434189e-05
Iter: 1800 loss: 1.1339529e-05
Iter: 1801 loss: 1.13345104e-05
Iter: 1802 loss: 1.13450533e-05
Iter: 1803 loss: 1.1332223e-05
Iter: 1804 loss: 1.13258247e-05
Iter: 1805 loss: 1.13450988e-05
Iter: 1806 loss: 1.13239139e-05
Iter: 1807 loss: 1.13257338e-05
Iter: 1808 loss: 1.13211381e-05
Iter: 1809 loss: 1.1320466e-05
Iter: 1810 loss: 1.13167262e-05
Iter: 1811 loss: 1.13224869e-05
Iter: 1812 loss: 1.13147489e-05
Iter: 1813 loss: 1.13090537e-05
Iter: 1814 loss: 1.13057467e-05
Iter: 1815 loss: 1.13038914e-05
Iter: 1816 loss: 1.12966663e-05
Iter: 1817 loss: 1.13533288e-05
Iter: 1818 loss: 1.12964663e-05
Iter: 1819 loss: 1.12911966e-05
Iter: 1820 loss: 1.12865637e-05
Iter: 1821 loss: 1.12847201e-05
Iter: 1822 loss: 1.12786192e-05
Iter: 1823 loss: 1.12783673e-05
Iter: 1824 loss: 1.12728103e-05
Iter: 1825 loss: 1.13107153e-05
Iter: 1826 loss: 1.12723237e-05
Iter: 1827 loss: 1.12694688e-05
Iter: 1828 loss: 1.12712196e-05
Iter: 1829 loss: 1.12671723e-05
Iter: 1830 loss: 1.12628077e-05
Iter: 1831 loss: 1.1264252e-05
Iter: 1832 loss: 1.12588878e-05
Iter: 1833 loss: 1.12528405e-05
Iter: 1834 loss: 1.12749731e-05
Iter: 1835 loss: 1.12513344e-05
Iter: 1836 loss: 1.12461039e-05
Iter: 1837 loss: 1.12620883e-05
Iter: 1838 loss: 1.12451935e-05
Iter: 1839 loss: 1.12457119e-05
Iter: 1840 loss: 1.12430571e-05
Iter: 1841 loss: 1.12416501e-05
Iter: 1842 loss: 1.1236657e-05
Iter: 1843 loss: 1.12567132e-05
Iter: 1844 loss: 1.12342022e-05
Iter: 1845 loss: 1.12286916e-05
Iter: 1846 loss: 1.12370053e-05
Iter: 1847 loss: 1.12262696e-05
Iter: 1848 loss: 1.12204389e-05
Iter: 1849 loss: 1.12192392e-05
Iter: 1850 loss: 1.12152266e-05
Iter: 1851 loss: 1.12072848e-05
Iter: 1852 loss: 1.12626722e-05
Iter: 1853 loss: 1.12069065e-05
Iter: 1854 loss: 1.12010457e-05
Iter: 1855 loss: 1.12183952e-05
Iter: 1856 loss: 1.11991376e-05
Iter: 1857 loss: 1.11936515e-05
Iter: 1858 loss: 1.12163816e-05
Iter: 1859 loss: 1.11923546e-05
Iter: 1860 loss: 1.11849286e-05
Iter: 1861 loss: 1.12007037e-05
Iter: 1862 loss: 1.11822646e-05
Iter: 1863 loss: 1.11763384e-05
Iter: 1864 loss: 1.11982899e-05
Iter: 1865 loss: 1.1175e-05
Iter: 1866 loss: 1.1170554e-05
Iter: 1867 loss: 1.1186361e-05
Iter: 1868 loss: 1.11701602e-05
Iter: 1869 loss: 1.11659901e-05
Iter: 1870 loss: 1.11629815e-05
Iter: 1871 loss: 1.11613654e-05
Iter: 1872 loss: 1.11611234e-05
Iter: 1873 loss: 1.11592635e-05
Iter: 1874 loss: 1.11563131e-05
Iter: 1875 loss: 1.11577056e-05
Iter: 1876 loss: 1.11540203e-05
Iter: 1877 loss: 1.11521585e-05
Iter: 1878 loss: 1.11470326e-05
Iter: 1879 loss: 1.11929e-05
Iter: 1880 loss: 1.11466e-05
Iter: 1881 loss: 1.11402132e-05
Iter: 1882 loss: 1.11521949e-05
Iter: 1883 loss: 1.11376849e-05
Iter: 1884 loss: 1.11322115e-05
Iter: 1885 loss: 1.11586014e-05
Iter: 1886 loss: 1.11318495e-05
Iter: 1887 loss: 1.11266872e-05
Iter: 1888 loss: 1.11306081e-05
Iter: 1889 loss: 1.11236259e-05
Iter: 1890 loss: 1.11168083e-05
Iter: 1891 loss: 1.11469308e-05
Iter: 1892 loss: 1.11153e-05
Iter: 1893 loss: 1.11118916e-05
Iter: 1894 loss: 1.11114987e-05
Iter: 1895 loss: 1.1108913e-05
Iter: 1896 loss: 1.11070312e-05
Iter: 1897 loss: 1.11057834e-05
Iter: 1898 loss: 1.1101286e-05
Iter: 1899 loss: 1.11126574e-05
Iter: 1900 loss: 1.10994833e-05
Iter: 1901 loss: 1.1094543e-05
Iter: 1902 loss: 1.11108511e-05
Iter: 1903 loss: 1.10929723e-05
Iter: 1904 loss: 1.10900473e-05
Iter: 1905 loss: 1.11124737e-05
Iter: 1906 loss: 1.10898509e-05
Iter: 1907 loss: 1.10866495e-05
Iter: 1908 loss: 1.11141617e-05
Iter: 1909 loss: 1.10863239e-05
Iter: 1910 loss: 1.10845322e-05
Iter: 1911 loss: 1.10795972e-05
Iter: 1912 loss: 1.10971332e-05
Iter: 1913 loss: 1.10773e-05
Iter: 1914 loss: 1.10711753e-05
Iter: 1915 loss: 1.10920901e-05
Iter: 1916 loss: 1.10692636e-05
Iter: 1917 loss: 1.10636847e-05
Iter: 1918 loss: 1.1078e-05
Iter: 1919 loss: 1.10612555e-05
Iter: 1920 loss: 1.10549681e-05
Iter: 1921 loss: 1.10675792e-05
Iter: 1922 loss: 1.10521678e-05
Iter: 1923 loss: 1.10464862e-05
Iter: 1924 loss: 1.10691835e-05
Iter: 1925 loss: 1.10455258e-05
Iter: 1926 loss: 1.10404026e-05
Iter: 1927 loss: 1.10663314e-05
Iter: 1928 loss: 1.10393266e-05
Iter: 1929 loss: 1.10331621e-05
Iter: 1930 loss: 1.10515302e-05
Iter: 1931 loss: 1.10319779e-05
Iter: 1932 loss: 1.10281198e-05
Iter: 1933 loss: 1.10356195e-05
Iter: 1934 loss: 1.10269229e-05
Iter: 1935 loss: 1.10222663e-05
Iter: 1936 loss: 1.10284254e-05
Iter: 1937 loss: 1.10204255e-05
Iter: 1938 loss: 1.10149249e-05
Iter: 1939 loss: 1.10150477e-05
Iter: 1940 loss: 1.10105839e-05
Iter: 1941 loss: 1.10171168e-05
Iter: 1942 loss: 1.10076799e-05
Iter: 1943 loss: 1.1006714e-05
Iter: 1944 loss: 1.10016281e-05
Iter: 1945 loss: 1.10196434e-05
Iter: 1946 loss: 1.0999398e-05
Iter: 1947 loss: 1.09948014e-05
Iter: 1948 loss: 1.10005967e-05
Iter: 1949 loss: 1.0991791e-05
Iter: 1950 loss: 1.09857283e-05
Iter: 1951 loss: 1.09968933e-05
Iter: 1952 loss: 1.0983038e-05
Iter: 1953 loss: 1.09776247e-05
Iter: 1954 loss: 1.09962439e-05
Iter: 1955 loss: 1.09766388e-05
Iter: 1956 loss: 1.09707289e-05
Iter: 1957 loss: 1.09753382e-05
Iter: 1958 loss: 1.09674811e-05
Iter: 1959 loss: 1.09594639e-05
Iter: 1960 loss: 1.09940911e-05
Iter: 1961 loss: 1.09582888e-05
Iter: 1962 loss: 1.09539142e-05
Iter: 1963 loss: 1.09538505e-05
Iter: 1964 loss: 1.09507473e-05
Iter: 1965 loss: 1.09459597e-05
Iter: 1966 loss: 1.09461935e-05
Iter: 1967 loss: 1.09396551e-05
Iter: 1968 loss: 1.09602515e-05
Iter: 1969 loss: 1.09377743e-05
Iter: 1970 loss: 1.09332777e-05
Iter: 1971 loss: 1.09588646e-05
Iter: 1972 loss: 1.09326102e-05
Iter: 1973 loss: 1.09297398e-05
Iter: 1974 loss: 1.09743632e-05
Iter: 1975 loss: 1.09295916e-05
Iter: 1976 loss: 1.09259672e-05
Iter: 1977 loss: 1.09313705e-05
Iter: 1978 loss: 1.09247248e-05
Iter: 1979 loss: 1.09226021e-05
Iter: 1980 loss: 1.09181037e-05
Iter: 1981 loss: 1.09490711e-05
Iter: 1982 loss: 1.09172142e-05
Iter: 1983 loss: 1.09123321e-05
Iter: 1984 loss: 1.09386838e-05
Iter: 1985 loss: 1.09114881e-05
Iter: 1986 loss: 1.09071416e-05
Iter: 1987 loss: 1.0904515e-05
Iter: 1988 loss: 1.09024486e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi2.8
+ date
Sun Nov  8 01:32:56 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4/300_100_100_100_1 --function f1 --psi -2 --phi 2.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5e5b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5fe2378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5dfb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5dfbea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5eb6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5d9a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5d4fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5cec7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5ceb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5cebe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5c3f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5c42f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5c576a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feab276ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feab276a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5d01598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5d15620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feae5d018c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feab268a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feab269cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feab26c8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7feab2753a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c58d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c593a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c5846a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c5319d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c5bd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c5167b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c4f7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c51c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c4c0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c441488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c441c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c432510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c41bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fea8c3d3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.026721803
test_loss: 0.029075652
train_loss: 0.018073943
test_loss: 0.019889846
train_loss: 0.014802783
test_loss: 0.017658249
train_loss: 0.013537081
test_loss: 0.015657865
train_loss: 0.013121934
test_loss: 0.014867537
train_loss: 0.011991188
test_loss: 0.014214165
train_loss: 0.011908446
test_loss: 0.013426697
train_loss: 0.012816363
test_loss: 0.01377726
train_loss: 0.011891254
test_loss: 0.013787259
train_loss: 0.011585777
test_loss: 0.013094265
train_loss: 0.010316537
test_loss: 0.012795868
train_loss: 0.011603088
test_loss: 0.012479339
train_loss: 0.011533702
test_loss: 0.012730946
train_loss: 0.010667486
test_loss: 0.0126009695
train_loss: 0.010062087
test_loss: 0.012099909
train_loss: 0.010341354
test_loss: 0.012196773
train_loss: 0.010415836
test_loss: 0.012213234
train_loss: 0.010395573
test_loss: 0.012188575
train_loss: 0.010475338
test_loss: 0.012580018
train_loss: 0.011155058
test_loss: 0.011812958
train_loss: 0.011176082
test_loss: 0.01223913
train_loss: 0.011287604
test_loss: 0.01237205
train_loss: 0.0100259725
test_loss: 0.012329841
train_loss: 0.009905608
test_loss: 0.011871932
train_loss: 0.009166603
test_loss: 0.011336899
train_loss: 0.009201173
test_loss: 0.0114273485
train_loss: 0.009719498
test_loss: 0.011379967
train_loss: 0.009509517
test_loss: 0.010978684
train_loss: 0.009944834
test_loss: 0.011739798
train_loss: 0.009831574
test_loss: 0.011572896
train_loss: 0.009213054
test_loss: 0.0111566335
train_loss: 0.009828787
test_loss: 0.011535542
train_loss: 0.008906202
test_loss: 0.011044249
train_loss: 0.01037655
test_loss: 0.011447705
train_loss: 0.009785278
test_loss: 0.011340331
train_loss: 0.008725255
test_loss: 0.0111839175
train_loss: 0.010299503
test_loss: 0.011836084
train_loss: 0.009115584
test_loss: 0.011094972
train_loss: 0.009208716
test_loss: 0.010884499
train_loss: 0.009257154
test_loss: 0.010741987
train_loss: 0.008319449
test_loss: 0.011269323
train_loss: 0.011069662
test_loss: 0.0122349225
train_loss: 0.008934822
test_loss: 0.010544903
train_loss: 0.009698202
test_loss: 0.0105613265
train_loss: 0.008842546
test_loss: 0.010792486
train_loss: 0.008188669
test_loss: 0.0103873955
train_loss: 0.009972926
test_loss: 0.011240915
train_loss: 0.008447957
test_loss: 0.01076381
train_loss: 0.008640146
test_loss: 0.01053872
train_loss: 0.009050854
test_loss: 0.0103717735
train_loss: 0.008383404
test_loss: 0.010724405
train_loss: 0.008824367
test_loss: 0.010639179
train_loss: 0.00865151
test_loss: 0.0104424115
train_loss: 0.008117715
test_loss: 0.01038926
train_loss: 0.008535062
test_loss: 0.010967345
train_loss: 0.009087769
test_loss: 0.010746994
train_loss: 0.008283455
test_loss: 0.010894339
train_loss: 0.008211707
test_loss: 0.010433544
train_loss: 0.008452349
test_loss: 0.010624758
train_loss: 0.008045079
test_loss: 0.010284554
train_loss: 0.008601455
test_loss: 0.010418348
train_loss: 0.008064451
test_loss: 0.010460282
train_loss: 0.00811598
test_loss: 0.010144241
train_loss: 0.009528203
test_loss: 0.011213972
train_loss: 0.008444482
test_loss: 0.010529356
train_loss: 0.007522193
test_loss: 0.010443983
train_loss: 0.008159421
test_loss: 0.01024085
train_loss: 0.00779988
test_loss: 0.010470169
train_loss: 0.008464562
test_loss: 0.010233447
train_loss: 0.008539283
test_loss: 0.010443389
train_loss: 0.008828591
test_loss: 0.010721541
train_loss: 0.008233036
test_loss: 0.010799524
train_loss: 0.008138776
test_loss: 0.010382452
train_loss: 0.008230858
test_loss: 0.011275721
train_loss: 0.007926027
test_loss: 0.009844593
train_loss: 0.007915889
test_loss: 0.01026959
train_loss: 0.00818729
test_loss: 0.009960977
train_loss: 0.008056899
test_loss: 0.010251532
train_loss: 0.0076178946
test_loss: 0.009924766
train_loss: 0.0077161025
test_loss: 0.0102172075
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi2.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi2.8/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd705b41e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd70670950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd7068ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd705f6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd705f6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd705f6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd705366a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd704e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd704d41e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd704d4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd704940d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd70468f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd7045ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd7045eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd703c5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd703c59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd703c5e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd703c5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd7036e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd7036df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd7030ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd702ca620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd702ca510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd702af840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd70295620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd70295b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd7024a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd702217b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd70221378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd701c78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd70191950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd7019b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd7019ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd7016c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd7011e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd700ce730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000114681774
Iter: 2 loss: 0.000276344479
Iter: 3 loss: 0.000104499406
Iter: 4 loss: 9.88615939e-05
Iter: 5 loss: 9.16930658e-05
Iter: 6 loss: 9.11463867e-05
Iter: 7 loss: 8.6631233e-05
Iter: 8 loss: 7.80525588e-05
Iter: 9 loss: 0.000251316174
Iter: 10 loss: 7.8011828e-05
Iter: 11 loss: 7.13899717e-05
Iter: 12 loss: 8.09702397e-05
Iter: 13 loss: 6.81326928e-05
Iter: 14 loss: 6.58663193e-05
Iter: 15 loss: 6.29933493e-05
Iter: 16 loss: 6.27787085e-05
Iter: 17 loss: 5.96660975e-05
Iter: 18 loss: 5.85581511e-05
Iter: 19 loss: 5.6806588e-05
Iter: 20 loss: 5.44669747e-05
Iter: 21 loss: 6.24426757e-05
Iter: 22 loss: 5.38382e-05
Iter: 23 loss: 5.14235726e-05
Iter: 24 loss: 4.94303895e-05
Iter: 25 loss: 4.87329526e-05
Iter: 26 loss: 4.6650719e-05
Iter: 27 loss: 4.66331112e-05
Iter: 28 loss: 4.55519112e-05
Iter: 29 loss: 4.44127072e-05
Iter: 30 loss: 4.42195123e-05
Iter: 31 loss: 4.27517552e-05
Iter: 32 loss: 5.80108426e-05
Iter: 33 loss: 4.27105333e-05
Iter: 34 loss: 4.16605508e-05
Iter: 35 loss: 4.10658358e-05
Iter: 36 loss: 4.06135659e-05
Iter: 37 loss: 3.95265888e-05
Iter: 38 loss: 4.40403601e-05
Iter: 39 loss: 3.92950315e-05
Iter: 40 loss: 3.86131142e-05
Iter: 41 loss: 3.84017112e-05
Iter: 42 loss: 3.79977e-05
Iter: 43 loss: 3.70668495e-05
Iter: 44 loss: 4.69390288e-05
Iter: 45 loss: 3.70440175e-05
Iter: 46 loss: 3.66254571e-05
Iter: 47 loss: 3.65602718e-05
Iter: 48 loss: 3.63170111e-05
Iter: 49 loss: 3.57223398e-05
Iter: 50 loss: 4.18547788e-05
Iter: 51 loss: 3.56534729e-05
Iter: 52 loss: 3.50339142e-05
Iter: 53 loss: 3.90823188e-05
Iter: 54 loss: 3.49686925e-05
Iter: 55 loss: 3.46204142e-05
Iter: 56 loss: 3.48459398e-05
Iter: 57 loss: 3.43999309e-05
Iter: 58 loss: 3.38598147e-05
Iter: 59 loss: 3.50269256e-05
Iter: 60 loss: 3.36487938e-05
Iter: 61 loss: 3.32011841e-05
Iter: 62 loss: 3.33948046e-05
Iter: 63 loss: 3.28944952e-05
Iter: 64 loss: 3.2335578e-05
Iter: 65 loss: 3.73907242e-05
Iter: 66 loss: 3.23107961e-05
Iter: 67 loss: 3.20079089e-05
Iter: 68 loss: 3.17772065e-05
Iter: 69 loss: 3.1678479e-05
Iter: 70 loss: 3.1124986e-05
Iter: 71 loss: 3.55419397e-05
Iter: 72 loss: 3.10879877e-05
Iter: 73 loss: 3.08056551e-05
Iter: 74 loss: 3.32306663e-05
Iter: 75 loss: 3.07888477e-05
Iter: 76 loss: 3.05674403e-05
Iter: 77 loss: 3.0293806e-05
Iter: 78 loss: 3.02699445e-05
Iter: 79 loss: 3.02359513e-05
Iter: 80 loss: 3.01001874e-05
Iter: 81 loss: 2.9948962e-05
Iter: 82 loss: 2.99711701e-05
Iter: 83 loss: 2.9834926e-05
Iter: 84 loss: 2.97040315e-05
Iter: 85 loss: 2.94947185e-05
Iter: 86 loss: 2.9493e-05
Iter: 87 loss: 2.91600336e-05
Iter: 88 loss: 3.00940555e-05
Iter: 89 loss: 2.90532735e-05
Iter: 90 loss: 2.89032723e-05
Iter: 91 loss: 3.03803281e-05
Iter: 92 loss: 2.88979936e-05
Iter: 93 loss: 2.87519924e-05
Iter: 94 loss: 2.84845937e-05
Iter: 95 loss: 3.46143534e-05
Iter: 96 loss: 2.84836733e-05
Iter: 97 loss: 2.82716592e-05
Iter: 98 loss: 3.06399597e-05
Iter: 99 loss: 2.82675755e-05
Iter: 100 loss: 2.80618515e-05
Iter: 101 loss: 2.81940393e-05
Iter: 102 loss: 2.79318447e-05
Iter: 103 loss: 2.77019935e-05
Iter: 104 loss: 2.8510729e-05
Iter: 105 loss: 2.76433602e-05
Iter: 106 loss: 2.74107115e-05
Iter: 107 loss: 2.87503553e-05
Iter: 108 loss: 2.73790811e-05
Iter: 109 loss: 2.72111247e-05
Iter: 110 loss: 2.84369871e-05
Iter: 111 loss: 2.71974259e-05
Iter: 112 loss: 2.71052e-05
Iter: 113 loss: 2.77941745e-05
Iter: 114 loss: 2.70978016e-05
Iter: 115 loss: 2.69852699e-05
Iter: 116 loss: 2.71506469e-05
Iter: 117 loss: 2.69307966e-05
Iter: 118 loss: 2.68438544e-05
Iter: 119 loss: 2.67842679e-05
Iter: 120 loss: 2.67533014e-05
Iter: 121 loss: 2.66557945e-05
Iter: 122 loss: 2.69839766e-05
Iter: 123 loss: 2.66285133e-05
Iter: 124 loss: 2.65092931e-05
Iter: 125 loss: 2.64441405e-05
Iter: 126 loss: 2.63912625e-05
Iter: 127 loss: 2.62513604e-05
Iter: 128 loss: 2.79836713e-05
Iter: 129 loss: 2.62495469e-05
Iter: 130 loss: 2.61440746e-05
Iter: 131 loss: 2.59536646e-05
Iter: 132 loss: 3.05997528e-05
Iter: 133 loss: 2.59542467e-05
Iter: 134 loss: 2.57720549e-05
Iter: 135 loss: 2.84847702e-05
Iter: 136 loss: 2.57715e-05
Iter: 137 loss: 2.56480307e-05
Iter: 138 loss: 2.55651394e-05
Iter: 139 loss: 2.55179093e-05
Iter: 140 loss: 2.53665421e-05
Iter: 141 loss: 2.7680584e-05
Iter: 142 loss: 2.5366362e-05
Iter: 143 loss: 2.52817372e-05
Iter: 144 loss: 2.55141022e-05
Iter: 145 loss: 2.52537084e-05
Iter: 146 loss: 2.51586789e-05
Iter: 147 loss: 2.59919525e-05
Iter: 148 loss: 2.51546535e-05
Iter: 149 loss: 2.50787562e-05
Iter: 150 loss: 2.5483565e-05
Iter: 151 loss: 2.50662652e-05
Iter: 152 loss: 2.50230223e-05
Iter: 153 loss: 2.49798886e-05
Iter: 154 loss: 2.497069e-05
Iter: 155 loss: 2.48951728e-05
Iter: 156 loss: 2.4828696e-05
Iter: 157 loss: 2.48092729e-05
Iter: 158 loss: 2.4670313e-05
Iter: 159 loss: 2.53804e-05
Iter: 160 loss: 2.46477648e-05
Iter: 161 loss: 2.45464253e-05
Iter: 162 loss: 2.48283468e-05
Iter: 163 loss: 2.4513718e-05
Iter: 164 loss: 2.4398505e-05
Iter: 165 loss: 2.45498049e-05
Iter: 166 loss: 2.43398936e-05
Iter: 167 loss: 2.424579e-05
Iter: 168 loss: 2.45597403e-05
Iter: 169 loss: 2.42202404e-05
Iter: 170 loss: 2.40929494e-05
Iter: 171 loss: 2.40422214e-05
Iter: 172 loss: 2.39743858e-05
Iter: 173 loss: 2.38714183e-05
Iter: 174 loss: 2.51928705e-05
Iter: 175 loss: 2.38708599e-05
Iter: 176 loss: 2.37707554e-05
Iter: 177 loss: 2.38306129e-05
Iter: 178 loss: 2.37058139e-05
Iter: 179 loss: 2.36758497e-05
Iter: 180 loss: 2.36472042e-05
Iter: 181 loss: 2.36120977e-05
Iter: 182 loss: 2.37941349e-05
Iter: 183 loss: 2.36068081e-05
Iter: 184 loss: 2.35809021e-05
Iter: 185 loss: 2.35216394e-05
Iter: 186 loss: 2.42726819e-05
Iter: 187 loss: 2.351683e-05
Iter: 188 loss: 2.34401632e-05
Iter: 189 loss: 2.36092837e-05
Iter: 190 loss: 2.34107738e-05
Iter: 191 loss: 2.33450319e-05
Iter: 192 loss: 2.36411761e-05
Iter: 193 loss: 2.33323444e-05
Iter: 194 loss: 2.32690581e-05
Iter: 195 loss: 2.33560258e-05
Iter: 196 loss: 2.32377206e-05
Iter: 197 loss: 2.31723661e-05
Iter: 198 loss: 2.34473282e-05
Iter: 199 loss: 2.31587146e-05
Iter: 200 loss: 2.30929145e-05
Iter: 201 loss: 2.3046694e-05
Iter: 202 loss: 2.30226015e-05
Iter: 203 loss: 2.29460966e-05
Iter: 204 loss: 2.40149166e-05
Iter: 205 loss: 2.29456055e-05
Iter: 206 loss: 2.28970421e-05
Iter: 207 loss: 2.2871749e-05
Iter: 208 loss: 2.28499339e-05
Iter: 209 loss: 2.27770433e-05
Iter: 210 loss: 2.34653489e-05
Iter: 211 loss: 2.27742748e-05
Iter: 212 loss: 2.27426026e-05
Iter: 213 loss: 2.27428609e-05
Iter: 214 loss: 2.27125984e-05
Iter: 215 loss: 2.28143654e-05
Iter: 216 loss: 2.27060555e-05
Iter: 217 loss: 2.26765733e-05
Iter: 218 loss: 2.26326229e-05
Iter: 219 loss: 2.26321972e-05
Iter: 220 loss: 2.25911244e-05
Iter: 221 loss: 2.26992779e-05
Iter: 222 loss: 2.25776857e-05
Iter: 223 loss: 2.25304102e-05
Iter: 224 loss: 2.25076474e-05
Iter: 225 loss: 2.24839496e-05
Iter: 226 loss: 2.24339437e-05
Iter: 227 loss: 2.32208895e-05
Iter: 228 loss: 2.24336e-05
Iter: 229 loss: 2.23978477e-05
Iter: 230 loss: 2.23600946e-05
Iter: 231 loss: 2.23534735e-05
Iter: 232 loss: 2.22805447e-05
Iter: 233 loss: 2.26262073e-05
Iter: 234 loss: 2.22679337e-05
Iter: 235 loss: 2.22182389e-05
Iter: 236 loss: 2.22679337e-05
Iter: 237 loss: 2.21904756e-05
Iter: 238 loss: 2.21205337e-05
Iter: 239 loss: 2.23176321e-05
Iter: 240 loss: 2.20977599e-05
Iter: 241 loss: 2.20430466e-05
Iter: 242 loss: 2.23160278e-05
Iter: 243 loss: 2.20334568e-05
Iter: 244 loss: 2.1982778e-05
Iter: 245 loss: 2.23590614e-05
Iter: 246 loss: 2.19782414e-05
Iter: 247 loss: 2.19404428e-05
Iter: 248 loss: 2.24660507e-05
Iter: 249 loss: 2.19402391e-05
Iter: 250 loss: 2.1920263e-05
Iter: 251 loss: 2.18997411e-05
Iter: 252 loss: 2.18963833e-05
Iter: 253 loss: 2.1867756e-05
Iter: 254 loss: 2.18335881e-05
Iter: 255 loss: 2.18304394e-05
Iter: 256 loss: 2.17652705e-05
Iter: 257 loss: 2.19469293e-05
Iter: 258 loss: 2.1744323e-05
Iter: 259 loss: 2.16986264e-05
Iter: 260 loss: 2.19362046e-05
Iter: 261 loss: 2.16924054e-05
Iter: 262 loss: 2.16418593e-05
Iter: 263 loss: 2.16040389e-05
Iter: 264 loss: 2.15870496e-05
Iter: 265 loss: 2.15216915e-05
Iter: 266 loss: 2.21256705e-05
Iter: 267 loss: 2.15184446e-05
Iter: 268 loss: 2.14649808e-05
Iter: 269 loss: 2.14625e-05
Iter: 270 loss: 2.14213651e-05
Iter: 271 loss: 2.13701387e-05
Iter: 272 loss: 2.18849273e-05
Iter: 273 loss: 2.13683816e-05
Iter: 274 loss: 2.13259955e-05
Iter: 275 loss: 2.13132607e-05
Iter: 276 loss: 2.12879258e-05
Iter: 277 loss: 2.12542891e-05
Iter: 278 loss: 2.12498926e-05
Iter: 279 loss: 2.12287705e-05
Iter: 280 loss: 2.12290633e-05
Iter: 281 loss: 2.12159e-05
Iter: 282 loss: 2.11895585e-05
Iter: 283 loss: 2.17345842e-05
Iter: 284 loss: 2.11891911e-05
Iter: 285 loss: 2.11574188e-05
Iter: 286 loss: 2.11323149e-05
Iter: 287 loss: 2.1121974e-05
Iter: 288 loss: 2.107619e-05
Iter: 289 loss: 2.14114225e-05
Iter: 290 loss: 2.10721046e-05
Iter: 291 loss: 2.10322978e-05
Iter: 292 loss: 2.10473e-05
Iter: 293 loss: 2.10047692e-05
Iter: 294 loss: 2.09592272e-05
Iter: 295 loss: 2.1339376e-05
Iter: 296 loss: 2.09555019e-05
Iter: 297 loss: 2.09264581e-05
Iter: 298 loss: 2.09117643e-05
Iter: 299 loss: 2.08974088e-05
Iter: 300 loss: 2.08472811e-05
Iter: 301 loss: 2.11281658e-05
Iter: 302 loss: 2.08401016e-05
Iter: 303 loss: 2.08102902e-05
Iter: 304 loss: 2.0836962e-05
Iter: 305 loss: 2.07926441e-05
Iter: 306 loss: 2.07490702e-05
Iter: 307 loss: 2.08848469e-05
Iter: 308 loss: 2.07363373e-05
Iter: 309 loss: 2.07025387e-05
Iter: 310 loss: 2.09724258e-05
Iter: 311 loss: 2.07012763e-05
Iter: 312 loss: 2.06773366e-05
Iter: 313 loss: 2.06772602e-05
Iter: 314 loss: 2.06610985e-05
Iter: 315 loss: 2.06344721e-05
Iter: 316 loss: 2.06342229e-05
Iter: 317 loss: 2.06117329e-05
Iter: 318 loss: 2.06504046e-05
Iter: 319 loss: 2.06003424e-05
Iter: 320 loss: 2.0571897e-05
Iter: 321 loss: 2.05520646e-05
Iter: 322 loss: 2.05432389e-05
Iter: 323 loss: 2.04906937e-05
Iter: 324 loss: 2.07794874e-05
Iter: 325 loss: 2.04838198e-05
Iter: 326 loss: 2.04535463e-05
Iter: 327 loss: 2.05787765e-05
Iter: 328 loss: 2.04467797e-05
Iter: 329 loss: 2.04131211e-05
Iter: 330 loss: 2.0396903e-05
Iter: 331 loss: 2.03806158e-05
Iter: 332 loss: 2.03428535e-05
Iter: 333 loss: 2.07137855e-05
Iter: 334 loss: 2.03416093e-05
Iter: 335 loss: 2.03104682e-05
Iter: 336 loss: 2.02906213e-05
Iter: 337 loss: 2.02784249e-05
Iter: 338 loss: 2.02401552e-05
Iter: 339 loss: 2.0595362e-05
Iter: 340 loss: 2.02386182e-05
Iter: 341 loss: 2.02061565e-05
Iter: 342 loss: 2.02650735e-05
Iter: 343 loss: 2.01922103e-05
Iter: 344 loss: 2.01780185e-05
Iter: 345 loss: 2.01706371e-05
Iter: 346 loss: 2.0156207e-05
Iter: 347 loss: 2.01434214e-05
Iter: 348 loss: 2.01394978e-05
Iter: 349 loss: 2.01217572e-05
Iter: 350 loss: 2.01001203e-05
Iter: 351 loss: 2.0097501e-05
Iter: 352 loss: 2.00582017e-05
Iter: 353 loss: 2.00992909e-05
Iter: 354 loss: 2.00368286e-05
Iter: 355 loss: 1.99999413e-05
Iter: 356 loss: 2.03013624e-05
Iter: 357 loss: 1.99978931e-05
Iter: 358 loss: 1.9967707e-05
Iter: 359 loss: 1.99817787e-05
Iter: 360 loss: 1.9947689e-05
Iter: 361 loss: 1.9913683e-05
Iter: 362 loss: 2.01763869e-05
Iter: 363 loss: 1.99116075e-05
Iter: 364 loss: 1.98891703e-05
Iter: 365 loss: 1.98755442e-05
Iter: 366 loss: 1.98653433e-05
Iter: 367 loss: 1.98252601e-05
Iter: 368 loss: 2.00201775e-05
Iter: 369 loss: 1.9818257e-05
Iter: 370 loss: 1.97909831e-05
Iter: 371 loss: 1.98068519e-05
Iter: 372 loss: 1.97738791e-05
Iter: 373 loss: 1.97335958e-05
Iter: 374 loss: 1.9948282e-05
Iter: 375 loss: 1.97271038e-05
Iter: 376 loss: 1.97302761e-05
Iter: 377 loss: 1.97151057e-05
Iter: 378 loss: 1.9706129e-05
Iter: 379 loss: 1.96899982e-05
Iter: 380 loss: 1.96902656e-05
Iter: 381 loss: 1.96701076e-05
Iter: 382 loss: 1.96565707e-05
Iter: 383 loss: 1.96491146e-05
Iter: 384 loss: 1.96239016e-05
Iter: 385 loss: 1.97351856e-05
Iter: 386 loss: 1.96190158e-05
Iter: 387 loss: 1.95939829e-05
Iter: 388 loss: 1.96109759e-05
Iter: 389 loss: 1.95785487e-05
Iter: 390 loss: 1.95509019e-05
Iter: 391 loss: 1.97681111e-05
Iter: 392 loss: 1.9548972e-05
Iter: 393 loss: 1.95308512e-05
Iter: 394 loss: 1.95468747e-05
Iter: 395 loss: 1.95206449e-05
Iter: 396 loss: 1.94929962e-05
Iter: 397 loss: 1.95150187e-05
Iter: 398 loss: 1.94766726e-05
Iter: 399 loss: 1.94498825e-05
Iter: 400 loss: 1.95848716e-05
Iter: 401 loss: 1.94454442e-05
Iter: 402 loss: 1.9416213e-05
Iter: 403 loss: 1.94146596e-05
Iter: 404 loss: 1.93928936e-05
Iter: 405 loss: 1.9366491e-05
Iter: 406 loss: 1.96813016e-05
Iter: 407 loss: 1.93658761e-05
Iter: 408 loss: 1.93493106e-05
Iter: 409 loss: 1.93496599e-05
Iter: 410 loss: 1.93325232e-05
Iter: 411 loss: 1.93246706e-05
Iter: 412 loss: 1.93158485e-05
Iter: 413 loss: 1.93004707e-05
Iter: 414 loss: 1.9318697e-05
Iter: 415 loss: 1.92932494e-05
Iter: 416 loss: 1.9276853e-05
Iter: 417 loss: 1.92568332e-05
Iter: 418 loss: 1.92548068e-05
Iter: 419 loss: 1.92242769e-05
Iter: 420 loss: 1.94529202e-05
Iter: 421 loss: 1.92217885e-05
Iter: 422 loss: 1.92004991e-05
Iter: 423 loss: 1.9234596e-05
Iter: 424 loss: 1.91914e-05
Iter: 425 loss: 1.91628387e-05
Iter: 426 loss: 1.92090702e-05
Iter: 427 loss: 1.9151008e-05
Iter: 428 loss: 1.91252948e-05
Iter: 429 loss: 1.92450825e-05
Iter: 430 loss: 1.91212384e-05
Iter: 431 loss: 1.90963074e-05
Iter: 432 loss: 1.90918618e-05
Iter: 433 loss: 1.90753108e-05
Iter: 434 loss: 1.9050125e-05
Iter: 435 loss: 1.93328488e-05
Iter: 436 loss: 1.90485371e-05
Iter: 437 loss: 1.90285828e-05
Iter: 438 loss: 1.89993189e-05
Iter: 439 loss: 1.89978637e-05
Iter: 440 loss: 1.9012843e-05
Iter: 441 loss: 1.89833609e-05
Iter: 442 loss: 1.89705497e-05
Iter: 443 loss: 1.89949897e-05
Iter: 444 loss: 1.89659459e-05
Iter: 445 loss: 1.89576895e-05
Iter: 446 loss: 1.89398306e-05
Iter: 447 loss: 1.92345979e-05
Iter: 448 loss: 1.89400016e-05
Iter: 449 loss: 1.89143811e-05
Iter: 450 loss: 1.89533075e-05
Iter: 451 loss: 1.89031161e-05
Iter: 452 loss: 1.88802696e-05
Iter: 453 loss: 1.894874e-05
Iter: 454 loss: 1.88750255e-05
Iter: 455 loss: 1.88494014e-05
Iter: 456 loss: 1.89061066e-05
Iter: 457 loss: 1.88397917e-05
Iter: 458 loss: 1.88190515e-05
Iter: 459 loss: 1.89246311e-05
Iter: 460 loss: 1.88161193e-05
Iter: 461 loss: 1.87942187e-05
Iter: 462 loss: 1.8781051e-05
Iter: 463 loss: 1.87727928e-05
Iter: 464 loss: 1.87479382e-05
Iter: 465 loss: 1.90444316e-05
Iter: 466 loss: 1.87477744e-05
Iter: 467 loss: 1.8731047e-05
Iter: 468 loss: 1.87109417e-05
Iter: 469 loss: 1.87080732e-05
Iter: 470 loss: 1.86760553e-05
Iter: 471 loss: 1.88997801e-05
Iter: 472 loss: 1.86728867e-05
Iter: 473 loss: 1.86545149e-05
Iter: 474 loss: 1.88183421e-05
Iter: 475 loss: 1.86538491e-05
Iter: 476 loss: 1.8632727e-05
Iter: 477 loss: 1.87462138e-05
Iter: 478 loss: 1.8629451e-05
Iter: 479 loss: 1.86197794e-05
Iter: 480 loss: 1.86081634e-05
Iter: 481 loss: 1.8606228e-05
Iter: 482 loss: 1.8592e-05
Iter: 483 loss: 1.85838271e-05
Iter: 484 loss: 1.85775862e-05
Iter: 485 loss: 1.85516456e-05
Iter: 486 loss: 1.8656734e-05
Iter: 487 loss: 1.85450135e-05
Iter: 488 loss: 1.85300432e-05
Iter: 489 loss: 1.85905774e-05
Iter: 490 loss: 1.8527393e-05
Iter: 491 loss: 1.85088902e-05
Iter: 492 loss: 1.85181743e-05
Iter: 493 loss: 1.8496552e-05
Iter: 494 loss: 1.84816799e-05
Iter: 495 loss: 1.86466277e-05
Iter: 496 loss: 1.84816599e-05
Iter: 497 loss: 1.84690944e-05
Iter: 498 loss: 1.84533492e-05
Iter: 499 loss: 1.84520395e-05
Iter: 500 loss: 1.84343826e-05
Iter: 501 loss: 1.86489378e-05
Iter: 502 loss: 1.84333094e-05
Iter: 503 loss: 1.84195887e-05
Iter: 504 loss: 1.8410683e-05
Iter: 505 loss: 1.84053224e-05
Iter: 506 loss: 1.83895463e-05
Iter: 507 loss: 1.8634044e-05
Iter: 508 loss: 1.8389379e-05
Iter: 509 loss: 1.83855263e-05
Iter: 510 loss: 1.8383329e-05
Iter: 511 loss: 1.83788234e-05
Iter: 512 loss: 1.83664306e-05
Iter: 513 loss: 1.83964e-05
Iter: 514 loss: 1.83596967e-05
Iter: 515 loss: 1.83394804e-05
Iter: 516 loss: 1.84317141e-05
Iter: 517 loss: 1.8335706e-05
Iter: 518 loss: 1.83227094e-05
Iter: 519 loss: 1.83242373e-05
Iter: 520 loss: 1.83128632e-05
Iter: 521 loss: 1.82898111e-05
Iter: 522 loss: 1.83638949e-05
Iter: 523 loss: 1.82832373e-05
Iter: 524 loss: 1.82683543e-05
Iter: 525 loss: 1.83458651e-05
Iter: 526 loss: 1.82656731e-05
Iter: 527 loss: 1.8250872e-05
Iter: 528 loss: 1.82521562e-05
Iter: 529 loss: 1.82385793e-05
Iter: 530 loss: 1.82220483e-05
Iter: 531 loss: 1.8340088e-05
Iter: 532 loss: 1.82208914e-05
Iter: 533 loss: 1.82045605e-05
Iter: 534 loss: 1.81946743e-05
Iter: 535 loss: 1.81879786e-05
Iter: 536 loss: 1.81719297e-05
Iter: 537 loss: 1.8391258e-05
Iter: 538 loss: 1.81716623e-05
Iter: 539 loss: 1.81586674e-05
Iter: 540 loss: 1.81585365e-05
Iter: 541 loss: 1.81479299e-05
Iter: 542 loss: 1.81580308e-05
Iter: 543 loss: 1.81418709e-05
Iter: 544 loss: 1.81367504e-05
Iter: 545 loss: 1.8127881e-05
Iter: 546 loss: 1.83426855e-05
Iter: 547 loss: 1.81279393e-05
Iter: 548 loss: 1.81192154e-05
Iter: 549 loss: 1.81053074e-05
Iter: 550 loss: 1.81064843e-05
Iter: 551 loss: 1.80872903e-05
Iter: 552 loss: 1.82424847e-05
Iter: 553 loss: 1.80861807e-05
Iter: 554 loss: 1.80736606e-05
Iter: 555 loss: 1.80924544e-05
Iter: 556 loss: 1.80678144e-05
Iter: 557 loss: 1.80502902e-05
Iter: 558 loss: 1.80680381e-05
Iter: 559 loss: 1.80402676e-05
Iter: 560 loss: 1.80243696e-05
Iter: 561 loss: 1.81231262e-05
Iter: 562 loss: 1.8022849e-05
Iter: 563 loss: 1.80043717e-05
Iter: 564 loss: 1.79863418e-05
Iter: 565 loss: 1.79827966e-05
Iter: 566 loss: 1.79657945e-05
Iter: 567 loss: 1.82359254e-05
Iter: 568 loss: 1.79655271e-05
Iter: 569 loss: 1.79505751e-05
Iter: 570 loss: 1.79459712e-05
Iter: 571 loss: 1.79370727e-05
Iter: 572 loss: 1.79204217e-05
Iter: 573 loss: 1.80469742e-05
Iter: 574 loss: 1.79188137e-05
Iter: 575 loss: 1.79126182e-05
Iter: 576 loss: 1.79105318e-05
Iter: 577 loss: 1.79008748e-05
Iter: 578 loss: 1.78905648e-05
Iter: 579 loss: 1.78882874e-05
Iter: 580 loss: 1.78798673e-05
Iter: 581 loss: 1.78811315e-05
Iter: 582 loss: 1.78728078e-05
Iter: 583 loss: 1.78600603e-05
Iter: 584 loss: 1.7855622e-05
Iter: 585 loss: 1.78480168e-05
Iter: 586 loss: 1.78337978e-05
Iter: 587 loss: 1.79855288e-05
Iter: 588 loss: 1.78332193e-05
Iter: 589 loss: 1.78207574e-05
Iter: 590 loss: 1.78257724e-05
Iter: 591 loss: 1.78113987e-05
Iter: 592 loss: 1.77970396e-05
Iter: 593 loss: 1.78938353e-05
Iter: 594 loss: 1.77952061e-05
Iter: 595 loss: 1.77835427e-05
Iter: 596 loss: 1.77812144e-05
Iter: 597 loss: 1.77740949e-05
Iter: 598 loss: 1.77541115e-05
Iter: 599 loss: 1.78093214e-05
Iter: 600 loss: 1.77471884e-05
Iter: 601 loss: 1.77325492e-05
Iter: 602 loss: 1.77810798e-05
Iter: 603 loss: 1.77282291e-05
Iter: 604 loss: 1.77078418e-05
Iter: 605 loss: 1.76960402e-05
Iter: 606 loss: 1.76862086e-05
Iter: 607 loss: 1.7680326e-05
Iter: 608 loss: 1.76752383e-05
Iter: 609 loss: 1.76634458e-05
Iter: 610 loss: 1.77168477e-05
Iter: 611 loss: 1.76611793e-05
Iter: 612 loss: 1.76533958e-05
Iter: 613 loss: 1.76298272e-05
Iter: 614 loss: 1.77398178e-05
Iter: 615 loss: 1.76223366e-05
Iter: 616 loss: 1.76008689e-05
Iter: 617 loss: 1.78670198e-05
Iter: 618 loss: 1.76009416e-05
Iter: 619 loss: 1.75852383e-05
Iter: 620 loss: 1.75846671e-05
Iter: 621 loss: 1.75724599e-05
Iter: 622 loss: 1.75577079e-05
Iter: 623 loss: 1.75579044e-05
Iter: 624 loss: 1.75484038e-05
Iter: 625 loss: 1.7543618e-05
Iter: 626 loss: 1.75394398e-05
Iter: 627 loss: 1.75212099e-05
Iter: 628 loss: 1.75561963e-05
Iter: 629 loss: 1.7513179e-05
Iter: 630 loss: 1.75001369e-05
Iter: 631 loss: 1.75615824e-05
Iter: 632 loss: 1.74980305e-05
Iter: 633 loss: 1.74833076e-05
Iter: 634 loss: 1.7480681e-05
Iter: 635 loss: 1.74718807e-05
Iter: 636 loss: 1.74566867e-05
Iter: 637 loss: 1.76233498e-05
Iter: 638 loss: 1.74562265e-05
Iter: 639 loss: 1.74440502e-05
Iter: 640 loss: 1.7456372e-05
Iter: 641 loss: 1.74380439e-05
Iter: 642 loss: 1.74377928e-05
Iter: 643 loss: 1.74305242e-05
Iter: 644 loss: 1.74277884e-05
Iter: 645 loss: 1.74202833e-05
Iter: 646 loss: 1.74768393e-05
Iter: 647 loss: 1.74190445e-05
Iter: 648 loss: 1.74072775e-05
Iter: 649 loss: 1.73959197e-05
Iter: 650 loss: 1.73937078e-05
Iter: 651 loss: 1.73808912e-05
Iter: 652 loss: 1.75660098e-05
Iter: 653 loss: 1.73811823e-05
Iter: 654 loss: 1.73717181e-05
Iter: 655 loss: 1.73557673e-05
Iter: 656 loss: 1.73553963e-05
Iter: 657 loss: 1.73403641e-05
Iter: 658 loss: 1.73403714e-05
Iter: 659 loss: 1.73291646e-05
Iter: 660 loss: 1.73149383e-05
Iter: 661 loss: 1.73136959e-05
Iter: 662 loss: 1.72975597e-05
Iter: 663 loss: 1.75139448e-05
Iter: 664 loss: 1.72974833e-05
Iter: 665 loss: 1.72869331e-05
Iter: 666 loss: 1.72704895e-05
Iter: 667 loss: 1.72707259e-05
Iter: 668 loss: 1.7250688e-05
Iter: 669 loss: 1.74631423e-05
Iter: 670 loss: 1.72501568e-05
Iter: 671 loss: 1.72383661e-05
Iter: 672 loss: 1.72981636e-05
Iter: 673 loss: 1.72368364e-05
Iter: 674 loss: 1.72320833e-05
Iter: 675 loss: 1.7230861e-05
Iter: 676 loss: 1.7223676e-05
Iter: 677 loss: 1.72145665e-05
Iter: 678 loss: 1.72144391e-05
Iter: 679 loss: 1.7207065e-05
Iter: 680 loss: 1.7196382e-05
Iter: 681 loss: 1.71960746e-05
Iter: 682 loss: 1.7179842e-05
Iter: 683 loss: 1.7280674e-05
Iter: 684 loss: 1.71773827e-05
Iter: 685 loss: 1.71659e-05
Iter: 686 loss: 1.71869688e-05
Iter: 687 loss: 1.71612846e-05
Iter: 688 loss: 1.71478896e-05
Iter: 689 loss: 1.71818319e-05
Iter: 690 loss: 1.71432075e-05
Iter: 691 loss: 1.71319261e-05
Iter: 692 loss: 1.72131331e-05
Iter: 693 loss: 1.7130933e-05
Iter: 694 loss: 1.71201282e-05
Iter: 695 loss: 1.71072061e-05
Iter: 696 loss: 1.71064457e-05
Iter: 697 loss: 1.70902185e-05
Iter: 698 loss: 1.7250808e-05
Iter: 699 loss: 1.70898129e-05
Iter: 700 loss: 1.70768108e-05
Iter: 701 loss: 1.70601506e-05
Iter: 702 loss: 1.70584044e-05
Iter: 703 loss: 1.70445455e-05
Iter: 704 loss: 1.70443782e-05
Iter: 705 loss: 1.70353651e-05
Iter: 706 loss: 1.70961212e-05
Iter: 707 loss: 1.70341373e-05
Iter: 708 loss: 1.70211315e-05
Iter: 709 loss: 1.70260064e-05
Iter: 710 loss: 1.70117382e-05
Iter: 711 loss: 1.70039566e-05
Iter: 712 loss: 1.70059466e-05
Iter: 713 loss: 1.69982432e-05
Iter: 714 loss: 1.6989281e-05
Iter: 715 loss: 1.69788127e-05
Iter: 716 loss: 1.69772e-05
Iter: 717 loss: 1.69621289e-05
Iter: 718 loss: 1.70846561e-05
Iter: 719 loss: 1.69608156e-05
Iter: 720 loss: 1.69476698e-05
Iter: 721 loss: 1.6938804e-05
Iter: 722 loss: 1.69337382e-05
Iter: 723 loss: 1.69149789e-05
Iter: 724 loss: 1.71357788e-05
Iter: 725 loss: 1.69148952e-05
Iter: 726 loss: 1.69049199e-05
Iter: 727 loss: 1.69396662e-05
Iter: 728 loss: 1.69020605e-05
Iter: 729 loss: 1.68911356e-05
Iter: 730 loss: 1.68831084e-05
Iter: 731 loss: 1.68789356e-05
Iter: 732 loss: 1.68659353e-05
Iter: 733 loss: 1.69507621e-05
Iter: 734 loss: 1.68637835e-05
Iter: 735 loss: 1.68494662e-05
Iter: 736 loss: 1.68469887e-05
Iter: 737 loss: 1.68366714e-05
Iter: 738 loss: 1.68208735e-05
Iter: 739 loss: 1.70392686e-05
Iter: 740 loss: 1.6820748e-05
Iter: 741 loss: 1.68173319e-05
Iter: 742 loss: 1.68147562e-05
Iter: 743 loss: 1.68112419e-05
Iter: 744 loss: 1.67979088e-05
Iter: 745 loss: 1.68188799e-05
Iter: 746 loss: 1.67889721e-05
Iter: 747 loss: 1.67764956e-05
Iter: 748 loss: 1.6937669e-05
Iter: 749 loss: 1.67769194e-05
Iter: 750 loss: 1.67659673e-05
Iter: 751 loss: 1.67498492e-05
Iter: 752 loss: 1.67497128e-05
Iter: 753 loss: 1.67387716e-05
Iter: 754 loss: 1.67380713e-05
Iter: 755 loss: 1.67305243e-05
Iter: 756 loss: 1.67205853e-05
Iter: 757 loss: 1.67196285e-05
Iter: 758 loss: 1.6706641e-05
Iter: 759 loss: 1.68588922e-05
Iter: 760 loss: 1.67066592e-05
Iter: 761 loss: 1.66955397e-05
Iter: 762 loss: 1.67143244e-05
Iter: 763 loss: 1.66909103e-05
Iter: 764 loss: 1.66789177e-05
Iter: 765 loss: 1.66956161e-05
Iter: 766 loss: 1.66721911e-05
Iter: 767 loss: 1.66611298e-05
Iter: 768 loss: 1.67020553e-05
Iter: 769 loss: 1.66579248e-05
Iter: 770 loss: 1.66445461e-05
Iter: 771 loss: 1.66657501e-05
Iter: 772 loss: 1.66383616e-05
Iter: 773 loss: 1.6642538e-05
Iter: 774 loss: 1.66337231e-05
Iter: 775 loss: 1.66296632e-05
Iter: 776 loss: 1.66239806e-05
Iter: 777 loss: 1.66236987e-05
Iter: 778 loss: 1.66165937e-05
Iter: 779 loss: 1.65992678e-05
Iter: 780 loss: 1.67780599e-05
Iter: 781 loss: 1.65970359e-05
Iter: 782 loss: 1.65821875e-05
Iter: 783 loss: 1.68100414e-05
Iter: 784 loss: 1.65827332e-05
Iter: 785 loss: 1.65703877e-05
Iter: 786 loss: 1.6563783e-05
Iter: 787 loss: 1.65587444e-05
Iter: 788 loss: 1.65432466e-05
Iter: 789 loss: 1.66878726e-05
Iter: 790 loss: 1.65426882e-05
Iter: 791 loss: 1.65295151e-05
Iter: 792 loss: 1.65376823e-05
Iter: 793 loss: 1.6521848e-05
Iter: 794 loss: 1.65035181e-05
Iter: 795 loss: 1.66157679e-05
Iter: 796 loss: 1.65012898e-05
Iter: 797 loss: 1.64914672e-05
Iter: 798 loss: 1.64825778e-05
Iter: 799 loss: 1.64800913e-05
Iter: 800 loss: 1.64614903e-05
Iter: 801 loss: 1.65156089e-05
Iter: 802 loss: 1.64552603e-05
Iter: 803 loss: 1.64395151e-05
Iter: 804 loss: 1.64999328e-05
Iter: 805 loss: 1.64360208e-05
Iter: 806 loss: 1.642e-05
Iter: 807 loss: 1.64432877e-05
Iter: 808 loss: 1.6412585e-05
Iter: 809 loss: 1.64195862e-05
Iter: 810 loss: 1.64061785e-05
Iter: 811 loss: 1.64025259e-05
Iter: 812 loss: 1.63928071e-05
Iter: 813 loss: 1.64261583e-05
Iter: 814 loss: 1.63887053e-05
Iter: 815 loss: 1.63724617e-05
Iter: 816 loss: 1.6375694e-05
Iter: 817 loss: 1.63618352e-05
Iter: 818 loss: 1.63463028e-05
Iter: 819 loss: 1.64231933e-05
Iter: 820 loss: 1.63436489e-05
Iter: 821 loss: 1.63256445e-05
Iter: 822 loss: 1.635433e-05
Iter: 823 loss: 1.63172463e-05
Iter: 824 loss: 1.63056175e-05
Iter: 825 loss: 1.63742807e-05
Iter: 826 loss: 1.63044551e-05
Iter: 827 loss: 1.62907381e-05
Iter: 828 loss: 1.62906344e-05
Iter: 829 loss: 1.62802771e-05
Iter: 830 loss: 1.6267164e-05
Iter: 831 loss: 1.64654703e-05
Iter: 832 loss: 1.62674e-05
Iter: 833 loss: 1.6258793e-05
Iter: 834 loss: 1.62545e-05
Iter: 835 loss: 1.62511187e-05
Iter: 836 loss: 1.6236625e-05
Iter: 837 loss: 1.62938377e-05
Iter: 838 loss: 1.62336801e-05
Iter: 839 loss: 1.62237593e-05
Iter: 840 loss: 1.62165397e-05
Iter: 841 loss: 1.62123197e-05
Iter: 842 loss: 1.61984826e-05
Iter: 843 loss: 1.61988e-05
Iter: 844 loss: 1.61975222e-05
Iter: 845 loss: 1.61948228e-05
Iter: 846 loss: 1.61924181e-05
Iter: 847 loss: 1.61864082e-05
Iter: 848 loss: 1.61753014e-05
Iter: 849 loss: 1.61745702e-05
Iter: 850 loss: 1.61576318e-05
Iter: 851 loss: 1.6308104e-05
Iter: 852 loss: 1.6156886e-05
Iter: 853 loss: 1.61457465e-05
Iter: 854 loss: 1.61503249e-05
Iter: 855 loss: 1.61383832e-05
Iter: 856 loss: 1.61227072e-05
Iter: 857 loss: 1.62055585e-05
Iter: 858 loss: 1.61199805e-05
Iter: 859 loss: 1.61083972e-05
Iter: 860 loss: 1.61239677e-05
Iter: 861 loss: 1.61034641e-05
Iter: 862 loss: 1.60864874e-05
Iter: 863 loss: 1.61358494e-05
Iter: 864 loss: 1.60814197e-05
Iter: 865 loss: 1.60713225e-05
Iter: 866 loss: 1.61259832e-05
Iter: 867 loss: 1.60694835e-05
Iter: 868 loss: 1.60566033e-05
Iter: 869 loss: 1.60711552e-05
Iter: 870 loss: 1.60497457e-05
Iter: 871 loss: 1.60394156e-05
Iter: 872 loss: 1.60888812e-05
Iter: 873 loss: 1.60375184e-05
Iter: 874 loss: 1.6026348e-05
Iter: 875 loss: 1.60169329e-05
Iter: 876 loss: 1.60133932e-05
Iter: 877 loss: 1.60060736e-05
Iter: 878 loss: 1.60044e-05
Iter: 879 loss: 1.5994734e-05
Iter: 880 loss: 1.60374912e-05
Iter: 881 loss: 1.59932752e-05
Iter: 882 loss: 1.59871033e-05
Iter: 883 loss: 1.59722804e-05
Iter: 884 loss: 1.60755189e-05
Iter: 885 loss: 1.59685987e-05
Iter: 886 loss: 1.59549436e-05
Iter: 887 loss: 1.60074651e-05
Iter: 888 loss: 1.59523734e-05
Iter: 889 loss: 1.59362935e-05
Iter: 890 loss: 1.59733863e-05
Iter: 891 loss: 1.59306219e-05
Iter: 892 loss: 1.59166848e-05
Iter: 893 loss: 1.59085757e-05
Iter: 894 loss: 1.59025712e-05
Iter: 895 loss: 1.58866369e-05
Iter: 896 loss: 1.58864113e-05
Iter: 897 loss: 1.58757557e-05
Iter: 898 loss: 1.58694675e-05
Iter: 899 loss: 1.58642742e-05
Iter: 900 loss: 1.58508228e-05
Iter: 901 loss: 1.59760439e-05
Iter: 902 loss: 1.58500479e-05
Iter: 903 loss: 1.58391085e-05
Iter: 904 loss: 1.58292714e-05
Iter: 905 loss: 1.5826512e-05
Iter: 906 loss: 1.58132025e-05
Iter: 907 loss: 1.58133662e-05
Iter: 908 loss: 1.58069779e-05
Iter: 909 loss: 1.59007213e-05
Iter: 910 loss: 1.58074345e-05
Iter: 911 loss: 1.58016355e-05
Iter: 912 loss: 1.58155744e-05
Iter: 913 loss: 1.58003859e-05
Iter: 914 loss: 1.5793521e-05
Iter: 915 loss: 1.57833147e-05
Iter: 916 loss: 1.5783e-05
Iter: 917 loss: 1.57740942e-05
Iter: 918 loss: 1.57722388e-05
Iter: 919 loss: 1.57666109e-05
Iter: 920 loss: 1.57494087e-05
Iter: 921 loss: 1.58003586e-05
Iter: 922 loss: 1.57450268e-05
Iter: 923 loss: 1.57354607e-05
Iter: 924 loss: 1.57575214e-05
Iter: 925 loss: 1.57327304e-05
Iter: 926 loss: 1.57187205e-05
Iter: 927 loss: 1.57219802e-05
Iter: 928 loss: 1.57096511e-05
Iter: 929 loss: 1.56966271e-05
Iter: 930 loss: 1.57605846e-05
Iter: 931 loss: 1.56941078e-05
Iter: 932 loss: 1.56810929e-05
Iter: 933 loss: 1.57285904e-05
Iter: 934 loss: 1.5677444e-05
Iter: 935 loss: 1.56691858e-05
Iter: 936 loss: 1.5665184e-05
Iter: 937 loss: 1.56614533e-05
Iter: 938 loss: 1.56454698e-05
Iter: 939 loss: 1.57414506e-05
Iter: 940 loss: 1.56438673e-05
Iter: 941 loss: 1.56357892e-05
Iter: 942 loss: 1.57373725e-05
Iter: 943 loss: 1.56359565e-05
Iter: 944 loss: 1.56276292e-05
Iter: 945 loss: 1.56621645e-05
Iter: 946 loss: 1.5625963e-05
Iter: 947 loss: 1.56197275e-05
Iter: 948 loss: 1.56171682e-05
Iter: 949 loss: 1.56138904e-05
Iter: 950 loss: 1.56075203e-05
Iter: 951 loss: 1.5601152e-05
Iter: 952 loss: 1.55995804e-05
Iter: 953 loss: 1.5586782e-05
Iter: 954 loss: 1.56388196e-05
Iter: 955 loss: 1.55842426e-05
Iter: 956 loss: 1.55746256e-05
Iter: 957 loss: 1.55754969e-05
Iter: 958 loss: 1.55674279e-05
Iter: 959 loss: 1.55527341e-05
Iter: 960 loss: 1.56500864e-05
Iter: 961 loss: 1.55516427e-05
Iter: 962 loss: 1.55423058e-05
Iter: 963 loss: 1.55433536e-05
Iter: 964 loss: 1.55351736e-05
Iter: 965 loss: 1.55207035e-05
Iter: 966 loss: 1.55952421e-05
Iter: 967 loss: 1.55180969e-05
Iter: 968 loss: 1.55092257e-05
Iter: 969 loss: 1.5507143e-05
Iter: 970 loss: 1.5500711e-05
Iter: 971 loss: 1.54858099e-05
Iter: 972 loss: 1.56011556e-05
Iter: 973 loss: 1.54848931e-05
Iter: 974 loss: 1.54762638e-05
Iter: 975 loss: 1.55062771e-05
Iter: 976 loss: 1.54737063e-05
Iter: 977 loss: 1.54682475e-05
Iter: 978 loss: 1.54678783e-05
Iter: 979 loss: 1.5463771e-05
Iter: 980 loss: 1.54563368e-05
Iter: 981 loss: 1.56171718e-05
Iter: 982 loss: 1.54562495e-05
Iter: 983 loss: 1.54495465e-05
Iter: 984 loss: 1.54541922e-05
Iter: 985 loss: 1.54451172e-05
Iter: 986 loss: 1.54359932e-05
Iter: 987 loss: 1.54512691e-05
Iter: 988 loss: 1.54313038e-05
Iter: 989 loss: 1.54223635e-05
Iter: 990 loss: 1.54328227e-05
Iter: 991 loss: 1.54171212e-05
Iter: 992 loss: 1.54037152e-05
Iter: 993 loss: 1.54391237e-05
Iter: 994 loss: 1.54001373e-05
Iter: 995 loss: 1.53897745e-05
Iter: 996 loss: 1.54129266e-05
Iter: 997 loss: 1.53855908e-05
Iter: 998 loss: 1.53723104e-05
Iter: 999 loss: 1.54143163e-05
Iter: 1000 loss: 1.53687288e-05
Iter: 1001 loss: 1.53573947e-05
Iter: 1002 loss: 1.53562123e-05
Iter: 1003 loss: 1.5349051e-05
Iter: 1004 loss: 1.53335368e-05
Iter: 1005 loss: 1.54686659e-05
Iter: 1006 loss: 1.53323381e-05
Iter: 1007 loss: 1.5322712e-05
Iter: 1008 loss: 1.53351793e-05
Iter: 1009 loss: 1.53184737e-05
Iter: 1010 loss: 1.53154724e-05
Iter: 1011 loss: 1.53117908e-05
Iter: 1012 loss: 1.53076253e-05
Iter: 1013 loss: 1.5300513e-05
Iter: 1014 loss: 1.53001201e-05
Iter: 1015 loss: 1.52932807e-05
Iter: 1016 loss: 1.52994744e-05
Iter: 1017 loss: 1.52899884e-05
Iter: 1018 loss: 1.52814773e-05
Iter: 1019 loss: 1.5289017e-05
Iter: 1020 loss: 1.52762295e-05
Iter: 1021 loss: 1.5265603e-05
Iter: 1022 loss: 1.52834073e-05
Iter: 1023 loss: 1.52611501e-05
Iter: 1024 loss: 1.52484718e-05
Iter: 1025 loss: 1.52866614e-05
Iter: 1026 loss: 1.524457e-05
Iter: 1027 loss: 1.52351167e-05
Iter: 1028 loss: 1.52414723e-05
Iter: 1029 loss: 1.52299563e-05
Iter: 1030 loss: 1.52165403e-05
Iter: 1031 loss: 1.52955708e-05
Iter: 1032 loss: 1.52147531e-05
Iter: 1033 loss: 1.5206384e-05
Iter: 1034 loss: 1.52123976e-05
Iter: 1035 loss: 1.52012099e-05
Iter: 1036 loss: 1.51882177e-05
Iter: 1037 loss: 1.52220637e-05
Iter: 1038 loss: 1.51840832e-05
Iter: 1039 loss: 1.51753466e-05
Iter: 1040 loss: 1.52176053e-05
Iter: 1041 loss: 1.51742315e-05
Iter: 1042 loss: 1.5168619e-05
Iter: 1043 loss: 1.51686481e-05
Iter: 1044 loss: 1.51612885e-05
Iter: 1045 loss: 1.51561308e-05
Iter: 1046 loss: 1.5153576e-05
Iter: 1047 loss: 1.51480399e-05
Iter: 1048 loss: 1.51448457e-05
Iter: 1049 loss: 1.51433942e-05
Iter: 1050 loss: 1.51338736e-05
Iter: 1051 loss: 1.51329123e-05
Iter: 1052 loss: 1.51252607e-05
Iter: 1053 loss: 1.51144386e-05
Iter: 1054 loss: 1.51939803e-05
Iter: 1055 loss: 1.51130425e-05
Iter: 1056 loss: 1.5103612e-05
Iter: 1057 loss: 1.51119311e-05
Iter: 1058 loss: 1.50982623e-05
Iter: 1059 loss: 1.50889937e-05
Iter: 1060 loss: 1.51295762e-05
Iter: 1061 loss: 1.50871983e-05
Iter: 1062 loss: 1.50769392e-05
Iter: 1063 loss: 1.5076118e-05
Iter: 1064 loss: 1.50675969e-05
Iter: 1065 loss: 1.50562109e-05
Iter: 1066 loss: 1.51291042e-05
Iter: 1067 loss: 1.50546412e-05
Iter: 1068 loss: 1.50426531e-05
Iter: 1069 loss: 1.504998e-05
Iter: 1070 loss: 1.50345149e-05
Iter: 1071 loss: 1.50252763e-05
Iter: 1072 loss: 1.51003896e-05
Iter: 1073 loss: 1.50240849e-05
Iter: 1074 loss: 1.50150154e-05
Iter: 1075 loss: 1.50519245e-05
Iter: 1076 loss: 1.50133601e-05
Iter: 1077 loss: 1.50014439e-05
Iter: 1078 loss: 1.50782671e-05
Iter: 1079 loss: 1.50003243e-05
Iter: 1080 loss: 1.49980333e-05
Iter: 1081 loss: 1.49890111e-05
Iter: 1082 loss: 1.50484684e-05
Iter: 1083 loss: 1.49870575e-05
Iter: 1084 loss: 1.4974672e-05
Iter: 1085 loss: 1.50096903e-05
Iter: 1086 loss: 1.49706775e-05
Iter: 1087 loss: 1.49623329e-05
Iter: 1088 loss: 1.50165824e-05
Iter: 1089 loss: 1.49616299e-05
Iter: 1090 loss: 1.49540992e-05
Iter: 1091 loss: 1.4953157e-05
Iter: 1092 loss: 1.49478319e-05
Iter: 1093 loss: 1.49391417e-05
Iter: 1094 loss: 1.50057513e-05
Iter: 1095 loss: 1.49385633e-05
Iter: 1096 loss: 1.49310799e-05
Iter: 1097 loss: 1.49282932e-05
Iter: 1098 loss: 1.49244343e-05
Iter: 1099 loss: 1.49150301e-05
Iter: 1100 loss: 1.49843554e-05
Iter: 1101 loss: 1.49141379e-05
Iter: 1102 loss: 1.49048174e-05
Iter: 1103 loss: 1.49020552e-05
Iter: 1104 loss: 1.48953386e-05
Iter: 1105 loss: 1.48868303e-05
Iter: 1106 loss: 1.49867137e-05
Iter: 1107 loss: 1.48864747e-05
Iter: 1108 loss: 1.48781728e-05
Iter: 1109 loss: 1.48925228e-05
Iter: 1110 loss: 1.48742301e-05
Iter: 1111 loss: 1.4866695e-05
Iter: 1112 loss: 1.48660474e-05
Iter: 1113 loss: 1.48639501e-05
Iter: 1114 loss: 1.48573299e-05
Iter: 1115 loss: 1.49095449e-05
Iter: 1116 loss: 1.48561539e-05
Iter: 1117 loss: 1.48475419e-05
Iter: 1118 loss: 1.48525805e-05
Iter: 1119 loss: 1.48423087e-05
Iter: 1120 loss: 1.48344861e-05
Iter: 1121 loss: 1.49113675e-05
Iter: 1122 loss: 1.48342042e-05
Iter: 1123 loss: 1.48254621e-05
Iter: 1124 loss: 1.4813364e-05
Iter: 1125 loss: 1.48132576e-05
Iter: 1126 loss: 1.48026375e-05
Iter: 1127 loss: 1.4950464e-05
Iter: 1128 loss: 1.48024556e-05
Iter: 1129 loss: 1.47945384e-05
Iter: 1130 loss: 1.47861483e-05
Iter: 1131 loss: 1.47841656e-05
Iter: 1132 loss: 1.47726605e-05
Iter: 1133 loss: 1.48677391e-05
Iter: 1134 loss: 1.47718529e-05
Iter: 1135 loss: 1.47615083e-05
Iter: 1136 loss: 1.47765459e-05
Iter: 1137 loss: 1.4756728e-05
Iter: 1138 loss: 1.47480641e-05
Iter: 1139 loss: 1.48072168e-05
Iter: 1140 loss: 1.47471528e-05
Iter: 1141 loss: 1.47390037e-05
Iter: 1142 loss: 1.47398951e-05
Iter: 1143 loss: 1.47324736e-05
Iter: 1144 loss: 1.47356386e-05
Iter: 1145 loss: 1.47274222e-05
Iter: 1146 loss: 1.47250921e-05
Iter: 1147 loss: 1.47192841e-05
Iter: 1148 loss: 1.4777258e-05
Iter: 1149 loss: 1.47187739e-05
Iter: 1150 loss: 1.47135033e-05
Iter: 1151 loss: 1.47036199e-05
Iter: 1152 loss: 1.47032551e-05
Iter: 1153 loss: 1.46937837e-05
Iter: 1154 loss: 1.47749797e-05
Iter: 1155 loss: 1.46933271e-05
Iter: 1156 loss: 1.46830234e-05
Iter: 1157 loss: 1.46855209e-05
Iter: 1158 loss: 1.46758339e-05
Iter: 1159 loss: 1.46653074e-05
Iter: 1160 loss: 1.47809014e-05
Iter: 1161 loss: 1.46649945e-05
Iter: 1162 loss: 1.46590155e-05
Iter: 1163 loss: 1.46498933e-05
Iter: 1164 loss: 1.46486655e-05
Iter: 1165 loss: 1.46397215e-05
Iter: 1166 loss: 1.47812189e-05
Iter: 1167 loss: 1.46399434e-05
Iter: 1168 loss: 1.46313614e-05
Iter: 1169 loss: 1.46222037e-05
Iter: 1170 loss: 1.46206758e-05
Iter: 1171 loss: 1.46076427e-05
Iter: 1172 loss: 1.47144565e-05
Iter: 1173 loss: 1.46070233e-05
Iter: 1174 loss: 1.45982021e-05
Iter: 1175 loss: 1.46528973e-05
Iter: 1176 loss: 1.45973008e-05
Iter: 1177 loss: 1.45972926e-05
Iter: 1178 loss: 1.45933072e-05
Iter: 1179 loss: 1.45898166e-05
Iter: 1180 loss: 1.45836184e-05
Iter: 1181 loss: 1.47274832e-05
Iter: 1182 loss: 1.45833064e-05
Iter: 1183 loss: 1.45778195e-05
Iter: 1184 loss: 1.45682789e-05
Iter: 1185 loss: 1.45684189e-05
Iter: 1186 loss: 1.45597587e-05
Iter: 1187 loss: 1.46724651e-05
Iter: 1188 loss: 1.45589702e-05
Iter: 1189 loss: 1.45535942e-05
Iter: 1190 loss: 1.45498398e-05
Iter: 1191 loss: 1.45475196e-05
Iter: 1192 loss: 1.45387367e-05
Iter: 1193 loss: 1.46341554e-05
Iter: 1194 loss: 1.45385566e-05
Iter: 1195 loss: 1.4533076e-05
Iter: 1196 loss: 1.45257745e-05
Iter: 1197 loss: 1.45254071e-05
Iter: 1198 loss: 1.45161785e-05
Iter: 1199 loss: 1.46261882e-05
Iter: 1200 loss: 1.45166196e-05
Iter: 1201 loss: 1.4510546e-05
Iter: 1202 loss: 1.45022377e-05
Iter: 1203 loss: 1.45021331e-05
Iter: 1204 loss: 1.44916494e-05
Iter: 1205 loss: 1.45320373e-05
Iter: 1206 loss: 1.4489312e-05
Iter: 1207 loss: 1.44826354e-05
Iter: 1208 loss: 1.44872465e-05
Iter: 1209 loss: 1.44780279e-05
Iter: 1210 loss: 1.44683681e-05
Iter: 1211 loss: 1.44612732e-05
Iter: 1212 loss: 1.44576479e-05
Iter: 1213 loss: 1.44580663e-05
Iter: 1214 loss: 1.44539608e-05
Iter: 1215 loss: 1.44505048e-05
Iter: 1216 loss: 1.448681e-05
Iter: 1217 loss: 1.44498408e-05
Iter: 1218 loss: 1.44480691e-05
Iter: 1219 loss: 1.44413625e-05
Iter: 1220 loss: 1.44535388e-05
Iter: 1221 loss: 1.44368551e-05
Iter: 1222 loss: 1.44303485e-05
Iter: 1223 loss: 1.45252125e-05
Iter: 1224 loss: 1.44301803e-05
Iter: 1225 loss: 1.44263176e-05
Iter: 1226 loss: 1.4421259e-05
Iter: 1227 loss: 1.44205178e-05
Iter: 1228 loss: 1.44120377e-05
Iter: 1229 loss: 1.44701216e-05
Iter: 1230 loss: 1.44105716e-05
Iter: 1231 loss: 1.44045243e-05
Iter: 1232 loss: 1.44298992e-05
Iter: 1233 loss: 1.44029545e-05
Iter: 1234 loss: 1.43969419e-05
Iter: 1235 loss: 1.43990646e-05
Iter: 1236 loss: 1.43927136e-05
Iter: 1237 loss: 1.43852121e-05
Iter: 1238 loss: 1.44170035e-05
Iter: 1239 loss: 1.4383495e-05
Iter: 1240 loss: 1.43775442e-05
Iter: 1241 loss: 1.43780726e-05
Iter: 1242 loss: 1.43724437e-05
Iter: 1243 loss: 1.4365125e-05
Iter: 1244 loss: 1.44258538e-05
Iter: 1245 loss: 1.43638699e-05
Iter: 1246 loss: 1.43585e-05
Iter: 1247 loss: 1.43673442e-05
Iter: 1248 loss: 1.43561338e-05
Iter: 1249 loss: 1.43535763e-05
Iter: 1250 loss: 1.43525222e-05
Iter: 1251 loss: 1.43488223e-05
Iter: 1252 loss: 1.43426105e-05
Iter: 1253 loss: 1.43425495e-05
Iter: 1254 loss: 1.43390198e-05
Iter: 1255 loss: 1.43367415e-05
Iter: 1256 loss: 1.43352281e-05
Iter: 1257 loss: 1.43265152e-05
Iter: 1258 loss: 1.43244215e-05
Iter: 1259 loss: 1.43197576e-05
Iter: 1260 loss: 1.43095276e-05
Iter: 1261 loss: 1.43674197e-05
Iter: 1262 loss: 1.43081061e-05
Iter: 1263 loss: 1.42996569e-05
Iter: 1264 loss: 1.43259931e-05
Iter: 1265 loss: 1.42975687e-05
Iter: 1266 loss: 1.42905474e-05
Iter: 1267 loss: 1.43235411e-05
Iter: 1268 loss: 1.4289154e-05
Iter: 1269 loss: 1.42826811e-05
Iter: 1270 loss: 1.42949784e-05
Iter: 1271 loss: 1.42798954e-05
Iter: 1272 loss: 1.42720692e-05
Iter: 1273 loss: 1.42764884e-05
Iter: 1274 loss: 1.42669041e-05
Iter: 1275 loss: 1.42590588e-05
Iter: 1276 loss: 1.43001716e-05
Iter: 1277 loss: 1.42573499e-05
Iter: 1278 loss: 1.42496128e-05
Iter: 1279 loss: 1.42479512e-05
Iter: 1280 loss: 1.42434565e-05
Iter: 1281 loss: 1.42366507e-05
Iter: 1282 loss: 1.42365716e-05
Iter: 1283 loss: 1.42302233e-05
Iter: 1284 loss: 1.4293707e-05
Iter: 1285 loss: 1.42301524e-05
Iter: 1286 loss: 1.42285116e-05
Iter: 1287 loss: 1.42233957e-05
Iter: 1288 loss: 1.42477156e-05
Iter: 1289 loss: 1.42215358e-05
Iter: 1290 loss: 1.42138624e-05
Iter: 1291 loss: 1.42399922e-05
Iter: 1292 loss: 1.42119497e-05
Iter: 1293 loss: 1.42057979e-05
Iter: 1294 loss: 1.42057534e-05
Iter: 1295 loss: 1.42017561e-05
Iter: 1296 loss: 1.4191658e-05
Iter: 1297 loss: 1.42434365e-05
Iter: 1298 loss: 1.41892133e-05
Iter: 1299 loss: 1.41819901e-05
Iter: 1300 loss: 1.42057079e-05
Iter: 1301 loss: 1.41799028e-05
Iter: 1302 loss: 1.41729442e-05
Iter: 1303 loss: 1.41927212e-05
Iter: 1304 loss: 1.41699338e-05
Iter: 1305 loss: 1.41634646e-05
Iter: 1306 loss: 1.41932105e-05
Iter: 1307 loss: 1.41618466e-05
Iter: 1308 loss: 1.41576256e-05
Iter: 1309 loss: 1.41651817e-05
Iter: 1310 loss: 1.41554401e-05
Iter: 1311 loss: 1.41494238e-05
Iter: 1312 loss: 1.41531418e-05
Iter: 1313 loss: 1.41449837e-05
Iter: 1314 loss: 1.41385872e-05
Iter: 1315 loss: 1.41773189e-05
Iter: 1316 loss: 1.41378005e-05
Iter: 1317 loss: 1.41387072e-05
Iter: 1318 loss: 1.41349628e-05
Iter: 1319 loss: 1.41336268e-05
Iter: 1320 loss: 1.41282744e-05
Iter: 1321 loss: 1.41350329e-05
Iter: 1322 loss: 1.41242635e-05
Iter: 1323 loss: 1.41153087e-05
Iter: 1324 loss: 1.41305836e-05
Iter: 1325 loss: 1.41114006e-05
Iter: 1326 loss: 1.41033597e-05
Iter: 1327 loss: 1.41306764e-05
Iter: 1328 loss: 1.4100744e-05
Iter: 1329 loss: 1.40933571e-05
Iter: 1330 loss: 1.4133504e-05
Iter: 1331 loss: 1.40916327e-05
Iter: 1332 loss: 1.40870688e-05
Iter: 1333 loss: 1.40866805e-05
Iter: 1334 loss: 1.40821458e-05
Iter: 1335 loss: 1.40736411e-05
Iter: 1336 loss: 1.41112369e-05
Iter: 1337 loss: 1.40713046e-05
Iter: 1338 loss: 1.40640386e-05
Iter: 1339 loss: 1.40986431e-05
Iter: 1340 loss: 1.40619868e-05
Iter: 1341 loss: 1.40556431e-05
Iter: 1342 loss: 1.40613856e-05
Iter: 1343 loss: 1.40520451e-05
Iter: 1344 loss: 1.40434222e-05
Iter: 1345 loss: 1.40645e-05
Iter: 1346 loss: 1.404047e-05
Iter: 1347 loss: 1.40336033e-05
Iter: 1348 loss: 1.40406491e-05
Iter: 1349 loss: 1.40295433e-05
Iter: 1350 loss: 1.40294487e-05
Iter: 1351 loss: 1.40257926e-05
Iter: 1352 loss: 1.40220509e-05
Iter: 1353 loss: 1.40151496e-05
Iter: 1354 loss: 1.40149523e-05
Iter: 1355 loss: 1.40096909e-05
Iter: 1356 loss: 1.40094626e-05
Iter: 1357 loss: 1.40060856e-05
Iter: 1358 loss: 1.40001339e-05
Iter: 1359 loss: 1.4016844e-05
Iter: 1360 loss: 1.39982585e-05
Iter: 1361 loss: 1.39913227e-05
Iter: 1362 loss: 1.39979393e-05
Iter: 1363 loss: 1.3988054e-05
Iter: 1364 loss: 1.39806671e-05
Iter: 1365 loss: 1.39920603e-05
Iter: 1366 loss: 1.39775366e-05
Iter: 1367 loss: 1.3969081e-05
Iter: 1368 loss: 1.40096436e-05
Iter: 1369 loss: 1.39670747e-05
Iter: 1370 loss: 1.39610938e-05
Iter: 1371 loss: 1.3970146e-05
Iter: 1372 loss: 1.39584618e-05
Iter: 1373 loss: 1.3951073e-05
Iter: 1374 loss: 1.39736085e-05
Iter: 1375 loss: 1.39485028e-05
Iter: 1376 loss: 1.39407493e-05
Iter: 1377 loss: 1.39861968e-05
Iter: 1378 loss: 1.39400845e-05
Iter: 1379 loss: 1.39359508e-05
Iter: 1380 loss: 1.39362473e-05
Iter: 1381 loss: 1.39320309e-05
Iter: 1382 loss: 1.39246449e-05
Iter: 1383 loss: 1.39429849e-05
Iter: 1384 loss: 1.3922202e-05
Iter: 1385 loss: 1.39170106e-05
Iter: 1386 loss: 1.39169551e-05
Iter: 1387 loss: 1.3912223e-05
Iter: 1388 loss: 1.39414042e-05
Iter: 1389 loss: 1.39111489e-05
Iter: 1390 loss: 1.39091226e-05
Iter: 1391 loss: 1.39036156e-05
Iter: 1392 loss: 1.39341901e-05
Iter: 1393 loss: 1.39018584e-05
Iter: 1394 loss: 1.38930154e-05
Iter: 1395 loss: 1.39075892e-05
Iter: 1396 loss: 1.38890173e-05
Iter: 1397 loss: 1.38808737e-05
Iter: 1398 loss: 1.39352542e-05
Iter: 1399 loss: 1.38797232e-05
Iter: 1400 loss: 1.38717169e-05
Iter: 1401 loss: 1.38707819e-05
Iter: 1402 loss: 1.38658124e-05
Iter: 1403 loss: 1.38577798e-05
Iter: 1404 loss: 1.39231852e-05
Iter: 1405 loss: 1.38569994e-05
Iter: 1406 loss: 1.38493906e-05
Iter: 1407 loss: 1.38520154e-05
Iter: 1408 loss: 1.38442647e-05
Iter: 1409 loss: 1.38357045e-05
Iter: 1410 loss: 1.38921314e-05
Iter: 1411 loss: 1.38345722e-05
Iter: 1412 loss: 1.38269879e-05
Iter: 1413 loss: 1.38494815e-05
Iter: 1414 loss: 1.38248852e-05
Iter: 1415 loss: 1.38165351e-05
Iter: 1416 loss: 1.38279174e-05
Iter: 1417 loss: 1.38122177e-05
Iter: 1418 loss: 1.38061396e-05
Iter: 1419 loss: 1.38596824e-05
Iter: 1420 loss: 1.38058094e-05
Iter: 1421 loss: 1.38058922e-05
Iter: 1422 loss: 1.38037012e-05
Iter: 1423 loss: 1.38022624e-05
Iter: 1424 loss: 1.37966945e-05
Iter: 1425 loss: 1.38181513e-05
Iter: 1426 loss: 1.37948446e-05
Iter: 1427 loss: 1.37887491e-05
Iter: 1428 loss: 1.3796629e-05
Iter: 1429 loss: 1.37860243e-05
Iter: 1430 loss: 1.37802508e-05
Iter: 1431 loss: 1.37937968e-05
Iter: 1432 loss: 1.37779389e-05
Iter: 1433 loss: 1.37713077e-05
Iter: 1434 loss: 1.37791203e-05
Iter: 1435 loss: 1.3767718e-05
Iter: 1436 loss: 1.3760864e-05
Iter: 1437 loss: 1.37874631e-05
Iter: 1438 loss: 1.37592e-05
Iter: 1439 loss: 1.3752121e-05
Iter: 1440 loss: 1.37739416e-05
Iter: 1441 loss: 1.37493043e-05
Iter: 1442 loss: 1.37432271e-05
Iter: 1443 loss: 1.37652678e-05
Iter: 1444 loss: 1.37421994e-05
Iter: 1445 loss: 1.37362113e-05
Iter: 1446 loss: 1.37472307e-05
Iter: 1447 loss: 1.37331299e-05
Iter: 1448 loss: 1.37265215e-05
Iter: 1449 loss: 1.37524239e-05
Iter: 1450 loss: 1.3724436e-05
Iter: 1451 loss: 1.37193183e-05
Iter: 1452 loss: 1.37330408e-05
Iter: 1453 loss: 1.37174811e-05
Iter: 1454 loss: 1.37155384e-05
Iter: 1455 loss: 1.37142561e-05
Iter: 1456 loss: 1.37113693e-05
Iter: 1457 loss: 1.37054367e-05
Iter: 1458 loss: 1.37056968e-05
Iter: 1459 loss: 1.37019533e-05
Iter: 1460 loss: 1.36999552e-05
Iter: 1461 loss: 1.36979088e-05
Iter: 1462 loss: 1.36916433e-05
Iter: 1463 loss: 1.36989875e-05
Iter: 1464 loss: 1.36883355e-05
Iter: 1465 loss: 1.36805602e-05
Iter: 1466 loss: 1.36987646e-05
Iter: 1467 loss: 1.36776034e-05
Iter: 1468 loss: 1.36702538e-05
Iter: 1469 loss: 1.37045154e-05
Iter: 1470 loss: 1.3669599e-05
Iter: 1471 loss: 1.36624203e-05
Iter: 1472 loss: 1.36723047e-05
Iter: 1473 loss: 1.36589852e-05
Iter: 1474 loss: 1.36534436e-05
Iter: 1475 loss: 1.36765357e-05
Iter: 1476 loss: 1.36526169e-05
Iter: 1477 loss: 1.3646526e-05
Iter: 1478 loss: 1.36507306e-05
Iter: 1479 loss: 1.36431427e-05
Iter: 1480 loss: 1.36359558e-05
Iter: 1481 loss: 1.36810713e-05
Iter: 1482 loss: 1.3634899e-05
Iter: 1483 loss: 1.36294557e-05
Iter: 1484 loss: 1.36395865e-05
Iter: 1485 loss: 1.36268227e-05
Iter: 1486 loss: 1.36245271e-05
Iter: 1487 loss: 1.36236686e-05
Iter: 1488 loss: 1.36197159e-05
Iter: 1489 loss: 1.36226863e-05
Iter: 1490 loss: 1.36173803e-05
Iter: 1491 loss: 1.36150848e-05
Iter: 1492 loss: 1.36101553e-05
Iter: 1493 loss: 1.37088637e-05
Iter: 1494 loss: 1.36103272e-05
Iter: 1495 loss: 1.36042954e-05
Iter: 1496 loss: 1.36132203e-05
Iter: 1497 loss: 1.36010449e-05
Iter: 1498 loss: 1.35945284e-05
Iter: 1499 loss: 1.36131875e-05
Iter: 1500 loss: 1.35927739e-05
Iter: 1501 loss: 1.35867358e-05
Iter: 1502 loss: 1.36079125e-05
Iter: 1503 loss: 1.35855198e-05
Iter: 1504 loss: 1.35793189e-05
Iter: 1505 loss: 1.35849523e-05
Iter: 1506 loss: 1.35749115e-05
Iter: 1507 loss: 1.35691789e-05
Iter: 1508 loss: 1.35935552e-05
Iter: 1509 loss: 1.35674754e-05
Iter: 1510 loss: 1.35599548e-05
Iter: 1511 loss: 1.35665432e-05
Iter: 1512 loss: 1.35556838e-05
Iter: 1513 loss: 1.35493983e-05
Iter: 1514 loss: 1.36003882e-05
Iter: 1515 loss: 1.3548367e-05
Iter: 1516 loss: 1.35426199e-05
Iter: 1517 loss: 1.35422615e-05
Iter: 1518 loss: 1.35380278e-05
Iter: 1519 loss: 1.3533172e-05
Iter: 1520 loss: 1.3533001e-05
Iter: 1521 loss: 1.35284845e-05
Iter: 1522 loss: 1.35562295e-05
Iter: 1523 loss: 1.35281207e-05
Iter: 1524 loss: 1.35262117e-05
Iter: 1525 loss: 1.35205428e-05
Iter: 1526 loss: 1.35471346e-05
Iter: 1527 loss: 1.35185965e-05
Iter: 1528 loss: 1.35111441e-05
Iter: 1529 loss: 1.35591163e-05
Iter: 1530 loss: 1.35107075e-05
Iter: 1531 loss: 1.35042364e-05
Iter: 1532 loss: 1.35073869e-05
Iter: 1533 loss: 1.34995489e-05
Iter: 1534 loss: 1.34919601e-05
Iter: 1535 loss: 1.35141599e-05
Iter: 1536 loss: 1.34902566e-05
Iter: 1537 loss: 1.3481309e-05
Iter: 1538 loss: 1.34909842e-05
Iter: 1539 loss: 1.34775282e-05
Iter: 1540 loss: 1.34700904e-05
Iter: 1541 loss: 1.3509889e-05
Iter: 1542 loss: 1.34685051e-05
Iter: 1543 loss: 1.346113e-05
Iter: 1544 loss: 1.3474566e-05
Iter: 1545 loss: 1.34584416e-05
Iter: 1546 loss: 1.34516131e-05
Iter: 1547 loss: 1.34802121e-05
Iter: 1548 loss: 1.34508136e-05
Iter: 1549 loss: 1.3444429e-05
Iter: 1550 loss: 1.34459642e-05
Iter: 1551 loss: 1.34397906e-05
Iter: 1552 loss: 1.34368302e-05
Iter: 1553 loss: 1.34360744e-05
Iter: 1554 loss: 1.34335896e-05
Iter: 1555 loss: 1.34646443e-05
Iter: 1556 loss: 1.34330439e-05
Iter: 1557 loss: 1.34321072e-05
Iter: 1558 loss: 1.34271613e-05
Iter: 1559 loss: 1.34314614e-05
Iter: 1560 loss: 1.34235734e-05
Iter: 1561 loss: 1.34167922e-05
Iter: 1562 loss: 1.34787833e-05
Iter: 1563 loss: 1.34164493e-05
Iter: 1564 loss: 1.34110178e-05
Iter: 1565 loss: 1.34123757e-05
Iter: 1566 loss: 1.34067e-05
Iter: 1567 loss: 1.33996218e-05
Iter: 1568 loss: 1.34292295e-05
Iter: 1569 loss: 1.33975682e-05
Iter: 1570 loss: 1.33910326e-05
Iter: 1571 loss: 1.34020038e-05
Iter: 1572 loss: 1.3387973e-05
Iter: 1573 loss: 1.33811973e-05
Iter: 1574 loss: 1.33965805e-05
Iter: 1575 loss: 1.33791782e-05
Iter: 1576 loss: 1.33708472e-05
Iter: 1577 loss: 1.3389018e-05
Iter: 1578 loss: 1.33673757e-05
Iter: 1579 loss: 1.33608182e-05
Iter: 1580 loss: 1.33862668e-05
Iter: 1581 loss: 1.33590056e-05
Iter: 1582 loss: 1.33512895e-05
Iter: 1583 loss: 1.33553713e-05
Iter: 1584 loss: 1.33460017e-05
Iter: 1585 loss: 1.33406211e-05
Iter: 1586 loss: 1.33401436e-05
Iter: 1587 loss: 1.33371723e-05
Iter: 1588 loss: 1.33370359e-05
Iter: 1589 loss: 1.33345402e-05
Iter: 1590 loss: 1.33272606e-05
Iter: 1591 loss: 1.33234762e-05
Iter: 1592 loss: 1.33186022e-05
Iter: 1593 loss: 1.33101385e-05
Iter: 1594 loss: 1.33096837e-05
Iter: 1595 loss: 1.33038129e-05
Iter: 1596 loss: 1.33007479e-05
Iter: 1597 loss: 1.32976966e-05
Iter: 1598 loss: 1.32879477e-05
Iter: 1599 loss: 1.33251033e-05
Iter: 1600 loss: 1.32855184e-05
Iter: 1601 loss: 1.32762098e-05
Iter: 1602 loss: 1.32999176e-05
Iter: 1603 loss: 1.32724908e-05
Iter: 1604 loss: 1.32637888e-05
Iter: 1605 loss: 1.32796604e-05
Iter: 1606 loss: 1.32605264e-05
Iter: 1607 loss: 1.32512305e-05
Iter: 1608 loss: 1.32759833e-05
Iter: 1609 loss: 1.32483956e-05
Iter: 1610 loss: 1.32404584e-05
Iter: 1611 loss: 1.32571922e-05
Iter: 1612 loss: 1.32376363e-05
Iter: 1613 loss: 1.32277937e-05
Iter: 1614 loss: 1.32496971e-05
Iter: 1615 loss: 1.32237328e-05
Iter: 1616 loss: 1.3216586e-05
Iter: 1617 loss: 1.32877449e-05
Iter: 1618 loss: 1.3216516e-05
Iter: 1619 loss: 1.32133482e-05
Iter: 1620 loss: 1.32131627e-05
Iter: 1621 loss: 1.32103687e-05
Iter: 1622 loss: 1.32032346e-05
Iter: 1623 loss: 1.32133691e-05
Iter: 1624 loss: 1.3198056e-05
Iter: 1625 loss: 1.31908628e-05
Iter: 1626 loss: 1.32669111e-05
Iter: 1627 loss: 1.31901234e-05
Iter: 1628 loss: 1.3184309e-05
Iter: 1629 loss: 1.31752095e-05
Iter: 1630 loss: 1.31753e-05
Iter: 1631 loss: 1.31646693e-05
Iter: 1632 loss: 1.32988825e-05
Iter: 1633 loss: 1.31646211e-05
Iter: 1634 loss: 1.31573261e-05
Iter: 1635 loss: 1.3183505e-05
Iter: 1636 loss: 1.31547413e-05
Iter: 1637 loss: 1.31493152e-05
Iter: 1638 loss: 1.31460956e-05
Iter: 1639 loss: 1.31434617e-05
Iter: 1640 loss: 1.31336774e-05
Iter: 1641 loss: 1.31750676e-05
Iter: 1642 loss: 1.31317775e-05
Iter: 1643 loss: 1.31242323e-05
Iter: 1644 loss: 1.31436109e-05
Iter: 1645 loss: 1.31211718e-05
Iter: 1646 loss: 1.31128281e-05
Iter: 1647 loss: 1.3156874e-05
Iter: 1648 loss: 1.31110965e-05
Iter: 1649 loss: 1.31051511e-05
Iter: 1650 loss: 1.31465222e-05
Iter: 1651 loss: 1.31050183e-05
Iter: 1652 loss: 1.3103474e-05
Iter: 1653 loss: 1.3102057e-05
Iter: 1654 loss: 1.30994586e-05
Iter: 1655 loss: 1.30928565e-05
Iter: 1656 loss: 1.31128445e-05
Iter: 1657 loss: 1.3089264e-05
Iter: 1658 loss: 1.30831395e-05
Iter: 1659 loss: 1.31157913e-05
Iter: 1660 loss: 1.30815988e-05
Iter: 1661 loss: 1.30748431e-05
Iter: 1662 loss: 1.30664139e-05
Iter: 1663 loss: 1.30652807e-05
Iter: 1664 loss: 1.30560293e-05
Iter: 1665 loss: 1.31913675e-05
Iter: 1666 loss: 1.30557328e-05
Iter: 1667 loss: 1.30491826e-05
Iter: 1668 loss: 1.30669005e-05
Iter: 1669 loss: 1.30465405e-05
Iter: 1670 loss: 1.30399676e-05
Iter: 1671 loss: 1.30531198e-05
Iter: 1672 loss: 1.30370945e-05
Iter: 1673 loss: 1.30300941e-05
Iter: 1674 loss: 1.30584867e-05
Iter: 1675 loss: 1.30286844e-05
Iter: 1676 loss: 1.30235985e-05
Iter: 1677 loss: 1.30295366e-05
Iter: 1678 loss: 1.30204853e-05
Iter: 1679 loss: 1.30133021e-05
Iter: 1680 loss: 1.30366434e-05
Iter: 1681 loss: 1.30114804e-05
Iter: 1682 loss: 1.30057188e-05
Iter: 1683 loss: 1.30276312e-05
Iter: 1684 loss: 1.30042417e-05
Iter: 1685 loss: 1.30047229e-05
Iter: 1686 loss: 1.30017343e-05
Iter: 1687 loss: 1.29996561e-05
Iter: 1688 loss: 1.29944283e-05
Iter: 1689 loss: 1.30335911e-05
Iter: 1690 loss: 1.29930231e-05
Iter: 1691 loss: 1.29892578e-05
Iter: 1692 loss: 1.2996551e-05
Iter: 1693 loss: 1.29873406e-05
Iter: 1694 loss: 1.29813188e-05
Iter: 1695 loss: 1.2975047e-05
Iter: 1696 loss: 1.29744149e-05
Iter: 1697 loss: 1.29674117e-05
Iter: 1698 loss: 1.30712806e-05
Iter: 1699 loss: 1.29671407e-05
Iter: 1700 loss: 1.29626751e-05
Iter: 1701 loss: 1.29636101e-05
Iter: 1702 loss: 1.29585524e-05
Iter: 1703 loss: 1.29508371e-05
Iter: 1704 loss: 1.2972183e-05
Iter: 1705 loss: 1.29485034e-05
Iter: 1706 loss: 1.29408827e-05
Iter: 1707 loss: 1.29689943e-05
Iter: 1708 loss: 1.29394575e-05
Iter: 1709 loss: 1.29334094e-05
Iter: 1710 loss: 1.29410419e-05
Iter: 1711 loss: 1.29305918e-05
Iter: 1712 loss: 1.29225409e-05
Iter: 1713 loss: 1.29447872e-05
Iter: 1714 loss: 1.29196378e-05
Iter: 1715 loss: 1.29128439e-05
Iter: 1716 loss: 1.29333594e-05
Iter: 1717 loss: 1.29108812e-05
Iter: 1718 loss: 1.29092014e-05
Iter: 1719 loss: 1.29077798e-05
Iter: 1720 loss: 1.290412e-05
Iter: 1721 loss: 1.28977681e-05
Iter: 1722 loss: 1.28977599e-05
Iter: 1723 loss: 1.28932261e-05
Iter: 1724 loss: 1.28883048e-05
Iter: 1725 loss: 1.28876127e-05
Iter: 1726 loss: 1.28781967e-05
Iter: 1727 loss: 1.28925785e-05
Iter: 1728 loss: 1.28733673e-05
Iter: 1729 loss: 1.28667434e-05
Iter: 1730 loss: 1.29600594e-05
Iter: 1731 loss: 1.28672655e-05
Iter: 1732 loss: 1.28612719e-05
Iter: 1733 loss: 1.28580978e-05
Iter: 1734 loss: 1.28558195e-05
Iter: 1735 loss: 1.2846649e-05
Iter: 1736 loss: 1.288776e-05
Iter: 1737 loss: 1.28446836e-05
Iter: 1738 loss: 1.28380216e-05
Iter: 1739 loss: 1.28706479e-05
Iter: 1740 loss: 1.28366701e-05
Iter: 1741 loss: 1.28305492e-05
Iter: 1742 loss: 1.28348765e-05
Iter: 1743 loss: 1.28263764e-05
Iter: 1744 loss: 1.28178199e-05
Iter: 1745 loss: 1.28403153e-05
Iter: 1746 loss: 1.28144757e-05
Iter: 1747 loss: 1.28073425e-05
Iter: 1748 loss: 1.28327083e-05
Iter: 1749 loss: 1.28053562e-05
Iter: 1750 loss: 1.28026913e-05
Iter: 1751 loss: 1.28022239e-05
Iter: 1752 loss: 1.27981e-05
Iter: 1753 loss: 1.27954027e-05
Iter: 1754 loss: 1.27931462e-05
Iter: 1755 loss: 1.27896219e-05
Iter: 1756 loss: 1.2782155e-05
Iter: 1757 loss: 1.2891388e-05
Iter: 1758 loss: 1.27814992e-05
Iter: 1759 loss: 1.2770879e-05
Iter: 1760 loss: 1.28025622e-05
Iter: 1761 loss: 1.27675776e-05
Iter: 1762 loss: 1.27599042e-05
Iter: 1763 loss: 1.2840168e-05
Iter: 1764 loss: 1.27597577e-05
Iter: 1765 loss: 1.27540234e-05
Iter: 1766 loss: 1.2750982e-05
Iter: 1767 loss: 1.27480525e-05
Iter: 1768 loss: 1.27396524e-05
Iter: 1769 loss: 1.2801278e-05
Iter: 1770 loss: 1.27384692e-05
Iter: 1771 loss: 1.27326e-05
Iter: 1772 loss: 1.27483754e-05
Iter: 1773 loss: 1.27311832e-05
Iter: 1774 loss: 1.27242556e-05
Iter: 1775 loss: 1.27311596e-05
Iter: 1776 loss: 1.27206677e-05
Iter: 1777 loss: 1.27127123e-05
Iter: 1778 loss: 1.2742471e-05
Iter: 1779 loss: 1.27110625e-05
Iter: 1780 loss: 1.27043295e-05
Iter: 1781 loss: 1.27198582e-05
Iter: 1782 loss: 1.27024705e-05
Iter: 1783 loss: 1.26967543e-05
Iter: 1784 loss: 1.27622206e-05
Iter: 1785 loss: 1.2696979e-05
Iter: 1786 loss: 1.26903597e-05
Iter: 1787 loss: 1.27168114e-05
Iter: 1788 loss: 1.26889281e-05
Iter: 1789 loss: 1.26865261e-05
Iter: 1790 loss: 1.26795367e-05
Iter: 1791 loss: 1.27251733e-05
Iter: 1792 loss: 1.2678458e-05
Iter: 1793 loss: 1.26681971e-05
Iter: 1794 loss: 1.26904142e-05
Iter: 1795 loss: 1.26645082e-05
Iter: 1796 loss: 1.26562427e-05
Iter: 1797 loss: 1.27030798e-05
Iter: 1798 loss: 1.26556961e-05
Iter: 1799 loss: 1.26472532e-05
Iter: 1800 loss: 1.26500981e-05
Iter: 1801 loss: 1.26420182e-05
Iter: 1802 loss: 1.26317373e-05
Iter: 1803 loss: 1.27093626e-05
Iter: 1804 loss: 1.26312243e-05
Iter: 1805 loss: 1.26249251e-05
Iter: 1806 loss: 1.26349642e-05
Iter: 1807 loss: 1.26219211e-05
Iter: 1808 loss: 1.26139512e-05
Iter: 1809 loss: 1.26335453e-05
Iter: 1810 loss: 1.26109944e-05
Iter: 1811 loss: 1.26025097e-05
Iter: 1812 loss: 1.262972e-05
Iter: 1813 loss: 1.26006144e-05
Iter: 1814 loss: 1.25925853e-05
Iter: 1815 loss: 1.26031482e-05
Iter: 1816 loss: 1.25886481e-05
Iter: 1817 loss: 1.25799306e-05
Iter: 1818 loss: 1.26313134e-05
Iter: 1819 loss: 1.25788065e-05
Iter: 1820 loss: 1.25724782e-05
Iter: 1821 loss: 1.2572511e-05
Iter: 1822 loss: 1.25703882e-05
Iter: 1823 loss: 1.256415e-05
Iter: 1824 loss: 1.25904953e-05
Iter: 1825 loss: 1.25613133e-05
Iter: 1826 loss: 1.25523566e-05
Iter: 1827 loss: 1.25742863e-05
Iter: 1828 loss: 1.25493225e-05
Iter: 1829 loss: 1.25416982e-05
Iter: 1830 loss: 1.25682072e-05
Iter: 1831 loss: 1.25393954e-05
Iter: 1832 loss: 1.25313909e-05
Iter: 1833 loss: 1.25412716e-05
Iter: 1834 loss: 1.25270344e-05
Iter: 1835 loss: 1.25191355e-05
Iter: 1836 loss: 1.25565248e-05
Iter: 1837 loss: 1.25176966e-05
Iter: 1838 loss: 1.25105526e-05
Iter: 1839 loss: 1.25142797e-05
Iter: 1840 loss: 1.25062315e-05
Iter: 1841 loss: 1.2495726e-05
Iter: 1842 loss: 1.25420984e-05
Iter: 1843 loss: 1.24942562e-05
Iter: 1844 loss: 1.24867529e-05
Iter: 1845 loss: 1.25311526e-05
Iter: 1846 loss: 1.24854487e-05
Iter: 1847 loss: 1.24802755e-05
Iter: 1848 loss: 1.24882245e-05
Iter: 1849 loss: 1.24774215e-05
Iter: 1850 loss: 1.24699127e-05
Iter: 1851 loss: 1.24849794e-05
Iter: 1852 loss: 1.24672497e-05
Iter: 1853 loss: 1.24679736e-05
Iter: 1854 loss: 1.24640374e-05
Iter: 1855 loss: 1.24619201e-05
Iter: 1856 loss: 1.24569015e-05
Iter: 1857 loss: 1.24790677e-05
Iter: 1858 loss: 1.2454585e-05
Iter: 1859 loss: 1.24478229e-05
Iter: 1860 loss: 1.24578546e-05
Iter: 1861 loss: 1.24452527e-05
Iter: 1862 loss: 1.24383432e-05
Iter: 1863 loss: 1.24554608e-05
Iter: 1864 loss: 1.24359394e-05
Iter: 1865 loss: 1.24291728e-05
Iter: 1866 loss: 1.2442104e-05
Iter: 1867 loss: 1.24258913e-05
Iter: 1868 loss: 1.24186681e-05
Iter: 1869 loss: 1.24432754e-05
Iter: 1870 loss: 1.24167827e-05
Iter: 1871 loss: 1.24105973e-05
Iter: 1872 loss: 1.24257758e-05
Iter: 1873 loss: 1.24072585e-05
Iter: 1874 loss: 1.23990831e-05
Iter: 1875 loss: 1.24183571e-05
Iter: 1876 loss: 1.2395496e-05
Iter: 1877 loss: 1.2388924e-05
Iter: 1878 loss: 1.24223207e-05
Iter: 1879 loss: 1.2387407e-05
Iter: 1880 loss: 1.23815753e-05
Iter: 1881 loss: 1.23872032e-05
Iter: 1882 loss: 1.23785458e-05
Iter: 1883 loss: 1.23711452e-05
Iter: 1884 loss: 1.24198068e-05
Iter: 1885 loss: 1.2370665e-05
Iter: 1886 loss: 1.23716018e-05
Iter: 1887 loss: 1.23681266e-05
Iter: 1888 loss: 1.23666687e-05
Iter: 1889 loss: 1.23622385e-05
Iter: 1890 loss: 1.24141825e-05
Iter: 1891 loss: 1.23622949e-05
Iter: 1892 loss: 1.23583104e-05
Iter: 1893 loss: 1.23557793e-05
Iter: 1894 loss: 1.23544487e-05
Iter: 1895 loss: 1.23477894e-05
Iter: 1896 loss: 1.2360264e-05
Iter: 1897 loss: 1.23452737e-05
Iter: 1898 loss: 1.23383979e-05
Iter: 1899 loss: 1.23568425e-05
Iter: 1900 loss: 1.23358668e-05
Iter: 1901 loss: 1.23290738e-05
Iter: 1902 loss: 1.23471018e-05
Iter: 1903 loss: 1.23264526e-05
Iter: 1904 loss: 1.23199061e-05
Iter: 1905 loss: 1.23363197e-05
Iter: 1906 loss: 1.2317194e-05
Iter: 1907 loss: 1.23096324e-05
Iter: 1908 loss: 1.2324821e-05
Iter: 1909 loss: 1.23064074e-05
Iter: 1910 loss: 1.23004611e-05
Iter: 1911 loss: 1.23411428e-05
Iter: 1912 loss: 1.23000063e-05
Iter: 1913 loss: 1.22946785e-05
Iter: 1914 loss: 1.22964375e-05
Iter: 1915 loss: 1.22908559e-05
Iter: 1916 loss: 1.22835918e-05
Iter: 1917 loss: 1.23130685e-05
Iter: 1918 loss: 1.22817373e-05
Iter: 1919 loss: 1.2284665e-05
Iter: 1920 loss: 1.22796064e-05
Iter: 1921 loss: 1.22782767e-05
Iter: 1922 loss: 1.22733354e-05
Iter: 1923 loss: 1.23379677e-05
Iter: 1924 loss: 1.22734909e-05
Iter: 1925 loss: 1.22691508e-05
Iter: 1926 loss: 1.22682886e-05
Iter: 1927 loss: 1.22649781e-05
Iter: 1928 loss: 1.22593374e-05
Iter: 1929 loss: 1.22838019e-05
Iter: 1930 loss: 1.22581132e-05
Iter: 1931 loss: 1.22537858e-05
Iter: 1932 loss: 1.22600995e-05
Iter: 1933 loss: 1.22510228e-05
Iter: 1934 loss: 1.22452639e-05
Iter: 1935 loss: 1.22565625e-05
Iter: 1936 loss: 1.22429774e-05
Iter: 1937 loss: 1.22374058e-05
Iter: 1938 loss: 1.22604088e-05
Iter: 1939 loss: 1.22359288e-05
Iter: 1940 loss: 1.22304646e-05
Iter: 1941 loss: 1.22419578e-05
Iter: 1942 loss: 1.22280762e-05
Iter: 1943 loss: 1.22231331e-05
Iter: 1944 loss: 1.22488009e-05
Iter: 1945 loss: 1.22219544e-05
Iter: 1946 loss: 1.22172578e-05
Iter: 1947 loss: 1.22135516e-05
Iter: 1948 loss: 1.22122383e-05
Iter: 1949 loss: 1.22045913e-05
Iter: 1950 loss: 1.22501369e-05
Iter: 1951 loss: 1.22031988e-05
Iter: 1952 loss: 1.22020274e-05
Iter: 1953 loss: 1.22008787e-05
Iter: 1954 loss: 1.21974572e-05
Iter: 1955 loss: 1.21961812e-05
Iter: 1956 loss: 1.21940902e-05
Iter: 1957 loss: 1.21910052e-05
Iter: 1958 loss: 1.21847188e-05
Iter: 1959 loss: 1.23097052e-05
Iter: 1960 loss: 1.21845987e-05
Iter: 1961 loss: 1.21780231e-05
Iter: 1962 loss: 1.22252786e-05
Iter: 1963 loss: 1.21776575e-05
Iter: 1964 loss: 1.21717649e-05
Iter: 1965 loss: 1.21761577e-05
Iter: 1966 loss: 1.21696685e-05
Iter: 1967 loss: 1.2162267e-05
Iter: 1968 loss: 1.21804296e-05
Iter: 1969 loss: 1.21601488e-05
Iter: 1970 loss: 1.21534513e-05
Iter: 1971 loss: 1.21745597e-05
Iter: 1972 loss: 1.21511202e-05
Iter: 1973 loss: 1.21449066e-05
Iter: 1974 loss: 1.21601151e-05
Iter: 1975 loss: 1.21425255e-05
Iter: 1976 loss: 1.21364219e-05
Iter: 1977 loss: 1.21582125e-05
Iter: 1978 loss: 1.21350859e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi3 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi3
+ date
Sun Nov  8 02:23:53 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi3/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8/300_100_100_100_1 --function f1 --psi -2 --phi 3 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi3/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc6455c3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc6210918c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc621030b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc621098ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5fc041268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5fc041c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e01ac7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e01acc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e0201268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e0201ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e016e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e00b9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e00b9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e00c1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e0104a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e012c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e0071510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5e00716a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5947857b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5947a4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5947d09d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5946bebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5946fe730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5946f9ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc594709488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc59470dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc59468c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc59468e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc59468e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc59476f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5945c0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5945fe730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5945feae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc594633b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc594659d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc5945756a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.008242629
test_loss: 0.0093034785
train_loss: 0.0074139326
test_loss: 0.008548034
train_loss: 0.007143419
test_loss: 0.0087515805
train_loss: 0.0070486874
test_loss: 0.00849917
train_loss: 0.0066308444
test_loss: 0.008325612
train_loss: 0.0070739216
test_loss: 0.0088105295
train_loss: 0.007087183
test_loss: 0.008416538
train_loss: 0.0068529537
test_loss: 0.008326804
train_loss: 0.007248597
test_loss: 0.008884391
train_loss: 0.006935954
test_loss: 0.008475594
train_loss: 0.0071193515
test_loss: 0.008376656
train_loss: 0.007173288
test_loss: 0.008352747
train_loss: 0.007047262
test_loss: 0.008317926
train_loss: 0.0068011186
test_loss: 0.008180379
train_loss: 0.006592029
test_loss: 0.00812033
train_loss: 0.0069020167
test_loss: 0.0084739495
train_loss: 0.006727084
test_loss: 0.007975856
train_loss: 0.0072536413
test_loss: 0.008523991
train_loss: 0.006567399
test_loss: 0.008249917
train_loss: 0.006394705
test_loss: 0.008082414
train_loss: 0.006528093
test_loss: 0.008090554
train_loss: 0.0060691396
test_loss: 0.007830864
train_loss: 0.0067275083
test_loss: 0.008057287
train_loss: 0.0064977235
test_loss: 0.0076492783
train_loss: 0.0067270147
test_loss: 0.008226093
train_loss: 0.0066460688
test_loss: 0.008118655
train_loss: 0.0067647328
test_loss: 0.008266437
train_loss: 0.0065241754
test_loss: 0.007956033
train_loss: 0.006173347
test_loss: 0.007739543
train_loss: 0.006482253
test_loss: 0.0079152575
train_loss: 0.0064806473
test_loss: 0.007840192
train_loss: 0.0063706427
test_loss: 0.007869028
train_loss: 0.0063608824
test_loss: 0.007655194
train_loss: 0.006383777
test_loss: 0.0078065316
train_loss: 0.0064062863
test_loss: 0.0078022564
train_loss: 0.0062021688
test_loss: 0.007610306
train_loss: 0.006121588
test_loss: 0.007898312
train_loss: 0.006527191
test_loss: 0.007961847
train_loss: 0.0063208025
test_loss: 0.007913657
train_loss: 0.00652102
test_loss: 0.0076541835
train_loss: 0.006245543
test_loss: 0.0080412375
train_loss: 0.006232392
test_loss: 0.007660712
train_loss: 0.0057906224
test_loss: 0.0075071165
train_loss: 0.0060989177
test_loss: 0.0078113438
train_loss: 0.006071209
test_loss: 0.007770361
train_loss: 0.006074953
test_loss: 0.007994468
train_loss: 0.005870576
test_loss: 0.007534763
train_loss: 0.006571137
test_loss: 0.0078388695
train_loss: 0.005844064
test_loss: 0.0071709626
train_loss: 0.0063035544
test_loss: 0.007761237
train_loss: 0.0060252687
test_loss: 0.007708779
train_loss: 0.0062380093
test_loss: 0.007611228
train_loss: 0.0061760703
test_loss: 0.0076428973
train_loss: 0.005910934
test_loss: 0.0074285837
train_loss: 0.006243022
test_loss: 0.007935522
train_loss: 0.006089721
test_loss: 0.007580829
train_loss: 0.006260822
test_loss: 0.008108763
train_loss: 0.0060599437
test_loss: 0.0074444567
train_loss: 0.006300327
test_loss: 0.007737638
train_loss: 0.006135453
test_loss: 0.0077382424
train_loss: 0.0062207426
test_loss: 0.007992607
train_loss: 0.0064161243
test_loss: 0.0075456076
train_loss: 0.0058818255
test_loss: 0.007307897
train_loss: 0.0057921857
test_loss: 0.0072857253
train_loss: 0.0063204137
test_loss: 0.007471942
train_loss: 0.0059155105
test_loss: 0.007385148
train_loss: 0.0055811526
test_loss: 0.007239919
train_loss: 0.0057899845
test_loss: 0.0074297884
train_loss: 0.0059887986
test_loss: 0.007445436
train_loss: 0.0060050376
test_loss: 0.0072740847
train_loss: 0.00565081
test_loss: 0.007436794
train_loss: 0.0055303974
test_loss: 0.0074736127
train_loss: 0.005586394
test_loss: 0.0075168964
train_loss: 0.0062083644
test_loss: 0.0073713744
train_loss: 0.005900426
test_loss: 0.0076778256
train_loss: 0.0054377685
test_loss: 0.0071851737
train_loss: 0.0056358185
test_loss: 0.0073141214
train_loss: 0.0057559405
test_loss: 0.0077512832
train_loss: 0.0054561356
test_loss: 0.0070504844
train_loss: 0.005340342
test_loss: 0.007479653
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi3/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi3/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 3 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-2_phi3/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a667a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a667a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a665a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a667aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a660a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a660a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a6523d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a64f5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a64f5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a64adae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a64ad730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a6490ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a6493d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a6493730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a6493a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a6395730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a6395510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a63be158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a63be378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a637bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a631e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a62d8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a630f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a62b7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6a62b7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6808b28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6808d6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe68089eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe68089e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6808458c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe680811b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe68081d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe68081dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6807c9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe680760bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe680731048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.2195526e-05
Iter: 2 loss: 4.95058484e-05
Iter: 3 loss: 4.83660406e-05
Iter: 4 loss: 4.72495885e-05
Iter: 5 loss: 4.64894547e-05
Iter: 6 loss: 4.50585139e-05
Iter: 7 loss: 4.13113e-05
Iter: 8 loss: 7.00238597e-05
Iter: 9 loss: 4.05695391e-05
Iter: 10 loss: 3.75242525e-05
Iter: 11 loss: 5.4588636e-05
Iter: 12 loss: 3.70978487e-05
Iter: 13 loss: 3.43902102e-05
Iter: 14 loss: 3.55235024e-05
Iter: 15 loss: 3.25350265e-05
Iter: 16 loss: 3.11188778e-05
Iter: 17 loss: 3.10491741e-05
Iter: 18 loss: 3.01546133e-05
Iter: 19 loss: 2.83495028e-05
Iter: 20 loss: 6.21984655e-05
Iter: 21 loss: 2.83230074e-05
Iter: 22 loss: 2.72303e-05
Iter: 23 loss: 2.71952504e-05
Iter: 24 loss: 2.66113857e-05
Iter: 25 loss: 2.62269805e-05
Iter: 26 loss: 2.60050547e-05
Iter: 27 loss: 2.52062673e-05
Iter: 28 loss: 2.49220757e-05
Iter: 29 loss: 2.44726943e-05
Iter: 30 loss: 2.41127018e-05
Iter: 31 loss: 2.3983237e-05
Iter: 32 loss: 2.37825461e-05
Iter: 33 loss: 2.32857619e-05
Iter: 34 loss: 2.28287936e-05
Iter: 35 loss: 2.27082201e-05
Iter: 36 loss: 2.2111637e-05
Iter: 37 loss: 2.69896918e-05
Iter: 38 loss: 2.2074466e-05
Iter: 39 loss: 2.17339984e-05
Iter: 40 loss: 2.49091e-05
Iter: 41 loss: 2.17195065e-05
Iter: 42 loss: 2.13980202e-05
Iter: 43 loss: 2.25262302e-05
Iter: 44 loss: 2.13148051e-05
Iter: 45 loss: 2.11388688e-05
Iter: 46 loss: 2.08107267e-05
Iter: 47 loss: 2.82876717e-05
Iter: 48 loss: 2.08097845e-05
Iter: 49 loss: 2.03925119e-05
Iter: 50 loss: 2.38678949e-05
Iter: 51 loss: 2.03674699e-05
Iter: 52 loss: 2.0148269e-05
Iter: 53 loss: 2.00340055e-05
Iter: 54 loss: 1.9932666e-05
Iter: 55 loss: 1.94743061e-05
Iter: 56 loss: 2.13340882e-05
Iter: 57 loss: 1.93717187e-05
Iter: 58 loss: 1.91360759e-05
Iter: 59 loss: 1.94143613e-05
Iter: 60 loss: 1.90114188e-05
Iter: 61 loss: 1.87126898e-05
Iter: 62 loss: 2.07961202e-05
Iter: 63 loss: 1.86850357e-05
Iter: 64 loss: 1.85626013e-05
Iter: 65 loss: 1.85607205e-05
Iter: 66 loss: 1.84372257e-05
Iter: 67 loss: 1.82224394e-05
Iter: 68 loss: 1.82219446e-05
Iter: 69 loss: 1.80931911e-05
Iter: 70 loss: 1.96497058e-05
Iter: 71 loss: 1.80923689e-05
Iter: 72 loss: 1.79991184e-05
Iter: 73 loss: 1.78857154e-05
Iter: 74 loss: 1.78753853e-05
Iter: 75 loss: 1.7693852e-05
Iter: 76 loss: 2.00632821e-05
Iter: 77 loss: 1.76927424e-05
Iter: 78 loss: 1.76189242e-05
Iter: 79 loss: 1.75465393e-05
Iter: 80 loss: 1.75310215e-05
Iter: 81 loss: 1.73980698e-05
Iter: 82 loss: 1.76223257e-05
Iter: 83 loss: 1.7338305e-05
Iter: 84 loss: 1.72058881e-05
Iter: 85 loss: 1.72316741e-05
Iter: 86 loss: 1.71066367e-05
Iter: 87 loss: 1.69383748e-05
Iter: 88 loss: 1.91934814e-05
Iter: 89 loss: 1.69374325e-05
Iter: 90 loss: 1.68609586e-05
Iter: 91 loss: 1.68931256e-05
Iter: 92 loss: 1.68084134e-05
Iter: 93 loss: 1.66707378e-05
Iter: 94 loss: 1.68786191e-05
Iter: 95 loss: 1.66048667e-05
Iter: 96 loss: 1.65212568e-05
Iter: 97 loss: 1.75981295e-05
Iter: 98 loss: 1.65203273e-05
Iter: 99 loss: 1.64305675e-05
Iter: 100 loss: 1.65962683e-05
Iter: 101 loss: 1.63922286e-05
Iter: 102 loss: 1.63201548e-05
Iter: 103 loss: 1.63835794e-05
Iter: 104 loss: 1.6277525e-05
Iter: 105 loss: 1.6197122e-05
Iter: 106 loss: 1.61440257e-05
Iter: 107 loss: 1.61140906e-05
Iter: 108 loss: 1.6096099e-05
Iter: 109 loss: 1.60558047e-05
Iter: 110 loss: 1.60199597e-05
Iter: 111 loss: 1.59771243e-05
Iter: 112 loss: 1.59725587e-05
Iter: 113 loss: 1.58923158e-05
Iter: 114 loss: 1.58055045e-05
Iter: 115 loss: 1.57923096e-05
Iter: 116 loss: 1.56895658e-05
Iter: 117 loss: 1.65244819e-05
Iter: 118 loss: 1.56834794e-05
Iter: 119 loss: 1.56032802e-05
Iter: 120 loss: 1.56853457e-05
Iter: 121 loss: 1.55594244e-05
Iter: 122 loss: 1.5474885e-05
Iter: 123 loss: 1.61554308e-05
Iter: 124 loss: 1.54694171e-05
Iter: 125 loss: 1.54074405e-05
Iter: 126 loss: 1.54178051e-05
Iter: 127 loss: 1.53602323e-05
Iter: 128 loss: 1.52931025e-05
Iter: 129 loss: 1.6167116e-05
Iter: 130 loss: 1.52927605e-05
Iter: 131 loss: 1.52636385e-05
Iter: 132 loss: 1.567861e-05
Iter: 133 loss: 1.52639441e-05
Iter: 134 loss: 1.52339817e-05
Iter: 135 loss: 1.51601553e-05
Iter: 136 loss: 1.59437259e-05
Iter: 137 loss: 1.51531422e-05
Iter: 138 loss: 1.5083715e-05
Iter: 139 loss: 1.57990289e-05
Iter: 140 loss: 1.5082087e-05
Iter: 141 loss: 1.50429605e-05
Iter: 142 loss: 1.50835294e-05
Iter: 143 loss: 1.50222186e-05
Iter: 144 loss: 1.49640864e-05
Iter: 145 loss: 1.53781384e-05
Iter: 146 loss: 1.49590142e-05
Iter: 147 loss: 1.49299594e-05
Iter: 148 loss: 1.48905665e-05
Iter: 149 loss: 1.48883355e-05
Iter: 150 loss: 1.48249183e-05
Iter: 151 loss: 1.49775096e-05
Iter: 152 loss: 1.48016516e-05
Iter: 153 loss: 1.47564597e-05
Iter: 154 loss: 1.47949359e-05
Iter: 155 loss: 1.47295395e-05
Iter: 156 loss: 1.46531092e-05
Iter: 157 loss: 1.48421423e-05
Iter: 158 loss: 1.46265411e-05
Iter: 159 loss: 1.45788235e-05
Iter: 160 loss: 1.48308955e-05
Iter: 161 loss: 1.45722279e-05
Iter: 162 loss: 1.45230169e-05
Iter: 163 loss: 1.46203474e-05
Iter: 164 loss: 1.45025197e-05
Iter: 165 loss: 1.44717505e-05
Iter: 166 loss: 1.44715941e-05
Iter: 167 loss: 1.44386104e-05
Iter: 168 loss: 1.44246369e-05
Iter: 169 loss: 1.44076039e-05
Iter: 170 loss: 1.43689804e-05
Iter: 171 loss: 1.44905516e-05
Iter: 172 loss: 1.4358091e-05
Iter: 173 loss: 1.43304842e-05
Iter: 174 loss: 1.43115649e-05
Iter: 175 loss: 1.43015213e-05
Iter: 176 loss: 1.42570098e-05
Iter: 177 loss: 1.49337084e-05
Iter: 178 loss: 1.42566014e-05
Iter: 179 loss: 1.42282624e-05
Iter: 180 loss: 1.42729896e-05
Iter: 181 loss: 1.42154076e-05
Iter: 182 loss: 1.41913279e-05
Iter: 183 loss: 1.41590908e-05
Iter: 184 loss: 1.41574365e-05
Iter: 185 loss: 1.41053115e-05
Iter: 186 loss: 1.42531107e-05
Iter: 187 loss: 1.40887714e-05
Iter: 188 loss: 1.40442344e-05
Iter: 189 loss: 1.42090703e-05
Iter: 190 loss: 1.40326129e-05
Iter: 191 loss: 1.39901813e-05
Iter: 192 loss: 1.41683995e-05
Iter: 193 loss: 1.39812055e-05
Iter: 194 loss: 1.39483254e-05
Iter: 195 loss: 1.39684798e-05
Iter: 196 loss: 1.39282965e-05
Iter: 197 loss: 1.38740561e-05
Iter: 198 loss: 1.40663215e-05
Iter: 199 loss: 1.386037e-05
Iter: 200 loss: 1.38468649e-05
Iter: 201 loss: 1.38389296e-05
Iter: 202 loss: 1.38258547e-05
Iter: 203 loss: 1.37963843e-05
Iter: 204 loss: 1.41639894e-05
Iter: 205 loss: 1.37943362e-05
Iter: 206 loss: 1.37601855e-05
Iter: 207 loss: 1.39214535e-05
Iter: 208 loss: 1.37539282e-05
Iter: 209 loss: 1.37273601e-05
Iter: 210 loss: 1.37896959e-05
Iter: 211 loss: 1.37181451e-05
Iter: 212 loss: 1.36795534e-05
Iter: 213 loss: 1.38010491e-05
Iter: 214 loss: 1.36679901e-05
Iter: 215 loss: 1.36480758e-05
Iter: 216 loss: 1.36765702e-05
Iter: 217 loss: 1.36383251e-05
Iter: 218 loss: 1.36107328e-05
Iter: 219 loss: 1.35638302e-05
Iter: 220 loss: 1.35640066e-05
Iter: 221 loss: 1.35272239e-05
Iter: 222 loss: 1.40987177e-05
Iter: 223 loss: 1.35275186e-05
Iter: 224 loss: 1.34964484e-05
Iter: 225 loss: 1.34996353e-05
Iter: 226 loss: 1.34725524e-05
Iter: 227 loss: 1.34374241e-05
Iter: 228 loss: 1.3632507e-05
Iter: 229 loss: 1.34330512e-05
Iter: 230 loss: 1.33928961e-05
Iter: 231 loss: 1.34241545e-05
Iter: 232 loss: 1.33690137e-05
Iter: 233 loss: 1.33674475e-05
Iter: 234 loss: 1.33542108e-05
Iter: 235 loss: 1.33412022e-05
Iter: 236 loss: 1.33162766e-05
Iter: 237 loss: 1.38351033e-05
Iter: 238 loss: 1.33158192e-05
Iter: 239 loss: 1.32825808e-05
Iter: 240 loss: 1.33720887e-05
Iter: 241 loss: 1.32712066e-05
Iter: 242 loss: 1.32495388e-05
Iter: 243 loss: 1.33704689e-05
Iter: 244 loss: 1.32469258e-05
Iter: 245 loss: 1.32253845e-05
Iter: 246 loss: 1.33303984e-05
Iter: 247 loss: 1.3221339e-05
Iter: 248 loss: 1.32029727e-05
Iter: 249 loss: 1.31990046e-05
Iter: 250 loss: 1.3187333e-05
Iter: 251 loss: 1.31624329e-05
Iter: 252 loss: 1.31512134e-05
Iter: 253 loss: 1.31393772e-05
Iter: 254 loss: 1.31087236e-05
Iter: 255 loss: 1.34211268e-05
Iter: 256 loss: 1.31077413e-05
Iter: 257 loss: 1.30865847e-05
Iter: 258 loss: 1.30823555e-05
Iter: 259 loss: 1.3068443e-05
Iter: 260 loss: 1.30332182e-05
Iter: 261 loss: 1.31490742e-05
Iter: 262 loss: 1.30231856e-05
Iter: 263 loss: 1.30005519e-05
Iter: 264 loss: 1.31326233e-05
Iter: 265 loss: 1.29973705e-05
Iter: 266 loss: 1.29742266e-05
Iter: 267 loss: 1.30307335e-05
Iter: 268 loss: 1.29655318e-05
Iter: 269 loss: 1.29497012e-05
Iter: 270 loss: 1.29491227e-05
Iter: 271 loss: 1.29399e-05
Iter: 272 loss: 1.2915596e-05
Iter: 273 loss: 1.31246798e-05
Iter: 274 loss: 1.29115842e-05
Iter: 275 loss: 1.28866377e-05
Iter: 276 loss: 1.31781298e-05
Iter: 277 loss: 1.28861993e-05
Iter: 278 loss: 1.28732845e-05
Iter: 279 loss: 1.30115704e-05
Iter: 280 loss: 1.287347e-05
Iter: 281 loss: 1.28591755e-05
Iter: 282 loss: 1.28281217e-05
Iter: 283 loss: 1.3309078e-05
Iter: 284 loss: 1.28269949e-05
Iter: 285 loss: 1.27972535e-05
Iter: 286 loss: 1.30306371e-05
Iter: 287 loss: 1.27952853e-05
Iter: 288 loss: 1.27758049e-05
Iter: 289 loss: 1.27627791e-05
Iter: 290 loss: 1.27548401e-05
Iter: 291 loss: 1.27308022e-05
Iter: 292 loss: 1.28927186e-05
Iter: 293 loss: 1.27285839e-05
Iter: 294 loss: 1.27006633e-05
Iter: 295 loss: 1.27113653e-05
Iter: 296 loss: 1.26812583e-05
Iter: 297 loss: 1.26617151e-05
Iter: 298 loss: 1.27886688e-05
Iter: 299 loss: 1.26597843e-05
Iter: 300 loss: 1.26376763e-05
Iter: 301 loss: 1.26582381e-05
Iter: 302 loss: 1.2625107e-05
Iter: 303 loss: 1.2614044e-05
Iter: 304 loss: 1.26105233e-05
Iter: 305 loss: 1.25998531e-05
Iter: 306 loss: 1.25798269e-05
Iter: 307 loss: 1.29666e-05
Iter: 308 loss: 1.25791876e-05
Iter: 309 loss: 1.25573388e-05
Iter: 310 loss: 1.26301129e-05
Iter: 311 loss: 1.25509905e-05
Iter: 312 loss: 1.25363822e-05
Iter: 313 loss: 1.25907609e-05
Iter: 314 loss: 1.25332426e-05
Iter: 315 loss: 1.25139204e-05
Iter: 316 loss: 1.25432816e-05
Iter: 317 loss: 1.25040842e-05
Iter: 318 loss: 1.24852422e-05
Iter: 319 loss: 1.25156112e-05
Iter: 320 loss: 1.24756498e-05
Iter: 321 loss: 1.24601629e-05
Iter: 322 loss: 1.24616763e-05
Iter: 323 loss: 1.24473936e-05
Iter: 324 loss: 1.24213266e-05
Iter: 325 loss: 1.24908111e-05
Iter: 326 loss: 1.24124026e-05
Iter: 327 loss: 1.23945692e-05
Iter: 328 loss: 1.25056522e-05
Iter: 329 loss: 1.23923837e-05
Iter: 330 loss: 1.23754071e-05
Iter: 331 loss: 1.23853961e-05
Iter: 332 loss: 1.23644186e-05
Iter: 333 loss: 1.23446098e-05
Iter: 334 loss: 1.24157268e-05
Iter: 335 loss: 1.23398995e-05
Iter: 336 loss: 1.23175469e-05
Iter: 337 loss: 1.24353628e-05
Iter: 338 loss: 1.23144e-05
Iter: 339 loss: 1.22975443e-05
Iter: 340 loss: 1.25339175e-05
Iter: 341 loss: 1.22977781e-05
Iter: 342 loss: 1.22910051e-05
Iter: 343 loss: 1.22766742e-05
Iter: 344 loss: 1.25139868e-05
Iter: 345 loss: 1.22765741e-05
Iter: 346 loss: 1.22571437e-05
Iter: 347 loss: 1.23327372e-05
Iter: 348 loss: 1.22522724e-05
Iter: 349 loss: 1.22402689e-05
Iter: 350 loss: 1.24290391e-05
Iter: 351 loss: 1.22404354e-05
Iter: 352 loss: 1.22301381e-05
Iter: 353 loss: 1.22139008e-05
Iter: 354 loss: 1.22138936e-05
Iter: 355 loss: 1.2196524e-05
Iter: 356 loss: 1.22933197e-05
Iter: 357 loss: 1.21944367e-05
Iter: 358 loss: 1.21788416e-05
Iter: 359 loss: 1.21490684e-05
Iter: 360 loss: 1.27696821e-05
Iter: 361 loss: 1.21486728e-05
Iter: 362 loss: 1.21350877e-05
Iter: 363 loss: 1.21312387e-05
Iter: 364 loss: 1.21177345e-05
Iter: 365 loss: 1.20974173e-05
Iter: 366 loss: 1.20965342e-05
Iter: 367 loss: 1.20757959e-05
Iter: 368 loss: 1.23178052e-05
Iter: 369 loss: 1.20750974e-05
Iter: 370 loss: 1.20590239e-05
Iter: 371 loss: 1.20852565e-05
Iter: 372 loss: 1.20516452e-05
Iter: 373 loss: 1.20416189e-05
Iter: 374 loss: 1.20404038e-05
Iter: 375 loss: 1.20326513e-05
Iter: 376 loss: 1.20219065e-05
Iter: 377 loss: 1.20214936e-05
Iter: 378 loss: 1.20070245e-05
Iter: 379 loss: 1.20190289e-05
Iter: 380 loss: 1.19978376e-05
Iter: 381 loss: 1.19856686e-05
Iter: 382 loss: 1.19852693e-05
Iter: 383 loss: 1.19760989e-05
Iter: 384 loss: 1.19804099e-05
Iter: 385 loss: 1.19698889e-05
Iter: 386 loss: 1.19588276e-05
Iter: 387 loss: 1.19508386e-05
Iter: 388 loss: 1.19465612e-05
Iter: 389 loss: 1.19296546e-05
Iter: 390 loss: 1.19785254e-05
Iter: 391 loss: 1.19243477e-05
Iter: 392 loss: 1.19050674e-05
Iter: 393 loss: 1.19486085e-05
Iter: 394 loss: 1.1897906e-05
Iter: 395 loss: 1.18827957e-05
Iter: 396 loss: 1.19003907e-05
Iter: 397 loss: 1.18748267e-05
Iter: 398 loss: 1.18545959e-05
Iter: 399 loss: 1.19788874e-05
Iter: 400 loss: 1.18519638e-05
Iter: 401 loss: 1.18405842e-05
Iter: 402 loss: 1.18602675e-05
Iter: 403 loss: 1.18353037e-05
Iter: 404 loss: 1.18217267e-05
Iter: 405 loss: 1.19626293e-05
Iter: 406 loss: 1.18210446e-05
Iter: 407 loss: 1.18085463e-05
Iter: 408 loss: 1.18341159e-05
Iter: 409 loss: 1.18035869e-05
Iter: 410 loss: 1.17966702e-05
Iter: 411 loss: 1.17866839e-05
Iter: 412 loss: 1.17859154e-05
Iter: 413 loss: 1.17744403e-05
Iter: 414 loss: 1.19212127e-05
Iter: 415 loss: 1.17742102e-05
Iter: 416 loss: 1.17655236e-05
Iter: 417 loss: 1.1811293e-05
Iter: 418 loss: 1.17641175e-05
Iter: 419 loss: 1.17570817e-05
Iter: 420 loss: 1.17387599e-05
Iter: 421 loss: 1.19214128e-05
Iter: 422 loss: 1.17368072e-05
Iter: 423 loss: 1.17203499e-05
Iter: 424 loss: 1.19633769e-05
Iter: 425 loss: 1.17202289e-05
Iter: 426 loss: 1.1709003e-05
Iter: 427 loss: 1.17033642e-05
Iter: 428 loss: 1.16980718e-05
Iter: 429 loss: 1.16827068e-05
Iter: 430 loss: 1.1753893e-05
Iter: 431 loss: 1.16804131e-05
Iter: 432 loss: 1.1663029e-05
Iter: 433 loss: 1.16831425e-05
Iter: 434 loss: 1.16538104e-05
Iter: 435 loss: 1.1639695e-05
Iter: 436 loss: 1.17658528e-05
Iter: 437 loss: 1.16382707e-05
Iter: 438 loss: 1.16280544e-05
Iter: 439 loss: 1.16818501e-05
Iter: 440 loss: 1.16263482e-05
Iter: 441 loss: 1.16139572e-05
Iter: 442 loss: 1.16608517e-05
Iter: 443 loss: 1.16110277e-05
Iter: 444 loss: 1.16029478e-05
Iter: 445 loss: 1.16088886e-05
Iter: 446 loss: 1.15986913e-05
Iter: 447 loss: 1.15907296e-05
Iter: 448 loss: 1.15969451e-05
Iter: 449 loss: 1.15858729e-05
Iter: 450 loss: 1.15747662e-05
Iter: 451 loss: 1.16533829e-05
Iter: 452 loss: 1.15732391e-05
Iter: 453 loss: 1.15646835e-05
Iter: 454 loss: 1.15665762e-05
Iter: 455 loss: 1.15584135e-05
Iter: 456 loss: 1.15467174e-05
Iter: 457 loss: 1.15376715e-05
Iter: 458 loss: 1.15339099e-05
Iter: 459 loss: 1.15224657e-05
Iter: 460 loss: 1.16692145e-05
Iter: 461 loss: 1.15222638e-05
Iter: 462 loss: 1.15118655e-05
Iter: 463 loss: 1.15041803e-05
Iter: 464 loss: 1.15010143e-05
Iter: 465 loss: 1.14882987e-05
Iter: 466 loss: 1.15601688e-05
Iter: 467 loss: 1.1486849e-05
Iter: 468 loss: 1.1473513e-05
Iter: 469 loss: 1.14969698e-05
Iter: 470 loss: 1.14679196e-05
Iter: 471 loss: 1.14574559e-05
Iter: 472 loss: 1.15594103e-05
Iter: 473 loss: 1.14569684e-05
Iter: 474 loss: 1.14468749e-05
Iter: 475 loss: 1.15072417e-05
Iter: 476 loss: 1.144622e-05
Iter: 477 loss: 1.1438362e-05
Iter: 478 loss: 1.1440874e-05
Iter: 479 loss: 1.14330269e-05
Iter: 480 loss: 1.14261302e-05
Iter: 481 loss: 1.14275563e-05
Iter: 482 loss: 1.14209697e-05
Iter: 483 loss: 1.14085342e-05
Iter: 484 loss: 1.14674722e-05
Iter: 485 loss: 1.14068253e-05
Iter: 486 loss: 1.13950482e-05
Iter: 487 loss: 1.14222221e-05
Iter: 488 loss: 1.13912974e-05
Iter: 489 loss: 1.13822043e-05
Iter: 490 loss: 1.13739097e-05
Iter: 491 loss: 1.13725791e-05
Iter: 492 loss: 1.13587366e-05
Iter: 493 loss: 1.14383693e-05
Iter: 494 loss: 1.13568076e-05
Iter: 495 loss: 1.13461856e-05
Iter: 496 loss: 1.13557599e-05
Iter: 497 loss: 1.13396518e-05
Iter: 498 loss: 1.13252772e-05
Iter: 499 loss: 1.1366692e-05
Iter: 500 loss: 1.13210272e-05
Iter: 501 loss: 1.13089536e-05
Iter: 502 loss: 1.13283068e-05
Iter: 503 loss: 1.13032966e-05
Iter: 504 loss: 1.12874532e-05
Iter: 505 loss: 1.13570422e-05
Iter: 506 loss: 1.12840271e-05
Iter: 507 loss: 1.12810158e-05
Iter: 508 loss: 1.12779799e-05
Iter: 509 loss: 1.12741418e-05
Iter: 510 loss: 1.12658545e-05
Iter: 511 loss: 1.14142276e-05
Iter: 512 loss: 1.12657553e-05
Iter: 513 loss: 1.12560738e-05
Iter: 514 loss: 1.12886382e-05
Iter: 515 loss: 1.12536272e-05
Iter: 516 loss: 1.12455218e-05
Iter: 517 loss: 1.12686084e-05
Iter: 518 loss: 1.12431117e-05
Iter: 519 loss: 1.12311936e-05
Iter: 520 loss: 1.12426151e-05
Iter: 521 loss: 1.12247808e-05
Iter: 522 loss: 1.12160933e-05
Iter: 523 loss: 1.12559128e-05
Iter: 524 loss: 1.12145462e-05
Iter: 525 loss: 1.12070493e-05
Iter: 526 loss: 1.11958707e-05
Iter: 527 loss: 1.11956033e-05
Iter: 528 loss: 1.11834552e-05
Iter: 529 loss: 1.13338956e-05
Iter: 530 loss: 1.11837735e-05
Iter: 531 loss: 1.11728968e-05
Iter: 532 loss: 1.11677455e-05
Iter: 533 loss: 1.11634208e-05
Iter: 534 loss: 1.11508925e-05
Iter: 535 loss: 1.12486177e-05
Iter: 536 loss: 1.11495829e-05
Iter: 537 loss: 1.11375193e-05
Iter: 538 loss: 1.11407644e-05
Iter: 539 loss: 1.11287636e-05
Iter: 540 loss: 1.11295476e-05
Iter: 541 loss: 1.11239106e-05
Iter: 542 loss: 1.11194986e-05
Iter: 543 loss: 1.11169265e-05
Iter: 544 loss: 1.11145882e-05
Iter: 545 loss: 1.11080371e-05
Iter: 546 loss: 1.11042564e-05
Iter: 547 loss: 1.11011796e-05
Iter: 548 loss: 1.1092071e-05
Iter: 549 loss: 1.11638647e-05
Iter: 550 loss: 1.10911478e-05
Iter: 551 loss: 1.10839101e-05
Iter: 552 loss: 1.1118942e-05
Iter: 553 loss: 1.10825094e-05
Iter: 554 loss: 1.10761139e-05
Iter: 555 loss: 1.10706287e-05
Iter: 556 loss: 1.10686551e-05
Iter: 557 loss: 1.1059843e-05
Iter: 558 loss: 1.10912706e-05
Iter: 559 loss: 1.10574474e-05
Iter: 560 loss: 1.1047603e-05
Iter: 561 loss: 1.10456303e-05
Iter: 562 loss: 1.10392293e-05
Iter: 563 loss: 1.10292049e-05
Iter: 564 loss: 1.11360423e-05
Iter: 565 loss: 1.10288329e-05
Iter: 566 loss: 1.10193268e-05
Iter: 567 loss: 1.1011367e-05
Iter: 568 loss: 1.10084784e-05
Iter: 569 loss: 1.09977518e-05
Iter: 570 loss: 1.10963792e-05
Iter: 571 loss: 1.09971743e-05
Iter: 572 loss: 1.09868597e-05
Iter: 573 loss: 1.10312467e-05
Iter: 574 loss: 1.09848452e-05
Iter: 575 loss: 1.09781249e-05
Iter: 576 loss: 1.09777784e-05
Iter: 577 loss: 1.09743332e-05
Iter: 578 loss: 1.09654775e-05
Iter: 579 loss: 1.10437477e-05
Iter: 580 loss: 1.09639041e-05
Iter: 581 loss: 1.09533721e-05
Iter: 582 loss: 1.10667679e-05
Iter: 583 loss: 1.09529065e-05
Iter: 584 loss: 1.09476441e-05
Iter: 585 loss: 1.09954008e-05
Iter: 586 loss: 1.09471293e-05
Iter: 587 loss: 1.09415723e-05
Iter: 588 loss: 1.09311459e-05
Iter: 589 loss: 1.11634654e-05
Iter: 590 loss: 1.09310704e-05
Iter: 591 loss: 1.09184521e-05
Iter: 592 loss: 1.09727025e-05
Iter: 593 loss: 1.09158327e-05
Iter: 594 loss: 1.0906625e-05
Iter: 595 loss: 1.09097055e-05
Iter: 596 loss: 1.08999957e-05
Iter: 597 loss: 1.08891218e-05
Iter: 598 loss: 1.09817238e-05
Iter: 599 loss: 1.08884806e-05
Iter: 600 loss: 1.0880035e-05
Iter: 601 loss: 1.08719378e-05
Iter: 602 loss: 1.0869986e-05
Iter: 603 loss: 1.08574077e-05
Iter: 604 loss: 1.09999783e-05
Iter: 605 loss: 1.08568029e-05
Iter: 606 loss: 1.08495e-05
Iter: 607 loss: 1.08689128e-05
Iter: 608 loss: 1.08462646e-05
Iter: 609 loss: 1.08404165e-05
Iter: 610 loss: 1.08400809e-05
Iter: 611 loss: 1.08357281e-05
Iter: 612 loss: 1.08311124e-05
Iter: 613 loss: 1.08302629e-05
Iter: 614 loss: 1.08234162e-05
Iter: 615 loss: 1.08313161e-05
Iter: 616 loss: 1.08203658e-05
Iter: 617 loss: 1.08128424e-05
Iter: 618 loss: 1.0885472e-05
Iter: 619 loss: 1.08121876e-05
Iter: 620 loss: 1.08065342e-05
Iter: 621 loss: 1.08166187e-05
Iter: 622 loss: 1.08037193e-05
Iter: 623 loss: 1.07971719e-05
Iter: 624 loss: 1.0786338e-05
Iter: 625 loss: 1.07864907e-05
Iter: 626 loss: 1.07767737e-05
Iter: 627 loss: 1.08927525e-05
Iter: 628 loss: 1.07767355e-05
Iter: 629 loss: 1.07689975e-05
Iter: 630 loss: 1.075816e-05
Iter: 631 loss: 1.0758e-05
Iter: 632 loss: 1.07466094e-05
Iter: 633 loss: 1.08901586e-05
Iter: 634 loss: 1.07467295e-05
Iter: 635 loss: 1.07369488e-05
Iter: 636 loss: 1.07403666e-05
Iter: 637 loss: 1.07306096e-05
Iter: 638 loss: 1.0722255e-05
Iter: 639 loss: 1.07894193e-05
Iter: 640 loss: 1.07220367e-05
Iter: 641 loss: 1.07154501e-05
Iter: 642 loss: 1.07943606e-05
Iter: 643 loss: 1.07158648e-05
Iter: 644 loss: 1.0709311e-05
Iter: 645 loss: 1.07074356e-05
Iter: 646 loss: 1.0703734e-05
Iter: 647 loss: 1.06978914e-05
Iter: 648 loss: 1.07028918e-05
Iter: 649 loss: 1.06941361e-05
Iter: 650 loss: 1.06869556e-05
Iter: 651 loss: 1.07234409e-05
Iter: 652 loss: 1.06854986e-05
Iter: 653 loss: 1.06781499e-05
Iter: 654 loss: 1.07106498e-05
Iter: 655 loss: 1.06765974e-05
Iter: 656 loss: 1.06712014e-05
Iter: 657 loss: 1.066404e-05
Iter: 658 loss: 1.06637754e-05
Iter: 659 loss: 1.06530788e-05
Iter: 660 loss: 1.06968964e-05
Iter: 661 loss: 1.06508905e-05
Iter: 662 loss: 1.06427269e-05
Iter: 663 loss: 1.06448788e-05
Iter: 664 loss: 1.06365633e-05
Iter: 665 loss: 1.06240777e-05
Iter: 666 loss: 1.06703756e-05
Iter: 667 loss: 1.06206262e-05
Iter: 668 loss: 1.06113821e-05
Iter: 669 loss: 1.06425032e-05
Iter: 670 loss: 1.06088737e-05
Iter: 671 loss: 1.05988847e-05
Iter: 672 loss: 1.06113512e-05
Iter: 673 loss: 1.05932322e-05
Iter: 674 loss: 1.05876879e-05
Iter: 675 loss: 1.05874005e-05
Iter: 676 loss: 1.05810086e-05
Iter: 677 loss: 1.05827812e-05
Iter: 678 loss: 1.05762e-05
Iter: 679 loss: 1.05691315e-05
Iter: 680 loss: 1.05747167e-05
Iter: 681 loss: 1.05645895e-05
Iter: 682 loss: 1.05579984e-05
Iter: 683 loss: 1.05841655e-05
Iter: 684 loss: 1.0555972e-05
Iter: 685 loss: 1.05476738e-05
Iter: 686 loss: 1.05610034e-05
Iter: 687 loss: 1.05435211e-05
Iter: 688 loss: 1.05351046e-05
Iter: 689 loss: 1.05724093e-05
Iter: 690 loss: 1.0533704e-05
Iter: 691 loss: 1.05284471e-05
Iter: 692 loss: 1.05216768e-05
Iter: 693 loss: 1.05210438e-05
Iter: 694 loss: 1.05121981e-05
Iter: 695 loss: 1.05904655e-05
Iter: 696 loss: 1.05119116e-05
Iter: 697 loss: 1.05043073e-05
Iter: 698 loss: 1.04971896e-05
Iter: 699 loss: 1.04958581e-05
Iter: 700 loss: 1.04873634e-05
Iter: 701 loss: 1.06076586e-05
Iter: 702 loss: 1.04870214e-05
Iter: 703 loss: 1.04799419e-05
Iter: 704 loss: 1.04756709e-05
Iter: 705 loss: 1.04730898e-05
Iter: 706 loss: 1.04657538e-05
Iter: 707 loss: 1.0465732e-05
Iter: 708 loss: 1.04598494e-05
Iter: 709 loss: 1.0503627e-05
Iter: 710 loss: 1.0459e-05
Iter: 711 loss: 1.04537448e-05
Iter: 712 loss: 1.04467254e-05
Iter: 713 loss: 1.04463033e-05
Iter: 714 loss: 1.04396622e-05
Iter: 715 loss: 1.05160752e-05
Iter: 716 loss: 1.04393139e-05
Iter: 717 loss: 1.04339215e-05
Iter: 718 loss: 1.04448081e-05
Iter: 719 loss: 1.04310948e-05
Iter: 720 loss: 1.04243773e-05
Iter: 721 loss: 1.04302435e-05
Iter: 722 loss: 1.04200253e-05
Iter: 723 loss: 1.04137689e-05
Iter: 724 loss: 1.04261553e-05
Iter: 725 loss: 1.04102292e-05
Iter: 726 loss: 1.04028113e-05
Iter: 727 loss: 1.03955335e-05
Iter: 728 loss: 1.03935663e-05
Iter: 729 loss: 1.0383279e-05
Iter: 730 loss: 1.05134277e-05
Iter: 731 loss: 1.03830243e-05
Iter: 732 loss: 1.03752436e-05
Iter: 733 loss: 1.03742859e-05
Iter: 734 loss: 1.03686434e-05
Iter: 735 loss: 1.03597322e-05
Iter: 736 loss: 1.04081046e-05
Iter: 737 loss: 1.03580751e-05
Iter: 738 loss: 1.03483198e-05
Iter: 739 loss: 1.03630082e-05
Iter: 740 loss: 1.03431594e-05
Iter: 741 loss: 1.03415568e-05
Iter: 742 loss: 1.03382008e-05
Iter: 743 loss: 1.03352259e-05
Iter: 744 loss: 1.03287748e-05
Iter: 745 loss: 1.04086976e-05
Iter: 746 loss: 1.03279217e-05
Iter: 747 loss: 1.03197062e-05
Iter: 748 loss: 1.03552411e-05
Iter: 749 loss: 1.03179818e-05
Iter: 750 loss: 1.03124066e-05
Iter: 751 loss: 1.03637822e-05
Iter: 752 loss: 1.03122784e-05
Iter: 753 loss: 1.03073835e-05
Iter: 754 loss: 1.0300615e-05
Iter: 755 loss: 1.03001985e-05
Iter: 756 loss: 1.02909198e-05
Iter: 757 loss: 1.0338068e-05
Iter: 758 loss: 1.02895519e-05
Iter: 759 loss: 1.02828235e-05
Iter: 760 loss: 1.02759714e-05
Iter: 761 loss: 1.02739414e-05
Iter: 762 loss: 1.02637832e-05
Iter: 763 loss: 1.03696821e-05
Iter: 764 loss: 1.02630675e-05
Iter: 765 loss: 1.02570375e-05
Iter: 766 loss: 1.02570866e-05
Iter: 767 loss: 1.02520971e-05
Iter: 768 loss: 1.02418508e-05
Iter: 769 loss: 1.02788072e-05
Iter: 770 loss: 1.02394097e-05
Iter: 771 loss: 1.02303693e-05
Iter: 772 loss: 1.02505055e-05
Iter: 773 loss: 1.02273298e-05
Iter: 774 loss: 1.02227532e-05
Iter: 775 loss: 1.02218009e-05
Iter: 776 loss: 1.02178929e-05
Iter: 777 loss: 1.02171743e-05
Iter: 778 loss: 1.02144313e-05
Iter: 779 loss: 1.0209742e-05
Iter: 780 loss: 1.02093218e-05
Iter: 781 loss: 1.02056292e-05
Iter: 782 loss: 1.01990554e-05
Iter: 783 loss: 1.02532413e-05
Iter: 784 loss: 1.01988644e-05
Iter: 785 loss: 1.01939595e-05
Iter: 786 loss: 1.02045287e-05
Iter: 787 loss: 1.01917976e-05
Iter: 788 loss: 1.0186689e-05
Iter: 789 loss: 1.01831902e-05
Iter: 790 loss: 1.01813357e-05
Iter: 791 loss: 1.01730338e-05
Iter: 792 loss: 1.02131808e-05
Iter: 793 loss: 1.01714613e-05
Iter: 794 loss: 1.01653386e-05
Iter: 795 loss: 1.01614078e-05
Iter: 796 loss: 1.01583573e-05
Iter: 797 loss: 1.01510195e-05
Iter: 798 loss: 1.02488e-05
Iter: 799 loss: 1.01506621e-05
Iter: 800 loss: 1.01444602e-05
Iter: 801 loss: 1.01344613e-05
Iter: 802 loss: 1.0134183e-05
Iter: 803 loss: 1.01252617e-05
Iter: 804 loss: 1.0125088e-05
Iter: 805 loss: 1.01196019e-05
Iter: 806 loss: 1.01607939e-05
Iter: 807 loss: 1.01196283e-05
Iter: 808 loss: 1.01127198e-05
Iter: 809 loss: 1.01217738e-05
Iter: 810 loss: 1.01099931e-05
Iter: 811 loss: 1.01054702e-05
Iter: 812 loss: 1.01064297e-05
Iter: 813 loss: 1.01020723e-05
Iter: 814 loss: 1.00952584e-05
Iter: 815 loss: 1.01235337e-05
Iter: 816 loss: 1.0094157e-05
Iter: 817 loss: 1.00871603e-05
Iter: 818 loss: 1.01074529e-05
Iter: 819 loss: 1.00851994e-05
Iter: 820 loss: 1.00801717e-05
Iter: 821 loss: 1.00852449e-05
Iter: 822 loss: 1.00776988e-05
Iter: 823 loss: 1.00710949e-05
Iter: 824 loss: 1.00720699e-05
Iter: 825 loss: 1.00657471e-05
Iter: 826 loss: 1.00580164e-05
Iter: 827 loss: 1.00953366e-05
Iter: 828 loss: 1.00562638e-05
Iter: 829 loss: 1.00497218e-05
Iter: 830 loss: 1.00567431e-05
Iter: 831 loss: 1.00458537e-05
Iter: 832 loss: 1.00379511e-05
Iter: 833 loss: 1.00672878e-05
Iter: 834 loss: 1.00360257e-05
Iter: 835 loss: 1.00273483e-05
Iter: 836 loss: 1.00318266e-05
Iter: 837 loss: 1.00218858e-05
Iter: 838 loss: 1.00184552e-05
Iter: 839 loss: 1.00176403e-05
Iter: 840 loss: 1.00134885e-05
Iter: 841 loss: 1.0028205e-05
Iter: 842 loss: 1.00124162e-05
Iter: 843 loss: 1.000886e-05
Iter: 844 loss: 1.00020043e-05
Iter: 845 loss: 1.00022353e-05
Iter: 846 loss: 9.9976e-06
Iter: 847 loss: 1.00649049e-05
Iter: 848 loss: 9.99742497e-06
Iter: 849 loss: 9.99284748e-06
Iter: 850 loss: 9.99387703e-06
Iter: 851 loss: 9.99023e-06
Iter: 852 loss: 9.98387532e-06
Iter: 853 loss: 9.99629083e-06
Iter: 854 loss: 9.9809e-06
Iter: 855 loss: 9.97493771e-06
Iter: 856 loss: 9.9842855e-06
Iter: 857 loss: 9.97221377e-06
Iter: 858 loss: 9.96478775e-06
Iter: 859 loss: 9.97227744e-06
Iter: 860 loss: 9.9606923e-06
Iter: 861 loss: 9.95401115e-06
Iter: 862 loss: 9.9754152e-06
Iter: 863 loss: 9.95220125e-06
Iter: 864 loss: 9.94376569e-06
Iter: 865 loss: 9.94671063e-06
Iter: 866 loss: 9.93776757e-06
Iter: 867 loss: 9.92887453e-06
Iter: 868 loss: 9.97568168e-06
Iter: 869 loss: 9.92752939e-06
Iter: 870 loss: 9.92099467e-06
Iter: 871 loss: 9.96737799e-06
Iter: 872 loss: 9.92009336e-06
Iter: 873 loss: 9.91631623e-06
Iter: 874 loss: 9.91583875e-06
Iter: 875 loss: 9.91297202e-06
Iter: 876 loss: 9.90677381e-06
Iter: 877 loss: 9.99817166e-06
Iter: 878 loss: 9.9071458e-06
Iter: 879 loss: 9.90018361e-06
Iter: 880 loss: 9.93263711e-06
Iter: 881 loss: 9.89904e-06
Iter: 882 loss: 9.89344881e-06
Iter: 883 loss: 9.93743288e-06
Iter: 884 loss: 9.89320779e-06
Iter: 885 loss: 9.88899228e-06
Iter: 886 loss: 9.88441298e-06
Iter: 887 loss: 9.88371539e-06
Iter: 888 loss: 9.87566273e-06
Iter: 889 loss: 9.90073e-06
Iter: 890 loss: 9.8734472e-06
Iter: 891 loss: 9.86724535e-06
Iter: 892 loss: 9.87947351e-06
Iter: 893 loss: 9.8647e-06
Iter: 894 loss: 9.85728548e-06
Iter: 895 loss: 9.86779742e-06
Iter: 896 loss: 9.85347469e-06
Iter: 897 loss: 9.84618782e-06
Iter: 898 loss: 9.85615497e-06
Iter: 899 loss: 9.84225e-06
Iter: 900 loss: 9.83358495e-06
Iter: 901 loss: 9.88828288e-06
Iter: 902 loss: 9.83238e-06
Iter: 903 loss: 9.82711208e-06
Iter: 904 loss: 9.83282553e-06
Iter: 905 loss: 9.82456e-06
Iter: 906 loss: 9.81906578e-06
Iter: 907 loss: 9.81879111e-06
Iter: 908 loss: 9.81474477e-06
Iter: 909 loss: 9.81223457e-06
Iter: 910 loss: 9.81075118e-06
Iter: 911 loss: 9.80564e-06
Iter: 912 loss: 9.81003723e-06
Iter: 913 loss: 9.80280129e-06
Iter: 914 loss: 9.79683136e-06
Iter: 915 loss: 9.8507935e-06
Iter: 916 loss: 9.79611195e-06
Iter: 917 loss: 9.7914035e-06
Iter: 918 loss: 9.79461583e-06
Iter: 919 loss: 9.7886923e-06
Iter: 920 loss: 9.78249591e-06
Iter: 921 loss: 9.78059325e-06
Iter: 922 loss: 9.77721356e-06
Iter: 923 loss: 9.7696684e-06
Iter: 924 loss: 9.82605e-06
Iter: 925 loss: 9.76888623e-06
Iter: 926 loss: 9.76325464e-06
Iter: 927 loss: 9.76174306e-06
Iter: 928 loss: 9.75805051e-06
Iter: 929 loss: 9.7494385e-06
Iter: 930 loss: 9.78903154e-06
Iter: 931 loss: 9.74806881e-06
Iter: 932 loss: 9.74123759e-06
Iter: 933 loss: 9.75203147e-06
Iter: 934 loss: 9.73847818e-06
Iter: 935 loss: 9.73087e-06
Iter: 936 loss: 9.76821684e-06
Iter: 937 loss: 9.72940506e-06
Iter: 938 loss: 9.72627367e-06
Iter: 939 loss: 9.72598264e-06
Iter: 940 loss: 9.72227645e-06
Iter: 941 loss: 9.71503323e-06
Iter: 942 loss: 9.88641477e-06
Iter: 943 loss: 9.71519512e-06
Iter: 944 loss: 9.70939254e-06
Iter: 945 loss: 9.74939394e-06
Iter: 946 loss: 9.70910878e-06
Iter: 947 loss: 9.70482688e-06
Iter: 948 loss: 9.72133785e-06
Iter: 949 loss: 9.70358633e-06
Iter: 950 loss: 9.69807661e-06
Iter: 951 loss: 9.69848497e-06
Iter: 952 loss: 9.69373377e-06
Iter: 953 loss: 9.68813492e-06
Iter: 954 loss: 9.70413e-06
Iter: 955 loss: 9.68616e-06
Iter: 956 loss: 9.680115e-06
Iter: 957 loss: 9.68091354e-06
Iter: 958 loss: 9.67507185e-06
Iter: 959 loss: 9.66817151e-06
Iter: 960 loss: 9.71615736e-06
Iter: 961 loss: 9.66737207e-06
Iter: 962 loss: 9.66167499e-06
Iter: 963 loss: 9.65727941e-06
Iter: 964 loss: 9.65493109e-06
Iter: 965 loss: 9.64795163e-06
Iter: 966 loss: 9.72727e-06
Iter: 967 loss: 9.64791616e-06
Iter: 968 loss: 9.64149e-06
Iter: 969 loss: 9.63501e-06
Iter: 970 loss: 9.63390812e-06
Iter: 971 loss: 9.63320144e-06
Iter: 972 loss: 9.62973354e-06
Iter: 973 loss: 9.6261665e-06
Iter: 974 loss: 9.62293052e-06
Iter: 975 loss: 9.62197191e-06
Iter: 976 loss: 9.61622391e-06
Iter: 977 loss: 9.6243657e-06
Iter: 978 loss: 9.61342448e-06
Iter: 979 loss: 9.60850684e-06
Iter: 980 loss: 9.65465e-06
Iter: 981 loss: 9.60862326e-06
Iter: 982 loss: 9.60396574e-06
Iter: 983 loss: 9.60729722e-06
Iter: 984 loss: 9.60110901e-06
Iter: 985 loss: 9.59524186e-06
Iter: 986 loss: 9.59408499e-06
Iter: 987 loss: 9.58988858e-06
Iter: 988 loss: 9.58380406e-06
Iter: 989 loss: 9.62625109e-06
Iter: 990 loss: 9.58322562e-06
Iter: 991 loss: 9.57721e-06
Iter: 992 loss: 9.57519478e-06
Iter: 993 loss: 9.57162047e-06
Iter: 994 loss: 9.56516305e-06
Iter: 995 loss: 9.60844409e-06
Iter: 996 loss: 9.56398799e-06
Iter: 997 loss: 9.55776068e-06
Iter: 998 loss: 9.56096483e-06
Iter: 999 loss: 9.5533e-06
Iter: 1000 loss: 9.54628649e-06
Iter: 1001 loss: 9.57325483e-06
Iter: 1002 loss: 9.54492862e-06
Iter: 1003 loss: 9.5376563e-06
Iter: 1004 loss: 9.58169221e-06
Iter: 1005 loss: 9.53685867e-06
Iter: 1006 loss: 9.53081417e-06
Iter: 1007 loss: 9.60452235e-06
Iter: 1008 loss: 9.53095e-06
Iter: 1009 loss: 9.52839764e-06
Iter: 1010 loss: 9.52357823e-06
Iter: 1011 loss: 9.60486796e-06
Iter: 1012 loss: 9.52315713e-06
Iter: 1013 loss: 9.51716902e-06
Iter: 1014 loss: 9.55608448e-06
Iter: 1015 loss: 9.51665606e-06
Iter: 1016 loss: 9.51175298e-06
Iter: 1017 loss: 9.54880124e-06
Iter: 1018 loss: 9.51118818e-06
Iter: 1019 loss: 9.50738831e-06
Iter: 1020 loss: 9.49882087e-06
Iter: 1021 loss: 9.62623199e-06
Iter: 1022 loss: 9.49819332e-06
Iter: 1023 loss: 9.49170499e-06
Iter: 1024 loss: 9.49172136e-06
Iter: 1025 loss: 9.48669367e-06
Iter: 1026 loss: 9.48249181e-06
Iter: 1027 loss: 9.4807383e-06
Iter: 1028 loss: 9.47393e-06
Iter: 1029 loss: 9.53321523e-06
Iter: 1030 loss: 9.47312128e-06
Iter: 1031 loss: 9.46774162e-06
Iter: 1032 loss: 9.46622276e-06
Iter: 1033 loss: 9.46284308e-06
Iter: 1034 loss: 9.45475585e-06
Iter: 1035 loss: 9.50059439e-06
Iter: 1036 loss: 9.45382817e-06
Iter: 1037 loss: 9.44782732e-06
Iter: 1038 loss: 9.4706229e-06
Iter: 1039 loss: 9.4462448e-06
Iter: 1040 loss: 9.44069598e-06
Iter: 1041 loss: 9.52515256e-06
Iter: 1042 loss: 9.44071e-06
Iter: 1043 loss: 9.43803661e-06
Iter: 1044 loss: 9.43577288e-06
Iter: 1045 loss: 9.43509258e-06
Iter: 1046 loss: 9.43102896e-06
Iter: 1047 loss: 9.42940187e-06
Iter: 1048 loss: 9.42732277e-06
Iter: 1049 loss: 9.42103725e-06
Iter: 1050 loss: 9.48189336e-06
Iter: 1051 loss: 9.42074803e-06
Iter: 1052 loss: 9.41611415e-06
Iter: 1053 loss: 9.4143e-06
Iter: 1054 loss: 9.41223425e-06
Iter: 1055 loss: 9.40518657e-06
Iter: 1056 loss: 9.41497274e-06
Iter: 1057 loss: 9.40217251e-06
Iter: 1058 loss: 9.39644087e-06
Iter: 1059 loss: 9.42773659e-06
Iter: 1060 loss: 9.39563142e-06
Iter: 1061 loss: 9.3901217e-06
Iter: 1062 loss: 9.39021265e-06
Iter: 1063 loss: 9.38558696e-06
Iter: 1064 loss: 9.37980167e-06
Iter: 1065 loss: 9.42386396e-06
Iter: 1066 loss: 9.37916047e-06
Iter: 1067 loss: 9.37372e-06
Iter: 1068 loss: 9.37087407e-06
Iter: 1069 loss: 9.36857941e-06
Iter: 1070 loss: 9.36323704e-06
Iter: 1071 loss: 9.44494e-06
Iter: 1072 loss: 9.3629551e-06
Iter: 1073 loss: 9.35968274e-06
Iter: 1074 loss: 9.35956359e-06
Iter: 1075 loss: 9.35700155e-06
Iter: 1076 loss: 9.35259777e-06
Iter: 1077 loss: 9.35295066e-06
Iter: 1078 loss: 9.34836316e-06
Iter: 1079 loss: 9.35151184e-06
Iter: 1080 loss: 9.34567652e-06
Iter: 1081 loss: 9.3392573e-06
Iter: 1082 loss: 9.38194717e-06
Iter: 1083 loss: 9.33874435e-06
Iter: 1084 loss: 9.3334238e-06
Iter: 1085 loss: 9.3417e-06
Iter: 1086 loss: 9.33155206e-06
Iter: 1087 loss: 9.32727562e-06
Iter: 1088 loss: 9.32673e-06
Iter: 1089 loss: 9.32334387e-06
Iter: 1090 loss: 9.31707e-06
Iter: 1091 loss: 9.34246236e-06
Iter: 1092 loss: 9.31557315e-06
Iter: 1093 loss: 9.30997703e-06
Iter: 1094 loss: 9.32332932e-06
Iter: 1095 loss: 9.30818533e-06
Iter: 1096 loss: 9.3015251e-06
Iter: 1097 loss: 9.31117575e-06
Iter: 1098 loss: 9.29853741e-06
Iter: 1099 loss: 9.29291946e-06
Iter: 1100 loss: 9.30604619e-06
Iter: 1101 loss: 9.29078305e-06
Iter: 1102 loss: 9.2832579e-06
Iter: 1103 loss: 9.29780072e-06
Iter: 1104 loss: 9.2797718e-06
Iter: 1105 loss: 9.27986e-06
Iter: 1106 loss: 9.27723886e-06
Iter: 1107 loss: 9.27509e-06
Iter: 1108 loss: 9.26973826e-06
Iter: 1109 loss: 9.35880598e-06
Iter: 1110 loss: 9.26953544e-06
Iter: 1111 loss: 9.26378e-06
Iter: 1112 loss: 9.27925066e-06
Iter: 1113 loss: 9.2616292e-06
Iter: 1114 loss: 9.25776294e-06
Iter: 1115 loss: 9.3087574e-06
Iter: 1116 loss: 9.25784479e-06
Iter: 1117 loss: 9.25367749e-06
Iter: 1118 loss: 9.2522605e-06
Iter: 1119 loss: 9.2502105e-06
Iter: 1120 loss: 9.24567848e-06
Iter: 1121 loss: 9.2624241e-06
Iter: 1122 loss: 9.24450705e-06
Iter: 1123 loss: 9.24046253e-06
Iter: 1124 loss: 9.23626158e-06
Iter: 1125 loss: 9.23497919e-06
Iter: 1126 loss: 9.22912568e-06
Iter: 1127 loss: 9.31231625e-06
Iter: 1128 loss: 9.22905838e-06
Iter: 1129 loss: 9.2252094e-06
Iter: 1130 loss: 9.22207801e-06
Iter: 1131 loss: 9.22091567e-06
Iter: 1132 loss: 9.21448e-06
Iter: 1133 loss: 9.25155473e-06
Iter: 1134 loss: 9.21388164e-06
Iter: 1135 loss: 9.20870571e-06
Iter: 1136 loss: 9.20686e-06
Iter: 1137 loss: 9.2035807e-06
Iter: 1138 loss: 9.20052389e-06
Iter: 1139 loss: 9.19953163e-06
Iter: 1140 loss: 9.19611375e-06
Iter: 1141 loss: 9.20776165e-06
Iter: 1142 loss: 9.19473678e-06
Iter: 1143 loss: 9.19203558e-06
Iter: 1144 loss: 9.18515525e-06
Iter: 1145 loss: 9.30128954e-06
Iter: 1146 loss: 9.18535443e-06
Iter: 1147 loss: 9.18220758e-06
Iter: 1148 loss: 9.18149271e-06
Iter: 1149 loss: 9.17820489e-06
Iter: 1150 loss: 9.18146543e-06
Iter: 1151 loss: 9.176817e-06
Iter: 1152 loss: 9.17243415e-06
Iter: 1153 loss: 9.16718636e-06
Iter: 1154 loss: 9.16668068e-06
Iter: 1155 loss: 9.16024237e-06
Iter: 1156 loss: 9.20551884e-06
Iter: 1157 loss: 9.15978e-06
Iter: 1158 loss: 9.15390501e-06
Iter: 1159 loss: 9.15940109e-06
Iter: 1160 loss: 9.15090914e-06
Iter: 1161 loss: 9.14570228e-06
Iter: 1162 loss: 9.18326077e-06
Iter: 1163 loss: 9.14518205e-06
Iter: 1164 loss: 9.14050725e-06
Iter: 1165 loss: 9.13617896e-06
Iter: 1166 loss: 9.13524673e-06
Iter: 1167 loss: 9.12909854e-06
Iter: 1168 loss: 9.19487229e-06
Iter: 1169 loss: 9.12902942e-06
Iter: 1170 loss: 9.12453106e-06
Iter: 1171 loss: 9.13853819e-06
Iter: 1172 loss: 9.12303585e-06
Iter: 1173 loss: 9.1188258e-06
Iter: 1174 loss: 9.17726629e-06
Iter: 1175 loss: 9.11878942e-06
Iter: 1176 loss: 9.1165075e-06
Iter: 1177 loss: 9.11291318e-06
Iter: 1178 loss: 9.19358354e-06
Iter: 1179 loss: 9.11283769e-06
Iter: 1180 loss: 9.10763811e-06
Iter: 1181 loss: 9.12561063e-06
Iter: 1182 loss: 9.10624567e-06
Iter: 1183 loss: 9.10229755e-06
Iter: 1184 loss: 9.14169686e-06
Iter: 1185 loss: 9.1025031e-06
Iter: 1186 loss: 9.09998926e-06
Iter: 1187 loss: 9.09509e-06
Iter: 1188 loss: 9.18603382e-06
Iter: 1189 loss: 9.09495611e-06
Iter: 1190 loss: 9.08849324e-06
Iter: 1191 loss: 9.11411371e-06
Iter: 1192 loss: 9.08673883e-06
Iter: 1193 loss: 9.08195e-06
Iter: 1194 loss: 9.10275776e-06
Iter: 1195 loss: 9.08120819e-06
Iter: 1196 loss: 9.0765252e-06
Iter: 1197 loss: 9.08055517e-06
Iter: 1198 loss: 9.07321919e-06
Iter: 1199 loss: 9.06814057e-06
Iter: 1200 loss: 9.09346545e-06
Iter: 1201 loss: 9.06732748e-06
Iter: 1202 loss: 9.06204332e-06
Iter: 1203 loss: 9.05839079e-06
Iter: 1204 loss: 9.05643719e-06
Iter: 1205 loss: 9.05093111e-06
Iter: 1206 loss: 9.13142685e-06
Iter: 1207 loss: 9.05062552e-06
Iter: 1208 loss: 9.04653e-06
Iter: 1209 loss: 9.09510527e-06
Iter: 1210 loss: 9.04682747e-06
Iter: 1211 loss: 9.04300123e-06
Iter: 1212 loss: 9.03877844e-06
Iter: 1213 loss: 9.03816e-06
Iter: 1214 loss: 9.03446e-06
Iter: 1215 loss: 9.05676279e-06
Iter: 1216 loss: 9.03429827e-06
Iter: 1217 loss: 9.03020555e-06
Iter: 1218 loss: 9.04455e-06
Iter: 1219 loss: 9.02942247e-06
Iter: 1220 loss: 9.02611737e-06
Iter: 1221 loss: 9.02668216e-06
Iter: 1222 loss: 9.02343072e-06
Iter: 1223 loss: 9.01922613e-06
Iter: 1224 loss: 9.01942258e-06
Iter: 1225 loss: 9.01604835e-06
Iter: 1226 loss: 9.01042495e-06
Iter: 1227 loss: 9.0490721e-06
Iter: 1228 loss: 9.00958094e-06
Iter: 1229 loss: 9.00600116e-06
Iter: 1230 loss: 9.00943269e-06
Iter: 1231 loss: 9.00383202e-06
Iter: 1232 loss: 8.99816223e-06
Iter: 1233 loss: 9.00300165e-06
Iter: 1234 loss: 8.99416e-06
Iter: 1235 loss: 8.98875805e-06
Iter: 1236 loss: 9.01898784e-06
Iter: 1237 loss: 8.98814505e-06
Iter: 1238 loss: 8.98307917e-06
Iter: 1239 loss: 8.98752569e-06
Iter: 1240 loss: 8.98005601e-06
Iter: 1241 loss: 8.97898281e-06
Iter: 1242 loss: 8.97729115e-06
Iter: 1243 loss: 8.97536484e-06
Iter: 1244 loss: 8.9722e-06
Iter: 1245 loss: 8.97176051e-06
Iter: 1246 loss: 8.96806523e-06
Iter: 1247 loss: 8.97155132e-06
Iter: 1248 loss: 8.96566598e-06
Iter: 1249 loss: 8.96231904e-06
Iter: 1250 loss: 9.01192743e-06
Iter: 1251 loss: 8.9619507e-06
Iter: 1252 loss: 8.95990524e-06
Iter: 1253 loss: 8.95619451e-06
Iter: 1254 loss: 8.95621633e-06
Iter: 1255 loss: 8.95079e-06
Iter: 1256 loss: 8.96132e-06
Iter: 1257 loss: 8.94865116e-06
Iter: 1258 loss: 8.94425648e-06
Iter: 1259 loss: 8.95629e-06
Iter: 1260 loss: 8.94275399e-06
Iter: 1261 loss: 8.93757078e-06
Iter: 1262 loss: 8.95052835e-06
Iter: 1263 loss: 8.93562719e-06
Iter: 1264 loss: 8.93083597e-06
Iter: 1265 loss: 8.93892229e-06
Iter: 1266 loss: 8.92859134e-06
Iter: 1267 loss: 8.92252e-06
Iter: 1268 loss: 8.94435652e-06
Iter: 1269 loss: 8.92121807e-06
Iter: 1270 loss: 8.91746458e-06
Iter: 1271 loss: 8.92415483e-06
Iter: 1272 loss: 8.9159621e-06
Iter: 1273 loss: 8.91182935e-06
Iter: 1274 loss: 8.95553057e-06
Iter: 1275 loss: 8.91197396e-06
Iter: 1276 loss: 8.90772753e-06
Iter: 1277 loss: 8.92050048e-06
Iter: 1278 loss: 8.90665433e-06
Iter: 1279 loss: 8.90424235e-06
Iter: 1280 loss: 8.90168e-06
Iter: 1281 loss: 8.90151114e-06
Iter: 1282 loss: 8.89697276e-06
Iter: 1283 loss: 8.92921798e-06
Iter: 1284 loss: 8.89653802e-06
Iter: 1285 loss: 8.89241164e-06
Iter: 1286 loss: 8.89997318e-06
Iter: 1287 loss: 8.89077455e-06
Iter: 1288 loss: 8.88731574e-06
Iter: 1289 loss: 8.88388422e-06
Iter: 1290 loss: 8.88331488e-06
Iter: 1291 loss: 8.87732494e-06
Iter: 1292 loss: 8.9142286e-06
Iter: 1293 loss: 8.87697e-06
Iter: 1294 loss: 8.87248461e-06
Iter: 1295 loss: 8.87820624e-06
Iter: 1296 loss: 8.87038823e-06
Iter: 1297 loss: 8.86416183e-06
Iter: 1298 loss: 8.87606438e-06
Iter: 1299 loss: 8.86152156e-06
Iter: 1300 loss: 8.85710415e-06
Iter: 1301 loss: 8.88583236e-06
Iter: 1302 loss: 8.85644658e-06
Iter: 1303 loss: 8.8519173e-06
Iter: 1304 loss: 8.85249119e-06
Iter: 1305 loss: 8.84892506e-06
Iter: 1306 loss: 8.84521432e-06
Iter: 1307 loss: 8.84505607e-06
Iter: 1308 loss: 8.84086e-06
Iter: 1309 loss: 8.84918245e-06
Iter: 1310 loss: 8.83922621e-06
Iter: 1311 loss: 8.8358629e-06
Iter: 1312 loss: 8.83474422e-06
Iter: 1313 loss: 8.83223584e-06
Iter: 1314 loss: 8.82874883e-06
Iter: 1315 loss: 8.8533252e-06
Iter: 1316 loss: 8.82824861e-06
Iter: 1317 loss: 8.82356926e-06
Iter: 1318 loss: 8.82405766e-06
Iter: 1319 loss: 8.82045151e-06
Iter: 1320 loss: 8.81560754e-06
Iter: 1321 loss: 8.82871427e-06
Iter: 1322 loss: 8.8139e-06
Iter: 1323 loss: 8.80990774e-06
Iter: 1324 loss: 8.8083334e-06
Iter: 1325 loss: 8.80562584e-06
Iter: 1326 loss: 8.79914842e-06
Iter: 1327 loss: 8.84339079e-06
Iter: 1328 loss: 8.79893378e-06
Iter: 1329 loss: 8.79372783e-06
Iter: 1330 loss: 8.79752406e-06
Iter: 1331 loss: 8.79014897e-06
Iter: 1332 loss: 8.78394e-06
Iter: 1333 loss: 8.81740834e-06
Iter: 1334 loss: 8.78319952e-06
Iter: 1335 loss: 8.77891853e-06
Iter: 1336 loss: 8.78217543e-06
Iter: 1337 loss: 8.77617458e-06
Iter: 1338 loss: 8.7693461e-06
Iter: 1339 loss: 8.79110303e-06
Iter: 1340 loss: 8.76766262e-06
Iter: 1341 loss: 8.76370905e-06
Iter: 1342 loss: 8.76315426e-06
Iter: 1343 loss: 8.7613189e-06
Iter: 1344 loss: 8.75677e-06
Iter: 1345 loss: 8.84301062e-06
Iter: 1346 loss: 8.75666501e-06
Iter: 1347 loss: 8.75238493e-06
Iter: 1348 loss: 8.77625644e-06
Iter: 1349 loss: 8.75122623e-06
Iter: 1350 loss: 8.74746456e-06
Iter: 1351 loss: 8.77567709e-06
Iter: 1352 loss: 8.74712532e-06
Iter: 1353 loss: 8.7438566e-06
Iter: 1354 loss: 8.73829049e-06
Iter: 1355 loss: 8.87006263e-06
Iter: 1356 loss: 8.73842237e-06
Iter: 1357 loss: 8.7329081e-06
Iter: 1358 loss: 8.77896309e-06
Iter: 1359 loss: 8.73293084e-06
Iter: 1360 loss: 8.72845885e-06
Iter: 1361 loss: 8.72581586e-06
Iter: 1362 loss: 8.72374585e-06
Iter: 1363 loss: 8.71780594e-06
Iter: 1364 loss: 8.78194078e-06
Iter: 1365 loss: 8.71719294e-06
Iter: 1366 loss: 8.71321845e-06
Iter: 1367 loss: 8.71574866e-06
Iter: 1368 loss: 8.71057273e-06
Iter: 1369 loss: 8.70543499e-06
Iter: 1370 loss: 8.72435157e-06
Iter: 1371 loss: 8.70387703e-06
Iter: 1372 loss: 8.69895666e-06
Iter: 1373 loss: 8.71254633e-06
Iter: 1374 loss: 8.69737232e-06
Iter: 1375 loss: 8.69380165e-06
Iter: 1376 loss: 8.69377436e-06
Iter: 1377 loss: 8.69088e-06
Iter: 1378 loss: 8.6895634e-06
Iter: 1379 loss: 8.68838561e-06
Iter: 1380 loss: 8.68518146e-06
Iter: 1381 loss: 8.68631651e-06
Iter: 1382 loss: 8.68324059e-06
Iter: 1383 loss: 8.67806375e-06
Iter: 1384 loss: 8.70448275e-06
Iter: 1385 loss: 8.6773116e-06
Iter: 1386 loss: 8.67312e-06
Iter: 1387 loss: 8.67494418e-06
Iter: 1388 loss: 8.67003837e-06
Iter: 1389 loss: 8.66547089e-06
Iter: 1390 loss: 8.66259506e-06
Iter: 1391 loss: 8.66120172e-06
Iter: 1392 loss: 8.65381e-06
Iter: 1393 loss: 8.70076565e-06
Iter: 1394 loss: 8.65286802e-06
Iter: 1395 loss: 8.64805588e-06
Iter: 1396 loss: 8.65607944e-06
Iter: 1397 loss: 8.64550293e-06
Iter: 1398 loss: 8.64030699e-06
Iter: 1399 loss: 8.67934432e-06
Iter: 1400 loss: 8.63991136e-06
Iter: 1401 loss: 8.6366781e-06
Iter: 1402 loss: 8.63332e-06
Iter: 1403 loss: 8.63230798e-06
Iter: 1404 loss: 8.62503111e-06
Iter: 1405 loss: 8.6609416e-06
Iter: 1406 loss: 8.62406614e-06
Iter: 1407 loss: 8.62193e-06
Iter: 1408 loss: 8.6211312e-06
Iter: 1409 loss: 8.61869921e-06
Iter: 1410 loss: 8.61375702e-06
Iter: 1411 loss: 8.61368699e-06
Iter: 1412 loss: 8.60972614e-06
Iter: 1413 loss: 8.62328125e-06
Iter: 1414 loss: 8.60855107e-06
Iter: 1415 loss: 8.60373802e-06
Iter: 1416 loss: 8.62978413e-06
Iter: 1417 loss: 8.60355249e-06
Iter: 1418 loss: 8.59816828e-06
Iter: 1419 loss: 8.59436568e-06
Iter: 1420 loss: 8.59242755e-06
Iter: 1421 loss: 8.58807471e-06
Iter: 1422 loss: 8.6133723e-06
Iter: 1423 loss: 8.58723888e-06
Iter: 1424 loss: 8.58341082e-06
Iter: 1425 loss: 8.58146723e-06
Iter: 1426 loss: 8.57935447e-06
Iter: 1427 loss: 8.57454143e-06
Iter: 1428 loss: 8.63743389e-06
Iter: 1429 loss: 8.57440682e-06
Iter: 1430 loss: 8.57113355e-06
Iter: 1431 loss: 8.57055329e-06
Iter: 1432 loss: 8.56845054e-06
Iter: 1433 loss: 8.56317274e-06
Iter: 1434 loss: 8.58342719e-06
Iter: 1435 loss: 8.56137649e-06
Iter: 1436 loss: 8.55672442e-06
Iter: 1437 loss: 8.55837061e-06
Iter: 1438 loss: 8.55295912e-06
Iter: 1439 loss: 8.54998689e-06
Iter: 1440 loss: 8.54939572e-06
Iter: 1441 loss: 8.54609152e-06
Iter: 1442 loss: 8.5576421e-06
Iter: 1443 loss: 8.54533846e-06
Iter: 1444 loss: 8.54330392e-06
Iter: 1445 loss: 8.53818528e-06
Iter: 1446 loss: 8.58509611e-06
Iter: 1447 loss: 8.5374113e-06
Iter: 1448 loss: 8.53224628e-06
Iter: 1449 loss: 8.53216261e-06
Iter: 1450 loss: 8.52842732e-06
Iter: 1451 loss: 8.54398058e-06
Iter: 1452 loss: 8.52749e-06
Iter: 1453 loss: 8.52482844e-06
Iter: 1454 loss: 8.52091671e-06
Iter: 1455 loss: 8.52109315e-06
Iter: 1456 loss: 8.51681398e-06
Iter: 1457 loss: 8.53720667e-06
Iter: 1458 loss: 8.51591449e-06
Iter: 1459 loss: 8.51105e-06
Iter: 1460 loss: 8.51714049e-06
Iter: 1461 loss: 8.50847209e-06
Iter: 1462 loss: 8.50470224e-06
Iter: 1463 loss: 8.5325928e-06
Iter: 1464 loss: 8.50450851e-06
Iter: 1465 loss: 8.5006377e-06
Iter: 1466 loss: 8.49652224e-06
Iter: 1467 loss: 8.49546086e-06
Iter: 1468 loss: 8.49051321e-06
Iter: 1469 loss: 8.55514918e-06
Iter: 1470 loss: 8.49040225e-06
Iter: 1471 loss: 8.48676427e-06
Iter: 1472 loss: 8.49115531e-06
Iter: 1473 loss: 8.48528907e-06
Iter: 1474 loss: 8.48122636e-06
Iter: 1475 loss: 8.48107356e-06
Iter: 1476 loss: 8.47930187e-06
Iter: 1477 loss: 8.47617503e-06
Iter: 1478 loss: 8.4760868e-06
Iter: 1479 loss: 8.47186584e-06
Iter: 1480 loss: 8.47611864e-06
Iter: 1481 loss: 8.46952116e-06
Iter: 1482 loss: 8.46523926e-06
Iter: 1483 loss: 8.52297853e-06
Iter: 1484 loss: 8.46517105e-06
Iter: 1485 loss: 8.46321473e-06
Iter: 1486 loss: 8.46066632e-06
Iter: 1487 loss: 8.46023431e-06
Iter: 1488 loss: 8.45579234e-06
Iter: 1489 loss: 8.45692193e-06
Iter: 1490 loss: 8.45236e-06
Iter: 1491 loss: 8.44814258e-06
Iter: 1492 loss: 8.4816e-06
Iter: 1493 loss: 8.44804526e-06
Iter: 1494 loss: 8.44456918e-06
Iter: 1495 loss: 8.45629074e-06
Iter: 1496 loss: 8.44361693e-06
Iter: 1497 loss: 8.44112219e-06
Iter: 1498 loss: 8.44439455e-06
Iter: 1499 loss: 8.43938869e-06
Iter: 1500 loss: 8.43527869e-06
Iter: 1501 loss: 8.44425631e-06
Iter: 1502 loss: 8.43373527e-06
Iter: 1503 loss: 8.43085672e-06
Iter: 1504 loss: 8.43998896e-06
Iter: 1505 loss: 8.4299e-06
Iter: 1506 loss: 8.42710415e-06
Iter: 1507 loss: 8.42724876e-06
Iter: 1508 loss: 8.42487498e-06
Iter: 1509 loss: 8.42202098e-06
Iter: 1510 loss: 8.42166719e-06
Iter: 1511 loss: 8.41940073e-06
Iter: 1512 loss: 8.42544432e-06
Iter: 1513 loss: 8.41848e-06
Iter: 1514 loss: 8.4158728e-06
Iter: 1515 loss: 8.42924e-06
Iter: 1516 loss: 8.41573546e-06
Iter: 1517 loss: 8.41269502e-06
Iter: 1518 loss: 8.41625297e-06
Iter: 1519 loss: 8.41143628e-06
Iter: 1520 loss: 8.40879e-06
Iter: 1521 loss: 8.40653411e-06
Iter: 1522 loss: 8.4058147e-06
Iter: 1523 loss: 8.40232315e-06
Iter: 1524 loss: 8.43857197e-06
Iter: 1525 loss: 8.40198845e-06
Iter: 1526 loss: 8.39975746e-06
Iter: 1527 loss: 8.40307348e-06
Iter: 1528 loss: 8.39858876e-06
Iter: 1529 loss: 8.39485438e-06
Iter: 1530 loss: 8.4001349e-06
Iter: 1531 loss: 8.39328459e-06
Iter: 1532 loss: 8.39033692e-06
Iter: 1533 loss: 8.40439589e-06
Iter: 1534 loss: 8.38977212e-06
Iter: 1535 loss: 8.38694359e-06
Iter: 1536 loss: 8.39209861e-06
Iter: 1537 loss: 8.38523192e-06
Iter: 1538 loss: 8.38481083e-06
Iter: 1539 loss: 8.38411233e-06
Iter: 1540 loss: 8.38283722e-06
Iter: 1541 loss: 8.38113738e-06
Iter: 1542 loss: 8.38116102e-06
Iter: 1543 loss: 8.37919742e-06
Iter: 1544 loss: 8.38102187e-06
Iter: 1545 loss: 8.37775224e-06
Iter: 1546 loss: 8.37604057e-06
Iter: 1547 loss: 8.38577216e-06
Iter: 1548 loss: 8.37606694e-06
Iter: 1549 loss: 8.3734e-06
Iter: 1550 loss: 8.37183325e-06
Iter: 1551 loss: 8.37046719e-06
Iter: 1552 loss: 8.36763775e-06
Iter: 1553 loss: 8.38428059e-06
Iter: 1554 loss: 8.36738e-06
Iter: 1555 loss: 8.36526124e-06
Iter: 1556 loss: 8.36359868e-06
Iter: 1557 loss: 8.36301933e-06
Iter: 1558 loss: 8.35992341e-06
Iter: 1559 loss: 8.38974483e-06
Iter: 1560 loss: 8.35987339e-06
Iter: 1561 loss: 8.35734227e-06
Iter: 1562 loss: 8.36019535e-06
Iter: 1563 loss: 8.35676656e-06
Iter: 1564 loss: 8.35366245e-06
Iter: 1565 loss: 8.36156e-06
Iter: 1566 loss: 8.3525465e-06
Iter: 1567 loss: 8.35019273e-06
Iter: 1568 loss: 8.35163519e-06
Iter: 1569 loss: 8.34859111e-06
Iter: 1570 loss: 8.34578896e-06
Iter: 1571 loss: 8.38171763e-06
Iter: 1572 loss: 8.34565617e-06
Iter: 1573 loss: 8.34332241e-06
Iter: 1574 loss: 8.36758954e-06
Iter: 1575 loss: 8.34334e-06
Iter: 1576 loss: 8.34238e-06
Iter: 1577 loss: 8.33972081e-06
Iter: 1578 loss: 8.36367508e-06
Iter: 1579 loss: 8.33954073e-06
Iter: 1580 loss: 8.33649938e-06
Iter: 1581 loss: 8.35742321e-06
Iter: 1582 loss: 8.33618469e-06
Iter: 1583 loss: 8.3337527e-06
Iter: 1584 loss: 8.35028e-06
Iter: 1585 loss: 8.33355898e-06
Iter: 1586 loss: 8.3317027e-06
Iter: 1587 loss: 8.33000377e-06
Iter: 1588 loss: 8.32963542e-06
Iter: 1589 loss: 8.32728529e-06
Iter: 1590 loss: 8.34320781e-06
Iter: 1591 loss: 8.32728256e-06
Iter: 1592 loss: 8.32514343e-06
Iter: 1593 loss: 8.32293881e-06
Iter: 1594 loss: 8.32244e-06
Iter: 1595 loss: 8.31987927e-06
Iter: 1596 loss: 8.31981e-06
Iter: 1597 loss: 8.31774196e-06
Iter: 1598 loss: 8.31645048e-06
Iter: 1599 loss: 8.31584657e-06
Iter: 1600 loss: 8.31271427e-06
Iter: 1601 loss: 8.33094055e-06
Iter: 1602 loss: 8.31184661e-06
Iter: 1603 loss: 8.30978934e-06
Iter: 1604 loss: 8.31503075e-06
Iter: 1605 loss: 8.3088371e-06
Iter: 1606 loss: 8.30745194e-06
Iter: 1607 loss: 8.3072664e-06
Iter: 1608 loss: 8.30609315e-06
Iter: 1609 loss: 8.30507088e-06
Iter: 1610 loss: 8.30458157e-06
Iter: 1611 loss: 8.30292e-06
Iter: 1612 loss: 8.30100271e-06
Iter: 1613 loss: 8.30085901e-06
Iter: 1614 loss: 8.29849068e-06
Iter: 1615 loss: 8.29847249e-06
Iter: 1616 loss: 8.29689e-06
Iter: 1617 loss: 8.29739383e-06
Iter: 1618 loss: 8.29566e-06
Iter: 1619 loss: 8.29327291e-06
Iter: 1620 loss: 8.28969132e-06
Iter: 1621 loss: 8.29017881e-06
Iter: 1622 loss: 8.28602242e-06
Iter: 1623 loss: 8.3220275e-06
Iter: 1624 loss: 8.28557859e-06
Iter: 1625 loss: 8.28277734e-06
Iter: 1626 loss: 8.28757584e-06
Iter: 1627 loss: 8.28111479e-06
Iter: 1628 loss: 8.27844269e-06
Iter: 1629 loss: 8.3035311e-06
Iter: 1630 loss: 8.27834e-06
Iter: 1631 loss: 8.27606709e-06
Iter: 1632 loss: 8.2742763e-06
Iter: 1633 loss: 8.27340409e-06
Iter: 1634 loss: 8.27089e-06
Iter: 1635 loss: 8.30245699e-06
Iter: 1636 loss: 8.27079e-06
Iter: 1637 loss: 8.26931864e-06
Iter: 1638 loss: 8.26941687e-06
Iter: 1639 loss: 8.2679e-06
Iter: 1640 loss: 8.26441465e-06
Iter: 1641 loss: 8.30835052e-06
Iter: 1642 loss: 8.26434734e-06
Iter: 1643 loss: 8.26175528e-06
Iter: 1644 loss: 8.26915948e-06
Iter: 1645 loss: 8.26037103e-06
Iter: 1646 loss: 8.25768802e-06
Iter: 1647 loss: 8.26425185e-06
Iter: 1648 loss: 8.25654e-06
Iter: 1649 loss: 8.25354e-06
Iter: 1650 loss: 8.28739303e-06
Iter: 1651 loss: 8.25374264e-06
Iter: 1652 loss: 8.25234e-06
Iter: 1653 loss: 8.25063216e-06
Iter: 1654 loss: 8.25041298e-06
Iter: 1655 loss: 8.24789e-06
Iter: 1656 loss: 8.25301322e-06
Iter: 1657 loss: 8.24684685e-06
Iter: 1658 loss: 8.24456856e-06
Iter: 1659 loss: 8.24958715e-06
Iter: 1660 loss: 8.24369454e-06
Iter: 1661 loss: 8.24063136e-06
Iter: 1662 loss: 8.24492145e-06
Iter: 1663 loss: 8.23927621e-06
Iter: 1664 loss: 8.2362094e-06
Iter: 1665 loss: 8.25935786e-06
Iter: 1666 loss: 8.23601749e-06
Iter: 1667 loss: 8.23343726e-06
Iter: 1668 loss: 8.23420851e-06
Iter: 1669 loss: 8.23168284e-06
Iter: 1670 loss: 8.23029495e-06
Iter: 1671 loss: 8.23010669e-06
Iter: 1672 loss: 8.22818765e-06
Iter: 1673 loss: 8.22855236e-06
Iter: 1674 loss: 8.2264869e-06
Iter: 1675 loss: 8.22446e-06
Iter: 1676 loss: 8.22696893e-06
Iter: 1677 loss: 8.22372294e-06
Iter: 1678 loss: 8.22176571e-06
Iter: 1679 loss: 8.22234e-06
Iter: 1680 loss: 8.22056245e-06
Iter: 1681 loss: 8.21738286e-06
Iter: 1682 loss: 8.23999835e-06
Iter: 1683 loss: 8.21737558e-06
Iter: 1684 loss: 8.21526555e-06
Iter: 1685 loss: 8.21557842e-06
Iter: 1686 loss: 8.21352842e-06
Iter: 1687 loss: 8.21132107e-06
Iter: 1688 loss: 8.2108927e-06
Iter: 1689 loss: 8.20865262e-06
Iter: 1690 loss: 8.20588866e-06
Iter: 1691 loss: 8.23578557e-06
Iter: 1692 loss: 8.20558671e-06
Iter: 1693 loss: 8.20368223e-06
Iter: 1694 loss: 8.20206424e-06
Iter: 1695 loss: 8.20135301e-06
Iter: 1696 loss: 8.19867364e-06
Iter: 1697 loss: 8.23809387e-06
Iter: 1698 loss: 8.1985072e-06
Iter: 1699 loss: 8.19693378e-06
Iter: 1700 loss: 8.19696288e-06
Iter: 1701 loss: 8.19544e-06
Iter: 1702 loss: 8.19264096e-06
Iter: 1703 loss: 8.19999877e-06
Iter: 1704 loss: 8.19154229e-06
Iter: 1705 loss: 8.19100933e-06
Iter: 1706 loss: 8.19025263e-06
Iter: 1707 loss: 8.18916578e-06
Iter: 1708 loss: 8.18718127e-06
Iter: 1709 loss: 8.20951573e-06
Iter: 1710 loss: 8.1871467e-06
Iter: 1711 loss: 8.1843009e-06
Iter: 1712 loss: 8.19059369e-06
Iter: 1713 loss: 8.183305e-06
Iter: 1714 loss: 8.18113585e-06
Iter: 1715 loss: 8.19280649e-06
Iter: 1716 loss: 8.18106855e-06
Iter: 1717 loss: 8.17776527e-06
Iter: 1718 loss: 8.17952514e-06
Iter: 1719 loss: 8.17577e-06
Iter: 1720 loss: 8.17376895e-06
Iter: 1721 loss: 8.18060835e-06
Iter: 1722 loss: 8.17317232e-06
Iter: 1723 loss: 8.17073487e-06
Iter: 1724 loss: 8.16744e-06
Iter: 1725 loss: 8.16748434e-06
Iter: 1726 loss: 8.16319334e-06
Iter: 1727 loss: 8.18988e-06
Iter: 1728 loss: 8.16262127e-06
Iter: 1729 loss: 8.15862222e-06
Iter: 1730 loss: 8.16613283e-06
Iter: 1731 loss: 8.15679505e-06
Iter: 1732 loss: 8.1541657e-06
Iter: 1733 loss: 8.17476757e-06
Iter: 1734 loss: 8.15331259e-06
Iter: 1735 loss: 8.15033945e-06
Iter: 1736 loss: 8.15171825e-06
Iter: 1737 loss: 8.14844407e-06
Iter: 1738 loss: 8.14772284e-06
Iter: 1739 loss: 8.14695341e-06
Iter: 1740 loss: 8.1454391e-06
Iter: 1741 loss: 8.14528175e-06
Iter: 1742 loss: 8.1442131e-06
Iter: 1743 loss: 8.14244777e-06
Iter: 1744 loss: 8.13858242e-06
Iter: 1745 loss: 8.19941761e-06
Iter: 1746 loss: 8.13849147e-06
Iter: 1747 loss: 8.13486531e-06
Iter: 1748 loss: 8.18384069e-06
Iter: 1749 loss: 8.13494171e-06
Iter: 1750 loss: 8.13242e-06
Iter: 1751 loss: 8.14235136e-06
Iter: 1752 loss: 8.13117731e-06
Iter: 1753 loss: 8.12818053e-06
Iter: 1754 loss: 8.12745657e-06
Iter: 1755 loss: 8.12550843e-06
Iter: 1756 loss: 8.12196595e-06
Iter: 1757 loss: 8.12630424e-06
Iter: 1758 loss: 8.12026883e-06
Iter: 1759 loss: 8.1152466e-06
Iter: 1760 loss: 8.12195594e-06
Iter: 1761 loss: 8.11270547e-06
Iter: 1762 loss: 8.10836445e-06
Iter: 1763 loss: 8.13222778e-06
Iter: 1764 loss: 8.10784877e-06
Iter: 1765 loss: 8.10354686e-06
Iter: 1766 loss: 8.10716119e-06
Iter: 1767 loss: 8.10068377e-06
Iter: 1768 loss: 8.09677294e-06
Iter: 1769 loss: 8.12960752e-06
Iter: 1770 loss: 8.09654284e-06
Iter: 1771 loss: 8.0928e-06
Iter: 1772 loss: 8.09839912e-06
Iter: 1773 loss: 8.09103403e-06
Iter: 1774 loss: 8.08741606e-06
Iter: 1775 loss: 8.0873524e-06
Iter: 1776 loss: 8.08599725e-06
Iter: 1777 loss: 8.08207733e-06
Iter: 1778 loss: 8.11262908e-06
Iter: 1779 loss: 8.08128152e-06
Iter: 1780 loss: 8.07630749e-06
Iter: 1781 loss: 8.10759138e-06
Iter: 1782 loss: 8.07602373e-06
Iter: 1783 loss: 8.07389551e-06
Iter: 1784 loss: 8.0978989e-06
Iter: 1785 loss: 8.0739137e-06
Iter: 1786 loss: 8.07153083e-06
Iter: 1787 loss: 8.06802927e-06
Iter: 1788 loss: 8.06771914e-06
Iter: 1789 loss: 8.06304161e-06
Iter: 1790 loss: 8.0808768e-06
Iter: 1791 loss: 8.06189928e-06
Iter: 1792 loss: 8.05844866e-06
Iter: 1793 loss: 8.05754553e-06
Iter: 1794 loss: 8.05508898e-06
Iter: 1795 loss: 8.05034324e-06
Iter: 1796 loss: 8.08605728e-06
Iter: 1797 loss: 8.05014406e-06
Iter: 1798 loss: 8.04563479e-06
Iter: 1799 loss: 8.04503725e-06
Iter: 1800 loss: 8.0422069e-06
Iter: 1801 loss: 8.03699e-06
Iter: 1802 loss: 8.08858e-06
Iter: 1803 loss: 8.03689181e-06
Iter: 1804 loss: 8.03311377e-06
Iter: 1805 loss: 8.04501633e-06
Iter: 1806 loss: 8.03211697e-06
Iter: 1807 loss: 8.02924842e-06
Iter: 1808 loss: 8.06852222e-06
Iter: 1809 loss: 8.02917475e-06
Iter: 1810 loss: 8.02696286e-06
Iter: 1811 loss: 8.02527e-06
Iter: 1812 loss: 8.02412615e-06
Iter: 1813 loss: 8.0218e-06
Iter: 1814 loss: 8.02079649e-06
Iter: 1815 loss: 8.01923306e-06
Iter: 1816 loss: 8.01555325e-06
Iter: 1817 loss: 8.03760122e-06
Iter: 1818 loss: 8.01483839e-06
Iter: 1819 loss: 8.01129863e-06
Iter: 1820 loss: 8.03022249e-06
Iter: 1821 loss: 8.01108763e-06
Iter: 1822 loss: 8.00728503e-06
Iter: 1823 loss: 8.00432281e-06
Iter: 1824 loss: 8.00343696e-06
Iter: 1825 loss: 8.00018097e-06
Iter: 1826 loss: 8.02073191e-06
Iter: 1827 loss: 7.99992631e-06
Iter: 1828 loss: 7.99607187e-06
Iter: 1829 loss: 7.99043482e-06
Iter: 1830 loss: 7.99037116e-06
Iter: 1831 loss: 7.9847523e-06
Iter: 1832 loss: 8.03723742e-06
Iter: 1833 loss: 7.98438577e-06
Iter: 1834 loss: 7.97920075e-06
Iter: 1835 loss: 7.99116151e-06
Iter: 1836 loss: 7.97716893e-06
Iter: 1837 loss: 7.97374e-06
Iter: 1838 loss: 7.99503141e-06
Iter: 1839 loss: 7.97319808e-06
Iter: 1840 loss: 7.97006396e-06
Iter: 1841 loss: 8.00755e-06
Iter: 1842 loss: 7.96984e-06
Iter: 1843 loss: 7.96700897e-06
Iter: 1844 loss: 7.96869608e-06
Iter: 1845 loss: 7.9649e-06
Iter: 1846 loss: 7.96260338e-06
Iter: 1847 loss: 7.96001586e-06
Iter: 1848 loss: 7.95958567e-06
Iter: 1849 loss: 7.95437336e-06
Iter: 1850 loss: 7.96183485e-06
Iter: 1851 loss: 7.95218057e-06
Iter: 1852 loss: 7.94802691e-06
Iter: 1853 loss: 7.94786865e-06
Iter: 1854 loss: 7.94507469e-06
Iter: 1855 loss: 7.94462e-06
Iter: 1856 loss: 7.94270727e-06
Iter: 1857 loss: 7.93901381e-06
Iter: 1858 loss: 7.94139669e-06
Iter: 1859 loss: 7.93656e-06
Iter: 1860 loss: 7.93208619e-06
Iter: 1861 loss: 7.94280822e-06
Iter: 1862 loss: 7.93035906e-06
Iter: 1863 loss: 7.92582341e-06
Iter: 1864 loss: 7.94589869e-06
Iter: 1865 loss: 7.92477567e-06
Iter: 1866 loss: 7.92158517e-06
Iter: 1867 loss: 7.92453284e-06
Iter: 1868 loss: 7.91983e-06
Iter: 1869 loss: 7.91496586e-06
Iter: 1870 loss: 7.92763876e-06
Iter: 1871 loss: 7.91298135e-06
Iter: 1872 loss: 7.91084312e-06
Iter: 1873 loss: 7.91039292e-06
Iter: 1874 loss: 7.90761078e-06
Iter: 1875 loss: 7.91092225e-06
Iter: 1876 loss: 7.90639569e-06
Iter: 1877 loss: 7.90329068e-06
Iter: 1878 loss: 7.90402555e-06
Iter: 1879 loss: 7.90123886e-06
Iter: 1880 loss: 7.89823116e-06
Iter: 1881 loss: 7.89833393e-06
Iter: 1882 loss: 7.89560454e-06
Iter: 1883 loss: 7.89171099e-06
Iter: 1884 loss: 7.92829087e-06
Iter: 1885 loss: 7.89152455e-06
Iter: 1886 loss: 7.88896705e-06
Iter: 1887 loss: 7.90513695e-06
Iter: 1888 loss: 7.88870238e-06
Iter: 1889 loss: 7.88639227e-06
Iter: 1890 loss: 7.88097e-06
Iter: 1891 loss: 7.92588799e-06
Iter: 1892 loss: 7.88002581e-06
Iter: 1893 loss: 7.87429235e-06
Iter: 1894 loss: 7.95620326e-06
Iter: 1895 loss: 7.87420322e-06
Iter: 1896 loss: 7.87036697e-06
Iter: 1897 loss: 7.8722569e-06
Iter: 1898 loss: 7.86709461e-06
Iter: 1899 loss: 7.86265e-06
Iter: 1900 loss: 7.8855046e-06
Iter: 1901 loss: 7.86197779e-06
Iter: 1902 loss: 7.85764314e-06
Iter: 1903 loss: 7.86604051e-06
Iter: 1904 loss: 7.85584325e-06
Iter: 1905 loss: 7.85285556e-06
Iter: 1906 loss: 7.89754449e-06
Iter: 1907 loss: 7.85239081e-06
Iter: 1908 loss: 7.85046e-06
Iter: 1909 loss: 7.87962e-06
Iter: 1910 loss: 7.85048724e-06
Iter: 1911 loss: 7.84888744e-06
Iter: 1912 loss: 7.84564872e-06
Iter: 1913 loss: 7.88171565e-06
Iter: 1914 loss: 7.84497843e-06
Iter: 1915 loss: 7.84039457e-06
Iter: 1916 loss: 7.85857083e-06
Iter: 1917 loss: 7.8392768e-06
Iter: 1918 loss: 7.83563701e-06
Iter: 1919 loss: 7.84211807e-06
Iter: 1920 loss: 7.8342855e-06
Iter: 1921 loss: 7.82973075e-06
Iter: 1922 loss: 7.85709562e-06
Iter: 1923 loss: 7.82925417e-06
Iter: 1924 loss: 7.8261246e-06
Iter: 1925 loss: 7.82765e-06
Iter: 1926 loss: 7.82419829e-06
Iter: 1927 loss: 7.8199846e-06
Iter: 1928 loss: 7.81816834e-06
Iter: 1929 loss: 7.81601193e-06
Iter: 1930 loss: 7.81247e-06
Iter: 1931 loss: 7.85622615e-06
Iter: 1932 loss: 7.81241761e-06
Iter: 1933 loss: 7.80877144e-06
Iter: 1934 loss: 7.80502069e-06
Iter: 1935 loss: 7.80408482e-06
Iter: 1936 loss: 7.80017945e-06
Iter: 1937 loss: 7.85147586e-06
Iter: 1938 loss: 7.80018217e-06
Iter: 1939 loss: 7.79699621e-06
Iter: 1940 loss: 7.80407936e-06
Iter: 1941 loss: 7.79521088e-06
Iter: 1942 loss: 7.79381298e-06
Iter: 1943 loss: 7.79320544e-06
Iter: 1944 loss: 7.79148104e-06
Iter: 1945 loss: 7.78858612e-06
Iter: 1946 loss: 7.84705117e-06
Iter: 1947 loss: 7.78858885e-06
Iter: 1948 loss: 7.78449703e-06
Iter: 1949 loss: 7.79149832e-06
Iter: 1950 loss: 7.78213689e-06
Iter: 1951 loss: 7.77952937e-06
Iter: 1952 loss: 7.7921386e-06
Iter: 1953 loss: 7.77903642e-06
Iter: 1954 loss: 7.77594869e-06
Iter: 1955 loss: 7.78706271e-06
Iter: 1956 loss: 7.77505375e-06
Iter: 1957 loss: 7.77145215e-06
Iter: 1958 loss: 7.77302193e-06
Iter: 1959 loss: 7.76896377e-06
Iter: 1960 loss: 7.76524939e-06
Iter: 1961 loss: 7.77049354e-06
Iter: 1962 loss: 7.76324941e-06
Iter: 1963 loss: 7.75931221e-06
Iter: 1964 loss: 7.76796514e-06
Iter: 1965 loss: 7.75715489e-06
Iter: 1966 loss: 7.75334411e-06
Iter: 1967 loss: 7.76180423e-06
Iter: 1968 loss: 7.75181434e-06
Iter: 1969 loss: 7.74694672e-06
Iter: 1970 loss: 7.76578781e-06
Iter: 1971 loss: 7.74605815e-06
Iter: 1972 loss: 7.74294949e-06
Iter: 1973 loss: 7.74567343e-06
Iter: 1974 loss: 7.74063301e-06
Iter: 1975 loss: 7.7375862e-06
Iter: 1976 loss: 7.7373852e-06
Iter: 1977 loss: 7.7343675e-06
Iter: 1978 loss: 7.74512e-06
Iter: 1979 loss: 7.73362808e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi3/300_100_100_100_1
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi0
+ date
Sun Nov  8 03:09:30 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_100_100_100_1 --function f1 --psi -1 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d26e0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d270b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d26e0bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7f0ba9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d2658400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d2658840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac6a6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac6a6268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d2658d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d268aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac5f5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac6a6048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac674950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac5b0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac548620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac548f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac548e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac4c4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac4c4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac4fcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac60da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac41e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac60dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac51d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac51d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac514e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac39d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac39dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac489378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac3dba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac2e7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac2e7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac3631e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac330d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac2158c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7ac218f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0030544165
test_loss: 0.003165209
train_loss: 0.002520237
test_loss: 0.00240477
train_loss: 0.0025898514
test_loss: 0.002519468
train_loss: 0.0024432484
test_loss: 0.0024143106
train_loss: 0.0023588827
test_loss: 0.0022722334
train_loss: 0.0025537992
test_loss: 0.0030543257
train_loss: 0.0020555523
test_loss: 0.002112058
train_loss: 0.0024854213
test_loss: 0.002168796
train_loss: 0.0022132946
test_loss: 0.002331326
train_loss: 0.0020836229
test_loss: 0.0020520466
train_loss: 0.0022670086
test_loss: 0.002288053
train_loss: 0.0020624842
test_loss: 0.0021817659
train_loss: 0.0022508574
test_loss: 0.0022200542
train_loss: 0.002184602
test_loss: 0.0021674603
train_loss: 0.0022007741
test_loss: 0.002158721
train_loss: 0.002065719
test_loss: 0.0021088973
train_loss: 0.0021557105
test_loss: 0.0019419971
train_loss: 0.0020012548
test_loss: 0.0021660922
train_loss: 0.0021088808
test_loss: 0.0020850468
train_loss: 0.0019338168
test_loss: 0.0021083783
train_loss: 0.0019918296
test_loss: 0.0020520028
train_loss: 0.0018944648
test_loss: 0.0019797005
train_loss: 0.0019594464
test_loss: 0.002057072
train_loss: 0.0018687341
test_loss: 0.0019296297
train_loss: 0.002059037
test_loss: 0.0021963094
train_loss: 0.0020150437
test_loss: 0.0020797367
train_loss: 0.0018569375
test_loss: 0.0019023663
train_loss: 0.002103294
test_loss: 0.0022086978
train_loss: 0.0022649134
test_loss: 0.0021344374
train_loss: 0.0019993414
test_loss: 0.0020967051
train_loss: 0.0020004997
test_loss: 0.0020062241
train_loss: 0.0017944459
test_loss: 0.001923229
train_loss: 0.0019222553
test_loss: 0.0020744572
train_loss: 0.0020537376
test_loss: 0.0020410789
train_loss: 0.002075662
test_loss: 0.0021845386
train_loss: 0.0019818835
test_loss: 0.0019476129
train_loss: 0.0020061338
test_loss: 0.0019510026
train_loss: 0.0019595956
test_loss: 0.0022069456
train_loss: 0.002089207
test_loss: 0.0019854172
train_loss: 0.0020106235
test_loss: 0.002029101
train_loss: 0.0018549446
test_loss: 0.001991818
train_loss: 0.001955234
test_loss: 0.0019155411
train_loss: 0.0020134524
test_loss: 0.0020130854
train_loss: 0.0018334982
test_loss: 0.0018662256
train_loss: 0.0018537963
test_loss: 0.0020880618
train_loss: 0.0019369845
test_loss: 0.001952261
train_loss: 0.0019013565
test_loss: 0.0021144398
train_loss: 0.0018808049
test_loss: 0.001974742
train_loss: 0.0019460567
test_loss: 0.0019077405
train_loss: 0.0018510382
test_loss: 0.002094228
train_loss: 0.0019856226
test_loss: 0.0017709655
train_loss: 0.0018633729
test_loss: 0.0018853504
train_loss: 0.0017768668
test_loss: 0.0018251722
train_loss: 0.0025152771
test_loss: 0.00207418
train_loss: 0.0017690833
test_loss: 0.0019731808
train_loss: 0.00206115
test_loss: 0.0020612613
train_loss: 0.0021001499
test_loss: 0.0020173094
train_loss: 0.0020423052
test_loss: 0.0018667
train_loss: 0.0018674816
test_loss: 0.001873067
train_loss: 0.0018591131
test_loss: 0.0019392086
train_loss: 0.0021271138
test_loss: 0.002282219
train_loss: 0.0016898371
test_loss: 0.0017738888
train_loss: 0.0018780297
test_loss: 0.0018095885
train_loss: 0.0020046616
test_loss: 0.0019081653
train_loss: 0.0020843556
test_loss: 0.0019235405
train_loss: 0.0018764953
test_loss: 0.0021132363
train_loss: 0.0017222875
test_loss: 0.0018751002
train_loss: 0.0019913362
test_loss: 0.0019516076
train_loss: 0.0017511077
test_loss: 0.002003544
train_loss: 0.0019329664
test_loss: 0.0022949593
train_loss: 0.0018749811
test_loss: 0.0019773284
train_loss: 0.001827768
test_loss: 0.0021458534
train_loss: 0.0018729778
test_loss: 0.0020869912
train_loss: 0.001891512
test_loss: 0.0019029203
train_loss: 0.0019454275
test_loss: 0.0019389439
train_loss: 0.0018934638
test_loss: 0.001845607
train_loss: 0.0017764458
test_loss: 0.0019473613
train_loss: 0.0019226025
test_loss: 0.0019339633
train_loss: 0.0019142278
test_loss: 0.0019464714
train_loss: 0.0017543624
test_loss: 0.0018907778
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi0/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -1 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi0/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa148078d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa148149c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa148149488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1480e02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1480d32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1480d3c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1480e69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1480d3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1480e6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa14804c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa13fc8e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa13fcd1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1480e6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa13fcb6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa13fc04a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa13fc047b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa13fbdb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa13fbde6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa13fb82598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa13fbded08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa13fbdb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0e76bb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0e7684400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0e76829d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0e7682ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0e7631378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0e75efb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0c06868c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0c06a8048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0c06a8b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0c06b40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0c06149d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0c0614b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0c05dea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0c05a2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0c05ae8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.41274494e-06
Iter: 2 loss: 8.50447395e-06
Iter: 3 loss: 4.46008107e-06
Iter: 4 loss: 4.12202553e-06
Iter: 5 loss: 5.14206158e-06
Iter: 6 loss: 4.02025262e-06
Iter: 7 loss: 3.86441297e-06
Iter: 8 loss: 3.72152681e-06
Iter: 9 loss: 3.68353813e-06
Iter: 10 loss: 3.46818456e-06
Iter: 11 loss: 3.46562661e-06
Iter: 12 loss: 3.3841618e-06
Iter: 13 loss: 3.4319894e-06
Iter: 14 loss: 3.33135495e-06
Iter: 15 loss: 3.3107558e-06
Iter: 16 loss: 3.30308399e-06
Iter: 17 loss: 3.28486749e-06
Iter: 18 loss: 3.22803407e-06
Iter: 19 loss: 3.32989657e-06
Iter: 20 loss: 3.19052924e-06
Iter: 21 loss: 3.13970827e-06
Iter: 22 loss: 3.75107584e-06
Iter: 23 loss: 3.13906276e-06
Iter: 24 loss: 3.0952624e-06
Iter: 25 loss: 3.4339057e-06
Iter: 26 loss: 3.09198731e-06
Iter: 27 loss: 3.05919116e-06
Iter: 28 loss: 2.96408939e-06
Iter: 29 loss: 3.36358312e-06
Iter: 30 loss: 2.92674622e-06
Iter: 31 loss: 2.840658e-06
Iter: 32 loss: 2.94166944e-06
Iter: 33 loss: 2.794887e-06
Iter: 34 loss: 2.72786201e-06
Iter: 35 loss: 2.76837682e-06
Iter: 36 loss: 2.6848079e-06
Iter: 37 loss: 2.62508502e-06
Iter: 38 loss: 2.81054872e-06
Iter: 39 loss: 2.60759043e-06
Iter: 40 loss: 2.56113435e-06
Iter: 41 loss: 2.5657987e-06
Iter: 42 loss: 2.52537961e-06
Iter: 43 loss: 2.52144628e-06
Iter: 44 loss: 2.5039958e-06
Iter: 45 loss: 2.47857247e-06
Iter: 46 loss: 2.44439843e-06
Iter: 47 loss: 2.44258217e-06
Iter: 48 loss: 2.4083713e-06
Iter: 49 loss: 2.63967581e-06
Iter: 50 loss: 2.40504141e-06
Iter: 51 loss: 2.37576728e-06
Iter: 52 loss: 2.53148528e-06
Iter: 53 loss: 2.37120253e-06
Iter: 54 loss: 2.34428762e-06
Iter: 55 loss: 2.28367276e-06
Iter: 56 loss: 3.11147073e-06
Iter: 57 loss: 2.28022873e-06
Iter: 58 loss: 2.21787423e-06
Iter: 59 loss: 2.73633646e-06
Iter: 60 loss: 2.21414575e-06
Iter: 61 loss: 2.18760442e-06
Iter: 62 loss: 2.55227337e-06
Iter: 63 loss: 2.18757486e-06
Iter: 64 loss: 2.16416174e-06
Iter: 65 loss: 2.19817889e-06
Iter: 66 loss: 2.15287082e-06
Iter: 67 loss: 2.12540499e-06
Iter: 68 loss: 2.19401772e-06
Iter: 69 loss: 2.11577094e-06
Iter: 70 loss: 2.0881464e-06
Iter: 71 loss: 2.17932484e-06
Iter: 72 loss: 2.08060624e-06
Iter: 73 loss: 2.06282834e-06
Iter: 74 loss: 2.01998819e-06
Iter: 75 loss: 2.4854296e-06
Iter: 76 loss: 2.01548687e-06
Iter: 77 loss: 1.96473366e-06
Iter: 78 loss: 2.35300854e-06
Iter: 79 loss: 1.96096198e-06
Iter: 80 loss: 1.94349195e-06
Iter: 81 loss: 1.94059612e-06
Iter: 82 loss: 1.91779691e-06
Iter: 83 loss: 1.88113e-06
Iter: 84 loss: 1.88089064e-06
Iter: 85 loss: 1.86831323e-06
Iter: 86 loss: 1.86673037e-06
Iter: 87 loss: 1.85535271e-06
Iter: 88 loss: 1.86377611e-06
Iter: 89 loss: 1.84844566e-06
Iter: 90 loss: 1.83466398e-06
Iter: 91 loss: 1.8181322e-06
Iter: 92 loss: 1.81647658e-06
Iter: 93 loss: 1.80182008e-06
Iter: 94 loss: 2.00650948e-06
Iter: 95 loss: 1.8017065e-06
Iter: 96 loss: 1.79256699e-06
Iter: 97 loss: 1.86619036e-06
Iter: 98 loss: 1.79199662e-06
Iter: 99 loss: 1.78267032e-06
Iter: 100 loss: 1.77683182e-06
Iter: 101 loss: 1.77309721e-06
Iter: 102 loss: 1.7555451e-06
Iter: 103 loss: 1.81089285e-06
Iter: 104 loss: 1.75051105e-06
Iter: 105 loss: 1.73375201e-06
Iter: 106 loss: 1.73340675e-06
Iter: 107 loss: 1.7203306e-06
Iter: 108 loss: 1.70031717e-06
Iter: 109 loss: 1.66793757e-06
Iter: 110 loss: 1.6677144e-06
Iter: 111 loss: 1.64934886e-06
Iter: 112 loss: 1.64715925e-06
Iter: 113 loss: 1.63410778e-06
Iter: 114 loss: 1.63405571e-06
Iter: 115 loss: 1.62514971e-06
Iter: 116 loss: 1.60537491e-06
Iter: 117 loss: 1.89138848e-06
Iter: 118 loss: 1.60439e-06
Iter: 119 loss: 1.60480033e-06
Iter: 120 loss: 1.5964647e-06
Iter: 121 loss: 1.5920964e-06
Iter: 122 loss: 1.58350463e-06
Iter: 123 loss: 1.75879859e-06
Iter: 124 loss: 1.58353089e-06
Iter: 125 loss: 1.57079444e-06
Iter: 126 loss: 1.56716521e-06
Iter: 127 loss: 1.55947919e-06
Iter: 128 loss: 1.55357588e-06
Iter: 129 loss: 1.55149235e-06
Iter: 130 loss: 1.54651411e-06
Iter: 131 loss: 1.56649276e-06
Iter: 132 loss: 1.54532177e-06
Iter: 133 loss: 1.54097415e-06
Iter: 134 loss: 1.54503471e-06
Iter: 135 loss: 1.53844576e-06
Iter: 136 loss: 1.53113842e-06
Iter: 137 loss: 1.52567247e-06
Iter: 138 loss: 1.5232348e-06
Iter: 139 loss: 1.51413167e-06
Iter: 140 loss: 1.54020813e-06
Iter: 141 loss: 1.51131314e-06
Iter: 142 loss: 1.50359142e-06
Iter: 143 loss: 1.49006041e-06
Iter: 144 loss: 1.49006075e-06
Iter: 145 loss: 1.4952991e-06
Iter: 146 loss: 1.4837085e-06
Iter: 147 loss: 1.47756691e-06
Iter: 148 loss: 1.47061473e-06
Iter: 149 loss: 1.46973571e-06
Iter: 150 loss: 1.46046364e-06
Iter: 151 loss: 1.4801933e-06
Iter: 152 loss: 1.45689489e-06
Iter: 153 loss: 1.44453634e-06
Iter: 154 loss: 1.5140032e-06
Iter: 155 loss: 1.44288356e-06
Iter: 156 loss: 1.43854982e-06
Iter: 157 loss: 1.43973966e-06
Iter: 158 loss: 1.43537204e-06
Iter: 159 loss: 1.42966815e-06
Iter: 160 loss: 1.44077978e-06
Iter: 161 loss: 1.42726299e-06
Iter: 162 loss: 1.42217618e-06
Iter: 163 loss: 1.49974096e-06
Iter: 164 loss: 1.42215822e-06
Iter: 165 loss: 1.41891064e-06
Iter: 166 loss: 1.42292754e-06
Iter: 167 loss: 1.4171959e-06
Iter: 168 loss: 1.41308374e-06
Iter: 169 loss: 1.42352872e-06
Iter: 170 loss: 1.41172518e-06
Iter: 171 loss: 1.40776046e-06
Iter: 172 loss: 1.40515033e-06
Iter: 173 loss: 1.40369684e-06
Iter: 174 loss: 1.39834572e-06
Iter: 175 loss: 1.39735812e-06
Iter: 176 loss: 1.39375857e-06
Iter: 177 loss: 1.38515611e-06
Iter: 178 loss: 1.42124236e-06
Iter: 179 loss: 1.38336941e-06
Iter: 180 loss: 1.38339851e-06
Iter: 181 loss: 1.3800925e-06
Iter: 182 loss: 1.37820098e-06
Iter: 183 loss: 1.37245934e-06
Iter: 184 loss: 1.38704286e-06
Iter: 185 loss: 1.36928122e-06
Iter: 186 loss: 1.3663946e-06
Iter: 187 loss: 1.36450149e-06
Iter: 188 loss: 1.36138294e-06
Iter: 189 loss: 1.35783046e-06
Iter: 190 loss: 1.35742368e-06
Iter: 191 loss: 1.35207165e-06
Iter: 192 loss: 1.34723348e-06
Iter: 193 loss: 1.34593256e-06
Iter: 194 loss: 1.33631181e-06
Iter: 195 loss: 1.44473529e-06
Iter: 196 loss: 1.3362087e-06
Iter: 197 loss: 1.33013214e-06
Iter: 198 loss: 1.37691086e-06
Iter: 199 loss: 1.32968898e-06
Iter: 200 loss: 1.32669811e-06
Iter: 201 loss: 1.33621666e-06
Iter: 202 loss: 1.32580203e-06
Iter: 203 loss: 1.32219611e-06
Iter: 204 loss: 1.31713921e-06
Iter: 205 loss: 1.31695219e-06
Iter: 206 loss: 1.31173942e-06
Iter: 207 loss: 1.33510923e-06
Iter: 208 loss: 1.31077036e-06
Iter: 209 loss: 1.30768649e-06
Iter: 210 loss: 1.3063692e-06
Iter: 211 loss: 1.30475655e-06
Iter: 212 loss: 1.30389731e-06
Iter: 213 loss: 1.30264891e-06
Iter: 214 loss: 1.30050364e-06
Iter: 215 loss: 1.29895909e-06
Iter: 216 loss: 1.29820478e-06
Iter: 217 loss: 1.29512648e-06
Iter: 218 loss: 1.29323212e-06
Iter: 219 loss: 1.29202249e-06
Iter: 220 loss: 1.2845428e-06
Iter: 221 loss: 1.30755791e-06
Iter: 222 loss: 1.28233432e-06
Iter: 223 loss: 1.27846783e-06
Iter: 224 loss: 1.27692238e-06
Iter: 225 loss: 1.27482633e-06
Iter: 226 loss: 1.26905206e-06
Iter: 227 loss: 1.29905425e-06
Iter: 228 loss: 1.26821101e-06
Iter: 229 loss: 1.26370719e-06
Iter: 230 loss: 1.31134743e-06
Iter: 231 loss: 1.26355712e-06
Iter: 232 loss: 1.26114344e-06
Iter: 233 loss: 1.27088663e-06
Iter: 234 loss: 1.26068039e-06
Iter: 235 loss: 1.25873885e-06
Iter: 236 loss: 1.26366353e-06
Iter: 237 loss: 1.25801091e-06
Iter: 238 loss: 1.25631936e-06
Iter: 239 loss: 1.25492625e-06
Iter: 240 loss: 1.25444205e-06
Iter: 241 loss: 1.25261226e-06
Iter: 242 loss: 1.25495399e-06
Iter: 243 loss: 1.25162069e-06
Iter: 244 loss: 1.24873668e-06
Iter: 245 loss: 1.24936128e-06
Iter: 246 loss: 1.24655799e-06
Iter: 247 loss: 1.24450332e-06
Iter: 248 loss: 1.24388782e-06
Iter: 249 loss: 1.24228904e-06
Iter: 250 loss: 1.23765881e-06
Iter: 251 loss: 1.25649763e-06
Iter: 252 loss: 1.23576615e-06
Iter: 253 loss: 1.23391146e-06
Iter: 254 loss: 1.23229847e-06
Iter: 255 loss: 1.23004372e-06
Iter: 256 loss: 1.22603342e-06
Iter: 257 loss: 1.2260823e-06
Iter: 258 loss: 1.22152528e-06
Iter: 259 loss: 1.22742449e-06
Iter: 260 loss: 1.21928292e-06
Iter: 261 loss: 1.21670291e-06
Iter: 262 loss: 1.21613482e-06
Iter: 263 loss: 1.21463e-06
Iter: 264 loss: 1.2204664e-06
Iter: 265 loss: 1.21429889e-06
Iter: 266 loss: 1.21305038e-06
Iter: 267 loss: 1.21511698e-06
Iter: 268 loss: 1.21246239e-06
Iter: 269 loss: 1.210655e-06
Iter: 270 loss: 1.20806681e-06
Iter: 271 loss: 1.20800257e-06
Iter: 272 loss: 1.20522691e-06
Iter: 273 loss: 1.21059452e-06
Iter: 274 loss: 1.204103e-06
Iter: 275 loss: 1.2005878e-06
Iter: 276 loss: 1.19837773e-06
Iter: 277 loss: 1.19710296e-06
Iter: 278 loss: 1.20115283e-06
Iter: 279 loss: 1.19577419e-06
Iter: 280 loss: 1.1946513e-06
Iter: 281 loss: 1.19309095e-06
Iter: 282 loss: 1.19301183e-06
Iter: 283 loss: 1.19108972e-06
Iter: 284 loss: 1.19315223e-06
Iter: 285 loss: 1.18999083e-06
Iter: 286 loss: 1.18716366e-06
Iter: 287 loss: 1.20966217e-06
Iter: 288 loss: 1.18697699e-06
Iter: 289 loss: 1.18592629e-06
Iter: 290 loss: 1.18347384e-06
Iter: 291 loss: 1.21379185e-06
Iter: 292 loss: 1.18326091e-06
Iter: 293 loss: 1.18114212e-06
Iter: 294 loss: 1.18113155e-06
Iter: 295 loss: 1.17926243e-06
Iter: 296 loss: 1.18429375e-06
Iter: 297 loss: 1.17864829e-06
Iter: 298 loss: 1.17668469e-06
Iter: 299 loss: 1.17959235e-06
Iter: 300 loss: 1.17576e-06
Iter: 301 loss: 1.17320769e-06
Iter: 302 loss: 1.18034086e-06
Iter: 303 loss: 1.17239028e-06
Iter: 304 loss: 1.17080333e-06
Iter: 305 loss: 1.17128798e-06
Iter: 306 loss: 1.16960246e-06
Iter: 307 loss: 1.16767399e-06
Iter: 308 loss: 1.16526326e-06
Iter: 309 loss: 1.1649945e-06
Iter: 310 loss: 1.16365982e-06
Iter: 311 loss: 1.16342869e-06
Iter: 312 loss: 1.16214574e-06
Iter: 313 loss: 1.17481068e-06
Iter: 314 loss: 1.16207934e-06
Iter: 315 loss: 1.16110482e-06
Iter: 316 loss: 1.15896603e-06
Iter: 317 loss: 1.1889972e-06
Iter: 318 loss: 1.1588985e-06
Iter: 319 loss: 1.15874502e-06
Iter: 320 loss: 1.1580106e-06
Iter: 321 loss: 1.1573029e-06
Iter: 322 loss: 1.15524756e-06
Iter: 323 loss: 1.16396529e-06
Iter: 324 loss: 1.15449052e-06
Iter: 325 loss: 1.15186072e-06
Iter: 326 loss: 1.16356205e-06
Iter: 327 loss: 1.15136254e-06
Iter: 328 loss: 1.15074306e-06
Iter: 329 loss: 1.1503987e-06
Iter: 330 loss: 1.1494227e-06
Iter: 331 loss: 1.14716795e-06
Iter: 332 loss: 1.17289073e-06
Iter: 333 loss: 1.1469408e-06
Iter: 334 loss: 1.144311e-06
Iter: 335 loss: 1.18123489e-06
Iter: 336 loss: 1.14432578e-06
Iter: 337 loss: 1.14274155e-06
Iter: 338 loss: 1.14249156e-06
Iter: 339 loss: 1.14150316e-06
Iter: 340 loss: 1.13916508e-06
Iter: 341 loss: 1.13582564e-06
Iter: 342 loss: 1.13572401e-06
Iter: 343 loss: 1.13278179e-06
Iter: 344 loss: 1.15813941e-06
Iter: 345 loss: 1.13258523e-06
Iter: 346 loss: 1.13076658e-06
Iter: 347 loss: 1.15232717e-06
Iter: 348 loss: 1.13071496e-06
Iter: 349 loss: 1.12887801e-06
Iter: 350 loss: 1.13570491e-06
Iter: 351 loss: 1.1283928e-06
Iter: 352 loss: 1.12771272e-06
Iter: 353 loss: 1.12745954e-06
Iter: 354 loss: 1.12707858e-06
Iter: 355 loss: 1.12580426e-06
Iter: 356 loss: 1.13538738e-06
Iter: 357 loss: 1.12570876e-06
Iter: 358 loss: 1.12514931e-06
Iter: 359 loss: 1.12421037e-06
Iter: 360 loss: 1.12416944e-06
Iter: 361 loss: 1.12296573e-06
Iter: 362 loss: 1.1238252e-06
Iter: 363 loss: 1.12231123e-06
Iter: 364 loss: 1.12058956e-06
Iter: 365 loss: 1.14088118e-06
Iter: 366 loss: 1.12053885e-06
Iter: 367 loss: 1.11982558e-06
Iter: 368 loss: 1.12014652e-06
Iter: 369 loss: 1.11932388e-06
Iter: 370 loss: 1.11792383e-06
Iter: 371 loss: 1.11827478e-06
Iter: 372 loss: 1.11688666e-06
Iter: 373 loss: 1.11505869e-06
Iter: 374 loss: 1.11661564e-06
Iter: 375 loss: 1.11394593e-06
Iter: 376 loss: 1.1124e-06
Iter: 377 loss: 1.11356371e-06
Iter: 378 loss: 1.11140798e-06
Iter: 379 loss: 1.10954306e-06
Iter: 380 loss: 1.11826967e-06
Iter: 381 loss: 1.10924941e-06
Iter: 382 loss: 1.10889437e-06
Iter: 383 loss: 1.10848828e-06
Iter: 384 loss: 1.10800988e-06
Iter: 385 loss: 1.10656424e-06
Iter: 386 loss: 1.1125635e-06
Iter: 387 loss: 1.1060306e-06
Iter: 388 loss: 1.10504061e-06
Iter: 389 loss: 1.10486371e-06
Iter: 390 loss: 1.1039092e-06
Iter: 391 loss: 1.10376641e-06
Iter: 392 loss: 1.10315227e-06
Iter: 393 loss: 1.1022978e-06
Iter: 394 loss: 1.10069323e-06
Iter: 395 loss: 1.13844487e-06
Iter: 396 loss: 1.10067356e-06
Iter: 397 loss: 1.10092799e-06
Iter: 398 loss: 1.09989764e-06
Iter: 399 loss: 1.09931943e-06
Iter: 400 loss: 1.09869143e-06
Iter: 401 loss: 1.0986123e-06
Iter: 402 loss: 1.09790062e-06
Iter: 403 loss: 1.10730684e-06
Iter: 404 loss: 1.09788471e-06
Iter: 405 loss: 1.0972301e-06
Iter: 406 loss: 1.09700886e-06
Iter: 407 loss: 1.09664063e-06
Iter: 408 loss: 1.09580628e-06
Iter: 409 loss: 1.0948429e-06
Iter: 410 loss: 1.09475013e-06
Iter: 411 loss: 1.09324992e-06
Iter: 412 loss: 1.09086727e-06
Iter: 413 loss: 1.09081225e-06
Iter: 414 loss: 1.08917015e-06
Iter: 415 loss: 1.10804933e-06
Iter: 416 loss: 1.08910297e-06
Iter: 417 loss: 1.0871729e-06
Iter: 418 loss: 1.09596976e-06
Iter: 419 loss: 1.0867584e-06
Iter: 420 loss: 1.08603922e-06
Iter: 421 loss: 1.08597976e-06
Iter: 422 loss: 1.08542918e-06
Iter: 423 loss: 1.08484971e-06
Iter: 424 loss: 1.08489735e-06
Iter: 425 loss: 1.08426298e-06
Iter: 426 loss: 1.08335269e-06
Iter: 427 loss: 1.0833902e-06
Iter: 428 loss: 1.08270137e-06
Iter: 429 loss: 1.08208701e-06
Iter: 430 loss: 1.0819349e-06
Iter: 431 loss: 1.08211e-06
Iter: 432 loss: 1.08146014e-06
Iter: 433 loss: 1.08102586e-06
Iter: 434 loss: 1.0800569e-06
Iter: 435 loss: 1.09098619e-06
Iter: 436 loss: 1.07994993e-06
Iter: 437 loss: 1.07879941e-06
Iter: 438 loss: 1.08846814e-06
Iter: 439 loss: 1.07881647e-06
Iter: 440 loss: 1.07750213e-06
Iter: 441 loss: 1.07809785e-06
Iter: 442 loss: 1.07658457e-06
Iter: 443 loss: 1.07582582e-06
Iter: 444 loss: 1.0780916e-06
Iter: 445 loss: 1.07553637e-06
Iter: 446 loss: 1.07446351e-06
Iter: 447 loss: 1.0771987e-06
Iter: 448 loss: 1.07413553e-06
Iter: 449 loss: 1.07316282e-06
Iter: 450 loss: 1.08517008e-06
Iter: 451 loss: 1.07317578e-06
Iter: 452 loss: 1.07266408e-06
Iter: 453 loss: 1.07160372e-06
Iter: 454 loss: 1.08796667e-06
Iter: 455 loss: 1.07150913e-06
Iter: 456 loss: 1.07033577e-06
Iter: 457 loss: 1.07766141e-06
Iter: 458 loss: 1.07020128e-06
Iter: 459 loss: 1.06896255e-06
Iter: 460 loss: 1.07361438e-06
Iter: 461 loss: 1.06861137e-06
Iter: 462 loss: 1.06793516e-06
Iter: 463 loss: 1.06689868e-06
Iter: 464 loss: 1.06685309e-06
Iter: 465 loss: 1.06546292e-06
Iter: 466 loss: 1.07481185e-06
Iter: 467 loss: 1.06530138e-06
Iter: 468 loss: 1.06420339e-06
Iter: 469 loss: 1.08124527e-06
Iter: 470 loss: 1.06421021e-06
Iter: 471 loss: 1.06392235e-06
Iter: 472 loss: 1.06353286e-06
Iter: 473 loss: 1.06353298e-06
Iter: 474 loss: 1.0628969e-06
Iter: 475 loss: 1.06807693e-06
Iter: 476 loss: 1.06284779e-06
Iter: 477 loss: 1.06233665e-06
Iter: 478 loss: 1.06128073e-06
Iter: 479 loss: 1.07951701e-06
Iter: 480 loss: 1.06123343e-06
Iter: 481 loss: 1.06050675e-06
Iter: 482 loss: 1.06957145e-06
Iter: 483 loss: 1.06049561e-06
Iter: 484 loss: 1.05981326e-06
Iter: 485 loss: 1.06456696e-06
Iter: 486 loss: 1.05976483e-06
Iter: 487 loss: 1.05904837e-06
Iter: 488 loss: 1.05834124e-06
Iter: 489 loss: 1.05819777e-06
Iter: 490 loss: 1.05725621e-06
Iter: 491 loss: 1.05608547e-06
Iter: 492 loss: 1.05596496e-06
Iter: 493 loss: 1.05571394e-06
Iter: 494 loss: 1.05504432e-06
Iter: 495 loss: 1.05464801e-06
Iter: 496 loss: 1.05404581e-06
Iter: 497 loss: 1.05406843e-06
Iter: 498 loss: 1.05335926e-06
Iter: 499 loss: 1.05249717e-06
Iter: 500 loss: 1.05244897e-06
Iter: 501 loss: 1.05265576e-06
Iter: 502 loss: 1.0521137e-06
Iter: 503 loss: 1.05169875e-06
Iter: 504 loss: 1.05117851e-06
Iter: 505 loss: 1.05115305e-06
Iter: 506 loss: 1.05066079e-06
Iter: 507 loss: 1.05558865e-06
Iter: 508 loss: 1.05069148e-06
Iter: 509 loss: 1.05019478e-06
Iter: 510 loss: 1.05086042e-06
Iter: 511 loss: 1.05000436e-06
Iter: 512 loss: 1.04960623e-06
Iter: 513 loss: 1.0484938e-06
Iter: 514 loss: 1.05227025e-06
Iter: 515 loss: 1.04794753e-06
Iter: 516 loss: 1.04692742e-06
Iter: 517 loss: 1.04691935e-06
Iter: 518 loss: 1.04642641e-06
Iter: 519 loss: 1.04638457e-06
Iter: 520 loss: 1.04595006e-06
Iter: 521 loss: 1.04470166e-06
Iter: 522 loss: 1.04909122e-06
Iter: 523 loss: 1.04417586e-06
Iter: 524 loss: 1.0444212e-06
Iter: 525 loss: 1.04366245e-06
Iter: 526 loss: 1.04324522e-06
Iter: 527 loss: 1.04251399e-06
Iter: 528 loss: 1.05904473e-06
Iter: 529 loss: 1.04249943e-06
Iter: 530 loss: 1.04167759e-06
Iter: 531 loss: 1.04188712e-06
Iter: 532 loss: 1.04106027e-06
Iter: 533 loss: 1.04045148e-06
Iter: 534 loss: 1.04226683e-06
Iter: 535 loss: 1.04028891e-06
Iter: 536 loss: 1.03973582e-06
Iter: 537 loss: 1.04428568e-06
Iter: 538 loss: 1.03964135e-06
Iter: 539 loss: 1.0391243e-06
Iter: 540 loss: 1.03863272e-06
Iter: 541 loss: 1.03844661e-06
Iter: 542 loss: 1.03832497e-06
Iter: 543 loss: 1.03818627e-06
Iter: 544 loss: 1.0379481e-06
Iter: 545 loss: 1.03723198e-06
Iter: 546 loss: 1.04115725e-06
Iter: 547 loss: 1.03704349e-06
Iter: 548 loss: 1.03623438e-06
Iter: 549 loss: 1.03990146e-06
Iter: 550 loss: 1.03614093e-06
Iter: 551 loss: 1.03545142e-06
Iter: 552 loss: 1.03601326e-06
Iter: 553 loss: 1.03497325e-06
Iter: 554 loss: 1.03404989e-06
Iter: 555 loss: 1.04779451e-06
Iter: 556 loss: 1.03403397e-06
Iter: 557 loss: 1.03371553e-06
Iter: 558 loss: 1.03313459e-06
Iter: 559 loss: 1.04735216e-06
Iter: 560 loss: 1.03310651e-06
Iter: 561 loss: 1.03211755e-06
Iter: 562 loss: 1.03576258e-06
Iter: 563 loss: 1.03187256e-06
Iter: 564 loss: 1.0309675e-06
Iter: 565 loss: 1.03118668e-06
Iter: 566 loss: 1.0303163e-06
Iter: 567 loss: 1.02953163e-06
Iter: 568 loss: 1.03521279e-06
Iter: 569 loss: 1.0294641e-06
Iter: 570 loss: 1.02892227e-06
Iter: 571 loss: 1.02826459e-06
Iter: 572 loss: 1.02819774e-06
Iter: 573 loss: 1.02709362e-06
Iter: 574 loss: 1.03690491e-06
Iter: 575 loss: 1.02706053e-06
Iter: 576 loss: 1.02661102e-06
Iter: 577 loss: 1.02601439e-06
Iter: 578 loss: 1.02601473e-06
Iter: 579 loss: 1.02541799e-06
Iter: 580 loss: 1.02533534e-06
Iter: 581 loss: 1.02496699e-06
Iter: 582 loss: 1.02431841e-06
Iter: 583 loss: 1.02431068e-06
Iter: 584 loss: 1.02372542e-06
Iter: 585 loss: 1.02390345e-06
Iter: 586 loss: 1.02333081e-06
Iter: 587 loss: 1.02279955e-06
Iter: 588 loss: 1.02281251e-06
Iter: 589 loss: 1.02249373e-06
Iter: 590 loss: 1.0222476e-06
Iter: 591 loss: 1.02216859e-06
Iter: 592 loss: 1.02164677e-06
Iter: 593 loss: 1.02396643e-06
Iter: 594 loss: 1.02159527e-06
Iter: 595 loss: 1.02119463e-06
Iter: 596 loss: 1.02077888e-06
Iter: 597 loss: 1.02067668e-06
Iter: 598 loss: 1.02013087e-06
Iter: 599 loss: 1.02632657e-06
Iter: 600 loss: 1.02008846e-06
Iter: 601 loss: 1.01958051e-06
Iter: 602 loss: 1.01936962e-06
Iter: 603 loss: 1.01909757e-06
Iter: 604 loss: 1.01849605e-06
Iter: 605 loss: 1.02504418e-06
Iter: 606 loss: 1.01846251e-06
Iter: 607 loss: 1.01794899e-06
Iter: 608 loss: 1.01745354e-06
Iter: 609 loss: 1.01741057e-06
Iter: 610 loss: 1.01675118e-06
Iter: 611 loss: 1.01672208e-06
Iter: 612 loss: 1.01628177e-06
Iter: 613 loss: 1.01573107e-06
Iter: 614 loss: 1.01569412e-06
Iter: 615 loss: 1.01510579e-06
Iter: 616 loss: 1.01663136e-06
Iter: 617 loss: 1.01486137e-06
Iter: 618 loss: 1.01448973e-06
Iter: 619 loss: 1.01442197e-06
Iter: 620 loss: 1.0141398e-06
Iter: 621 loss: 1.01419823e-06
Iter: 622 loss: 1.01393061e-06
Iter: 623 loss: 1.01365652e-06
Iter: 624 loss: 1.01603632e-06
Iter: 625 loss: 1.01365617e-06
Iter: 626 loss: 1.01342266e-06
Iter: 627 loss: 1.01295564e-06
Iter: 628 loss: 1.02109402e-06
Iter: 629 loss: 1.01292574e-06
Iter: 630 loss: 1.01248702e-06
Iter: 631 loss: 1.01674595e-06
Iter: 632 loss: 1.01246246e-06
Iter: 633 loss: 1.01197691e-06
Iter: 634 loss: 1.01223588e-06
Iter: 635 loss: 1.01173009e-06
Iter: 636 loss: 1.01119986e-06
Iter: 637 loss: 1.0142993e-06
Iter: 638 loss: 1.01112414e-06
Iter: 639 loss: 1.01071146e-06
Iter: 640 loss: 1.01035221e-06
Iter: 641 loss: 1.0102035e-06
Iter: 642 loss: 1.00983368e-06
Iter: 643 loss: 1.00979923e-06
Iter: 644 loss: 1.00943737e-06
Iter: 645 loss: 1.00900706e-06
Iter: 646 loss: 1.00896477e-06
Iter: 647 loss: 1.00842885e-06
Iter: 648 loss: 1.00785041e-06
Iter: 649 loss: 1.00778118e-06
Iter: 650 loss: 1.00803993e-06
Iter: 651 loss: 1.00732109e-06
Iter: 652 loss: 1.00695081e-06
Iter: 653 loss: 1.0067838e-06
Iter: 654 loss: 1.00655257e-06
Iter: 655 loss: 1.00614591e-06
Iter: 656 loss: 1.0075621e-06
Iter: 657 loss: 1.00606144e-06
Iter: 658 loss: 1.00541547e-06
Iter: 659 loss: 1.00504531e-06
Iter: 660 loss: 1.00475427e-06
Iter: 661 loss: 1.00427008e-06
Iter: 662 loss: 1.00644115e-06
Iter: 663 loss: 1.00414888e-06
Iter: 664 loss: 1.0037046e-06
Iter: 665 loss: 1.00540137e-06
Iter: 666 loss: 1.0035867e-06
Iter: 667 loss: 1.00324087e-06
Iter: 668 loss: 1.00431487e-06
Iter: 669 loss: 1.00314355e-06
Iter: 670 loss: 1.0028042e-06
Iter: 671 loss: 1.00371949e-06
Iter: 672 loss: 1.00271518e-06
Iter: 673 loss: 1.0024836e-06
Iter: 674 loss: 1.00401917e-06
Iter: 675 loss: 1.00244711e-06
Iter: 676 loss: 1.00216357e-06
Iter: 677 loss: 1.00217869e-06
Iter: 678 loss: 1.00189618e-06
Iter: 679 loss: 1.00162526e-06
Iter: 680 loss: 1.00083412e-06
Iter: 681 loss: 1.00600994e-06
Iter: 682 loss: 1.00062198e-06
Iter: 683 loss: 1.00017587e-06
Iter: 684 loss: 1.00005514e-06
Iter: 685 loss: 9.99399276e-07
Iter: 686 loss: 1.00213799e-06
Iter: 687 loss: 9.99323674e-07
Iter: 688 loss: 9.98894e-07
Iter: 689 loss: 9.98088353e-07
Iter: 690 loss: 1.01462274e-06
Iter: 691 loss: 9.98081077e-07
Iter: 692 loss: 9.97263101e-07
Iter: 693 loss: 9.97233883e-07
Iter: 694 loss: 9.96978542e-07
Iter: 695 loss: 9.9680733e-07
Iter: 696 loss: 9.96762651e-07
Iter: 697 loss: 9.96393396e-07
Iter: 698 loss: 1.00048919e-06
Iter: 699 loss: 9.96399e-07
Iter: 700 loss: 9.9614e-07
Iter: 701 loss: 9.96285394e-07
Iter: 702 loss: 9.95990604e-07
Iter: 703 loss: 9.95704454e-07
Iter: 704 loss: 9.96448648e-07
Iter: 705 loss: 9.95657e-07
Iter: 706 loss: 9.95335085e-07
Iter: 707 loss: 9.95618279e-07
Iter: 708 loss: 9.9516717e-07
Iter: 709 loss: 9.94727429e-07
Iter: 710 loss: 9.96979907e-07
Iter: 711 loss: 9.94634547e-07
Iter: 712 loss: 9.94346124e-07
Iter: 713 loss: 9.9367162e-07
Iter: 714 loss: 9.9859767e-07
Iter: 715 loss: 9.93525646e-07
Iter: 716 loss: 9.92747346e-07
Iter: 717 loss: 9.98109e-07
Iter: 718 loss: 9.92652303e-07
Iter: 719 loss: 9.92551463e-07
Iter: 720 loss: 9.92320679e-07
Iter: 721 loss: 9.92101e-07
Iter: 722 loss: 9.91416e-07
Iter: 723 loss: 9.94261882e-07
Iter: 724 loss: 9.91136e-07
Iter: 725 loss: 9.91037268e-07
Iter: 726 loss: 9.90862645e-07
Iter: 727 loss: 9.90551939e-07
Iter: 728 loss: 9.89841851e-07
Iter: 729 loss: 9.971493e-07
Iter: 730 loss: 9.89784439e-07
Iter: 731 loss: 9.88812644e-07
Iter: 732 loss: 9.89967702e-07
Iter: 733 loss: 9.88306851e-07
Iter: 734 loss: 9.88028546e-07
Iter: 735 loss: 9.87994326e-07
Iter: 736 loss: 9.87571184e-07
Iter: 737 loss: 9.87656e-07
Iter: 738 loss: 9.87332442e-07
Iter: 739 loss: 9.86955e-07
Iter: 740 loss: 9.87674639e-07
Iter: 741 loss: 9.86805617e-07
Iter: 742 loss: 9.86523e-07
Iter: 743 loss: 9.91155503e-07
Iter: 744 loss: 9.86521741e-07
Iter: 745 loss: 9.86292775e-07
Iter: 746 loss: 9.85919428e-07
Iter: 747 loss: 9.94801553e-07
Iter: 748 loss: 9.85950919e-07
Iter: 749 loss: 9.85485e-07
Iter: 750 loss: 9.86039822e-07
Iter: 751 loss: 9.85251177e-07
Iter: 752 loss: 9.84701728e-07
Iter: 753 loss: 9.86908844e-07
Iter: 754 loss: 9.84633857e-07
Iter: 755 loss: 9.83763357e-07
Iter: 756 loss: 9.83692e-07
Iter: 757 loss: 9.83074415e-07
Iter: 758 loss: 9.8242549e-07
Iter: 759 loss: 9.82185838e-07
Iter: 760 loss: 9.81792482e-07
Iter: 761 loss: 9.81262247e-07
Iter: 762 loss: 9.8127407e-07
Iter: 763 loss: 9.80717232e-07
Iter: 764 loss: 9.8131386e-07
Iter: 765 loss: 9.8042517e-07
Iter: 766 loss: 9.80137656e-07
Iter: 767 loss: 9.79675519e-07
Iter: 768 loss: 9.91133106e-07
Iter: 769 loss: 9.79669608e-07
Iter: 770 loss: 9.79448714e-07
Iter: 771 loss: 9.7936163e-07
Iter: 772 loss: 9.79092079e-07
Iter: 773 loss: 9.80502136e-07
Iter: 774 loss: 9.79084234e-07
Iter: 775 loss: 9.78939e-07
Iter: 776 loss: 9.78626076e-07
Iter: 777 loss: 9.82410256e-07
Iter: 778 loss: 9.785972e-07
Iter: 779 loss: 9.78075263e-07
Iter: 780 loss: 9.81831818e-07
Iter: 781 loss: 9.78037406e-07
Iter: 782 loss: 9.777375e-07
Iter: 783 loss: 9.77684522e-07
Iter: 784 loss: 9.77445552e-07
Iter: 785 loss: 9.77095624e-07
Iter: 786 loss: 9.76967158e-07
Iter: 787 loss: 9.76743081e-07
Iter: 788 loss: 9.76336537e-07
Iter: 789 loss: 9.82255528e-07
Iter: 790 loss: 9.76335286e-07
Iter: 791 loss: 9.76022307e-07
Iter: 792 loss: 9.79159e-07
Iter: 793 loss: 9.75984904e-07
Iter: 794 loss: 9.75816874e-07
Iter: 795 loss: 9.75329272e-07
Iter: 796 loss: 9.77060154e-07
Iter: 797 loss: 9.75048124e-07
Iter: 798 loss: 9.7499219e-07
Iter: 799 loss: 9.7477573e-07
Iter: 800 loss: 9.74460363e-07
Iter: 801 loss: 9.7442171e-07
Iter: 802 loss: 9.7419e-07
Iter: 803 loss: 9.73835768e-07
Iter: 804 loss: 9.73235e-07
Iter: 805 loss: 9.73184342e-07
Iter: 806 loss: 9.73273927e-07
Iter: 807 loss: 9.73042688e-07
Iter: 808 loss: 9.72806e-07
Iter: 809 loss: 9.72415933e-07
Iter: 810 loss: 9.79442461e-07
Iter: 811 loss: 9.72423663e-07
Iter: 812 loss: 9.72258476e-07
Iter: 813 loss: 9.72241537e-07
Iter: 814 loss: 9.7207112e-07
Iter: 815 loss: 9.7223e-07
Iter: 816 loss: 9.7198847e-07
Iter: 817 loss: 9.71760414e-07
Iter: 818 loss: 9.7132272e-07
Iter: 819 loss: 9.79658e-07
Iter: 820 loss: 9.71361374e-07
Iter: 821 loss: 9.71032e-07
Iter: 822 loss: 9.710011e-07
Iter: 823 loss: 9.70783731e-07
Iter: 824 loss: 9.74452064e-07
Iter: 825 loss: 9.70771566e-07
Iter: 826 loss: 9.70555448e-07
Iter: 827 loss: 9.70057499e-07
Iter: 828 loss: 9.75229455e-07
Iter: 829 loss: 9.70015e-07
Iter: 830 loss: 9.69676e-07
Iter: 831 loss: 9.74539716e-07
Iter: 832 loss: 9.6966e-07
Iter: 833 loss: 9.69350822e-07
Iter: 834 loss: 9.69568305e-07
Iter: 835 loss: 9.69108669e-07
Iter: 836 loss: 9.68744757e-07
Iter: 837 loss: 9.68855147e-07
Iter: 838 loss: 9.68489076e-07
Iter: 839 loss: 9.68287395e-07
Iter: 840 loss: 9.70328188e-07
Iter: 841 loss: 9.68283189e-07
Iter: 842 loss: 9.67971573e-07
Iter: 843 loss: 9.67820711e-07
Iter: 844 loss: 9.67638698e-07
Iter: 845 loss: 9.6731992e-07
Iter: 846 loss: 9.6799829e-07
Iter: 847 loss: 9.67201458e-07
Iter: 848 loss: 9.66867333e-07
Iter: 849 loss: 9.71530881e-07
Iter: 850 loss: 9.66845846e-07
Iter: 851 loss: 9.66678158e-07
Iter: 852 loss: 9.66182142e-07
Iter: 853 loss: 9.72902853e-07
Iter: 854 loss: 9.66155653e-07
Iter: 855 loss: 9.65732625e-07
Iter: 856 loss: 9.66910534e-07
Iter: 857 loss: 9.65592335e-07
Iter: 858 loss: 9.6536553e-07
Iter: 859 loss: 9.65326763e-07
Iter: 860 loss: 9.65074832e-07
Iter: 861 loss: 9.64850528e-07
Iter: 862 loss: 9.64792321e-07
Iter: 863 loss: 9.64574e-07
Iter: 864 loss: 9.64373839e-07
Iter: 865 loss: 9.64343144e-07
Iter: 866 loss: 9.63964567e-07
Iter: 867 loss: 9.63968205e-07
Iter: 868 loss: 9.63794832e-07
Iter: 869 loss: 9.63500497e-07
Iter: 870 loss: 9.635271e-07
Iter: 871 loss: 9.63136586e-07
Iter: 872 loss: 9.63435127e-07
Iter: 873 loss: 9.62919785e-07
Iter: 874 loss: 9.62468903e-07
Iter: 875 loss: 9.68156655e-07
Iter: 876 loss: 9.6245526e-07
Iter: 877 loss: 9.62307922e-07
Iter: 878 loss: 9.62018589e-07
Iter: 879 loss: 9.6205008e-07
Iter: 880 loss: 9.61785418e-07
Iter: 881 loss: 9.61801e-07
Iter: 882 loss: 9.61632395e-07
Iter: 883 loss: 9.61323394e-07
Iter: 884 loss: 9.61293381e-07
Iter: 885 loss: 9.61059641e-07
Iter: 886 loss: 9.60622629e-07
Iter: 887 loss: 9.60629e-07
Iter: 888 loss: 9.60499392e-07
Iter: 889 loss: 9.60457328e-07
Iter: 890 loss: 9.60263151e-07
Iter: 891 loss: 9.60277248e-07
Iter: 892 loss: 9.60112743e-07
Iter: 893 loss: 9.599039e-07
Iter: 894 loss: 9.59599106e-07
Iter: 895 loss: 9.59566478e-07
Iter: 896 loss: 9.59329213e-07
Iter: 897 loss: 9.59324211e-07
Iter: 898 loss: 9.59134809e-07
Iter: 899 loss: 9.59412318e-07
Iter: 900 loss: 9.59014415e-07
Iter: 901 loss: 9.58891292e-07
Iter: 902 loss: 9.5877e-07
Iter: 903 loss: 9.58715759e-07
Iter: 904 loss: 9.58472128e-07
Iter: 905 loss: 9.58600367e-07
Iter: 906 loss: 9.58307e-07
Iter: 907 loss: 9.58006467e-07
Iter: 908 loss: 9.59028512e-07
Iter: 909 loss: 9.57971338e-07
Iter: 910 loss: 9.57678822e-07
Iter: 911 loss: 9.57362545e-07
Iter: 912 loss: 9.57325938e-07
Iter: 913 loss: 9.5722271e-07
Iter: 914 loss: 9.57057637e-07
Iter: 915 loss: 9.56952363e-07
Iter: 916 loss: 9.5663745e-07
Iter: 917 loss: 9.6163717e-07
Iter: 918 loss: 9.56636427e-07
Iter: 919 loss: 9.56360395e-07
Iter: 920 loss: 9.56440545e-07
Iter: 921 loss: 9.56126087e-07
Iter: 922 loss: 9.55977612e-07
Iter: 923 loss: 9.5587643e-07
Iter: 924 loss: 9.55773316e-07
Iter: 925 loss: 9.55613586e-07
Iter: 926 loss: 9.59667e-07
Iter: 927 loss: 9.55617224e-07
Iter: 928 loss: 9.55315e-07
Iter: 929 loss: 9.55533096e-07
Iter: 930 loss: 9.55207724e-07
Iter: 931 loss: 9.54889174e-07
Iter: 932 loss: 9.54915777e-07
Iter: 933 loss: 9.54747179e-07
Iter: 934 loss: 9.54741267e-07
Iter: 935 loss: 9.54625079e-07
Iter: 936 loss: 9.54287543e-07
Iter: 937 loss: 9.54747065e-07
Iter: 938 loss: 9.5418693e-07
Iter: 939 loss: 9.53852577e-07
Iter: 940 loss: 9.5454061e-07
Iter: 941 loss: 9.53746166e-07
Iter: 942 loss: 9.53492076e-07
Iter: 943 loss: 9.54296638e-07
Iter: 944 loss: 9.53425229e-07
Iter: 945 loss: 9.53261747e-07
Iter: 946 loss: 9.54743e-07
Iter: 947 loss: 9.5325322e-07
Iter: 948 loss: 9.53085816e-07
Iter: 949 loss: 9.53098947e-07
Iter: 950 loss: 9.52918185e-07
Iter: 951 loss: 9.5281257e-07
Iter: 952 loss: 9.52543132e-07
Iter: 953 loss: 9.52563369e-07
Iter: 954 loss: 9.52386358e-07
Iter: 955 loss: 9.52327753e-07
Iter: 956 loss: 9.52115272e-07
Iter: 957 loss: 9.52655114e-07
Iter: 958 loss: 9.52047799e-07
Iter: 959 loss: 9.51937182e-07
Iter: 960 loss: 9.51666323e-07
Iter: 961 loss: 9.56799568e-07
Iter: 962 loss: 9.51681727e-07
Iter: 963 loss: 9.51370225e-07
Iter: 964 loss: 9.54128723e-07
Iter: 965 loss: 9.51383072e-07
Iter: 966 loss: 9.51119e-07
Iter: 967 loss: 9.53001404e-07
Iter: 968 loss: 9.51105903e-07
Iter: 969 loss: 9.50923209e-07
Iter: 970 loss: 9.51153424e-07
Iter: 971 loss: 9.5086142e-07
Iter: 972 loss: 9.50698393e-07
Iter: 973 loss: 9.50799404e-07
Iter: 974 loss: 9.50564299e-07
Iter: 975 loss: 9.50372396e-07
Iter: 976 loss: 9.5083027e-07
Iter: 977 loss: 9.5027292e-07
Iter: 978 loss: 9.50080278e-07
Iter: 979 loss: 9.5017549e-07
Iter: 980 loss: 9.49913897e-07
Iter: 981 loss: 9.4963184e-07
Iter: 982 loss: 9.49651678e-07
Iter: 983 loss: 9.49467562e-07
Iter: 984 loss: 9.49272305e-07
Iter: 985 loss: 9.49255877e-07
Iter: 986 loss: 9.49024e-07
Iter: 987 loss: 9.48532715e-07
Iter: 988 loss: 9.56247163e-07
Iter: 989 loss: 9.48545e-07
Iter: 990 loss: 9.49051923e-07
Iter: 991 loss: 9.48352351e-07
Iter: 992 loss: 9.48218826e-07
Iter: 993 loss: 9.48172215e-07
Iter: 994 loss: 9.48118782e-07
Iter: 995 loss: 9.47926935e-07
Iter: 996 loss: 9.4763675e-07
Iter: 997 loss: 9.54163738e-07
Iter: 998 loss: 9.4764215e-07
Iter: 999 loss: 9.47516469e-07
Iter: 1000 loss: 9.47449848e-07
Iter: 1001 loss: 9.4719752e-07
Iter: 1002 loss: 9.4744712e-07
Iter: 1003 loss: 9.47088722e-07
Iter: 1004 loss: 9.46865441e-07
Iter: 1005 loss: 9.47484125e-07
Iter: 1006 loss: 9.4685447e-07
Iter: 1007 loss: 9.46575824e-07
Iter: 1008 loss: 9.48521688e-07
Iter: 1009 loss: 9.46576108e-07
Iter: 1010 loss: 9.46468617e-07
Iter: 1011 loss: 9.46407681e-07
Iter: 1012 loss: 9.46309228e-07
Iter: 1013 loss: 9.46124e-07
Iter: 1014 loss: 9.46329919e-07
Iter: 1015 loss: 9.46034277e-07
Iter: 1016 loss: 9.45849e-07
Iter: 1017 loss: 9.47926878e-07
Iter: 1018 loss: 9.45854936e-07
Iter: 1019 loss: 9.457396e-07
Iter: 1020 loss: 9.46163595e-07
Iter: 1021 loss: 9.45702368e-07
Iter: 1022 loss: 9.45562874e-07
Iter: 1023 loss: 9.45343288e-07
Iter: 1024 loss: 9.4970892e-07
Iter: 1025 loss: 9.45380521e-07
Iter: 1026 loss: 9.45182251e-07
Iter: 1027 loss: 9.47064677e-07
Iter: 1028 loss: 9.45234092e-07
Iter: 1029 loss: 9.45005e-07
Iter: 1030 loss: 9.45511772e-07
Iter: 1031 loss: 9.44911733e-07
Iter: 1032 loss: 9.44795374e-07
Iter: 1033 loss: 9.44678334e-07
Iter: 1034 loss: 9.44650765e-07
Iter: 1035 loss: 9.44446356e-07
Iter: 1036 loss: 9.45502109e-07
Iter: 1037 loss: 9.44394969e-07
Iter: 1038 loss: 9.44142357e-07
Iter: 1039 loss: 9.44027136e-07
Iter: 1040 loss: 9.43856719e-07
Iter: 1041 loss: 9.4369e-07
Iter: 1042 loss: 9.43713758e-07
Iter: 1043 loss: 9.43523332e-07
Iter: 1044 loss: 9.43509121e-07
Iter: 1045 loss: 9.43428859e-07
Iter: 1046 loss: 9.43192447e-07
Iter: 1047 loss: 9.42997758e-07
Iter: 1048 loss: 9.42953e-07
Iter: 1049 loss: 9.42746055e-07
Iter: 1050 loss: 9.42753445e-07
Iter: 1051 loss: 9.42602355e-07
Iter: 1052 loss: 9.43546354e-07
Iter: 1053 loss: 9.42623842e-07
Iter: 1054 loss: 9.42483155e-07
Iter: 1055 loss: 9.42139877e-07
Iter: 1056 loss: 9.46996465e-07
Iter: 1057 loss: 9.42183931e-07
Iter: 1058 loss: 9.4199072e-07
Iter: 1059 loss: 9.42844281e-07
Iter: 1060 loss: 9.41924611e-07
Iter: 1061 loss: 9.41800408e-07
Iter: 1062 loss: 9.41819849e-07
Iter: 1063 loss: 9.41695077e-07
Iter: 1064 loss: 9.41362828e-07
Iter: 1065 loss: 9.45690147e-07
Iter: 1066 loss: 9.41389e-07
Iter: 1067 loss: 9.41202813e-07
Iter: 1068 loss: 9.41767439e-07
Iter: 1069 loss: 9.41073608e-07
Iter: 1070 loss: 9.40873235e-07
Iter: 1071 loss: 9.43159307e-07
Iter: 1072 loss: 9.40847599e-07
Iter: 1073 loss: 9.40677523e-07
Iter: 1074 loss: 9.4040422e-07
Iter: 1075 loss: 9.40389498e-07
Iter: 1076 loss: 9.40156042e-07
Iter: 1077 loss: 9.40167695e-07
Iter: 1078 loss: 9.39970846e-07
Iter: 1079 loss: 9.39610743e-07
Iter: 1080 loss: 9.46233172e-07
Iter: 1081 loss: 9.39604547e-07
Iter: 1082 loss: 9.39370636e-07
Iter: 1083 loss: 9.40194582e-07
Iter: 1084 loss: 9.39272468e-07
Iter: 1085 loss: 9.39049755e-07
Iter: 1086 loss: 9.38953576e-07
Iter: 1087 loss: 9.38850633e-07
Iter: 1088 loss: 9.38649521e-07
Iter: 1089 loss: 9.38601318e-07
Iter: 1090 loss: 9.38520316e-07
Iter: 1091 loss: 9.38473363e-07
Iter: 1092 loss: 9.38408618e-07
Iter: 1093 loss: 9.38234734e-07
Iter: 1094 loss: 9.3945755e-07
Iter: 1095 loss: 9.38223195e-07
Iter: 1096 loss: 9.38044423e-07
Iter: 1097 loss: 9.38286291e-07
Iter: 1098 loss: 9.37952507e-07
Iter: 1099 loss: 9.37859284e-07
Iter: 1100 loss: 9.37674e-07
Iter: 1101 loss: 9.37668e-07
Iter: 1102 loss: 9.37430286e-07
Iter: 1103 loss: 9.39653773e-07
Iter: 1104 loss: 9.37439154e-07
Iter: 1105 loss: 9.37272262e-07
Iter: 1106 loss: 9.37050572e-07
Iter: 1107 loss: 9.37007258e-07
Iter: 1108 loss: 9.36853098e-07
Iter: 1109 loss: 9.36832691e-07
Iter: 1110 loss: 9.3663084e-07
Iter: 1111 loss: 9.36442291e-07
Iter: 1112 loss: 9.3640017e-07
Iter: 1113 loss: 9.36235892e-07
Iter: 1114 loss: 9.36404604e-07
Iter: 1115 loss: 9.36085314e-07
Iter: 1116 loss: 9.35769947e-07
Iter: 1117 loss: 9.37947e-07
Iter: 1118 loss: 9.35775802e-07
Iter: 1119 loss: 9.35518869e-07
Iter: 1120 loss: 9.35718163e-07
Iter: 1121 loss: 9.3536255e-07
Iter: 1122 loss: 9.35118692e-07
Iter: 1123 loss: 9.36519712e-07
Iter: 1124 loss: 9.35121761e-07
Iter: 1125 loss: 9.34922525e-07
Iter: 1126 loss: 9.35810817e-07
Iter: 1127 loss: 9.34896605e-07
Iter: 1128 loss: 9.34715388e-07
Iter: 1129 loss: 9.35156e-07
Iter: 1130 loss: 9.34632112e-07
Iter: 1131 loss: 9.34536502e-07
Iter: 1132 loss: 9.34544062e-07
Iter: 1133 loss: 9.34495233e-07
Iter: 1134 loss: 9.34334e-07
Iter: 1135 loss: 9.35903472e-07
Iter: 1136 loss: 9.34340846e-07
Iter: 1137 loss: 9.34247396e-07
Iter: 1138 loss: 9.34136551e-07
Iter: 1139 loss: 9.34136494e-07
Iter: 1140 loss: 9.34021159e-07
Iter: 1141 loss: 9.34036507e-07
Iter: 1142 loss: 9.33928163e-07
Iter: 1143 loss: 9.33735066e-07
Iter: 1144 loss: 9.35769663e-07
Iter: 1145 loss: 9.33690103e-07
Iter: 1146 loss: 9.33626154e-07
Iter: 1147 loss: 9.33396109e-07
Iter: 1148 loss: 9.33397814e-07
Iter: 1149 loss: 9.33200056e-07
Iter: 1150 loss: 9.35121705e-07
Iter: 1151 loss: 9.33184197e-07
Iter: 1152 loss: 9.32906346e-07
Iter: 1153 loss: 9.33199658e-07
Iter: 1154 loss: 9.32848366e-07
Iter: 1155 loss: 9.32522767e-07
Iter: 1156 loss: 9.32805904e-07
Iter: 1157 loss: 9.32377816e-07
Iter: 1158 loss: 9.32174203e-07
Iter: 1159 loss: 9.33343813e-07
Iter: 1160 loss: 9.32173407e-07
Iter: 1161 loss: 9.31929719e-07
Iter: 1162 loss: 9.3260752e-07
Iter: 1163 loss: 9.31856277e-07
Iter: 1164 loss: 9.31707916e-07
Iter: 1165 loss: 9.31481338e-07
Iter: 1166 loss: 9.31419891e-07
Iter: 1167 loss: 9.31264765e-07
Iter: 1168 loss: 9.31281363e-07
Iter: 1169 loss: 9.31108559e-07
Iter: 1170 loss: 9.30901592e-07
Iter: 1171 loss: 9.30881e-07
Iter: 1172 loss: 9.30671263e-07
Iter: 1173 loss: 9.31095258e-07
Iter: 1174 loss: 9.30579e-07
Iter: 1175 loss: 9.30490501e-07
Iter: 1176 loss: 9.304689e-07
Iter: 1177 loss: 9.3036175e-07
Iter: 1178 loss: 9.3013e-07
Iter: 1179 loss: 9.34869604e-07
Iter: 1180 loss: 9.30141596e-07
Iter: 1181 loss: 9.29981354e-07
Iter: 1182 loss: 9.31006639e-07
Iter: 1183 loss: 9.29958958e-07
Iter: 1184 loss: 9.29816849e-07
Iter: 1185 loss: 9.30781141e-07
Iter: 1186 loss: 9.29812472e-07
Iter: 1187 loss: 9.29691168e-07
Iter: 1188 loss: 9.29744772e-07
Iter: 1189 loss: 9.29629721e-07
Iter: 1190 loss: 9.2950711e-07
Iter: 1191 loss: 9.29550595e-07
Iter: 1192 loss: 9.29431621e-07
Iter: 1193 loss: 9.29237331e-07
Iter: 1194 loss: 9.31267664e-07
Iter: 1195 loss: 9.29270755e-07
Iter: 1196 loss: 9.29185944e-07
Iter: 1197 loss: 9.29061684e-07
Iter: 1198 loss: 9.29041221e-07
Iter: 1199 loss: 9.28939812e-07
Iter: 1200 loss: 9.29287353e-07
Iter: 1201 loss: 9.28895247e-07
Iter: 1202 loss: 9.28764791e-07
Iter: 1203 loss: 9.28898089e-07
Iter: 1204 loss: 9.28630811e-07
Iter: 1205 loss: 9.28516499e-07
Iter: 1206 loss: 9.2844158e-07
Iter: 1207 loss: 9.28370582e-07
Iter: 1208 loss: 9.28209e-07
Iter: 1209 loss: 9.30073099e-07
Iter: 1210 loss: 9.28206703e-07
Iter: 1211 loss: 9.27971598e-07
Iter: 1212 loss: 9.27886674e-07
Iter: 1213 loss: 9.27770373e-07
Iter: 1214 loss: 9.27573637e-07
Iter: 1215 loss: 9.27468818e-07
Iter: 1216 loss: 9.27448127e-07
Iter: 1217 loss: 9.27217286e-07
Iter: 1218 loss: 9.27194719e-07
Iter: 1219 loss: 9.27070346e-07
Iter: 1220 loss: 9.27067447e-07
Iter: 1221 loss: 9.26937787e-07
Iter: 1222 loss: 9.2682626e-07
Iter: 1223 loss: 9.26985763e-07
Iter: 1224 loss: 9.26732582e-07
Iter: 1225 loss: 9.26589792e-07
Iter: 1226 loss: 9.26593316e-07
Iter: 1227 loss: 9.26537837e-07
Iter: 1228 loss: 9.26395046e-07
Iter: 1229 loss: 9.26375719e-07
Iter: 1230 loss: 9.26226221e-07
Iter: 1231 loss: 9.26316602e-07
Iter: 1232 loss: 9.26133566e-07
Iter: 1233 loss: 9.25908239e-07
Iter: 1234 loss: 9.28275881e-07
Iter: 1235 loss: 9.25892778e-07
Iter: 1236 loss: 9.2578648e-07
Iter: 1237 loss: 9.25530799e-07
Iter: 1238 loss: 9.25543418e-07
Iter: 1239 loss: 9.25257041e-07
Iter: 1240 loss: 9.25371921e-07
Iter: 1241 loss: 9.25064171e-07
Iter: 1242 loss: 9.24881306e-07
Iter: 1243 loss: 9.24840265e-07
Iter: 1244 loss: 9.24753749e-07
Iter: 1245 loss: 9.24607036e-07
Iter: 1246 loss: 9.26298355e-07
Iter: 1247 loss: 9.24525182e-07
Iter: 1248 loss: 9.2433379e-07
Iter: 1249 loss: 9.25233167e-07
Iter: 1250 loss: 9.24246081e-07
Iter: 1251 loss: 9.24062476e-07
Iter: 1252 loss: 9.2754135e-07
Iter: 1253 loss: 9.24072936e-07
Iter: 1254 loss: 9.23914399e-07
Iter: 1255 loss: 9.23753305e-07
Iter: 1256 loss: 9.26248731e-07
Iter: 1257 loss: 9.23738298e-07
Iter: 1258 loss: 9.23564e-07
Iter: 1259 loss: 9.23564755e-07
Iter: 1260 loss: 9.23371772e-07
Iter: 1261 loss: 9.23306402e-07
Iter: 1262 loss: 9.23243874e-07
Iter: 1263 loss: 9.22989443e-07
Iter: 1264 loss: 9.22809761e-07
Iter: 1265 loss: 9.22756499e-07
Iter: 1266 loss: 9.22720403e-07
Iter: 1267 loss: 9.22581421e-07
Iter: 1268 loss: 9.22488368e-07
Iter: 1269 loss: 9.22221261e-07
Iter: 1270 loss: 9.26763732e-07
Iter: 1271 loss: 9.22223649e-07
Iter: 1272 loss: 9.21911123e-07
Iter: 1273 loss: 9.221244e-07
Iter: 1274 loss: 9.21725132e-07
Iter: 1275 loss: 9.21847516e-07
Iter: 1276 loss: 9.21587684e-07
Iter: 1277 loss: 9.21553578e-07
Iter: 1278 loss: 9.21189951e-07
Iter: 1279 loss: 9.23043e-07
Iter: 1280 loss: 9.21120829e-07
Iter: 1281 loss: 9.20668185e-07
Iter: 1282 loss: 9.21413744e-07
Iter: 1283 loss: 9.20509365e-07
Iter: 1284 loss: 9.2035981e-07
Iter: 1285 loss: 9.2030939e-07
Iter: 1286 loss: 9.20072125e-07
Iter: 1287 loss: 9.19737886e-07
Iter: 1288 loss: 9.19710942e-07
Iter: 1289 loss: 9.19420927e-07
Iter: 1290 loss: 9.19470835e-07
Iter: 1291 loss: 9.19187244e-07
Iter: 1292 loss: 9.19166609e-07
Iter: 1293 loss: 9.19015349e-07
Iter: 1294 loss: 9.18859541e-07
Iter: 1295 loss: 9.1862978e-07
Iter: 1296 loss: 9.18616422e-07
Iter: 1297 loss: 9.18349087e-07
Iter: 1298 loss: 9.18244723e-07
Iter: 1299 loss: 9.18166108e-07
Iter: 1300 loss: 9.17912757e-07
Iter: 1301 loss: 9.17896728e-07
Iter: 1302 loss: 9.17751549e-07
Iter: 1303 loss: 9.17801231e-07
Iter: 1304 loss: 9.17642637e-07
Iter: 1305 loss: 9.1747944e-07
Iter: 1306 loss: 9.17714942e-07
Iter: 1307 loss: 9.17405714e-07
Iter: 1308 loss: 9.17232967e-07
Iter: 1309 loss: 9.18159e-07
Iter: 1310 loss: 9.17194427e-07
Iter: 1311 loss: 9.17056582e-07
Iter: 1312 loss: 9.16806869e-07
Iter: 1313 loss: 9.20755724e-07
Iter: 1314 loss: 9.16768272e-07
Iter: 1315 loss: 9.16432555e-07
Iter: 1316 loss: 9.17033333e-07
Iter: 1317 loss: 9.16284421e-07
Iter: 1318 loss: 9.15938926e-07
Iter: 1319 loss: 9.16320687e-07
Iter: 1320 loss: 9.15789883e-07
Iter: 1321 loss: 9.15368e-07
Iter: 1322 loss: 9.17965167e-07
Iter: 1323 loss: 9.15222756e-07
Iter: 1324 loss: 9.14952466e-07
Iter: 1325 loss: 9.1494752e-07
Iter: 1326 loss: 9.14802513e-07
Iter: 1327 loss: 9.14515738e-07
Iter: 1328 loss: 9.16105421e-07
Iter: 1329 loss: 9.14359248e-07
Iter: 1330 loss: 9.14579914e-07
Iter: 1331 loss: 9.14170244e-07
Iter: 1332 loss: 9.1405127e-07
Iter: 1333 loss: 9.13856809e-07
Iter: 1334 loss: 9.16647537e-07
Iter: 1335 loss: 9.13839358e-07
Iter: 1336 loss: 9.13653707e-07
Iter: 1337 loss: 9.1417985e-07
Iter: 1338 loss: 9.13566e-07
Iter: 1339 loss: 9.13477152e-07
Iter: 1340 loss: 9.13422241e-07
Iter: 1341 loss: 9.13388078e-07
Iter: 1342 loss: 9.132591e-07
Iter: 1343 loss: 9.13294912e-07
Iter: 1344 loss: 9.13119e-07
Iter: 1345 loss: 9.12960957e-07
Iter: 1346 loss: 9.12938049e-07
Iter: 1347 loss: 9.12694645e-07
Iter: 1348 loss: 9.14285351e-07
Iter: 1349 loss: 9.12732958e-07
Iter: 1350 loss: 9.12412702e-07
Iter: 1351 loss: 9.1310244e-07
Iter: 1352 loss: 9.12326755e-07
Iter: 1353 loss: 9.1210552e-07
Iter: 1354 loss: 9.1174644e-07
Iter: 1355 loss: 9.11719837e-07
Iter: 1356 loss: 9.11408108e-07
Iter: 1357 loss: 9.13331291e-07
Iter: 1358 loss: 9.11326197e-07
Iter: 1359 loss: 9.10983601e-07
Iter: 1360 loss: 9.14826728e-07
Iter: 1361 loss: 9.1098741e-07
Iter: 1362 loss: 9.10820631e-07
Iter: 1363 loss: 9.10473545e-07
Iter: 1364 loss: 9.14173597e-07
Iter: 1365 loss: 9.1043961e-07
Iter: 1366 loss: 9.10358381e-07
Iter: 1367 loss: 9.10268e-07
Iter: 1368 loss: 9.10052393e-07
Iter: 1369 loss: 9.1002812e-07
Iter: 1370 loss: 9.09909261e-07
Iter: 1371 loss: 9.09760104e-07
Iter: 1372 loss: 9.09381583e-07
Iter: 1373 loss: 9.15015676e-07
Iter: 1374 loss: 9.09390621e-07
Iter: 1375 loss: 9.09312121e-07
Iter: 1376 loss: 9.09223104e-07
Iter: 1377 loss: 9.0905661e-07
Iter: 1378 loss: 9.09038135e-07
Iter: 1379 loss: 9.08900347e-07
Iter: 1380 loss: 9.08735e-07
Iter: 1381 loss: 9.08723678e-07
Iter: 1382 loss: 9.0851546e-07
Iter: 1383 loss: 9.08460606e-07
Iter: 1384 loss: 9.0845549e-07
Iter: 1385 loss: 9.08360562e-07
Iter: 1386 loss: 9.08248239e-07
Iter: 1387 loss: 9.08218567e-07
Iter: 1388 loss: 9.08054062e-07
Iter: 1389 loss: 9.07672529e-07
Iter: 1390 loss: 9.1296e-07
Iter: 1391 loss: 9.07662695e-07
Iter: 1392 loss: 9.07517062e-07
Iter: 1393 loss: 9.07494439e-07
Iter: 1394 loss: 9.07271e-07
Iter: 1395 loss: 9.07832032e-07
Iter: 1396 loss: 9.07175377e-07
Iter: 1397 loss: 9.06978926e-07
Iter: 1398 loss: 9.06581477e-07
Iter: 1399 loss: 9.14900852e-07
Iter: 1400 loss: 9.06546632e-07
Iter: 1401 loss: 9.06370815e-07
Iter: 1402 loss: 9.0944576e-07
Iter: 1403 loss: 9.06388379e-07
Iter: 1404 loss: 9.06210516e-07
Iter: 1405 loss: 9.07475737e-07
Iter: 1406 loss: 9.06168566e-07
Iter: 1407 loss: 9.06067498e-07
Iter: 1408 loss: 9.05821764e-07
Iter: 1409 loss: 9.11039763e-07
Iter: 1410 loss: 9.05835861e-07
Iter: 1411 loss: 9.05716263e-07
Iter: 1412 loss: 9.05726324e-07
Iter: 1413 loss: 9.05537092e-07
Iter: 1414 loss: 9.05282548e-07
Iter: 1415 loss: 9.0526e-07
Iter: 1416 loss: 9.05075808e-07
Iter: 1417 loss: 9.06572836e-07
Iter: 1418 loss: 9.05047727e-07
Iter: 1419 loss: 9.04913236e-07
Iter: 1420 loss: 9.06148898e-07
Iter: 1421 loss: 9.04856506e-07
Iter: 1422 loss: 9.04760668e-07
Iter: 1423 loss: 9.04693252e-07
Iter: 1424 loss: 9.04580816e-07
Iter: 1425 loss: 9.04435353e-07
Iter: 1426 loss: 9.04690239e-07
Iter: 1427 loss: 9.04330534e-07
Iter: 1428 loss: 9.0409327e-07
Iter: 1429 loss: 9.03967e-07
Iter: 1430 loss: 9.03901594e-07
Iter: 1431 loss: 9.03968157e-07
Iter: 1432 loss: 9.03814794e-07
Iter: 1433 loss: 9.03705541e-07
Iter: 1434 loss: 9.03465775e-07
Iter: 1435 loss: 9.06412936e-07
Iter: 1436 loss: 9.03428e-07
Iter: 1437 loss: 9.03201339e-07
Iter: 1438 loss: 9.05268848e-07
Iter: 1439 loss: 9.03197702e-07
Iter: 1440 loss: 9.02996305e-07
Iter: 1441 loss: 9.04805631e-07
Iter: 1442 loss: 9.02977376e-07
Iter: 1443 loss: 9.02921215e-07
Iter: 1444 loss: 9.02642967e-07
Iter: 1445 loss: 9.04267722e-07
Iter: 1446 loss: 9.02586862e-07
Iter: 1447 loss: 9.02638931e-07
Iter: 1448 loss: 9.02449187e-07
Iter: 1449 loss: 9.02366878e-07
Iter: 1450 loss: 9.02238639e-07
Iter: 1451 loss: 9.02239435e-07
Iter: 1452 loss: 9.02059583e-07
Iter: 1453 loss: 9.01867793e-07
Iter: 1454 loss: 9.01823455e-07
Iter: 1455 loss: 9.01936289e-07
Iter: 1456 loss: 9.01730914e-07
Iter: 1457 loss: 9.01665487e-07
Iter: 1458 loss: 9.0147887e-07
Iter: 1459 loss: 9.01489557e-07
Iter: 1460 loss: 9.0125e-07
Iter: 1461 loss: 9.01077101e-07
Iter: 1462 loss: 9.00958639e-07
Iter: 1463 loss: 9.00708585e-07
Iter: 1464 loss: 9.02702368e-07
Iter: 1465 loss: 9.00633268e-07
Iter: 1466 loss: 9.00446e-07
Iter: 1467 loss: 9.00458872e-07
Iter: 1468 loss: 9.00278678e-07
Iter: 1469 loss: 9.00004807e-07
Iter: 1470 loss: 9.00003158e-07
Iter: 1471 loss: 9.00009297e-07
Iter: 1472 loss: 8.9991795e-07
Iter: 1473 loss: 8.99835925e-07
Iter: 1474 loss: 8.99651866e-07
Iter: 1475 loss: 9.01818794e-07
Iter: 1476 loss: 8.99654083e-07
Iter: 1477 loss: 8.99464567e-07
Iter: 1478 loss: 9.00587054e-07
Iter: 1479 loss: 8.99433e-07
Iter: 1480 loss: 8.99405279e-07
Iter: 1481 loss: 8.99363044e-07
Iter: 1482 loss: 8.99309e-07
Iter: 1483 loss: 8.99184e-07
Iter: 1484 loss: 8.99444331e-07
Iter: 1485 loss: 8.99098438e-07
Iter: 1486 loss: 8.98904887e-07
Iter: 1487 loss: 8.99793122e-07
Iter: 1488 loss: 8.98912788e-07
Iter: 1489 loss: 8.98759936e-07
Iter: 1490 loss: 8.98807059e-07
Iter: 1491 loss: 8.98678479e-07
Iter: 1492 loss: 8.98587132e-07
Iter: 1493 loss: 8.98564565e-07
Iter: 1494 loss: 8.98456165e-07
Iter: 1495 loss: 8.9847191e-07
Iter: 1496 loss: 8.98369e-07
Iter: 1497 loss: 8.98094e-07
Iter: 1498 loss: 8.9870241e-07
Iter: 1499 loss: 8.98031146e-07
Iter: 1500 loss: 8.97876419e-07
Iter: 1501 loss: 8.97859877e-07
Iter: 1502 loss: 8.97803204e-07
Iter: 1503 loss: 8.97621931e-07
Iter: 1504 loss: 9.00567727e-07
Iter: 1505 loss: 8.97604536e-07
Iter: 1506 loss: 8.97354198e-07
Iter: 1507 loss: 8.99997701e-07
Iter: 1508 loss: 8.9735579e-07
Iter: 1509 loss: 8.97253869e-07
Iter: 1510 loss: 8.9702678e-07
Iter: 1511 loss: 8.9700842e-07
Iter: 1512 loss: 8.96854715e-07
Iter: 1513 loss: 8.9695277e-07
Iter: 1514 loss: 8.96712493e-07
Iter: 1515 loss: 8.96509391e-07
Iter: 1516 loss: 8.96547704e-07
Iter: 1517 loss: 8.96421056e-07
Iter: 1518 loss: 8.96387348e-07
Iter: 1519 loss: 8.96348809e-07
Iter: 1520 loss: 8.96270421e-07
Iter: 1521 loss: 8.96576125e-07
Iter: 1522 loss: 8.96232507e-07
Iter: 1523 loss: 8.96122913e-07
Iter: 1524 loss: 8.96281165e-07
Iter: 1525 loss: 8.96099436e-07
Iter: 1526 loss: 8.95973869e-07
Iter: 1527 loss: 8.96060271e-07
Iter: 1528 loss: 8.9588508e-07
Iter: 1529 loss: 8.95797939e-07
Iter: 1530 loss: 8.95599612e-07
Iter: 1531 loss: 8.95590631e-07
Iter: 1532 loss: 8.95307039e-07
Iter: 1533 loss: 8.96343295e-07
Iter: 1534 loss: 8.95235246e-07
Iter: 1535 loss: 8.95025494e-07
Iter: 1536 loss: 8.94998152e-07
Iter: 1537 loss: 8.94889581e-07
Iter: 1538 loss: 8.94843822e-07
Iter: 1539 loss: 8.94755317e-07
Iter: 1540 loss: 8.94496225e-07
Iter: 1541 loss: 8.94735422e-07
Iter: 1542 loss: 8.94326604e-07
Iter: 1543 loss: 8.94145046e-07
Iter: 1544 loss: 8.94359061e-07
Iter: 1545 loss: 8.94072457e-07
Iter: 1546 loss: 8.93954848e-07
Iter: 1547 loss: 8.95447045e-07
Iter: 1548 loss: 8.93983156e-07
Iter: 1549 loss: 8.93844231e-07
Iter: 1550 loss: 8.9399191e-07
Iter: 1551 loss: 8.93780623e-07
Iter: 1552 loss: 8.93732e-07
Iter: 1553 loss: 8.93710308e-07
Iter: 1554 loss: 8.93651304e-07
Iter: 1555 loss: 8.93558422e-07
Iter: 1556 loss: 8.9488492e-07
Iter: 1557 loss: 8.93560753e-07
Iter: 1558 loss: 8.93452921e-07
Iter: 1559 loss: 8.93294839e-07
Iter: 1560 loss: 8.93297056e-07
Iter: 1561 loss: 8.93177571e-07
Iter: 1562 loss: 8.93774597e-07
Iter: 1563 loss: 8.93106744e-07
Iter: 1564 loss: 8.92963726e-07
Iter: 1565 loss: 8.93291315e-07
Iter: 1566 loss: 8.92943604e-07
Iter: 1567 loss: 8.92797914e-07
Iter: 1568 loss: 8.93641072e-07
Iter: 1569 loss: 8.92750791e-07
Iter: 1570 loss: 8.92641e-07
Iter: 1571 loss: 8.92913874e-07
Iter: 1572 loss: 8.92573439e-07
Iter: 1573 loss: 8.9247078e-07
Iter: 1574 loss: 8.93178139e-07
Iter: 1575 loss: 8.92474191e-07
Iter: 1576 loss: 8.92360106e-07
Iter: 1577 loss: 8.92161438e-07
Iter: 1578 loss: 8.92190656e-07
Iter: 1579 loss: 8.92066396e-07
Iter: 1580 loss: 8.9277512e-07
Iter: 1581 loss: 8.92025923e-07
Iter: 1582 loss: 8.91905e-07
Iter: 1583 loss: 8.92259663e-07
Iter: 1584 loss: 8.91839761e-07
Iter: 1585 loss: 8.9173551e-07
Iter: 1586 loss: 8.91761943e-07
Iter: 1587 loss: 8.91647e-07
Iter: 1588 loss: 8.91569925e-07
Iter: 1589 loss: 8.91544346e-07
Iter: 1590 loss: 8.91489947e-07
Iter: 1591 loss: 8.91404284e-07
Iter: 1592 loss: 8.91359264e-07
Iter: 1593 loss: 8.91275079e-07
Iter: 1594 loss: 8.91750176e-07
Iter: 1595 loss: 8.91276045e-07
Iter: 1596 loss: 8.91217951e-07
Iter: 1597 loss: 8.91290313e-07
Iter: 1598 loss: 8.91209879e-07
Iter: 1599 loss: 8.91068112e-07
Iter: 1600 loss: 8.91303387e-07
Iter: 1601 loss: 8.91031959e-07
Iter: 1602 loss: 8.90907302e-07
Iter: 1603 loss: 8.9161324e-07
Iter: 1604 loss: 8.90899457e-07
Iter: 1605 loss: 8.90810895e-07
Iter: 1606 loss: 8.91090394e-07
Iter: 1607 loss: 8.90771275e-07
Iter: 1608 loss: 8.90698686e-07
Iter: 1609 loss: 8.90565502e-07
Iter: 1610 loss: 8.90573233e-07
Iter: 1611 loss: 8.90421518e-07
Iter: 1612 loss: 8.90504111e-07
Iter: 1613 loss: 8.90337049e-07
Iter: 1614 loss: 8.90152251e-07
Iter: 1615 loss: 8.92778246e-07
Iter: 1616 loss: 8.90157196e-07
Iter: 1617 loss: 8.90075398e-07
Iter: 1618 loss: 8.89866101e-07
Iter: 1619 loss: 8.89868261e-07
Iter: 1620 loss: 8.89734736e-07
Iter: 1621 loss: 8.89749344e-07
Iter: 1622 loss: 8.89649243e-07
Iter: 1623 loss: 8.89911576e-07
Iter: 1624 loss: 8.89620253e-07
Iter: 1625 loss: 8.89536409e-07
Iter: 1626 loss: 8.89458533e-07
Iter: 1627 loss: 8.89448302e-07
Iter: 1628 loss: 8.89400098e-07
Iter: 1629 loss: 8.90586534e-07
Iter: 1630 loss: 8.89365765e-07
Iter: 1631 loss: 8.89274247e-07
Iter: 1632 loss: 8.8937594e-07
Iter: 1633 loss: 8.89251851e-07
Iter: 1634 loss: 8.89163744e-07
Iter: 1635 loss: 8.89573812e-07
Iter: 1636 loss: 8.8916147e-07
Iter: 1637 loss: 8.89082e-07
Iter: 1638 loss: 8.89254409e-07
Iter: 1639 loss: 8.89054775e-07
Iter: 1640 loss: 8.88963484e-07
Iter: 1641 loss: 8.89201829e-07
Iter: 1642 loss: 8.88922e-07
Iter: 1643 loss: 8.88878219e-07
Iter: 1644 loss: 8.88849911e-07
Iter: 1645 loss: 8.88763338e-07
Iter: 1646 loss: 8.88712407e-07
Iter: 1647 loss: 8.88695126e-07
Iter: 1648 loss: 8.8866193e-07
Iter: 1649 loss: 8.88577347e-07
Iter: 1650 loss: 8.88556542e-07
Iter: 1651 loss: 8.88446777e-07
Iter: 1652 loss: 8.88489581e-07
Iter: 1653 loss: 8.88369868e-07
Iter: 1654 loss: 8.88191892e-07
Iter: 1655 loss: 8.89773219e-07
Iter: 1656 loss: 8.88200248e-07
Iter: 1657 loss: 8.88061663e-07
Iter: 1658 loss: 8.88027e-07
Iter: 1659 loss: 8.87974238e-07
Iter: 1660 loss: 8.87832584e-07
Iter: 1661 loss: 8.88178363e-07
Iter: 1662 loss: 8.8778279e-07
Iter: 1663 loss: 8.87511305e-07
Iter: 1664 loss: 8.89042553e-07
Iter: 1665 loss: 8.87492661e-07
Iter: 1666 loss: 8.8736931e-07
Iter: 1667 loss: 8.88067348e-07
Iter: 1668 loss: 8.87395686e-07
Iter: 1669 loss: 8.87278816e-07
Iter: 1670 loss: 8.87776821e-07
Iter: 1671 loss: 8.87255055e-07
Iter: 1672 loss: 8.87157512e-07
Iter: 1673 loss: 8.87309398e-07
Iter: 1674 loss: 8.87104704e-07
Iter: 1675 loss: 8.87073384e-07
Iter: 1676 loss: 8.87090437e-07
Iter: 1677 loss: 8.87023816e-07
Iter: 1678 loss: 8.86928717e-07
Iter: 1679 loss: 8.86956627e-07
Iter: 1680 loss: 8.86899556e-07
Iter: 1681 loss: 8.868289e-07
Iter: 1682 loss: 8.86822932e-07
Iter: 1683 loss: 8.86707483e-07
Iter: 1684 loss: 8.86698672e-07
Iter: 1685 loss: 8.86623639e-07
Iter: 1686 loss: 8.86508133e-07
Iter: 1687 loss: 8.86504154e-07
Iter: 1688 loss: 8.86391717e-07
Iter: 1689 loss: 8.86177872e-07
Iter: 1690 loss: 8.89581031e-07
Iter: 1691 loss: 8.86195778e-07
Iter: 1692 loss: 8.85984491e-07
Iter: 1693 loss: 8.86807754e-07
Iter: 1694 loss: 8.85903887e-07
Iter: 1695 loss: 8.8570215e-07
Iter: 1696 loss: 8.87069348e-07
Iter: 1697 loss: 8.85700842e-07
Iter: 1698 loss: 8.85552311e-07
Iter: 1699 loss: 8.85739951e-07
Iter: 1700 loss: 8.85452607e-07
Iter: 1701 loss: 8.85329882e-07
Iter: 1702 loss: 8.85326699e-07
Iter: 1703 loss: 8.85226655e-07
Iter: 1704 loss: 8.85296345e-07
Iter: 1705 loss: 8.85174302e-07
Iter: 1706 loss: 8.85042823e-07
Iter: 1707 loss: 8.84993256e-07
Iter: 1708 loss: 8.84963924e-07
Iter: 1709 loss: 8.84881445e-07
Iter: 1710 loss: 8.8488332e-07
Iter: 1711 loss: 8.84816131e-07
Iter: 1712 loss: 8.84726262e-07
Iter: 1713 loss: 8.84681185e-07
Iter: 1714 loss: 8.84631334e-07
Iter: 1715 loss: 8.84839039e-07
Iter: 1716 loss: 8.84569829e-07
Iter: 1717 loss: 8.8452714e-07
Iter: 1718 loss: 8.84481494e-07
Iter: 1719 loss: 8.84451083e-07
Iter: 1720 loss: 8.8438469e-07
Iter: 1721 loss: 8.84369967e-07
Iter: 1722 loss: 8.84302437e-07
Iter: 1723 loss: 8.84466544e-07
Iter: 1724 loss: 8.8426566e-07
Iter: 1725 loss: 8.84175222e-07
Iter: 1726 loss: 8.84788903e-07
Iter: 1727 loss: 8.84203416e-07
Iter: 1728 loss: 8.84161e-07
Iter: 1729 loss: 8.8421632e-07
Iter: 1730 loss: 8.84116616e-07
Iter: 1731 loss: 8.84064264e-07
Iter: 1732 loss: 8.84575286e-07
Iter: 1733 loss: 8.84053406e-07
Iter: 1734 loss: 8.83966436e-07
Iter: 1735 loss: 8.83901293e-07
Iter: 1736 loss: 8.83883274e-07
Iter: 1737 loss: 8.83791927e-07
Iter: 1738 loss: 8.83811822e-07
Iter: 1739 loss: 8.83681878e-07
Iter: 1740 loss: 8.83597068e-07
Iter: 1741 loss: 8.84187273e-07
Iter: 1742 loss: 8.83600705e-07
Iter: 1743 loss: 8.83466157e-07
Iter: 1744 loss: 8.83663063e-07
Iter: 1745 loss: 8.834146e-07
Iter: 1746 loss: 8.83305177e-07
Iter: 1747 loss: 8.83229745e-07
Iter: 1748 loss: 8.83170173e-07
Iter: 1749 loss: 8.83154371e-07
Iter: 1750 loss: 8.83122937e-07
Iter: 1751 loss: 8.82993049e-07
Iter: 1752 loss: 8.8306e-07
Iter: 1753 loss: 8.82966674e-07
Iter: 1754 loss: 8.82900849e-07
Iter: 1755 loss: 8.82906306e-07
Iter: 1756 loss: 8.82814675e-07
Iter: 1757 loss: 8.82745837e-07
Iter: 1758 loss: 8.8275732e-07
Iter: 1759 loss: 8.82712186e-07
Iter: 1760 loss: 8.82624e-07
Iter: 1761 loss: 8.82644372e-07
Iter: 1762 loss: 8.82546942e-07
Iter: 1763 loss: 8.83230371e-07
Iter: 1764 loss: 8.82534152e-07
Iter: 1765 loss: 8.82484755e-07
Iter: 1766 loss: 8.82663244e-07
Iter: 1767 loss: 8.82425525e-07
Iter: 1768 loss: 8.82392101e-07
Iter: 1769 loss: 8.82324684e-07
Iter: 1770 loss: 8.82296945e-07
Iter: 1771 loss: 8.82168251e-07
Iter: 1772 loss: 8.82679387e-07
Iter: 1773 loss: 8.82050927e-07
Iter: 1774 loss: 8.81972369e-07
Iter: 1775 loss: 8.83135783e-07
Iter: 1776 loss: 8.81994e-07
Iter: 1777 loss: 8.81942071e-07
Iter: 1778 loss: 8.8179786e-07
Iter: 1779 loss: 8.83161704e-07
Iter: 1780 loss: 8.81765231e-07
Iter: 1781 loss: 8.81612436e-07
Iter: 1782 loss: 8.82529548e-07
Iter: 1783 loss: 8.81567075e-07
Iter: 1784 loss: 8.8141104e-07
Iter: 1785 loss: 8.81387734e-07
Iter: 1786 loss: 8.81367384e-07
Iter: 1787 loss: 8.81148139e-07
Iter: 1788 loss: 8.84258043e-07
Iter: 1789 loss: 8.81139556e-07
Iter: 1790 loss: 8.8106e-07
Iter: 1791 loss: 8.81096071e-07
Iter: 1792 loss: 8.81032861e-07
Iter: 1793 loss: 8.8100694e-07
Iter: 1794 loss: 8.8090826e-07
Iter: 1795 loss: 8.80842663e-07
Iter: 1796 loss: 8.81236531e-07
Iter: 1797 loss: 8.80792754e-07
Iter: 1798 loss: 8.80738867e-07
Iter: 1799 loss: 8.8074114e-07
Iter: 1800 loss: 8.80703737e-07
Iter: 1801 loss: 8.80662128e-07
Iter: 1802 loss: 8.80660764e-07
Iter: 1803 loss: 8.80596872e-07
Iter: 1804 loss: 8.80750804e-07
Iter: 1805 loss: 8.80560208e-07
Iter: 1806 loss: 8.80496373e-07
Iter: 1807 loss: 8.8123619e-07
Iter: 1808 loss: 8.80508651e-07
Iter: 1809 loss: 8.80483753e-07
Iter: 1810 loss: 8.80271614e-07
Iter: 1811 loss: 8.81081121e-07
Iter: 1812 loss: 8.80238758e-07
Iter: 1813 loss: 8.80035827e-07
Iter: 1814 loss: 8.80739549e-07
Iter: 1815 loss: 8.80001835e-07
Iter: 1816 loss: 8.79816412e-07
Iter: 1817 loss: 8.81114943e-07
Iter: 1818 loss: 8.79823119e-07
Iter: 1819 loss: 8.79626327e-07
Iter: 1820 loss: 8.80807931e-07
Iter: 1821 loss: 8.79598701e-07
Iter: 1822 loss: 8.79559252e-07
Iter: 1823 loss: 8.79427319e-07
Iter: 1824 loss: 8.81872836e-07
Iter: 1825 loss: 8.79445622e-07
Iter: 1826 loss: 8.79344384e-07
Iter: 1827 loss: 8.79338359e-07
Iter: 1828 loss: 8.79305844e-07
Iter: 1829 loss: 8.79295953e-07
Iter: 1830 loss: 8.79226661e-07
Iter: 1831 loss: 8.79220806e-07
Iter: 1832 loss: 8.79661741e-07
Iter: 1833 loss: 8.79192385e-07
Iter: 1834 loss: 8.7911252e-07
Iter: 1835 loss: 8.79095296e-07
Iter: 1836 loss: 8.79081881e-07
Iter: 1837 loss: 8.79023958e-07
Iter: 1838 loss: 8.79459321e-07
Iter: 1839 loss: 8.79021172e-07
Iter: 1840 loss: 8.78974106e-07
Iter: 1841 loss: 8.79178856e-07
Iter: 1842 loss: 8.78961259e-07
Iter: 1843 loss: 8.78897e-07
Iter: 1844 loss: 8.78812671e-07
Iter: 1845 loss: 8.78810624e-07
Iter: 1846 loss: 8.78732862e-07
Iter: 1847 loss: 8.7867e-07
Iter: 1848 loss: 8.7862071e-07
Iter: 1849 loss: 8.78506455e-07
Iter: 1850 loss: 8.78863148e-07
Iter: 1851 loss: 8.78453136e-07
Iter: 1852 loss: 8.78425283e-07
Iter: 1853 loss: 8.78413857e-07
Iter: 1854 loss: 8.78339108e-07
Iter: 1855 loss: 8.78223432e-07
Iter: 1856 loss: 8.79577954e-07
Iter: 1857 loss: 8.78196829e-07
Iter: 1858 loss: 8.7810588e-07
Iter: 1859 loss: 8.79263723e-07
Iter: 1860 loss: 8.7809542e-07
Iter: 1861 loss: 8.77958e-07
Iter: 1862 loss: 8.78371566e-07
Iter: 1863 loss: 8.77946832e-07
Iter: 1864 loss: 8.77845082e-07
Iter: 1865 loss: 8.77706498e-07
Iter: 1866 loss: 8.77709454e-07
Iter: 1867 loss: 8.77721732e-07
Iter: 1868 loss: 8.77618731e-07
Iter: 1869 loss: 8.77626e-07
Iter: 1870 loss: 8.77587468e-07
Iter: 1871 loss: 8.7760759e-07
Iter: 1872 loss: 8.7759463e-07
Iter: 1873 loss: 8.77634875e-07
Iter: 1874 loss: 8.77614923e-07
Iter: 1875 loss: 8.77632033e-07
Iter: 1876 loss: 8.77598211e-07
Iter: 1877 loss: 8.77607135e-07
Iter: 1878 loss: 8.77609182e-07
Iter: 1879 loss: 8.77614468e-07
Iter: 1880 loss: 8.77623052e-07
Iter: 1881 loss: 8.77615435e-07
Iter: 1882 loss: 8.77612877e-07
Iter: 1883 loss: 8.77618845e-07
Iter: 1884 loss: 8.77623e-07
Iter: 1885 loss: 8.77619186e-07
Iter: 1886 loss: 8.77622824e-07
Iter: 1887 loss: 8.77618504e-07
Iter: 1888 loss: 8.77619129e-07
Iter: 1889 loss: 8.77622938e-07
Iter: 1890 loss: 8.77623393e-07
Iter: 1891 loss: 8.77619129e-07
Iter: 1892 loss: 8.77623393e-07
Iter: 1893 loss: 8.77623393e-07
Iter: 1894 loss: 8.77619129e-07
Iter: 1895 loss: 8.774565e-07
Iter: 1896 loss: 8.78521291e-07
Iter: 1897 loss: 8.77466164e-07
Iter: 1898 loss: 8.77422394e-07
Iter: 1899 loss: 8.77394e-07
Iter: 1900 loss: 8.77351e-07
Iter: 1901 loss: 8.77485661e-07
Iter: 1902 loss: 8.77312459e-07
Iter: 1903 loss: 8.77294326e-07
Iter: 1904 loss: 8.77203775e-07
Iter: 1905 loss: 8.77802506e-07
Iter: 1906 loss: 8.77215371e-07
Iter: 1907 loss: 8.77111e-07
Iter: 1908 loss: 8.77115667e-07
Iter: 1909 loss: 8.76994704e-07
Iter: 1910 loss: 8.76993965e-07
Iter: 1911 loss: 8.76930244e-07
Iter: 1912 loss: 8.76913532e-07
Iter: 1913 loss: 8.76743059e-07
Iter: 1914 loss: 8.77001298e-07
Iter: 1915 loss: 8.76647562e-07
Iter: 1916 loss: 8.76613512e-07
Iter: 1917 loss: 8.76588501e-07
Iter: 1918 loss: 8.76488798e-07
Iter: 1919 loss: 8.76559909e-07
Iter: 1920 loss: 8.76441732e-07
Iter: 1921 loss: 8.76401771e-07
Iter: 1922 loss: 8.76612717e-07
Iter: 1923 loss: 8.76383297e-07
Iter: 1924 loss: 8.76290187e-07
Iter: 1925 loss: 8.76499371e-07
Iter: 1926 loss: 8.76246361e-07
Iter: 1927 loss: 8.76243689e-07
Iter: 1928 loss: 8.76225727e-07
Iter: 1929 loss: 8.7623846e-07
Iter: 1930 loss: 8.7618082e-07
Iter: 1931 loss: 8.76132049e-07
Iter: 1932 loss: 8.76142281e-07
Iter: 1933 loss: 8.76113404e-07
Iter: 1934 loss: 8.76105219e-07
Iter: 1935 loss: 8.76079071e-07
Iter: 1936 loss: 8.76017396e-07
Iter: 1937 loss: 8.7602109e-07
Iter: 1938 loss: 8.759514e-07
Iter: 1939 loss: 8.75737e-07
Iter: 1940 loss: 8.78403796e-07
Iter: 1941 loss: 8.75685828e-07
Iter: 1942 loss: 8.75502e-07
Iter: 1943 loss: 8.76945364e-07
Iter: 1944 loss: 8.7549347e-07
Iter: 1945 loss: 8.75292756e-07
Iter: 1946 loss: 8.7734827e-07
Iter: 1947 loss: 8.75272917e-07
Iter: 1948 loss: 8.75156957e-07
Iter: 1949 loss: 8.75116712e-07
Iter: 1950 loss: 8.75063506e-07
Iter: 1951 loss: 8.74982788e-07
Iter: 1952 loss: 8.74972272e-07
Iter: 1953 loss: 8.749318e-07
Iter: 1954 loss: 8.74868078e-07
Iter: 1955 loss: 8.74831358e-07
Iter: 1956 loss: 8.74761099e-07
Iter: 1957 loss: 8.74735804e-07
Iter: 1958 loss: 8.74745467e-07
Iter: 1959 loss: 8.74767295e-07
Iter: 1960 loss: 8.74761554e-07
Iter: 1961 loss: 8.74731654e-07
Iter: 1962 loss: 8.74753e-07
Iter: 1963 loss: 8.7473893e-07
Iter: 1964 loss: 8.74734553e-07
Iter: 1965 loss: 8.74740351e-07
Iter: 1966 loss: 8.74755301e-07
Iter: 1967 loss: 8.74732677e-07
Iter: 1968 loss: 8.74723469e-07
Iter: 1969 loss: 8.74739669e-07
Iter: 1970 loss: 8.74735406e-07
Iter: 1971 loss: 8.74732677e-07
Iter: 1972 loss: 8.74732279e-07
Iter: 1973 loss: 8.74739726e-07
Iter: 1974 loss: 8.74733189e-07
Iter: 1975 loss: 8.74733132e-07
Iter: 1976 loss: 8.74736e-07
Iter: 1977 loss: 8.74737907e-07
Iter: 1978 loss: 8.74738e-07
Iter: 1979 loss: 8.74736e-07
Iter: 1980 loss: 8.74736486e-07
Iter: 1981 loss: 8.74738e-07
Iter: 1982 loss: 8.74736486e-07
Iter: 1983 loss: 8.74736486e-07
Iter: 1984 loss: 8.74736486e-07
Iter: 1985 loss: 8.74738e-07
Iter: 1986 loss: 8.74639568e-07
Iter: 1987 loss: 8.75514218e-07
Iter: 1988 loss: 8.746e-07
Iter: 1989 loss: 8.74538841e-07
Iter: 1990 loss: 8.74491718e-07
Iter: 1991 loss: 8.74491661e-07
Iter: 1992 loss: 8.74372518e-07
Iter: 1993 loss: 8.74380248e-07
Iter: 1994 loss: 8.74324e-07
Iter: 1995 loss: 8.74249054e-07
Iter: 1996 loss: 8.74249963e-07
Iter: 1997 loss: 8.74160207e-07
Iter: 1998 loss: 8.74368482e-07
Iter: 1999 loss: 8.74118768e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi0.4
+ date
Sun Nov  8 03:56:08 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0/300_100_100_100_1 --function f1 --psi -1 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31ebd461e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31ebda2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31ebd46730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31ebe6ee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31ebe6f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31ebe6fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31be0806a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31bdfad510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31bdfad488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31bdfd7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31980698c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3198042840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3198042b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31bdfe96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3180103950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3180119048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31801192f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31801191e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31be0547b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31be045f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31800648c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f313472d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3134711730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31346f6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31346f6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f313477a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3180095620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f313466c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f313466c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f313466c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31346317b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3134622730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3134622e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3134616840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31346af840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f313458a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.00599914
test_loss: 0.006276992
train_loss: 0.003273906
test_loss: 0.0032117658
train_loss: 0.0028658644
test_loss: 0.0028679452
train_loss: 0.0027546734
test_loss: 0.0028478422
train_loss: 0.0026781342
test_loss: 0.0026245029
train_loss: 0.0026869557
test_loss: 0.0027276184
train_loss: 0.0024702447
test_loss: 0.002496425
train_loss: 0.0023683088
test_loss: 0.0027217583
train_loss: 0.0024372262
test_loss: 0.0024783167
train_loss: 0.0022911385
test_loss: 0.002470125
train_loss: 0.0023262326
test_loss: 0.0022749242
train_loss: 0.0025017946
test_loss: 0.002658238
train_loss: 0.002372366
test_loss: 0.002664277
train_loss: 0.002407352
test_loss: 0.0024965901
train_loss: 0.0021882826
test_loss: 0.0023532065
train_loss: 0.002157804
test_loss: 0.0022280377
train_loss: 0.002204125
test_loss: 0.0022599036
train_loss: 0.0022341558
test_loss: 0.0022944384
train_loss: 0.002332773
test_loss: 0.0023175583
train_loss: 0.0020737285
test_loss: 0.0020534901
train_loss: 0.0022649416
test_loss: 0.002198475
train_loss: 0.0021423763
test_loss: 0.0024514154
train_loss: 0.002140001
test_loss: 0.0022111712
train_loss: 0.0022287113
test_loss: 0.002368017
train_loss: 0.0021912241
test_loss: 0.0021911615
train_loss: 0.002246931
test_loss: 0.0022216712
train_loss: 0.0022605548
test_loss: 0.0024514839
train_loss: 0.0020162913
test_loss: 0.002119042
train_loss: 0.0022112255
test_loss: 0.0023020536
train_loss: 0.0020662122
test_loss: 0.0023092586
train_loss: 0.0020425695
test_loss: 0.002218006
train_loss: 0.002041567
test_loss: 0.0022434872
train_loss: 0.002241699
test_loss: 0.0021461998
train_loss: 0.0021966316
test_loss: 0.0021775668
train_loss: 0.002277111
test_loss: 0.0022089123
train_loss: 0.0023165043
test_loss: 0.0022715651
train_loss: 0.0020194217
test_loss: 0.0022664547
train_loss: 0.0019675933
test_loss: 0.00226951
train_loss: 0.0019595865
test_loss: 0.002331579
train_loss: 0.0021006684
test_loss: 0.002153723
train_loss: 0.002112917
test_loss: 0.0021239594
train_loss: 0.0019587325
test_loss: 0.0021122347
train_loss: 0.0020941386
test_loss: 0.0021755728
train_loss: 0.002033163
test_loss: 0.0022472304
train_loss: 0.0021808543
test_loss: 0.0022269667
train_loss: 0.0023224538
test_loss: 0.0021014113
train_loss: 0.0019339971
test_loss: 0.002199819
train_loss: 0.0021334419
test_loss: 0.0022673297
train_loss: 0.0019645686
test_loss: 0.0021097288
train_loss: 0.0019651838
test_loss: 0.002178639
train_loss: 0.0021460208
test_loss: 0.0021493898
train_loss: 0.0020672972
test_loss: 0.0020077298
train_loss: 0.002155433
test_loss: 0.0022545327
train_loss: 0.0019208821
test_loss: 0.0020816734
train_loss: 0.0021680524
test_loss: 0.0021941713
train_loss: 0.0019656683
test_loss: 0.0020150198
train_loss: 0.001977629
test_loss: 0.002174655
train_loss: 0.0021383252
test_loss: 0.0023858391
train_loss: 0.0018784063
test_loss: 0.0020165192
train_loss: 0.002036817
test_loss: 0.0021581391
train_loss: 0.0019354459
test_loss: 0.0019577083
train_loss: 0.0020349864
test_loss: 0.0021268332
train_loss: 0.001962841
test_loss: 0.0020391496
train_loss: 0.0020093266
test_loss: 0.0021301985
train_loss: 0.0020755285
test_loss: 0.0022290621
train_loss: 0.0020675166
test_loss: 0.0021325473
train_loss: 0.002131173
test_loss: 0.0021349946
train_loss: 0.0020371696
test_loss: 0.0020806321
train_loss: 0.0022338529
test_loss: 0.0021069662
train_loss: 0.0018447968
test_loss: 0.0020002217
train_loss: 0.0019908655
test_loss: 0.0020724356
train_loss: 0.002029731
test_loss: 0.0020878287
train_loss: 0.0019100839
test_loss: 0.0020405464
train_loss: 0.0020435012
test_loss: 0.002767299
train_loss: 0.0019939723
test_loss: 0.0022450113
train_loss: 0.0019096698
test_loss: 0.0020838706
train_loss: 0.0019393754
test_loss: 0.0019905537
train_loss: 0.0018492637
test_loss: 0.0020736754
train_loss: 0.0018672612
test_loss: 0.0020499998
train_loss: 0.0018966474
test_loss: 0.002292658
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi0.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -1 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi0.4/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3194c51e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd319526e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd319526ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd31946ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd319476510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd319476ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd31938cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd319350730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd31936a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd31936ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3192c0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3192ef378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3192de048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd319287a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd319287c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd319240378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd319274510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd319274f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3191efa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3191ef268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3191f0620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d7a7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d75a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d75a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d77a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d77aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d6e7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d7100d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d77a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d6cf598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d66c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d66c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d663730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d618ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d5d56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd30d5a7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 7.53066342e-06
Iter: 2 loss: 2.91278e-05
Iter: 3 loss: 5.18809247e-06
Iter: 4 loss: 4.85212513e-06
Iter: 5 loss: 4.80365043e-06
Iter: 6 loss: 4.56844646e-06
Iter: 7 loss: 4.42261035e-06
Iter: 8 loss: 4.42222199e-06
Iter: 9 loss: 4.29459124e-06
Iter: 10 loss: 4.13644739e-06
Iter: 11 loss: 4.12292047e-06
Iter: 12 loss: 3.96557289e-06
Iter: 13 loss: 5.3319427e-06
Iter: 14 loss: 3.95744701e-06
Iter: 15 loss: 3.86739111e-06
Iter: 16 loss: 4.96137363e-06
Iter: 17 loss: 3.86635929e-06
Iter: 18 loss: 3.83780025e-06
Iter: 19 loss: 3.76621961e-06
Iter: 20 loss: 4.43823274e-06
Iter: 21 loss: 3.75602622e-06
Iter: 22 loss: 3.69170812e-06
Iter: 23 loss: 3.68855012e-06
Iter: 24 loss: 3.63311028e-06
Iter: 25 loss: 3.53999985e-06
Iter: 26 loss: 3.53979158e-06
Iter: 27 loss: 3.44551654e-06
Iter: 28 loss: 3.32894251e-06
Iter: 29 loss: 3.31873775e-06
Iter: 30 loss: 3.20272e-06
Iter: 31 loss: 3.49498987e-06
Iter: 32 loss: 3.16235401e-06
Iter: 33 loss: 3.07718165e-06
Iter: 34 loss: 3.11448161e-06
Iter: 35 loss: 3.01909267e-06
Iter: 36 loss: 2.96733515e-06
Iter: 37 loss: 2.94796337e-06
Iter: 38 loss: 2.9146413e-06
Iter: 39 loss: 2.874762e-06
Iter: 40 loss: 2.87066155e-06
Iter: 41 loss: 2.82963015e-06
Iter: 42 loss: 3.44895466e-06
Iter: 43 loss: 2.82969017e-06
Iter: 44 loss: 2.78696689e-06
Iter: 45 loss: 2.77434037e-06
Iter: 46 loss: 2.74876e-06
Iter: 47 loss: 2.71395584e-06
Iter: 48 loss: 2.95100722e-06
Iter: 49 loss: 2.71060048e-06
Iter: 50 loss: 2.67298196e-06
Iter: 51 loss: 2.70967212e-06
Iter: 52 loss: 2.65184053e-06
Iter: 53 loss: 2.61493415e-06
Iter: 54 loss: 2.62354274e-06
Iter: 55 loss: 2.58796263e-06
Iter: 56 loss: 2.56959083e-06
Iter: 57 loss: 2.56542057e-06
Iter: 58 loss: 2.54751512e-06
Iter: 59 loss: 2.51777806e-06
Iter: 60 loss: 2.51765118e-06
Iter: 61 loss: 2.47915295e-06
Iter: 62 loss: 2.47280468e-06
Iter: 63 loss: 2.44621583e-06
Iter: 64 loss: 2.4001306e-06
Iter: 65 loss: 2.52421341e-06
Iter: 66 loss: 2.3848736e-06
Iter: 67 loss: 2.34485378e-06
Iter: 68 loss: 2.94988786e-06
Iter: 69 loss: 2.34479944e-06
Iter: 70 loss: 2.30142541e-06
Iter: 71 loss: 2.30026671e-06
Iter: 72 loss: 2.26636575e-06
Iter: 73 loss: 2.2306167e-06
Iter: 74 loss: 2.33160154e-06
Iter: 75 loss: 2.21932714e-06
Iter: 76 loss: 2.19264075e-06
Iter: 77 loss: 2.19219e-06
Iter: 78 loss: 2.17913725e-06
Iter: 79 loss: 2.16435478e-06
Iter: 80 loss: 2.16238595e-06
Iter: 81 loss: 2.1510948e-06
Iter: 82 loss: 2.15039177e-06
Iter: 83 loss: 2.14116699e-06
Iter: 84 loss: 2.11907718e-06
Iter: 85 loss: 2.36105893e-06
Iter: 86 loss: 2.11681481e-06
Iter: 87 loss: 2.10407916e-06
Iter: 88 loss: 2.10292751e-06
Iter: 89 loss: 2.09326095e-06
Iter: 90 loss: 2.13311728e-06
Iter: 91 loss: 2.09119548e-06
Iter: 92 loss: 2.08285792e-06
Iter: 93 loss: 2.05761921e-06
Iter: 94 loss: 2.12383429e-06
Iter: 95 loss: 2.04378921e-06
Iter: 96 loss: 2.00247405e-06
Iter: 97 loss: 2.33314427e-06
Iter: 98 loss: 1.99963642e-06
Iter: 99 loss: 1.96812789e-06
Iter: 100 loss: 2.02204319e-06
Iter: 101 loss: 1.95403413e-06
Iter: 102 loss: 1.91649178e-06
Iter: 103 loss: 2.40431427e-06
Iter: 104 loss: 1.91629033e-06
Iter: 105 loss: 1.90124513e-06
Iter: 106 loss: 1.89526577e-06
Iter: 107 loss: 1.8872006e-06
Iter: 108 loss: 1.87881267e-06
Iter: 109 loss: 1.87775584e-06
Iter: 110 loss: 1.86906823e-06
Iter: 111 loss: 1.85338968e-06
Iter: 112 loss: 2.23607958e-06
Iter: 113 loss: 1.85342253e-06
Iter: 114 loss: 1.84677435e-06
Iter: 115 loss: 1.84611747e-06
Iter: 116 loss: 1.83988993e-06
Iter: 117 loss: 1.83614247e-06
Iter: 118 loss: 1.8336325e-06
Iter: 119 loss: 1.82608733e-06
Iter: 120 loss: 1.82825545e-06
Iter: 121 loss: 1.82070232e-06
Iter: 122 loss: 1.81309167e-06
Iter: 123 loss: 1.81294797e-06
Iter: 124 loss: 1.80769439e-06
Iter: 125 loss: 1.79734775e-06
Iter: 126 loss: 2.00344402e-06
Iter: 127 loss: 1.79725168e-06
Iter: 128 loss: 1.78258551e-06
Iter: 129 loss: 1.7844925e-06
Iter: 130 loss: 1.77150673e-06
Iter: 131 loss: 1.7584407e-06
Iter: 132 loss: 1.89290733e-06
Iter: 133 loss: 1.75808486e-06
Iter: 134 loss: 1.75033767e-06
Iter: 135 loss: 1.75030289e-06
Iter: 136 loss: 1.74302545e-06
Iter: 137 loss: 1.72873865e-06
Iter: 138 loss: 2.01434455e-06
Iter: 139 loss: 1.72858665e-06
Iter: 140 loss: 1.72005082e-06
Iter: 141 loss: 1.72000068e-06
Iter: 142 loss: 1.71140198e-06
Iter: 143 loss: 1.73190938e-06
Iter: 144 loss: 1.70819271e-06
Iter: 145 loss: 1.70179374e-06
Iter: 146 loss: 1.69910845e-06
Iter: 147 loss: 1.69583166e-06
Iter: 148 loss: 1.688466e-06
Iter: 149 loss: 1.68837676e-06
Iter: 150 loss: 1.68479437e-06
Iter: 151 loss: 1.67635972e-06
Iter: 152 loss: 1.775074e-06
Iter: 153 loss: 1.67561871e-06
Iter: 154 loss: 1.67062899e-06
Iter: 155 loss: 1.67009637e-06
Iter: 156 loss: 1.66501684e-06
Iter: 157 loss: 1.67128678e-06
Iter: 158 loss: 1.6622821e-06
Iter: 159 loss: 1.65802351e-06
Iter: 160 loss: 1.65130666e-06
Iter: 161 loss: 1.65118615e-06
Iter: 162 loss: 1.64186122e-06
Iter: 163 loss: 1.68190479e-06
Iter: 164 loss: 1.63994423e-06
Iter: 165 loss: 1.63467712e-06
Iter: 166 loss: 1.71638703e-06
Iter: 167 loss: 1.63468485e-06
Iter: 168 loss: 1.62879928e-06
Iter: 169 loss: 1.62756328e-06
Iter: 170 loss: 1.62367473e-06
Iter: 171 loss: 1.61548235e-06
Iter: 172 loss: 1.610764e-06
Iter: 173 loss: 1.60723471e-06
Iter: 174 loss: 1.60404034e-06
Iter: 175 loss: 1.60107061e-06
Iter: 176 loss: 1.59731633e-06
Iter: 177 loss: 1.58713397e-06
Iter: 178 loss: 1.65328947e-06
Iter: 179 loss: 1.58463126e-06
Iter: 180 loss: 1.58102e-06
Iter: 181 loss: 1.57824707e-06
Iter: 182 loss: 1.57443458e-06
Iter: 183 loss: 1.57068405e-06
Iter: 184 loss: 1.56989972e-06
Iter: 185 loss: 1.56512522e-06
Iter: 186 loss: 1.56290935e-06
Iter: 187 loss: 1.5605674e-06
Iter: 188 loss: 1.55449675e-06
Iter: 189 loss: 1.55436351e-06
Iter: 190 loss: 1.5517553e-06
Iter: 191 loss: 1.54854865e-06
Iter: 192 loss: 1.5482608e-06
Iter: 193 loss: 1.54322538e-06
Iter: 194 loss: 1.53602014e-06
Iter: 195 loss: 1.53574263e-06
Iter: 196 loss: 1.5270806e-06
Iter: 197 loss: 1.65064171e-06
Iter: 198 loss: 1.5270582e-06
Iter: 199 loss: 1.5220437e-06
Iter: 200 loss: 1.5963451e-06
Iter: 201 loss: 1.52207303e-06
Iter: 202 loss: 1.51844665e-06
Iter: 203 loss: 1.51059658e-06
Iter: 204 loss: 1.62521656e-06
Iter: 205 loss: 1.51016934e-06
Iter: 206 loss: 1.50720871e-06
Iter: 207 loss: 1.50614778e-06
Iter: 208 loss: 1.50247479e-06
Iter: 209 loss: 1.50221922e-06
Iter: 210 loss: 1.49940763e-06
Iter: 211 loss: 1.49664174e-06
Iter: 212 loss: 1.50879737e-06
Iter: 213 loss: 1.49602749e-06
Iter: 214 loss: 1.49309335e-06
Iter: 215 loss: 1.50933602e-06
Iter: 216 loss: 1.49265213e-06
Iter: 217 loss: 1.49136235e-06
Iter: 218 loss: 1.48877109e-06
Iter: 219 loss: 1.53806695e-06
Iter: 220 loss: 1.4887346e-06
Iter: 221 loss: 1.48631955e-06
Iter: 222 loss: 1.48629545e-06
Iter: 223 loss: 1.48378194e-06
Iter: 224 loss: 1.48083268e-06
Iter: 225 loss: 1.48050367e-06
Iter: 226 loss: 1.4772429e-06
Iter: 227 loss: 1.47870583e-06
Iter: 228 loss: 1.47499509e-06
Iter: 229 loss: 1.46938464e-06
Iter: 230 loss: 1.47116327e-06
Iter: 231 loss: 1.46547643e-06
Iter: 232 loss: 1.46323896e-06
Iter: 233 loss: 1.46179877e-06
Iter: 234 loss: 1.45923264e-06
Iter: 235 loss: 1.45828676e-06
Iter: 236 loss: 1.45686658e-06
Iter: 237 loss: 1.45364208e-06
Iter: 238 loss: 1.45567924e-06
Iter: 239 loss: 1.45165882e-06
Iter: 240 loss: 1.44740807e-06
Iter: 241 loss: 1.50282665e-06
Iter: 242 loss: 1.44733406e-06
Iter: 243 loss: 1.44594583e-06
Iter: 244 loss: 1.44411877e-06
Iter: 245 loss: 1.44399883e-06
Iter: 246 loss: 1.44149601e-06
Iter: 247 loss: 1.47415119e-06
Iter: 248 loss: 1.44153364e-06
Iter: 249 loss: 1.43938769e-06
Iter: 250 loss: 1.43522198e-06
Iter: 251 loss: 1.5116334e-06
Iter: 252 loss: 1.43515945e-06
Iter: 253 loss: 1.43246893e-06
Iter: 254 loss: 1.47289188e-06
Iter: 255 loss: 1.43249031e-06
Iter: 256 loss: 1.42952774e-06
Iter: 257 loss: 1.43295665e-06
Iter: 258 loss: 1.42793624e-06
Iter: 259 loss: 1.42543809e-06
Iter: 260 loss: 1.42429496e-06
Iter: 261 loss: 1.4230535e-06
Iter: 262 loss: 1.41985254e-06
Iter: 263 loss: 1.43122406e-06
Iter: 264 loss: 1.41908504e-06
Iter: 265 loss: 1.41659746e-06
Iter: 266 loss: 1.44915e-06
Iter: 267 loss: 1.41660644e-06
Iter: 268 loss: 1.41422811e-06
Iter: 269 loss: 1.41597275e-06
Iter: 270 loss: 1.41271676e-06
Iter: 271 loss: 1.41040471e-06
Iter: 272 loss: 1.40831571e-06
Iter: 273 loss: 1.40777593e-06
Iter: 274 loss: 1.40477437e-06
Iter: 275 loss: 1.40471479e-06
Iter: 276 loss: 1.40231168e-06
Iter: 277 loss: 1.39877557e-06
Iter: 278 loss: 1.39871e-06
Iter: 279 loss: 1.39676831e-06
Iter: 280 loss: 1.39674216e-06
Iter: 281 loss: 1.39460735e-06
Iter: 282 loss: 1.39174017e-06
Iter: 283 loss: 1.39156782e-06
Iter: 284 loss: 1.38913788e-06
Iter: 285 loss: 1.39955375e-06
Iter: 286 loss: 1.38856512e-06
Iter: 287 loss: 1.38779478e-06
Iter: 288 loss: 1.38746918e-06
Iter: 289 loss: 1.38680252e-06
Iter: 290 loss: 1.38468329e-06
Iter: 291 loss: 1.39352096e-06
Iter: 292 loss: 1.38391761e-06
Iter: 293 loss: 1.38173266e-06
Iter: 294 loss: 1.40185978e-06
Iter: 295 loss: 1.38162306e-06
Iter: 296 loss: 1.37971938e-06
Iter: 297 loss: 1.38009523e-06
Iter: 298 loss: 1.37829841e-06
Iter: 299 loss: 1.37491566e-06
Iter: 300 loss: 1.40013549e-06
Iter: 301 loss: 1.37463951e-06
Iter: 302 loss: 1.37266852e-06
Iter: 303 loss: 1.37026439e-06
Iter: 304 loss: 1.37003417e-06
Iter: 305 loss: 1.3675492e-06
Iter: 306 loss: 1.40404632e-06
Iter: 307 loss: 1.36757194e-06
Iter: 308 loss: 1.36486472e-06
Iter: 309 loss: 1.36351582e-06
Iter: 310 loss: 1.36227402e-06
Iter: 311 loss: 1.36029257e-06
Iter: 312 loss: 1.37179882e-06
Iter: 313 loss: 1.36001768e-06
Iter: 314 loss: 1.35833795e-06
Iter: 315 loss: 1.37119423e-06
Iter: 316 loss: 1.35819778e-06
Iter: 317 loss: 1.35713e-06
Iter: 318 loss: 1.35487414e-06
Iter: 319 loss: 1.39487724e-06
Iter: 320 loss: 1.35491189e-06
Iter: 321 loss: 1.35452069e-06
Iter: 322 loss: 1.35398398e-06
Iter: 323 loss: 1.35308437e-06
Iter: 324 loss: 1.3513137e-06
Iter: 325 loss: 1.38418579e-06
Iter: 326 loss: 1.35126311e-06
Iter: 327 loss: 1.3492e-06
Iter: 328 loss: 1.3497322e-06
Iter: 329 loss: 1.34769209e-06
Iter: 330 loss: 1.34577476e-06
Iter: 331 loss: 1.36555741e-06
Iter: 332 loss: 1.34569677e-06
Iter: 333 loss: 1.34406787e-06
Iter: 334 loss: 1.35541779e-06
Iter: 335 loss: 1.34388415e-06
Iter: 336 loss: 1.34213906e-06
Iter: 337 loss: 1.33979484e-06
Iter: 338 loss: 1.33968274e-06
Iter: 339 loss: 1.3376266e-06
Iter: 340 loss: 1.34653533e-06
Iter: 341 loss: 1.33722062e-06
Iter: 342 loss: 1.33517142e-06
Iter: 343 loss: 1.355721e-06
Iter: 344 loss: 1.33510173e-06
Iter: 345 loss: 1.33373055e-06
Iter: 346 loss: 1.331053e-06
Iter: 347 loss: 1.38346229e-06
Iter: 348 loss: 1.3310048e-06
Iter: 349 loss: 1.32984167e-06
Iter: 350 loss: 1.3295321e-06
Iter: 351 loss: 1.3282239e-06
Iter: 352 loss: 1.32537411e-06
Iter: 353 loss: 1.36223741e-06
Iter: 354 loss: 1.32512e-06
Iter: 355 loss: 1.3233896e-06
Iter: 356 loss: 1.32338437e-06
Iter: 357 loss: 1.32203149e-06
Iter: 358 loss: 1.33134768e-06
Iter: 359 loss: 1.32190462e-06
Iter: 360 loss: 1.32104094e-06
Iter: 361 loss: 1.31861054e-06
Iter: 362 loss: 1.33509343e-06
Iter: 363 loss: 1.31803358e-06
Iter: 364 loss: 1.31623528e-06
Iter: 365 loss: 1.31620629e-06
Iter: 366 loss: 1.3150518e-06
Iter: 367 loss: 1.33237029e-06
Iter: 368 loss: 1.31505772e-06
Iter: 369 loss: 1.31387935e-06
Iter: 370 loss: 1.31333104e-06
Iter: 371 loss: 1.31275897e-06
Iter: 372 loss: 1.31133402e-06
Iter: 373 loss: 1.31312913e-06
Iter: 374 loss: 1.31060415e-06
Iter: 375 loss: 1.30946523e-06
Iter: 376 loss: 1.30949979e-06
Iter: 377 loss: 1.30832723e-06
Iter: 378 loss: 1.30557692e-06
Iter: 379 loss: 1.33040942e-06
Iter: 380 loss: 1.30511921e-06
Iter: 381 loss: 1.30340368e-06
Iter: 382 loss: 1.30327726e-06
Iter: 383 loss: 1.30163949e-06
Iter: 384 loss: 1.30401918e-06
Iter: 385 loss: 1.30087983e-06
Iter: 386 loss: 1.29965156e-06
Iter: 387 loss: 1.29865862e-06
Iter: 388 loss: 1.29831915e-06
Iter: 389 loss: 1.29680564e-06
Iter: 390 loss: 1.29678097e-06
Iter: 391 loss: 1.29616569e-06
Iter: 392 loss: 1.29520947e-06
Iter: 393 loss: 1.29518276e-06
Iter: 394 loss: 1.29394357e-06
Iter: 395 loss: 1.29333785e-06
Iter: 396 loss: 1.29275429e-06
Iter: 397 loss: 1.29220825e-06
Iter: 398 loss: 1.29192335e-06
Iter: 399 loss: 1.29115551e-06
Iter: 400 loss: 1.29156e-06
Iter: 401 loss: 1.29055763e-06
Iter: 402 loss: 1.28945805e-06
Iter: 403 loss: 1.28883323e-06
Iter: 404 loss: 1.2883678e-06
Iter: 405 loss: 1.28731881e-06
Iter: 406 loss: 1.28730244e-06
Iter: 407 loss: 1.28619308e-06
Iter: 408 loss: 1.28539409e-06
Iter: 409 loss: 1.28499141e-06
Iter: 410 loss: 1.28353042e-06
Iter: 411 loss: 1.28508668e-06
Iter: 412 loss: 1.28272745e-06
Iter: 413 loss: 1.28134036e-06
Iter: 414 loss: 1.28131512e-06
Iter: 415 loss: 1.28071974e-06
Iter: 416 loss: 1.27964518e-06
Iter: 417 loss: 1.27963813e-06
Iter: 418 loss: 1.2787923e-06
Iter: 419 loss: 1.27874068e-06
Iter: 420 loss: 1.27808858e-06
Iter: 421 loss: 1.27716976e-06
Iter: 422 loss: 1.27713747e-06
Iter: 423 loss: 1.27616579e-06
Iter: 424 loss: 1.27590522e-06
Iter: 425 loss: 1.27527119e-06
Iter: 426 loss: 1.27381611e-06
Iter: 427 loss: 1.28258375e-06
Iter: 428 loss: 1.27364569e-06
Iter: 429 loss: 1.27294197e-06
Iter: 430 loss: 1.27283647e-06
Iter: 431 loss: 1.27229805e-06
Iter: 432 loss: 1.27089731e-06
Iter: 433 loss: 1.2817884e-06
Iter: 434 loss: 1.27059297e-06
Iter: 435 loss: 1.26899306e-06
Iter: 436 loss: 1.28481e-06
Iter: 437 loss: 1.26891291e-06
Iter: 438 loss: 1.2680166e-06
Iter: 439 loss: 1.26797363e-06
Iter: 440 loss: 1.26751399e-06
Iter: 441 loss: 1.26638065e-06
Iter: 442 loss: 1.27755243e-06
Iter: 443 loss: 1.26613668e-06
Iter: 444 loss: 1.26516693e-06
Iter: 445 loss: 1.26518466e-06
Iter: 446 loss: 1.26422083e-06
Iter: 447 loss: 1.26343673e-06
Iter: 448 loss: 1.26322311e-06
Iter: 449 loss: 1.26219777e-06
Iter: 450 loss: 1.26797249e-06
Iter: 451 loss: 1.2620385e-06
Iter: 452 loss: 1.26065481e-06
Iter: 453 loss: 1.26021632e-06
Iter: 454 loss: 1.2593556e-06
Iter: 455 loss: 1.25804286e-06
Iter: 456 loss: 1.25991e-06
Iter: 457 loss: 1.2574144e-06
Iter: 458 loss: 1.25621057e-06
Iter: 459 loss: 1.25644226e-06
Iter: 460 loss: 1.2552772e-06
Iter: 461 loss: 1.25535416e-06
Iter: 462 loss: 1.25460906e-06
Iter: 463 loss: 1.25406893e-06
Iter: 464 loss: 1.2535869e-06
Iter: 465 loss: 1.25345957e-06
Iter: 466 loss: 1.25276688e-06
Iter: 467 loss: 1.25281201e-06
Iter: 468 loss: 1.25224653e-06
Iter: 469 loss: 1.25164274e-06
Iter: 470 loss: 1.25162114e-06
Iter: 471 loss: 1.25110046e-06
Iter: 472 loss: 1.25033057e-06
Iter: 473 loss: 1.25034592e-06
Iter: 474 loss: 1.24936832e-06
Iter: 475 loss: 1.25137251e-06
Iter: 476 loss: 1.24910878e-06
Iter: 477 loss: 1.24770941e-06
Iter: 478 loss: 1.25071188e-06
Iter: 479 loss: 1.24712051e-06
Iter: 480 loss: 1.24607277e-06
Iter: 481 loss: 1.24660278e-06
Iter: 482 loss: 1.2454318e-06
Iter: 483 loss: 1.24428857e-06
Iter: 484 loss: 1.26165764e-06
Iter: 485 loss: 1.24429994e-06
Iter: 486 loss: 1.24365033e-06
Iter: 487 loss: 1.24217581e-06
Iter: 488 loss: 1.26727059e-06
Iter: 489 loss: 1.24212283e-06
Iter: 490 loss: 1.2410261e-06
Iter: 491 loss: 1.24425617e-06
Iter: 492 loss: 1.24068083e-06
Iter: 493 loss: 1.23952793e-06
Iter: 494 loss: 1.24646726e-06
Iter: 495 loss: 1.23940117e-06
Iter: 496 loss: 1.23837822e-06
Iter: 497 loss: 1.25161546e-06
Iter: 498 loss: 1.23838254e-06
Iter: 499 loss: 1.23808502e-06
Iter: 500 loss: 1.23730638e-06
Iter: 501 loss: 1.24870508e-06
Iter: 502 loss: 1.23729e-06
Iter: 503 loss: 1.23664972e-06
Iter: 504 loss: 1.2366238e-06
Iter: 505 loss: 1.23608288e-06
Iter: 506 loss: 1.23635709e-06
Iter: 507 loss: 1.23564257e-06
Iter: 508 loss: 1.23514371e-06
Iter: 509 loss: 1.23549194e-06
Iter: 510 loss: 1.23484085e-06
Iter: 511 loss: 1.23415907e-06
Iter: 512 loss: 1.23988616e-06
Iter: 513 loss: 1.23406448e-06
Iter: 514 loss: 1.23352129e-06
Iter: 515 loss: 1.23296036e-06
Iter: 516 loss: 1.23284394e-06
Iter: 517 loss: 1.23235372e-06
Iter: 518 loss: 1.23237385e-06
Iter: 519 loss: 1.23178313e-06
Iter: 520 loss: 1.23028963e-06
Iter: 521 loss: 1.24373719e-06
Iter: 522 loss: 1.23014797e-06
Iter: 523 loss: 1.22848849e-06
Iter: 524 loss: 1.23309087e-06
Iter: 525 loss: 1.22793119e-06
Iter: 526 loss: 1.22643371e-06
Iter: 527 loss: 1.22932511e-06
Iter: 528 loss: 1.22585163e-06
Iter: 529 loss: 1.22532583e-06
Iter: 530 loss: 1.22504548e-06
Iter: 531 loss: 1.22439519e-06
Iter: 532 loss: 1.22370272e-06
Iter: 533 loss: 1.22362223e-06
Iter: 534 loss: 1.22305653e-06
Iter: 535 loss: 1.23054929e-06
Iter: 536 loss: 1.22304664e-06
Iter: 537 loss: 1.22257882e-06
Iter: 538 loss: 1.22270922e-06
Iter: 539 loss: 1.22219899e-06
Iter: 540 loss: 1.22173708e-06
Iter: 541 loss: 1.22290055e-06
Iter: 542 loss: 1.22162498e-06
Iter: 543 loss: 1.22121912e-06
Iter: 544 loss: 1.22532424e-06
Iter: 545 loss: 1.22126494e-06
Iter: 546 loss: 1.22079132e-06
Iter: 547 loss: 1.22018082e-06
Iter: 548 loss: 1.22014387e-06
Iter: 549 loss: 1.21959908e-06
Iter: 550 loss: 1.22393317e-06
Iter: 551 loss: 1.21957487e-06
Iter: 552 loss: 1.21893459e-06
Iter: 553 loss: 1.21947187e-06
Iter: 554 loss: 1.21857852e-06
Iter: 555 loss: 1.21785797e-06
Iter: 556 loss: 1.2164453e-06
Iter: 557 loss: 1.24452094e-06
Iter: 558 loss: 1.21640437e-06
Iter: 559 loss: 1.21489256e-06
Iter: 560 loss: 1.22116012e-06
Iter: 561 loss: 1.21462426e-06
Iter: 562 loss: 1.21379765e-06
Iter: 563 loss: 1.21370965e-06
Iter: 564 loss: 1.21281209e-06
Iter: 565 loss: 1.21381436e-06
Iter: 566 loss: 1.21231085e-06
Iter: 567 loss: 1.21186827e-06
Iter: 568 loss: 1.2128545e-06
Iter: 569 loss: 1.21160133e-06
Iter: 570 loss: 1.21102312e-06
Iter: 571 loss: 1.21447079e-06
Iter: 572 loss: 1.21088112e-06
Iter: 573 loss: 1.21045991e-06
Iter: 574 loss: 1.21013841e-06
Iter: 575 loss: 1.2099938e-06
Iter: 576 loss: 1.20954701e-06
Iter: 577 loss: 1.21412313e-06
Iter: 578 loss: 1.20952018e-06
Iter: 579 loss: 1.20904087e-06
Iter: 580 loss: 1.20998379e-06
Iter: 581 loss: 1.20887466e-06
Iter: 582 loss: 1.20836467e-06
Iter: 583 loss: 1.20787604e-06
Iter: 584 loss: 1.20780589e-06
Iter: 585 loss: 1.20728214e-06
Iter: 586 loss: 1.20723871e-06
Iter: 587 loss: 1.20689708e-06
Iter: 588 loss: 1.20597292e-06
Iter: 589 loss: 1.21304049e-06
Iter: 590 loss: 1.20581285e-06
Iter: 591 loss: 1.20476136e-06
Iter: 592 loss: 1.20834943e-06
Iter: 593 loss: 1.20446521e-06
Iter: 594 loss: 1.20348784e-06
Iter: 595 loss: 1.20504774e-06
Iter: 596 loss: 1.20305026e-06
Iter: 597 loss: 1.20209017e-06
Iter: 598 loss: 1.20206062e-06
Iter: 599 loss: 1.20165032e-06
Iter: 600 loss: 1.20106279e-06
Iter: 601 loss: 1.20101106e-06
Iter: 602 loss: 1.20042068e-06
Iter: 603 loss: 1.20044876e-06
Iter: 604 loss: 1.19998731e-06
Iter: 605 loss: 1.19904848e-06
Iter: 606 loss: 1.21480093e-06
Iter: 607 loss: 1.19903143e-06
Iter: 608 loss: 1.19837387e-06
Iter: 609 loss: 1.20066966e-06
Iter: 610 loss: 1.19823176e-06
Iter: 611 loss: 1.19778178e-06
Iter: 612 loss: 1.19948254e-06
Iter: 613 loss: 1.19767697e-06
Iter: 614 loss: 1.19715287e-06
Iter: 615 loss: 1.19882498e-06
Iter: 616 loss: 1.19698325e-06
Iter: 617 loss: 1.19676633e-06
Iter: 618 loss: 1.19647814e-06
Iter: 619 loss: 1.19646324e-06
Iter: 620 loss: 1.19600327e-06
Iter: 621 loss: 1.20092636e-06
Iter: 622 loss: 1.1959587e-06
Iter: 623 loss: 1.19555011e-06
Iter: 624 loss: 1.1949661e-06
Iter: 625 loss: 1.19494655e-06
Iter: 626 loss: 1.19419963e-06
Iter: 627 loss: 1.19391075e-06
Iter: 628 loss: 1.19352853e-06
Iter: 629 loss: 1.19228912e-06
Iter: 630 loss: 1.19622734e-06
Iter: 631 loss: 1.19193066e-06
Iter: 632 loss: 1.19235926e-06
Iter: 633 loss: 1.19163838e-06
Iter: 634 loss: 1.19145227e-06
Iter: 635 loss: 1.19103083e-06
Iter: 636 loss: 1.19702293e-06
Iter: 637 loss: 1.19098922e-06
Iter: 638 loss: 1.19059814e-06
Iter: 639 loss: 1.19431365e-06
Iter: 640 loss: 1.19059382e-06
Iter: 641 loss: 1.19026197e-06
Iter: 642 loss: 1.19248466e-06
Iter: 643 loss: 1.19021138e-06
Iter: 644 loss: 1.19009508e-06
Iter: 645 loss: 1.18979028e-06
Iter: 646 loss: 1.1922898e-06
Iter: 647 loss: 1.18978517e-06
Iter: 648 loss: 1.18937123e-06
Iter: 649 loss: 1.19493893e-06
Iter: 650 loss: 1.18934918e-06
Iter: 651 loss: 1.18912521e-06
Iter: 652 loss: 1.18874482e-06
Iter: 653 loss: 1.19928745e-06
Iter: 654 loss: 1.18873e-06
Iter: 655 loss: 1.18818298e-06
Iter: 656 loss: 1.19241417e-06
Iter: 657 loss: 1.188135e-06
Iter: 658 loss: 1.18753292e-06
Iter: 659 loss: 1.18826665e-06
Iter: 660 loss: 1.18721152e-06
Iter: 661 loss: 1.18673563e-06
Iter: 662 loss: 1.18675302e-06
Iter: 663 loss: 1.1863649e-06
Iter: 664 loss: 1.18578816e-06
Iter: 665 loss: 1.18693e-06
Iter: 666 loss: 1.18553567e-06
Iter: 667 loss: 1.18514072e-06
Iter: 668 loss: 1.18801938e-06
Iter: 669 loss: 1.18510161e-06
Iter: 670 loss: 1.18479943e-06
Iter: 671 loss: 1.18481728e-06
Iter: 672 loss: 1.184555e-06
Iter: 673 loss: 1.18440244e-06
Iter: 674 loss: 1.18434014e-06
Iter: 675 loss: 1.18415562e-06
Iter: 676 loss: 1.18394701e-06
Iter: 677 loss: 1.18391267e-06
Iter: 678 loss: 1.18364017e-06
Iter: 679 loss: 1.18401817e-06
Iter: 680 loss: 1.18352648e-06
Iter: 681 loss: 1.18296725e-06
Iter: 682 loss: 1.18319781e-06
Iter: 683 loss: 1.18255673e-06
Iter: 684 loss: 1.18220987e-06
Iter: 685 loss: 1.1819194e-06
Iter: 686 loss: 1.18180515e-06
Iter: 687 loss: 1.18111598e-06
Iter: 688 loss: 1.18311937e-06
Iter: 689 loss: 1.18094499e-06
Iter: 690 loss: 1.18033e-06
Iter: 691 loss: 1.18958303e-06
Iter: 692 loss: 1.18032153e-06
Iter: 693 loss: 1.18002652e-06
Iter: 694 loss: 1.1797e-06
Iter: 695 loss: 1.1796235e-06
Iter: 696 loss: 1.17921491e-06
Iter: 697 loss: 1.18011349e-06
Iter: 698 loss: 1.17898037e-06
Iter: 699 loss: 1.17865943e-06
Iter: 700 loss: 1.17865784e-06
Iter: 701 loss: 1.17838522e-06
Iter: 702 loss: 1.17822174e-06
Iter: 703 loss: 1.17808008e-06
Iter: 704 loss: 1.17761272e-06
Iter: 705 loss: 1.17752825e-06
Iter: 706 loss: 1.17727041e-06
Iter: 707 loss: 1.17716593e-06
Iter: 708 loss: 1.1770137e-06
Iter: 709 loss: 1.17683749e-06
Iter: 710 loss: 1.17651416e-06
Iter: 711 loss: 1.17650495e-06
Iter: 712 loss: 1.17616798e-06
Iter: 713 loss: 1.17736283e-06
Iter: 714 loss: 1.17606783e-06
Iter: 715 loss: 1.17555953e-06
Iter: 716 loss: 1.17759748e-06
Iter: 717 loss: 1.17550383e-06
Iter: 718 loss: 1.17523007e-06
Iter: 719 loss: 1.17487912e-06
Iter: 720 loss: 1.18445655e-06
Iter: 721 loss: 1.17483444e-06
Iter: 722 loss: 1.17450361e-06
Iter: 723 loss: 1.17934428e-06
Iter: 724 loss: 1.17447689e-06
Iter: 725 loss: 1.1740924e-06
Iter: 726 loss: 1.17515765e-06
Iter: 727 loss: 1.17397292e-06
Iter: 728 loss: 1.17367267e-06
Iter: 729 loss: 1.17288209e-06
Iter: 730 loss: 1.17913976e-06
Iter: 731 loss: 1.17270952e-06
Iter: 732 loss: 1.17211948e-06
Iter: 733 loss: 1.17211175e-06
Iter: 734 loss: 1.17164723e-06
Iter: 735 loss: 1.1746489e-06
Iter: 736 loss: 1.17159857e-06
Iter: 737 loss: 1.17134755e-06
Iter: 738 loss: 1.17093896e-06
Iter: 739 loss: 1.17093066e-06
Iter: 740 loss: 1.17048194e-06
Iter: 741 loss: 1.17505419e-06
Iter: 742 loss: 1.17047648e-06
Iter: 743 loss: 1.17021023e-06
Iter: 744 loss: 1.17285197e-06
Iter: 745 loss: 1.17012462e-06
Iter: 746 loss: 1.16996341e-06
Iter: 747 loss: 1.16954084e-06
Iter: 748 loss: 1.17562115e-06
Iter: 749 loss: 1.16953595e-06
Iter: 750 loss: 1.16920046e-06
Iter: 751 loss: 1.16915862e-06
Iter: 752 loss: 1.16895399e-06
Iter: 753 loss: 1.16863976e-06
Iter: 754 loss: 1.16865795e-06
Iter: 755 loss: 1.16831302e-06
Iter: 756 loss: 1.16801516e-06
Iter: 757 loss: 1.16788851e-06
Iter: 758 loss: 1.16722867e-06
Iter: 759 loss: 1.16727063e-06
Iter: 760 loss: 1.16688113e-06
Iter: 761 loss: 1.16640922e-06
Iter: 762 loss: 1.16633714e-06
Iter: 763 loss: 1.16578144e-06
Iter: 764 loss: 1.16539502e-06
Iter: 765 loss: 1.16516753e-06
Iter: 766 loss: 1.16470301e-06
Iter: 767 loss: 1.16457704e-06
Iter: 768 loss: 1.16425565e-06
Iter: 769 loss: 1.16380102e-06
Iter: 770 loss: 1.16375304e-06
Iter: 771 loss: 1.1632892e-06
Iter: 772 loss: 1.16495335e-06
Iter: 773 loss: 1.16313186e-06
Iter: 774 loss: 1.16266051e-06
Iter: 775 loss: 1.16731155e-06
Iter: 776 loss: 1.16263163e-06
Iter: 777 loss: 1.16231104e-06
Iter: 778 loss: 1.16189108e-06
Iter: 779 loss: 1.16188573e-06
Iter: 780 loss: 1.16144065e-06
Iter: 781 loss: 1.1655859e-06
Iter: 782 loss: 1.16140711e-06
Iter: 783 loss: 1.16087222e-06
Iter: 784 loss: 1.16144497e-06
Iter: 785 loss: 1.16059209e-06
Iter: 786 loss: 1.16012927e-06
Iter: 787 loss: 1.15979105e-06
Iter: 788 loss: 1.15967782e-06
Iter: 789 loss: 1.15913303e-06
Iter: 790 loss: 1.16265687e-06
Iter: 791 loss: 1.15903413e-06
Iter: 792 loss: 1.15830494e-06
Iter: 793 loss: 1.16109709e-06
Iter: 794 loss: 1.15807825e-06
Iter: 795 loss: 1.15775504e-06
Iter: 796 loss: 1.15710452e-06
Iter: 797 loss: 1.15709611e-06
Iter: 798 loss: 1.15632974e-06
Iter: 799 loss: 1.16001536e-06
Iter: 800 loss: 1.15618161e-06
Iter: 801 loss: 1.15544526e-06
Iter: 802 loss: 1.1644081e-06
Iter: 803 loss: 1.15540661e-06
Iter: 804 loss: 1.15510329e-06
Iter: 805 loss: 1.1543699e-06
Iter: 806 loss: 1.16418732e-06
Iter: 807 loss: 1.15431408e-06
Iter: 808 loss: 1.15386695e-06
Iter: 809 loss: 1.15371427e-06
Iter: 810 loss: 1.15328339e-06
Iter: 811 loss: 1.15331557e-06
Iter: 812 loss: 1.15288026e-06
Iter: 813 loss: 1.15257535e-06
Iter: 814 loss: 1.15275384e-06
Iter: 815 loss: 1.15234911e-06
Iter: 816 loss: 1.15202897e-06
Iter: 817 loss: 1.15762759e-06
Iter: 818 loss: 1.15200919e-06
Iter: 819 loss: 1.1516828e-06
Iter: 820 loss: 1.15151363e-06
Iter: 821 loss: 1.15135958e-06
Iter: 822 loss: 1.15104808e-06
Iter: 823 loss: 1.15078421e-06
Iter: 824 loss: 1.15072407e-06
Iter: 825 loss: 1.15029286e-06
Iter: 826 loss: 1.15027638e-06
Iter: 827 loss: 1.14986642e-06
Iter: 828 loss: 1.14910267e-06
Iter: 829 loss: 1.16581919e-06
Iter: 830 loss: 1.14910142e-06
Iter: 831 loss: 1.14846284e-06
Iter: 832 loss: 1.14791408e-06
Iter: 833 loss: 1.14769841e-06
Iter: 834 loss: 1.1474383e-06
Iter: 835 loss: 1.14711929e-06
Iter: 836 loss: 1.14667807e-06
Iter: 837 loss: 1.14659383e-06
Iter: 838 loss: 1.14631098e-06
Iter: 839 loss: 1.1459008e-06
Iter: 840 loss: 1.1461118e-06
Iter: 841 loss: 1.14561522e-06
Iter: 842 loss: 1.14493105e-06
Iter: 843 loss: 1.1481203e-06
Iter: 844 loss: 1.14484806e-06
Iter: 845 loss: 1.14459033e-06
Iter: 846 loss: 1.14459658e-06
Iter: 847 loss: 1.14434954e-06
Iter: 848 loss: 1.14412614e-06
Iter: 849 loss: 1.14447175e-06
Iter: 850 loss: 1.1440618e-06
Iter: 851 loss: 1.14366185e-06
Iter: 852 loss: 1.14328384e-06
Iter: 853 loss: 1.1431863e-06
Iter: 854 loss: 1.14278691e-06
Iter: 855 loss: 1.14337956e-06
Iter: 856 loss: 1.14259115e-06
Iter: 857 loss: 1.14209888e-06
Iter: 858 loss: 1.14223553e-06
Iter: 859 loss: 1.14174304e-06
Iter: 860 loss: 1.14154682e-06
Iter: 861 loss: 1.14138686e-06
Iter: 862 loss: 1.14108525e-06
Iter: 863 loss: 1.14122463e-06
Iter: 864 loss: 1.1409262e-06
Iter: 865 loss: 1.14050727e-06
Iter: 866 loss: 1.1397658e-06
Iter: 867 loss: 1.13979229e-06
Iter: 868 loss: 1.13944361e-06
Iter: 869 loss: 1.13934152e-06
Iter: 870 loss: 1.13899705e-06
Iter: 871 loss: 1.13907254e-06
Iter: 872 loss: 1.13875251e-06
Iter: 873 loss: 1.13842339e-06
Iter: 874 loss: 1.13866258e-06
Iter: 875 loss: 1.13822011e-06
Iter: 876 loss: 1.13767919e-06
Iter: 877 loss: 1.14101033e-06
Iter: 878 loss: 1.13758142e-06
Iter: 879 loss: 1.13732222e-06
Iter: 880 loss: 1.13691067e-06
Iter: 881 loss: 1.14372699e-06
Iter: 882 loss: 1.13682734e-06
Iter: 883 loss: 1.13659121e-06
Iter: 884 loss: 1.13651288e-06
Iter: 885 loss: 1.13615977e-06
Iter: 886 loss: 1.13573344e-06
Iter: 887 loss: 1.1357057e-06
Iter: 888 loss: 1.13526244e-06
Iter: 889 loss: 1.13483009e-06
Iter: 890 loss: 1.13480439e-06
Iter: 891 loss: 1.13393526e-06
Iter: 892 loss: 1.1367199e-06
Iter: 893 loss: 1.13370561e-06
Iter: 894 loss: 1.13324722e-06
Iter: 895 loss: 1.13841872e-06
Iter: 896 loss: 1.13322551e-06
Iter: 897 loss: 1.13269687e-06
Iter: 898 loss: 1.13510737e-06
Iter: 899 loss: 1.13264127e-06
Iter: 900 loss: 1.13213946e-06
Iter: 901 loss: 1.13199144e-06
Iter: 902 loss: 1.13173303e-06
Iter: 903 loss: 1.13133433e-06
Iter: 904 loss: 1.13132864e-06
Iter: 905 loss: 1.13099736e-06
Iter: 906 loss: 1.13071519e-06
Iter: 907 loss: 1.13061196e-06
Iter: 908 loss: 1.13013232e-06
Iter: 909 loss: 1.13354827e-06
Iter: 910 loss: 1.13011129e-06
Iter: 911 loss: 1.12970667e-06
Iter: 912 loss: 1.13185172e-06
Iter: 913 loss: 1.1296504e-06
Iter: 914 loss: 1.12948101e-06
Iter: 915 loss: 1.12905695e-06
Iter: 916 loss: 1.13562419e-06
Iter: 917 loss: 1.12906093e-06
Iter: 918 loss: 1.12889347e-06
Iter: 919 loss: 1.12879604e-06
Iter: 920 loss: 1.12863893e-06
Iter: 921 loss: 1.12832959e-06
Iter: 922 loss: 1.12834107e-06
Iter: 923 loss: 1.12797943e-06
Iter: 924 loss: 1.12728515e-06
Iter: 925 loss: 1.14052852e-06
Iter: 926 loss: 1.12724467e-06
Iter: 927 loss: 1.12654834e-06
Iter: 928 loss: 1.13086514e-06
Iter: 929 loss: 1.12647308e-06
Iter: 930 loss: 1.12628902e-06
Iter: 931 loss: 1.12612906e-06
Iter: 932 loss: 1.12579346e-06
Iter: 933 loss: 1.12532905e-06
Iter: 934 loss: 1.12532348e-06
Iter: 935 loss: 1.12483917e-06
Iter: 936 loss: 1.12543501e-06
Iter: 937 loss: 1.12467876e-06
Iter: 938 loss: 1.12416774e-06
Iter: 939 loss: 1.13165424e-06
Iter: 940 loss: 1.1241807e-06
Iter: 941 loss: 1.12392172e-06
Iter: 942 loss: 1.12362113e-06
Iter: 943 loss: 1.12355133e-06
Iter: 944 loss: 1.12336966e-06
Iter: 945 loss: 1.12334135e-06
Iter: 946 loss: 1.12311193e-06
Iter: 947 loss: 1.12279508e-06
Iter: 948 loss: 1.12276166e-06
Iter: 949 loss: 1.12244891e-06
Iter: 950 loss: 1.12494047e-06
Iter: 951 loss: 1.12239979e-06
Iter: 952 loss: 1.12205294e-06
Iter: 953 loss: 1.1239897e-06
Iter: 954 loss: 1.12202213e-06
Iter: 955 loss: 1.12182488e-06
Iter: 956 loss: 1.12155431e-06
Iter: 957 loss: 1.12149655e-06
Iter: 958 loss: 1.12110047e-06
Iter: 959 loss: 1.12145267e-06
Iter: 960 loss: 1.12079613e-06
Iter: 961 loss: 1.12038174e-06
Iter: 962 loss: 1.12208522e-06
Iter: 963 loss: 1.1202967e-06
Iter: 964 loss: 1.12005239e-06
Iter: 965 loss: 1.11997429e-06
Iter: 966 loss: 1.11981194e-06
Iter: 967 loss: 1.11920758e-06
Iter: 968 loss: 1.12423106e-06
Iter: 969 loss: 1.11904865e-06
Iter: 970 loss: 1.11834015e-06
Iter: 971 loss: 1.11840598e-06
Iter: 972 loss: 1.11767417e-06
Iter: 973 loss: 1.11734698e-06
Iter: 974 loss: 1.11730333e-06
Iter: 975 loss: 1.11692702e-06
Iter: 976 loss: 1.1200766e-06
Iter: 977 loss: 1.11689212e-06
Iter: 978 loss: 1.11671841e-06
Iter: 979 loss: 1.1162499e-06
Iter: 980 loss: 1.11627753e-06
Iter: 981 loss: 1.11632971e-06
Iter: 982 loss: 1.11610905e-06
Iter: 983 loss: 1.11602822e-06
Iter: 984 loss: 1.11573968e-06
Iter: 985 loss: 1.11756117e-06
Iter: 986 loss: 1.11568852e-06
Iter: 987 loss: 1.11542818e-06
Iter: 988 loss: 1.11544e-06
Iter: 989 loss: 1.11514419e-06
Iter: 990 loss: 1.11504528e-06
Iter: 991 loss: 1.11490226e-06
Iter: 992 loss: 1.11460213e-06
Iter: 993 loss: 1.11408394e-06
Iter: 994 loss: 1.12475391e-06
Iter: 995 loss: 1.11400959e-06
Iter: 996 loss: 1.1133319e-06
Iter: 997 loss: 1.11487509e-06
Iter: 998 loss: 1.11305701e-06
Iter: 999 loss: 1.1127272e-06
Iter: 1000 loss: 1.11268105e-06
Iter: 1001 loss: 1.11229269e-06
Iter: 1002 loss: 1.11248926e-06
Iter: 1003 loss: 1.11207396e-06
Iter: 1004 loss: 1.11170925e-06
Iter: 1005 loss: 1.11194163e-06
Iter: 1006 loss: 1.11147119e-06
Iter: 1007 loss: 1.11111467e-06
Iter: 1008 loss: 1.11206475e-06
Iter: 1009 loss: 1.1109928e-06
Iter: 1010 loss: 1.11064014e-06
Iter: 1011 loss: 1.11308748e-06
Iter: 1012 loss: 1.11062661e-06
Iter: 1013 loss: 1.11040026e-06
Iter: 1014 loss: 1.11012184e-06
Iter: 1015 loss: 1.11011e-06
Iter: 1016 loss: 1.10985445e-06
Iter: 1017 loss: 1.10984627e-06
Iter: 1018 loss: 1.10965357e-06
Iter: 1019 loss: 1.10917119e-06
Iter: 1020 loss: 1.11416443e-06
Iter: 1021 loss: 1.10907342e-06
Iter: 1022 loss: 1.10888027e-06
Iter: 1023 loss: 1.108785e-06
Iter: 1024 loss: 1.10853694e-06
Iter: 1025 loss: 1.10815586e-06
Iter: 1026 loss: 1.11696488e-06
Iter: 1027 loss: 1.10816632e-06
Iter: 1028 loss: 1.10771134e-06
Iter: 1029 loss: 1.10779115e-06
Iter: 1030 loss: 1.10733754e-06
Iter: 1031 loss: 1.10687017e-06
Iter: 1032 loss: 1.10862788e-06
Iter: 1033 loss: 1.10676228e-06
Iter: 1034 loss: 1.10646647e-06
Iter: 1035 loss: 1.10642645e-06
Iter: 1036 loss: 1.1060954e-06
Iter: 1037 loss: 1.10596829e-06
Iter: 1038 loss: 1.10577082e-06
Iter: 1039 loss: 1.1055381e-06
Iter: 1040 loss: 1.10586734e-06
Iter: 1041 loss: 1.10540168e-06
Iter: 1042 loss: 1.10504868e-06
Iter: 1043 loss: 1.10894939e-06
Iter: 1044 loss: 1.10504902e-06
Iter: 1045 loss: 1.10462463e-06
Iter: 1046 loss: 1.10425526e-06
Iter: 1047 loss: 1.10410724e-06
Iter: 1048 loss: 1.10384099e-06
Iter: 1049 loss: 1.10378846e-06
Iter: 1050 loss: 1.10355006e-06
Iter: 1051 loss: 1.1038187e-06
Iter: 1052 loss: 1.10339329e-06
Iter: 1053 loss: 1.10318058e-06
Iter: 1054 loss: 1.10342671e-06
Iter: 1055 loss: 1.10302119e-06
Iter: 1056 loss: 1.10271242e-06
Iter: 1057 loss: 1.10549672e-06
Iter: 1058 loss: 1.10271048e-06
Iter: 1059 loss: 1.10251881e-06
Iter: 1060 loss: 1.10204076e-06
Iter: 1061 loss: 1.10759731e-06
Iter: 1062 loss: 1.10203587e-06
Iter: 1063 loss: 1.101669e-06
Iter: 1064 loss: 1.10309315e-06
Iter: 1065 loss: 1.10153985e-06
Iter: 1066 loss: 1.10114138e-06
Iter: 1067 loss: 1.10265614e-06
Iter: 1068 loss: 1.10100348e-06
Iter: 1069 loss: 1.10068322e-06
Iter: 1070 loss: 1.10067117e-06
Iter: 1071 loss: 1.10052929e-06
Iter: 1072 loss: 1.10010524e-06
Iter: 1073 loss: 1.10375549e-06
Iter: 1074 loss: 1.1000302e-06
Iter: 1075 loss: 1.09951134e-06
Iter: 1076 loss: 1.10068049e-06
Iter: 1077 loss: 1.09926395e-06
Iter: 1078 loss: 1.0994894e-06
Iter: 1079 loss: 1.09909502e-06
Iter: 1080 loss: 1.09899304e-06
Iter: 1081 loss: 1.0986945e-06
Iter: 1082 loss: 1.10257122e-06
Iter: 1083 loss: 1.09866482e-06
Iter: 1084 loss: 1.09841494e-06
Iter: 1085 loss: 1.09846007e-06
Iter: 1086 loss: 1.09818666e-06
Iter: 1087 loss: 1.09792882e-06
Iter: 1088 loss: 1.09786822e-06
Iter: 1089 loss: 1.09768132e-06
Iter: 1090 loss: 1.09994369e-06
Iter: 1091 loss: 1.09766347e-06
Iter: 1092 loss: 1.09740711e-06
Iter: 1093 loss: 1.09729194e-06
Iter: 1094 loss: 1.09718314e-06
Iter: 1095 loss: 1.09685482e-06
Iter: 1096 loss: 1.09712471e-06
Iter: 1097 loss: 1.09668031e-06
Iter: 1098 loss: 1.09630741e-06
Iter: 1099 loss: 1.09604434e-06
Iter: 1100 loss: 1.09597693e-06
Iter: 1101 loss: 1.09549455e-06
Iter: 1102 loss: 1.09553241e-06
Iter: 1103 loss: 1.09533357e-06
Iter: 1104 loss: 1.09532095e-06
Iter: 1105 loss: 1.09520192e-06
Iter: 1106 loss: 1.0948329e-06
Iter: 1107 loss: 1.09644577e-06
Iter: 1108 loss: 1.09476491e-06
Iter: 1109 loss: 1.09431676e-06
Iter: 1110 loss: 1.09588416e-06
Iter: 1111 loss: 1.09426651e-06
Iter: 1112 loss: 1.09391942e-06
Iter: 1113 loss: 1.09594532e-06
Iter: 1114 loss: 1.09381767e-06
Iter: 1115 loss: 1.09349753e-06
Iter: 1116 loss: 1.09769064e-06
Iter: 1117 loss: 1.0934516e-06
Iter: 1118 loss: 1.09333928e-06
Iter: 1119 loss: 1.09305779e-06
Iter: 1120 loss: 1.09308053e-06
Iter: 1121 loss: 1.09278903e-06
Iter: 1122 loss: 1.09682708e-06
Iter: 1123 loss: 1.09278665e-06
Iter: 1124 loss: 1.09253881e-06
Iter: 1125 loss: 1.09247651e-06
Iter: 1126 loss: 1.09241091e-06
Iter: 1127 loss: 1.09222742e-06
Iter: 1128 loss: 1.0922372e-06
Iter: 1129 loss: 1.09213318e-06
Iter: 1130 loss: 1.09184055e-06
Iter: 1131 loss: 1.09481743e-06
Iter: 1132 loss: 1.09180064e-06
Iter: 1133 loss: 1.09158441e-06
Iter: 1134 loss: 1.09207269e-06
Iter: 1135 loss: 1.09145265e-06
Iter: 1136 loss: 1.09128564e-06
Iter: 1137 loss: 1.09151847e-06
Iter: 1138 loss: 1.09123357e-06
Iter: 1139 loss: 1.09089888e-06
Iter: 1140 loss: 1.09103132e-06
Iter: 1141 loss: 1.09073949e-06
Iter: 1142 loss: 1.09040116e-06
Iter: 1143 loss: 1.09070947e-06
Iter: 1144 loss: 1.09021187e-06
Iter: 1145 loss: 1.08979521e-06
Iter: 1146 loss: 1.08969311e-06
Iter: 1147 loss: 1.08946551e-06
Iter: 1148 loss: 1.08931329e-06
Iter: 1149 loss: 1.08917413e-06
Iter: 1150 loss: 1.08898894e-06
Iter: 1151 loss: 1.08999041e-06
Iter: 1152 loss: 1.08897075e-06
Iter: 1153 loss: 1.08886286e-06
Iter: 1154 loss: 1.08882693e-06
Iter: 1155 loss: 1.08874315e-06
Iter: 1156 loss: 1.0884944e-06
Iter: 1157 loss: 1.08880954e-06
Iter: 1158 loss: 1.08830011e-06
Iter: 1159 loss: 1.08809581e-06
Iter: 1160 loss: 1.08851282e-06
Iter: 1161 loss: 1.08800634e-06
Iter: 1162 loss: 1.08778863e-06
Iter: 1163 loss: 1.0906042e-06
Iter: 1164 loss: 1.0877784e-06
Iter: 1165 loss: 1.08766585e-06
Iter: 1166 loss: 1.08736845e-06
Iter: 1167 loss: 1.09027246e-06
Iter: 1168 loss: 1.08724862e-06
Iter: 1169 loss: 1.08707491e-06
Iter: 1170 loss: 1.08704444e-06
Iter: 1171 loss: 1.08686277e-06
Iter: 1172 loss: 1.08712334e-06
Iter: 1173 loss: 1.0867534e-06
Iter: 1174 loss: 1.08655263e-06
Iter: 1175 loss: 1.08625557e-06
Iter: 1176 loss: 1.08626477e-06
Iter: 1177 loss: 1.08596009e-06
Iter: 1178 loss: 1.08766972e-06
Iter: 1179 loss: 1.08589609e-06
Iter: 1180 loss: 1.08564507e-06
Iter: 1181 loss: 1.08610948e-06
Iter: 1182 loss: 1.08549284e-06
Iter: 1183 loss: 1.08530799e-06
Iter: 1184 loss: 1.08534243e-06
Iter: 1185 loss: 1.08520499e-06
Iter: 1186 loss: 1.08486699e-06
Iter: 1187 loss: 1.08961706e-06
Iter: 1188 loss: 1.08483687e-06
Iter: 1189 loss: 1.08452059e-06
Iter: 1190 loss: 1.08455015e-06
Iter: 1191 loss: 1.08428674e-06
Iter: 1192 loss: 1.08388906e-06
Iter: 1193 loss: 1.08388463e-06
Iter: 1194 loss: 1.08367715e-06
Iter: 1195 loss: 1.08369454e-06
Iter: 1196 loss: 1.08346967e-06
Iter: 1197 loss: 1.08331631e-06
Iter: 1198 loss: 1.08327231e-06
Iter: 1199 loss: 1.08301026e-06
Iter: 1200 loss: 1.08352071e-06
Iter: 1201 loss: 1.08288441e-06
Iter: 1202 loss: 1.08274025e-06
Iter: 1203 loss: 1.08295831e-06
Iter: 1204 loss: 1.08260213e-06
Iter: 1205 loss: 1.08239169e-06
Iter: 1206 loss: 1.08232871e-06
Iter: 1207 loss: 1.08229176e-06
Iter: 1208 loss: 1.08201698e-06
Iter: 1209 loss: 1.08259246e-06
Iter: 1210 loss: 1.08189272e-06
Iter: 1211 loss: 1.08169115e-06
Iter: 1212 loss: 1.08159884e-06
Iter: 1213 loss: 1.08146435e-06
Iter: 1214 loss: 1.0810561e-06
Iter: 1215 loss: 1.08264157e-06
Iter: 1216 loss: 1.08095537e-06
Iter: 1217 loss: 1.08074664e-06
Iter: 1218 loss: 1.08072618e-06
Iter: 1219 loss: 1.0806034e-06
Iter: 1220 loss: 1.08040922e-06
Iter: 1221 loss: 1.08416214e-06
Iter: 1222 loss: 1.08041331e-06
Iter: 1223 loss: 1.08019833e-06
Iter: 1224 loss: 1.0801291e-06
Iter: 1225 loss: 1.08000154e-06
Iter: 1226 loss: 1.0798833e-06
Iter: 1227 loss: 1.07982498e-06
Iter: 1228 loss: 1.07976246e-06
Iter: 1229 loss: 1.07975234e-06
Iter: 1230 loss: 1.07961353e-06
Iter: 1231 loss: 1.07950746e-06
Iter: 1232 loss: 1.07950075e-06
Iter: 1233 loss: 1.07932749e-06
Iter: 1234 loss: 1.0817048e-06
Iter: 1235 loss: 1.07934329e-06
Iter: 1236 loss: 1.07917288e-06
Iter: 1237 loss: 1.07911046e-06
Iter: 1238 loss: 1.07910409e-06
Iter: 1239 loss: 1.07887786e-06
Iter: 1240 loss: 1.07891378e-06
Iter: 1241 loss: 1.07875428e-06
Iter: 1242 loss: 1.07843834e-06
Iter: 1243 loss: 1.07888195e-06
Iter: 1244 loss: 1.0782569e-06
Iter: 1245 loss: 1.07794199e-06
Iter: 1246 loss: 1.07813241e-06
Iter: 1247 loss: 1.0778042e-06
Iter: 1248 loss: 1.07735718e-06
Iter: 1249 loss: 1.08009192e-06
Iter: 1250 loss: 1.07733945e-06
Iter: 1251 loss: 1.076942e-06
Iter: 1252 loss: 1.07935966e-06
Iter: 1253 loss: 1.07695132e-06
Iter: 1254 loss: 1.07680114e-06
Iter: 1255 loss: 1.077045e-06
Iter: 1256 loss: 1.0767717e-06
Iter: 1257 loss: 1.07657991e-06
Iter: 1258 loss: 1.07687663e-06
Iter: 1259 loss: 1.07649294e-06
Iter: 1260 loss: 1.07628807e-06
Iter: 1261 loss: 1.07621565e-06
Iter: 1262 loss: 1.07613835e-06
Iter: 1263 loss: 1.07589403e-06
Iter: 1264 loss: 1.07591597e-06
Iter: 1265 loss: 1.07579683e-06
Iter: 1266 loss: 1.07568883e-06
Iter: 1267 loss: 1.07566325e-06
Iter: 1268 loss: 1.07552637e-06
Iter: 1269 loss: 1.07723736e-06
Iter: 1270 loss: 1.07549317e-06
Iter: 1271 loss: 1.0753663e-06
Iter: 1272 loss: 1.07532901e-06
Iter: 1273 loss: 1.07523715e-06
Iter: 1274 loss: 1.07508629e-06
Iter: 1275 loss: 1.07542519e-06
Iter: 1276 loss: 1.07506332e-06
Iter: 1277 loss: 1.07492247e-06
Iter: 1278 loss: 1.07477695e-06
Iter: 1279 loss: 1.07473272e-06
Iter: 1280 loss: 1.07448568e-06
Iter: 1281 loss: 1.07716278e-06
Iter: 1282 loss: 1.07448011e-06
Iter: 1283 loss: 1.07432425e-06
Iter: 1284 loss: 1.07594747e-06
Iter: 1285 loss: 1.07429969e-06
Iter: 1286 loss: 1.07415099e-06
Iter: 1287 loss: 1.07397204e-06
Iter: 1288 loss: 1.07397079e-06
Iter: 1289 loss: 1.07374888e-06
Iter: 1290 loss: 1.07671065e-06
Iter: 1291 loss: 1.07379219e-06
Iter: 1292 loss: 1.07366805e-06
Iter: 1293 loss: 1.07354037e-06
Iter: 1294 loss: 1.07350093e-06
Iter: 1295 loss: 1.07335723e-06
Iter: 1296 loss: 1.07531196e-06
Iter: 1297 loss: 1.07336768e-06
Iter: 1298 loss: 1.07320466e-06
Iter: 1299 loss: 1.07303617e-06
Iter: 1300 loss: 1.07299411e-06
Iter: 1301 loss: 1.07286621e-06
Iter: 1302 loss: 1.07286041e-06
Iter: 1303 loss: 1.07280368e-06
Iter: 1304 loss: 1.07282858e-06
Iter: 1305 loss: 1.07274491e-06
Iter: 1306 loss: 1.07259041e-06
Iter: 1307 loss: 1.07260905e-06
Iter: 1308 loss: 1.07253027e-06
Iter: 1309 loss: 1.07237361e-06
Iter: 1310 loss: 1.07278504e-06
Iter: 1311 loss: 1.07227606e-06
Iter: 1312 loss: 1.07207416e-06
Iter: 1313 loss: 1.07220581e-06
Iter: 1314 loss: 1.07195103e-06
Iter: 1315 loss: 1.07179676e-06
Iter: 1316 loss: 1.07176447e-06
Iter: 1317 loss: 1.07166784e-06
Iter: 1318 loss: 1.07160326e-06
Iter: 1319 loss: 1.07157246e-06
Iter: 1320 loss: 1.07140966e-06
Iter: 1321 loss: 1.0723279e-06
Iter: 1322 loss: 1.07143319e-06
Iter: 1323 loss: 1.07125186e-06
Iter: 1324 loss: 1.07104324e-06
Iter: 1325 loss: 1.07102073e-06
Iter: 1326 loss: 1.07085816e-06
Iter: 1327 loss: 1.0736585e-06
Iter: 1328 loss: 1.0708327e-06
Iter: 1329 loss: 1.07069582e-06
Iter: 1330 loss: 1.07088636e-06
Iter: 1331 loss: 1.07065011e-06
Iter: 1332 loss: 1.07058804e-06
Iter: 1333 loss: 1.0705852e-06
Iter: 1334 loss: 1.0704706e-06
Iter: 1335 loss: 1.07025858e-06
Iter: 1336 loss: 1.07176402e-06
Iter: 1337 loss: 1.07026176e-06
Iter: 1338 loss: 1.07013489e-06
Iter: 1339 loss: 1.0699523e-06
Iter: 1340 loss: 1.06993048e-06
Iter: 1341 loss: 1.06976904e-06
Iter: 1342 loss: 1.07015603e-06
Iter: 1343 loss: 1.06966036e-06
Iter: 1344 loss: 1.06939228e-06
Iter: 1345 loss: 1.06988375e-06
Iter: 1346 loss: 1.06929701e-06
Iter: 1347 loss: 1.06918446e-06
Iter: 1348 loss: 1.06918446e-06
Iter: 1349 loss: 1.06907237e-06
Iter: 1350 loss: 1.06894549e-06
Iter: 1351 loss: 1.06890775e-06
Iter: 1352 loss: 1.06872585e-06
Iter: 1353 loss: 1.06952632e-06
Iter: 1354 loss: 1.06870266e-06
Iter: 1355 loss: 1.06846505e-06
Iter: 1356 loss: 1.06926188e-06
Iter: 1357 loss: 1.06837172e-06
Iter: 1358 loss: 1.06828429e-06
Iter: 1359 loss: 1.06847688e-06
Iter: 1360 loss: 1.06824018e-06
Iter: 1361 loss: 1.06810876e-06
Iter: 1362 loss: 1.06841412e-06
Iter: 1363 loss: 1.06799962e-06
Iter: 1364 loss: 1.06780863e-06
Iter: 1365 loss: 1.06763787e-06
Iter: 1366 loss: 1.06761e-06
Iter: 1367 loss: 1.06746108e-06
Iter: 1368 loss: 1.06743812e-06
Iter: 1369 loss: 1.0672951e-06
Iter: 1370 loss: 1.06699349e-06
Iter: 1371 loss: 1.07274388e-06
Iter: 1372 loss: 1.06698872e-06
Iter: 1373 loss: 1.06670791e-06
Iter: 1374 loss: 1.06731216e-06
Iter: 1375 loss: 1.06657262e-06
Iter: 1376 loss: 1.06632137e-06
Iter: 1377 loss: 1.06900677e-06
Iter: 1378 loss: 1.06634479e-06
Iter: 1379 loss: 1.0662136e-06
Iter: 1380 loss: 1.06661093e-06
Iter: 1381 loss: 1.06617085e-06
Iter: 1382 loss: 1.06596542e-06
Iter: 1383 loss: 1.0665824e-06
Iter: 1384 loss: 1.06594075e-06
Iter: 1385 loss: 1.06581069e-06
Iter: 1386 loss: 1.06594575e-06
Iter: 1387 loss: 1.06574794e-06
Iter: 1388 loss: 1.06557832e-06
Iter: 1389 loss: 1.06646746e-06
Iter: 1390 loss: 1.06554603e-06
Iter: 1391 loss: 1.06540324e-06
Iter: 1392 loss: 1.065172e-06
Iter: 1393 loss: 1.06519258e-06
Iter: 1394 loss: 1.06492837e-06
Iter: 1395 loss: 1.06492553e-06
Iter: 1396 loss: 1.06478251e-06
Iter: 1397 loss: 1.06453103e-06
Iter: 1398 loss: 1.06452103e-06
Iter: 1399 loss: 1.06429161e-06
Iter: 1400 loss: 1.06732318e-06
Iter: 1401 loss: 1.06425045e-06
Iter: 1402 loss: 1.064005e-06
Iter: 1403 loss: 1.06403195e-06
Iter: 1404 loss: 1.06380207e-06
Iter: 1405 loss: 1.06358857e-06
Iter: 1406 loss: 1.06381776e-06
Iter: 1407 loss: 1.06349e-06
Iter: 1408 loss: 1.06327309e-06
Iter: 1409 loss: 1.06363154e-06
Iter: 1410 loss: 1.06316145e-06
Iter: 1411 loss: 1.06297375e-06
Iter: 1412 loss: 1.0655898e-06
Iter: 1413 loss: 1.06296693e-06
Iter: 1414 loss: 1.06287973e-06
Iter: 1415 loss: 1.06321113e-06
Iter: 1416 loss: 1.06282187e-06
Iter: 1417 loss: 1.06266009e-06
Iter: 1418 loss: 1.06239656e-06
Iter: 1419 loss: 1.0682088e-06
Iter: 1420 loss: 1.06238872e-06
Iter: 1421 loss: 1.06231096e-06
Iter: 1422 loss: 1.06220318e-06
Iter: 1423 loss: 1.06209359e-06
Iter: 1424 loss: 1.06184848e-06
Iter: 1425 loss: 1.06782022e-06
Iter: 1426 loss: 1.06187542e-06
Iter: 1427 loss: 1.06156199e-06
Iter: 1428 loss: 1.06378127e-06
Iter: 1429 loss: 1.06155244e-06
Iter: 1430 loss: 1.0612481e-06
Iter: 1431 loss: 1.06139123e-06
Iter: 1432 loss: 1.06100913e-06
Iter: 1433 loss: 1.06085304e-06
Iter: 1434 loss: 1.06077391e-06
Iter: 1435 loss: 1.06065727e-06
Iter: 1436 loss: 1.06040318e-06
Iter: 1437 loss: 1.06035588e-06
Iter: 1438 loss: 1.06019252e-06
Iter: 1439 loss: 1.05995616e-06
Iter: 1440 loss: 1.05993695e-06
Iter: 1441 loss: 1.05973788e-06
Iter: 1442 loss: 1.06040454e-06
Iter: 1443 loss: 1.05966433e-06
Iter: 1444 loss: 1.05944321e-06
Iter: 1445 loss: 1.05921106e-06
Iter: 1446 loss: 1.05915501e-06
Iter: 1447 loss: 1.05902973e-06
Iter: 1448 loss: 1.05895083e-06
Iter: 1449 loss: 1.05882214e-06
Iter: 1450 loss: 1.05886625e-06
Iter: 1451 loss: 1.05872232e-06
Iter: 1452 loss: 1.05852939e-06
Iter: 1453 loss: 1.05870629e-06
Iter: 1454 loss: 1.05846709e-06
Iter: 1455 loss: 1.05818685e-06
Iter: 1456 loss: 1.05857043e-06
Iter: 1457 loss: 1.05804656e-06
Iter: 1458 loss: 1.05793015e-06
Iter: 1459 loss: 1.05848949e-06
Iter: 1460 loss: 1.05791469e-06
Iter: 1461 loss: 1.05773609e-06
Iter: 1462 loss: 1.05854201e-06
Iter: 1463 loss: 1.05775325e-06
Iter: 1464 loss: 1.05763206e-06
Iter: 1465 loss: 1.05727167e-06
Iter: 1466 loss: 1.05999754e-06
Iter: 1467 loss: 1.05718925e-06
Iter: 1468 loss: 1.05677532e-06
Iter: 1469 loss: 1.05779486e-06
Iter: 1470 loss: 1.05660502e-06
Iter: 1471 loss: 1.05618483e-06
Iter: 1472 loss: 1.05868412e-06
Iter: 1473 loss: 1.05610911e-06
Iter: 1474 loss: 1.05600975e-06
Iter: 1475 loss: 1.05593449e-06
Iter: 1476 loss: 1.05585684e-06
Iter: 1477 loss: 1.0555832e-06
Iter: 1478 loss: 1.05622632e-06
Iter: 1479 loss: 1.05547383e-06
Iter: 1480 loss: 1.05529875e-06
Iter: 1481 loss: 1.05524668e-06
Iter: 1482 loss: 1.0551297e-06
Iter: 1483 loss: 1.05594984e-06
Iter: 1484 loss: 1.05507877e-06
Iter: 1485 loss: 1.05495633e-06
Iter: 1486 loss: 1.05481149e-06
Iter: 1487 loss: 1.05479398e-06
Iter: 1488 loss: 1.05469474e-06
Iter: 1489 loss: 1.05469553e-06
Iter: 1490 loss: 1.05459458e-06
Iter: 1491 loss: 1.05452602e-06
Iter: 1492 loss: 1.05445815e-06
Iter: 1493 loss: 1.05430581e-06
Iter: 1494 loss: 1.05403581e-06
Iter: 1495 loss: 1.06052232e-06
Iter: 1496 loss: 1.0540283e-06
Iter: 1497 loss: 1.05368508e-06
Iter: 1498 loss: 1.05367701e-06
Iter: 1499 loss: 1.05359413e-06
Iter: 1500 loss: 1.05330707e-06
Iter: 1501 loss: 1.0577628e-06
Iter: 1502 loss: 1.05330082e-06
Iter: 1503 loss: 1.05294612e-06
Iter: 1504 loss: 1.05262018e-06
Iter: 1505 loss: 1.05249546e-06
Iter: 1506 loss: 1.05252957e-06
Iter: 1507 loss: 1.0523047e-06
Iter: 1508 loss: 1.05210415e-06
Iter: 1509 loss: 1.05207096e-06
Iter: 1510 loss: 1.05186575e-06
Iter: 1511 loss: 1.05164258e-06
Iter: 1512 loss: 1.05126298e-06
Iter: 1513 loss: 1.05125798e-06
Iter: 1514 loss: 1.0510455e-06
Iter: 1515 loss: 1.05090442e-06
Iter: 1516 loss: 1.05073843e-06
Iter: 1517 loss: 1.05054357e-06
Iter: 1518 loss: 1.05051163e-06
Iter: 1519 loss: 1.05021275e-06
Iter: 1520 loss: 1.05106847e-06
Iter: 1521 loss: 1.05006507e-06
Iter: 1522 loss: 1.04986714e-06
Iter: 1523 loss: 1.04984952e-06
Iter: 1524 loss: 1.04968103e-06
Iter: 1525 loss: 1.0496326e-06
Iter: 1526 loss: 1.04962692e-06
Iter: 1527 loss: 1.0494457e-06
Iter: 1528 loss: 1.05080971e-06
Iter: 1529 loss: 1.04941103e-06
Iter: 1530 loss: 1.04921662e-06
Iter: 1531 loss: 1.04906849e-06
Iter: 1532 loss: 1.04902915e-06
Iter: 1533 loss: 1.04882099e-06
Iter: 1534 loss: 1.04872322e-06
Iter: 1535 loss: 1.04864216e-06
Iter: 1536 loss: 1.0482687e-06
Iter: 1537 loss: 1.04937305e-06
Iter: 1538 loss: 1.04820367e-06
Iter: 1539 loss: 1.04794151e-06
Iter: 1540 loss: 1.04794799e-06
Iter: 1541 loss: 1.04774813e-06
Iter: 1542 loss: 1.04743492e-06
Iter: 1543 loss: 1.05467927e-06
Iter: 1544 loss: 1.04741935e-06
Iter: 1545 loss: 1.04709773e-06
Iter: 1546 loss: 1.05095592e-06
Iter: 1547 loss: 1.04709477e-06
Iter: 1548 loss: 1.04683249e-06
Iter: 1549 loss: 1.04730987e-06
Iter: 1550 loss: 1.04669562e-06
Iter: 1551 loss: 1.04652611e-06
Iter: 1552 loss: 1.04656465e-06
Iter: 1553 loss: 1.04639571e-06
Iter: 1554 loss: 1.04619971e-06
Iter: 1555 loss: 1.0488493e-06
Iter: 1556 loss: 1.04618084e-06
Iter: 1557 loss: 1.04595927e-06
Iter: 1558 loss: 1.04608989e-06
Iter: 1559 loss: 1.04582239e-06
Iter: 1560 loss: 1.04568039e-06
Iter: 1561 loss: 1.0460551e-06
Iter: 1562 loss: 1.04562923e-06
Iter: 1563 loss: 1.04544176e-06
Iter: 1564 loss: 1.0465094e-06
Iter: 1565 loss: 1.04544e-06
Iter: 1566 loss: 1.04530955e-06
Iter: 1567 loss: 1.04506466e-06
Iter: 1568 loss: 1.04508581e-06
Iter: 1569 loss: 1.04480523e-06
Iter: 1570 loss: 1.0446106e-06
Iter: 1571 loss: 1.04452829e-06
Iter: 1572 loss: 1.04438834e-06
Iter: 1573 loss: 1.04431774e-06
Iter: 1574 loss: 1.04412061e-06
Iter: 1575 loss: 1.04449009e-06
Iter: 1576 loss: 1.04400613e-06
Iter: 1577 loss: 1.04389255e-06
Iter: 1578 loss: 1.04377193e-06
Iter: 1579 loss: 1.04373044e-06
Iter: 1580 loss: 1.04342962e-06
Iter: 1581 loss: 1.04559058e-06
Iter: 1582 loss: 1.0434228e-06
Iter: 1583 loss: 1.04330366e-06
Iter: 1584 loss: 1.04312062e-06
Iter: 1585 loss: 1.04306127e-06
Iter: 1586 loss: 1.04274386e-06
Iter: 1587 loss: 1.04273272e-06
Iter: 1588 loss: 1.04253627e-06
Iter: 1589 loss: 1.04232868e-06
Iter: 1590 loss: 1.04221056e-06
Iter: 1591 loss: 1.04210562e-06
Iter: 1592 loss: 1.04188973e-06
Iter: 1593 loss: 1.04187393e-06
Iter: 1594 loss: 1.04164292e-06
Iter: 1595 loss: 1.04359526e-06
Iter: 1596 loss: 1.04167657e-06
Iter: 1597 loss: 1.04136871e-06
Iter: 1598 loss: 1.04129936e-06
Iter: 1599 loss: 1.04115634e-06
Iter: 1600 loss: 1.04091191e-06
Iter: 1601 loss: 1.04158858e-06
Iter: 1602 loss: 1.04086621e-06
Iter: 1603 loss: 1.0406402e-06
Iter: 1604 loss: 1.04036462e-06
Iter: 1605 loss: 1.04039611e-06
Iter: 1606 loss: 1.04014441e-06
Iter: 1607 loss: 1.04010655e-06
Iter: 1608 loss: 1.03989862e-06
Iter: 1609 loss: 1.0396916e-06
Iter: 1610 loss: 1.0396609e-06
Iter: 1611 loss: 1.0394117e-06
Iter: 1612 loss: 1.04132744e-06
Iter: 1613 loss: 1.03938237e-06
Iter: 1614 loss: 1.03897719e-06
Iter: 1615 loss: 1.03843877e-06
Iter: 1616 loss: 1.03846219e-06
Iter: 1617 loss: 1.03803677e-06
Iter: 1618 loss: 1.03988316e-06
Iter: 1619 loss: 1.037934e-06
Iter: 1620 loss: 1.03768355e-06
Iter: 1621 loss: 1.03856382e-06
Iter: 1622 loss: 1.03764819e-06
Iter: 1623 loss: 1.03730827e-06
Iter: 1624 loss: 1.03988452e-06
Iter: 1625 loss: 1.03729064e-06
Iter: 1626 loss: 1.03720436e-06
Iter: 1627 loss: 1.03723914e-06
Iter: 1628 loss: 1.03713251e-06
Iter: 1629 loss: 1.03704906e-06
Iter: 1630 loss: 1.03800971e-06
Iter: 1631 loss: 1.03703462e-06
Iter: 1632 loss: 1.03691775e-06
Iter: 1633 loss: 1.03664308e-06
Iter: 1634 loss: 1.03887851e-06
Iter: 1635 loss: 1.03658658e-06
Iter: 1636 loss: 1.03631669e-06
Iter: 1637 loss: 1.0384083e-06
Iter: 1638 loss: 1.03627929e-06
Iter: 1639 loss: 1.03607294e-06
Iter: 1640 loss: 1.03712728e-06
Iter: 1641 loss: 1.03607624e-06
Iter: 1642 loss: 1.03577179e-06
Iter: 1643 loss: 1.03627076e-06
Iter: 1644 loss: 1.03561831e-06
Iter: 1645 loss: 1.03533557e-06
Iter: 1646 loss: 1.03580896e-06
Iter: 1647 loss: 1.03530192e-06
Iter: 1648 loss: 1.03500872e-06
Iter: 1649 loss: 1.03727325e-06
Iter: 1650 loss: 1.03502953e-06
Iter: 1651 loss: 1.03488219e-06
Iter: 1652 loss: 1.03451634e-06
Iter: 1653 loss: 1.03830121e-06
Iter: 1654 loss: 1.03446951e-06
Iter: 1655 loss: 1.03418131e-06
Iter: 1656 loss: 1.03609102e-06
Iter: 1657 loss: 1.03417574e-06
Iter: 1658 loss: 1.0338988e-06
Iter: 1659 loss: 1.03379182e-06
Iter: 1660 loss: 1.03368291e-06
Iter: 1661 loss: 1.03371758e-06
Iter: 1662 loss: 1.0335508e-06
Iter: 1663 loss: 1.03338402e-06
Iter: 1664 loss: 1.03326579e-06
Iter: 1665 loss: 1.03323441e-06
Iter: 1666 loss: 1.03306604e-06
Iter: 1667 loss: 1.03342518e-06
Iter: 1668 loss: 1.03301375e-06
Iter: 1669 loss: 1.03279717e-06
Iter: 1670 loss: 1.03431239e-06
Iter: 1671 loss: 1.03278853e-06
Iter: 1672 loss: 1.03274272e-06
Iter: 1673 loss: 1.03258822e-06
Iter: 1674 loss: 1.0325881e-06
Iter: 1675 loss: 1.03235811e-06
Iter: 1676 loss: 1.03379273e-06
Iter: 1677 loss: 1.03234993e-06
Iter: 1678 loss: 1.03220395e-06
Iter: 1679 loss: 1.0320598e-06
Iter: 1680 loss: 1.03204434e-06
Iter: 1681 loss: 1.03181401e-06
Iter: 1682 loss: 1.03369746e-06
Iter: 1683 loss: 1.03180957e-06
Iter: 1684 loss: 1.03158493e-06
Iter: 1685 loss: 1.03138461e-06
Iter: 1686 loss: 1.03136745e-06
Iter: 1687 loss: 1.03110176e-06
Iter: 1688 loss: 1.03126411e-06
Iter: 1689 loss: 1.0309418e-06
Iter: 1690 loss: 1.03069203e-06
Iter: 1691 loss: 1.03081629e-06
Iter: 1692 loss: 1.03051946e-06
Iter: 1693 loss: 1.03030402e-06
Iter: 1694 loss: 1.0333315e-06
Iter: 1695 loss: 1.03028162e-06
Iter: 1696 loss: 1.0301884e-06
Iter: 1697 loss: 1.03178093e-06
Iter: 1698 loss: 1.03018624e-06
Iter: 1699 loss: 1.03007187e-06
Iter: 1700 loss: 1.02989668e-06
Iter: 1701 loss: 1.0299309e-06
Iter: 1702 loss: 1.02985e-06
Iter: 1703 loss: 1.02982403e-06
Iter: 1704 loss: 1.02970068e-06
Iter: 1705 loss: 1.02983427e-06
Iter: 1706 loss: 1.02967851e-06
Iter: 1707 loss: 1.02957893e-06
Iter: 1708 loss: 1.02982756e-06
Iter: 1709 loss: 1.02954448e-06
Iter: 1710 loss: 1.02940453e-06
Iter: 1711 loss: 1.02925651e-06
Iter: 1712 loss: 1.02917784e-06
Iter: 1713 loss: 1.02900663e-06
Iter: 1714 loss: 1.03022126e-06
Iter: 1715 loss: 1.02902163e-06
Iter: 1716 loss: 1.02884167e-06
Iter: 1717 loss: 1.02947968e-06
Iter: 1718 loss: 1.0287979e-06
Iter: 1719 loss: 1.02864146e-06
Iter: 1720 loss: 1.02835384e-06
Iter: 1721 loss: 1.02838794e-06
Iter: 1722 loss: 1.02806962e-06
Iter: 1723 loss: 1.03037826e-06
Iter: 1724 loss: 1.02805075e-06
Iter: 1725 loss: 1.02788715e-06
Iter: 1726 loss: 1.02778517e-06
Iter: 1727 loss: 1.02774925e-06
Iter: 1728 loss: 1.02755826e-06
Iter: 1729 loss: 1.02753677e-06
Iter: 1730 loss: 1.02742786e-06
Iter: 1731 loss: 1.02880711e-06
Iter: 1732 loss: 1.02738784e-06
Iter: 1733 loss: 1.02737363e-06
Iter: 1734 loss: 1.02721071e-06
Iter: 1735 loss: 1.02794206e-06
Iter: 1736 loss: 1.02714239e-06
Iter: 1737 loss: 1.02700051e-06
Iter: 1738 loss: 1.02696981e-06
Iter: 1739 loss: 1.02684771e-06
Iter: 1740 loss: 1.02682736e-06
Iter: 1741 loss: 1.02671277e-06
Iter: 1742 loss: 1.02660579e-06
Iter: 1743 loss: 1.02816898e-06
Iter: 1744 loss: 1.02662807e-06
Iter: 1745 loss: 1.02653519e-06
Iter: 1746 loss: 1.02632282e-06
Iter: 1747 loss: 1.02895979e-06
Iter: 1748 loss: 1.0263135e-06
Iter: 1749 loss: 1.02618674e-06
Iter: 1750 loss: 1.02617173e-06
Iter: 1751 loss: 1.02605441e-06
Iter: 1752 loss: 1.02606236e-06
Iter: 1753 loss: 1.02593719e-06
Iter: 1754 loss: 1.02575768e-06
Iter: 1755 loss: 1.02548165e-06
Iter: 1756 loss: 1.02547835e-06
Iter: 1757 loss: 1.02524632e-06
Iter: 1758 loss: 1.02770503e-06
Iter: 1759 loss: 1.02521153e-06
Iter: 1760 loss: 1.02501315e-06
Iter: 1761 loss: 1.02478441e-06
Iter: 1762 loss: 1.02475246e-06
Iter: 1763 loss: 1.02452236e-06
Iter: 1764 loss: 1.0244969e-06
Iter: 1765 loss: 1.02432352e-06
Iter: 1766 loss: 1.0243009e-06
Iter: 1767 loss: 1.02421973e-06
Iter: 1768 loss: 1.0240318e-06
Iter: 1769 loss: 1.02671356e-06
Iter: 1770 loss: 1.02403453e-06
Iter: 1771 loss: 1.02396643e-06
Iter: 1772 loss: 1.02395927e-06
Iter: 1773 loss: 1.02386684e-06
Iter: 1774 loss: 1.02383183e-06
Iter: 1775 loss: 1.02379943e-06
Iter: 1776 loss: 1.02371018e-06
Iter: 1777 loss: 1.02402782e-06
Iter: 1778 loss: 1.02366209e-06
Iter: 1779 loss: 1.02352124e-06
Iter: 1780 loss: 1.02358104e-06
Iter: 1781 loss: 1.02342642e-06
Iter: 1782 loss: 1.02331205e-06
Iter: 1783 loss: 1.02304921e-06
Iter: 1784 loss: 1.02809122e-06
Iter: 1785 loss: 1.02304102e-06
Iter: 1786 loss: 1.02273327e-06
Iter: 1787 loss: 1.02355148e-06
Iter: 1788 loss: 1.02260901e-06
Iter: 1789 loss: 1.02242893e-06
Iter: 1790 loss: 1.02235583e-06
Iter: 1791 loss: 1.02231843e-06
Iter: 1792 loss: 1.02205718e-06
Iter: 1793 loss: 1.02364015e-06
Iter: 1794 loss: 1.02200534e-06
Iter: 1795 loss: 1.02174204e-06
Iter: 1796 loss: 1.02322542e-06
Iter: 1797 loss: 1.02174761e-06
Iter: 1798 loss: 1.02168951e-06
Iter: 1799 loss: 1.02167428e-06
Iter: 1800 loss: 1.02160914e-06
Iter: 1801 loss: 1.02154786e-06
Iter: 1802 loss: 1.02150864e-06
Iter: 1803 loss: 1.02142928e-06
Iter: 1804 loss: 1.0212666e-06
Iter: 1805 loss: 1.02128377e-06
Iter: 1806 loss: 1.02109129e-06
Iter: 1807 loss: 1.02182344e-06
Iter: 1808 loss: 1.02109152e-06
Iter: 1809 loss: 1.02078104e-06
Iter: 1810 loss: 1.02079161e-06
Iter: 1811 loss: 1.02055833e-06
Iter: 1812 loss: 1.02057652e-06
Iter: 1813 loss: 1.0204659e-06
Iter: 1814 loss: 1.02032436e-06
Iter: 1815 loss: 1.02008914e-06
Iter: 1816 loss: 1.02389083e-06
Iter: 1817 loss: 1.02008028e-06
Iter: 1818 loss: 1.01983949e-06
Iter: 1819 loss: 1.0208621e-06
Iter: 1820 loss: 1.01981129e-06
Iter: 1821 loss: 1.01972398e-06
Iter: 1822 loss: 1.01964747e-06
Iter: 1823 loss: 1.01960632e-06
Iter: 1824 loss: 1.01949104e-06
Iter: 1825 loss: 1.02084641e-06
Iter: 1826 loss: 1.01946864e-06
Iter: 1827 loss: 1.01933358e-06
Iter: 1828 loss: 1.01931562e-06
Iter: 1829 loss: 1.01926253e-06
Iter: 1830 loss: 1.01910678e-06
Iter: 1831 loss: 1.02167064e-06
Iter: 1832 loss: 1.019107e-06
Iter: 1833 loss: 1.01894466e-06
Iter: 1834 loss: 1.02034937e-06
Iter: 1835 loss: 1.01898718e-06
Iter: 1836 loss: 1.01882472e-06
Iter: 1837 loss: 1.01906164e-06
Iter: 1838 loss: 1.01878e-06
Iter: 1839 loss: 1.0186784e-06
Iter: 1840 loss: 1.01863066e-06
Iter: 1841 loss: 1.01856119e-06
Iter: 1842 loss: 1.01845808e-06
Iter: 1843 loss: 1.0186294e-06
Iter: 1844 loss: 1.01837577e-06
Iter: 1845 loss: 1.01824116e-06
Iter: 1846 loss: 1.01821786e-06
Iter: 1847 loss: 1.01813612e-06
Iter: 1848 loss: 1.01837327e-06
Iter: 1849 loss: 1.01811929e-06
Iter: 1850 loss: 1.01796945e-06
Iter: 1851 loss: 1.01777e-06
Iter: 1852 loss: 1.02263925e-06
Iter: 1853 loss: 1.01777573e-06
Iter: 1854 loss: 1.01760202e-06
Iter: 1855 loss: 1.01989531e-06
Iter: 1856 loss: 1.01761373e-06
Iter: 1857 loss: 1.01744831e-06
Iter: 1858 loss: 1.01883234e-06
Iter: 1859 loss: 1.01743183e-06
Iter: 1860 loss: 1.01735418e-06
Iter: 1861 loss: 1.01718479e-06
Iter: 1862 loss: 1.01718865e-06
Iter: 1863 loss: 1.01708429e-06
Iter: 1864 loss: 1.01706621e-06
Iter: 1865 loss: 1.01703836e-06
Iter: 1866 loss: 1.01691217e-06
Iter: 1867 loss: 1.01815749e-06
Iter: 1868 loss: 1.0168942e-06
Iter: 1869 loss: 1.01682883e-06
Iter: 1870 loss: 1.01681314e-06
Iter: 1871 loss: 1.01672572e-06
Iter: 1872 loss: 1.01677153e-06
Iter: 1873 loss: 1.01664091e-06
Iter: 1874 loss: 1.01658088e-06
Iter: 1875 loss: 1.0164548e-06
Iter: 1876 loss: 1.01877822e-06
Iter: 1877 loss: 1.01646663e-06
Iter: 1878 loss: 1.01627506e-06
Iter: 1879 loss: 1.01652381e-06
Iter: 1880 loss: 1.01616638e-06
Iter: 1881 loss: 1.01610601e-06
Iter: 1882 loss: 1.01603769e-06
Iter: 1883 loss: 1.01588375e-06
Iter: 1884 loss: 1.01577416e-06
Iter: 1885 loss: 1.01579008e-06
Iter: 1886 loss: 1.01547994e-06
Iter: 1887 loss: 1.01545311e-06
Iter: 1888 loss: 1.01524756e-06
Iter: 1889 loss: 1.01507703e-06
Iter: 1890 loss: 1.0165611e-06
Iter: 1891 loss: 1.01501178e-06
Iter: 1892 loss: 1.01490127e-06
Iter: 1893 loss: 1.01492094e-06
Iter: 1894 loss: 1.0148176e-06
Iter: 1895 loss: 1.01469982e-06
Iter: 1896 loss: 1.01470073e-06
Iter: 1897 loss: 1.01461956e-06
Iter: 1898 loss: 1.0146149e-06
Iter: 1899 loss: 1.01452861e-06
Iter: 1900 loss: 1.01456055e-06
Iter: 1901 loss: 1.01445789e-06
Iter: 1902 loss: 1.01444107e-06
Iter: 1903 loss: 1.01451269e-06
Iter: 1904 loss: 1.01438582e-06
Iter: 1905 loss: 1.01423484e-06
Iter: 1906 loss: 1.01470187e-06
Iter: 1907 loss: 1.01417993e-06
Iter: 1908 loss: 1.01410058e-06
Iter: 1909 loss: 1.01394528e-06
Iter: 1910 loss: 1.01624903e-06
Iter: 1911 loss: 1.01393391e-06
Iter: 1912 loss: 1.01376986e-06
Iter: 1913 loss: 1.0156441e-06
Iter: 1914 loss: 1.01376236e-06
Iter: 1915 loss: 1.01367903e-06
Iter: 1916 loss: 1.01366049e-06
Iter: 1917 loss: 1.01359433e-06
Iter: 1918 loss: 1.01338696e-06
Iter: 1919 loss: 1.01492617e-06
Iter: 1920 loss: 1.01334876e-06
Iter: 1921 loss: 1.01319085e-06
Iter: 1922 loss: 1.01393948e-06
Iter: 1923 loss: 1.01310297e-06
Iter: 1924 loss: 1.01296e-06
Iter: 1925 loss: 1.01329363e-06
Iter: 1926 loss: 1.01287299e-06
Iter: 1927 loss: 1.01276964e-06
Iter: 1928 loss: 1.01275975e-06
Iter: 1929 loss: 1.01266471e-06
Iter: 1930 loss: 1.01251e-06
Iter: 1931 loss: 1.01365163e-06
Iter: 1932 loss: 1.01249088e-06
Iter: 1933 loss: 1.01234491e-06
Iter: 1934 loss: 1.01234764e-06
Iter: 1935 loss: 1.01222793e-06
Iter: 1936 loss: 1.01223941e-06
Iter: 1937 loss: 1.01219348e-06
Iter: 1938 loss: 1.0120915e-06
Iter: 1939 loss: 1.01324542e-06
Iter: 1940 loss: 1.0120832e-06
Iter: 1941 loss: 1.0120209e-06
Iter: 1942 loss: 1.01188834e-06
Iter: 1943 loss: 1.014379e-06
Iter: 1944 loss: 1.01192245e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi0.8
+ date
Sun Nov  8 04:42:36 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4/300_100_100_100_1 --function f1 --psi -1 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c40a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c4279d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c427e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c427378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c427488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c427598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c32df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c2a2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c2b52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c2a2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c219950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c21ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c2207b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c2206a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c293a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c207598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c2077b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c1e6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c13b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c14af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c0e2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c1a96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c0568c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c056620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c055510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c0782f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c0a0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c0c3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c01d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f0c02a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5ef01b0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5ef020b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5ef020bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5ef0217ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5ef0127840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5ef00d3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0028880422
test_loss: 0.0028944872
train_loss: 0.002564558
test_loss: 0.0026261213
train_loss: 0.0023865649
test_loss: 0.0026963693
train_loss: 0.0023549965
test_loss: 0.0025861487
train_loss: 0.0024654856
test_loss: 0.0024622926
train_loss: 0.0024798096
test_loss: 0.002672992
train_loss: 0.0025202772
test_loss: 0.0025976603
train_loss: 0.0025938465
test_loss: 0.0024744347
train_loss: 0.0023560831
test_loss: 0.0025499528
train_loss: 0.0024201588
test_loss: 0.002409572
train_loss: 0.0024107033
test_loss: 0.002516192
train_loss: 0.0021784105
test_loss: 0.0024035305
train_loss: 0.0027118349
test_loss: 0.0028769325
train_loss: 0.0021745865
test_loss: 0.0022859164
train_loss: 0.0023765157
test_loss: 0.0026584377
train_loss: 0.0022590891
test_loss: 0.0022918764
train_loss: 0.0022535713
test_loss: 0.0022959996
train_loss: 0.0022442883
test_loss: 0.0024753672
train_loss: 0.0023135967
test_loss: 0.0023347817
train_loss: 0.0021753465
test_loss: 0.0023335426
train_loss: 0.0021328367
test_loss: 0.0023000487
train_loss: 0.0021942772
test_loss: 0.002325152
train_loss: 0.0022567215
test_loss: 0.0024300856
train_loss: 0.002263845
test_loss: 0.0022469456
train_loss: 0.0027120588
test_loss: 0.002560327
train_loss: 0.0021520515
test_loss: 0.002238768
train_loss: 0.0021291808
test_loss: 0.0022901252
train_loss: 0.0022182702
test_loss: 0.002387316
train_loss: 0.002031462
test_loss: 0.0022723076
train_loss: 0.0019459727
test_loss: 0.002214995
train_loss: 0.0022228449
test_loss: 0.0024233393
train_loss: 0.0021917208
test_loss: 0.002285782
train_loss: 0.002106859
test_loss: 0.0022575571
train_loss: 0.0021026826
test_loss: 0.0023107657
train_loss: 0.0023048746
test_loss: 0.0026124115
train_loss: 0.0024550338
test_loss: 0.0026039742
train_loss: 0.0020538971
test_loss: 0.0024766496
train_loss: 0.002542465
test_loss: 0.0023430076
train_loss: 0.0021089301
test_loss: 0.0022627972
train_loss: 0.002064742
test_loss: 0.0022403686
train_loss: 0.0021801002
test_loss: 0.00227731
train_loss: 0.002280071
test_loss: 0.0022776066
train_loss: 0.0020582075
test_loss: 0.0023837956
train_loss: 0.0022602384
test_loss: 0.0022410923
train_loss: 0.0020647866
test_loss: 0.0022753023
train_loss: 0.0021812147
test_loss: 0.0022922712
train_loss: 0.0021646244
test_loss: 0.0023576578
train_loss: 0.0020990237
test_loss: 0.0021554965
train_loss: 0.001982844
test_loss: 0.0024755225
train_loss: 0.002198377
test_loss: 0.0022222593
train_loss: 0.0020830692
test_loss: 0.0021380137
train_loss: 0.0020369666
test_loss: 0.002241707
train_loss: 0.0021632828
test_loss: 0.002380653
train_loss: 0.002136213
test_loss: 0.002334734
train_loss: 0.0020817423
test_loss: 0.002487677
train_loss: 0.002101265
test_loss: 0.002152402
train_loss: 0.0022056457
test_loss: 0.0027148738
train_loss: 0.0023074872
test_loss: 0.002345953
train_loss: 0.0020838857
test_loss: 0.0023077182
train_loss: 0.0020779604
test_loss: 0.0021552257
train_loss: 0.0021008062
test_loss: 0.0021536702
train_loss: 0.0018958452
test_loss: 0.0022227436
train_loss: 0.0021278863
test_loss: 0.0022737377
train_loss: 0.0020289824
test_loss: 0.0021644584
train_loss: 0.001966885
test_loss: 0.0021573904
train_loss: 0.0020483625
test_loss: 0.0021411104
train_loss: 0.0019743536
test_loss: 0.00224476
train_loss: 0.0019909197
test_loss: 0.0022750897
train_loss: 0.0020384386
test_loss: 0.002357946
train_loss: 0.002132592
test_loss: 0.0023177129
train_loss: 0.0020383443
test_loss: 0.0022072808
train_loss: 0.0019859225
test_loss: 0.0021998738
train_loss: 0.0020193774
test_loss: 0.0021172569
train_loss: 0.0021049937
test_loss: 0.0022016908
train_loss: 0.0021871554
test_loss: 0.0023149422
train_loss: 0.0021141437
test_loss: 0.0025096177
train_loss: 0.0020294976
test_loss: 0.0024687678
train_loss: 0.0020133925
test_loss: 0.0022401533
train_loss: 0.0019409414
test_loss: 0.0022600444
train_loss: 0.0018615916
test_loss: 0.0021125404
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi0.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -1 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi0.8/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3fe31e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb4037e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb4037ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3f8c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3f8cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3efc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3ed0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3e7d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3e7d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3e70a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3e31158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3e36d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3dfc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3d97598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3dfcea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3d97c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3d8c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3d976a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3d06620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3d03f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfb3ca5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf767fc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf767c8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf767d6620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf767cf730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf76792378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf76737b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf76766378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf767660d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf767256a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf507b07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf507ae400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf50768840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf50792620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf50743b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbf506f12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.04543493e-06
Iter: 2 loss: 7.62214768e-06
Iter: 3 loss: 5.26198619e-06
Iter: 4 loss: 4.91841365e-06
Iter: 5 loss: 5.68725363e-06
Iter: 6 loss: 4.78789343e-06
Iter: 7 loss: 4.59656212e-06
Iter: 8 loss: 5.28846795e-06
Iter: 9 loss: 4.54838755e-06
Iter: 10 loss: 4.33672449e-06
Iter: 11 loss: 4.68637427e-06
Iter: 12 loss: 4.24039035e-06
Iter: 13 loss: 4.09428776e-06
Iter: 14 loss: 4.34322283e-06
Iter: 15 loss: 4.02870046e-06
Iter: 16 loss: 3.98733255e-06
Iter: 17 loss: 3.97613849e-06
Iter: 18 loss: 3.93139953e-06
Iter: 19 loss: 3.8762837e-06
Iter: 20 loss: 3.87138607e-06
Iter: 21 loss: 3.79832272e-06
Iter: 22 loss: 4.12103645e-06
Iter: 23 loss: 3.78372488e-06
Iter: 24 loss: 3.69624036e-06
Iter: 25 loss: 3.83007409e-06
Iter: 26 loss: 3.65480241e-06
Iter: 27 loss: 3.57949398e-06
Iter: 28 loss: 3.45500166e-06
Iter: 29 loss: 3.45449371e-06
Iter: 30 loss: 3.2893463e-06
Iter: 31 loss: 3.48179447e-06
Iter: 32 loss: 3.20145455e-06
Iter: 33 loss: 3.10476207e-06
Iter: 34 loss: 3.10394307e-06
Iter: 35 loss: 3.04183231e-06
Iter: 36 loss: 3.38431096e-06
Iter: 37 loss: 3.03289721e-06
Iter: 38 loss: 2.97076167e-06
Iter: 39 loss: 3.44684895e-06
Iter: 40 loss: 2.96612916e-06
Iter: 41 loss: 2.94024812e-06
Iter: 42 loss: 2.94640836e-06
Iter: 43 loss: 2.92126333e-06
Iter: 44 loss: 2.87710486e-06
Iter: 45 loss: 3.03095749e-06
Iter: 46 loss: 2.86547561e-06
Iter: 47 loss: 2.83640088e-06
Iter: 48 loss: 2.80917538e-06
Iter: 49 loss: 2.80233e-06
Iter: 50 loss: 2.77927916e-06
Iter: 51 loss: 2.77609524e-06
Iter: 52 loss: 2.75317143e-06
Iter: 53 loss: 2.74051104e-06
Iter: 54 loss: 2.7303779e-06
Iter: 55 loss: 2.69346219e-06
Iter: 56 loss: 2.71462227e-06
Iter: 57 loss: 2.66938196e-06
Iter: 58 loss: 2.61047421e-06
Iter: 59 loss: 2.90559569e-06
Iter: 60 loss: 2.60063121e-06
Iter: 61 loss: 2.57343618e-06
Iter: 62 loss: 2.54805377e-06
Iter: 63 loss: 2.54167321e-06
Iter: 64 loss: 2.4947858e-06
Iter: 65 loss: 2.56945646e-06
Iter: 66 loss: 2.47291609e-06
Iter: 67 loss: 2.42531019e-06
Iter: 68 loss: 2.505054e-06
Iter: 69 loss: 2.40371696e-06
Iter: 70 loss: 2.36033293e-06
Iter: 71 loss: 2.67179939e-06
Iter: 72 loss: 2.35644302e-06
Iter: 73 loss: 2.33655237e-06
Iter: 74 loss: 2.33649644e-06
Iter: 75 loss: 2.31469721e-06
Iter: 76 loss: 2.34155664e-06
Iter: 77 loss: 2.30341766e-06
Iter: 78 loss: 2.28307636e-06
Iter: 79 loss: 2.29108036e-06
Iter: 80 loss: 2.26913335e-06
Iter: 81 loss: 2.2392046e-06
Iter: 82 loss: 2.44157241e-06
Iter: 83 loss: 2.23623556e-06
Iter: 84 loss: 2.21824939e-06
Iter: 85 loss: 2.19056415e-06
Iter: 86 loss: 2.19013282e-06
Iter: 87 loss: 2.18436344e-06
Iter: 88 loss: 2.17556067e-06
Iter: 89 loss: 2.16345506e-06
Iter: 90 loss: 2.1402534e-06
Iter: 91 loss: 2.63357379e-06
Iter: 92 loss: 2.14018451e-06
Iter: 93 loss: 2.1226258e-06
Iter: 94 loss: 2.40037821e-06
Iter: 95 loss: 2.12257805e-06
Iter: 96 loss: 2.10633971e-06
Iter: 97 loss: 2.10011308e-06
Iter: 98 loss: 2.09136351e-06
Iter: 99 loss: 2.07124049e-06
Iter: 100 loss: 2.06919367e-06
Iter: 101 loss: 2.05452193e-06
Iter: 102 loss: 2.03274453e-06
Iter: 103 loss: 2.09693758e-06
Iter: 104 loss: 2.02605679e-06
Iter: 105 loss: 1.99875922e-06
Iter: 106 loss: 2.06085883e-06
Iter: 107 loss: 1.98859971e-06
Iter: 108 loss: 1.96583642e-06
Iter: 109 loss: 1.99998544e-06
Iter: 110 loss: 1.95493249e-06
Iter: 111 loss: 1.95062e-06
Iter: 112 loss: 1.94214954e-06
Iter: 113 loss: 1.93362484e-06
Iter: 114 loss: 1.91952859e-06
Iter: 115 loss: 1.91947674e-06
Iter: 116 loss: 1.91314689e-06
Iter: 117 loss: 1.91257823e-06
Iter: 118 loss: 1.90616811e-06
Iter: 119 loss: 1.89337379e-06
Iter: 120 loss: 2.14677107e-06
Iter: 121 loss: 1.8933199e-06
Iter: 122 loss: 1.88398303e-06
Iter: 123 loss: 1.88398258e-06
Iter: 124 loss: 1.87618866e-06
Iter: 125 loss: 1.90776518e-06
Iter: 126 loss: 1.87436217e-06
Iter: 127 loss: 1.86853276e-06
Iter: 128 loss: 1.85560089e-06
Iter: 129 loss: 2.0385055e-06
Iter: 130 loss: 1.85493911e-06
Iter: 131 loss: 1.84016255e-06
Iter: 132 loss: 1.84015198e-06
Iter: 133 loss: 1.8335536e-06
Iter: 134 loss: 1.81929158e-06
Iter: 135 loss: 2.03353397e-06
Iter: 136 loss: 1.81873463e-06
Iter: 137 loss: 1.80273048e-06
Iter: 138 loss: 1.83335817e-06
Iter: 139 loss: 1.79598658e-06
Iter: 140 loss: 1.7776199e-06
Iter: 141 loss: 1.87429919e-06
Iter: 142 loss: 1.77474317e-06
Iter: 143 loss: 1.76576657e-06
Iter: 144 loss: 1.80273639e-06
Iter: 145 loss: 1.76379626e-06
Iter: 146 loss: 1.7570112e-06
Iter: 147 loss: 1.75689775e-06
Iter: 148 loss: 1.75176092e-06
Iter: 149 loss: 1.74457705e-06
Iter: 150 loss: 1.74432876e-06
Iter: 151 loss: 1.73921171e-06
Iter: 152 loss: 1.73923263e-06
Iter: 153 loss: 1.73500041e-06
Iter: 154 loss: 1.73804347e-06
Iter: 155 loss: 1.73246326e-06
Iter: 156 loss: 1.72675163e-06
Iter: 157 loss: 1.72064358e-06
Iter: 158 loss: 1.7197209e-06
Iter: 159 loss: 1.71169893e-06
Iter: 160 loss: 1.71152533e-06
Iter: 161 loss: 1.70795511e-06
Iter: 162 loss: 1.70292878e-06
Iter: 163 loss: 1.70272619e-06
Iter: 164 loss: 1.69615817e-06
Iter: 165 loss: 1.73297155e-06
Iter: 166 loss: 1.6952456e-06
Iter: 167 loss: 1.68698784e-06
Iter: 168 loss: 1.68667884e-06
Iter: 169 loss: 1.68034512e-06
Iter: 170 loss: 1.67411872e-06
Iter: 171 loss: 1.67656367e-06
Iter: 172 loss: 1.66991697e-06
Iter: 173 loss: 1.66204802e-06
Iter: 174 loss: 1.67459848e-06
Iter: 175 loss: 1.6584213e-06
Iter: 176 loss: 1.64846983e-06
Iter: 177 loss: 1.66998348e-06
Iter: 178 loss: 1.64470362e-06
Iter: 179 loss: 1.63659706e-06
Iter: 180 loss: 1.68823976e-06
Iter: 181 loss: 1.63569848e-06
Iter: 182 loss: 1.63133961e-06
Iter: 183 loss: 1.63082495e-06
Iter: 184 loss: 1.62842389e-06
Iter: 185 loss: 1.6214226e-06
Iter: 186 loss: 1.64812263e-06
Iter: 187 loss: 1.61837772e-06
Iter: 188 loss: 1.61838852e-06
Iter: 189 loss: 1.61450407e-06
Iter: 190 loss: 1.61183686e-06
Iter: 191 loss: 1.6110846e-06
Iter: 192 loss: 1.60942432e-06
Iter: 193 loss: 1.60624472e-06
Iter: 194 loss: 1.62199717e-06
Iter: 195 loss: 1.60568356e-06
Iter: 196 loss: 1.60152899e-06
Iter: 197 loss: 1.60063519e-06
Iter: 198 loss: 1.59790181e-06
Iter: 199 loss: 1.59349679e-06
Iter: 200 loss: 1.59273225e-06
Iter: 201 loss: 1.58971238e-06
Iter: 202 loss: 1.58446278e-06
Iter: 203 loss: 1.58443947e-06
Iter: 204 loss: 1.58089483e-06
Iter: 205 loss: 1.57336308e-06
Iter: 206 loss: 1.69285352e-06
Iter: 207 loss: 1.5730551e-06
Iter: 208 loss: 1.56555711e-06
Iter: 209 loss: 1.61009507e-06
Iter: 210 loss: 1.56459578e-06
Iter: 211 loss: 1.55947816e-06
Iter: 212 loss: 1.56974079e-06
Iter: 213 loss: 1.55747557e-06
Iter: 214 loss: 1.55514249e-06
Iter: 215 loss: 1.55488669e-06
Iter: 216 loss: 1.55239252e-06
Iter: 217 loss: 1.5517528e-06
Iter: 218 loss: 1.55016312e-06
Iter: 219 loss: 1.54810994e-06
Iter: 220 loss: 1.54942404e-06
Iter: 221 loss: 1.5468421e-06
Iter: 222 loss: 1.54447457e-06
Iter: 223 loss: 1.5687026e-06
Iter: 224 loss: 1.54442967e-06
Iter: 225 loss: 1.54202621e-06
Iter: 226 loss: 1.5425228e-06
Iter: 227 loss: 1.54026861e-06
Iter: 228 loss: 1.53846986e-06
Iter: 229 loss: 1.55867269e-06
Iter: 230 loss: 1.53844053e-06
Iter: 231 loss: 1.53632266e-06
Iter: 232 loss: 1.53017845e-06
Iter: 233 loss: 1.55557e-06
Iter: 234 loss: 1.52778262e-06
Iter: 235 loss: 1.52355892e-06
Iter: 236 loss: 1.52337884e-06
Iter: 237 loss: 1.51880477e-06
Iter: 238 loss: 1.52446e-06
Iter: 239 loss: 1.51649078e-06
Iter: 240 loss: 1.51219956e-06
Iter: 241 loss: 1.50814719e-06
Iter: 242 loss: 1.50724406e-06
Iter: 243 loss: 1.50283086e-06
Iter: 244 loss: 1.56821125e-06
Iter: 245 loss: 1.50283813e-06
Iter: 246 loss: 1.50020605e-06
Iter: 247 loss: 1.49838388e-06
Iter: 248 loss: 1.49742482e-06
Iter: 249 loss: 1.49654079e-06
Iter: 250 loss: 1.49563857e-06
Iter: 251 loss: 1.49389007e-06
Iter: 252 loss: 1.49457185e-06
Iter: 253 loss: 1.49266964e-06
Iter: 254 loss: 1.4907821e-06
Iter: 255 loss: 1.4876033e-06
Iter: 256 loss: 1.48758431e-06
Iter: 257 loss: 1.48502863e-06
Iter: 258 loss: 1.48497588e-06
Iter: 259 loss: 1.48225467e-06
Iter: 260 loss: 1.48172558e-06
Iter: 261 loss: 1.4798818e-06
Iter: 262 loss: 1.47712217e-06
Iter: 263 loss: 1.48726372e-06
Iter: 264 loss: 1.47648211e-06
Iter: 265 loss: 1.47257356e-06
Iter: 266 loss: 1.47558558e-06
Iter: 267 loss: 1.47018886e-06
Iter: 268 loss: 1.46671209e-06
Iter: 269 loss: 1.46844809e-06
Iter: 270 loss: 1.46437287e-06
Iter: 271 loss: 1.4620249e-06
Iter: 272 loss: 1.46194554e-06
Iter: 273 loss: 1.45971944e-06
Iter: 274 loss: 1.45711817e-06
Iter: 275 loss: 1.45678314e-06
Iter: 276 loss: 1.45355227e-06
Iter: 277 loss: 1.45459535e-06
Iter: 278 loss: 1.45131935e-06
Iter: 279 loss: 1.44826902e-06
Iter: 280 loss: 1.45615968e-06
Iter: 281 loss: 1.44725277e-06
Iter: 282 loss: 1.44442481e-06
Iter: 283 loss: 1.47885828e-06
Iter: 284 loss: 1.44433e-06
Iter: 285 loss: 1.44192381e-06
Iter: 286 loss: 1.46103901e-06
Iter: 287 loss: 1.44171622e-06
Iter: 288 loss: 1.44052126e-06
Iter: 289 loss: 1.4381751e-06
Iter: 290 loss: 1.48690219e-06
Iter: 291 loss: 1.43821967e-06
Iter: 292 loss: 1.43572674e-06
Iter: 293 loss: 1.45411536e-06
Iter: 294 loss: 1.43549494e-06
Iter: 295 loss: 1.43291754e-06
Iter: 296 loss: 1.44441901e-06
Iter: 297 loss: 1.43241778e-06
Iter: 298 loss: 1.43027592e-06
Iter: 299 loss: 1.43014529e-06
Iter: 300 loss: 1.42853628e-06
Iter: 301 loss: 1.42642443e-06
Iter: 302 loss: 1.42640988e-06
Iter: 303 loss: 1.42517274e-06
Iter: 304 loss: 1.42300621e-06
Iter: 305 loss: 1.47446383e-06
Iter: 306 loss: 1.42295698e-06
Iter: 307 loss: 1.42103238e-06
Iter: 308 loss: 1.4321713e-06
Iter: 309 loss: 1.42079296e-06
Iter: 310 loss: 1.41830174e-06
Iter: 311 loss: 1.42590306e-06
Iter: 312 loss: 1.41759074e-06
Iter: 313 loss: 1.41643454e-06
Iter: 314 loss: 1.41490227e-06
Iter: 315 loss: 1.41480814e-06
Iter: 316 loss: 1.41260352e-06
Iter: 317 loss: 1.41804799e-06
Iter: 318 loss: 1.41176633e-06
Iter: 319 loss: 1.40918587e-06
Iter: 320 loss: 1.41438977e-06
Iter: 321 loss: 1.40824966e-06
Iter: 322 loss: 1.40753616e-06
Iter: 323 loss: 1.40692919e-06
Iter: 324 loss: 1.40576799e-06
Iter: 325 loss: 1.40271482e-06
Iter: 326 loss: 1.42845488e-06
Iter: 327 loss: 1.4022612e-06
Iter: 328 loss: 1.39931717e-06
Iter: 329 loss: 1.40840643e-06
Iter: 330 loss: 1.39838971e-06
Iter: 331 loss: 1.39651252e-06
Iter: 332 loss: 1.39642452e-06
Iter: 333 loss: 1.39465487e-06
Iter: 334 loss: 1.39326471e-06
Iter: 335 loss: 1.39265535e-06
Iter: 336 loss: 1.39062718e-06
Iter: 337 loss: 1.40765496e-06
Iter: 338 loss: 1.39045062e-06
Iter: 339 loss: 1.38853261e-06
Iter: 340 loss: 1.39154929e-06
Iter: 341 loss: 1.38764563e-06
Iter: 342 loss: 1.38629605e-06
Iter: 343 loss: 1.38428e-06
Iter: 344 loss: 1.38426856e-06
Iter: 345 loss: 1.38469363e-06
Iter: 346 loss: 1.38332507e-06
Iter: 347 loss: 1.38274845e-06
Iter: 348 loss: 1.38126427e-06
Iter: 349 loss: 1.39368365e-06
Iter: 350 loss: 1.38092651e-06
Iter: 351 loss: 1.37887901e-06
Iter: 352 loss: 1.38029532e-06
Iter: 353 loss: 1.37756274e-06
Iter: 354 loss: 1.37529798e-06
Iter: 355 loss: 1.37975189e-06
Iter: 356 loss: 1.37436746e-06
Iter: 357 loss: 1.37270217e-06
Iter: 358 loss: 1.37262214e-06
Iter: 359 loss: 1.370777e-06
Iter: 360 loss: 1.36987694e-06
Iter: 361 loss: 1.36899348e-06
Iter: 362 loss: 1.36723133e-06
Iter: 363 loss: 1.3662675e-06
Iter: 364 loss: 1.36546305e-06
Iter: 365 loss: 1.3635306e-06
Iter: 366 loss: 1.36342419e-06
Iter: 367 loss: 1.36175731e-06
Iter: 368 loss: 1.36283256e-06
Iter: 369 loss: 1.36059975e-06
Iter: 370 loss: 1.35968935e-06
Iter: 371 loss: 1.36741426e-06
Iter: 372 loss: 1.3596009e-06
Iter: 373 loss: 1.35867913e-06
Iter: 374 loss: 1.35875609e-06
Iter: 375 loss: 1.35790731e-06
Iter: 376 loss: 1.35684559e-06
Iter: 377 loss: 1.35675441e-06
Iter: 378 loss: 1.35607e-06
Iter: 379 loss: 1.35456571e-06
Iter: 380 loss: 1.35795051e-06
Iter: 381 loss: 1.35401e-06
Iter: 382 loss: 1.3525937e-06
Iter: 383 loss: 1.35229425e-06
Iter: 384 loss: 1.35126902e-06
Iter: 385 loss: 1.34935465e-06
Iter: 386 loss: 1.34933907e-06
Iter: 387 loss: 1.34812751e-06
Iter: 388 loss: 1.34635877e-06
Iter: 389 loss: 1.34635991e-06
Iter: 390 loss: 1.34437289e-06
Iter: 391 loss: 1.34577067e-06
Iter: 392 loss: 1.3431428e-06
Iter: 393 loss: 1.34096945e-06
Iter: 394 loss: 1.35461755e-06
Iter: 395 loss: 1.3406717e-06
Iter: 396 loss: 1.34026754e-06
Iter: 397 loss: 1.33984042e-06
Iter: 398 loss: 1.33919184e-06
Iter: 399 loss: 1.33756919e-06
Iter: 400 loss: 1.35496032e-06
Iter: 401 loss: 1.33744823e-06
Iter: 402 loss: 1.33588082e-06
Iter: 403 loss: 1.33799347e-06
Iter: 404 loss: 1.33513481e-06
Iter: 405 loss: 1.33374044e-06
Iter: 406 loss: 1.33376886e-06
Iter: 407 loss: 1.33246215e-06
Iter: 408 loss: 1.33599542e-06
Iter: 409 loss: 1.33199387e-06
Iter: 410 loss: 1.33124092e-06
Iter: 411 loss: 1.3297813e-06
Iter: 412 loss: 1.36062658e-06
Iter: 413 loss: 1.32980574e-06
Iter: 414 loss: 1.328287e-06
Iter: 415 loss: 1.32825949e-06
Iter: 416 loss: 1.32741127e-06
Iter: 417 loss: 1.3264679e-06
Iter: 418 loss: 1.32634784e-06
Iter: 419 loss: 1.32512605e-06
Iter: 420 loss: 1.32847504e-06
Iter: 421 loss: 1.32479499e-06
Iter: 422 loss: 1.32356172e-06
Iter: 423 loss: 1.32623927e-06
Iter: 424 loss: 1.32310049e-06
Iter: 425 loss: 1.32165792e-06
Iter: 426 loss: 1.33704043e-06
Iter: 427 loss: 1.32163768e-06
Iter: 428 loss: 1.3209044e-06
Iter: 429 loss: 1.31958575e-06
Iter: 430 loss: 1.34785557e-06
Iter: 431 loss: 1.31958745e-06
Iter: 432 loss: 1.31793377e-06
Iter: 433 loss: 1.32317632e-06
Iter: 434 loss: 1.31743354e-06
Iter: 435 loss: 1.31659942e-06
Iter: 436 loss: 1.31643151e-06
Iter: 437 loss: 1.31577349e-06
Iter: 438 loss: 1.31438071e-06
Iter: 439 loss: 1.33221226e-06
Iter: 440 loss: 1.31426441e-06
Iter: 441 loss: 1.31276511e-06
Iter: 442 loss: 1.3144878e-06
Iter: 443 loss: 1.31203706e-06
Iter: 444 loss: 1.31130389e-06
Iter: 445 loss: 1.31094384e-06
Iter: 446 loss: 1.31038121e-06
Iter: 447 loss: 1.30969693e-06
Iter: 448 loss: 1.30960268e-06
Iter: 449 loss: 1.30883518e-06
Iter: 450 loss: 1.31096499e-06
Iter: 451 loss: 1.30854676e-06
Iter: 452 loss: 1.30732406e-06
Iter: 453 loss: 1.30840135e-06
Iter: 454 loss: 1.30661863e-06
Iter: 455 loss: 1.30555236e-06
Iter: 456 loss: 1.30437081e-06
Iter: 457 loss: 1.30418755e-06
Iter: 458 loss: 1.30222259e-06
Iter: 459 loss: 1.30700676e-06
Iter: 460 loss: 1.30157684e-06
Iter: 461 loss: 1.29986984e-06
Iter: 462 loss: 1.32021194e-06
Iter: 463 loss: 1.29984721e-06
Iter: 464 loss: 1.29860791e-06
Iter: 465 loss: 1.30803301e-06
Iter: 466 loss: 1.29855698e-06
Iter: 467 loss: 1.29755358e-06
Iter: 468 loss: 1.29546459e-06
Iter: 469 loss: 1.32434275e-06
Iter: 470 loss: 1.29535943e-06
Iter: 471 loss: 1.29381237e-06
Iter: 472 loss: 1.31632032e-06
Iter: 473 loss: 1.29379896e-06
Iter: 474 loss: 1.29320051e-06
Iter: 475 loss: 1.30273133e-06
Iter: 476 loss: 1.29316925e-06
Iter: 477 loss: 1.29249088e-06
Iter: 478 loss: 1.29277282e-06
Iter: 479 loss: 1.29200316e-06
Iter: 480 loss: 1.29140506e-06
Iter: 481 loss: 1.29021964e-06
Iter: 482 loss: 1.31570926e-06
Iter: 483 loss: 1.29025966e-06
Iter: 484 loss: 1.29002456e-06
Iter: 485 loss: 1.28954696e-06
Iter: 486 loss: 1.28897113e-06
Iter: 487 loss: 1.28763668e-06
Iter: 488 loss: 1.30091689e-06
Iter: 489 loss: 1.28739555e-06
Iter: 490 loss: 1.28623401e-06
Iter: 491 loss: 1.29860098e-06
Iter: 492 loss: 1.28621014e-06
Iter: 493 loss: 1.28518263e-06
Iter: 494 loss: 1.28899137e-06
Iter: 495 loss: 1.28490092e-06
Iter: 496 loss: 1.28396994e-06
Iter: 497 loss: 1.28260319e-06
Iter: 498 loss: 1.2826049e-06
Iter: 499 loss: 1.28096485e-06
Iter: 500 loss: 1.2866135e-06
Iter: 501 loss: 1.28053819e-06
Iter: 502 loss: 1.28030149e-06
Iter: 503 loss: 1.27997271e-06
Iter: 504 loss: 1.27952967e-06
Iter: 505 loss: 1.27852809e-06
Iter: 506 loss: 1.2965578e-06
Iter: 507 loss: 1.27849307e-06
Iter: 508 loss: 1.27751173e-06
Iter: 509 loss: 1.28289e-06
Iter: 510 loss: 1.27737098e-06
Iter: 511 loss: 1.27670819e-06
Iter: 512 loss: 1.28128227e-06
Iter: 513 loss: 1.27666317e-06
Iter: 514 loss: 1.27587555e-06
Iter: 515 loss: 1.27760359e-06
Iter: 516 loss: 1.27556018e-06
Iter: 517 loss: 1.27494957e-06
Iter: 518 loss: 1.2741358e-06
Iter: 519 loss: 1.27407372e-06
Iter: 520 loss: 1.27295584e-06
Iter: 521 loss: 1.27818737e-06
Iter: 522 loss: 1.27276883e-06
Iter: 523 loss: 1.27183307e-06
Iter: 524 loss: 1.28684633e-06
Iter: 525 loss: 1.27181261e-06
Iter: 526 loss: 1.27128851e-06
Iter: 527 loss: 1.27008718e-06
Iter: 528 loss: 1.28503007e-06
Iter: 529 loss: 1.27006331e-06
Iter: 530 loss: 1.26850546e-06
Iter: 531 loss: 1.27042267e-06
Iter: 532 loss: 1.26774455e-06
Iter: 533 loss: 1.26716259e-06
Iter: 534 loss: 1.26674831e-06
Iter: 535 loss: 1.26644943e-06
Iter: 536 loss: 1.26561895e-06
Iter: 537 loss: 1.27482247e-06
Iter: 538 loss: 1.26558587e-06
Iter: 539 loss: 1.26461316e-06
Iter: 540 loss: 1.26878524e-06
Iter: 541 loss: 1.26442012e-06
Iter: 542 loss: 1.26394082e-06
Iter: 543 loss: 1.26390228e-06
Iter: 544 loss: 1.2634498e-06
Iter: 545 loss: 1.26251734e-06
Iter: 546 loss: 1.27954422e-06
Iter: 547 loss: 1.26252576e-06
Iter: 548 loss: 1.26156874e-06
Iter: 549 loss: 1.26234704e-06
Iter: 550 loss: 1.26101179e-06
Iter: 551 loss: 1.26038071e-06
Iter: 552 loss: 1.26030272e-06
Iter: 553 loss: 1.2596031e-06
Iter: 554 loss: 1.26002192e-06
Iter: 555 loss: 1.25914528e-06
Iter: 556 loss: 1.25832025e-06
Iter: 557 loss: 1.25709573e-06
Iter: 558 loss: 1.25703116e-06
Iter: 559 loss: 1.2561668e-06
Iter: 560 loss: 1.25614406e-06
Iter: 561 loss: 1.2554633e-06
Iter: 562 loss: 1.26094574e-06
Iter: 563 loss: 1.25544113e-06
Iter: 564 loss: 1.25499207e-06
Iter: 565 loss: 1.25400527e-06
Iter: 566 loss: 1.26820441e-06
Iter: 567 loss: 1.25397787e-06
Iter: 568 loss: 1.25274596e-06
Iter: 569 loss: 1.25561837e-06
Iter: 570 loss: 1.2522645e-06
Iter: 571 loss: 1.25187023e-06
Iter: 572 loss: 1.25168629e-06
Iter: 573 loss: 1.25126974e-06
Iter: 574 loss: 1.25018482e-06
Iter: 575 loss: 1.25819861e-06
Iter: 576 loss: 1.24997928e-06
Iter: 577 loss: 1.24883093e-06
Iter: 578 loss: 1.25633551e-06
Iter: 579 loss: 1.24872122e-06
Iter: 580 loss: 1.24810663e-06
Iter: 581 loss: 1.25661586e-06
Iter: 582 loss: 1.24808275e-06
Iter: 583 loss: 1.24742269e-06
Iter: 584 loss: 1.24835094e-06
Iter: 585 loss: 1.24708595e-06
Iter: 586 loss: 1.24653036e-06
Iter: 587 loss: 1.24562018e-06
Iter: 588 loss: 1.2455655e-06
Iter: 589 loss: 1.24520022e-06
Iter: 590 loss: 1.2450206e-06
Iter: 591 loss: 1.24438793e-06
Iter: 592 loss: 1.24347525e-06
Iter: 593 loss: 1.24348946e-06
Iter: 594 loss: 1.24270161e-06
Iter: 595 loss: 1.24267694e-06
Iter: 596 loss: 1.24200369e-06
Iter: 597 loss: 1.24134385e-06
Iter: 598 loss: 1.24125688e-06
Iter: 599 loss: 1.24052531e-06
Iter: 600 loss: 1.24050302e-06
Iter: 601 loss: 1.23988866e-06
Iter: 602 loss: 1.23924269e-06
Iter: 603 loss: 1.23854852e-06
Iter: 604 loss: 1.2384512e-06
Iter: 605 loss: 1.2373913e-06
Iter: 606 loss: 1.24682219e-06
Iter: 607 loss: 1.23735458e-06
Iter: 608 loss: 1.23689779e-06
Iter: 609 loss: 1.24317864e-06
Iter: 610 loss: 1.23692598e-06
Iter: 611 loss: 1.23649636e-06
Iter: 612 loss: 1.23649988e-06
Iter: 613 loss: 1.23616394e-06
Iter: 614 loss: 1.23564359e-06
Iter: 615 loss: 1.23484404e-06
Iter: 616 loss: 1.23484233e-06
Iter: 617 loss: 1.23451809e-06
Iter: 618 loss: 1.23437894e-06
Iter: 619 loss: 1.23383279e-06
Iter: 620 loss: 1.2333519e-06
Iter: 621 loss: 1.23321138e-06
Iter: 622 loss: 1.23249379e-06
Iter: 623 loss: 1.23332893e-06
Iter: 624 loss: 1.23210941e-06
Iter: 625 loss: 1.23101506e-06
Iter: 626 loss: 1.24028134e-06
Iter: 627 loss: 1.23097038e-06
Iter: 628 loss: 1.2304547e-06
Iter: 629 loss: 1.22996357e-06
Iter: 630 loss: 1.22986535e-06
Iter: 631 loss: 1.22935137e-06
Iter: 632 loss: 1.23719951e-06
Iter: 633 loss: 1.22931942e-06
Iter: 634 loss: 1.22872461e-06
Iter: 635 loss: 1.22820325e-06
Iter: 636 loss: 1.22801396e-06
Iter: 637 loss: 1.22750703e-06
Iter: 638 loss: 1.22731012e-06
Iter: 639 loss: 1.22702897e-06
Iter: 640 loss: 1.22617166e-06
Iter: 641 loss: 1.2300884e-06
Iter: 642 loss: 1.22596293e-06
Iter: 643 loss: 1.22551296e-06
Iter: 644 loss: 1.22548954e-06
Iter: 645 loss: 1.22503832e-06
Iter: 646 loss: 1.22416111e-06
Iter: 647 loss: 1.23783752e-06
Iter: 648 loss: 1.22412575e-06
Iter: 649 loss: 1.2229566e-06
Iter: 650 loss: 1.22564074e-06
Iter: 651 loss: 1.22251981e-06
Iter: 652 loss: 1.22186179e-06
Iter: 653 loss: 1.22173662e-06
Iter: 654 loss: 1.22127562e-06
Iter: 655 loss: 1.22106485e-06
Iter: 656 loss: 1.22083213e-06
Iter: 657 loss: 1.22019389e-06
Iter: 658 loss: 1.2212289e-06
Iter: 659 loss: 1.21987e-06
Iter: 660 loss: 1.21899768e-06
Iter: 661 loss: 1.22586266e-06
Iter: 662 loss: 1.21894891e-06
Iter: 663 loss: 1.21853418e-06
Iter: 664 loss: 1.21770574e-06
Iter: 665 loss: 1.2303135e-06
Iter: 666 loss: 1.21766652e-06
Iter: 667 loss: 1.21739095e-06
Iter: 668 loss: 1.21723428e-06
Iter: 669 loss: 1.21679636e-06
Iter: 670 loss: 1.21661469e-06
Iter: 671 loss: 1.21638436e-06
Iter: 672 loss: 1.2158979e-06
Iter: 673 loss: 1.2156645e-06
Iter: 674 loss: 1.21540836e-06
Iter: 675 loss: 1.21464939e-06
Iter: 676 loss: 1.21440405e-06
Iter: 677 loss: 1.21393418e-06
Iter: 678 loss: 1.21279129e-06
Iter: 679 loss: 1.22123583e-06
Iter: 680 loss: 1.21272046e-06
Iter: 681 loss: 1.21199218e-06
Iter: 682 loss: 1.21417838e-06
Iter: 683 loss: 1.21175401e-06
Iter: 684 loss: 1.21107689e-06
Iter: 685 loss: 1.21816242e-06
Iter: 686 loss: 1.21107223e-06
Iter: 687 loss: 1.21038477e-06
Iter: 688 loss: 1.21120729e-06
Iter: 689 loss: 1.21005451e-06
Iter: 690 loss: 1.20950756e-06
Iter: 691 loss: 1.2095536e-06
Iter: 692 loss: 1.20918162e-06
Iter: 693 loss: 1.20868708e-06
Iter: 694 loss: 1.20864297e-06
Iter: 695 loss: 1.20819686e-06
Iter: 696 loss: 1.2086125e-06
Iter: 697 loss: 1.20796358e-06
Iter: 698 loss: 1.20749644e-06
Iter: 699 loss: 1.20681943e-06
Iter: 700 loss: 1.20682444e-06
Iter: 701 loss: 1.20669017e-06
Iter: 702 loss: 1.2064213e-06
Iter: 703 loss: 1.20611776e-06
Iter: 704 loss: 1.20538277e-06
Iter: 705 loss: 1.21672485e-06
Iter: 706 loss: 1.20539653e-06
Iter: 707 loss: 1.2045241e-06
Iter: 708 loss: 1.20477716e-06
Iter: 709 loss: 1.20390337e-06
Iter: 710 loss: 1.20343384e-06
Iter: 711 loss: 1.20336335e-06
Iter: 712 loss: 1.20290883e-06
Iter: 713 loss: 1.20305845e-06
Iter: 714 loss: 1.20249808e-06
Iter: 715 loss: 1.2020181e-06
Iter: 716 loss: 1.20143648e-06
Iter: 717 loss: 1.20141817e-06
Iter: 718 loss: 1.20074833e-06
Iter: 719 loss: 1.20759205e-06
Iter: 720 loss: 1.20075219e-06
Iter: 721 loss: 1.20027153e-06
Iter: 722 loss: 1.20481536e-06
Iter: 723 loss: 1.20023219e-06
Iter: 724 loss: 1.19980336e-06
Iter: 725 loss: 1.1992679e-06
Iter: 726 loss: 1.19922458e-06
Iter: 727 loss: 1.19841411e-06
Iter: 728 loss: 1.19936226e-06
Iter: 729 loss: 1.19801859e-06
Iter: 730 loss: 1.19762956e-06
Iter: 731 loss: 1.19755032e-06
Iter: 732 loss: 1.19710103e-06
Iter: 733 loss: 1.19682625e-06
Iter: 734 loss: 1.19669312e-06
Iter: 735 loss: 1.19600509e-06
Iter: 736 loss: 1.19564766e-06
Iter: 737 loss: 1.19539186e-06
Iter: 738 loss: 1.19525907e-06
Iter: 739 loss: 1.19496644e-06
Iter: 740 loss: 1.19473e-06
Iter: 741 loss: 1.19419951e-06
Iter: 742 loss: 1.20105619e-06
Iter: 743 loss: 1.19412846e-06
Iter: 744 loss: 1.19358697e-06
Iter: 745 loss: 1.19391871e-06
Iter: 746 loss: 1.1931993e-06
Iter: 747 loss: 1.1929061e-06
Iter: 748 loss: 1.19282186e-06
Iter: 749 loss: 1.19248352e-06
Iter: 750 loss: 1.19245124e-06
Iter: 751 loss: 1.19224e-06
Iter: 752 loss: 1.19188144e-06
Iter: 753 loss: 1.19088782e-06
Iter: 754 loss: 1.20478285e-06
Iter: 755 loss: 1.19085303e-06
Iter: 756 loss: 1.19049287e-06
Iter: 757 loss: 1.19024708e-06
Iter: 758 loss: 1.18975652e-06
Iter: 759 loss: 1.18959906e-06
Iter: 760 loss: 1.18934179e-06
Iter: 761 loss: 1.18864318e-06
Iter: 762 loss: 1.18832236e-06
Iter: 763 loss: 1.187983e-06
Iter: 764 loss: 1.18731009e-06
Iter: 765 loss: 1.1962577e-06
Iter: 766 loss: 1.18729486e-06
Iter: 767 loss: 1.18673881e-06
Iter: 768 loss: 1.19297874e-06
Iter: 769 loss: 1.18673074e-06
Iter: 770 loss: 1.18650155e-06
Iter: 771 loss: 1.18593425e-06
Iter: 772 loss: 1.19833703e-06
Iter: 773 loss: 1.18592811e-06
Iter: 774 loss: 1.1857129e-06
Iter: 775 loss: 1.18566527e-06
Iter: 776 loss: 1.18538208e-06
Iter: 777 loss: 1.18513844e-06
Iter: 778 loss: 1.18504772e-06
Iter: 779 loss: 1.18466096e-06
Iter: 780 loss: 1.18458672e-06
Iter: 781 loss: 1.18435878e-06
Iter: 782 loss: 1.18405637e-06
Iter: 783 loss: 1.18404637e-06
Iter: 784 loss: 1.1837094e-06
Iter: 785 loss: 1.18308208e-06
Iter: 786 loss: 1.1951131e-06
Iter: 787 loss: 1.18302762e-06
Iter: 788 loss: 1.18228536e-06
Iter: 789 loss: 1.18383662e-06
Iter: 790 loss: 1.18195726e-06
Iter: 791 loss: 1.18126127e-06
Iter: 792 loss: 1.18852563e-06
Iter: 793 loss: 1.1813072e-06
Iter: 794 loss: 1.18070784e-06
Iter: 795 loss: 1.18130038e-06
Iter: 796 loss: 1.18028163e-06
Iter: 797 loss: 1.17976901e-06
Iter: 798 loss: 1.17939021e-06
Iter: 799 loss: 1.17912828e-06
Iter: 800 loss: 1.17857212e-06
Iter: 801 loss: 1.17858826e-06
Iter: 802 loss: 1.17811248e-06
Iter: 803 loss: 1.18152639e-06
Iter: 804 loss: 1.17810441e-06
Iter: 805 loss: 1.17787135e-06
Iter: 806 loss: 1.17756611e-06
Iter: 807 loss: 1.17746492e-06
Iter: 808 loss: 1.1773194e-06
Iter: 809 loss: 1.17725426e-06
Iter: 810 loss: 1.17703087e-06
Iter: 811 loss: 1.1765801e-06
Iter: 812 loss: 1.17660261e-06
Iter: 813 loss: 1.17615673e-06
Iter: 814 loss: 1.17667878e-06
Iter: 815 loss: 1.17589377e-06
Iter: 816 loss: 1.17548427e-06
Iter: 817 loss: 1.17548802e-06
Iter: 818 loss: 1.17511081e-06
Iter: 819 loss: 1.17486616e-06
Iter: 820 loss: 1.17472268e-06
Iter: 821 loss: 1.17428363e-06
Iter: 822 loss: 1.17381251e-06
Iter: 823 loss: 1.1736854e-06
Iter: 824 loss: 1.17315756e-06
Iter: 825 loss: 1.17317745e-06
Iter: 826 loss: 1.17267257e-06
Iter: 827 loss: 1.17374884e-06
Iter: 828 loss: 1.17249795e-06
Iter: 829 loss: 1.1721022e-06
Iter: 830 loss: 1.1713812e-06
Iter: 831 loss: 1.17140485e-06
Iter: 832 loss: 1.1710265e-06
Iter: 833 loss: 1.17099864e-06
Iter: 834 loss: 1.17068157e-06
Iter: 835 loss: 1.17238255e-06
Iter: 836 loss: 1.17058653e-06
Iter: 837 loss: 1.17030856e-06
Iter: 838 loss: 1.16979754e-06
Iter: 839 loss: 1.1811801e-06
Iter: 840 loss: 1.16984756e-06
Iter: 841 loss: 1.16965e-06
Iter: 842 loss: 1.16958961e-06
Iter: 843 loss: 1.16935871e-06
Iter: 844 loss: 1.16934666e-06
Iter: 845 loss: 1.16915851e-06
Iter: 846 loss: 1.16887452e-06
Iter: 847 loss: 1.16837873e-06
Iter: 848 loss: 1.17817808e-06
Iter: 849 loss: 1.16837202e-06
Iter: 850 loss: 1.16799072e-06
Iter: 851 loss: 1.16795786e-06
Iter: 852 loss: 1.16756371e-06
Iter: 853 loss: 1.16792887e-06
Iter: 854 loss: 1.1673427e-06
Iter: 855 loss: 1.16701017e-06
Iter: 856 loss: 1.16660851e-06
Iter: 857 loss: 1.16656361e-06
Iter: 858 loss: 1.16586398e-06
Iter: 859 loss: 1.16722572e-06
Iter: 860 loss: 1.16554907e-06
Iter: 861 loss: 1.16518618e-06
Iter: 862 loss: 1.16513763e-06
Iter: 863 loss: 1.16480601e-06
Iter: 864 loss: 1.16430465e-06
Iter: 865 loss: 1.16433876e-06
Iter: 866 loss: 1.16371052e-06
Iter: 867 loss: 1.16449496e-06
Iter: 868 loss: 1.16339697e-06
Iter: 869 loss: 1.16311105e-06
Iter: 870 loss: 1.16294962e-06
Iter: 871 loss: 1.1627867e-06
Iter: 872 loss: 1.16240221e-06
Iter: 873 loss: 1.16831382e-06
Iter: 874 loss: 1.162329e-06
Iter: 875 loss: 1.16193678e-06
Iter: 876 loss: 1.16605815e-06
Iter: 877 loss: 1.16193928e-06
Iter: 878 loss: 1.16139086e-06
Iter: 879 loss: 1.16208776e-06
Iter: 880 loss: 1.16121464e-06
Iter: 881 loss: 1.16085789e-06
Iter: 882 loss: 1.16101683e-06
Iter: 883 loss: 1.16069589e-06
Iter: 884 loss: 1.16028446e-06
Iter: 885 loss: 1.16059766e-06
Iter: 886 loss: 1.16004821e-06
Iter: 887 loss: 1.15943271e-06
Iter: 888 loss: 1.16363333e-06
Iter: 889 loss: 1.15939906e-06
Iter: 890 loss: 1.15899024e-06
Iter: 891 loss: 1.15868988e-06
Iter: 892 loss: 1.15853345e-06
Iter: 893 loss: 1.15789908e-06
Iter: 894 loss: 1.15765965e-06
Iter: 895 loss: 1.15729699e-06
Iter: 896 loss: 1.15673151e-06
Iter: 897 loss: 1.15666171e-06
Iter: 898 loss: 1.15614534e-06
Iter: 899 loss: 1.15726948e-06
Iter: 900 loss: 1.15598448e-06
Iter: 901 loss: 1.15566831e-06
Iter: 902 loss: 1.15528542e-06
Iter: 903 loss: 1.15521391e-06
Iter: 904 loss: 1.1548633e-06
Iter: 905 loss: 1.1548675e-06
Iter: 906 loss: 1.15451701e-06
Iter: 907 loss: 1.15511693e-06
Iter: 908 loss: 1.15436421e-06
Iter: 909 loss: 1.15409807e-06
Iter: 910 loss: 1.15389332e-06
Iter: 911 loss: 1.15386774e-06
Iter: 912 loss: 1.15362991e-06
Iter: 913 loss: 1.15358e-06
Iter: 914 loss: 1.15334421e-06
Iter: 915 loss: 1.15279136e-06
Iter: 916 loss: 1.15874855e-06
Iter: 917 loss: 1.15274429e-06
Iter: 918 loss: 1.15209446e-06
Iter: 919 loss: 1.15428509e-06
Iter: 920 loss: 1.15197417e-06
Iter: 921 loss: 1.15122e-06
Iter: 922 loss: 1.15212799e-06
Iter: 923 loss: 1.15096691e-06
Iter: 924 loss: 1.15056991e-06
Iter: 925 loss: 1.15049136e-06
Iter: 926 loss: 1.15021442e-06
Iter: 927 loss: 1.14986892e-06
Iter: 928 loss: 1.14983538e-06
Iter: 929 loss: 1.14933891e-06
Iter: 930 loss: 1.14902218e-06
Iter: 931 loss: 1.14882801e-06
Iter: 932 loss: 1.1483861e-06
Iter: 933 loss: 1.14839327e-06
Iter: 934 loss: 1.14796126e-06
Iter: 935 loss: 1.14979809e-06
Iter: 936 loss: 1.14795523e-06
Iter: 937 loss: 1.1476817e-06
Iter: 938 loss: 1.14726572e-06
Iter: 939 loss: 1.15787248e-06
Iter: 940 loss: 1.14728709e-06
Iter: 941 loss: 1.14676709e-06
Iter: 942 loss: 1.14783961e-06
Iter: 943 loss: 1.14650129e-06
Iter: 944 loss: 1.14600903e-06
Iter: 945 loss: 1.15105649e-06
Iter: 946 loss: 1.14604825e-06
Iter: 947 loss: 1.14557133e-06
Iter: 948 loss: 1.14981037e-06
Iter: 949 loss: 1.1455536e-06
Iter: 950 loss: 1.14532577e-06
Iter: 951 loss: 1.14485806e-06
Iter: 952 loss: 1.15014939e-06
Iter: 953 loss: 1.14477712e-06
Iter: 954 loss: 1.14419208e-06
Iter: 955 loss: 1.14675481e-06
Iter: 956 loss: 1.14407715e-06
Iter: 957 loss: 1.14371176e-06
Iter: 958 loss: 1.14371539e-06
Iter: 959 loss: 1.14341572e-06
Iter: 960 loss: 1.14296722e-06
Iter: 961 loss: 1.14296051e-06
Iter: 962 loss: 1.14252111e-06
Iter: 963 loss: 1.1448567e-06
Iter: 964 loss: 1.14247769e-06
Iter: 965 loss: 1.14211991e-06
Iter: 966 loss: 1.14552779e-06
Iter: 967 loss: 1.14212276e-06
Iter: 968 loss: 1.14188e-06
Iter: 969 loss: 1.14154432e-06
Iter: 970 loss: 1.14155534e-06
Iter: 971 loss: 1.14107684e-06
Iter: 972 loss: 1.14148611e-06
Iter: 973 loss: 1.14084241e-06
Iter: 974 loss: 1.14034242e-06
Iter: 975 loss: 1.14302384e-06
Iter: 976 loss: 1.14029331e-06
Iter: 977 loss: 1.13982128e-06
Iter: 978 loss: 1.14380941e-06
Iter: 979 loss: 1.13978672e-06
Iter: 980 loss: 1.13950671e-06
Iter: 981 loss: 1.13901024e-06
Iter: 982 loss: 1.13900137e-06
Iter: 983 loss: 1.13847955e-06
Iter: 984 loss: 1.14038392e-06
Iter: 985 loss: 1.13828401e-06
Iter: 986 loss: 1.13811802e-06
Iter: 987 loss: 1.13803605e-06
Iter: 988 loss: 1.13778083e-06
Iter: 989 loss: 1.13726128e-06
Iter: 990 loss: 1.14022941e-06
Iter: 991 loss: 1.13704039e-06
Iter: 992 loss: 1.13653664e-06
Iter: 993 loss: 1.14192551e-06
Iter: 994 loss: 1.13647354e-06
Iter: 995 loss: 1.13617841e-06
Iter: 996 loss: 1.14045611e-06
Iter: 997 loss: 1.13620422e-06
Iter: 998 loss: 1.13591113e-06
Iter: 999 loss: 1.13569286e-06
Iter: 1000 loss: 1.13555382e-06
Iter: 1001 loss: 1.13524607e-06
Iter: 1002 loss: 1.13600481e-06
Iter: 1003 loss: 1.13507804e-06
Iter: 1004 loss: 1.13481792e-06
Iter: 1005 loss: 1.13917736e-06
Iter: 1006 loss: 1.13478609e-06
Iter: 1007 loss: 1.13453621e-06
Iter: 1008 loss: 1.13405e-06
Iter: 1009 loss: 1.14060867e-06
Iter: 1010 loss: 1.13399483e-06
Iter: 1011 loss: 1.13354486e-06
Iter: 1012 loss: 1.13759381e-06
Iter: 1013 loss: 1.13355952e-06
Iter: 1014 loss: 1.13319857e-06
Iter: 1015 loss: 1.13412318e-06
Iter: 1016 loss: 1.1331216e-06
Iter: 1017 loss: 1.13266356e-06
Iter: 1018 loss: 1.13405463e-06
Iter: 1019 loss: 1.13252622e-06
Iter: 1020 loss: 1.13214151e-06
Iter: 1021 loss: 1.1323234e-06
Iter: 1022 loss: 1.13192846e-06
Iter: 1023 loss: 1.13177725e-06
Iter: 1024 loss: 1.13177236e-06
Iter: 1025 loss: 1.13156023e-06
Iter: 1026 loss: 1.13107501e-06
Iter: 1027 loss: 1.13618967e-06
Iter: 1028 loss: 1.13103056e-06
Iter: 1029 loss: 1.1305815e-06
Iter: 1030 loss: 1.13331873e-06
Iter: 1031 loss: 1.13053454e-06
Iter: 1032 loss: 1.13021872e-06
Iter: 1033 loss: 1.13520741e-06
Iter: 1034 loss: 1.13020224e-06
Iter: 1035 loss: 1.13005729e-06
Iter: 1036 loss: 1.1296388e-06
Iter: 1037 loss: 1.13454882e-06
Iter: 1038 loss: 1.12960083e-06
Iter: 1039 loss: 1.12926409e-06
Iter: 1040 loss: 1.12927648e-06
Iter: 1041 loss: 1.12896123e-06
Iter: 1042 loss: 1.12871658e-06
Iter: 1043 loss: 1.12860266e-06
Iter: 1044 loss: 1.12827661e-06
Iter: 1045 loss: 1.12918156e-06
Iter: 1046 loss: 1.12817315e-06
Iter: 1047 loss: 1.12780117e-06
Iter: 1048 loss: 1.12763053e-06
Iter: 1049 loss: 1.1275016e-06
Iter: 1050 loss: 1.1271826e-06
Iter: 1051 loss: 1.12711314e-06
Iter: 1052 loss: 1.12688053e-06
Iter: 1053 loss: 1.12655039e-06
Iter: 1054 loss: 1.12654948e-06
Iter: 1055 loss: 1.12622365e-06
Iter: 1056 loss: 1.12775194e-06
Iter: 1057 loss: 1.12611133e-06
Iter: 1058 loss: 1.12572434e-06
Iter: 1059 loss: 1.12888642e-06
Iter: 1060 loss: 1.12572548e-06
Iter: 1061 loss: 1.12552607e-06
Iter: 1062 loss: 1.12518853e-06
Iter: 1063 loss: 1.12517318e-06
Iter: 1064 loss: 1.12496332e-06
Iter: 1065 loss: 1.12494286e-06
Iter: 1066 loss: 1.12473549e-06
Iter: 1067 loss: 1.1243618e-06
Iter: 1068 loss: 1.12432338e-06
Iter: 1069 loss: 1.12395742e-06
Iter: 1070 loss: 1.12479597e-06
Iter: 1071 loss: 1.12378348e-06
Iter: 1072 loss: 1.12335169e-06
Iter: 1073 loss: 1.12959879e-06
Iter: 1074 loss: 1.12335988e-06
Iter: 1075 loss: 1.12314569e-06
Iter: 1076 loss: 1.12286784e-06
Iter: 1077 loss: 1.12284738e-06
Iter: 1078 loss: 1.12248404e-06
Iter: 1079 loss: 1.12273324e-06
Iter: 1080 loss: 1.12225712e-06
Iter: 1081 loss: 1.12176212e-06
Iter: 1082 loss: 1.12427745e-06
Iter: 1083 loss: 1.12174666e-06
Iter: 1084 loss: 1.12152884e-06
Iter: 1085 loss: 1.12152e-06
Iter: 1086 loss: 1.12134285e-06
Iter: 1087 loss: 1.12101657e-06
Iter: 1088 loss: 1.12099576e-06
Iter: 1089 loss: 1.12063e-06
Iter: 1090 loss: 1.12137491e-06
Iter: 1091 loss: 1.12050657e-06
Iter: 1092 loss: 1.12041903e-06
Iter: 1093 loss: 1.12031262e-06
Iter: 1094 loss: 1.12018483e-06
Iter: 1095 loss: 1.11973634e-06
Iter: 1096 loss: 1.12135558e-06
Iter: 1097 loss: 1.11951385e-06
Iter: 1098 loss: 1.11901431e-06
Iter: 1099 loss: 1.12448458e-06
Iter: 1100 loss: 1.11901477e-06
Iter: 1101 loss: 1.11871839e-06
Iter: 1102 loss: 1.11870645e-06
Iter: 1103 loss: 1.11848783e-06
Iter: 1104 loss: 1.11808481e-06
Iter: 1105 loss: 1.12402483e-06
Iter: 1106 loss: 1.11801467e-06
Iter: 1107 loss: 1.11767406e-06
Iter: 1108 loss: 1.12121688e-06
Iter: 1109 loss: 1.11764075e-06
Iter: 1110 loss: 1.11734232e-06
Iter: 1111 loss: 1.11894985e-06
Iter: 1112 loss: 1.11727718e-06
Iter: 1113 loss: 1.11711415e-06
Iter: 1114 loss: 1.11671238e-06
Iter: 1115 loss: 1.12499436e-06
Iter: 1116 loss: 1.11674763e-06
Iter: 1117 loss: 1.11641793e-06
Iter: 1118 loss: 1.11907377e-06
Iter: 1119 loss: 1.11639338e-06
Iter: 1120 loss: 1.11622933e-06
Iter: 1121 loss: 1.11811562e-06
Iter: 1122 loss: 1.1162108e-06
Iter: 1123 loss: 1.11602208e-06
Iter: 1124 loss: 1.11597512e-06
Iter: 1125 loss: 1.11580869e-06
Iter: 1126 loss: 1.11553049e-06
Iter: 1127 loss: 1.11540635e-06
Iter: 1128 loss: 1.11527334e-06
Iter: 1129 loss: 1.1150205e-06
Iter: 1130 loss: 1.11500799e-06
Iter: 1131 loss: 1.11477334e-06
Iter: 1132 loss: 1.11533336e-06
Iter: 1133 loss: 1.11466579e-06
Iter: 1134 loss: 1.11440113e-06
Iter: 1135 loss: 1.11396719e-06
Iter: 1136 loss: 1.11396776e-06
Iter: 1137 loss: 1.11365455e-06
Iter: 1138 loss: 1.11636359e-06
Iter: 1139 loss: 1.11366126e-06
Iter: 1140 loss: 1.1133061e-06
Iter: 1141 loss: 1.11435588e-06
Iter: 1142 loss: 1.11325585e-06
Iter: 1143 loss: 1.11306031e-06
Iter: 1144 loss: 1.11298664e-06
Iter: 1145 loss: 1.11287477e-06
Iter: 1146 loss: 1.11275483e-06
Iter: 1147 loss: 1.11273732e-06
Iter: 1148 loss: 1.11260147e-06
Iter: 1149 loss: 1.11241195e-06
Iter: 1150 loss: 1.11236454e-06
Iter: 1151 loss: 1.11209363e-06
Iter: 1152 loss: 1.11201302e-06
Iter: 1153 loss: 1.11185477e-06
Iter: 1154 loss: 1.11150075e-06
Iter: 1155 loss: 1.11258282e-06
Iter: 1156 loss: 1.11134625e-06
Iter: 1157 loss: 1.11100621e-06
Iter: 1158 loss: 1.11685063e-06
Iter: 1159 loss: 1.11099507e-06
Iter: 1160 loss: 1.11078657e-06
Iter: 1161 loss: 1.11020563e-06
Iter: 1162 loss: 1.11987856e-06
Iter: 1163 loss: 1.11017027e-06
Iter: 1164 loss: 1.10975429e-06
Iter: 1165 loss: 1.11591623e-06
Iter: 1166 loss: 1.10976544e-06
Iter: 1167 loss: 1.10952544e-06
Iter: 1168 loss: 1.10955341e-06
Iter: 1169 loss: 1.10943472e-06
Iter: 1170 loss: 1.10914152e-06
Iter: 1171 loss: 1.11198847e-06
Iter: 1172 loss: 1.10910787e-06
Iter: 1173 loss: 1.10882138e-06
Iter: 1174 loss: 1.11001771e-06
Iter: 1175 loss: 1.10875953e-06
Iter: 1176 loss: 1.10863039e-06
Iter: 1177 loss: 1.10855353e-06
Iter: 1178 loss: 1.10843723e-06
Iter: 1179 loss: 1.10828682e-06
Iter: 1180 loss: 1.10826659e-06
Iter: 1181 loss: 1.10809788e-06
Iter: 1182 loss: 1.10790279e-06
Iter: 1183 loss: 1.10783253e-06
Iter: 1184 loss: 1.10766257e-06
Iter: 1185 loss: 1.1076695e-06
Iter: 1186 loss: 1.10752728e-06
Iter: 1187 loss: 1.10724977e-06
Iter: 1188 loss: 1.10728183e-06
Iter: 1189 loss: 1.10693509e-06
Iter: 1190 loss: 1.10687927e-06
Iter: 1191 loss: 1.10668248e-06
Iter: 1192 loss: 1.10637643e-06
Iter: 1193 loss: 1.10638484e-06
Iter: 1194 loss: 1.10611268e-06
Iter: 1195 loss: 1.10604196e-06
Iter: 1196 loss: 1.10590054e-06
Iter: 1197 loss: 1.10564179e-06
Iter: 1198 loss: 1.10602173e-06
Iter: 1199 loss: 1.10556221e-06
Iter: 1200 loss: 1.10513372e-06
Iter: 1201 loss: 1.1069078e-06
Iter: 1202 loss: 1.1050272e-06
Iter: 1203 loss: 1.1048628e-06
Iter: 1204 loss: 1.10480619e-06
Iter: 1205 loss: 1.10468068e-06
Iter: 1206 loss: 1.10437111e-06
Iter: 1207 loss: 1.10466374e-06
Iter: 1208 loss: 1.10422934e-06
Iter: 1209 loss: 1.10408268e-06
Iter: 1210 loss: 1.10404858e-06
Iter: 1211 loss: 1.10389476e-06
Iter: 1212 loss: 1.10374231e-06
Iter: 1213 loss: 1.1037215e-06
Iter: 1214 loss: 1.10352414e-06
Iter: 1215 loss: 1.10369228e-06
Iter: 1216 loss: 1.10346434e-06
Iter: 1217 loss: 1.10316842e-06
Iter: 1218 loss: 1.10499604e-06
Iter: 1219 loss: 1.10316489e-06
Iter: 1220 loss: 1.102939e-06
Iter: 1221 loss: 1.10283622e-06
Iter: 1222 loss: 1.10274232e-06
Iter: 1223 loss: 1.10247026e-06
Iter: 1224 loss: 1.10236317e-06
Iter: 1225 loss: 1.10224119e-06
Iter: 1226 loss: 1.1021009e-06
Iter: 1227 loss: 1.10202836e-06
Iter: 1228 loss: 1.10184362e-06
Iter: 1229 loss: 1.10157953e-06
Iter: 1230 loss: 1.10158635e-06
Iter: 1231 loss: 1.10129804e-06
Iter: 1232 loss: 1.10274812e-06
Iter: 1233 loss: 1.10123779e-06
Iter: 1234 loss: 1.1009754e-06
Iter: 1235 loss: 1.10385372e-06
Iter: 1236 loss: 1.10097858e-06
Iter: 1237 loss: 1.10087524e-06
Iter: 1238 loss: 1.10055078e-06
Iter: 1239 loss: 1.10458836e-06
Iter: 1240 loss: 1.10049268e-06
Iter: 1241 loss: 1.10014969e-06
Iter: 1242 loss: 1.10083101e-06
Iter: 1243 loss: 1.10002406e-06
Iter: 1244 loss: 1.10000678e-06
Iter: 1245 loss: 1.09985808e-06
Iter: 1246 loss: 1.09976418e-06
Iter: 1247 loss: 1.0994853e-06
Iter: 1248 loss: 1.10455039e-06
Iter: 1249 loss: 1.09947337e-06
Iter: 1250 loss: 1.09927043e-06
Iter: 1251 loss: 1.09969596e-06
Iter: 1252 loss: 1.09919665e-06
Iter: 1253 loss: 1.09893188e-06
Iter: 1254 loss: 1.10131123e-06
Iter: 1255 loss: 1.09890891e-06
Iter: 1256 loss: 1.09871223e-06
Iter: 1257 loss: 1.09861321e-06
Iter: 1258 loss: 1.09847167e-06
Iter: 1259 loss: 1.09820235e-06
Iter: 1260 loss: 1.09794314e-06
Iter: 1261 loss: 1.09786436e-06
Iter: 1262 loss: 1.09745315e-06
Iter: 1263 loss: 1.10168946e-06
Iter: 1264 loss: 1.09741e-06
Iter: 1265 loss: 1.09713e-06
Iter: 1266 loss: 1.09983193e-06
Iter: 1267 loss: 1.09709504e-06
Iter: 1268 loss: 1.0968663e-06
Iter: 1269 loss: 1.09666644e-06
Iter: 1270 loss: 1.09662096e-06
Iter: 1271 loss: 1.09658333e-06
Iter: 1272 loss: 1.09648431e-06
Iter: 1273 loss: 1.0963588e-06
Iter: 1274 loss: 1.09609391e-06
Iter: 1275 loss: 1.09976361e-06
Iter: 1276 loss: 1.09604593e-06
Iter: 1277 loss: 1.09577127e-06
Iter: 1278 loss: 1.0965141e-06
Iter: 1279 loss: 1.09565303e-06
Iter: 1280 loss: 1.09542793e-06
Iter: 1281 loss: 1.09919733e-06
Iter: 1282 loss: 1.09538234e-06
Iter: 1283 loss: 1.09527741e-06
Iter: 1284 loss: 1.09501798e-06
Iter: 1285 loss: 1.10000803e-06
Iter: 1286 loss: 1.09501229e-06
Iter: 1287 loss: 1.09467692e-06
Iter: 1288 loss: 1.09524797e-06
Iter: 1289 loss: 1.09458529e-06
Iter: 1290 loss: 1.09438497e-06
Iter: 1291 loss: 1.09434666e-06
Iter: 1292 loss: 1.09419898e-06
Iter: 1293 loss: 1.09385292e-06
Iter: 1294 loss: 1.10021028e-06
Iter: 1295 loss: 1.09389907e-06
Iter: 1296 loss: 1.09352823e-06
Iter: 1297 loss: 1.09460689e-06
Iter: 1298 loss: 1.0934092e-06
Iter: 1299 loss: 1.09313476e-06
Iter: 1300 loss: 1.09519669e-06
Iter: 1301 loss: 1.09308701e-06
Iter: 1302 loss: 1.09288374e-06
Iter: 1303 loss: 1.09479856e-06
Iter: 1304 loss: 1.09286771e-06
Iter: 1305 loss: 1.09269899e-06
Iter: 1306 loss: 1.092449e-06
Iter: 1307 loss: 1.09245286e-06
Iter: 1308 loss: 1.09226266e-06
Iter: 1309 loss: 1.09228108e-06
Iter: 1310 loss: 1.09207349e-06
Iter: 1311 loss: 1.09192501e-06
Iter: 1312 loss: 1.0918917e-06
Iter: 1313 loss: 1.09174152e-06
Iter: 1314 loss: 1.09319967e-06
Iter: 1315 loss: 1.09175232e-06
Iter: 1316 loss: 1.09149812e-06
Iter: 1317 loss: 1.09146913e-06
Iter: 1318 loss: 1.09130269e-06
Iter: 1319 loss: 1.09103394e-06
Iter: 1320 loss: 1.09084783e-06
Iter: 1321 loss: 1.09074256e-06
Iter: 1322 loss: 1.09042082e-06
Iter: 1323 loss: 1.09137727e-06
Iter: 1324 loss: 1.09029622e-06
Iter: 1325 loss: 1.08999006e-06
Iter: 1326 loss: 1.09483335e-06
Iter: 1327 loss: 1.08995528e-06
Iter: 1328 loss: 1.08967549e-06
Iter: 1329 loss: 1.08981988e-06
Iter: 1330 loss: 1.08951122e-06
Iter: 1331 loss: 1.08930021e-06
Iter: 1332 loss: 1.08899474e-06
Iter: 1333 loss: 1.08893e-06
Iter: 1334 loss: 1.08852794e-06
Iter: 1335 loss: 1.09139091e-06
Iter: 1336 loss: 1.08847382e-06
Iter: 1337 loss: 1.08822951e-06
Iter: 1338 loss: 1.08822258e-06
Iter: 1339 loss: 1.08803465e-06
Iter: 1340 loss: 1.08809024e-06
Iter: 1341 loss: 1.08796962e-06
Iter: 1342 loss: 1.08776612e-06
Iter: 1343 loss: 1.08864151e-06
Iter: 1344 loss: 1.08771906e-06
Iter: 1345 loss: 1.08755887e-06
Iter: 1346 loss: 1.08791846e-06
Iter: 1347 loss: 1.08748429e-06
Iter: 1348 loss: 1.08729773e-06
Iter: 1349 loss: 1.08704103e-06
Iter: 1350 loss: 1.08704035e-06
Iter: 1351 loss: 1.08675e-06
Iter: 1352 loss: 1.08900031e-06
Iter: 1353 loss: 1.08676352e-06
Iter: 1354 loss: 1.08656468e-06
Iter: 1355 loss: 1.0887652e-06
Iter: 1356 loss: 1.08657832e-06
Iter: 1357 loss: 1.08645781e-06
Iter: 1358 loss: 1.08613074e-06
Iter: 1359 loss: 1.08965151e-06
Iter: 1360 loss: 1.08615086e-06
Iter: 1361 loss: 1.0857691e-06
Iter: 1362 loss: 1.08589052e-06
Iter: 1363 loss: 1.0855174e-06
Iter: 1364 loss: 1.08536778e-06
Iter: 1365 loss: 1.08519271e-06
Iter: 1366 loss: 1.08507174e-06
Iter: 1367 loss: 1.08473864e-06
Iter: 1368 loss: 1.09115922e-06
Iter: 1369 loss: 1.08473591e-06
Iter: 1370 loss: 1.084475e-06
Iter: 1371 loss: 1.08503696e-06
Iter: 1372 loss: 1.08430913e-06
Iter: 1373 loss: 1.08394727e-06
Iter: 1374 loss: 1.0859078e-06
Iter: 1375 loss: 1.08385052e-06
Iter: 1376 loss: 1.08361826e-06
Iter: 1377 loss: 1.08751874e-06
Iter: 1378 loss: 1.08360609e-06
Iter: 1379 loss: 1.08344216e-06
Iter: 1380 loss: 1.08317886e-06
Iter: 1381 loss: 1.08314669e-06
Iter: 1382 loss: 1.08304744e-06
Iter: 1383 loss: 1.08303277e-06
Iter: 1384 loss: 1.08290351e-06
Iter: 1385 loss: 1.08266408e-06
Iter: 1386 loss: 1.08265726e-06
Iter: 1387 loss: 1.08239931e-06
Iter: 1388 loss: 1.08422739e-06
Iter: 1389 loss: 1.08236554e-06
Iter: 1390 loss: 1.08221684e-06
Iter: 1391 loss: 1.08277504e-06
Iter: 1392 loss: 1.08220661e-06
Iter: 1393 loss: 1.08204915e-06
Iter: 1394 loss: 1.08177528e-06
Iter: 1395 loss: 1.08478264e-06
Iter: 1396 loss: 1.0817389e-06
Iter: 1397 loss: 1.08139989e-06
Iter: 1398 loss: 1.08407312e-06
Iter: 1399 loss: 1.08141626e-06
Iter: 1400 loss: 1.08123186e-06
Iter: 1401 loss: 1.08124073e-06
Iter: 1402 loss: 1.08108543e-06
Iter: 1403 loss: 1.08087511e-06
Iter: 1404 loss: 1.08630775e-06
Iter: 1405 loss: 1.08083009e-06
Iter: 1406 loss: 1.08045811e-06
Iter: 1407 loss: 1.08085476e-06
Iter: 1408 loss: 1.08029917e-06
Iter: 1409 loss: 1.07992844e-06
Iter: 1410 loss: 1.08304471e-06
Iter: 1411 loss: 1.07989194e-06
Iter: 1412 loss: 1.07962842e-06
Iter: 1413 loss: 1.08132645e-06
Iter: 1414 loss: 1.07950723e-06
Iter: 1415 loss: 1.07940832e-06
Iter: 1416 loss: 1.07942469e-06
Iter: 1417 loss: 1.07924097e-06
Iter: 1418 loss: 1.07907169e-06
Iter: 1419 loss: 1.07906635e-06
Iter: 1420 loss: 1.07892913e-06
Iter: 1421 loss: 1.07868016e-06
Iter: 1422 loss: 1.08209656e-06
Iter: 1423 loss: 1.07864628e-06
Iter: 1424 loss: 1.07854703e-06
Iter: 1425 loss: 1.07851633e-06
Iter: 1426 loss: 1.07839708e-06
Iter: 1427 loss: 1.07844198e-06
Iter: 1428 loss: 1.07828805e-06
Iter: 1429 loss: 1.07816288e-06
Iter: 1430 loss: 1.07809137e-06
Iter: 1431 loss: 1.07808387e-06
Iter: 1432 loss: 1.07785286e-06
Iter: 1433 loss: 1.07809319e-06
Iter: 1434 loss: 1.07777237e-06
Iter: 1435 loss: 1.07749031e-06
Iter: 1436 loss: 1.07876281e-06
Iter: 1437 loss: 1.0774487e-06
Iter: 1438 loss: 1.0772809e-06
Iter: 1439 loss: 1.07721758e-06
Iter: 1440 loss: 1.07708308e-06
Iter: 1441 loss: 1.07685878e-06
Iter: 1442 loss: 1.07660242e-06
Iter: 1443 loss: 1.07652772e-06
Iter: 1444 loss: 1.07624646e-06
Iter: 1445 loss: 1.07624044e-06
Iter: 1446 loss: 1.07601193e-06
Iter: 1447 loss: 1.07645792e-06
Iter: 1448 loss: 1.07590438e-06
Iter: 1449 loss: 1.07573305e-06
Iter: 1450 loss: 1.07575261e-06
Iter: 1451 loss: 1.07557651e-06
Iter: 1452 loss: 1.07526375e-06
Iter: 1453 loss: 1.0780102e-06
Iter: 1454 loss: 1.0752542e-06
Iter: 1455 loss: 1.07511619e-06
Iter: 1456 loss: 1.07491735e-06
Iter: 1457 loss: 1.07490359e-06
Iter: 1458 loss: 1.07480309e-06
Iter: 1459 loss: 1.07479309e-06
Iter: 1460 loss: 1.07461324e-06
Iter: 1461 loss: 1.07443816e-06
Iter: 1462 loss: 1.07442941e-06
Iter: 1463 loss: 1.07416406e-06
Iter: 1464 loss: 1.07455639e-06
Iter: 1465 loss: 1.07402138e-06
Iter: 1466 loss: 1.07374058e-06
Iter: 1467 loss: 1.07756637e-06
Iter: 1468 loss: 1.07374012e-06
Iter: 1469 loss: 1.07359301e-06
Iter: 1470 loss: 1.07343863e-06
Iter: 1471 loss: 1.07340213e-06
Iter: 1472 loss: 1.07301901e-06
Iter: 1473 loss: 1.07296842e-06
Iter: 1474 loss: 1.0727814e-06
Iter: 1475 loss: 1.07233745e-06
Iter: 1476 loss: 1.07501637e-06
Iter: 1477 loss: 1.07234052e-06
Iter: 1478 loss: 1.07206483e-06
Iter: 1479 loss: 1.07501126e-06
Iter: 1480 loss: 1.07201799e-06
Iter: 1481 loss: 1.07179244e-06
Iter: 1482 loss: 1.07189965e-06
Iter: 1483 loss: 1.07167477e-06
Iter: 1484 loss: 1.07151413e-06
Iter: 1485 loss: 1.07290714e-06
Iter: 1486 loss: 1.07149094e-06
Iter: 1487 loss: 1.07129415e-06
Iter: 1488 loss: 1.07166284e-06
Iter: 1489 loss: 1.07122537e-06
Iter: 1490 loss: 1.07109872e-06
Iter: 1491 loss: 1.07085839e-06
Iter: 1492 loss: 1.0708859e-06
Iter: 1493 loss: 1.07068229e-06
Iter: 1494 loss: 1.07069707e-06
Iter: 1495 loss: 1.07055212e-06
Iter: 1496 loss: 1.07034111e-06
Iter: 1497 loss: 1.07037067e-06
Iter: 1498 loss: 1.07010919e-06
Iter: 1499 loss: 1.07037636e-06
Iter: 1500 loss: 1.07001233e-06
Iter: 1501 loss: 1.06970333e-06
Iter: 1502 loss: 1.07360677e-06
Iter: 1503 loss: 1.06971402e-06
Iter: 1504 loss: 1.0695187e-06
Iter: 1505 loss: 1.06915104e-06
Iter: 1506 loss: 1.07688697e-06
Iter: 1507 loss: 1.06913581e-06
Iter: 1508 loss: 1.06873892e-06
Iter: 1509 loss: 1.07103756e-06
Iter: 1510 loss: 1.06866469e-06
Iter: 1511 loss: 1.06839923e-06
Iter: 1512 loss: 1.06843117e-06
Iter: 1513 loss: 1.06817708e-06
Iter: 1514 loss: 1.0677611e-06
Iter: 1515 loss: 1.06898028e-06
Iter: 1516 loss: 1.06762923e-06
Iter: 1517 loss: 1.06748155e-06
Iter: 1518 loss: 1.06745529e-06
Iter: 1519 loss: 1.06724792e-06
Iter: 1520 loss: 1.06710229e-06
Iter: 1521 loss: 1.06705852e-06
Iter: 1522 loss: 1.0668125e-06
Iter: 1523 loss: 1.06902382e-06
Iter: 1524 loss: 1.06681853e-06
Iter: 1525 loss: 1.06657603e-06
Iter: 1526 loss: 1.06637515e-06
Iter: 1527 loss: 1.06636287e-06
Iter: 1528 loss: 1.06611628e-06
Iter: 1529 loss: 1.0667934e-06
Iter: 1530 loss: 1.06609991e-06
Iter: 1531 loss: 1.06588584e-06
Iter: 1532 loss: 1.06870357e-06
Iter: 1533 loss: 1.06587709e-06
Iter: 1534 loss: 1.06574362e-06
Iter: 1535 loss: 1.06540983e-06
Iter: 1536 loss: 1.06858067e-06
Iter: 1537 loss: 1.06536072e-06
Iter: 1538 loss: 1.06495793e-06
Iter: 1539 loss: 1.06654534e-06
Iter: 1540 loss: 1.0648821e-06
Iter: 1541 loss: 1.06459402e-06
Iter: 1542 loss: 1.06467655e-06
Iter: 1543 loss: 1.064343e-06
Iter: 1544 loss: 1.06402808e-06
Iter: 1545 loss: 1.06402183e-06
Iter: 1546 loss: 1.06377479e-06
Iter: 1547 loss: 1.06425114e-06
Iter: 1548 loss: 1.06360426e-06
Iter: 1549 loss: 1.06342873e-06
Iter: 1550 loss: 1.06321897e-06
Iter: 1551 loss: 1.06323512e-06
Iter: 1552 loss: 1.06296625e-06
Iter: 1553 loss: 1.06296659e-06
Iter: 1554 loss: 1.0627424e-06
Iter: 1555 loss: 1.06298194e-06
Iter: 1556 loss: 1.06262655e-06
Iter: 1557 loss: 1.06243533e-06
Iter: 1558 loss: 1.06409425e-06
Iter: 1559 loss: 1.06246296e-06
Iter: 1560 loss: 1.06234643e-06
Iter: 1561 loss: 1.06211519e-06
Iter: 1562 loss: 1.06604898e-06
Iter: 1563 loss: 1.0621211e-06
Iter: 1564 loss: 1.06189725e-06
Iter: 1565 loss: 1.06287087e-06
Iter: 1566 loss: 1.06183177e-06
Iter: 1567 loss: 1.06167329e-06
Iter: 1568 loss: 1.06168068e-06
Iter: 1569 loss: 1.06153891e-06
Iter: 1570 loss: 1.06121217e-06
Iter: 1571 loss: 1.06687423e-06
Iter: 1572 loss: 1.06121672e-06
Iter: 1573 loss: 1.06095581e-06
Iter: 1574 loss: 1.06094899e-06
Iter: 1575 loss: 1.06072889e-06
Iter: 1576 loss: 1.06036327e-06
Iter: 1577 loss: 1.06195466e-06
Iter: 1578 loss: 1.06026846e-06
Iter: 1579 loss: 1.05988954e-06
Iter: 1580 loss: 1.06062839e-06
Iter: 1581 loss: 1.0597812e-06
Iter: 1582 loss: 1.05945128e-06
Iter: 1583 loss: 1.06203038e-06
Iter: 1584 loss: 1.05942854e-06
Iter: 1585 loss: 1.05911226e-06
Iter: 1586 loss: 1.06096331e-06
Iter: 1587 loss: 1.05902302e-06
Iter: 1588 loss: 1.05884772e-06
Iter: 1589 loss: 1.05879644e-06
Iter: 1590 loss: 1.05866309e-06
Iter: 1591 loss: 1.05839058e-06
Iter: 1592 loss: 1.05994764e-06
Iter: 1593 loss: 1.05837728e-06
Iter: 1594 loss: 1.05813592e-06
Iter: 1595 loss: 1.05905599e-06
Iter: 1596 loss: 1.05806339e-06
Iter: 1597 loss: 1.0578508e-06
Iter: 1598 loss: 1.057801e-06
Iter: 1599 loss: 1.05770164e-06
Iter: 1600 loss: 1.05736433e-06
Iter: 1601 loss: 1.06077778e-06
Iter: 1602 loss: 1.057384e-06
Iter: 1603 loss: 1.05728486e-06
Iter: 1604 loss: 1.05705237e-06
Iter: 1605 loss: 1.05703634e-06
Iter: 1606 loss: 1.05680874e-06
Iter: 1607 loss: 1.05678896e-06
Iter: 1608 loss: 1.05661e-06
Iter: 1609 loss: 1.05620779e-06
Iter: 1610 loss: 1.05678077e-06
Iter: 1611 loss: 1.05603374e-06
Iter: 1612 loss: 1.05582899e-06
Iter: 1613 loss: 1.05579272e-06
Iter: 1614 loss: 1.05554091e-06
Iter: 1615 loss: 1.05569711e-06
Iter: 1616 loss: 1.05537833e-06
Iter: 1617 loss: 1.05512572e-06
Iter: 1618 loss: 1.05478e-06
Iter: 1619 loss: 1.05472259e-06
Iter: 1620 loss: 1.05430956e-06
Iter: 1621 loss: 1.05740173e-06
Iter: 1622 loss: 1.05432616e-06
Iter: 1623 loss: 1.05414119e-06
Iter: 1624 loss: 1.05415029e-06
Iter: 1625 loss: 1.05398476e-06
Iter: 1626 loss: 1.05382458e-06
Iter: 1627 loss: 1.05382685e-06
Iter: 1628 loss: 1.05365143e-06
Iter: 1629 loss: 1.05428103e-06
Iter: 1630 loss: 1.05359538e-06
Iter: 1631 loss: 1.05346646e-06
Iter: 1632 loss: 1.05348113e-06
Iter: 1633 loss: 1.05334868e-06
Iter: 1634 loss: 1.05319646e-06
Iter: 1635 loss: 1.05321476e-06
Iter: 1636 loss: 1.05303764e-06
Iter: 1637 loss: 1.05420622e-06
Iter: 1638 loss: 1.05300035e-06
Iter: 1639 loss: 1.05280719e-06
Iter: 1640 loss: 1.05259528e-06
Iter: 1641 loss: 1.05253991e-06
Iter: 1642 loss: 1.05226366e-06
Iter: 1643 loss: 1.05252866e-06
Iter: 1644 loss: 1.05212598e-06
Iter: 1645 loss: 1.05177241e-06
Iter: 1646 loss: 1.05310232e-06
Iter: 1647 loss: 1.05170488e-06
Iter: 1648 loss: 1.05137008e-06
Iter: 1649 loss: 1.05615391e-06
Iter: 1650 loss: 1.0513844e-06
Iter: 1651 loss: 1.05124843e-06
Iter: 1652 loss: 1.05093477e-06
Iter: 1653 loss: 1.0551214e-06
Iter: 1654 loss: 1.05093852e-06
Iter: 1655 loss: 1.05063441e-06
Iter: 1656 loss: 1.05118136e-06
Iter: 1657 loss: 1.05049878e-06
Iter: 1658 loss: 1.05038089e-06
Iter: 1659 loss: 1.05030097e-06
Iter: 1660 loss: 1.05017841e-06
Iter: 1661 loss: 1.050131e-06
Iter: 1662 loss: 1.05004756e-06
Iter: 1663 loss: 1.04990374e-06
Iter: 1664 loss: 1.04959793e-06
Iter: 1665 loss: 1.04957689e-06
Iter: 1666 loss: 1.04963897e-06
Iter: 1667 loss: 1.04945343e-06
Iter: 1668 loss: 1.04930461e-06
Iter: 1669 loss: 1.04906917e-06
Iter: 1670 loss: 1.05224672e-06
Iter: 1671 loss: 1.04908497e-06
Iter: 1672 loss: 1.04879837e-06
Iter: 1673 loss: 1.05217077e-06
Iter: 1674 loss: 1.04884282e-06
Iter: 1675 loss: 1.04866569e-06
Iter: 1676 loss: 1.0485021e-06
Iter: 1677 loss: 1.04850164e-06
Iter: 1678 loss: 1.04829e-06
Iter: 1679 loss: 1.0488825e-06
Iter: 1680 loss: 1.04818787e-06
Iter: 1681 loss: 1.04803723e-06
Iter: 1682 loss: 1.04861647e-06
Iter: 1683 loss: 1.04796197e-06
Iter: 1684 loss: 1.04769219e-06
Iter: 1685 loss: 1.04812239e-06
Iter: 1686 loss: 1.04754395e-06
Iter: 1687 loss: 1.04738069e-06
Iter: 1688 loss: 1.0471382e-06
Iter: 1689 loss: 1.04716833e-06
Iter: 1690 loss: 1.04688286e-06
Iter: 1691 loss: 1.04842979e-06
Iter: 1692 loss: 1.04681021e-06
Iter: 1693 loss: 1.04661933e-06
Iter: 1694 loss: 1.04661979e-06
Iter: 1695 loss: 1.04645892e-06
Iter: 1696 loss: 1.04635365e-06
Iter: 1697 loss: 1.04631772e-06
Iter: 1698 loss: 1.04611013e-06
Iter: 1699 loss: 1.0460783e-06
Iter: 1700 loss: 1.04596518e-06
Iter: 1701 loss: 1.04593755e-06
Iter: 1702 loss: 1.04583251e-06
Iter: 1703 loss: 1.04574951e-06
Iter: 1704 loss: 1.04556057e-06
Iter: 1705 loss: 1.04930314e-06
Iter: 1706 loss: 1.04553851e-06
Iter: 1707 loss: 1.04529522e-06
Iter: 1708 loss: 1.04785715e-06
Iter: 1709 loss: 1.04530511e-06
Iter: 1710 loss: 1.04514902e-06
Iter: 1711 loss: 1.04486253e-06
Iter: 1712 loss: 1.04487424e-06
Iter: 1713 loss: 1.04464016e-06
Iter: 1714 loss: 1.04661422e-06
Iter: 1715 loss: 1.04464573e-06
Iter: 1716 loss: 1.04439027e-06
Iter: 1717 loss: 1.04514788e-06
Iter: 1718 loss: 1.04431365e-06
Iter: 1719 loss: 1.04401693e-06
Iter: 1720 loss: 1.04363869e-06
Iter: 1721 loss: 1.04362528e-06
Iter: 1722 loss: 1.04327034e-06
Iter: 1723 loss: 1.04602691e-06
Iter: 1724 loss: 1.04330093e-06
Iter: 1725 loss: 1.04308469e-06
Iter: 1726 loss: 1.04337312e-06
Iter: 1727 loss: 1.04299374e-06
Iter: 1728 loss: 1.04276387e-06
Iter: 1729 loss: 1.04277217e-06
Iter: 1730 loss: 1.04264745e-06
Iter: 1731 loss: 1.04252672e-06
Iter: 1732 loss: 1.04254411e-06
Iter: 1733 loss: 1.04233141e-06
Iter: 1734 loss: 1.04231958e-06
Iter: 1735 loss: 1.04222636e-06
Iter: 1736 loss: 1.04205117e-06
Iter: 1737 loss: 1.04201195e-06
Iter: 1738 loss: 1.04184369e-06
Iter: 1739 loss: 1.04177491e-06
Iter: 1740 loss: 1.04166975e-06
Iter: 1741 loss: 1.04148262e-06
Iter: 1742 loss: 1.04163269e-06
Iter: 1743 loss: 1.04136871e-06
Iter: 1744 loss: 1.04105766e-06
Iter: 1745 loss: 1.04276512e-06
Iter: 1746 loss: 1.04103935e-06
Iter: 1747 loss: 1.04085382e-06
Iter: 1748 loss: 1.04034552e-06
Iter: 1749 loss: 1.04484468e-06
Iter: 1750 loss: 1.04025332e-06
Iter: 1751 loss: 1.04029289e-06
Iter: 1752 loss: 1.0399815e-06
Iter: 1753 loss: 1.0398071e-06
Iter: 1754 loss: 1.03965635e-06
Iter: 1755 loss: 1.03960269e-06
Iter: 1756 loss: 1.03937259e-06
Iter: 1757 loss: 1.03931598e-06
Iter: 1758 loss: 1.03918228e-06
Iter: 1759 loss: 1.0390163e-06
Iter: 1760 loss: 1.04002083e-06
Iter: 1761 loss: 1.03891819e-06
Iter: 1762 loss: 1.03870502e-06
Iter: 1763 loss: 1.03956859e-06
Iter: 1764 loss: 1.03865727e-06
Iter: 1765 loss: 1.03851892e-06
Iter: 1766 loss: 1.03863965e-06
Iter: 1767 loss: 1.03842808e-06
Iter: 1768 loss: 1.03825607e-06
Iter: 1769 loss: 1.03798857e-06
Iter: 1770 loss: 1.03795492e-06
Iter: 1771 loss: 1.03762909e-06
Iter: 1772 loss: 1.04138007e-06
Iter: 1773 loss: 1.03760624e-06
Iter: 1774 loss: 1.03731918e-06
Iter: 1775 loss: 1.03927505e-06
Iter: 1776 loss: 1.03725336e-06
Iter: 1777 loss: 1.03709567e-06
Iter: 1778 loss: 1.03683374e-06
Iter: 1779 loss: 1.03676621e-06
Iter: 1780 loss: 1.03650541e-06
Iter: 1781 loss: 1.03652553e-06
Iter: 1782 loss: 1.03627212e-06
Iter: 1783 loss: 1.03611978e-06
Iter: 1784 loss: 1.03606158e-06
Iter: 1785 loss: 1.03582431e-06
Iter: 1786 loss: 1.03593084e-06
Iter: 1787 loss: 1.03566242e-06
Iter: 1788 loss: 1.03533864e-06
Iter: 1789 loss: 1.03533466e-06
Iter: 1790 loss: 1.03523462e-06
Iter: 1791 loss: 1.0350517e-06
Iter: 1792 loss: 1.03874481e-06
Iter: 1793 loss: 1.03506375e-06
Iter: 1794 loss: 1.03483251e-06
Iter: 1795 loss: 1.03532921e-06
Iter: 1796 loss: 1.03476418e-06
Iter: 1797 loss: 1.03465129e-06
Iter: 1798 loss: 1.03463367e-06
Iter: 1799 loss: 1.03451839e-06
Iter: 1800 loss: 1.03430477e-06
Iter: 1801 loss: 1.0342801e-06
Iter: 1802 loss: 1.03401135e-06
Iter: 1803 loss: 1.03372736e-06
Iter: 1804 loss: 1.03370326e-06
Iter: 1805 loss: 1.03326886e-06
Iter: 1806 loss: 1.03606442e-06
Iter: 1807 loss: 1.0332426e-06
Iter: 1808 loss: 1.03308901e-06
Iter: 1809 loss: 1.03304092e-06
Iter: 1810 loss: 1.03285788e-06
Iter: 1811 loss: 1.03268303e-06
Iter: 1812 loss: 1.03267928e-06
Iter: 1813 loss: 1.03235993e-06
Iter: 1814 loss: 1.0326512e-06
Iter: 1815 loss: 1.03224602e-06
Iter: 1816 loss: 1.03194964e-06
Iter: 1817 loss: 1.03196044e-06
Iter: 1818 loss: 1.03179514e-06
Iter: 1819 loss: 1.03157595e-06
Iter: 1820 loss: 1.03157595e-06
Iter: 1821 loss: 1.03135949e-06
Iter: 1822 loss: 1.03390539e-06
Iter: 1823 loss: 1.03134516e-06
Iter: 1824 loss: 1.03117748e-06
Iter: 1825 loss: 1.03184402e-06
Iter: 1826 loss: 1.03110381e-06
Iter: 1827 loss: 1.03096272e-06
Iter: 1828 loss: 1.0307399e-06
Iter: 1829 loss: 1.03071659e-06
Iter: 1830 loss: 1.03044113e-06
Iter: 1831 loss: 1.03110767e-06
Iter: 1832 loss: 1.03043362e-06
Iter: 1833 loss: 1.03030425e-06
Iter: 1834 loss: 1.0302806e-06
Iter: 1835 loss: 1.0301452e-06
Iter: 1836 loss: 1.02991953e-06
Iter: 1837 loss: 1.03538139e-06
Iter: 1838 loss: 1.02992226e-06
Iter: 1839 loss: 1.02968716e-06
Iter: 1840 loss: 1.03026423e-06
Iter: 1841 loss: 1.02959984e-06
Iter: 1842 loss: 1.02938895e-06
Iter: 1843 loss: 1.02934098e-06
Iter: 1844 loss: 1.02910099e-06
Iter: 1845 loss: 1.02894e-06
Iter: 1846 loss: 1.02889874e-06
Iter: 1847 loss: 1.02879608e-06
Iter: 1848 loss: 1.02864101e-06
Iter: 1849 loss: 1.03230491e-06
Iter: 1850 loss: 1.02863578e-06
Iter: 1851 loss: 1.02845434e-06
Iter: 1852 loss: 1.02922695e-06
Iter: 1853 loss: 1.02843615e-06
Iter: 1854 loss: 1.0281608e-06
Iter: 1855 loss: 1.0284748e-06
Iter: 1856 loss: 1.02805154e-06
Iter: 1857 loss: 1.02790182e-06
Iter: 1858 loss: 1.02827732e-06
Iter: 1859 loss: 1.02787567e-06
Iter: 1860 loss: 1.027699e-06
Iter: 1861 loss: 1.02911656e-06
Iter: 1862 loss: 1.02768126e-06
Iter: 1863 loss: 1.02757986e-06
Iter: 1864 loss: 1.02730269e-06
Iter: 1865 loss: 1.03035234e-06
Iter: 1866 loss: 1.02726801e-06
Iter: 1867 loss: 1.02698834e-06
Iter: 1868 loss: 1.02830404e-06
Iter: 1869 loss: 1.02698527e-06
Iter: 1870 loss: 1.02675642e-06
Iter: 1871 loss: 1.02676916e-06
Iter: 1872 loss: 1.02655076e-06
Iter: 1873 loss: 1.02645538e-06
Iter: 1874 loss: 1.02635386e-06
Iter: 1875 loss: 1.02614285e-06
Iter: 1876 loss: 1.02618992e-06
Iter: 1877 loss: 1.02601336e-06
Iter: 1878 loss: 1.02571767e-06
Iter: 1879 loss: 1.02667218e-06
Iter: 1880 loss: 1.02565059e-06
Iter: 1881 loss: 1.02534818e-06
Iter: 1882 loss: 1.0259447e-06
Iter: 1883 loss: 1.02531158e-06
Iter: 1884 loss: 1.0250717e-06
Iter: 1885 loss: 1.02559136e-06
Iter: 1886 loss: 1.0249513e-06
Iter: 1887 loss: 1.02492777e-06
Iter: 1888 loss: 1.02481022e-06
Iter: 1889 loss: 1.02476849e-06
Iter: 1890 loss: 1.02454499e-06
Iter: 1891 loss: 1.0275952e-06
Iter: 1892 loss: 1.02454192e-06
Iter: 1893 loss: 1.02440322e-06
Iter: 1894 loss: 1.02438662e-06
Iter: 1895 loss: 1.02426316e-06
Iter: 1896 loss: 1.02406716e-06
Iter: 1897 loss: 1.02806848e-06
Iter: 1898 loss: 1.02402964e-06
Iter: 1899 loss: 1.02382023e-06
Iter: 1900 loss: 1.02438946e-06
Iter: 1901 loss: 1.02373713e-06
Iter: 1902 loss: 1.02361332e-06
Iter: 1903 loss: 1.02359172e-06
Iter: 1904 loss: 1.02346303e-06
Iter: 1905 loss: 1.02328477e-06
Iter: 1906 loss: 1.02325782e-06
Iter: 1907 loss: 1.0230483e-06
Iter: 1908 loss: 1.02292404e-06
Iter: 1909 loss: 1.02285151e-06
Iter: 1910 loss: 1.02257309e-06
Iter: 1911 loss: 1.02471768e-06
Iter: 1912 loss: 1.02254421e-06
Iter: 1913 loss: 1.02224385e-06
Iter: 1914 loss: 1.02425531e-06
Iter: 1915 loss: 1.02216143e-06
Iter: 1916 loss: 1.02202398e-06
Iter: 1917 loss: 1.02211447e-06
Iter: 1918 loss: 1.02192598e-06
Iter: 1919 loss: 1.02168883e-06
Iter: 1920 loss: 1.02141598e-06
Iter: 1921 loss: 1.0213912e-06
Iter: 1922 loss: 1.02119645e-06
Iter: 1923 loss: 1.02115223e-06
Iter: 1924 loss: 1.02103866e-06
Iter: 1925 loss: 1.02299737e-06
Iter: 1926 loss: 1.02103797e-06
Iter: 1927 loss: 1.02096749e-06
Iter: 1928 loss: 1.02074682e-06
Iter: 1929 loss: 1.02073295e-06
Iter: 1930 loss: 1.02066542e-06
Iter: 1931 loss: 1.02063143e-06
Iter: 1932 loss: 1.02055765e-06
Iter: 1933 loss: 1.02037939e-06
Iter: 1934 loss: 1.02040576e-06
Iter: 1935 loss: 1.02020658e-06
Iter: 1936 loss: 1.02034619e-06
Iter: 1937 loss: 1.0201195e-06
Iter: 1938 loss: 1.01993533e-06
Iter: 1939 loss: 1.02247463e-06
Iter: 1940 loss: 1.01991782e-06
Iter: 1941 loss: 1.01977184e-06
Iter: 1942 loss: 1.01954811e-06
Iter: 1943 loss: 1.01952594e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.2 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi1.2
+ date
Sun Nov  8 05:28:01 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8/300_100_100_100_1 --function f1 --psi -1 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1cb61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1c242f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1c34d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1c93ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1cf61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1b9e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1b2d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1af4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1af9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1af9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1b94048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1a068c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1a06b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1a51730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1a7e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1a51a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1a85598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1a85d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1aa87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7c1ab9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bd01fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bcf556a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bcebb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bcebc840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bcebc048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bcf791e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bce919d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bce39378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bce39158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bce41510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bce00378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bcd89048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bcd89378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bcda1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bcd3c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7bcdac158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0031683436
test_loss: 0.0031801322
train_loss: 0.0026221331
test_loss: 0.0029063313
train_loss: 0.002513703
test_loss: 0.002502058
train_loss: 0.002543241
test_loss: 0.0026248472
train_loss: 0.0023561467
test_loss: 0.0025145148
train_loss: 0.0024902562
test_loss: 0.002711781
train_loss: 0.0025366102
test_loss: 0.0026541927
train_loss: 0.0022500453
test_loss: 0.002551688
train_loss: 0.002365318
test_loss: 0.0025976533
train_loss: 0.0023359014
test_loss: 0.002463141
train_loss: 0.0024638246
test_loss: 0.0026154665
train_loss: 0.002540091
test_loss: 0.00252891
train_loss: 0.002268586
test_loss: 0.0025350181
train_loss: 0.0023301619
test_loss: 0.002588696
train_loss: 0.0022300938
test_loss: 0.002542089
train_loss: 0.0025013767
test_loss: 0.0027325111
train_loss: 0.002264914
test_loss: 0.00247424
train_loss: 0.0024124489
test_loss: 0.0025943387
train_loss: 0.002362522
test_loss: 0.0025897091
train_loss: 0.0024912863
test_loss: 0.002608819
train_loss: 0.0021997273
test_loss: 0.0023286704
train_loss: 0.0025435707
test_loss: 0.0024113446
train_loss: 0.002169193
test_loss: 0.002384074
train_loss: 0.0021996857
test_loss: 0.0024136575
train_loss: 0.0023639135
test_loss: 0.0025163929
train_loss: 0.0021507211
test_loss: 0.0024344497
train_loss: 0.0022336245
test_loss: 0.0023202626
train_loss: 0.00231296
test_loss: 0.002586568
train_loss: 0.0023922762
test_loss: 0.0024965347
train_loss: 0.0022078415
test_loss: 0.0024924853
train_loss: 0.0021473689
test_loss: 0.0026009304
train_loss: 0.0025431549
test_loss: 0.0027070944
train_loss: 0.0021919762
test_loss: 0.002769958
train_loss: 0.0023269993
test_loss: 0.0025184928
train_loss: 0.0022699477
test_loss: 0.0024958097
train_loss: 0.0023677486
test_loss: 0.002476339
train_loss: 0.00248216
test_loss: 0.0025435179
train_loss: 0.0023922408
test_loss: 0.0024485383
train_loss: 0.0021898337
test_loss: 0.0024941547
train_loss: 0.0022592815
test_loss: 0.0024722891
train_loss: 0.002327538
test_loss: 0.002641398
train_loss: 0.0024197006
test_loss: 0.0023729308
train_loss: 0.002414799
test_loss: 0.0026006878
train_loss: 0.0023006294
test_loss: 0.0025370966
train_loss: 0.0023095736
test_loss: 0.0024109492
train_loss: 0.0024019347
test_loss: 0.0024303852
train_loss: 0.0022495026
test_loss: 0.002311596
train_loss: 0.0021918523
test_loss: 0.002397316
train_loss: 0.0021913114
test_loss: 0.002372628
train_loss: 0.0023797795
test_loss: 0.0024636837
train_loss: 0.0021687532
test_loss: 0.0025320062
train_loss: 0.0020476747
test_loss: 0.0023870326
train_loss: 0.0024029722
test_loss: 0.0026005593
train_loss: 0.0022118308
test_loss: 0.0024783227
train_loss: 0.0021484836
test_loss: 0.0023576736
train_loss: 0.0023745347
test_loss: 0.0026041043
train_loss: 0.0022532209
test_loss: 0.0024041259
train_loss: 0.0022650103
test_loss: 0.0025085253
train_loss: 0.002362642
test_loss: 0.002486642
train_loss: 0.00229427
test_loss: 0.0026878386
train_loss: 0.0021950975
test_loss: 0.002373073
train_loss: 0.0024167362
test_loss: 0.0026064375
train_loss: 0.0023406544
test_loss: 0.002480671
train_loss: 0.002258436
test_loss: 0.0025669155
train_loss: 0.0021959087
test_loss: 0.0026361984
train_loss: 0.002314297
test_loss: 0.002388107
train_loss: 0.0020122612
test_loss: 0.0023272608
train_loss: 0.0021626058
test_loss: 0.0025091455
train_loss: 0.0023802067
test_loss: 0.0026595932
train_loss: 0.00221053
test_loss: 0.0024712689
train_loss: 0.0021918893
test_loss: 0.0024227952
train_loss: 0.0020642034
test_loss: 0.0023276578
train_loss: 0.0022296864
test_loss: 0.0022697772
train_loss: 0.0021062375
test_loss: 0.0024171146
train_loss: 0.0020444267
test_loss: 0.0023848314
train_loss: 0.0023955668
test_loss: 0.0023790777
train_loss: 0.0023597442
test_loss: 0.0025255068
train_loss: 0.0021193812
test_loss: 0.0024775523
train_loss: 0.002470574
test_loss: 0.0024604911
train_loss: 0.0022268598
test_loss: 0.0023159918
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi1.2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.2/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -1 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi1.2/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa6e1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa72e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa72e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa72e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa6862f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa5cdd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa5baae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa55b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa579378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa579510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa545b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa4e9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa506598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa506b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa492b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa4888c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa481598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa481d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa3f29d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa3f2488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9fa3ff620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9b1ef0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9b1e9a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9b1e9a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9b1ec5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9b1ebfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9b1e31048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc9b1e53950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c6e4048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c6eb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c6ac840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c65b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c65bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c67cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c62eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc98c5dd730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.86602471e-06
Iter: 2 loss: 7.42259363e-06
Iter: 3 loss: 6.02731507e-06
Iter: 4 loss: 5.61865318e-06
Iter: 5 loss: 7.49434184e-06
Iter: 6 loss: 5.54093185e-06
Iter: 7 loss: 5.35066738e-06
Iter: 8 loss: 6.10167353e-06
Iter: 9 loss: 5.30724719e-06
Iter: 10 loss: 5.11950566e-06
Iter: 11 loss: 5.46860156e-06
Iter: 12 loss: 5.03960609e-06
Iter: 13 loss: 4.89859531e-06
Iter: 14 loss: 5.54445705e-06
Iter: 15 loss: 4.87171837e-06
Iter: 16 loss: 4.72522606e-06
Iter: 17 loss: 5.00700025e-06
Iter: 18 loss: 4.66397705e-06
Iter: 19 loss: 4.58278555e-06
Iter: 20 loss: 4.49966728e-06
Iter: 21 loss: 4.48397805e-06
Iter: 22 loss: 4.31792068e-06
Iter: 23 loss: 4.63418382e-06
Iter: 24 loss: 4.2480342e-06
Iter: 25 loss: 4.11552355e-06
Iter: 26 loss: 4.11532847e-06
Iter: 27 loss: 4.0001978e-06
Iter: 28 loss: 3.91787489e-06
Iter: 29 loss: 3.87774344e-06
Iter: 30 loss: 3.74042838e-06
Iter: 31 loss: 3.73665125e-06
Iter: 32 loss: 3.62936862e-06
Iter: 33 loss: 3.50581536e-06
Iter: 34 loss: 5.47799891e-06
Iter: 35 loss: 3.50581945e-06
Iter: 36 loss: 3.44281898e-06
Iter: 37 loss: 3.46843944e-06
Iter: 38 loss: 3.39947769e-06
Iter: 39 loss: 3.36550374e-06
Iter: 40 loss: 3.35572e-06
Iter: 41 loss: 3.31036631e-06
Iter: 42 loss: 3.27694715e-06
Iter: 43 loss: 3.26167037e-06
Iter: 44 loss: 3.21694097e-06
Iter: 45 loss: 3.83378074e-06
Iter: 46 loss: 3.21669313e-06
Iter: 47 loss: 3.18147704e-06
Iter: 48 loss: 3.18311504e-06
Iter: 49 loss: 3.15362263e-06
Iter: 50 loss: 3.11367057e-06
Iter: 51 loss: 3.32806e-06
Iter: 52 loss: 3.10762243e-06
Iter: 53 loss: 3.06020388e-06
Iter: 54 loss: 3.00161582e-06
Iter: 55 loss: 2.99653152e-06
Iter: 56 loss: 2.92451477e-06
Iter: 57 loss: 2.95032305e-06
Iter: 58 loss: 2.87387e-06
Iter: 59 loss: 2.80961694e-06
Iter: 60 loss: 3.42219096e-06
Iter: 61 loss: 2.80704262e-06
Iter: 62 loss: 2.74412196e-06
Iter: 63 loss: 3.00409692e-06
Iter: 64 loss: 2.73044475e-06
Iter: 65 loss: 2.67751102e-06
Iter: 66 loss: 2.77564391e-06
Iter: 67 loss: 2.65481481e-06
Iter: 68 loss: 2.62054596e-06
Iter: 69 loss: 2.58854288e-06
Iter: 70 loss: 2.58056525e-06
Iter: 71 loss: 2.54129964e-06
Iter: 72 loss: 3.09130473e-06
Iter: 73 loss: 2.5411041e-06
Iter: 74 loss: 2.51360916e-06
Iter: 75 loss: 2.60427169e-06
Iter: 76 loss: 2.50607309e-06
Iter: 77 loss: 2.49155619e-06
Iter: 78 loss: 2.48995934e-06
Iter: 79 loss: 2.476482e-06
Iter: 80 loss: 2.44354828e-06
Iter: 81 loss: 2.78051789e-06
Iter: 82 loss: 2.43950217e-06
Iter: 83 loss: 2.42141778e-06
Iter: 84 loss: 2.41692692e-06
Iter: 85 loss: 2.40272811e-06
Iter: 86 loss: 2.37312952e-06
Iter: 87 loss: 2.87333432e-06
Iter: 88 loss: 2.37246013e-06
Iter: 89 loss: 2.33580249e-06
Iter: 90 loss: 2.70229611e-06
Iter: 91 loss: 2.33453329e-06
Iter: 92 loss: 2.31220019e-06
Iter: 93 loss: 2.29004058e-06
Iter: 94 loss: 2.2854374e-06
Iter: 95 loss: 2.24742325e-06
Iter: 96 loss: 2.2860745e-06
Iter: 97 loss: 2.22614358e-06
Iter: 98 loss: 2.20233278e-06
Iter: 99 loss: 2.42898295e-06
Iter: 100 loss: 2.20128027e-06
Iter: 101 loss: 2.17475076e-06
Iter: 102 loss: 2.26313341e-06
Iter: 103 loss: 2.16753e-06
Iter: 104 loss: 2.15382397e-06
Iter: 105 loss: 2.15685327e-06
Iter: 106 loss: 2.14382703e-06
Iter: 107 loss: 2.12339069e-06
Iter: 108 loss: 2.12246619e-06
Iter: 109 loss: 2.10682038e-06
Iter: 110 loss: 2.07920152e-06
Iter: 111 loss: 2.19058165e-06
Iter: 112 loss: 2.07304288e-06
Iter: 113 loss: 2.06352593e-06
Iter: 114 loss: 2.05843776e-06
Iter: 115 loss: 2.04791627e-06
Iter: 116 loss: 2.02468891e-06
Iter: 117 loss: 2.35427092e-06
Iter: 118 loss: 2.02349429e-06
Iter: 119 loss: 2.00322847e-06
Iter: 120 loss: 2.0030177e-06
Iter: 121 loss: 1.98839416e-06
Iter: 122 loss: 1.99206625e-06
Iter: 123 loss: 1.97760392e-06
Iter: 124 loss: 1.96354108e-06
Iter: 125 loss: 2.04068351e-06
Iter: 126 loss: 1.96138899e-06
Iter: 127 loss: 1.94750805e-06
Iter: 128 loss: 1.96283941e-06
Iter: 129 loss: 1.93989422e-06
Iter: 130 loss: 1.9302031e-06
Iter: 131 loss: 1.93067831e-06
Iter: 132 loss: 1.92258631e-06
Iter: 133 loss: 1.9101883e-06
Iter: 134 loss: 1.96492601e-06
Iter: 135 loss: 1.90781134e-06
Iter: 136 loss: 1.90189439e-06
Iter: 137 loss: 1.90190167e-06
Iter: 138 loss: 1.89550235e-06
Iter: 139 loss: 1.88439139e-06
Iter: 140 loss: 1.88436707e-06
Iter: 141 loss: 1.87029491e-06
Iter: 142 loss: 1.87219848e-06
Iter: 143 loss: 1.85953172e-06
Iter: 144 loss: 1.8452173e-06
Iter: 145 loss: 1.92659263e-06
Iter: 146 loss: 1.84329019e-06
Iter: 147 loss: 1.83293173e-06
Iter: 148 loss: 1.83286625e-06
Iter: 149 loss: 1.82285919e-06
Iter: 150 loss: 1.82700433e-06
Iter: 151 loss: 1.81605526e-06
Iter: 152 loss: 1.80852362e-06
Iter: 153 loss: 1.83161683e-06
Iter: 154 loss: 1.80632355e-06
Iter: 155 loss: 1.79461858e-06
Iter: 156 loss: 1.79324661e-06
Iter: 157 loss: 1.78486596e-06
Iter: 158 loss: 1.77960885e-06
Iter: 159 loss: 1.85443184e-06
Iter: 160 loss: 1.77960487e-06
Iter: 161 loss: 1.77433071e-06
Iter: 162 loss: 1.76445019e-06
Iter: 163 loss: 1.98626412e-06
Iter: 164 loss: 1.76441642e-06
Iter: 165 loss: 1.75305229e-06
Iter: 166 loss: 1.80299185e-06
Iter: 167 loss: 1.75074956e-06
Iter: 168 loss: 1.74350839e-06
Iter: 169 loss: 1.74818285e-06
Iter: 170 loss: 1.73892681e-06
Iter: 171 loss: 1.7312957e-06
Iter: 172 loss: 1.81767916e-06
Iter: 173 loss: 1.73120225e-06
Iter: 174 loss: 1.7227328e-06
Iter: 175 loss: 1.72430236e-06
Iter: 176 loss: 1.71640613e-06
Iter: 177 loss: 1.70872408e-06
Iter: 178 loss: 1.72737896e-06
Iter: 179 loss: 1.70589738e-06
Iter: 180 loss: 1.69673274e-06
Iter: 181 loss: 1.68912652e-06
Iter: 182 loss: 1.68640315e-06
Iter: 183 loss: 1.68852671e-06
Iter: 184 loss: 1.68187808e-06
Iter: 185 loss: 1.67730502e-06
Iter: 186 loss: 1.67319206e-06
Iter: 187 loss: 1.67199551e-06
Iter: 188 loss: 1.66733162e-06
Iter: 189 loss: 1.69873294e-06
Iter: 190 loss: 1.66688187e-06
Iter: 191 loss: 1.66204973e-06
Iter: 192 loss: 1.67107135e-06
Iter: 193 loss: 1.65998495e-06
Iter: 194 loss: 1.65582878e-06
Iter: 195 loss: 1.65380959e-06
Iter: 196 loss: 1.65177903e-06
Iter: 197 loss: 1.64761991e-06
Iter: 198 loss: 1.64751691e-06
Iter: 199 loss: 1.64477353e-06
Iter: 200 loss: 1.63839582e-06
Iter: 201 loss: 1.72708178e-06
Iter: 202 loss: 1.63808568e-06
Iter: 203 loss: 1.63106665e-06
Iter: 204 loss: 1.65171264e-06
Iter: 205 loss: 1.62892957e-06
Iter: 206 loss: 1.62224012e-06
Iter: 207 loss: 1.65544952e-06
Iter: 208 loss: 1.62106653e-06
Iter: 209 loss: 1.61548257e-06
Iter: 210 loss: 1.67593078e-06
Iter: 211 loss: 1.61530056e-06
Iter: 212 loss: 1.61092157e-06
Iter: 213 loss: 1.60551861e-06
Iter: 214 loss: 1.60506772e-06
Iter: 215 loss: 1.59904675e-06
Iter: 216 loss: 1.60665945e-06
Iter: 217 loss: 1.59599301e-06
Iter: 218 loss: 1.5923938e-06
Iter: 219 loss: 1.59171827e-06
Iter: 220 loss: 1.58769467e-06
Iter: 221 loss: 1.58876696e-06
Iter: 222 loss: 1.5847487e-06
Iter: 223 loss: 1.58070759e-06
Iter: 224 loss: 1.5853775e-06
Iter: 225 loss: 1.57857448e-06
Iter: 226 loss: 1.57422937e-06
Iter: 227 loss: 1.62765764e-06
Iter: 228 loss: 1.57422187e-06
Iter: 229 loss: 1.57220438e-06
Iter: 230 loss: 1.56781948e-06
Iter: 231 loss: 1.63056484e-06
Iter: 232 loss: 1.567661e-06
Iter: 233 loss: 1.56141573e-06
Iter: 234 loss: 1.6008496e-06
Iter: 235 loss: 1.56074816e-06
Iter: 236 loss: 1.55515806e-06
Iter: 237 loss: 1.56581518e-06
Iter: 238 loss: 1.55280111e-06
Iter: 239 loss: 1.54902057e-06
Iter: 240 loss: 1.54811789e-06
Iter: 241 loss: 1.54571762e-06
Iter: 242 loss: 1.5407204e-06
Iter: 243 loss: 1.54521194e-06
Iter: 244 loss: 1.53773476e-06
Iter: 245 loss: 1.53675421e-06
Iter: 246 loss: 1.53472877e-06
Iter: 247 loss: 1.53316967e-06
Iter: 248 loss: 1.52967482e-06
Iter: 249 loss: 1.5817385e-06
Iter: 250 loss: 1.52959888e-06
Iter: 251 loss: 1.52468e-06
Iter: 252 loss: 1.5362458e-06
Iter: 253 loss: 1.52281018e-06
Iter: 254 loss: 1.52246491e-06
Iter: 255 loss: 1.52135453e-06
Iter: 256 loss: 1.51974041e-06
Iter: 257 loss: 1.51594929e-06
Iter: 258 loss: 1.55944372e-06
Iter: 259 loss: 1.51561699e-06
Iter: 260 loss: 1.51219854e-06
Iter: 261 loss: 1.54146937e-06
Iter: 262 loss: 1.51202426e-06
Iter: 263 loss: 1.50806943e-06
Iter: 264 loss: 1.51350264e-06
Iter: 265 loss: 1.50610299e-06
Iter: 266 loss: 1.50308279e-06
Iter: 267 loss: 1.50460266e-06
Iter: 268 loss: 1.50110611e-06
Iter: 269 loss: 1.49757079e-06
Iter: 270 loss: 1.52639109e-06
Iter: 271 loss: 1.49732591e-06
Iter: 272 loss: 1.49398375e-06
Iter: 273 loss: 1.49120126e-06
Iter: 274 loss: 1.49015477e-06
Iter: 275 loss: 1.48607637e-06
Iter: 276 loss: 1.48906702e-06
Iter: 277 loss: 1.48357219e-06
Iter: 278 loss: 1.48016852e-06
Iter: 279 loss: 1.51137806e-06
Iter: 280 loss: 1.48003278e-06
Iter: 281 loss: 1.47684887e-06
Iter: 282 loss: 1.49375455e-06
Iter: 283 loss: 1.47641163e-06
Iter: 284 loss: 1.47368951e-06
Iter: 285 loss: 1.47923356e-06
Iter: 286 loss: 1.47263188e-06
Iter: 287 loss: 1.47059563e-06
Iter: 288 loss: 1.46952755e-06
Iter: 289 loss: 1.46864102e-06
Iter: 290 loss: 1.46609068e-06
Iter: 291 loss: 1.50361552e-06
Iter: 292 loss: 1.46611478e-06
Iter: 293 loss: 1.46314471e-06
Iter: 294 loss: 1.46224443e-06
Iter: 295 loss: 1.4604218e-06
Iter: 296 loss: 1.45748663e-06
Iter: 297 loss: 1.46311709e-06
Iter: 298 loss: 1.45622198e-06
Iter: 299 loss: 1.45391368e-06
Iter: 300 loss: 1.45387594e-06
Iter: 301 loss: 1.45250851e-06
Iter: 302 loss: 1.4482373e-06
Iter: 303 loss: 1.47009155e-06
Iter: 304 loss: 1.44683781e-06
Iter: 305 loss: 1.44556122e-06
Iter: 306 loss: 1.44464457e-06
Iter: 307 loss: 1.44271144e-06
Iter: 308 loss: 1.44284854e-06
Iter: 309 loss: 1.44116348e-06
Iter: 310 loss: 1.43881311e-06
Iter: 311 loss: 1.44008072e-06
Iter: 312 loss: 1.43733803e-06
Iter: 313 loss: 1.43456305e-06
Iter: 314 loss: 1.43440775e-06
Iter: 315 loss: 1.43231716e-06
Iter: 316 loss: 1.43247075e-06
Iter: 317 loss: 1.43105365e-06
Iter: 318 loss: 1.43001353e-06
Iter: 319 loss: 1.42800332e-06
Iter: 320 loss: 1.47133676e-06
Iter: 321 loss: 1.42799536e-06
Iter: 322 loss: 1.42511306e-06
Iter: 323 loss: 1.42958402e-06
Iter: 324 loss: 1.42379895e-06
Iter: 325 loss: 1.42166027e-06
Iter: 326 loss: 1.44079172e-06
Iter: 327 loss: 1.42164413e-06
Iter: 328 loss: 1.41957992e-06
Iter: 329 loss: 1.43392504e-06
Iter: 330 loss: 1.41942019e-06
Iter: 331 loss: 1.41829184e-06
Iter: 332 loss: 1.4152879e-06
Iter: 333 loss: 1.43436648e-06
Iter: 334 loss: 1.41456974e-06
Iter: 335 loss: 1.41160376e-06
Iter: 336 loss: 1.41144642e-06
Iter: 337 loss: 1.40939324e-06
Iter: 338 loss: 1.40808834e-06
Iter: 339 loss: 1.40723409e-06
Iter: 340 loss: 1.40516647e-06
Iter: 341 loss: 1.41021496e-06
Iter: 342 loss: 1.40436794e-06
Iter: 343 loss: 1.40245834e-06
Iter: 344 loss: 1.42334443e-06
Iter: 345 loss: 1.40242457e-06
Iter: 346 loss: 1.40075804e-06
Iter: 347 loss: 1.39779e-06
Iter: 348 loss: 1.3978181e-06
Iter: 349 loss: 1.39583653e-06
Iter: 350 loss: 1.41404564e-06
Iter: 351 loss: 1.39574536e-06
Iter: 352 loss: 1.39409269e-06
Iter: 353 loss: 1.39641315e-06
Iter: 354 loss: 1.39336339e-06
Iter: 355 loss: 1.39163626e-06
Iter: 356 loss: 1.41252042e-06
Iter: 357 loss: 1.39160557e-06
Iter: 358 loss: 1.39023746e-06
Iter: 359 loss: 1.38696078e-06
Iter: 360 loss: 1.42682097e-06
Iter: 361 loss: 1.38666337e-06
Iter: 362 loss: 1.38300425e-06
Iter: 363 loss: 1.3917637e-06
Iter: 364 loss: 1.38167684e-06
Iter: 365 loss: 1.38215182e-06
Iter: 366 loss: 1.38041628e-06
Iter: 367 loss: 1.37918e-06
Iter: 368 loss: 1.3785542e-06
Iter: 369 loss: 1.37804886e-06
Iter: 370 loss: 1.37653501e-06
Iter: 371 loss: 1.37563711e-06
Iter: 372 loss: 1.37494146e-06
Iter: 373 loss: 1.37499796e-06
Iter: 374 loss: 1.37416305e-06
Iter: 375 loss: 1.37361872e-06
Iter: 376 loss: 1.37182928e-06
Iter: 377 loss: 1.37616848e-06
Iter: 378 loss: 1.37083805e-06
Iter: 379 loss: 1.36904816e-06
Iter: 380 loss: 1.38456664e-06
Iter: 381 loss: 1.3689596e-06
Iter: 382 loss: 1.36710105e-06
Iter: 383 loss: 1.37834036e-06
Iter: 384 loss: 1.3668996e-06
Iter: 385 loss: 1.36527183e-06
Iter: 386 loss: 1.36649896e-06
Iter: 387 loss: 1.36437359e-06
Iter: 388 loss: 1.36272558e-06
Iter: 389 loss: 1.36093286e-06
Iter: 390 loss: 1.36065239e-06
Iter: 391 loss: 1.35761843e-06
Iter: 392 loss: 1.37815e-06
Iter: 393 loss: 1.35730181e-06
Iter: 394 loss: 1.35529888e-06
Iter: 395 loss: 1.36332801e-06
Iter: 396 loss: 1.35490052e-06
Iter: 397 loss: 1.35297262e-06
Iter: 398 loss: 1.36923848e-06
Iter: 399 loss: 1.35282971e-06
Iter: 400 loss: 1.35163555e-06
Iter: 401 loss: 1.34980451e-06
Iter: 402 loss: 1.3497297e-06
Iter: 403 loss: 1.34927154e-06
Iter: 404 loss: 1.34879519e-06
Iter: 405 loss: 1.3478184e-06
Iter: 406 loss: 1.34694687e-06
Iter: 407 loss: 1.34675281e-06
Iter: 408 loss: 1.34539823e-06
Iter: 409 loss: 1.3448016e-06
Iter: 410 loss: 1.34414449e-06
Iter: 411 loss: 1.34338711e-06
Iter: 412 loss: 1.34326956e-06
Iter: 413 loss: 1.3422989e-06
Iter: 414 loss: 1.34048696e-06
Iter: 415 loss: 1.38219718e-06
Iter: 416 loss: 1.34047809e-06
Iter: 417 loss: 1.33912658e-06
Iter: 418 loss: 1.34464744e-06
Iter: 419 loss: 1.33880144e-06
Iter: 420 loss: 1.33752667e-06
Iter: 421 loss: 1.33895458e-06
Iter: 422 loss: 1.33681e-06
Iter: 423 loss: 1.33510798e-06
Iter: 424 loss: 1.35142227e-06
Iter: 425 loss: 1.33507331e-06
Iter: 426 loss: 1.33402909e-06
Iter: 427 loss: 1.33215e-06
Iter: 428 loss: 1.37685799e-06
Iter: 429 loss: 1.33215713e-06
Iter: 430 loss: 1.33068761e-06
Iter: 431 loss: 1.34866548e-06
Iter: 432 loss: 1.33069557e-06
Iter: 433 loss: 1.32962259e-06
Iter: 434 loss: 1.32965636e-06
Iter: 435 loss: 1.32870707e-06
Iter: 436 loss: 1.32685864e-06
Iter: 437 loss: 1.34082939e-06
Iter: 438 loss: 1.32670061e-06
Iter: 439 loss: 1.32536798e-06
Iter: 440 loss: 1.32408468e-06
Iter: 441 loss: 1.32383354e-06
Iter: 442 loss: 1.32221328e-06
Iter: 443 loss: 1.32906223e-06
Iter: 444 loss: 1.3218671e-06
Iter: 445 loss: 1.32007347e-06
Iter: 446 loss: 1.32202445e-06
Iter: 447 loss: 1.31914157e-06
Iter: 448 loss: 1.3175611e-06
Iter: 449 loss: 1.31749391e-06
Iter: 450 loss: 1.31693378e-06
Iter: 451 loss: 1.31557908e-06
Iter: 452 loss: 1.32502873e-06
Iter: 453 loss: 1.31530294e-06
Iter: 454 loss: 1.31467664e-06
Iter: 455 loss: 1.31431227e-06
Iter: 456 loss: 1.31368347e-06
Iter: 457 loss: 1.31306751e-06
Iter: 458 loss: 1.31287197e-06
Iter: 459 loss: 1.31207219e-06
Iter: 460 loss: 1.31616548e-06
Iter: 461 loss: 1.31195725e-06
Iter: 462 loss: 1.31103502e-06
Iter: 463 loss: 1.31245292e-06
Iter: 464 loss: 1.31063678e-06
Iter: 465 loss: 1.30961917e-06
Iter: 466 loss: 1.30824446e-06
Iter: 467 loss: 1.30817841e-06
Iter: 468 loss: 1.30647413e-06
Iter: 469 loss: 1.31153843e-06
Iter: 470 loss: 1.30590581e-06
Iter: 471 loss: 1.30423314e-06
Iter: 472 loss: 1.31134698e-06
Iter: 473 loss: 1.30389844e-06
Iter: 474 loss: 1.30204035e-06
Iter: 475 loss: 1.30213039e-06
Iter: 476 loss: 1.30061539e-06
Iter: 477 loss: 1.29950786e-06
Iter: 478 loss: 1.29905709e-06
Iter: 479 loss: 1.2982789e-06
Iter: 480 loss: 1.29679211e-06
Iter: 481 loss: 1.32624905e-06
Iter: 482 loss: 1.29680859e-06
Iter: 483 loss: 1.29538944e-06
Iter: 484 loss: 1.31310298e-06
Iter: 485 loss: 1.29543696e-06
Iter: 486 loss: 1.29433749e-06
Iter: 487 loss: 1.30352794e-06
Iter: 488 loss: 1.2942661e-06
Iter: 489 loss: 1.29373484e-06
Iter: 490 loss: 1.29282216e-06
Iter: 491 loss: 1.29279704e-06
Iter: 492 loss: 1.29188766e-06
Iter: 493 loss: 1.29686612e-06
Iter: 494 loss: 1.29171576e-06
Iter: 495 loss: 1.29051023e-06
Iter: 496 loss: 1.29301952e-06
Iter: 497 loss: 1.28994884e-06
Iter: 498 loss: 1.28912438e-06
Iter: 499 loss: 1.28946499e-06
Iter: 500 loss: 1.28853344e-06
Iter: 501 loss: 1.28733086e-06
Iter: 502 loss: 1.28945499e-06
Iter: 503 loss: 1.28680097e-06
Iter: 504 loss: 1.2850353e-06
Iter: 505 loss: 1.29131854e-06
Iter: 506 loss: 1.28452984e-06
Iter: 507 loss: 1.28370903e-06
Iter: 508 loss: 1.28245324e-06
Iter: 509 loss: 1.28235979e-06
Iter: 510 loss: 1.28063334e-06
Iter: 511 loss: 1.29265663e-06
Iter: 512 loss: 1.28039198e-06
Iter: 513 loss: 1.27938097e-06
Iter: 514 loss: 1.28034105e-06
Iter: 515 loss: 1.27880708e-06
Iter: 516 loss: 1.2783629e-06
Iter: 517 loss: 1.27820977e-06
Iter: 518 loss: 1.27762087e-06
Iter: 519 loss: 1.27683984e-06
Iter: 520 loss: 1.27681119e-06
Iter: 521 loss: 1.27581541e-06
Iter: 522 loss: 1.27967883e-06
Iter: 523 loss: 1.27551129e-06
Iter: 524 loss: 1.2747048e-06
Iter: 525 loss: 1.27471299e-06
Iter: 526 loss: 1.27427325e-06
Iter: 527 loss: 1.27304247e-06
Iter: 528 loss: 1.28090949e-06
Iter: 529 loss: 1.27273188e-06
Iter: 530 loss: 1.27129135e-06
Iter: 531 loss: 1.2740843e-06
Iter: 532 loss: 1.27069973e-06
Iter: 533 loss: 1.27012413e-06
Iter: 534 loss: 1.26976965e-06
Iter: 535 loss: 1.26918178e-06
Iter: 536 loss: 1.26802558e-06
Iter: 537 loss: 1.29585601e-06
Iter: 538 loss: 1.26804355e-06
Iter: 539 loss: 1.26672853e-06
Iter: 540 loss: 1.26755458e-06
Iter: 541 loss: 1.26598138e-06
Iter: 542 loss: 1.26537111e-06
Iter: 543 loss: 1.26523173e-06
Iter: 544 loss: 1.26465511e-06
Iter: 545 loss: 1.26434145e-06
Iter: 546 loss: 1.2640819e-06
Iter: 547 loss: 1.26345878e-06
Iter: 548 loss: 1.26440489e-06
Iter: 549 loss: 1.26321891e-06
Iter: 550 loss: 1.2622786e-06
Iter: 551 loss: 1.26131818e-06
Iter: 552 loss: 1.26117743e-06
Iter: 553 loss: 1.25959059e-06
Iter: 554 loss: 1.26433156e-06
Iter: 555 loss: 1.25914835e-06
Iter: 556 loss: 1.2583439e-06
Iter: 557 loss: 1.25824852e-06
Iter: 558 loss: 1.25748215e-06
Iter: 559 loss: 1.26081068e-06
Iter: 560 loss: 1.25730844e-06
Iter: 561 loss: 1.25661359e-06
Iter: 562 loss: 1.25496081e-06
Iter: 563 loss: 1.27216936e-06
Iter: 564 loss: 1.25473946e-06
Iter: 565 loss: 1.25274835e-06
Iter: 566 loss: 1.25990982e-06
Iter: 567 loss: 1.25216195e-06
Iter: 568 loss: 1.25290399e-06
Iter: 569 loss: 1.251605e-06
Iter: 570 loss: 1.25130816e-06
Iter: 571 loss: 1.25051235e-06
Iter: 572 loss: 1.25500651e-06
Iter: 573 loss: 1.2502959e-06
Iter: 574 loss: 1.24921712e-06
Iter: 575 loss: 1.26038037e-06
Iter: 576 loss: 1.24921735e-06
Iter: 577 loss: 1.24850612e-06
Iter: 578 loss: 1.2515427e-06
Iter: 579 loss: 1.24834719e-06
Iter: 580 loss: 1.24784106e-06
Iter: 581 loss: 1.24696942e-06
Iter: 582 loss: 1.24693099e-06
Iter: 583 loss: 1.24580492e-06
Iter: 584 loss: 1.24916983e-06
Iter: 585 loss: 1.24557278e-06
Iter: 586 loss: 1.24482676e-06
Iter: 587 loss: 1.2448246e-06
Iter: 588 loss: 1.24420376e-06
Iter: 589 loss: 1.24437463e-06
Iter: 590 loss: 1.24370331e-06
Iter: 591 loss: 1.24290932e-06
Iter: 592 loss: 1.24225267e-06
Iter: 593 loss: 1.24201301e-06
Iter: 594 loss: 1.24111239e-06
Iter: 595 loss: 1.24936582e-06
Iter: 596 loss: 1.24106168e-06
Iter: 597 loss: 1.24019607e-06
Iter: 598 loss: 1.24690632e-06
Iter: 599 loss: 1.24012649e-06
Iter: 600 loss: 1.23975053e-06
Iter: 601 loss: 1.23890936e-06
Iter: 602 loss: 1.25413703e-06
Iter: 603 loss: 1.23891198e-06
Iter: 604 loss: 1.23770042e-06
Iter: 605 loss: 1.23726511e-06
Iter: 606 loss: 1.23655502e-06
Iter: 607 loss: 1.235441e-06
Iter: 608 loss: 1.2434266e-06
Iter: 609 loss: 1.23536825e-06
Iter: 610 loss: 1.23438508e-06
Iter: 611 loss: 1.24126313e-06
Iter: 612 loss: 1.23424797e-06
Iter: 613 loss: 1.23337247e-06
Iter: 614 loss: 1.23787743e-06
Iter: 615 loss: 1.23321297e-06
Iter: 616 loss: 1.23272412e-06
Iter: 617 loss: 1.23423752e-06
Iter: 618 loss: 1.23254813e-06
Iter: 619 loss: 1.23187533e-06
Iter: 620 loss: 1.232062e-06
Iter: 621 loss: 1.23134043e-06
Iter: 622 loss: 1.23063489e-06
Iter: 623 loss: 1.22951803e-06
Iter: 624 loss: 1.2295643e-06
Iter: 625 loss: 1.22847837e-06
Iter: 626 loss: 1.23800578e-06
Iter: 627 loss: 1.22843085e-06
Iter: 628 loss: 1.22740596e-06
Iter: 629 loss: 1.23091445e-06
Iter: 630 loss: 1.22714744e-06
Iter: 631 loss: 1.22603137e-06
Iter: 632 loss: 1.23357165e-06
Iter: 633 loss: 1.22596884e-06
Iter: 634 loss: 1.22536528e-06
Iter: 635 loss: 1.22447852e-06
Iter: 636 loss: 1.22449478e-06
Iter: 637 loss: 1.22445658e-06
Iter: 638 loss: 1.22403094e-06
Iter: 639 loss: 1.22366373e-06
Iter: 640 loss: 1.22301446e-06
Iter: 641 loss: 1.2351469e-06
Iter: 642 loss: 1.22297615e-06
Iter: 643 loss: 1.22222718e-06
Iter: 644 loss: 1.22210827e-06
Iter: 645 loss: 1.22156848e-06
Iter: 646 loss: 1.22088818e-06
Iter: 647 loss: 1.22413064e-06
Iter: 648 loss: 1.22076653e-06
Iter: 649 loss: 1.22002234e-06
Iter: 650 loss: 1.22541564e-06
Iter: 651 loss: 1.21995527e-06
Iter: 652 loss: 1.21946698e-06
Iter: 653 loss: 1.21972334e-06
Iter: 654 loss: 1.21912808e-06
Iter: 655 loss: 1.21845278e-06
Iter: 656 loss: 1.22209076e-06
Iter: 657 loss: 1.21835888e-06
Iter: 658 loss: 1.21782637e-06
Iter: 659 loss: 1.21736116e-06
Iter: 660 loss: 1.21724122e-06
Iter: 661 loss: 1.2163423e-06
Iter: 662 loss: 1.21640244e-06
Iter: 663 loss: 1.21556639e-06
Iter: 664 loss: 1.21446828e-06
Iter: 665 loss: 1.22123026e-06
Iter: 666 loss: 1.21427684e-06
Iter: 667 loss: 1.21340645e-06
Iter: 668 loss: 1.21852281e-06
Iter: 669 loss: 1.21330197e-06
Iter: 670 loss: 1.21251117e-06
Iter: 671 loss: 1.21847961e-06
Iter: 672 loss: 1.2124076e-06
Iter: 673 loss: 1.21195126e-06
Iter: 674 loss: 1.21127982e-06
Iter: 675 loss: 1.21124879e-06
Iter: 676 loss: 1.21103278e-06
Iter: 677 loss: 1.2108078e-06
Iter: 678 loss: 1.21055859e-06
Iter: 679 loss: 1.20986829e-06
Iter: 680 loss: 1.21113544e-06
Iter: 681 loss: 1.20940228e-06
Iter: 682 loss: 1.2085776e-06
Iter: 683 loss: 1.21967491e-06
Iter: 684 loss: 1.20862819e-06
Iter: 685 loss: 1.2081731e-06
Iter: 686 loss: 1.21435733e-06
Iter: 687 loss: 1.20815048e-06
Iter: 688 loss: 1.20780794e-06
Iter: 689 loss: 1.20774871e-06
Iter: 690 loss: 1.20741311e-06
Iter: 691 loss: 1.2068806e-06
Iter: 692 loss: 1.20734717e-06
Iter: 693 loss: 1.20654772e-06
Iter: 694 loss: 1.20571508e-06
Iter: 695 loss: 1.20804771e-06
Iter: 696 loss: 1.20541335e-06
Iter: 697 loss: 1.20483639e-06
Iter: 698 loss: 1.20418599e-06
Iter: 699 loss: 1.20412597e-06
Iter: 700 loss: 1.20318623e-06
Iter: 701 loss: 1.20755897e-06
Iter: 702 loss: 1.20299501e-06
Iter: 703 loss: 1.20210598e-06
Iter: 704 loss: 1.20282562e-06
Iter: 705 loss: 1.20154982e-06
Iter: 706 loss: 1.2009649e-06
Iter: 707 loss: 1.20094637e-06
Iter: 708 loss: 1.20031427e-06
Iter: 709 loss: 1.20185473e-06
Iter: 710 loss: 1.20008406e-06
Iter: 711 loss: 1.19950346e-06
Iter: 712 loss: 1.20042534e-06
Iter: 713 loss: 1.19920173e-06
Iter: 714 loss: 1.19897902e-06
Iter: 715 loss: 1.19894389e-06
Iter: 716 loss: 1.19861647e-06
Iter: 717 loss: 1.19809124e-06
Iter: 718 loss: 1.20608991e-06
Iter: 719 loss: 1.19803497e-06
Iter: 720 loss: 1.19749802e-06
Iter: 721 loss: 1.19829383e-06
Iter: 722 loss: 1.19727349e-06
Iter: 723 loss: 1.19670619e-06
Iter: 724 loss: 1.2039684e-06
Iter: 725 loss: 1.19674905e-06
Iter: 726 loss: 1.19617152e-06
Iter: 727 loss: 1.19651963e-06
Iter: 728 loss: 1.19584104e-06
Iter: 729 loss: 1.19522122e-06
Iter: 730 loss: 1.19559218e-06
Iter: 731 loss: 1.19487777e-06
Iter: 732 loss: 1.19412493e-06
Iter: 733 loss: 1.20059099e-06
Iter: 734 loss: 1.19409356e-06
Iter: 735 loss: 1.19361789e-06
Iter: 736 loss: 1.1924202e-06
Iter: 737 loss: 1.20845573e-06
Iter: 738 loss: 1.19232573e-06
Iter: 739 loss: 1.19135109e-06
Iter: 740 loss: 1.20250797e-06
Iter: 741 loss: 1.19137383e-06
Iter: 742 loss: 1.19078015e-06
Iter: 743 loss: 1.19207607e-06
Iter: 744 loss: 1.1905579e-06
Iter: 745 loss: 1.19010429e-06
Iter: 746 loss: 1.19008519e-06
Iter: 747 loss: 1.18976061e-06
Iter: 748 loss: 1.18978051e-06
Iter: 749 loss: 1.18946764e-06
Iter: 750 loss: 1.18929142e-06
Iter: 751 loss: 1.18930666e-06
Iter: 752 loss: 1.18911169e-06
Iter: 753 loss: 1.18869536e-06
Iter: 754 loss: 1.19212132e-06
Iter: 755 loss: 1.18861294e-06
Iter: 756 loss: 1.18804383e-06
Iter: 757 loss: 1.19013532e-06
Iter: 758 loss: 1.18786807e-06
Iter: 759 loss: 1.18732407e-06
Iter: 760 loss: 1.19133301e-06
Iter: 761 loss: 1.18725825e-06
Iter: 762 loss: 1.18678975e-06
Iter: 763 loss: 1.18648541e-06
Iter: 764 loss: 1.18626986e-06
Iter: 765 loss: 1.18565526e-06
Iter: 766 loss: 1.19029619e-06
Iter: 767 loss: 1.18570063e-06
Iter: 768 loss: 1.18507205e-06
Iter: 769 loss: 1.18518142e-06
Iter: 770 loss: 1.18466915e-06
Iter: 771 loss: 1.18391279e-06
Iter: 772 loss: 1.18363073e-06
Iter: 773 loss: 1.18311391e-06
Iter: 774 loss: 1.18224739e-06
Iter: 775 loss: 1.18595779e-06
Iter: 776 loss: 1.18214678e-06
Iter: 777 loss: 1.18139928e-06
Iter: 778 loss: 1.18463504e-06
Iter: 779 loss: 1.18125502e-06
Iter: 780 loss: 1.18075127e-06
Iter: 781 loss: 1.1807457e-06
Iter: 782 loss: 1.18042169e-06
Iter: 783 loss: 1.18068078e-06
Iter: 784 loss: 1.18020694e-06
Iter: 785 loss: 1.17991351e-06
Iter: 786 loss: 1.18421485e-06
Iter: 787 loss: 1.17991408e-06
Iter: 788 loss: 1.1796692e-06
Iter: 789 loss: 1.17900038e-06
Iter: 790 loss: 1.18514504e-06
Iter: 791 loss: 1.17895127e-06
Iter: 792 loss: 1.17832315e-06
Iter: 793 loss: 1.18062349e-06
Iter: 794 loss: 1.1781583e-06
Iter: 795 loss: 1.17762886e-06
Iter: 796 loss: 1.17767604e-06
Iter: 797 loss: 1.17725017e-06
Iter: 798 loss: 1.17674199e-06
Iter: 799 loss: 1.17671561e-06
Iter: 800 loss: 1.17599e-06
Iter: 801 loss: 1.17750574e-06
Iter: 802 loss: 1.17574825e-06
Iter: 803 loss: 1.17472746e-06
Iter: 804 loss: 1.17858644e-06
Iter: 805 loss: 1.17454181e-06
Iter: 806 loss: 1.17400714e-06
Iter: 807 loss: 1.17342177e-06
Iter: 808 loss: 1.17333627e-06
Iter: 809 loss: 1.17245531e-06
Iter: 810 loss: 1.17544084e-06
Iter: 811 loss: 1.17220782e-06
Iter: 812 loss: 1.17157447e-06
Iter: 813 loss: 1.17599677e-06
Iter: 814 loss: 1.17144168e-06
Iter: 815 loss: 1.17115189e-06
Iter: 816 loss: 1.17114109e-06
Iter: 817 loss: 1.17080572e-06
Iter: 818 loss: 1.17046136e-06
Iter: 819 loss: 1.17044e-06
Iter: 820 loss: 1.16990418e-06
Iter: 821 loss: 1.17579361e-06
Iter: 822 loss: 1.1699201e-06
Iter: 823 loss: 1.16962235e-06
Iter: 824 loss: 1.16925958e-06
Iter: 825 loss: 1.16923491e-06
Iter: 826 loss: 1.16858359e-06
Iter: 827 loss: 1.16809656e-06
Iter: 828 loss: 1.16787839e-06
Iter: 829 loss: 1.16808326e-06
Iter: 830 loss: 1.16752858e-06
Iter: 831 loss: 1.1672314e-06
Iter: 832 loss: 1.16685874e-06
Iter: 833 loss: 1.16686078e-06
Iter: 834 loss: 1.16635488e-06
Iter: 835 loss: 1.16735134e-06
Iter: 836 loss: 1.16618367e-06
Iter: 837 loss: 1.16562683e-06
Iter: 838 loss: 1.17049615e-06
Iter: 839 loss: 1.16561728e-06
Iter: 840 loss: 1.16521846e-06
Iter: 841 loss: 1.1645875e-06
Iter: 842 loss: 1.16462707e-06
Iter: 843 loss: 1.1639537e-06
Iter: 844 loss: 1.16520528e-06
Iter: 845 loss: 1.1637776e-06
Iter: 846 loss: 1.16308252e-06
Iter: 847 loss: 1.16476281e-06
Iter: 848 loss: 1.16286878e-06
Iter: 849 loss: 1.16223111e-06
Iter: 850 loss: 1.17045465e-06
Iter: 851 loss: 1.16222668e-06
Iter: 852 loss: 1.16167632e-06
Iter: 853 loss: 1.16355534e-06
Iter: 854 loss: 1.16154411e-06
Iter: 855 loss: 1.16124102e-06
Iter: 856 loss: 1.16327294e-06
Iter: 857 loss: 1.16117292e-06
Iter: 858 loss: 1.16085721e-06
Iter: 859 loss: 1.16019942e-06
Iter: 860 loss: 1.17208663e-06
Iter: 861 loss: 1.16019783e-06
Iter: 862 loss: 1.15954549e-06
Iter: 863 loss: 1.16016895e-06
Iter: 864 loss: 1.159151e-06
Iter: 865 loss: 1.15867419e-06
Iter: 866 loss: 1.15866806e-06
Iter: 867 loss: 1.15815646e-06
Iter: 868 loss: 1.16023273e-06
Iter: 869 loss: 1.15796399e-06
Iter: 870 loss: 1.15773503e-06
Iter: 871 loss: 1.15728767e-06
Iter: 872 loss: 1.15723333e-06
Iter: 873 loss: 1.15676767e-06
Iter: 874 loss: 1.16056913e-06
Iter: 875 loss: 1.15665921e-06
Iter: 876 loss: 1.15614296e-06
Iter: 877 loss: 1.15660691e-06
Iter: 878 loss: 1.15580974e-06
Iter: 879 loss: 1.15534772e-06
Iter: 880 loss: 1.15608191e-06
Iter: 881 loss: 1.15511011e-06
Iter: 882 loss: 1.15448893e-06
Iter: 883 loss: 1.15403463e-06
Iter: 884 loss: 1.15385501e-06
Iter: 885 loss: 1.15367891e-06
Iter: 886 loss: 1.15340049e-06
Iter: 887 loss: 1.15307353e-06
Iter: 888 loss: 1.15352259e-06
Iter: 889 loss: 1.15288481e-06
Iter: 890 loss: 1.15255727e-06
Iter: 891 loss: 1.15378134e-06
Iter: 892 loss: 1.15244416e-06
Iter: 893 loss: 1.15209605e-06
Iter: 894 loss: 1.15204512e-06
Iter: 895 loss: 1.15177102e-06
Iter: 896 loss: 1.15133639e-06
Iter: 897 loss: 1.15121929e-06
Iter: 898 loss: 1.15094429e-06
Iter: 899 loss: 1.15065359e-06
Iter: 900 loss: 1.15067019e-06
Iter: 901 loss: 1.15030412e-06
Iter: 902 loss: 1.15093087e-06
Iter: 903 loss: 1.15018133e-06
Iter: 904 loss: 1.14975751e-06
Iter: 905 loss: 1.14905197e-06
Iter: 906 loss: 1.16091542e-06
Iter: 907 loss: 1.14898012e-06
Iter: 908 loss: 1.14819272e-06
Iter: 909 loss: 1.15583782e-06
Iter: 910 loss: 1.14814429e-06
Iter: 911 loss: 1.14757245e-06
Iter: 912 loss: 1.15369369e-06
Iter: 913 loss: 1.14754971e-06
Iter: 914 loss: 1.14720467e-06
Iter: 915 loss: 1.1462547e-06
Iter: 916 loss: 1.15830335e-06
Iter: 917 loss: 1.14627755e-06
Iter: 918 loss: 1.14533054e-06
Iter: 919 loss: 1.15078228e-06
Iter: 920 loss: 1.14524642e-06
Iter: 921 loss: 1.14464945e-06
Iter: 922 loss: 1.14776537e-06
Iter: 923 loss: 1.14456179e-06
Iter: 924 loss: 1.14436807e-06
Iter: 925 loss: 1.14432237e-06
Iter: 926 loss: 1.1440593e-06
Iter: 927 loss: 1.14355112e-06
Iter: 928 loss: 1.15176067e-06
Iter: 929 loss: 1.14348586e-06
Iter: 930 loss: 1.14320585e-06
Iter: 931 loss: 1.1431996e-06
Iter: 932 loss: 1.1428408e-06
Iter: 933 loss: 1.14269108e-06
Iter: 934 loss: 1.14256386e-06
Iter: 935 loss: 1.14217937e-06
Iter: 936 loss: 1.142315e-06
Iter: 937 loss: 1.14191789e-06
Iter: 938 loss: 1.14142597e-06
Iter: 939 loss: 1.14792192e-06
Iter: 940 loss: 1.14139993e-06
Iter: 941 loss: 1.14097224e-06
Iter: 942 loss: 1.14030695e-06
Iter: 943 loss: 1.14030422e-06
Iter: 944 loss: 1.13969065e-06
Iter: 945 loss: 1.13929332e-06
Iter: 946 loss: 1.13911699e-06
Iter: 947 loss: 1.13870669e-06
Iter: 948 loss: 1.13853116e-06
Iter: 949 loss: 1.13813667e-06
Iter: 950 loss: 1.13777742e-06
Iter: 951 loss: 1.13768397e-06
Iter: 952 loss: 1.13703663e-06
Iter: 953 loss: 1.1379102e-06
Iter: 954 loss: 1.13671399e-06
Iter: 955 loss: 1.13620479e-06
Iter: 956 loss: 1.13861813e-06
Iter: 957 loss: 1.13604256e-06
Iter: 958 loss: 1.1356999e-06
Iter: 959 loss: 1.1393646e-06
Iter: 960 loss: 1.13570354e-06
Iter: 961 loss: 1.13534691e-06
Iter: 962 loss: 1.13685655e-06
Iter: 963 loss: 1.13526062e-06
Iter: 964 loss: 1.13504325e-06
Iter: 965 loss: 1.13448391e-06
Iter: 966 loss: 1.14426325e-06
Iter: 967 loss: 1.1344589e-06
Iter: 968 loss: 1.1339431e-06
Iter: 969 loss: 1.13979445e-06
Iter: 970 loss: 1.13390911e-06
Iter: 971 loss: 1.13343583e-06
Iter: 972 loss: 1.13479564e-06
Iter: 973 loss: 1.13325962e-06
Iter: 974 loss: 1.13289525e-06
Iter: 975 loss: 1.13329327e-06
Iter: 976 loss: 1.13269527e-06
Iter: 977 loss: 1.13213366e-06
Iter: 978 loss: 1.13449471e-06
Iter: 979 loss: 1.13204362e-06
Iter: 980 loss: 1.13162037e-06
Iter: 981 loss: 1.13124224e-06
Iter: 982 loss: 1.13108308e-06
Iter: 983 loss: 1.13062697e-06
Iter: 984 loss: 1.1324239e-06
Iter: 985 loss: 1.13051578e-06
Iter: 986 loss: 1.13017313e-06
Iter: 987 loss: 1.13533122e-06
Iter: 988 loss: 1.13014835e-06
Iter: 989 loss: 1.12985254e-06
Iter: 990 loss: 1.12950408e-06
Iter: 991 loss: 1.12948351e-06
Iter: 992 loss: 1.12901603e-06
Iter: 993 loss: 1.12976886e-06
Iter: 994 loss: 1.12878502e-06
Iter: 995 loss: 1.12812916e-06
Iter: 996 loss: 1.12828843e-06
Iter: 997 loss: 1.12764155e-06
Iter: 998 loss: 1.12793691e-06
Iter: 999 loss: 1.12736825e-06
Iter: 1000 loss: 1.12711109e-06
Iter: 1001 loss: 1.12664168e-06
Iter: 1002 loss: 1.13584792e-06
Iter: 1003 loss: 1.12663133e-06
Iter: 1004 loss: 1.12625207e-06
Iter: 1005 loss: 1.12693533e-06
Iter: 1006 loss: 1.12606676e-06
Iter: 1007 loss: 1.12562316e-06
Iter: 1008 loss: 1.12844293e-06
Iter: 1009 loss: 1.12556597e-06
Iter: 1010 loss: 1.12513976e-06
Iter: 1011 loss: 1.12602561e-06
Iter: 1012 loss: 1.1249092e-06
Iter: 1013 loss: 1.12450448e-06
Iter: 1014 loss: 1.12471253e-06
Iter: 1015 loss: 1.12426255e-06
Iter: 1016 loss: 1.12396651e-06
Iter: 1017 loss: 1.12392559e-06
Iter: 1018 loss: 1.1236325e-06
Iter: 1019 loss: 1.12315672e-06
Iter: 1020 loss: 1.13294652e-06
Iter: 1021 loss: 1.12320913e-06
Iter: 1022 loss: 1.12255611e-06
Iter: 1023 loss: 1.12356338e-06
Iter: 1024 loss: 1.12228099e-06
Iter: 1025 loss: 1.12173973e-06
Iter: 1026 loss: 1.12176917e-06
Iter: 1027 loss: 1.12134649e-06
Iter: 1028 loss: 1.12091857e-06
Iter: 1029 loss: 1.1208549e-06
Iter: 1030 loss: 1.12040186e-06
Iter: 1031 loss: 1.12239923e-06
Iter: 1032 loss: 1.1202942e-06
Iter: 1033 loss: 1.11998565e-06
Iter: 1034 loss: 1.12229282e-06
Iter: 1035 loss: 1.11996246e-06
Iter: 1036 loss: 1.11958275e-06
Iter: 1037 loss: 1.12173495e-06
Iter: 1038 loss: 1.11957706e-06
Iter: 1039 loss: 1.11934332e-06
Iter: 1040 loss: 1.11896566e-06
Iter: 1041 loss: 1.12493058e-06
Iter: 1042 loss: 1.11892757e-06
Iter: 1043 loss: 1.11846202e-06
Iter: 1044 loss: 1.12125917e-06
Iter: 1045 loss: 1.11838949e-06
Iter: 1046 loss: 1.1179618e-06
Iter: 1047 loss: 1.12150713e-06
Iter: 1048 loss: 1.11796794e-06
Iter: 1049 loss: 1.11756663e-06
Iter: 1050 loss: 1.11710426e-06
Iter: 1051 loss: 1.11703116e-06
Iter: 1052 loss: 1.11659199e-06
Iter: 1053 loss: 1.12383373e-06
Iter: 1054 loss: 1.11659983e-06
Iter: 1055 loss: 1.11608051e-06
Iter: 1056 loss: 1.11577253e-06
Iter: 1057 loss: 1.11557131e-06
Iter: 1058 loss: 1.11512145e-06
Iter: 1059 loss: 1.11593204e-06
Iter: 1060 loss: 1.11485747e-06
Iter: 1061 loss: 1.11445411e-06
Iter: 1062 loss: 1.11790405e-06
Iter: 1063 loss: 1.11442716e-06
Iter: 1064 loss: 1.11402665e-06
Iter: 1065 loss: 1.11522763e-06
Iter: 1066 loss: 1.11394138e-06
Iter: 1067 loss: 1.11357599e-06
Iter: 1068 loss: 1.11328495e-06
Iter: 1069 loss: 1.11320924e-06
Iter: 1070 loss: 1.11290854e-06
Iter: 1071 loss: 1.11706174e-06
Iter: 1072 loss: 1.11288466e-06
Iter: 1073 loss: 1.11263307e-06
Iter: 1074 loss: 1.11268696e-06
Iter: 1075 loss: 1.11250108e-06
Iter: 1076 loss: 1.11211352e-06
Iter: 1077 loss: 1.11411464e-06
Iter: 1078 loss: 1.11195368e-06
Iter: 1079 loss: 1.11134523e-06
Iter: 1080 loss: 1.11411009e-06
Iter: 1081 loss: 1.11125678e-06
Iter: 1082 loss: 1.11088582e-06
Iter: 1083 loss: 1.11087047e-06
Iter: 1084 loss: 1.11047962e-06
Iter: 1085 loss: 1.10994779e-06
Iter: 1086 loss: 1.10993528e-06
Iter: 1087 loss: 1.10934047e-06
Iter: 1088 loss: 1.11495365e-06
Iter: 1089 loss: 1.10935821e-06
Iter: 1090 loss: 1.10896417e-06
Iter: 1091 loss: 1.11217469e-06
Iter: 1092 loss: 1.10893143e-06
Iter: 1093 loss: 1.10870758e-06
Iter: 1094 loss: 1.10824533e-06
Iter: 1095 loss: 1.11520399e-06
Iter: 1096 loss: 1.10819599e-06
Iter: 1097 loss: 1.10778865e-06
Iter: 1098 loss: 1.11288705e-06
Iter: 1099 loss: 1.10778137e-06
Iter: 1100 loss: 1.10743463e-06
Iter: 1101 loss: 1.10921792e-06
Iter: 1102 loss: 1.10741121e-06
Iter: 1103 loss: 1.1071262e-06
Iter: 1104 loss: 1.10696124e-06
Iter: 1105 loss: 1.10678627e-06
Iter: 1106 loss: 1.10642236e-06
Iter: 1107 loss: 1.10626604e-06
Iter: 1108 loss: 1.10604435e-06
Iter: 1109 loss: 1.1055763e-06
Iter: 1110 loss: 1.11154895e-06
Iter: 1111 loss: 1.10562951e-06
Iter: 1112 loss: 1.10521182e-06
Iter: 1113 loss: 1.10877488e-06
Iter: 1114 loss: 1.1052141e-06
Iter: 1115 loss: 1.10498559e-06
Iter: 1116 loss: 1.10451106e-06
Iter: 1117 loss: 1.11292491e-06
Iter: 1118 loss: 1.1044848e-06
Iter: 1119 loss: 1.10398685e-06
Iter: 1120 loss: 1.10546671e-06
Iter: 1121 loss: 1.10386884e-06
Iter: 1122 loss: 1.10353835e-06
Iter: 1123 loss: 1.10354e-06
Iter: 1124 loss: 1.10331723e-06
Iter: 1125 loss: 1.10305359e-06
Iter: 1126 loss: 1.1030404e-06
Iter: 1127 loss: 1.10283952e-06
Iter: 1128 loss: 1.10279689e-06
Iter: 1129 loss: 1.10254064e-06
Iter: 1130 loss: 1.10243377e-06
Iter: 1131 loss: 1.10234851e-06
Iter: 1132 loss: 1.1020561e-06
Iter: 1133 loss: 1.10201211e-06
Iter: 1134 loss: 1.10184567e-06
Iter: 1135 loss: 1.10142958e-06
Iter: 1136 loss: 1.10166616e-06
Iter: 1137 loss: 1.10111807e-06
Iter: 1138 loss: 1.10071505e-06
Iter: 1139 loss: 1.10499991e-06
Iter: 1140 loss: 1.10070641e-06
Iter: 1141 loss: 1.10036342e-06
Iter: 1142 loss: 1.10230792e-06
Iter: 1143 loss: 1.10037786e-06
Iter: 1144 loss: 1.10002384e-06
Iter: 1145 loss: 1.0998283e-06
Iter: 1146 loss: 1.09963526e-06
Iter: 1147 loss: 1.09935695e-06
Iter: 1148 loss: 1.09929806e-06
Iter: 1149 loss: 1.09901646e-06
Iter: 1150 loss: 1.09885718e-06
Iter: 1151 loss: 1.09873224e-06
Iter: 1152 loss: 1.0984038e-06
Iter: 1153 loss: 1.098648e-06
Iter: 1154 loss: 1.09824441e-06
Iter: 1155 loss: 1.09774612e-06
Iter: 1156 loss: 1.09743166e-06
Iter: 1157 loss: 1.0973281e-06
Iter: 1158 loss: 1.09733458e-06
Iter: 1159 loss: 1.09696543e-06
Iter: 1160 loss: 1.09684629e-06
Iter: 1161 loss: 1.09659265e-06
Iter: 1162 loss: 1.09658345e-06
Iter: 1163 loss: 1.09627285e-06
Iter: 1164 loss: 1.10031669e-06
Iter: 1165 loss: 1.0962433e-06
Iter: 1166 loss: 1.09607208e-06
Iter: 1167 loss: 1.09572545e-06
Iter: 1168 loss: 1.09572738e-06
Iter: 1169 loss: 1.09533335e-06
Iter: 1170 loss: 1.09571613e-06
Iter: 1171 loss: 1.09509642e-06
Iter: 1172 loss: 1.09458574e-06
Iter: 1173 loss: 1.09556561e-06
Iter: 1174 loss: 1.09444e-06
Iter: 1175 loss: 1.09394114e-06
Iter: 1176 loss: 1.09570681e-06
Iter: 1177 loss: 1.093793e-06
Iter: 1178 loss: 1.09344046e-06
Iter: 1179 loss: 1.09947757e-06
Iter: 1180 loss: 1.09344023e-06
Iter: 1181 loss: 1.09314396e-06
Iter: 1182 loss: 1.09333814e-06
Iter: 1183 loss: 1.09293524e-06
Iter: 1184 loss: 1.09271286e-06
Iter: 1185 loss: 1.09651887e-06
Iter: 1186 loss: 1.09271207e-06
Iter: 1187 loss: 1.09246753e-06
Iter: 1188 loss: 1.09219206e-06
Iter: 1189 loss: 1.09219229e-06
Iter: 1190 loss: 1.09181008e-06
Iter: 1191 loss: 1.09174448e-06
Iter: 1192 loss: 1.09144742e-06
Iter: 1193 loss: 1.09093685e-06
Iter: 1194 loss: 1.09244957e-06
Iter: 1195 loss: 1.09078633e-06
Iter: 1196 loss: 1.09048767e-06
Iter: 1197 loss: 1.0904896e-06
Iter: 1198 loss: 1.090193e-06
Iter: 1199 loss: 1.09032112e-06
Iter: 1200 loss: 1.09000814e-06
Iter: 1201 loss: 1.08981465e-06
Iter: 1202 loss: 1.09149516e-06
Iter: 1203 loss: 1.08981226e-06
Iter: 1204 loss: 1.08955953e-06
Iter: 1205 loss: 1.08917095e-06
Iter: 1206 loss: 1.09912e-06
Iter: 1207 loss: 1.08920494e-06
Iter: 1208 loss: 1.08875975e-06
Iter: 1209 loss: 1.09013865e-06
Iter: 1210 loss: 1.08857034e-06
Iter: 1211 loss: 1.08819813e-06
Iter: 1212 loss: 1.08807319e-06
Iter: 1213 loss: 1.08787344e-06
Iter: 1214 loss: 1.08747327e-06
Iter: 1215 loss: 1.08745633e-06
Iter: 1216 loss: 1.08720371e-06
Iter: 1217 loss: 1.09003827e-06
Iter: 1218 loss: 1.08716165e-06
Iter: 1219 loss: 1.08699624e-06
Iter: 1220 loss: 1.08723634e-06
Iter: 1221 loss: 1.08691188e-06
Iter: 1222 loss: 1.08664483e-06
Iter: 1223 loss: 1.0864793e-06
Iter: 1224 loss: 1.08630968e-06
Iter: 1225 loss: 1.08606287e-06
Iter: 1226 loss: 1.08628933e-06
Iter: 1227 loss: 1.08584e-06
Iter: 1228 loss: 1.08547977e-06
Iter: 1229 loss: 1.08552842e-06
Iter: 1230 loss: 1.08519851e-06
Iter: 1231 loss: 1.08495419e-06
Iter: 1232 loss: 1.08494726e-06
Iter: 1233 loss: 1.08460881e-06
Iter: 1234 loss: 1.08448535e-06
Iter: 1235 loss: 1.08434892e-06
Iter: 1236 loss: 1.08399945e-06
Iter: 1237 loss: 1.08656729e-06
Iter: 1238 loss: 1.08402605e-06
Iter: 1239 loss: 1.08376162e-06
Iter: 1240 loss: 1.08413667e-06
Iter: 1241 loss: 1.08359552e-06
Iter: 1242 loss: 1.08334268e-06
Iter: 1243 loss: 1.0833221e-06
Iter: 1244 loss: 1.08316885e-06
Iter: 1245 loss: 1.08284098e-06
Iter: 1246 loss: 1.08307495e-06
Iter: 1247 loss: 1.08264976e-06
Iter: 1248 loss: 1.0821642e-06
Iter: 1249 loss: 1.0835123e-06
Iter: 1250 loss: 1.08203415e-06
Iter: 1251 loss: 1.08183247e-06
Iter: 1252 loss: 1.0818585e-06
Iter: 1253 loss: 1.08158588e-06
Iter: 1254 loss: 1.08145173e-06
Iter: 1255 loss: 1.08139409e-06
Iter: 1256 loss: 1.08114557e-06
Iter: 1257 loss: 1.08339441e-06
Iter: 1258 loss: 1.08111271e-06
Iter: 1259 loss: 1.08088989e-06
Iter: 1260 loss: 1.08042582e-06
Iter: 1261 loss: 1.08806262e-06
Iter: 1262 loss: 1.08045651e-06
Iter: 1263 loss: 1.07995106e-06
Iter: 1264 loss: 1.08255267e-06
Iter: 1265 loss: 1.0798683e-06
Iter: 1266 loss: 1.07949575e-06
Iter: 1267 loss: 1.07971721e-06
Iter: 1268 loss: 1.07922699e-06
Iter: 1269 loss: 1.07896381e-06
Iter: 1270 loss: 1.07891333e-06
Iter: 1271 loss: 1.07871961e-06
Iter: 1272 loss: 1.07845256e-06
Iter: 1273 loss: 1.07844528e-06
Iter: 1274 loss: 1.07823212e-06
Iter: 1275 loss: 1.0782353e-06
Iter: 1276 loss: 1.07803965e-06
Iter: 1277 loss: 1.07779954e-06
Iter: 1278 loss: 1.07774065e-06
Iter: 1279 loss: 1.07745245e-06
Iter: 1280 loss: 1.07785536e-06
Iter: 1281 loss: 1.07727772e-06
Iter: 1282 loss: 1.07698042e-06
Iter: 1283 loss: 1.07710127e-06
Iter: 1284 loss: 1.07677442e-06
Iter: 1285 loss: 1.07646179e-06
Iter: 1286 loss: 1.07649953e-06
Iter: 1287 loss: 1.07615597e-06
Iter: 1288 loss: 1.07665267e-06
Iter: 1289 loss: 1.0759851e-06
Iter: 1290 loss: 1.07567416e-06
Iter: 1291 loss: 1.07609196e-06
Iter: 1292 loss: 1.07555923e-06
Iter: 1293 loss: 1.07513915e-06
Iter: 1294 loss: 1.07577603e-06
Iter: 1295 loss: 1.07500284e-06
Iter: 1296 loss: 1.07469759e-06
Iter: 1297 loss: 1.07425876e-06
Iter: 1298 loss: 1.07425581e-06
Iter: 1299 loss: 1.07355982e-06
Iter: 1300 loss: 1.07668461e-06
Iter: 1301 loss: 1.07353208e-06
Iter: 1302 loss: 1.07343499e-06
Iter: 1303 loss: 1.0733487e-06
Iter: 1304 loss: 1.07316248e-06
Iter: 1305 loss: 1.0733213e-06
Iter: 1306 loss: 1.073009e-06
Iter: 1307 loss: 1.07286155e-06
Iter: 1308 loss: 1.07254061e-06
Iter: 1309 loss: 1.07255073e-06
Iter: 1310 loss: 1.07220342e-06
Iter: 1311 loss: 1.07224037e-06
Iter: 1312 loss: 1.07203311e-06
Iter: 1313 loss: 1.07173094e-06
Iter: 1314 loss: 1.07170104e-06
Iter: 1315 loss: 1.07137748e-06
Iter: 1316 loss: 1.07190749e-06
Iter: 1317 loss: 1.07117489e-06
Iter: 1318 loss: 1.07066023e-06
Iter: 1319 loss: 1.07162111e-06
Iter: 1320 loss: 1.07041524e-06
Iter: 1321 loss: 1.07028711e-06
Iter: 1322 loss: 1.07015785e-06
Iter: 1323 loss: 1.07000551e-06
Iter: 1324 loss: 1.06970538e-06
Iter: 1325 loss: 1.06968128e-06
Iter: 1326 loss: 1.0692645e-06
Iter: 1327 loss: 1.07151459e-06
Iter: 1328 loss: 1.06923721e-06
Iter: 1329 loss: 1.06898324e-06
Iter: 1330 loss: 1.06885523e-06
Iter: 1331 loss: 1.06873745e-06
Iter: 1332 loss: 1.06845675e-06
Iter: 1333 loss: 1.06939342e-06
Iter: 1334 loss: 1.06841946e-06
Iter: 1335 loss: 1.06816651e-06
Iter: 1336 loss: 1.06850041e-06
Iter: 1337 loss: 1.06807124e-06
Iter: 1338 loss: 1.06778543e-06
Iter: 1339 loss: 1.06806226e-06
Iter: 1340 loss: 1.0675958e-06
Iter: 1341 loss: 1.06732705e-06
Iter: 1342 loss: 1.06782636e-06
Iter: 1343 loss: 1.06720984e-06
Iter: 1344 loss: 1.06696245e-06
Iter: 1345 loss: 1.06948198e-06
Iter: 1346 loss: 1.06691482e-06
Iter: 1347 loss: 1.06674315e-06
Iter: 1348 loss: 1.06675031e-06
Iter: 1349 loss: 1.06648304e-06
Iter: 1350 loss: 1.06615619e-06
Iter: 1351 loss: 1.06586754e-06
Iter: 1352 loss: 1.06578011e-06
Iter: 1353 loss: 1.06528353e-06
Iter: 1354 loss: 1.06790412e-06
Iter: 1355 loss: 1.06524931e-06
Iter: 1356 loss: 1.06478296e-06
Iter: 1357 loss: 1.06599009e-06
Iter: 1358 loss: 1.06464699e-06
Iter: 1359 loss: 1.06435209e-06
Iter: 1360 loss: 1.06435914e-06
Iter: 1361 loss: 1.06406935e-06
Iter: 1362 loss: 1.06371715e-06
Iter: 1363 loss: 1.06368577e-06
Iter: 1364 loss: 1.06343032e-06
Iter: 1365 loss: 1.06340212e-06
Iter: 1366 loss: 1.06312564e-06
Iter: 1367 loss: 1.06298955e-06
Iter: 1368 loss: 1.06292487e-06
Iter: 1369 loss: 1.06258085e-06
Iter: 1370 loss: 1.06366326e-06
Iter: 1371 loss: 1.06251719e-06
Iter: 1372 loss: 1.06221489e-06
Iter: 1373 loss: 1.06439097e-06
Iter: 1374 loss: 1.06218567e-06
Iter: 1375 loss: 1.06199707e-06
Iter: 1376 loss: 1.0616784e-06
Iter: 1377 loss: 1.06170614e-06
Iter: 1378 loss: 1.06123753e-06
Iter: 1379 loss: 1.06213395e-06
Iter: 1380 loss: 1.06103107e-06
Iter: 1381 loss: 1.06069228e-06
Iter: 1382 loss: 1.06287303e-06
Iter: 1383 loss: 1.06061793e-06
Iter: 1384 loss: 1.06031689e-06
Iter: 1385 loss: 1.0622241e-06
Iter: 1386 loss: 1.06028119e-06
Iter: 1387 loss: 1.06001107e-06
Iter: 1388 loss: 1.05965444e-06
Iter: 1389 loss: 1.0596774e-06
Iter: 1390 loss: 1.05927961e-06
Iter: 1391 loss: 1.0623163e-06
Iter: 1392 loss: 1.05921345e-06
Iter: 1393 loss: 1.05895344e-06
Iter: 1394 loss: 1.06024049e-06
Iter: 1395 loss: 1.05890888e-06
Iter: 1396 loss: 1.05854633e-06
Iter: 1397 loss: 1.05921094e-06
Iter: 1398 loss: 1.05838865e-06
Iter: 1399 loss: 1.05811978e-06
Iter: 1400 loss: 1.05908566e-06
Iter: 1401 loss: 1.05806339e-06
Iter: 1402 loss: 1.05772779e-06
Iter: 1403 loss: 1.05802e-06
Iter: 1404 loss: 1.05752474e-06
Iter: 1405 loss: 1.0571822e-06
Iter: 1406 loss: 1.05775143e-06
Iter: 1407 loss: 1.05707477e-06
Iter: 1408 loss: 1.05677941e-06
Iter: 1409 loss: 1.05996583e-06
Iter: 1410 loss: 1.05676622e-06
Iter: 1411 loss: 1.05658478e-06
Iter: 1412 loss: 1.05619074e-06
Iter: 1413 loss: 1.05620268e-06
Iter: 1414 loss: 1.05586594e-06
Iter: 1415 loss: 1.05707932e-06
Iter: 1416 loss: 1.05579477e-06
Iter: 1417 loss: 1.05553477e-06
Iter: 1418 loss: 1.05615e-06
Iter: 1419 loss: 1.05546303e-06
Iter: 1420 loss: 1.05530353e-06
Iter: 1421 loss: 1.05524873e-06
Iter: 1422 loss: 1.05511845e-06
Iter: 1423 loss: 1.05482195e-06
Iter: 1424 loss: 1.05940171e-06
Iter: 1425 loss: 1.0547717e-06
Iter: 1426 loss: 1.05446702e-06
Iter: 1427 loss: 1.05515301e-06
Iter: 1428 loss: 1.05432673e-06
Iter: 1429 loss: 1.05399749e-06
Iter: 1430 loss: 1.05563527e-06
Iter: 1431 loss: 1.05394383e-06
Iter: 1432 loss: 1.05358686e-06
Iter: 1433 loss: 1.05562742e-06
Iter: 1434 loss: 1.05354047e-06
Iter: 1435 loss: 1.05318531e-06
Iter: 1436 loss: 1.05359175e-06
Iter: 1437 loss: 1.0530232e-06
Iter: 1438 loss: 1.05260278e-06
Iter: 1439 loss: 1.05549452e-06
Iter: 1440 loss: 1.0525755e-06
Iter: 1441 loss: 1.05244408e-06
Iter: 1442 loss: 1.05206323e-06
Iter: 1443 loss: 1.05577737e-06
Iter: 1444 loss: 1.05204106e-06
Iter: 1445 loss: 1.05188656e-06
Iter: 1446 loss: 1.05182426e-06
Iter: 1447 loss: 1.05161143e-06
Iter: 1448 loss: 1.05166578e-06
Iter: 1449 loss: 1.05147842e-06
Iter: 1450 loss: 1.05129288e-06
Iter: 1451 loss: 1.05129755e-06
Iter: 1452 loss: 1.05115885e-06
Iter: 1453 loss: 1.05091272e-06
Iter: 1454 loss: 1.05144579e-06
Iter: 1455 loss: 1.05083041e-06
Iter: 1456 loss: 1.05047047e-06
Iter: 1457 loss: 1.05150389e-06
Iter: 1458 loss: 1.0503768e-06
Iter: 1459 loss: 1.05011554e-06
Iter: 1460 loss: 1.05001413e-06
Iter: 1461 loss: 1.04984758e-06
Iter: 1462 loss: 1.04941569e-06
Iter: 1463 loss: 1.05072343e-06
Iter: 1464 loss: 1.04924925e-06
Iter: 1465 loss: 1.04900528e-06
Iter: 1466 loss: 1.0490146e-06
Iter: 1467 loss: 1.04876108e-06
Iter: 1468 loss: 1.0489631e-06
Iter: 1469 loss: 1.04863011e-06
Iter: 1470 loss: 1.0484157e-06
Iter: 1471 loss: 1.04839364e-06
Iter: 1472 loss: 1.04827836e-06
Iter: 1473 loss: 1.04799426e-06
Iter: 1474 loss: 1.04920082e-06
Iter: 1475 loss: 1.04787591e-06
Iter: 1476 loss: 1.04766048e-06
Iter: 1477 loss: 1.04762989e-06
Iter: 1478 loss: 1.04742799e-06
Iter: 1479 loss: 1.04913852e-06
Iter: 1480 loss: 1.04739922e-06
Iter: 1481 loss: 1.04725405e-06
Iter: 1482 loss: 1.04705157e-06
Iter: 1483 loss: 1.04702372e-06
Iter: 1484 loss: 1.0468043e-06
Iter: 1485 loss: 1.04649951e-06
Iter: 1486 loss: 1.04643141e-06
Iter: 1487 loss: 1.04618744e-06
Iter: 1488 loss: 1.04614401e-06
Iter: 1489 loss: 1.04590174e-06
Iter: 1490 loss: 1.045609e-06
Iter: 1491 loss: 1.04556284e-06
Iter: 1492 loss: 1.04516266e-06
Iter: 1493 loss: 1.04525009e-06
Iter: 1494 loss: 1.04483649e-06
Iter: 1495 loss: 1.04438493e-06
Iter: 1496 loss: 1.04901358e-06
Iter: 1497 loss: 1.04441381e-06
Iter: 1498 loss: 1.04408196e-06
Iter: 1499 loss: 1.04676587e-06
Iter: 1500 loss: 1.04407457e-06
Iter: 1501 loss: 1.04388846e-06
Iter: 1502 loss: 1.04513231e-06
Iter: 1503 loss: 1.04388459e-06
Iter: 1504 loss: 1.04371475e-06
Iter: 1505 loss: 1.04341439e-06
Iter: 1506 loss: 1.04799267e-06
Iter: 1507 loss: 1.04338039e-06
Iter: 1508 loss: 1.04310288e-06
Iter: 1509 loss: 1.04501248e-06
Iter: 1510 loss: 1.04308742e-06
Iter: 1511 loss: 1.04286664e-06
Iter: 1512 loss: 1.04551054e-06
Iter: 1513 loss: 1.04288176e-06
Iter: 1514 loss: 1.04269361e-06
Iter: 1515 loss: 1.04246078e-06
Iter: 1516 loss: 1.04241633e-06
Iter: 1517 loss: 1.04215883e-06
Iter: 1518 loss: 1.04205708e-06
Iter: 1519 loss: 1.04193305e-06
Iter: 1520 loss: 1.04160324e-06
Iter: 1521 loss: 1.0415863e-06
Iter: 1522 loss: 1.04131277e-06
Iter: 1523 loss: 1.04174978e-06
Iter: 1524 loss: 1.04119022e-06
Iter: 1525 loss: 1.04094465e-06
Iter: 1526 loss: 1.04078492e-06
Iter: 1527 loss: 1.0406909e-06
Iter: 1528 loss: 1.04036462e-06
Iter: 1529 loss: 1.04162802e-06
Iter: 1530 loss: 1.04028823e-06
Iter: 1531 loss: 1.0400571e-06
Iter: 1532 loss: 1.04007108e-06
Iter: 1533 loss: 1.03984735e-06
Iter: 1534 loss: 1.04017204e-06
Iter: 1535 loss: 1.03976515e-06
Iter: 1536 loss: 1.03953539e-06
Iter: 1537 loss: 1.0399981e-06
Iter: 1538 loss: 1.03948537e-06
Iter: 1539 loss: 1.03922298e-06
Iter: 1540 loss: 1.03904472e-06
Iter: 1541 loss: 1.03904335e-06
Iter: 1542 loss: 1.03870775e-06
Iter: 1543 loss: 1.03949685e-06
Iter: 1544 loss: 1.03864795e-06
Iter: 1545 loss: 1.03827779e-06
Iter: 1546 loss: 1.04053845e-06
Iter: 1547 loss: 1.03821787e-06
Iter: 1548 loss: 1.03802017e-06
Iter: 1549 loss: 1.0376184e-06
Iter: 1550 loss: 1.04290757e-06
Iter: 1551 loss: 1.03758316e-06
Iter: 1552 loss: 1.03715638e-06
Iter: 1553 loss: 1.04191588e-06
Iter: 1554 loss: 1.03713876e-06
Iter: 1555 loss: 1.03681464e-06
Iter: 1556 loss: 1.03962827e-06
Iter: 1557 loss: 1.03682191e-06
Iter: 1558 loss: 1.03659409e-06
Iter: 1559 loss: 1.03639184e-06
Iter: 1560 loss: 1.03631498e-06
Iter: 1561 loss: 1.03609727e-06
Iter: 1562 loss: 1.03646983e-06
Iter: 1563 loss: 1.03593504e-06
Iter: 1564 loss: 1.03564491e-06
Iter: 1565 loss: 1.0387356e-06
Iter: 1566 loss: 1.03565856e-06
Iter: 1567 loss: 1.03543448e-06
Iter: 1568 loss: 1.03672312e-06
Iter: 1569 loss: 1.03535763e-06
Iter: 1570 loss: 1.03528328e-06
Iter: 1571 loss: 1.0357121e-06
Iter: 1572 loss: 1.03523439e-06
Iter: 1573 loss: 1.03502759e-06
Iter: 1574 loss: 1.03487878e-06
Iter: 1575 loss: 1.03480431e-06
Iter: 1576 loss: 1.0345052e-06
Iter: 1577 loss: 1.03436878e-06
Iter: 1578 loss: 1.03426942e-06
Iter: 1579 loss: 1.03405932e-06
Iter: 1580 loss: 1.03400271e-06
Iter: 1581 loss: 1.03371383e-06
Iter: 1582 loss: 1.03346542e-06
Iter: 1583 loss: 1.03342677e-06
Iter: 1584 loss: 1.03299783e-06
Iter: 1585 loss: 1.03299033e-06
Iter: 1586 loss: 1.0327492e-06
Iter: 1587 loss: 1.03261095e-06
Iter: 1588 loss: 1.03250875e-06
Iter: 1589 loss: 1.03232151e-06
Iter: 1590 loss: 1.03248385e-06
Iter: 1591 loss: 1.03220077e-06
Iter: 1592 loss: 1.03201751e-06
Iter: 1593 loss: 1.03207117e-06
Iter: 1594 loss: 1.03190507e-06
Iter: 1595 loss: 1.03167429e-06
Iter: 1596 loss: 1.03201683e-06
Iter: 1597 loss: 1.03151729e-06
Iter: 1598 loss: 1.03144953e-06
Iter: 1599 loss: 1.03141303e-06
Iter: 1600 loss: 1.03134903e-06
Iter: 1601 loss: 1.03112257e-06
Iter: 1602 loss: 1.03115599e-06
Iter: 1603 loss: 1.03089042e-06
Iter: 1604 loss: 1.03143714e-06
Iter: 1605 loss: 1.03074854e-06
Iter: 1606 loss: 1.03051434e-06
Iter: 1607 loss: 1.03044067e-06
Iter: 1608 loss: 1.03026809e-06
Iter: 1609 loss: 1.03002992e-06
Iter: 1610 loss: 1.03281479e-06
Iter: 1611 loss: 1.02998899e-06
Iter: 1612 loss: 1.02974479e-06
Iter: 1613 loss: 1.03020921e-06
Iter: 1614 loss: 1.0297133e-06
Iter: 1615 loss: 1.02943818e-06
Iter: 1616 loss: 1.02899241e-06
Iter: 1617 loss: 1.03839113e-06
Iter: 1618 loss: 1.0289566e-06
Iter: 1619 loss: 1.02867341e-06
Iter: 1620 loss: 1.02866079e-06
Iter: 1621 loss: 1.02846161e-06
Iter: 1622 loss: 1.02976276e-06
Iter: 1623 loss: 1.02844297e-06
Iter: 1624 loss: 1.02820843e-06
Iter: 1625 loss: 1.02805893e-06
Iter: 1626 loss: 1.02803619e-06
Iter: 1627 loss: 1.0277513e-06
Iter: 1628 loss: 1.02778915e-06
Iter: 1629 loss: 1.0275304e-06
Iter: 1630 loss: 1.02741899e-06
Iter: 1631 loss: 1.02739625e-06
Iter: 1632 loss: 1.02719673e-06
Iter: 1633 loss: 1.02724925e-06
Iter: 1634 loss: 1.02707577e-06
Iter: 1635 loss: 1.02692752e-06
Iter: 1636 loss: 1.02763738e-06
Iter: 1637 loss: 1.02680178e-06
Iter: 1638 loss: 1.02661727e-06
Iter: 1639 loss: 1.02661033e-06
Iter: 1640 loss: 1.02648858e-06
Iter: 1641 loss: 1.02625984e-06
Iter: 1642 loss: 1.02629247e-06
Iter: 1643 loss: 1.02604974e-06
Iter: 1644 loss: 1.02582771e-06
Iter: 1645 loss: 1.02581771e-06
Iter: 1646 loss: 1.02567594e-06
Iter: 1647 loss: 1.02530794e-06
Iter: 1648 loss: 1.02895081e-06
Iter: 1649 loss: 1.02524234e-06
Iter: 1650 loss: 1.02490662e-06
Iter: 1651 loss: 1.0285994e-06
Iter: 1652 loss: 1.02490912e-06
Iter: 1653 loss: 1.02469414e-06
Iter: 1654 loss: 1.02661102e-06
Iter: 1655 loss: 1.02464185e-06
Iter: 1656 loss: 1.02447837e-06
Iter: 1657 loss: 1.02441595e-06
Iter: 1658 loss: 1.02428783e-06
Iter: 1659 loss: 1.02409786e-06
Iter: 1660 loss: 1.02398576e-06
Iter: 1661 loss: 1.02390902e-06
Iter: 1662 loss: 1.0236522e-06
Iter: 1663 loss: 1.02682964e-06
Iter: 1664 loss: 1.02362742e-06
Iter: 1665 loss: 1.02342483e-06
Iter: 1666 loss: 1.02450781e-06
Iter: 1667 loss: 1.02337913e-06
Iter: 1668 loss: 1.02323145e-06
Iter: 1669 loss: 1.02341244e-06
Iter: 1670 loss: 1.02315403e-06
Iter: 1671 loss: 1.02288618e-06
Iter: 1672 loss: 1.02314334e-06
Iter: 1673 loss: 1.02277306e-06
Iter: 1674 loss: 1.02248021e-06
Iter: 1675 loss: 1.02218814e-06
Iter: 1676 loss: 1.02219326e-06
Iter: 1677 loss: 1.02184129e-06
Iter: 1678 loss: 1.02183094e-06
Iter: 1679 loss: 1.02156969e-06
Iter: 1680 loss: 1.02172032e-06
Iter: 1681 loss: 1.02139074e-06
Iter: 1682 loss: 1.02118804e-06
Iter: 1683 loss: 1.02098556e-06
Iter: 1684 loss: 1.02092326e-06
Iter: 1685 loss: 1.02064769e-06
Iter: 1686 loss: 1.02321837e-06
Iter: 1687 loss: 1.0206403e-06
Iter: 1688 loss: 1.02043646e-06
Iter: 1689 loss: 1.02198396e-06
Iter: 1690 loss: 1.02039542e-06
Iter: 1691 loss: 1.02015179e-06
Iter: 1692 loss: 1.02014269e-06
Iter: 1693 loss: 1.02004992e-06
Iter: 1694 loss: 1.01979026e-06
Iter: 1695 loss: 1.01983994e-06
Iter: 1696 loss: 1.01964906e-06
Iter: 1697 loss: 1.01955175e-06
Iter: 1698 loss: 1.01946807e-06
Iter: 1699 loss: 1.01939304e-06
Iter: 1700 loss: 1.01915953e-06
Iter: 1701 loss: 1.01914884e-06
Iter: 1702 loss: 1.01885473e-06
Iter: 1703 loss: 1.01960882e-06
Iter: 1704 loss: 1.01876128e-06
Iter: 1705 loss: 1.01845296e-06
Iter: 1706 loss: 1.01877015e-06
Iter: 1707 loss: 1.01834257e-06
Iter: 1708 loss: 1.01804642e-06
Iter: 1709 loss: 1.01923149e-06
Iter: 1710 loss: 1.01799321e-06
Iter: 1711 loss: 1.0177489e-06
Iter: 1712 loss: 1.01909779e-06
Iter: 1713 loss: 1.01769751e-06
Iter: 1714 loss: 1.01758633e-06
Iter: 1715 loss: 1.01729665e-06
Iter: 1716 loss: 1.02266551e-06
Iter: 1717 loss: 1.01723867e-06
Iter: 1718 loss: 1.01688397e-06
Iter: 1719 loss: 1.01849264e-06
Iter: 1720 loss: 1.0168867e-06
Iter: 1721 loss: 1.01670378e-06
Iter: 1722 loss: 1.01669752e-06
Iter: 1723 loss: 1.01649903e-06
Iter: 1724 loss: 1.01634942e-06
Iter: 1725 loss: 1.01629325e-06
Iter: 1726 loss: 1.01607645e-06
Iter: 1727 loss: 1.01616649e-06
Iter: 1728 loss: 1.01591274e-06
Iter: 1729 loss: 1.01570799e-06
Iter: 1730 loss: 1.01574506e-06
Iter: 1731 loss: 1.01556179e-06
Iter: 1732 loss: 1.01566968e-06
Iter: 1733 loss: 1.0154065e-06
Iter: 1734 loss: 1.01524165e-06
Iter: 1735 loss: 1.01584396e-06
Iter: 1736 loss: 1.01515184e-06
Iter: 1737 loss: 1.01496255e-06
Iter: 1738 loss: 1.01497255e-06
Iter: 1739 loss: 1.01474905e-06
Iter: 1740 loss: 1.0146357e-06
Iter: 1741 loss: 1.01532578e-06
Iter: 1742 loss: 1.01461046e-06
Iter: 1743 loss: 1.01443607e-06
Iter: 1744 loss: 1.01554599e-06
Iter: 1745 loss: 1.01443277e-06
Iter: 1746 loss: 1.01424689e-06
Iter: 1747 loss: 1.01399974e-06
Iter: 1748 loss: 1.0140011e-06
Iter: 1749 loss: 1.01369233e-06
Iter: 1750 loss: 1.01374144e-06
Iter: 1751 loss: 1.01349315e-06
Iter: 1752 loss: 1.01327237e-06
Iter: 1753 loss: 1.01327225e-06
Iter: 1754 loss: 1.01301828e-06
Iter: 1755 loss: 1.01373143e-06
Iter: 1756 loss: 1.01288902e-06
Iter: 1757 loss: 1.01265891e-06
Iter: 1758 loss: 1.01248372e-06
Iter: 1759 loss: 1.01243313e-06
Iter: 1760 loss: 1.01208877e-06
Iter: 1761 loss: 1.01355704e-06
Iter: 1762 loss: 1.01201113e-06
Iter: 1763 loss: 1.01177477e-06
Iter: 1764 loss: 1.01172384e-06
Iter: 1765 loss: 1.01164846e-06
Iter: 1766 loss: 1.01149112e-06
Iter: 1767 loss: 1.01148066e-06
Iter: 1768 loss: 1.01123669e-06
Iter: 1769 loss: 1.01218484e-06
Iter: 1770 loss: 1.01119122e-06
Iter: 1771 loss: 1.0110117e-06
Iter: 1772 loss: 1.01101591e-06
Iter: 1773 loss: 1.01087505e-06
Iter: 1774 loss: 1.01070452e-06
Iter: 1775 loss: 1.01209184e-06
Iter: 1776 loss: 1.01071635e-06
Iter: 1777 loss: 1.0104693e-06
Iter: 1778 loss: 1.0106462e-06
Iter: 1779 loss: 1.01032947e-06
Iter: 1780 loss: 1.01012733e-06
Iter: 1781 loss: 1.00982493e-06
Iter: 1782 loss: 1.0098338e-06
Iter: 1783 loss: 1.00944158e-06
Iter: 1784 loss: 1.01151966e-06
Iter: 1785 loss: 1.00938314e-06
Iter: 1786 loss: 1.00910688e-06
Iter: 1787 loss: 1.01250976e-06
Iter: 1788 loss: 1.00913894e-06
Iter: 1789 loss: 1.00885393e-06
Iter: 1790 loss: 1.00867305e-06
Iter: 1791 loss: 1.00863883e-06
Iter: 1792 loss: 1.00832028e-06
Iter: 1793 loss: 1.00817761e-06
Iter: 1794 loss: 1.0080015e-06
Iter: 1795 loss: 1.00778709e-06
Iter: 1796 loss: 1.00768261e-06
Iter: 1797 loss: 1.00751527e-06
Iter: 1798 loss: 1.00763202e-06
Iter: 1799 loss: 1.00740294e-06
Iter: 1800 loss: 1.00721718e-06
Iter: 1801 loss: 1.00796478e-06
Iter: 1802 loss: 1.00721684e-06
Iter: 1803 loss: 1.00700208e-06
Iter: 1804 loss: 1.00695149e-06
Iter: 1805 loss: 1.00690272e-06
Iter: 1806 loss: 1.00667853e-06
Iter: 1807 loss: 1.0067431e-06
Iter: 1808 loss: 1.00656348e-06
Iter: 1809 loss: 1.00636646e-06
Iter: 1810 loss: 1.00638567e-06
Iter: 1811 loss: 1.00621014e-06
Iter: 1812 loss: 1.00598857e-06
Iter: 1813 loss: 1.00598106e-06
Iter: 1814 loss: 1.00572652e-06
Iter: 1815 loss: 1.00566513e-06
Iter: 1816 loss: 1.00548323e-06
Iter: 1817 loss: 1.00528564e-06
Iter: 1818 loss: 1.00760622e-06
Iter: 1819 loss: 1.00526199e-06
Iter: 1820 loss: 1.00500642e-06
Iter: 1821 loss: 1.00638897e-06
Iter: 1822 loss: 1.00498858e-06
Iter: 1823 loss: 1.00480224e-06
Iter: 1824 loss: 1.00489842e-06
Iter: 1825 loss: 1.00469856e-06
Iter: 1826 loss: 1.00449233e-06
Iter: 1827 loss: 1.00482487e-06
Iter: 1828 loss: 1.00439604e-06
Iter: 1829 loss: 1.00428736e-06
Iter: 1830 loss: 1.00426632e-06
Iter: 1831 loss: 1.00416185e-06
Iter: 1832 loss: 1.00398688e-06
Iter: 1833 loss: 1.00626471e-06
Iter: 1834 loss: 1.00399905e-06
Iter: 1835 loss: 1.00372404e-06
Iter: 1836 loss: 1.00501006e-06
Iter: 1837 loss: 1.00365389e-06
Iter: 1838 loss: 1.00345414e-06
Iter: 1839 loss: 1.00338411e-06
Iter: 1840 loss: 1.00322154e-06
Iter: 1841 loss: 1.00293641e-06
Iter: 1842 loss: 1.00412808e-06
Iter: 1843 loss: 1.00292186e-06
Iter: 1844 loss: 1.002668e-06
Iter: 1845 loss: 1.00463694e-06
Iter: 1846 loss: 1.00260058e-06
Iter: 1847 loss: 1.00247973e-06
Iter: 1848 loss: 1.00221757e-06
Iter: 1849 loss: 1.00227714e-06
Iter: 1850 loss: 1.00201692e-06
Iter: 1851 loss: 1.0025467e-06
Iter: 1852 loss: 1.00192813e-06
Iter: 1853 loss: 1.00164823e-06
Iter: 1854 loss: 1.00301816e-06
Iter: 1855 loss: 1.00158809e-06
Iter: 1856 loss: 1.00136276e-06
Iter: 1857 loss: 1.00333682e-06
Iter: 1858 loss: 1.00134912e-06
Iter: 1859 loss: 1.0012194e-06
Iter: 1860 loss: 1.00098271e-06
Iter: 1861 loss: 1.00476984e-06
Iter: 1862 loss: 1.00099271e-06
Iter: 1863 loss: 1.00093234e-06
Iter: 1864 loss: 1.00087118e-06
Iter: 1865 loss: 1.00076431e-06
Iter: 1866 loss: 1.00066666e-06
Iter: 1867 loss: 1.00063266e-06
Iter: 1868 loss: 1.00045452e-06
Iter: 1869 loss: 1.00100328e-06
Iter: 1870 loss: 1.0003713e-06
Iter: 1871 loss: 1.0001969e-06
Iter: 1872 loss: 1.0004e-06
Iter: 1873 loss: 1.00002057e-06
Iter: 1874 loss: 9.99922804e-07
Iter: 1875 loss: 9.99979193e-07
Iter: 1876 loss: 9.99789563e-07
Iter: 1877 loss: 9.99621534e-07
Iter: 1878 loss: 9.99617896e-07
Iter: 1879 loss: 9.9948943e-07
Iter: 1880 loss: 9.99334247e-07
Iter: 1881 loss: 9.9936608e-07
Iter: 1882 loss: 9.99104486e-07
Iter: 1883 loss: 9.99071517e-07
Iter: 1884 loss: 9.98915539e-07
Iter: 1885 loss: 9.98704081e-07
Iter: 1886 loss: 1.00002399e-06
Iter: 1887 loss: 9.98691121e-07
Iter: 1888 loss: 9.98461e-07
Iter: 1889 loss: 1.00111106e-06
Iter: 1890 loss: 9.98426799e-07
Iter: 1891 loss: 9.9832414e-07
Iter: 1892 loss: 9.98134396e-07
Iter: 1893 loss: 9.98083806e-07
Iter: 1894 loss: 9.97823236e-07
Iter: 1895 loss: 9.9802287e-07
Iter: 1896 loss: 9.97663506e-07
Iter: 1897 loss: 9.9746876e-07
Iter: 1898 loss: 9.97410211e-07
Iter: 1899 loss: 9.97298912e-07
Iter: 1900 loss: 9.97016741e-07
Iter: 1901 loss: 1.0002725e-06
Iter: 1902 loss: 9.96994345e-07
Iter: 1903 loss: 9.96676818e-07
Iter: 1904 loss: 1.00064858e-06
Iter: 1905 loss: 9.96696e-07
Iter: 1906 loss: 9.96519361e-07
Iter: 1907 loss: 9.96466269e-07
Iter: 1908 loss: 9.96306426e-07
Iter: 1909 loss: 9.96145104e-07
Iter: 1910 loss: 9.97948177e-07
Iter: 1911 loss: 9.96156814e-07
Iter: 1912 loss: 9.95929668e-07
Iter: 1913 loss: 9.96534936e-07
Iter: 1914 loss: 9.95872824e-07
Iter: 1915 loss: 9.95770847e-07
Iter: 1916 loss: 9.9557883e-07
Iter: 1917 loss: 9.95556888e-07
Iter: 1918 loss: 9.95363393e-07
Iter: 1919 loss: 9.95944902e-07
Iter: 1920 loss: 9.95291884e-07
Iter: 1921 loss: 9.95125902e-07
Iter: 1922 loss: 9.96092e-07
Iter: 1923 loss: 9.95106916e-07
Iter: 1924 loss: 9.94897619e-07
Iter: 1925 loss: 9.95653295e-07
Iter: 1926 loss: 9.94834181e-07
Iter: 1927 loss: 9.94691845e-07
Iter: 1928 loss: 9.946001e-07
Iter: 1929 loss: 9.94562924e-07
Iter: 1930 loss: 9.94324751e-07
Iter: 1931 loss: 9.96423751e-07
Iter: 1932 loss: 9.94311222e-07
Iter: 1933 loss: 9.94085212e-07
Iter: 1934 loss: 9.93923777e-07
Iter: 1935 loss: 9.93866706e-07
Iter: 1936 loss: 9.93546223e-07
Iter: 1937 loss: 9.94307698e-07
Iter: 1938 loss: 9.93498134e-07
Iter: 1939 loss: 9.93154117e-07
Iter: 1940 loss: 9.94595894e-07
Iter: 1941 loss: 9.93132744e-07
Iter: 1942 loss: 9.929729e-07
Iter: 1943 loss: 9.92886e-07
Iter: 1944 loss: 9.9276258e-07
Iter: 1945 loss: 9.92563628e-07
Iter: 1946 loss: 9.92547712e-07
Iter: 1947 loss: 9.92443347e-07
Iter: 1948 loss: 9.92424248e-07
Iter: 1949 loss: 9.92347168e-07
Iter: 1950 loss: 9.92209152e-07
Iter: 1951 loss: 9.92077503e-07
Iter: 1952 loss: 9.92043624e-07
Iter: 1953 loss: 9.91843535e-07
Iter: 1954 loss: 9.92520313e-07
Iter: 1955 loss: 9.91719162e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.2/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.6 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi1.6
+ date
Sun Nov  8 06:13:10 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.6/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.2/300_100_100_100_1 --function f1 --psi -1 --phi 1.6 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.6/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84b4694bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84b46942f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84e0b5dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84b45ed9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84b45edae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f848c08e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f848c0206a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84b458c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84b458c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84b458e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f847018f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f847015f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f847015fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84701396a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f848c0b2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8470139a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f848c0cb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84700b4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8470010730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8470017f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84700dc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f847005d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8430592840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f843056b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f843056b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f843059c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84304b08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84305d52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84305d51e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84305d5048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f843047b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84305521e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84305522f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8430530510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8430419a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f843039d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0039789723
test_loss: 0.004068917
train_loss: 0.0031244436
test_loss: 0.0034826025
train_loss: 0.0030149047
test_loss: 0.0031352506
train_loss: 0.0032300537
test_loss: 0.003392205
train_loss: 0.0027795224
test_loss: 0.0033244812
train_loss: 0.0027651805
test_loss: 0.0032699078
train_loss: 0.0029826097
test_loss: 0.0032438573
train_loss: 0.0026811357
test_loss: 0.002948377
train_loss: 0.0028894814
test_loss: 0.0030295388
train_loss: 0.0028918628
test_loss: 0.0031651452
train_loss: 0.0025593173
test_loss: 0.0029062633
train_loss: 0.0029179202
test_loss: 0.0032189854
train_loss: 0.0026368112
test_loss: 0.0030708648
train_loss: 0.0029211012
test_loss: 0.0030597611
train_loss: 0.0028023429
test_loss: 0.0028740014
train_loss: 0.002578043
test_loss: 0.0028707501
train_loss: 0.0027538687
test_loss: 0.0029356994
train_loss: 0.0026222472
test_loss: 0.002752094
train_loss: 0.002815498
test_loss: 0.0030845993
train_loss: 0.0027386546
test_loss: 0.0029915255
train_loss: 0.0025164247
test_loss: 0.0029749775
train_loss: 0.0026452024
test_loss: 0.0029849466
train_loss: 0.0025617743
test_loss: 0.0030374927
train_loss: 0.0026025272
test_loss: 0.0030767461
train_loss: 0.0026734944
test_loss: 0.0030352112
train_loss: 0.00254607
test_loss: 0.0030801191
train_loss: 0.0026940748
test_loss: 0.0029839387
train_loss: 0.0029355783
test_loss: 0.0032926425
train_loss: 0.002389258
test_loss: 0.002775866
train_loss: 0.00249347
test_loss: 0.0027788964
train_loss: 0.0025582623
test_loss: 0.0028268422
train_loss: 0.0026088676
test_loss: 0.0028910385
train_loss: 0.0030869425
test_loss: 0.0030158684
train_loss: 0.0025823552
test_loss: 0.0027437694
train_loss: 0.002922737
test_loss: 0.002984816
train_loss: 0.0030557024
test_loss: 0.0031903435
train_loss: 0.0025957385
test_loss: 0.0028004365
train_loss: 0.0026677055
test_loss: 0.0029411623
train_loss: 0.002604419
test_loss: 0.002937112
train_loss: 0.0028033208
test_loss: 0.0029566227
train_loss: 0.0026555376
test_loss: 0.0030290599
train_loss: 0.0024865812
test_loss: 0.002870396
train_loss: 0.002620579
test_loss: 0.0030738346
train_loss: 0.0025415472
test_loss: 0.0029148483
train_loss: 0.0026046047
test_loss: 0.0029954577
train_loss: 0.0025394363
test_loss: 0.002786589
train_loss: 0.0025574588
test_loss: 0.0028384072
train_loss: 0.0025885147
test_loss: 0.0028158398
train_loss: 0.0025712335
test_loss: 0.0029481922
train_loss: 0.0024853614
test_loss: 0.002709787
train_loss: 0.0023726528
test_loss: 0.0028165698
train_loss: 0.0025274938
test_loss: 0.0029306158
train_loss: 0.0026957244
test_loss: 0.0030841962
train_loss: 0.0025249007
test_loss: 0.002930942
train_loss: 0.002443269
test_loss: 0.0027896229
train_loss: 0.0024773898
test_loss: 0.0028564078
train_loss: 0.002408664
test_loss: 0.0029102678
train_loss: 0.0026337595
test_loss: 0.0028354893
train_loss: 0.0029642903
test_loss: 0.0031271908
train_loss: 0.0025614277
test_loss: 0.0031196813
train_loss: 0.0026056566
test_loss: 0.0028890558
train_loss: 0.0024452049
test_loss: 0.0027945992
train_loss: 0.0026470462
test_loss: 0.0029437447
train_loss: 0.0026502516
test_loss: 0.0029592575
train_loss: 0.002439359
test_loss: 0.0029084166
train_loss: 0.002444175
test_loss: 0.003022512
train_loss: 0.0026807254
test_loss: 0.0028269412
train_loss: 0.0025535459
test_loss: 0.0028011075
train_loss: 0.0024666202
test_loss: 0.0028599275
train_loss: 0.0025085807
test_loss: 0.002985981
train_loss: 0.0024970854
test_loss: 0.0028240464
train_loss: 0.0026313441
test_loss: 0.002966632
train_loss: 0.0023294976
test_loss: 0.0028073515
train_loss: 0.0024790335
test_loss: 0.0029249263
train_loss: 0.0022961546
test_loss: 0.0027159583
train_loss: 0.0024219074
test_loss: 0.0027413804
train_loss: 0.0023765822
test_loss: 0.0028749157
train_loss: 0.0024259915
test_loss: 0.002955381
train_loss: 0.002280911
test_loss: 0.002731398
train_loss: 0.0024606702
test_loss: 0.0027794593
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi1.6/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.6/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -1 --phi 1.6 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi1.6/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c4a00510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c4af1d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c4af1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c4a4f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c4a4fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c49bd2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c498c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c493d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c49342f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c4934bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c48fa6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c48c0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c48c1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c48c1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c4822c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c485c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c484f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c484fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c47cb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c4763268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97c484fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f978a292ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f978a24f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f978a2746a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f978a274620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f978a20f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f978a1d7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f978a206400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f978a18d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f978a1c37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f976424f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f976427b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f976427b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9764230b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9764230ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f976418b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.0089364e-05
Iter: 2 loss: 9.74107843e-06
Iter: 3 loss: 8.57129453e-06
Iter: 4 loss: 7.93035633e-06
Iter: 5 loss: 1.27499088e-05
Iter: 6 loss: 7.88018588e-06
Iter: 7 loss: 7.53551558e-06
Iter: 8 loss: 9.94190941e-06
Iter: 9 loss: 7.50323443e-06
Iter: 10 loss: 7.23285757e-06
Iter: 11 loss: 7.48944649e-06
Iter: 12 loss: 7.07751224e-06
Iter: 13 loss: 6.84530914e-06
Iter: 14 loss: 8.09777248e-06
Iter: 15 loss: 6.81027677e-06
Iter: 16 loss: 6.61280183e-06
Iter: 17 loss: 6.50284437e-06
Iter: 18 loss: 6.41611859e-06
Iter: 19 loss: 6.22603238e-06
Iter: 20 loss: 6.82243626e-06
Iter: 21 loss: 6.17092519e-06
Iter: 22 loss: 5.93390814e-06
Iter: 23 loss: 6.72207e-06
Iter: 24 loss: 5.86893566e-06
Iter: 25 loss: 5.67570532e-06
Iter: 26 loss: 5.71725559e-06
Iter: 27 loss: 5.53293194e-06
Iter: 28 loss: 5.24770894e-06
Iter: 29 loss: 5.38357108e-06
Iter: 30 loss: 5.05600474e-06
Iter: 31 loss: 4.80551398e-06
Iter: 32 loss: 6.35455308e-06
Iter: 33 loss: 4.77576668e-06
Iter: 34 loss: 4.57165788e-06
Iter: 35 loss: 5.39905795e-06
Iter: 36 loss: 4.5263796e-06
Iter: 37 loss: 4.38365305e-06
Iter: 38 loss: 4.51170854e-06
Iter: 39 loss: 4.30041564e-06
Iter: 40 loss: 4.16681269e-06
Iter: 41 loss: 5.07364348e-06
Iter: 42 loss: 4.15385784e-06
Iter: 43 loss: 4.08437472e-06
Iter: 44 loss: 4.0753348e-06
Iter: 45 loss: 4.01564193e-06
Iter: 46 loss: 4.05294259e-06
Iter: 47 loss: 3.97750227e-06
Iter: 48 loss: 3.92594575e-06
Iter: 49 loss: 4.22970879e-06
Iter: 50 loss: 3.91922e-06
Iter: 51 loss: 3.86643342e-06
Iter: 52 loss: 3.74569254e-06
Iter: 53 loss: 5.31652131e-06
Iter: 54 loss: 3.7376949e-06
Iter: 55 loss: 3.64632956e-06
Iter: 56 loss: 3.64214679e-06
Iter: 57 loss: 3.59132309e-06
Iter: 58 loss: 3.46460592e-06
Iter: 59 loss: 4.68336475e-06
Iter: 60 loss: 3.44749424e-06
Iter: 61 loss: 3.36717176e-06
Iter: 62 loss: 3.35725963e-06
Iter: 63 loss: 3.30423973e-06
Iter: 64 loss: 3.46674801e-06
Iter: 65 loss: 3.28866872e-06
Iter: 66 loss: 3.23018321e-06
Iter: 67 loss: 3.15280977e-06
Iter: 68 loss: 3.14819408e-06
Iter: 69 loss: 3.06618153e-06
Iter: 70 loss: 3.26741315e-06
Iter: 71 loss: 3.0369215e-06
Iter: 72 loss: 2.98113105e-06
Iter: 73 loss: 3.12866587e-06
Iter: 74 loss: 2.96251164e-06
Iter: 75 loss: 2.89458785e-06
Iter: 76 loss: 3.4114928e-06
Iter: 77 loss: 2.8893428e-06
Iter: 78 loss: 2.88695583e-06
Iter: 79 loss: 2.87166336e-06
Iter: 80 loss: 2.85629949e-06
Iter: 81 loss: 2.817329e-06
Iter: 82 loss: 3.17178592e-06
Iter: 83 loss: 2.81163057e-06
Iter: 84 loss: 2.77864683e-06
Iter: 85 loss: 2.77820118e-06
Iter: 86 loss: 2.75561797e-06
Iter: 87 loss: 2.70479904e-06
Iter: 88 loss: 3.39269809e-06
Iter: 89 loss: 2.7018009e-06
Iter: 90 loss: 2.65006884e-06
Iter: 91 loss: 3.43732427e-06
Iter: 92 loss: 2.65001199e-06
Iter: 93 loss: 2.62072035e-06
Iter: 94 loss: 2.60601678e-06
Iter: 95 loss: 2.59220155e-06
Iter: 96 loss: 2.55287659e-06
Iter: 97 loss: 2.74297236e-06
Iter: 98 loss: 2.54586894e-06
Iter: 99 loss: 2.51673237e-06
Iter: 100 loss: 2.71846739e-06
Iter: 101 loss: 2.51391202e-06
Iter: 102 loss: 2.48704555e-06
Iter: 103 loss: 2.46275977e-06
Iter: 104 loss: 2.45595379e-06
Iter: 105 loss: 2.4291e-06
Iter: 106 loss: 2.50571202e-06
Iter: 107 loss: 2.42064925e-06
Iter: 108 loss: 2.38909479e-06
Iter: 109 loss: 2.48804508e-06
Iter: 110 loss: 2.37993027e-06
Iter: 111 loss: 2.35545622e-06
Iter: 112 loss: 2.40348754e-06
Iter: 113 loss: 2.34525805e-06
Iter: 114 loss: 2.35688572e-06
Iter: 115 loss: 2.33593e-06
Iter: 116 loss: 2.32965249e-06
Iter: 117 loss: 2.30944534e-06
Iter: 118 loss: 2.34004938e-06
Iter: 119 loss: 2.29513444e-06
Iter: 120 loss: 2.2718159e-06
Iter: 121 loss: 2.27059309e-06
Iter: 122 loss: 2.25704775e-06
Iter: 123 loss: 2.25447229e-06
Iter: 124 loss: 2.24547944e-06
Iter: 125 loss: 2.22807921e-06
Iter: 126 loss: 2.24993619e-06
Iter: 127 loss: 2.21896562e-06
Iter: 128 loss: 2.19107483e-06
Iter: 129 loss: 2.24218957e-06
Iter: 130 loss: 2.17893967e-06
Iter: 131 loss: 2.16657645e-06
Iter: 132 loss: 2.19234425e-06
Iter: 133 loss: 2.16148896e-06
Iter: 134 loss: 2.14626107e-06
Iter: 135 loss: 2.22741573e-06
Iter: 136 loss: 2.14395254e-06
Iter: 137 loss: 2.13170279e-06
Iter: 138 loss: 2.15679165e-06
Iter: 139 loss: 2.12663144e-06
Iter: 140 loss: 2.11550673e-06
Iter: 141 loss: 2.10553139e-06
Iter: 142 loss: 2.10271787e-06
Iter: 143 loss: 2.08491429e-06
Iter: 144 loss: 2.10493636e-06
Iter: 145 loss: 2.07535845e-06
Iter: 146 loss: 2.05910646e-06
Iter: 147 loss: 2.05915717e-06
Iter: 148 loss: 2.04905723e-06
Iter: 149 loss: 2.04896696e-06
Iter: 150 loss: 2.03920808e-06
Iter: 151 loss: 2.01163607e-06
Iter: 152 loss: 2.15786849e-06
Iter: 153 loss: 2.00321188e-06
Iter: 154 loss: 1.99832471e-06
Iter: 155 loss: 1.99146211e-06
Iter: 156 loss: 1.98158068e-06
Iter: 157 loss: 1.97308327e-06
Iter: 158 loss: 1.97041572e-06
Iter: 159 loss: 1.95876441e-06
Iter: 160 loss: 1.98259681e-06
Iter: 161 loss: 1.95407119e-06
Iter: 162 loss: 1.94262338e-06
Iter: 163 loss: 2.0337784e-06
Iter: 164 loss: 1.94171071e-06
Iter: 165 loss: 1.93325968e-06
Iter: 166 loss: 1.95172902e-06
Iter: 167 loss: 1.92989546e-06
Iter: 168 loss: 1.92346693e-06
Iter: 169 loss: 1.91468803e-06
Iter: 170 loss: 1.91425443e-06
Iter: 171 loss: 1.90590129e-06
Iter: 172 loss: 1.90529056e-06
Iter: 173 loss: 1.89997468e-06
Iter: 174 loss: 1.88847071e-06
Iter: 175 loss: 2.07090784e-06
Iter: 176 loss: 1.88793433e-06
Iter: 177 loss: 1.87494857e-06
Iter: 178 loss: 1.9716008e-06
Iter: 179 loss: 1.87390083e-06
Iter: 180 loss: 1.86548971e-06
Iter: 181 loss: 1.86197121e-06
Iter: 182 loss: 1.85755266e-06
Iter: 183 loss: 1.84976489e-06
Iter: 184 loss: 1.8494394e-06
Iter: 185 loss: 1.84276848e-06
Iter: 186 loss: 1.90679077e-06
Iter: 187 loss: 1.84266673e-06
Iter: 188 loss: 1.83850375e-06
Iter: 189 loss: 1.8272807e-06
Iter: 190 loss: 1.90212052e-06
Iter: 191 loss: 1.8246144e-06
Iter: 192 loss: 1.82221652e-06
Iter: 193 loss: 1.81943517e-06
Iter: 194 loss: 1.81429016e-06
Iter: 195 loss: 1.805436e-06
Iter: 196 loss: 1.8054277e-06
Iter: 197 loss: 1.7978673e-06
Iter: 198 loss: 1.81157941e-06
Iter: 199 loss: 1.79455435e-06
Iter: 200 loss: 1.78530013e-06
Iter: 201 loss: 1.85119893e-06
Iter: 202 loss: 1.78451853e-06
Iter: 203 loss: 1.77836364e-06
Iter: 204 loss: 1.81659493e-06
Iter: 205 loss: 1.77762251e-06
Iter: 206 loss: 1.77447646e-06
Iter: 207 loss: 1.76709295e-06
Iter: 208 loss: 1.85436397e-06
Iter: 209 loss: 1.76643709e-06
Iter: 210 loss: 1.761921e-06
Iter: 211 loss: 1.76070523e-06
Iter: 212 loss: 1.75725199e-06
Iter: 213 loss: 1.75002435e-06
Iter: 214 loss: 1.87119235e-06
Iter: 215 loss: 1.74983415e-06
Iter: 216 loss: 1.74043589e-06
Iter: 217 loss: 1.77593665e-06
Iter: 218 loss: 1.73825129e-06
Iter: 219 loss: 1.73290107e-06
Iter: 220 loss: 1.79854931e-06
Iter: 221 loss: 1.73285366e-06
Iter: 222 loss: 1.72661464e-06
Iter: 223 loss: 1.73452793e-06
Iter: 224 loss: 1.72332886e-06
Iter: 225 loss: 1.71938541e-06
Iter: 226 loss: 1.71733677e-06
Iter: 227 loss: 1.71544593e-06
Iter: 228 loss: 1.71082706e-06
Iter: 229 loss: 1.77092238e-06
Iter: 230 loss: 1.71075953e-06
Iter: 231 loss: 1.70670819e-06
Iter: 232 loss: 1.71226907e-06
Iter: 233 loss: 1.70462863e-06
Iter: 234 loss: 1.70139083e-06
Iter: 235 loss: 1.69416728e-06
Iter: 236 loss: 1.79915969e-06
Iter: 237 loss: 1.69390387e-06
Iter: 238 loss: 1.69050008e-06
Iter: 239 loss: 1.68935435e-06
Iter: 240 loss: 1.68522172e-06
Iter: 241 loss: 1.68144277e-06
Iter: 242 loss: 1.68042857e-06
Iter: 243 loss: 1.67498831e-06
Iter: 244 loss: 1.69637735e-06
Iter: 245 loss: 1.67378289e-06
Iter: 246 loss: 1.6684877e-06
Iter: 247 loss: 1.68724819e-06
Iter: 248 loss: 1.66716427e-06
Iter: 249 loss: 1.66125346e-06
Iter: 250 loss: 1.6746726e-06
Iter: 251 loss: 1.65910717e-06
Iter: 252 loss: 1.65372512e-06
Iter: 253 loss: 1.64792323e-06
Iter: 254 loss: 1.64695666e-06
Iter: 255 loss: 1.64922335e-06
Iter: 256 loss: 1.64394658e-06
Iter: 257 loss: 1.64125561e-06
Iter: 258 loss: 1.64051892e-06
Iter: 259 loss: 1.6387927e-06
Iter: 260 loss: 1.63554012e-06
Iter: 261 loss: 1.62952881e-06
Iter: 262 loss: 1.75883474e-06
Iter: 263 loss: 1.62955416e-06
Iter: 264 loss: 1.62577464e-06
Iter: 265 loss: 1.67249368e-06
Iter: 266 loss: 1.62568483e-06
Iter: 267 loss: 1.62179072e-06
Iter: 268 loss: 1.64078131e-06
Iter: 269 loss: 1.62120182e-06
Iter: 270 loss: 1.61714604e-06
Iter: 271 loss: 1.61551145e-06
Iter: 272 loss: 1.61334924e-06
Iter: 273 loss: 1.60981631e-06
Iter: 274 loss: 1.60938202e-06
Iter: 275 loss: 1.60684499e-06
Iter: 276 loss: 1.60250477e-06
Iter: 277 loss: 1.60251409e-06
Iter: 278 loss: 1.5987622e-06
Iter: 279 loss: 1.59394983e-06
Iter: 280 loss: 1.59362378e-06
Iter: 281 loss: 1.58784223e-06
Iter: 282 loss: 1.62692925e-06
Iter: 283 loss: 1.58732064e-06
Iter: 284 loss: 1.58370597e-06
Iter: 285 loss: 1.60178183e-06
Iter: 286 loss: 1.58307341e-06
Iter: 287 loss: 1.57876411e-06
Iter: 288 loss: 1.57987506e-06
Iter: 289 loss: 1.5756641e-06
Iter: 290 loss: 1.57247007e-06
Iter: 291 loss: 1.58320177e-06
Iter: 292 loss: 1.57167892e-06
Iter: 293 loss: 1.5678072e-06
Iter: 294 loss: 1.60764966e-06
Iter: 295 loss: 1.56773842e-06
Iter: 296 loss: 1.56590488e-06
Iter: 297 loss: 1.56214446e-06
Iter: 298 loss: 1.6276831e-06
Iter: 299 loss: 1.56211229e-06
Iter: 300 loss: 1.55789508e-06
Iter: 301 loss: 1.56580131e-06
Iter: 302 loss: 1.55611451e-06
Iter: 303 loss: 1.55317446e-06
Iter: 304 loss: 1.55312455e-06
Iter: 305 loss: 1.54976533e-06
Iter: 306 loss: 1.54392922e-06
Iter: 307 loss: 1.54392387e-06
Iter: 308 loss: 1.54031727e-06
Iter: 309 loss: 1.56767283e-06
Iter: 310 loss: 1.54012173e-06
Iter: 311 loss: 1.53663746e-06
Iter: 312 loss: 1.53324686e-06
Iter: 313 loss: 1.53248072e-06
Iter: 314 loss: 1.52803045e-06
Iter: 315 loss: 1.52785981e-06
Iter: 316 loss: 1.52561859e-06
Iter: 317 loss: 1.52097618e-06
Iter: 318 loss: 1.60389629e-06
Iter: 319 loss: 1.52086818e-06
Iter: 320 loss: 1.51836e-06
Iter: 321 loss: 1.51822906e-06
Iter: 322 loss: 1.51572556e-06
Iter: 323 loss: 1.51786753e-06
Iter: 324 loss: 1.51431209e-06
Iter: 325 loss: 1.51162976e-06
Iter: 326 loss: 1.52526081e-06
Iter: 327 loss: 1.51115103e-06
Iter: 328 loss: 1.50944447e-06
Iter: 329 loss: 1.50947471e-06
Iter: 330 loss: 1.50836229e-06
Iter: 331 loss: 1.50515189e-06
Iter: 332 loss: 1.51443078e-06
Iter: 333 loss: 1.50342294e-06
Iter: 334 loss: 1.49946345e-06
Iter: 335 loss: 1.50418327e-06
Iter: 336 loss: 1.4973557e-06
Iter: 337 loss: 1.49334721e-06
Iter: 338 loss: 1.55354087e-06
Iter: 339 loss: 1.4933712e-06
Iter: 340 loss: 1.4908602e-06
Iter: 341 loss: 1.4865243e-06
Iter: 342 loss: 1.48652975e-06
Iter: 343 loss: 1.48697018e-06
Iter: 344 loss: 1.48416916e-06
Iter: 345 loss: 1.48255867e-06
Iter: 346 loss: 1.4846014e-06
Iter: 347 loss: 1.48176264e-06
Iter: 348 loss: 1.47998344e-06
Iter: 349 loss: 1.47730441e-06
Iter: 350 loss: 1.47734681e-06
Iter: 351 loss: 1.47501407e-06
Iter: 352 loss: 1.47497531e-06
Iter: 353 loss: 1.47275796e-06
Iter: 354 loss: 1.47164951e-06
Iter: 355 loss: 1.47058518e-06
Iter: 356 loss: 1.46816933e-06
Iter: 357 loss: 1.47586206e-06
Iter: 358 loss: 1.46745094e-06
Iter: 359 loss: 1.4653267e-06
Iter: 360 loss: 1.48139884e-06
Iter: 361 loss: 1.46516788e-06
Iter: 362 loss: 1.462706e-06
Iter: 363 loss: 1.4717458e-06
Iter: 364 loss: 1.4620781e-06
Iter: 365 loss: 1.46083278e-06
Iter: 366 loss: 1.46061018e-06
Iter: 367 loss: 1.45978709e-06
Iter: 368 loss: 1.45738306e-06
Iter: 369 loss: 1.45612466e-06
Iter: 370 loss: 1.45501531e-06
Iter: 371 loss: 1.45258855e-06
Iter: 372 loss: 1.46927664e-06
Iter: 373 loss: 1.4523614e-06
Iter: 374 loss: 1.45024751e-06
Iter: 375 loss: 1.44660248e-06
Iter: 376 loss: 1.53782207e-06
Iter: 377 loss: 1.44659896e-06
Iter: 378 loss: 1.44524756e-06
Iter: 379 loss: 1.44472278e-06
Iter: 380 loss: 1.44273179e-06
Iter: 381 loss: 1.44689125e-06
Iter: 382 loss: 1.44192245e-06
Iter: 383 loss: 1.43999262e-06
Iter: 384 loss: 1.44057515e-06
Iter: 385 loss: 1.43866544e-06
Iter: 386 loss: 1.43631758e-06
Iter: 387 loss: 1.43816135e-06
Iter: 388 loss: 1.43496675e-06
Iter: 389 loss: 1.43188402e-06
Iter: 390 loss: 1.46159732e-06
Iter: 391 loss: 1.43180409e-06
Iter: 392 loss: 1.43006525e-06
Iter: 393 loss: 1.42730539e-06
Iter: 394 loss: 1.42726981e-06
Iter: 395 loss: 1.42512113e-06
Iter: 396 loss: 1.42511544e-06
Iter: 397 loss: 1.42296221e-06
Iter: 398 loss: 1.43193597e-06
Iter: 399 loss: 1.42245642e-06
Iter: 400 loss: 1.42085389e-06
Iter: 401 loss: 1.41868441e-06
Iter: 402 loss: 1.41865348e-06
Iter: 403 loss: 1.41701253e-06
Iter: 404 loss: 1.41695682e-06
Iter: 405 loss: 1.41577755e-06
Iter: 406 loss: 1.41356804e-06
Iter: 407 loss: 1.45492572e-06
Iter: 408 loss: 1.41354826e-06
Iter: 409 loss: 1.41090186e-06
Iter: 410 loss: 1.42429496e-06
Iter: 411 loss: 1.41054761e-06
Iter: 412 loss: 1.40846259e-06
Iter: 413 loss: 1.41181511e-06
Iter: 414 loss: 1.40750149e-06
Iter: 415 loss: 1.40532507e-06
Iter: 416 loss: 1.4061095e-06
Iter: 417 loss: 1.4038078e-06
Iter: 418 loss: 1.40245243e-06
Iter: 419 loss: 1.40192685e-06
Iter: 420 loss: 1.40087275e-06
Iter: 421 loss: 1.40057682e-06
Iter: 422 loss: 1.39993199e-06
Iter: 423 loss: 1.39831116e-06
Iter: 424 loss: 1.39532631e-06
Iter: 425 loss: 1.46394609e-06
Iter: 426 loss: 1.39532744e-06
Iter: 427 loss: 1.3946501e-06
Iter: 428 loss: 1.39364897e-06
Iter: 429 loss: 1.39271287e-06
Iter: 430 loss: 1.39088729e-06
Iter: 431 loss: 1.42827685e-06
Iter: 432 loss: 1.39080248e-06
Iter: 433 loss: 1.3890608e-06
Iter: 434 loss: 1.41413193e-06
Iter: 435 loss: 1.38911128e-06
Iter: 436 loss: 1.38791347e-06
Iter: 437 loss: 1.40508064e-06
Iter: 438 loss: 1.38793644e-06
Iter: 439 loss: 1.38736152e-06
Iter: 440 loss: 1.38558016e-06
Iter: 441 loss: 1.39015e-06
Iter: 442 loss: 1.38454334e-06
Iter: 443 loss: 1.38306496e-06
Iter: 444 loss: 1.38299833e-06
Iter: 445 loss: 1.38135886e-06
Iter: 446 loss: 1.38074779e-06
Iter: 447 loss: 1.37987104e-06
Iter: 448 loss: 1.37833399e-06
Iter: 449 loss: 1.38006328e-06
Iter: 450 loss: 1.37747315e-06
Iter: 451 loss: 1.37505413e-06
Iter: 452 loss: 1.37977668e-06
Iter: 453 loss: 1.37408983e-06
Iter: 454 loss: 1.37338213e-06
Iter: 455 loss: 1.37319523e-06
Iter: 456 loss: 1.37214829e-06
Iter: 457 loss: 1.37043708e-06
Iter: 458 loss: 1.41191163e-06
Iter: 459 loss: 1.3703991e-06
Iter: 460 loss: 1.36882193e-06
Iter: 461 loss: 1.3714523e-06
Iter: 462 loss: 1.36808944e-06
Iter: 463 loss: 1.36615233e-06
Iter: 464 loss: 1.37144525e-06
Iter: 465 loss: 1.36554411e-06
Iter: 466 loss: 1.36393726e-06
Iter: 467 loss: 1.38807286e-06
Iter: 468 loss: 1.36390372e-06
Iter: 469 loss: 1.36289964e-06
Iter: 470 loss: 1.36128187e-06
Iter: 471 loss: 1.36130143e-06
Iter: 472 loss: 1.36061522e-06
Iter: 473 loss: 1.36018809e-06
Iter: 474 loss: 1.35931282e-06
Iter: 475 loss: 1.35717482e-06
Iter: 476 loss: 1.38254723e-06
Iter: 477 loss: 1.35709479e-06
Iter: 478 loss: 1.35497226e-06
Iter: 479 loss: 1.3618419e-06
Iter: 480 loss: 1.35434516e-06
Iter: 481 loss: 1.35300559e-06
Iter: 482 loss: 1.36091455e-06
Iter: 483 loss: 1.35287769e-06
Iter: 484 loss: 1.35107985e-06
Iter: 485 loss: 1.35042797e-06
Iter: 486 loss: 1.34938364e-06
Iter: 487 loss: 1.34785057e-06
Iter: 488 loss: 1.35324069e-06
Iter: 489 loss: 1.3474164e-06
Iter: 490 loss: 1.34589618e-06
Iter: 491 loss: 1.35705659e-06
Iter: 492 loss: 1.3458432e-06
Iter: 493 loss: 1.34431025e-06
Iter: 494 loss: 1.3490004e-06
Iter: 495 loss: 1.34381673e-06
Iter: 496 loss: 1.34266509e-06
Iter: 497 loss: 1.34087804e-06
Iter: 498 loss: 1.34081756e-06
Iter: 499 loss: 1.33939932e-06
Iter: 500 loss: 1.34560355e-06
Iter: 501 loss: 1.33908e-06
Iter: 502 loss: 1.33765184e-06
Iter: 503 loss: 1.35167102e-06
Iter: 504 loss: 1.33761171e-06
Iter: 505 loss: 1.33655431e-06
Iter: 506 loss: 1.34179436e-06
Iter: 507 loss: 1.33645585e-06
Iter: 508 loss: 1.33581875e-06
Iter: 509 loss: 1.34207926e-06
Iter: 510 loss: 1.33582137e-06
Iter: 511 loss: 1.33507694e-06
Iter: 512 loss: 1.33332219e-06
Iter: 513 loss: 1.34627703e-06
Iter: 514 loss: 1.33296544e-06
Iter: 515 loss: 1.3314243e-06
Iter: 516 loss: 1.33507729e-06
Iter: 517 loss: 1.33079516e-06
Iter: 518 loss: 1.32923594e-06
Iter: 519 loss: 1.33930882e-06
Iter: 520 loss: 1.32900288e-06
Iter: 521 loss: 1.32772698e-06
Iter: 522 loss: 1.33546541e-06
Iter: 523 loss: 1.32750949e-06
Iter: 524 loss: 1.326296e-06
Iter: 525 loss: 1.32727723e-06
Iter: 526 loss: 1.32563878e-06
Iter: 527 loss: 1.32461071e-06
Iter: 528 loss: 1.32928108e-06
Iter: 529 loss: 1.32442881e-06
Iter: 530 loss: 1.32320542e-06
Iter: 531 loss: 1.32701371e-06
Iter: 532 loss: 1.3228497e-06
Iter: 533 loss: 1.32205059e-06
Iter: 534 loss: 1.32175398e-06
Iter: 535 loss: 1.32130958e-06
Iter: 536 loss: 1.32007926e-06
Iter: 537 loss: 1.3212441e-06
Iter: 538 loss: 1.3194383e-06
Iter: 539 loss: 1.31824027e-06
Iter: 540 loss: 1.32647733e-06
Iter: 541 loss: 1.31815841e-06
Iter: 542 loss: 1.31693514e-06
Iter: 543 loss: 1.32529965e-06
Iter: 544 loss: 1.31684783e-06
Iter: 545 loss: 1.31592674e-06
Iter: 546 loss: 1.32064486e-06
Iter: 547 loss: 1.31583738e-06
Iter: 548 loss: 1.31517e-06
Iter: 549 loss: 1.31431375e-06
Iter: 550 loss: 1.31427635e-06
Iter: 551 loss: 1.31307775e-06
Iter: 552 loss: 1.31275283e-06
Iter: 553 loss: 1.31203581e-06
Iter: 554 loss: 1.31060824e-06
Iter: 555 loss: 1.31377556e-06
Iter: 556 loss: 1.31007505e-06
Iter: 557 loss: 1.30885132e-06
Iter: 558 loss: 1.30889816e-06
Iter: 559 loss: 1.3081459e-06
Iter: 560 loss: 1.3094525e-06
Iter: 561 loss: 1.30789238e-06
Iter: 562 loss: 1.30704007e-06
Iter: 563 loss: 1.30678109e-06
Iter: 564 loss: 1.30638443e-06
Iter: 565 loss: 1.30495391e-06
Iter: 566 loss: 1.31522722e-06
Iter: 567 loss: 1.30476087e-06
Iter: 568 loss: 1.30407238e-06
Iter: 569 loss: 1.30278806e-06
Iter: 570 loss: 1.32459422e-06
Iter: 571 loss: 1.30270666e-06
Iter: 572 loss: 1.3010881e-06
Iter: 573 loss: 1.30954163e-06
Iter: 574 loss: 1.30081025e-06
Iter: 575 loss: 1.29970829e-06
Iter: 576 loss: 1.31435536e-06
Iter: 577 loss: 1.29971318e-06
Iter: 578 loss: 1.29878038e-06
Iter: 579 loss: 1.30450189e-06
Iter: 580 loss: 1.29864725e-06
Iter: 581 loss: 1.29808859e-06
Iter: 582 loss: 1.29904333e-06
Iter: 583 loss: 1.29773343e-06
Iter: 584 loss: 1.2969067e-06
Iter: 585 loss: 1.29519685e-06
Iter: 586 loss: 1.32400805e-06
Iter: 587 loss: 1.29523221e-06
Iter: 588 loss: 1.2938508e-06
Iter: 589 loss: 1.29711555e-06
Iter: 590 loss: 1.29339048e-06
Iter: 591 loss: 1.29170155e-06
Iter: 592 loss: 1.29929549e-06
Iter: 593 loss: 1.29146599e-06
Iter: 594 loss: 1.29022737e-06
Iter: 595 loss: 1.29817147e-06
Iter: 596 loss: 1.29007924e-06
Iter: 597 loss: 1.28902491e-06
Iter: 598 loss: 1.29332102e-06
Iter: 599 loss: 1.28876786e-06
Iter: 600 loss: 1.28811018e-06
Iter: 601 loss: 1.28978684e-06
Iter: 602 loss: 1.28785564e-06
Iter: 603 loss: 1.28682439e-06
Iter: 604 loss: 1.28643296e-06
Iter: 605 loss: 1.28576039e-06
Iter: 606 loss: 1.28510806e-06
Iter: 607 loss: 1.28577744e-06
Iter: 608 loss: 1.28473084e-06
Iter: 609 loss: 1.2836681e-06
Iter: 610 loss: 1.28424938e-06
Iter: 611 loss: 1.28294187e-06
Iter: 612 loss: 1.28277316e-06
Iter: 613 loss: 1.28235729e-06
Iter: 614 loss: 1.28185843e-06
Iter: 615 loss: 1.28093154e-06
Iter: 616 loss: 1.28089391e-06
Iter: 617 loss: 1.27990529e-06
Iter: 618 loss: 1.28711179e-06
Iter: 619 loss: 1.27984526e-06
Iter: 620 loss: 1.27918315e-06
Iter: 621 loss: 1.27817475e-06
Iter: 622 loss: 1.27809631e-06
Iter: 623 loss: 1.27686314e-06
Iter: 624 loss: 1.28345619e-06
Iter: 625 loss: 1.27670137e-06
Iter: 626 loss: 1.27574822e-06
Iter: 627 loss: 1.27623025e-06
Iter: 628 loss: 1.27507678e-06
Iter: 629 loss: 1.27420026e-06
Iter: 630 loss: 1.27425096e-06
Iter: 631 loss: 1.27365342e-06
Iter: 632 loss: 1.27494468e-06
Iter: 633 loss: 1.2734381e-06
Iter: 634 loss: 1.27272097e-06
Iter: 635 loss: 1.27603664e-06
Iter: 636 loss: 1.27262024e-06
Iter: 637 loss: 1.27216185e-06
Iter: 638 loss: 1.27073963e-06
Iter: 639 loss: 1.27788076e-06
Iter: 640 loss: 1.27035946e-06
Iter: 641 loss: 1.26873988e-06
Iter: 642 loss: 1.28384454e-06
Iter: 643 loss: 1.26859777e-06
Iter: 644 loss: 1.26804593e-06
Iter: 645 loss: 1.26796044e-06
Iter: 646 loss: 1.26717043e-06
Iter: 647 loss: 1.26626628e-06
Iter: 648 loss: 1.26617635e-06
Iter: 649 loss: 1.26530199e-06
Iter: 650 loss: 1.27324211e-06
Iter: 651 loss: 1.26530097e-06
Iter: 652 loss: 1.26462987e-06
Iter: 653 loss: 1.26449913e-06
Iter: 654 loss: 1.26412306e-06
Iter: 655 loss: 1.26325517e-06
Iter: 656 loss: 1.26336568e-06
Iter: 657 loss: 1.26249745e-06
Iter: 658 loss: 1.26134569e-06
Iter: 659 loss: 1.26282555e-06
Iter: 660 loss: 1.26073053e-06
Iter: 661 loss: 1.25993461e-06
Iter: 662 loss: 1.25997735e-06
Iter: 663 loss: 1.25921008e-06
Iter: 664 loss: 1.26207988e-06
Iter: 665 loss: 1.25908093e-06
Iter: 666 loss: 1.25841541e-06
Iter: 667 loss: 1.26017824e-06
Iter: 668 loss: 1.25824306e-06
Iter: 669 loss: 1.25759811e-06
Iter: 670 loss: 1.25932229e-06
Iter: 671 loss: 1.25726956e-06
Iter: 672 loss: 1.25663405e-06
Iter: 673 loss: 1.25523616e-06
Iter: 674 loss: 1.28105057e-06
Iter: 675 loss: 1.2552166e-06
Iter: 676 loss: 1.25405018e-06
Iter: 677 loss: 1.26365353e-06
Iter: 678 loss: 1.25397753e-06
Iter: 679 loss: 1.25366148e-06
Iter: 680 loss: 1.25348333e-06
Iter: 681 loss: 1.25306849e-06
Iter: 682 loss: 1.25251961e-06
Iter: 683 loss: 1.25245765e-06
Iter: 684 loss: 1.2517678e-06
Iter: 685 loss: 1.2532862e-06
Iter: 686 loss: 1.25153679e-06
Iter: 687 loss: 1.25068368e-06
Iter: 688 loss: 1.25262818e-06
Iter: 689 loss: 1.25046427e-06
Iter: 690 loss: 1.249859e-06
Iter: 691 loss: 1.24887833e-06
Iter: 692 loss: 1.27110684e-06
Iter: 693 loss: 1.24880103e-06
Iter: 694 loss: 1.24778626e-06
Iter: 695 loss: 1.26060172e-06
Iter: 696 loss: 1.24775295e-06
Iter: 697 loss: 1.24691064e-06
Iter: 698 loss: 1.24732492e-06
Iter: 699 loss: 1.24637791e-06
Iter: 700 loss: 1.24513292e-06
Iter: 701 loss: 1.25607789e-06
Iter: 702 loss: 1.24509052e-06
Iter: 703 loss: 1.24445728e-06
Iter: 704 loss: 1.24730923e-06
Iter: 705 loss: 1.24443534e-06
Iter: 706 loss: 1.24375322e-06
Iter: 707 loss: 1.24292114e-06
Iter: 708 loss: 1.24285327e-06
Iter: 709 loss: 1.2418991e-06
Iter: 710 loss: 1.24431313e-06
Iter: 711 loss: 1.24151961e-06
Iter: 712 loss: 1.24054941e-06
Iter: 713 loss: 1.24290307e-06
Iter: 714 loss: 1.24026008e-06
Iter: 715 loss: 1.24041844e-06
Iter: 716 loss: 1.23996381e-06
Iter: 717 loss: 1.23973837e-06
Iter: 718 loss: 1.23906136e-06
Iter: 719 loss: 1.24325436e-06
Iter: 720 loss: 1.23891687e-06
Iter: 721 loss: 1.23821701e-06
Iter: 722 loss: 1.24312749e-06
Iter: 723 loss: 1.23817676e-06
Iter: 724 loss: 1.237692e-06
Iter: 725 loss: 1.2398682e-06
Iter: 726 loss: 1.23757832e-06
Iter: 727 loss: 1.23708594e-06
Iter: 728 loss: 1.23618e-06
Iter: 729 loss: 1.25118891e-06
Iter: 730 loss: 1.2361179e-06
Iter: 731 loss: 1.2350946e-06
Iter: 732 loss: 1.23850191e-06
Iter: 733 loss: 1.23480731e-06
Iter: 734 loss: 1.233906e-06
Iter: 735 loss: 1.24142377e-06
Iter: 736 loss: 1.23382245e-06
Iter: 737 loss: 1.2330745e-06
Iter: 738 loss: 1.23852601e-06
Iter: 739 loss: 1.23303289e-06
Iter: 740 loss: 1.23235679e-06
Iter: 741 loss: 1.23479685e-06
Iter: 742 loss: 1.23225072e-06
Iter: 743 loss: 1.23179461e-06
Iter: 744 loss: 1.23458301e-06
Iter: 745 loss: 1.23174414e-06
Iter: 746 loss: 1.23131645e-06
Iter: 747 loss: 1.23066638e-06
Iter: 748 loss: 1.2474618e-06
Iter: 749 loss: 1.23063842e-06
Iter: 750 loss: 1.23002e-06
Iter: 751 loss: 1.23324799e-06
Iter: 752 loss: 1.2298849e-06
Iter: 753 loss: 1.22958863e-06
Iter: 754 loss: 1.22951201e-06
Iter: 755 loss: 1.22919346e-06
Iter: 756 loss: 1.22893687e-06
Iter: 757 loss: 1.22884649e-06
Iter: 758 loss: 1.22851952e-06
Iter: 759 loss: 1.22773486e-06
Iter: 760 loss: 1.23761606e-06
Iter: 761 loss: 1.22763686e-06
Iter: 762 loss: 1.22709434e-06
Iter: 763 loss: 1.22704159e-06
Iter: 764 loss: 1.22650818e-06
Iter: 765 loss: 1.22774736e-06
Iter: 766 loss: 1.22635129e-06
Iter: 767 loss: 1.22591905e-06
Iter: 768 loss: 1.22626989e-06
Iter: 769 loss: 1.22565893e-06
Iter: 770 loss: 1.22514325e-06
Iter: 771 loss: 1.22440417e-06
Iter: 772 loss: 1.22444544e-06
Iter: 773 loss: 1.22380527e-06
Iter: 774 loss: 1.22374456e-06
Iter: 775 loss: 1.22332472e-06
Iter: 776 loss: 1.22675056e-06
Iter: 777 loss: 1.22326514e-06
Iter: 778 loss: 1.22273786e-06
Iter: 779 loss: 1.22288145e-06
Iter: 780 loss: 1.2223652e-06
Iter: 781 loss: 1.22198753e-06
Iter: 782 loss: 1.22136498e-06
Iter: 783 loss: 1.2213759e-06
Iter: 784 loss: 1.22080428e-06
Iter: 785 loss: 1.22079064e-06
Iter: 786 loss: 1.22043639e-06
Iter: 787 loss: 1.22149868e-06
Iter: 788 loss: 1.22041752e-06
Iter: 789 loss: 1.22002632e-06
Iter: 790 loss: 1.22009692e-06
Iter: 791 loss: 1.21974688e-06
Iter: 792 loss: 1.21928247e-06
Iter: 793 loss: 1.21947539e-06
Iter: 794 loss: 1.21901985e-06
Iter: 795 loss: 1.21845687e-06
Iter: 796 loss: 1.21776134e-06
Iter: 797 loss: 1.21775133e-06
Iter: 798 loss: 1.21701487e-06
Iter: 799 loss: 1.22632457e-06
Iter: 800 loss: 1.21695314e-06
Iter: 801 loss: 1.21620064e-06
Iter: 802 loss: 1.21782114e-06
Iter: 803 loss: 1.21598191e-06
Iter: 804 loss: 1.21534219e-06
Iter: 805 loss: 1.2220064e-06
Iter: 806 loss: 1.21526955e-06
Iter: 807 loss: 1.21491973e-06
Iter: 808 loss: 1.21413609e-06
Iter: 809 loss: 1.23005907e-06
Iter: 810 loss: 1.21412199e-06
Iter: 811 loss: 1.21347807e-06
Iter: 812 loss: 1.21860739e-06
Iter: 813 loss: 1.21342794e-06
Iter: 814 loss: 1.21286132e-06
Iter: 815 loss: 1.21872756e-06
Iter: 816 loss: 1.21278777e-06
Iter: 817 loss: 1.21249957e-06
Iter: 818 loss: 1.21249832e-06
Iter: 819 loss: 1.2121609e-06
Iter: 820 loss: 1.21178221e-06
Iter: 821 loss: 1.21590097e-06
Iter: 822 loss: 1.21177072e-06
Iter: 823 loss: 1.2113195e-06
Iter: 824 loss: 1.21128573e-06
Iter: 825 loss: 1.2110263e-06
Iter: 826 loss: 1.21052381e-06
Iter: 827 loss: 1.2129583e-06
Iter: 828 loss: 1.21050334e-06
Iter: 829 loss: 1.21009452e-06
Iter: 830 loss: 1.20922209e-06
Iter: 831 loss: 1.2183848e-06
Iter: 832 loss: 1.20908089e-06
Iter: 833 loss: 1.20828554e-06
Iter: 834 loss: 1.21871562e-06
Iter: 835 loss: 1.20831942e-06
Iter: 836 loss: 1.20770801e-06
Iter: 837 loss: 1.20672348e-06
Iter: 838 loss: 1.2067112e-06
Iter: 839 loss: 1.20632012e-06
Iter: 840 loss: 1.20620598e-06
Iter: 841 loss: 1.20572281e-06
Iter: 842 loss: 1.20714037e-06
Iter: 843 loss: 1.20564403e-06
Iter: 844 loss: 1.20520383e-06
Iter: 845 loss: 1.20481309e-06
Iter: 846 loss: 1.20470042e-06
Iter: 847 loss: 1.20404798e-06
Iter: 848 loss: 1.20613709e-06
Iter: 849 loss: 1.20391246e-06
Iter: 850 loss: 1.20319919e-06
Iter: 851 loss: 1.20887239e-06
Iter: 852 loss: 1.20314007e-06
Iter: 853 loss: 1.20279469e-06
Iter: 854 loss: 1.20207471e-06
Iter: 855 loss: 1.21642688e-06
Iter: 856 loss: 1.20200298e-06
Iter: 857 loss: 1.201749e-06
Iter: 858 loss: 1.20163429e-06
Iter: 859 loss: 1.20126742e-06
Iter: 860 loss: 1.20142658e-06
Iter: 861 loss: 1.20100094e-06
Iter: 862 loss: 1.20068546e-06
Iter: 863 loss: 1.20055188e-06
Iter: 864 loss: 1.20043774e-06
Iter: 865 loss: 1.19970696e-06
Iter: 866 loss: 1.20081859e-06
Iter: 867 loss: 1.19935839e-06
Iter: 868 loss: 1.19889546e-06
Iter: 869 loss: 1.19863012e-06
Iter: 870 loss: 1.19831975e-06
Iter: 871 loss: 1.19762149e-06
Iter: 872 loss: 1.19930928e-06
Iter: 873 loss: 1.19742731e-06
Iter: 874 loss: 1.1965426e-06
Iter: 875 loss: 1.20194545e-06
Iter: 876 loss: 1.19644596e-06
Iter: 877 loss: 1.1959944e-06
Iter: 878 loss: 1.19546621e-06
Iter: 879 loss: 1.19538151e-06
Iter: 880 loss: 1.19494621e-06
Iter: 881 loss: 1.19483639e-06
Iter: 882 loss: 1.19449646e-06
Iter: 883 loss: 1.19439551e-06
Iter: 884 loss: 1.19423e-06
Iter: 885 loss: 1.19377933e-06
Iter: 886 loss: 1.19691924e-06
Iter: 887 loss: 1.1937725e-06
Iter: 888 loss: 1.19332333e-06
Iter: 889 loss: 1.19398771e-06
Iter: 890 loss: 1.19305525e-06
Iter: 891 loss: 1.19281776e-06
Iter: 892 loss: 1.19344918e-06
Iter: 893 loss: 1.19271056e-06
Iter: 894 loss: 1.19221011e-06
Iter: 895 loss: 1.19358845e-06
Iter: 896 loss: 1.19203673e-06
Iter: 897 loss: 1.19181368e-06
Iter: 898 loss: 1.19186791e-06
Iter: 899 loss: 1.19157528e-06
Iter: 900 loss: 1.19119159e-06
Iter: 901 loss: 1.19138508e-06
Iter: 902 loss: 1.19084984e-06
Iter: 903 loss: 1.19029983e-06
Iter: 904 loss: 1.19180447e-06
Iter: 905 loss: 1.19014612e-06
Iter: 906 loss: 1.18969797e-06
Iter: 907 loss: 1.18905962e-06
Iter: 908 loss: 1.18900732e-06
Iter: 909 loss: 1.18833532e-06
Iter: 910 loss: 1.19605897e-06
Iter: 911 loss: 1.1882529e-06
Iter: 912 loss: 1.18762068e-06
Iter: 913 loss: 1.18848448e-06
Iter: 914 loss: 1.18723574e-06
Iter: 915 loss: 1.18669277e-06
Iter: 916 loss: 1.1902423e-06
Iter: 917 loss: 1.18659432e-06
Iter: 918 loss: 1.18582625e-06
Iter: 919 loss: 1.1882712e-06
Iter: 920 loss: 1.18564822e-06
Iter: 921 loss: 1.18514458e-06
Iter: 922 loss: 1.18748039e-06
Iter: 923 loss: 1.18511514e-06
Iter: 924 loss: 1.18457388e-06
Iter: 925 loss: 1.18510241e-06
Iter: 926 loss: 1.18423827e-06
Iter: 927 loss: 1.18389266e-06
Iter: 928 loss: 1.18768116e-06
Iter: 929 loss: 1.18389244e-06
Iter: 930 loss: 1.18352136e-06
Iter: 931 loss: 1.1845666e-06
Iter: 932 loss: 1.18347225e-06
Iter: 933 loss: 1.18319156e-06
Iter: 934 loss: 1.18271976e-06
Iter: 935 loss: 1.19130186e-06
Iter: 936 loss: 1.182657e-06
Iter: 937 loss: 1.18232492e-06
Iter: 938 loss: 1.18231958e-06
Iter: 939 loss: 1.18206685e-06
Iter: 940 loss: 1.18139724e-06
Iter: 941 loss: 1.19178856e-06
Iter: 942 loss: 1.18134426e-06
Iter: 943 loss: 1.18071034e-06
Iter: 944 loss: 1.18442222e-06
Iter: 945 loss: 1.180612e-06
Iter: 946 loss: 1.18011587e-06
Iter: 947 loss: 1.18095727e-06
Iter: 948 loss: 1.17982972e-06
Iter: 949 loss: 1.17934383e-06
Iter: 950 loss: 1.17927061e-06
Iter: 951 loss: 1.17885452e-06
Iter: 952 loss: 1.17820287e-06
Iter: 953 loss: 1.18582045e-06
Iter: 954 loss: 1.1782239e-06
Iter: 955 loss: 1.17769218e-06
Iter: 956 loss: 1.18096432e-06
Iter: 957 loss: 1.17764807e-06
Iter: 958 loss: 1.17717673e-06
Iter: 959 loss: 1.17723926e-06
Iter: 960 loss: 1.17677132e-06
Iter: 961 loss: 1.17642105e-06
Iter: 962 loss: 1.17640866e-06
Iter: 963 loss: 1.17624847e-06
Iter: 964 loss: 1.17611739e-06
Iter: 965 loss: 1.17600371e-06
Iter: 966 loss: 1.1756415e-06
Iter: 967 loss: 1.17906234e-06
Iter: 968 loss: 1.17565912e-06
Iter: 969 loss: 1.17550792e-06
Iter: 970 loss: 1.17516663e-06
Iter: 971 loss: 1.18379819e-06
Iter: 972 loss: 1.17516163e-06
Iter: 973 loss: 1.17467437e-06
Iter: 974 loss: 1.17490595e-06
Iter: 975 loss: 1.17438083e-06
Iter: 976 loss: 1.17395359e-06
Iter: 977 loss: 1.17837271e-06
Iter: 978 loss: 1.17400293e-06
Iter: 979 loss: 1.17353318e-06
Iter: 980 loss: 1.17289346e-06
Iter: 981 loss: 1.17285629e-06
Iter: 982 loss: 1.172201e-06
Iter: 983 loss: 1.17383502e-06
Iter: 984 loss: 1.17203217e-06
Iter: 985 loss: 1.17131458e-06
Iter: 986 loss: 1.17389288e-06
Iter: 987 loss: 1.17115428e-06
Iter: 988 loss: 1.1705248e-06
Iter: 989 loss: 1.17147545e-06
Iter: 990 loss: 1.17032437e-06
Iter: 991 loss: 1.16998649e-06
Iter: 992 loss: 1.16993544e-06
Iter: 993 loss: 1.16964907e-06
Iter: 994 loss: 1.16965589e-06
Iter: 995 loss: 1.16940248e-06
Iter: 996 loss: 1.16893921e-06
Iter: 997 loss: 1.17150284e-06
Iter: 998 loss: 1.16892363e-06
Iter: 999 loss: 1.16858439e-06
Iter: 1000 loss: 1.16947513e-06
Iter: 1001 loss: 1.16843842e-06
Iter: 1002 loss: 1.16809781e-06
Iter: 1003 loss: 1.17035643e-06
Iter: 1004 loss: 1.16806564e-06
Iter: 1005 loss: 1.16778654e-06
Iter: 1006 loss: 1.1670561e-06
Iter: 1007 loss: 1.17359036e-06
Iter: 1008 loss: 1.16698857e-06
Iter: 1009 loss: 1.16638876e-06
Iter: 1010 loss: 1.16870888e-06
Iter: 1011 loss: 1.16628439e-06
Iter: 1012 loss: 1.16563444e-06
Iter: 1013 loss: 1.16871365e-06
Iter: 1014 loss: 1.16552678e-06
Iter: 1015 loss: 1.16495949e-06
Iter: 1016 loss: 1.16936644e-06
Iter: 1017 loss: 1.16495585e-06
Iter: 1018 loss: 1.16467049e-06
Iter: 1019 loss: 1.16410615e-06
Iter: 1020 loss: 1.1773534e-06
Iter: 1021 loss: 1.16410831e-06
Iter: 1022 loss: 1.16344472e-06
Iter: 1023 loss: 1.16423894e-06
Iter: 1024 loss: 1.16308468e-06
Iter: 1025 loss: 1.16251431e-06
Iter: 1026 loss: 1.17088598e-06
Iter: 1027 loss: 1.1625234e-06
Iter: 1028 loss: 1.16199794e-06
Iter: 1029 loss: 1.1621579e-06
Iter: 1030 loss: 1.16174374e-06
Iter: 1031 loss: 1.16114688e-06
Iter: 1032 loss: 1.16113108e-06
Iter: 1033 loss: 1.1608081e-06
Iter: 1034 loss: 1.1611902e-06
Iter: 1035 loss: 1.16069191e-06
Iter: 1036 loss: 1.16024376e-06
Iter: 1037 loss: 1.16091417e-06
Iter: 1038 loss: 1.15999978e-06
Iter: 1039 loss: 1.15958869e-06
Iter: 1040 loss: 1.1622692e-06
Iter: 1041 loss: 1.15950627e-06
Iter: 1042 loss: 1.15916259e-06
Iter: 1043 loss: 1.15911416e-06
Iter: 1044 loss: 1.15889452e-06
Iter: 1045 loss: 1.15843e-06
Iter: 1046 loss: 1.15824093e-06
Iter: 1047 loss: 1.1580189e-06
Iter: 1048 loss: 1.15738931e-06
Iter: 1049 loss: 1.1578843e-06
Iter: 1050 loss: 1.15713112e-06
Iter: 1051 loss: 1.15651483e-06
Iter: 1052 loss: 1.16549313e-06
Iter: 1053 loss: 1.15653654e-06
Iter: 1054 loss: 1.15618275e-06
Iter: 1055 loss: 1.15771547e-06
Iter: 1056 loss: 1.15605621e-06
Iter: 1057 loss: 1.15562966e-06
Iter: 1058 loss: 1.15475382e-06
Iter: 1059 loss: 1.174982e-06
Iter: 1060 loss: 1.15474063e-06
Iter: 1061 loss: 1.15410796e-06
Iter: 1062 loss: 1.15686157e-06
Iter: 1063 loss: 1.15398484e-06
Iter: 1064 loss: 1.15340436e-06
Iter: 1065 loss: 1.15506782e-06
Iter: 1066 loss: 1.15319472e-06
Iter: 1067 loss: 1.15247121e-06
Iter: 1068 loss: 1.15441287e-06
Iter: 1069 loss: 1.15216108e-06
Iter: 1070 loss: 1.15204398e-06
Iter: 1071 loss: 1.15192688e-06
Iter: 1072 loss: 1.15169416e-06
Iter: 1073 loss: 1.15128046e-06
Iter: 1074 loss: 1.15126272e-06
Iter: 1075 loss: 1.15077876e-06
Iter: 1076 loss: 1.15812327e-06
Iter: 1077 loss: 1.15074545e-06
Iter: 1078 loss: 1.15055707e-06
Iter: 1079 loss: 1.1507567e-06
Iter: 1080 loss: 1.15045464e-06
Iter: 1081 loss: 1.15012608e-06
Iter: 1082 loss: 1.15007504e-06
Iter: 1083 loss: 1.14988779e-06
Iter: 1084 loss: 1.14944146e-06
Iter: 1085 loss: 1.14935938e-06
Iter: 1086 loss: 1.14903673e-06
Iter: 1087 loss: 1.1485597e-06
Iter: 1088 loss: 1.14930572e-06
Iter: 1089 loss: 1.1483678e-06
Iter: 1090 loss: 1.1479774e-06
Iter: 1091 loss: 1.14798468e-06
Iter: 1092 loss: 1.1476933e-06
Iter: 1093 loss: 1.14744614e-06
Iter: 1094 loss: 1.14734576e-06
Iter: 1095 loss: 1.1468004e-06
Iter: 1096 loss: 1.14787736e-06
Iter: 1097 loss: 1.14659497e-06
Iter: 1098 loss: 1.14604154e-06
Iter: 1099 loss: 1.14555223e-06
Iter: 1100 loss: 1.14544071e-06
Iter: 1101 loss: 1.14486761e-06
Iter: 1102 loss: 1.14483237e-06
Iter: 1103 loss: 1.14447425e-06
Iter: 1104 loss: 1.14719899e-06
Iter: 1105 loss: 1.14440104e-06
Iter: 1106 loss: 1.14396016e-06
Iter: 1107 loss: 1.14591626e-06
Iter: 1108 loss: 1.14390764e-06
Iter: 1109 loss: 1.14370152e-06
Iter: 1110 loss: 1.14532077e-06
Iter: 1111 loss: 1.1437221e-06
Iter: 1112 loss: 1.14350689e-06
Iter: 1113 loss: 1.14311808e-06
Iter: 1114 loss: 1.15017622e-06
Iter: 1115 loss: 1.14320528e-06
Iter: 1116 loss: 1.14280829e-06
Iter: 1117 loss: 1.14686964e-06
Iter: 1118 loss: 1.14278043e-06
Iter: 1119 loss: 1.14260524e-06
Iter: 1120 loss: 1.14218346e-06
Iter: 1121 loss: 1.14212435e-06
Iter: 1122 loss: 1.14166687e-06
Iter: 1123 loss: 1.14411228e-06
Iter: 1124 loss: 1.14158e-06
Iter: 1125 loss: 1.141133e-06
Iter: 1126 loss: 1.14142506e-06
Iter: 1127 loss: 1.14090699e-06
Iter: 1128 loss: 1.14026534e-06
Iter: 1129 loss: 1.14467389e-06
Iter: 1130 loss: 1.14023862e-06
Iter: 1131 loss: 1.1399394e-06
Iter: 1132 loss: 1.13925046e-06
Iter: 1133 loss: 1.15043986e-06
Iter: 1134 loss: 1.13933629e-06
Iter: 1135 loss: 1.13860506e-06
Iter: 1136 loss: 1.14563636e-06
Iter: 1137 loss: 1.13848364e-06
Iter: 1138 loss: 1.13810574e-06
Iter: 1139 loss: 1.13923409e-06
Iter: 1140 loss: 1.13789486e-06
Iter: 1141 loss: 1.13766373e-06
Iter: 1142 loss: 1.13763542e-06
Iter: 1143 loss: 1.13728493e-06
Iter: 1144 loss: 1.13730209e-06
Iter: 1145 loss: 1.13701446e-06
Iter: 1146 loss: 1.13666238e-06
Iter: 1147 loss: 1.14118745e-06
Iter: 1148 loss: 1.13665976e-06
Iter: 1149 loss: 1.13651629e-06
Iter: 1150 loss: 1.13650526e-06
Iter: 1151 loss: 1.13641727e-06
Iter: 1152 loss: 1.13607712e-06
Iter: 1153 loss: 1.13564874e-06
Iter: 1154 loss: 1.13560145e-06
Iter: 1155 loss: 1.13492104e-06
Iter: 1156 loss: 1.13780482e-06
Iter: 1157 loss: 1.13490523e-06
Iter: 1158 loss: 1.13442616e-06
Iter: 1159 loss: 1.13445572e-06
Iter: 1160 loss: 1.13404542e-06
Iter: 1161 loss: 1.13351791e-06
Iter: 1162 loss: 1.14119746e-06
Iter: 1163 loss: 1.13350916e-06
Iter: 1164 loss: 1.13318288e-06
Iter: 1165 loss: 1.13334897e-06
Iter: 1166 loss: 1.13294539e-06
Iter: 1167 loss: 1.13242686e-06
Iter: 1168 loss: 1.13254964e-06
Iter: 1169 loss: 1.13212525e-06
Iter: 1170 loss: 1.13149281e-06
Iter: 1171 loss: 1.13191277e-06
Iter: 1172 loss: 1.13109513e-06
Iter: 1173 loss: 1.13067813e-06
Iter: 1174 loss: 1.13061606e-06
Iter: 1175 loss: 1.13033582e-06
Iter: 1176 loss: 1.13316264e-06
Iter: 1177 loss: 1.13032797e-06
Iter: 1178 loss: 1.13003864e-06
Iter: 1179 loss: 1.1299544e-06
Iter: 1180 loss: 1.12978319e-06
Iter: 1181 loss: 1.12944758e-06
Iter: 1182 loss: 1.13246119e-06
Iter: 1183 loss: 1.12936061e-06
Iter: 1184 loss: 1.12920191e-06
Iter: 1185 loss: 1.12880332e-06
Iter: 1186 loss: 1.13700105e-06
Iter: 1187 loss: 1.1288513e-06
Iter: 1188 loss: 1.12812222e-06
Iter: 1189 loss: 1.13143574e-06
Iter: 1190 loss: 1.12808e-06
Iter: 1191 loss: 1.12770829e-06
Iter: 1192 loss: 1.12751752e-06
Iter: 1193 loss: 1.12740258e-06
Iter: 1194 loss: 1.12684086e-06
Iter: 1195 loss: 1.12862313e-06
Iter: 1196 loss: 1.12660405e-06
Iter: 1197 loss: 1.12611815e-06
Iter: 1198 loss: 1.12983525e-06
Iter: 1199 loss: 1.12613679e-06
Iter: 1200 loss: 1.12553073e-06
Iter: 1201 loss: 1.12581313e-06
Iter: 1202 loss: 1.12521138e-06
Iter: 1203 loss: 1.12467126e-06
Iter: 1204 loss: 1.124753e-06
Iter: 1205 loss: 1.12435873e-06
Iter: 1206 loss: 1.12363705e-06
Iter: 1207 loss: 1.12752946e-06
Iter: 1208 loss: 1.12355895e-06
Iter: 1209 loss: 1.12302155e-06
Iter: 1210 loss: 1.12388864e-06
Iter: 1211 loss: 1.12278758e-06
Iter: 1212 loss: 1.12223779e-06
Iter: 1213 loss: 1.12228315e-06
Iter: 1214 loss: 1.12198904e-06
Iter: 1215 loss: 1.12247881e-06
Iter: 1216 loss: 1.1218857e-06
Iter: 1217 loss: 1.12156079e-06
Iter: 1218 loss: 1.12151747e-06
Iter: 1219 loss: 1.12137218e-06
Iter: 1220 loss: 1.12090947e-06
Iter: 1221 loss: 1.1214654e-06
Iter: 1222 loss: 1.12078669e-06
Iter: 1223 loss: 1.12026964e-06
Iter: 1224 loss: 1.12208329e-06
Iter: 1225 loss: 1.12011026e-06
Iter: 1226 loss: 1.11964243e-06
Iter: 1227 loss: 1.11909128e-06
Iter: 1228 loss: 1.11906672e-06
Iter: 1229 loss: 1.11836312e-06
Iter: 1230 loss: 1.12028101e-06
Iter: 1231 loss: 1.11819395e-06
Iter: 1232 loss: 1.11734994e-06
Iter: 1233 loss: 1.12234773e-06
Iter: 1234 loss: 1.11720306e-06
Iter: 1235 loss: 1.11669146e-06
Iter: 1236 loss: 1.11835971e-06
Iter: 1237 loss: 1.11654708e-06
Iter: 1238 loss: 1.11598206e-06
Iter: 1239 loss: 1.1152049e-06
Iter: 1240 loss: 1.11515556e-06
Iter: 1241 loss: 1.11429381e-06
Iter: 1242 loss: 1.11949339e-06
Iter: 1243 loss: 1.1141401e-06
Iter: 1244 loss: 1.11404063e-06
Iter: 1245 loss: 1.11380211e-06
Iter: 1246 loss: 1.11352904e-06
Iter: 1247 loss: 1.11374379e-06
Iter: 1248 loss: 1.11335203e-06
Iter: 1249 loss: 1.11301927e-06
Iter: 1250 loss: 1.11374754e-06
Iter: 1251 loss: 1.11290342e-06
Iter: 1252 loss: 1.11254894e-06
Iter: 1253 loss: 1.1122155e-06
Iter: 1254 loss: 1.11208203e-06
Iter: 1255 loss: 1.11167913e-06
Iter: 1256 loss: 1.11344457e-06
Iter: 1257 loss: 1.11156328e-06
Iter: 1258 loss: 1.11093834e-06
Iter: 1259 loss: 1.11092982e-06
Iter: 1260 loss: 1.11046211e-06
Iter: 1261 loss: 1.10990288e-06
Iter: 1262 loss: 1.11157954e-06
Iter: 1263 loss: 1.10969495e-06
Iter: 1264 loss: 1.10905023e-06
Iter: 1265 loss: 1.10917e-06
Iter: 1266 loss: 1.1085607e-06
Iter: 1267 loss: 1.10821497e-06
Iter: 1268 loss: 1.1081105e-06
Iter: 1269 loss: 1.10784674e-06
Iter: 1270 loss: 1.10724113e-06
Iter: 1271 loss: 1.11620557e-06
Iter: 1272 loss: 1.1072284e-06
Iter: 1273 loss: 1.1065232e-06
Iter: 1274 loss: 1.10989038e-06
Iter: 1275 loss: 1.10634494e-06
Iter: 1276 loss: 1.10569022e-06
Iter: 1277 loss: 1.10713336e-06
Iter: 1278 loss: 1.10551196e-06
Iter: 1279 loss: 1.10554231e-06
Iter: 1280 loss: 1.10522797e-06
Iter: 1281 loss: 1.10499354e-06
Iter: 1282 loss: 1.10439032e-06
Iter: 1283 loss: 1.10926862e-06
Iter: 1284 loss: 1.10423923e-06
Iter: 1285 loss: 1.10361054e-06
Iter: 1286 loss: 1.103606e-06
Iter: 1287 loss: 1.1031949e-06
Iter: 1288 loss: 1.10330382e-06
Iter: 1289 loss: 1.10292524e-06
Iter: 1290 loss: 1.10242195e-06
Iter: 1291 loss: 1.10148585e-06
Iter: 1292 loss: 1.11933878e-06
Iter: 1293 loss: 1.10145299e-06
Iter: 1294 loss: 1.10086e-06
Iter: 1295 loss: 1.10071778e-06
Iter: 1296 loss: 1.10020869e-06
Iter: 1297 loss: 1.10009182e-06
Iter: 1298 loss: 1.09974371e-06
Iter: 1299 loss: 1.09903203e-06
Iter: 1300 loss: 1.10043175e-06
Iter: 1301 loss: 1.09886241e-06
Iter: 1302 loss: 1.09826374e-06
Iter: 1303 loss: 1.10397946e-06
Iter: 1304 loss: 1.09829011e-06
Iter: 1305 loss: 1.09780274e-06
Iter: 1306 loss: 1.09843108e-06
Iter: 1307 loss: 1.097574e-06
Iter: 1308 loss: 1.09722055e-06
Iter: 1309 loss: 1.09666541e-06
Iter: 1310 loss: 1.09663176e-06
Iter: 1311 loss: 1.09617167e-06
Iter: 1312 loss: 1.09609698e-06
Iter: 1313 loss: 1.09563086e-06
Iter: 1314 loss: 1.09789062e-06
Iter: 1315 loss: 1.0955016e-06
Iter: 1316 loss: 1.09530015e-06
Iter: 1317 loss: 1.09505515e-06
Iter: 1318 loss: 1.09495932e-06
Iter: 1319 loss: 1.0944043e-06
Iter: 1320 loss: 1.09636403e-06
Iter: 1321 loss: 1.09425798e-06
Iter: 1322 loss: 1.09384e-06
Iter: 1323 loss: 1.09351515e-06
Iter: 1324 loss: 1.09343648e-06
Iter: 1325 loss: 1.09269274e-06
Iter: 1326 loss: 1.09349219e-06
Iter: 1327 loss: 1.09227449e-06
Iter: 1328 loss: 1.09179393e-06
Iter: 1329 loss: 1.09176358e-06
Iter: 1330 loss: 1.09146708e-06
Iter: 1331 loss: 1.09089819e-06
Iter: 1332 loss: 1.10428425e-06
Iter: 1333 loss: 1.09089228e-06
Iter: 1334 loss: 1.09029e-06
Iter: 1335 loss: 1.09521363e-06
Iter: 1336 loss: 1.09024029e-06
Iter: 1337 loss: 1.08981385e-06
Iter: 1338 loss: 1.0921724e-06
Iter: 1339 loss: 1.0897661e-06
Iter: 1340 loss: 1.08933591e-06
Iter: 1341 loss: 1.08930612e-06
Iter: 1342 loss: 1.08894506e-06
Iter: 1343 loss: 1.08859501e-06
Iter: 1344 loss: 1.08864458e-06
Iter: 1345 loss: 1.08829681e-06
Iter: 1346 loss: 1.08823428e-06
Iter: 1347 loss: 1.08795587e-06
Iter: 1348 loss: 1.0878282e-06
Iter: 1349 loss: 1.08747611e-06
Iter: 1350 loss: 1.08750453e-06
Iter: 1351 loss: 1.08720587e-06
Iter: 1352 loss: 1.08747884e-06
Iter: 1353 loss: 1.08696304e-06
Iter: 1354 loss: 1.08635936e-06
Iter: 1355 loss: 1.08855443e-06
Iter: 1356 loss: 1.08619554e-06
Iter: 1357 loss: 1.08590825e-06
Iter: 1358 loss: 1.08551217e-06
Iter: 1359 loss: 1.08549159e-06
Iter: 1360 loss: 1.08483e-06
Iter: 1361 loss: 1.08835843e-06
Iter: 1362 loss: 1.08471818e-06
Iter: 1363 loss: 1.08412974e-06
Iter: 1364 loss: 1.08646441e-06
Iter: 1365 loss: 1.08402389e-06
Iter: 1366 loss: 1.08356016e-06
Iter: 1367 loss: 1.08371341e-06
Iter: 1368 loss: 1.08328857e-06
Iter: 1369 loss: 1.08287111e-06
Iter: 1370 loss: 1.08680706e-06
Iter: 1371 loss: 1.08289623e-06
Iter: 1372 loss: 1.08251879e-06
Iter: 1373 loss: 1.08310837e-06
Iter: 1374 loss: 1.08236179e-06
Iter: 1375 loss: 1.08203187e-06
Iter: 1376 loss: 1.08261668e-06
Iter: 1377 loss: 1.08189977e-06
Iter: 1378 loss: 1.08148811e-06
Iter: 1379 loss: 1.08098925e-06
Iter: 1380 loss: 1.08095844e-06
Iter: 1381 loss: 1.08173094e-06
Iter: 1382 loss: 1.0807621e-06
Iter: 1383 loss: 1.08057941e-06
Iter: 1384 loss: 1.0801291e-06
Iter: 1385 loss: 1.08225072e-06
Iter: 1386 loss: 1.07993242e-06
Iter: 1387 loss: 1.07954077e-06
Iter: 1388 loss: 1.07953622e-06
Iter: 1389 loss: 1.07916685e-06
Iter: 1390 loss: 1.07975495e-06
Iter: 1391 loss: 1.0789754e-06
Iter: 1392 loss: 1.07865094e-06
Iter: 1393 loss: 1.07855647e-06
Iter: 1394 loss: 1.07831704e-06
Iter: 1395 loss: 1.07801793e-06
Iter: 1396 loss: 1.0809116e-06
Iter: 1397 loss: 1.0779537e-06
Iter: 1398 loss: 1.07758387e-06
Iter: 1399 loss: 1.07771427e-06
Iter: 1400 loss: 1.07732865e-06
Iter: 1401 loss: 1.07700521e-06
Iter: 1402 loss: 1.07693927e-06
Iter: 1403 loss: 1.07674884e-06
Iter: 1404 loss: 1.07614335e-06
Iter: 1405 loss: 1.07977576e-06
Iter: 1406 loss: 1.07612482e-06
Iter: 1407 loss: 1.07577603e-06
Iter: 1408 loss: 1.07871369e-06
Iter: 1409 loss: 1.07572669e-06
Iter: 1410 loss: 1.07548078e-06
Iter: 1411 loss: 1.07490121e-06
Iter: 1412 loss: 1.08457198e-06
Iter: 1413 loss: 1.07483879e-06
Iter: 1414 loss: 1.07452206e-06
Iter: 1415 loss: 1.07451763e-06
Iter: 1416 loss: 1.07416406e-06
Iter: 1417 loss: 1.07537903e-06
Iter: 1418 loss: 1.07401513e-06
Iter: 1419 loss: 1.07378651e-06
Iter: 1420 loss: 1.07357778e-06
Iter: 1421 loss: 1.07349604e-06
Iter: 1422 loss: 1.07328844e-06
Iter: 1423 loss: 1.07326355e-06
Iter: 1424 loss: 1.0730804e-06
Iter: 1425 loss: 1.07319556e-06
Iter: 1426 loss: 1.0729425e-06
Iter: 1427 loss: 1.07274457e-06
Iter: 1428 loss: 1.07258268e-06
Iter: 1429 loss: 1.07252799e-06
Iter: 1430 loss: 1.07217215e-06
Iter: 1431 loss: 1.07311212e-06
Iter: 1432 loss: 1.07204596e-06
Iter: 1433 loss: 1.07164988e-06
Iter: 1434 loss: 1.07561266e-06
Iter: 1435 loss: 1.07163771e-06
Iter: 1436 loss: 1.07144285e-06
Iter: 1437 loss: 1.07112032e-06
Iter: 1438 loss: 1.07110645e-06
Iter: 1439 loss: 1.07061271e-06
Iter: 1440 loss: 1.07193819e-06
Iter: 1441 loss: 1.07053506e-06
Iter: 1442 loss: 1.07014762e-06
Iter: 1443 loss: 1.07461619e-06
Iter: 1444 loss: 1.07012056e-06
Iter: 1445 loss: 1.0698875e-06
Iter: 1446 loss: 1.07029825e-06
Iter: 1447 loss: 1.0697612e-06
Iter: 1448 loss: 1.0695353e-06
Iter: 1449 loss: 1.07071958e-06
Iter: 1450 loss: 1.06949972e-06
Iter: 1451 loss: 1.0692346e-06
Iter: 1452 loss: 1.06998698e-06
Iter: 1453 loss: 1.06909954e-06
Iter: 1454 loss: 1.06901462e-06
Iter: 1455 loss: 1.06861648e-06
Iter: 1456 loss: 1.07214191e-06
Iter: 1457 loss: 1.06864081e-06
Iter: 1458 loss: 1.0683766e-06
Iter: 1459 loss: 1.06834841e-06
Iter: 1460 loss: 1.06817561e-06
Iter: 1461 loss: 1.06818209e-06
Iter: 1462 loss: 1.06800167e-06
Iter: 1463 loss: 1.06774758e-06
Iter: 1464 loss: 1.06759956e-06
Iter: 1465 loss: 1.06745699e-06
Iter: 1466 loss: 1.06714174e-06
Iter: 1467 loss: 1.06710718e-06
Iter: 1468 loss: 1.06691164e-06
Iter: 1469 loss: 1.06653579e-06
Iter: 1470 loss: 1.06648235e-06
Iter: 1471 loss: 1.06631649e-06
Iter: 1472 loss: 1.06600487e-06
Iter: 1473 loss: 1.06601556e-06
Iter: 1474 loss: 1.06569541e-06
Iter: 1475 loss: 1.0663689e-06
Iter: 1476 loss: 1.06550874e-06
Iter: 1477 loss: 1.06510811e-06
Iter: 1478 loss: 1.06553091e-06
Iter: 1479 loss: 1.06496179e-06
Iter: 1480 loss: 1.06459447e-06
Iter: 1481 loss: 1.06456832e-06
Iter: 1482 loss: 1.06432856e-06
Iter: 1483 loss: 1.06620746e-06
Iter: 1484 loss: 1.06438483e-06
Iter: 1485 loss: 1.06420885e-06
Iter: 1486 loss: 1.06402592e-06
Iter: 1487 loss: 1.06397329e-06
Iter: 1488 loss: 1.06370783e-06
Iter: 1489 loss: 1.06428445e-06
Iter: 1490 loss: 1.06364018e-06
Iter: 1491 loss: 1.06343964e-06
Iter: 1492 loss: 1.06338689e-06
Iter: 1493 loss: 1.06324546e-06
Iter: 1494 loss: 1.06300615e-06
Iter: 1495 loss: 1.0629991e-06
Iter: 1496 loss: 1.06281573e-06
Iter: 1497 loss: 1.06260336e-06
Iter: 1498 loss: 1.06260086e-06
Iter: 1499 loss: 1.06223774e-06
Iter: 1500 loss: 1.06212872e-06
Iter: 1501 loss: 1.0619483e-06
Iter: 1502 loss: 1.06178209e-06
Iter: 1503 loss: 1.06171638e-06
Iter: 1504 loss: 1.06155017e-06
Iter: 1505 loss: 1.06117523e-06
Iter: 1506 loss: 1.06758193e-06
Iter: 1507 loss: 1.06119171e-06
Iter: 1508 loss: 1.06077584e-06
Iter: 1509 loss: 1.06142897e-06
Iter: 1510 loss: 1.06059974e-06
Iter: 1511 loss: 1.06023663e-06
Iter: 1512 loss: 1.06249138e-06
Iter: 1513 loss: 1.06015477e-06
Iter: 1514 loss: 1.05999993e-06
Iter: 1515 loss: 1.06300536e-06
Iter: 1516 loss: 1.05996833e-06
Iter: 1517 loss: 1.0597098e-06
Iter: 1518 loss: 1.06003449e-06
Iter: 1519 loss: 1.05962806e-06
Iter: 1520 loss: 1.05945583e-06
Iter: 1521 loss: 1.05999629e-06
Iter: 1522 loss: 1.05943218e-06
Iter: 1523 loss: 1.05925051e-06
Iter: 1524 loss: 1.05911977e-06
Iter: 1525 loss: 1.05910544e-06
Iter: 1526 loss: 1.05878189e-06
Iter: 1527 loss: 1.05992331e-06
Iter: 1528 loss: 1.05866945e-06
Iter: 1529 loss: 1.05846107e-06
Iter: 1530 loss: 1.05845197e-06
Iter: 1531 loss: 1.05837853e-06
Iter: 1532 loss: 1.05797881e-06
Iter: 1533 loss: 1.06001357e-06
Iter: 1534 loss: 1.05793754e-06
Iter: 1535 loss: 1.05746744e-06
Iter: 1536 loss: 1.06030711e-06
Iter: 1537 loss: 1.05747517e-06
Iter: 1538 loss: 1.05708295e-06
Iter: 1539 loss: 1.0595395e-06
Iter: 1540 loss: 1.05703214e-06
Iter: 1541 loss: 1.05673769e-06
Iter: 1542 loss: 1.05760671e-06
Iter: 1543 loss: 1.05671e-06
Iter: 1544 loss: 1.05647666e-06
Iter: 1545 loss: 1.05596393e-06
Iter: 1546 loss: 1.06262974e-06
Iter: 1547 loss: 1.0559454e-06
Iter: 1548 loss: 1.05560116e-06
Iter: 1549 loss: 1.05561583e-06
Iter: 1550 loss: 1.05539539e-06
Iter: 1551 loss: 1.05838024e-06
Iter: 1552 loss: 1.05540062e-06
Iter: 1553 loss: 1.05520382e-06
Iter: 1554 loss: 1.05526988e-06
Iter: 1555 loss: 1.05509287e-06
Iter: 1556 loss: 1.05485424e-06
Iter: 1557 loss: 1.05529705e-06
Iter: 1558 loss: 1.05485447e-06
Iter: 1559 loss: 1.0546712e-06
Iter: 1560 loss: 1.05451568e-06
Iter: 1561 loss: 1.0545275e-06
Iter: 1562 loss: 1.05430865e-06
Iter: 1563 loss: 1.05428353e-06
Iter: 1564 loss: 1.05414244e-06
Iter: 1565 loss: 1.05432537e-06
Iter: 1566 loss: 1.05407514e-06
Iter: 1567 loss: 1.05385971e-06
Iter: 1568 loss: 1.05369031e-06
Iter: 1569 loss: 1.05363677e-06
Iter: 1570 loss: 1.0533131e-06
Iter: 1571 loss: 1.05364666e-06
Iter: 1572 loss: 1.05316633e-06
Iter: 1573 loss: 1.05293213e-06
Iter: 1574 loss: 1.05292474e-06
Iter: 1575 loss: 1.05269862e-06
Iter: 1576 loss: 1.05275649e-06
Iter: 1577 loss: 1.05260665e-06
Iter: 1578 loss: 1.05237041e-06
Iter: 1579 loss: 1.05231607e-06
Iter: 1580 loss: 1.05212371e-06
Iter: 1581 loss: 1.05180811e-06
Iter: 1582 loss: 1.05227582e-06
Iter: 1583 loss: 1.05169397e-06
Iter: 1584 loss: 1.05152253e-06
Iter: 1585 loss: 1.0514832e-06
Iter: 1586 loss: 1.051303e-06
Iter: 1587 loss: 1.05120307e-06
Iter: 1588 loss: 1.05109348e-06
Iter: 1589 loss: 1.05095341e-06
Iter: 1590 loss: 1.05077061e-06
Iter: 1591 loss: 1.05068648e-06
Iter: 1592 loss: 1.05046206e-06
Iter: 1593 loss: 1.05387437e-06
Iter: 1594 loss: 1.0504898e-06
Iter: 1595 loss: 1.05024617e-06
Iter: 1596 loss: 1.04985236e-06
Iter: 1597 loss: 1.0498685e-06
Iter: 1598 loss: 1.04960191e-06
Iter: 1599 loss: 1.05139839e-06
Iter: 1600 loss: 1.04957985e-06
Iter: 1601 loss: 1.04930757e-06
Iter: 1602 loss: 1.0522399e-06
Iter: 1603 loss: 1.04928381e-06
Iter: 1604 loss: 1.04907326e-06
Iter: 1605 loss: 1.04898822e-06
Iter: 1606 loss: 1.0488784e-06
Iter: 1607 loss: 1.04864785e-06
Iter: 1608 loss: 1.04851438e-06
Iter: 1609 loss: 1.0484323e-06
Iter: 1610 loss: 1.04811909e-06
Iter: 1611 loss: 1.05286904e-06
Iter: 1612 loss: 1.04815069e-06
Iter: 1613 loss: 1.04785283e-06
Iter: 1614 loss: 1.04820117e-06
Iter: 1615 loss: 1.04777598e-06
Iter: 1616 loss: 1.047507e-06
Iter: 1617 loss: 1.04760159e-06
Iter: 1618 loss: 1.04738706e-06
Iter: 1619 loss: 1.04718117e-06
Iter: 1620 loss: 1.04824721e-06
Iter: 1621 loss: 1.04712808e-06
Iter: 1622 loss: 1.04680612e-06
Iter: 1623 loss: 1.04841922e-06
Iter: 1624 loss: 1.04670676e-06
Iter: 1625 loss: 1.04660967e-06
Iter: 1626 loss: 1.04635274e-06
Iter: 1627 loss: 1.05186768e-06
Iter: 1628 loss: 1.04638718e-06
Iter: 1629 loss: 1.04602441e-06
Iter: 1630 loss: 1.04614924e-06
Iter: 1631 loss: 1.04581477e-06
Iter: 1632 loss: 1.04540118e-06
Iter: 1633 loss: 1.04596734e-06
Iter: 1634 loss: 1.04522633e-06
Iter: 1635 loss: 1.04497667e-06
Iter: 1636 loss: 1.04489311e-06
Iter: 1637 loss: 1.04466881e-06
Iter: 1638 loss: 1.04516857e-06
Iter: 1639 loss: 1.04461947e-06
Iter: 1640 loss: 1.04439982e-06
Iter: 1641 loss: 1.04434798e-06
Iter: 1642 loss: 1.04421076e-06
Iter: 1643 loss: 1.0439652e-06
Iter: 1644 loss: 1.04693243e-06
Iter: 1645 loss: 1.04391984e-06
Iter: 1646 loss: 1.04376431e-06
Iter: 1647 loss: 1.043628e-06
Iter: 1648 loss: 1.04360265e-06
Iter: 1649 loss: 1.04339711e-06
Iter: 1650 loss: 1.04604749e-06
Iter: 1651 loss: 1.04341802e-06
Iter: 1652 loss: 1.043255e-06
Iter: 1653 loss: 1.04310482e-06
Iter: 1654 loss: 1.04299454e-06
Iter: 1655 loss: 1.04282458e-06
Iter: 1656 loss: 1.04281366e-06
Iter: 1657 loss: 1.04260812e-06
Iter: 1658 loss: 1.04289097e-06
Iter: 1659 loss: 1.04253581e-06
Iter: 1660 loss: 1.0423538e-06
Iter: 1661 loss: 1.04192816e-06
Iter: 1662 loss: 1.04644641e-06
Iter: 1663 loss: 1.04192577e-06
Iter: 1664 loss: 1.04155561e-06
Iter: 1665 loss: 1.04367086e-06
Iter: 1666 loss: 1.04146409e-06
Iter: 1667 loss: 1.04110131e-06
Iter: 1668 loss: 1.0426304e-06
Iter: 1669 loss: 1.04103219e-06
Iter: 1670 loss: 1.04085632e-06
Iter: 1671 loss: 1.04090873e-06
Iter: 1672 loss: 1.04077446e-06
Iter: 1673 loss: 1.04055425e-06
Iter: 1674 loss: 1.04050491e-06
Iter: 1675 loss: 1.04030823e-06
Iter: 1676 loss: 1.04100263e-06
Iter: 1677 loss: 1.04025025e-06
Iter: 1678 loss: 1.04003379e-06
Iter: 1679 loss: 1.04048593e-06
Iter: 1680 loss: 1.03988896e-06
Iter: 1681 loss: 1.03969e-06
Iter: 1682 loss: 1.03997968e-06
Iter: 1683 loss: 1.03959678e-06
Iter: 1684 loss: 1.03928687e-06
Iter: 1685 loss: 1.03903017e-06
Iter: 1686 loss: 1.0389939e-06
Iter: 1687 loss: 1.03865341e-06
Iter: 1688 loss: 1.04031278e-06
Iter: 1689 loss: 1.03848674e-06
Iter: 1690 loss: 1.03847697e-06
Iter: 1691 loss: 1.03829029e-06
Iter: 1692 loss: 1.03820537e-06
Iter: 1693 loss: 1.03810726e-06
Iter: 1694 loss: 1.03805587e-06
Iter: 1695 loss: 1.03780064e-06
Iter: 1696 loss: 1.03832713e-06
Iter: 1697 loss: 1.03775619e-06
Iter: 1698 loss: 1.03759135e-06
Iter: 1699 loss: 1.0376308e-06
Iter: 1700 loss: 1.03746811e-06
Iter: 1701 loss: 1.03729326e-06
Iter: 1702 loss: 1.03737591e-06
Iter: 1703 loss: 1.03713842e-06
Iter: 1704 loss: 1.03692378e-06
Iter: 1705 loss: 1.03841035e-06
Iter: 1706 loss: 1.03687114e-06
Iter: 1707 loss: 1.03668242e-06
Iter: 1708 loss: 1.0390537e-06
Iter: 1709 loss: 1.03667196e-06
Iter: 1710 loss: 1.03657089e-06
Iter: 1711 loss: 1.0362171e-06
Iter: 1712 loss: 1.03898958e-06
Iter: 1713 loss: 1.03612319e-06
Iter: 1714 loss: 1.03582988e-06
Iter: 1715 loss: 1.03831576e-06
Iter: 1716 loss: 1.03584603e-06
Iter: 1717 loss: 1.0354488e-06
Iter: 1718 loss: 1.03655589e-06
Iter: 1719 loss: 1.03535763e-06
Iter: 1720 loss: 1.03506113e-06
Iter: 1721 loss: 1.03579032e-06
Iter: 1722 loss: 1.03502111e-06
Iter: 1723 loss: 1.03476475e-06
Iter: 1724 loss: 1.03456205e-06
Iter: 1725 loss: 1.03449e-06
Iter: 1726 loss: 1.03456273e-06
Iter: 1727 loss: 1.03434138e-06
Iter: 1728 loss: 1.0342419e-06
Iter: 1729 loss: 1.03408217e-06
Iter: 1730 loss: 1.03403431e-06
Iter: 1731 loss: 1.03387242e-06
Iter: 1732 loss: 1.03461571e-06
Iter: 1733 loss: 1.03378397e-06
Iter: 1734 loss: 1.03363186e-06
Iter: 1735 loss: 1.03386242e-06
Iter: 1736 loss: 1.03354023e-06
Iter: 1737 loss: 1.03336015e-06
Iter: 1738 loss: 1.03325601e-06
Iter: 1739 loss: 1.03312163e-06
Iter: 1740 loss: 1.03293587e-06
Iter: 1741 loss: 1.03419052e-06
Iter: 1742 loss: 1.03290245e-06
Iter: 1743 loss: 1.03264006e-06
Iter: 1744 loss: 1.03448701e-06
Iter: 1745 loss: 1.03265552e-06
Iter: 1746 loss: 1.03243599e-06
Iter: 1747 loss: 1.03221555e-06
Iter: 1748 loss: 1.0321437e-06
Iter: 1749 loss: 1.03189848e-06
Iter: 1750 loss: 1.03169441e-06
Iter: 1751 loss: 1.03156685e-06
Iter: 1752 loss: 1.03130037e-06
Iter: 1753 loss: 1.03126581e-06
Iter: 1754 loss: 1.03097341e-06
Iter: 1755 loss: 1.03073342e-06
Iter: 1756 loss: 1.03072091e-06
Iter: 1757 loss: 1.0303022e-06
Iter: 1758 loss: 1.03101593e-06
Iter: 1759 loss: 1.03013e-06
Iter: 1760 loss: 1.02987644e-06
Iter: 1761 loss: 1.02988372e-06
Iter: 1762 loss: 1.02955482e-06
Iter: 1763 loss: 1.02937815e-06
Iter: 1764 loss: 1.02923377e-06
Iter: 1765 loss: 1.02903971e-06
Iter: 1766 loss: 1.02949025e-06
Iter: 1767 loss: 1.02902641e-06
Iter: 1768 loss: 1.02864624e-06
Iter: 1769 loss: 1.0294616e-06
Iter: 1770 loss: 1.02859735e-06
Iter: 1771 loss: 1.02831e-06
Iter: 1772 loss: 1.02852755e-06
Iter: 1773 loss: 1.02819354e-06
Iter: 1774 loss: 1.02785179e-06
Iter: 1775 loss: 1.02800277e-06
Iter: 1776 loss: 1.02765603e-06
Iter: 1777 loss: 1.02758418e-06
Iter: 1778 loss: 1.02749186e-06
Iter: 1779 loss: 1.02733054e-06
Iter: 1780 loss: 1.02699505e-06
Iter: 1781 loss: 1.02928777e-06
Iter: 1782 loss: 1.02687022e-06
Iter: 1783 loss: 1.02647073e-06
Iter: 1784 loss: 1.0299924e-06
Iter: 1785 loss: 1.02646891e-06
Iter: 1786 loss: 1.026213e-06
Iter: 1787 loss: 1.02776653e-06
Iter: 1788 loss: 1.02620788e-06
Iter: 1789 loss: 1.02586955e-06
Iter: 1790 loss: 1.02654076e-06
Iter: 1791 loss: 1.02581771e-06
Iter: 1792 loss: 1.02562581e-06
Iter: 1793 loss: 1.02530771e-06
Iter: 1794 loss: 1.0252935e-06
Iter: 1795 loss: 1.02533249e-06
Iter: 1796 loss: 1.02504987e-06
Iter: 1797 loss: 1.02491924e-06
Iter: 1798 loss: 1.02475428e-06
Iter: 1799 loss: 1.02472791e-06
Iter: 1800 loss: 1.024433e-06
Iter: 1801 loss: 1.02449599e-06
Iter: 1802 loss: 1.02423e-06
Iter: 1803 loss: 1.02388935e-06
Iter: 1804 loss: 1.02388935e-06
Iter: 1805 loss: 1.02369859e-06
Iter: 1806 loss: 1.02327431e-06
Iter: 1807 loss: 1.0327808e-06
Iter: 1808 loss: 1.02326271e-06
Iter: 1809 loss: 1.02291278e-06
Iter: 1810 loss: 1.02295189e-06
Iter: 1811 loss: 1.02275021e-06
Iter: 1812 loss: 1.02407853e-06
Iter: 1813 loss: 1.02269087e-06
Iter: 1814 loss: 1.02248555e-06
Iter: 1815 loss: 1.0220358e-06
Iter: 1816 loss: 1.02690842e-06
Iter: 1817 loss: 1.02196395e-06
Iter: 1818 loss: 1.0215781e-06
Iter: 1819 loss: 1.0241888e-06
Iter: 1820 loss: 1.02152467e-06
Iter: 1821 loss: 1.02114541e-06
Iter: 1822 loss: 1.02280342e-06
Iter: 1823 loss: 1.02108447e-06
Iter: 1824 loss: 1.02074978e-06
Iter: 1825 loss: 1.02376316e-06
Iter: 1826 loss: 1.02076126e-06
Iter: 1827 loss: 1.02057493e-06
Iter: 1828 loss: 1.02012962e-06
Iter: 1829 loss: 1.02700108e-06
Iter: 1830 loss: 1.02012541e-06
Iter: 1831 loss: 1.02019249e-06
Iter: 1832 loss: 1.01999626e-06
Iter: 1833 loss: 1.01981891e-06
Iter: 1834 loss: 1.0194891e-06
Iter: 1835 loss: 1.01948376e-06
Iter: 1836 loss: 1.01923604e-06
Iter: 1837 loss: 1.02004242e-06
Iter: 1838 loss: 1.01914691e-06
Iter: 1839 loss: 1.01892556e-06
Iter: 1840 loss: 1.01950627e-06
Iter: 1841 loss: 1.01880289e-06
Iter: 1842 loss: 1.01849105e-06
Iter: 1843 loss: 1.01869091e-06
Iter: 1844 loss: 1.01831597e-06
Iter: 1845 loss: 1.01806893e-06
Iter: 1846 loss: 1.02005333e-06
Iter: 1847 loss: 1.01803676e-06
Iter: 1848 loss: 1.01782143e-06
Iter: 1849 loss: 1.01772412e-06
Iter: 1850 loss: 1.01762464e-06
Iter: 1851 loss: 1.01730325e-06
Iter: 1852 loss: 1.01787714e-06
Iter: 1853 loss: 1.01717956e-06
Iter: 1854 loss: 1.01679086e-06
Iter: 1855 loss: 1.01678472e-06
Iter: 1856 loss: 1.01652404e-06
Iter: 1857 loss: 1.01616104e-06
Iter: 1858 loss: 1.01617161e-06
Iter: 1859 loss: 1.01583225e-06
Iter: 1860 loss: 1.01591013e-06
Iter: 1861 loss: 1.01562932e-06
Iter: 1862 loss: 1.01521039e-06
Iter: 1863 loss: 1.01644582e-06
Iter: 1864 loss: 1.01508545e-06
Iter: 1865 loss: 1.01487296e-06
Iter: 1866 loss: 1.01488899e-06
Iter: 1867 loss: 1.01472483e-06
Iter: 1868 loss: 1.01432511e-06
Iter: 1869 loss: 1.02079969e-06
Iter: 1870 loss: 1.01430351e-06
Iter: 1871 loss: 1.01404748e-06
Iter: 1872 loss: 1.01446699e-06
Iter: 1873 loss: 1.01386036e-06
Iter: 1874 loss: 1.01354726e-06
Iter: 1875 loss: 1.01471301e-06
Iter: 1876 loss: 1.01345404e-06
Iter: 1877 loss: 1.01309945e-06
Iter: 1878 loss: 1.01593082e-06
Iter: 1879 loss: 1.01314095e-06
Iter: 1880 loss: 1.01297508e-06
Iter: 1881 loss: 1.01271155e-06
Iter: 1882 loss: 1.01270439e-06
Iter: 1883 loss: 1.01248634e-06
Iter: 1884 loss: 1.01242176e-06
Iter: 1885 loss: 1.01227022e-06
Iter: 1886 loss: 1.01209685e-06
Iter: 1887 loss: 1.01204432e-06
Iter: 1888 loss: 1.01173225e-06
Iter: 1889 loss: 1.01178557e-06
Iter: 1890 loss: 1.01146929e-06
Iter: 1891 loss: 1.01120372e-06
Iter: 1892 loss: 1.01400099e-06
Iter: 1893 loss: 1.01117371e-06
Iter: 1894 loss: 1.01081514e-06
Iter: 1895 loss: 1.0111969e-06
Iter: 1896 loss: 1.01064472e-06
Iter: 1897 loss: 1.01043338e-06
Iter: 1898 loss: 1.01136925e-06
Iter: 1899 loss: 1.01036312e-06
Iter: 1900 loss: 1.01019873e-06
Iter: 1901 loss: 1.01017622e-06
Iter: 1902 loss: 1.01003741e-06
Iter: 1903 loss: 1.00955879e-06
Iter: 1904 loss: 1.01481692e-06
Iter: 1905 loss: 1.00953343e-06
Iter: 1906 loss: 1.00928344e-06
Iter: 1907 loss: 1.00926343e-06
Iter: 1908 loss: 1.00907585e-06
Iter: 1909 loss: 1.00858642e-06
Iter: 1910 loss: 1.01244e-06
Iter: 1911 loss: 1.00857426e-06
Iter: 1912 loss: 1.00822513e-06
Iter: 1913 loss: 1.00920727e-06
Iter: 1914 loss: 1.00815748e-06
Iter: 1915 loss: 1.00790032e-06
Iter: 1916 loss: 1.00819727e-06
Iter: 1917 loss: 1.00775924e-06
Iter: 1918 loss: 1.00751845e-06
Iter: 1919 loss: 1.00754e-06
Iter: 1920 loss: 1.00743421e-06
Iter: 1921 loss: 1.00714669e-06
Iter: 1922 loss: 1.01267813e-06
Iter: 1923 loss: 1.00716579e-06
Iter: 1924 loss: 1.00692216e-06
Iter: 1925 loss: 1.00839497e-06
Iter: 1926 loss: 1.00688328e-06
Iter: 1927 loss: 1.00662362e-06
Iter: 1928 loss: 1.00721445e-06
Iter: 1929 loss: 1.0065454e-06
Iter: 1930 loss: 1.00628e-06
Iter: 1931 loss: 1.00645582e-06
Iter: 1932 loss: 1.00605905e-06
Iter: 1933 loss: 1.00591024e-06
Iter: 1934 loss: 1.00859643e-06
Iter: 1935 loss: 1.00588647e-06
Iter: 1936 loss: 1.0056367e-06
Iter: 1937 loss: 1.00611123e-06
Iter: 1938 loss: 1.00556758e-06
Iter: 1939 loss: 1.00544946e-06
Iter: 1940 loss: 1.00504235e-06
Iter: 1941 loss: 1.01239618e-06
Iter: 1942 loss: 1.00503803e-06
Iter: 1943 loss: 1.00462648e-06
Iter: 1944 loss: 1.00492673e-06
Iter: 1945 loss: 1.00441991e-06
Iter: 1946 loss: 1.00406419e-06
Iter: 1947 loss: 1.00402826e-06
Iter: 1948 loss: 1.00385807e-06
Iter: 1949 loss: 1.00362047e-06
Iter: 1950 loss: 1.00359796e-06
Iter: 1951 loss: 1.00327918e-06
Iter: 1952 loss: 1.00741386e-06
Iter: 1953 loss: 1.00328418e-06
Iter: 1954 loss: 1.00313741e-06
Iter: 1955 loss: 1.0037221e-06
Iter: 1956 loss: 1.00304101e-06
Iter: 1957 loss: 1.00283137e-06
Iter: 1958 loss: 1.00248121e-06
Iter: 1959 loss: 1.00896364e-06
Iter: 1960 loss: 1.00246382e-06
Iter: 1961 loss: 1.00221814e-06
Iter: 1962 loss: 1.00226578e-06
Iter: 1963 loss: 1.00198554e-06
Iter: 1964 loss: 1.00194939e-06
Iter: 1965 loss: 1.0017925e-06
Iter: 1966 loss: 1.00145439e-06
Iter: 1967 loss: 1.00258217e-06
Iter: 1968 loss: 1.00135662e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.6/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi2
+ date
Sun Nov  8 06:57:38 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.6/300_100_100_100_1 --function f1 --psi -1 --phi 2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee57d12048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee57c62d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee57d1e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee57c81e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee57ca02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee57ca0ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee57b422f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee57b4e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee57bdd158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee57bdd1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee57bfb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee5209df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee520bd7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee57b25d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee52131d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee521458c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee52145620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee5201a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee51fff950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee51febf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee52070b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee52065950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee51f82488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee52103b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee520e5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee520d8d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee51ee0048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee51ecd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee51ecd158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee51f9e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee51eb07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee51ec07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee51ec0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee51de17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee51e84ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee51e30510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.007937533
test_loss: 0.008479824
train_loss: 0.0059864745
test_loss: 0.006333611
train_loss: 0.005423696
test_loss: 0.006021832
train_loss: 0.0047551002
test_loss: 0.0061442275
train_loss: 0.0046828594
test_loss: 0.0052420557
train_loss: 0.0043465593
test_loss: 0.0049765203
train_loss: 0.005036738
test_loss: 0.005786724
train_loss: 0.0048799664
test_loss: 0.0052341884
train_loss: 0.0046814894
test_loss: 0.004779572
train_loss: 0.0044570006
test_loss: 0.004817073
train_loss: 0.0043887678
test_loss: 0.004843222
train_loss: 0.0043897913
test_loss: 0.005202097
train_loss: 0.0044428613
test_loss: 0.0054428293
train_loss: 0.004135307
test_loss: 0.0046505015
train_loss: 0.0041030757
test_loss: 0.0048440257
train_loss: 0.0037910896
test_loss: 0.004634185
train_loss: 0.004234464
test_loss: 0.004869782
train_loss: 0.0044967174
test_loss: 0.0049592345
train_loss: 0.004171848
test_loss: 0.0049044183
train_loss: 0.00448887
test_loss: 0.005509076
train_loss: 0.0038083948
test_loss: 0.0047194995
train_loss: 0.004349702
test_loss: 0.004667237
train_loss: 0.0037749931
test_loss: 0.004582492
train_loss: 0.0039616553
test_loss: 0.0047357944
train_loss: 0.0038435464
test_loss: 0.0046968856
train_loss: 0.004202859
test_loss: 0.0048878817
train_loss: 0.0040980624
test_loss: 0.004654473
train_loss: 0.0044670124
test_loss: 0.0049777813
train_loss: 0.0039204448
test_loss: 0.004456646
train_loss: 0.003942724
test_loss: 0.004827615
train_loss: 0.0037610815
test_loss: 0.004604652
train_loss: 0.003963705
test_loss: 0.004665803
train_loss: 0.0038840552
test_loss: 0.0048231827
train_loss: 0.004289578
test_loss: 0.004682681
train_loss: 0.003791656
test_loss: 0.004555191
train_loss: 0.0039915657
test_loss: 0.0046847463
train_loss: 0.003689548
test_loss: 0.0046934756
train_loss: 0.004035661
test_loss: 0.0049625784
train_loss: 0.0037089586
test_loss: 0.004617743
train_loss: 0.0038344893
test_loss: 0.0046688626
train_loss: 0.0036637208
test_loss: 0.004540374
train_loss: 0.0041595846
test_loss: 0.0047542523
train_loss: 0.0039509847
test_loss: 0.0044981353
train_loss: 0.0036887282
test_loss: 0.0046847276
train_loss: 0.0036448292
test_loss: 0.0046725012
train_loss: 0.0037200789
test_loss: 0.0048185806
train_loss: 0.0041145976
test_loss: 0.0045873425
train_loss: 0.003813718
test_loss: 0.0043920586
train_loss: 0.004162562
test_loss: 0.004915606
train_loss: 0.004112808
test_loss: 0.0047639958
train_loss: 0.003519434
test_loss: 0.0043878327
train_loss: 0.003368368
test_loss: 0.0043461705
train_loss: 0.00380077
test_loss: 0.004707303
train_loss: 0.0036525293
test_loss: 0.004630791
train_loss: 0.003464167
test_loss: 0.0045309695
train_loss: 0.0036098051
test_loss: 0.0045016655
train_loss: 0.0038568832
test_loss: 0.0043742433
train_loss: 0.0043277284
test_loss: 0.0045650336
train_loss: 0.0039060386
test_loss: 0.0046648732
train_loss: 0.0038247553
test_loss: 0.0047589024
train_loss: 0.0033211333
test_loss: 0.0043429825
train_loss: 0.0039344537
test_loss: 0.004541558
train_loss: 0.0038191616
test_loss: 0.004932032
train_loss: 0.0038013468
test_loss: 0.004408787
train_loss: 0.0035404654
test_loss: 0.0043050447
train_loss: 0.003388268
test_loss: 0.004459724
train_loss: 0.00398994
test_loss: 0.004872756
train_loss: 0.0034217378
test_loss: 0.004258644
train_loss: 0.0040257536
test_loss: 0.004616006
train_loss: 0.0034791636
test_loss: 0.004354119
train_loss: 0.0036612381
test_loss: 0.004434945
train_loss: 0.003819482
test_loss: 0.0043010185
train_loss: 0.0035786019
test_loss: 0.004541371
train_loss: 0.0038860603
test_loss: 0.004911749
train_loss: 0.0040031844
test_loss: 0.004369895
train_loss: 0.0041256375
test_loss: 0.004747489
train_loss: 0.003694696
test_loss: 0.0044928486
train_loss: 0.003580001
test_loss: 0.0048526074
train_loss: 0.0036143437
test_loss: 0.00457427
train_loss: 0.0033817776
test_loss: 0.004449835
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -1 --phi 2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi2/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd81a8268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd825a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd81d8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd8212d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd820c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd820c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd8122d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd80df730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd80ff2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd80b5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd80b5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd8073f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd8080620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd801ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd801bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd801b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd16a3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd1666840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd16337b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd16172f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd15c26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd1578598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd157d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd157dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd1578950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd157d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd14bc8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd14d3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd14d3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd14d3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd1447268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd13f01e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd13f0158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd141d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd13b2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9fd1380620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.24020514e-05
Iter: 2 loss: 3.77343458e-05
Iter: 3 loss: 1.97034042e-05
Iter: 4 loss: 1.83969114e-05
Iter: 5 loss: 2.65381077e-05
Iter: 6 loss: 1.82420845e-05
Iter: 7 loss: 1.76582926e-05
Iter: 8 loss: 1.66899936e-05
Iter: 9 loss: 1.66864138e-05
Iter: 10 loss: 1.54372465e-05
Iter: 11 loss: 1.87603036e-05
Iter: 12 loss: 1.50215192e-05
Iter: 13 loss: 1.44143642e-05
Iter: 14 loss: 1.44071755e-05
Iter: 15 loss: 1.39852109e-05
Iter: 16 loss: 1.30521057e-05
Iter: 17 loss: 2.64747123e-05
Iter: 18 loss: 1.30072831e-05
Iter: 19 loss: 1.21699195e-05
Iter: 20 loss: 2.01822113e-05
Iter: 21 loss: 1.2137265e-05
Iter: 22 loss: 1.14414033e-05
Iter: 23 loss: 1.16800693e-05
Iter: 24 loss: 1.09508119e-05
Iter: 25 loss: 1.02679269e-05
Iter: 26 loss: 2.08128695e-05
Iter: 27 loss: 1.02676822e-05
Iter: 28 loss: 9.98500946e-06
Iter: 29 loss: 9.51045331e-06
Iter: 30 loss: 9.50887443e-06
Iter: 31 loss: 8.97389691e-06
Iter: 32 loss: 1.27215453e-05
Iter: 33 loss: 8.92478602e-06
Iter: 34 loss: 8.47630781e-06
Iter: 35 loss: 9.62846389e-06
Iter: 36 loss: 8.3223822e-06
Iter: 37 loss: 8.16392276e-06
Iter: 38 loss: 8.14089617e-06
Iter: 39 loss: 7.91991e-06
Iter: 40 loss: 8.12739836e-06
Iter: 41 loss: 7.79333368e-06
Iter: 42 loss: 7.66100493e-06
Iter: 43 loss: 7.57705811e-06
Iter: 44 loss: 7.5255175e-06
Iter: 45 loss: 7.33669731e-06
Iter: 46 loss: 8.54142399e-06
Iter: 47 loss: 7.31571072e-06
Iter: 48 loss: 7.14817679e-06
Iter: 49 loss: 7.3675883e-06
Iter: 50 loss: 7.06361243e-06
Iter: 51 loss: 6.86678413e-06
Iter: 52 loss: 7.67683832e-06
Iter: 53 loss: 6.82312111e-06
Iter: 54 loss: 6.63674746e-06
Iter: 55 loss: 6.41479937e-06
Iter: 56 loss: 6.39111522e-06
Iter: 57 loss: 6.15047156e-06
Iter: 58 loss: 7.3617739e-06
Iter: 59 loss: 6.10998586e-06
Iter: 60 loss: 5.91603111e-06
Iter: 61 loss: 6.59414445e-06
Iter: 62 loss: 5.86574879e-06
Iter: 63 loss: 5.74845126e-06
Iter: 64 loss: 6.62209e-06
Iter: 65 loss: 5.73879515e-06
Iter: 66 loss: 5.62022205e-06
Iter: 67 loss: 5.89184037e-06
Iter: 68 loss: 5.57631392e-06
Iter: 69 loss: 5.48207072e-06
Iter: 70 loss: 5.35836443e-06
Iter: 71 loss: 5.35073741e-06
Iter: 72 loss: 5.27975135e-06
Iter: 73 loss: 5.25935457e-06
Iter: 74 loss: 5.21366292e-06
Iter: 75 loss: 5.21126049e-06
Iter: 76 loss: 5.17810258e-06
Iter: 77 loss: 5.07465302e-06
Iter: 78 loss: 5.27100383e-06
Iter: 79 loss: 5.00760325e-06
Iter: 80 loss: 4.92921572e-06
Iter: 81 loss: 5.46969113e-06
Iter: 82 loss: 4.92144136e-06
Iter: 83 loss: 4.83519761e-06
Iter: 84 loss: 5.26782151e-06
Iter: 85 loss: 4.82086671e-06
Iter: 86 loss: 4.7404028e-06
Iter: 87 loss: 4.95836e-06
Iter: 88 loss: 4.71397061e-06
Iter: 89 loss: 4.65397488e-06
Iter: 90 loss: 4.68888538e-06
Iter: 91 loss: 4.61501395e-06
Iter: 92 loss: 4.52474887e-06
Iter: 93 loss: 4.88400474e-06
Iter: 94 loss: 4.50434482e-06
Iter: 95 loss: 4.45895148e-06
Iter: 96 loss: 4.41579505e-06
Iter: 97 loss: 4.40581425e-06
Iter: 98 loss: 4.31215449e-06
Iter: 99 loss: 4.60024512e-06
Iter: 100 loss: 4.28451767e-06
Iter: 101 loss: 4.21966934e-06
Iter: 102 loss: 4.54403471e-06
Iter: 103 loss: 4.20862307e-06
Iter: 104 loss: 4.15002205e-06
Iter: 105 loss: 4.41258771e-06
Iter: 106 loss: 4.13853559e-06
Iter: 107 loss: 4.07714606e-06
Iter: 108 loss: 4.1966573e-06
Iter: 109 loss: 4.05139417e-06
Iter: 110 loss: 4.01875286e-06
Iter: 111 loss: 4.3640639e-06
Iter: 112 loss: 4.01788839e-06
Iter: 113 loss: 3.99786359e-06
Iter: 114 loss: 3.99750388e-06
Iter: 115 loss: 3.9850097e-06
Iter: 116 loss: 3.94327799e-06
Iter: 117 loss: 3.9365973e-06
Iter: 118 loss: 3.89829438e-06
Iter: 119 loss: 3.8516132e-06
Iter: 120 loss: 4.54141355e-06
Iter: 121 loss: 3.85149951e-06
Iter: 122 loss: 3.81188784e-06
Iter: 123 loss: 4.03408239e-06
Iter: 124 loss: 3.80628126e-06
Iter: 125 loss: 3.7694283e-06
Iter: 126 loss: 3.89509933e-06
Iter: 127 loss: 3.75935088e-06
Iter: 128 loss: 3.73367288e-06
Iter: 129 loss: 3.69463828e-06
Iter: 130 loss: 3.69407303e-06
Iter: 131 loss: 3.65513779e-06
Iter: 132 loss: 3.65474352e-06
Iter: 133 loss: 3.63503864e-06
Iter: 134 loss: 3.59170326e-06
Iter: 135 loss: 4.23173196e-06
Iter: 136 loss: 3.59002365e-06
Iter: 137 loss: 3.53888072e-06
Iter: 138 loss: 4.06429854e-06
Iter: 139 loss: 3.53743508e-06
Iter: 140 loss: 3.51117865e-06
Iter: 141 loss: 3.48101548e-06
Iter: 142 loss: 3.47740774e-06
Iter: 143 loss: 3.4585114e-06
Iter: 144 loss: 3.45480521e-06
Iter: 145 loss: 3.43671127e-06
Iter: 146 loss: 3.47143805e-06
Iter: 147 loss: 3.42946282e-06
Iter: 148 loss: 3.41121245e-06
Iter: 149 loss: 3.41133682e-06
Iter: 150 loss: 3.40124416e-06
Iter: 151 loss: 3.38321252e-06
Iter: 152 loss: 3.80400365e-06
Iter: 153 loss: 3.3831393e-06
Iter: 154 loss: 3.36187622e-06
Iter: 155 loss: 3.32541276e-06
Iter: 156 loss: 3.32549e-06
Iter: 157 loss: 3.30501371e-06
Iter: 158 loss: 3.30099215e-06
Iter: 159 loss: 3.28112037e-06
Iter: 160 loss: 3.33886214e-06
Iter: 161 loss: 3.27495309e-06
Iter: 162 loss: 3.26122449e-06
Iter: 163 loss: 3.24769962e-06
Iter: 164 loss: 3.24484427e-06
Iter: 165 loss: 3.2162186e-06
Iter: 166 loss: 3.27723774e-06
Iter: 167 loss: 3.20528193e-06
Iter: 168 loss: 3.18013554e-06
Iter: 169 loss: 3.48389972e-06
Iter: 170 loss: 3.1798063e-06
Iter: 171 loss: 3.16774435e-06
Iter: 172 loss: 3.14359522e-06
Iter: 173 loss: 3.60832064e-06
Iter: 174 loss: 3.14323984e-06
Iter: 175 loss: 3.11602253e-06
Iter: 176 loss: 3.39274311e-06
Iter: 177 loss: 3.11545705e-06
Iter: 178 loss: 3.09771167e-06
Iter: 179 loss: 3.09502911e-06
Iter: 180 loss: 3.0827191e-06
Iter: 181 loss: 3.08101198e-06
Iter: 182 loss: 3.06996139e-06
Iter: 183 loss: 3.06045558e-06
Iter: 184 loss: 3.06148195e-06
Iter: 185 loss: 3.05301728e-06
Iter: 186 loss: 3.04239279e-06
Iter: 187 loss: 3.02255762e-06
Iter: 188 loss: 3.46450361e-06
Iter: 189 loss: 3.02228273e-06
Iter: 190 loss: 3.00056354e-06
Iter: 191 loss: 3.11804615e-06
Iter: 192 loss: 2.99736917e-06
Iter: 193 loss: 2.98487362e-06
Iter: 194 loss: 3.11584199e-06
Iter: 195 loss: 2.98469058e-06
Iter: 196 loss: 2.97143561e-06
Iter: 197 loss: 2.9678904e-06
Iter: 198 loss: 2.95964264e-06
Iter: 199 loss: 2.94767e-06
Iter: 200 loss: 2.96339249e-06
Iter: 201 loss: 2.94152369e-06
Iter: 202 loss: 2.92657523e-06
Iter: 203 loss: 2.99852627e-06
Iter: 204 loss: 2.92378627e-06
Iter: 205 loss: 2.91039078e-06
Iter: 206 loss: 2.94983852e-06
Iter: 207 loss: 2.90643311e-06
Iter: 208 loss: 2.89317495e-06
Iter: 209 loss: 2.88220531e-06
Iter: 210 loss: 2.87818352e-06
Iter: 211 loss: 2.86412478e-06
Iter: 212 loss: 2.96628605e-06
Iter: 213 loss: 2.86277827e-06
Iter: 214 loss: 2.84845896e-06
Iter: 215 loss: 2.88605224e-06
Iter: 216 loss: 2.84374892e-06
Iter: 217 loss: 2.83284658e-06
Iter: 218 loss: 2.83199e-06
Iter: 219 loss: 2.82824476e-06
Iter: 220 loss: 2.8217105e-06
Iter: 221 loss: 2.82173187e-06
Iter: 222 loss: 2.81206053e-06
Iter: 223 loss: 2.79774122e-06
Iter: 224 loss: 2.79730421e-06
Iter: 225 loss: 2.78740572e-06
Iter: 226 loss: 2.78735183e-06
Iter: 227 loss: 2.77962886e-06
Iter: 228 loss: 2.81513826e-06
Iter: 229 loss: 2.77822755e-06
Iter: 230 loss: 2.76992296e-06
Iter: 231 loss: 2.76002106e-06
Iter: 232 loss: 2.75899629e-06
Iter: 233 loss: 2.74738181e-06
Iter: 234 loss: 2.75872389e-06
Iter: 235 loss: 2.74093691e-06
Iter: 236 loss: 2.72544503e-06
Iter: 237 loss: 2.85469332e-06
Iter: 238 loss: 2.72452371e-06
Iter: 239 loss: 2.71434715e-06
Iter: 240 loss: 2.74068952e-06
Iter: 241 loss: 2.71112822e-06
Iter: 242 loss: 2.70176042e-06
Iter: 243 loss: 2.6992866e-06
Iter: 244 loss: 2.69359725e-06
Iter: 245 loss: 2.6820419e-06
Iter: 246 loss: 2.73244859e-06
Iter: 247 loss: 2.67962491e-06
Iter: 248 loss: 2.68022973e-06
Iter: 249 loss: 2.6755024e-06
Iter: 250 loss: 2.67136193e-06
Iter: 251 loss: 2.66445159e-06
Iter: 252 loss: 2.66435313e-06
Iter: 253 loss: 2.65721633e-06
Iter: 254 loss: 2.65569383e-06
Iter: 255 loss: 2.65102426e-06
Iter: 256 loss: 2.6416476e-06
Iter: 257 loss: 2.70336432e-06
Iter: 258 loss: 2.64086384e-06
Iter: 259 loss: 2.6342384e-06
Iter: 260 loss: 2.63806351e-06
Iter: 261 loss: 2.63005154e-06
Iter: 262 loss: 2.6226171e-06
Iter: 263 loss: 2.72674242e-06
Iter: 264 loss: 2.62245703e-06
Iter: 265 loss: 2.6180478e-06
Iter: 266 loss: 2.61301693e-06
Iter: 267 loss: 2.61249716e-06
Iter: 268 loss: 2.60302477e-06
Iter: 269 loss: 2.60317893e-06
Iter: 270 loss: 2.59569197e-06
Iter: 271 loss: 2.5881759e-06
Iter: 272 loss: 2.69536849e-06
Iter: 273 loss: 2.58835075e-06
Iter: 274 loss: 2.58024647e-06
Iter: 275 loss: 2.57110014e-06
Iter: 276 loss: 2.56977978e-06
Iter: 277 loss: 2.56037765e-06
Iter: 278 loss: 2.6607263e-06
Iter: 279 loss: 2.56005251e-06
Iter: 280 loss: 2.55318309e-06
Iter: 281 loss: 2.54446559e-06
Iter: 282 loss: 2.54375232e-06
Iter: 283 loss: 2.5497302e-06
Iter: 284 loss: 2.53845906e-06
Iter: 285 loss: 2.53629287e-06
Iter: 286 loss: 2.53077383e-06
Iter: 287 loss: 2.57336205e-06
Iter: 288 loss: 2.52984773e-06
Iter: 289 loss: 2.52247901e-06
Iter: 290 loss: 2.5178856e-06
Iter: 291 loss: 2.51512029e-06
Iter: 292 loss: 2.50844914e-06
Iter: 293 loss: 2.508139e-06
Iter: 294 loss: 2.50333187e-06
Iter: 295 loss: 2.50318431e-06
Iter: 296 loss: 2.49949017e-06
Iter: 297 loss: 2.49261848e-06
Iter: 298 loss: 2.56000931e-06
Iter: 299 loss: 2.49239179e-06
Iter: 300 loss: 2.48937863e-06
Iter: 301 loss: 2.48582e-06
Iter: 302 loss: 2.48544393e-06
Iter: 303 loss: 2.47854177e-06
Iter: 304 loss: 2.47719731e-06
Iter: 305 loss: 2.47249363e-06
Iter: 306 loss: 2.4661249e-06
Iter: 307 loss: 2.46619629e-06
Iter: 308 loss: 2.460833e-06
Iter: 309 loss: 2.45707e-06
Iter: 310 loss: 2.45524279e-06
Iter: 311 loss: 2.44831244e-06
Iter: 312 loss: 2.47674052e-06
Iter: 313 loss: 2.44669627e-06
Iter: 314 loss: 2.44294e-06
Iter: 315 loss: 2.44289549e-06
Iter: 316 loss: 2.43829027e-06
Iter: 317 loss: 2.44479497e-06
Iter: 318 loss: 2.4361766e-06
Iter: 319 loss: 2.43358727e-06
Iter: 320 loss: 2.42696592e-06
Iter: 321 loss: 2.48905189e-06
Iter: 322 loss: 2.42602914e-06
Iter: 323 loss: 2.41859129e-06
Iter: 324 loss: 2.47463549e-06
Iter: 325 loss: 2.41802354e-06
Iter: 326 loss: 2.41223097e-06
Iter: 327 loss: 2.43333898e-06
Iter: 328 loss: 2.41070597e-06
Iter: 329 loss: 2.40498935e-06
Iter: 330 loss: 2.43409e-06
Iter: 331 loss: 2.40415443e-06
Iter: 332 loss: 2.39957e-06
Iter: 333 loss: 2.42081092e-06
Iter: 334 loss: 2.39878045e-06
Iter: 335 loss: 2.394494e-06
Iter: 336 loss: 2.38890789e-06
Iter: 337 loss: 2.38854682e-06
Iter: 338 loss: 2.38213829e-06
Iter: 339 loss: 2.40642953e-06
Iter: 340 loss: 2.38080111e-06
Iter: 341 loss: 2.374e-06
Iter: 342 loss: 2.40408281e-06
Iter: 343 loss: 2.37291442e-06
Iter: 344 loss: 2.36787628e-06
Iter: 345 loss: 2.39679321e-06
Iter: 346 loss: 2.36709934e-06
Iter: 347 loss: 2.3629309e-06
Iter: 348 loss: 2.35735433e-06
Iter: 349 loss: 2.35698462e-06
Iter: 350 loss: 2.36654387e-06
Iter: 351 loss: 2.35526568e-06
Iter: 352 loss: 2.35397943e-06
Iter: 353 loss: 2.35002494e-06
Iter: 354 loss: 2.36935193e-06
Iter: 355 loss: 2.34878098e-06
Iter: 356 loss: 2.34321124e-06
Iter: 357 loss: 2.34261779e-06
Iter: 358 loss: 2.33861442e-06
Iter: 359 loss: 2.33195533e-06
Iter: 360 loss: 2.34419281e-06
Iter: 361 loss: 2.32903267e-06
Iter: 362 loss: 2.32475622e-06
Iter: 363 loss: 2.32417506e-06
Iter: 364 loss: 2.32163188e-06
Iter: 365 loss: 2.32374896e-06
Iter: 366 loss: 2.32001e-06
Iter: 367 loss: 2.31608146e-06
Iter: 368 loss: 2.32044727e-06
Iter: 369 loss: 2.31369813e-06
Iter: 370 loss: 2.31021704e-06
Iter: 371 loss: 2.32604179e-06
Iter: 372 loss: 2.30958744e-06
Iter: 373 loss: 2.3070611e-06
Iter: 374 loss: 2.3007542e-06
Iter: 375 loss: 2.37537756e-06
Iter: 376 loss: 2.30026967e-06
Iter: 377 loss: 2.29539523e-06
Iter: 378 loss: 2.29490752e-06
Iter: 379 loss: 2.29137049e-06
Iter: 380 loss: 2.29457692e-06
Iter: 381 loss: 2.28942827e-06
Iter: 382 loss: 2.28517e-06
Iter: 383 loss: 2.29813099e-06
Iter: 384 loss: 2.28397039e-06
Iter: 385 loss: 2.28168187e-06
Iter: 386 loss: 2.28114845e-06
Iter: 387 loss: 2.28000795e-06
Iter: 388 loss: 2.27653868e-06
Iter: 389 loss: 2.28441831e-06
Iter: 390 loss: 2.2745437e-06
Iter: 391 loss: 2.26962948e-06
Iter: 392 loss: 2.28813042e-06
Iter: 393 loss: 2.26866132e-06
Iter: 394 loss: 2.26358861e-06
Iter: 395 loss: 2.26950033e-06
Iter: 396 loss: 2.26073848e-06
Iter: 397 loss: 2.25502436e-06
Iter: 398 loss: 2.2718134e-06
Iter: 399 loss: 2.25328608e-06
Iter: 400 loss: 2.2488689e-06
Iter: 401 loss: 2.24873747e-06
Iter: 402 loss: 2.24659743e-06
Iter: 403 loss: 2.25021085e-06
Iter: 404 loss: 2.2454642e-06
Iter: 405 loss: 2.24316864e-06
Iter: 406 loss: 2.24368114e-06
Iter: 407 loss: 2.2412878e-06
Iter: 408 loss: 2.2376455e-06
Iter: 409 loss: 2.24062205e-06
Iter: 410 loss: 2.23531515e-06
Iter: 411 loss: 2.23111897e-06
Iter: 412 loss: 2.23084476e-06
Iter: 413 loss: 2.2275251e-06
Iter: 414 loss: 2.22286121e-06
Iter: 415 loss: 2.26409657e-06
Iter: 416 loss: 2.22266476e-06
Iter: 417 loss: 2.21839127e-06
Iter: 418 loss: 2.23706229e-06
Iter: 419 loss: 2.2176539e-06
Iter: 420 loss: 2.21552841e-06
Iter: 421 loss: 2.21542973e-06
Iter: 422 loss: 2.21351138e-06
Iter: 423 loss: 2.21802838e-06
Iter: 424 loss: 2.21291702e-06
Iter: 425 loss: 2.21126402e-06
Iter: 426 loss: 2.20662582e-06
Iter: 427 loss: 2.22381937e-06
Iter: 428 loss: 2.20447669e-06
Iter: 429 loss: 2.19941603e-06
Iter: 430 loss: 2.2337872e-06
Iter: 431 loss: 2.19880349e-06
Iter: 432 loss: 2.19405092e-06
Iter: 433 loss: 2.20578931e-06
Iter: 434 loss: 2.19234562e-06
Iter: 435 loss: 2.18839932e-06
Iter: 436 loss: 2.19441267e-06
Iter: 437 loss: 2.1865776e-06
Iter: 438 loss: 2.18316836e-06
Iter: 439 loss: 2.18311243e-06
Iter: 440 loss: 2.18075365e-06
Iter: 441 loss: 2.18470768e-06
Iter: 442 loss: 2.17974753e-06
Iter: 443 loss: 2.17748e-06
Iter: 444 loss: 2.17675097e-06
Iter: 445 loss: 2.1754413e-06
Iter: 446 loss: 2.17159368e-06
Iter: 447 loss: 2.18543687e-06
Iter: 448 loss: 2.17082425e-06
Iter: 449 loss: 2.16733179e-06
Iter: 450 loss: 2.16796047e-06
Iter: 451 loss: 2.16473745e-06
Iter: 452 loss: 2.16109902e-06
Iter: 453 loss: 2.18692821e-06
Iter: 454 loss: 2.16065337e-06
Iter: 455 loss: 2.15801606e-06
Iter: 456 loss: 2.16185845e-06
Iter: 457 loss: 2.15679256e-06
Iter: 458 loss: 2.15563341e-06
Iter: 459 loss: 2.15491787e-06
Iter: 460 loss: 2.15394175e-06
Iter: 461 loss: 2.15079535e-06
Iter: 462 loss: 2.16134185e-06
Iter: 463 loss: 2.14950319e-06
Iter: 464 loss: 2.14517809e-06
Iter: 465 loss: 2.17946376e-06
Iter: 466 loss: 2.1449e-06
Iter: 467 loss: 2.14202214e-06
Iter: 468 loss: 2.14012607e-06
Iter: 469 loss: 2.13912517e-06
Iter: 470 loss: 2.13493649e-06
Iter: 471 loss: 2.16313538e-06
Iter: 472 loss: 2.13466683e-06
Iter: 473 loss: 2.13285648e-06
Iter: 474 loss: 2.13279986e-06
Iter: 475 loss: 2.1313615e-06
Iter: 476 loss: 2.1274459e-06
Iter: 477 loss: 2.16742524e-06
Iter: 478 loss: 2.12696432e-06
Iter: 479 loss: 2.12407326e-06
Iter: 480 loss: 2.12407872e-06
Iter: 481 loss: 2.12216719e-06
Iter: 482 loss: 2.11987981e-06
Iter: 483 loss: 2.1195649e-06
Iter: 484 loss: 2.11580891e-06
Iter: 485 loss: 2.13506655e-06
Iter: 486 loss: 2.11526594e-06
Iter: 487 loss: 2.11269571e-06
Iter: 488 loss: 2.11300835e-06
Iter: 489 loss: 2.11081488e-06
Iter: 490 loss: 2.11229781e-06
Iter: 491 loss: 2.10979283e-06
Iter: 492 loss: 2.10858889e-06
Iter: 493 loss: 2.10618259e-06
Iter: 494 loss: 2.15006162e-06
Iter: 495 loss: 2.10618305e-06
Iter: 496 loss: 2.10359849e-06
Iter: 497 loss: 2.10399594e-06
Iter: 498 loss: 2.10177677e-06
Iter: 499 loss: 2.09917084e-06
Iter: 500 loss: 2.12508417e-06
Iter: 501 loss: 2.09917698e-06
Iter: 502 loss: 2.0968464e-06
Iter: 503 loss: 2.09636505e-06
Iter: 504 loss: 2.09485097e-06
Iter: 505 loss: 2.09210293e-06
Iter: 506 loss: 2.09514337e-06
Iter: 507 loss: 2.09064228e-06
Iter: 508 loss: 2.08797428e-06
Iter: 509 loss: 2.11206407e-06
Iter: 510 loss: 2.08787833e-06
Iter: 511 loss: 2.08557049e-06
Iter: 512 loss: 2.10021199e-06
Iter: 513 loss: 2.08539609e-06
Iter: 514 loss: 2.0832781e-06
Iter: 515 loss: 2.08138295e-06
Iter: 516 loss: 2.08094116e-06
Iter: 517 loss: 2.07878293e-06
Iter: 518 loss: 2.07870471e-06
Iter: 519 loss: 2.07691528e-06
Iter: 520 loss: 2.07417224e-06
Iter: 521 loss: 2.11086012e-06
Iter: 522 loss: 2.07423182e-06
Iter: 523 loss: 2.0723769e-06
Iter: 524 loss: 2.07563767e-06
Iter: 525 loss: 2.07158814e-06
Iter: 526 loss: 2.0701159e-06
Iter: 527 loss: 2.07014682e-06
Iter: 528 loss: 2.06856453e-06
Iter: 529 loss: 2.06928598e-06
Iter: 530 loss: 2.06774325e-06
Iter: 531 loss: 2.06633331e-06
Iter: 532 loss: 2.06260256e-06
Iter: 533 loss: 2.09876407e-06
Iter: 534 loss: 2.06228401e-06
Iter: 535 loss: 2.05914466e-06
Iter: 536 loss: 2.10121266e-06
Iter: 537 loss: 2.05915785e-06
Iter: 538 loss: 2.05693937e-06
Iter: 539 loss: 2.05927768e-06
Iter: 540 loss: 2.05572633e-06
Iter: 541 loss: 2.05225592e-06
Iter: 542 loss: 2.06021446e-06
Iter: 543 loss: 2.05086894e-06
Iter: 544 loss: 2.04895036e-06
Iter: 545 loss: 2.05612969e-06
Iter: 546 loss: 2.0485104e-06
Iter: 547 loss: 2.04623575e-06
Iter: 548 loss: 2.05652486e-06
Iter: 549 loss: 2.04589742e-06
Iter: 550 loss: 2.04354706e-06
Iter: 551 loss: 2.04295884e-06
Iter: 552 loss: 2.0415e-06
Iter: 553 loss: 2.0394682e-06
Iter: 554 loss: 2.03888567e-06
Iter: 555 loss: 2.03752279e-06
Iter: 556 loss: 2.03464106e-06
Iter: 557 loss: 2.05179526e-06
Iter: 558 loss: 2.03417653e-06
Iter: 559 loss: 2.03164382e-06
Iter: 560 loss: 2.04005823e-06
Iter: 561 loss: 2.03102172e-06
Iter: 562 loss: 2.03018544e-06
Iter: 563 loss: 2.02954652e-06
Iter: 564 loss: 2.02855358e-06
Iter: 565 loss: 2.02616525e-06
Iter: 566 loss: 2.05217384e-06
Iter: 567 loss: 2.02595083e-06
Iter: 568 loss: 2.02275896e-06
Iter: 569 loss: 2.02782189e-06
Iter: 570 loss: 2.02134197e-06
Iter: 571 loss: 2.01850571e-06
Iter: 572 loss: 2.01802868e-06
Iter: 573 loss: 2.01632929e-06
Iter: 574 loss: 2.01377679e-06
Iter: 575 loss: 2.01370744e-06
Iter: 576 loss: 2.01212015e-06
Iter: 577 loss: 2.0133009e-06
Iter: 578 loss: 2.01094417e-06
Iter: 579 loss: 2.00811746e-06
Iter: 580 loss: 2.01156695e-06
Iter: 581 loss: 2.00663749e-06
Iter: 582 loss: 2.00433442e-06
Iter: 583 loss: 2.00524232e-06
Iter: 584 loss: 2.00278873e-06
Iter: 585 loss: 2.00089721e-06
Iter: 586 loss: 2.00084787e-06
Iter: 587 loss: 1.99927172e-06
Iter: 588 loss: 1.99947158e-06
Iter: 589 loss: 1.99802389e-06
Iter: 590 loss: 1.99586611e-06
Iter: 591 loss: 1.99283886e-06
Iter: 592 loss: 1.99278793e-06
Iter: 593 loss: 1.99038254e-06
Iter: 594 loss: 2.00621e-06
Iter: 595 loss: 1.98997873e-06
Iter: 596 loss: 1.98915041e-06
Iter: 597 loss: 1.98894077e-06
Iter: 598 loss: 1.98749512e-06
Iter: 599 loss: 1.98502653e-06
Iter: 600 loss: 1.98495536e-06
Iter: 601 loss: 1.98368207e-06
Iter: 602 loss: 1.98254543e-06
Iter: 603 loss: 1.98198882e-06
Iter: 604 loss: 1.97986583e-06
Iter: 605 loss: 1.99881742e-06
Iter: 606 loss: 1.9797651e-06
Iter: 607 loss: 1.97804502e-06
Iter: 608 loss: 1.9828276e-06
Iter: 609 loss: 1.97761824e-06
Iter: 610 loss: 1.97595682e-06
Iter: 611 loss: 1.97761415e-06
Iter: 612 loss: 1.97511486e-06
Iter: 613 loss: 1.97291411e-06
Iter: 614 loss: 1.97461895e-06
Iter: 615 loss: 1.97178042e-06
Iter: 616 loss: 1.96950145e-06
Iter: 617 loss: 1.99929241e-06
Iter: 618 loss: 1.96946303e-06
Iter: 619 loss: 1.96789483e-06
Iter: 620 loss: 1.96574615e-06
Iter: 621 loss: 1.9656004e-06
Iter: 622 loss: 1.96414749e-06
Iter: 623 loss: 1.96394853e-06
Iter: 624 loss: 1.96283713e-06
Iter: 625 loss: 1.9692784e-06
Iter: 626 loss: 1.96243536e-06
Iter: 627 loss: 1.96118299e-06
Iter: 628 loss: 1.9590907e-06
Iter: 629 loss: 1.95907273e-06
Iter: 630 loss: 1.95816347e-06
Iter: 631 loss: 1.95811845e-06
Iter: 632 loss: 1.95662187e-06
Iter: 633 loss: 1.95570988e-06
Iter: 634 loss: 1.95542952e-06
Iter: 635 loss: 1.95380562e-06
Iter: 636 loss: 1.95235452e-06
Iter: 637 loss: 1.95212169e-06
Iter: 638 loss: 1.94976019e-06
Iter: 639 loss: 1.94996574e-06
Iter: 640 loss: 1.94791528e-06
Iter: 641 loss: 1.94557242e-06
Iter: 642 loss: 1.94562722e-06
Iter: 643 loss: 1.94388099e-06
Iter: 644 loss: 1.9463896e-06
Iter: 645 loss: 1.94302584e-06
Iter: 646 loss: 1.94079848e-06
Iter: 647 loss: 1.94762879e-06
Iter: 648 loss: 1.94032532e-06
Iter: 649 loss: 1.93860865e-06
Iter: 650 loss: 1.94030054e-06
Iter: 651 loss: 1.93768778e-06
Iter: 652 loss: 1.93564802e-06
Iter: 653 loss: 1.95231883e-06
Iter: 654 loss: 1.9354668e-06
Iter: 655 loss: 1.93406527e-06
Iter: 656 loss: 1.93411825e-06
Iter: 657 loss: 1.93291612e-06
Iter: 658 loss: 1.93106712e-06
Iter: 659 loss: 1.93961614e-06
Iter: 660 loss: 1.93052369e-06
Iter: 661 loss: 1.92910102e-06
Iter: 662 loss: 1.9458505e-06
Iter: 663 loss: 1.92907e-06
Iter: 664 loss: 1.92782682e-06
Iter: 665 loss: 1.92587277e-06
Iter: 666 loss: 1.92594143e-06
Iter: 667 loss: 1.92412085e-06
Iter: 668 loss: 1.93443975e-06
Iter: 669 loss: 1.92382549e-06
Iter: 670 loss: 1.922102e-06
Iter: 671 loss: 1.94431277e-06
Iter: 672 loss: 1.92206039e-06
Iter: 673 loss: 1.92124912e-06
Iter: 674 loss: 1.91944036e-06
Iter: 675 loss: 1.94982817e-06
Iter: 676 loss: 1.91930644e-06
Iter: 677 loss: 1.91773779e-06
Iter: 678 loss: 1.91637673e-06
Iter: 679 loss: 1.91576555e-06
Iter: 680 loss: 1.91319418e-06
Iter: 681 loss: 1.92795051e-06
Iter: 682 loss: 1.91287677e-06
Iter: 683 loss: 1.91016852e-06
Iter: 684 loss: 1.92535799e-06
Iter: 685 loss: 1.9098668e-06
Iter: 686 loss: 1.90824426e-06
Iter: 687 loss: 1.91309255e-06
Iter: 688 loss: 1.90780611e-06
Iter: 689 loss: 1.9060725e-06
Iter: 690 loss: 1.90458263e-06
Iter: 691 loss: 1.90407741e-06
Iter: 692 loss: 1.90299374e-06
Iter: 693 loss: 1.90263893e-06
Iter: 694 loss: 1.90178071e-06
Iter: 695 loss: 1.90057676e-06
Iter: 696 loss: 1.90042533e-06
Iter: 697 loss: 1.89850516e-06
Iter: 698 loss: 1.90597382e-06
Iter: 699 loss: 1.89790069e-06
Iter: 700 loss: 1.89616662e-06
Iter: 701 loss: 1.91344247e-06
Iter: 702 loss: 1.89593379e-06
Iter: 703 loss: 1.89469188e-06
Iter: 704 loss: 1.89273146e-06
Iter: 705 loss: 1.89276943e-06
Iter: 706 loss: 1.89473826e-06
Iter: 707 loss: 1.89216257e-06
Iter: 708 loss: 1.89159641e-06
Iter: 709 loss: 1.8902773e-06
Iter: 710 loss: 1.89703883e-06
Iter: 711 loss: 1.88993192e-06
Iter: 712 loss: 1.88815466e-06
Iter: 713 loss: 1.89079776e-06
Iter: 714 loss: 1.88725608e-06
Iter: 715 loss: 1.88562456e-06
Iter: 716 loss: 1.88684078e-06
Iter: 717 loss: 1.88460876e-06
Iter: 718 loss: 1.88226022e-06
Iter: 719 loss: 1.89458137e-06
Iter: 720 loss: 1.88201534e-06
Iter: 721 loss: 1.87971e-06
Iter: 722 loss: 1.88586023e-06
Iter: 723 loss: 1.87912678e-06
Iter: 724 loss: 1.87725834e-06
Iter: 725 loss: 1.88778074e-06
Iter: 726 loss: 1.87709668e-06
Iter: 727 loss: 1.87573301e-06
Iter: 728 loss: 1.87490014e-06
Iter: 729 loss: 1.87444527e-06
Iter: 730 loss: 1.87302498e-06
Iter: 731 loss: 1.89388595e-06
Iter: 732 loss: 1.873065e-06
Iter: 733 loss: 1.87183616e-06
Iter: 734 loss: 1.87076773e-06
Iter: 735 loss: 1.87037449e-06
Iter: 736 loss: 1.86854982e-06
Iter: 737 loss: 1.89027048e-06
Iter: 738 loss: 1.86866805e-06
Iter: 739 loss: 1.86766454e-06
Iter: 740 loss: 1.87406908e-06
Iter: 741 loss: 1.86758371e-06
Iter: 742 loss: 1.86666762e-06
Iter: 743 loss: 1.86861757e-06
Iter: 744 loss: 1.86635714e-06
Iter: 745 loss: 1.86481532e-06
Iter: 746 loss: 1.86684065e-06
Iter: 747 loss: 1.86424018e-06
Iter: 748 loss: 1.86361603e-06
Iter: 749 loss: 1.86265379e-06
Iter: 750 loss: 1.86273e-06
Iter: 751 loss: 1.86129262e-06
Iter: 752 loss: 1.86006025e-06
Iter: 753 loss: 1.85946953e-06
Iter: 754 loss: 1.85744238e-06
Iter: 755 loss: 1.86494253e-06
Iter: 756 loss: 1.85692898e-06
Iter: 757 loss: 1.85491672e-06
Iter: 758 loss: 1.86972352e-06
Iter: 759 loss: 1.85469526e-06
Iter: 760 loss: 1.85346789e-06
Iter: 761 loss: 1.85495276e-06
Iter: 762 loss: 1.85263889e-06
Iter: 763 loss: 1.85094405e-06
Iter: 764 loss: 1.86758655e-06
Iter: 765 loss: 1.85101464e-06
Iter: 766 loss: 1.85008037e-06
Iter: 767 loss: 1.84865621e-06
Iter: 768 loss: 1.84856299e-06
Iter: 769 loss: 1.84682631e-06
Iter: 770 loss: 1.85906822e-06
Iter: 771 loss: 1.84663986e-06
Iter: 772 loss: 1.84496923e-06
Iter: 773 loss: 1.85098088e-06
Iter: 774 loss: 1.84453006e-06
Iter: 775 loss: 1.8429904e-06
Iter: 776 loss: 1.85577619e-06
Iter: 777 loss: 1.84287046e-06
Iter: 778 loss: 1.84201497e-06
Iter: 779 loss: 1.85493809e-06
Iter: 780 loss: 1.84194914e-06
Iter: 781 loss: 1.84144585e-06
Iter: 782 loss: 1.84117528e-06
Iter: 783 loss: 1.84094608e-06
Iter: 784 loss: 1.84014766e-06
Iter: 785 loss: 1.83931979e-06
Iter: 786 loss: 1.83918098e-06
Iter: 787 loss: 1.83781e-06
Iter: 788 loss: 1.83648876e-06
Iter: 789 loss: 1.83628299e-06
Iter: 790 loss: 1.83468126e-06
Iter: 791 loss: 1.83464078e-06
Iter: 792 loss: 1.83344525e-06
Iter: 793 loss: 1.83218117e-06
Iter: 794 loss: 1.83181612e-06
Iter: 795 loss: 1.83040424e-06
Iter: 796 loss: 1.85004342e-06
Iter: 797 loss: 1.83047564e-06
Iter: 798 loss: 1.8290059e-06
Iter: 799 loss: 1.83041732e-06
Iter: 800 loss: 1.8282401e-06
Iter: 801 loss: 1.82682845e-06
Iter: 802 loss: 1.82980625e-06
Iter: 803 loss: 1.82638973e-06
Iter: 804 loss: 1.82471035e-06
Iter: 805 loss: 1.82735494e-06
Iter: 806 loss: 1.82393217e-06
Iter: 807 loss: 1.82278563e-06
Iter: 808 loss: 1.82276017e-06
Iter: 809 loss: 1.82217502e-06
Iter: 810 loss: 1.82555095e-06
Iter: 811 loss: 1.82207589e-06
Iter: 812 loss: 1.82151598e-06
Iter: 813 loss: 1.82146891e-06
Iter: 814 loss: 1.8210759e-06
Iter: 815 loss: 1.82007261e-06
Iter: 816 loss: 1.82097938e-06
Iter: 817 loss: 1.81963355e-06
Iter: 818 loss: 1.81885332e-06
Iter: 819 loss: 1.81823771e-06
Iter: 820 loss: 1.81790176e-06
Iter: 821 loss: 1.81631469e-06
Iter: 822 loss: 1.81822861e-06
Iter: 823 loss: 1.81561086e-06
Iter: 824 loss: 1.81395944e-06
Iter: 825 loss: 1.81849725e-06
Iter: 826 loss: 1.81346184e-06
Iter: 827 loss: 1.81205951e-06
Iter: 828 loss: 1.81685948e-06
Iter: 829 loss: 1.81176051e-06
Iter: 830 loss: 1.81016583e-06
Iter: 831 loss: 1.81707264e-06
Iter: 832 loss: 1.81000041e-06
Iter: 833 loss: 1.80916084e-06
Iter: 834 loss: 1.81817643e-06
Iter: 835 loss: 1.80913139e-06
Iter: 836 loss: 1.80829841e-06
Iter: 837 loss: 1.80656411e-06
Iter: 838 loss: 1.8339822e-06
Iter: 839 loss: 1.8064984e-06
Iter: 840 loss: 1.80606332e-06
Iter: 841 loss: 1.80565723e-06
Iter: 842 loss: 1.80501365e-06
Iter: 843 loss: 1.80929032e-06
Iter: 844 loss: 1.80502627e-06
Iter: 845 loss: 1.80459767e-06
Iter: 846 loss: 1.80528423e-06
Iter: 847 loss: 1.80428651e-06
Iter: 848 loss: 1.80359712e-06
Iter: 849 loss: 1.80321786e-06
Iter: 850 loss: 1.80292568e-06
Iter: 851 loss: 1.80225925e-06
Iter: 852 loss: 1.80537631e-06
Iter: 853 loss: 1.80205564e-06
Iter: 854 loss: 1.80137181e-06
Iter: 855 loss: 1.79982453e-06
Iter: 856 loss: 1.82907252e-06
Iter: 857 loss: 1.79974495e-06
Iter: 858 loss: 1.79845222e-06
Iter: 859 loss: 1.80827783e-06
Iter: 860 loss: 1.79836479e-06
Iter: 861 loss: 1.79701033e-06
Iter: 862 loss: 1.79965491e-06
Iter: 863 loss: 1.7967011e-06
Iter: 864 loss: 1.79544077e-06
Iter: 865 loss: 1.79741915e-06
Iter: 866 loss: 1.79499227e-06
Iter: 867 loss: 1.79388803e-06
Iter: 868 loss: 1.80193115e-06
Iter: 869 loss: 1.79383733e-06
Iter: 870 loss: 1.79270626e-06
Iter: 871 loss: 1.79415167e-06
Iter: 872 loss: 1.79217045e-06
Iter: 873 loss: 1.79089079e-06
Iter: 874 loss: 1.79734366e-06
Iter: 875 loss: 1.79068297e-06
Iter: 876 loss: 1.79006565e-06
Iter: 877 loss: 1.79806852e-06
Iter: 878 loss: 1.7901591e-06
Iter: 879 loss: 1.78935784e-06
Iter: 880 loss: 1.79005622e-06
Iter: 881 loss: 1.78891503e-06
Iter: 882 loss: 1.78807977e-06
Iter: 883 loss: 1.79153358e-06
Iter: 884 loss: 1.78788309e-06
Iter: 885 loss: 1.78741311e-06
Iter: 886 loss: 1.78669825e-06
Iter: 887 loss: 1.78673736e-06
Iter: 888 loss: 1.78569303e-06
Iter: 889 loss: 1.7904274e-06
Iter: 890 loss: 1.78565301e-06
Iter: 891 loss: 1.78486505e-06
Iter: 892 loss: 1.78367509e-06
Iter: 893 loss: 1.78364508e-06
Iter: 894 loss: 1.78184416e-06
Iter: 895 loss: 1.7938728e-06
Iter: 896 loss: 1.78164055e-06
Iter: 897 loss: 1.78065068e-06
Iter: 898 loss: 1.78075754e-06
Iter: 899 loss: 1.7799249e-06
Iter: 900 loss: 1.77859806e-06
Iter: 901 loss: 1.78687526e-06
Iter: 902 loss: 1.77850097e-06
Iter: 903 loss: 1.77724473e-06
Iter: 904 loss: 1.77924926e-06
Iter: 905 loss: 1.77688185e-06
Iter: 906 loss: 1.77567028e-06
Iter: 907 loss: 1.79067763e-06
Iter: 908 loss: 1.7757045e-06
Iter: 909 loss: 1.77498453e-06
Iter: 910 loss: 1.77533138e-06
Iter: 911 loss: 1.77451057e-06
Iter: 912 loss: 1.77357299e-06
Iter: 913 loss: 1.78799417e-06
Iter: 914 loss: 1.77355628e-06
Iter: 915 loss: 1.7730282e-06
Iter: 916 loss: 1.77399215e-06
Iter: 917 loss: 1.77280265e-06
Iter: 918 loss: 1.77226673e-06
Iter: 919 loss: 1.77146785e-06
Iter: 920 loss: 1.77137372e-06
Iter: 921 loss: 1.7702049e-06
Iter: 922 loss: 1.77299637e-06
Iter: 923 loss: 1.76994229e-06
Iter: 924 loss: 1.76889637e-06
Iter: 925 loss: 1.77379275e-06
Iter: 926 loss: 1.76875392e-06
Iter: 927 loss: 1.76763331e-06
Iter: 928 loss: 1.76719482e-06
Iter: 929 loss: 1.76664139e-06
Iter: 930 loss: 1.76537833e-06
Iter: 931 loss: 1.76810136e-06
Iter: 932 loss: 1.76475419e-06
Iter: 933 loss: 1.76361573e-06
Iter: 934 loss: 1.77299955e-06
Iter: 935 loss: 1.76345418e-06
Iter: 936 loss: 1.76235824e-06
Iter: 937 loss: 1.76249796e-06
Iter: 938 loss: 1.7615148e-06
Iter: 939 loss: 1.76047433e-06
Iter: 940 loss: 1.76904939e-06
Iter: 941 loss: 1.76046626e-06
Iter: 942 loss: 1.75932121e-06
Iter: 943 loss: 1.76117555e-06
Iter: 944 loss: 1.75881951e-06
Iter: 945 loss: 1.7581566e-06
Iter: 946 loss: 1.75811294e-06
Iter: 947 loss: 1.75739922e-06
Iter: 948 loss: 1.75874868e-06
Iter: 949 loss: 1.75731736e-06
Iter: 950 loss: 1.75681544e-06
Iter: 951 loss: 1.75651007e-06
Iter: 952 loss: 1.75634386e-06
Iter: 953 loss: 1.75522723e-06
Iter: 954 loss: 1.75553907e-06
Iter: 955 loss: 1.75454e-06
Iter: 956 loss: 1.75355262e-06
Iter: 957 loss: 1.75423224e-06
Iter: 958 loss: 1.7531861e-06
Iter: 959 loss: 1.75183743e-06
Iter: 960 loss: 1.75664127e-06
Iter: 961 loss: 1.75140576e-06
Iter: 962 loss: 1.75044931e-06
Iter: 963 loss: 1.75161676e-06
Iter: 964 loss: 1.74985075e-06
Iter: 965 loss: 1.74870968e-06
Iter: 966 loss: 1.74943443e-06
Iter: 967 loss: 1.74799084e-06
Iter: 968 loss: 1.74657907e-06
Iter: 969 loss: 1.75266484e-06
Iter: 970 loss: 1.74639638e-06
Iter: 971 loss: 1.74546255e-06
Iter: 972 loss: 1.75010325e-06
Iter: 973 loss: 1.74529225e-06
Iter: 974 loss: 1.74431966e-06
Iter: 975 loss: 1.74412799e-06
Iter: 976 loss: 1.74339027e-06
Iter: 977 loss: 1.74262186e-06
Iter: 978 loss: 1.74248078e-06
Iter: 979 loss: 1.74184572e-06
Iter: 980 loss: 1.74551451e-06
Iter: 981 loss: 1.74177239e-06
Iter: 982 loss: 1.74117758e-06
Iter: 983 loss: 1.74038416e-06
Iter: 984 loss: 1.7403255e-06
Iter: 985 loss: 1.73935359e-06
Iter: 986 loss: 1.74702404e-06
Iter: 987 loss: 1.73927697e-06
Iter: 988 loss: 1.73880449e-06
Iter: 989 loss: 1.73787066e-06
Iter: 990 loss: 1.7378909e-06
Iter: 991 loss: 1.7367106e-06
Iter: 992 loss: 1.74362572e-06
Iter: 993 loss: 1.73653416e-06
Iter: 994 loss: 1.73587239e-06
Iter: 995 loss: 1.739303e-06
Iter: 996 loss: 1.7357022e-06
Iter: 997 loss: 1.73495368e-06
Iter: 998 loss: 1.73373667e-06
Iter: 999 loss: 1.73371961e-06
Iter: 1000 loss: 1.73258786e-06
Iter: 1001 loss: 1.73662556e-06
Iter: 1002 loss: 1.73219109e-06
Iter: 1003 loss: 1.73092349e-06
Iter: 1004 loss: 1.73399928e-06
Iter: 1005 loss: 1.73038256e-06
Iter: 1006 loss: 1.72887189e-06
Iter: 1007 loss: 1.72931846e-06
Iter: 1008 loss: 1.72794944e-06
Iter: 1009 loss: 1.72709497e-06
Iter: 1010 loss: 1.72692683e-06
Iter: 1011 loss: 1.72602927e-06
Iter: 1012 loss: 1.72892624e-06
Iter: 1013 loss: 1.7259166e-06
Iter: 1014 loss: 1.72502314e-06
Iter: 1015 loss: 1.72632906e-06
Iter: 1016 loss: 1.72467e-06
Iter: 1017 loss: 1.72383784e-06
Iter: 1018 loss: 1.72469186e-06
Iter: 1019 loss: 1.72340867e-06
Iter: 1020 loss: 1.72241562e-06
Iter: 1021 loss: 1.72287037e-06
Iter: 1022 loss: 1.72181569e-06
Iter: 1023 loss: 1.72040461e-06
Iter: 1024 loss: 1.72330442e-06
Iter: 1025 loss: 1.72001e-06
Iter: 1026 loss: 1.71914064e-06
Iter: 1027 loss: 1.71893191e-06
Iter: 1028 loss: 1.71836007e-06
Iter: 1029 loss: 1.71669194e-06
Iter: 1030 loss: 1.7325566e-06
Iter: 1031 loss: 1.71673469e-06
Iter: 1032 loss: 1.71589545e-06
Iter: 1033 loss: 1.71495049e-06
Iter: 1034 loss: 1.7148767e-06
Iter: 1035 loss: 1.71356362e-06
Iter: 1036 loss: 1.72270211e-06
Iter: 1037 loss: 1.71332977e-06
Iter: 1038 loss: 1.71221507e-06
Iter: 1039 loss: 1.7130302e-06
Iter: 1040 loss: 1.71138504e-06
Iter: 1041 loss: 1.71045463e-06
Iter: 1042 loss: 1.71813031e-06
Iter: 1043 loss: 1.71042063e-06
Iter: 1044 loss: 1.7099095e-06
Iter: 1045 loss: 1.70969702e-06
Iter: 1046 loss: 1.70938529e-06
Iter: 1047 loss: 1.70870862e-06
Iter: 1048 loss: 1.708632e-06
Iter: 1049 loss: 1.70791122e-06
Iter: 1050 loss: 1.71026932e-06
Iter: 1051 loss: 1.70773956e-06
Iter: 1052 loss: 1.70692294e-06
Iter: 1053 loss: 1.70664066e-06
Iter: 1054 loss: 1.70616204e-06
Iter: 1055 loss: 1.70491137e-06
Iter: 1056 loss: 1.70939859e-06
Iter: 1057 loss: 1.70451426e-06
Iter: 1058 loss: 1.70365377e-06
Iter: 1059 loss: 1.70476187e-06
Iter: 1060 loss: 1.70336341e-06
Iter: 1061 loss: 1.70226235e-06
Iter: 1062 loss: 1.70369503e-06
Iter: 1063 loss: 1.70168801e-06
Iter: 1064 loss: 1.70062708e-06
Iter: 1065 loss: 1.70624253e-06
Iter: 1066 loss: 1.70039743e-06
Iter: 1067 loss: 1.69922669e-06
Iter: 1068 loss: 1.69994667e-06
Iter: 1069 loss: 1.69834914e-06
Iter: 1070 loss: 1.69733767e-06
Iter: 1071 loss: 1.69845316e-06
Iter: 1072 loss: 1.69669784e-06
Iter: 1073 loss: 1.69544558e-06
Iter: 1074 loss: 1.70531791e-06
Iter: 1075 loss: 1.6953436e-06
Iter: 1076 loss: 1.69451982e-06
Iter: 1077 loss: 1.6949823e-06
Iter: 1078 loss: 1.69387749e-06
Iter: 1079 loss: 1.69388318e-06
Iter: 1080 loss: 1.69331975e-06
Iter: 1081 loss: 1.69308191e-06
Iter: 1082 loss: 1.69246778e-06
Iter: 1083 loss: 1.69478585e-06
Iter: 1084 loss: 1.69209204e-06
Iter: 1085 loss: 1.69090868e-06
Iter: 1086 loss: 1.69645568e-06
Iter: 1087 loss: 1.69069688e-06
Iter: 1088 loss: 1.68979238e-06
Iter: 1089 loss: 1.69337909e-06
Iter: 1090 loss: 1.6896538e-06
Iter: 1091 loss: 1.68887811e-06
Iter: 1092 loss: 1.68826864e-06
Iter: 1093 loss: 1.68799147e-06
Iter: 1094 loss: 1.68689292e-06
Iter: 1095 loss: 1.69160876e-06
Iter: 1096 loss: 1.68664246e-06
Iter: 1097 loss: 1.68572637e-06
Iter: 1098 loss: 1.68544307e-06
Iter: 1099 loss: 1.68489601e-06
Iter: 1100 loss: 1.68348322e-06
Iter: 1101 loss: 1.70193857e-06
Iter: 1102 loss: 1.68357667e-06
Iter: 1103 loss: 1.68283646e-06
Iter: 1104 loss: 1.68212694e-06
Iter: 1105 loss: 1.68192e-06
Iter: 1106 loss: 1.68080692e-06
Iter: 1107 loss: 1.68457632e-06
Iter: 1108 loss: 1.68063104e-06
Iter: 1109 loss: 1.67948872e-06
Iter: 1110 loss: 1.68203405e-06
Iter: 1111 loss: 1.67903522e-06
Iter: 1112 loss: 1.67856933e-06
Iter: 1113 loss: 1.67849203e-06
Iter: 1114 loss: 1.67771179e-06
Iter: 1115 loss: 1.67798623e-06
Iter: 1116 loss: 1.6772218e-06
Iter: 1117 loss: 1.67653775e-06
Iter: 1118 loss: 1.67578401e-06
Iter: 1119 loss: 1.67567828e-06
Iter: 1120 loss: 1.67481187e-06
Iter: 1121 loss: 1.67480869e-06
Iter: 1122 loss: 1.67409155e-06
Iter: 1123 loss: 1.67286942e-06
Iter: 1124 loss: 1.70035878e-06
Iter: 1125 loss: 1.67287396e-06
Iter: 1126 loss: 1.67194094e-06
Iter: 1127 loss: 1.67194821e-06
Iter: 1128 loss: 1.67144162e-06
Iter: 1129 loss: 1.67072176e-06
Iter: 1130 loss: 1.6707354e-06
Iter: 1131 loss: 1.66948087e-06
Iter: 1132 loss: 1.67142389e-06
Iter: 1133 loss: 1.66891914e-06
Iter: 1134 loss: 1.66765722e-06
Iter: 1135 loss: 1.66719531e-06
Iter: 1136 loss: 1.66664825e-06
Iter: 1137 loss: 1.66561904e-06
Iter: 1138 loss: 1.66566679e-06
Iter: 1139 loss: 1.66471284e-06
Iter: 1140 loss: 1.66617451e-06
Iter: 1141 loss: 1.66427151e-06
Iter: 1142 loss: 1.66340408e-06
Iter: 1143 loss: 1.66889879e-06
Iter: 1144 loss: 1.66327493e-06
Iter: 1145 loss: 1.66280006e-06
Iter: 1146 loss: 1.66277096e-06
Iter: 1147 loss: 1.66238817e-06
Iter: 1148 loss: 1.66087966e-06
Iter: 1149 loss: 1.66774794e-06
Iter: 1150 loss: 1.66051404e-06
Iter: 1151 loss: 1.65959455e-06
Iter: 1152 loss: 1.67162125e-06
Iter: 1153 loss: 1.659502e-06
Iter: 1154 loss: 1.65880726e-06
Iter: 1155 loss: 1.66229665e-06
Iter: 1156 loss: 1.65857205e-06
Iter: 1157 loss: 1.6579163e-06
Iter: 1158 loss: 1.65800145e-06
Iter: 1159 loss: 1.65730228e-06
Iter: 1160 loss: 1.6565873e-06
Iter: 1161 loss: 1.65775032e-06
Iter: 1162 loss: 1.65626204e-06
Iter: 1163 loss: 1.65524386e-06
Iter: 1164 loss: 1.65761776e-06
Iter: 1165 loss: 1.65474125e-06
Iter: 1166 loss: 1.65395636e-06
Iter: 1167 loss: 1.65492895e-06
Iter: 1168 loss: 1.6534982e-06
Iter: 1169 loss: 1.65253073e-06
Iter: 1170 loss: 1.65395682e-06
Iter: 1171 loss: 1.65195138e-06
Iter: 1172 loss: 1.6511068e-06
Iter: 1173 loss: 1.65460585e-06
Iter: 1174 loss: 1.65079132e-06
Iter: 1175 loss: 1.64989615e-06
Iter: 1176 loss: 1.6559693e-06
Iter: 1177 loss: 1.64983328e-06
Iter: 1178 loss: 1.64914309e-06
Iter: 1179 loss: 1.65563984e-06
Iter: 1180 loss: 1.64909761e-06
Iter: 1181 loss: 1.64836911e-06
Iter: 1182 loss: 1.65117513e-06
Iter: 1183 loss: 1.64828532e-06
Iter: 1184 loss: 1.6477552e-06
Iter: 1185 loss: 1.64641301e-06
Iter: 1186 loss: 1.66209236e-06
Iter: 1187 loss: 1.64642029e-06
Iter: 1188 loss: 1.6453597e-06
Iter: 1189 loss: 1.65031508e-06
Iter: 1190 loss: 1.64537744e-06
Iter: 1191 loss: 1.6445066e-06
Iter: 1192 loss: 1.65313418e-06
Iter: 1193 loss: 1.64452058e-06
Iter: 1194 loss: 1.64385278e-06
Iter: 1195 loss: 1.64336382e-06
Iter: 1196 loss: 1.64327264e-06
Iter: 1197 loss: 1.64243534e-06
Iter: 1198 loss: 1.6472145e-06
Iter: 1199 loss: 1.64223911e-06
Iter: 1200 loss: 1.64151993e-06
Iter: 1201 loss: 1.64334688e-06
Iter: 1202 loss: 1.64146104e-06
Iter: 1203 loss: 1.64052278e-06
Iter: 1204 loss: 1.64004837e-06
Iter: 1205 loss: 1.63975096e-06
Iter: 1206 loss: 1.63883976e-06
Iter: 1207 loss: 1.64288815e-06
Iter: 1208 loss: 1.63859841e-06
Iter: 1209 loss: 1.63760922e-06
Iter: 1210 loss: 1.6408419e-06
Iter: 1211 loss: 1.63741038e-06
Iter: 1212 loss: 1.63664527e-06
Iter: 1213 loss: 1.63948198e-06
Iter: 1214 loss: 1.63659593e-06
Iter: 1215 loss: 1.63590732e-06
Iter: 1216 loss: 1.64385119e-06
Iter: 1217 loss: 1.63594746e-06
Iter: 1218 loss: 1.63534287e-06
Iter: 1219 loss: 1.63491313e-06
Iter: 1220 loss: 1.63471941e-06
Iter: 1221 loss: 1.63427478e-06
Iter: 1222 loss: 1.63352706e-06
Iter: 1223 loss: 1.63348795e-06
Iter: 1224 loss: 1.63247432e-06
Iter: 1225 loss: 1.63830191e-06
Iter: 1226 loss: 1.63228958e-06
Iter: 1227 loss: 1.63176196e-06
Iter: 1228 loss: 1.6419167e-06
Iter: 1229 loss: 1.63175514e-06
Iter: 1230 loss: 1.63137281e-06
Iter: 1231 loss: 1.63054096e-06
Iter: 1232 loss: 1.64282494e-06
Iter: 1233 loss: 1.63055427e-06
Iter: 1234 loss: 1.62975255e-06
Iter: 1235 loss: 1.63764525e-06
Iter: 1236 loss: 1.629812e-06
Iter: 1237 loss: 1.62921015e-06
Iter: 1238 loss: 1.62997981e-06
Iter: 1239 loss: 1.62883987e-06
Iter: 1240 loss: 1.62815797e-06
Iter: 1241 loss: 1.62837523e-06
Iter: 1242 loss: 1.62752201e-06
Iter: 1243 loss: 1.62667311e-06
Iter: 1244 loss: 1.62701781e-06
Iter: 1245 loss: 1.62605988e-06
Iter: 1246 loss: 1.62497986e-06
Iter: 1247 loss: 1.6362419e-06
Iter: 1248 loss: 1.62487106e-06
Iter: 1249 loss: 1.62445406e-06
Iter: 1250 loss: 1.62452216e-06
Iter: 1251 loss: 1.62397828e-06
Iter: 1252 loss: 1.62418712e-06
Iter: 1253 loss: 1.62367542e-06
Iter: 1254 loss: 1.62306856e-06
Iter: 1255 loss: 1.62324238e-06
Iter: 1256 loss: 1.62274341e-06
Iter: 1257 loss: 1.62205163e-06
Iter: 1258 loss: 1.62170386e-06
Iter: 1259 loss: 1.62156971e-06
Iter: 1260 loss: 1.6210065e-06
Iter: 1261 loss: 1.62099161e-06
Iter: 1262 loss: 1.62053448e-06
Iter: 1263 loss: 1.62065248e-06
Iter: 1264 loss: 1.62013009e-06
Iter: 1265 loss: 1.6194665e-06
Iter: 1266 loss: 1.61947878e-06
Iter: 1267 loss: 1.61907792e-06
Iter: 1268 loss: 1.61823618e-06
Iter: 1269 loss: 1.62146125e-06
Iter: 1270 loss: 1.61809112e-06
Iter: 1271 loss: 1.61726689e-06
Iter: 1272 loss: 1.61935213e-06
Iter: 1273 loss: 1.61690832e-06
Iter: 1274 loss: 1.61630919e-06
Iter: 1275 loss: 1.61762068e-06
Iter: 1276 loss: 1.61602065e-06
Iter: 1277 loss: 1.61530397e-06
Iter: 1278 loss: 1.6165377e-06
Iter: 1279 loss: 1.61493676e-06
Iter: 1280 loss: 1.61428875e-06
Iter: 1281 loss: 1.61704361e-06
Iter: 1282 loss: 1.61406228e-06
Iter: 1283 loss: 1.61383389e-06
Iter: 1284 loss: 1.61369951e-06
Iter: 1285 loss: 1.61326579e-06
Iter: 1286 loss: 1.61288574e-06
Iter: 1287 loss: 1.61283947e-06
Iter: 1288 loss: 1.61219725e-06
Iter: 1289 loss: 1.61220055e-06
Iter: 1290 loss: 1.61186722e-06
Iter: 1291 loss: 1.6109401e-06
Iter: 1292 loss: 1.61203548e-06
Iter: 1293 loss: 1.61034961e-06
Iter: 1294 loss: 1.61002265e-06
Iter: 1295 loss: 1.60991567e-06
Iter: 1296 loss: 1.60966647e-06
Iter: 1297 loss: 1.60882655e-06
Iter: 1298 loss: 1.61868525e-06
Iter: 1299 loss: 1.60868728e-06
Iter: 1300 loss: 1.60787238e-06
Iter: 1301 loss: 1.61377625e-06
Iter: 1302 loss: 1.60787613e-06
Iter: 1303 loss: 1.6072056e-06
Iter: 1304 loss: 1.60849709e-06
Iter: 1305 loss: 1.60698653e-06
Iter: 1306 loss: 1.60616423e-06
Iter: 1307 loss: 1.60915329e-06
Iter: 1308 loss: 1.60599e-06
Iter: 1309 loss: 1.60541663e-06
Iter: 1310 loss: 1.60446325e-06
Iter: 1311 loss: 1.62293372e-06
Iter: 1312 loss: 1.60435434e-06
Iter: 1313 loss: 1.60345462e-06
Iter: 1314 loss: 1.61299477e-06
Iter: 1315 loss: 1.60339903e-06
Iter: 1316 loss: 1.60298259e-06
Iter: 1317 loss: 1.60295372e-06
Iter: 1318 loss: 1.60231662e-06
Iter: 1319 loss: 1.60221828e-06
Iter: 1320 loss: 1.60186596e-06
Iter: 1321 loss: 1.60140917e-06
Iter: 1322 loss: 1.60064383e-06
Iter: 1323 loss: 1.60070272e-06
Iter: 1324 loss: 1.59978072e-06
Iter: 1325 loss: 1.60460252e-06
Iter: 1326 loss: 1.59964748e-06
Iter: 1327 loss: 1.59910496e-06
Iter: 1328 loss: 1.59906222e-06
Iter: 1329 loss: 1.59867261e-06
Iter: 1330 loss: 1.5983228e-06
Iter: 1331 loss: 1.5982871e-06
Iter: 1332 loss: 1.59748095e-06
Iter: 1333 loss: 1.60015361e-06
Iter: 1334 loss: 1.59744047e-06
Iter: 1335 loss: 1.59687386e-06
Iter: 1336 loss: 1.59602462e-06
Iter: 1337 loss: 1.59603087e-06
Iter: 1338 loss: 1.59535159e-06
Iter: 1339 loss: 1.5952935e-06
Iter: 1340 loss: 1.59467709e-06
Iter: 1341 loss: 1.59520732e-06
Iter: 1342 loss: 1.59428691e-06
Iter: 1343 loss: 1.59352294e-06
Iter: 1344 loss: 1.59481942e-06
Iter: 1345 loss: 1.5932294e-06
Iter: 1346 loss: 1.59241722e-06
Iter: 1347 loss: 1.59307911e-06
Iter: 1348 loss: 1.591922e-06
Iter: 1349 loss: 1.59262322e-06
Iter: 1350 loss: 1.59168235e-06
Iter: 1351 loss: 1.59139927e-06
Iter: 1352 loss: 1.59076058e-06
Iter: 1353 loss: 1.59169394e-06
Iter: 1354 loss: 1.59017941e-06
Iter: 1355 loss: 1.58947444e-06
Iter: 1356 loss: 1.60156196e-06
Iter: 1357 loss: 1.58944101e-06
Iter: 1358 loss: 1.58890145e-06
Iter: 1359 loss: 1.58861872e-06
Iter: 1360 loss: 1.58840021e-06
Iter: 1361 loss: 1.5874715e-06
Iter: 1362 loss: 1.5925109e-06
Iter: 1363 loss: 1.58755495e-06
Iter: 1364 loss: 1.58689932e-06
Iter: 1365 loss: 1.58919443e-06
Iter: 1366 loss: 1.58673629e-06
Iter: 1367 loss: 1.58585817e-06
Iter: 1368 loss: 1.58714965e-06
Iter: 1369 loss: 1.58548028e-06
Iter: 1370 loss: 1.58486341e-06
Iter: 1371 loss: 1.58659873e-06
Iter: 1372 loss: 1.58468993e-06
Iter: 1373 loss: 1.58410808e-06
Iter: 1374 loss: 1.5837893e-06
Iter: 1375 loss: 1.58349189e-06
Iter: 1376 loss: 1.58268597e-06
Iter: 1377 loss: 1.58643388e-06
Iter: 1378 loss: 1.58255193e-06
Iter: 1379 loss: 1.58173793e-06
Iter: 1380 loss: 1.58279727e-06
Iter: 1381 loss: 1.58152397e-06
Iter: 1382 loss: 1.58106e-06
Iter: 1383 loss: 1.58093576e-06
Iter: 1384 loss: 1.5805299e-06
Iter: 1385 loss: 1.58310922e-06
Iter: 1386 loss: 1.58046316e-06
Iter: 1387 loss: 1.58024989e-06
Iter: 1388 loss: 1.57954241e-06
Iter: 1389 loss: 1.58378771e-06
Iter: 1390 loss: 1.57935722e-06
Iter: 1391 loss: 1.57849047e-06
Iter: 1392 loss: 1.58090779e-06
Iter: 1393 loss: 1.57820318e-06
Iter: 1394 loss: 1.57741499e-06
Iter: 1395 loss: 1.57916952e-06
Iter: 1396 loss: 1.57714112e-06
Iter: 1397 loss: 1.57628233e-06
Iter: 1398 loss: 1.57800832e-06
Iter: 1399 loss: 1.57597196e-06
Iter: 1400 loss: 1.57513546e-06
Iter: 1401 loss: 1.58686316e-06
Iter: 1402 loss: 1.57515092e-06
Iter: 1403 loss: 1.57463512e-06
Iter: 1404 loss: 1.57515581e-06
Iter: 1405 loss: 1.57435954e-06
Iter: 1406 loss: 1.5736822e-06
Iter: 1407 loss: 1.57425359e-06
Iter: 1408 loss: 1.57334603e-06
Iter: 1409 loss: 1.57271654e-06
Iter: 1410 loss: 1.57535271e-06
Iter: 1411 loss: 1.57263639e-06
Iter: 1412 loss: 1.57192994e-06
Iter: 1413 loss: 1.57124475e-06
Iter: 1414 loss: 1.57119939e-06
Iter: 1415 loss: 1.57038676e-06
Iter: 1416 loss: 1.57040427e-06
Iter: 1417 loss: 1.56995225e-06
Iter: 1418 loss: 1.5699967e-06
Iter: 1419 loss: 1.56951319e-06
Iter: 1420 loss: 1.56902195e-06
Iter: 1421 loss: 1.56893759e-06
Iter: 1422 loss: 1.56832039e-06
Iter: 1423 loss: 1.56919145e-06
Iter: 1424 loss: 1.56803094e-06
Iter: 1425 loss: 1.56733097e-06
Iter: 1426 loss: 1.56635815e-06
Iter: 1427 loss: 1.56616272e-06
Iter: 1428 loss: 1.56531871e-06
Iter: 1429 loss: 1.56824615e-06
Iter: 1430 loss: 1.56506496e-06
Iter: 1431 loss: 1.56388762e-06
Iter: 1432 loss: 1.56968122e-06
Iter: 1433 loss: 1.56370959e-06
Iter: 1434 loss: 1.56309807e-06
Iter: 1435 loss: 1.56607939e-06
Iter: 1436 loss: 1.56311899e-06
Iter: 1437 loss: 1.56252122e-06
Iter: 1438 loss: 1.56649025e-06
Iter: 1439 loss: 1.56239116e-06
Iter: 1440 loss: 1.56194653e-06
Iter: 1441 loss: 1.56205056e-06
Iter: 1442 loss: 1.56167289e-06
Iter: 1443 loss: 1.56104943e-06
Iter: 1444 loss: 1.56226793e-06
Iter: 1445 loss: 1.56080114e-06
Iter: 1446 loss: 1.56017029e-06
Iter: 1447 loss: 1.5620044e-06
Iter: 1448 loss: 1.55982718e-06
Iter: 1449 loss: 1.55924181e-06
Iter: 1450 loss: 1.55886801e-06
Iter: 1451 loss: 1.55868941e-06
Iter: 1452 loss: 1.55803855e-06
Iter: 1453 loss: 1.558142e-06
Iter: 1454 loss: 1.55747307e-06
Iter: 1455 loss: 1.55860812e-06
Iter: 1456 loss: 1.55711678e-06
Iter: 1457 loss: 1.55674411e-06
Iter: 1458 loss: 1.55653174e-06
Iter: 1459 loss: 1.5562639e-06
Iter: 1460 loss: 1.55566863e-06
Iter: 1461 loss: 1.5568437e-06
Iter: 1462 loss: 1.55544797e-06
Iter: 1463 loss: 1.55472503e-06
Iter: 1464 loss: 1.55422356e-06
Iter: 1465 loss: 1.55399687e-06
Iter: 1466 loss: 1.55308157e-06
Iter: 1467 loss: 1.56098304e-06
Iter: 1468 loss: 1.55298017e-06
Iter: 1469 loss: 1.5522204e-06
Iter: 1470 loss: 1.55270482e-06
Iter: 1471 loss: 1.55174826e-06
Iter: 1472 loss: 1.550874e-06
Iter: 1473 loss: 1.5523533e-06
Iter: 1474 loss: 1.55032808e-06
Iter: 1475 loss: 1.54975464e-06
Iter: 1476 loss: 1.54968984e-06
Iter: 1477 loss: 1.54936583e-06
Iter: 1478 loss: 1.54897111e-06
Iter: 1479 loss: 1.5489353e-06
Iter: 1480 loss: 1.54834129e-06
Iter: 1481 loss: 1.54921122e-06
Iter: 1482 loss: 1.5479377e-06
Iter: 1483 loss: 1.54725524e-06
Iter: 1484 loss: 1.55236762e-06
Iter: 1485 loss: 1.54722272e-06
Iter: 1486 loss: 1.54688428e-06
Iter: 1487 loss: 1.54682618e-06
Iter: 1488 loss: 1.54643419e-06
Iter: 1489 loss: 1.54594068e-06
Iter: 1490 loss: 1.54596273e-06
Iter: 1491 loss: 1.54537281e-06
Iter: 1492 loss: 1.54633824e-06
Iter: 1493 loss: 1.54514532e-06
Iter: 1494 loss: 1.54458837e-06
Iter: 1495 loss: 1.54456484e-06
Iter: 1496 loss: 1.54402937e-06
Iter: 1497 loss: 1.5432413e-06
Iter: 1498 loss: 1.54562804e-06
Iter: 1499 loss: 1.54297459e-06
Iter: 1500 loss: 1.54228701e-06
Iter: 1501 loss: 1.54186216e-06
Iter: 1502 loss: 1.54160341e-06
Iter: 1503 loss: 1.54059421e-06
Iter: 1504 loss: 1.55362181e-06
Iter: 1505 loss: 1.54058125e-06
Iter: 1506 loss: 1.53989868e-06
Iter: 1507 loss: 1.53925112e-06
Iter: 1508 loss: 1.53913152e-06
Iter: 1509 loss: 1.538741e-06
Iter: 1510 loss: 1.53853853e-06
Iter: 1511 loss: 1.53806059e-06
Iter: 1512 loss: 1.53879853e-06
Iter: 1513 loss: 1.53781673e-06
Iter: 1514 loss: 1.53717235e-06
Iter: 1515 loss: 1.53735186e-06
Iter: 1516 loss: 1.53678388e-06
Iter: 1517 loss: 1.53630469e-06
Iter: 1518 loss: 1.54006648e-06
Iter: 1519 loss: 1.53633596e-06
Iter: 1520 loss: 1.53594544e-06
Iter: 1521 loss: 1.5409546e-06
Iter: 1522 loss: 1.5358595e-06
Iter: 1523 loss: 1.53550877e-06
Iter: 1524 loss: 1.53510473e-06
Iter: 1525 loss: 1.535205e-06
Iter: 1526 loss: 1.5345139e-06
Iter: 1527 loss: 1.53364738e-06
Iter: 1528 loss: 1.53368683e-06
Iter: 1529 loss: 1.5330761e-06
Iter: 1530 loss: 1.53312158e-06
Iter: 1531 loss: 1.53268229e-06
Iter: 1532 loss: 1.53192593e-06
Iter: 1533 loss: 1.5318376e-06
Iter: 1534 loss: 1.53110318e-06
Iter: 1535 loss: 1.53533688e-06
Iter: 1536 loss: 1.53105668e-06
Iter: 1537 loss: 1.53036399e-06
Iter: 1538 loss: 1.53049837e-06
Iter: 1539 loss: 1.52985683e-06
Iter: 1540 loss: 1.52900111e-06
Iter: 1541 loss: 1.53282554e-06
Iter: 1542 loss: 1.5290002e-06
Iter: 1543 loss: 1.52830626e-06
Iter: 1544 loss: 1.53391784e-06
Iter: 1545 loss: 1.52817779e-06
Iter: 1546 loss: 1.52775829e-06
Iter: 1547 loss: 1.53172391e-06
Iter: 1548 loss: 1.52773714e-06
Iter: 1549 loss: 1.52739153e-06
Iter: 1550 loss: 1.52655207e-06
Iter: 1551 loss: 1.53354654e-06
Iter: 1552 loss: 1.52659595e-06
Iter: 1553 loss: 1.52665837e-06
Iter: 1554 loss: 1.52618225e-06
Iter: 1555 loss: 1.52578946e-06
Iter: 1556 loss: 1.52614416e-06
Iter: 1557 loss: 1.52561358e-06
Iter: 1558 loss: 1.52530333e-06
Iter: 1559 loss: 1.52464918e-06
Iter: 1560 loss: 1.52468897e-06
Iter: 1561 loss: 1.52412213e-06
Iter: 1562 loss: 1.52904897e-06
Iter: 1563 loss: 1.52415714e-06
Iter: 1564 loss: 1.5236144e-06
Iter: 1565 loss: 1.52317989e-06
Iter: 1566 loss: 1.52297162e-06
Iter: 1567 loss: 1.52236146e-06
Iter: 1568 loss: 1.52471671e-06
Iter: 1569 loss: 1.52227267e-06
Iter: 1570 loss: 1.52157543e-06
Iter: 1571 loss: 1.52322218e-06
Iter: 1572 loss: 1.52126358e-06
Iter: 1573 loss: 1.52077496e-06
Iter: 1574 loss: 1.52366158e-06
Iter: 1575 loss: 1.52066013e-06
Iter: 1576 loss: 1.52001689e-06
Iter: 1577 loss: 1.52049824e-06
Iter: 1578 loss: 1.51970585e-06
Iter: 1579 loss: 1.51923791e-06
Iter: 1580 loss: 1.52587165e-06
Iter: 1581 loss: 1.5192328e-06
Iter: 1582 loss: 1.5188682e-06
Iter: 1583 loss: 1.51847541e-06
Iter: 1584 loss: 1.51836412e-06
Iter: 1585 loss: 1.51782456e-06
Iter: 1586 loss: 1.51828226e-06
Iter: 1587 loss: 1.51738845e-06
Iter: 1588 loss: 1.51668974e-06
Iter: 1589 loss: 1.51983772e-06
Iter: 1590 loss: 1.51650488e-06
Iter: 1591 loss: 1.51653899e-06
Iter: 1592 loss: 1.51635641e-06
Iter: 1593 loss: 1.51613392e-06
Iter: 1594 loss: 1.51562313e-06
Iter: 1595 loss: 1.51961717e-06
Iter: 1596 loss: 1.51546374e-06
Iter: 1597 loss: 1.51486881e-06
Iter: 1598 loss: 1.5161927e-06
Iter: 1599 loss: 1.51469339e-06
Iter: 1600 loss: 1.51413064e-06
Iter: 1601 loss: 1.51520419e-06
Iter: 1602 loss: 1.51400286e-06
Iter: 1603 loss: 1.51322865e-06
Iter: 1604 loss: 1.51505174e-06
Iter: 1605 loss: 1.51300674e-06
Iter: 1606 loss: 1.51248344e-06
Iter: 1607 loss: 1.51373422e-06
Iter: 1608 loss: 1.5123012e-06
Iter: 1609 loss: 1.51166012e-06
Iter: 1610 loss: 1.5125911e-06
Iter: 1611 loss: 1.51127256e-06
Iter: 1612 loss: 1.51075619e-06
Iter: 1613 loss: 1.51705626e-06
Iter: 1614 loss: 1.51067775e-06
Iter: 1615 loss: 1.51045606e-06
Iter: 1616 loss: 1.51115751e-06
Iter: 1617 loss: 1.51030395e-06
Iter: 1618 loss: 1.50975438e-06
Iter: 1619 loss: 1.50996925e-06
Iter: 1620 loss: 1.50940014e-06
Iter: 1621 loss: 1.50909761e-06
Iter: 1622 loss: 1.50907226e-06
Iter: 1623 loss: 1.50878122e-06
Iter: 1624 loss: 1.5080775e-06
Iter: 1625 loss: 1.51005281e-06
Iter: 1626 loss: 1.50785183e-06
Iter: 1627 loss: 1.50805522e-06
Iter: 1628 loss: 1.50762094e-06
Iter: 1629 loss: 1.50754283e-06
Iter: 1630 loss: 1.50703659e-06
Iter: 1631 loss: 1.50719256e-06
Iter: 1632 loss: 1.5066214e-06
Iter: 1633 loss: 1.5059677e-06
Iter: 1634 loss: 1.50978371e-06
Iter: 1635 loss: 1.50587994e-06
Iter: 1636 loss: 1.50524158e-06
Iter: 1637 loss: 1.50675726e-06
Iter: 1638 loss: 1.50507822e-06
Iter: 1639 loss: 1.5045332e-06
Iter: 1640 loss: 1.50949222e-06
Iter: 1641 loss: 1.50463666e-06
Iter: 1642 loss: 1.50412211e-06
Iter: 1643 loss: 1.50386882e-06
Iter: 1644 loss: 1.50374524e-06
Iter: 1645 loss: 1.50331584e-06
Iter: 1646 loss: 1.50853145e-06
Iter: 1647 loss: 1.50340406e-06
Iter: 1648 loss: 1.5029309e-06
Iter: 1649 loss: 1.50306528e-06
Iter: 1650 loss: 1.50273411e-06
Iter: 1651 loss: 1.50226094e-06
Iter: 1652 loss: 1.50292567e-06
Iter: 1653 loss: 1.50213009e-06
Iter: 1654 loss: 1.50169853e-06
Iter: 1655 loss: 1.50655023e-06
Iter: 1656 loss: 1.50171275e-06
Iter: 1657 loss: 1.50133906e-06
Iter: 1658 loss: 1.50152187e-06
Iter: 1659 loss: 1.50115761e-06
Iter: 1660 loss: 1.50080928e-06
Iter: 1661 loss: 1.5009532e-06
Iter: 1662 loss: 1.5005877e-06
Iter: 1663 loss: 1.50019537e-06
Iter: 1664 loss: 1.50723974e-06
Iter: 1665 loss: 1.50016581e-06
Iter: 1666 loss: 1.49998448e-06
Iter: 1667 loss: 1.49951904e-06
Iter: 1668 loss: 1.50739902e-06
Iter: 1669 loss: 1.49952245e-06
Iter: 1670 loss: 1.49922198e-06
Iter: 1671 loss: 1.49898028e-06
Iter: 1672 loss: 1.49874973e-06
Iter: 1673 loss: 1.49835591e-06
Iter: 1674 loss: 1.50066364e-06
Iter: 1675 loss: 1.49824223e-06
Iter: 1676 loss: 1.49765629e-06
Iter: 1677 loss: 1.4986573e-06
Iter: 1678 loss: 1.4972826e-06
Iter: 1679 loss: 1.49698826e-06
Iter: 1680 loss: 1.5027033e-06
Iter: 1681 loss: 1.49699349e-06
Iter: 1682 loss: 1.49659365e-06
Iter: 1683 loss: 1.49611287e-06
Iter: 1684 loss: 1.49610901e-06
Iter: 1685 loss: 1.49555945e-06
Iter: 1686 loss: 1.5005196e-06
Iter: 1687 loss: 1.49555206e-06
Iter: 1688 loss: 1.49525613e-06
Iter: 1689 loss: 1.49555251e-06
Iter: 1690 loss: 1.49507548e-06
Iter: 1691 loss: 1.49465382e-06
Iter: 1692 loss: 1.49463096e-06
Iter: 1693 loss: 1.49449772e-06
Iter: 1694 loss: 1.49432583e-06
Iter: 1695 loss: 1.49428899e-06
Iter: 1696 loss: 1.49396749e-06
Iter: 1697 loss: 1.49598247e-06
Iter: 1698 loss: 1.49391019e-06
Iter: 1699 loss: 1.49370749e-06
Iter: 1700 loss: 1.49345919e-06
Iter: 1701 loss: 1.49345612e-06
Iter: 1702 loss: 1.49302377e-06
Iter: 1703 loss: 1.49374159e-06
Iter: 1704 loss: 1.49293601e-06
Iter: 1705 loss: 1.49244443e-06
Iter: 1706 loss: 1.49343407e-06
Iter: 1707 loss: 1.49239736e-06
Iter: 1708 loss: 1.49184825e-06
Iter: 1709 loss: 1.49377547e-06
Iter: 1710 loss: 1.49165487e-06
Iter: 1711 loss: 1.49140101e-06
Iter: 1712 loss: 1.4912107e-06
Iter: 1713 loss: 1.49109428e-06
Iter: 1714 loss: 1.4904889e-06
Iter: 1715 loss: 1.4937915e-06
Iter: 1716 loss: 1.49030552e-06
Iter: 1717 loss: 1.48999789e-06
Iter: 1718 loss: 1.49236223e-06
Iter: 1719 loss: 1.48994729e-06
Iter: 1720 loss: 1.48956394e-06
Iter: 1721 loss: 1.4893385e-06
Iter: 1722 loss: 1.48913875e-06
Iter: 1723 loss: 1.48882714e-06
Iter: 1724 loss: 1.49171149e-06
Iter: 1725 loss: 1.48880645e-06
Iter: 1726 loss: 1.48837114e-06
Iter: 1727 loss: 1.49014193e-06
Iter: 1728 loss: 1.48828133e-06
Iter: 1729 loss: 1.48806282e-06
Iter: 1730 loss: 1.49127163e-06
Iter: 1731 loss: 1.48810273e-06
Iter: 1732 loss: 1.48786171e-06
Iter: 1733 loss: 1.48769982e-06
Iter: 1734 loss: 1.48766912e-06
Iter: 1735 loss: 1.4874073e-06
Iter: 1736 loss: 1.48693505e-06
Iter: 1737 loss: 1.49455946e-06
Iter: 1738 loss: 1.48694949e-06
Iter: 1739 loss: 1.48649906e-06
Iter: 1740 loss: 1.48967183e-06
Iter: 1741 loss: 1.48645859e-06
Iter: 1742 loss: 1.48608501e-06
Iter: 1743 loss: 1.4885029e-06
Iter: 1744 loss: 1.48587867e-06
Iter: 1745 loss: 1.48537026e-06
Iter: 1746 loss: 1.48531308e-06
Iter: 1747 loss: 1.48505023e-06
Iter: 1748 loss: 1.48453307e-06
Iter: 1749 loss: 1.48774529e-06
Iter: 1750 loss: 1.48441939e-06
Iter: 1751 loss: 1.48401364e-06
Iter: 1752 loss: 1.48423521e-06
Iter: 1753 loss: 1.48373056e-06
Iter: 1754 loss: 1.48325103e-06
Iter: 1755 loss: 1.48865865e-06
Iter: 1756 loss: 1.48325955e-06
Iter: 1757 loss: 1.48288e-06
Iter: 1758 loss: 1.4828347e-06
Iter: 1759 loss: 1.48255765e-06
Iter: 1760 loss: 1.4821029e-06
Iter: 1761 loss: 1.48455786e-06
Iter: 1762 loss: 1.48211268e-06
Iter: 1763 loss: 1.48169897e-06
Iter: 1764 loss: 1.48169806e-06
Iter: 1765 loss: 1.48152355e-06
Iter: 1766 loss: 1.48262893e-06
Iter: 1767 loss: 1.48141794e-06
Iter: 1768 loss: 1.48126105e-06
Iter: 1769 loss: 1.4807531e-06
Iter: 1770 loss: 1.48654635e-06
Iter: 1771 loss: 1.48062861e-06
Iter: 1772 loss: 1.48017216e-06
Iter: 1773 loss: 1.48078379e-06
Iter: 1774 loss: 1.47996536e-06
Iter: 1775 loss: 1.47934588e-06
Iter: 1776 loss: 1.4796135e-06
Iter: 1777 loss: 1.47891615e-06
Iter: 1778 loss: 1.47822482e-06
Iter: 1779 loss: 1.47988874e-06
Iter: 1780 loss: 1.4779082e-06
Iter: 1781 loss: 1.47703975e-06
Iter: 1782 loss: 1.48538368e-06
Iter: 1783 loss: 1.47705839e-06
Iter: 1784 loss: 1.47671346e-06
Iter: 1785 loss: 1.47715878e-06
Iter: 1786 loss: 1.47642697e-06
Iter: 1787 loss: 1.47589253e-06
Iter: 1788 loss: 1.47590026e-06
Iter: 1789 loss: 1.47541232e-06
Iter: 1790 loss: 1.47506432e-06
Iter: 1791 loss: 1.48183517e-06
Iter: 1792 loss: 1.47501544e-06
Iter: 1793 loss: 1.47454432e-06
Iter: 1794 loss: 1.47431354e-06
Iter: 1795 loss: 1.47405206e-06
Iter: 1796 loss: 1.47367086e-06
Iter: 1797 loss: 1.47371293e-06
Iter: 1798 loss: 1.47325875e-06
Iter: 1799 loss: 1.47529954e-06
Iter: 1800 loss: 1.47318519e-06
Iter: 1801 loss: 1.47284857e-06
Iter: 1802 loss: 1.47363789e-06
Iter: 1803 loss: 1.47266394e-06
Iter: 1804 loss: 1.47225751e-06
Iter: 1805 loss: 1.47198557e-06
Iter: 1806 loss: 1.47180049e-06
Iter: 1807 loss: 1.47131504e-06
Iter: 1808 loss: 1.47161609e-06
Iter: 1809 loss: 1.47095079e-06
Iter: 1810 loss: 1.47035098e-06
Iter: 1811 loss: 1.47158516e-06
Iter: 1812 loss: 1.47019205e-06
Iter: 1813 loss: 1.46941079e-06
Iter: 1814 loss: 1.47317382e-06
Iter: 1815 loss: 1.46931131e-06
Iter: 1816 loss: 1.46880734e-06
Iter: 1817 loss: 1.47118033e-06
Iter: 1818 loss: 1.46876914e-06
Iter: 1819 loss: 1.46819093e-06
Iter: 1820 loss: 1.46930643e-06
Iter: 1821 loss: 1.46803143e-06
Iter: 1822 loss: 1.46760954e-06
Iter: 1823 loss: 1.46809475e-06
Iter: 1824 loss: 1.46732827e-06
Iter: 1825 loss: 1.46679247e-06
Iter: 1826 loss: 1.47035689e-06
Iter: 1827 loss: 1.466801e-06
Iter: 1828 loss: 1.46643697e-06
Iter: 1829 loss: 1.46816456e-06
Iter: 1830 loss: 1.46640025e-06
Iter: 1831 loss: 1.46600394e-06
Iter: 1832 loss: 1.46636546e-06
Iter: 1833 loss: 1.46579691e-06
Iter: 1834 loss: 1.46534035e-06
Iter: 1835 loss: 1.47053811e-06
Iter: 1836 loss: 1.46532705e-06
Iter: 1837 loss: 1.46510934e-06
Iter: 1838 loss: 1.46544915e-06
Iter: 1839 loss: 1.46501259e-06
Iter: 1840 loss: 1.46487776e-06
Iter: 1841 loss: 1.46440618e-06
Iter: 1842 loss: 1.47158642e-06
Iter: 1843 loss: 1.46439766e-06
Iter: 1844 loss: 1.46379966e-06
Iter: 1845 loss: 1.46474213e-06
Iter: 1846 loss: 1.46355114e-06
Iter: 1847 loss: 1.46288221e-06
Iter: 1848 loss: 1.46258128e-06
Iter: 1849 loss: 1.46226216e-06
Iter: 1850 loss: 1.46171851e-06
Iter: 1851 loss: 1.46170737e-06
Iter: 1852 loss: 1.46125899e-06
Iter: 1853 loss: 1.46188972e-06
Iter: 1854 loss: 1.46098455e-06
Iter: 1855 loss: 1.46056357e-06
Iter: 1856 loss: 1.4647394e-06
Iter: 1857 loss: 1.46045522e-06
Iter: 1858 loss: 1.46015566e-06
Iter: 1859 loss: 1.45976287e-06
Iter: 1860 loss: 1.45964054e-06
Iter: 1861 loss: 1.45912566e-06
Iter: 1862 loss: 1.46135642e-06
Iter: 1863 loss: 1.45900208e-06
Iter: 1864 loss: 1.45857507e-06
Iter: 1865 loss: 1.46231582e-06
Iter: 1866 loss: 1.45855688e-06
Iter: 1867 loss: 1.45831e-06
Iter: 1868 loss: 1.4617317e-06
Iter: 1869 loss: 1.45836009e-06
Iter: 1870 loss: 1.45812146e-06
Iter: 1871 loss: 1.45837987e-06
Iter: 1872 loss: 1.45800163e-06
Iter: 1873 loss: 1.45762851e-06
Iter: 1874 loss: 1.45746912e-06
Iter: 1875 loss: 1.45742104e-06
Iter: 1876 loss: 1.45709237e-06
Iter: 1877 loss: 1.45947206e-06
Iter: 1878 loss: 1.45711556e-06
Iter: 1879 loss: 1.45666809e-06
Iter: 1880 loss: 1.45622221e-06
Iter: 1881 loss: 1.46710181e-06
Iter: 1882 loss: 1.45619356e-06
Iter: 1883 loss: 1.45575075e-06
Iter: 1884 loss: 1.45802176e-06
Iter: 1885 loss: 1.4556058e-06
Iter: 1886 loss: 1.4550003e-06
Iter: 1887 loss: 1.45690024e-06
Iter: 1888 loss: 1.45495073e-06
Iter: 1889 loss: 1.45461922e-06
Iter: 1890 loss: 1.45448939e-06
Iter: 1891 loss: 1.45429703e-06
Iter: 1892 loss: 1.4537228e-06
Iter: 1893 loss: 1.45913145e-06
Iter: 1894 loss: 1.45371462e-06
Iter: 1895 loss: 1.45323474e-06
Iter: 1896 loss: 1.4538997e-06
Iter: 1897 loss: 1.45301101e-06
Iter: 1898 loss: 1.45257548e-06
Iter: 1899 loss: 1.45327397e-06
Iter: 1900 loss: 1.45229399e-06
Iter: 1901 loss: 1.45186164e-06
Iter: 1902 loss: 1.45232195e-06
Iter: 1903 loss: 1.45154331e-06
Iter: 1904 loss: 1.45136721e-06
Iter: 1905 loss: 1.45124727e-06
Iter: 1906 loss: 1.45099841e-06
Iter: 1907 loss: 1.4513929e-06
Iter: 1908 loss: 1.45087893e-06
Iter: 1909 loss: 1.45071022e-06
Iter: 1910 loss: 1.45068407e-06
Iter: 1911 loss: 1.45056288e-06
Iter: 1912 loss: 1.45025763e-06
Iter: 1913 loss: 1.45012052e-06
Iter: 1914 loss: 1.45002377e-06
Iter: 1915 loss: 1.44947421e-06
Iter: 1916 loss: 1.45044032e-06
Iter: 1917 loss: 1.44917431e-06
Iter: 1918 loss: 1.44886872e-06
Iter: 1919 loss: 1.44888179e-06
Iter: 1920 loss: 1.44849605e-06
Iter: 1921 loss: 1.44799128e-06
Iter: 1922 loss: 1.45060028e-06
Iter: 1923 loss: 1.44776027e-06
Iter: 1924 loss: 1.44733804e-06
Iter: 1925 loss: 1.44718911e-06
Iter: 1926 loss: 1.44694218e-06
Iter: 1927 loss: 1.44639398e-06
Iter: 1928 loss: 1.45137119e-06
Iter: 1929 loss: 1.44630758e-06
Iter: 1930 loss: 1.44597107e-06
Iter: 1931 loss: 1.4465802e-06
Iter: 1932 loss: 1.44560977e-06
Iter: 1933 loss: 1.44522437e-06
Iter: 1934 loss: 1.44870273e-06
Iter: 1935 loss: 1.44517969e-06
Iter: 1936 loss: 1.44473574e-06
Iter: 1937 loss: 1.44570606e-06
Iter: 1938 loss: 1.44463502e-06
Iter: 1939 loss: 1.44449882e-06
Iter: 1940 loss: 1.44441333e-06
Iter: 1941 loss: 1.44429805e-06
Iter: 1942 loss: 1.44415981e-06
Iter: 1943 loss: 1.44586852e-06
Iter: 1944 loss: 1.44403089e-06
Iter: 1945 loss: 1.44358103e-06
Iter: 1946 loss: 1.44495357e-06
Iter: 1947 loss: 1.4433698e-06
Iter: 1948 loss: 1.44298201e-06
Iter: 1949 loss: 1.44573403e-06
Iter: 1950 loss: 1.44301157e-06
Iter: 1951 loss: 1.44276373e-06
Iter: 1952 loss: 1.44229011e-06
Iter: 1953 loss: 1.44228034e-06
Iter: 1954 loss: 1.44193723e-06
Iter: 1955 loss: 1.44363662e-06
Iter: 1956 loss: 1.44180876e-06
Iter: 1957 loss: 1.44131343e-06
Iter: 1958 loss: 1.44114597e-06
Iter: 1959 loss: 1.4408231e-06
Iter: 1960 loss: 1.44030537e-06
Iter: 1961 loss: 1.44136629e-06
Iter: 1962 loss: 1.44007595e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.4 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi2.4
+ date
Sun Nov  8 07:40:41 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2/300_100_100_100_1 --function f1 --psi -1 --phi 2.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.4/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a967c0400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a7001bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a7001bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a96811ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a96819268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a96819510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a5038bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a503587b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a5036a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a502b9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a502b9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a50253840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a50258510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a502589d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a5033eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a5033ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a503287b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a50328378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a5016a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a5023bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a501e36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a501a2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a500d2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a500cd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a500cd400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a500a1378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a500178c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a500448c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a50044730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a50143598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a500757b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a1052d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a1052dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a1054b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a105d69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8a10583840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 1.996586
test_loss: 2.0446563
train_loss: 1.9967811
test_loss: 2.3387434
train_loss: 1.9966277
test_loss: 2.4500756
train_loss: 1.9627973
test_loss: 2.0681565
train_loss: 1.9698579
test_loss: 2.0231428
train_loss: 1.9973135
test_loss: 2.7927258
train_loss: 1.980069
test_loss: 1.9904696
train_loss: 1.9949421
test_loss: 1.996161
train_loss: 1.9982219
test_loss: 2.0257666
train_loss: 2.2940307
test_loss: 2.0106137
train_loss: 1.9791064
test_loss: 2.052192
train_loss: 0.67977077
test_loss: 0.77593017
train_loss: 0.71966016
test_loss: 0.777722
train_loss: 0.7367671
test_loss: 0.77773654
train_loss: 0.715113
test_loss: 0.7779788
train_loss: 0.6749544
test_loss: 0.7778516
train_loss: 0.7694091
test_loss: 0.7772731
train_loss: 0.6673956
test_loss: 0.7778086
train_loss: 0.7527427
test_loss: 0.7779593
train_loss: 0.77218914
test_loss: 0.77754104
train_loss: 0.7415994
test_loss: 0.7781717
train_loss: 0.7701434
test_loss: 0.77780175
train_loss: 0.733312
test_loss: 0.77767366
train_loss: 0.6653547
test_loss: 0.7776262
train_loss: 0.693357
test_loss: 0.7778086
train_loss: 0.7230929
test_loss: 0.777953
train_loss: 0.71445906
test_loss: 0.77760506
train_loss: 0.7696359
test_loss: 0.7776445
train_loss: 0.756477
test_loss: 0.7778869
train_loss: 0.80537367
test_loss: 0.77796173
train_loss: 0.7778744
test_loss: 0.77768016
train_loss: 0.7872044
test_loss: 0.7771651
train_loss: 0.78171873
test_loss: 0.7771298
train_loss: 0.7059046
test_loss: 0.7774443
train_loss: 0.71906793
test_loss: 0.7774103
train_loss: 0.78014505
test_loss: 0.7773072
train_loss: 0.7281463
test_loss: 0.7771884
train_loss: 0.7797948
test_loss: 0.77729267
train_loss: 0.7381781
test_loss: 0.7771088
train_loss: 0.7352347
test_loss: 0.7773537
train_loss: 0.76075464
test_loss: 0.77698916
train_loss: 0.6988905
test_loss: 0.77733
train_loss: 0.7231808
test_loss: 0.7766259
train_loss: 0.70098895
test_loss: 0.7770069
train_loss: 0.7337152
test_loss: 0.77661276
train_loss: 0.6832286
test_loss: 0.77656865
train_loss: 0.72082055
test_loss: 0.77663434
train_loss: 0.72890556
test_loss: 0.7763372
train_loss: 0.7806079
test_loss: 0.77667737
train_loss: 0.79062593
test_loss: 0.77625054
train_loss: 0.71823174
test_loss: 0.77607334
train_loss: 0.7490751
test_loss: 0.77596426
train_loss: 0.6794662
test_loss: 0.77608114
train_loss: 0.7600175
test_loss: 0.7758931
train_loss: 0.80870163
test_loss: 0.7756741
train_loss: 0.7568512
test_loss: 0.7756919
train_loss: 0.75316143
test_loss: 0.77561975
train_loss: 0.73051184
test_loss: 0.77512854
train_loss: 0.7150777
test_loss: 0.77512974
train_loss: 0.74817336
test_loss: 0.77510583
train_loss: 0.7379488
test_loss: 0.7751774
train_loss: 0.7000699
test_loss: 0.77459204
train_loss: 0.6880709
test_loss: 0.774337
train_loss: 0.7449992
test_loss: 0.774317
train_loss: 0.7394806
test_loss: 0.77398723
train_loss: 0.76216567
test_loss: 0.7739993
train_loss: 0.73889774
test_loss: 0.77355903
train_loss: 0.726423
test_loss: 0.773525
train_loss: 0.7596014
test_loss: 0.77305865
train_loss: 0.7274357
test_loss: 0.77325964
train_loss: 0.72814184
test_loss: 0.77270687
train_loss: 0.8184839
test_loss: 0.7731263
train_loss: 0.80631495
test_loss: 0.77199817
train_loss: 0.76885164
test_loss: 0.77160347
train_loss: 0.7246457
test_loss: 0.77176934
train_loss: 0.72467434
test_loss: 0.7712128
train_loss: 0.74856454
test_loss: 0.7708127
train_loss: 0.7287293
test_loss: 0.77022403
train_loss: 0.7219293
test_loss: 0.7701519
train_loss: 0.70399314
test_loss: 0.7696003
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi2.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.4/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -1 --phi 2.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi2.4/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ee025c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ee01eed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ee01eed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ee013f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ee013fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ee00ae268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ee007f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ee00377b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ee00451e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ee0045ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ee00457b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4edffb2378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4edffa7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4edff4bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4edff1a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4edfed4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4edfed4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4edfefb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4edfec09d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4edfefb510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4edfe52e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4edfe1d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4edfe1d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ebbc43620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4edfe1d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ebbc406a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ebbbf5d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ebbbc7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ebbbc70d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ebbb5eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ebbb267b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ebbaee158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ebbaee2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ebbb028c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ebba91bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ebba69598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.855567
Iter: 2 loss: 0.324915528
Iter: 3 loss: 0.298307
Iter: 4 loss: 0.189980626
Iter: 5 loss: 0.188254416
Iter: 6 loss: 0.124708235
Iter: 7 loss: 0.524774551
Iter: 8 loss: 0.124856792
Iter: 9 loss: 0.105043702
Iter: 10 loss: 0.0754702166
Iter: 11 loss: 0.466234058
Iter: 12 loss: 0.0754352584
Iter: 13 loss: 0.0849611461
Iter: 14 loss: 0.0621467903
Iter: 15 loss: 0.0542330667
Iter: 16 loss: 0.0667081624
Iter: 17 loss: 0.0511271618
Iter: 18 loss: 0.0433608592
Iter: 19 loss: 0.0481403284
Iter: 20 loss: 0.0379570127
Iter: 21 loss: 0.0292541571
Iter: 22 loss: 0.0339361764
Iter: 23 loss: 0.0257726535
Iter: 24 loss: 0.0225309543
Iter: 25 loss: 0.0225207768
Iter: 26 loss: 0.0209429674
Iter: 27 loss: 0.0348706916
Iter: 28 loss: 0.0208871905
Iter: 29 loss: 0.0192681216
Iter: 30 loss: 0.0206317659
Iter: 31 loss: 0.018285485
Iter: 32 loss: 0.0163898598
Iter: 33 loss: 0.028291367
Iter: 34 loss: 0.0161592364
Iter: 35 loss: 0.0147376563
Iter: 36 loss: 0.0217419546
Iter: 37 loss: 0.0145529928
Iter: 38 loss: 0.0137355607
Iter: 39 loss: 0.0137536507
Iter: 40 loss: 0.0130525073
Iter: 41 loss: 0.012157863
Iter: 42 loss: 0.0138270697
Iter: 43 loss: 0.0118326638
Iter: 44 loss: 0.0110038556
Iter: 45 loss: 0.0158152357
Iter: 46 loss: 0.010890441
Iter: 47 loss: 0.0104078362
Iter: 48 loss: 0.017938843
Iter: 49 loss: 0.0104053244
Iter: 50 loss: 0.0100645013
Iter: 51 loss: 0.0116071422
Iter: 52 loss: 0.0100136092
Iter: 53 loss: 0.00963129662
Iter: 54 loss: 0.00994642358
Iter: 55 loss: 0.00937922858
Iter: 56 loss: 0.00903761
Iter: 57 loss: 0.011362642
Iter: 58 loss: 0.0090131443
Iter: 59 loss: 0.00878237
Iter: 60 loss: 0.0100421775
Iter: 61 loss: 0.00874625426
Iter: 62 loss: 0.00851473212
Iter: 63 loss: 0.00883963518
Iter: 64 loss: 0.00839711912
Iter: 65 loss: 0.00813031755
Iter: 66 loss: 0.00868454576
Iter: 67 loss: 0.00802560337
Iter: 68 loss: 0.00779066654
Iter: 69 loss: 0.00890577398
Iter: 70 loss: 0.00773600861
Iter: 71 loss: 0.00754028466
Iter: 72 loss: 0.00872246455
Iter: 73 loss: 0.00752197811
Iter: 74 loss: 0.00733058201
Iter: 75 loss: 0.00842293
Iter: 76 loss: 0.00729646767
Iter: 77 loss: 0.00713030295
Iter: 78 loss: 0.00776045863
Iter: 79 loss: 0.00709542446
Iter: 80 loss: 0.00693578
Iter: 81 loss: 0.00693184976
Iter: 82 loss: 0.0068086707
Iter: 83 loss: 0.00728537794
Iter: 84 loss: 0.00677915663
Iter: 85 loss: 0.00666776858
Iter: 86 loss: 0.00653338758
Iter: 87 loss: 0.00652263081
Iter: 88 loss: 0.00634384202
Iter: 89 loss: 0.00796351396
Iter: 90 loss: 0.00632154383
Iter: 91 loss: 0.00613114657
Iter: 92 loss: 0.00698681735
Iter: 93 loss: 0.00609780941
Iter: 94 loss: 0.00592246652
Iter: 95 loss: 0.00620708242
Iter: 96 loss: 0.00582641503
Iter: 97 loss: 0.00567246787
Iter: 98 loss: 0.00738529116
Iter: 99 loss: 0.00567067973
Iter: 100 loss: 0.00557825062
Iter: 101 loss: 0.00609341776
Iter: 102 loss: 0.00555709377
Iter: 103 loss: 0.00543659553
Iter: 104 loss: 0.00559353
Iter: 105 loss: 0.00537278783
Iter: 106 loss: 0.00521298265
Iter: 107 loss: 0.0055430159
Iter: 108 loss: 0.00514336023
Iter: 109 loss: 0.00502557261
Iter: 110 loss: 0.00502535189
Iter: 111 loss: 0.0049503725
Iter: 112 loss: 0.00509906933
Iter: 113 loss: 0.00492389686
Iter: 114 loss: 0.00483032456
Iter: 115 loss: 0.00663465634
Iter: 116 loss: 0.00482911337
Iter: 117 loss: 0.00476829335
Iter: 118 loss: 0.00490768719
Iter: 119 loss: 0.00474551599
Iter: 120 loss: 0.00466412632
Iter: 121 loss: 0.00467312336
Iter: 122 loss: 0.00459632557
Iter: 123 loss: 0.00448913081
Iter: 124 loss: 0.00497149723
Iter: 125 loss: 0.00446821703
Iter: 126 loss: 0.00437938794
Iter: 127 loss: 0.00535942521
Iter: 128 loss: 0.00437621633
Iter: 129 loss: 0.00429834798
Iter: 130 loss: 0.00468331203
Iter: 131 loss: 0.00428154226
Iter: 132 loss: 0.0042176554
Iter: 133 loss: 0.0046772589
Iter: 134 loss: 0.00421430776
Iter: 135 loss: 0.00411650911
Iter: 136 loss: 0.00573051535
Iter: 137 loss: 0.0041154176
Iter: 138 loss: 0.00404927321
Iter: 139 loss: 0.00434620352
Iter: 140 loss: 0.00403648568
Iter: 141 loss: 0.00400311034
Iter: 142 loss: 0.00398430135
Iter: 143 loss: 0.00394151919
Iter: 144 loss: 0.00394691154
Iter: 145 loss: 0.0039103711
Iter: 146 loss: 0.00383284292
Iter: 147 loss: 0.00452091405
Iter: 148 loss: 0.00382588687
Iter: 149 loss: 0.00376853626
Iter: 150 loss: 0.00454394147
Iter: 151 loss: 0.00376818608
Iter: 152 loss: 0.00373223564
Iter: 153 loss: 0.00370372063
Iter: 154 loss: 0.00369262137
Iter: 155 loss: 0.0036193172
Iter: 156 loss: 0.00423874427
Iter: 157 loss: 0.00361127662
Iter: 158 loss: 0.00355636864
Iter: 159 loss: 0.00410927041
Iter: 160 loss: 0.00355566619
Iter: 161 loss: 0.00350661506
Iter: 162 loss: 0.00377018983
Iter: 163 loss: 0.00349556794
Iter: 164 loss: 0.00345148332
Iter: 165 loss: 0.00348735228
Iter: 166 loss: 0.00342576345
Iter: 167 loss: 0.00337056397
Iter: 168 loss: 0.00342548802
Iter: 169 loss: 0.00333863404
Iter: 170 loss: 0.00328368018
Iter: 171 loss: 0.00360320089
Iter: 172 loss: 0.00327563938
Iter: 173 loss: 0.00324955164
Iter: 174 loss: 0.00342689618
Iter: 175 loss: 0.00324737653
Iter: 176 loss: 0.00321648829
Iter: 177 loss: 0.00338911312
Iter: 178 loss: 0.00321022724
Iter: 179 loss: 0.00316300988
Iter: 180 loss: 0.00333127729
Iter: 181 loss: 0.00314972154
Iter: 182 loss: 0.00311359088
Iter: 183 loss: 0.00337072322
Iter: 184 loss: 0.00310942065
Iter: 185 loss: 0.00309102703
Iter: 186 loss: 0.00316082453
Iter: 187 loss: 0.00308610103
Iter: 188 loss: 0.00307065318
Iter: 189 loss: 0.00310312351
Iter: 190 loss: 0.00306342077
Iter: 191 loss: 0.00303468527
Iter: 192 loss: 0.00309998356
Iter: 193 loss: 0.00302415038
Iter: 194 loss: 0.00299209403
Iter: 195 loss: 0.00304490328
Iter: 196 loss: 0.00297723035
Iter: 197 loss: 0.00293890666
Iter: 198 loss: 0.00315732276
Iter: 199 loss: 0.0029308619
Iter: 200 loss: 0.00289813057
Iter: 201 loss: 0.0029981134
Iter: 202 loss: 0.00288839685
Iter: 203 loss: 0.00286059082
Iter: 204 loss: 0.00320151355
Iter: 205 loss: 0.00285910862
Iter: 206 loss: 0.00283721602
Iter: 207 loss: 0.00298787118
Iter: 208 loss: 0.00283539412
Iter: 209 loss: 0.00281270966
Iter: 210 loss: 0.00293353852
Iter: 211 loss: 0.00280879252
Iter: 212 loss: 0.00278703962
Iter: 213 loss: 0.00279853749
Iter: 214 loss: 0.00277215336
Iter: 215 loss: 0.00275529502
Iter: 216 loss: 0.00285371346
Iter: 217 loss: 0.00275347126
Iter: 218 loss: 0.00273804041
Iter: 219 loss: 0.0027957689
Iter: 220 loss: 0.00273329322
Iter: 221 loss: 0.00271223346
Iter: 222 loss: 0.0027416558
Iter: 223 loss: 0.0027018
Iter: 224 loss: 0.00268180599
Iter: 225 loss: 0.00278217345
Iter: 226 loss: 0.00267668534
Iter: 227 loss: 0.00265911967
Iter: 228 loss: 0.00267866207
Iter: 229 loss: 0.00264975848
Iter: 230 loss: 0.00262956
Iter: 231 loss: 0.00268473
Iter: 232 loss: 0.00262198434
Iter: 233 loss: 0.00260487851
Iter: 234 loss: 0.00270259706
Iter: 235 loss: 0.00260300492
Iter: 236 loss: 0.0025800406
Iter: 237 loss: 0.00268472824
Iter: 238 loss: 0.00257388502
Iter: 239 loss: 0.00256451895
Iter: 240 loss: 0.00255594938
Iter: 241 loss: 0.00254049478
Iter: 242 loss: 0.00260342099
Iter: 243 loss: 0.00253687473
Iter: 244 loss: 0.00252400595
Iter: 245 loss: 0.00252752472
Iter: 246 loss: 0.00251429062
Iter: 247 loss: 0.00250309659
Iter: 248 loss: 0.00252187951
Iter: 249 loss: 0.00249812892
Iter: 250 loss: 0.00248812651
Iter: 251 loss: 0.00249354634
Iter: 252 loss: 0.00248131482
Iter: 253 loss: 0.0024705315
Iter: 254 loss: 0.00246677082
Iter: 255 loss: 0.00246056751
Iter: 256 loss: 0.00244159624
Iter: 257 loss: 0.00249423878
Iter: 258 loss: 0.00243554544
Iter: 259 loss: 0.00242287107
Iter: 260 loss: 0.00259862421
Iter: 261 loss: 0.00242230715
Iter: 262 loss: 0.00240639644
Iter: 263 loss: 0.00254656188
Iter: 264 loss: 0.0024057941
Iter: 265 loss: 0.00238622027
Iter: 266 loss: 0.00243596174
Iter: 267 loss: 0.00237968448
Iter: 268 loss: 0.00236713956
Iter: 269 loss: 0.00238904031
Iter: 270 loss: 0.00236214045
Iter: 271 loss: 0.00235473644
Iter: 272 loss: 0.00235998887
Iter: 273 loss: 0.00235004444
Iter: 274 loss: 0.00234328583
Iter: 275 loss: 0.00239596749
Iter: 276 loss: 0.00234232051
Iter: 277 loss: 0.00231993292
Iter: 278 loss: 0.0023892289
Iter: 279 loss: 0.0023130232
Iter: 280 loss: 0.00231020036
Iter: 281 loss: 0.00230183336
Iter: 282 loss: 0.00228634756
Iter: 283 loss: 0.00230214559
Iter: 284 loss: 0.0022778213
Iter: 285 loss: 0.00226295274
Iter: 286 loss: 0.00231549423
Iter: 287 loss: 0.00225803582
Iter: 288 loss: 0.0022497247
Iter: 289 loss: 0.00230043754
Iter: 290 loss: 0.0022487347
Iter: 291 loss: 0.00223938283
Iter: 292 loss: 0.00225961977
Iter: 293 loss: 0.00223498885
Iter: 294 loss: 0.00222106511
Iter: 295 loss: 0.00227438333
Iter: 296 loss: 0.00221785
Iter: 297 loss: 0.00220624986
Iter: 298 loss: 0.00230622198
Iter: 299 loss: 0.00220548199
Iter: 300 loss: 0.00219853455
Iter: 301 loss: 0.00225596
Iter: 302 loss: 0.00219790544
Iter: 303 loss: 0.00219379575
Iter: 304 loss: 0.00220236
Iter: 305 loss: 0.0021921657
Iter: 306 loss: 0.00218385528
Iter: 307 loss: 0.00218809117
Iter: 308 loss: 0.00217794534
Iter: 309 loss: 0.00216588657
Iter: 310 loss: 0.00218086317
Iter: 311 loss: 0.00216006953
Iter: 312 loss: 0.0021643755
Iter: 313 loss: 0.00214353437
Iter: 314 loss: 0.0021349031
Iter: 315 loss: 0.00215963041
Iter: 316 loss: 0.00213208911
Iter: 317 loss: 0.00212677661
Iter: 318 loss: 0.00214437861
Iter: 319 loss: 0.00212565716
Iter: 320 loss: 0.00211774884
Iter: 321 loss: 0.00211845944
Iter: 322 loss: 0.00211109244
Iter: 323 loss: 0.00210188981
Iter: 324 loss: 0.00212139147
Iter: 325 loss: 0.0020979913
Iter: 326 loss: 0.002090781
Iter: 327 loss: 0.00209607719
Iter: 328 loss: 0.00208631577
Iter: 329 loss: 0.00207822584
Iter: 330 loss: 0.00212604692
Iter: 331 loss: 0.00207684585
Iter: 332 loss: 0.00207194407
Iter: 333 loss: 0.00208547385
Iter: 334 loss: 0.00207031029
Iter: 335 loss: 0.00206428254
Iter: 336 loss: 0.00206581689
Iter: 337 loss: 0.00205967086
Iter: 338 loss: 0.00205300143
Iter: 339 loss: 0.00206926279
Iter: 340 loss: 0.00205078255
Iter: 341 loss: 0.00204109494
Iter: 342 loss: 0.00204008119
Iter: 343 loss: 0.00202693883
Iter: 344 loss: 0.00213662675
Iter: 345 loss: 0.00202586083
Iter: 346 loss: 0.00201630522
Iter: 347 loss: 0.00208104867
Iter: 348 loss: 0.00201531686
Iter: 349 loss: 0.00200949935
Iter: 350 loss: 0.00202372903
Iter: 351 loss: 0.00200739
Iter: 352 loss: 0.00200123619
Iter: 353 loss: 0.00199161703
Iter: 354 loss: 0.00199150341
Iter: 355 loss: 0.00198141974
Iter: 356 loss: 0.00200076262
Iter: 357 loss: 0.00197728025
Iter: 358 loss: 0.00197026948
Iter: 359 loss: 0.00204209238
Iter: 360 loss: 0.00196936377
Iter: 361 loss: 0.00196244963
Iter: 362 loss: 0.00201079971
Iter: 363 loss: 0.00196183752
Iter: 364 loss: 0.00195633061
Iter: 365 loss: 0.0019576631
Iter: 366 loss: 0.00195217202
Iter: 367 loss: 0.00194494647
Iter: 368 loss: 0.00194494263
Iter: 369 loss: 0.00193838705
Iter: 370 loss: 0.00196560682
Iter: 371 loss: 0.00193729589
Iter: 372 loss: 0.00192775868
Iter: 373 loss: 0.0019531718
Iter: 374 loss: 0.00192390627
Iter: 375 loss: 0.00191377604
Iter: 376 loss: 0.00193463813
Iter: 377 loss: 0.00190967135
Iter: 378 loss: 0.00189842936
Iter: 379 loss: 0.00199191365
Iter: 380 loss: 0.00189781096
Iter: 381 loss: 0.00189093966
Iter: 382 loss: 0.00191212143
Iter: 383 loss: 0.00188879133
Iter: 384 loss: 0.00188547745
Iter: 385 loss: 0.00188127242
Iter: 386 loss: 0.00188096601
Iter: 387 loss: 0.00186964613
Iter: 388 loss: 0.00202382542
Iter: 389 loss: 0.00186939491
Iter: 390 loss: 0.00186305086
Iter: 391 loss: 0.0018913548
Iter: 392 loss: 0.00186180219
Iter: 393 loss: 0.00184983853
Iter: 394 loss: 0.00190370344
Iter: 395 loss: 0.00184777565
Iter: 396 loss: 0.00183854136
Iter: 397 loss: 0.00192687497
Iter: 398 loss: 0.00183786266
Iter: 399 loss: 0.00183463295
Iter: 400 loss: 0.00184749486
Iter: 401 loss: 0.00183401164
Iter: 402 loss: 0.00182528864
Iter: 403 loss: 0.00184109691
Iter: 404 loss: 0.00182137382
Iter: 405 loss: 0.00185343274
Iter: 406 loss: 0.00181705027
Iter: 407 loss: 0.00181383803
Iter: 408 loss: 0.00182478398
Iter: 409 loss: 0.00181302684
Iter: 410 loss: 0.00181093218
Iter: 411 loss: 0.00180897536
Iter: 412 loss: 0.00180844904
Iter: 413 loss: 0.00180437788
Iter: 414 loss: 0.00180419069
Iter: 415 loss: 0.00179931358
Iter: 416 loss: 0.00179403869
Iter: 417 loss: 0.00179321982
Iter: 418 loss: 0.00177707104
Iter: 419 loss: 0.00198280113
Iter: 420 loss: 0.00177618396
Iter: 421 loss: 0.00176458166
Iter: 422 loss: 0.00188090315
Iter: 423 loss: 0.00176433742
Iter: 424 loss: 0.00175927277
Iter: 425 loss: 0.00175568496
Iter: 426 loss: 0.00175394164
Iter: 427 loss: 0.00174802472
Iter: 428 loss: 0.00178630976
Iter: 429 loss: 0.00174687628
Iter: 430 loss: 0.00173852162
Iter: 431 loss: 0.00177939609
Iter: 432 loss: 0.00173721462
Iter: 433 loss: 0.00173383835
Iter: 434 loss: 0.00173747947
Iter: 435 loss: 0.00173199642
Iter: 436 loss: 0.00172931468
Iter: 437 loss: 0.00172480685
Iter: 438 loss: 0.00172478729
Iter: 439 loss: 0.00172228343
Iter: 440 loss: 0.00172221893
Iter: 441 loss: 0.00172063941
Iter: 442 loss: 0.00171965163
Iter: 443 loss: 0.00171897095
Iter: 444 loss: 0.00171488873
Iter: 445 loss: 0.00175655354
Iter: 446 loss: 0.00171476777
Iter: 447 loss: 0.00170708448
Iter: 448 loss: 0.00174459442
Iter: 449 loss: 0.00170565885
Iter: 450 loss: 0.00170096813
Iter: 451 loss: 0.0017195578
Iter: 452 loss: 0.00169955287
Iter: 453 loss: 0.00169561489
Iter: 454 loss: 0.00169329112
Iter: 455 loss: 0.00169165933
Iter: 456 loss: 0.00168208021
Iter: 457 loss: 0.00168163702
Iter: 458 loss: 0.00167398783
Iter: 459 loss: 0.00166698708
Iter: 460 loss: 0.00174383412
Iter: 461 loss: 0.00166690641
Iter: 462 loss: 0.00166223804
Iter: 463 loss: 0.0017000963
Iter: 464 loss: 0.00166195875
Iter: 465 loss: 0.00166270847
Iter: 466 loss: 0.00165876525
Iter: 467 loss: 0.00165695546
Iter: 468 loss: 0.001659845
Iter: 469 loss: 0.00165608362
Iter: 470 loss: 0.00165381911
Iter: 471 loss: 0.00165431364
Iter: 472 loss: 0.0016521361
Iter: 473 loss: 0.0016494249
Iter: 474 loss: 0.0016466676
Iter: 475 loss: 0.00164613687
Iter: 476 loss: 0.00164027186
Iter: 477 loss: 0.00165943149
Iter: 478 loss: 0.00163839664
Iter: 479 loss: 0.00163222721
Iter: 480 loss: 0.00168451737
Iter: 481 loss: 0.00163183804
Iter: 482 loss: 0.00162787677
Iter: 483 loss: 0.00162445079
Iter: 484 loss: 0.00162329536
Iter: 485 loss: 0.00161598157
Iter: 486 loss: 0.00167421473
Iter: 487 loss: 0.00161548739
Iter: 488 loss: 0.00161070982
Iter: 489 loss: 0.00161065545
Iter: 490 loss: 0.00160575239
Iter: 491 loss: 0.00161533151
Iter: 492 loss: 0.00160345831
Iter: 493 loss: 0.00160074292
Iter: 494 loss: 0.00160930422
Iter: 495 loss: 0.0016000031
Iter: 496 loss: 0.00159778958
Iter: 497 loss: 0.0015949019
Iter: 498 loss: 0.00159463799
Iter: 499 loss: 0.00159276777
Iter: 500 loss: 0.00159264158
Iter: 501 loss: 0.00159217929
Iter: 502 loss: 0.00159097509
Iter: 503 loss: 0.00158955739
Iter: 504 loss: 0.0015886575
Iter: 505 loss: 0.00158806355
Iter: 506 loss: 0.00158497819
Iter: 507 loss: 0.00158847473
Iter: 508 loss: 0.00158332696
Iter: 509 loss: 0.00157814031
Iter: 510 loss: 0.00159381283
Iter: 511 loss: 0.00157641619
Iter: 512 loss: 0.00157390872
Iter: 513 loss: 0.00157245796
Iter: 514 loss: 0.00156833371
Iter: 515 loss: 0.00157121802
Iter: 516 loss: 0.0015657742
Iter: 517 loss: 0.00155962585
Iter: 518 loss: 0.00158870756
Iter: 519 loss: 0.00155847461
Iter: 520 loss: 0.00155256793
Iter: 521 loss: 0.00156323775
Iter: 522 loss: 0.00154991937
Iter: 523 loss: 0.00154925138
Iter: 524 loss: 0.00154777151
Iter: 525 loss: 0.00154577289
Iter: 526 loss: 0.00156253984
Iter: 527 loss: 0.00154564972
Iter: 528 loss: 0.00154336006
Iter: 529 loss: 0.00153802871
Iter: 530 loss: 0.00159777887
Iter: 531 loss: 0.0015375684
Iter: 532 loss: 0.00153114414
Iter: 533 loss: 0.00166999246
Iter: 534 loss: 0.00153113459
Iter: 535 loss: 0.00152818719
Iter: 536 loss: 0.00152780151
Iter: 537 loss: 0.00152620417
Iter: 538 loss: 0.00153333647
Iter: 539 loss: 0.00152589078
Iter: 540 loss: 0.00152377132
Iter: 541 loss: 0.00151833356
Iter: 542 loss: 0.00156590948
Iter: 543 loss: 0.00151741458
Iter: 544 loss: 0.00150878762
Iter: 545 loss: 0.00155740208
Iter: 546 loss: 0.00150750589
Iter: 547 loss: 0.00150022027
Iter: 548 loss: 0.00154089765
Iter: 549 loss: 0.00149918068
Iter: 550 loss: 0.00149229891
Iter: 551 loss: 0.00151538453
Iter: 552 loss: 0.00149038865
Iter: 553 loss: 0.00148469256
Iter: 554 loss: 0.00154666766
Iter: 555 loss: 0.0014845687
Iter: 556 loss: 0.00148111512
Iter: 557 loss: 0.00148111233
Iter: 558 loss: 0.00147594977
Iter: 559 loss: 0.00148856803
Iter: 560 loss: 0.00147408573
Iter: 561 loss: 0.00146961771
Iter: 562 loss: 0.00152580696
Iter: 563 loss: 0.00146957347
Iter: 564 loss: 0.00147091248
Iter: 565 loss: 0.00146852108
Iter: 566 loss: 0.001467005
Iter: 567 loss: 0.001467
Iter: 568 loss: 0.00146629859
Iter: 569 loss: 0.00146615074
Iter: 570 loss: 0.00146513386
Iter: 571 loss: 0.00146328355
Iter: 572 loss: 0.00150555372
Iter: 573 loss: 0.00146327587
Iter: 574 loss: 0.00146040251
Iter: 575 loss: 0.00145951484
Iter: 576 loss: 0.00145781366
Iter: 577 loss: 0.00145450165
Iter: 578 loss: 0.00145344227
Iter: 579 loss: 0.00145154598
Iter: 580 loss: 0.0014476194
Iter: 581 loss: 0.00149601698
Iter: 582 loss: 0.00144756434
Iter: 583 loss: 0.00144449761
Iter: 584 loss: 0.00143741234
Iter: 585 loss: 0.00153554033
Iter: 586 loss: 0.00143692235
Iter: 587 loss: 0.00143089786
Iter: 588 loss: 0.00143917371
Iter: 589 loss: 0.0014280054
Iter: 590 loss: 0.00142245088
Iter: 591 loss: 0.00146728568
Iter: 592 loss: 0.00142211828
Iter: 593 loss: 0.00141649309
Iter: 594 loss: 0.00148856593
Iter: 595 loss: 0.00141644303
Iter: 596 loss: 0.00141262379
Iter: 597 loss: 0.00146415783
Iter: 598 loss: 0.00141249225
Iter: 599 loss: 0.0014102282
Iter: 600 loss: 0.00140905171
Iter: 601 loss: 0.00140773889
Iter: 602 loss: 0.00140771759
Iter: 603 loss: 0.00140681234
Iter: 604 loss: 0.0014085744
Iter: 605 loss: 0.00140642584
Iter: 606 loss: 0.00140520278
Iter: 607 loss: 0.00140409695
Iter: 608 loss: 0.00140378845
Iter: 609 loss: 0.0013994429
Iter: 610 loss: 0.00140082836
Iter: 611 loss: 0.0013962849
Iter: 612 loss: 0.00139286439
Iter: 613 loss: 0.00141406129
Iter: 614 loss: 0.00139246834
Iter: 615 loss: 0.0013901433
Iter: 616 loss: 0.0013849379
Iter: 617 loss: 0.00144834747
Iter: 618 loss: 0.00138460228
Iter: 619 loss: 0.00137909479
Iter: 620 loss: 0.00138718938
Iter: 621 loss: 0.00137621525
Iter: 622 loss: 0.00137259
Iter: 623 loss: 0.00141150085
Iter: 624 loss: 0.00137253013
Iter: 625 loss: 0.00136964151
Iter: 626 loss: 0.0013655579
Iter: 627 loss: 0.0013653955
Iter: 628 loss: 0.00136159197
Iter: 629 loss: 0.00139756233
Iter: 630 loss: 0.00136116799
Iter: 631 loss: 0.00135829463
Iter: 632 loss: 0.00136218162
Iter: 633 loss: 0.00135681569
Iter: 634 loss: 0.00135679625
Iter: 635 loss: 0.00135566446
Iter: 636 loss: 0.00135445176
Iter: 637 loss: 0.00135801488
Iter: 638 loss: 0.00135409855
Iter: 639 loss: 0.00135148293
Iter: 640 loss: 0.00134773436
Iter: 641 loss: 0.00134761026
Iter: 642 loss: 0.00134438253
Iter: 643 loss: 0.0013478545
Iter: 644 loss: 0.00134264049
Iter: 645 loss: 0.00134032196
Iter: 646 loss: 0.00133614463
Iter: 647 loss: 0.00133614708
Iter: 648 loss: 0.00133187172
Iter: 649 loss: 0.00133163261
Iter: 650 loss: 0.00133011269
Iter: 651 loss: 0.00132974726
Iter: 652 loss: 0.00132747064
Iter: 653 loss: 0.00132852094
Iter: 654 loss: 0.00132591638
Iter: 655 loss: 0.00132290344
Iter: 656 loss: 0.0013205409
Iter: 657 loss: 0.00131957862
Iter: 658 loss: 0.00131676
Iter: 659 loss: 0.00132019771
Iter: 660 loss: 0.00131517625
Iter: 661 loss: 0.00131289777
Iter: 662 loss: 0.00131568545
Iter: 663 loss: 0.00131170033
Iter: 664 loss: 0.00130991172
Iter: 665 loss: 0.00131314329
Iter: 666 loss: 0.00130908168
Iter: 667 loss: 0.00130631239
Iter: 668 loss: 0.0013197
Iter: 669 loss: 0.00130579248
Iter: 670 loss: 0.00130284287
Iter: 671 loss: 0.00132349436
Iter: 672 loss: 0.00130257395
Iter: 673 loss: 0.00130114425
Iter: 674 loss: 0.00130325882
Iter: 675 loss: 0.00130043
Iter: 676 loss: 0.00129810045
Iter: 677 loss: 0.0012986894
Iter: 678 loss: 0.00129640801
Iter: 679 loss: 0.00129306
Iter: 680 loss: 0.00129258516
Iter: 681 loss: 0.00129022449
Iter: 682 loss: 0.00128671923
Iter: 683 loss: 0.00132950651
Iter: 684 loss: 0.00128669117
Iter: 685 loss: 0.00128464436
Iter: 686 loss: 0.0012845801
Iter: 687 loss: 0.00128304982
Iter: 688 loss: 0.0012899182
Iter: 689 loss: 0.00128276111
Iter: 690 loss: 0.00128220231
Iter: 691 loss: 0.00128098438
Iter: 692 loss: 0.00130036392
Iter: 693 loss: 0.00128093478
Iter: 694 loss: 0.00127862
Iter: 695 loss: 0.00127676805
Iter: 696 loss: 0.00127606932
Iter: 697 loss: 0.00127415312
Iter: 698 loss: 0.00127288094
Iter: 699 loss: 0.00126978371
Iter: 700 loss: 0.00127237802
Iter: 701 loss: 0.00126797368
Iter: 702 loss: 0.00126860838
Iter: 703 loss: 0.00126603059
Iter: 704 loss: 0.0012643293
Iter: 705 loss: 0.00127757771
Iter: 706 loss: 0.00126419426
Iter: 707 loss: 0.00126281544
Iter: 708 loss: 0.00125949923
Iter: 709 loss: 0.00129380682
Iter: 710 loss: 0.00125913252
Iter: 711 loss: 0.00125485379
Iter: 712 loss: 0.00125703448
Iter: 713 loss: 0.00125197787
Iter: 714 loss: 0.00125051697
Iter: 715 loss: 0.00125089241
Iter: 716 loss: 0.00124946889
Iter: 717 loss: 0.00124778925
Iter: 718 loss: 0.00124608015
Iter: 719 loss: 0.00124575524
Iter: 720 loss: 0.00125160895
Iter: 721 loss: 0.0012448153
Iter: 722 loss: 0.00124390109
Iter: 723 loss: 0.0012485974
Iter: 724 loss: 0.00124375022
Iter: 725 loss: 0.00124139746
Iter: 726 loss: 0.00123716809
Iter: 727 loss: 0.00133917155
Iter: 728 loss: 0.00123716565
Iter: 729 loss: 0.00123188575
Iter: 730 loss: 0.00125078787
Iter: 731 loss: 0.00123061531
Iter: 732 loss: 0.00122739514
Iter: 733 loss: 0.00123977428
Iter: 734 loss: 0.00122665311
Iter: 735 loss: 0.00122491783
Iter: 736 loss: 0.00124253728
Iter: 737 loss: 0.00122486521
Iter: 738 loss: 0.00122333399
Iter: 739 loss: 0.00121983571
Iter: 740 loss: 0.00126318936
Iter: 741 loss: 0.00121959462
Iter: 742 loss: 0.00121613941
Iter: 743 loss: 0.00121613068
Iter: 744 loss: 0.0012146245
Iter: 745 loss: 0.00121533847
Iter: 746 loss: 0.00121361681
Iter: 747 loss: 0.00121223123
Iter: 748 loss: 0.00121150562
Iter: 749 loss: 0.00121085218
Iter: 750 loss: 0.00121707714
Iter: 751 loss: 0.00120982854
Iter: 752 loss: 0.00120895903
Iter: 753 loss: 0.00120879314
Iter: 754 loss: 0.00120822142
Iter: 755 loss: 0.00120564667
Iter: 756 loss: 0.00120330858
Iter: 757 loss: 0.00120268599
Iter: 758 loss: 0.00120213593
Iter: 759 loss: 0.00119994488
Iter: 760 loss: 0.00119561469
Iter: 761 loss: 0.00120290788
Iter: 762 loss: 0.00119365309
Iter: 763 loss: 0.0011893356
Iter: 764 loss: 0.00120178843
Iter: 765 loss: 0.00118797412
Iter: 766 loss: 0.00118451333
Iter: 767 loss: 0.00121681078
Iter: 768 loss: 0.00118437095
Iter: 769 loss: 0.00118150888
Iter: 770 loss: 0.00120608695
Iter: 771 loss: 0.00118136685
Iter: 772 loss: 0.00118028536
Iter: 773 loss: 0.00117803726
Iter: 774 loss: 0.00121665129
Iter: 775 loss: 0.00117798429
Iter: 776 loss: 0.00117589
Iter: 777 loss: 0.001191027
Iter: 778 loss: 0.00117570139
Iter: 779 loss: 0.00117439847
Iter: 780 loss: 0.0011757639
Iter: 781 loss: 0.00117368018
Iter: 782 loss: 0.00117161393
Iter: 783 loss: 0.00117116154
Iter: 784 loss: 0.00116893346
Iter: 785 loss: 0.00117268681
Iter: 786 loss: 0.00116793206
Iter: 787 loss: 0.00116386265
Iter: 788 loss: 0.00116662
Iter: 789 loss: 0.00116126146
Iter: 790 loss: 0.00115711137
Iter: 791 loss: 0.0012099219
Iter: 792 loss: 0.00115707458
Iter: 793 loss: 0.00115421601
Iter: 794 loss: 0.00115424593
Iter: 795 loss: 0.00115186721
Iter: 796 loss: 0.00114768569
Iter: 797 loss: 0.00117443316
Iter: 798 loss: 0.0011472092
Iter: 799 loss: 0.00114524155
Iter: 800 loss: 0.00114522839
Iter: 801 loss: 0.00114399078
Iter: 802 loss: 0.00115997251
Iter: 803 loss: 0.00114398
Iter: 804 loss: 0.00114299019
Iter: 805 loss: 0.00114089274
Iter: 806 loss: 0.00117733609
Iter: 807 loss: 0.00114083639
Iter: 808 loss: 0.00113814662
Iter: 809 loss: 0.00114172488
Iter: 810 loss: 0.00113682589
Iter: 811 loss: 0.00113453774
Iter: 812 loss: 0.00114090671
Iter: 813 loss: 0.00113381865
Iter: 814 loss: 0.0011317404
Iter: 815 loss: 0.00112873339
Iter: 816 loss: 0.00112864
Iter: 817 loss: 0.00112463709
Iter: 818 loss: 0.00117228425
Iter: 819 loss: 0.00112460274
Iter: 820 loss: 0.00112046534
Iter: 821 loss: 0.00113611668
Iter: 822 loss: 0.00111942389
Iter: 823 loss: 0.00111541338
Iter: 824 loss: 0.00113703834
Iter: 825 loss: 0.00111482386
Iter: 826 loss: 0.00110920728
Iter: 827 loss: 0.00116254506
Iter: 828 loss: 0.00110895664
Iter: 829 loss: 0.0011042736
Iter: 830 loss: 0.0011252315
Iter: 831 loss: 0.00110335194
Iter: 832 loss: 0.00110062829
Iter: 833 loss: 0.00110061455
Iter: 834 loss: 0.00110039476
Iter: 835 loss: 0.00109988789
Iter: 836 loss: 0.00109940022
Iter: 837 loss: 0.0010985604
Iter: 838 loss: 0.00109856261
Iter: 839 loss: 0.00109663594
Iter: 840 loss: 0.00109146256
Iter: 841 loss: 0.00112627645
Iter: 842 loss: 0.00109023973
Iter: 843 loss: 0.00112684222
Iter: 844 loss: 0.00108941016
Iter: 845 loss: 0.00108846091
Iter: 846 loss: 0.00109194452
Iter: 847 loss: 0.00108822936
Iter: 848 loss: 0.00108558359
Iter: 849 loss: 0.00108394015
Iter: 850 loss: 0.00108288
Iter: 851 loss: 0.00107860565
Iter: 852 loss: 0.00107923883
Iter: 853 loss: 0.00107536057
Iter: 854 loss: 0.0010712865
Iter: 855 loss: 0.00109384384
Iter: 856 loss: 0.00107073737
Iter: 857 loss: 0.00106745563
Iter: 858 loss: 0.00111316447
Iter: 859 loss: 0.00106745306
Iter: 860 loss: 0.00106500788
Iter: 861 loss: 0.00106781465
Iter: 862 loss: 0.00106367446
Iter: 863 loss: 0.00106068526
Iter: 864 loss: 0.00105757033
Iter: 865 loss: 0.00105700153
Iter: 866 loss: 0.00105383678
Iter: 867 loss: 0.00108355982
Iter: 868 loss: 0.00105370837
Iter: 869 loss: 0.00105169346
Iter: 870 loss: 0.00105135853
Iter: 871 loss: 0.00105078449
Iter: 872 loss: 0.00104944524
Iter: 873 loss: 0.00106519624
Iter: 874 loss: 0.00104933581
Iter: 875 loss: 0.0010472287
Iter: 876 loss: 0.00104721636
Iter: 877 loss: 0.00104524044
Iter: 878 loss: 0.00105367426
Iter: 879 loss: 0.00104482216
Iter: 880 loss: 0.00104348012
Iter: 881 loss: 0.00104324892
Iter: 882 loss: 0.0010423318
Iter: 883 loss: 0.00104011421
Iter: 884 loss: 0.00103684154
Iter: 885 loss: 0.00103674293
Iter: 886 loss: 0.00103422138
Iter: 887 loss: 0.00104355742
Iter: 888 loss: 0.00103359506
Iter: 889 loss: 0.00103093451
Iter: 890 loss: 0.00102893659
Iter: 891 loss: 0.00102801016
Iter: 892 loss: 0.00102521863
Iter: 893 loss: 0.00102508441
Iter: 894 loss: 0.00102309603
Iter: 895 loss: 0.00102509209
Iter: 896 loss: 0.00102197798
Iter: 897 loss: 0.00101931777
Iter: 898 loss: 0.00102585123
Iter: 899 loss: 0.00101833162
Iter: 900 loss: 0.00101569947
Iter: 901 loss: 0.0010184194
Iter: 902 loss: 0.00101423555
Iter: 903 loss: 0.0010275949
Iter: 904 loss: 0.00101334217
Iter: 905 loss: 0.00101264624
Iter: 906 loss: 0.00101245171
Iter: 907 loss: 0.00101202738
Iter: 908 loss: 0.0010094184
Iter: 909 loss: 0.00101240654
Iter: 910 loss: 0.00100798986
Iter: 911 loss: 0.00100568868
Iter: 912 loss: 0.00103169633
Iter: 913 loss: 0.00100564281
Iter: 914 loss: 0.00100504234
Iter: 915 loss: 0.00100366748
Iter: 916 loss: 0.00102186203
Iter: 917 loss: 0.00100357539
Iter: 918 loss: 0.00100124232
Iter: 919 loss: 0.00100018433
Iter: 920 loss: 0.000998917734
Iter: 921 loss: 0.000994393835
Iter: 922 loss: 0.00101740728
Iter: 923 loss: 0.000993548
Iter: 924 loss: 0.000989385648
Iter: 925 loss: 0.00104683498
Iter: 926 loss: 0.00098935049
Iter: 927 loss: 0.000986489467
Iter: 928 loss: 0.00102778326
Iter: 929 loss: 0.000986482482
Iter: 930 loss: 0.000985250575
Iter: 931 loss: 0.000983840902
Iter: 932 loss: 0.000983662787
Iter: 933 loss: 0.000980992336
Iter: 934 loss: 0.000982602127
Iter: 935 loss: 0.000979265431
Iter: 936 loss: 0.000977063
Iter: 937 loss: 0.00098291575
Iter: 938 loss: 0.000976325071
Iter: 939 loss: 0.000980284298
Iter: 940 loss: 0.000975714298
Iter: 941 loss: 0.000974723487
Iter: 942 loss: 0.000975853181
Iter: 943 loss: 0.000974181574
Iter: 944 loss: 0.000973127666
Iter: 945 loss: 0.000972491747
Iter: 946 loss: 0.000972036156
Iter: 947 loss: 0.000968796143
Iter: 948 loss: 0.00100195745
Iter: 949 loss: 0.000968677632
Iter: 950 loss: 0.000966994325
Iter: 951 loss: 0.000970092486
Iter: 952 loss: 0.000966284424
Iter: 953 loss: 0.000964852
Iter: 954 loss: 0.000962285907
Iter: 955 loss: 0.00096229295
Iter: 956 loss: 0.000960114
Iter: 957 loss: 0.000968878507
Iter: 958 loss: 0.000959618192
Iter: 959 loss: 0.000957686803
Iter: 960 loss: 0.000963386497
Iter: 961 loss: 0.000957107346
Iter: 962 loss: 0.000954614719
Iter: 963 loss: 0.000966054446
Iter: 964 loss: 0.000954134623
Iter: 965 loss: 0.00095226476
Iter: 966 loss: 0.00095835363
Iter: 967 loss: 0.000951718597
Iter: 968 loss: 0.000949869631
Iter: 969 loss: 0.000946381828
Iter: 970 loss: 0.00103563967
Iter: 971 loss: 0.000946373155
Iter: 972 loss: 0.000945856213
Iter: 973 loss: 0.000945154636
Iter: 974 loss: 0.000944522442
Iter: 975 loss: 0.000946755521
Iter: 976 loss: 0.000944354804
Iter: 977 loss: 0.000943080289
Iter: 978 loss: 0.000939802499
Iter: 979 loss: 0.000967071741
Iter: 980 loss: 0.000939235848
Iter: 981 loss: 0.000937706209
Iter: 982 loss: 0.000937488105
Iter: 983 loss: 0.000936557539
Iter: 984 loss: 0.000935632037
Iter: 985 loss: 0.000935441582
Iter: 986 loss: 0.000932777883
Iter: 987 loss: 0.000929306319
Iter: 988 loss: 0.000929043395
Iter: 989 loss: 0.000927998684
Iter: 990 loss: 0.000927285
Iter: 991 loss: 0.000925866072
Iter: 992 loss: 0.000925466185
Iter: 993 loss: 0.000924608263
Iter: 994 loss: 0.000922891137
Iter: 995 loss: 0.000921786064
Iter: 996 loss: 0.000921117
Iter: 997 loss: 0.00091904751
Iter: 998 loss: 0.000918203266
Iter: 999 loss: 0.000917102676
Iter: 1000 loss: 0.000914439326
Iter: 1001 loss: 0.000916666
Iter: 1002 loss: 0.000912831514
Iter: 1003 loss: 0.000910120725
Iter: 1004 loss: 0.000938136829
Iter: 1005 loss: 0.000910027535
Iter: 1006 loss: 0.000908961694
Iter: 1007 loss: 0.000908955699
Iter: 1008 loss: 0.00090860622
Iter: 1009 loss: 0.000907517271
Iter: 1010 loss: 0.000909392606
Iter: 1011 loss: 0.000906785659
Iter: 1012 loss: 0.00090495497
Iter: 1013 loss: 0.000904430228
Iter: 1014 loss: 0.000903278473
Iter: 1015 loss: 0.000902188709
Iter: 1016 loss: 0.000900780782
Iter: 1017 loss: 0.000899152074
Iter: 1018 loss: 0.00090690417
Iter: 1019 loss: 0.000898869592
Iter: 1020 loss: 0.000897548394
Iter: 1021 loss: 0.000895291159
Iter: 1022 loss: 0.000895292324
Iter: 1023 loss: 0.000890453288
Iter: 1024 loss: 0.000932463212
Iter: 1025 loss: 0.000890187395
Iter: 1026 loss: 0.000888727198
Iter: 1027 loss: 0.000888390583
Iter: 1028 loss: 0.000886850758
Iter: 1029 loss: 0.000887349364
Iter: 1030 loss: 0.000885764079
Iter: 1031 loss: 0.000884065172
Iter: 1032 loss: 0.000882838
Iter: 1033 loss: 0.000882239081
Iter: 1034 loss: 0.00088080531
Iter: 1035 loss: 0.00087938708
Iter: 1036 loss: 0.000879089872
Iter: 1037 loss: 0.000877855229
Iter: 1038 loss: 0.000882760738
Iter: 1039 loss: 0.000877594575
Iter: 1040 loss: 0.000876657781
Iter: 1041 loss: 0.000885881134
Iter: 1042 loss: 0.000876623672
Iter: 1043 loss: 0.000875921571
Iter: 1044 loss: 0.000874289544
Iter: 1045 loss: 0.000893766526
Iter: 1046 loss: 0.000874143268
Iter: 1047 loss: 0.000872369797
Iter: 1048 loss: 0.000873302692
Iter: 1049 loss: 0.000871184515
Iter: 1050 loss: 0.00086913741
Iter: 1051 loss: 0.000865712878
Iter: 1052 loss: 0.000865704496
Iter: 1053 loss: 0.000863558962
Iter: 1054 loss: 0.000892957207
Iter: 1055 loss: 0.000863555353
Iter: 1056 loss: 0.000860987639
Iter: 1057 loss: 0.000861865235
Iter: 1058 loss: 0.000859169406
Iter: 1059 loss: 0.000855507562
Iter: 1060 loss: 0.000903397391
Iter: 1061 loss: 0.000855467399
Iter: 1062 loss: 0.000851811492
Iter: 1063 loss: 0.000872097095
Iter: 1064 loss: 0.000851239311
Iter: 1065 loss: 0.000848380267
Iter: 1066 loss: 0.000848381722
Iter: 1067 loss: 0.000848543
Iter: 1068 loss: 0.000846921641
Iter: 1069 loss: 0.000846130191
Iter: 1070 loss: 0.000845491712
Iter: 1071 loss: 0.000845267088
Iter: 1072 loss: 0.000844203227
Iter: 1073 loss: 0.000844194728
Iter: 1074 loss: 0.000843587157
Iter: 1075 loss: 0.000846157724
Iter: 1076 loss: 0.000843460963
Iter: 1077 loss: 0.000842945243
Iter: 1078 loss: 0.000841595698
Iter: 1079 loss: 0.000851218239
Iter: 1080 loss: 0.00084130012
Iter: 1081 loss: 0.000839254819
Iter: 1082 loss: 0.000837851781
Iter: 1083 loss: 0.000837104628
Iter: 1084 loss: 0.000835158629
Iter: 1085 loss: 0.000833937607
Iter: 1086 loss: 0.000833165483
Iter: 1087 loss: 0.000830807374
Iter: 1088 loss: 0.000853283331
Iter: 1089 loss: 0.00083071558
Iter: 1090 loss: 0.000828381744
Iter: 1091 loss: 0.000832200341
Iter: 1092 loss: 0.000827305194
Iter: 1093 loss: 0.000824890449
Iter: 1094 loss: 0.000827059965
Iter: 1095 loss: 0.000823399052
Iter: 1096 loss: 0.000822116795
Iter: 1097 loss: 0.000821961672
Iter: 1098 loss: 0.000821054389
Iter: 1099 loss: 0.000818614149
Iter: 1100 loss: 0.000832220074
Iter: 1101 loss: 0.000818274915
Iter: 1102 loss: 0.000816857
Iter: 1103 loss: 0.000816674
Iter: 1104 loss: 0.000816466403
Iter: 1105 loss: 0.000815924781
Iter: 1106 loss: 0.000815429317
Iter: 1107 loss: 0.000820981688
Iter: 1108 loss: 0.000815413543
Iter: 1109 loss: 0.000815145322
Iter: 1110 loss: 0.000814384723
Iter: 1111 loss: 0.000818713394
Iter: 1112 loss: 0.00081416266
Iter: 1113 loss: 0.000811914797
Iter: 1114 loss: 0.000815023261
Iter: 1115 loss: 0.000810845522
Iter: 1116 loss: 0.000807723787
Iter: 1117 loss: 0.00083452079
Iter: 1118 loss: 0.000807539793
Iter: 1119 loss: 0.00080580404
Iter: 1120 loss: 0.000804877724
Iter: 1121 loss: 0.000804078532
Iter: 1122 loss: 0.000800550042
Iter: 1123 loss: 0.000812557118
Iter: 1124 loss: 0.000799616682
Iter: 1125 loss: 0.000797873538
Iter: 1126 loss: 0.000810315076
Iter: 1127 loss: 0.000797724468
Iter: 1128 loss: 0.000796369801
Iter: 1129 loss: 0.000803470961
Iter: 1130 loss: 0.000796159613
Iter: 1131 loss: 0.000795292086
Iter: 1132 loss: 0.000797607529
Iter: 1133 loss: 0.000795014261
Iter: 1134 loss: 0.000793798245
Iter: 1135 loss: 0.000791740429
Iter: 1136 loss: 0.000791734201
Iter: 1137 loss: 0.000806718948
Iter: 1138 loss: 0.000790942577
Iter: 1139 loss: 0.000789284823
Iter: 1140 loss: 0.000801113958
Iter: 1141 loss: 0.000789145124
Iter: 1142 loss: 0.000788509904
Iter: 1143 loss: 0.000793154119
Iter: 1144 loss: 0.00078846165
Iter: 1145 loss: 0.000787977944
Iter: 1146 loss: 0.000786301098
Iter: 1147 loss: 0.000784314645
Iter: 1148 loss: 0.000783771393
Iter: 1149 loss: 0.000779585
Iter: 1150 loss: 0.000789794838
Iter: 1151 loss: 0.000778089
Iter: 1152 loss: 0.000777291774
Iter: 1153 loss: 0.00077678141
Iter: 1154 loss: 0.000775627
Iter: 1155 loss: 0.000773571141
Iter: 1156 loss: 0.000773571839
Iter: 1157 loss: 0.00077231368
Iter: 1158 loss: 0.000771906809
Iter: 1159 loss: 0.000770707848
Iter: 1160 loss: 0.000770616112
Iter: 1161 loss: 0.000769902836
Iter: 1162 loss: 0.000770828919
Iter: 1163 loss: 0.000769538106
Iter: 1164 loss: 0.000768755563
Iter: 1165 loss: 0.000767326623
Iter: 1166 loss: 0.00080031273
Iter: 1167 loss: 0.000767324178
Iter: 1168 loss: 0.000767611433
Iter: 1169 loss: 0.000766053272
Iter: 1170 loss: 0.000764609838
Iter: 1171 loss: 0.000771729858
Iter: 1172 loss: 0.000764356053
Iter: 1173 loss: 0.00076365564
Iter: 1174 loss: 0.000762606505
Iter: 1175 loss: 0.000762590556
Iter: 1176 loss: 0.000760819064
Iter: 1177 loss: 0.000767880469
Iter: 1178 loss: 0.000760422496
Iter: 1179 loss: 0.000759337447
Iter: 1180 loss: 0.000758238253
Iter: 1181 loss: 0.000758007169
Iter: 1182 loss: 0.000755194225
Iter: 1183 loss: 0.000756409427
Iter: 1184 loss: 0.000753302244
Iter: 1185 loss: 0.000752637046
Iter: 1186 loss: 0.000751649262
Iter: 1187 loss: 0.000749890867
Iter: 1188 loss: 0.000756549067
Iter: 1189 loss: 0.000749446102
Iter: 1190 loss: 0.000748353545
Iter: 1191 loss: 0.000755101908
Iter: 1192 loss: 0.000748208258
Iter: 1193 loss: 0.000747557613
Iter: 1194 loss: 0.000748154474
Iter: 1195 loss: 0.000747171696
Iter: 1196 loss: 0.000746226404
Iter: 1197 loss: 0.000743871671
Iter: 1198 loss: 0.000765491743
Iter: 1199 loss: 0.000743528653
Iter: 1200 loss: 0.000743423589
Iter: 1201 loss: 0.000741787
Iter: 1202 loss: 0.000740929099
Iter: 1203 loss: 0.000743108
Iter: 1204 loss: 0.00074064068
Iter: 1205 loss: 0.000739890267
Iter: 1206 loss: 0.000738456147
Iter: 1207 loss: 0.000769499165
Iter: 1208 loss: 0.000738448754
Iter: 1209 loss: 0.000737109338
Iter: 1210 loss: 0.000739659648
Iter: 1211 loss: 0.000736538554
Iter: 1212 loss: 0.000734240399
Iter: 1213 loss: 0.000745308585
Iter: 1214 loss: 0.00073380454
Iter: 1215 loss: 0.000731350039
Iter: 1216 loss: 0.00074028736
Iter: 1217 loss: 0.000730722037
Iter: 1218 loss: 0.000729746884
Iter: 1219 loss: 0.00072904129
Iter: 1220 loss: 0.000728704617
Iter: 1221 loss: 0.000728237559
Iter: 1222 loss: 0.00072765263
Iter: 1223 loss: 0.000726943254
Iter: 1224 loss: 0.000728234765
Iter: 1225 loss: 0.000726644241
Iter: 1226 loss: 0.000725638645
Iter: 1227 loss: 0.000724710175
Iter: 1228 loss: 0.000724479672
Iter: 1229 loss: 0.000723589852
Iter: 1230 loss: 0.0007234636
Iter: 1231 loss: 0.000722840836
Iter: 1232 loss: 0.000721385644
Iter: 1233 loss: 0.000720934826
Iter: 1234 loss: 0.000720057869
Iter: 1235 loss: 0.000718855299
Iter: 1236 loss: 0.000718694646
Iter: 1237 loss: 0.000716637
Iter: 1238 loss: 0.000713687274
Iter: 1239 loss: 0.000713590649
Iter: 1240 loss: 0.000712323
Iter: 1241 loss: 0.0007132967
Iter: 1242 loss: 0.000711548
Iter: 1243 loss: 0.000710183347
Iter: 1244 loss: 0.00071005919
Iter: 1245 loss: 0.000709047774
Iter: 1246 loss: 0.000707109109
Iter: 1247 loss: 0.000715016387
Iter: 1248 loss: 0.000706683379
Iter: 1249 loss: 0.0007045893
Iter: 1250 loss: 0.000723274
Iter: 1251 loss: 0.000704478123
Iter: 1252 loss: 0.00070317951
Iter: 1253 loss: 0.000703049882
Iter: 1254 loss: 0.000702088582
Iter: 1255 loss: 0.000701038458
Iter: 1256 loss: 0.000700993754
Iter: 1257 loss: 0.000699822209
Iter: 1258 loss: 0.000702132
Iter: 1259 loss: 0.000699335476
Iter: 1260 loss: 0.000697786396
Iter: 1261 loss: 0.000698674703
Iter: 1262 loss: 0.000696786738
Iter: 1263 loss: 0.000695939059
Iter: 1264 loss: 0.000695912167
Iter: 1265 loss: 0.000695592666
Iter: 1266 loss: 0.000695218332
Iter: 1267 loss: 0.000695178751
Iter: 1268 loss: 0.000694304123
Iter: 1269 loss: 0.000702877587
Iter: 1270 loss: 0.000694273855
Iter: 1271 loss: 0.000693851034
Iter: 1272 loss: 0.000692630245
Iter: 1273 loss: 0.00069769728
Iter: 1274 loss: 0.000692158297
Iter: 1275 loss: 0.000690844841
Iter: 1276 loss: 0.000691276276
Iter: 1277 loss: 0.000689908396
Iter: 1278 loss: 0.000688949251
Iter: 1279 loss: 0.00068664737
Iter: 1280 loss: 0.000712784124
Iter: 1281 loss: 0.000686430431
Iter: 1282 loss: 0.000683052815
Iter: 1283 loss: 0.000683063117
Iter: 1284 loss: 0.000681678
Iter: 1285 loss: 0.000681354373
Iter: 1286 loss: 0.000680541969
Iter: 1287 loss: 0.000678590208
Iter: 1288 loss: 0.000700134784
Iter: 1289 loss: 0.000678379263
Iter: 1290 loss: 0.000680247729
Iter: 1291 loss: 0.000677593926
Iter: 1292 loss: 0.000676804164
Iter: 1293 loss: 0.00067789125
Iter: 1294 loss: 0.000676409167
Iter: 1295 loss: 0.000674299314
Iter: 1296 loss: 0.000674213516
Iter: 1297 loss: 0.000672594237
Iter: 1298 loss: 0.000675323536
Iter: 1299 loss: 0.000671962916
Iter: 1300 loss: 0.000671275542
Iter: 1301 loss: 0.000673865434
Iter: 1302 loss: 0.000671113143
Iter: 1303 loss: 0.000670697191
Iter: 1304 loss: 0.000669734669
Iter: 1305 loss: 0.00068179029
Iter: 1306 loss: 0.000669668079
Iter: 1307 loss: 0.000673206174
Iter: 1308 loss: 0.000668862252
Iter: 1309 loss: 0.000667708577
Iter: 1310 loss: 0.000669091067
Iter: 1311 loss: 0.000667069922
Iter: 1312 loss: 0.000665988249
Iter: 1313 loss: 0.000665327068
Iter: 1314 loss: 0.000664891151
Iter: 1315 loss: 0.000663756742
Iter: 1316 loss: 0.000662735139
Iter: 1317 loss: 0.000662448059
Iter: 1318 loss: 0.000660723716
Iter: 1319 loss: 0.00066435372
Iter: 1320 loss: 0.000660048798
Iter: 1321 loss: 0.000658824923
Iter: 1322 loss: 0.000658792676
Iter: 1323 loss: 0.000657627243
Iter: 1324 loss: 0.000667310553
Iter: 1325 loss: 0.000657553319
Iter: 1326 loss: 0.000657128694
Iter: 1327 loss: 0.000656356
Iter: 1328 loss: 0.000675548567
Iter: 1329 loss: 0.000656355522
Iter: 1330 loss: 0.000654950098
Iter: 1331 loss: 0.000660910271
Iter: 1332 loss: 0.000654665753
Iter: 1333 loss: 0.000653070456
Iter: 1334 loss: 0.000653048512
Iter: 1335 loss: 0.000652441115
Iter: 1336 loss: 0.000654718897
Iter: 1337 loss: 0.000652280345
Iter: 1338 loss: 0.000651243434
Iter: 1339 loss: 0.000652746821
Iter: 1340 loss: 0.000650722475
Iter: 1341 loss: 0.000649658265
Iter: 1342 loss: 0.000653612893
Iter: 1343 loss: 0.00064938725
Iter: 1344 loss: 0.000648473739
Iter: 1345 loss: 0.000656516175
Iter: 1346 loss: 0.000648423797
Iter: 1347 loss: 0.000647736597
Iter: 1348 loss: 0.00064576749
Iter: 1349 loss: 0.000653652707
Iter: 1350 loss: 0.000644969638
Iter: 1351 loss: 0.000642575789
Iter: 1352 loss: 0.00064238545
Iter: 1353 loss: 0.000640586251
Iter: 1354 loss: 0.00063794927
Iter: 1355 loss: 0.000649468042
Iter: 1356 loss: 0.000637440127
Iter: 1357 loss: 0.000636758865
Iter: 1358 loss: 0.000636267767
Iter: 1359 loss: 0.000635477074
Iter: 1360 loss: 0.000635011587
Iter: 1361 loss: 0.000634682248
Iter: 1362 loss: 0.000633303309
Iter: 1363 loss: 0.000639399164
Iter: 1364 loss: 0.000633009477
Iter: 1365 loss: 0.000632121926
Iter: 1366 loss: 0.000640988641
Iter: 1367 loss: 0.0006320955
Iter: 1368 loss: 0.000631640491
Iter: 1369 loss: 0.000630394905
Iter: 1370 loss: 0.000637544494
Iter: 1371 loss: 0.000630055554
Iter: 1372 loss: 0.000628111127
Iter: 1373 loss: 0.000626123452
Iter: 1374 loss: 0.00062574714
Iter: 1375 loss: 0.000624350097
Iter: 1376 loss: 0.000624389038
Iter: 1377 loss: 0.000623252
Iter: 1378 loss: 0.000622528
Iter: 1379 loss: 0.000631072209
Iter: 1380 loss: 0.000622517
Iter: 1381 loss: 0.000622086576
Iter: 1382 loss: 0.00062182371
Iter: 1383 loss: 0.000621650077
Iter: 1384 loss: 0.000620143721
Iter: 1385 loss: 0.000619656465
Iter: 1386 loss: 0.000618771301
Iter: 1387 loss: 0.000616344158
Iter: 1388 loss: 0.000616941601
Iter: 1389 loss: 0.000614564691
Iter: 1390 loss: 0.000618943362
Iter: 1391 loss: 0.000613727781
Iter: 1392 loss: 0.000612624048
Iter: 1393 loss: 0.00061400095
Iter: 1394 loss: 0.000612057745
Iter: 1395 loss: 0.000610872288
Iter: 1396 loss: 0.000613081153
Iter: 1397 loss: 0.000610365532
Iter: 1398 loss: 0.000610383227
Iter: 1399 loss: 0.000609853305
Iter: 1400 loss: 0.000609553943
Iter: 1401 loss: 0.000608992064
Iter: 1402 loss: 0.000621081
Iter: 1403 loss: 0.000608988863
Iter: 1404 loss: 0.000608169707
Iter: 1405 loss: 0.000607883791
Iter: 1406 loss: 0.000607401191
Iter: 1407 loss: 0.000606379588
Iter: 1408 loss: 0.000606104149
Iter: 1409 loss: 0.000605473295
Iter: 1410 loss: 0.000604479748
Iter: 1411 loss: 0.000610155053
Iter: 1412 loss: 0.00060435
Iter: 1413 loss: 0.000603161636
Iter: 1414 loss: 0.000603145454
Iter: 1415 loss: 0.000602198765
Iter: 1416 loss: 0.000600847066
Iter: 1417 loss: 0.000602694286
Iter: 1418 loss: 0.000600158062
Iter: 1419 loss: 0.000599063467
Iter: 1420 loss: 0.000600398169
Iter: 1421 loss: 0.000598497922
Iter: 1422 loss: 0.000597395701
Iter: 1423 loss: 0.00060449203
Iter: 1424 loss: 0.000597283361
Iter: 1425 loss: 0.000596601923
Iter: 1426 loss: 0.000596511236
Iter: 1427 loss: 0.000596134691
Iter: 1428 loss: 0.000595953898
Iter: 1429 loss: 0.000595770776
Iter: 1430 loss: 0.000595378049
Iter: 1431 loss: 0.000595811522
Iter: 1432 loss: 0.000595168327
Iter: 1433 loss: 0.000594657147
Iter: 1434 loss: 0.000593842356
Iter: 1435 loss: 0.000593835721
Iter: 1436 loss: 0.000592107361
Iter: 1437 loss: 0.000600075698
Iter: 1438 loss: 0.000591804273
Iter: 1439 loss: 0.000591137272
Iter: 1440 loss: 0.000590812298
Iter: 1441 loss: 0.000590379059
Iter: 1442 loss: 0.000589025905
Iter: 1443 loss: 0.000591571094
Iter: 1444 loss: 0.000588136434
Iter: 1445 loss: 0.000586847076
Iter: 1446 loss: 0.000605317415
Iter: 1447 loss: 0.000586842187
Iter: 1448 loss: 0.000586015754
Iter: 1449 loss: 0.000586738228
Iter: 1450 loss: 0.000585528149
Iter: 1451 loss: 0.000584519526
Iter: 1452 loss: 0.000583312649
Iter: 1453 loss: 0.00058318337
Iter: 1454 loss: 0.000582038716
Iter: 1455 loss: 0.000581913278
Iter: 1456 loss: 0.0005810773
Iter: 1457 loss: 0.000579403772
Iter: 1458 loss: 0.000597310951
Iter: 1459 loss: 0.000579362037
Iter: 1460 loss: 0.000577823899
Iter: 1461 loss: 0.000586287351
Iter: 1462 loss: 0.000577593863
Iter: 1463 loss: 0.000576939434
Iter: 1464 loss: 0.000576366088
Iter: 1465 loss: 0.000576190418
Iter: 1466 loss: 0.000574562931
Iter: 1467 loss: 0.000573148485
Iter: 1468 loss: 0.000572710298
Iter: 1469 loss: 0.000571116339
Iter: 1470 loss: 0.000572632882
Iter: 1471 loss: 0.000570204284
Iter: 1472 loss: 0.00056910346
Iter: 1473 loss: 0.000572890509
Iter: 1474 loss: 0.000568813412
Iter: 1475 loss: 0.000567885407
Iter: 1476 loss: 0.000574183941
Iter: 1477 loss: 0.000567795
Iter: 1478 loss: 0.000566816307
Iter: 1479 loss: 0.000565826194
Iter: 1480 loss: 0.000565631315
Iter: 1481 loss: 0.000564219896
Iter: 1482 loss: 0.000571959827
Iter: 1483 loss: 0.000564019952
Iter: 1484 loss: 0.000563031295
Iter: 1485 loss: 0.000573793193
Iter: 1486 loss: 0.00056300615
Iter: 1487 loss: 0.000562241708
Iter: 1488 loss: 0.000560637098
Iter: 1489 loss: 0.000586663373
Iter: 1490 loss: 0.000560600369
Iter: 1491 loss: 0.000559251639
Iter: 1492 loss: 0.000564950053
Iter: 1493 loss: 0.000558974105
Iter: 1494 loss: 0.000559706823
Iter: 1495 loss: 0.000558541389
Iter: 1496 loss: 0.000558408326
Iter: 1497 loss: 0.00055804936
Iter: 1498 loss: 0.000559673761
Iter: 1499 loss: 0.000557914434
Iter: 1500 loss: 0.000557131774
Iter: 1501 loss: 0.000556435785
Iter: 1502 loss: 0.000556243234
Iter: 1503 loss: 0.000555212726
Iter: 1504 loss: 0.000555188861
Iter: 1505 loss: 0.000554297527
Iter: 1506 loss: 0.000555267441
Iter: 1507 loss: 0.000553811667
Iter: 1508 loss: 0.000553212187
Iter: 1509 loss: 0.000551897509
Iter: 1510 loss: 0.000571147422
Iter: 1511 loss: 0.000551830395
Iter: 1512 loss: 0.000550439814
Iter: 1513 loss: 0.000555080886
Iter: 1514 loss: 0.000550068798
Iter: 1515 loss: 0.000549009186
Iter: 1516 loss: 0.000549360702
Iter: 1517 loss: 0.000548253767
Iter: 1518 loss: 0.000547017378
Iter: 1519 loss: 0.000547007425
Iter: 1520 loss: 0.000546212192
Iter: 1521 loss: 0.000544964278
Iter: 1522 loss: 0.000544957235
Iter: 1523 loss: 0.00054440333
Iter: 1524 loss: 0.000544323819
Iter: 1525 loss: 0.000544680515
Iter: 1526 loss: 0.000544089
Iter: 1527 loss: 0.000543978182
Iter: 1528 loss: 0.000543796
Iter: 1529 loss: 0.000543794653
Iter: 1530 loss: 0.00054342777
Iter: 1531 loss: 0.000542616064
Iter: 1532 loss: 0.000554320053
Iter: 1533 loss: 0.000542582362
Iter: 1534 loss: 0.000541682879
Iter: 1535 loss: 0.000544007577
Iter: 1536 loss: 0.00054137886
Iter: 1537 loss: 0.00054044358
Iter: 1538 loss: 0.000550198718
Iter: 1539 loss: 0.000540416746
Iter: 1540 loss: 0.000539449684
Iter: 1541 loss: 0.000554160448
Iter: 1542 loss: 0.000539445493
Iter: 1543 loss: 0.000538931228
Iter: 1544 loss: 0.000539059634
Iter: 1545 loss: 0.000538559805
Iter: 1546 loss: 0.000537961139
Iter: 1547 loss: 0.000536393432
Iter: 1548 loss: 0.000548657845
Iter: 1549 loss: 0.000536101696
Iter: 1550 loss: 0.000535716943
Iter: 1551 loss: 0.000535210711
Iter: 1552 loss: 0.000534417457
Iter: 1553 loss: 0.000535590341
Iter: 1554 loss: 0.000534034451
Iter: 1555 loss: 0.000532577047
Iter: 1556 loss: 0.00053300953
Iter: 1557 loss: 0.000531542406
Iter: 1558 loss: 0.000554484
Iter: 1559 loss: 0.000531360391
Iter: 1560 loss: 0.000531182042
Iter: 1561 loss: 0.000530804507
Iter: 1562 loss: 0.000536667649
Iter: 1563 loss: 0.000530785
Iter: 1564 loss: 0.000528603734
Iter: 1565 loss: 0.000536662352
Iter: 1566 loss: 0.000528048957
Iter: 1567 loss: 0.000526046846
Iter: 1568 loss: 0.000526011339
Iter: 1569 loss: 0.000525522511
Iter: 1570 loss: 0.000525823329
Iter: 1571 loss: 0.000525205163
Iter: 1572 loss: 0.000524451956
Iter: 1573 loss: 0.000524675357
Iter: 1574 loss: 0.000523897586
Iter: 1575 loss: 0.000523072784
Iter: 1576 loss: 0.000530058809
Iter: 1577 loss: 0.000523021212
Iter: 1578 loss: 0.000522507937
Iter: 1579 loss: 0.000521938782
Iter: 1580 loss: 0.000521857524
Iter: 1581 loss: 0.000521041511
Iter: 1582 loss: 0.000520663627
Iter: 1583 loss: 0.000520253088
Iter: 1584 loss: 0.000519262743
Iter: 1585 loss: 0.000518949877
Iter: 1586 loss: 0.000517806038
Iter: 1587 loss: 0.000530380639
Iter: 1588 loss: 0.000517778913
Iter: 1589 loss: 0.00051717012
Iter: 1590 loss: 0.000520391739
Iter: 1591 loss: 0.000517076522
Iter: 1592 loss: 0.000516245374
Iter: 1593 loss: 0.000520860776
Iter: 1594 loss: 0.000516131404
Iter: 1595 loss: 0.00051586976
Iter: 1596 loss: 0.000515620923
Iter: 1597 loss: 0.000515566906
Iter: 1598 loss: 0.000514758751
Iter: 1599 loss: 0.000514044776
Iter: 1600 loss: 0.000513835053
Iter: 1601 loss: 0.000513091567
Iter: 1602 loss: 0.000515373191
Iter: 1603 loss: 0.000512873288
Iter: 1604 loss: 0.000511561579
Iter: 1605 loss: 0.000514976447
Iter: 1606 loss: 0.000511110178
Iter: 1607 loss: 0.000510279147
Iter: 1608 loss: 0.00050973543
Iter: 1609 loss: 0.00050941587
Iter: 1610 loss: 0.000507811666
Iter: 1611 loss: 0.000506984768
Iter: 1612 loss: 0.000506232493
Iter: 1613 loss: 0.000504885218
Iter: 1614 loss: 0.000504814787
Iter: 1615 loss: 0.000504173862
Iter: 1616 loss: 0.000503343705
Iter: 1617 loss: 0.000503294286
Iter: 1618 loss: 0.000501962611
Iter: 1619 loss: 0.00051049958
Iter: 1620 loss: 0.000501816336
Iter: 1621 loss: 0.000500794384
Iter: 1622 loss: 0.000500866503
Iter: 1623 loss: 0.000499993
Iter: 1624 loss: 0.000499506947
Iter: 1625 loss: 0.000499133
Iter: 1626 loss: 0.000498489477
Iter: 1627 loss: 0.000500872033
Iter: 1628 loss: 0.00049833185
Iter: 1629 loss: 0.000497958099
Iter: 1630 loss: 0.000498821726
Iter: 1631 loss: 0.000497816247
Iter: 1632 loss: 0.000497279572
Iter: 1633 loss: 0.000497271889
Iter: 1634 loss: 0.000496682362
Iter: 1635 loss: 0.000496813911
Iter: 1636 loss: 0.00049624627
Iter: 1637 loss: 0.000495661341
Iter: 1638 loss: 0.000495297252
Iter: 1639 loss: 0.000495061453
Iter: 1640 loss: 0.000494449516
Iter: 1641 loss: 0.000493147
Iter: 1642 loss: 0.000514511485
Iter: 1643 loss: 0.000493106549
Iter: 1644 loss: 0.000491242448
Iter: 1645 loss: 0.000493624713
Iter: 1646 loss: 0.000490260893
Iter: 1647 loss: 0.000489585451
Iter: 1648 loss: 0.000490147737
Iter: 1649 loss: 0.000489184051
Iter: 1650 loss: 0.000488068268
Iter: 1651 loss: 0.000490406121
Iter: 1652 loss: 0.000487626967
Iter: 1653 loss: 0.000486762467
Iter: 1654 loss: 0.000488765421
Iter: 1655 loss: 0.000486430305
Iter: 1656 loss: 0.000485753262
Iter: 1657 loss: 0.000489622238
Iter: 1658 loss: 0.000485668832
Iter: 1659 loss: 0.000484885764
Iter: 1660 loss: 0.000488462276
Iter: 1661 loss: 0.000484735967
Iter: 1662 loss: 0.000484175107
Iter: 1663 loss: 0.000483059906
Iter: 1664 loss: 0.000504780794
Iter: 1665 loss: 0.000483044685
Iter: 1666 loss: 0.000482300209
Iter: 1667 loss: 0.000484192657
Iter: 1668 loss: 0.000482045143
Iter: 1669 loss: 0.000481661846
Iter: 1670 loss: 0.000481632538
Iter: 1671 loss: 0.000481346564
Iter: 1672 loss: 0.000481737719
Iter: 1673 loss: 0.000481203693
Iter: 1674 loss: 0.000480328425
Iter: 1675 loss: 0.000479329028
Iter: 1676 loss: 0.000479211594
Iter: 1677 loss: 0.000478796428
Iter: 1678 loss: 0.000478587172
Iter: 1679 loss: 0.00047792698
Iter: 1680 loss: 0.000477920519
Iter: 1681 loss: 0.000477399328
Iter: 1682 loss: 0.000476664834
Iter: 1683 loss: 0.000479443202
Iter: 1684 loss: 0.000476491521
Iter: 1685 loss: 0.000476516871
Iter: 1686 loss: 0.000476133369
Iter: 1687 loss: 0.000475956214
Iter: 1688 loss: 0.000477907597
Iter: 1689 loss: 0.000475948676
Iter: 1690 loss: 0.000475814508
Iter: 1691 loss: 0.000475480745
Iter: 1692 loss: 0.000478790869
Iter: 1693 loss: 0.00047544
Iter: 1694 loss: 0.000474913686
Iter: 1695 loss: 0.00047529221
Iter: 1696 loss: 0.000474586588
Iter: 1697 loss: 0.000473933469
Iter: 1698 loss: 0.000472476677
Iter: 1699 loss: 0.000493840431
Iter: 1700 loss: 0.000472399173
Iter: 1701 loss: 0.000484729535
Iter: 1702 loss: 0.0004718813
Iter: 1703 loss: 0.000471477746
Iter: 1704 loss: 0.000471098727
Iter: 1705 loss: 0.000471007981
Iter: 1706 loss: 0.000470349623
Iter: 1707 loss: 0.000470564584
Iter: 1708 loss: 0.000469886349
Iter: 1709 loss: 0.000469508377
Iter: 1710 loss: 0.000468998536
Iter: 1711 loss: 0.00046896952
Iter: 1712 loss: 0.000468179118
Iter: 1713 loss: 0.000469059858
Iter: 1714 loss: 0.000467730075
Iter: 1715 loss: 0.000467186852
Iter: 1716 loss: 0.00046666438
Iter: 1717 loss: 0.000466543133
Iter: 1718 loss: 0.000465631892
Iter: 1719 loss: 0.000466978643
Iter: 1720 loss: 0.000465197896
Iter: 1721 loss: 0.000463763427
Iter: 1722 loss: 0.000468763697
Iter: 1723 loss: 0.000463389908
Iter: 1724 loss: 0.00046353112
Iter: 1725 loss: 0.000462813477
Iter: 1726 loss: 0.000462182419
Iter: 1727 loss: 0.000462708063
Iter: 1728 loss: 0.000461804098
Iter: 1729 loss: 0.000461376738
Iter: 1730 loss: 0.000461173448
Iter: 1731 loss: 0.000460961135
Iter: 1732 loss: 0.000460261828
Iter: 1733 loss: 0.000459821516
Iter: 1734 loss: 0.000459533854
Iter: 1735 loss: 0.000458725379
Iter: 1736 loss: 0.000458550028
Iter: 1737 loss: 0.000458021881
Iter: 1738 loss: 0.000459419913
Iter: 1739 loss: 0.000457467511
Iter: 1740 loss: 0.000457208866
Iter: 1741 loss: 0.000456935843
Iter: 1742 loss: 0.0004568859
Iter: 1743 loss: 0.000455814414
Iter: 1744 loss: 0.00046025039
Iter: 1745 loss: 0.00045558787
Iter: 1746 loss: 0.000455091358
Iter: 1747 loss: 0.000454971741
Iter: 1748 loss: 0.000454620895
Iter: 1749 loss: 0.000454028515
Iter: 1750 loss: 0.0004540277
Iter: 1751 loss: 0.000453144778
Iter: 1752 loss: 0.000452317967
Iter: 1753 loss: 0.000452106644
Iter: 1754 loss: 0.000450973224
Iter: 1755 loss: 0.000453228771
Iter: 1756 loss: 0.000450503954
Iter: 1757 loss: 0.000450168853
Iter: 1758 loss: 0.000450054882
Iter: 1759 loss: 0.000449655519
Iter: 1760 loss: 0.000454072841
Iter: 1761 loss: 0.000449646963
Iter: 1762 loss: 0.000449427549
Iter: 1763 loss: 0.000448794395
Iter: 1764 loss: 0.000450971769
Iter: 1765 loss: 0.000448498176
Iter: 1766 loss: 0.000447753293
Iter: 1767 loss: 0.000448538689
Iter: 1768 loss: 0.000447331317
Iter: 1769 loss: 0.000446849823
Iter: 1770 loss: 0.000449149462
Iter: 1771 loss: 0.000446761551
Iter: 1772 loss: 0.000446396822
Iter: 1773 loss: 0.000446087564
Iter: 1774 loss: 0.000445983023
Iter: 1775 loss: 0.000445249199
Iter: 1776 loss: 0.000445247744
Iter: 1777 loss: 0.000445087964
Iter: 1778 loss: 0.000444972713
Iter: 1779 loss: 0.0004447471
Iter: 1780 loss: 0.000444279955
Iter: 1781 loss: 0.00045271928
Iter: 1782 loss: 0.000444270467
Iter: 1783 loss: 0.000443444529
Iter: 1784 loss: 0.00045221491
Iter: 1785 loss: 0.000443428056
Iter: 1786 loss: 0.000442587945
Iter: 1787 loss: 0.000441733981
Iter: 1788 loss: 0.00044157196
Iter: 1789 loss: 0.000440367585
Iter: 1790 loss: 0.000444888254
Iter: 1791 loss: 0.000440082018
Iter: 1792 loss: 0.000439342111
Iter: 1793 loss: 0.00043921516
Iter: 1794 loss: 0.00043880468
Iter: 1795 loss: 0.000443832774
Iter: 1796 loss: 0.000438802788
Iter: 1797 loss: 0.00043858969
Iter: 1798 loss: 0.000437935058
Iter: 1799 loss: 0.000439335126
Iter: 1800 loss: 0.000437539711
Iter: 1801 loss: 0.000436880655
Iter: 1802 loss: 0.000436497357
Iter: 1803 loss: 0.000435894064
Iter: 1804 loss: 0.000439132156
Iter: 1805 loss: 0.0004358023
Iter: 1806 loss: 0.000435356458
Iter: 1807 loss: 0.000434372632
Iter: 1808 loss: 0.000447692815
Iter: 1809 loss: 0.000434322399
Iter: 1810 loss: 0.000433182169
Iter: 1811 loss: 0.000434158865
Iter: 1812 loss: 0.000432499393
Iter: 1813 loss: 0.000431137159
Iter: 1814 loss: 0.000435857539
Iter: 1815 loss: 0.000430790853
Iter: 1816 loss: 0.000429724605
Iter: 1817 loss: 0.000433635782
Iter: 1818 loss: 0.000429468637
Iter: 1819 loss: 0.000427779509
Iter: 1820 loss: 0.000438521558
Iter: 1821 loss: 0.000427566498
Iter: 1822 loss: 0.000426599669
Iter: 1823 loss: 0.000434235262
Iter: 1824 loss: 0.000426529616
Iter: 1825 loss: 0.000426185288
Iter: 1826 loss: 0.000426103
Iter: 1827 loss: 0.000425886479
Iter: 1828 loss: 0.000426042883
Iter: 1829 loss: 0.000425539911
Iter: 1830 loss: 0.000425071572
Iter: 1831 loss: 0.000425820035
Iter: 1832 loss: 0.000424851576
Iter: 1833 loss: 0.000424536294
Iter: 1834 loss: 0.000424537517
Iter: 1835 loss: 0.000424274418
Iter: 1836 loss: 0.000426375162
Iter: 1837 loss: 0.000424253085
Iter: 1838 loss: 0.000424079655
Iter: 1839 loss: 0.000423770049
Iter: 1840 loss: 0.000423770864
Iter: 1841 loss: 0.000423093152
Iter: 1842 loss: 0.000422851124
Iter: 1843 loss: 0.000422309706
Iter: 1844 loss: 0.000423896883
Iter: 1845 loss: 0.000422146375
Iter: 1846 loss: 0.000421524252
Iter: 1847 loss: 0.000421910576
Iter: 1848 loss: 0.000421119883
Iter: 1849 loss: 0.000420414319
Iter: 1850 loss: 0.000420327153
Iter: 1851 loss: 0.000419815216
Iter: 1852 loss: 0.000419086806
Iter: 1853 loss: 0.000419459015
Iter: 1854 loss: 0.00041860179
Iter: 1855 loss: 0.000417583942
Iter: 1856 loss: 0.000428827334
Iter: 1857 loss: 0.0004175629
Iter: 1858 loss: 0.000416630326
Iter: 1859 loss: 0.000418352545
Iter: 1860 loss: 0.0004162314
Iter: 1861 loss: 0.000415582268
Iter: 1862 loss: 0.000415402901
Iter: 1863 loss: 0.000415006652
Iter: 1864 loss: 0.000414575712
Iter: 1865 loss: 0.000414356095
Iter: 1866 loss: 0.000414152339
Iter: 1867 loss: 0.000413445145
Iter: 1868 loss: 0.000421666075
Iter: 1869 loss: 0.000413437956
Iter: 1870 loss: 0.000413235219
Iter: 1871 loss: 0.00041267171
Iter: 1872 loss: 0.000415867311
Iter: 1873 loss: 0.000412508933
Iter: 1874 loss: 0.000410917244
Iter: 1875 loss: 0.000413356931
Iter: 1876 loss: 0.000410162145
Iter: 1877 loss: 0.000409424858
Iter: 1878 loss: 0.000409196684
Iter: 1879 loss: 0.000408889609
Iter: 1880 loss: 0.00040854176
Iter: 1881 loss: 0.000408496358
Iter: 1882 loss: 0.000407919928
Iter: 1883 loss: 0.000408065738
Iter: 1884 loss: 0.000407499523
Iter: 1885 loss: 0.000406961655
Iter: 1886 loss: 0.000406494422
Iter: 1887 loss: 0.00040635062
Iter: 1888 loss: 0.000405620609
Iter: 1889 loss: 0.000407032814
Iter: 1890 loss: 0.000405314844
Iter: 1891 loss: 0.00040452939
Iter: 1892 loss: 0.000405854051
Iter: 1893 loss: 0.000404166814
Iter: 1894 loss: 0.000404030376
Iter: 1895 loss: 0.000403775543
Iter: 1896 loss: 0.000403562444
Iter: 1897 loss: 0.000403720071
Iter: 1898 loss: 0.000403435406
Iter: 1899 loss: 0.000403028564
Iter: 1900 loss: 0.000405682251
Iter: 1901 loss: 0.000402991602
Iter: 1902 loss: 0.000402644742
Iter: 1903 loss: 0.000403428159
Iter: 1904 loss: 0.000402512262
Iter: 1905 loss: 0.0004020255
Iter: 1906 loss: 0.000401106605
Iter: 1907 loss: 0.000421319448
Iter: 1908 loss: 0.000401104742
Iter: 1909 loss: 0.000400243356
Iter: 1910 loss: 0.000400558551
Iter: 1911 loss: 0.000399641052
Iter: 1912 loss: 0.000399037322
Iter: 1913 loss: 0.00039902766
Iter: 1914 loss: 0.000398354052
Iter: 1915 loss: 0.000397983531
Iter: 1916 loss: 0.000397679454
Iter: 1917 loss: 0.00039658777
Iter: 1918 loss: 0.000399912329
Iter: 1919 loss: 0.000396260875
Iter: 1920 loss: 0.000395666459
Iter: 1921 loss: 0.000397213618
Iter: 1922 loss: 0.000395458308
Iter: 1923 loss: 0.000394506438
Iter: 1924 loss: 0.000400851946
Iter: 1925 loss: 0.000394408242
Iter: 1926 loss: 0.000394239934
Iter: 1927 loss: 0.000394044968
Iter: 1928 loss: 0.000393731229
Iter: 1929 loss: 0.000397994037
Iter: 1930 loss: 0.000393729773
Iter: 1931 loss: 0.000393560389
Iter: 1932 loss: 0.000394711271
Iter: 1933 loss: 0.000393544062
Iter: 1934 loss: 0.000393382041
Iter: 1935 loss: 0.000393201888
Iter: 1936 loss: 0.000393175083
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.4/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.8 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi2.8
+ date
Sun Nov  8 08:23:11 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.4/300_100_100_100_1 --function f1 --psi -1 --phi 2.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.8/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93303a2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93303a26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93303e3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9330306c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93303121e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9330312620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93301b26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9330207510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9330207158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f933021a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9330245f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9330103840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9330103c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f933014a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f933014a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93300981e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f933002d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f933002a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f933002af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93287daf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93300aa9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93301767b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9328792840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9328798510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9328798598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9328738950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93286dd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93286e7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93286e7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93286a8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f932861c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9328767620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93287672f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9328776510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93286682f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f932855d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 1.9901844
test_loss: 2.0001502
train_loss: 1.9956394
test_loss: 2.0953026
train_loss: 0.269191
test_loss: 0.28333452
train_loss: 0.26422632
test_loss: 0.28359616
train_loss: 0.2783778
test_loss: 0.28354073
train_loss: 0.2544222
test_loss: 0.28359032
train_loss: 0.2859097
test_loss: 0.28347522
train_loss: 0.2841637
test_loss: 0.2835777
train_loss: 0.2800297
test_loss: 0.2834131
train_loss: 0.27929294
test_loss: 0.28344375
train_loss: 0.2982808
test_loss: 0.28349802
train_loss: 0.28065866
test_loss: 0.28346375
train_loss: 0.27287143
test_loss: 0.28337628
train_loss: 0.26571295
test_loss: 0.28338856
train_loss: 0.2666381
test_loss: 0.2833266
train_loss: 0.2702539
test_loss: 0.2833129
train_loss: 0.2634446
test_loss: 0.28323302
train_loss: 0.27884802
test_loss: 0.28326666
train_loss: 0.28611037
test_loss: 0.28312865
train_loss: 0.27093565
test_loss: 0.2830244
train_loss: 0.27006486
test_loss: 0.28302315
train_loss: 0.27153096
test_loss: 0.28295383
train_loss: 0.28457037
test_loss: 0.28292826
train_loss: 0.2568413
test_loss: 0.2828687
train_loss: 0.26459756
test_loss: 0.28285557
train_loss: 0.28120112
test_loss: 0.28275928
train_loss: 0.26415947
test_loss: 0.2826255
train_loss: 0.26496843
test_loss: 0.282514
train_loss: 0.27607152
test_loss: 0.28247297
train_loss: 0.2657116
test_loss: 0.28239846
train_loss: 0.26983047
test_loss: 0.2822091
train_loss: 0.25928652
test_loss: 0.2821966
train_loss: 0.2601354
test_loss: 0.2820184
train_loss: 0.2768324
test_loss: 0.28188345
train_loss: 0.27220792
test_loss: 0.28182465
train_loss: 0.2542025
test_loss: 0.28166404
train_loss: 0.2750157
test_loss: 0.2815449
train_loss: 0.26501763
test_loss: 0.28145888
train_loss: 0.26928747
test_loss: 0.28132296
train_loss: 0.27855298
test_loss: 0.28120145
train_loss: 0.29462528
test_loss: 0.28091955
train_loss: 0.26276004
test_loss: 0.2807852
train_loss: 0.26783448
test_loss: 0.2806694
train_loss: 0.27837503
test_loss: 0.2805076
train_loss: 0.2671864
test_loss: 0.28020334
train_loss: 0.27311885
test_loss: 0.28004593
train_loss: 0.2641057
test_loss: 0.27980718
train_loss: 0.25636858
test_loss: 0.2797058
train_loss: 0.28299212
test_loss: 0.2793725
train_loss: 0.2817771
test_loss: 0.27911603
train_loss: 0.27477565
test_loss: 0.27897498
train_loss: 0.2514117
test_loss: 0.27871326
train_loss: 0.26692218
test_loss: 0.27828923
train_loss: 0.2677861
test_loss: 0.27799258
train_loss: 0.2502645
test_loss: 0.27769092
train_loss: 0.28116062
test_loss: 0.27736104
train_loss: 0.2507203
test_loss: 0.27699754
train_loss: 0.27327886
test_loss: 0.2766709
train_loss: 0.2763703
test_loss: 0.27641577
train_loss: 0.2530332
test_loss: 0.27588972
train_loss: 0.26480365
test_loss: 0.27544972
train_loss: 0.25073034
test_loss: 0.27503684
train_loss: 0.27024257
test_loss: 0.2746696
train_loss: 0.25821483
test_loss: 0.2741544
train_loss: 0.26707596
test_loss: 0.27368945
train_loss: 0.2637961
test_loss: 0.2731218
train_loss: 0.26583797
test_loss: 0.272626
train_loss: 0.2669235
test_loss: 0.2720455
train_loss: 0.2439979
test_loss: 0.2714799
train_loss: 0.26257998
test_loss: 0.27085388
train_loss: 0.25619292
test_loss: 0.2701979
train_loss: 0.24490404
test_loss: 0.2696831
train_loss: 0.24620332
test_loss: 0.26890665
train_loss: 0.25215787
test_loss: 0.26817086
train_loss: 0.25602064
test_loss: 0.26730788
train_loss: 0.25059006
test_loss: 0.2665267
train_loss: 0.26321346
test_loss: 0.26576632
train_loss: 0.25087973
test_loss: 0.2648907
train_loss: 0.23338898
test_loss: 0.26399702
train_loss: 0.27090716
test_loss: 0.26308906
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi2.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.8/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -1 --phi 2.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi2.8/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d4ba268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d4f5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d4f50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d45eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d45eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d3d52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d39c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d34fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d34f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d309bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d2fe0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d2e4268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d2e4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d280b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d230840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d1fd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d1eb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d2151e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d1db620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d2152f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d215378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052d1db6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0502c7a048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0502ca2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0502ca2158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0502c47488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0502c069d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0502c17840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0502bb80d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0502bc9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0502b90840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04dc2e8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04dc2e8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04dc30d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04dc2a3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04dc273620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.108488478
Iter: 2 loss: 0.0711199343
Iter: 3 loss: 0.0695466697
Iter: 4 loss: 0.0473514833
Iter: 5 loss: 0.143265992
Iter: 6 loss: 0.0457178615
Iter: 7 loss: 0.0338956155
Iter: 8 loss: 0.0890634656
Iter: 9 loss: 0.031057395
Iter: 10 loss: 0.0253723226
Iter: 11 loss: 0.0312716812
Iter: 12 loss: 0.0220823232
Iter: 13 loss: 0.0185408741
Iter: 14 loss: 0.018516371
Iter: 15 loss: 0.0168319624
Iter: 16 loss: 0.0228009149
Iter: 17 loss: 0.0163570233
Iter: 18 loss: 0.0151333455
Iter: 19 loss: 0.0158322752
Iter: 20 loss: 0.0143580241
Iter: 21 loss: 0.0131681198
Iter: 22 loss: 0.0176338665
Iter: 23 loss: 0.0128751351
Iter: 24 loss: 0.0121792611
Iter: 25 loss: 0.0140009979
Iter: 26 loss: 0.01195422
Iter: 27 loss: 0.0113239372
Iter: 28 loss: 0.0119576268
Iter: 29 loss: 0.0109833563
Iter: 30 loss: 0.010589242
Iter: 31 loss: 0.0171534251
Iter: 32 loss: 0.0105885146
Iter: 33 loss: 0.0102917189
Iter: 34 loss: 0.0103486218
Iter: 35 loss: 0.0100729801
Iter: 36 loss: 0.00964765809
Iter: 37 loss: 0.0116765797
Iter: 38 loss: 0.00954733416
Iter: 39 loss: 0.00919067208
Iter: 40 loss: 0.0120539647
Iter: 41 loss: 0.00916782767
Iter: 42 loss: 0.00887438376
Iter: 43 loss: 0.00898202416
Iter: 44 loss: 0.00867804047
Iter: 45 loss: 0.00835193507
Iter: 46 loss: 0.012347674
Iter: 47 loss: 0.00833134539
Iter: 48 loss: 0.00807006098
Iter: 49 loss: 0.0112912823
Iter: 50 loss: 0.00806835666
Iter: 51 loss: 0.00777891651
Iter: 52 loss: 0.0116038788
Iter: 53 loss: 0.00776023325
Iter: 54 loss: 0.0074183112
Iter: 55 loss: 0.00807718839
Iter: 56 loss: 0.00727106957
Iter: 57 loss: 0.00693609659
Iter: 58 loss: 0.010597216
Iter: 59 loss: 0.00692780409
Iter: 60 loss: 0.00666922145
Iter: 61 loss: 0.00772472844
Iter: 62 loss: 0.0066156988
Iter: 63 loss: 0.00646385364
Iter: 64 loss: 0.00724677
Iter: 65 loss: 0.00642985851
Iter: 66 loss: 0.00626909127
Iter: 67 loss: 0.00625220686
Iter: 68 loss: 0.00610810611
Iter: 69 loss: 0.00775073888
Iter: 70 loss: 0.00609944155
Iter: 71 loss: 0.00595594104
Iter: 72 loss: 0.00604313193
Iter: 73 loss: 0.00586461462
Iter: 74 loss: 0.0057064807
Iter: 75 loss: 0.00677648932
Iter: 76 loss: 0.00568974
Iter: 77 loss: 0.00556483259
Iter: 78 loss: 0.0057143718
Iter: 79 loss: 0.00549898809
Iter: 80 loss: 0.00536366506
Iter: 81 loss: 0.00595190562
Iter: 82 loss: 0.00532883266
Iter: 83 loss: 0.00521674752
Iter: 84 loss: 0.00615421217
Iter: 85 loss: 0.00521305203
Iter: 86 loss: 0.00509352516
Iter: 87 loss: 0.00552073214
Iter: 88 loss: 0.00505528785
Iter: 89 loss: 0.00493304618
Iter: 90 loss: 0.00541135855
Iter: 91 loss: 0.00490450906
Iter: 92 loss: 0.00480411807
Iter: 93 loss: 0.00515924767
Iter: 94 loss: 0.00477770297
Iter: 95 loss: 0.00465941522
Iter: 96 loss: 0.00486215204
Iter: 97 loss: 0.00460616173
Iter: 98 loss: 0.00449085282
Iter: 99 loss: 0.00468217954
Iter: 100 loss: 0.00443768082
Iter: 101 loss: 0.00431011897
Iter: 102 loss: 0.00495189056
Iter: 103 loss: 0.00428593904
Iter: 104 loss: 0.00417287135
Iter: 105 loss: 0.00535277231
Iter: 106 loss: 0.00416999217
Iter: 107 loss: 0.00407605432
Iter: 108 loss: 0.00435175747
Iter: 109 loss: 0.00404482335
Iter: 110 loss: 0.00396608701
Iter: 111 loss: 0.00456031226
Iter: 112 loss: 0.00396046601
Iter: 113 loss: 0.00387507025
Iter: 114 loss: 0.00408426486
Iter: 115 loss: 0.00384189421
Iter: 116 loss: 0.00377516868
Iter: 117 loss: 0.00456687389
Iter: 118 loss: 0.0037747724
Iter: 119 loss: 0.00370215229
Iter: 120 loss: 0.00388621539
Iter: 121 loss: 0.00367618515
Iter: 122 loss: 0.00360440859
Iter: 123 loss: 0.00356377894
Iter: 124 loss: 0.0035327957
Iter: 125 loss: 0.00350155076
Iter: 126 loss: 0.00346405362
Iter: 127 loss: 0.00339771667
Iter: 128 loss: 0.00349140773
Iter: 129 loss: 0.00336509477
Iter: 130 loss: 0.00329497317
Iter: 131 loss: 0.00332667958
Iter: 132 loss: 0.00324651087
Iter: 133 loss: 0.00315500377
Iter: 134 loss: 0.00399487885
Iter: 135 loss: 0.00315080583
Iter: 136 loss: 0.00309188711
Iter: 137 loss: 0.00312418165
Iter: 138 loss: 0.00305286632
Iter: 139 loss: 0.00298592029
Iter: 140 loss: 0.00352767715
Iter: 141 loss: 0.00298181735
Iter: 142 loss: 0.00292020896
Iter: 143 loss: 0.0030710781
Iter: 144 loss: 0.0028976528
Iter: 145 loss: 0.00284728315
Iter: 146 loss: 0.00292802788
Iter: 147 loss: 0.00282453466
Iter: 148 loss: 0.00278094504
Iter: 149 loss: 0.00278016785
Iter: 150 loss: 0.00273673981
Iter: 151 loss: 0.0026886831
Iter: 152 loss: 0.00268220389
Iter: 153 loss: 0.0026234535
Iter: 154 loss: 0.00290011615
Iter: 155 loss: 0.00261249882
Iter: 156 loss: 0.00256084348
Iter: 157 loss: 0.00260982104
Iter: 158 loss: 0.00253164792
Iter: 159 loss: 0.00247206027
Iter: 160 loss: 0.00278146286
Iter: 161 loss: 0.00246232725
Iter: 162 loss: 0.00242522382
Iter: 163 loss: 0.00297253253
Iter: 164 loss: 0.002425201
Iter: 165 loss: 0.00239767297
Iter: 166 loss: 0.00235614064
Iter: 167 loss: 0.0023553404
Iter: 168 loss: 0.00230186339
Iter: 169 loss: 0.00296343141
Iter: 170 loss: 0.00230098353
Iter: 171 loss: 0.00226378324
Iter: 172 loss: 0.00232549151
Iter: 173 loss: 0.00224661012
Iter: 174 loss: 0.00220757443
Iter: 175 loss: 0.00257137348
Iter: 176 loss: 0.00220611831
Iter: 177 loss: 0.00218001753
Iter: 178 loss: 0.00218465226
Iter: 179 loss: 0.00215982879
Iter: 180 loss: 0.00213095266
Iter: 181 loss: 0.00249280874
Iter: 182 loss: 0.0021306905
Iter: 183 loss: 0.00210487051
Iter: 184 loss: 0.00210440345
Iter: 185 loss: 0.00208354322
Iter: 186 loss: 0.00204648497
Iter: 187 loss: 0.00224314467
Iter: 188 loss: 0.00204092683
Iter: 189 loss: 0.00200909562
Iter: 190 loss: 0.00213800184
Iter: 191 loss: 0.00200188044
Iter: 192 loss: 0.00197466859
Iter: 193 loss: 0.00195522094
Iter: 194 loss: 0.00194593612
Iter: 195 loss: 0.00190829672
Iter: 196 loss: 0.00208497979
Iter: 197 loss: 0.00190111808
Iter: 198 loss: 0.00187607342
Iter: 199 loss: 0.00187562511
Iter: 200 loss: 0.00185713964
Iter: 201 loss: 0.00182099128
Iter: 202 loss: 0.00253932318
Iter: 203 loss: 0.0018206879
Iter: 204 loss: 0.00178850407
Iter: 205 loss: 0.00212455913
Iter: 206 loss: 0.00178761838
Iter: 207 loss: 0.00176461297
Iter: 208 loss: 0.00184684631
Iter: 209 loss: 0.0017586886
Iter: 210 loss: 0.00173395197
Iter: 211 loss: 0.00177954976
Iter: 212 loss: 0.00172317063
Iter: 213 loss: 0.00170351705
Iter: 214 loss: 0.00190963503
Iter: 215 loss: 0.00170307141
Iter: 216 loss: 0.00168633868
Iter: 217 loss: 0.00187474024
Iter: 218 loss: 0.00168594159
Iter: 219 loss: 0.00167219643
Iter: 220 loss: 0.00165618281
Iter: 221 loss: 0.00165435451
Iter: 222 loss: 0.00162367302
Iter: 223 loss: 0.00173118338
Iter: 224 loss: 0.00161538355
Iter: 225 loss: 0.00158683059
Iter: 226 loss: 0.00169257971
Iter: 227 loss: 0.00157982588
Iter: 228 loss: 0.00155568472
Iter: 229 loss: 0.00169435341
Iter: 230 loss: 0.00155228237
Iter: 231 loss: 0.00153358188
Iter: 232 loss: 0.00166065514
Iter: 233 loss: 0.00153176801
Iter: 234 loss: 0.00151453679
Iter: 235 loss: 0.00151369139
Iter: 236 loss: 0.00150036055
Iter: 237 loss: 0.0014773712
Iter: 238 loss: 0.00154400663
Iter: 239 loss: 0.0014703155
Iter: 240 loss: 0.00145590538
Iter: 241 loss: 0.00145529513
Iter: 242 loss: 0.00144595839
Iter: 243 loss: 0.00143464259
Iter: 244 loss: 0.00143351033
Iter: 245 loss: 0.00141570834
Iter: 246 loss: 0.00149810943
Iter: 247 loss: 0.00141239935
Iter: 248 loss: 0.00139722403
Iter: 249 loss: 0.00143791607
Iter: 250 loss: 0.00139192515
Iter: 251 loss: 0.00137779885
Iter: 252 loss: 0.00146747287
Iter: 253 loss: 0.00137622235
Iter: 254 loss: 0.0013631481
Iter: 255 loss: 0.00135848369
Iter: 256 loss: 0.00135117676
Iter: 257 loss: 0.0013326573
Iter: 258 loss: 0.00134951051
Iter: 259 loss: 0.00132162916
Iter: 260 loss: 0.00129988976
Iter: 261 loss: 0.00142344018
Iter: 262 loss: 0.00129687984
Iter: 263 loss: 0.00127689575
Iter: 264 loss: 0.00137696078
Iter: 265 loss: 0.00127356441
Iter: 266 loss: 0.00125954463
Iter: 267 loss: 0.00133352901
Iter: 268 loss: 0.00125729153
Iter: 269 loss: 0.0012448316
Iter: 270 loss: 0.00125272397
Iter: 271 loss: 0.00123689813
Iter: 272 loss: 0.00122157321
Iter: 273 loss: 0.00129305548
Iter: 274 loss: 0.00121871231
Iter: 275 loss: 0.0012062703
Iter: 276 loss: 0.00125917757
Iter: 277 loss: 0.00120358728
Iter: 278 loss: 0.00118922023
Iter: 279 loss: 0.00124532799
Iter: 280 loss: 0.00118583022
Iter: 281 loss: 0.00117454305
Iter: 282 loss: 0.00118399132
Iter: 283 loss: 0.00116787141
Iter: 284 loss: 0.0011576775
Iter: 285 loss: 0.00115748716
Iter: 286 loss: 0.00114907883
Iter: 287 loss: 0.00114439533
Iter: 288 loss: 0.00114070368
Iter: 289 loss: 0.00112781348
Iter: 290 loss: 0.00117785623
Iter: 291 loss: 0.00112477457
Iter: 292 loss: 0.00111312082
Iter: 293 loss: 0.00114607206
Iter: 294 loss: 0.00110938353
Iter: 295 loss: 0.00109667249
Iter: 296 loss: 0.00110162341
Iter: 297 loss: 0.00108786102
Iter: 298 loss: 0.0010746629
Iter: 299 loss: 0.00127730239
Iter: 300 loss: 0.00107465673
Iter: 301 loss: 0.00106597482
Iter: 302 loss: 0.00107796723
Iter: 303 loss: 0.00106161297
Iter: 304 loss: 0.00104943407
Iter: 305 loss: 0.00107903196
Iter: 306 loss: 0.00104504544
Iter: 307 loss: 0.00103307934
Iter: 308 loss: 0.00107183238
Iter: 309 loss: 0.00102973438
Iter: 310 loss: 0.0010213363
Iter: 311 loss: 0.00111622654
Iter: 312 loss: 0.00102117588
Iter: 313 loss: 0.00101466221
Iter: 314 loss: 0.00101890881
Iter: 315 loss: 0.00101051235
Iter: 316 loss: 0.00100321742
Iter: 317 loss: 0.00105909805
Iter: 318 loss: 0.00100268261
Iter: 319 loss: 0.000996193383
Iter: 320 loss: 0.00100256084
Iter: 321 loss: 0.0009923334
Iter: 322 loss: 0.000983885489
Iter: 323 loss: 0.00100457156
Iter: 324 loss: 0.000980895129
Iter: 325 loss: 0.000970978115
Iter: 326 loss: 0.00100640883
Iter: 327 loss: 0.000968447945
Iter: 328 loss: 0.000958643388
Iter: 329 loss: 0.000992083107
Iter: 330 loss: 0.000955978176
Iter: 331 loss: 0.000946862216
Iter: 332 loss: 0.000960877049
Iter: 333 loss: 0.000942567131
Iter: 334 loss: 0.000933834934
Iter: 335 loss: 0.000960256089
Iter: 336 loss: 0.000931146729
Iter: 337 loss: 0.000923121464
Iter: 338 loss: 0.000945774605
Iter: 339 loss: 0.000920555671
Iter: 340 loss: 0.000912406947
Iter: 341 loss: 0.000920854858
Iter: 342 loss: 0.000907878741
Iter: 343 loss: 0.00089954515
Iter: 344 loss: 0.000910703209
Iter: 345 loss: 0.000895200297
Iter: 346 loss: 0.000885578454
Iter: 347 loss: 0.00091029692
Iter: 348 loss: 0.000882355846
Iter: 349 loss: 0.000873383484
Iter: 350 loss: 0.00100824935
Iter: 351 loss: 0.000873375
Iter: 352 loss: 0.000868439034
Iter: 353 loss: 0.000863948837
Iter: 354 loss: 0.00086270005
Iter: 355 loss: 0.000856805709
Iter: 356 loss: 0.000856695639
Iter: 357 loss: 0.000851000892
Iter: 358 loss: 0.000860728498
Iter: 359 loss: 0.000848405762
Iter: 360 loss: 0.000842978188
Iter: 361 loss: 0.000840656285
Iter: 362 loss: 0.000837856554
Iter: 363 loss: 0.000829745666
Iter: 364 loss: 0.000879355241
Iter: 365 loss: 0.000828766497
Iter: 366 loss: 0.00082039868
Iter: 367 loss: 0.000850085518
Iter: 368 loss: 0.000818175497
Iter: 369 loss: 0.00081127265
Iter: 370 loss: 0.000815943466
Iter: 371 loss: 0.000806947588
Iter: 372 loss: 0.00079850771
Iter: 373 loss: 0.000842010719
Iter: 374 loss: 0.00079715508
Iter: 375 loss: 0.000789421494
Iter: 376 loss: 0.0008515683
Iter: 377 loss: 0.000788857928
Iter: 378 loss: 0.00078427617
Iter: 379 loss: 0.000784698932
Iter: 380 loss: 0.000780744478
Iter: 381 loss: 0.000774566492
Iter: 382 loss: 0.000780544477
Iter: 383 loss: 0.000770875718
Iter: 384 loss: 0.00076412654
Iter: 385 loss: 0.000817201217
Iter: 386 loss: 0.000763664139
Iter: 387 loss: 0.00075813313
Iter: 388 loss: 0.00079678616
Iter: 389 loss: 0.000757618283
Iter: 390 loss: 0.000752462
Iter: 391 loss: 0.000755329733
Iter: 392 loss: 0.00074904639
Iter: 393 loss: 0.00074432086
Iter: 394 loss: 0.000753433385
Iter: 395 loss: 0.000742358156
Iter: 396 loss: 0.000736759277
Iter: 397 loss: 0.000737675175
Iter: 398 loss: 0.000732541201
Iter: 399 loss: 0.000724666927
Iter: 400 loss: 0.000793285901
Iter: 401 loss: 0.000724198821
Iter: 402 loss: 0.000716767274
Iter: 403 loss: 0.000741557335
Iter: 404 loss: 0.000714702299
Iter: 405 loss: 0.000708806096
Iter: 406 loss: 0.000741350581
Iter: 407 loss: 0.000707963365
Iter: 408 loss: 0.000703604077
Iter: 409 loss: 0.000718329218
Iter: 410 loss: 0.000702369
Iter: 411 loss: 0.000697867712
Iter: 412 loss: 0.000753933098
Iter: 413 loss: 0.000697828829
Iter: 414 loss: 0.00069234526
Iter: 415 loss: 0.000706316612
Iter: 416 loss: 0.00069047272
Iter: 417 loss: 0.000686076586
Iter: 418 loss: 0.000691225636
Iter: 419 loss: 0.000683671911
Iter: 420 loss: 0.000678928569
Iter: 421 loss: 0.000675940304
Iter: 422 loss: 0.000674071256
Iter: 423 loss: 0.000669850735
Iter: 424 loss: 0.000669055153
Iter: 425 loss: 0.00066455768
Iter: 426 loss: 0.000677635311
Iter: 427 loss: 0.00066311704
Iter: 428 loss: 0.000658898498
Iter: 429 loss: 0.000661274884
Iter: 430 loss: 0.000656164368
Iter: 431 loss: 0.000650486501
Iter: 432 loss: 0.000669581117
Iter: 433 loss: 0.00064889912
Iter: 434 loss: 0.0006435088
Iter: 435 loss: 0.000655684038
Iter: 436 loss: 0.000641463208
Iter: 437 loss: 0.000636030396
Iter: 438 loss: 0.000647500798
Iter: 439 loss: 0.000633918913
Iter: 440 loss: 0.000628011
Iter: 441 loss: 0.000708246953
Iter: 442 loss: 0.000627995236
Iter: 443 loss: 0.000623964472
Iter: 444 loss: 0.000662608421
Iter: 445 loss: 0.000623772095
Iter: 446 loss: 0.000620904786
Iter: 447 loss: 0.000637689
Iter: 448 loss: 0.000620529521
Iter: 449 loss: 0.000618227874
Iter: 450 loss: 0.000616562087
Iter: 451 loss: 0.000615773839
Iter: 452 loss: 0.000611661
Iter: 453 loss: 0.000622019812
Iter: 454 loss: 0.000610226882
Iter: 455 loss: 0.000607199
Iter: 456 loss: 0.000610801275
Iter: 457 loss: 0.000605605543
Iter: 458 loss: 0.000601528562
Iter: 459 loss: 0.000608130649
Iter: 460 loss: 0.00059965
Iter: 461 loss: 0.000595544
Iter: 462 loss: 0.000615039724
Iter: 463 loss: 0.000594802375
Iter: 464 loss: 0.0005912903
Iter: 465 loss: 0.000619125552
Iter: 466 loss: 0.000591047632
Iter: 467 loss: 0.000587869436
Iter: 468 loss: 0.000585791888
Iter: 469 loss: 0.000584556139
Iter: 470 loss: 0.000579777581
Iter: 471 loss: 0.000588255469
Iter: 472 loss: 0.000577694038
Iter: 473 loss: 0.000573714962
Iter: 474 loss: 0.000604732893
Iter: 475 loss: 0.000573435274
Iter: 476 loss: 0.000569775177
Iter: 477 loss: 0.000571738696
Iter: 478 loss: 0.000567374926
Iter: 479 loss: 0.000564009475
Iter: 480 loss: 0.000614478951
Iter: 481 loss: 0.000564006856
Iter: 482 loss: 0.000561416266
Iter: 483 loss: 0.000584850437
Iter: 484 loss: 0.00056129694
Iter: 485 loss: 0.00055979786
Iter: 486 loss: 0.00055668375
Iter: 487 loss: 0.000611314
Iter: 488 loss: 0.000556608138
Iter: 489 loss: 0.000552259618
Iter: 490 loss: 0.000573498197
Iter: 491 loss: 0.000551500299
Iter: 492 loss: 0.000547827862
Iter: 493 loss: 0.000561603
Iter: 494 loss: 0.000546945725
Iter: 495 loss: 0.000543994247
Iter: 496 loss: 0.000547333562
Iter: 497 loss: 0.000542401802
Iter: 498 loss: 0.000538436812
Iter: 499 loss: 0.000547194504
Iter: 500 loss: 0.000536917243
Iter: 501 loss: 0.000534463441
Iter: 502 loss: 0.000534259
Iter: 503 loss: 0.000532466
Iter: 504 loss: 0.000529581739
Iter: 505 loss: 0.000529559038
Iter: 506 loss: 0.000525653828
Iter: 507 loss: 0.000534473569
Iter: 508 loss: 0.000524186413
Iter: 509 loss: 0.000519846333
Iter: 510 loss: 0.000533067097
Iter: 511 loss: 0.000518534507
Iter: 512 loss: 0.000515180524
Iter: 513 loss: 0.000533706218
Iter: 514 loss: 0.000514696818
Iter: 515 loss: 0.000512102852
Iter: 516 loss: 0.000543185859
Iter: 517 loss: 0.000512066414
Iter: 518 loss: 0.000509801903
Iter: 519 loss: 0.000509797188
Iter: 520 loss: 0.000507986872
Iter: 521 loss: 0.000505161646
Iter: 522 loss: 0.000505312521
Iter: 523 loss: 0.000502930605
Iter: 524 loss: 0.000499643618
Iter: 525 loss: 0.000511026825
Iter: 526 loss: 0.00049877516
Iter: 527 loss: 0.000495809596
Iter: 528 loss: 0.000505830743
Iter: 529 loss: 0.000495010172
Iter: 530 loss: 0.000492352236
Iter: 531 loss: 0.00050597568
Iter: 532 loss: 0.000491918239
Iter: 533 loss: 0.000489615137
Iter: 534 loss: 0.000495859596
Iter: 535 loss: 0.000488853373
Iter: 536 loss: 0.000486767123
Iter: 537 loss: 0.000504734344
Iter: 538 loss: 0.000486650126
Iter: 539 loss: 0.000484903401
Iter: 540 loss: 0.000481798
Iter: 541 loss: 0.000481796975
Iter: 542 loss: 0.00047826278
Iter: 543 loss: 0.000495571236
Iter: 544 loss: 0.000477658759
Iter: 545 loss: 0.000474775123
Iter: 546 loss: 0.000476881
Iter: 547 loss: 0.000472992164
Iter: 548 loss: 0.000471296167
Iter: 549 loss: 0.000471050444
Iter: 550 loss: 0.00046907115
Iter: 551 loss: 0.00047393248
Iter: 552 loss: 0.000468351645
Iter: 553 loss: 0.000466418918
Iter: 554 loss: 0.000466272468
Iter: 555 loss: 0.000464829762
Iter: 556 loss: 0.000462411233
Iter: 557 loss: 0.000466294528
Iter: 558 loss: 0.000461284973
Iter: 559 loss: 0.000458556111
Iter: 560 loss: 0.000465997553
Iter: 561 loss: 0.000457650342
Iter: 562 loss: 0.000455071218
Iter: 563 loss: 0.000462839322
Iter: 564 loss: 0.000454296154
Iter: 565 loss: 0.000451690517
Iter: 566 loss: 0.000466381025
Iter: 567 loss: 0.000451322878
Iter: 568 loss: 0.000449297309
Iter: 569 loss: 0.000460854964
Iter: 570 loss: 0.000449024315
Iter: 571 loss: 0.000447112514
Iter: 572 loss: 0.000451104133
Iter: 573 loss: 0.000446351652
Iter: 574 loss: 0.000444473058
Iter: 575 loss: 0.000443980651
Iter: 576 loss: 0.000442806806
Iter: 577 loss: 0.000440106756
Iter: 578 loss: 0.000447947619
Iter: 579 loss: 0.000439262629
Iter: 580 loss: 0.000436425849
Iter: 581 loss: 0.000442517688
Iter: 582 loss: 0.000435315887
Iter: 583 loss: 0.000435399241
Iter: 584 loss: 0.000434355054
Iter: 585 loss: 0.000433414069
Iter: 586 loss: 0.000431679538
Iter: 587 loss: 0.00047319266
Iter: 588 loss: 0.000431676744
Iter: 589 loss: 0.000429044594
Iter: 590 loss: 0.000434353075
Iter: 591 loss: 0.000427975319
Iter: 592 loss: 0.000425236358
Iter: 593 loss: 0.000432778703
Iter: 594 loss: 0.000424350699
Iter: 595 loss: 0.000421694189
Iter: 596 loss: 0.000429828069
Iter: 597 loss: 0.000420902099
Iter: 598 loss: 0.000418124488
Iter: 599 loss: 0.000422450365
Iter: 600 loss: 0.000416815019
Iter: 601 loss: 0.000413754286
Iter: 602 loss: 0.000438204093
Iter: 603 loss: 0.000413541595
Iter: 604 loss: 0.000411403016
Iter: 605 loss: 0.000425342412
Iter: 606 loss: 0.000411177811
Iter: 607 loss: 0.000409294473
Iter: 608 loss: 0.000408713357
Iter: 609 loss: 0.000407600077
Iter: 610 loss: 0.000404733175
Iter: 611 loss: 0.000409956556
Iter: 612 loss: 0.000403490907
Iter: 613 loss: 0.000400930759
Iter: 614 loss: 0.000410484732
Iter: 615 loss: 0.000400309218
Iter: 616 loss: 0.000398170057
Iter: 617 loss: 0.000407913467
Iter: 618 loss: 0.000397753436
Iter: 619 loss: 0.000395772106
Iter: 620 loss: 0.000422738347
Iter: 621 loss: 0.000395763433
Iter: 622 loss: 0.000394806848
Iter: 623 loss: 0.000392795453
Iter: 624 loss: 0.000427353138
Iter: 625 loss: 0.000392745569
Iter: 626 loss: 0.000390183355
Iter: 627 loss: 0.000397114054
Iter: 628 loss: 0.000389340275
Iter: 629 loss: 0.000386711909
Iter: 630 loss: 0.000407680112
Iter: 631 loss: 0.000386536238
Iter: 632 loss: 0.000384452753
Iter: 633 loss: 0.000386332744
Iter: 634 loss: 0.000383237813
Iter: 635 loss: 0.00038086006
Iter: 636 loss: 0.000389267341
Iter: 637 loss: 0.000380250334
Iter: 638 loss: 0.000378011202
Iter: 639 loss: 0.000398615957
Iter: 640 loss: 0.000377908233
Iter: 641 loss: 0.000376290642
Iter: 642 loss: 0.000380060694
Iter: 643 loss: 0.000375693548
Iter: 644 loss: 0.000373999355
Iter: 645 loss: 0.000373179209
Iter: 646 loss: 0.000372359
Iter: 647 loss: 0.00036969
Iter: 648 loss: 0.000379177975
Iter: 649 loss: 0.000368998386
Iter: 650 loss: 0.000366957625
Iter: 651 loss: 0.00037897937
Iter: 652 loss: 0.00036669051
Iter: 653 loss: 0.000365548709
Iter: 654 loss: 0.000365485379
Iter: 655 loss: 0.000364553474
Iter: 656 loss: 0.000362815976
Iter: 657 loss: 0.000402452453
Iter: 658 loss: 0.000362811028
Iter: 659 loss: 0.000360981474
Iter: 660 loss: 0.000361881219
Iter: 661 loss: 0.000359751808
Iter: 662 loss: 0.00035774766
Iter: 663 loss: 0.000367873465
Iter: 664 loss: 0.000357412966
Iter: 665 loss: 0.000355533673
Iter: 666 loss: 0.000363854982
Iter: 667 loss: 0.000355159311
Iter: 668 loss: 0.000353408279
Iter: 669 loss: 0.000357370503
Iter: 670 loss: 0.000352744
Iter: 671 loss: 0.000351328956
Iter: 672 loss: 0.000358302641
Iter: 673 loss: 0.000351085037
Iter: 674 loss: 0.000349624024
Iter: 675 loss: 0.000355150201
Iter: 676 loss: 0.000349272392
Iter: 677 loss: 0.000348035974
Iter: 678 loss: 0.000348761969
Iter: 679 loss: 0.000347235589
Iter: 680 loss: 0.000345577777
Iter: 681 loss: 0.000346788904
Iter: 682 loss: 0.000344557513
Iter: 683 loss: 0.000342422631
Iter: 684 loss: 0.000351766765
Iter: 685 loss: 0.000341986073
Iter: 686 loss: 0.000341079402
Iter: 687 loss: 0.00034095376
Iter: 688 loss: 0.000339865102
Iter: 689 loss: 0.000338639162
Iter: 690 loss: 0.000338477548
Iter: 691 loss: 0.000336865196
Iter: 692 loss: 0.000338296348
Iter: 693 loss: 0.000335921039
Iter: 694 loss: 0.000334411976
Iter: 695 loss: 0.000335237681
Iter: 696 loss: 0.000333412754
Iter: 697 loss: 0.000331394753
Iter: 698 loss: 0.000343658321
Iter: 699 loss: 0.000331152871
Iter: 700 loss: 0.000329481496
Iter: 701 loss: 0.000335428747
Iter: 702 loss: 0.000329051982
Iter: 703 loss: 0.00032753963
Iter: 704 loss: 0.000332570926
Iter: 705 loss: 0.00032712036
Iter: 706 loss: 0.000325760659
Iter: 707 loss: 0.000333754
Iter: 708 loss: 0.000325586705
Iter: 709 loss: 0.00032425439
Iter: 710 loss: 0.00032546313
Iter: 711 loss: 0.000323481043
Iter: 712 loss: 0.000322155189
Iter: 713 loss: 0.000323463697
Iter: 714 loss: 0.000321410131
Iter: 715 loss: 0.000319894723
Iter: 716 loss: 0.000323537912
Iter: 717 loss: 0.000319351617
Iter: 718 loss: 0.000317953
Iter: 719 loss: 0.000332139782
Iter: 720 loss: 0.000317908271
Iter: 721 loss: 0.000316609134
Iter: 722 loss: 0.000324285123
Iter: 723 loss: 0.000316442369
Iter: 724 loss: 0.000315658486
Iter: 725 loss: 0.000314396631
Iter: 726 loss: 0.000314390665
Iter: 727 loss: 0.000312779855
Iter: 728 loss: 0.000315415033
Iter: 729 loss: 0.000312045333
Iter: 730 loss: 0.000310215662
Iter: 731 loss: 0.00031532679
Iter: 732 loss: 0.000309629482
Iter: 733 loss: 0.000307919923
Iter: 734 loss: 0.000316935213
Iter: 735 loss: 0.000307655893
Iter: 736 loss: 0.000306310627
Iter: 737 loss: 0.0003129018
Iter: 738 loss: 0.000306080619
Iter: 739 loss: 0.00030481859
Iter: 740 loss: 0.000308404153
Iter: 741 loss: 0.000304419344
Iter: 742 loss: 0.000303149573
Iter: 743 loss: 0.000309136667
Iter: 744 loss: 0.000302922446
Iter: 745 loss: 0.000301815278
Iter: 746 loss: 0.000301478052
Iter: 747 loss: 0.000300819578
Iter: 748 loss: 0.000299448904
Iter: 749 loss: 0.000302317494
Iter: 750 loss: 0.000298904022
Iter: 751 loss: 0.000297424442
Iter: 752 loss: 0.000302049186
Iter: 753 loss: 0.000296993792
Iter: 754 loss: 0.000296299317
Iter: 755 loss: 0.000296171173
Iter: 756 loss: 0.000295405567
Iter: 757 loss: 0.000294245663
Iter: 758 loss: 0.000294226775
Iter: 759 loss: 0.000293217949
Iter: 760 loss: 0.000293941528
Iter: 761 loss: 0.000292595214
Iter: 762 loss: 0.000291180477
Iter: 763 loss: 0.000290939759
Iter: 764 loss: 0.00028997293
Iter: 765 loss: 0.000288292475
Iter: 766 loss: 0.00030117936
Iter: 767 loss: 0.000288167241
Iter: 768 loss: 0.00028659706
Iter: 769 loss: 0.000290527969
Iter: 770 loss: 0.000286048715
Iter: 771 loss: 0.00028471023
Iter: 772 loss: 0.000293518824
Iter: 773 loss: 0.00028457283
Iter: 774 loss: 0.00028341834
Iter: 775 loss: 0.000287301431
Iter: 776 loss: 0.000283102418
Iter: 777 loss: 0.000281986489
Iter: 778 loss: 0.000285275164
Iter: 779 loss: 0.000281643093
Iter: 780 loss: 0.000280594715
Iter: 781 loss: 0.00028056858
Iter: 782 loss: 0.000279749336
Iter: 783 loss: 0.0002785135
Iter: 784 loss: 0.000281379034
Iter: 785 loss: 0.000278054737
Iter: 786 loss: 0.00027679684
Iter: 787 loss: 0.000279679749
Iter: 788 loss: 0.000276330684
Iter: 789 loss: 0.000275692786
Iter: 790 loss: 0.000275586586
Iter: 791 loss: 0.00027491941
Iter: 792 loss: 0.0002755897
Iter: 793 loss: 0.000274546095
Iter: 794 loss: 0.00027395715
Iter: 795 loss: 0.000272839505
Iter: 796 loss: 0.000296875311
Iter: 797 loss: 0.000272839563
Iter: 798 loss: 0.000271319674
Iter: 799 loss: 0.000275097147
Iter: 800 loss: 0.000270786521
Iter: 801 loss: 0.000269498909
Iter: 802 loss: 0.000272181525
Iter: 803 loss: 0.000268987293
Iter: 804 loss: 0.000267605297
Iter: 805 loss: 0.00027473
Iter: 806 loss: 0.000267383642
Iter: 807 loss: 0.000266249175
Iter: 808 loss: 0.000272597885
Iter: 809 loss: 0.000266087096
Iter: 810 loss: 0.000265242939
Iter: 811 loss: 0.000271285244
Iter: 812 loss: 0.000265168375
Iter: 813 loss: 0.00026449014
Iter: 814 loss: 0.000265396666
Iter: 815 loss: 0.000264150498
Iter: 816 loss: 0.000263214315
Iter: 817 loss: 0.000262666435
Iter: 818 loss: 0.000262269401
Iter: 819 loss: 0.000261013425
Iter: 820 loss: 0.000267464231
Iter: 821 loss: 0.00026080996
Iter: 822 loss: 0.000259760855
Iter: 823 loss: 0.000260895584
Iter: 824 loss: 0.000259186927
Iter: 825 loss: 0.000258664018
Iter: 826 loss: 0.000258529792
Iter: 827 loss: 0.000257902691
Iter: 828 loss: 0.000257462612
Iter: 829 loss: 0.000257240084
Iter: 830 loss: 0.000256471045
Iter: 831 loss: 0.000256432162
Iter: 832 loss: 0.00025584313
Iter: 833 loss: 0.0002547123
Iter: 834 loss: 0.000257045671
Iter: 835 loss: 0.000254261744
Iter: 836 loss: 0.000253165286
Iter: 837 loss: 0.000255067833
Iter: 838 loss: 0.000252679427
Iter: 839 loss: 0.00025145628
Iter: 840 loss: 0.000256301661
Iter: 841 loss: 0.000251176651
Iter: 842 loss: 0.000250137877
Iter: 843 loss: 0.00025274395
Iter: 844 loss: 0.000249774399
Iter: 845 loss: 0.0002488069
Iter: 846 loss: 0.000262448972
Iter: 847 loss: 0.000248805183
Iter: 848 loss: 0.000248136843
Iter: 849 loss: 0.000248797296
Iter: 850 loss: 0.000247757474
Iter: 851 loss: 0.000246961688
Iter: 852 loss: 0.000248390221
Iter: 853 loss: 0.000246612646
Iter: 854 loss: 0.0002458159
Iter: 855 loss: 0.000246994518
Iter: 856 loss: 0.000245431205
Iter: 857 loss: 0.000244648865
Iter: 858 loss: 0.00024893941
Iter: 859 loss: 0.000244529743
Iter: 860 loss: 0.000243918839
Iter: 861 loss: 0.000251904683
Iter: 862 loss: 0.000243915871
Iter: 863 loss: 0.000243376489
Iter: 864 loss: 0.000242738592
Iter: 865 loss: 0.000242671551
Iter: 866 loss: 0.000241928661
Iter: 867 loss: 0.000241677233
Iter: 868 loss: 0.00024125271
Iter: 869 loss: 0.000239978806
Iter: 870 loss: 0.000243607166
Iter: 871 loss: 0.00023957518
Iter: 872 loss: 0.000238512584
Iter: 873 loss: 0.000241800983
Iter: 874 loss: 0.000238202148
Iter: 875 loss: 0.000237142682
Iter: 876 loss: 0.000239204557
Iter: 877 loss: 0.00023670496
Iter: 878 loss: 0.000235670974
Iter: 879 loss: 0.000241505171
Iter: 880 loss: 0.000235527405
Iter: 881 loss: 0.00023468757
Iter: 882 loss: 0.000243196744
Iter: 883 loss: 0.000234660707
Iter: 884 loss: 0.000234031555
Iter: 885 loss: 0.00023378995
Iter: 886 loss: 0.000233447427
Iter: 887 loss: 0.000232588616
Iter: 888 loss: 0.000236142048
Iter: 889 loss: 0.000232401522
Iter: 890 loss: 0.000231566286
Iter: 891 loss: 0.000231803773
Iter: 892 loss: 0.000230965379
Iter: 893 loss: 0.000230272301
Iter: 894 loss: 0.000230269114
Iter: 895 loss: 0.000229609301
Iter: 896 loss: 0.000231445156
Iter: 897 loss: 0.000229394907
Iter: 898 loss: 0.00022886091
Iter: 899 loss: 0.000228487392
Iter: 900 loss: 0.000228299352
Iter: 901 loss: 0.000227432669
Iter: 902 loss: 0.000228006902
Iter: 903 loss: 0.000226888136
Iter: 904 loss: 0.000225875
Iter: 905 loss: 0.00022880896
Iter: 906 loss: 0.000225559124
Iter: 907 loss: 0.00022462476
Iter: 908 loss: 0.000227797675
Iter: 909 loss: 0.000224375588
Iter: 910 loss: 0.00022340985
Iter: 911 loss: 0.0002263762
Iter: 912 loss: 0.000223127252
Iter: 913 loss: 0.000222296978
Iter: 914 loss: 0.000225110023
Iter: 915 loss: 0.000222073402
Iter: 916 loss: 0.000221249
Iter: 917 loss: 0.00022896321
Iter: 918 loss: 0.000221213893
Iter: 919 loss: 0.000220540183
Iter: 920 loss: 0.000219988273
Iter: 921 loss: 0.000219787282
Iter: 922 loss: 0.00021890529
Iter: 923 loss: 0.00022249615
Iter: 924 loss: 0.000218710775
Iter: 925 loss: 0.000217879773
Iter: 926 loss: 0.000221036593
Iter: 927 loss: 0.000217680252
Iter: 928 loss: 0.000217082386
Iter: 929 loss: 0.000223885538
Iter: 930 loss: 0.000217073539
Iter: 931 loss: 0.000216439992
Iter: 932 loss: 0.000216771179
Iter: 933 loss: 0.000216018598
Iter: 934 loss: 0.000215389708
Iter: 935 loss: 0.000214961692
Iter: 936 loss: 0.000214728949
Iter: 937 loss: 0.000213739811
Iter: 938 loss: 0.000217815992
Iter: 939 loss: 0.000213523905
Iter: 940 loss: 0.000212494604
Iter: 941 loss: 0.000214934
Iter: 942 loss: 0.000212120271
Iter: 943 loss: 0.000211249644
Iter: 944 loss: 0.000214930027
Iter: 945 loss: 0.000211065839
Iter: 946 loss: 0.000210255617
Iter: 947 loss: 0.000211894338
Iter: 948 loss: 0.0002099259
Iter: 949 loss: 0.000209103833
Iter: 950 loss: 0.000211114195
Iter: 951 loss: 0.000208809943
Iter: 952 loss: 0.0002079978
Iter: 953 loss: 0.000216819462
Iter: 954 loss: 0.000207977107
Iter: 955 loss: 0.000207211779
Iter: 956 loss: 0.000207011384
Iter: 957 loss: 0.000206536672
Iter: 958 loss: 0.000205735589
Iter: 959 loss: 0.000209349266
Iter: 960 loss: 0.000205580305
Iter: 961 loss: 0.000204965356
Iter: 962 loss: 0.000206397148
Iter: 963 loss: 0.000204736541
Iter: 964 loss: 0.000204219643
Iter: 965 loss: 0.000204217708
Iter: 966 loss: 0.000203757285
Iter: 967 loss: 0.000203672898
Iter: 968 loss: 0.000203360803
Iter: 969 loss: 0.000202712574
Iter: 970 loss: 0.000202763345
Iter: 971 loss: 0.000202208612
Iter: 972 loss: 0.000201428775
Iter: 973 loss: 0.000202608295
Iter: 974 loss: 0.000201059927
Iter: 975 loss: 0.000200213341
Iter: 976 loss: 0.00020212146
Iter: 977 loss: 0.0001998925
Iter: 978 loss: 0.000198960697
Iter: 979 loss: 0.000199813949
Iter: 980 loss: 0.000198421534
Iter: 981 loss: 0.000197387883
Iter: 982 loss: 0.000206277648
Iter: 983 loss: 0.00019733209
Iter: 984 loss: 0.000196577719
Iter: 985 loss: 0.000197640475
Iter: 986 loss: 0.000196207082
Iter: 987 loss: 0.0001955521
Iter: 988 loss: 0.000195549947
Iter: 989 loss: 0.000194981345
Iter: 990 loss: 0.000195594883
Iter: 991 loss: 0.000194668537
Iter: 992 loss: 0.000194071472
Iter: 993 loss: 0.000194510561
Iter: 994 loss: 0.000193703614
Iter: 995 loss: 0.000193056039
Iter: 996 loss: 0.000195371686
Iter: 997 loss: 0.000192890235
Iter: 998 loss: 0.000192458479
Iter: 999 loss: 0.000192447973
Iter: 1000 loss: 0.000192100095
Iter: 1001 loss: 0.000191527914
Iter: 1002 loss: 0.000191523679
Iter: 1003 loss: 0.000190843639
Iter: 1004 loss: 0.000192690044
Iter: 1005 loss: 0.000190619437
Iter: 1006 loss: 0.000189982864
Iter: 1007 loss: 0.000190511855
Iter: 1008 loss: 0.000189605431
Iter: 1009 loss: 0.000188894104
Iter: 1010 loss: 0.000192216161
Iter: 1011 loss: 0.000188763981
Iter: 1012 loss: 0.000188144302
Iter: 1013 loss: 0.000188075268
Iter: 1014 loss: 0.000187627986
Iter: 1015 loss: 0.000186791527
Iter: 1016 loss: 0.000192309759
Iter: 1017 loss: 0.000186704812
Iter: 1018 loss: 0.000185942903
Iter: 1019 loss: 0.000187299447
Iter: 1020 loss: 0.000185607933
Iter: 1021 loss: 0.000184987264
Iter: 1022 loss: 0.000189667786
Iter: 1023 loss: 0.00018493997
Iter: 1024 loss: 0.000184265402
Iter: 1025 loss: 0.000186794205
Iter: 1026 loss: 0.000184101344
Iter: 1027 loss: 0.000183547847
Iter: 1028 loss: 0.00018327024
Iter: 1029 loss: 0.000183009077
Iter: 1030 loss: 0.000182470627
Iter: 1031 loss: 0.00019012134
Iter: 1032 loss: 0.000182470278
Iter: 1033 loss: 0.000181908341
Iter: 1034 loss: 0.000182664924
Iter: 1035 loss: 0.000181626383
Iter: 1036 loss: 0.000181100986
Iter: 1037 loss: 0.000181172203
Iter: 1038 loss: 0.000180700561
Iter: 1039 loss: 0.000180013187
Iter: 1040 loss: 0.000180674309
Iter: 1041 loss: 0.000179619194
Iter: 1042 loss: 0.000178919261
Iter: 1043 loss: 0.000182258576
Iter: 1044 loss: 0.000178792834
Iter: 1045 loss: 0.000178156828
Iter: 1046 loss: 0.000179402909
Iter: 1047 loss: 0.000177892813
Iter: 1048 loss: 0.000177209658
Iter: 1049 loss: 0.00017770374
Iter: 1050 loss: 0.000176785441
Iter: 1051 loss: 0.000176033907
Iter: 1052 loss: 0.000179149443
Iter: 1053 loss: 0.000175870868
Iter: 1054 loss: 0.000175079738
Iter: 1055 loss: 0.000176038127
Iter: 1056 loss: 0.000174662942
Iter: 1057 loss: 0.000174129862
Iter: 1058 loss: 0.000174121698
Iter: 1059 loss: 0.000173594657
Iter: 1060 loss: 0.000174466302
Iter: 1061 loss: 0.000173357053
Iter: 1062 loss: 0.000172852015
Iter: 1063 loss: 0.0001737118
Iter: 1064 loss: 0.000172625
Iter: 1065 loss: 0.000172235596
Iter: 1066 loss: 0.000172234897
Iter: 1067 loss: 0.000171933294
Iter: 1068 loss: 0.000171502252
Iter: 1069 loss: 0.000171487176
Iter: 1070 loss: 0.000170974745
Iter: 1071 loss: 0.00017152392
Iter: 1072 loss: 0.000170692379
Iter: 1073 loss: 0.000169993044
Iter: 1074 loss: 0.000171168504
Iter: 1075 loss: 0.000169678649
Iter: 1076 loss: 0.000168956554
Iter: 1077 loss: 0.000172643107
Iter: 1078 loss: 0.000168838567
Iter: 1079 loss: 0.000168312908
Iter: 1080 loss: 0.000168846062
Iter: 1081 loss: 0.000168015482
Iter: 1082 loss: 0.00016728387
Iter: 1083 loss: 0.000168086524
Iter: 1084 loss: 0.000166885904
Iter: 1085 loss: 0.000166125188
Iter: 1086 loss: 0.000171984473
Iter: 1087 loss: 0.000166066748
Iter: 1088 loss: 0.000165521691
Iter: 1089 loss: 0.000165097139
Iter: 1090 loss: 0.000164923782
Iter: 1091 loss: 0.000164309662
Iter: 1092 loss: 0.000164283527
Iter: 1093 loss: 0.000163700752
Iter: 1094 loss: 0.000164842815
Iter: 1095 loss: 0.0001634582
Iter: 1096 loss: 0.00016300469
Iter: 1097 loss: 0.000165855337
Iter: 1098 loss: 0.000162952405
Iter: 1099 loss: 0.000162481811
Iter: 1100 loss: 0.000163254212
Iter: 1101 loss: 0.000162265962
Iter: 1102 loss: 0.000161886244
Iter: 1103 loss: 0.000161419186
Iter: 1104 loss: 0.000161376258
Iter: 1105 loss: 0.000160701136
Iter: 1106 loss: 0.000165002406
Iter: 1107 loss: 0.000160627533
Iter: 1108 loss: 0.00016011644
Iter: 1109 loss: 0.000160668525
Iter: 1110 loss: 0.000159835501
Iter: 1111 loss: 0.000159152725
Iter: 1112 loss: 0.000161359465
Iter: 1113 loss: 0.00015895805
Iter: 1114 loss: 0.000158402865
Iter: 1115 loss: 0.00015953806
Iter: 1116 loss: 0.000158178736
Iter: 1117 loss: 0.000157549133
Iter: 1118 loss: 0.000158475566
Iter: 1119 loss: 0.000157246133
Iter: 1120 loss: 0.000156609909
Iter: 1121 loss: 0.000159467279
Iter: 1122 loss: 0.000156483642
Iter: 1123 loss: 0.000155912508
Iter: 1124 loss: 0.0001577043
Iter: 1125 loss: 0.000155746064
Iter: 1126 loss: 0.000155385933
Iter: 1127 loss: 0.000155366026
Iter: 1128 loss: 0.000155063695
Iter: 1129 loss: 0.000155328802
Iter: 1130 loss: 0.000154885405
Iter: 1131 loss: 0.000154533453
Iter: 1132 loss: 0.000156661059
Iter: 1133 loss: 0.000154491237
Iter: 1134 loss: 0.000154198395
Iter: 1135 loss: 0.000153793313
Iter: 1136 loss: 0.000153776578
Iter: 1137 loss: 0.000153297733
Iter: 1138 loss: 0.000153923247
Iter: 1139 loss: 0.000153055298
Iter: 1140 loss: 0.000152509951
Iter: 1141 loss: 0.000155611458
Iter: 1142 loss: 0.000152434543
Iter: 1143 loss: 0.000151999266
Iter: 1144 loss: 0.000153313129
Iter: 1145 loss: 0.000151866407
Iter: 1146 loss: 0.000151402288
Iter: 1147 loss: 0.00015173982
Iter: 1148 loss: 0.000151114145
Iter: 1149 loss: 0.000150629
Iter: 1150 loss: 0.000152516426
Iter: 1151 loss: 0.000150517313
Iter: 1152 loss: 0.000149942498
Iter: 1153 loss: 0.000150331733
Iter: 1154 loss: 0.000149578031
Iter: 1155 loss: 0.000149023108
Iter: 1156 loss: 0.000151030428
Iter: 1157 loss: 0.000148883177
Iter: 1158 loss: 0.000148431311
Iter: 1159 loss: 0.000150848573
Iter: 1160 loss: 0.000148363441
Iter: 1161 loss: 0.000148007559
Iter: 1162 loss: 0.000148007035
Iter: 1163 loss: 0.000147757892
Iter: 1164 loss: 0.000147928338
Iter: 1165 loss: 0.000147601095
Iter: 1166 loss: 0.00014719514
Iter: 1167 loss: 0.000147624378
Iter: 1168 loss: 0.000146971302
Iter: 1169 loss: 0.000146597726
Iter: 1170 loss: 0.00014636002
Iter: 1171 loss: 0.000146210747
Iter: 1172 loss: 0.000145598809
Iter: 1173 loss: 0.000147866231
Iter: 1174 loss: 0.000145448299
Iter: 1175 loss: 0.000144980499
Iter: 1176 loss: 0.000146948179
Iter: 1177 loss: 0.0001448822
Iter: 1178 loss: 0.000144401885
Iter: 1179 loss: 0.000145635495
Iter: 1180 loss: 0.00014423758
Iter: 1181 loss: 0.000143730867
Iter: 1182 loss: 0.0001444222
Iter: 1183 loss: 0.000143478799
Iter: 1184 loss: 0.000142968696
Iter: 1185 loss: 0.000144263278
Iter: 1186 loss: 0.000142793549
Iter: 1187 loss: 0.000142175413
Iter: 1188 loss: 0.000143307188
Iter: 1189 loss: 0.000141911558
Iter: 1190 loss: 0.0001413016
Iter: 1191 loss: 0.000143088953
Iter: 1192 loss: 0.000141114855
Iter: 1193 loss: 0.000140541844
Iter: 1194 loss: 0.000142470089
Iter: 1195 loss: 0.000140385644
Iter: 1196 loss: 0.000140175776
Iter: 1197 loss: 0.000140118529
Iter: 1198 loss: 0.000139832729
Iter: 1199 loss: 0.000139571901
Iter: 1200 loss: 0.000139501091
Iter: 1201 loss: 0.000139099124
Iter: 1202 loss: 0.000142290199
Iter: 1203 loss: 0.000139071068
Iter: 1204 loss: 0.00013874298
Iter: 1205 loss: 0.000138456977
Iter: 1206 loss: 0.000138370902
Iter: 1207 loss: 0.000137872703
Iter: 1208 loss: 0.000138432573
Iter: 1209 loss: 0.000137604191
Iter: 1210 loss: 0.000137038674
Iter: 1211 loss: 0.000139567594
Iter: 1212 loss: 0.000136928633
Iter: 1213 loss: 0.000136427931
Iter: 1214 loss: 0.000137776296
Iter: 1215 loss: 0.000136260089
Iter: 1216 loss: 0.000135717914
Iter: 1217 loss: 0.000137668248
Iter: 1218 loss: 0.000135581213
Iter: 1219 loss: 0.000135127149
Iter: 1220 loss: 0.00013700797
Iter: 1221 loss: 0.000135028444
Iter: 1222 loss: 0.0001345567
Iter: 1223 loss: 0.000134596019
Iter: 1224 loss: 0.000134189642
Iter: 1225 loss: 0.000133671827
Iter: 1226 loss: 0.000135876413
Iter: 1227 loss: 0.000133564055
Iter: 1228 loss: 0.000133061127
Iter: 1229 loss: 0.000134367612
Iter: 1230 loss: 0.00013288742
Iter: 1231 loss: 0.000132852554
Iter: 1232 loss: 0.000132671601
Iter: 1233 loss: 0.00013250114
Iter: 1234 loss: 0.000132068584
Iter: 1235 loss: 0.000136079907
Iter: 1236 loss: 0.000132004832
Iter: 1237 loss: 0.000131585577
Iter: 1238 loss: 0.000137326249
Iter: 1239 loss: 0.000131584151
Iter: 1240 loss: 0.000131309484
Iter: 1241 loss: 0.000131688474
Iter: 1242 loss: 0.000131174456
Iter: 1243 loss: 0.000130854067
Iter: 1244 loss: 0.000130868561
Iter: 1245 loss: 0.000130603381
Iter: 1246 loss: 0.000130235712
Iter: 1247 loss: 0.000130630462
Iter: 1248 loss: 0.000130034008
Iter: 1249 loss: 0.000129559659
Iter: 1250 loss: 0.000131547145
Iter: 1251 loss: 0.000129456806
Iter: 1252 loss: 0.000128988031
Iter: 1253 loss: 0.000130813758
Iter: 1254 loss: 0.000128880012
Iter: 1255 loss: 0.000128404616
Iter: 1256 loss: 0.000129170454
Iter: 1257 loss: 0.000128183921
Iter: 1258 loss: 0.000127728767
Iter: 1259 loss: 0.000128790212
Iter: 1260 loss: 0.000127559877
Iter: 1261 loss: 0.000127066844
Iter: 1262 loss: 0.00012830754
Iter: 1263 loss: 0.000126892977
Iter: 1264 loss: 0.000126522078
Iter: 1265 loss: 0.000132381349
Iter: 1266 loss: 0.000126522689
Iter: 1267 loss: 0.000126195839
Iter: 1268 loss: 0.000127948821
Iter: 1269 loss: 0.000126143059
Iter: 1270 loss: 0.000125959894
Iter: 1271 loss: 0.000125627674
Iter: 1272 loss: 0.000133661611
Iter: 1273 loss: 0.000125627557
Iter: 1274 loss: 0.000125234801
Iter: 1275 loss: 0.000129555963
Iter: 1276 loss: 0.000125223858
Iter: 1277 loss: 0.000124938262
Iter: 1278 loss: 0.000124902974
Iter: 1279 loss: 0.000124696235
Iter: 1280 loss: 0.000124298982
Iter: 1281 loss: 0.000124525221
Iter: 1282 loss: 0.000124039943
Iter: 1283 loss: 0.00012357581
Iter: 1284 loss: 0.000124942992
Iter: 1285 loss: 0.000123435471
Iter: 1286 loss: 0.000122941172
Iter: 1287 loss: 0.000125314458
Iter: 1288 loss: 0.000122852623
Iter: 1289 loss: 0.000122425132
Iter: 1290 loss: 0.000123223959
Iter: 1291 loss: 0.000122241618
Iter: 1292 loss: 0.000121789904
Iter: 1293 loss: 0.000124055456
Iter: 1294 loss: 0.000121712008
Iter: 1295 loss: 0.000121335805
Iter: 1296 loss: 0.000121699748
Iter: 1297 loss: 0.000121120233
Iter: 1298 loss: 0.000120731449
Iter: 1299 loss: 0.000122284095
Iter: 1300 loss: 0.000120644232
Iter: 1301 loss: 0.000120414945
Iter: 1302 loss: 0.000120388147
Iter: 1303 loss: 0.000120209581
Iter: 1304 loss: 0.000119872493
Iter: 1305 loss: 0.000127221283
Iter: 1306 loss: 0.000119871009
Iter: 1307 loss: 0.000119530654
Iter: 1308 loss: 0.000120832934
Iter: 1309 loss: 0.000119450691
Iter: 1310 loss: 0.000119075907
Iter: 1311 loss: 0.000120431519
Iter: 1312 loss: 0.0001189826
Iter: 1313 loss: 0.000118669785
Iter: 1314 loss: 0.000119068107
Iter: 1315 loss: 0.000118508615
Iter: 1316 loss: 0.00011818
Iter: 1317 loss: 0.000117895863
Iter: 1318 loss: 0.000117807103
Iter: 1319 loss: 0.000117326199
Iter: 1320 loss: 0.000120645695
Iter: 1321 loss: 0.000117280404
Iter: 1322 loss: 0.00011687963
Iter: 1323 loss: 0.000118395677
Iter: 1324 loss: 0.000116783573
Iter: 1325 loss: 0.000116395779
Iter: 1326 loss: 0.000117556105
Iter: 1327 loss: 0.000116277224
Iter: 1328 loss: 0.000115885217
Iter: 1329 loss: 0.000116622374
Iter: 1330 loss: 0.000115717514
Iter: 1331 loss: 0.000115350049
Iter: 1332 loss: 0.000116574491
Iter: 1333 loss: 0.000115248724
Iter: 1334 loss: 0.000114999581
Iter: 1335 loss: 0.000114999624
Iter: 1336 loss: 0.000114720315
Iter: 1337 loss: 0.000114937618
Iter: 1338 loss: 0.00011455
Iter: 1339 loss: 0.000114277231
Iter: 1340 loss: 0.000114124865
Iter: 1341 loss: 0.000114003837
Iter: 1342 loss: 0.000113679831
Iter: 1343 loss: 0.000117582946
Iter: 1344 loss: 0.000113677117
Iter: 1345 loss: 0.000113375034
Iter: 1346 loss: 0.000113607261
Iter: 1347 loss: 0.000113191876
Iter: 1348 loss: 0.000112881782
Iter: 1349 loss: 0.000113505048
Iter: 1350 loss: 0.000112755217
Iter: 1351 loss: 0.000112406677
Iter: 1352 loss: 0.000112702939
Iter: 1353 loss: 0.000112199552
Iter: 1354 loss: 0.000111794317
Iter: 1355 loss: 0.000112954804
Iter: 1356 loss: 0.000111666552
Iter: 1357 loss: 0.000111264031
Iter: 1358 loss: 0.000112417394
Iter: 1359 loss: 0.000111137306
Iter: 1360 loss: 0.000110775276
Iter: 1361 loss: 0.000113477836
Iter: 1362 loss: 0.000110746318
Iter: 1363 loss: 0.000110425666
Iter: 1364 loss: 0.000110338173
Iter: 1365 loss: 0.000110139867
Iter: 1366 loss: 0.000109805827
Iter: 1367 loss: 0.00011295361
Iter: 1368 loss: 0.000109792149
Iter: 1369 loss: 0.000109527806
Iter: 1370 loss: 0.000113691582
Iter: 1371 loss: 0.000109527187
Iter: 1372 loss: 0.000109359658
Iter: 1373 loss: 0.000109065659
Iter: 1374 loss: 0.000109066743
Iter: 1375 loss: 0.000108759283
Iter: 1376 loss: 0.00011034694
Iter: 1377 loss: 0.000108710621
Iter: 1378 loss: 0.000108428263
Iter: 1379 loss: 0.000110750392
Iter: 1380 loss: 0.000108411623
Iter: 1381 loss: 0.000108200271
Iter: 1382 loss: 0.00010801907
Iter: 1383 loss: 0.000107960623
Iter: 1384 loss: 0.000107650412
Iter: 1385 loss: 0.000108605986
Iter: 1386 loss: 0.000107556581
Iter: 1387 loss: 0.000107220047
Iter: 1388 loss: 0.000107848107
Iter: 1389 loss: 0.000107076245
Iter: 1390 loss: 0.000106752472
Iter: 1391 loss: 0.000108113272
Iter: 1392 loss: 0.000106684791
Iter: 1393 loss: 0.000106394364
Iter: 1394 loss: 0.000106892519
Iter: 1395 loss: 0.000106265055
Iter: 1396 loss: 0.000105965213
Iter: 1397 loss: 0.000108166299
Iter: 1398 loss: 0.000105940817
Iter: 1399 loss: 0.000105691208
Iter: 1400 loss: 0.000105790627
Iter: 1401 loss: 0.000105517684
Iter: 1402 loss: 0.000105374987
Iter: 1403 loss: 0.000105347572
Iter: 1404 loss: 0.000105184328
Iter: 1405 loss: 0.00010506736
Iter: 1406 loss: 0.000105010738
Iter: 1407 loss: 0.000104797437
Iter: 1408 loss: 0.000104511346
Iter: 1409 loss: 0.000104496212
Iter: 1410 loss: 0.000104211453
Iter: 1411 loss: 0.00010845701
Iter: 1412 loss: 0.000104210136
Iter: 1413 loss: 0.000103923179
Iter: 1414 loss: 0.000104196522
Iter: 1415 loss: 0.000103759099
Iter: 1416 loss: 0.000103475861
Iter: 1417 loss: 0.000103954459
Iter: 1418 loss: 0.00010334704
Iter: 1419 loss: 0.000103058846
Iter: 1420 loss: 0.000103828934
Iter: 1421 loss: 0.000102962258
Iter: 1422 loss: 0.000102687583
Iter: 1423 loss: 0.00010305336
Iter: 1424 loss: 0.000102550344
Iter: 1425 loss: 0.000102247548
Iter: 1426 loss: 0.000103983461
Iter: 1427 loss: 0.000102205566
Iter: 1428 loss: 0.000101968137
Iter: 1429 loss: 0.000102284786
Iter: 1430 loss: 0.000101847632
Iter: 1431 loss: 0.000101558551
Iter: 1432 loss: 0.000103072634
Iter: 1433 loss: 0.000101515849
Iter: 1434 loss: 0.000101268088
Iter: 1435 loss: 0.000102094942
Iter: 1436 loss: 0.000101200276
Iter: 1437 loss: 0.000100959063
Iter: 1438 loss: 0.000103867635
Iter: 1439 loss: 0.000100956007
Iter: 1440 loss: 0.000100822363
Iter: 1441 loss: 0.000100549638
Iter: 1442 loss: 0.000105587947
Iter: 1443 loss: 0.000100545112
Iter: 1444 loss: 0.000100236954
Iter: 1445 loss: 0.000100383055
Iter: 1446 loss: 0.000100028527
Iter: 1447 loss: 9.98283649e-05
Iter: 1448 loss: 9.98095784e-05
Iter: 1449 loss: 9.96247472e-05
Iter: 1450 loss: 9.97101306e-05
Iter: 1451 loss: 9.9498342e-05
Iter: 1452 loss: 9.92605273e-05
Iter: 1453 loss: 9.92377318e-05
Iter: 1454 loss: 9.90648e-05
Iter: 1455 loss: 9.87673557e-05
Iter: 1456 loss: 9.98090836e-05
Iter: 1457 loss: 9.86891537e-05
Iter: 1458 loss: 9.83886421e-05
Iter: 1459 loss: 9.93988724e-05
Iter: 1460 loss: 9.83070204e-05
Iter: 1461 loss: 9.80356926e-05
Iter: 1462 loss: 9.83108112e-05
Iter: 1463 loss: 9.78833559e-05
Iter: 1464 loss: 9.75722796e-05
Iter: 1465 loss: 9.89575419e-05
Iter: 1466 loss: 9.75105359e-05
Iter: 1467 loss: 9.7232878e-05
Iter: 1468 loss: 9.91103152e-05
Iter: 1469 loss: 9.72053e-05
Iter: 1470 loss: 9.70586407e-05
Iter: 1471 loss: 9.70591718e-05
Iter: 1472 loss: 9.69086614e-05
Iter: 1473 loss: 9.67789e-05
Iter: 1474 loss: 9.67371452e-05
Iter: 1475 loss: 9.65295476e-05
Iter: 1476 loss: 9.66108637e-05
Iter: 1477 loss: 9.63853308e-05
Iter: 1478 loss: 9.61285114e-05
Iter: 1479 loss: 9.62949271e-05
Iter: 1480 loss: 9.59645913e-05
Iter: 1481 loss: 9.57521916e-05
Iter: 1482 loss: 9.57495649e-05
Iter: 1483 loss: 9.55633077e-05
Iter: 1484 loss: 9.55309515e-05
Iter: 1485 loss: 9.54048446e-05
Iter: 1486 loss: 9.51428228e-05
Iter: 1487 loss: 9.52393166e-05
Iter: 1488 loss: 9.49590321e-05
Iter: 1489 loss: 9.46767541e-05
Iter: 1490 loss: 9.56465e-05
Iter: 1491 loss: 9.46017681e-05
Iter: 1492 loss: 9.43412815e-05
Iter: 1493 loss: 9.54355128e-05
Iter: 1494 loss: 9.42872866e-05
Iter: 1495 loss: 9.40397586e-05
Iter: 1496 loss: 9.42257466e-05
Iter: 1497 loss: 9.38888843e-05
Iter: 1498 loss: 9.35906e-05
Iter: 1499 loss: 9.53666895e-05
Iter: 1500 loss: 9.35532225e-05
Iter: 1501 loss: 9.3344679e-05
Iter: 1502 loss: 9.44821149e-05
Iter: 1503 loss: 9.33151168e-05
Iter: 1504 loss: 9.31106915e-05
Iter: 1505 loss: 9.48798115e-05
Iter: 1506 loss: 9.309905e-05
Iter: 1507 loss: 9.29433227e-05
Iter: 1508 loss: 9.28833761e-05
Iter: 1509 loss: 9.28016379e-05
Iter: 1510 loss: 9.26176872e-05
Iter: 1511 loss: 9.24609412e-05
Iter: 1512 loss: 9.24108e-05
Iter: 1513 loss: 9.21277388e-05
Iter: 1514 loss: 9.34864947e-05
Iter: 1515 loss: 9.20777165e-05
Iter: 1516 loss: 9.18659207e-05
Iter: 1517 loss: 9.45269712e-05
Iter: 1518 loss: 9.18647711e-05
Iter: 1519 loss: 9.1693626e-05
Iter: 1520 loss: 9.15977798e-05
Iter: 1521 loss: 9.15226774e-05
Iter: 1522 loss: 9.12773612e-05
Iter: 1523 loss: 9.19168815e-05
Iter: 1524 loss: 9.1195936e-05
Iter: 1525 loss: 9.09899682e-05
Iter: 1526 loss: 9.10729577e-05
Iter: 1527 loss: 9.08489601e-05
Iter: 1528 loss: 9.05890338e-05
Iter: 1529 loss: 9.20751772e-05
Iter: 1530 loss: 9.05526103e-05
Iter: 1531 loss: 9.03282198e-05
Iter: 1532 loss: 9.04374174e-05
Iter: 1533 loss: 9.01774838e-05
Iter: 1534 loss: 8.98671933e-05
Iter: 1535 loss: 9.18540463e-05
Iter: 1536 loss: 8.98334692e-05
Iter: 1537 loss: 8.96603233e-05
Iter: 1538 loss: 9.13377735e-05
Iter: 1539 loss: 8.96536949e-05
Iter: 1540 loss: 8.94344703e-05
Iter: 1541 loss: 8.95873964e-05
Iter: 1542 loss: 8.92995522e-05
Iter: 1543 loss: 8.90966912e-05
Iter: 1544 loss: 8.94345576e-05
Iter: 1545 loss: 8.90023948e-05
Iter: 1546 loss: 8.88018185e-05
Iter: 1547 loss: 8.8787383e-05
Iter: 1548 loss: 8.86367561e-05
Iter: 1549 loss: 8.83925604e-05
Iter: 1550 loss: 8.96696729e-05
Iter: 1551 loss: 8.83542234e-05
Iter: 1552 loss: 8.81767919e-05
Iter: 1553 loss: 9.01662133e-05
Iter: 1554 loss: 8.81726446e-05
Iter: 1555 loss: 8.801902e-05
Iter: 1556 loss: 8.78260544e-05
Iter: 1557 loss: 8.78104911e-05
Iter: 1558 loss: 8.75529222e-05
Iter: 1559 loss: 8.80207081e-05
Iter: 1560 loss: 8.74406e-05
Iter: 1561 loss: 8.71562297e-05
Iter: 1562 loss: 8.79055297e-05
Iter: 1563 loss: 8.70596559e-05
Iter: 1564 loss: 8.67817143e-05
Iter: 1565 loss: 8.76419799e-05
Iter: 1566 loss: 8.67001727e-05
Iter: 1567 loss: 8.64367903e-05
Iter: 1568 loss: 8.70158838e-05
Iter: 1569 loss: 8.63361784e-05
Iter: 1570 loss: 8.60734654e-05
Iter: 1571 loss: 8.74474354e-05
Iter: 1572 loss: 8.60324653e-05
Iter: 1573 loss: 8.58878193e-05
Iter: 1574 loss: 8.58798e-05
Iter: 1575 loss: 8.5739317e-05
Iter: 1576 loss: 8.55989492e-05
Iter: 1577 loss: 8.55706603e-05
Iter: 1578 loss: 8.53787496e-05
Iter: 1579 loss: 8.55987528e-05
Iter: 1580 loss: 8.52735247e-05
Iter: 1581 loss: 8.50713914e-05
Iter: 1582 loss: 8.56216138e-05
Iter: 1583 loss: 8.50062497e-05
Iter: 1584 loss: 8.48203636e-05
Iter: 1585 loss: 8.56500046e-05
Iter: 1586 loss: 8.47843257e-05
Iter: 1587 loss: 8.46273324e-05
Iter: 1588 loss: 8.54924292e-05
Iter: 1589 loss: 8.46040348e-05
Iter: 1590 loss: 8.4434876e-05
Iter: 1591 loss: 8.43220478e-05
Iter: 1592 loss: 8.42586524e-05
Iter: 1593 loss: 8.40400171e-05
Iter: 1594 loss: 8.44070892e-05
Iter: 1595 loss: 8.39405839e-05
Iter: 1596 loss: 8.36820545e-05
Iter: 1597 loss: 8.41271758e-05
Iter: 1598 loss: 8.35659885e-05
Iter: 1599 loss: 8.3323619e-05
Iter: 1600 loss: 8.39792774e-05
Iter: 1601 loss: 8.32443475e-05
Iter: 1602 loss: 8.2990271e-05
Iter: 1603 loss: 8.40752691e-05
Iter: 1604 loss: 8.29383425e-05
Iter: 1605 loss: 8.27528856e-05
Iter: 1606 loss: 8.4596235e-05
Iter: 1607 loss: 8.27462063e-05
Iter: 1608 loss: 8.25747702e-05
Iter: 1609 loss: 8.38286069e-05
Iter: 1610 loss: 8.25604729e-05
Iter: 1611 loss: 8.24317831e-05
Iter: 1612 loss: 8.21931608e-05
Iter: 1613 loss: 8.75459227e-05
Iter: 1614 loss: 8.21924332e-05
Iter: 1615 loss: 8.19589623e-05
Iter: 1616 loss: 8.31747311e-05
Iter: 1617 loss: 8.19230627e-05
Iter: 1618 loss: 8.17169421e-05
Iter: 1619 loss: 8.2100225e-05
Iter: 1620 loss: 8.16291649e-05
Iter: 1621 loss: 8.14454543e-05
Iter: 1622 loss: 8.25431489e-05
Iter: 1623 loss: 8.14215164e-05
Iter: 1624 loss: 8.12443468e-05
Iter: 1625 loss: 8.19557608e-05
Iter: 1626 loss: 8.12047365e-05
Iter: 1627 loss: 8.10423e-05
Iter: 1628 loss: 8.08766636e-05
Iter: 1629 loss: 8.08450714e-05
Iter: 1630 loss: 8.05826421e-05
Iter: 1631 loss: 8.19327106e-05
Iter: 1632 loss: 8.0540718e-05
Iter: 1633 loss: 8.03353e-05
Iter: 1634 loss: 8.03496587e-05
Iter: 1635 loss: 8.0176047e-05
Iter: 1636 loss: 7.991046e-05
Iter: 1637 loss: 8.05556119e-05
Iter: 1638 loss: 7.98152469e-05
Iter: 1639 loss: 7.95307569e-05
Iter: 1640 loss: 8.08470795e-05
Iter: 1641 loss: 7.94778389e-05
Iter: 1642 loss: 7.94379303e-05
Iter: 1643 loss: 7.93673826e-05
Iter: 1644 loss: 7.92643259e-05
Iter: 1645 loss: 7.90947743e-05
Iter: 1646 loss: 7.90943886e-05
Iter: 1647 loss: 7.89060359e-05
Iter: 1648 loss: 7.91378e-05
Iter: 1649 loss: 7.8807192e-05
Iter: 1650 loss: 7.85927114e-05
Iter: 1651 loss: 7.9401063e-05
Iter: 1652 loss: 7.85431403e-05
Iter: 1653 loss: 7.83465512e-05
Iter: 1654 loss: 7.87221361e-05
Iter: 1655 loss: 7.82646239e-05
Iter: 1656 loss: 7.80478877e-05
Iter: 1657 loss: 7.92282881e-05
Iter: 1658 loss: 7.80156e-05
Iter: 1659 loss: 7.78354e-05
Iter: 1660 loss: 7.84715157e-05
Iter: 1661 loss: 7.77879322e-05
Iter: 1662 loss: 7.76355664e-05
Iter: 1663 loss: 7.7640434e-05
Iter: 1664 loss: 7.75163498e-05
Iter: 1665 loss: 7.73146821e-05
Iter: 1666 loss: 7.76118686e-05
Iter: 1667 loss: 7.72158164e-05
Iter: 1668 loss: 7.69832e-05
Iter: 1669 loss: 7.7310513e-05
Iter: 1670 loss: 7.68682803e-05
Iter: 1671 loss: 7.66221e-05
Iter: 1672 loss: 7.77116802e-05
Iter: 1673 loss: 7.65724835e-05
Iter: 1674 loss: 7.63475182e-05
Iter: 1675 loss: 7.7464967e-05
Iter: 1676 loss: 7.63100179e-05
Iter: 1677 loss: 7.62828568e-05
Iter: 1678 loss: 7.62269847e-05
Iter: 1679 loss: 7.61651609e-05
Iter: 1680 loss: 7.5997712e-05
Iter: 1681 loss: 7.70527258e-05
Iter: 1682 loss: 7.59533068e-05
Iter: 1683 loss: 7.57618764e-05
Iter: 1684 loss: 7.65050499e-05
Iter: 1685 loss: 7.57164162e-05
Iter: 1686 loss: 7.55406e-05
Iter: 1687 loss: 7.61026313e-05
Iter: 1688 loss: 7.54911307e-05
Iter: 1689 loss: 7.53277127e-05
Iter: 1690 loss: 7.61167466e-05
Iter: 1691 loss: 7.52991182e-05
Iter: 1692 loss: 7.51655898e-05
Iter: 1693 loss: 7.56609952e-05
Iter: 1694 loss: 7.51318657e-05
Iter: 1695 loss: 7.49775209e-05
Iter: 1696 loss: 7.51599073e-05
Iter: 1697 loss: 7.48947496e-05
Iter: 1698 loss: 7.47304657e-05
Iter: 1699 loss: 7.47683807e-05
Iter: 1700 loss: 7.46093283e-05
Iter: 1701 loss: 7.43866694e-05
Iter: 1702 loss: 7.49118699e-05
Iter: 1703 loss: 7.43044293e-05
Iter: 1704 loss: 7.40784162e-05
Iter: 1705 loss: 7.50762847e-05
Iter: 1706 loss: 7.4033087e-05
Iter: 1707 loss: 7.38539893e-05
Iter: 1708 loss: 7.39171592e-05
Iter: 1709 loss: 7.37299124e-05
Iter: 1710 loss: 7.35157591e-05
Iter: 1711 loss: 7.56427e-05
Iter: 1712 loss: 7.35093345e-05
Iter: 1713 loss: 7.3314528e-05
Iter: 1714 loss: 7.51150656e-05
Iter: 1715 loss: 7.3305906e-05
Iter: 1716 loss: 7.32182962e-05
Iter: 1717 loss: 7.29971871e-05
Iter: 1718 loss: 7.50804684e-05
Iter: 1719 loss: 7.29662715e-05
Iter: 1720 loss: 7.27323422e-05
Iter: 1721 loss: 7.45944e-05
Iter: 1722 loss: 7.27164588e-05
Iter: 1723 loss: 7.25214777e-05
Iter: 1724 loss: 7.28944e-05
Iter: 1725 loss: 7.24391e-05
Iter: 1726 loss: 7.22491e-05
Iter: 1727 loss: 7.36786e-05
Iter: 1728 loss: 7.22344121e-05
Iter: 1729 loss: 7.20931566e-05
Iter: 1730 loss: 7.2543553e-05
Iter: 1731 loss: 7.20525059e-05
Iter: 1732 loss: 7.18886877e-05
Iter: 1733 loss: 7.19837553e-05
Iter: 1734 loss: 7.17823059e-05
Iter: 1735 loss: 7.16077484e-05
Iter: 1736 loss: 7.18834781e-05
Iter: 1737 loss: 7.15262358e-05
Iter: 1738 loss: 7.13129702e-05
Iter: 1739 loss: 7.13737e-05
Iter: 1740 loss: 7.11597386e-05
Iter: 1741 loss: 7.09357701e-05
Iter: 1742 loss: 7.26948e-05
Iter: 1743 loss: 7.0918104e-05
Iter: 1744 loss: 7.07278959e-05
Iter: 1745 loss: 7.11269895e-05
Iter: 1746 loss: 7.06528954e-05
Iter: 1747 loss: 7.05741768e-05
Iter: 1748 loss: 7.0554277e-05
Iter: 1749 loss: 7.0453083e-05
Iter: 1750 loss: 7.02965335e-05
Iter: 1751 loss: 7.02946709e-05
Iter: 1752 loss: 7.01442405e-05
Iter: 1753 loss: 7.00969613e-05
Iter: 1754 loss: 7.00086821e-05
Iter: 1755 loss: 6.97970827e-05
Iter: 1756 loss: 7.06883e-05
Iter: 1757 loss: 6.97521755e-05
Iter: 1758 loss: 6.95438e-05
Iter: 1759 loss: 7.06510837e-05
Iter: 1760 loss: 6.95110502e-05
Iter: 1761 loss: 6.93390612e-05
Iter: 1762 loss: 7.02091493e-05
Iter: 1763 loss: 6.93085167e-05
Iter: 1764 loss: 6.91693058e-05
Iter: 1765 loss: 6.97600772e-05
Iter: 1766 loss: 6.91397508e-05
Iter: 1767 loss: 6.90013476e-05
Iter: 1768 loss: 6.89357e-05
Iter: 1769 loss: 6.8868394e-05
Iter: 1770 loss: 6.8688285e-05
Iter: 1771 loss: 6.91535824e-05
Iter: 1772 loss: 6.86271378e-05
Iter: 1773 loss: 6.8421883e-05
Iter: 1774 loss: 6.8777561e-05
Iter: 1775 loss: 6.8329e-05
Iter: 1776 loss: 6.8144087e-05
Iter: 1777 loss: 6.86465792e-05
Iter: 1778 loss: 6.80821686e-05
Iter: 1779 loss: 6.79473451e-05
Iter: 1780 loss: 6.99879529e-05
Iter: 1781 loss: 6.79475124e-05
Iter: 1782 loss: 6.78354118e-05
Iter: 1783 loss: 6.87441643e-05
Iter: 1784 loss: 6.78283686e-05
Iter: 1785 loss: 6.77518328e-05
Iter: 1786 loss: 6.7575922e-05
Iter: 1787 loss: 6.98038784e-05
Iter: 1788 loss: 6.75632e-05
Iter: 1789 loss: 6.73854956e-05
Iter: 1790 loss: 6.82213868e-05
Iter: 1791 loss: 6.73508475e-05
Iter: 1792 loss: 6.72026435e-05
Iter: 1793 loss: 6.72377937e-05
Iter: 1794 loss: 6.70934824e-05
Iter: 1795 loss: 6.69196161e-05
Iter: 1796 loss: 6.8918278e-05
Iter: 1797 loss: 6.69172732e-05
Iter: 1798 loss: 6.67664135e-05
Iter: 1799 loss: 6.725204e-05
Iter: 1800 loss: 6.6724846e-05
Iter: 1801 loss: 6.65817643e-05
Iter: 1802 loss: 6.70611043e-05
Iter: 1803 loss: 6.65439293e-05
Iter: 1804 loss: 6.64061954e-05
Iter: 1805 loss: 6.64466352e-05
Iter: 1806 loss: 6.63077226e-05
Iter: 1807 loss: 6.61322e-05
Iter: 1808 loss: 6.62129896e-05
Iter: 1809 loss: 6.60124642e-05
Iter: 1810 loss: 6.57926503e-05
Iter: 1811 loss: 6.68910652e-05
Iter: 1812 loss: 6.57538767e-05
Iter: 1813 loss: 6.55966214e-05
Iter: 1814 loss: 6.61962113e-05
Iter: 1815 loss: 6.55599579e-05
Iter: 1816 loss: 6.5484579e-05
Iter: 1817 loss: 6.54738105e-05
Iter: 1818 loss: 6.53903553e-05
Iter: 1819 loss: 6.53129391e-05
Iter: 1820 loss: 6.52929884e-05
Iter: 1821 loss: 6.51824084e-05
Iter: 1822 loss: 6.50285219e-05
Iter: 1823 loss: 6.50216825e-05
Iter: 1824 loss: 6.48371497e-05
Iter: 1825 loss: 6.6396693e-05
Iter: 1826 loss: 6.48278219e-05
Iter: 1827 loss: 6.46818298e-05
Iter: 1828 loss: 6.47483685e-05
Iter: 1829 loss: 6.45833061e-05
Iter: 1830 loss: 6.44145475e-05
Iter: 1831 loss: 6.60751684e-05
Iter: 1832 loss: 6.44089523e-05
Iter: 1833 loss: 6.42714731e-05
Iter: 1834 loss: 6.49085705e-05
Iter: 1835 loss: 6.42448431e-05
Iter: 1836 loss: 6.41292281e-05
Iter: 1837 loss: 6.43161475e-05
Iter: 1838 loss: 6.40746e-05
Iter: 1839 loss: 6.39599166e-05
Iter: 1840 loss: 6.42050145e-05
Iter: 1841 loss: 6.39152277e-05
Iter: 1842 loss: 6.37860066e-05
Iter: 1843 loss: 6.3780637e-05
Iter: 1844 loss: 6.36822515e-05
Iter: 1845 loss: 6.350888e-05
Iter: 1846 loss: 6.40212384e-05
Iter: 1847 loss: 6.34571479e-05
Iter: 1848 loss: 6.32780866e-05
Iter: 1849 loss: 6.38450074e-05
Iter: 1850 loss: 6.32268057e-05
Iter: 1851 loss: 6.31676958e-05
Iter: 1852 loss: 6.31273433e-05
Iter: 1853 loss: 6.30727591e-05
Iter: 1854 loss: 6.29244387e-05
Iter: 1855 loss: 6.39501523e-05
Iter: 1856 loss: 6.28905109e-05
Iter: 1857 loss: 6.27101253e-05
Iter: 1858 loss: 6.32222e-05
Iter: 1859 loss: 6.26516266e-05
Iter: 1860 loss: 6.24730383e-05
Iter: 1861 loss: 6.30298164e-05
Iter: 1862 loss: 6.24197637e-05
Iter: 1863 loss: 6.22440857e-05
Iter: 1864 loss: 6.24681707e-05
Iter: 1865 loss: 6.21530489e-05
Iter: 1866 loss: 6.20249557e-05
Iter: 1867 loss: 6.20248611e-05
Iter: 1868 loss: 6.19137354e-05
Iter: 1869 loss: 6.20581559e-05
Iter: 1870 loss: 6.18567501e-05
Iter: 1871 loss: 6.17239129e-05
Iter: 1872 loss: 6.18717459e-05
Iter: 1873 loss: 6.16513862e-05
Iter: 1874 loss: 6.15000245e-05
Iter: 1875 loss: 6.19541679e-05
Iter: 1876 loss: 6.14543387e-05
Iter: 1877 loss: 6.13027805e-05
Iter: 1878 loss: 6.1353152e-05
Iter: 1879 loss: 6.11946962e-05
Iter: 1880 loss: 6.10442512e-05
Iter: 1881 loss: 6.1851184e-05
Iter: 1882 loss: 6.10216593e-05
Iter: 1883 loss: 6.08900191e-05
Iter: 1884 loss: 6.16789403e-05
Iter: 1885 loss: 6.08734372e-05
Iter: 1886 loss: 6.07296279e-05
Iter: 1887 loss: 6.16505495e-05
Iter: 1888 loss: 6.07137627e-05
Iter: 1889 loss: 6.06483736e-05
Iter: 1890 loss: 6.05111563e-05
Iter: 1891 loss: 6.28435955e-05
Iter: 1892 loss: 6.05084497e-05
Iter: 1893 loss: 6.03480185e-05
Iter: 1894 loss: 6.07230395e-05
Iter: 1895 loss: 6.02887412e-05
Iter: 1896 loss: 6.01217362e-05
Iter: 1897 loss: 6.07071852e-05
Iter: 1898 loss: 6.00773928e-05
Iter: 1899 loss: 5.99167033e-05
Iter: 1900 loss: 6.04969537e-05
Iter: 1901 loss: 5.98764309e-05
Iter: 1902 loss: 5.97432227e-05
Iter: 1903 loss: 6.04722845e-05
Iter: 1904 loss: 5.97231265e-05
Iter: 1905 loss: 5.95910024e-05
Iter: 1906 loss: 6.01335923e-05
Iter: 1907 loss: 5.95613601e-05
Iter: 1908 loss: 5.94609737e-05
Iter: 1909 loss: 5.94136218e-05
Iter: 1910 loss: 5.93656441e-05
Iter: 1911 loss: 5.92192773e-05
Iter: 1912 loss: 6.00979147e-05
Iter: 1913 loss: 5.92008219e-05
Iter: 1914 loss: 5.90809686e-05
Iter: 1915 loss: 5.91554344e-05
Iter: 1916 loss: 5.90052914e-05
Iter: 1917 loss: 5.88667135e-05
Iter: 1918 loss: 5.9141541e-05
Iter: 1919 loss: 5.88096518e-05
Iter: 1920 loss: 5.87748764e-05
Iter: 1921 loss: 5.87380746e-05
Iter: 1922 loss: 5.86729257e-05
Iter: 1923 loss: 5.8562011e-05
Iter: 1924 loss: 5.85613889e-05
Iter: 1925 loss: 5.84516238e-05
Iter: 1926 loss: 5.84567133e-05
Iter: 1927 loss: 5.83640576e-05
Iter: 1928 loss: 5.82121138e-05
Iter: 1929 loss: 5.86957867e-05
Iter: 1930 loss: 5.81671411e-05
Iter: 1931 loss: 5.80078231e-05
Iter: 1932 loss: 5.81031854e-05
Iter: 1933 loss: 5.79057087e-05
Iter: 1934 loss: 5.775112e-05
Iter: 1935 loss: 5.94680241e-05
Iter: 1936 loss: 5.77483806e-05
Iter: 1937 loss: 5.76129933e-05
Iter: 1938 loss: 5.7869478e-05
Iter: 1939 loss: 5.75572194e-05
Iter: 1940 loss: 5.74068254e-05
Iter: 1941 loss: 5.83489345e-05
Iter: 1942 loss: 5.73893121e-05
Iter: 1943 loss: 5.73026191e-05
Iter: 1944 loss: 5.72418612e-05
Iter: 1945 loss: 5.72120189e-05
Iter: 1946 loss: 5.70672673e-05
Iter: 1947 loss: 5.75361155e-05
Iter: 1948 loss: 5.70272e-05
Iter: 1949 loss: 5.68799805e-05
Iter: 1950 loss: 5.74284713e-05
Iter: 1951 loss: 5.68447394e-05
Iter: 1952 loss: 5.67248135e-05
Iter: 1953 loss: 5.69342883e-05
Iter: 1954 loss: 5.66724484e-05
Iter: 1955 loss: 5.66110102e-05
Iter: 1956 loss: 5.65914706e-05
Iter: 1957 loss: 5.65429218e-05
Iter: 1958 loss: 5.64231341e-05
Iter: 1959 loss: 5.75875529e-05
Iter: 1960 loss: 5.64073271e-05
Iter: 1961 loss: 5.62790228e-05
Iter: 1962 loss: 5.64158181e-05
Iter: 1963 loss: 5.62085552e-05
Iter: 1964 loss: 5.60578846e-05
Iter: 1965 loss: 5.6768542e-05
Iter: 1966 loss: 5.60304325e-05
Iter: 1967 loss: 5.589184e-05
Iter: 1968 loss: 5.60677e-05
Iter: 1969 loss: 5.58204774e-05
Iter: 1970 loss: 5.56622981e-05
Iter: 1971 loss: 5.65012488e-05
Iter: 1972 loss: 5.56376908e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script120
+ '[' -r STOP.script120 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.8/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi3 /home/mrdouglas/Manifold/experiments.final/output121/f1_psi-1_phi3
+ date
Sun Nov  8 09:05:23 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi3/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi2.8/300_100_100_100_1 --function f1 --psi -1 --phi 3 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi3/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc42c88a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc42c944510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc42c984d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc42c8b6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc42c8931e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc42c7ad2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc42c83df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc42c7eea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc42c7ee510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc42c77e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc42c78a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3ff264f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3ff264d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc42c73cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc42c73c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3ff234a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3ff22b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3ff22b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3ff1ab9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3d80a7158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3d80a7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3ff148378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3ff15ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3c00e5ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3c00f3598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3c00e7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3c003b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3c00e5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3c00e7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3c00e7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc374776d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc37475e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc37475ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc374736620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc374736730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc3d804fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
