+ RUN=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='0 1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 100000 				 --batch_size 5000 				 --max_epochs 1000 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0
+ date
Tue Oct 20 16:32:33 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23a408b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23a40a8048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23a414da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23a4196510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23a4192f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23a40c89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23a40dd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23a4048ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f23a4077950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c695400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c6a1e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c64e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c60c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c609d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c5b5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c5e22f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c5dfd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c5a3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c5431e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c557bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c51a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c52a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c4b1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c4e3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c48c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c4a6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c457598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c3f0048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c40aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c3b3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c3c9ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c3769d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c397488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c396ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c35c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f239c2fc400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.18028845e-06
Iter: 2 loss: 1.85188219e-06
Iter: 3 loss: 9.49571074e-07
Iter: 4 loss: 7.19206355e-07
Iter: 5 loss: 5.37082656e-07
Iter: 6 loss: 4.66246036e-07
Iter: 7 loss: 4.65099674e-07
Iter: 8 loss: 3.83866222e-07
Iter: 9 loss: 3.52302095e-07
Iter: 10 loss: 2.66333416e-07
Iter: 11 loss: 8.03708417e-07
Iter: 12 loss: 2.43955753e-07
Iter: 13 loss: 3.43721211e-07
Iter: 14 loss: 2.31525064e-07
Iter: 15 loss: 2.27431315e-07
Iter: 16 loss: 2.20915581e-07
Iter: 17 loss: 2.20856919e-07
Iter: 18 loss: 2.16860812e-07
Iter: 19 loss: 2.25973309e-07
Iter: 20 loss: 2.15389704e-07
Iter: 21 loss: 2.11780389e-07
Iter: 22 loss: 2.3607025e-07
Iter: 23 loss: 2.1142516e-07
Iter: 24 loss: 2.0821318e-07
Iter: 25 loss: 2.26867769e-07
Iter: 26 loss: 2.07790009e-07
Iter: 27 loss: 2.05301461e-07
Iter: 28 loss: 1.98163164e-07
Iter: 29 loss: 2.28045053e-07
Iter: 30 loss: 1.95337975e-07
Iter: 31 loss: 1.85905265e-07
Iter: 32 loss: 1.87489064e-07
Iter: 33 loss: 1.78805635e-07
Iter: 34 loss: 1.7538126e-07
Iter: 35 loss: 1.75072444e-07
Iter: 36 loss: 1.72683e-07
Iter: 37 loss: 1.74114945e-07
Iter: 38 loss: 1.71151e-07
Iter: 39 loss: 1.70331361e-07
Iter: 40 loss: 1.70302258e-07
Iter: 41 loss: 1.69433804e-07
Iter: 42 loss: 1.69348084e-07
Iter: 43 loss: 1.68705299e-07
Iter: 44 loss: 1.67980019e-07
Iter: 45 loss: 1.67841932e-07
Iter: 46 loss: 1.67356788e-07
Iter: 47 loss: 1.66188784e-07
Iter: 48 loss: 1.79423026e-07
Iter: 49 loss: 1.66177585e-07
Iter: 50 loss: 1.65558404e-07
Iter: 51 loss: 1.64041623e-07
Iter: 52 loss: 1.78286257e-07
Iter: 53 loss: 1.63816367e-07
Iter: 54 loss: 1.61777081e-07
Iter: 55 loss: 1.72804477e-07
Iter: 56 loss: 1.61463944e-07
Iter: 57 loss: 1.59911252e-07
Iter: 58 loss: 1.76892627e-07
Iter: 59 loss: 1.59874205e-07
Iter: 60 loss: 1.58724333e-07
Iter: 61 loss: 1.61066652e-07
Iter: 62 loss: 1.58259112e-07
Iter: 63 loss: 1.57588204e-07
Iter: 64 loss: 1.56176029e-07
Iter: 65 loss: 1.80633464e-07
Iter: 66 loss: 1.56146768e-07
Iter: 67 loss: 1.55537066e-07
Iter: 68 loss: 1.55297883e-07
Iter: 69 loss: 1.54501436e-07
Iter: 70 loss: 1.54117e-07
Iter: 71 loss: 1.53733453e-07
Iter: 72 loss: 1.5270021e-07
Iter: 73 loss: 1.52550683e-07
Iter: 74 loss: 1.51827308e-07
Iter: 75 loss: 1.52587717e-07
Iter: 76 loss: 1.51369306e-07
Iter: 77 loss: 1.51129186e-07
Iter: 78 loss: 1.50495381e-07
Iter: 79 loss: 1.55776959e-07
Iter: 80 loss: 1.50378909e-07
Iter: 81 loss: 1.5018469e-07
Iter: 82 loss: 1.50129154e-07
Iter: 83 loss: 1.49862402e-07
Iter: 84 loss: 1.49906697e-07
Iter: 85 loss: 1.49662895e-07
Iter: 86 loss: 1.49401458e-07
Iter: 87 loss: 1.48684705e-07
Iter: 88 loss: 1.53499428e-07
Iter: 89 loss: 1.48516648e-07
Iter: 90 loss: 1.47539225e-07
Iter: 91 loss: 1.61150851e-07
Iter: 92 loss: 1.47537037e-07
Iter: 93 loss: 1.46765601e-07
Iter: 94 loss: 1.51589944e-07
Iter: 95 loss: 1.46669507e-07
Iter: 96 loss: 1.45946501e-07
Iter: 97 loss: 1.45851971e-07
Iter: 98 loss: 1.4533093e-07
Iter: 99 loss: 1.4447744e-07
Iter: 100 loss: 1.44462e-07
Iter: 101 loss: 1.4378773e-07
Iter: 102 loss: 1.43564563e-07
Iter: 103 loss: 1.43488322e-07
Iter: 104 loss: 1.43186199e-07
Iter: 105 loss: 1.43368965e-07
Iter: 106 loss: 1.42993187e-07
Iter: 107 loss: 1.42831254e-07
Iter: 108 loss: 1.43643604e-07
Iter: 109 loss: 1.42811658e-07
Iter: 110 loss: 1.42563138e-07
Iter: 111 loss: 1.42094336e-07
Iter: 112 loss: 1.52583567e-07
Iter: 113 loss: 1.42097363e-07
Iter: 114 loss: 1.41680204e-07
Iter: 115 loss: 1.41705769e-07
Iter: 116 loss: 1.4135955e-07
Iter: 117 loss: 1.41190981e-07
Iter: 118 loss: 1.41069492e-07
Iter: 119 loss: 1.40853444e-07
Iter: 120 loss: 1.40196875e-07
Iter: 121 loss: 1.41943559e-07
Iter: 122 loss: 1.39845497e-07
Iter: 123 loss: 1.39141477e-07
Iter: 124 loss: 1.42912128e-07
Iter: 125 loss: 1.390287e-07
Iter: 126 loss: 1.38605344e-07
Iter: 127 loss: 1.38608215e-07
Iter: 128 loss: 1.38141942e-07
Iter: 129 loss: 1.3796037e-07
Iter: 130 loss: 1.37720207e-07
Iter: 131 loss: 1.3712534e-07
Iter: 132 loss: 1.39081124e-07
Iter: 133 loss: 1.36959287e-07
Iter: 134 loss: 1.36535959e-07
Iter: 135 loss: 1.37319901e-07
Iter: 136 loss: 1.36357301e-07
Iter: 137 loss: 1.36142063e-07
Iter: 138 loss: 1.36099857e-07
Iter: 139 loss: 1.35995165e-07
Iter: 140 loss: 1.35750071e-07
Iter: 141 loss: 1.39291927e-07
Iter: 142 loss: 1.35740919e-07
Iter: 143 loss: 1.35537078e-07
Iter: 144 loss: 1.35514867e-07
Iter: 145 loss: 1.35428081e-07
Iter: 146 loss: 1.35162622e-07
Iter: 147 loss: 1.36249582e-07
Iter: 148 loss: 1.35056226e-07
Iter: 149 loss: 1.34861537e-07
Iter: 150 loss: 1.34838047e-07
Iter: 151 loss: 1.34557069e-07
Iter: 152 loss: 1.33925425e-07
Iter: 153 loss: 1.42224707e-07
Iter: 154 loss: 1.33882935e-07
Iter: 155 loss: 1.33079425e-07
Iter: 156 loss: 1.32543207e-07
Iter: 157 loss: 1.32246726e-07
Iter: 158 loss: 1.31309562e-07
Iter: 159 loss: 1.36882548e-07
Iter: 160 loss: 1.31187022e-07
Iter: 161 loss: 1.31375458e-07
Iter: 162 loss: 1.30881787e-07
Iter: 163 loss: 1.3070877e-07
Iter: 164 loss: 1.30362309e-07
Iter: 165 loss: 1.37182795e-07
Iter: 166 loss: 1.30360604e-07
Iter: 167 loss: 1.30141032e-07
Iter: 168 loss: 1.3054165e-07
Iter: 169 loss: 1.30035772e-07
Iter: 170 loss: 1.29787423e-07
Iter: 171 loss: 1.30583558e-07
Iter: 172 loss: 1.2972211e-07
Iter: 173 loss: 1.29458613e-07
Iter: 174 loss: 1.29975319e-07
Iter: 175 loss: 1.29337622e-07
Iter: 176 loss: 1.29051557e-07
Iter: 177 loss: 1.28574101e-07
Iter: 178 loss: 1.2857042e-07
Iter: 179 loss: 1.28090164e-07
Iter: 180 loss: 1.29771877e-07
Iter: 181 loss: 1.2797237e-07
Iter: 182 loss: 1.27407418e-07
Iter: 183 loss: 1.27602547e-07
Iter: 184 loss: 1.27019035e-07
Iter: 185 loss: 1.2671542e-07
Iter: 186 loss: 1.26651969e-07
Iter: 187 loss: 1.26160813e-07
Iter: 188 loss: 1.25488796e-07
Iter: 189 loss: 1.25453752e-07
Iter: 190 loss: 1.25202192e-07
Iter: 191 loss: 1.25200629e-07
Iter: 192 loss: 1.2492464e-07
Iter: 193 loss: 1.2486575e-07
Iter: 194 loss: 1.24687304e-07
Iter: 195 loss: 1.2450576e-07
Iter: 196 loss: 1.24496196e-07
Iter: 197 loss: 1.24347608e-07
Iter: 198 loss: 1.24315264e-07
Iter: 199 loss: 1.24253475e-07
Iter: 200 loss: 1.24169603e-07
Iter: 201 loss: 1.23944659e-07
Iter: 202 loss: 1.25930399e-07
Iter: 203 loss: 1.23907256e-07
Iter: 204 loss: 1.23779827e-07
Iter: 205 loss: 1.23767222e-07
Iter: 206 loss: 1.23636099e-07
Iter: 207 loss: 1.23525126e-07
Iter: 208 loss: 1.23500456e-07
Iter: 209 loss: 1.23242827e-07
Iter: 210 loss: 1.228033e-07
Iter: 211 loss: 1.33347612e-07
Iter: 212 loss: 1.22803769e-07
Iter: 213 loss: 1.22366487e-07
Iter: 214 loss: 1.22548727e-07
Iter: 215 loss: 1.22064847e-07
Iter: 216 loss: 1.21751441e-07
Iter: 217 loss: 1.21938271e-07
Iter: 218 loss: 1.2154058e-07
Iter: 219 loss: 1.21404398e-07
Iter: 220 loss: 1.21321e-07
Iter: 221 loss: 1.21171553e-07
Iter: 222 loss: 1.20937116e-07
Iter: 223 loss: 1.20932867e-07
Iter: 224 loss: 1.20841563e-07
Iter: 225 loss: 1.20809716e-07
Iter: 226 loss: 1.20686849e-07
Iter: 227 loss: 1.20364135e-07
Iter: 228 loss: 1.2327375e-07
Iter: 229 loss: 1.20309309e-07
Iter: 230 loss: 1.20071007e-07
Iter: 231 loss: 1.2035656e-07
Iter: 232 loss: 1.19941888e-07
Iter: 233 loss: 1.19883339e-07
Iter: 234 loss: 1.19834766e-07
Iter: 235 loss: 1.19706954e-07
Iter: 236 loss: 1.19474336e-07
Iter: 237 loss: 1.19480589e-07
Iter: 238 loss: 1.19212331e-07
Iter: 239 loss: 1.19522014e-07
Iter: 240 loss: 1.1907543e-07
Iter: 241 loss: 1.18594528e-07
Iter: 242 loss: 1.19364984e-07
Iter: 243 loss: 1.18378296e-07
Iter: 244 loss: 1.18101759e-07
Iter: 245 loss: 1.18358862e-07
Iter: 246 loss: 1.17949433e-07
Iter: 247 loss: 1.17627735e-07
Iter: 248 loss: 1.19340044e-07
Iter: 249 loss: 1.17575794e-07
Iter: 250 loss: 1.17391188e-07
Iter: 251 loss: 1.17824442e-07
Iter: 252 loss: 1.1732034e-07
Iter: 253 loss: 1.17098878e-07
Iter: 254 loss: 1.18378921e-07
Iter: 255 loss: 1.17073675e-07
Iter: 256 loss: 1.1696816e-07
Iter: 257 loss: 1.16973943e-07
Iter: 258 loss: 1.16885772e-07
Iter: 259 loss: 1.16669952e-07
Iter: 260 loss: 1.17402628e-07
Iter: 261 loss: 1.16612853e-07
Iter: 262 loss: 1.16478027e-07
Iter: 263 loss: 1.16148428e-07
Iter: 264 loss: 1.19755498e-07
Iter: 265 loss: 1.16121569e-07
Iter: 266 loss: 1.15894338e-07
Iter: 267 loss: 1.18227405e-07
Iter: 268 loss: 1.15879942e-07
Iter: 269 loss: 1.15717228e-07
Iter: 270 loss: 1.15722081e-07
Iter: 271 loss: 1.15630492e-07
Iter: 272 loss: 1.15431007e-07
Iter: 273 loss: 1.18398781e-07
Iter: 274 loss: 1.15417279e-07
Iter: 275 loss: 1.15259e-07
Iter: 276 loss: 1.15251538e-07
Iter: 277 loss: 1.15155629e-07
Iter: 278 loss: 1.14914727e-07
Iter: 279 loss: 1.17985067e-07
Iter: 280 loss: 1.14903912e-07
Iter: 281 loss: 1.14641644e-07
Iter: 282 loss: 1.17515739e-07
Iter: 283 loss: 1.14634084e-07
Iter: 284 loss: 1.14400407e-07
Iter: 285 loss: 1.14600617e-07
Iter: 286 loss: 1.14251115e-07
Iter: 287 loss: 1.14037505e-07
Iter: 288 loss: 1.171015e-07
Iter: 289 loss: 1.1404569e-07
Iter: 290 loss: 1.13934064e-07
Iter: 291 loss: 1.13947209e-07
Iter: 292 loss: 1.13859748e-07
Iter: 293 loss: 1.13757849e-07
Iter: 294 loss: 1.13758929e-07
Iter: 295 loss: 1.13688699e-07
Iter: 296 loss: 1.13491346e-07
Iter: 297 loss: 1.14868101e-07
Iter: 298 loss: 1.13456956e-07
Iter: 299 loss: 1.13309397e-07
Iter: 300 loss: 1.13449019e-07
Iter: 301 loss: 1.13220793e-07
Iter: 302 loss: 1.13111334e-07
Iter: 303 loss: 1.13096633e-07
Iter: 304 loss: 1.13027532e-07
Iter: 305 loss: 1.12892266e-07
Iter: 306 loss: 1.12841448e-07
Iter: 307 loss: 1.12778466e-07
Iter: 308 loss: 1.1276947e-07
Iter: 309 loss: 1.12705678e-07
Iter: 310 loss: 1.12631852e-07
Iter: 311 loss: 1.12484855e-07
Iter: 312 loss: 1.1564449e-07
Iter: 313 loss: 1.12490056e-07
Iter: 314 loss: 1.12284781e-07
Iter: 315 loss: 1.13326379e-07
Iter: 316 loss: 1.12251243e-07
Iter: 317 loss: 1.12059979e-07
Iter: 318 loss: 1.13621169e-07
Iter: 319 loss: 1.12037931e-07
Iter: 320 loss: 1.11899908e-07
Iter: 321 loss: 1.11889491e-07
Iter: 322 loss: 1.11785539e-07
Iter: 323 loss: 1.11598624e-07
Iter: 324 loss: 1.1166712e-07
Iter: 325 loss: 1.11465006e-07
Iter: 326 loss: 1.11364734e-07
Iter: 327 loss: 1.11346061e-07
Iter: 328 loss: 1.11248447e-07
Iter: 329 loss: 1.11097108e-07
Iter: 330 loss: 1.11090266e-07
Iter: 331 loss: 1.110418e-07
Iter: 332 loss: 1.11021834e-07
Iter: 333 loss: 1.10974923e-07
Iter: 334 loss: 1.10929363e-07
Iter: 335 loss: 1.10923629e-07
Iter: 336 loss: 1.10836574e-07
Iter: 337 loss: 1.10764219e-07
Iter: 338 loss: 1.10739748e-07
Iter: 339 loss: 1.10640649e-07
Iter: 340 loss: 1.11985202e-07
Iter: 341 loss: 1.10637103e-07
Iter: 342 loss: 1.10532461e-07
Iter: 343 loss: 1.10361455e-07
Iter: 344 loss: 1.10356254e-07
Iter: 345 loss: 1.10183642e-07
Iter: 346 loss: 1.11252767e-07
Iter: 347 loss: 1.10163185e-07
Iter: 348 loss: 1.10041725e-07
Iter: 349 loss: 1.10248408e-07
Iter: 350 loss: 1.09989429e-07
Iter: 351 loss: 1.09830111e-07
Iter: 352 loss: 1.10871127e-07
Iter: 353 loss: 1.09810493e-07
Iter: 354 loss: 1.09737478e-07
Iter: 355 loss: 1.09589983e-07
Iter: 356 loss: 1.12718119e-07
Iter: 357 loss: 1.09593785e-07
Iter: 358 loss: 1.09444215e-07
Iter: 359 loss: 1.10681171e-07
Iter: 360 loss: 1.09439355e-07
Iter: 361 loss: 1.0925379e-07
Iter: 362 loss: 1.09420959e-07
Iter: 363 loss: 1.09145319e-07
Iter: 364 loss: 1.0901551e-07
Iter: 365 loss: 1.09537964e-07
Iter: 366 loss: 1.08992467e-07
Iter: 367 loss: 1.08878723e-07
Iter: 368 loss: 1.09537332e-07
Iter: 369 loss: 1.08864789e-07
Iter: 370 loss: 1.08803825e-07
Iter: 371 loss: 1.08689953e-07
Iter: 372 loss: 1.11129921e-07
Iter: 373 loss: 1.0868952e-07
Iter: 374 loss: 1.08574042e-07
Iter: 375 loss: 1.0974648e-07
Iter: 376 loss: 1.08568976e-07
Iter: 377 loss: 1.0847134e-07
Iter: 378 loss: 1.08359032e-07
Iter: 379 loss: 1.08349475e-07
Iter: 380 loss: 1.08167775e-07
Iter: 381 loss: 1.08751323e-07
Iter: 382 loss: 1.08112651e-07
Iter: 383 loss: 1.07976803e-07
Iter: 384 loss: 1.08433888e-07
Iter: 385 loss: 1.07942029e-07
Iter: 386 loss: 1.07843903e-07
Iter: 387 loss: 1.07849189e-07
Iter: 388 loss: 1.07783229e-07
Iter: 389 loss: 1.07644965e-07
Iter: 390 loss: 1.09865717e-07
Iter: 391 loss: 1.0763857e-07
Iter: 392 loss: 1.07545858e-07
Iter: 393 loss: 1.08520574e-07
Iter: 394 loss: 1.07543897e-07
Iter: 395 loss: 1.07487637e-07
Iter: 396 loss: 1.084845e-07
Iter: 397 loss: 1.07487182e-07
Iter: 398 loss: 1.07448898e-07
Iter: 399 loss: 1.0739717e-07
Iter: 400 loss: 1.073943e-07
Iter: 401 loss: 1.07314115e-07
Iter: 402 loss: 1.07787471e-07
Iter: 403 loss: 1.07314129e-07
Iter: 404 loss: 1.0724785e-07
Iter: 405 loss: 1.07158471e-07
Iter: 406 loss: 1.07150811e-07
Iter: 407 loss: 1.07053765e-07
Iter: 408 loss: 1.08292156e-07
Iter: 409 loss: 1.07050496e-07
Iter: 410 loss: 1.06974696e-07
Iter: 411 loss: 1.06929093e-07
Iter: 412 loss: 1.06906327e-07
Iter: 413 loss: 1.0679603e-07
Iter: 414 loss: 1.07162577e-07
Iter: 415 loss: 1.06769093e-07
Iter: 416 loss: 1.06651058e-07
Iter: 417 loss: 1.0668316e-07
Iter: 418 loss: 1.06566645e-07
Iter: 419 loss: 1.06489097e-07
Iter: 420 loss: 1.06483085e-07
Iter: 421 loss: 1.06408862e-07
Iter: 422 loss: 1.06297975e-07
Iter: 423 loss: 1.06301528e-07
Iter: 424 loss: 1.06179463e-07
Iter: 425 loss: 1.06295246e-07
Iter: 426 loss: 1.06117248e-07
Iter: 427 loss: 1.06006667e-07
Iter: 428 loss: 1.07184569e-07
Iter: 429 loss: 1.06002801e-07
Iter: 430 loss: 1.05940266e-07
Iter: 431 loss: 1.05960041e-07
Iter: 432 loss: 1.05888446e-07
Iter: 433 loss: 1.05834374e-07
Iter: 434 loss: 1.06605896e-07
Iter: 435 loss: 1.05838105e-07
Iter: 436 loss: 1.05788402e-07
Iter: 437 loss: 1.05747063e-07
Iter: 438 loss: 1.0573757e-07
Iter: 439 loss: 1.0568462e-07
Iter: 440 loss: 1.06301655e-07
Iter: 441 loss: 1.05685757e-07
Iter: 442 loss: 1.05613125e-07
Iter: 443 loss: 1.05559991e-07
Iter: 444 loss: 1.05534539e-07
Iter: 445 loss: 1.05432747e-07
Iter: 446 loss: 1.05484986e-07
Iter: 447 loss: 1.05374177e-07
Iter: 448 loss: 1.05212074e-07
Iter: 449 loss: 1.05527874e-07
Iter: 450 loss: 1.05157419e-07
Iter: 451 loss: 1.05078357e-07
Iter: 452 loss: 1.05071877e-07
Iter: 453 loss: 1.04997476e-07
Iter: 454 loss: 1.04880158e-07
Iter: 455 loss: 1.04876506e-07
Iter: 456 loss: 1.04755941e-07
Iter: 457 loss: 1.054159e-07
Iter: 458 loss: 1.04734909e-07
Iter: 459 loss: 1.0465611e-07
Iter: 460 loss: 1.05756627e-07
Iter: 461 loss: 1.04653978e-07
Iter: 462 loss: 1.0460024e-07
Iter: 463 loss: 1.04584643e-07
Iter: 464 loss: 1.04551617e-07
Iter: 465 loss: 1.04498241e-07
Iter: 466 loss: 1.05182046e-07
Iter: 467 loss: 1.04492869e-07
Iter: 468 loss: 1.04443252e-07
Iter: 469 loss: 1.04382337e-07
Iter: 470 loss: 1.0437742e-07
Iter: 471 loss: 1.04332535e-07
Iter: 472 loss: 1.0432889e-07
Iter: 473 loss: 1.04289427e-07
Iter: 474 loss: 1.04285668e-07
Iter: 475 loss: 1.04247277e-07
Iter: 476 loss: 1.04185624e-07
Iter: 477 loss: 1.04202371e-07
Iter: 478 loss: 1.04148512e-07
Iter: 479 loss: 1.04042989e-07
Iter: 480 loss: 1.04261531e-07
Iter: 481 loss: 1.04015299e-07
Iter: 482 loss: 1.03943705e-07
Iter: 483 loss: 1.04639234e-07
Iter: 484 loss: 1.03934497e-07
Iter: 485 loss: 1.03847626e-07
Iter: 486 loss: 1.03803949e-07
Iter: 487 loss: 1.0375993e-07
Iter: 488 loss: 1.03657065e-07
Iter: 489 loss: 1.03788786e-07
Iter: 490 loss: 1.03612088e-07
Iter: 491 loss: 1.03528194e-07
Iter: 492 loss: 1.03529047e-07
Iter: 493 loss: 1.03459804e-07
Iter: 494 loss: 1.03441593e-07
Iter: 495 loss: 1.03418245e-07
Iter: 496 loss: 1.03337129e-07
Iter: 497 loss: 1.03858959e-07
Iter: 498 loss: 1.03333093e-07
Iter: 499 loss: 1.03254138e-07
Iter: 500 loss: 1.03250848e-07
Iter: 501 loss: 1.03188427e-07
Iter: 502 loss: 1.03134212e-07
Iter: 503 loss: 1.03537801e-07
Iter: 504 loss: 1.03124989e-07
Iter: 505 loss: 1.030674e-07
Iter: 506 loss: 1.03160424e-07
Iter: 507 loss: 1.03031709e-07
Iter: 508 loss: 1.02975449e-07
Iter: 509 loss: 1.03010407e-07
Iter: 510 loss: 1.02933193e-07
Iter: 511 loss: 1.02864767e-07
Iter: 512 loss: 1.03153376e-07
Iter: 513 loss: 1.02853079e-07
Iter: 514 loss: 1.02797785e-07
Iter: 515 loss: 1.02938202e-07
Iter: 516 loss: 1.02782131e-07
Iter: 517 loss: 1.02692354e-07
Iter: 518 loss: 1.02637031e-07
Iter: 519 loss: 1.02599529e-07
Iter: 520 loss: 1.02501275e-07
Iter: 521 loss: 1.0260969e-07
Iter: 522 loss: 1.02437184e-07
Iter: 523 loss: 1.02334141e-07
Iter: 524 loss: 1.03453836e-07
Iter: 525 loss: 1.02325103e-07
Iter: 526 loss: 1.02234267e-07
Iter: 527 loss: 1.02205881e-07
Iter: 528 loss: 1.02146586e-07
Iter: 529 loss: 1.02043359e-07
Iter: 530 loss: 1.02784931e-07
Iter: 531 loss: 1.02031933e-07
Iter: 532 loss: 1.01935761e-07
Iter: 533 loss: 1.02055587e-07
Iter: 534 loss: 1.01885711e-07
Iter: 535 loss: 1.0181973e-07
Iter: 536 loss: 1.01943243e-07
Iter: 537 loss: 1.01785176e-07
Iter: 538 loss: 1.01685828e-07
Iter: 539 loss: 1.0203496e-07
Iter: 540 loss: 1.01660845e-07
Iter: 541 loss: 1.01590146e-07
Iter: 542 loss: 1.01582351e-07
Iter: 543 loss: 1.01535591e-07
Iter: 544 loss: 1.01439525e-07
Iter: 545 loss: 1.01811729e-07
Iter: 546 loss: 1.01420085e-07
Iter: 547 loss: 1.01341726e-07
Iter: 548 loss: 1.01787442e-07
Iter: 549 loss: 1.01329007e-07
Iter: 550 loss: 1.01231208e-07
Iter: 551 loss: 1.01463257e-07
Iter: 552 loss: 1.01210588e-07
Iter: 553 loss: 1.01160829e-07
Iter: 554 loss: 1.01118431e-07
Iter: 555 loss: 1.01103808e-07
Iter: 556 loss: 1.010132e-07
Iter: 557 loss: 1.01896205e-07
Iter: 558 loss: 1.01006407e-07
Iter: 559 loss: 1.00938934e-07
Iter: 560 loss: 1.00969594e-07
Iter: 561 loss: 1.00893068e-07
Iter: 562 loss: 1.00816386e-07
Iter: 563 loss: 1.01176099e-07
Iter: 564 loss: 1.00800719e-07
Iter: 565 loss: 1.00712143e-07
Iter: 566 loss: 1.00779488e-07
Iter: 567 loss: 1.00661666e-07
Iter: 568 loss: 1.00568514e-07
Iter: 569 loss: 1.00679287e-07
Iter: 570 loss: 1.00518122e-07
Iter: 571 loss: 1.00412365e-07
Iter: 572 loss: 1.01471073e-07
Iter: 573 loss: 1.00408101e-07
Iter: 574 loss: 1.00345687e-07
Iter: 575 loss: 1.00288332e-07
Iter: 576 loss: 1.00270981e-07
Iter: 577 loss: 1.00177012e-07
Iter: 578 loss: 1.00598534e-07
Iter: 579 loss: 1.00159824e-07
Iter: 580 loss: 1.00080584e-07
Iter: 581 loss: 1.00612908e-07
Iter: 582 loss: 1.00077287e-07
Iter: 583 loss: 1.00010524e-07
Iter: 584 loss: 1.0033736e-07
Iter: 585 loss: 9.99974148e-08
Iter: 586 loss: 9.99647654e-08
Iter: 587 loss: 9.98823e-08
Iter: 588 loss: 1.01316601e-07
Iter: 589 loss: 9.98786618e-08
Iter: 590 loss: 9.98379193e-08
Iter: 591 loss: 9.98285543e-08
Iter: 592 loss: 9.97784042e-08
Iter: 593 loss: 9.9800765e-08
Iter: 594 loss: 9.97344358e-08
Iter: 595 loss: 9.96762139e-08
Iter: 596 loss: 9.98263801e-08
Iter: 597 loss: 9.96622305e-08
Iter: 598 loss: 9.95811078e-08
Iter: 599 loss: 9.98149901e-08
Iter: 600 loss: 9.95603529e-08
Iter: 601 loss: 9.95053142e-08
Iter: 602 loss: 9.94941445e-08
Iter: 603 loss: 9.94651685e-08
Iter: 604 loss: 9.93773384e-08
Iter: 605 loss: 1.0004058e-07
Iter: 606 loss: 9.93738922e-08
Iter: 607 loss: 9.93093323e-08
Iter: 608 loss: 9.92306e-08
Iter: 609 loss: 9.92237119e-08
Iter: 610 loss: 9.91321301e-08
Iter: 611 loss: 9.97741765e-08
Iter: 612 loss: 9.91198661e-08
Iter: 613 loss: 9.90459128e-08
Iter: 614 loss: 9.9248922e-08
Iter: 615 loss: 9.9012631e-08
Iter: 616 loss: 9.89416122e-08
Iter: 617 loss: 9.98048222e-08
Iter: 618 loss: 9.894719e-08
Iter: 619 loss: 9.88987381e-08
Iter: 620 loss: 9.87941036e-08
Iter: 621 loss: 1.0000042e-07
Iter: 622 loss: 9.87819391e-08
Iter: 623 loss: 9.86785267e-08
Iter: 624 loss: 9.92910287e-08
Iter: 625 loss: 9.86636337e-08
Iter: 626 loss: 9.852468e-08
Iter: 627 loss: 9.90951392e-08
Iter: 628 loss: 9.85030937e-08
Iter: 629 loss: 9.84266251e-08
Iter: 630 loss: 9.85364252e-08
Iter: 631 loss: 9.83934783e-08
Iter: 632 loss: 9.83028485e-08
Iter: 633 loss: 9.86069253e-08
Iter: 634 loss: 9.82819515e-08
Iter: 635 loss: 9.82233104e-08
Iter: 636 loss: 9.82265718e-08
Iter: 637 loss: 9.81815163e-08
Iter: 638 loss: 9.81022481e-08
Iter: 639 loss: 9.84485595e-08
Iter: 640 loss: 9.8087682e-08
Iter: 641 loss: 9.80121087e-08
Iter: 642 loss: 9.79737891e-08
Iter: 643 loss: 9.79375656e-08
Iter: 644 loss: 9.78484707e-08
Iter: 645 loss: 9.83349153e-08
Iter: 646 loss: 9.78270833e-08
Iter: 647 loss: 9.77210135e-08
Iter: 648 loss: 9.77373205e-08
Iter: 649 loss: 9.76460797e-08
Iter: 650 loss: 9.75318386e-08
Iter: 651 loss: 9.7533551e-08
Iter: 652 loss: 9.74609833e-08
Iter: 653 loss: 9.73099219e-08
Iter: 654 loss: 1.00174077e-07
Iter: 655 loss: 9.73083161e-08
Iter: 656 loss: 9.71566578e-08
Iter: 657 loss: 9.82100516e-08
Iter: 658 loss: 9.7136386e-08
Iter: 659 loss: 9.702255e-08
Iter: 660 loss: 9.79180754e-08
Iter: 661 loss: 9.702336e-08
Iter: 662 loss: 9.69541389e-08
Iter: 663 loss: 9.68883853e-08
Iter: 664 loss: 9.68688809e-08
Iter: 665 loss: 9.67418572e-08
Iter: 666 loss: 9.81898e-08
Iter: 667 loss: 9.67522809e-08
Iter: 668 loss: 9.66844e-08
Iter: 669 loss: 9.66965e-08
Iter: 670 loss: 9.66376916e-08
Iter: 671 loss: 9.65649463e-08
Iter: 672 loss: 9.7061708e-08
Iter: 673 loss: 9.65634257e-08
Iter: 674 loss: 9.6501104e-08
Iter: 675 loss: 9.64992424e-08
Iter: 676 loss: 9.64639923e-08
Iter: 677 loss: 9.63974145e-08
Iter: 678 loss: 9.65895239e-08
Iter: 679 loss: 9.6382422e-08
Iter: 680 loss: 9.63163913e-08
Iter: 681 loss: 9.6458777e-08
Iter: 682 loss: 9.62969295e-08
Iter: 683 loss: 9.62572173e-08
Iter: 684 loss: 9.62556754e-08
Iter: 685 loss: 9.62215694e-08
Iter: 686 loss: 9.61436e-08
Iter: 687 loss: 9.74925101e-08
Iter: 688 loss: 9.61428199e-08
Iter: 689 loss: 9.60455964e-08
Iter: 690 loss: 9.62998143e-08
Iter: 691 loss: 9.60096429e-08
Iter: 692 loss: 9.59301047e-08
Iter: 693 loss: 9.66993525e-08
Iter: 694 loss: 9.59241078e-08
Iter: 695 loss: 9.58577431e-08
Iter: 696 loss: 9.58227844e-08
Iter: 697 loss: 9.5801532e-08
Iter: 698 loss: 9.57268398e-08
Iter: 699 loss: 9.57230455e-08
Iter: 700 loss: 9.56810879e-08
Iter: 701 loss: 9.5617338e-08
Iter: 702 loss: 9.56179065e-08
Iter: 703 loss: 9.55529771e-08
Iter: 704 loss: 9.64803917e-08
Iter: 705 loss: 9.55531902e-08
Iter: 706 loss: 9.5514e-08
Iter: 707 loss: 9.54506234e-08
Iter: 708 loss: 9.54457917e-08
Iter: 709 loss: 9.53651949e-08
Iter: 710 loss: 9.55562172e-08
Iter: 711 loss: 9.53285593e-08
Iter: 712 loss: 9.52294172e-08
Iter: 713 loss: 9.57074207e-08
Iter: 714 loss: 9.52136432e-08
Iter: 715 loss: 9.51475272e-08
Iter: 716 loss: 9.56140269e-08
Iter: 717 loss: 9.51471293e-08
Iter: 718 loss: 9.50715418e-08
Iter: 719 loss: 9.51297636e-08
Iter: 720 loss: 9.50335632e-08
Iter: 721 loss: 9.49635179e-08
Iter: 722 loss: 9.50947125e-08
Iter: 723 loss: 9.49430401e-08
Iter: 724 loss: 9.48915186e-08
Iter: 725 loss: 9.51818038e-08
Iter: 726 loss: 9.48827292e-08
Iter: 727 loss: 9.48317052e-08
Iter: 728 loss: 9.48225249e-08
Iter: 729 loss: 9.47872891e-08
Iter: 730 loss: 9.47353271e-08
Iter: 731 loss: 9.54529185e-08
Iter: 732 loss: 9.47287262e-08
Iter: 733 loss: 9.46865271e-08
Iter: 734 loss: 9.46338616e-08
Iter: 735 loss: 9.46305363e-08
Iter: 736 loss: 9.45591e-08
Iter: 737 loss: 9.52627346e-08
Iter: 738 loss: 9.45588283e-08
Iter: 739 loss: 9.45084508e-08
Iter: 740 loss: 9.44314422e-08
Iter: 741 loss: 9.44273424e-08
Iter: 742 loss: 9.43310567e-08
Iter: 743 loss: 9.46853405e-08
Iter: 744 loss: 9.43135348e-08
Iter: 745 loss: 9.42327e-08
Iter: 746 loss: 9.481235e-08
Iter: 747 loss: 9.42208089e-08
Iter: 748 loss: 9.41707157e-08
Iter: 749 loss: 9.44029779e-08
Iter: 750 loss: 9.41738278e-08
Iter: 751 loss: 9.41149452e-08
Iter: 752 loss: 9.43079925e-08
Iter: 753 loss: 9.4106241e-08
Iter: 754 loss: 9.40663583e-08
Iter: 755 loss: 9.41131617e-08
Iter: 756 loss: 9.40474507e-08
Iter: 757 loss: 9.40026368e-08
Iter: 758 loss: 9.4160491e-08
Iter: 759 loss: 9.39944087e-08
Iter: 760 loss: 9.39503835e-08
Iter: 761 loss: 9.3950149e-08
Iter: 762 loss: 9.39129947e-08
Iter: 763 loss: 9.38651397e-08
Iter: 764 loss: 9.43256566e-08
Iter: 765 loss: 9.38602369e-08
Iter: 766 loss: 9.38082394e-08
Iter: 767 loss: 9.37793914e-08
Iter: 768 loss: 9.37456264e-08
Iter: 769 loss: 9.36869355e-08
Iter: 770 loss: 9.42330658e-08
Iter: 771 loss: 9.36850171e-08
Iter: 772 loss: 9.36322522e-08
Iter: 773 loss: 9.35743429e-08
Iter: 774 loss: 9.35679481e-08
Iter: 775 loss: 9.34851556e-08
Iter: 776 loss: 9.38073725e-08
Iter: 777 loss: 9.34663547e-08
Iter: 778 loss: 9.34000326e-08
Iter: 779 loss: 9.37792493e-08
Iter: 780 loss: 9.33841733e-08
Iter: 781 loss: 9.33235782e-08
Iter: 782 loss: 9.34743838e-08
Iter: 783 loss: 9.33022761e-08
Iter: 784 loss: 9.3225232e-08
Iter: 785 loss: 9.36222833e-08
Iter: 786 loss: 9.32148652e-08
Iter: 787 loss: 9.31634787e-08
Iter: 788 loss: 9.31814128e-08
Iter: 789 loss: 9.31216135e-08
Iter: 790 loss: 9.30597821e-08
Iter: 791 loss: 9.33211339e-08
Iter: 792 loss: 9.30462249e-08
Iter: 793 loss: 9.29774515e-08
Iter: 794 loss: 9.31080777e-08
Iter: 795 loss: 9.29582e-08
Iter: 796 loss: 9.29135098e-08
Iter: 797 loss: 9.3338727e-08
Iter: 798 loss: 9.2912515e-08
Iter: 799 loss: 9.28761068e-08
Iter: 800 loss: 9.28909643e-08
Iter: 801 loss: 9.2846463e-08
Iter: 802 loss: 9.28128827e-08
Iter: 803 loss: 9.30229263e-08
Iter: 804 loss: 9.28054291e-08
Iter: 805 loss: 9.27721828e-08
Iter: 806 loss: 9.27561672e-08
Iter: 807 loss: 9.27352914e-08
Iter: 808 loss: 9.26934e-08
Iter: 809 loss: 9.27715433e-08
Iter: 810 loss: 9.26752648e-08
Iter: 811 loss: 9.26266566e-08
Iter: 812 loss: 9.28775705e-08
Iter: 813 loss: 9.26199561e-08
Iter: 814 loss: 9.25797607e-08
Iter: 815 loss: 9.27593717e-08
Iter: 816 loss: 9.25651e-08
Iter: 817 loss: 9.25222565e-08
Iter: 818 loss: 9.28013e-08
Iter: 819 loss: 9.25196773e-08
Iter: 820 loss: 9.25e-08
Iter: 821 loss: 9.24740959e-08
Iter: 822 loss: 9.24691079e-08
Iter: 823 loss: 9.24195476e-08
Iter: 824 loss: 9.25518435e-08
Iter: 825 loss: 9.24046404e-08
Iter: 826 loss: 9.2348e-08
Iter: 827 loss: 9.24825372e-08
Iter: 828 loss: 9.23317e-08
Iter: 829 loss: 9.22819581e-08
Iter: 830 loss: 9.25296533e-08
Iter: 831 loss: 9.22725718e-08
Iter: 832 loss: 9.22279781e-08
Iter: 833 loss: 9.22792225e-08
Iter: 834 loss: 9.21994143e-08
Iter: 835 loss: 9.21604482e-08
Iter: 836 loss: 9.24319323e-08
Iter: 837 loss: 9.21510548e-08
Iter: 838 loss: 9.21146039e-08
Iter: 839 loss: 9.21087064e-08
Iter: 840 loss: 9.20864096e-08
Iter: 841 loss: 9.20441678e-08
Iter: 842 loss: 9.20667e-08
Iter: 843 loss: 9.20177854e-08
Iter: 844 loss: 9.19670242e-08
Iter: 845 loss: 9.22702554e-08
Iter: 846 loss: 9.19556271e-08
Iter: 847 loss: 9.1910735e-08
Iter: 848 loss: 9.21934245e-08
Iter: 849 loss: 9.19004606e-08
Iter: 850 loss: 9.18636118e-08
Iter: 851 loss: 9.20160588e-08
Iter: 852 loss: 9.18575296e-08
Iter: 853 loss: 9.18160481e-08
Iter: 854 loss: 9.17939502e-08
Iter: 855 loss: 9.17838605e-08
Iter: 856 loss: 9.17249494e-08
Iter: 857 loss: 9.2002395e-08
Iter: 858 loss: 9.1708543e-08
Iter: 859 loss: 9.16460294e-08
Iter: 860 loss: 9.1858972e-08
Iter: 861 loss: 9.16351297e-08
Iter: 862 loss: 9.1606104e-08
Iter: 863 loss: 9.17580536e-08
Iter: 864 loss: 9.15911187e-08
Iter: 865 loss: 9.15454663e-08
Iter: 866 loss: 9.16020753e-08
Iter: 867 loss: 9.15294578e-08
Iter: 868 loss: 9.14887579e-08
Iter: 869 loss: 9.17255107e-08
Iter: 870 loss: 9.14806719e-08
Iter: 871 loss: 9.14404836e-08
Iter: 872 loss: 9.14095253e-08
Iter: 873 loss: 9.13973395e-08
Iter: 874 loss: 9.1332808e-08
Iter: 875 loss: 9.13515095e-08
Iter: 876 loss: 9.12821037e-08
Iter: 877 loss: 9.12091451e-08
Iter: 878 loss: 9.20589898e-08
Iter: 879 loss: 9.12030416e-08
Iter: 880 loss: 9.11741083e-08
Iter: 881 loss: 9.15362861e-08
Iter: 882 loss: 9.11658162e-08
Iter: 883 loss: 9.11399809e-08
Iter: 884 loss: 9.12384763e-08
Iter: 885 loss: 9.11356892e-08
Iter: 886 loss: 9.11024216e-08
Iter: 887 loss: 9.10933551e-08
Iter: 888 loss: 9.10784053e-08
Iter: 889 loss: 9.10376343e-08
Iter: 890 loss: 9.12324793e-08
Iter: 891 loss: 9.10384159e-08
Iter: 892 loss: 9.10107332e-08
Iter: 893 loss: 9.11142308e-08
Iter: 894 loss: 9.10020574e-08
Iter: 895 loss: 9.09785e-08
Iter: 896 loss: 9.10164175e-08
Iter: 897 loss: 9.09697206e-08
Iter: 898 loss: 9.0934428e-08
Iter: 899 loss: 9.10132911e-08
Iter: 900 loss: 9.09165578e-08
Iter: 901 loss: 9.08883635e-08
Iter: 902 loss: 9.10440718e-08
Iter: 903 loss: 9.08893227e-08
Iter: 904 loss: 9.08598281e-08
Iter: 905 loss: 9.08320317e-08
Iter: 906 loss: 9.0819313e-08
Iter: 907 loss: 9.07785136e-08
Iter: 908 loss: 9.08389737e-08
Iter: 909 loss: 9.07537299e-08
Iter: 910 loss: 9.071357e-08
Iter: 911 loss: 9.1041727e-08
Iter: 912 loss: 9.07099746e-08
Iter: 913 loss: 9.067886e-08
Iter: 914 loss: 9.08024e-08
Iter: 915 loss: 9.06755773e-08
Iter: 916 loss: 9.06427147e-08
Iter: 917 loss: 9.07595705e-08
Iter: 918 loss: 9.06387783e-08
Iter: 919 loss: 9.05923727e-08
Iter: 920 loss: 9.06267559e-08
Iter: 921 loss: 9.05760942e-08
Iter: 922 loss: 9.05383217e-08
Iter: 923 loss: 9.07286477e-08
Iter: 924 loss: 9.0535e-08
Iter: 925 loss: 9.05069e-08
Iter: 926 loss: 9.06163962e-08
Iter: 927 loss: 9.04974939e-08
Iter: 928 loss: 9.04726249e-08
Iter: 929 loss: 9.05318274e-08
Iter: 930 loss: 9.04638284e-08
Iter: 931 loss: 9.04354e-08
Iter: 932 loss: 9.05249209e-08
Iter: 933 loss: 9.0431783e-08
Iter: 934 loss: 9.04101398e-08
Iter: 935 loss: 9.05105679e-08
Iter: 936 loss: 9.04120299e-08
Iter: 937 loss: 9.03895554e-08
Iter: 938 loss: 9.03626614e-08
Iter: 939 loss: 9.03521e-08
Iter: 940 loss: 9.03247326e-08
Iter: 941 loss: 9.03973643e-08
Iter: 942 loss: 9.03045247e-08
Iter: 943 loss: 9.02741348e-08
Iter: 944 loss: 9.05336108e-08
Iter: 945 loss: 9.02716835e-08
Iter: 946 loss: 9.02496708e-08
Iter: 947 loss: 9.03041411e-08
Iter: 948 loss: 9.02436952e-08
Iter: 949 loss: 9.02074149e-08
Iter: 950 loss: 9.03374868e-08
Iter: 951 loss: 9.01964228e-08
Iter: 952 loss: 9.01604906e-08
Iter: 953 loss: 9.01894239e-08
Iter: 954 loss: 9.01456119e-08
Iter: 955 loss: 9.01077613e-08
Iter: 956 loss: 9.02022279e-08
Iter: 957 loss: 9.01012314e-08
Iter: 958 loss: 9.00570925e-08
Iter: 959 loss: 9.01401265e-08
Iter: 960 loss: 9.0046882e-08
Iter: 961 loss: 9.00135e-08
Iter: 962 loss: 9.01083723e-08
Iter: 963 loss: 8.99963482e-08
Iter: 964 loss: 8.9958732e-08
Iter: 965 loss: 9.01247e-08
Iter: 966 loss: 8.99494e-08
Iter: 967 loss: 8.99250736e-08
Iter: 968 loss: 9.0023093e-08
Iter: 969 loss: 8.99231765e-08
Iter: 970 loss: 8.98946837e-08
Iter: 971 loss: 8.98750869e-08
Iter: 972 loss: 8.98639669e-08
Iter: 973 loss: 8.98353321e-08
Iter: 974 loss: 8.99009152e-08
Iter: 975 loss: 8.98207873e-08
Iter: 976 loss: 8.97939714e-08
Iter: 977 loss: 8.99659e-08
Iter: 978 loss: 8.97924934e-08
Iter: 979 loss: 8.97628496e-08
Iter: 980 loss: 8.98385153e-08
Iter: 981 loss: 8.97605759e-08
Iter: 982 loss: 8.97206576e-08
Iter: 983 loss: 8.98670436e-08
Iter: 984 loss: 8.97097436e-08
Iter: 985 loss: 8.96888253e-08
Iter: 986 loss: 8.97190162e-08
Iter: 987 loss: 8.96790269e-08
Iter: 988 loss: 8.96512802e-08
Iter: 989 loss: 8.97136729e-08
Iter: 990 loss: 8.96468748e-08
Iter: 991 loss: 8.96172452e-08
Iter: 992 loss: 8.96913761e-08
Iter: 993 loss: 8.96093226e-08
Iter: 994 loss: 8.95812917e-08
Iter: 995 loss: 8.96353924e-08
Iter: 996 loss: 8.95716923e-08
Iter: 997 loss: 8.95453383e-08
Iter: 998 loss: 8.96463348e-08
Iter: 999 loss: 8.95315537e-08
Iter: 1000 loss: 8.95108627e-08
Iter: 1001 loss: 8.95784e-08
Iter: 1002 loss: 8.95042263e-08
Iter: 1003 loss: 8.94781493e-08
Iter: 1004 loss: 8.94572594e-08
Iter: 1005 loss: 8.9445507e-08
Iter: 1006 loss: 8.94101575e-08
Iter: 1007 loss: 8.94998209e-08
Iter: 1008 loss: 8.94010128e-08
Iter: 1009 loss: 8.93690242e-08
Iter: 1010 loss: 8.9616627e-08
Iter: 1011 loss: 8.93619685e-08
Iter: 1012 loss: 8.93305e-08
Iter: 1013 loss: 8.93853311e-08
Iter: 1014 loss: 8.9314689e-08
Iter: 1015 loss: 8.92829561e-08
Iter: 1016 loss: 8.94054324e-08
Iter: 1017 loss: 8.92856065e-08
Iter: 1018 loss: 8.92510315e-08
Iter: 1019 loss: 8.93465568e-08
Iter: 1020 loss: 8.9240487e-08
Iter: 1021 loss: 8.92296725e-08
Iter: 1022 loss: 8.92491059e-08
Iter: 1023 loss: 8.92203644e-08
Iter: 1024 loss: 8.91880916e-08
Iter: 1025 loss: 8.92435779e-08
Iter: 1026 loss: 8.91800411e-08
Iter: 1027 loss: 8.91537368e-08
Iter: 1028 loss: 8.92279246e-08
Iter: 1029 loss: 8.91409115e-08
Iter: 1030 loss: 8.91114453e-08
Iter: 1031 loss: 8.92844199e-08
Iter: 1032 loss: 8.91087666e-08
Iter: 1033 loss: 8.90910954e-08
Iter: 1034 loss: 8.91310208e-08
Iter: 1035 loss: 8.90785898e-08
Iter: 1036 loss: 8.9044164e-08
Iter: 1037 loss: 8.90366394e-08
Iter: 1038 loss: 8.90213343e-08
Iter: 1039 loss: 8.89847911e-08
Iter: 1040 loss: 8.89964298e-08
Iter: 1041 loss: 8.89667575e-08
Iter: 1042 loss: 8.8911932e-08
Iter: 1043 loss: 8.91719907e-08
Iter: 1044 loss: 8.89081093e-08
Iter: 1045 loss: 8.88561544e-08
Iter: 1046 loss: 8.91022864e-08
Iter: 1047 loss: 8.88513583e-08
Iter: 1048 loss: 8.88113192e-08
Iter: 1049 loss: 8.90175613e-08
Iter: 1050 loss: 8.88065159e-08
Iter: 1051 loss: 8.87783926e-08
Iter: 1052 loss: 8.88792542e-08
Iter: 1053 loss: 8.87637128e-08
Iter: 1054 loss: 8.87443363e-08
Iter: 1055 loss: 8.87497862e-08
Iter: 1056 loss: 8.87264946e-08
Iter: 1057 loss: 8.86932128e-08
Iter: 1058 loss: 8.87740512e-08
Iter: 1059 loss: 8.86705607e-08
Iter: 1060 loss: 8.86357796e-08
Iter: 1061 loss: 8.88154403e-08
Iter: 1062 loss: 8.86302e-08
Iter: 1063 loss: 8.85954208e-08
Iter: 1064 loss: 8.87753373e-08
Iter: 1065 loss: 8.85950442e-08
Iter: 1066 loss: 8.8574815e-08
Iter: 1067 loss: 8.86028175e-08
Iter: 1068 loss: 8.85602205e-08
Iter: 1069 loss: 8.85293616e-08
Iter: 1070 loss: 8.85835334e-08
Iter: 1071 loss: 8.85190303e-08
Iter: 1072 loss: 8.84963782e-08
Iter: 1073 loss: 8.84726248e-08
Iter: 1074 loss: 8.84750904e-08
Iter: 1075 loss: 8.84297791e-08
Iter: 1076 loss: 8.8782329e-08
Iter: 1077 loss: 8.84348736e-08
Iter: 1078 loss: 8.8410772e-08
Iter: 1079 loss: 8.85281e-08
Iter: 1080 loss: 8.84011797e-08
Iter: 1081 loss: 8.83674787e-08
Iter: 1082 loss: 8.84856917e-08
Iter: 1083 loss: 8.83600393e-08
Iter: 1084 loss: 8.83352129e-08
Iter: 1085 loss: 8.84125342e-08
Iter: 1086 loss: 8.83312055e-08
Iter: 1087 loss: 8.8302869e-08
Iter: 1088 loss: 8.8309271e-08
Iter: 1089 loss: 8.82875852e-08
Iter: 1090 loss: 8.82451374e-08
Iter: 1091 loss: 8.83920848e-08
Iter: 1092 loss: 8.82377194e-08
Iter: 1093 loss: 8.82046223e-08
Iter: 1094 loss: 8.83139393e-08
Iter: 1095 loss: 8.81992861e-08
Iter: 1096 loss: 8.81715039e-08
Iter: 1097 loss: 8.83353835e-08
Iter: 1098 loss: 8.81669848e-08
Iter: 1099 loss: 8.81443896e-08
Iter: 1100 loss: 8.81625e-08
Iter: 1101 loss: 8.81260291e-08
Iter: 1102 loss: 8.81004922e-08
Iter: 1103 loss: 8.81494131e-08
Iter: 1104 loss: 8.80894504e-08
Iter: 1105 loss: 8.80665212e-08
Iter: 1106 loss: 8.80312427e-08
Iter: 1107 loss: 8.80286848e-08
Iter: 1108 loss: 8.79919497e-08
Iter: 1109 loss: 8.84543141e-08
Iter: 1110 loss: 8.79898465e-08
Iter: 1111 loss: 8.79563729e-08
Iter: 1112 loss: 8.80368702e-08
Iter: 1113 loss: 8.79487345e-08
Iter: 1114 loss: 8.79220465e-08
Iter: 1115 loss: 8.80496174e-08
Iter: 1116 loss: 8.79137687e-08
Iter: 1117 loss: 8.78863631e-08
Iter: 1118 loss: 8.79425954e-08
Iter: 1119 loss: 8.78765718e-08
Iter: 1120 loss: 8.78563853e-08
Iter: 1121 loss: 8.78575293e-08
Iter: 1122 loss: 8.78368454e-08
Iter: 1123 loss: 8.77976092e-08
Iter: 1124 loss: 8.79385e-08
Iter: 1125 loss: 8.7789978e-08
Iter: 1126 loss: 8.77600357e-08
Iter: 1127 loss: 8.78252848e-08
Iter: 1128 loss: 8.77437074e-08
Iter: 1129 loss: 8.7698723e-08
Iter: 1130 loss: 8.78861925e-08
Iter: 1131 loss: 8.76972308e-08
Iter: 1132 loss: 8.76515855e-08
Iter: 1133 loss: 8.77034125e-08
Iter: 1134 loss: 8.76396058e-08
Iter: 1135 loss: 8.76005828e-08
Iter: 1136 loss: 8.76661304e-08
Iter: 1137 loss: 8.75800481e-08
Iter: 1138 loss: 8.75402648e-08
Iter: 1139 loss: 8.74776944e-08
Iter: 1140 loss: 8.74740209e-08
Iter: 1141 loss: 8.7411081e-08
Iter: 1142 loss: 8.82804443e-08
Iter: 1143 loss: 8.74117347e-08
Iter: 1144 loss: 8.73774795e-08
Iter: 1145 loss: 8.74932411e-08
Iter: 1146 loss: 8.73680577e-08
Iter: 1147 loss: 8.73280257e-08
Iter: 1148 loss: 8.77278055e-08
Iter: 1149 loss: 8.73313297e-08
Iter: 1150 loss: 8.7297451e-08
Iter: 1151 loss: 8.73409505e-08
Iter: 1152 loss: 8.72852723e-08
Iter: 1153 loss: 8.72579733e-08
Iter: 1154 loss: 8.73221566e-08
Iter: 1155 loss: 8.72526869e-08
Iter: 1156 loss: 8.72249188e-08
Iter: 1157 loss: 8.73083792e-08
Iter: 1158 loss: 8.72220838e-08
Iter: 1159 loss: 8.71874448e-08
Iter: 1160 loss: 8.72700809e-08
Iter: 1161 loss: 8.71862156e-08
Iter: 1162 loss: 8.71546249e-08
Iter: 1163 loss: 8.72247199e-08
Iter: 1164 loss: 8.71535448e-08
Iter: 1165 loss: 8.71177122e-08
Iter: 1166 loss: 8.71792594e-08
Iter: 1167 loss: 8.71114167e-08
Iter: 1168 loss: 8.70711574e-08
Iter: 1169 loss: 8.71270203e-08
Iter: 1170 loss: 8.70592345e-08
Iter: 1171 loss: 8.70239276e-08
Iter: 1172 loss: 8.69647252e-08
Iter: 1173 loss: 8.69624159e-08
Iter: 1174 loss: 8.69233787e-08
Iter: 1175 loss: 8.6922114e-08
Iter: 1176 loss: 8.68893224e-08
Iter: 1177 loss: 8.69162e-08
Iter: 1178 loss: 8.68700951e-08
Iter: 1179 loss: 8.68275833e-08
Iter: 1180 loss: 8.72381136e-08
Iter: 1181 loss: 8.68364296e-08
Iter: 1182 loss: 8.67981385e-08
Iter: 1183 loss: 8.68121148e-08
Iter: 1184 loss: 8.67790746e-08
Iter: 1185 loss: 8.67394405e-08
Iter: 1186 loss: 8.68984245e-08
Iter: 1187 loss: 8.67365344e-08
Iter: 1188 loss: 8.67051426e-08
Iter: 1189 loss: 8.67764385e-08
Iter: 1190 loss: 8.66976748e-08
Iter: 1191 loss: 8.6672614e-08
Iter: 1192 loss: 8.67632508e-08
Iter: 1193 loss: 8.66584e-08
Iter: 1194 loss: 8.6633662e-08
Iter: 1195 loss: 8.67003109e-08
Iter: 1196 loss: 8.6619842e-08
Iter: 1197 loss: 8.65886136e-08
Iter: 1198 loss: 8.66744045e-08
Iter: 1199 loss: 8.65802576e-08
Iter: 1200 loss: 8.65504575e-08
Iter: 1201 loss: 8.66217178e-08
Iter: 1202 loss: 8.6542542e-08
Iter: 1203 loss: 8.65136798e-08
Iter: 1204 loss: 8.64559269e-08
Iter: 1205 loss: 8.71152039e-08
Iter: 1206 loss: 8.64455671e-08
Iter: 1207 loss: 8.64067289e-08
Iter: 1208 loss: 8.64057483e-08
Iter: 1209 loss: 8.63521308e-08
Iter: 1210 loss: 8.64474359e-08
Iter: 1211 loss: 8.63387e-08
Iter: 1212 loss: 8.62964669e-08
Iter: 1213 loss: 8.65502727e-08
Iter: 1214 loss: 8.62909e-08
Iter: 1215 loss: 8.62379679e-08
Iter: 1216 loss: 8.6211152e-08
Iter: 1217 loss: 8.61909e-08
Iter: 1218 loss: 8.61148308e-08
Iter: 1219 loss: 8.65317276e-08
Iter: 1220 loss: 8.61106173e-08
Iter: 1221 loss: 8.60686526e-08
Iter: 1222 loss: 8.61624e-08
Iter: 1223 loss: 8.60525589e-08
Iter: 1224 loss: 8.60017195e-08
Iter: 1225 loss: 8.61219434e-08
Iter: 1226 loss: 8.59835296e-08
Iter: 1227 loss: 8.59398952e-08
Iter: 1228 loss: 8.60032685e-08
Iter: 1229 loss: 8.592e-08
Iter: 1230 loss: 8.58761311e-08
Iter: 1231 loss: 8.62096812e-08
Iter: 1232 loss: 8.58685425e-08
Iter: 1233 loss: 8.5837e-08
Iter: 1234 loss: 8.5849365e-08
Iter: 1235 loss: 8.58086509e-08
Iter: 1236 loss: 8.57615703e-08
Iter: 1237 loss: 8.57281677e-08
Iter: 1238 loss: 8.5714241e-08
Iter: 1239 loss: 8.56877946e-08
Iter: 1240 loss: 8.6228475e-08
Iter: 1241 loss: 8.56866649e-08
Iter: 1242 loss: 8.56473292e-08
Iter: 1243 loss: 8.57448299e-08
Iter: 1244 loss: 8.5642526e-08
Iter: 1245 loss: 8.56064304e-08
Iter: 1246 loss: 8.57007123e-08
Iter: 1247 loss: 8.55999076e-08
Iter: 1248 loss: 8.55680682e-08
Iter: 1249 loss: 8.56495461e-08
Iter: 1250 loss: 8.55511e-08
Iter: 1251 loss: 8.55255564e-08
Iter: 1252 loss: 8.57150724e-08
Iter: 1253 loss: 8.55251585e-08
Iter: 1254 loss: 8.54990958e-08
Iter: 1255 loss: 8.55038422e-08
Iter: 1256 loss: 8.5490214e-08
Iter: 1257 loss: 8.54611955e-08
Iter: 1258 loss: 8.55902513e-08
Iter: 1259 loss: 8.54516e-08
Iter: 1260 loss: 8.54250501e-08
Iter: 1261 loss: 8.54598312e-08
Iter: 1262 loss: 8.54183213e-08
Iter: 1263 loss: 8.53866737e-08
Iter: 1264 loss: 8.54288373e-08
Iter: 1265 loss: 8.53759445e-08
Iter: 1266 loss: 8.53231228e-08
Iter: 1267 loss: 8.54226556e-08
Iter: 1268 loss: 8.53052313e-08
Iter: 1269 loss: 8.52711679e-08
Iter: 1270 loss: 8.5225814e-08
Iter: 1271 loss: 8.52180193e-08
Iter: 1272 loss: 8.51733475e-08
Iter: 1273 loss: 8.5206679e-08
Iter: 1274 loss: 8.51444213e-08
Iter: 1275 loss: 8.5081318e-08
Iter: 1276 loss: 8.59631513e-08
Iter: 1277 loss: 8.50811617e-08
Iter: 1278 loss: 8.50553263e-08
Iter: 1279 loss: 8.5013923e-08
Iter: 1280 loss: 8.50147615e-08
Iter: 1281 loss: 8.49666151e-08
Iter: 1282 loss: 8.56933866e-08
Iter: 1283 loss: 8.49687112e-08
Iter: 1284 loss: 8.49394794e-08
Iter: 1285 loss: 8.50492654e-08
Iter: 1286 loss: 8.49368931e-08
Iter: 1287 loss: 8.49111146e-08
Iter: 1288 loss: 8.49088408e-08
Iter: 1289 loss: 8.48914823e-08
Iter: 1290 loss: 8.48644106e-08
Iter: 1291 loss: 8.50744186e-08
Iter: 1292 loss: 8.48543635e-08
Iter: 1293 loss: 8.48303401e-08
Iter: 1294 loss: 8.48633164e-08
Iter: 1295 loss: 8.48108925e-08
Iter: 1296 loss: 8.4789761e-08
Iter: 1297 loss: 8.48149782e-08
Iter: 1298 loss: 8.4775948e-08
Iter: 1299 loss: 8.47410746e-08
Iter: 1300 loss: 8.50935038e-08
Iter: 1301 loss: 8.47337134e-08
Iter: 1302 loss: 8.47144292e-08
Iter: 1303 loss: 8.46713704e-08
Iter: 1304 loss: 8.55223163e-08
Iter: 1305 loss: 8.46655794e-08
Iter: 1306 loss: 8.46101074e-08
Iter: 1307 loss: 8.46754631e-08
Iter: 1308 loss: 8.4584e-08
Iter: 1309 loss: 8.45525108e-08
Iter: 1310 loss: 8.4549896e-08
Iter: 1311 loss: 8.45260857e-08
Iter: 1312 loss: 8.44949923e-08
Iter: 1313 loss: 8.44828705e-08
Iter: 1314 loss: 8.44563957e-08
Iter: 1315 loss: 8.44583141e-08
Iter: 1316 loss: 8.44353352e-08
Iter: 1317 loss: 8.44414885e-08
Iter: 1318 loss: 8.44225454e-08
Iter: 1319 loss: 8.4384439e-08
Iter: 1320 loss: 8.44334807e-08
Iter: 1321 loss: 8.43632435e-08
Iter: 1322 loss: 8.43289882e-08
Iter: 1323 loss: 8.44884056e-08
Iter: 1324 loss: 8.43290451e-08
Iter: 1325 loss: 8.42903063e-08
Iter: 1326 loss: 8.4351754e-08
Iter: 1327 loss: 8.42826182e-08
Iter: 1328 loss: 8.42547223e-08
Iter: 1329 loss: 8.42632844e-08
Iter: 1330 loss: 8.42288159e-08
Iter: 1331 loss: 8.42002379e-08
Iter: 1332 loss: 8.45404742e-08
Iter: 1333 loss: 8.41941343e-08
Iter: 1334 loss: 8.41502299e-08
Iter: 1335 loss: 8.41033341e-08
Iter: 1336 loss: 8.40960723e-08
Iter: 1337 loss: 8.40481533e-08
Iter: 1338 loss: 8.408513e-08
Iter: 1339 loss: 8.40145162e-08
Iter: 1340 loss: 8.39646432e-08
Iter: 1341 loss: 8.43229e-08
Iter: 1342 loss: 8.39607139e-08
Iter: 1343 loss: 8.39091285e-08
Iter: 1344 loss: 8.39886525e-08
Iter: 1345 loss: 8.38847214e-08
Iter: 1346 loss: 8.38630427e-08
Iter: 1347 loss: 8.38567189e-08
Iter: 1348 loss: 8.38306846e-08
Iter: 1349 loss: 8.3839808e-08
Iter: 1350 loss: 8.38183709e-08
Iter: 1351 loss: 8.37863894e-08
Iter: 1352 loss: 8.39562375e-08
Iter: 1353 loss: 8.37811598e-08
Iter: 1354 loss: 8.3757584e-08
Iter: 1355 loss: 8.37821901e-08
Iter: 1356 loss: 8.37502228e-08
Iter: 1357 loss: 8.37188e-08
Iter: 1358 loss: 8.37058849e-08
Iter: 1359 loss: 8.36878868e-08
Iter: 1360 loss: 8.36430729e-08
Iter: 1361 loss: 8.36799074e-08
Iter: 1362 loss: 8.36117238e-08
Iter: 1363 loss: 8.35779304e-08
Iter: 1364 loss: 8.40773566e-08
Iter: 1365 loss: 8.35759764e-08
Iter: 1366 loss: 8.35442364e-08
Iter: 1367 loss: 8.37504643e-08
Iter: 1368 loss: 8.3542794e-08
Iter: 1369 loss: 8.35288745e-08
Iter: 1370 loss: 8.3513811e-08
Iter: 1371 loss: 8.3507345e-08
Iter: 1372 loss: 8.34855172e-08
Iter: 1373 loss: 8.36915675e-08
Iter: 1374 loss: 8.34863627e-08
Iter: 1375 loss: 8.34577065e-08
Iter: 1376 loss: 8.34956921e-08
Iter: 1377 loss: 8.34459755e-08
Iter: 1378 loss: 8.34231813e-08
Iter: 1379 loss: 8.35120062e-08
Iter: 1380 loss: 8.34172482e-08
Iter: 1381 loss: 8.33887128e-08
Iter: 1382 loss: 8.34001384e-08
Iter: 1383 loss: 8.33732443e-08
Iter: 1384 loss: 8.33239326e-08
Iter: 1385 loss: 8.34806286e-08
Iter: 1386 loss: 8.33105247e-08
Iter: 1387 loss: 8.32720701e-08
Iter: 1388 loss: 8.33133811e-08
Iter: 1389 loss: 8.32469e-08
Iter: 1390 loss: 8.31885671e-08
Iter: 1391 loss: 8.33497253e-08
Iter: 1392 loss: 8.31723526e-08
Iter: 1393 loss: 8.3132079e-08
Iter: 1394 loss: 8.31805167e-08
Iter: 1395 loss: 8.31117e-08
Iter: 1396 loss: 8.30869595e-08
Iter: 1397 loss: 8.32297786e-08
Iter: 1398 loss: 8.30803302e-08
Iter: 1399 loss: 8.30513898e-08
Iter: 1400 loss: 8.33219e-08
Iter: 1401 loss: 8.30535214e-08
Iter: 1402 loss: 8.30406535e-08
Iter: 1403 loss: 8.30013036e-08
Iter: 1404 loss: 8.37079099e-08
Iter: 1405 loss: 8.29995628e-08
Iter: 1406 loss: 8.29745517e-08
Iter: 1407 loss: 8.32106792e-08
Iter: 1408 loss: 8.29765696e-08
Iter: 1409 loss: 8.29465421e-08
Iter: 1410 loss: 8.30268618e-08
Iter: 1411 loss: 8.29420514e-08
Iter: 1412 loss: 8.2915669e-08
Iter: 1413 loss: 8.3050729e-08
Iter: 1414 loss: 8.29103612e-08
Iter: 1415 loss: 8.28874818e-08
Iter: 1416 loss: 8.29329849e-08
Iter: 1417 loss: 8.28775271e-08
Iter: 1418 loss: 8.2866876e-08
Iter: 1419 loss: 8.29287572e-08
Iter: 1420 loss: 8.28613338e-08
Iter: 1421 loss: 8.28436e-08
Iter: 1422 loss: 8.28495814e-08
Iter: 1423 loss: 8.28220763e-08
Iter: 1424 loss: 8.28033677e-08
Iter: 1425 loss: 8.28977136e-08
Iter: 1426 loss: 8.28015e-08
Iter: 1427 loss: 8.27770421e-08
Iter: 1428 loss: 8.27387652e-08
Iter: 1429 loss: 8.274435e-08
Iter: 1430 loss: 8.26926225e-08
Iter: 1431 loss: 8.26979303e-08
Iter: 1432 loss: 8.26423729e-08
Iter: 1433 loss: 8.25969835e-08
Iter: 1434 loss: 8.25941413e-08
Iter: 1435 loss: 8.25498461e-08
Iter: 1436 loss: 8.24998523e-08
Iter: 1437 loss: 8.2494708e-08
Iter: 1438 loss: 8.24528215e-08
Iter: 1439 loss: 8.27444353e-08
Iter: 1440 loss: 8.24444726e-08
Iter: 1441 loss: 8.24101107e-08
Iter: 1442 loss: 8.26079e-08
Iter: 1443 loss: 8.24070483e-08
Iter: 1444 loss: 8.23817885e-08
Iter: 1445 loss: 8.24785715e-08
Iter: 1446 loss: 8.23756068e-08
Iter: 1447 loss: 8.23445134e-08
Iter: 1448 loss: 8.24082633e-08
Iter: 1449 loss: 8.23341963e-08
Iter: 1450 loss: 8.23060802e-08
Iter: 1451 loss: 8.23770705e-08
Iter: 1452 loss: 8.22995361e-08
Iter: 1453 loss: 8.2271427e-08
Iter: 1454 loss: 8.23261672e-08
Iter: 1455 loss: 8.22572588e-08
Iter: 1456 loss: 8.22235506e-08
Iter: 1457 loss: 8.23665687e-08
Iter: 1458 loss: 8.22211206e-08
Iter: 1459 loss: 8.21850392e-08
Iter: 1460 loss: 8.21579533e-08
Iter: 1461 loss: 8.2155708e-08
Iter: 1462 loss: 8.2102531e-08
Iter: 1463 loss: 8.21816428e-08
Iter: 1464 loss: 8.20836163e-08
Iter: 1465 loss: 8.20610779e-08
Iter: 1466 loss: 8.20604242e-08
Iter: 1467 loss: 8.20281798e-08
Iter: 1468 loss: 8.19856609e-08
Iter: 1469 loss: 8.19866131e-08
Iter: 1470 loss: 8.19270696e-08
Iter: 1471 loss: 8.20160722e-08
Iter: 1472 loss: 8.19110824e-08
Iter: 1473 loss: 8.18432042e-08
Iter: 1474 loss: 8.21945036e-08
Iter: 1475 loss: 8.18370438e-08
Iter: 1476 loss: 8.17853447e-08
Iter: 1477 loss: 8.18892687e-08
Iter: 1478 loss: 8.17706507e-08
Iter: 1479 loss: 8.17206356e-08
Iter: 1480 loss: 8.2160625e-08
Iter: 1481 loss: 8.17121517e-08
Iter: 1482 loss: 8.16838792e-08
Iter: 1483 loss: 8.17258723e-08
Iter: 1484 loss: 8.16717645e-08
Iter: 1485 loss: 8.16418648e-08
Iter: 1486 loss: 8.17418737e-08
Iter: 1487 loss: 8.16332744e-08
Iter: 1488 loss: 8.16081211e-08
Iter: 1489 loss: 8.17551182e-08
Iter: 1490 loss: 8.16036589e-08
Iter: 1491 loss: 8.15787331e-08
Iter: 1492 loss: 8.15815895e-08
Iter: 1493 loss: 8.15646786e-08
Iter: 1494 loss: 8.15345089e-08
Iter: 1495 loss: 8.15083823e-08
Iter: 1496 loss: 8.15003887e-08
Iter: 1497 loss: 8.1444e-08
Iter: 1498 loss: 8.20582358e-08
Iter: 1499 loss: 8.14427494e-08
Iter: 1500 loss: 8.13915477e-08
Iter: 1501 loss: 8.13562053e-08
Iter: 1502 loss: 8.13398415e-08
Iter: 1503 loss: 8.12616747e-08
Iter: 1504 loss: 8.13577898e-08
Iter: 1505 loss: 8.12260268e-08
Iter: 1506 loss: 8.11546457e-08
Iter: 1507 loss: 8.16523453e-08
Iter: 1508 loss: 8.11519385e-08
Iter: 1509 loss: 8.11018381e-08
Iter: 1510 loss: 8.12394205e-08
Iter: 1511 loss: 8.10795342e-08
Iter: 1512 loss: 8.10391043e-08
Iter: 1513 loss: 8.14891905e-08
Iter: 1514 loss: 8.1037129e-08
Iter: 1515 loss: 8.1006867e-08
Iter: 1516 loss: 8.09863536e-08
Iter: 1517 loss: 8.09818133e-08
Iter: 1518 loss: 8.09171e-08
Iter: 1519 loss: 8.10839964e-08
Iter: 1520 loss: 8.08965126e-08
Iter: 1521 loss: 8.08406497e-08
Iter: 1522 loss: 8.1067931e-08
Iter: 1523 loss: 8.08258136e-08
Iter: 1524 loss: 8.07749103e-08
Iter: 1525 loss: 8.08044618e-08
Iter: 1526 loss: 8.07376637e-08
Iter: 1527 loss: 8.06846145e-08
Iter: 1528 loss: 8.06829235e-08
Iter: 1529 loss: 8.06356724e-08
Iter: 1530 loss: 8.05961378e-08
Iter: 1531 loss: 8.0601211e-08
Iter: 1532 loss: 8.05567808e-08
Iter: 1533 loss: 8.06775091e-08
Iter: 1534 loss: 8.05457887e-08
Iter: 1535 loss: 8.05329563e-08
Iter: 1536 loss: 8.05001505e-08
Iter: 1537 loss: 8.04949067e-08
Iter: 1538 loss: 8.04515494e-08
Iter: 1539 loss: 8.05700466e-08
Iter: 1540 loss: 8.04339209e-08
Iter: 1541 loss: 8.0384126e-08
Iter: 1542 loss: 8.0493777e-08
Iter: 1543 loss: 8.03664406e-08
Iter: 1544 loss: 8.03159423e-08
Iter: 1545 loss: 8.08330896e-08
Iter: 1546 loss: 8.03150044e-08
Iter: 1547 loss: 8.02693307e-08
Iter: 1548 loss: 8.02669362e-08
Iter: 1549 loss: 8.02350399e-08
Iter: 1550 loss: 8.01839235e-08
Iter: 1551 loss: 8.05546136e-08
Iter: 1552 loss: 8.01785376e-08
Iter: 1553 loss: 8.01484674e-08
Iter: 1554 loss: 8.02572657e-08
Iter: 1555 loss: 8.01403e-08
Iter: 1556 loss: 8.01182e-08
Iter: 1557 loss: 8.01699542e-08
Iter: 1558 loss: 8.01059059e-08
Iter: 1559 loss: 8.00846323e-08
Iter: 1560 loss: 8.00787205e-08
Iter: 1561 loss: 8.00698743e-08
Iter: 1562 loss: 8.00442876e-08
Iter: 1563 loss: 8.02428772e-08
Iter: 1564 loss: 8.00440105e-08
Iter: 1565 loss: 8.00125e-08
Iter: 1566 loss: 8.01374e-08
Iter: 1567 loss: 8.00132796e-08
Iter: 1568 loss: 7.99978181e-08
Iter: 1569 loss: 7.99758411e-08
Iter: 1570 loss: 7.99777311e-08
Iter: 1571 loss: 7.99469149e-08
Iter: 1572 loss: 8.0106517e-08
Iter: 1573 loss: 7.9948876e-08
Iter: 1574 loss: 7.99299e-08
Iter: 1575 loss: 7.99923612e-08
Iter: 1576 loss: 7.99247e-08
Iter: 1577 loss: 7.99029891e-08
Iter: 1578 loss: 8.00307305e-08
Iter: 1579 loss: 7.98987685e-08
Iter: 1580 loss: 7.98834066e-08
Iter: 1581 loss: 7.98903e-08
Iter: 1582 loss: 7.98663677e-08
Iter: 1583 loss: 7.9846771e-08
Iter: 1584 loss: 7.99202269e-08
Iter: 1585 loss: 7.98459325e-08
Iter: 1586 loss: 7.98138728e-08
Iter: 1587 loss: 7.98207722e-08
Iter: 1588 loss: 7.97891815e-08
Iter: 1589 loss: 7.97481903e-08
Iter: 1590 loss: 7.98620619e-08
Iter: 1591 loss: 7.97304551e-08
Iter: 1592 loss: 7.96877373e-08
Iter: 1593 loss: 7.9629018e-08
Iter: 1594 loss: 7.96281796e-08
Iter: 1595 loss: 7.95541268e-08
Iter: 1596 loss: 7.98147255e-08
Iter: 1597 loss: 7.95359085e-08
Iter: 1598 loss: 7.95377915e-08
Iter: 1599 loss: 7.95146136e-08
Iter: 1600 loss: 7.94960329e-08
Iter: 1601 loss: 7.9481e-08
Iter: 1602 loss: 7.94788946e-08
Iter: 1603 loss: 7.94615431e-08
Iter: 1604 loss: 7.95152246e-08
Iter: 1605 loss: 7.94481849e-08
Iter: 1606 loss: 7.94326951e-08
Iter: 1607 loss: 7.94976245e-08
Iter: 1608 loss: 7.94227475e-08
Iter: 1609 loss: 7.94006922e-08
Iter: 1610 loss: 7.94711568e-08
Iter: 1611 loss: 7.93914836e-08
Iter: 1612 loss: 7.93674246e-08
Iter: 1613 loss: 7.94265418e-08
Iter: 1614 loss: 7.93543933e-08
Iter: 1615 loss: 7.93313291e-08
Iter: 1616 loss: 7.94182284e-08
Iter: 1617 loss: 7.93269663e-08
Iter: 1618 loss: 7.92992267e-08
Iter: 1619 loss: 7.93407153e-08
Iter: 1620 loss: 7.92839145e-08
Iter: 1621 loss: 7.92607e-08
Iter: 1622 loss: 7.94452504e-08
Iter: 1623 loss: 7.92629962e-08
Iter: 1624 loss: 7.92457229e-08
Iter: 1625 loss: 7.92066e-08
Iter: 1626 loss: 7.98103912e-08
Iter: 1627 loss: 7.92030832e-08
Iter: 1628 loss: 7.9154816e-08
Iter: 1629 loss: 7.91761465e-08
Iter: 1630 loss: 7.91211789e-08
Iter: 1631 loss: 7.90913646e-08
Iter: 1632 loss: 7.9081758e-08
Iter: 1633 loss: 7.9049812e-08
Iter: 1634 loss: 7.90014e-08
Iter: 1635 loss: 7.89983687e-08
Iter: 1636 loss: 7.89625503e-08
Iter: 1637 loss: 7.91925885e-08
Iter: 1638 loss: 7.89525458e-08
Iter: 1639 loss: 7.89219712e-08
Iter: 1640 loss: 7.91333221e-08
Iter: 1641 loss: 7.89213317e-08
Iter: 1642 loss: 7.89090393e-08
Iter: 1643 loss: 7.89331125e-08
Iter: 1644 loss: 7.88988359e-08
Iter: 1645 loss: 7.88779815e-08
Iter: 1646 loss: 7.90494568e-08
Iter: 1647 loss: 7.88771359e-08
Iter: 1648 loss: 7.88674583e-08
Iter: 1649 loss: 7.88637635e-08
Iter: 1650 loss: 7.88544199e-08
Iter: 1651 loss: 7.8823966e-08
Iter: 1652 loss: 7.88676573e-08
Iter: 1653 loss: 7.8823021e-08
Iter: 1654 loss: 7.87867194e-08
Iter: 1655 loss: 7.88644e-08
Iter: 1656 loss: 7.87784842e-08
Iter: 1657 loss: 7.87279859e-08
Iter: 1658 loss: 7.86603778e-08
Iter: 1659 loss: 7.86584e-08
Iter: 1660 loss: 7.85922722e-08
Iter: 1661 loss: 7.87185428e-08
Iter: 1662 loss: 7.85608e-08
Iter: 1663 loss: 7.85144181e-08
Iter: 1664 loss: 7.92173296e-08
Iter: 1665 loss: 7.85144181e-08
Iter: 1666 loss: 7.84721266e-08
Iter: 1667 loss: 7.86247298e-08
Iter: 1668 loss: 7.84512366e-08
Iter: 1669 loss: 7.84328904e-08
Iter: 1670 loss: 7.8479971e-08
Iter: 1671 loss: 7.84231844e-08
Iter: 1672 loss: 7.8403005e-08
Iter: 1673 loss: 7.84623282e-08
Iter: 1674 loss: 7.83942937e-08
Iter: 1675 loss: 7.83662841e-08
Iter: 1676 loss: 7.83782141e-08
Iter: 1677 loss: 7.83501619e-08
Iter: 1678 loss: 7.83231684e-08
Iter: 1679 loss: 7.83256411e-08
Iter: 1680 loss: 7.82972194e-08
Iter: 1681 loss: 7.82875773e-08
Iter: 1682 loss: 7.82782479e-08
Iter: 1683 loss: 7.82434881e-08
Iter: 1684 loss: 7.84217704e-08
Iter: 1685 loss: 7.82395801e-08
Iter: 1686 loss: 7.82215892e-08
Iter: 1687 loss: 7.83268206e-08
Iter: 1688 loss: 7.82141e-08
Iter: 1689 loss: 7.81986103e-08
Iter: 1690 loss: 7.82137377e-08
Iter: 1691 loss: 7.81849678e-08
Iter: 1692 loss: 7.81656126e-08
Iter: 1693 loss: 7.81391378e-08
Iter: 1694 loss: 7.81376173e-08
Iter: 1695 loss: 7.80959297e-08
Iter: 1696 loss: 7.82637102e-08
Iter: 1697 loss: 7.80892577e-08
Iter: 1698 loss: 7.80434064e-08
Iter: 1699 loss: 7.83517109e-08
Iter: 1700 loss: 7.80377931e-08
Iter: 1701 loss: 7.80127962e-08
Iter: 1702 loss: 7.79734961e-08
Iter: 1703 loss: 7.79669e-08
Iter: 1704 loss: 7.79117357e-08
Iter: 1705 loss: 7.83500482e-08
Iter: 1706 loss: 7.78978873e-08
Iter: 1707 loss: 7.78620191e-08
Iter: 1708 loss: 7.79949119e-08
Iter: 1709 loss: 7.78454e-08
Iter: 1710 loss: 7.78194504e-08
Iter: 1711 loss: 7.78212197e-08
Iter: 1712 loss: 7.78054e-08
Iter: 1713 loss: 7.77907e-08
Iter: 1714 loss: 7.77885774e-08
Iter: 1715 loss: 7.7759978e-08
Iter: 1716 loss: 7.7919843e-08
Iter: 1717 loss: 7.77611433e-08
Iter: 1718 loss: 7.77428752e-08
Iter: 1719 loss: 7.77744589e-08
Iter: 1720 loss: 7.77323237e-08
Iter: 1721 loss: 7.7704442e-08
Iter: 1722 loss: 7.77596938e-08
Iter: 1723 loss: 7.76927322e-08
Iter: 1724 loss: 7.76666909e-08
Iter: 1725 loss: 7.76002906e-08
Iter: 1726 loss: 7.84587542e-08
Iter: 1727 loss: 7.75914e-08
Iter: 1728 loss: 7.75015465e-08
Iter: 1729 loss: 7.78555318e-08
Iter: 1730 loss: 7.74884512e-08
Iter: 1731 loss: 7.7444696e-08
Iter: 1732 loss: 7.7430812e-08
Iter: 1733 loss: 7.74043443e-08
Iter: 1734 loss: 7.73466837e-08
Iter: 1735 loss: 7.82960043e-08
Iter: 1736 loss: 7.73403528e-08
Iter: 1737 loss: 7.72941888e-08
Iter: 1738 loss: 7.79883891e-08
Iter: 1739 loss: 7.72953896e-08
Iter: 1740 loss: 7.72647226e-08
Iter: 1741 loss: 7.73377806e-08
Iter: 1742 loss: 7.72537e-08
Iter: 1743 loss: 7.72194397e-08
Iter: 1744 loss: 7.72548105e-08
Iter: 1745 loss: 7.7204e-08
Iter: 1746 loss: 7.71524213e-08
Iter: 1747 loss: 7.72786706e-08
Iter: 1748 loss: 7.71370878e-08
Iter: 1749 loss: 7.71082256e-08
Iter: 1750 loss: 7.72007667e-08
Iter: 1751 loss: 7.70934108e-08
Iter: 1752 loss: 7.70610242e-08
Iter: 1753 loss: 7.71720323e-08
Iter: 1754 loss: 7.7056086e-08
Iter: 1755 loss: 7.70244242e-08
Iter: 1756 loss: 7.72031825e-08
Iter: 1757 loss: 7.70256037e-08
Iter: 1758 loss: 7.70106467e-08
Iter: 1759 loss: 7.69921158e-08
Iter: 1760 loss: 7.69871278e-08
Iter: 1761 loss: 7.69632749e-08
Iter: 1762 loss: 7.6966586e-08
Iter: 1763 loss: 7.69428254e-08
Iter: 1764 loss: 7.69170612e-08
Iter: 1765 loss: 7.69206139e-08
Iter: 1766 loss: 7.6894878e-08
Iter: 1767 loss: 7.68343398e-08
Iter: 1768 loss: 7.79658293e-08
Iter: 1769 loss: 7.68369262e-08
Iter: 1770 loss: 7.67930075e-08
Iter: 1771 loss: 7.71214275e-08
Iter: 1772 loss: 7.67844455e-08
Iter: 1773 loss: 7.67435537e-08
Iter: 1774 loss: 7.68149e-08
Iter: 1775 loss: 7.67246604e-08
Iter: 1776 loss: 7.66824115e-08
Iter: 1777 loss: 7.67911104e-08
Iter: 1778 loss: 7.66656854e-08
Iter: 1779 loss: 7.66388411e-08
Iter: 1780 loss: 7.66395516e-08
Iter: 1781 loss: 7.66260726e-08
Iter: 1782 loss: 7.66250778e-08
Iter: 1783 loss: 7.66187753e-08
Iter: 1784 loss: 7.65940911e-08
Iter: 1785 loss: 7.67203261e-08
Iter: 1786 loss: 7.65934871e-08
Iter: 1787 loss: 7.65813368e-08
Iter: 1788 loss: 7.66296395e-08
Iter: 1789 loss: 7.65791057e-08
Iter: 1790 loss: 7.65653425e-08
Iter: 1791 loss: 7.65707213e-08
Iter: 1792 loss: 7.6546705e-08
Iter: 1793 loss: 7.65332686e-08
Iter: 1794 loss: 7.65122152e-08
Iter: 1795 loss: 7.65119808e-08
Iter: 1796 loss: 7.65012658e-08
Iter: 1797 loss: 7.64976562e-08
Iter: 1798 loss: 7.64785852e-08
Iter: 1799 loss: 7.64860886e-08
Iter: 1800 loss: 7.64708687e-08
Iter: 1801 loss: 7.64538584e-08
Iter: 1802 loss: 7.64444792e-08
Iter: 1803 loss: 7.6437054e-08
Iter: 1804 loss: 7.64120855e-08
Iter: 1805 loss: 7.65027e-08
Iter: 1806 loss: 7.64065717e-08
Iter: 1807 loss: 7.63853336e-08
Iter: 1808 loss: 7.63630936e-08
Iter: 1809 loss: 7.63587451e-08
Iter: 1810 loss: 7.63114869e-08
Iter: 1811 loss: 7.6546371e-08
Iter: 1812 loss: 7.63038628e-08
Iter: 1813 loss: 7.625e-08
Iter: 1814 loss: 7.64148069e-08
Iter: 1815 loss: 7.6226776e-08
Iter: 1816 loss: 7.6202241e-08
Iter: 1817 loss: 7.63426939e-08
Iter: 1818 loss: 7.61980488e-08
Iter: 1819 loss: 7.61703802e-08
Iter: 1820 loss: 7.62438503e-08
Iter: 1821 loss: 7.61625e-08
Iter: 1822 loss: 7.61472734e-08
Iter: 1823 loss: 7.63299255e-08
Iter: 1824 loss: 7.61472592e-08
Iter: 1825 loss: 7.61371695e-08
Iter: 1826 loss: 7.61194059e-08
Iter: 1827 loss: 7.61133236e-08
Iter: 1828 loss: 7.61104673e-08
Iter: 1829 loss: 7.60961072e-08
Iter: 1830 loss: 7.60951053e-08
Iter: 1831 loss: 7.6076212e-08
Iter: 1832 loss: 7.60762546e-08
Iter: 1833 loss: 7.60632943e-08
Iter: 1834 loss: 7.60628893e-08
Iter: 1835 loss: 7.60527286e-08
Iter: 1836 loss: 7.60339276e-08
Iter: 1837 loss: 7.60571268e-08
Iter: 1838 loss: 7.60234329e-08
Iter: 1839 loss: 7.59960699e-08
Iter: 1840 loss: 7.59862431e-08
Iter: 1841 loss: 7.59743202e-08
Iter: 1842 loss: 7.59299681e-08
Iter: 1843 loss: 7.59746683e-08
Iter: 1844 loss: 7.59062431e-08
Iter: 1845 loss: 7.59003242e-08
Iter: 1846 loss: 7.58851826e-08
Iter: 1847 loss: 7.58715e-08
Iter: 1848 loss: 7.5849087e-08
Iter: 1849 loss: 7.58517515e-08
Iter: 1850 loss: 7.58300303e-08
Iter: 1851 loss: 7.60483942e-08
Iter: 1852 loss: 7.58284102e-08
Iter: 1853 loss: 7.58090479e-08
Iter: 1854 loss: 7.58111298e-08
Iter: 1855 loss: 7.58044933e-08
Iter: 1856 loss: 7.5781287e-08
Iter: 1857 loss: 7.59495862e-08
Iter: 1858 loss: 7.57823244e-08
Iter: 1859 loss: 7.57712471e-08
Iter: 1860 loss: 7.5735592e-08
Iter: 1861 loss: 7.59828538e-08
Iter: 1862 loss: 7.57279963e-08
Iter: 1863 loss: 7.56981535e-08
Iter: 1864 loss: 7.5697983e-08
Iter: 1865 loss: 7.56738672e-08
Iter: 1866 loss: 7.56429586e-08
Iter: 1867 loss: 7.56360805e-08
Iter: 1868 loss: 7.56191554e-08
Iter: 1869 loss: 7.56170309e-08
Iter: 1870 loss: 7.56000631e-08
Iter: 1871 loss: 7.55875e-08
Iter: 1872 loss: 7.5586243e-08
Iter: 1873 loss: 7.55583471e-08
Iter: 1874 loss: 7.55599174e-08
Iter: 1875 loss: 7.55360432e-08
Iter: 1876 loss: 7.55191e-08
Iter: 1877 loss: 7.55673923e-08
Iter: 1878 loss: 7.55114087e-08
Iter: 1879 loss: 7.54884724e-08
Iter: 1880 loss: 7.55394751e-08
Iter: 1881 loss: 7.54753415e-08
Iter: 1882 loss: 7.54517799e-08
Iter: 1883 loss: 7.55283907e-08
Iter: 1884 loss: 7.54482841e-08
Iter: 1885 loss: 7.54151799e-08
Iter: 1886 loss: 7.54911582e-08
Iter: 1887 loss: 7.54027099e-08
Iter: 1888 loss: 7.53859197e-08
Iter: 1889 loss: 7.53854863e-08
Iter: 1890 loss: 7.53829568e-08
Iter: 1891 loss: 7.53933378e-08
Iter: 1892 loss: 7.53797877e-08
Iter: 1893 loss: 7.53613278e-08
Iter: 1894 loss: 7.53887122e-08
Iter: 1895 loss: 7.53593099e-08
Iter: 1896 loss: 7.53449e-08
Iter: 1897 loss: 7.53713394e-08
Iter: 1898 loss: 7.53358123e-08
Iter: 1899 loss: 7.53189724e-08
Iter: 1900 loss: 7.53449143e-08
Iter: 1901 loss: 7.53175e-08
Iter: 1902 loss: 7.52999867e-08
Iter: 1903 loss: 7.53436211e-08
Iter: 1904 loss: 7.52909344e-08
Iter: 1905 loss: 7.52720766e-08
Iter: 1906 loss: 7.52414451e-08
Iter: 1907 loss: 7.57265468e-08
Iter: 1908 loss: 7.52335438e-08
Iter: 1909 loss: 7.52163771e-08
Iter: 1910 loss: 7.52053921e-08
Iter: 1911 loss: 7.51850493e-08
Iter: 1912 loss: 7.52611271e-08
Iter: 1913 loss: 7.51799263e-08
Iter: 1914 loss: 7.51635909e-08
Iter: 1915 loss: 7.51877209e-08
Iter: 1916 loss: 7.51536504e-08
Iter: 1917 loss: 7.5139269e-08
Iter: 1918 loss: 7.51314886e-08
Iter: 1919 loss: 7.51260316e-08
Iter: 1920 loss: 7.51049498e-08
Iter: 1921 loss: 7.51402212e-08
Iter: 1922 loss: 7.50922737e-08
Iter: 1923 loss: 7.50798463e-08
Iter: 1924 loss: 7.52751532e-08
Iter: 1925 loss: 7.50787734e-08
Iter: 1926 loss: 7.50632267e-08
Iter: 1927 loss: 7.50454774e-08
Iter: 1928 loss: 7.50422515e-08
Iter: 1929 loss: 7.50138156e-08
Iter: 1930 loss: 7.50546576e-08
Iter: 1931 loss: 7.50004858e-08
Iter: 1932 loss: 7.50063e-08
Iter: 1933 loss: 7.49913482e-08
Iter: 1934 loss: 7.49896145e-08
Iter: 1935 loss: 7.49595e-08
Iter: 1936 loss: 7.51347429e-08
Iter: 1937 loss: 7.49529221e-08
Iter: 1938 loss: 7.49447e-08
Iter: 1939 loss: 7.49458238e-08
Iter: 1940 loss: 7.49280815e-08
Iter: 1941 loss: 7.49187166e-08
Iter: 1942 loss: 7.49071916e-08
Iter: 1943 loss: 7.48917444e-08
Iter: 1944 loss: 7.48785212e-08
Iter: 1945 loss: 7.48769295e-08
Iter: 1946 loss: 7.48532756e-08
Iter: 1947 loss: 7.50103339e-08
Iter: 1948 loss: 7.48516555e-08
Iter: 1949 loss: 7.48371605e-08
Iter: 1950 loss: 7.4973e-08
Iter: 1951 loss: 7.48396047e-08
Iter: 1952 loss: 7.482965e-08
Iter: 1953 loss: 7.48352278e-08
Iter: 1954 loss: 7.48233475e-08
Iter: 1955 loss: 7.48196243e-08
Iter: 1956 loss: 7.48092077e-08
Iter: 1957 loss: 7.48110551e-08
Iter: 1958 loss: 7.47980096e-08
Iter: 1959 loss: 7.47996651e-08
Iter: 1960 loss: 7.47871098e-08
Iter: 1961 loss: 7.47695807e-08
Iter: 1962 loss: 7.50042801e-08
Iter: 1963 loss: 7.47678115e-08
Iter: 1964 loss: 7.4753828e-08
Iter: 1965 loss: 7.4753693e-08
Iter: 1966 loss: 7.47409672e-08
Iter: 1967 loss: 7.47356e-08
Iter: 1968 loss: 7.47235163e-08
Iter: 1969 loss: 7.47123e-08
Iter: 1970 loss: 7.47419193e-08
Iter: 1971 loss: 7.47065201e-08
Iter: 1972 loss: 7.46855378e-08
Iter: 1973 loss: 7.47585e-08
Iter: 1974 loss: 7.46837e-08
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4
+ date
Tue Oct 20 16:38:54 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f44f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f408a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f4639d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f3f8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f50a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f3bd730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f38e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f33f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f3db1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f38e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f3db9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f2c12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f2c1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53383f9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52ef5f3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52ef5a6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52ef5b9ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f314e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f532f314f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52ef4ca840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52ef4eca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c842e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52ef4ec9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c84289d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c84288c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c8428bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c8428950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c83a0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c83a0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c83367b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c8367268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c82b9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c82b9730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c830dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c82980d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52c822ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.92927598e-06
Iter: 2 loss: 2.57956435e-06
Iter: 3 loss: 1.47750484e-06
Iter: 4 loss: 1.14851082e-06
Iter: 5 loss: 1.0865715e-06
Iter: 6 loss: 8.65744823e-07
Iter: 7 loss: 8.07002152e-07
Iter: 8 loss: 7.72701412e-07
Iter: 9 loss: 7.35391325e-07
Iter: 10 loss: 6.69731946e-07
Iter: 11 loss: 6.69731833e-07
Iter: 12 loss: 6.31177727e-07
Iter: 13 loss: 6.30395959e-07
Iter: 14 loss: 6.09358381e-07
Iter: 15 loss: 7.48503e-07
Iter: 16 loss: 6.07176219e-07
Iter: 17 loss: 5.97867484e-07
Iter: 18 loss: 5.84688451e-07
Iter: 19 loss: 5.84223471e-07
Iter: 20 loss: 5.74073397e-07
Iter: 21 loss: 5.73576358e-07
Iter: 22 loss: 5.62056e-07
Iter: 23 loss: 5.3486508e-07
Iter: 24 loss: 8.53389963e-07
Iter: 25 loss: 5.324722e-07
Iter: 26 loss: 5.06816605e-07
Iter: 27 loss: 5.39710129e-07
Iter: 28 loss: 4.93647576e-07
Iter: 29 loss: 4.76932826e-07
Iter: 30 loss: 6.1628333e-07
Iter: 31 loss: 4.75952703e-07
Iter: 32 loss: 4.67796781e-07
Iter: 33 loss: 5.29864451e-07
Iter: 34 loss: 4.67177074e-07
Iter: 35 loss: 4.63424271e-07
Iter: 36 loss: 4.74929465e-07
Iter: 37 loss: 4.62301244e-07
Iter: 38 loss: 4.61295201e-07
Iter: 39 loss: 4.60242433e-07
Iter: 40 loss: 4.58755835e-07
Iter: 41 loss: 4.54631675e-07
Iter: 42 loss: 4.76828973e-07
Iter: 43 loss: 4.53360371e-07
Iter: 44 loss: 4.52194655e-07
Iter: 45 loss: 4.5065093e-07
Iter: 46 loss: 4.49100696e-07
Iter: 47 loss: 4.44924e-07
Iter: 48 loss: 4.73354589e-07
Iter: 49 loss: 4.43935903e-07
Iter: 50 loss: 4.42254731e-07
Iter: 51 loss: 4.41331395e-07
Iter: 52 loss: 4.39256326e-07
Iter: 53 loss: 4.36233591e-07
Iter: 54 loss: 4.3614429e-07
Iter: 55 loss: 4.33954824e-07
Iter: 56 loss: 4.55328575e-07
Iter: 57 loss: 4.33884225e-07
Iter: 58 loss: 4.31725539e-07
Iter: 59 loss: 4.39247259e-07
Iter: 60 loss: 4.31158611e-07
Iter: 61 loss: 4.29594536e-07
Iter: 62 loss: 4.29105711e-07
Iter: 63 loss: 4.28177714e-07
Iter: 64 loss: 4.25950901e-07
Iter: 65 loss: 4.26737245e-07
Iter: 66 loss: 4.24392312e-07
Iter: 67 loss: 4.22282113e-07
Iter: 68 loss: 4.25570192e-07
Iter: 69 loss: 4.21296591e-07
Iter: 70 loss: 4.18947792e-07
Iter: 71 loss: 4.28827548e-07
Iter: 72 loss: 4.18448565e-07
Iter: 73 loss: 4.18586666e-07
Iter: 74 loss: 4.17475746e-07
Iter: 75 loss: 4.16855585e-07
Iter: 76 loss: 4.15155853e-07
Iter: 77 loss: 4.23748361e-07
Iter: 78 loss: 4.14583383e-07
Iter: 79 loss: 4.13333879e-07
Iter: 80 loss: 4.1313109e-07
Iter: 81 loss: 4.11975691e-07
Iter: 82 loss: 4.09082418e-07
Iter: 83 loss: 4.36553279e-07
Iter: 84 loss: 4.08692301e-07
Iter: 85 loss: 4.08174429e-07
Iter: 86 loss: 4.07486226e-07
Iter: 87 loss: 4.06383123e-07
Iter: 88 loss: 4.04887345e-07
Iter: 89 loss: 4.0480171e-07
Iter: 90 loss: 4.03752495e-07
Iter: 91 loss: 4.03734191e-07
Iter: 92 loss: 4.02762879e-07
Iter: 93 loss: 4.01361945e-07
Iter: 94 loss: 4.01326474e-07
Iter: 95 loss: 3.99945577e-07
Iter: 96 loss: 4.01862223e-07
Iter: 97 loss: 3.99256891e-07
Iter: 98 loss: 3.97546785e-07
Iter: 99 loss: 4.10997245e-07
Iter: 100 loss: 3.97437759e-07
Iter: 101 loss: 3.96021761e-07
Iter: 102 loss: 3.96806797e-07
Iter: 103 loss: 3.95080093e-07
Iter: 104 loss: 3.94049778e-07
Iter: 105 loss: 3.94047305e-07
Iter: 106 loss: 3.93006019e-07
Iter: 107 loss: 3.97784959e-07
Iter: 108 loss: 3.92816304e-07
Iter: 109 loss: 3.92265349e-07
Iter: 110 loss: 3.91476192e-07
Iter: 111 loss: 3.91469257e-07
Iter: 112 loss: 3.90455739e-07
Iter: 113 loss: 3.90451504e-07
Iter: 114 loss: 3.89959837e-07
Iter: 115 loss: 3.88930289e-07
Iter: 116 loss: 4.06431553e-07
Iter: 117 loss: 3.88886065e-07
Iter: 118 loss: 3.88514508e-07
Iter: 119 loss: 3.88313e-07
Iter: 120 loss: 3.87863395e-07
Iter: 121 loss: 3.8706446e-07
Iter: 122 loss: 3.87069917e-07
Iter: 123 loss: 3.8652189e-07
Iter: 124 loss: 3.86503018e-07
Iter: 125 loss: 3.86069416e-07
Iter: 126 loss: 3.85199428e-07
Iter: 127 loss: 3.99042392e-07
Iter: 128 loss: 3.85161144e-07
Iter: 129 loss: 3.84423572e-07
Iter: 130 loss: 3.85208409e-07
Iter: 131 loss: 3.84025753e-07
Iter: 132 loss: 3.83107135e-07
Iter: 133 loss: 3.84135774e-07
Iter: 134 loss: 3.82636671e-07
Iter: 135 loss: 3.81689517e-07
Iter: 136 loss: 3.84942467e-07
Iter: 137 loss: 3.8145339e-07
Iter: 138 loss: 3.80931453e-07
Iter: 139 loss: 3.87639943e-07
Iter: 140 loss: 3.80923808e-07
Iter: 141 loss: 3.80308e-07
Iter: 142 loss: 3.81852942e-07
Iter: 143 loss: 3.80103e-07
Iter: 144 loss: 3.79792567e-07
Iter: 145 loss: 3.79796063e-07
Iter: 146 loss: 3.79527961e-07
Iter: 147 loss: 3.79056075e-07
Iter: 148 loss: 3.83543352e-07
Iter: 149 loss: 3.79051528e-07
Iter: 150 loss: 3.78805481e-07
Iter: 151 loss: 3.78216157e-07
Iter: 152 loss: 3.8329182e-07
Iter: 153 loss: 3.78126913e-07
Iter: 154 loss: 3.77820015e-07
Iter: 155 loss: 3.77677367e-07
Iter: 156 loss: 3.77382378e-07
Iter: 157 loss: 3.76688121e-07
Iter: 158 loss: 3.86918089e-07
Iter: 159 loss: 3.7665319e-07
Iter: 160 loss: 3.76541834e-07
Iter: 161 loss: 3.76331457e-07
Iter: 162 loss: 3.76082284e-07
Iter: 163 loss: 3.75666218e-07
Iter: 164 loss: 3.86234888e-07
Iter: 165 loss: 3.75658942e-07
Iter: 166 loss: 3.74884905e-07
Iter: 167 loss: 3.7500007e-07
Iter: 168 loss: 3.74321019e-07
Iter: 169 loss: 3.73679285e-07
Iter: 170 loss: 3.73684458e-07
Iter: 171 loss: 3.73193927e-07
Iter: 172 loss: 3.73888838e-07
Iter: 173 loss: 3.72958198e-07
Iter: 174 loss: 3.72716215e-07
Iter: 175 loss: 3.72682621e-07
Iter: 176 loss: 3.72424751e-07
Iter: 177 loss: 3.72043587e-07
Iter: 178 loss: 3.7204444e-07
Iter: 179 loss: 3.71767953e-07
Iter: 180 loss: 3.74445051e-07
Iter: 181 loss: 3.71767072e-07
Iter: 182 loss: 3.71477483e-07
Iter: 183 loss: 3.71475892e-07
Iter: 184 loss: 3.71224758e-07
Iter: 185 loss: 3.70884607e-07
Iter: 186 loss: 3.70069444e-07
Iter: 187 loss: 3.81686505e-07
Iter: 188 loss: 3.70035593e-07
Iter: 189 loss: 3.69041885e-07
Iter: 190 loss: 3.71935101e-07
Iter: 191 loss: 3.68734533e-07
Iter: 192 loss: 3.69118254e-07
Iter: 193 loss: 3.68406177e-07
Iter: 194 loss: 3.68162489e-07
Iter: 195 loss: 3.67679775e-07
Iter: 196 loss: 3.77639765e-07
Iter: 197 loss: 3.67681679e-07
Iter: 198 loss: 3.67491907e-07
Iter: 199 loss: 3.6748736e-07
Iter: 200 loss: 3.67269251e-07
Iter: 201 loss: 3.6712305e-07
Iter: 202 loss: 3.67043071e-07
Iter: 203 loss: 3.66824793e-07
Iter: 204 loss: 3.66670434e-07
Iter: 205 loss: 3.66608788e-07
Iter: 206 loss: 3.66326333e-07
Iter: 207 loss: 3.69041629e-07
Iter: 208 loss: 3.66318687e-07
Iter: 209 loss: 3.66054792e-07
Iter: 210 loss: 3.67819837e-07
Iter: 211 loss: 3.66020686e-07
Iter: 212 loss: 3.65770404e-07
Iter: 213 loss: 3.65540444e-07
Iter: 214 loss: 3.65495794e-07
Iter: 215 loss: 3.65275412e-07
Iter: 216 loss: 3.65270751e-07
Iter: 217 loss: 3.65059691e-07
Iter: 218 loss: 3.6491133e-07
Iter: 219 loss: 3.64838399e-07
Iter: 220 loss: 3.64631745e-07
Iter: 221 loss: 3.64488699e-07
Iter: 222 loss: 3.6439971e-07
Iter: 223 loss: 3.64051e-07
Iter: 224 loss: 3.64215737e-07
Iter: 225 loss: 3.63811267e-07
Iter: 226 loss: 3.63464409e-07
Iter: 227 loss: 3.66079178e-07
Iter: 228 loss: 3.63416063e-07
Iter: 229 loss: 3.63173342e-07
Iter: 230 loss: 3.65078904e-07
Iter: 231 loss: 3.63139094e-07
Iter: 232 loss: 3.6288435e-07
Iter: 233 loss: 3.64849711e-07
Iter: 234 loss: 3.62884e-07
Iter: 235 loss: 3.62747073e-07
Iter: 236 loss: 3.62500828e-07
Iter: 237 loss: 3.66979975e-07
Iter: 238 loss: 3.62490624e-07
Iter: 239 loss: 3.62367985e-07
Iter: 240 loss: 3.62322965e-07
Iter: 241 loss: 3.62176308e-07
Iter: 242 loss: 3.61769594e-07
Iter: 243 loss: 3.62995422e-07
Iter: 244 loss: 3.61556147e-07
Iter: 245 loss: 3.61111518e-07
Iter: 246 loss: 3.66891442e-07
Iter: 247 loss: 3.61123625e-07
Iter: 248 loss: 3.60769377e-07
Iter: 249 loss: 3.64678243e-07
Iter: 250 loss: 3.60741808e-07
Iter: 251 loss: 3.60655264e-07
Iter: 252 loss: 3.60439e-07
Iter: 253 loss: 3.64341872e-07
Iter: 254 loss: 3.60435251e-07
Iter: 255 loss: 3.60257445e-07
Iter: 256 loss: 3.60899719e-07
Iter: 257 loss: 3.60204552e-07
Iter: 258 loss: 3.60099e-07
Iter: 259 loss: 3.60079184e-07
Iter: 260 loss: 3.5999858e-07
Iter: 261 loss: 3.60053974e-07
Iter: 262 loss: 3.59939349e-07
Iter: 263 loss: 3.59800424e-07
Iter: 264 loss: 3.59522829e-07
Iter: 265 loss: 3.6389622e-07
Iter: 266 loss: 3.59513393e-07
Iter: 267 loss: 3.59210134e-07
Iter: 268 loss: 3.60688546e-07
Iter: 269 loss: 3.59161e-07
Iter: 270 loss: 3.59014109e-07
Iter: 271 loss: 3.58971363e-07
Iter: 272 loss: 3.58884165e-07
Iter: 273 loss: 3.58598982e-07
Iter: 274 loss: 3.60856205e-07
Iter: 275 loss: 3.58559191e-07
Iter: 276 loss: 3.58490297e-07
Iter: 277 loss: 3.58410489e-07
Iter: 278 loss: 3.58280602e-07
Iter: 279 loss: 3.58046435e-07
Iter: 280 loss: 3.63416177e-07
Iter: 281 loss: 3.58042939e-07
Iter: 282 loss: 3.57882868e-07
Iter: 283 loss: 3.58004797e-07
Iter: 284 loss: 3.57785524e-07
Iter: 285 loss: 3.57783165e-07
Iter: 286 loss: 3.57705972e-07
Iter: 287 loss: 3.57628636e-07
Iter: 288 loss: 3.57502813e-07
Iter: 289 loss: 3.60162034e-07
Iter: 290 loss: 3.57504746e-07
Iter: 291 loss: 3.57427581e-07
Iter: 292 loss: 3.58183712e-07
Iter: 293 loss: 3.57422493e-07
Iter: 294 loss: 3.57344817e-07
Iter: 295 loss: 3.57341918e-07
Iter: 296 loss: 3.57269386e-07
Iter: 297 loss: 3.57150554e-07
Iter: 298 loss: 3.5710525e-07
Iter: 299 loss: 3.57046673e-07
Iter: 300 loss: 3.56855622e-07
Iter: 301 loss: 3.56824557e-07
Iter: 302 loss: 3.56684751e-07
Iter: 303 loss: 3.5658951e-07
Iter: 304 loss: 3.56542557e-07
Iter: 305 loss: 3.56426483e-07
Iter: 306 loss: 3.56218067e-07
Iter: 307 loss: 3.56217186e-07
Iter: 308 loss: 3.56049497e-07
Iter: 309 loss: 3.57809824e-07
Iter: 310 loss: 3.56031819e-07
Iter: 311 loss: 3.558728e-07
Iter: 312 loss: 3.57091665e-07
Iter: 313 loss: 3.55862369e-07
Iter: 314 loss: 3.55801319e-07
Iter: 315 loss: 3.55698887e-07
Iter: 316 loss: 3.57453644e-07
Iter: 317 loss: 3.55685728e-07
Iter: 318 loss: 3.55629197e-07
Iter: 319 loss: 3.55616436e-07
Iter: 320 loss: 3.5553154e-07
Iter: 321 loss: 3.55477312e-07
Iter: 322 loss: 3.55452613e-07
Iter: 323 loss: 3.55357798e-07
Iter: 324 loss: 3.55350664e-07
Iter: 325 loss: 3.55287455e-07
Iter: 326 loss: 3.55154071e-07
Iter: 327 loss: 3.56555688e-07
Iter: 328 loss: 3.5514708e-07
Iter: 329 loss: 3.55069e-07
Iter: 330 loss: 3.54923657e-07
Iter: 331 loss: 3.5829521e-07
Iter: 332 loss: 3.54923372e-07
Iter: 333 loss: 3.54752444e-07
Iter: 334 loss: 3.55544671e-07
Iter: 335 loss: 3.54721692e-07
Iter: 336 loss: 3.545494e-07
Iter: 337 loss: 3.54734482e-07
Iter: 338 loss: 3.54456091e-07
Iter: 339 loss: 3.54335327e-07
Iter: 340 loss: 3.54329046e-07
Iter: 341 loss: 3.54246538e-07
Iter: 342 loss: 3.54052673e-07
Iter: 343 loss: 3.56582774e-07
Iter: 344 loss: 3.54037439e-07
Iter: 345 loss: 3.5391588e-07
Iter: 346 loss: 3.53897917e-07
Iter: 347 loss: 3.53799862e-07
Iter: 348 loss: 3.54021211e-07
Iter: 349 loss: 3.5375723e-07
Iter: 350 loss: 3.53682651e-07
Iter: 351 loss: 3.5376712e-07
Iter: 352 loss: 3.53638313e-07
Iter: 353 loss: 3.53569192e-07
Iter: 354 loss: 3.53432938e-07
Iter: 355 loss: 3.55410407e-07
Iter: 356 loss: 3.53421513e-07
Iter: 357 loss: 3.53292023e-07
Iter: 358 loss: 3.54594448e-07
Iter: 359 loss: 3.5328145e-07
Iter: 360 loss: 3.53192718e-07
Iter: 361 loss: 3.54362015e-07
Iter: 362 loss: 3.53186664e-07
Iter: 363 loss: 3.53083806e-07
Iter: 364 loss: 3.5295011e-07
Iter: 365 loss: 3.52970403e-07
Iter: 366 loss: 3.5278498e-07
Iter: 367 loss: 3.5298109e-07
Iter: 368 loss: 3.52712505e-07
Iter: 369 loss: 3.52534158e-07
Iter: 370 loss: 3.53232934e-07
Iter: 371 loss: 3.52488769e-07
Iter: 372 loss: 3.52343477e-07
Iter: 373 loss: 3.52394977e-07
Iter: 374 loss: 3.52225641e-07
Iter: 375 loss: 3.52069435e-07
Iter: 376 loss: 3.53395421e-07
Iter: 377 loss: 3.52057441e-07
Iter: 378 loss: 3.51973512e-07
Iter: 379 loss: 3.51966662e-07
Iter: 380 loss: 3.51917805e-07
Iter: 381 loss: 3.51846495e-07
Iter: 382 loss: 3.51850815e-07
Iter: 383 loss: 3.5178121e-07
Iter: 384 loss: 3.51780557e-07
Iter: 385 loss: 3.51735338e-07
Iter: 386 loss: 3.51643052e-07
Iter: 387 loss: 3.5257824e-07
Iter: 388 loss: 3.51620599e-07
Iter: 389 loss: 3.51566712e-07
Iter: 390 loss: 3.5155665e-07
Iter: 391 loss: 3.51499807e-07
Iter: 392 loss: 3.51388621e-07
Iter: 393 loss: 3.54192622e-07
Iter: 394 loss: 3.51397262e-07
Iter: 395 loss: 3.5131913e-07
Iter: 396 loss: 3.51314839e-07
Iter: 397 loss: 3.51241027e-07
Iter: 398 loss: 3.51123674e-07
Iter: 399 loss: 3.53328801e-07
Iter: 400 loss: 3.51131064e-07
Iter: 401 loss: 3.50975199e-07
Iter: 402 loss: 3.51759837e-07
Iter: 403 loss: 3.5095087e-07
Iter: 404 loss: 3.50835421e-07
Iter: 405 loss: 3.50832522e-07
Iter: 406 loss: 3.50733558e-07
Iter: 407 loss: 3.50621065e-07
Iter: 408 loss: 3.50726566e-07
Iter: 409 loss: 3.50558309e-07
Iter: 410 loss: 3.50477166e-07
Iter: 411 loss: 3.50746177e-07
Iter: 412 loss: 3.50453746e-07
Iter: 413 loss: 3.50389882e-07
Iter: 414 loss: 3.50447436e-07
Iter: 415 loss: 3.50338638e-07
Iter: 416 loss: 3.5025198e-07
Iter: 417 loss: 3.5020247e-07
Iter: 418 loss: 3.50165379e-07
Iter: 419 loss: 3.50058087e-07
Iter: 420 loss: 3.50314167e-07
Iter: 421 loss: 3.50007696e-07
Iter: 422 loss: 3.49921379e-07
Iter: 423 loss: 3.50150572e-07
Iter: 424 loss: 3.49886307e-07
Iter: 425 loss: 3.4979297e-07
Iter: 426 loss: 3.50782955e-07
Iter: 427 loss: 3.49787399e-07
Iter: 428 loss: 3.49739082e-07
Iter: 429 loss: 3.49621871e-07
Iter: 430 loss: 3.49621729e-07
Iter: 431 loss: 3.49501249e-07
Iter: 432 loss: 3.51374183e-07
Iter: 433 loss: 3.49510174e-07
Iter: 434 loss: 3.49435595e-07
Iter: 435 loss: 3.49298944e-07
Iter: 436 loss: 3.51833421e-07
Iter: 437 loss: 3.49313638e-07
Iter: 438 loss: 3.49158796e-07
Iter: 439 loss: 3.49557354e-07
Iter: 440 loss: 3.49108859e-07
Iter: 441 loss: 3.49013902e-07
Iter: 442 loss: 3.503024e-07
Iter: 443 loss: 3.48988806e-07
Iter: 444 loss: 3.48921901e-07
Iter: 445 loss: 3.49594245e-07
Iter: 446 loss: 3.48901921e-07
Iter: 447 loss: 3.48879439e-07
Iter: 448 loss: 3.48839137e-07
Iter: 449 loss: 3.48811056e-07
Iter: 450 loss: 3.48764786e-07
Iter: 451 loss: 3.48762597e-07
Iter: 452 loss: 3.48716242e-07
Iter: 453 loss: 3.4862407e-07
Iter: 454 loss: 3.49335892e-07
Iter: 455 loss: 3.48589822e-07
Iter: 456 loss: 3.484692e-07
Iter: 457 loss: 3.49005376e-07
Iter: 458 loss: 3.48445553e-07
Iter: 459 loss: 3.48379444e-07
Iter: 460 loss: 3.48367507e-07
Iter: 461 loss: 3.48310721e-07
Iter: 462 loss: 3.48262404e-07
Iter: 463 loss: 3.48242253e-07
Iter: 464 loss: 3.48166253e-07
Iter: 465 loss: 3.48852495e-07
Iter: 466 loss: 3.48162843e-07
Iter: 467 loss: 3.48088264e-07
Iter: 468 loss: 3.48132971e-07
Iter: 469 loss: 3.48045688e-07
Iter: 470 loss: 3.47985747e-07
Iter: 471 loss: 3.47958689e-07
Iter: 472 loss: 3.47948202e-07
Iter: 473 loss: 3.47856655e-07
Iter: 474 loss: 3.47980773e-07
Iter: 475 loss: 3.47831701e-07
Iter: 476 loss: 3.47745868e-07
Iter: 477 loss: 3.48785079e-07
Iter: 478 loss: 3.47743025e-07
Iter: 479 loss: 3.47679787e-07
Iter: 480 loss: 3.47675893e-07
Iter: 481 loss: 3.47618879e-07
Iter: 482 loss: 3.47542255e-07
Iter: 483 loss: 3.47901022e-07
Iter: 484 loss: 3.47537167e-07
Iter: 485 loss: 3.47439368e-07
Iter: 486 loss: 3.47641958e-07
Iter: 487 loss: 3.47407195e-07
Iter: 488 loss: 3.47349e-07
Iter: 489 loss: 3.47268326e-07
Iter: 490 loss: 3.47272817e-07
Iter: 491 loss: 3.47198466e-07
Iter: 492 loss: 3.48222585e-07
Iter: 493 loss: 3.47194941e-07
Iter: 494 loss: 3.47124796e-07
Iter: 495 loss: 3.47395883e-07
Iter: 496 loss: 3.47107289e-07
Iter: 497 loss: 3.47032483e-07
Iter: 498 loss: 3.47021683e-07
Iter: 499 loss: 3.46987974e-07
Iter: 500 loss: 3.46928402e-07
Iter: 501 loss: 3.46922093e-07
Iter: 502 loss: 3.4688253e-07
Iter: 503 loss: 3.46786e-07
Iter: 504 loss: 3.48291621e-07
Iter: 505 loss: 3.46804114e-07
Iter: 506 loss: 3.4670984e-07
Iter: 507 loss: 3.46831484e-07
Iter: 508 loss: 3.46684857e-07
Iter: 509 loss: 3.46566907e-07
Iter: 510 loss: 3.47150831e-07
Iter: 511 loss: 3.46549768e-07
Iter: 512 loss: 3.46468937e-07
Iter: 513 loss: 3.47481773e-07
Iter: 514 loss: 3.46453191e-07
Iter: 515 loss: 3.46387651e-07
Iter: 516 loss: 3.46318132e-07
Iter: 517 loss: 3.4631239e-07
Iter: 518 loss: 3.46256854e-07
Iter: 519 loss: 3.46261686e-07
Iter: 520 loss: 3.46199016e-07
Iter: 521 loss: 3.46076177e-07
Iter: 522 loss: 3.47748539e-07
Iter: 523 loss: 3.46058414e-07
Iter: 524 loss: 3.45970079e-07
Iter: 525 loss: 3.46259867e-07
Iter: 526 loss: 3.45945523e-07
Iter: 527 loss: 3.45904311e-07
Iter: 528 loss: 3.45909285e-07
Iter: 529 loss: 3.45861679e-07
Iter: 530 loss: 3.4581376e-07
Iter: 531 loss: 3.45803954e-07
Iter: 532 loss: 3.45746514e-07
Iter: 533 loss: 3.46003674e-07
Iter: 534 loss: 3.45744581e-07
Iter: 535 loss: 3.45654541e-07
Iter: 536 loss: 3.45765955e-07
Iter: 537 loss: 3.4563115e-07
Iter: 538 loss: 3.45571721e-07
Iter: 539 loss: 3.45537217e-07
Iter: 540 loss: 3.45502684e-07
Iter: 541 loss: 3.45405567e-07
Iter: 542 loss: 3.45506578e-07
Iter: 543 loss: 3.45324139e-07
Iter: 544 loss: 3.45263828e-07
Iter: 545 loss: 3.45252488e-07
Iter: 546 loss: 3.45191381e-07
Iter: 547 loss: 3.45150255e-07
Iter: 548 loss: 3.4512513e-07
Iter: 549 loss: 3.45051831e-07
Iter: 550 loss: 3.4527875e-07
Iter: 551 loss: 3.45042e-07
Iter: 552 loss: 3.44928623e-07
Iter: 553 loss: 3.45266898e-07
Iter: 554 loss: 3.44914099e-07
Iter: 555 loss: 3.44837957e-07
Iter: 556 loss: 3.44787651e-07
Iter: 557 loss: 3.44782961e-07
Iter: 558 loss: 3.44730097e-07
Iter: 559 loss: 3.45255046e-07
Iter: 560 loss: 3.44731e-07
Iter: 561 loss: 3.44680217e-07
Iter: 562 loss: 3.4482963e-07
Iter: 563 loss: 3.44667114e-07
Iter: 564 loss: 3.44619423e-07
Iter: 565 loss: 3.44578723e-07
Iter: 566 loss: 3.44571333e-07
Iter: 567 loss: 3.44516877e-07
Iter: 568 loss: 3.44513808e-07
Iter: 569 loss: 3.44470806e-07
Iter: 570 loss: 3.44363855e-07
Iter: 571 loss: 3.45344603e-07
Iter: 572 loss: 3.44357716e-07
Iter: 573 loss: 3.44232461e-07
Iter: 574 loss: 3.44579917e-07
Iter: 575 loss: 3.44184542e-07
Iter: 576 loss: 3.44140233e-07
Iter: 577 loss: 3.44109111e-07
Iter: 578 loss: 3.44061391e-07
Iter: 579 loss: 3.44118291e-07
Iter: 580 loss: 3.44029274e-07
Iter: 581 loss: 3.43997044e-07
Iter: 582 loss: 3.44024045e-07
Iter: 583 loss: 3.4397317e-07
Iter: 584 loss: 3.43939575e-07
Iter: 585 loss: 3.43934687e-07
Iter: 586 loss: 3.43913882e-07
Iter: 587 loss: 3.43856669e-07
Iter: 588 loss: 3.44371074e-07
Iter: 589 loss: 3.43852491e-07
Iter: 590 loss: 3.43796046e-07
Iter: 591 loss: 3.43937643e-07
Iter: 592 loss: 3.43788258e-07
Iter: 593 loss: 3.43725844e-07
Iter: 594 loss: 3.43721695e-07
Iter: 595 loss: 3.43690971e-07
Iter: 596 loss: 3.43615227e-07
Iter: 597 loss: 3.44257842e-07
Iter: 598 loss: 3.43592887e-07
Iter: 599 loss: 3.43558924e-07
Iter: 600 loss: 3.43541359e-07
Iter: 601 loss: 3.43496197e-07
Iter: 602 loss: 3.43442196e-07
Iter: 603 loss: 3.45037591e-07
Iter: 604 loss: 3.43435346e-07
Iter: 605 loss: 3.43354031e-07
Iter: 606 loss: 3.43385466e-07
Iter: 607 loss: 3.43299803e-07
Iter: 608 loss: 3.4325123e-07
Iter: 609 loss: 3.43678835e-07
Iter: 610 loss: 3.43239066e-07
Iter: 611 loss: 3.43174236e-07
Iter: 612 loss: 3.43412694e-07
Iter: 613 loss: 3.43153488e-07
Iter: 614 loss: 3.43102442e-07
Iter: 615 loss: 3.43044235e-07
Iter: 616 loss: 3.43042757e-07
Iter: 617 loss: 3.4301226e-07
Iter: 618 loss: 3.42993616e-07
Iter: 619 loss: 3.42969543e-07
Iter: 620 loss: 3.42919918e-07
Iter: 621 loss: 3.42917332e-07
Iter: 622 loss: 3.42861398e-07
Iter: 623 loss: 3.428475e-07
Iter: 624 loss: 3.42817799e-07
Iter: 625 loss: 3.42752713e-07
Iter: 626 loss: 3.42761723e-07
Iter: 627 loss: 3.42710308e-07
Iter: 628 loss: 3.4259881e-07
Iter: 629 loss: 3.44519236e-07
Iter: 630 loss: 3.42609098e-07
Iter: 631 loss: 3.42523208e-07
Iter: 632 loss: 3.43011493e-07
Iter: 633 loss: 3.42499504e-07
Iter: 634 loss: 3.42372061e-07
Iter: 635 loss: 3.42934356e-07
Iter: 636 loss: 3.42348187e-07
Iter: 637 loss: 3.42302627e-07
Iter: 638 loss: 3.42294356e-07
Iter: 639 loss: 3.42265139e-07
Iter: 640 loss: 3.42189878e-07
Iter: 641 loss: 3.42428962e-07
Iter: 642 loss: 3.42173365e-07
Iter: 643 loss: 3.42121268e-07
Iter: 644 loss: 3.4260151e-07
Iter: 645 loss: 3.42127521e-07
Iter: 646 loss: 3.42072383e-07
Iter: 647 loss: 3.42039129e-07
Iter: 648 loss: 3.42013209e-07
Iter: 649 loss: 3.41969411e-07
Iter: 650 loss: 3.42077186e-07
Iter: 651 loss: 3.41946219e-07
Iter: 652 loss: 3.41889688e-07
Iter: 653 loss: 3.42212786e-07
Iter: 654 loss: 3.41869736e-07
Iter: 655 loss: 3.41822613e-07
Iter: 656 loss: 3.4179476e-07
Iter: 657 loss: 3.41772562e-07
Iter: 658 loss: 3.4173479e-07
Iter: 659 loss: 3.41734847e-07
Iter: 660 loss: 3.41682494e-07
Iter: 661 loss: 3.41724e-07
Iter: 662 loss: 3.41651912e-07
Iter: 663 loss: 3.41615419e-07
Iter: 664 loss: 3.41514863e-07
Iter: 665 loss: 3.43280817e-07
Iter: 666 loss: 3.41501902e-07
Iter: 667 loss: 3.41447787e-07
Iter: 668 loss: 3.41439858e-07
Iter: 669 loss: 3.4138111e-07
Iter: 670 loss: 3.41520661e-07
Iter: 671 loss: 3.41363432e-07
Iter: 672 loss: 3.41319719e-07
Iter: 673 loss: 3.41234937e-07
Iter: 674 loss: 3.41239684e-07
Iter: 675 loss: 3.41208647e-07
Iter: 676 loss: 3.41186279e-07
Iter: 677 loss: 3.41157261e-07
Iter: 678 loss: 3.41225416e-07
Iter: 679 loss: 3.41140549e-07
Iter: 680 loss: 3.41108375e-07
Iter: 681 loss: 3.4111261e-07
Iter: 682 loss: 3.41068926e-07
Iter: 683 loss: 3.41048093e-07
Iter: 684 loss: 3.4104778e-07
Iter: 685 loss: 3.4102743e-07
Iter: 686 loss: 3.40965073e-07
Iter: 687 loss: 3.41781401e-07
Iter: 688 loss: 3.40967375e-07
Iter: 689 loss: 3.40909423e-07
Iter: 690 loss: 3.4101032e-07
Iter: 691 loss: 3.40871566e-07
Iter: 692 loss: 3.40839165e-07
Iter: 693 loss: 3.40820066e-07
Iter: 694 loss: 3.40792582e-07
Iter: 695 loss: 3.40739e-07
Iter: 696 loss: 3.41737774e-07
Iter: 697 loss: 3.40729912e-07
Iter: 698 loss: 3.40653173e-07
Iter: 699 loss: 3.40906126e-07
Iter: 700 loss: 3.40619295e-07
Iter: 701 loss: 3.40600138e-07
Iter: 702 loss: 3.40602583e-07
Iter: 703 loss: 3.40573933e-07
Iter: 704 loss: 3.40514248e-07
Iter: 705 loss: 3.40516095e-07
Iter: 706 loss: 3.40464283e-07
Iter: 707 loss: 3.40429949e-07
Iter: 708 loss: 3.40422616e-07
Iter: 709 loss: 3.4035304e-07
Iter: 710 loss: 3.40824272e-07
Iter: 711 loss: 3.40361481e-07
Iter: 712 loss: 3.40319303e-07
Iter: 713 loss: 3.40869917e-07
Iter: 714 loss: 3.40316888e-07
Iter: 715 loss: 3.40286761e-07
Iter: 716 loss: 3.40223949e-07
Iter: 717 loss: 3.41245e-07
Iter: 718 loss: 3.40228297e-07
Iter: 719 loss: 3.40154685e-07
Iter: 720 loss: 3.41161183e-07
Iter: 721 loss: 3.40156191e-07
Iter: 722 loss: 3.40110319e-07
Iter: 723 loss: 3.4004043e-07
Iter: 724 loss: 3.40697227e-07
Iter: 725 loss: 3.4001647e-07
Iter: 726 loss: 3.39952436e-07
Iter: 727 loss: 3.40449276e-07
Iter: 728 loss: 3.39947576e-07
Iter: 729 loss: 3.39904602e-07
Iter: 730 loss: 3.39905455e-07
Iter: 731 loss: 3.39868734e-07
Iter: 732 loss: 3.39798817e-07
Iter: 733 loss: 3.41007251e-07
Iter: 734 loss: 3.39797651e-07
Iter: 735 loss: 3.39749278e-07
Iter: 736 loss: 3.39923702e-07
Iter: 737 loss: 3.3973734e-07
Iter: 738 loss: 3.39680923e-07
Iter: 739 loss: 3.4015747e-07
Iter: 740 loss: 3.39671828e-07
Iter: 741 loss: 3.39600945e-07
Iter: 742 loss: 3.39527702e-07
Iter: 743 loss: 3.39521705e-07
Iter: 744 loss: 3.39452839e-07
Iter: 745 loss: 3.39686551e-07
Iter: 746 loss: 3.39432574e-07
Iter: 747 loss: 3.39369251e-07
Iter: 748 loss: 3.39872912e-07
Iter: 749 loss: 3.39363595e-07
Iter: 750 loss: 3.39294132e-07
Iter: 751 loss: 3.39538587e-07
Iter: 752 loss: 3.39274948e-07
Iter: 753 loss: 3.39237033e-07
Iter: 754 loss: 3.39190592e-07
Iter: 755 loss: 3.39178825e-07
Iter: 756 loss: 3.39125563e-07
Iter: 757 loss: 3.39129542e-07
Iter: 758 loss: 3.39098165e-07
Iter: 759 loss: 3.39035353e-07
Iter: 760 loss: 3.39572068e-07
Iter: 761 loss: 3.39019323e-07
Iter: 762 loss: 3.38954436e-07
Iter: 763 loss: 3.39535916e-07
Iter: 764 loss: 3.38962792e-07
Iter: 765 loss: 3.38879545e-07
Iter: 766 loss: 3.39058118e-07
Iter: 767 loss: 3.38855727e-07
Iter: 768 loss: 3.38827647e-07
Iter: 769 loss: 3.38852033e-07
Iter: 770 loss: 3.38798571e-07
Iter: 771 loss: 3.38759378e-07
Iter: 772 loss: 3.39014321e-07
Iter: 773 loss: 3.38741671e-07
Iter: 774 loss: 3.38686135e-07
Iter: 775 loss: 3.38884774e-07
Iter: 776 loss: 3.38663739e-07
Iter: 777 loss: 3.3864e-07
Iter: 778 loss: 3.38561392e-07
Iter: 779 loss: 3.40359094e-07
Iter: 780 loss: 3.38561335e-07
Iter: 781 loss: 3.38483858e-07
Iter: 782 loss: 3.38864027e-07
Iter: 783 loss: 3.38469704e-07
Iter: 784 loss: 3.38431164e-07
Iter: 785 loss: 3.38427412e-07
Iter: 786 loss: 3.38385348e-07
Iter: 787 loss: 3.38288658e-07
Iter: 788 loss: 3.40277239e-07
Iter: 789 loss: 3.38301504e-07
Iter: 790 loss: 3.3826268e-07
Iter: 791 loss: 3.38254125e-07
Iter: 792 loss: 3.38218e-07
Iter: 793 loss: 3.38276152e-07
Iter: 794 loss: 3.38210441e-07
Iter: 795 loss: 3.38182701e-07
Iter: 796 loss: 3.38129638e-07
Iter: 797 loss: 3.38923968e-07
Iter: 798 loss: 3.38136971e-07
Iter: 799 loss: 3.38095731e-07
Iter: 800 loss: 3.38088228e-07
Iter: 801 loss: 3.3805884e-07
Iter: 802 loss: 3.38004099e-07
Iter: 803 loss: 3.38003986e-07
Iter: 804 loss: 3.37950837e-07
Iter: 805 loss: 3.37912184e-07
Iter: 806 loss: 3.3789064e-07
Iter: 807 loss: 3.37833e-07
Iter: 808 loss: 3.37844938e-07
Iter: 809 loss: 3.37779852e-07
Iter: 810 loss: 3.37741966e-07
Iter: 811 loss: 3.37750976e-07
Iter: 812 loss: 3.37709253e-07
Iter: 813 loss: 3.37737504e-07
Iter: 814 loss: 3.37696406e-07
Iter: 815 loss: 3.37679921e-07
Iter: 816 loss: 3.37638085e-07
Iter: 817 loss: 3.38661152e-07
Iter: 818 loss: 3.37625e-07
Iter: 819 loss: 3.37562142e-07
Iter: 820 loss: 3.3764195e-07
Iter: 821 loss: 3.37538211e-07
Iter: 822 loss: 3.37529457e-07
Iter: 823 loss: 3.37518316e-07
Iter: 824 loss: 3.37490633e-07
Iter: 825 loss: 3.37433022e-07
Iter: 826 loss: 3.38552553e-07
Iter: 827 loss: 3.37429469e-07
Iter: 828 loss: 3.37391953e-07
Iter: 829 loss: 3.37397523e-07
Iter: 830 loss: 3.37355942e-07
Iter: 831 loss: 3.37330277e-07
Iter: 832 loss: 3.37318824e-07
Iter: 833 loss: 3.37271842e-07
Iter: 834 loss: 3.37246718e-07
Iter: 835 loss: 3.37240891e-07
Iter: 836 loss: 3.3717123e-07
Iter: 837 loss: 3.37776157e-07
Iter: 838 loss: 3.37162135e-07
Iter: 839 loss: 3.37109668e-07
Iter: 840 loss: 3.37592326e-07
Iter: 841 loss: 3.37104552e-07
Iter: 842 loss: 3.37079825e-07
Iter: 843 loss: 3.37157843e-07
Iter: 844 loss: 3.3707235e-07
Iter: 845 loss: 3.37032475e-07
Iter: 846 loss: 3.37031025e-07
Iter: 847 loss: 3.37017042e-07
Iter: 848 loss: 3.36988592e-07
Iter: 849 loss: 3.37001609e-07
Iter: 850 loss: 3.36974892e-07
Iter: 851 loss: 3.36959062e-07
Iter: 852 loss: 3.36953462e-07
Iter: 853 loss: 3.36939365e-07
Iter: 854 loss: 3.36914155e-07
Iter: 855 loss: 3.36903156e-07
Iter: 856 loss: 3.3689372e-07
Iter: 857 loss: 3.36890906e-07
Iter: 858 loss: 3.36866862e-07
Iter: 859 loss: 3.36831334e-07
Iter: 860 loss: 3.37457948e-07
Iter: 861 loss: 3.36833835e-07
Iter: 862 loss: 3.36804248e-07
Iter: 863 loss: 3.36796688e-07
Iter: 864 loss: 3.36773638e-07
Iter: 865 loss: 3.36749537e-07
Iter: 866 loss: 3.36746211e-07
Iter: 867 loss: 3.36717619e-07
Iter: 868 loss: 3.36694427e-07
Iter: 869 loss: 3.36672713e-07
Iter: 870 loss: 3.36642358e-07
Iter: 871 loss: 3.36601545e-07
Iter: 872 loss: 3.36604927e-07
Iter: 873 loss: 3.36585202e-07
Iter: 874 loss: 3.36581763e-07
Iter: 875 loss: 3.36551182e-07
Iter: 876 loss: 3.36524465e-07
Iter: 877 loss: 3.36521879e-07
Iter: 878 loss: 3.36476091e-07
Iter: 879 loss: 3.36637072e-07
Iter: 880 loss: 3.3646424e-07
Iter: 881 loss: 3.36427973e-07
Iter: 882 loss: 3.36907959e-07
Iter: 883 loss: 3.36435335e-07
Iter: 884 loss: 3.36412256e-07
Iter: 885 loss: 3.36371386e-07
Iter: 886 loss: 3.37090256e-07
Iter: 887 loss: 3.36373489e-07
Iter: 888 loss: 3.36298541e-07
Iter: 889 loss: 3.36337081e-07
Iter: 890 loss: 3.36273501e-07
Iter: 891 loss: 3.3617556e-07
Iter: 892 loss: 3.37438848e-07
Iter: 893 loss: 3.36187071e-07
Iter: 894 loss: 3.3613054e-07
Iter: 895 loss: 3.36093e-07
Iter: 896 loss: 3.36066876e-07
Iter: 897 loss: 3.36050448e-07
Iter: 898 loss: 3.36042348e-07
Iter: 899 loss: 3.36019383e-07
Iter: 900 loss: 3.35989199e-07
Iter: 901 loss: 3.35985646e-07
Iter: 902 loss: 3.35949551e-07
Iter: 903 loss: 3.36270432e-07
Iter: 904 loss: 3.35945e-07
Iter: 905 loss: 3.35929826e-07
Iter: 906 loss: 3.35984282e-07
Iter: 907 loss: 3.35917548e-07
Iter: 908 loss: 3.3590743e-07
Iter: 909 loss: 3.35996589e-07
Iter: 910 loss: 3.3590095e-07
Iter: 911 loss: 3.35878354e-07
Iter: 912 loss: 3.35846067e-07
Iter: 913 loss: 3.36494452e-07
Iter: 914 loss: 3.3584783e-07
Iter: 915 loss: 3.35841662e-07
Iter: 916 loss: 3.35836205e-07
Iter: 917 loss: 3.35806135e-07
Iter: 918 loss: 3.35762763e-07
Iter: 919 loss: 3.36386108e-07
Iter: 920 loss: 3.35774303e-07
Iter: 921 loss: 3.35702566e-07
Iter: 922 loss: 3.35722149e-07
Iter: 923 loss: 3.35673064e-07
Iter: 924 loss: 3.35625629e-07
Iter: 925 loss: 3.35616e-07
Iter: 926 loss: 3.35590471e-07
Iter: 927 loss: 3.35544769e-07
Iter: 928 loss: 3.35543746e-07
Iter: 929 loss: 3.3553286e-07
Iter: 930 loss: 3.35734796e-07
Iter: 931 loss: 3.355307e-07
Iter: 932 loss: 3.35519815e-07
Iter: 933 loss: 3.35474795e-07
Iter: 934 loss: 3.3585377e-07
Iter: 935 loss: 3.35470247e-07
Iter: 936 loss: 3.35439807e-07
Iter: 937 loss: 3.35475647e-07
Iter: 938 loss: 3.35430741e-07
Iter: 939 loss: 3.35405701e-07
Iter: 940 loss: 3.35409055e-07
Iter: 941 loss: 3.35383419e-07
Iter: 942 loss: 3.35389672e-07
Iter: 943 loss: 3.35369634e-07
Iter: 944 loss: 3.35343714e-07
Iter: 945 loss: 3.35304492e-07
Iter: 946 loss: 3.35300456e-07
Iter: 947 loss: 3.35250462e-07
Iter: 948 loss: 3.35342236e-07
Iter: 949 loss: 3.35214736e-07
Iter: 950 loss: 3.35205641e-07
Iter: 951 loss: 3.35188332e-07
Iter: 952 loss: 3.35168693e-07
Iter: 953 loss: 3.35129755e-07
Iter: 954 loss: 3.35843765e-07
Iter: 955 loss: 3.35131062e-07
Iter: 956 loss: 3.35096928e-07
Iter: 957 loss: 3.35356731e-07
Iter: 958 loss: 3.35092267e-07
Iter: 959 loss: 3.35070069e-07
Iter: 960 loss: 3.35052e-07
Iter: 961 loss: 3.35039033e-07
Iter: 962 loss: 3.34995036e-07
Iter: 963 loss: 3.35304378e-07
Iter: 964 loss: 3.34985771e-07
Iter: 965 loss: 3.34977301e-07
Iter: 966 loss: 3.34925971e-07
Iter: 967 loss: 3.35793686e-07
Iter: 968 loss: 3.34917274e-07
Iter: 969 loss: 3.34853354e-07
Iter: 970 loss: 3.35066346e-07
Iter: 971 loss: 3.34842753e-07
Iter: 972 loss: 3.34785312e-07
Iter: 973 loss: 3.34944446e-07
Iter: 974 loss: 3.34766071e-07
Iter: 975 loss: 3.34728924e-07
Iter: 976 loss: 3.35016352e-07
Iter: 977 loss: 3.34734409e-07
Iter: 978 loss: 3.34677026e-07
Iter: 979 loss: 3.34761694e-07
Iter: 980 loss: 3.34652384e-07
Iter: 981 loss: 3.34627799e-07
Iter: 982 loss: 3.3458744e-07
Iter: 983 loss: 3.34580761e-07
Iter: 984 loss: 3.34520138e-07
Iter: 985 loss: 3.34964682e-07
Iter: 986 loss: 3.34508456e-07
Iter: 987 loss: 3.34464431e-07
Iter: 988 loss: 3.34467302e-07
Iter: 989 loss: 3.3444141e-07
Iter: 990 loss: 3.34444678e-07
Iter: 991 loss: 3.34409833e-07
Iter: 992 loss: 3.34384765e-07
Iter: 993 loss: 3.34603925e-07
Iter: 994 loss: 3.3436487e-07
Iter: 995 loss: 3.34351824e-07
Iter: 996 loss: 3.34311466e-07
Iter: 997 loss: 3.34317349e-07
Iter: 998 loss: 3.34284863e-07
Iter: 999 loss: 3.34276564e-07
Iter: 1000 loss: 3.34270624e-07
Iter: 1001 loss: 3.34236177e-07
Iter: 1002 loss: 3.34614583e-07
Iter: 1003 loss: 3.34222705e-07
Iter: 1004 loss: 3.34165946e-07
Iter: 1005 loss: 3.34187689e-07
Iter: 1006 loss: 3.34120955e-07
Iter: 1007 loss: 3.34047627e-07
Iter: 1008 loss: 3.3452659e-07
Iter: 1009 loss: 3.34042e-07
Iter: 1010 loss: 3.3398598e-07
Iter: 1011 loss: 3.34569563e-07
Iter: 1012 loss: 3.33973503e-07
Iter: 1013 loss: 3.33930217e-07
Iter: 1014 loss: 3.33981177e-07
Iter: 1015 loss: 3.33929e-07
Iter: 1016 loss: 3.33859731e-07
Iter: 1017 loss: 3.33843445e-07
Iter: 1018 loss: 3.33817724e-07
Iter: 1019 loss: 3.33797516e-07
Iter: 1020 loss: 3.33791377e-07
Iter: 1021 loss: 3.33776654e-07
Iter: 1022 loss: 3.33766252e-07
Iter: 1023 loss: 3.33757953e-07
Iter: 1024 loss: 3.33730384e-07
Iter: 1025 loss: 3.33715093e-07
Iter: 1026 loss: 3.33693777e-07
Iter: 1027 loss: 3.33652e-07
Iter: 1028 loss: 3.33667657e-07
Iter: 1029 loss: 3.33621045e-07
Iter: 1030 loss: 3.33600809e-07
Iter: 1031 loss: 3.33594528e-07
Iter: 1032 loss: 3.33566192e-07
Iter: 1033 loss: 3.33516908e-07
Iter: 1034 loss: 3.34335908e-07
Iter: 1035 loss: 3.33519324e-07
Iter: 1036 loss: 3.33479392e-07
Iter: 1037 loss: 3.33476379e-07
Iter: 1038 loss: 3.33447929e-07
Iter: 1039 loss: 3.33403364e-07
Iter: 1040 loss: 3.33994194e-07
Iter: 1041 loss: 3.33399214e-07
Iter: 1042 loss: 3.33362891e-07
Iter: 1043 loss: 3.33367524e-07
Iter: 1044 loss: 3.33324806e-07
Iter: 1045 loss: 3.33417518e-07
Iter: 1046 loss: 3.33306872e-07
Iter: 1047 loss: 3.33276063e-07
Iter: 1048 loss: 3.3328871e-07
Iter: 1049 loss: 3.33253e-07
Iter: 1050 loss: 3.33207083e-07
Iter: 1051 loss: 3.33357633e-07
Iter: 1052 loss: 3.33189888e-07
Iter: 1053 loss: 3.33160131e-07
Iter: 1054 loss: 3.33544079e-07
Iter: 1055 loss: 3.33170703e-07
Iter: 1056 loss: 3.33127787e-07
Iter: 1057 loss: 3.33100843e-07
Iter: 1058 loss: 3.33096921e-07
Iter: 1059 loss: 3.33067078e-07
Iter: 1060 loss: 3.33110961e-07
Iter: 1061 loss: 3.33035644e-07
Iter: 1062 loss: 3.32995683e-07
Iter: 1063 loss: 3.33318155e-07
Iter: 1064 loss: 3.32987952e-07
Iter: 1065 loss: 3.32930568e-07
Iter: 1066 loss: 3.32995171e-07
Iter: 1067 loss: 3.32921019e-07
Iter: 1068 loss: 3.32892057e-07
Iter: 1069 loss: 3.33216349e-07
Iter: 1070 loss: 3.32887964e-07
Iter: 1071 loss: 3.32856132e-07
Iter: 1072 loss: 3.32820377e-07
Iter: 1073 loss: 3.32813272e-07
Iter: 1074 loss: 3.32783969e-07
Iter: 1075 loss: 3.32923946e-07
Iter: 1076 loss: 3.32786982e-07
Iter: 1077 loss: 3.32758646e-07
Iter: 1078 loss: 3.33020182e-07
Iter: 1079 loss: 3.32752649e-07
Iter: 1080 loss: 3.32742502e-07
Iter: 1081 loss: 3.32711693e-07
Iter: 1082 loss: 3.32707742e-07
Iter: 1083 loss: 3.32687335e-07
Iter: 1084 loss: 3.32880091e-07
Iter: 1085 loss: 3.32683214e-07
Iter: 1086 loss: 3.32660079e-07
Iter: 1087 loss: 3.32688273e-07
Iter: 1088 loss: 3.32646096e-07
Iter: 1089 loss: 3.32606248e-07
Iter: 1090 loss: 3.32624154e-07
Iter: 1091 loss: 3.32581806e-07
Iter: 1092 loss: 3.32543095e-07
Iter: 1093 loss: 3.32519392e-07
Iter: 1094 loss: 3.32479715e-07
Iter: 1095 loss: 3.3244936e-07
Iter: 1096 loss: 3.32874464e-07
Iter: 1097 loss: 3.32449218e-07
Iter: 1098 loss: 3.32406273e-07
Iter: 1099 loss: 3.32813215e-07
Iter: 1100 loss: 3.32414658e-07
Iter: 1101 loss: 3.32390641e-07
Iter: 1102 loss: 3.32382797e-07
Iter: 1103 loss: 3.32360202e-07
Iter: 1104 loss: 3.3232314e-07
Iter: 1105 loss: 3.32590275e-07
Iter: 1106 loss: 3.32308474e-07
Iter: 1107 loss: 3.32287811e-07
Iter: 1108 loss: 3.32256718e-07
Iter: 1109 loss: 3.32256377e-07
Iter: 1110 loss: 3.32225625e-07
Iter: 1111 loss: 3.32218804e-07
Iter: 1112 loss: 3.32218576e-07
Iter: 1113 loss: 3.32191178e-07
Iter: 1114 loss: 3.32184527e-07
Iter: 1115 loss: 3.32157867e-07
Iter: 1116 loss: 3.3231936e-07
Iter: 1117 loss: 3.32148659e-07
Iter: 1118 loss: 3.32136665e-07
Iter: 1119 loss: 3.32159914e-07
Iter: 1120 loss: 3.32124699e-07
Iter: 1121 loss: 3.3208e-07
Iter: 1122 loss: 3.32208913e-07
Iter: 1123 loss: 3.32074592e-07
Iter: 1124 loss: 3.32045602e-07
Iter: 1125 loss: 3.31987337e-07
Iter: 1126 loss: 3.33125968e-07
Iter: 1127 loss: 3.3198063e-07
Iter: 1128 loss: 3.31923559e-07
Iter: 1129 loss: 3.32064616e-07
Iter: 1130 loss: 3.31917e-07
Iter: 1131 loss: 3.31891101e-07
Iter: 1132 loss: 3.31887151e-07
Iter: 1133 loss: 3.3185475e-07
Iter: 1134 loss: 3.31822378e-07
Iter: 1135 loss: 3.31816267e-07
Iter: 1136 loss: 3.31802198e-07
Iter: 1137 loss: 3.31798333e-07
Iter: 1138 loss: 3.31782047e-07
Iter: 1139 loss: 3.31747e-07
Iter: 1140 loss: 3.31745355e-07
Iter: 1141 loss: 3.31727762e-07
Iter: 1142 loss: 3.31915857e-07
Iter: 1143 loss: 3.31728359e-07
Iter: 1144 loss: 3.31702466e-07
Iter: 1145 loss: 3.31709714e-07
Iter: 1146 loss: 3.31678052e-07
Iter: 1147 loss: 3.31661681e-07
Iter: 1148 loss: 3.31636073e-07
Iter: 1149 loss: 3.31632521e-07
Iter: 1150 loss: 3.31625813e-07
Iter: 1151 loss: 3.32027867e-07
Iter: 1152 loss: 3.31623767e-07
Iter: 1153 loss: 3.31594634e-07
Iter: 1154 loss: 3.3164531e-07
Iter: 1155 loss: 3.31575336e-07
Iter: 1156 loss: 3.31550609e-07
Iter: 1157 loss: 3.31552087e-07
Iter: 1158 loss: 3.31528526e-07
Iter: 1159 loss: 3.3149135e-07
Iter: 1160 loss: 3.31461308e-07
Iter: 1161 loss: 3.31451503e-07
Iter: 1162 loss: 3.31400088e-07
Iter: 1163 loss: 3.31403612e-07
Iter: 1164 loss: 3.31370359e-07
Iter: 1165 loss: 3.313435e-07
Iter: 1166 loss: 3.31323321e-07
Iter: 1167 loss: 3.3133e-07
Iter: 1168 loss: 3.31315391e-07
Iter: 1169 loss: 3.3127975e-07
Iter: 1170 loss: 3.31309195e-07
Iter: 1171 loss: 3.31264175e-07
Iter: 1172 loss: 3.31248856e-07
Iter: 1173 loss: 3.31311639e-07
Iter: 1174 loss: 3.31244451e-07
Iter: 1175 loss: 3.31208042e-07
Iter: 1176 loss: 3.31304022e-07
Iter: 1177 loss: 3.31203807e-07
Iter: 1178 loss: 3.31191188e-07
Iter: 1179 loss: 3.31164671e-07
Iter: 1180 loss: 3.31171037e-07
Iter: 1181 loss: 3.31112375e-07
Iter: 1182 loss: 3.31121555e-07
Iter: 1183 loss: 3.3108654e-07
Iter: 1184 loss: 3.31058828e-07
Iter: 1185 loss: 3.31044617e-07
Iter: 1186 loss: 3.31019e-07
Iter: 1187 loss: 3.31016025e-07
Iter: 1188 loss: 3.30995903e-07
Iter: 1189 loss: 3.3096461e-07
Iter: 1190 loss: 3.31026513e-07
Iter: 1191 loss: 3.30939372e-07
Iter: 1192 loss: 3.30924223e-07
Iter: 1193 loss: 3.30883381e-07
Iter: 1194 loss: 3.30889691e-07
Iter: 1195 loss: 3.30866925e-07
Iter: 1196 loss: 3.30863855e-07
Iter: 1197 loss: 3.30830801e-07
Iter: 1198 loss: 3.30855954e-07
Iter: 1199 loss: 3.30812895e-07
Iter: 1200 loss: 3.30783365e-07
Iter: 1201 loss: 3.31017588e-07
Iter: 1202 loss: 3.30788254e-07
Iter: 1203 loss: 3.3075591e-07
Iter: 1204 loss: 3.30717398e-07
Iter: 1205 loss: 3.315821e-07
Iter: 1206 loss: 3.30715125e-07
Iter: 1207 loss: 3.30680422e-07
Iter: 1208 loss: 3.31362287e-07
Iter: 1209 loss: 3.306846e-07
Iter: 1210 loss: 3.30639978e-07
Iter: 1211 loss: 3.30642763e-07
Iter: 1212 loss: 3.30611186e-07
Iter: 1213 loss: 3.30581202e-07
Iter: 1214 loss: 3.30534533e-07
Iter: 1215 loss: 3.30532032e-07
Iter: 1216 loss: 3.30480589e-07
Iter: 1217 loss: 3.30481669e-07
Iter: 1218 loss: 3.30425763e-07
Iter: 1219 loss: 3.30619116e-07
Iter: 1220 loss: 3.30415787e-07
Iter: 1221 loss: 3.30369687e-07
Iter: 1222 loss: 3.30455919e-07
Iter: 1223 loss: 3.30356215e-07
Iter: 1224 loss: 3.30325349e-07
Iter: 1225 loss: 3.30318102e-07
Iter: 1226 loss: 3.30285161e-07
Iter: 1227 loss: 3.30241448e-07
Iter: 1228 loss: 3.30388445e-07
Iter: 1229 loss: 3.30229057e-07
Iter: 1230 loss: 3.30215983e-07
Iter: 1231 loss: 3.30210412e-07
Iter: 1232 loss: 3.30210639e-07
Iter: 1233 loss: 3.30202795e-07
Iter: 1234 loss: 3.30191767e-07
Iter: 1235 loss: 3.30175681e-07
Iter: 1236 loss: 3.30215443e-07
Iter: 1237 loss: 3.30161072e-07
Iter: 1238 loss: 3.30149277e-07
Iter: 1239 loss: 3.30142882e-07
Iter: 1240 loss: 3.30130604e-07
Iter: 1241 loss: 3.3010275e-07
Iter: 1242 loss: 3.30339645e-07
Iter: 1243 loss: 3.30100079e-07
Iter: 1244 loss: 3.30075039e-07
Iter: 1245 loss: 3.30028115e-07
Iter: 1246 loss: 3.30556617e-07
Iter: 1247 loss: 3.30028911e-07
Iter: 1248 loss: 3.29987046e-07
Iter: 1249 loss: 3.30162436e-07
Iter: 1250 loss: 3.29980026e-07
Iter: 1251 loss: 3.29935176e-07
Iter: 1252 loss: 3.29939155e-07
Iter: 1253 loss: 3.29899137e-07
Iter: 1254 loss: 3.29913121e-07
Iter: 1255 loss: 3.29898882e-07
Iter: 1256 loss: 3.2985281e-07
Iter: 1257 loss: 3.29994236e-07
Iter: 1258 loss: 3.29841527e-07
Iter: 1259 loss: 3.29837121e-07
Iter: 1260 loss: 3.29837633e-07
Iter: 1261 loss: 3.29809211e-07
Iter: 1262 loss: 3.29801082e-07
Iter: 1263 loss: 3.29915508e-07
Iter: 1264 loss: 3.29786531e-07
Iter: 1265 loss: 3.29764049e-07
Iter: 1266 loss: 3.298195e-07
Iter: 1267 loss: 3.29763566e-07
Iter: 1268 loss: 3.2972747e-07
Iter: 1269 loss: 3.29859716e-07
Iter: 1270 loss: 3.29731193e-07
Iter: 1271 loss: 3.29707177e-07
Iter: 1272 loss: 3.2969416e-07
Iter: 1273 loss: 3.29686799e-07
Iter: 1274 loss: 3.29671025e-07
Iter: 1275 loss: 3.29677846e-07
Iter: 1276 loss: 3.29660537e-07
Iter: 1277 loss: 3.29601363e-07
Iter: 1278 loss: 3.3006387e-07
Iter: 1279 loss: 3.29586129e-07
Iter: 1280 loss: 3.29538125e-07
Iter: 1281 loss: 3.29638567e-07
Iter: 1282 loss: 3.29515586e-07
Iter: 1283 loss: 3.29453371e-07
Iter: 1284 loss: 3.29507941e-07
Iter: 1285 loss: 3.29435522e-07
Iter: 1286 loss: 3.2942296e-07
Iter: 1287 loss: 3.29404401e-07
Iter: 1288 loss: 3.29380384e-07
Iter: 1289 loss: 3.29378167e-07
Iter: 1290 loss: 3.29365832e-07
Iter: 1291 loss: 3.29356652e-07
Iter: 1292 loss: 3.29397977e-07
Iter: 1293 loss: 3.29332863e-07
Iter: 1294 loss: 3.29303191e-07
Iter: 1295 loss: 3.29372e-07
Iter: 1296 loss: 3.29303134e-07
Iter: 1297 loss: 3.29278919e-07
Iter: 1298 loss: 3.29325047e-07
Iter: 1299 loss: 3.29269085e-07
Iter: 1300 loss: 3.29220398e-07
Iter: 1301 loss: 3.29215851e-07
Iter: 1302 loss: 3.29209342e-07
Iter: 1303 loss: 3.29183933e-07
Iter: 1304 loss: 3.29163527e-07
Iter: 1305 loss: 3.29166369e-07
Iter: 1306 loss: 3.29121377e-07
Iter: 1307 loss: 3.29311462e-07
Iter: 1308 loss: 3.29100885e-07
Iter: 1309 loss: 3.29075647e-07
Iter: 1310 loss: 3.29506094e-07
Iter: 1311 loss: 3.29083377e-07
Iter: 1312 loss: 3.29052739e-07
Iter: 1313 loss: 3.29158468e-07
Iter: 1314 loss: 3.29048817e-07
Iter: 1315 loss: 3.29027444e-07
Iter: 1316 loss: 3.29013801e-07
Iter: 1317 loss: 3.29801196e-07
Iter: 1318 loss: 3.29015e-07
Iter: 1319 loss: 3.28957924e-07
Iter: 1320 loss: 3.29124475e-07
Iter: 1321 loss: 3.28958947e-07
Iter: 1322 loss: 3.28930582e-07
Iter: 1323 loss: 3.29237565e-07
Iter: 1324 loss: 3.28917253e-07
Iter: 1325 loss: 3.28906708e-07
Iter: 1326 loss: 3.28906339e-07
Iter: 1327 loss: 3.28900626e-07
Iter: 1328 loss: 3.28879793e-07
Iter: 1329 loss: 3.29083775e-07
Iter: 1330 loss: 3.28875558e-07
Iter: 1331 loss: 3.28859016e-07
Iter: 1332 loss: 3.28885562e-07
Iter: 1333 loss: 3.28858846e-07
Iter: 1334 loss: 3.28824854e-07
Iter: 1335 loss: 3.28873e-07
Iter: 1336 loss: 3.28822324e-07
Iter: 1337 loss: 3.28800184e-07
Iter: 1338 loss: 3.28752776e-07
Iter: 1339 loss: 3.29642432e-07
Iter: 1340 loss: 3.28760564e-07
Iter: 1341 loss: 3.28712048e-07
Iter: 1342 loss: 3.28816554e-07
Iter: 1343 loss: 3.28706705e-07
Iter: 1344 loss: 3.28704857e-07
Iter: 1345 loss: 3.28694057e-07
Iter: 1346 loss: 3.28679363e-07
Iter: 1347 loss: 3.28746125e-07
Iter: 1348 loss: 3.28673963e-07
Iter: 1349 loss: 3.28655801e-07
Iter: 1350 loss: 3.28680187e-07
Iter: 1351 loss: 3.28657279e-07
Iter: 1352 loss: 3.28642983e-07
Iter: 1353 loss: 3.28622889e-07
Iter: 1354 loss: 3.28624253e-07
Iter: 1355 loss: 3.2860396e-07
Iter: 1356 loss: 3.28597508e-07
Iter: 1357 loss: 3.28579461e-07
Iter: 1358 loss: 3.28554876e-07
Iter: 1359 loss: 3.28559594e-07
Iter: 1360 loss: 3.28534895e-07
Iter: 1361 loss: 3.28830367e-07
Iter: 1362 loss: 3.28533702e-07
Iter: 1363 loss: 3.28508435e-07
Iter: 1364 loss: 3.28467365e-07
Iter: 1365 loss: 3.28473618e-07
Iter: 1366 loss: 3.28442042e-07
Iter: 1367 loss: 3.28447697e-07
Iter: 1368 loss: 3.28416093e-07
Iter: 1369 loss: 3.28395856e-07
Iter: 1370 loss: 3.28395657e-07
Iter: 1371 loss: 3.28382214e-07
Iter: 1372 loss: 3.28393469e-07
Iter: 1373 loss: 3.28383777e-07
Iter: 1374 loss: 3.28361125e-07
Iter: 1375 loss: 3.28354645e-07
Iter: 1376 loss: 3.2834879e-07
Iter: 1377 loss: 3.2832213e-07
Iter: 1378 loss: 3.28321789e-07
Iter: 1379 loss: 3.28296778e-07
Iter: 1380 loss: 3.28279071e-07
Iter: 1381 loss: 3.28445168e-07
Iter: 1382 loss: 3.28273984e-07
Iter: 1383 loss: 3.2825335e-07
Iter: 1384 loss: 3.28350211e-07
Iter: 1385 loss: 3.28250053e-07
Iter: 1386 loss: 3.28242152e-07
Iter: 1387 loss: 3.28254259e-07
Iter: 1388 loss: 3.28222882e-07
Iter: 1389 loss: 3.28215521e-07
Iter: 1390 loss: 3.28367491e-07
Iter: 1391 loss: 3.28216402e-07
Iter: 1392 loss: 3.28200571e-07
Iter: 1393 loss: 3.2820887e-07
Iter: 1394 loss: 3.28187298e-07
Iter: 1395 loss: 3.28159558e-07
Iter: 1396 loss: 3.28201622e-07
Iter: 1397 loss: 3.28154414e-07
Iter: 1398 loss: 3.28128294e-07
Iter: 1399 loss: 3.28113742e-07
Iter: 1400 loss: 3.28099816e-07
Iter: 1401 loss: 3.28063066e-07
Iter: 1402 loss: 3.28486976e-07
Iter: 1403 loss: 3.28054398e-07
Iter: 1404 loss: 3.28049225e-07
Iter: 1405 loss: 3.28022168e-07
Iter: 1406 loss: 3.28022423e-07
Iter: 1407 loss: 3.28000453e-07
Iter: 1408 loss: 3.28285921e-07
Iter: 1409 loss: 3.27986925e-07
Iter: 1410 loss: 3.27978796e-07
Iter: 1411 loss: 3.27982434e-07
Iter: 1412 loss: 3.27981496e-07
Iter: 1413 loss: 3.27955377e-07
Iter: 1414 loss: 3.27995252e-07
Iter: 1415 loss: 3.27942928e-07
Iter: 1416 loss: 3.27918485e-07
Iter: 1417 loss: 3.28045189e-07
Iter: 1418 loss: 3.27911266e-07
Iter: 1419 loss: 3.27884266e-07
Iter: 1420 loss: 3.27924056e-07
Iter: 1421 loss: 3.27863688e-07
Iter: 1422 loss: 3.27843793e-07
Iter: 1423 loss: 3.28064971e-07
Iter: 1424 loss: 3.27847687e-07
Iter: 1425 loss: 3.27819151e-07
Iter: 1426 loss: 3.27791923e-07
Iter: 1427 loss: 3.27789337e-07
Iter: 1428 loss: 3.27745056e-07
Iter: 1429 loss: 3.28350296e-07
Iter: 1430 loss: 3.27750797e-07
Iter: 1431 loss: 3.27721295e-07
Iter: 1432 loss: 3.27720443e-07
Iter: 1433 loss: 3.27708847e-07
Iter: 1434 loss: 3.27692476e-07
Iter: 1435 loss: 3.27695e-07
Iter: 1436 loss: 3.27670961e-07
Iter: 1437 loss: 3.27647967e-07
Iter: 1438 loss: 3.28157739e-07
Iter: 1439 loss: 3.27643704e-07
Iter: 1440 loss: 3.27610337e-07
Iter: 1441 loss: 3.27720812e-07
Iter: 1442 loss: 3.27612526e-07
Iter: 1443 loss: 3.27600532e-07
Iter: 1444 loss: 3.27595899e-07
Iter: 1445 loss: 3.27575606e-07
Iter: 1446 loss: 3.27549799e-07
Iter: 1447 loss: 3.2751143e-07
Iter: 1448 loss: 3.27525896e-07
Iter: 1449 loss: 3.27486532e-07
Iter: 1450 loss: 3.27488351e-07
Iter: 1451 loss: 3.27467433e-07
Iter: 1452 loss: 3.27435032e-07
Iter: 1453 loss: 3.27479569e-07
Iter: 1454 loss: 3.27427131e-07
Iter: 1455 loss: 3.27391945e-07
Iter: 1456 loss: 3.2747937e-07
Iter: 1457 loss: 3.27379155e-07
Iter: 1458 loss: 3.27359686e-07
Iter: 1459 loss: 3.27407889e-07
Iter: 1460 loss: 3.27347493e-07
Iter: 1461 loss: 3.27319782e-07
Iter: 1462 loss: 3.27554034e-07
Iter: 1463 loss: 3.27316968e-07
Iter: 1464 loss: 3.27290365e-07
Iter: 1465 loss: 3.27292042e-07
Iter: 1466 loss: 3.27277064e-07
Iter: 1467 loss: 3.27260835e-07
Iter: 1468 loss: 3.27353661e-07
Iter: 1469 loss: 3.27253133e-07
Iter: 1470 loss: 3.27226303e-07
Iter: 1471 loss: 3.27285306e-07
Iter: 1472 loss: 3.27218174e-07
Iter: 1473 loss: 3.27196432e-07
Iter: 1474 loss: 3.27187649e-07
Iter: 1475 loss: 3.27164344e-07
Iter: 1476 loss: 3.27140299e-07
Iter: 1477 loss: 3.27247221e-07
Iter: 1478 loss: 3.27133705e-07
Iter: 1479 loss: 3.2711597e-07
Iter: 1480 loss: 3.27136775e-07
Iter: 1481 loss: 3.2709525e-07
Iter: 1482 loss: 3.27057421e-07
Iter: 1483 loss: 3.27316258e-07
Iter: 1484 loss: 3.27068335e-07
Iter: 1485 loss: 3.27035281e-07
Iter: 1486 loss: 3.2701098e-07
Iter: 1487 loss: 3.27002425e-07
Iter: 1488 loss: 3.26967438e-07
Iter: 1489 loss: 3.27226246e-07
Iter: 1490 loss: 3.2696974e-07
Iter: 1491 loss: 3.26956e-07
Iter: 1492 loss: 3.269638e-07
Iter: 1493 loss: 3.26917927e-07
Iter: 1494 loss: 3.26889648e-07
Iter: 1495 loss: 3.2708374e-07
Iter: 1496 loss: 3.26899112e-07
Iter: 1497 loss: 3.26869497e-07
Iter: 1498 loss: 3.26905251e-07
Iter: 1499 loss: 3.26858043e-07
Iter: 1500 loss: 3.26831952e-07
Iter: 1501 loss: 3.26836897e-07
Iter: 1502 loss: 3.26806401e-07
Iter: 1503 loss: 3.26779855e-07
Iter: 1504 loss: 3.26784203e-07
Iter: 1505 loss: 3.26768145e-07
Iter: 1506 loss: 3.26733e-07
Iter: 1507 loss: 3.26742622e-07
Iter: 1508 loss: 3.26704907e-07
Iter: 1509 loss: 3.26788268e-07
Iter: 1510 loss: 3.26692401e-07
Iter: 1511 loss: 3.2664633e-07
Iter: 1512 loss: 3.26928841e-07
Iter: 1513 loss: 3.26650536e-07
Iter: 1514 loss: 3.26628651e-07
Iter: 1515 loss: 3.2664866e-07
Iter: 1516 loss: 3.26615805e-07
Iter: 1517 loss: 3.26581215e-07
Iter: 1518 loss: 3.26594431e-07
Iter: 1519 loss: 3.26551856e-07
Iter: 1520 loss: 3.26506495e-07
Iter: 1521 loss: 3.26492852e-07
Iter: 1522 loss: 3.26460679e-07
Iter: 1523 loss: 3.26421969e-07
Iter: 1524 loss: 3.26431405e-07
Iter: 1525 loss: 3.26415147e-07
Iter: 1526 loss: 3.26425493e-07
Iter: 1527 loss: 3.26396503e-07
Iter: 1528 loss: 3.26379251e-07
Iter: 1529 loss: 3.26621858e-07
Iter: 1530 loss: 3.26387521e-07
Iter: 1531 loss: 3.26357622e-07
Iter: 1532 loss: 3.26362738e-07
Iter: 1533 loss: 3.26346395e-07
Iter: 1534 loss: 3.26315416e-07
Iter: 1535 loss: 3.26472986e-07
Iter: 1536 loss: 3.26325477e-07
Iter: 1537 loss: 3.26304502e-07
Iter: 1538 loss: 3.26312716e-07
Iter: 1539 loss: 3.26301176e-07
Iter: 1540 loss: 3.26280286e-07
Iter: 1541 loss: 3.26284919e-07
Iter: 1542 loss: 3.26250415e-07
Iter: 1543 loss: 3.26223244e-07
Iter: 1544 loss: 3.2650081e-07
Iter: 1545 loss: 3.26231429e-07
Iter: 1546 loss: 3.26198e-07
Iter: 1547 loss: 3.26251438e-07
Iter: 1548 loss: 3.26181862e-07
Iter: 1549 loss: 3.26161597e-07
Iter: 1550 loss: 3.26283327e-07
Iter: 1551 loss: 3.26160944e-07
Iter: 1552 loss: 3.26139741e-07
Iter: 1553 loss: 3.26127491e-07
Iter: 1554 loss: 3.26113479e-07
Iter: 1555 loss: 3.26098984e-07
Iter: 1556 loss: 3.26343184e-07
Iter: 1557 loss: 3.26105152e-07
Iter: 1558 loss: 3.26095403e-07
Iter: 1559 loss: 3.26084091e-07
Iter: 1560 loss: 3.26084546e-07
Iter: 1561 loss: 3.26063628e-07
Iter: 1562 loss: 3.26224267e-07
Iter: 1563 loss: 3.26049246e-07
Iter: 1564 loss: 3.26040634e-07
Iter: 1565 loss: 3.26022757e-07
Iter: 1566 loss: 3.26031056e-07
Iter: 1567 loss: 3.26002834e-07
Iter: 1568 loss: 3.26222334e-07
Iter: 1569 loss: 3.25999878e-07
Iter: 1570 loss: 3.25983024e-07
Iter: 1571 loss: 3.25968699e-07
Iter: 1572 loss: 3.25975e-07
Iter: 1573 loss: 3.25923622e-07
Iter: 1574 loss: 3.26021024e-07
Iter: 1575 loss: 3.25921974e-07
Iter: 1576 loss: 3.25907507e-07
Iter: 1577 loss: 3.26097819e-07
Iter: 1578 loss: 3.25904637e-07
Iter: 1579 loss: 3.25888664e-07
Iter: 1580 loss: 3.25914385e-07
Iter: 1581 loss: 3.25882155e-07
Iter: 1582 loss: 3.25862061e-07
Iter: 1583 loss: 3.25923793e-07
Iter: 1584 loss: 3.25862203e-07
Iter: 1585 loss: 3.25838812e-07
Iter: 1586 loss: 3.2583381e-07
Iter: 1587 loss: 3.25816188e-07
Iter: 1588 loss: 3.25799419e-07
Iter: 1589 loss: 3.25837902e-07
Iter: 1590 loss: 3.25800954e-07
Iter: 1591 loss: 3.25775432e-07
Iter: 1592 loss: 3.25879228e-07
Iter: 1593 loss: 3.25757725e-07
Iter: 1594 loss: 3.25722255e-07
Iter: 1595 loss: 3.25859e-07
Iter: 1596 loss: 3.25724471e-07
Iter: 1597 loss: 3.25688e-07
Iter: 1598 loss: 3.25796e-07
Iter: 1599 loss: 3.25691161e-07
Iter: 1600 loss: 3.2566254e-07
Iter: 1601 loss: 3.25823e-07
Iter: 1602 loss: 3.25673966e-07
Iter: 1603 loss: 3.25654128e-07
Iter: 1604 loss: 3.25634517e-07
Iter: 1605 loss: 3.25637473e-07
Iter: 1606 loss: 3.2561465e-07
Iter: 1607 loss: 3.25674705e-07
Iter: 1608 loss: 3.25610927e-07
Iter: 1609 loss: 3.25592282e-07
Iter: 1610 loss: 3.25644464e-07
Iter: 1611 loss: 3.25584665e-07
Iter: 1612 loss: 3.25560904e-07
Iter: 1613 loss: 3.25647932e-07
Iter: 1614 loss: 3.25567271e-07
Iter: 1615 loss: 3.25551184e-07
Iter: 1616 loss: 3.25601775e-07
Iter: 1617 loss: 3.25552264e-07
Iter: 1618 loss: 3.25531914e-07
Iter: 1619 loss: 3.2553362e-07
Iter: 1620 loss: 3.25518045e-07
Iter: 1621 loss: 3.25487918e-07
Iter: 1622 loss: 3.25486752e-07
Iter: 1623 loss: 3.25470126e-07
Iter: 1624 loss: 3.25442443e-07
Iter: 1625 loss: 3.25445455e-07
Iter: 1626 loss: 3.25428e-07
Iter: 1627 loss: 3.25444375e-07
Iter: 1628 loss: 3.25410753e-07
Iter: 1629 loss: 3.25404528e-07
Iter: 1630 loss: 3.25506619e-07
Iter: 1631 loss: 3.25386679e-07
Iter: 1632 loss: 3.2538415e-07
Iter: 1633 loss: 3.25404756e-07
Iter: 1634 loss: 3.25374543e-07
Iter: 1635 loss: 3.2535263e-07
Iter: 1636 loss: 3.25381507e-07
Iter: 1637 loss: 3.25337226e-07
Iter: 1638 loss: 3.25327903e-07
Iter: 1639 loss: 3.25351891e-07
Iter: 1640 loss: 3.25310964e-07
Iter: 1641 loss: 3.25308491e-07
Iter: 1642 loss: 3.25332024e-07
Iter: 1643 loss: 3.25303461e-07
Iter: 1644 loss: 3.25280467e-07
Iter: 1645 loss: 3.25391824e-07
Iter: 1646 loss: 3.25279615e-07
Iter: 1647 loss: 3.25268303e-07
Iter: 1648 loss: 3.2529482e-07
Iter: 1649 loss: 3.25272623e-07
Iter: 1650 loss: 3.25257e-07
Iter: 1651 loss: 3.25259776e-07
Iter: 1652 loss: 3.25255655e-07
Iter: 1653 loss: 3.25229763e-07
Iter: 1654 loss: 3.25222231e-07
Iter: 1655 loss: 3.25212824e-07
Iter: 1656 loss: 3.252e-07
Iter: 1657 loss: 3.25283793e-07
Iter: 1658 loss: 3.25195771e-07
Iter: 1659 loss: 3.25165075e-07
Iter: 1660 loss: 3.25207111e-07
Iter: 1661 loss: 3.2514464e-07
Iter: 1662 loss: 3.25126166e-07
Iter: 1663 loss: 3.25369854e-07
Iter: 1664 loss: 3.25129463e-07
Iter: 1665 loss: 3.25110562e-07
Iter: 1666 loss: 3.25127985e-07
Iter: 1667 loss: 3.25116957e-07
Iter: 1668 loss: 3.25104679e-07
Iter: 1669 loss: 3.25136909e-07
Iter: 1670 loss: 3.25093396e-07
Iter: 1671 loss: 3.25082397e-07
Iter: 1672 loss: 3.25075973e-07
Iter: 1673 loss: 3.2508234e-07
Iter: 1674 loss: 3.25068356e-07
Iter: 1675 loss: 3.25092287e-07
Iter: 1676 loss: 3.25063866e-07
Iter: 1677 loss: 3.25043658e-07
Iter: 1678 loss: 3.25076599e-07
Iter: 1679 loss: 3.25039508e-07
Iter: 1680 loss: 3.25027685e-07
Iter: 1681 loss: 3.25062e-07
Iter: 1682 loss: 3.25026349e-07
Iter: 1683 loss: 3.25006681e-07
Iter: 1684 loss: 3.25017766e-07
Iter: 1685 loss: 3.25000087e-07
Iter: 1686 loss: 3.24977805e-07
Iter: 1687 loss: 3.25005857e-07
Iter: 1688 loss: 3.24982864e-07
Iter: 1689 loss: 3.24960297e-07
Iter: 1690 loss: 3.25010205e-07
Iter: 1691 loss: 3.2495106e-07
Iter: 1692 loss: 3.24925196e-07
Iter: 1693 loss: 3.25083107e-07
Iter: 1694 loss: 3.24909081e-07
Iter: 1695 loss: 3.2490496e-07
Iter: 1696 loss: 3.25000627e-07
Iter: 1697 loss: 3.24908342e-07
Iter: 1698 loss: 3.24892085e-07
Iter: 1699 loss: 3.24885661e-07
Iter: 1700 loss: 3.24880261e-07
Iter: 1701 loss: 3.24869347e-07
Iter: 1702 loss: 3.24965868e-07
Iter: 1703 loss: 3.24848344e-07
Iter: 1704 loss: 3.24836435e-07
Iter: 1705 loss: 3.24838936e-07
Iter: 1706 loss: 3.24838311e-07
Iter: 1707 loss: 3.24823475e-07
Iter: 1708 loss: 3.24954442e-07
Iter: 1709 loss: 3.2482285e-07
Iter: 1710 loss: 3.24809719e-07
Iter: 1711 loss: 3.24804944e-07
Iter: 1712 loss: 3.24812362e-07
Iter: 1713 loss: 3.24792097e-07
Iter: 1714 loss: 3.2487327e-07
Iter: 1715 loss: 3.24791245e-07
Iter: 1716 loss: 3.24779251e-07
Iter: 1717 loss: 3.24761714e-07
Iter: 1718 loss: 3.24759128e-07
Iter: 1719 loss: 3.24729655e-07
Iter: 1720 loss: 3.24742871e-07
Iter: 1721 loss: 3.24711891e-07
Iter: 1722 loss: 3.24684748e-07
Iter: 1723 loss: 3.24752023e-07
Iter: 1724 loss: 3.24689552e-07
Iter: 1725 loss: 3.24647544e-07
Iter: 1726 loss: 3.24894017e-07
Iter: 1727 loss: 3.24644304e-07
Iter: 1728 loss: 3.24621368e-07
Iter: 1729 loss: 3.24692508e-07
Iter: 1730 loss: 3.24614803e-07
Iter: 1731 loss: 3.24590019e-07
Iter: 1732 loss: 3.24701944e-07
Iter: 1733 loss: 3.24594936e-07
Iter: 1734 loss: 3.24582686e-07
Iter: 1735 loss: 3.24666416e-07
Iter: 1736 loss: 3.24579901e-07
Iter: 1737 loss: 3.24568646e-07
Iter: 1738 loss: 3.2455273e-07
Iter: 1739 loss: 3.24547329e-07
Iter: 1740 loss: 3.24533403e-07
Iter: 1741 loss: 3.24703109e-07
Iter: 1742 loss: 3.24529651e-07
Iter: 1743 loss: 3.24517828e-07
Iter: 1744 loss: 3.24513849e-07
Iter: 1745 loss: 3.24516066e-07
Iter: 1746 loss: 3.24488184e-07
Iter: 1747 loss: 3.24632339e-07
Iter: 1748 loss: 3.24484745e-07
Iter: 1749 loss: 3.24467464e-07
Iter: 1750 loss: 3.24469539e-07
Iter: 1751 loss: 3.24453481e-07
Iter: 1752 loss: 3.24426622e-07
Iter: 1753 loss: 3.24464594e-07
Iter: 1754 loss: 3.24430573e-07
Iter: 1755 loss: 3.24395216e-07
Iter: 1756 loss: 3.24409939e-07
Iter: 1757 loss: 3.24380267e-07
Iter: 1758 loss: 3.24348633e-07
Iter: 1759 loss: 3.24352754e-07
Iter: 1760 loss: 3.24343432e-07
Iter: 1761 loss: 3.24343546e-07
Iter: 1762 loss: 3.24324134e-07
Iter: 1763 loss: 3.24308701e-07
Iter: 1764 loss: 3.24405448e-07
Iter: 1765 loss: 3.24298867e-07
Iter: 1766 loss: 3.24293694e-07
Iter: 1767 loss: 3.2429071e-07
Iter: 1768 loss: 3.24285367e-07
Iter: 1769 loss: 3.24254586e-07
Iter: 1770 loss: 3.24287385e-07
Iter: 1771 loss: 3.24238698e-07
Iter: 1772 loss: 3.24224061e-07
Iter: 1773 loss: 3.24351163e-07
Iter: 1774 loss: 3.24216956e-07
Iter: 1775 loss: 3.24196833e-07
Iter: 1776 loss: 3.24202176e-07
Iter: 1777 loss: 3.24176369e-07
Iter: 1778 loss: 3.24157924e-07
Iter: 1779 loss: 3.24295399e-07
Iter: 1780 loss: 3.24157895e-07
Iter: 1781 loss: 3.24162045e-07
Iter: 1782 loss: 3.24165853e-07
Iter: 1783 loss: 3.24156474e-07
Iter: 1784 loss: 3.24156446e-07
Iter: 1785 loss: 3.24159146e-07
Iter: 1786 loss: 3.24162443e-07
Iter: 1787 loss: 3.24164375e-07
Iter: 1788 loss: 3.24160823e-07
Iter: 1789 loss: 3.24155792e-07
Iter: 1790 loss: 3.24159117e-07
Iter: 1791 loss: 3.24162443e-07
Iter: 1792 loss: 3.24157639e-07
Iter: 1793 loss: 3.2416051e-07
Iter: 1794 loss: 3.241652e-07
Iter: 1795 loss: 3.24160339e-07
Iter: 1796 loss: 3.24159686e-07
Iter: 1797 loss: 3.24158577e-07
Iter: 1798 loss: 3.24158123e-07
Iter: 1799 loss: 3.24158037e-07
Iter: 1800 loss: 3.24157583e-07
Iter: 1801 loss: 3.24157725e-07
Iter: 1802 loss: 3.24158577e-07
Iter: 1803 loss: 3.24157782e-07
Iter: 1804 loss: 3.24158577e-07
Iter: 1805 loss: 3.24158577e-07
Iter: 1806 loss: 3.2415781e-07
Iter: 1807 loss: 3.24158577e-07
Iter: 1808 loss: 3.24129303e-07
Iter: 1809 loss: 3.24338089e-07
Iter: 1810 loss: 3.2411765e-07
Iter: 1811 loss: 3.24116911e-07
Iter: 1812 loss: 3.2409767e-07
Iter: 1813 loss: 3.24686454e-07
Iter: 1814 loss: 3.24097357e-07
Iter: 1815 loss: 3.24078371e-07
Iter: 1816 loss: 3.24289886e-07
Iter: 1817 loss: 3.24064956e-07
Iter: 1818 loss: 3.24030907e-07
Iter: 1819 loss: 3.24075756e-07
Iter: 1820 loss: 3.24019254e-07
Iter: 1821 loss: 3.23998677e-07
Iter: 1822 loss: 3.2428818e-07
Iter: 1823 loss: 3.23999643e-07
Iter: 1824 loss: 3.23986711e-07
Iter: 1825 loss: 3.2399592e-07
Iter: 1826 loss: 3.23956669e-07
Iter: 1827 loss: 3.23946807e-07
Iter: 1828 loss: 3.240458e-07
Iter: 1829 loss: 3.23943652e-07
Iter: 1830 loss: 3.23937599e-07
Iter: 1831 loss: 3.23963434e-07
Iter: 1832 loss: 3.23915543e-07
Iter: 1833 loss: 3.23904487e-07
Iter: 1834 loss: 3.23887036e-07
Iter: 1835 loss: 3.23883285e-07
Iter: 1836 loss: 3.23879533e-07
Iter: 1837 loss: 3.23856057e-07
Iter: 1838 loss: 3.2384844e-07
Iter: 1839 loss: 3.23810781e-07
Iter: 1840 loss: 3.24038581e-07
Iter: 1841 loss: 3.23815243e-07
Iter: 1842 loss: 3.23781933e-07
Iter: 1843 loss: 3.24004816e-07
Iter: 1844 loss: 3.23784263e-07
Iter: 1845 loss: 3.23758172e-07
Iter: 1846 loss: 3.23751408e-07
Iter: 1847 loss: 3.2375354e-07
Iter: 1848 loss: 3.2372634e-07
Iter: 1849 loss: 3.23758911e-07
Iter: 1850 loss: 3.23712811e-07
Iter: 1851 loss: 3.236878e-07
Iter: 1852 loss: 3.23922507e-07
Iter: 1853 loss: 3.2368871e-07
Iter: 1854 loss: 3.23665546e-07
Iter: 1855 loss: 3.23780228e-07
Iter: 1856 loss: 3.23661368e-07
Iter: 1857 loss: 3.23636755e-07
Iter: 1858 loss: 3.23635732e-07
Iter: 1859 loss: 3.23618366e-07
Iter: 1860 loss: 3.23615041e-07
Iter: 1861 loss: 3.23613364e-07
Iter: 1862 loss: 3.23602308e-07
Iter: 1863 loss: 3.2357309e-07
Iter: 1864 loss: 3.23580906e-07
Iter: 1865 loss: 3.23559675e-07
Iter: 1866 loss: 3.23564166e-07
Iter: 1867 loss: 3.23548534e-07
Iter: 1868 loss: 3.23535431e-07
Iter: 1869 loss: 3.23532049e-07
Iter: 1870 loss: 3.23511301e-07
Iter: 1871 loss: 3.2356769e-07
Iter: 1872 loss: 3.23509141e-07
Iter: 1873 loss: 3.23477e-07
Iter: 1874 loss: 3.23489161e-07
Iter: 1875 loss: 3.23452042e-07
Iter: 1876 loss: 3.23429504e-07
Iter: 1877 loss: 3.23495271e-07
Iter: 1878 loss: 3.23417026e-07
Iter: 1879 loss: 3.23378202e-07
Iter: 1880 loss: 3.23360581e-07
Iter: 1881 loss: 3.23342476e-07
Iter: 1882 loss: 3.2330621e-07
Iter: 1883 loss: 3.23528297e-07
Iter: 1884 loss: 3.23278186e-07
Iter: 1885 loss: 3.23234843e-07
Iter: 1886 loss: 3.23610664e-07
Iter: 1887 loss: 3.23253147e-07
Iter: 1888 loss: 3.23223446e-07
Iter: 1889 loss: 3.23334177e-07
Iter: 1890 loss: 3.23214636e-07
Iter: 1891 loss: 3.23183343e-07
Iter: 1892 loss: 3.23198549e-07
Iter: 1893 loss: 3.23169786e-07
Iter: 1894 loss: 3.23140711e-07
Iter: 1895 loss: 3.23320307e-07
Iter: 1896 loss: 3.23136362e-07
Iter: 1897 loss: 3.23122208e-07
Iter: 1898 loss: 3.23086255e-07
Iter: 1899 loss: 3.23990889e-07
Iter: 1900 loss: 3.23092905e-07
Iter: 1901 loss: 3.23069855e-07
Iter: 1902 loss: 3.23061556e-07
Iter: 1903 loss: 3.23051268e-07
Iter: 1904 loss: 3.23041519e-07
Iter: 1905 loss: 3.23036346e-07
Iter: 1906 loss: 3.23012159e-07
Iter: 1907 loss: 3.23189397e-07
Iter: 1908 loss: 3.23001871e-07
Iter: 1909 loss: 3.22968901e-07
Iter: 1910 loss: 3.23059282e-07
Iter: 1911 loss: 3.22963785e-07
Iter: 1912 loss: 3.22947415e-07
Iter: 1913 loss: 3.22940082e-07
Iter: 1914 loss: 3.22923768e-07
Iter: 1915 loss: 3.22894209e-07
Iter: 1916 loss: 3.22967708e-07
Iter: 1917 loss: 3.22893726e-07
Iter: 1918 loss: 3.2286772e-07
Iter: 1919 loss: 3.23063659e-07
Iter: 1920 loss: 3.2285368e-07
Iter: 1921 loss: 3.22837508e-07
Iter: 1922 loss: 3.22930873e-07
Iter: 1923 loss: 3.22848592e-07
Iter: 1924 loss: 3.22803089e-07
Iter: 1925 loss: 3.22813236e-07
Iter: 1926 loss: 3.22798144e-07
Iter: 1927 loss: 3.22774298e-07
Iter: 1928 loss: 3.23028928e-07
Iter: 1929 loss: 3.22767761e-07
Iter: 1930 loss: 3.22743233e-07
Iter: 1931 loss: 3.22707535e-07
Iter: 1932 loss: 3.23635305e-07
Iter: 1933 loss: 3.22710321e-07
Iter: 1934 loss: 3.22665869e-07
Iter: 1935 loss: 3.23047658e-07
Iter: 1936 loss: 3.22673657e-07
Iter: 1937 loss: 3.22620508e-07
Iter: 1938 loss: 3.22597145e-07
Iter: 1939 loss: 3.22584839e-07
Iter: 1940 loss: 3.22542292e-07
Iter: 1941 loss: 3.22731069e-07
Iter: 1942 loss: 3.22526631e-07
Iter: 1943 loss: 3.22471323e-07
Iter: 1944 loss: 3.22772053e-07
Iter: 1945 loss: 3.22476637e-07
Iter: 1946 loss: 3.22453445e-07
Iter: 1947 loss: 3.22440712e-07
Iter: 1948 loss: 3.22403622e-07
Iter: 1949 loss: 3.22363491e-07
Iter: 1950 loss: 3.22436733e-07
Iter: 1951 loss: 3.22366816e-07
Iter: 1952 loss: 3.22324894e-07
Iter: 1953 loss: 3.2232407e-07
Iter: 1954 loss: 3.22298831e-07
Iter: 1955 loss: 3.22360336e-07
Iter: 1956 loss: 3.22291783e-07
Iter: 1957 loss: 3.2226751e-07
Iter: 1958 loss: 3.22295705e-07
Iter: 1959 loss: 3.22251935e-07
Iter: 1960 loss: 3.22221354e-07
Iter: 1961 loss: 3.22342061e-07
Iter: 1962 loss: 3.22224054e-07
Iter: 1963 loss: 3.22199526e-07
Iter: 1964 loss: 3.22162975e-07
Iter: 1965 loss: 3.22145127e-07
Iter: 1966 loss: 3.22111e-07
Iter: 1967 loss: 3.22205693e-07
Iter: 1968 loss: 3.22102267e-07
Iter: 1969 loss: 3.22061851e-07
Iter: 1970 loss: 3.22357e-07
Iter: 1971 loss: 3.22047782e-07
Iter: 1972 loss: 3.22027319e-07
Iter: 1973 loss: 3.22029791e-07
Iter: 1974 loss: 3.22007793e-07
Iter: 1975 loss: 3.2198335e-07
Iter: 1976 loss: 3.21989774e-07
Iter: 1977 loss: 3.21969651e-07
Iter: 1978 loss: 3.21938217e-07
Iter: 1979 loss: 3.21935886e-07
Iter: 1980 loss: 3.21895186e-07
Iter: 1981 loss: 3.21942593e-07
Iter: 1982 loss: 3.21869e-07
Iter: 1983 loss: 3.21832943e-07
Iter: 1984 loss: 3.22312189e-07
Iter: 1985 loss: 3.21840616e-07
Iter: 1986 loss: 3.21800684e-07
Iter: 1987 loss: 3.21914428e-07
Iter: 1988 loss: 3.21800741e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8
+ date
Tue Oct 20 16:45:37 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi0.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3a009772f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3a008dc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3a0091e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3a008f0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3a7ae85e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3a008b7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3a0086c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39dc118a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39dc1188c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3a0086c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39dc118c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39dc098d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39dc0a1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39dc06f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39dc0a1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d4698b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d4698ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d4671048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39dc06f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d4698620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d45f62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d45870d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d45f6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d45629d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d4562d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d4562e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d4562840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d44df950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d44dff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d44717b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d44a1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d43f8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d43f8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d4440d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d43d10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f39d43e9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.77153083e-06
Iter: 2 loss: 1.28600686e-06
Iter: 3 loss: 1.21609196e-06
Iter: 4 loss: 9.38019809e-07
Iter: 5 loss: 1.16595504e-06
Iter: 6 loss: 7.71786176e-07
Iter: 7 loss: 6.86542705e-07
Iter: 8 loss: 1.74481397e-06
Iter: 9 loss: 6.85713246e-07
Iter: 10 loss: 6.10889515e-07
Iter: 11 loss: 6.28776547e-07
Iter: 12 loss: 5.56074156e-07
Iter: 13 loss: 5.2761186e-07
Iter: 14 loss: 9.65110416e-07
Iter: 15 loss: 5.27599923e-07
Iter: 16 loss: 5.08316418e-07
Iter: 17 loss: 5.66921926e-07
Iter: 18 loss: 5.02559942e-07
Iter: 19 loss: 4.91209732e-07
Iter: 20 loss: 4.95463212e-07
Iter: 21 loss: 4.83292297e-07
Iter: 22 loss: 4.6944146e-07
Iter: 23 loss: 5.48975606e-07
Iter: 24 loss: 4.67584357e-07
Iter: 25 loss: 4.52073579e-07
Iter: 26 loss: 4.8791378e-07
Iter: 27 loss: 4.46299794e-07
Iter: 28 loss: 4.35954519e-07
Iter: 29 loss: 4.22572356e-07
Iter: 30 loss: 4.21656864e-07
Iter: 31 loss: 4.06963068e-07
Iter: 32 loss: 4.54171641e-07
Iter: 33 loss: 4.02788885e-07
Iter: 34 loss: 3.93528637e-07
Iter: 35 loss: 4.72503729e-07
Iter: 36 loss: 3.93002892e-07
Iter: 37 loss: 3.87699515e-07
Iter: 38 loss: 3.9038224e-07
Iter: 39 loss: 3.84111587e-07
Iter: 40 loss: 3.81051166e-07
Iter: 41 loss: 3.80189732e-07
Iter: 42 loss: 3.7612503e-07
Iter: 43 loss: 3.79056246e-07
Iter: 44 loss: 3.73602973e-07
Iter: 45 loss: 3.70443e-07
Iter: 46 loss: 3.71818487e-07
Iter: 47 loss: 3.68279302e-07
Iter: 48 loss: 3.61780394e-07
Iter: 49 loss: 3.72149628e-07
Iter: 50 loss: 3.58739328e-07
Iter: 51 loss: 3.55648922e-07
Iter: 52 loss: 3.6303544e-07
Iter: 53 loss: 3.54505573e-07
Iter: 54 loss: 3.5158692e-07
Iter: 55 loss: 3.85777838e-07
Iter: 56 loss: 3.51555713e-07
Iter: 57 loss: 3.50384397e-07
Iter: 58 loss: 3.48577515e-07
Iter: 59 loss: 3.48555631e-07
Iter: 60 loss: 3.47936805e-07
Iter: 61 loss: 3.47539839e-07
Iter: 62 loss: 3.46657572e-07
Iter: 63 loss: 3.44622492e-07
Iter: 64 loss: 3.70225735e-07
Iter: 65 loss: 3.44470664e-07
Iter: 66 loss: 3.42072155e-07
Iter: 67 loss: 3.46454556e-07
Iter: 68 loss: 3.41043403e-07
Iter: 69 loss: 3.38013422e-07
Iter: 70 loss: 3.43394163e-07
Iter: 71 loss: 3.36685389e-07
Iter: 72 loss: 3.33872521e-07
Iter: 73 loss: 3.38407403e-07
Iter: 74 loss: 3.32579816e-07
Iter: 75 loss: 3.32000866e-07
Iter: 76 loss: 3.31352851e-07
Iter: 77 loss: 3.30104115e-07
Iter: 78 loss: 3.28790691e-07
Iter: 79 loss: 3.28577585e-07
Iter: 80 loss: 3.27263763e-07
Iter: 81 loss: 3.33774949e-07
Iter: 82 loss: 3.27060462e-07
Iter: 83 loss: 3.25352744e-07
Iter: 84 loss: 3.27309891e-07
Iter: 85 loss: 3.24452543e-07
Iter: 86 loss: 3.23082247e-07
Iter: 87 loss: 3.22043263e-07
Iter: 88 loss: 3.21588914e-07
Iter: 89 loss: 3.19683181e-07
Iter: 90 loss: 3.19671358e-07
Iter: 91 loss: 3.1890761e-07
Iter: 92 loss: 3.17326339e-07
Iter: 93 loss: 3.44408022e-07
Iter: 94 loss: 3.17280183e-07
Iter: 95 loss: 3.16985449e-07
Iter: 96 loss: 3.16478861e-07
Iter: 97 loss: 3.15878765e-07
Iter: 98 loss: 3.15023954e-07
Iter: 99 loss: 3.14993542e-07
Iter: 100 loss: 3.14193244e-07
Iter: 101 loss: 3.14738458e-07
Iter: 102 loss: 3.13670682e-07
Iter: 103 loss: 3.1259475e-07
Iter: 104 loss: 3.1713e-07
Iter: 105 loss: 3.12384174e-07
Iter: 106 loss: 3.11221015e-07
Iter: 107 loss: 3.10651899e-07
Iter: 108 loss: 3.10124562e-07
Iter: 109 loss: 3.08767596e-07
Iter: 110 loss: 3.21939211e-07
Iter: 111 loss: 3.087153e-07
Iter: 112 loss: 3.07540517e-07
Iter: 113 loss: 3.21740515e-07
Iter: 114 loss: 3.07523521e-07
Iter: 115 loss: 3.07009259e-07
Iter: 116 loss: 3.06144727e-07
Iter: 117 loss: 3.0614e-07
Iter: 118 loss: 3.05469314e-07
Iter: 119 loss: 3.05416563e-07
Iter: 120 loss: 3.05035485e-07
Iter: 121 loss: 3.0424826e-07
Iter: 122 loss: 3.17323241e-07
Iter: 123 loss: 3.04216741e-07
Iter: 124 loss: 3.03510944e-07
Iter: 125 loss: 3.12419473e-07
Iter: 126 loss: 3.03507164e-07
Iter: 127 loss: 3.02659544e-07
Iter: 128 loss: 3.0164108e-07
Iter: 129 loss: 3.01551438e-07
Iter: 130 loss: 3.00775781e-07
Iter: 131 loss: 3.06566392e-07
Iter: 132 loss: 3.00698105e-07
Iter: 133 loss: 2.99697945e-07
Iter: 134 loss: 3.00110884e-07
Iter: 135 loss: 2.98993768e-07
Iter: 136 loss: 2.9836491e-07
Iter: 137 loss: 2.9818159e-07
Iter: 138 loss: 2.97788404e-07
Iter: 139 loss: 2.97135273e-07
Iter: 140 loss: 2.99798074e-07
Iter: 141 loss: 2.96998394e-07
Iter: 142 loss: 2.96431239e-07
Iter: 143 loss: 2.9790283e-07
Iter: 144 loss: 2.96227938e-07
Iter: 145 loss: 2.95673743e-07
Iter: 146 loss: 3.00914451e-07
Iter: 147 loss: 2.95652796e-07
Iter: 148 loss: 2.95237129e-07
Iter: 149 loss: 2.99784915e-07
Iter: 150 loss: 2.9523369e-07
Iter: 151 loss: 2.94885695e-07
Iter: 152 loss: 2.94060783e-07
Iter: 153 loss: 3.04560274e-07
Iter: 154 loss: 2.93996237e-07
Iter: 155 loss: 2.93556752e-07
Iter: 156 loss: 2.93515399e-07
Iter: 157 loss: 2.93032599e-07
Iter: 158 loss: 2.92627675e-07
Iter: 159 loss: 2.92486106e-07
Iter: 160 loss: 2.92039914e-07
Iter: 161 loss: 2.92628044e-07
Iter: 162 loss: 2.91799381e-07
Iter: 163 loss: 2.91043705e-07
Iter: 164 loss: 2.93969663e-07
Iter: 165 loss: 2.90860442e-07
Iter: 166 loss: 2.90407343e-07
Iter: 167 loss: 2.90383838e-07
Iter: 168 loss: 2.90034649e-07
Iter: 169 loss: 2.89787778e-07
Iter: 170 loss: 2.89721584e-07
Iter: 171 loss: 2.89472581e-07
Iter: 172 loss: 2.88829142e-07
Iter: 173 loss: 2.94199367e-07
Iter: 174 loss: 2.88716109e-07
Iter: 175 loss: 2.88162795e-07
Iter: 176 loss: 2.89780786e-07
Iter: 177 loss: 2.87991355e-07
Iter: 178 loss: 2.87567332e-07
Iter: 179 loss: 2.88249794e-07
Iter: 180 loss: 2.87384921e-07
Iter: 181 loss: 2.87258302e-07
Iter: 182 loss: 2.8716812e-07
Iter: 183 loss: 2.86950524e-07
Iter: 184 loss: 2.86539063e-07
Iter: 185 loss: 2.95788936e-07
Iter: 186 loss: 2.86534458e-07
Iter: 187 loss: 2.86125328e-07
Iter: 188 loss: 2.88263863e-07
Iter: 189 loss: 2.86060356e-07
Iter: 190 loss: 2.8571796e-07
Iter: 191 loss: 2.87880027e-07
Iter: 192 loss: 2.85681494e-07
Iter: 193 loss: 2.85344413e-07
Iter: 194 loss: 2.84943155e-07
Iter: 195 loss: 2.84907458e-07
Iter: 196 loss: 2.84479142e-07
Iter: 197 loss: 2.86834961e-07
Iter: 198 loss: 2.84418462e-07
Iter: 199 loss: 2.84113213e-07
Iter: 200 loss: 2.88567094e-07
Iter: 201 loss: 2.8410912e-07
Iter: 202 loss: 2.83949674e-07
Iter: 203 loss: 2.83601224e-07
Iter: 204 loss: 2.88607197e-07
Iter: 205 loss: 2.83577435e-07
Iter: 206 loss: 2.8343652e-07
Iter: 207 loss: 2.83358588e-07
Iter: 208 loss: 2.83193515e-07
Iter: 209 loss: 2.82745475e-07
Iter: 210 loss: 2.85539215e-07
Iter: 211 loss: 2.82616412e-07
Iter: 212 loss: 2.82082e-07
Iter: 213 loss: 2.829849e-07
Iter: 214 loss: 2.8186119e-07
Iter: 215 loss: 2.81260981e-07
Iter: 216 loss: 2.83646528e-07
Iter: 217 loss: 2.81134788e-07
Iter: 218 loss: 2.80954509e-07
Iter: 219 loss: 2.80903919e-07
Iter: 220 loss: 2.80679302e-07
Iter: 221 loss: 2.81034062e-07
Iter: 222 loss: 2.80570134e-07
Iter: 223 loss: 2.80444482e-07
Iter: 224 loss: 2.80379e-07
Iter: 225 loss: 2.80335826e-07
Iter: 226 loss: 2.80115103e-07
Iter: 227 loss: 2.82242269e-07
Iter: 228 loss: 2.80105326e-07
Iter: 229 loss: 2.79962535e-07
Iter: 230 loss: 2.79725384e-07
Iter: 231 loss: 2.79722883e-07
Iter: 232 loss: 2.7948937e-07
Iter: 233 loss: 2.82454408e-07
Iter: 234 loss: 2.79491957e-07
Iter: 235 loss: 2.7924915e-07
Iter: 236 loss: 2.79022771e-07
Iter: 237 loss: 2.78978177e-07
Iter: 238 loss: 2.7868316e-07
Iter: 239 loss: 2.8099771e-07
Iter: 240 loss: 2.78654682e-07
Iter: 241 loss: 2.78426654e-07
Iter: 242 loss: 2.79775e-07
Iter: 243 loss: 2.78403832e-07
Iter: 244 loss: 2.78209711e-07
Iter: 245 loss: 2.77881981e-07
Iter: 246 loss: 2.77885135e-07
Iter: 247 loss: 2.7758e-07
Iter: 248 loss: 2.78521043e-07
Iter: 249 loss: 2.77481433e-07
Iter: 250 loss: 2.77204919e-07
Iter: 251 loss: 2.77319259e-07
Iter: 252 loss: 2.77004347e-07
Iter: 253 loss: 2.76738149e-07
Iter: 254 loss: 2.79907e-07
Iter: 255 loss: 2.76738376e-07
Iter: 256 loss: 2.76609683e-07
Iter: 257 loss: 2.76590299e-07
Iter: 258 loss: 2.76509041e-07
Iter: 259 loss: 2.76295879e-07
Iter: 260 loss: 2.78018035e-07
Iter: 261 loss: 2.76261233e-07
Iter: 262 loss: 2.76147347e-07
Iter: 263 loss: 2.76135324e-07
Iter: 264 loss: 2.75996229e-07
Iter: 265 loss: 2.75825073e-07
Iter: 266 loss: 2.75815694e-07
Iter: 267 loss: 2.75611256e-07
Iter: 268 loss: 2.75411963e-07
Iter: 269 loss: 2.75366915e-07
Iter: 270 loss: 2.75294411e-07
Iter: 271 loss: 2.75209459e-07
Iter: 272 loss: 2.75052173e-07
Iter: 273 loss: 2.74709521e-07
Iter: 274 loss: 2.79748861e-07
Iter: 275 loss: 2.74693178e-07
Iter: 276 loss: 2.74576223e-07
Iter: 277 loss: 2.74507528e-07
Iter: 278 loss: 2.74397649e-07
Iter: 279 loss: 2.74177665e-07
Iter: 280 loss: 2.79261883e-07
Iter: 281 loss: 2.74178547e-07
Iter: 282 loss: 2.73950434e-07
Iter: 283 loss: 2.74039138e-07
Iter: 284 loss: 2.73799685e-07
Iter: 285 loss: 2.73646435e-07
Iter: 286 loss: 2.73464792e-07
Iter: 287 loss: 2.73439241e-07
Iter: 288 loss: 2.73167473e-07
Iter: 289 loss: 2.73151187e-07
Iter: 290 loss: 2.72934699e-07
Iter: 291 loss: 2.72656678e-07
Iter: 292 loss: 2.76273568e-07
Iter: 293 loss: 2.72654745e-07
Iter: 294 loss: 2.72400143e-07
Iter: 295 loss: 2.7375296e-07
Iter: 296 loss: 2.72377321e-07
Iter: 297 loss: 2.72232171e-07
Iter: 298 loss: 2.72507322e-07
Iter: 299 loss: 2.72165181e-07
Iter: 300 loss: 2.72046861e-07
Iter: 301 loss: 2.73272661e-07
Iter: 302 loss: 2.72029951e-07
Iter: 303 loss: 2.71925757e-07
Iter: 304 loss: 2.7186735e-07
Iter: 305 loss: 2.71823126e-07
Iter: 306 loss: 2.71658678e-07
Iter: 307 loss: 2.7188895e-07
Iter: 308 loss: 2.71567984e-07
Iter: 309 loss: 2.71424256e-07
Iter: 310 loss: 2.71427e-07
Iter: 311 loss: 2.71308295e-07
Iter: 312 loss: 2.70970787e-07
Iter: 313 loss: 2.73930198e-07
Iter: 314 loss: 2.70940603e-07
Iter: 315 loss: 2.70709393e-07
Iter: 316 loss: 2.70669e-07
Iter: 317 loss: 2.70540198e-07
Iter: 318 loss: 2.70284659e-07
Iter: 319 loss: 2.7619248e-07
Iter: 320 loss: 2.70284147e-07
Iter: 321 loss: 2.70061662e-07
Iter: 322 loss: 2.70678214e-07
Iter: 323 loss: 2.69993564e-07
Iter: 324 loss: 2.69885817e-07
Iter: 325 loss: 2.69890279e-07
Iter: 326 loss: 2.69799926e-07
Iter: 327 loss: 2.69947918e-07
Iter: 328 loss: 2.69751837e-07
Iter: 329 loss: 2.69655658e-07
Iter: 330 loss: 2.69773608e-07
Iter: 331 loss: 2.69602964e-07
Iter: 332 loss: 2.69514544e-07
Iter: 333 loss: 2.70041312e-07
Iter: 334 loss: 2.69502436e-07
Iter: 335 loss: 2.69385339e-07
Iter: 336 loss: 2.69335914e-07
Iter: 337 loss: 2.69280292e-07
Iter: 338 loss: 2.69114082e-07
Iter: 339 loss: 2.69461623e-07
Iter: 340 loss: 2.69047092e-07
Iter: 341 loss: 2.68922804e-07
Iter: 342 loss: 2.70056944e-07
Iter: 343 loss: 2.68920928e-07
Iter: 344 loss: 2.68800846e-07
Iter: 345 loss: 2.68912146e-07
Iter: 346 loss: 2.68717258e-07
Iter: 347 loss: 2.686225e-07
Iter: 348 loss: 2.68883042e-07
Iter: 349 loss: 2.68587286e-07
Iter: 350 loss: 2.68414112e-07
Iter: 351 loss: 2.68571569e-07
Iter: 352 loss: 2.68326232e-07
Iter: 353 loss: 2.68208339e-07
Iter: 354 loss: 2.67983353e-07
Iter: 355 loss: 2.73520243e-07
Iter: 356 loss: 2.67980624e-07
Iter: 357 loss: 2.67737022e-07
Iter: 358 loss: 2.70302365e-07
Iter: 359 loss: 2.677331e-07
Iter: 360 loss: 2.67559869e-07
Iter: 361 loss: 2.67571863e-07
Iter: 362 loss: 2.67484666e-07
Iter: 363 loss: 2.67395933e-07
Iter: 364 loss: 2.67375128e-07
Iter: 365 loss: 2.67246548e-07
Iter: 366 loss: 2.68495768e-07
Iter: 367 loss: 2.67249845e-07
Iter: 368 loss: 2.67139569e-07
Iter: 369 loss: 2.67188341e-07
Iter: 370 loss: 2.67070448e-07
Iter: 371 loss: 2.66947836e-07
Iter: 372 loss: 2.67137466e-07
Iter: 373 loss: 2.66892926e-07
Iter: 374 loss: 2.66758e-07
Iter: 375 loss: 2.67155201e-07
Iter: 376 loss: 2.66732229e-07
Iter: 377 loss: 2.66602e-07
Iter: 378 loss: 2.67556857e-07
Iter: 379 loss: 2.66589126e-07
Iter: 380 loss: 2.66484903e-07
Iter: 381 loss: 2.66272536e-07
Iter: 382 loss: 2.7021639e-07
Iter: 383 loss: 2.6627464e-07
Iter: 384 loss: 2.66066195e-07
Iter: 385 loss: 2.66068355e-07
Iter: 386 loss: 2.65935e-07
Iter: 387 loss: 2.65714277e-07
Iter: 388 loss: 2.65719592e-07
Iter: 389 loss: 2.65539484e-07
Iter: 390 loss: 2.65640779e-07
Iter: 391 loss: 2.6541386e-07
Iter: 392 loss: 2.65507396e-07
Iter: 393 loss: 2.65349826e-07
Iter: 394 loss: 2.65289884e-07
Iter: 395 loss: 2.65189186e-07
Iter: 396 loss: 2.65182024e-07
Iter: 397 loss: 2.65087976e-07
Iter: 398 loss: 2.65577341e-07
Iter: 399 loss: 2.65066888e-07
Iter: 400 loss: 2.64958828e-07
Iter: 401 loss: 2.65247365e-07
Iter: 402 loss: 2.64910511e-07
Iter: 403 loss: 2.64807653e-07
Iter: 404 loss: 2.64747342e-07
Iter: 405 loss: 2.64708092e-07
Iter: 406 loss: 2.64525255e-07
Iter: 407 loss: 2.65385268e-07
Iter: 408 loss: 2.64505331e-07
Iter: 409 loss: 2.64377775e-07
Iter: 410 loss: 2.65774247e-07
Iter: 411 loss: 2.64377832e-07
Iter: 412 loss: 2.64281852e-07
Iter: 413 loss: 2.64208097e-07
Iter: 414 loss: 2.64193289e-07
Iter: 415 loss: 2.64123258e-07
Iter: 416 loss: 2.64115414e-07
Iter: 417 loss: 2.64058031e-07
Iter: 418 loss: 2.63969355e-07
Iter: 419 loss: 2.6397052e-07
Iter: 420 loss: 2.6387562e-07
Iter: 421 loss: 2.63822187e-07
Iter: 422 loss: 2.63782e-07
Iter: 423 loss: 2.63652083e-07
Iter: 424 loss: 2.64650879e-07
Iter: 425 loss: 2.63649497e-07
Iter: 426 loss: 2.6349e-07
Iter: 427 loss: 2.6440523e-07
Iter: 428 loss: 2.63474419e-07
Iter: 429 loss: 2.63390376e-07
Iter: 430 loss: 2.6327146e-07
Iter: 431 loss: 2.6327055e-07
Iter: 432 loss: 2.63121336e-07
Iter: 433 loss: 2.63122843e-07
Iter: 434 loss: 2.63033542e-07
Iter: 435 loss: 2.6293651e-07
Iter: 436 loss: 2.62923209e-07
Iter: 437 loss: 2.62776553e-07
Iter: 438 loss: 2.63411494e-07
Iter: 439 loss: 2.62750746e-07
Iter: 440 loss: 2.62662638e-07
Iter: 441 loss: 2.63641766e-07
Iter: 442 loss: 2.62663292e-07
Iter: 443 loss: 2.62579e-07
Iter: 444 loss: 2.62624212e-07
Iter: 445 loss: 2.62518512e-07
Iter: 446 loss: 2.62458855e-07
Iter: 447 loss: 2.62625576e-07
Iter: 448 loss: 2.62416222e-07
Iter: 449 loss: 2.62303445e-07
Iter: 450 loss: 2.62438675e-07
Iter: 451 loss: 2.62243134e-07
Iter: 452 loss: 2.6215298e-07
Iter: 453 loss: 2.62089827e-07
Iter: 454 loss: 2.62062656e-07
Iter: 455 loss: 2.61931e-07
Iter: 456 loss: 2.62141555e-07
Iter: 457 loss: 2.61876465e-07
Iter: 458 loss: 2.61909236e-07
Iter: 459 loss: 2.61830053e-07
Iter: 460 loss: 2.61777302e-07
Iter: 461 loss: 2.61683567e-07
Iter: 462 loss: 2.6320987e-07
Iter: 463 loss: 2.61672199e-07
Iter: 464 loss: 2.61618254e-07
Iter: 465 loss: 2.6243319e-07
Iter: 466 loss: 2.61620357e-07
Iter: 467 loss: 2.61538673e-07
Iter: 468 loss: 2.61429079e-07
Iter: 469 loss: 2.6142024e-07
Iter: 470 loss: 2.61316984e-07
Iter: 471 loss: 2.61794554e-07
Iter: 472 loss: 2.6129635e-07
Iter: 473 loss: 2.61209493e-07
Iter: 474 loss: 2.61715144e-07
Iter: 475 loss: 2.6121154e-07
Iter: 476 loss: 2.61115133e-07
Iter: 477 loss: 2.61288307e-07
Iter: 478 loss: 2.61082675e-07
Iter: 479 loss: 2.60999315e-07
Iter: 480 loss: 2.61070113e-07
Iter: 481 loss: 2.60948866e-07
Iter: 482 loss: 2.60875623e-07
Iter: 483 loss: 2.60874572e-07
Iter: 484 loss: 2.60838306e-07
Iter: 485 loss: 2.60747868e-07
Iter: 486 loss: 2.62109552e-07
Iter: 487 loss: 2.60749715e-07
Iter: 488 loss: 2.60635829e-07
Iter: 489 loss: 2.60554827e-07
Iter: 490 loss: 2.60510177e-07
Iter: 491 loss: 2.60438526e-07
Iter: 492 loss: 2.60422354e-07
Iter: 493 loss: 2.60320348e-07
Iter: 494 loss: 2.60393847e-07
Iter: 495 loss: 2.60264301e-07
Iter: 496 loss: 2.60186766e-07
Iter: 497 loss: 2.60161301e-07
Iter: 498 loss: 2.60126598e-07
Iter: 499 loss: 2.60043e-07
Iter: 500 loss: 2.6003778e-07
Iter: 501 loss: 2.59992703e-07
Iter: 502 loss: 2.59914e-07
Iter: 503 loss: 2.61484701e-07
Iter: 504 loss: 2.59899679e-07
Iter: 505 loss: 2.59816261e-07
Iter: 506 loss: 2.60631253e-07
Iter: 507 loss: 2.59825583e-07
Iter: 508 loss: 2.59747139e-07
Iter: 509 loss: 2.60040366e-07
Iter: 510 loss: 2.59735941e-07
Iter: 511 loss: 2.59656417e-07
Iter: 512 loss: 2.59645248e-07
Iter: 513 loss: 2.59601052e-07
Iter: 514 loss: 2.59523e-07
Iter: 515 loss: 2.60369404e-07
Iter: 516 loss: 2.59508568e-07
Iter: 517 loss: 2.59423757e-07
Iter: 518 loss: 2.59315982e-07
Iter: 519 loss: 2.59312458e-07
Iter: 520 loss: 2.59183224e-07
Iter: 521 loss: 2.59223441e-07
Iter: 522 loss: 2.59084175e-07
Iter: 523 loss: 2.58977252e-07
Iter: 524 loss: 2.59548472e-07
Iter: 525 loss: 2.58949598e-07
Iter: 526 loss: 2.58912337e-07
Iter: 527 loss: 2.58880277e-07
Iter: 528 loss: 2.58852026e-07
Iter: 529 loss: 2.58767216e-07
Iter: 530 loss: 2.59492708e-07
Iter: 531 loss: 2.58754369e-07
Iter: 532 loss: 2.58696531e-07
Iter: 533 loss: 2.58693859e-07
Iter: 534 loss: 2.58627665e-07
Iter: 535 loss: 2.58535806e-07
Iter: 536 loss: 2.5853987e-07
Iter: 537 loss: 2.58469413e-07
Iter: 538 loss: 2.58725379e-07
Iter: 539 loss: 2.58441247e-07
Iter: 540 loss: 2.58367351e-07
Iter: 541 loss: 2.58618655e-07
Iter: 542 loss: 2.5834737e-07
Iter: 543 loss: 2.58280153e-07
Iter: 544 loss: 2.58255056e-07
Iter: 545 loss: 2.58214527e-07
Iter: 546 loss: 2.58096975e-07
Iter: 547 loss: 2.58372751e-07
Iter: 548 loss: 2.5805511e-07
Iter: 549 loss: 2.57972744e-07
Iter: 550 loss: 2.59180894e-07
Iter: 551 loss: 2.57958334e-07
Iter: 552 loss: 2.57890889e-07
Iter: 553 loss: 2.57754607e-07
Iter: 554 loss: 2.59946432e-07
Iter: 555 loss: 2.5774915e-07
Iter: 556 loss: 2.57614147e-07
Iter: 557 loss: 2.58208331e-07
Iter: 558 loss: 2.57589534e-07
Iter: 559 loss: 2.57512454e-07
Iter: 560 loss: 2.58193836e-07
Iter: 561 loss: 2.57505178e-07
Iter: 562 loss: 2.57431964e-07
Iter: 563 loss: 2.57403372e-07
Iter: 564 loss: 2.57365542e-07
Iter: 565 loss: 2.57293834e-07
Iter: 566 loss: 2.57449386e-07
Iter: 567 loss: 2.57267288e-07
Iter: 568 loss: 2.5720405e-07
Iter: 569 loss: 2.58057469e-07
Iter: 570 loss: 2.57208484e-07
Iter: 571 loss: 2.57170399e-07
Iter: 572 loss: 2.57102329e-07
Iter: 573 loss: 2.58478451e-07
Iter: 574 loss: 2.57097128e-07
Iter: 575 loss: 2.57062652e-07
Iter: 576 loss: 2.57052363e-07
Iter: 577 loss: 2.57011777e-07
Iter: 578 loss: 2.56958316e-07
Iter: 579 loss: 2.56947146e-07
Iter: 580 loss: 2.56851109e-07
Iter: 581 loss: 2.56874785e-07
Iter: 582 loss: 2.56790599e-07
Iter: 583 loss: 2.56696694e-07
Iter: 584 loss: 2.57999716e-07
Iter: 585 loss: 2.56696751e-07
Iter: 586 loss: 2.56604096e-07
Iter: 587 loss: 2.56708745e-07
Iter: 588 loss: 2.56571184e-07
Iter: 589 loss: 2.56491234e-07
Iter: 590 loss: 2.56499e-07
Iter: 591 loss: 2.56443e-07
Iter: 592 loss: 2.56374335e-07
Iter: 593 loss: 2.56379508e-07
Iter: 594 loss: 2.56316611e-07
Iter: 595 loss: 2.56319652e-07
Iter: 596 loss: 2.56288644e-07
Iter: 597 loss: 2.5622461e-07
Iter: 598 loss: 2.5614662e-07
Iter: 599 loss: 2.56142471e-07
Iter: 600 loss: 2.56089379e-07
Iter: 601 loss: 2.56077954e-07
Iter: 602 loss: 2.56027334e-07
Iter: 603 loss: 2.55932633e-07
Iter: 604 loss: 2.57631314e-07
Iter: 605 loss: 2.55932207e-07
Iter: 606 loss: 2.55847397e-07
Iter: 607 loss: 2.5659574e-07
Iter: 608 loss: 2.55842906e-07
Iter: 609 loss: 2.55761904e-07
Iter: 610 loss: 2.5583779e-07
Iter: 611 loss: 2.5572362e-07
Iter: 612 loss: 2.5566186e-07
Iter: 613 loss: 2.55685052e-07
Iter: 614 loss: 2.55626162e-07
Iter: 615 loss: 2.55562782e-07
Iter: 616 loss: 2.55536747e-07
Iter: 617 loss: 2.5550321e-07
Iter: 618 loss: 2.55477914e-07
Iter: 619 loss: 2.55450061e-07
Iter: 620 loss: 2.5541965e-07
Iter: 621 loss: 2.55353228e-07
Iter: 622 loss: 2.56450079e-07
Iter: 623 loss: 2.55344702e-07
Iter: 624 loss: 2.55238774e-07
Iter: 625 loss: 2.55309175e-07
Iter: 626 loss: 2.55185853e-07
Iter: 627 loss: 2.5513117e-07
Iter: 628 loss: 2.55130089e-07
Iter: 629 loss: 2.55066112e-07
Iter: 630 loss: 2.55178719e-07
Iter: 631 loss: 2.55031068e-07
Iter: 632 loss: 2.54967489e-07
Iter: 633 loss: 2.54881456e-07
Iter: 634 loss: 2.56962124e-07
Iter: 635 loss: 2.54885123e-07
Iter: 636 loss: 2.54809692e-07
Iter: 637 loss: 2.54798948e-07
Iter: 638 loss: 2.5472869e-07
Iter: 639 loss: 2.54725592e-07
Iter: 640 loss: 2.54667697e-07
Iter: 641 loss: 2.54590532e-07
Iter: 642 loss: 2.54706947e-07
Iter: 643 loss: 2.54554607e-07
Iter: 644 loss: 2.54456722e-07
Iter: 645 loss: 2.55295532e-07
Iter: 646 loss: 2.54445609e-07
Iter: 647 loss: 2.54402607e-07
Iter: 648 loss: 2.54358838e-07
Iter: 649 loss: 2.54357644e-07
Iter: 650 loss: 2.54311601e-07
Iter: 651 loss: 2.54907548e-07
Iter: 652 loss: 2.54306144e-07
Iter: 653 loss: 2.54264307e-07
Iter: 654 loss: 2.5435105e-07
Iter: 655 loss: 2.54253791e-07
Iter: 656 loss: 2.54207293e-07
Iter: 657 loss: 2.5416395e-07
Iter: 658 loss: 2.54142407e-07
Iter: 659 loss: 2.54095085e-07
Iter: 660 loss: 2.54125496e-07
Iter: 661 loss: 2.54051827e-07
Iter: 662 loss: 2.53959541e-07
Iter: 663 loss: 2.54297873e-07
Iter: 664 loss: 2.53940186e-07
Iter: 665 loss: 2.53808139e-07
Iter: 666 loss: 2.54471928e-07
Iter: 667 loss: 2.53787789e-07
Iter: 668 loss: 2.53730065e-07
Iter: 669 loss: 2.5367541e-07
Iter: 670 loss: 2.53664439e-07
Iter: 671 loss: 2.53615781e-07
Iter: 672 loss: 2.53618481e-07
Iter: 673 loss: 2.53566952e-07
Iter: 674 loss: 2.53486405e-07
Iter: 675 loss: 2.53491578e-07
Iter: 676 loss: 2.53412111e-07
Iter: 677 loss: 2.53421859e-07
Iter: 678 loss: 2.53381302e-07
Iter: 679 loss: 2.53424446e-07
Iter: 680 loss: 2.53342563e-07
Iter: 681 loss: 2.53295298e-07
Iter: 682 loss: 2.53207816e-07
Iter: 683 loss: 2.53210857e-07
Iter: 684 loss: 2.5317371e-07
Iter: 685 loss: 2.53160124e-07
Iter: 686 loss: 2.53121527e-07
Iter: 687 loss: 2.53042344e-07
Iter: 688 loss: 2.53042742e-07
Iter: 689 loss: 2.52970551e-07
Iter: 690 loss: 2.53240444e-07
Iter: 691 loss: 2.52939486e-07
Iter: 692 loss: 2.52855472e-07
Iter: 693 loss: 2.52984933e-07
Iter: 694 loss: 2.52829977e-07
Iter: 695 loss: 2.52778534e-07
Iter: 696 loss: 2.52786947e-07
Iter: 697 loss: 2.52755115e-07
Iter: 698 loss: 2.52702506e-07
Iter: 699 loss: 2.52708048e-07
Iter: 700 loss: 2.52649613e-07
Iter: 701 loss: 2.53014065e-07
Iter: 702 loss: 2.52650636e-07
Iter: 703 loss: 2.52599648e-07
Iter: 704 loss: 2.52547494e-07
Iter: 705 loss: 2.52533908e-07
Iter: 706 loss: 2.5248454e-07
Iter: 707 loss: 2.52418062e-07
Iter: 708 loss: 2.52416214e-07
Iter: 709 loss: 2.5231617e-07
Iter: 710 loss: 2.52962025e-07
Iter: 711 loss: 2.52316397e-07
Iter: 712 loss: 2.52277033e-07
Iter: 713 loss: 2.52271633e-07
Iter: 714 loss: 2.52224368e-07
Iter: 715 loss: 2.52126171e-07
Iter: 716 loss: 2.53744361e-07
Iter: 717 loss: 2.52130747e-07
Iter: 718 loss: 2.52039598e-07
Iter: 719 loss: 2.52520721e-07
Iter: 720 loss: 2.52034255e-07
Iter: 721 loss: 2.51989519e-07
Iter: 722 loss: 2.5284686e-07
Iter: 723 loss: 2.51980566e-07
Iter: 724 loss: 2.51939582e-07
Iter: 725 loss: 2.51901497e-07
Iter: 726 loss: 2.51896438e-07
Iter: 727 loss: 2.51848093e-07
Iter: 728 loss: 2.52388475e-07
Iter: 729 loss: 2.51858751e-07
Iter: 730 loss: 2.51814981e-07
Iter: 731 loss: 2.51885353e-07
Iter: 732 loss: 2.51802561e-07
Iter: 733 loss: 2.5178224e-07
Iter: 734 loss: 2.51769762e-07
Iter: 735 loss: 2.51752738e-07
Iter: 736 loss: 2.51718376e-07
Iter: 737 loss: 2.52166927e-07
Iter: 738 loss: 2.5172281e-07
Iter: 739 loss: 2.51668951e-07
Iter: 740 loss: 2.51614324e-07
Iter: 741 loss: 2.51609208e-07
Iter: 742 loss: 2.51546396e-07
Iter: 743 loss: 2.5148222e-07
Iter: 744 loss: 2.51470226e-07
Iter: 745 loss: 2.51404572e-07
Iter: 746 loss: 2.52615365e-07
Iter: 747 loss: 2.51403321e-07
Iter: 748 loss: 2.51344318e-07
Iter: 749 loss: 2.51841925e-07
Iter: 750 loss: 2.51340765e-07
Iter: 751 loss: 2.51284888e-07
Iter: 752 loss: 2.51249872e-07
Iter: 753 loss: 2.51235917e-07
Iter: 754 loss: 2.51197463e-07
Iter: 755 loss: 2.51206785e-07
Iter: 756 loss: 2.51166227e-07
Iter: 757 loss: 2.51159804e-07
Iter: 758 loss: 2.5112854e-07
Iter: 759 loss: 2.51096424e-07
Iter: 760 loss: 2.5124811e-07
Iter: 761 loss: 2.51095628e-07
Iter: 762 loss: 2.5104481e-07
Iter: 763 loss: 2.51187231e-07
Iter: 764 loss: 2.51023465e-07
Iter: 765 loss: 2.50987512e-07
Iter: 766 loss: 2.50937774e-07
Iter: 767 loss: 2.50936381e-07
Iter: 768 loss: 2.50863422e-07
Iter: 769 loss: 2.50972846e-07
Iter: 770 loss: 2.50834063e-07
Iter: 771 loss: 2.50812946e-07
Iter: 772 loss: 2.50799559e-07
Iter: 773 loss: 2.50773695e-07
Iter: 774 loss: 2.50719637e-07
Iter: 775 loss: 2.51435154e-07
Iter: 776 loss: 2.50714379e-07
Iter: 777 loss: 2.50662481e-07
Iter: 778 loss: 2.50752208e-07
Iter: 779 loss: 2.50639459e-07
Iter: 780 loss: 2.50594979e-07
Iter: 781 loss: 2.51189647e-07
Iter: 782 loss: 2.5060163e-07
Iter: 783 loss: 2.50540779e-07
Iter: 784 loss: 2.5059876e-07
Iter: 785 loss: 2.50520486e-07
Iter: 786 loss: 2.50471544e-07
Iter: 787 loss: 2.50439939e-07
Iter: 788 loss: 2.50420584e-07
Iter: 789 loss: 2.50378775e-07
Iter: 790 loss: 2.50367975e-07
Iter: 791 loss: 2.50328526e-07
Iter: 792 loss: 2.50277679e-07
Iter: 793 loss: 2.50276344e-07
Iter: 794 loss: 2.50216e-07
Iter: 795 loss: 2.50770796e-07
Iter: 796 loss: 2.50230869e-07
Iter: 797 loss: 2.50179028e-07
Iter: 798 loss: 2.50360017e-07
Iter: 799 loss: 2.50171468e-07
Iter: 800 loss: 2.50130398e-07
Iter: 801 loss: 2.50070798e-07
Iter: 802 loss: 2.51138914e-07
Iter: 803 loss: 2.50055166e-07
Iter: 804 loss: 2.49965979e-07
Iter: 805 loss: 2.50238855e-07
Iter: 806 loss: 2.49944492e-07
Iter: 807 loss: 2.49869743e-07
Iter: 808 loss: 2.50195228e-07
Iter: 809 loss: 2.49852491e-07
Iter: 810 loss: 2.49755914e-07
Iter: 811 loss: 2.50460118e-07
Iter: 812 loss: 2.49755857e-07
Iter: 813 loss: 2.49708052e-07
Iter: 814 loss: 2.49654192e-07
Iter: 815 loss: 2.49659223e-07
Iter: 816 loss: 2.49601953e-07
Iter: 817 loss: 2.49976324e-07
Iter: 818 loss: 2.4959013e-07
Iter: 819 loss: 2.49546758e-07
Iter: 820 loss: 2.50174196e-07
Iter: 821 loss: 2.49552102e-07
Iter: 822 loss: 2.49529648e-07
Iter: 823 loss: 2.49480848e-07
Iter: 824 loss: 2.49862808e-07
Iter: 825 loss: 2.49465415e-07
Iter: 826 loss: 2.49438926e-07
Iter: 827 loss: 2.4942733e-07
Iter: 828 loss: 2.4939402e-07
Iter: 829 loss: 2.49314667e-07
Iter: 830 loss: 2.50361211e-07
Iter: 831 loss: 2.49298694e-07
Iter: 832 loss: 2.49244749e-07
Iter: 833 loss: 2.49248842e-07
Iter: 834 loss: 2.49176566e-07
Iter: 835 loss: 2.4917685e-07
Iter: 836 loss: 2.49131176e-07
Iter: 837 loss: 2.49060548e-07
Iter: 838 loss: 2.49117676e-07
Iter: 839 loss: 2.49018939e-07
Iter: 840 loss: 2.48959083e-07
Iter: 841 loss: 2.49153715e-07
Iter: 842 loss: 2.48952176e-07
Iter: 843 loss: 2.48907867e-07
Iter: 844 loss: 2.48905678e-07
Iter: 845 loss: 2.48871629e-07
Iter: 846 loss: 2.48840792e-07
Iter: 847 loss: 2.48825728e-07
Iter: 848 loss: 2.48788723e-07
Iter: 849 loss: 2.48790542e-07
Iter: 850 loss: 2.48762063e-07
Iter: 851 loss: 2.48701866e-07
Iter: 852 loss: 2.49483492e-07
Iter: 853 loss: 2.48707209e-07
Iter: 854 loss: 2.48661365e-07
Iter: 855 loss: 2.48601197e-07
Iter: 856 loss: 2.48592983e-07
Iter: 857 loss: 2.48534946e-07
Iter: 858 loss: 2.49064442e-07
Iter: 859 loss: 2.48529489e-07
Iter: 860 loss: 2.48481484e-07
Iter: 861 loss: 2.48620495e-07
Iter: 862 loss: 2.48472759e-07
Iter: 863 loss: 2.48417734e-07
Iter: 864 loss: 2.48361e-07
Iter: 865 loss: 2.4836686e-07
Iter: 866 loss: 2.48346254e-07
Iter: 867 loss: 2.48318486e-07
Iter: 868 loss: 2.4829842e-07
Iter: 869 loss: 2.48249307e-07
Iter: 870 loss: 2.491511e-07
Iter: 871 loss: 2.48249023e-07
Iter: 872 loss: 2.48193544e-07
Iter: 873 loss: 2.48308368e-07
Iter: 874 loss: 2.48171375e-07
Iter: 875 loss: 2.48101173e-07
Iter: 876 loss: 2.4845798e-07
Iter: 877 loss: 2.48096399e-07
Iter: 878 loss: 2.48049901e-07
Iter: 879 loss: 2.48345657e-07
Iter: 880 loss: 2.48044159e-07
Iter: 881 loss: 2.4800687e-07
Iter: 882 loss: 2.48354127e-07
Iter: 883 loss: 2.48002095e-07
Iter: 884 loss: 2.47989249e-07
Iter: 885 loss: 2.47947241e-07
Iter: 886 loss: 2.48795942e-07
Iter: 887 loss: 2.47947611e-07
Iter: 888 loss: 2.47901937e-07
Iter: 889 loss: 2.48524202e-07
Iter: 890 loss: 2.47907622e-07
Iter: 891 loss: 2.47880678e-07
Iter: 892 loss: 2.47833668e-07
Iter: 893 loss: 2.4874538e-07
Iter: 894 loss: 2.47822669e-07
Iter: 895 loss: 2.47798681e-07
Iter: 896 loss: 2.47794532e-07
Iter: 897 loss: 2.47766508e-07
Iter: 898 loss: 2.4772558e-07
Iter: 899 loss: 2.47721346e-07
Iter: 900 loss: 2.47663621e-07
Iter: 901 loss: 2.47681498e-07
Iter: 902 loss: 2.47633039e-07
Iter: 903 loss: 2.47634546e-07
Iter: 904 loss: 2.47606607e-07
Iter: 905 loss: 2.47587849e-07
Iter: 906 loss: 2.47541067e-07
Iter: 907 loss: 2.48395537e-07
Iter: 908 loss: 2.47547348e-07
Iter: 909 loss: 2.47505852e-07
Iter: 910 loss: 2.47587622e-07
Iter: 911 loss: 2.47490675e-07
Iter: 912 loss: 2.47433945e-07
Iter: 913 loss: 2.47396343e-07
Iter: 914 loss: 2.47386367e-07
Iter: 915 loss: 2.47384492e-07
Iter: 916 loss: 2.47346236e-07
Iter: 917 loss: 2.47311817e-07
Iter: 918 loss: 2.47280553e-07
Iter: 919 loss: 2.47270663e-07
Iter: 920 loss: 2.47211489e-07
Iter: 921 loss: 2.47315853e-07
Iter: 922 loss: 2.47196937e-07
Iter: 923 loss: 2.47145124e-07
Iter: 924 loss: 2.47145095e-07
Iter: 925 loss: 2.47128526e-07
Iter: 926 loss: 2.4711477e-07
Iter: 927 loss: 2.47105703e-07
Iter: 928 loss: 2.47065287e-07
Iter: 929 loss: 2.47194208e-07
Iter: 930 loss: 2.47055652e-07
Iter: 931 loss: 2.47015436e-07
Iter: 932 loss: 2.46963594e-07
Iter: 933 loss: 2.46960155e-07
Iter: 934 loss: 2.46909195e-07
Iter: 935 loss: 2.47686046e-07
Iter: 936 loss: 2.46903539e-07
Iter: 937 loss: 2.46887168e-07
Iter: 938 loss: 2.46929574e-07
Iter: 939 loss: 2.46850789e-07
Iter: 940 loss: 2.4678593e-07
Iter: 941 loss: 2.46749579e-07
Iter: 942 loss: 2.46740342e-07
Iter: 943 loss: 2.46683157e-07
Iter: 944 loss: 2.46839079e-07
Iter: 945 loss: 2.46668208e-07
Iter: 946 loss: 2.46599257e-07
Iter: 947 loss: 2.46762795e-07
Iter: 948 loss: 2.46588e-07
Iter: 949 loss: 2.46527463e-07
Iter: 950 loss: 2.46544289e-07
Iter: 951 loss: 2.46510638e-07
Iter: 952 loss: 2.46471728e-07
Iter: 953 loss: 2.46473292e-07
Iter: 954 loss: 2.46439328e-07
Iter: 955 loss: 2.46936395e-07
Iter: 956 loss: 2.46441545e-07
Iter: 957 loss: 2.46408149e-07
Iter: 958 loss: 2.46502367e-07
Iter: 959 loss: 2.46381774e-07
Iter: 960 loss: 2.46361623e-07
Iter: 961 loss: 2.46424918e-07
Iter: 962 loss: 2.46355029e-07
Iter: 963 loss: 2.46315807e-07
Iter: 964 loss: 2.46282923e-07
Iter: 965 loss: 2.46270758e-07
Iter: 966 loss: 2.46229064e-07
Iter: 967 loss: 2.46435093e-07
Iter: 968 loss: 2.4622608e-07
Iter: 969 loss: 2.46170544e-07
Iter: 970 loss: 2.46438844e-07
Iter: 971 loss: 2.46176683e-07
Iter: 972 loss: 2.46135784e-07
Iter: 973 loss: 2.46087495e-07
Iter: 974 loss: 2.4609372e-07
Iter: 975 loss: 2.4601411e-07
Iter: 976 loss: 2.46113785e-07
Iter: 977 loss: 2.45984609e-07
Iter: 978 loss: 2.45911394e-07
Iter: 979 loss: 2.45984154e-07
Iter: 980 loss: 2.4585816e-07
Iter: 981 loss: 2.4581027e-07
Iter: 982 loss: 2.45805779e-07
Iter: 983 loss: 2.45769854e-07
Iter: 984 loss: 2.45877914e-07
Iter: 985 loss: 2.45751863e-07
Iter: 986 loss: 2.45719832e-07
Iter: 987 loss: 2.45730689e-07
Iter: 988 loss: 2.45696896e-07
Iter: 989 loss: 2.45661766e-07
Iter: 990 loss: 2.46008142e-07
Iter: 991 loss: 2.45663216e-07
Iter: 992 loss: 2.45635306e-07
Iter: 993 loss: 2.45678933e-07
Iter: 994 loss: 2.45625301e-07
Iter: 995 loss: 2.45586222e-07
Iter: 996 loss: 2.45621095e-07
Iter: 997 loss: 2.45562717e-07
Iter: 998 loss: 2.45513888e-07
Iter: 999 loss: 2.45488934e-07
Iter: 1000 loss: 2.45451048e-07
Iter: 1001 loss: 2.45426463e-07
Iter: 1002 loss: 2.4542291e-07
Iter: 1003 loss: 2.45395029e-07
Iter: 1004 loss: 2.45378374e-07
Iter: 1005 loss: 2.45369677e-07
Iter: 1006 loss: 2.45337844e-07
Iter: 1007 loss: 2.45377407e-07
Iter: 1008 loss: 2.45320507e-07
Iter: 1009 loss: 2.45283474e-07
Iter: 1010 loss: 2.45439708e-07
Iter: 1011 loss: 2.45287424e-07
Iter: 1012 loss: 2.4525815e-07
Iter: 1013 loss: 2.45262783e-07
Iter: 1014 loss: 2.45232172e-07
Iter: 1015 loss: 2.45207133e-07
Iter: 1016 loss: 2.45207502e-07
Iter: 1017 loss: 2.45192808e-07
Iter: 1018 loss: 2.45142473e-07
Iter: 1019 loss: 2.45650341e-07
Iter: 1020 loss: 2.45141621e-07
Iter: 1021 loss: 2.4510814e-07
Iter: 1022 loss: 2.45742569e-07
Iter: 1023 loss: 2.45105127e-07
Iter: 1024 loss: 2.4506943e-07
Iter: 1025 loss: 2.45029412e-07
Iter: 1026 loss: 2.45007953e-07
Iter: 1027 loss: 2.44969357e-07
Iter: 1028 loss: 2.45587103e-07
Iter: 1029 loss: 2.4496785e-07
Iter: 1030 loss: 2.44928401e-07
Iter: 1031 loss: 2.44982516e-07
Iter: 1032 loss: 2.44926952e-07
Iter: 1033 loss: 2.44910154e-07
Iter: 1034 loss: 2.45020601e-07
Iter: 1035 loss: 2.44890771e-07
Iter: 1036 loss: 2.44876787e-07
Iter: 1037 loss: 2.44998063e-07
Iter: 1038 loss: 2.44860217e-07
Iter: 1039 loss: 2.44851321e-07
Iter: 1040 loss: 2.44832677e-07
Iter: 1041 loss: 2.44819631e-07
Iter: 1042 loss: 2.44799367e-07
Iter: 1043 loss: 2.44786833e-07
Iter: 1044 loss: 2.44770945e-07
Iter: 1045 loss: 2.44702335e-07
Iter: 1046 loss: 2.44852174e-07
Iter: 1047 loss: 2.44691762e-07
Iter: 1048 loss: 2.44640205e-07
Iter: 1049 loss: 2.44909472e-07
Iter: 1050 loss: 2.44621361e-07
Iter: 1051 loss: 2.44589899e-07
Iter: 1052 loss: 2.45172799e-07
Iter: 1053 loss: 2.44586289e-07
Iter: 1054 loss: 2.44566081e-07
Iter: 1055 loss: 2.44514524e-07
Iter: 1056 loss: 2.44526206e-07
Iter: 1057 loss: 2.44490309e-07
Iter: 1058 loss: 2.44492e-07
Iter: 1059 loss: 2.44468566e-07
Iter: 1060 loss: 2.44467458e-07
Iter: 1061 loss: 2.44455833e-07
Iter: 1062 loss: 2.44414508e-07
Iter: 1063 loss: 2.44488973e-07
Iter: 1064 loss: 2.44410558e-07
Iter: 1065 loss: 2.44399217e-07
Iter: 1066 loss: 2.44371478e-07
Iter: 1067 loss: 2.44370142e-07
Iter: 1068 loss: 2.44334757e-07
Iter: 1069 loss: 2.447768e-07
Iter: 1070 loss: 2.44332114e-07
Iter: 1071 loss: 2.4429832e-07
Iter: 1072 loss: 2.44300765e-07
Iter: 1073 loss: 2.44286667e-07
Iter: 1074 loss: 2.44228488e-07
Iter: 1075 loss: 2.44187703e-07
Iter: 1076 loss: 2.44179944e-07
Iter: 1077 loss: 2.44122674e-07
Iter: 1078 loss: 2.44750936e-07
Iter: 1079 loss: 2.44121225e-07
Iter: 1080 loss: 2.44068502e-07
Iter: 1081 loss: 2.44087175e-07
Iter: 1082 loss: 2.44043463e-07
Iter: 1083 loss: 2.43979258e-07
Iter: 1084 loss: 2.44385092e-07
Iter: 1085 loss: 2.43970362e-07
Iter: 1086 loss: 2.43912268e-07
Iter: 1087 loss: 2.43936825e-07
Iter: 1088 loss: 2.43872648e-07
Iter: 1089 loss: 2.438818e-07
Iter: 1090 loss: 2.43850394e-07
Iter: 1091 loss: 2.43833824e-07
Iter: 1092 loss: 2.43794261e-07
Iter: 1093 loss: 2.44647595e-07
Iter: 1094 loss: 2.43787213e-07
Iter: 1095 loss: 2.4372244e-07
Iter: 1096 loss: 2.44203477e-07
Iter: 1097 loss: 2.4373395e-07
Iter: 1098 loss: 2.43695666e-07
Iter: 1099 loss: 2.43694302e-07
Iter: 1100 loss: 2.43667841e-07
Iter: 1101 loss: 2.43638141e-07
Iter: 1102 loss: 2.43976444e-07
Iter: 1103 loss: 2.43633679e-07
Iter: 1104 loss: 2.43601477e-07
Iter: 1105 loss: 2.43660338e-07
Iter: 1106 loss: 2.43604916e-07
Iter: 1107 loss: 2.43578199e-07
Iter: 1108 loss: 2.43532384e-07
Iter: 1109 loss: 2.43531986e-07
Iter: 1110 loss: 2.43508651e-07
Iter: 1111 loss: 2.43503877e-07
Iter: 1112 loss: 2.43467866e-07
Iter: 1113 loss: 2.43435238e-07
Iter: 1114 loss: 2.43421368e-07
Iter: 1115 loss: 2.43389479e-07
Iter: 1116 loss: 2.4340585e-07
Iter: 1117 loss: 2.43370266e-07
Iter: 1118 loss: 2.43337752e-07
Iter: 1119 loss: 2.43463546e-07
Iter: 1120 loss: 2.43329225e-07
Iter: 1121 loss: 2.43280766e-07
Iter: 1122 loss: 2.43447971e-07
Iter: 1123 loss: 2.43273263e-07
Iter: 1124 loss: 2.43234467e-07
Iter: 1125 loss: 2.43611794e-07
Iter: 1126 loss: 2.43229778e-07
Iter: 1127 loss: 2.4319209e-07
Iter: 1128 loss: 2.43163925e-07
Iter: 1129 loss: 2.44042923e-07
Iter: 1130 loss: 2.43142807e-07
Iter: 1131 loss: 2.43114584e-07
Iter: 1132 loss: 2.43113476e-07
Iter: 1133 loss: 2.43087868e-07
Iter: 1134 loss: 2.43083718e-07
Iter: 1135 loss: 2.43059731e-07
Iter: 1136 loss: 2.43003967e-07
Iter: 1137 loss: 2.4332968e-07
Iter: 1138 loss: 2.43009424e-07
Iter: 1139 loss: 2.42977819e-07
Iter: 1140 loss: 2.42964433e-07
Iter: 1141 loss: 2.42936608e-07
Iter: 1142 loss: 2.42909948e-07
Iter: 1143 loss: 2.42963324e-07
Iter: 1144 loss: 2.42891844e-07
Iter: 1145 loss: 2.42867884e-07
Iter: 1146 loss: 2.42868339e-07
Iter: 1147 loss: 2.42849296e-07
Iter: 1148 loss: 2.4281573e-07
Iter: 1149 loss: 2.42823802e-07
Iter: 1150 loss: 2.42776508e-07
Iter: 1151 loss: 2.42745301e-07
Iter: 1152 loss: 2.42740157e-07
Iter: 1153 loss: 2.42684877e-07
Iter: 1154 loss: 2.43219262e-07
Iter: 1155 loss: 2.42686696e-07
Iter: 1156 loss: 2.42653044e-07
Iter: 1157 loss: 2.42842447e-07
Iter: 1158 loss: 2.42648127e-07
Iter: 1159 loss: 2.42600834e-07
Iter: 1160 loss: 2.42853389e-07
Iter: 1161 loss: 2.42604045e-07
Iter: 1162 loss: 2.42590545e-07
Iter: 1163 loss: 2.42564909e-07
Iter: 1164 loss: 2.42557434e-07
Iter: 1165 loss: 2.42537737e-07
Iter: 1166 loss: 2.42913927e-07
Iter: 1167 loss: 2.42529097e-07
Iter: 1168 loss: 2.42507468e-07
Iter: 1169 loss: 2.42528102e-07
Iter: 1170 loss: 2.42496185e-07
Iter: 1171 loss: 2.42461596e-07
Iter: 1172 loss: 2.42534526e-07
Iter: 1173 loss: 2.4245e-07
Iter: 1174 loss: 2.42432861e-07
Iter: 1175 loss: 2.4240876e-07
Iter: 1176 loss: 2.42406315e-07
Iter: 1177 loss: 2.42360045e-07
Iter: 1178 loss: 2.42628573e-07
Iter: 1179 loss: 2.42358368e-07
Iter: 1180 loss: 2.42332078e-07
Iter: 1181 loss: 2.42360386e-07
Iter: 1182 loss: 2.42303543e-07
Iter: 1183 loss: 2.42276343e-07
Iter: 1184 loss: 2.42284102e-07
Iter: 1185 loss: 2.42247097e-07
Iter: 1186 loss: 2.42219045e-07
Iter: 1187 loss: 2.42276968e-07
Iter: 1188 loss: 2.42216828e-07
Iter: 1189 loss: 2.42174707e-07
Iter: 1190 loss: 2.42389888e-07
Iter: 1191 loss: 2.42174735e-07
Iter: 1192 loss: 2.42138128e-07
Iter: 1193 loss: 2.42403019e-07
Iter: 1194 loss: 2.42135314e-07
Iter: 1195 loss: 2.42118631e-07
Iter: 1196 loss: 2.42094927e-07
Iter: 1197 loss: 2.42080858e-07
Iter: 1198 loss: 2.42051158e-07
Iter: 1199 loss: 2.42309284e-07
Iter: 1200 loss: 2.42051414e-07
Iter: 1201 loss: 2.41996162e-07
Iter: 1202 loss: 2.42089982e-07
Iter: 1203 loss: 2.41988289e-07
Iter: 1204 loss: 2.4194938e-07
Iter: 1205 loss: 2.42304736e-07
Iter: 1206 loss: 2.41952591e-07
Iter: 1207 loss: 2.4193443e-07
Iter: 1208 loss: 2.41901745e-07
Iter: 1209 loss: 2.41894753e-07
Iter: 1210 loss: 2.41865735e-07
Iter: 1211 loss: 2.41956116e-07
Iter: 1212 loss: 2.41856213e-07
Iter: 1213 loss: 2.41824324e-07
Iter: 1214 loss: 2.4215862e-07
Iter: 1215 loss: 2.41823898e-07
Iter: 1216 loss: 2.41802155e-07
Iter: 1217 loss: 2.4175705e-07
Iter: 1218 loss: 2.42711252e-07
Iter: 1219 loss: 2.41748438e-07
Iter: 1220 loss: 2.41697421e-07
Iter: 1221 loss: 2.41925136e-07
Iter: 1222 loss: 2.41691026e-07
Iter: 1223 loss: 2.41655869e-07
Iter: 1224 loss: 2.41806845e-07
Iter: 1225 loss: 2.41653879e-07
Iter: 1226 loss: 2.41627475e-07
Iter: 1227 loss: 2.41631511e-07
Iter: 1228 loss: 2.41602436e-07
Iter: 1229 loss: 2.41637593e-07
Iter: 1230 loss: 2.41600389e-07
Iter: 1231 loss: 2.41573105e-07
Iter: 1232 loss: 2.41566624e-07
Iter: 1233 loss: 2.41568557e-07
Iter: 1234 loss: 2.41530586e-07
Iter: 1235 loss: 2.4177e-07
Iter: 1236 loss: 2.41521775e-07
Iter: 1237 loss: 2.41512623e-07
Iter: 1238 loss: 2.41531865e-07
Iter: 1239 loss: 2.41499094e-07
Iter: 1240 loss: 2.41476471e-07
Iter: 1241 loss: 2.41573616e-07
Iter: 1242 loss: 2.41471184e-07
Iter: 1243 loss: 2.41450834e-07
Iter: 1244 loss: 2.41429e-07
Iter: 1245 loss: 2.42235501e-07
Iter: 1246 loss: 2.41423834e-07
Iter: 1247 loss: 2.41382224e-07
Iter: 1248 loss: 2.41384384e-07
Iter: 1249 loss: 2.41359885e-07
Iter: 1250 loss: 2.4134593e-07
Iter: 1251 loss: 2.41326973e-07
Iter: 1252 loss: 2.41296647e-07
Iter: 1253 loss: 2.41304321e-07
Iter: 1254 loss: 2.41286443e-07
Iter: 1255 loss: 2.41242304e-07
Iter: 1256 loss: 2.41466239e-07
Iter: 1257 loss: 2.41240343e-07
Iter: 1258 loss: 2.41214138e-07
Iter: 1259 loss: 2.41571627e-07
Iter: 1260 loss: 2.41217833e-07
Iter: 1261 loss: 2.41196346e-07
Iter: 1262 loss: 2.41182505e-07
Iter: 1263 loss: 2.41175542e-07
Iter: 1264 loss: 2.41146324e-07
Iter: 1265 loss: 2.41196744e-07
Iter: 1266 loss: 2.4113973e-07
Iter: 1267 loss: 2.4111273e-07
Iter: 1268 loss: 2.41383475e-07
Iter: 1269 loss: 2.41113128e-07
Iter: 1270 loss: 2.41084564e-07
Iter: 1271 loss: 2.41143937e-07
Iter: 1272 loss: 2.41078851e-07
Iter: 1273 loss: 2.4106788e-07
Iter: 1274 loss: 2.41259329e-07
Iter: 1275 loss: 2.41071206e-07
Iter: 1276 loss: 2.41054551e-07
Iter: 1277 loss: 2.41029795e-07
Iter: 1278 loss: 2.41422782e-07
Iter: 1279 loss: 2.41033717e-07
Iter: 1280 loss: 2.41009047e-07
Iter: 1281 loss: 2.41301592e-07
Iter: 1282 loss: 2.41011094e-07
Iter: 1283 loss: 2.40987e-07
Iter: 1284 loss: 2.40992051e-07
Iter: 1285 loss: 2.40984889e-07
Iter: 1286 loss: 2.40955558e-07
Iter: 1287 loss: 2.40916961e-07
Iter: 1288 loss: 2.40913778e-07
Iter: 1289 loss: 2.40859549e-07
Iter: 1290 loss: 2.40925203e-07
Iter: 1291 loss: 2.40815496e-07
Iter: 1292 loss: 2.40769e-07
Iter: 1293 loss: 2.41151952e-07
Iter: 1294 loss: 2.40771158e-07
Iter: 1295 loss: 2.40741088e-07
Iter: 1296 loss: 2.41192e-07
Iter: 1297 loss: 2.40739865e-07
Iter: 1298 loss: 2.40720595e-07
Iter: 1299 loss: 2.40706527e-07
Iter: 1300 loss: 2.40702e-07
Iter: 1301 loss: 2.40682e-07
Iter: 1302 loss: 2.40874726e-07
Iter: 1303 loss: 2.40679782e-07
Iter: 1304 loss: 2.40637945e-07
Iter: 1305 loss: 2.40692259e-07
Iter: 1306 loss: 2.40643629e-07
Iter: 1307 loss: 2.4062075e-07
Iter: 1308 loss: 2.40693339e-07
Iter: 1309 loss: 2.40616885e-07
Iter: 1310 loss: 2.40570103e-07
Iter: 1311 loss: 2.40572263e-07
Iter: 1312 loss: 2.40544637e-07
Iter: 1313 loss: 2.40510047e-07
Iter: 1314 loss: 2.40506779e-07
Iter: 1315 loss: 2.40475202e-07
Iter: 1316 loss: 2.40419865e-07
Iter: 1317 loss: 2.40937936e-07
Iter: 1318 loss: 2.40414749e-07
Iter: 1319 loss: 2.40394684e-07
Iter: 1320 loss: 2.40366546e-07
Iter: 1321 loss: 2.40370355e-07
Iter: 1322 loss: 2.40337386e-07
Iter: 1323 loss: 2.40488674e-07
Iter: 1324 loss: 2.40323743e-07
Iter: 1325 loss: 2.40307372e-07
Iter: 1326 loss: 2.40483502e-07
Iter: 1327 loss: 2.40297766e-07
Iter: 1328 loss: 2.40283271e-07
Iter: 1329 loss: 2.40434872e-07
Iter: 1330 loss: 2.40279832e-07
Iter: 1331 loss: 2.40259e-07
Iter: 1332 loss: 2.40248426e-07
Iter: 1333 loss: 2.40253058e-07
Iter: 1334 loss: 2.40226655e-07
Iter: 1335 loss: 2.40260647e-07
Iter: 1336 loss: 2.4021324e-07
Iter: 1337 loss: 2.4018712e-07
Iter: 1338 loss: 2.40330223e-07
Iter: 1339 loss: 2.40174927e-07
Iter: 1340 loss: 2.40155117e-07
Iter: 1341 loss: 2.40138519e-07
Iter: 1342 loss: 2.40115753e-07
Iter: 1343 loss: 2.40101343e-07
Iter: 1344 loss: 2.4009546e-07
Iter: 1345 loss: 2.4007926e-07
Iter: 1346 loss: 2.40033074e-07
Iter: 1347 loss: 2.40395053e-07
Iter: 1348 loss: 2.40030801e-07
Iter: 1349 loss: 2.40009911e-07
Iter: 1350 loss: 2.40014401e-07
Iter: 1351 loss: 2.39985269e-07
Iter: 1352 loss: 2.39955e-07
Iter: 1353 loss: 2.40693623e-07
Iter: 1354 loss: 2.39956819e-07
Iter: 1355 loss: 2.39919e-07
Iter: 1356 loss: 2.39914471e-07
Iter: 1357 loss: 2.39901482e-07
Iter: 1358 loss: 2.39873174e-07
Iter: 1359 loss: 2.39858508e-07
Iter: 1360 loss: 2.39847623e-07
Iter: 1361 loss: 2.39827898e-07
Iter: 1362 loss: 2.39824658e-07
Iter: 1363 loss: 2.39797487e-07
Iter: 1364 loss: 2.39801636e-07
Iter: 1365 loss: 2.397922e-07
Iter: 1366 loss: 2.39771765e-07
Iter: 1367 loss: 2.39853335e-07
Iter: 1368 loss: 2.39759714e-07
Iter: 1369 loss: 2.3974809e-07
Iter: 1370 loss: 2.39781173e-07
Iter: 1371 loss: 2.39734646e-07
Iter: 1372 loss: 2.39701393e-07
Iter: 1373 loss: 2.39746441e-07
Iter: 1374 loss: 2.3969676e-07
Iter: 1375 loss: 2.39676098e-07
Iter: 1376 loss: 2.39967e-07
Iter: 1377 loss: 2.39679366e-07
Iter: 1378 loss: 2.39647875e-07
Iter: 1379 loss: 2.39668083e-07
Iter: 1380 loss: 2.39647449e-07
Iter: 1381 loss: 2.39623944e-07
Iter: 1382 loss: 2.39648983e-07
Iter: 1383 loss: 2.39619908e-07
Iter: 1384 loss: 2.39598648e-07
Iter: 1385 loss: 2.39707106e-07
Iter: 1386 loss: 2.39587507e-07
Iter: 1387 loss: 2.3957017e-07
Iter: 1388 loss: 2.3956531e-07
Iter: 1389 loss: 2.39557039e-07
Iter: 1390 loss: 2.39531886e-07
Iter: 1391 loss: 2.39542032e-07
Iter: 1392 loss: 2.39505312e-07
Iter: 1393 loss: 2.39492408e-07
Iter: 1394 loss: 2.39485416e-07
Iter: 1395 loss: 2.39461087e-07
Iter: 1396 loss: 2.39451907e-07
Iter: 1397 loss: 2.39437611e-07
Iter: 1398 loss: 2.39416636e-07
Iter: 1399 loss: 2.39414476e-07
Iter: 1400 loss: 2.3940504e-07
Iter: 1401 loss: 2.39373804e-07
Iter: 1402 loss: 2.39449093e-07
Iter: 1403 loss: 2.3936451e-07
Iter: 1404 loss: 2.39345269e-07
Iter: 1405 loss: 2.39391341e-07
Iter: 1406 loss: 2.39336259e-07
Iter: 1407 loss: 2.39314431e-07
Iter: 1408 loss: 2.39394694e-07
Iter: 1409 loss: 2.39322418e-07
Iter: 1410 loss: 2.39310623e-07
Iter: 1411 loss: 2.39311305e-07
Iter: 1412 loss: 2.39300107e-07
Iter: 1413 loss: 2.39270435e-07
Iter: 1414 loss: 2.39546466e-07
Iter: 1415 loss: 2.39267052e-07
Iter: 1416 loss: 2.39256735e-07
Iter: 1417 loss: 2.39523871e-07
Iter: 1418 loss: 2.39249573e-07
Iter: 1419 loss: 2.39234623e-07
Iter: 1420 loss: 2.3921649e-07
Iter: 1421 loss: 2.39212653e-07
Iter: 1422 loss: 2.39182384e-07
Iter: 1423 loss: 2.39272197e-07
Iter: 1424 loss: 2.39160386e-07
Iter: 1425 loss: 2.39130202e-07
Iter: 1426 loss: 2.39151632e-07
Iter: 1427 loss: 2.39108601e-07
Iter: 1428 loss: 2.39084102e-07
Iter: 1429 loss: 2.39081345e-07
Iter: 1430 loss: 2.39069095e-07
Iter: 1431 loss: 2.39105304e-07
Iter: 1432 loss: 2.39056789e-07
Iter: 1433 loss: 2.39030925e-07
Iter: 1434 loss: 2.39060711e-07
Iter: 1435 loss: 2.3902885e-07
Iter: 1436 loss: 2.39006908e-07
Iter: 1437 loss: 2.39051133e-07
Iter: 1438 loss: 2.38990737e-07
Iter: 1439 loss: 2.3897789e-07
Iter: 1440 loss: 2.38974309e-07
Iter: 1441 loss: 2.38956602e-07
Iter: 1442 loss: 2.389217e-07
Iter: 1443 loss: 2.39132959e-07
Iter: 1444 loss: 2.38923747e-07
Iter: 1445 loss: 2.38902601e-07
Iter: 1446 loss: 2.3894961e-07
Iter: 1447 loss: 2.38880602e-07
Iter: 1448 loss: 2.38872786e-07
Iter: 1449 loss: 2.38868211e-07
Iter: 1450 loss: 2.38844905e-07
Iter: 1451 loss: 2.38810287e-07
Iter: 1452 loss: 2.39226e-07
Iter: 1453 loss: 2.38816e-07
Iter: 1454 loss: 2.38801306e-07
Iter: 1455 loss: 2.38777091e-07
Iter: 1456 loss: 2.39395519e-07
Iter: 1457 loss: 2.38779592e-07
Iter: 1458 loss: 2.38746253e-07
Iter: 1459 loss: 2.39047836e-07
Iter: 1460 loss: 2.38748271e-07
Iter: 1461 loss: 2.38719338e-07
Iter: 1462 loss: 2.3871354e-07
Iter: 1463 loss: 2.38704899e-07
Iter: 1464 loss: 2.38675085e-07
Iter: 1465 loss: 2.389055e-07
Iter: 1466 loss: 2.38676e-07
Iter: 1467 loss: 2.38635636e-07
Iter: 1468 loss: 2.38650557e-07
Iter: 1469 loss: 2.38611449e-07
Iter: 1470 loss: 2.38592463e-07
Iter: 1471 loss: 2.38662778e-07
Iter: 1472 loss: 2.38580981e-07
Iter: 1473 loss: 2.38542356e-07
Iter: 1474 loss: 2.38639046e-07
Iter: 1475 loss: 2.38539656e-07
Iter: 1476 loss: 2.38507937e-07
Iter: 1477 loss: 2.38735652e-07
Iter: 1478 loss: 2.3849941e-07
Iter: 1479 loss: 2.38486876e-07
Iter: 1480 loss: 2.38531641e-07
Iter: 1481 loss: 2.38470619e-07
Iter: 1482 loss: 2.38444699e-07
Iter: 1483 loss: 2.38479203e-07
Iter: 1484 loss: 2.38444755e-07
Iter: 1485 loss: 2.38424491e-07
Iter: 1486 loss: 2.38447711e-07
Iter: 1487 loss: 2.38417641e-07
Iter: 1488 loss: 2.38371115e-07
Iter: 1489 loss: 2.3844072e-07
Iter: 1490 loss: 2.38365175e-07
Iter: 1491 loss: 2.38334465e-07
Iter: 1492 loss: 2.3831376e-07
Iter: 1493 loss: 2.3831366e-07
Iter: 1494 loss: 2.3827198e-07
Iter: 1495 loss: 2.38689495e-07
Iter: 1496 loss: 2.38264278e-07
Iter: 1497 loss: 2.38242365e-07
Iter: 1498 loss: 2.38224473e-07
Iter: 1499 loss: 2.38217e-07
Iter: 1500 loss: 2.38183532e-07
Iter: 1501 loss: 2.3819041e-07
Iter: 1502 loss: 2.3817077e-07
Iter: 1503 loss: 2.38181144e-07
Iter: 1504 loss: 2.38152239e-07
Iter: 1505 loss: 2.38140444e-07
Iter: 1506 loss: 2.38138853e-07
Iter: 1507 loss: 2.38119526e-07
Iter: 1508 loss: 2.38105798e-07
Iter: 1509 loss: 2.38104093e-07
Iter: 1510 loss: 2.38070754e-07
Iter: 1511 loss: 2.38046965e-07
Iter: 1512 loss: 2.38035852e-07
Iter: 1513 loss: 2.38018018e-07
Iter: 1514 loss: 2.3796116e-07
Iter: 1515 loss: 2.38000439e-07
Iter: 1516 loss: 2.37936959e-07
Iter: 1517 loss: 2.37904231e-07
Iter: 1518 loss: 2.37900736e-07
Iter: 1519 loss: 2.37867226e-07
Iter: 1520 loss: 2.37908168e-07
Iter: 1521 loss: 2.37857748e-07
Iter: 1522 loss: 2.37834655e-07
Iter: 1523 loss: 2.3799268e-07
Iter: 1524 loss: 2.37823556e-07
Iter: 1525 loss: 2.37792008e-07
Iter: 1526 loss: 2.37788612e-07
Iter: 1527 loss: 2.37779801e-07
Iter: 1528 loss: 2.37741972e-07
Iter: 1529 loss: 2.37710921e-07
Iter: 1530 loss: 2.37694195e-07
Iter: 1531 loss: 2.37637991e-07
Iter: 1532 loss: 2.38152921e-07
Iter: 1533 loss: 2.37633913e-07
Iter: 1534 loss: 2.37590243e-07
Iter: 1535 loss: 2.38102089e-07
Iter: 1536 loss: 2.3758281e-07
Iter: 1537 loss: 2.37551092e-07
Iter: 1538 loss: 2.37523324e-07
Iter: 1539 loss: 2.3751079e-07
Iter: 1540 loss: 2.37464093e-07
Iter: 1541 loss: 2.38096561e-07
Iter: 1542 loss: 2.37471795e-07
Iter: 1543 loss: 2.37443956e-07
Iter: 1544 loss: 2.37402134e-07
Iter: 1545 loss: 2.37407932e-07
Iter: 1546 loss: 2.37376014e-07
Iter: 1547 loss: 2.37508758e-07
Iter: 1548 loss: 2.37371268e-07
Iter: 1549 loss: 2.3733179e-07
Iter: 1550 loss: 2.37367658e-07
Iter: 1551 loss: 2.37305386e-07
Iter: 1552 loss: 2.37280659e-07
Iter: 1553 loss: 2.37482254e-07
Iter: 1554 loss: 2.37278599e-07
Iter: 1555 loss: 2.3724192e-07
Iter: 1556 loss: 2.3748909e-07
Iter: 1557 loss: 2.37239078e-07
Iter: 1558 loss: 2.3722481e-07
Iter: 1559 loss: 2.37191585e-07
Iter: 1560 loss: 2.3719177e-07
Iter: 1561 loss: 2.3716791e-07
Iter: 1562 loss: 2.37166361e-07
Iter: 1563 loss: 2.3714702e-07
Iter: 1564 loss: 2.37131658e-07
Iter: 1565 loss: 2.37275543e-07
Iter: 1566 loss: 2.37120176e-07
Iter: 1567 loss: 2.37082219e-07
Iter: 1568 loss: 2.37343031e-07
Iter: 1569 loss: 2.37091228e-07
Iter: 1570 loss: 2.37054095e-07
Iter: 1571 loss: 2.37207161e-07
Iter: 1572 loss: 2.37042514e-07
Iter: 1573 loss: 2.37025745e-07
Iter: 1574 loss: 2.36980696e-07
Iter: 1575 loss: 2.38145446e-07
Iter: 1576 loss: 2.36984874e-07
Iter: 1577 loss: 2.36921e-07
Iter: 1578 loss: 2.37277021e-07
Iter: 1579 loss: 2.36914161e-07
Iter: 1580 loss: 2.3687933e-07
Iter: 1581 loss: 2.36885199e-07
Iter: 1582 loss: 2.36869283e-07
Iter: 1583 loss: 2.36858767e-07
Iter: 1584 loss: 2.36847498e-07
Iter: 1585 loss: 2.36823695e-07
Iter: 1586 loss: 2.36873831e-07
Iter: 1587 loss: 2.36800474e-07
Iter: 1588 loss: 2.36775207e-07
Iter: 1589 loss: 2.36928017e-07
Iter: 1590 loss: 2.36775961e-07
Iter: 1591 loss: 2.36742522e-07
Iter: 1592 loss: 2.36889022e-07
Iter: 1593 loss: 2.36733484e-07
Iter: 1594 loss: 2.36713078e-07
Iter: 1595 loss: 2.36719757e-07
Iter: 1596 loss: 2.3670988e-07
Iter: 1597 loss: 2.36667518e-07
Iter: 1598 loss: 2.36694419e-07
Iter: 1599 loss: 2.36628125e-07
Iter: 1600 loss: 2.36584484e-07
Iter: 1601 loss: 2.36599476e-07
Iter: 1602 loss: 2.36567e-07
Iter: 1603 loss: 2.3655241e-07
Iter: 1604 loss: 2.3653898e-07
Iter: 1605 loss: 2.36526489e-07
Iter: 1606 loss: 2.36486414e-07
Iter: 1607 loss: 2.37382e-07
Iter: 1608 loss: 2.36479707e-07
Iter: 1609 loss: 2.36434587e-07
Iter: 1610 loss: 2.36544e-07
Iter: 1611 loss: 2.36427127e-07
Iter: 1612 loss: 2.36396943e-07
Iter: 1613 loss: 2.36768699e-07
Iter: 1614 loss: 2.36384608e-07
Iter: 1615 loss: 2.36358275e-07
Iter: 1616 loss: 2.36479508e-07
Iter: 1617 loss: 2.36358503e-07
Iter: 1618 loss: 2.36324496e-07
Iter: 1619 loss: 2.36289e-07
Iter: 1620 loss: 2.36294298e-07
Iter: 1621 loss: 2.36232339e-07
Iter: 1622 loss: 2.36479266e-07
Iter: 1623 loss: 2.3622033e-07
Iter: 1624 loss: 2.36180369e-07
Iter: 1625 loss: 2.36617893e-07
Iter: 1626 loss: 2.36181648e-07
Iter: 1627 loss: 2.36155984e-07
Iter: 1628 loss: 2.36162549e-07
Iter: 1629 loss: 2.361339e-07
Iter: 1630 loss: 2.36106786e-07
Iter: 1631 loss: 2.36472715e-07
Iter: 1632 loss: 2.36115724e-07
Iter: 1633 loss: 2.36099794e-07
Iter: 1634 loss: 2.36073845e-07
Iter: 1635 loss: 2.36077199e-07
Iter: 1636 loss: 2.3605638e-07
Iter: 1637 loss: 2.36243125e-07
Iter: 1638 loss: 2.36062306e-07
Iter: 1639 loss: 2.36020213e-07
Iter: 1640 loss: 2.36060018e-07
Iter: 1641 loss: 2.36015069e-07
Iter: 1642 loss: 2.35988637e-07
Iter: 1643 loss: 2.35965189e-07
Iter: 1644 loss: 2.3594832e-07
Iter: 1645 loss: 2.35913063e-07
Iter: 1646 loss: 2.36078165e-07
Iter: 1647 loss: 2.35910619e-07
Iter: 1648 loss: 2.35884414e-07
Iter: 1649 loss: 2.35889019e-07
Iter: 1650 loss: 2.35850024e-07
Iter: 1651 loss: 2.35821474e-07
Iter: 1652 loss: 2.35827102e-07
Iter: 1653 loss: 2.35808443e-07
Iter: 1654 loss: 2.3596408e-07
Iter: 1655 loss: 2.35804237e-07
Iter: 1656 loss: 2.35765199e-07
Iter: 1657 loss: 2.35972138e-07
Iter: 1658 loss: 2.3577968e-07
Iter: 1659 loss: 2.35756971e-07
Iter: 1660 loss: 2.35731918e-07
Iter: 1661 loss: 2.35727484e-07
Iter: 1662 loss: 2.35706835e-07
Iter: 1663 loss: 2.35947198e-07
Iter: 1664 loss: 2.35697598e-07
Iter: 1665 loss: 2.35669361e-07
Iter: 1666 loss: 2.3563554e-07
Iter: 1667 loss: 2.35626771e-07
Iter: 1668 loss: 2.35601135e-07
Iter: 1669 loss: 2.35713358e-07
Iter: 1670 loss: 2.35594058e-07
Iter: 1671 loss: 2.35566844e-07
Iter: 1672 loss: 2.35924574e-07
Iter: 1673 loss: 2.35565068e-07
Iter: 1674 loss: 2.35555632e-07
Iter: 1675 loss: 2.35528802e-07
Iter: 1676 loss: 2.3552461e-07
Iter: 1677 loss: 2.3549498e-07
Iter: 1678 loss: 2.35528717e-07
Iter: 1679 loss: 2.35479e-07
Iter: 1680 loss: 2.3544257e-07
Iter: 1681 loss: 2.3580219e-07
Iter: 1682 loss: 2.35438634e-07
Iter: 1683 loss: 2.35408692e-07
Iter: 1684 loss: 2.35494213e-07
Iter: 1685 loss: 2.35400961e-07
Iter: 1686 loss: 2.35382373e-07
Iter: 1687 loss: 2.353664e-07
Iter: 1688 loss: 2.35350242e-07
Iter: 1689 loss: 2.35318751e-07
Iter: 1690 loss: 2.35540782e-07
Iter: 1691 loss: 2.35320769e-07
Iter: 1692 loss: 2.35298671e-07
Iter: 1693 loss: 2.35387574e-07
Iter: 1694 loss: 2.35277554e-07
Iter: 1695 loss: 2.35244258e-07
Iter: 1696 loss: 2.35276389e-07
Iter: 1697 loss: 2.35227077e-07
Iter: 1698 loss: 2.35197618e-07
Iter: 1699 loss: 2.35467269e-07
Iter: 1700 loss: 2.35196396e-07
Iter: 1701 loss: 2.35177708e-07
Iter: 1702 loss: 2.35154943e-07
Iter: 1703 loss: 2.35147269e-07
Iter: 1704 loss: 2.35122684e-07
Iter: 1705 loss: 2.35136014e-07
Iter: 1706 loss: 2.35111415e-07
Iter: 1707 loss: 2.35068342e-07
Iter: 1708 loss: 2.3565147e-07
Iter: 1709 loss: 2.35078829e-07
Iter: 1710 loss: 2.35047438e-07
Iter: 1711 loss: 2.35070729e-07
Iter: 1712 loss: 2.35015932e-07
Iter: 1713 loss: 2.34991788e-07
Iter: 1714 loss: 2.35167974e-07
Iter: 1715 loss: 2.34983219e-07
Iter: 1716 loss: 2.34964944e-07
Iter: 1717 loss: 2.34964176e-07
Iter: 1718 loss: 2.34955294e-07
Iter: 1719 loss: 2.34946825e-07
Iter: 1720 loss: 2.34936181e-07
Iter: 1721 loss: 2.34930809e-07
Iter: 1722 loss: 2.34924798e-07
Iter: 1723 loss: 2.34918886e-07
Iter: 1724 loss: 2.3490179e-07
Iter: 1725 loss: 2.34891687e-07
Iter: 1726 loss: 2.34873681e-07
Iter: 1727 loss: 2.34866974e-07
Iter: 1728 loss: 2.34859385e-07
Iter: 1729 loss: 2.34834602e-07
Iter: 1730 loss: 2.35039124e-07
Iter: 1731 loss: 2.34829699e-07
Iter: 1732 loss: 2.34802187e-07
Iter: 1733 loss: 2.34823e-07
Iter: 1734 loss: 2.34786441e-07
Iter: 1735 loss: 2.34762268e-07
Iter: 1736 loss: 2.34803878e-07
Iter: 1737 loss: 2.34751241e-07
Iter: 1738 loss: 2.34721441e-07
Iter: 1739 loss: 2.34995738e-07
Iter: 1740 loss: 2.34723203e-07
Iter: 1741 loss: 2.34707969e-07
Iter: 1742 loss: 2.34682574e-07
Iter: 1743 loss: 2.35206812e-07
Iter: 1744 loss: 2.34680329e-07
Iter: 1745 loss: 2.34662053e-07
Iter: 1746 loss: 2.34787564e-07
Iter: 1747 loss: 2.34663247e-07
Iter: 1748 loss: 2.34628359e-07
Iter: 1749 loss: 2.3466481e-07
Iter: 1750 loss: 2.3462961e-07
Iter: 1751 loss: 2.34605849e-07
Iter: 1752 loss: 2.34804e-07
Iter: 1753 loss: 2.34595745e-07
Iter: 1754 loss: 2.34598872e-07
Iter: 1755 loss: 2.34593458e-07
Iter: 1756 loss: 2.34588015e-07
Iter: 1757 loss: 2.34588583e-07
Iter: 1758 loss: 2.34598232e-07
Iter: 1759 loss: 2.34590971e-07
Iter: 1760 loss: 2.34582814e-07
Iter: 1761 loss: 2.3459603e-07
Iter: 1762 loss: 2.34589663e-07
Iter: 1763 loss: 2.34596257e-07
Iter: 1764 loss: 2.34595319e-07
Iter: 1765 loss: 2.34592619e-07
Iter: 1766 loss: 2.34593884e-07
Iter: 1767 loss: 2.34589933e-07
Iter: 1768 loss: 2.34592022e-07
Iter: 1769 loss: 2.34593898e-07
Iter: 1770 loss: 2.34593983e-07
Iter: 1771 loss: 2.34596286e-07
Iter: 1772 loss: 2.34595703e-07
Iter: 1773 loss: 2.34595518e-07
Iter: 1774 loss: 2.34595859e-07
Iter: 1775 loss: 2.34596328e-07
Iter: 1776 loss: 2.34596499e-07
Iter: 1777 loss: 2.34595859e-07
Iter: 1778 loss: 2.34596499e-07
Iter: 1779 loss: 2.34595859e-07
Iter: 1780 loss: 2.34572127e-07
Iter: 1781 loss: 2.34607256e-07
Iter: 1782 loss: 2.34556978e-07
Iter: 1783 loss: 2.3453228e-07
Iter: 1784 loss: 2.34519689e-07
Iter: 1785 loss: 2.34509031e-07
Iter: 1786 loss: 2.34486578e-07
Iter: 1787 loss: 2.34481945e-07
Iter: 1788 loss: 2.34468615e-07
Iter: 1789 loss: 2.34445196e-07
Iter: 1790 loss: 2.3507323e-07
Iter: 1791 loss: 2.34447114e-07
Iter: 1792 loss: 2.34413008e-07
Iter: 1793 loss: 2.34415424e-07
Iter: 1794 loss: 2.34402052e-07
Iter: 1795 loss: 2.3439793e-07
Iter: 1796 loss: 2.34393454e-07
Iter: 1797 loss: 2.34363512e-07
Iter: 1798 loss: 2.34423823e-07
Iter: 1799 loss: 2.34356364e-07
Iter: 1800 loss: 2.34315962e-07
Iter: 1801 loss: 2.34454589e-07
Iter: 1802 loss: 2.34314058e-07
Iter: 1803 loss: 2.34284201e-07
Iter: 1804 loss: 2.3425342e-07
Iter: 1805 loss: 2.34250052e-07
Iter: 1806 loss: 2.34230399e-07
Iter: 1807 loss: 2.34336284e-07
Iter: 1808 loss: 2.34207675e-07
Iter: 1809 loss: 2.34182039e-07
Iter: 1810 loss: 2.34493456e-07
Iter: 1811 loss: 2.34183403e-07
Iter: 1812 loss: 2.34160851e-07
Iter: 1813 loss: 2.34274594e-07
Iter: 1814 loss: 2.3416321e-07
Iter: 1815 loss: 2.341514e-07
Iter: 1816 loss: 2.34156559e-07
Iter: 1817 loss: 2.34139719e-07
Iter: 1818 loss: 2.34128564e-07
Iter: 1819 loss: 2.34214369e-07
Iter: 1820 loss: 2.34122354e-07
Iter: 1821 loss: 2.34104618e-07
Iter: 1822 loss: 2.34129388e-07
Iter: 1823 loss: 2.34101151e-07
Iter: 1824 loss: 2.3408731e-07
Iter: 1825 loss: 2.34090322e-07
Iter: 1826 loss: 2.34065595e-07
Iter: 1827 loss: 2.34038083e-07
Iter: 1828 loss: 2.34231962e-07
Iter: 1829 loss: 2.34043156e-07
Iter: 1830 loss: 2.34029201e-07
Iter: 1831 loss: 2.33997781e-07
Iter: 1832 loss: 2.3463798e-07
Iter: 1833 loss: 2.33999771e-07
Iter: 1834 loss: 2.33977261e-07
Iter: 1835 loss: 2.3397115e-07
Iter: 1836 loss: 2.33949564e-07
Iter: 1837 loss: 2.3392866e-07
Iter: 1838 loss: 2.33926897e-07
Iter: 1839 loss: 2.33893928e-07
Iter: 1840 loss: 2.33963405e-07
Iter: 1841 loss: 2.33886794e-07
Iter: 1842 loss: 2.33857179e-07
Iter: 1843 loss: 2.33889921e-07
Iter: 1844 loss: 2.33845768e-07
Iter: 1845 loss: 2.33824977e-07
Iter: 1846 loss: 2.3381547e-07
Iter: 1847 loss: 2.33817531e-07
Iter: 1848 loss: 2.33815086e-07
Iter: 1849 loss: 2.3380619e-07
Iter: 1850 loss: 2.33809473e-07
Iter: 1851 loss: 2.33818099e-07
Iter: 1852 loss: 2.33815399e-07
Iter: 1853 loss: 2.33812187e-07
Iter: 1854 loss: 2.33809374e-07
Iter: 1855 loss: 2.33815939e-07
Iter: 1856 loss: 2.33814546e-07
Iter: 1857 loss: 2.33811051e-07
Iter: 1858 loss: 2.33813665e-07
Iter: 1859 loss: 2.3381503e-07
Iter: 1860 loss: 2.33813907e-07
Iter: 1861 loss: 2.33814035e-07
Iter: 1862 loss: 2.33814973e-07
Iter: 1863 loss: 2.33815882e-07
Iter: 1864 loss: 2.33814049e-07
Iter: 1865 loss: 2.33814831e-07
Iter: 1866 loss: 2.33814774e-07
Iter: 1867 loss: 2.33815882e-07
Iter: 1868 loss: 2.33815896e-07
Iter: 1869 loss: 2.33816166e-07
Iter: 1870 loss: 2.33816195e-07
Iter: 1871 loss: 2.33816195e-07
Iter: 1872 loss: 2.33814774e-07
Iter: 1873 loss: 2.33763473e-07
Iter: 1874 loss: 2.34087452e-07
Iter: 1875 loss: 2.33765832e-07
Iter: 1876 loss: 2.33750043e-07
Iter: 1877 loss: 2.33747272e-07
Iter: 1878 loss: 2.33732209e-07
Iter: 1879 loss: 2.33722659e-07
Iter: 1880 loss: 2.33701755e-07
Iter: 1881 loss: 2.33701229e-07
Iter: 1882 loss: 2.33683181e-07
Iter: 1883 loss: 2.33679231e-07
Iter: 1884 loss: 2.336417e-07
Iter: 1885 loss: 2.33786068e-07
Iter: 1886 loss: 2.33640293e-07
Iter: 1887 loss: 2.3362729e-07
Iter: 1888 loss: 2.3361919e-07
Iter: 1889 loss: 2.33609796e-07
Iter: 1890 loss: 2.33583236e-07
Iter: 1891 loss: 2.3390011e-07
Iter: 1892 loss: 2.33579584e-07
Iter: 1893 loss: 2.33542025e-07
Iter: 1894 loss: 2.33512154e-07
Iter: 1895 loss: 2.3350519e-07
Iter: 1896 loss: 2.33474751e-07
Iter: 1897 loss: 2.3357353e-07
Iter: 1898 loss: 2.33461293e-07
Iter: 1899 loss: 2.33435628e-07
Iter: 1900 loss: 2.33528056e-07
Iter: 1901 loss: 2.33416131e-07
Iter: 1902 loss: 2.33387581e-07
Iter: 1903 loss: 2.33392129e-07
Iter: 1904 loss: 2.33365824e-07
Iter: 1905 loss: 2.33359486e-07
Iter: 1906 loss: 2.33362925e-07
Iter: 1907 loss: 2.3333898e-07
Iter: 1908 loss: 2.33354143e-07
Iter: 1909 loss: 2.33326233e-07
Iter: 1910 loss: 2.33295324e-07
Iter: 1911 loss: 2.33473557e-07
Iter: 1912 loss: 2.3328019e-07
Iter: 1913 loss: 2.33264629e-07
Iter: 1914 loss: 2.33234829e-07
Iter: 1915 loss: 2.33226686e-07
Iter: 1916 loss: 2.33196189e-07
Iter: 1917 loss: 2.33690258e-07
Iter: 1918 loss: 2.33196971e-07
Iter: 1919 loss: 2.3317773e-07
Iter: 1920 loss: 2.33218316e-07
Iter: 1921 loss: 2.33163192e-07
Iter: 1922 loss: 2.3314351e-07
Iter: 1923 loss: 2.33232399e-07
Iter: 1924 loss: 2.33143211e-07
Iter: 1925 loss: 2.33122464e-07
Iter: 1926 loss: 2.33185801e-07
Iter: 1927 loss: 2.33109461e-07
Iter: 1928 loss: 2.33098831e-07
Iter: 1929 loss: 2.33097168e-07
Iter: 1930 loss: 2.33076818e-07
Iter: 1931 loss: 2.3305823e-07
Iter: 1932 loss: 2.33054465e-07
Iter: 1933 loss: 2.33044688e-07
Iter: 1934 loss: 2.33002083e-07
Iter: 1935 loss: 2.33086212e-07
Iter: 1936 loss: 2.32996882e-07
Iter: 1937 loss: 2.32947983e-07
Iter: 1938 loss: 2.33422313e-07
Iter: 1939 loss: 2.32944288e-07
Iter: 1940 loss: 2.32919561e-07
Iter: 1941 loss: 2.32909088e-07
Iter: 1942 loss: 2.32887601e-07
Iter: 1943 loss: 2.32868047e-07
Iter: 1944 loss: 2.33056682e-07
Iter: 1945 loss: 2.32858298e-07
Iter: 1946 loss: 2.32809938e-07
Iter: 1947 loss: 2.32908633e-07
Iter: 1948 loss: 2.32800943e-07
Iter: 1949 loss: 2.3278659e-07
Iter: 1950 loss: 2.32814102e-07
Iter: 1951 loss: 2.32763185e-07
Iter: 1952 loss: 2.32723792e-07
Iter: 1953 loss: 2.32826778e-07
Iter: 1954 loss: 2.32715436e-07
Iter: 1955 loss: 2.32699307e-07
Iter: 1956 loss: 2.3271987e-07
Iter: 1957 loss: 2.32679582e-07
Iter: 1958 loss: 2.32633e-07
Iter: 1959 loss: 2.32834367e-07
Iter: 1960 loss: 2.3263182e-07
Iter: 1961 loss: 2.32602133e-07
Iter: 1962 loss: 2.32623904e-07
Iter: 1963 loss: 2.32587368e-07
Iter: 1964 loss: 2.32562215e-07
Iter: 1965 loss: 2.32632829e-07
Iter: 1966 loss: 2.32535768e-07
Iter: 1967 loss: 2.32513884e-07
Iter: 1968 loss: 2.3258535e-07
Iter: 1969 loss: 2.32518e-07
Iter: 1970 loss: 2.32488873e-07
Iter: 1971 loss: 2.32640417e-07
Iter: 1972 loss: 2.32481227e-07
Iter: 1973 loss: 2.32466888e-07
Iter: 1974 loss: 2.32468921e-07
Iter: 1975 loss: 2.32449e-07
Iter: 1976 loss: 2.32420959e-07
Iter: 1977 loss: 2.32423517e-07
Iter: 1978 loss: 2.32411537e-07
Iter: 1979 loss: 2.32373537e-07
Iter: 1980 loss: 2.32854362e-07
Iter: 1981 loss: 2.32372315e-07
Iter: 1982 loss: 2.32350061e-07
Iter: 1983 loss: 2.32310569e-07
Iter: 1984 loss: 2.33268636e-07
Iter: 1985 loss: 2.32309603e-07
Iter: 1986 loss: 2.32283398e-07
Iter: 1987 loss: 2.32286467e-07
Iter: 1988 loss: 2.32249519e-07
Iter: 1989 loss: 2.32224096e-07
Iter: 1990 loss: 2.32217388e-07
Iter: 1991 loss: 2.3221223e-07
Iter: 1992 loss: 2.32199795e-07
Iter: 1993 loss: 2.32178138e-07
Iter: 1994 loss: 2.32214774e-07
Iter: 1995 loss: 2.32166485e-07
Iter: 1996 loss: 2.32142767e-07
Iter: 1997 loss: 2.32158413e-07
Iter: 1998 loss: 2.32133e-07
Iter: 1999 loss: 2.32113607e-07
Iter: 2000 loss: 2.32379591e-07
Iter: 2001 loss: 2.32111816e-07
Iter: 2002 loss: 2.32090855e-07
Iter: 2003 loss: 2.32116747e-07
Iter: 2004 loss: 2.32079316e-07
Iter: 2005 loss: 2.32059932e-07
Iter: 2006 loss: 2.32108022e-07
Iter: 2007 loss: 2.32037337e-07
Iter: 2008 loss: 2.32015e-07
Iter: 2009 loss: 2.32049445e-07
Iter: 2010 loss: 2.32003259e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi0_phi1.2
+ date
Tue Oct 20 16:52:20 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1 --function f1 --psi 0 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 100000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a04edea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4b28f32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4b28f99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a048f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a048f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a04cf6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a0231378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a03da6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a03da730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a03da1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a04161e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a011ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a01079d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a02c92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a0107ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a00e8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4546feb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4546fed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a0165ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a01e4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a01e7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a04552f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a018f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a0191d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a0191bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a0191d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a0191268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4546e7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4546e7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a003df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a00661e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a00667b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4a0068840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb454785510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4545e4ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb4545c3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.0022082936
test_loss: 0.0018610534
train_loss: 0.0017895276
test_loss: 0.0018201636
train_loss: 0.0017459125
test_loss: 0.0016511412
train_loss: 0.0016395549
test_loss: 0.001836603
train_loss: 0.0017499535
test_loss: 0.0016510009
train_loss: 0.0016129431
test_loss: 0.0017183069
train_loss: 0.0015513952
test_loss: 0.0017010166
train_loss: 0.00185349
test_loss: 0.001788965
train_loss: 0.0018027457
test_loss: 0.0016282568
train_loss: 0.0017960165
test_loss: 0.001683417
train_loss: 0.0016405636
test_loss: 0.0015060337
train_loss: 0.0017609656
test_loss: 0.0016874466
train_loss: 0.0016107449
test_loss: 0.0015516436
train_loss: 0.0016616401
test_loss: 0.0018708942
train_loss: 0.0014476143
test_loss: 0.0014976297
train_loss: 0.0014223197
test_loss: 0.0017587971
train_loss: 0.001498953
test_loss: 0.0014069128
train_loss: 0.0015659701
test_loss: 0.0015269129
train_loss: 0.0016771443
test_loss: 0.0015612798
train_loss: 0.0014369716
test_loss: 0.0016723154
train_loss: 0.0013993146
test_loss: 0.0016972601
train_loss: 0.0014155494
test_loss: 0.0016032229
train_loss: 0.0015809513
test_loss: 0.0017410916
train_loss: 0.0014687683
test_loss: 0.0017311666
train_loss: 0.0014817022
test_loss: 0.001484834
train_loss: 0.0016412633
test_loss: 0.0013887982
train_loss: 0.0016407494
test_loss: 0.0016959592
train_loss: 0.0014288619
test_loss: 0.0014614585
train_loss: 0.0016180985
test_loss: 0.0018251928
train_loss: 0.0015454013
test_loss: 0.0014821232
train_loss: 0.001283373
test_loss: 0.0014322286
train_loss: 0.0014180468
test_loss: 0.0016059994
train_loss: 0.0015004019
test_loss: 0.0014085718
train_loss: 0.0015430542
test_loss: 0.0015319658
train_loss: 0.0015914079
test_loss: 0.0015434397
train_loss: 0.0016895902
test_loss: 0.0015715905
train_loss: 0.0015551363
test_loss: 0.0015628829
train_loss: 0.0014656027
test_loss: 0.0016653709
train_loss: 0.0015408513
test_loss: 0.0014117871
train_loss: 0.0016214481
test_loss: 0.0015980729
train_loss: 0.001556257
test_loss: 0.0017218177
train_loss: 0.0013721045
test_loss: 0.0013402931
train_loss: 0.002137681
test_loss: 0.001762838
train_loss: 0.0013866548
test_loss: 0.0014315244
train_loss: 0.0017362322
test_loss: 0.001523785
train_loss: 0.00166231
test_loss: 0.0013179932
train_loss: 0.0012898165
test_loss: 0.0013804913
train_loss: 0.0014119209
test_loss: 0.0014253238
train_loss: 0.0014833901
test_loss: 0.001454962
train_loss: 0.0013510968
test_loss: 0.0013908484
train_loss: 0.0016072709
test_loss: 0.0016068082
train_loss: 0.0015996358
test_loss: 0.0016156982
train_loss: 0.0014750032
test_loss: 0.0014553824
train_loss: 0.001567001
test_loss: 0.0013540769
train_loss: 0.0014310386
test_loss: 0.0013881394
train_loss: 0.0013297629
test_loss: 0.0014948323
train_loss: 0.0013175893
test_loss: 0.0014814793
train_loss: 0.0014778266
test_loss: 0.0018340043
train_loss: 0.0017293505
test_loss: 0.0015988292
train_loss: 0.001260031
test_loss: 0.0014102502
train_loss: 0.0014870495
test_loss: 0.0013967789
train_loss: 0.0015951158
test_loss: 0.0014750273
train_loss: 0.0013658438
test_loss: 0.0014119786
train_loss: 0.0014920713
test_loss: 0.0016826133
train_loss: 0.0014748017
test_loss: 0.0015407538
train_loss: 0.001595629
test_loss: 0.0014251026
train_loss: 0.001693737
test_loss: 0.0013290013
train_loss: 0.0012765888
test_loss: 0.0016434013
train_loss: 0.0014529991
test_loss: 0.0014231139
train_loss: 0.0014366321
test_loss: 0.0015358734
train_loss: 0.0012908009
test_loss: 0.0013896523
train_loss: 0.0015563959
test_loss: 0.0015289889
train_loss: 0.0014554239
test_loss: 0.0016339609
train_loss: 0.0014914398
test_loss: 0.0013873716
train_loss: 0.0014072966
test_loss: 0.0015065795
train_loss: 0.0014653646
test_loss: 0.0014569369
train_loss: 0.0016516715
test_loss: 0.0013432171
train_loss: 0.0014789246
test_loss: 0.0015458792
train_loss: 0.0013899291
test_loss: 0.0016125166
train_loss: 0.0012730906
test_loss: 0.00145026
train_loss: 0.0014430631
test_loss: 0.0012656715
train_loss: 0.0016745811
test_loss: 0.0014892779
train_loss: 0.0014361724
test_loss: 0.0014709865
train_loss: 0.0018435658
test_loss: 0.0015118823
train_loss: 0.0018239084
test_loss: 0.0013296789
train_loss: 0.0015013262
test_loss: 0.0015475396
train_loss: 0.0013941678
test_loss: 0.0013832983
train_loss: 0.0013421308
test_loss: 0.0014286039
train_loss: 0.0016177136
test_loss: 0.0014270353
train_loss: 0.001361061
test_loss: 0.001429408
train_loss: 0.001658398
test_loss: 0.0012880606
train_loss: 0.0015375386
test_loss: 0.0015429046
train_loss: 0.0013092029
test_loss: 0.0014020487
train_loss: 0.0014735376
test_loss: 0.0014541548
train_loss: 0.00130281
test_loss: 0.0014205753
train_loss: 0.0014257401
test_loss: 0.0014276672
train_loss: 0.0012251936
test_loss: 0.0012833154
train_loss: 0.0013937904
test_loss: 0.0012856364
train_loss: 0.0011704125
test_loss: 0.0012060513
train_loss: 0.0015115091
test_loss: 0.0013445141
train_loss: 0.0013521309
test_loss: 0.001667925
train_loss: 0.0014351497
test_loss: 0.0014280311
train_loss: 0.0014052398
test_loss: 0.0014509553
train_loss: 0.0015025036
test_loss: 0.0014088278
train_loss: 0.0013161094
test_loss: 0.0013188411
train_loss: 0.0015073641
test_loss: 0.0014046578
train_loss: 0.0012366609
test_loss: 0.0016484744
train_loss: 0.0012873977
test_loss: 0.0013363993
train_loss: 0.0012768558
test_loss: 0.0013670819
train_loss: 0.0013245258
test_loss: 0.0012746284
train_loss: 0.0015005142
test_loss: 0.0014977326
train_loss: 0.0013464234
test_loss: 0.001337717
train_loss: 0.0012672966
test_loss: 0.0012799568
train_loss: 0.0015659861
test_loss: 0.0014855091
train_loss: 0.0013434247
test_loss: 0.0012304516
train_loss: 0.001619651
test_loss: 0.001343203
train_loss: 0.0014277395
test_loss: 0.0015024922
train_loss: 0.0015261051
test_loss: 0.0015880228
train_loss: 0.0014827483
test_loss: 0.0014228082
train_loss: 0.0015756577
test_loss: 0.001321172
train_loss: 0.0012573346
test_loss: 0.0014450471
train_loss: 0.0015154144
test_loss: 0.0015661337
train_loss: 0.0012494145
test_loss: 0.0012881763
train_loss: 0.0013706441
test_loss: 0.0014139795
train_loss: 0.0011870429
test_loss: 0.0016226224
train_loss: 0.0013160233
test_loss: 0.0014630522
train_loss: 0.0014416979
test_loss: 0.0013550078
train_loss: 0.0013342963
test_loss: 0.0013418631
train_loss: 0.0015730348
test_loss: 0.0015027722
train_loss: 0.0015629997
test_loss: 0.0014597705
train_loss: 0.0012071957
test_loss: 0.0014157518
train_loss: 0.0015084264
test_loss: 0.0014263397
train_loss: 0.0013282229
test_loss: 0.0016403589
train_loss: 0.0016956303
test_loss: 0.0013216434
train_loss: 0.0014256595
test_loss: 0.0012501635
train_loss: 0.0011688578
test_loss: 0.0014240417
train_loss: 0.0013261108
test_loss: 0.0015010355
train_loss: 0.0013237959
test_loss: 0.0013345336
train_loss: 0.0015764737
test_loss: 0.0013615694
train_loss: 0.0016406251
test_loss: 0.0013478125
train_loss: 0.0013409731
test_loss: 0.001376592
train_loss: 0.0012494407
test_loss: 0.0011592763
train_loss: 0.0014586856
test_loss: 0.0014407994
train_loss: 0.0013154352
test_loss: 0.0015892673
train_loss: 0.0012774825
test_loss: 0.001281829
train_loss: 0.0012052834
test_loss: 0.0011906718
train_loss: 0.0013603368
test_loss: 0.0015541448
train_loss: 0.0013329801
test_loss: 0.0013154615
train_loss: 0.0014225137
test_loss: 0.0012835631
train_loss: 0.0013406565
test_loss: 0.0013806326
train_loss: 0.0014293534
test_loss: 0.0014994331
train_loss: 0.0012211703
test_loss: 0.001168926
train_loss: 0.0014374335
test_loss: 0.0012939443
train_loss: 0.0021645168
test_loss: 0.0021709825
train_loss: 0.0014393278
test_loss: 0.0014957921
train_loss: 0.001364266
test_loss: 0.0014851268
train_loss: 0.0014079815
test_loss: 0.0013207468
train_loss: 0.001333436
test_loss: 0.0014640699
train_loss: 0.0012552951
test_loss: 0.0013226762
train_loss: 0.0015671029
test_loss: 0.0013960534
train_loss: 0.0013342204
test_loss: 0.0013440583
train_loss: 0.0013932022
test_loss: 0.0013997746
train_loss: 0.0019866815
test_loss: 0.0014310182
train_loss: 0.001419613
test_loss: 0.0015488953
train_loss: 0.0014094516
test_loss: 0.0014075038
train_loss: 0.0013162303
test_loss: 0.0015684402
train_loss: 0.0012025184
test_loss: 0.0013319699
train_loss: 0.0012976952
test_loss: 0.0014460977