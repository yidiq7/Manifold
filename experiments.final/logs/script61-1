+ RUN=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='-2 -1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 4000 				 --batch_size 5000 				 --max_epochs 1000 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi -2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 4000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051ca55048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c993d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c993b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051ca550d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c9dd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c9e9ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c9dd048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c9ddd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c907b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c933c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c933a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c8abf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c842950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f052f1c5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c87f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c842ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c7ce620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c7ced08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c7ce8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c7b1e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f051c7b1378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04e82a0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04e82849d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04e8282ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04e82867b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04e8229730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04e81ec840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04e8284400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04e81a1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04e8286ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04c0103d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04c00d09d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04c00d08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04c00d0a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04c00d06a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04c003d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.039784405
test_loss: 0.038056828
train_loss: 0.034947332
test_loss: 0.033587035
train_loss: 0.031835444
test_loss: 0.029864654
train_loss: 0.027638085
test_loss: 0.02667365
train_loss: 0.024837688
test_loss: 0.023778588
train_loss: 0.021314105
test_loss: 0.020863002
train_loss: 0.018780041
test_loss: 0.018674606
train_loss: 0.016759735
test_loss: 0.016991919
train_loss: 0.014057897
test_loss: 0.01535464
train_loss: 0.012245956
test_loss: 0.014172707
train_loss: 0.0112225395
test_loss: 0.012731147
train_loss: 0.010796986
test_loss: 0.011663796
train_loss: 0.009942863
test_loss: 0.010812775
train_loss: 0.008700186
test_loss: 0.010454672
train_loss: 0.008587772
test_loss: 0.009718649
train_loss: 0.007753223
test_loss: 0.009253446
train_loss: 0.0071145156
test_loss: 0.008946139
train_loss: 0.007243614
test_loss: 0.008534482
train_loss: 0.0077238595
test_loss: 0.008395313
train_loss: 0.0069436245
test_loss: 0.007832793
train_loss: 0.006224666
test_loss: 0.007730553
train_loss: 0.006068634
test_loss: 0.007854725
train_loss: 0.005745095
test_loss: 0.0074380315
train_loss: 0.0059933253
test_loss: 0.0071718115
train_loss: 0.006403885
test_loss: 0.0075915544
train_loss: 0.005826886
test_loss: 0.007347136
train_loss: 0.0055780723
test_loss: 0.0072684586
train_loss: 0.0054473197
test_loss: 0.0068805628
train_loss: 0.005695747
test_loss: 0.0067985975
train_loss: 0.0058228937
test_loss: 0.0067080827
train_loss: 0.0057876166
test_loss: 0.006987538
train_loss: 0.005139845
test_loss: 0.0066367886
train_loss: 0.0057732956
test_loss: 0.0068481425
train_loss: 0.005030529
test_loss: 0.0064533604
train_loss: 0.0051099127
test_loss: 0.0064830673
train_loss: 0.00509331
test_loss: 0.0065393415
train_loss: 0.0050659305
test_loss: 0.006512946
train_loss: 0.0047118342
test_loss: 0.0062297937
train_loss: 0.0047680186
test_loss: 0.0064829444
train_loss: 0.0049725873
test_loss: 0.006323897
train_loss: 0.0049764174
test_loss: 0.0063007604
train_loss: 0.0050422717
test_loss: 0.00618172
train_loss: 0.004812279
test_loss: 0.006707772
train_loss: 0.0048773903
test_loss: 0.006439617
train_loss: 0.004944202
test_loss: 0.0063893376
train_loss: 0.004828564
test_loss: 0.006449548
train_loss: 0.004828351
test_loss: 0.006720462
train_loss: 0.005330174
test_loss: 0.0066384478
train_loss: 0.006015632
test_loss: 0.006255409
train_loss: 0.0051312163
test_loss: 0.006882485
train_loss: 0.005201499
test_loss: 0.0061880536
train_loss: 0.0049090297
test_loss: 0.006067014
train_loss: 0.0046513407
test_loss: 0.005831044
train_loss: 0.004341884
test_loss: 0.00597123
train_loss: 0.00441098
test_loss: 0.005775333
train_loss: 0.004594262
test_loss: 0.00587406
train_loss: 0.004567692
test_loss: 0.005858325
train_loss: 0.0043935515
test_loss: 0.005747693
train_loss: 0.0041512116
test_loss: 0.005862298
train_loss: 0.0042579398
test_loss: 0.0058538453
train_loss: 0.0048172
test_loss: 0.005954791
train_loss: 0.0042662844
test_loss: 0.0057308413
train_loss: 0.0044767256
test_loss: 0.00568405
train_loss: 0.0042336797
test_loss: 0.005652862
train_loss: 0.0052700215
test_loss: 0.0066515473
train_loss: 0.0057513276
test_loss: 0.006081153
train_loss: 0.0049223932
test_loss: 0.0057309987
train_loss: 0.0043305345
test_loss: 0.005668378
train_loss: 0.0043395786
test_loss: 0.005675037
train_loss: 0.0043675806
test_loss: 0.0055802804
train_loss: 0.004509158
test_loss: 0.0055776597
train_loss: 0.0039958013
test_loss: 0.0056196474
train_loss: 0.00499262
test_loss: 0.0065306863
train_loss: 0.0054446226
test_loss: 0.0059734476
train_loss: 0.0050414824
test_loss: 0.0056018615
train_loss: 0.0046950323
test_loss: 0.005580388
train_loss: 0.0043732366
test_loss: 0.0058469027
train_loss: 0.0038183522
test_loss: 0.0055123633
train_loss: 0.004629696
test_loss: 0.005389277
train_loss: 0.004249756
test_loss: 0.00546173
train_loss: 0.00445055
test_loss: 0.0058954395
train_loss: 0.004642695
test_loss: 0.0063375407
train_loss: 0.004359742
test_loss: 0.0059505883
train_loss: 0.0040941634
test_loss: 0.005638527
train_loss: 0.0041389465
test_loss: 0.0054241675
train_loss: 0.0042098286
test_loss: 0.0055278563
train_loss: 0.004419913
test_loss: 0.0058137854
train_loss: 0.004078232
test_loss: 0.0059568495
train_loss: 0.003919306
test_loss: 0.005730621
train_loss: 0.004835294
test_loss: 0.00561822
train_loss: 0.0047380105
test_loss: 0.0055612675
train_loss: 0.0039662533
test_loss: 0.005564881
train_loss: 0.003912161
test_loss: 0.0053560613
train_loss: 0.0040393225
test_loss: 0.00556072
train_loss: 0.004294716
test_loss: 0.0055194073
train_loss: 0.0039271163
test_loss: 0.0053517953
train_loss: 0.004173156
test_loss: 0.0053456137
train_loss: 0.0051618814
test_loss: 0.005637382
train_loss: 0.0047698403
test_loss: 0.005555421
train_loss: 0.003984155
test_loss: 0.0057344045
train_loss: 0.0039187935
test_loss: 0.0053598066
train_loss: 0.004277406
test_loss: 0.005319257
train_loss: 0.0042688632
test_loss: 0.006042209
train_loss: 0.0042369827
test_loss: 0.005641342
train_loss: 0.0038051505
test_loss: 0.005437955
train_loss: 0.0038390665
test_loss: 0.005534017
train_loss: 0.003869812
test_loss: 0.006220793
train_loss: 0.0040074983
test_loss: 0.0054590725
train_loss: 0.0040295683
test_loss: 0.0053947144
train_loss: 0.0038122274
test_loss: 0.0053883144
train_loss: 0.0040791156
test_loss: 0.005365213
train_loss: 0.0040817014
test_loss: 0.0052860305
train_loss: 0.004078728
test_loss: 0.005433979
train_loss: 0.0038177215
test_loss: 0.0052307765
train_loss: 0.004246967
test_loss: 0.005417254
train_loss: 0.0041302945
test_loss: 0.0057941903
train_loss: 0.003999849
test_loss: 0.0056414707
train_loss: 0.0038685303
test_loss: 0.00576229
train_loss: 0.0039096847
test_loss: 0.006137559
train_loss: 0.004223389
test_loss: 0.005697328
train_loss: 0.00433427
test_loss: 0.005664183
train_loss: 0.0042037014
test_loss: 0.0059573683
train_loss: 0.004074639
test_loss: 0.005691186
train_loss: 0.003961807
test_loss: 0.005796123
train_loss: 0.0038278885
test_loss: 0.005445335
train_loss: 0.0036714987
test_loss: 0.0052753114
train_loss: 0.0035430246
test_loss: 0.0053635896
train_loss: 0.0037590638
test_loss: 0.005466324
train_loss: 0.0036501433
test_loss: 0.005383212
train_loss: 0.0036740322
test_loss: 0.0055322465
train_loss: 0.0037098478
test_loss: 0.0052078823
train_loss: 0.0036506047
test_loss: 0.005406175
train_loss: 0.0038529036
test_loss: 0.00521766
train_loss: 0.003912863
test_loss: 0.0051809484
train_loss: 0.0042338218
test_loss: 0.0053552436
train_loss: 0.0041879625
test_loss: 0.0052790344
train_loss: 0.0036190813
test_loss: 0.0052073705
train_loss: 0.0036217608
test_loss: 0.0052166977
train_loss: 0.0037638429
test_loss: 0.005363605
train_loss: 0.003537864
test_loss: 0.0052560903
train_loss: 0.0037730916
test_loss: 0.0052063195
train_loss: 0.0034500477
test_loss: 0.0051045776
train_loss: 0.003760721
test_loss: 0.0053571262
train_loss: 0.0035637692
test_loss: 0.0053023333
train_loss: 0.0037421947
test_loss: 0.005235743
train_loss: 0.003951015
test_loss: 0.005310044
train_loss: 0.0036719376
test_loss: 0.00530441
train_loss: 0.0035269677
test_loss: 0.0053183376
train_loss: 0.0038302545
test_loss: 0.0054799854
train_loss: 0.003711924
test_loss: 0.0052151037
train_loss: 0.003674677
test_loss: 0.005363423
train_loss: 0.003906197
test_loss: 0.005384478
train_loss: 0.0042109773
test_loss: 0.0055541703
train_loss: 0.0034059035
test_loss: 0.005149844
train_loss: 0.0035221723
test_loss: 0.0052082264
train_loss: 0.0038847304
test_loss: 0.005531733
train_loss: 0.0034146346
test_loss: 0.0054819766
train_loss: 0.0033093095
test_loss: 0.0054442105
train_loss: 0.0035247344
test_loss: 0.005277269
train_loss: 0.0035884948
test_loss: 0.00517803
train_loss: 0.0038981878
test_loss: 0.0055670394
train_loss: 0.0034811322
test_loss: 0.005528899
train_loss: 0.0035397776
test_loss: 0.0055195475
train_loss: 0.0038318837
test_loss: 0.0061193053
train_loss: 0.0041382643
test_loss: 0.0056489827
train_loss: 0.0035921282
test_loss: 0.0053861886
train_loss: 0.0034036615
test_loss: 0.005423806
train_loss: 0.0034792754
test_loss: 0.0051314444
train_loss: 0.003650245
test_loss: 0.005251015
train_loss: 0.0035426323
test_loss: 0.0051309564
train_loss: 0.0035449362
test_loss: 0.005180201
train_loss: 0.0036388787
test_loss: 0.0051286523
train_loss: 0.0033103847
test_loss: 0.0053332164
train_loss: 0.0034642322
test_loss: 0.00519565
train_loss: 0.003218562
test_loss: 0.005389168
train_loss: 0.0034285719
test_loss: 0.0053406362
train_loss: 0.0034284424
test_loss: 0.0055764513
train_loss: 0.0036280018
test_loss: 0.0055828765
train_loss: 0.0038712597
test_loss: 0.005730053
train_loss: 0.004259441
test_loss: 0.0054082675
train_loss: 0.0041134553
test_loss: 0.005342932
train_loss: 0.0040116035
test_loss: 0.005294152
train_loss: 0.003572415
test_loss: 0.00531235
train_loss: 0.0033930512
test_loss: 0.0052105314
train_loss: 0.0035413583
test_loss: 0.0052692425
train_loss: 0.0033150557
test_loss: 0.0052721645
train_loss: 0.003395101
test_loss: 0.0051402883
train_loss: 0.00345984
test_loss: 0.005067446
train_loss: 0.0033094627
test_loss: 0.0050978973
train_loss: 0.003594039
test_loss: 0.0052827396
train_loss: 0.0031699352
test_loss: 0.005121804
train_loss: 0.0038967854
test_loss: 0.0051617264
train_loss: 0.003684077
test_loss: 0.005330673
train_loss: 0.0032706445
test_loss: 0.0050064386
train_loss: 0.003374184
test_loss: 0.005274358
train_loss: 0.0033037928
test_loss: 0.004971396
train_loss: 0.003171343
test_loss: 0.0050365427
train_loss: 0.0033664615
test_loss: 0.0054197446
train_loss: 0.00334974
test_loss: 0.005156819
train_loss: 0.003830654
test_loss: 0.0051233955
train_loss: 0.0034569171
test_loss: 0.0052232
train_loss: 0.0035320735
test_loss: 0.0052194656
train_loss: 0.003189334
test_loss: 0.005126896
train_loss: 0.003146723
test_loss: 0.005073021
train_loss: 0.0033991756
test_loss: 0.005244048
train_loss: 0.0035837055
test_loss: 0.0051299552
train_loss: 0.0036239016
test_loss: 0.0052668112
train_loss: 0.0031562399
test_loss: 0.00504424
train_loss: 0.0036276404
test_loss: 0.0053751855
train_loss: 0.0031443296
test_loss: 0.005385175
train_loss: 0.0029728254
test_loss: 0.005182766
train_loss: 0.0031211586
test_loss: 0.0052672913
train_loss: 0.0033118578
test_loss: 0.005118416
train_loss: 0.0034152982
test_loss: 0.0056288196
train_loss: 0.0035230587
test_loss: 0.005520414
train_loss: 0.0036949138
test_loss: 0.0052647656
train_loss: 0.003170643
test_loss: 0.005468437
train_loss: 0.0032264707
test_loss: 0.005503539
train_loss: 0.0032695425
test_loss: 0.005470789
train_loss: 0.003648143
test_loss: 0.0055240616
train_loss: 0.0036851151
test_loss: 0.0053883432
train_loss: 0.0036511458
test_loss: 0.0054658195
train_loss: 0.0034165038
test_loss: 0.0051587163
train_loss: 0.0036069723
test_loss: 0.00542269
train_loss: 0.003918699
test_loss: 0.005281279
train_loss: 0.0036542893
test_loss: 0.005132783
train_loss: 0.003429341
test_loss: 0.0051758243
train_loss: 0.0034454074
test_loss: 0.005243986
train_loss: 0.003452971
test_loss: 0.0054322444
train_loss: 0.0033896621
test_loss: 0.0052236994
train_loss: 0.003370232
test_loss: 0.005293636
train_loss: 0.0030465228
test_loss: 0.005316327
train_loss: 0.0034436027
test_loss: 0.0056332406
train_loss: 0.0033894177
test_loss: 0.005735206
train_loss: 0.0037774027
test_loss: 0.0055069095
train_loss: 0.0038378246
test_loss: 0.005174852
train_loss: 0.0033219808
test_loss: 0.0052550477
train_loss: 0.0031731678
test_loss: 0.0051317057
train_loss: 0.0031599589
test_loss: 0.0051067253
train_loss: 0.002973527
test_loss: 0.0050034244
train_loss: 0.002933884
test_loss: 0.0049756905
train_loss: 0.0028715318
test_loss: 0.0050408537
train_loss: 0.0031376816
test_loss: 0.0050938656
train_loss: 0.0029345737
test_loss: 0.004959297
train_loss: 0.0029381835
test_loss: 0.004962043
train_loss: 0.0031671585
test_loss: 0.0049650385
train_loss: 0.0030739424
test_loss: 0.005066687
train_loss: 0.0029929138
test_loss: 0.0053853914
train_loss: 0.0030953428
test_loss: 0.005529331
train_loss: 0.0030909397
test_loss: 0.005270823
train_loss: 0.002864571
test_loss: 0.004960401
train_loss: 0.0027416635
test_loss: 0.005040986
train_loss: 0.0031414547
test_loss: 0.0050388896
train_loss: 0.0032862816
test_loss: 0.005130972
train_loss: 0.003135182
test_loss: 0.00509485
train_loss: 0.0027880783
test_loss: 0.0050631957
train_loss: 0.0030292617
test_loss: 0.0051773153
train_loss: 0.0029460993
test_loss: 0.0049880045
train_loss: 0.0032244094
test_loss: 0.0050857672
train_loss: 0.0032391287
test_loss: 0.00500073
train_loss: 0.003080148
test_loss: 0.0052636736
train_loss: 0.0029518714
test_loss: 0.005011063
train_loss: 0.002955392
test_loss: 0.005036657
train_loss: 0.0031964611
test_loss: 0.005005409
train_loss: 0.0030009337
test_loss: 0.0051299264
train_loss: 0.0028665513
test_loss: 0.0052561727
train_loss: 0.002681742
test_loss: 0.0051043285
train_loss: 0.0030607758
test_loss: 0.005152313
train_loss: 0.0030987803
test_loss: 0.005508421
train_loss: 0.0030897213
test_loss: 0.0054350263
train_loss: 0.0035639605
test_loss: 0.005442228
train_loss: 0.0032363613
test_loss: 0.0052703163
train_loss: 0.0031137206
test_loss: 0.005158742
train_loss: 0.0027040862
test_loss: 0.0050318106
train_loss: 0.0028621685
test_loss: 0.0051887496
train_loss: 0.0034144425
test_loss: 0.00523763
train_loss: 0.0033352114
test_loss: 0.005384229
train_loss: 0.0032910756
test_loss: 0.005210926
train_loss: 0.0029027155
test_loss: 0.0051009757
train_loss: 0.0028003366
test_loss: 0.0050748326
train_loss: 0.0030268473
test_loss: 0.005011124
train_loss: 0.0026453743
test_loss: 0.00500449
train_loss: 0.0031181783
test_loss: 0.0052408637
train_loss: 0.0029424173
test_loss: 0.005501791
train_loss: 0.0035357447
test_loss: 0.00520669
train_loss: 0.0028747034
test_loss: 0.0050369124
train_loss: 0.0028825705
test_loss: 0.005116014
train_loss: 0.0029516954
test_loss: 0.0049500894
train_loss: 0.002720382
test_loss: 0.005087813
train_loss: 0.0029942628
test_loss: 0.0051665693
train_loss: 0.0031087582
test_loss: 0.0052819317
train_loss: 0.003243853
test_loss: 0.005366087
train_loss: 0.0030790074
test_loss: 0.005271455
train_loss: 0.0033029518
test_loss: 0.0053530876
train_loss: 0.003161409
test_loss: 0.0051214434
train_loss: 0.002978747
test_loss: 0.0050732964
train_loss: 0.0029132068
test_loss: 0.0051278425
train_loss: 0.0026086504
test_loss: 0.005125463
train_loss: 0.002759689
test_loss: 0.005037537
train_loss: 0.0026915057
test_loss: 0.0049736663
train_loss: 0.0030632783
test_loss: 0.0049670897
train_loss: 0.002956765
test_loss: 0.0050980104
train_loss: 0.0029363493
test_loss: 0.0049980176
train_loss: 0.0026793987
test_loss: 0.005142984
train_loss: 0.003166031
test_loss: 0.005565677
train_loss: 0.0037714676
test_loss: 0.0051383995
train_loss: 0.003052536
test_loss: 0.004962609
train_loss: 0.002785665
test_loss: 0.0050486755
train_loss: 0.0027159248
test_loss: 0.005089992
train_loss: 0.0028743641
test_loss: 0.005282725
train_loss: 0.0029773433
test_loss: 0.0053792647
train_loss: 0.0028316984
test_loss: 0.0050808857
train_loss: 0.0024897973
test_loss: 0.0050960854
train_loss: 0.002740712
test_loss: 0.005241597
train_loss: 0.0026662035
test_loss: 0.0050795223
train_loss: 0.0026025171
test_loss: 0.005061918
train_loss: 0.0027136717
test_loss: 0.00503817
train_loss: 0.0026769848
test_loss: 0.004930266
train_loss: 0.0025592626
test_loss: 0.005000878
train_loss: 0.0026571292
test_loss: 0.0049568024
train_loss: 0.0025831617
test_loss: 0.0048831003
train_loss: 0.0027117631
test_loss: 0.0049834508
train_loss: 0.0028887084
test_loss: 0.0050946735
train_loss: 0.0031983643
test_loss: 0.005072811
train_loss: 0.0025034393
test_loss: 0.0053049885
train_loss: 0.002761359
test_loss: 0.005078794
train_loss: 0.0024585628
test_loss: 0.004924364
train_loss: 0.0029587147
test_loss: 0.0049325544
train_loss: 0.0028607012
test_loss: 0.004973505
train_loss: 0.0030816942
test_loss: 0.0049204873
train_loss: 0.0024830038
test_loss: 0.004977805
train_loss: 0.0026102227
test_loss: 0.0049048252
train_loss: 0.0029197815
test_loss: 0.005079659
train_loss: 0.0032503156
test_loss: 0.005172666
train_loss: 0.0032431993
test_loss: 0.0051275156
train_loss: 0.0032355012
test_loss: 0.0049717766
train_loss: 0.0029420513
test_loss: 0.0050865617
train_loss: 0.0029619643
test_loss: 0.0050132247
train_loss: 0.002700552
test_loss: 0.005073716
train_loss: 0.0026394208
test_loss: 0.0049427226
train_loss: 0.0027245488
test_loss: 0.004956838
train_loss: 0.002793635
test_loss: 0.004852483
train_loss: 0.0027151294
test_loss: 0.0051282104
train_loss: 0.0027163504
test_loss: 0.0050213193
train_loss: 0.0026062422
test_loss: 0.0053682276
train_loss: 0.0030139312
test_loss: 0.00497487
train_loss: 0.0031622946
test_loss: 0.0052506546
train_loss: 0.0033153694
test_loss: 0.005087798
train_loss: 0.0028961538
test_loss: 0.0050504687
train_loss: 0.0028218941
test_loss: 0.0049880035
train_loss: 0.0028205873
test_loss: 0.004915759
train_loss: 0.0026041823
test_loss: 0.0048720045
train_loss: 0.0025740254
test_loss: 0.0049331305
train_loss: 0.002526607
test_loss: 0.004849835
train_loss: 0.0026070608
test_loss: 0.004880971
train_loss: 0.002568043
test_loss: 0.0047953036
train_loss: 0.002628775
test_loss: 0.004918633
train_loss: 0.0023545
test_loss: 0.0048937667
train_loss: 0.002516043
test_loss: 0.0048390427
train_loss: 0.0023933877
test_loss: 0.004885918
train_loss: 0.0025807119
test_loss: 0.0048383405
train_loss: 0.0025793535
test_loss: 0.005064908
train_loss: 0.0025707944
test_loss: 0.0049438816
train_loss: 0.0027933503
test_loss: 0.0049909367
train_loss: 0.0023581758
test_loss: 0.004829562
train_loss: 0.0028320784
test_loss: 0.0049325284
train_loss: 0.0024292467
test_loss: 0.0049419026
train_loss: 0.0027853472
test_loss: 0.005080026
train_loss: 0.0026242554
test_loss: 0.005099347
train_loss: 0.0026224155
test_loss: 0.0048983106
train_loss: 0.002563183
test_loss: 0.004843868
train_loss: 0.0029243228
test_loss: 0.005044371
train_loss: 0.0031141809
test_loss: 0.004978116
train_loss: 0.0028204336
test_loss: 0.0050184266
train_loss: 0.0026766993
test_loss: 0.005024606
train_loss: 0.0025347113
test_loss: 0.0049161706
train_loss: 0.002582867
test_loss: 0.0049299565
train_loss: 0.0027690793
test_loss: 0.005051275
train_loss: 0.0026876684
test_loss: 0.0049847113
train_loss: 0.0023990667
test_loss: 0.00494275
train_loss: 0.0024953107
test_loss: 0.004872157
train_loss: 0.0026064357
test_loss: 0.005023328
train_loss: 0.0029119703
test_loss: 0.0049386825
train_loss: 0.0029621879
test_loss: 0.00511585
train_loss: 0.002881331
test_loss: 0.004976114
train_loss: 0.0029745717
test_loss: 0.005176004
train_loss: 0.0030921712
test_loss: 0.005011152
train_loss: 0.0028965762
test_loss: 0.0049954997
train_loss: 0.003103351
test_loss: 0.0049893325
train_loss: 0.0028586986
test_loss: 0.0049762824
train_loss: 0.00293704
test_loss: 0.005013185
train_loss: 0.0026738457
test_loss: 0.0051015764
train_loss: 0.0027842307
test_loss: 0.0051663895
train_loss: 0.002809093
test_loss: 0.005232337
train_loss: 0.0030860917
test_loss: 0.0051699877
train_loss: 0.0028102528
test_loss: 0.0051442957
train_loss: 0.0026145931
test_loss: 0.0050744866
train_loss: 0.0023824512
test_loss: 0.0049622757
train_loss: 0.0025657397
test_loss: 0.004914414
train_loss: 0.0026663335
test_loss: 0.0049853777
train_loss: 0.0025902959
test_loss: 0.005162358
train_loss: 0.0024293773
test_loss: 0.0048558414
train_loss: 0.0024046171
test_loss: 0.004896156
train_loss: 0.002444167
test_loss: 0.0048863944
train_loss: 0.0026398578
test_loss: 0.004918902
train_loss: 0.002431735
test_loss: 0.005046191
train_loss: 0.0024775353
test_loss: 0.0049481206
train_loss: 0.0025438243
test_loss: 0.005206099
train_loss: 0.0027983882
test_loss: 0.005001995
train_loss: 0.0026226863
test_loss: 0.0048426758
train_loss: 0.0023940175
test_loss: 0.0049650897
train_loss: 0.002568915
test_loss: 0.0049662087
train_loss: 0.0028077862
test_loss: 0.00500849
train_loss: 0.0026278244
test_loss: 0.0049950774
train_loss: 0.00262891
test_loss: 0.004994981
train_loss: 0.00282155
test_loss: 0.004982417
train_loss: 0.002614385
test_loss: 0.0049050897
train_loss: 0.0025781495
test_loss: 0.0049280226
train_loss: 0.0025596817
test_loss: 0.005043417
train_loss: 0.0025015608
test_loss: 0.0050295917
train_loss: 0.002287464
test_loss: 0.0048690275
train_loss: 0.0024978807
test_loss: 0.0052005625
train_loss: 0.0028477912
test_loss: 0.0051684985
train_loss: 0.0028076856
test_loss: 0.005309923
train_loss: 0.0028809044
test_loss: 0.005118641
train_loss: 0.0025924528
test_loss: 0.004994558
train_loss: 0.0024519097
test_loss: 0.0049935835
train_loss: 0.0024908069
test_loss: 0.004970456
train_loss: 0.0026062336
test_loss: 0.0048504192
train_loss: 0.002700033
test_loss: 0.0048526153
train_loss: 0.002740352
test_loss: 0.0048668943
train_loss: 0.0027476782
test_loss: 0.005143807
train_loss: 0.0032235326
test_loss: 0.0049452516
train_loss: 0.0027611721
test_loss: 0.004956498
train_loss: 0.002479582
test_loss: 0.0049892724
train_loss: 0.003178245
test_loss: 0.004938995
train_loss: 0.0029050438
test_loss: 0.004920224
train_loss: 0.0025259827
test_loss: 0.005069591
train_loss: 0.0025330817
test_loss: 0.0048580524
train_loss: 0.002503418
test_loss: 0.0048632333
train_loss: 0.002275337
test_loss: 0.004808428
train_loss: 0.0022413328
test_loss: 0.004983156
train_loss: 0.0024436917
test_loss: 0.0049551465
train_loss: 0.0024179094
test_loss: 0.004800056
train_loss: 0.0023318958
test_loss: 0.0048813373
train_loss: 0.0024025012
test_loss: 0.00499954
train_loss: 0.0026138143
test_loss: 0.0049764593
train_loss: 0.0022685654
test_loss: 0.004956723
train_loss: 0.0026750285
test_loss: 0.004932082
train_loss: 0.002401648
test_loss: 0.005132975
train_loss: 0.0024828273
test_loss: 0.004954727
train_loss: 0.0023765017
test_loss: 0.005119675
train_loss: 0.0026547955
test_loss: 0.0051684827
train_loss: 0.0026791021
test_loss: 0.0051829778
train_loss: 0.0029350242
test_loss: 0.005093281
train_loss: 0.0025980663
test_loss: 0.0051657245
train_loss: 0.0024713909
test_loss: 0.0050679203
train_loss: 0.0024046737
test_loss: 0.0049878405
train_loss: 0.002560799
test_loss: 0.0049050516
train_loss: 0.002521148
test_loss: 0.0049185036
train_loss: 0.0023126756
test_loss: 0.004934911
train_loss: 0.002335888
test_loss: 0.004862385
train_loss: 0.0021588139
test_loss: 0.0048728986
train_loss: 0.0023120632
test_loss: 0.0049299225
train_loss: 0.0024148975
test_loss: 0.004916414
train_loss: 0.0023692418
test_loss: 0.004801143
train_loss: 0.00241015
test_loss: 0.004812022
train_loss: 0.0022581383
test_loss: 0.0049307668
train_loss: 0.002526098
test_loss: 0.0050890017
train_loss: 0.0024829106
test_loss: 0.005060458
train_loss: 0.0026228093
test_loss: 0.0051929383
train_loss: 0.002670059
test_loss: 0.005091913
train_loss: 0.002771812
test_loss: 0.005124191
train_loss: 0.0027523288
test_loss: 0.005092347
train_loss: 0.0029289413
test_loss: 0.004987922
train_loss: 0.0026081023
test_loss: 0.0049237325
train_loss: 0.002536244
test_loss: 0.004923088
train_loss: 0.0022256076
test_loss: 0.0048923707
train_loss: 0.002328197
test_loss: 0.004917637
train_loss: 0.0024381597
test_loss: 0.0049781683
train_loss: 0.0022959413
test_loss: 0.0049057235
train_loss: 0.0023319935
test_loss: 0.00490164
train_loss: 0.0024529793
test_loss: 0.004920907
train_loss: 0.0025362705
test_loss: 0.004870756
train_loss: 0.002397078
test_loss: 0.0048730006
train_loss: 0.0023402101
test_loss: 0.004801193
train_loss: 0.0025493074
test_loss: 0.00490795
train_loss: 0.002163951
test_loss: 0.004866288
train_loss: 0.0024689632
test_loss: 0.004861745
train_loss: 0.0026492584
test_loss: 0.004948782
train_loss: 0.002571111
test_loss: 0.0050058924
train_loss: 0.0027289684
test_loss: 0.0048843594
train_loss: 0.0026081095
test_loss: 0.0049254205
train_loss: 0.0022729845
test_loss: 0.004958145
train_loss: 0.0023890077
test_loss: 0.0048947944
train_loss: 0.0022523254
test_loss: 0.004812099
train_loss: 0.0023896946
test_loss: 0.004778425
train_loss: 0.0023588634
test_loss: 0.004782131
train_loss: 0.0026110895
test_loss: 0.0048712133
train_loss: 0.0023427643
test_loss: 0.00502538
train_loss: 0.002381487
test_loss: 0.004915426
train_loss: 0.00227195
test_loss: 0.0048500174
train_loss: 0.0023769168
test_loss: 0.0049025454
train_loss: 0.002220564
test_loss: 0.0048626675
train_loss: 0.0023527879
test_loss: 0.0048489138
train_loss: 0.0023865849
test_loss: 0.004840019
train_loss: 0.002638646
test_loss: 0.0048233103
train_loss: 0.0024717285
test_loss: 0.004981417
train_loss: 0.003056345
test_loss: 0.005035284
train_loss: 0.0032271154
test_loss: 0.0050115497
train_loss: 0.002519133
test_loss: 0.0050006444
train_loss: 0.0028466452
test_loss: 0.0053051244
train_loss: 0.0027534014
test_loss: 0.0049808775
train_loss: 0.002604072
test_loss: 0.0049289563
train_loss: 0.0025793263
test_loss: 0.0049099517
train_loss: 0.0025749365
test_loss: 0.0051002973
train_loss: 0.0024500524
test_loss: 0.004808239
train_loss: 0.0021853542
test_loss: 0.004800892
train_loss: 0.0022732844
test_loss: 0.004793526
train_loss: 0.0021660638
test_loss: 0.0048108106
train_loss: 0.0022382496
test_loss: 0.0048781107
train_loss: 0.0023417193
test_loss: 0.0048162853
train_loss: 0.002540966
test_loss: 0.0048504607
train_loss: 0.00226138
test_loss: 0.0050703217
train_loss: 0.0030307546
test_loss: 0.005263534
train_loss: 0.002782586
test_loss: 0.0049531357
train_loss: 0.002521825
test_loss: 0.004948575
train_loss: 0.002420833
test_loss: 0.004967379
train_loss: 0.0026093924
test_loss: 0.0048451535
train_loss: 0.0022438127
test_loss: 0.004943656
train_loss: 0.0024719245
test_loss: 0.004926801
train_loss: 0.0021814017
test_loss: 0.004996195
train_loss: 0.0022330289
test_loss: 0.004829942
train_loss: 0.0023243881
test_loss: 0.0048269005
train_loss: 0.0025588155
test_loss: 0.0049300715
train_loss: 0.0027772961
test_loss: 0.0049416996
train_loss: 0.002449155
test_loss: 0.0048432834
train_loss: 0.0024988386
test_loss: 0.005153694
train_loss: 0.0024743895
test_loss: 0.0049296436
train_loss: 0.0022641425
test_loss: 0.0050194245
train_loss: 0.0022582219
test_loss: 0.0049432237
train_loss: 0.0023738202
test_loss: 0.0049005393
train_loss: 0.002389978
test_loss: 0.004866271
train_loss: 0.002400509
test_loss: 0.004922718
train_loss: 0.002169769
test_loss: 0.004904958
train_loss: 0.0022925362
test_loss: 0.0047881273
train_loss: 0.002296021
test_loss: 0.0048519145
train_loss: 0.002271857
test_loss: 0.004961328
train_loss: 0.0020474596
test_loss: 0.004777122
train_loss: 0.0022191056
test_loss: 0.0048662373
train_loss: 0.002364504
test_loss: 0.004740941
train_loss: 0.0023344965
test_loss: 0.0048186462
train_loss: 0.002103277
test_loss: 0.0048732595
train_loss: 0.0021181197
test_loss: 0.004832761
train_loss: 0.002379624
test_loss: 0.0048562987
train_loss: 0.0021440028
test_loss: 0.0047702515
train_loss: 0.0020455164
test_loss: 0.004825476
train_loss: 0.0022486588
test_loss: 0.0048025353
train_loss: 0.0021114962
test_loss: 0.0047846735
train_loss: 0.0023173064
test_loss: 0.0050200406
train_loss: 0.0023883148
test_loss: 0.0050215046
train_loss: 0.0021975844
test_loss: 0.0048455414
train_loss: 0.0020960544
test_loss: 0.0047202804
train_loss: 0.0023647519
test_loss: 0.00480026
train_loss: 0.0023388444
test_loss: 0.0048284405
train_loss: 0.0022470606
test_loss: 0.004869163
train_loss: 0.0022987416
test_loss: 0.004850186
train_loss: 0.002159545
test_loss: 0.004817822
train_loss: 0.0022389004
test_loss: 0.0047826758
train_loss: 0.0023564382
test_loss: 0.0048602317
train_loss: 0.002054349
test_loss: 0.0047922763
train_loss: 0.0022443088
test_loss: 0.0048518605
train_loss: 0.0022900382
test_loss: 0.0048474
train_loss: 0.0022089262
test_loss: 0.004843234
train_loss: 0.0022110124
test_loss: 0.0048323846
train_loss: 0.0022090147
test_loss: 0.004857064
train_loss: 0.0021231205
test_loss: 0.0048024314
train_loss: 0.0024298297
test_loss: 0.0049571646
train_loss: 0.0022725635
test_loss: 0.004892077
train_loss: 0.002051841
test_loss: 0.0047655315
train_loss: 0.002111092
test_loss: 0.0048763035
train_loss: 0.0021243517
test_loss: 0.0048903385
train_loss: 0.002271083
test_loss: 0.0048253485
train_loss: 0.0020253977
test_loss: 0.0048056263
train_loss: 0.0021030554
test_loss: 0.0047563985
train_loss: 0.0020672032
test_loss: 0.004804126
train_loss: 0.0022729575
test_loss: 0.0048083453
train_loss: 0.0023488495
test_loss: 0.0048419656
train_loss: 0.0023781445
test_loss: 0.0049067494
train_loss: 0.0022091789
test_loss: 0.0048673637
train_loss: 0.002253275
test_loss: 0.00475453
train_loss: 0.002130806
test_loss: 0.004784333
train_loss: 0.0024069592
test_loss: 0.004843024
train_loss: 0.002068543
test_loss: 0.0048557334
train_loss: 0.0020399184
test_loss: 0.004798293
train_loss: 0.0021494585
test_loss: 0.004804539
train_loss: 0.0025111071
test_loss: 0.0047869976
train_loss: 0.0024354325
test_loss: 0.0048351553
train_loss: 0.0022188737
test_loss: 0.0049015386
train_loss: 0.0021353108
test_loss: 0.0047461772
train_loss: 0.0019803047
test_loss: 0.00473681
train_loss: 0.0020793413
test_loss: 0.0048327325
train_loss: 0.0021474862
test_loss: 0.004814534
train_loss: 0.0023273819
test_loss: 0.0048040645
train_loss: 0.00217631
test_loss: 0.004811454
train_loss: 0.002323857
test_loss: 0.0049235756
train_loss: 0.0021568388
test_loss: 0.004862911
train_loss: 0.0023237977
test_loss: 0.00490579
train_loss: 0.002331854
test_loss: 0.004845387
train_loss: 0.0022652312
test_loss: 0.0048580226
train_loss: 0.0024769325
test_loss: 0.0048523787
train_loss: 0.002252612
test_loss: 0.004986219
train_loss: 0.0023329686
test_loss: 0.0048302384
train_loss: 0.0022696364
test_loss: 0.0049848175
train_loss: 0.0024078714
test_loss: 0.004916255
train_loss: 0.0022088578
test_loss: 0.0049424656
train_loss: 0.0022442117
test_loss: 0.0050208606
train_loss: 0.0021180594
test_loss: 0.004888875
train_loss: 0.002286939
test_loss: 0.004808677
train_loss: 0.0022591944
test_loss: 0.004736891
train_loss: 0.0022115372
test_loss: 0.004827646
train_loss: 0.0020306366
test_loss: 0.0048143174
train_loss: 0.002247442
test_loss: 0.004900032
train_loss: 0.0021116165
test_loss: 0.0047774
train_loss: 0.0021079646
test_loss: 0.004736988
train_loss: 0.0019865064
test_loss: 0.004801583
train_loss: 0.0020001512
test_loss: 0.0047251936
train_loss: 0.0022514984
test_loss: 0.0049169012
train_loss: 0.0020481572
test_loss: 0.0048637497
train_loss: 0.0022145123
test_loss: 0.004902348
train_loss: 0.0020767874
test_loss: 0.004804151
train_loss: 0.0023718597
test_loss: 0.004860076
train_loss: 0.002333986
test_loss: 0.004898122
train_loss: 0.0022132727
test_loss: 0.0048681097
train_loss: 0.002176307
test_loss: 0.004840155
train_loss: 0.0020783045
test_loss: 0.0048758476
train_loss: 0.0021463525
test_loss: 0.0049404497
train_loss: 0.0023495494
test_loss: 0.0049770023
train_loss: 0.002147229
test_loss: 0.0048183147
train_loss: 0.0022408487
test_loss: 0.0048628068
train_loss: 0.0024803865
test_loss: 0.0049093324
train_loss: 0.00248205
test_loss: 0.004959513
train_loss: 0.0022692198
test_loss: 0.004984316
train_loss: 0.0021377504
test_loss: 0.0049103964
train_loss: 0.002351828
test_loss: 0.0048658047
train_loss: 0.0024131094
test_loss: 0.0049416325
train_loss: 0.0021802927
test_loss: 0.0048965258
train_loss: 0.0022417381
test_loss: 0.0051542693
train_loss: 0.0028633776
test_loss: 0.005042826
train_loss: 0.0025364673
test_loss: 0.00494804
train_loss: 0.0024301056
test_loss: 0.004882823
train_loss: 0.0024845027
test_loss: 0.0049640816
train_loss: 0.0022990052
test_loss: 0.0049752323
train_loss: 0.002376536
test_loss: 0.0049207285
train_loss: 0.002159351
test_loss: 0.004934462
train_loss: 0.0020706102
test_loss: 0.0048137936
train_loss: 0.002096131
test_loss: 0.004808359
train_loss: 0.0021358028
test_loss: 0.0048228735
train_loss: 0.0024939761
test_loss: 0.004822219
train_loss: 0.0023634613
test_loss: 0.0049249055
train_loss: 0.0023398811
test_loss: 0.0049341414
train_loss: 0.0020926807
test_loss: 0.0050059194
train_loss: 0.0021118387
test_loss: 0.0048487587
train_loss: 0.002048133
test_loss: 0.004786298
train_loss: 0.0020812782
test_loss: 0.0048327427
train_loss: 0.0022517417
test_loss: 0.004951417
train_loss: 0.0021629084
test_loss: 0.005077279
train_loss: 0.0023290382
test_loss: 0.0050438265
train_loss: 0.0022551336
test_loss: 0.004892502
train_loss: 0.0020802107
test_loss: 0.004852085
train_loss: 0.0020620967
test_loss: 0.0049058828
train_loss: 0.0021481279
test_loss: 0.004877412
train_loss: 0.0022884647
test_loss: 0.004817906
train_loss: 0.001994905
test_loss: 0.0048467778
train_loss: 0.0021945694
test_loss: 0.004899193
train_loss: 0.0022434413
test_loss: 0.005051137
train_loss: 0.002241433
test_loss: 0.0050694393
train_loss: 0.002539857
test_loss: 0.005182083
train_loss: 0.0027040993
test_loss: 0.004935051
train_loss: 0.002273138
test_loss: 0.005024828
train_loss: 0.002316408
test_loss: 0.004886257
train_loss: 0.002225161
test_loss: 0.0048923083
train_loss: 0.0021553843
test_loss: 0.004856873
train_loss: 0.0020473893
test_loss: 0.0049016266
train_loss: 0.0021632758
test_loss: 0.004807475
train_loss: 0.002165197
test_loss: 0.004837525
train_loss: 0.0020592415
test_loss: 0.0048279525
train_loss: 0.0020788745
test_loss: 0.004771809
train_loss: 0.0021393795
test_loss: 0.0048154066
train_loss: 0.0021539303
test_loss: 0.0048397514
train_loss: 0.002360578
test_loss: 0.0049555767
train_loss: 0.0021953422
test_loss: 0.00488892
train_loss: 0.0022725104
test_loss: 0.004844192
train_loss: 0.0022997626
test_loss: 0.00480716
train_loss: 0.0022519822
test_loss: 0.004771898
train_loss: 0.0021567447
test_loss: 0.0048336294
train_loss: 0.0022321455
test_loss: 0.0049452977
train_loss: 0.0023136097
test_loss: 0.004823574
train_loss: 0.0021311545
test_loss: 0.004972293
train_loss: 0.002379427
test_loss: 0.004918329
train_loss: 0.0022820877
test_loss: 0.005108333
train_loss: 0.0023331512
test_loss: 0.0048966566
train_loss: 0.0021762585
test_loss: 0.0049231974
train_loss: 0.002037793
test_loss: 0.004816151
train_loss: 0.0020828135
test_loss: 0.0048451973
train_loss: 0.0022227755
test_loss: 0.00487279
train_loss: 0.002065722
test_loss: 0.004865806
train_loss: 0.0023211122
test_loss: 0.0047603888
train_loss: 0.0021383967
test_loss: 0.0048065106
train_loss: 0.0021198094
test_loss: 0.0047796774
train_loss: 0.0020653827
test_loss: 0.004801118
train_loss: 0.002410194
test_loss: 0.004981253
train_loss: 0.002216803
test_loss: 0.004819214
train_loss: 0.002291497
test_loss: 0.004884134
train_loss: 0.0021176538
test_loss: 0.004919283
train_loss: 0.0020135571
test_loss: 0.0049589267
train_loss: 0.0021468434
test_loss: 0.00479072
train_loss: 0.0021971967
test_loss: 0.0049615963
train_loss: 0.002315956
test_loss: 0.0049121776
train_loss: 0.0020491634
test_loss: 0.0048046983
train_loss: 0.002074567
test_loss: 0.004928994
train_loss: 0.002260443
test_loss: 0.0047609084
train_loss: 0.002271366
test_loss: 0.0048764306
train_loss: 0.0024909903
test_loss: 0.0048242756
train_loss: 0.0022886235
test_loss: 0.004859883
train_loss: 0.0020154964
test_loss: 0.0047857403
train_loss: 0.002032472
test_loss: 0.004862696
train_loss: 0.0020637636
test_loss: 0.0048348284
train_loss: 0.0021169079
test_loss: 0.004823852
train_loss: 0.002189962
test_loss: 0.004833634
train_loss: 0.0018259515
test_loss: 0.00477488
train_loss: 0.0024746112
test_loss: 0.0047916714
train_loss: 0.0024313822
test_loss: 0.004817612
train_loss: 0.00234087
test_loss: 0.004929454
train_loss: 0.002002127
test_loss: 0.00499606
train_loss: 0.0020000464
test_loss: 0.005037112
train_loss: 0.0020451527
test_loss: 0.005011877
train_loss: 0.0023017905
test_loss: 0.0051285676
train_loss: 0.002517075
test_loss: 0.0049533835
train_loss: 0.0023345235
test_loss: 0.004910776
train_loss: 0.0021893082
test_loss: 0.004875285
train_loss: 0.0020112619
test_loss: 0.0048636394
train_loss: 0.0021831032
test_loss: 0.0047168303
train_loss: 0.0020528878
test_loss: 0.004954843
train_loss: 0.0022348044
test_loss: 0.0048919604
train_loss: 0.0023963796
test_loss: 0.0049058227
train_loss: 0.0020562992
test_loss: 0.0047636842
train_loss: 0.0019471942
test_loss: 0.004806906
train_loss: 0.0020722558
test_loss: 0.004761098
train_loss: 0.0020616525
test_loss: 0.0048088646
train_loss: 0.0020161592
test_loss: 0.0048763696
train_loss: 0.0021536052
test_loss: 0.004750958
train_loss: 0.0024337755
test_loss: 0.0048884973
train_loss: 0.0021631443
test_loss: 0.004994787
train_loss: 0.0019754118
test_loss: 0.0048917835
train_loss: 0.0021102198
test_loss: 0.0048300386
train_loss: 0.0021611357
test_loss: 0.0048446083
train_loss: 0.0020707452
test_loss: 0.004848845
train_loss: 0.0018300654
test_loss: 0.0048013977
train_loss: 0.0018788266
test_loss: 0.004823644
train_loss: 0.001906529
test_loss: 0.0048330417
train_loss: 0.0019773128
test_loss: 0.0048665307
train_loss: 0.0021774878
test_loss: 0.0049134917
train_loss: 0.0022582859
test_loss: 0.004800479
train_loss: 0.0021828231
test_loss: 0.0048583844
train_loss: 0.0020950618
test_loss: 0.0047969026
train_loss: 0.0020534978
test_loss: 0.0048155715
train_loss: 0.0019022233
test_loss: 0.0047646114
train_loss: 0.0019910876
test_loss: 0.00476012
train_loss: 0.0021573086
test_loss: 0.0048438017
train_loss: 0.0019068818
test_loss: 0.00477239
train_loss: 0.0019378501
test_loss: 0.0050963177
train_loss: 0.0024992453
test_loss: 0.0050229128
train_loss: 0.0022433822
test_loss: 0.0048149205
train_loss: 0.001922366
test_loss: 0.004755075
train_loss: 0.002052636
test_loss: 0.005013144
train_loss: 0.0019309089
test_loss: 0.0047892923
train_loss: 0.0022655157
test_loss: 0.004812571
train_loss: 0.0023042525
test_loss: 0.0048669945
train_loss: 0.0020574536
test_loss: 0.0048833597
train_loss: 0.0021676575
test_loss: 0.004854781
train_loss: 0.002227162
test_loss: 0.0048683733
train_loss: 0.002213841
test_loss: 0.0049667447
train_loss: 0.0023055281
test_loss: 0.0047104154
train_loss: 0.0020284283
test_loss: 0.004967965
train_loss: 0.0022686785
test_loss: 0.00491062
train_loss: 0.0023861767
test_loss: 0.0049765822
train_loss: 0.002236491
test_loss: 0.0048985137
train_loss: 0.0021433178
test_loss: 0.004878961
train_loss: 0.0022133025
test_loss: 0.004867931
train_loss: 0.0020651333
test_loss: 0.0048627784
train_loss: 0.0021958777
test_loss: 0.0050125597
train_loss: 0.0023041803
test_loss: 0.005109698
train_loss: 0.0020294178
test_loss: 0.005026863
train_loss: 0.002254598
test_loss: 0.0050411387
train_loss: 0.0020886622
test_loss: 0.0048615756
train_loss: 0.0019123023
test_loss: 0.0047704917
train_loss: 0.0020889807
test_loss: 0.0048041814
train_loss: 0.0022804386
test_loss: 0.004906087
train_loss: 0.002128729
test_loss: 0.0048885234
train_loss: 0.0020187132
test_loss: 0.0048225434
train_loss: 0.002099728
test_loss: 0.004848562
train_loss: 0.0020542159
test_loss: 0.004839462
train_loss: 0.0021453241
test_loss: 0.0049012494
train_loss: 0.0020601742
test_loss: 0.004841167
train_loss: 0.001971256
test_loss: 0.004903333
train_loss: 0.0022337607
test_loss: 0.004903974
train_loss: 0.002005525
test_loss: 0.0048335986
train_loss: 0.0019983603
test_loss: 0.004817996
train_loss: 0.0019835294
test_loss: 0.0048846155
train_loss: 0.0021391413
test_loss: 0.0048271515
train_loss: 0.0021544872
test_loss: 0.0048020156
train_loss: 0.0020132398
test_loss: 0.0048735826
train_loss: 0.0019787366
test_loss: 0.0050465525
train_loss: 0.0022905022
test_loss: 0.0047992887
train_loss: 0.001973987
test_loss: 0.0049637305
train_loss: 0.002182321
test_loss: 0.004905522
train_loss: 0.0022875553
test_loss: 0.0048149563
train_loss: 0.0022102594
test_loss: 0.004800243
train_loss: 0.0023388222
test_loss: 0.0049735364
train_loss: 0.00226267
test_loss: 0.004928285
train_loss: 0.0023512517
test_loss: 0.005018417
train_loss: 0.002159655
test_loss: 0.0049394253
train_loss: 0.0023290357
test_loss: 0.004991657
train_loss: 0.002116307
test_loss: 0.0049219965
train_loss: 0.0021638595
test_loss: 0.0049418914
train_loss: 0.0021677865
test_loss: 0.004849829
train_loss: 0.002033016
test_loss: 0.004812083
train_loss: 0.002019171
test_loss: 0.0048647444
train_loss: 0.0020135338
test_loss: 0.004852827
train_loss: 0.002020339
test_loss: 0.004856445
train_loss: 0.0018674477
test_loss: 0.004819168
train_loss: 0.0021460662
test_loss: 0.004929523
train_loss: 0.0019611323
test_loss: 0.004958606
train_loss: 0.0020351629
test_loss: 0.0049333335
train_loss: 0.0020029282
test_loss: 0.0048989775
train_loss: 0.002234299
test_loss: 0.0052069565
train_loss: 0.0024649918
test_loss: 0.004927149
train_loss: 0.002126959
test_loss: 0.0050655007
train_loss: 0.0023350413
test_loss: 0.005019651
train_loss: 0.0024106773
test_loss: 0.0050243814
train_loss: 0.0023783562
test_loss: 0.0050198953
train_loss: 0.0022945385
test_loss: 0.004841792
train_loss: 0.0022606393
test_loss: 0.0048974846
train_loss: 0.002255493
test_loss: 0.0049683847
train_loss: 0.0020700507
test_loss: 0.0048431465
train_loss: 0.0019191538
test_loss: 0.0049043233
train_loss: 0.0021336256
test_loss: 0.0049227094
train_loss: 0.0021297887
test_loss: 0.0047971923
train_loss: 0.0019292611
test_loss: 0.004790922
train_loss: 0.001867902
test_loss: 0.0047666226
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0022038294
test_loss: 0.004877119
train_loss: 0.0020970926
test_loss: 0.0048721954
train_loss: 0.0019846566
test_loss: 0.0047957515
train_loss: 0.0019121194
test_loss: 0.004748845
train_loss: 0.0021284637
test_loss: 0.0047807046
train_loss: 0.0022031078
test_loss: 0.004879139
train_loss: 0.0023350636
test_loss: 0.0048954138
train_loss: 0.002371258
test_loss: 0.004996075
train_loss: 0.0022409982
test_loss: 0.005072583
train_loss: 0.0021919762
test_loss: 0.004922072
train_loss: 0.001959667
test_loss: 0.0049439357
train_loss: 0.0021174294
test_loss: 0.005116164
train_loss: 0.0023804386
test_loss: 0.005098503
train_loss: 0.0022643679
test_loss: 0.004892652
train_loss: 0.002083357
test_loss: 0.0050112563
train_loss: 0.0022672925
test_loss: 0.005040506
train_loss: 0.0021279359
test_loss: 0.004986597
train_loss: 0.002110886
test_loss: 0.0049872147
train_loss: 0.0020295999
test_loss: 0.0048820553
train_loss: 0.0018511275
test_loss: 0.004880753
train_loss: 0.0020226683
test_loss: 0.0048750485
train_loss: 0.0019902412
test_loss: 0.0048945905
train_loss: 0.0020490324
test_loss: 0.005030745
train_loss: 0.002102086
test_loss: 0.0049744896
train_loss: 0.0019546023
test_loss: 0.0050795902
train_loss: 0.0023457787
test_loss: 0.0052196495
train_loss: 0.0024729557
test_loss: 0.004968906
train_loss: 0.0023075198
test_loss: 0.0050207465
train_loss: 0.002127926
test_loss: 0.0050667184
train_loss: 0.0021082151
test_loss: 0.0051054056
train_loss: 0.0022970033
test_loss: 0.005001173
train_loss: 0.0024519293
test_loss: 0.0049555884
train_loss: 0.0025059967
test_loss: 0.004940782
train_loss: 0.0023009344
test_loss: 0.0048915595
train_loss: 0.002242707
test_loss: 0.0049752905
train_loss: 0.0019634082
test_loss: 0.004878822
train_loss: 0.0018684448
test_loss: 0.0048082876
train_loss: 0.0019886196
test_loss: 0.0048008324
train_loss: 0.0019859108
test_loss: 0.0048671244
train_loss: 0.0020596436
test_loss: 0.0048488122
train_loss: 0.0021059308
test_loss: 0.004860688
train_loss: 0.0021609282
test_loss: 0.004858866
train_loss: 0.0019672564
test_loss: 0.0048400466
train_loss: 0.0021206657
test_loss: 0.004832232
train_loss: 0.002071231
test_loss: 0.004825576
train_loss: 0.0018415084
test_loss: 0.004907292
train_loss: 0.0019450008
test_loss: 0.004823198
train_loss: 0.0018415832
test_loss: 0.0048297876
train_loss: 0.002180416
test_loss: 0.0048814905
train_loss: 0.0021905776
test_loss: 0.004800329
train_loss: 0.0021548015
test_loss: 0.004846103
train_loss: 0.0019300071
test_loss: 0.0050784564
train_loss: 0.0022448406
test_loss: 0.0050495984
train_loss: 0.0022702985
test_loss: 0.004987094
train_loss: 0.0023477743
test_loss: 0.005066478
train_loss: 0.0022751777
test_loss: 0.005010671
train_loss: 0.002287322
test_loss: 0.004883605
train_loss: 0.0019509194
test_loss: 0.004976208
train_loss: 0.002249421
test_loss: 0.0050439043
train_loss: 0.0026434364
test_loss: 0.0049432707
train_loss: 0.0020950427
test_loss: 0.004849497
train_loss: 0.0020727182
test_loss: 0.0048750853
train_loss: 0.0019735873
test_loss: 0.0049360706
train_loss: 0.002044044
test_loss: 0.0048619756
train_loss: 0.0019033405
test_loss: 0.0048259636
train_loss: 0.0018301606
test_loss: 0.004807915
train_loss: 0.0019061094
test_loss: 0.0048165494
train_loss: 0.0021406796
test_loss: 0.0048306976
train_loss: 0.0019505534
test_loss: 0.0049322173
train_loss: 0.0019963612
test_loss: 0.0049934313
train_loss: 0.002126174
test_loss: 0.0049506333
train_loss: 0.0020492948
test_loss: 0.0048919013
train_loss: 0.0021314058
test_loss: 0.004929972
train_loss: 0.0022302049
test_loss: 0.004973274
train_loss: 0.0019938264
test_loss: 0.0049221683
train_loss: 0.0019522761
test_loss: 0.0048930175
train_loss: 0.0020455997
test_loss: 0.0049316864
train_loss: 0.001872421
test_loss: 0.0048570717
train_loss: 0.0018882557
test_loss: 0.0048844586
train_loss: 0.0019412973
test_loss: 0.00478978
train_loss: 0.0019773706
test_loss: 0.0048227324
train_loss: 0.0021292237
test_loss: 0.004953352
train_loss: 0.002049236
test_loss: 0.0048914417
train_loss: 0.0020279638
test_loss: 0.0048196577
train_loss: 0.0021518073
test_loss: 0.004827792
train_loss: 0.0020348069
test_loss: 0.004889117
train_loss: 0.0019273236
test_loss: 0.004900118
train_loss: 0.0019614317
test_loss: 0.0048614307
train_loss: 0.0019318268
test_loss: 0.004860239
train_loss: 0.0019700194
test_loss: 0.004788658
train_loss: 0.0019492044
test_loss: 0.0048390524
train_loss: 0.0019970066
test_loss: 0.0048104892
train_loss: 0.0018607768
test_loss: 0.0048817247
train_loss: 0.0018179797
test_loss: 0.004929765
train_loss: 0.0021732626
test_loss: 0.004928944
train_loss: 0.0019978469
test_loss: 0.0050142854
train_loss: 0.0021125353
test_loss: 0.0048543145
train_loss: 0.0019498094
test_loss: 0.004836748
train_loss: 0.0018751521
test_loss: 0.0048424643
train_loss: 0.0019830805
test_loss: 0.004812291
train_loss: 0.0018332028
test_loss: 0.0048048156
train_loss: 0.0017615478
test_loss: 0.0047942423
train_loss: 0.0019866491
test_loss: 0.0049077533
train_loss: 0.0019954483
test_loss: 0.0049864133
train_loss: 0.0019849294
test_loss: 0.0047774124
train_loss: 0.0020942343
test_loss: 0.0048981053
train_loss: 0.001958151
test_loss: 0.0049079717
train_loss: 0.0019462047
test_loss: 0.0049082004
train_loss: 0.0018520914
test_loss: 0.0048474385
train_loss: 0.001814679
test_loss: 0.0048611676
train_loss: 0.0018834459
test_loss: 0.0048147733
train_loss: 0.0020674986
test_loss: 0.0050327643
train_loss: 0.001987109
test_loss: 0.0048491424
train_loss: 0.0020644576
test_loss: 0.004830647
train_loss: 0.0020013796
test_loss: 0.0049363547
train_loss: 0.002039254
test_loss: 0.0050628223
train_loss: 0.0023813497
test_loss: 0.004992788
train_loss: 0.0021570425
test_loss: 0.004950422
train_loss: 0.00212567
test_loss: 0.0048955097
train_loss: 0.0021862
test_loss: 0.0048621264
train_loss: 0.0020078132
test_loss: 0.0049317903
train_loss: 0.0021357532
test_loss: 0.004873975
train_loss: 0.002261244
test_loss: 0.0049784505
train_loss: 0.0019467778
test_loss: 0.004925666
train_loss: 0.0019805657
test_loss: 0.005051403
train_loss: 0.0020351568
test_loss: 0.0049362886
train_loss: 0.0019544794
test_loss: 0.004905119
train_loss: 0.001962773
test_loss: 0.004967114
train_loss: 0.0021901825
test_loss: 0.004943459
train_loss: 0.0019731042
test_loss: 0.0049541495
train_loss: 0.0019691514
test_loss: 0.004925926
train_loss: 0.0020282888
test_loss: 0.0049244855
train_loss: 0.0020339885
test_loss: 0.0049426
train_loss: 0.0019542081
test_loss: 0.005062714
train_loss: 0.0022881904
test_loss: 0.0051012198
train_loss: 0.0022241406
test_loss: 0.0051334584
train_loss: 0.002125129
test_loss: 0.005005888
train_loss: 0.0021392247
test_loss: 0.005009637
train_loss: 0.0019486074
test_loss: 0.0048549357
train_loss: 0.0018663867
test_loss: 0.004837747
train_loss: 0.002039054
test_loss: 0.00479584
train_loss: 0.0019507211
test_loss: 0.0049021137
train_loss: 0.001904913
test_loss: 0.004936308
train_loss: 0.0020516857
test_loss: 0.004884462
train_loss: 0.001959287
test_loss: 0.0048349216
train_loss: 0.0020752808
test_loss: 0.0048334002
train_loss: 0.002019828
test_loss: 0.0048918105
train_loss: 0.0017946444
test_loss: 0.0048825988
train_loss: 0.0018923987
test_loss: 0.004985496
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1 --function f1 --psi -2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 4000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc681c5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc680b0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc681c58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc680f6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc68044620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc680772f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc60ea49d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc60e59950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc60e590d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc60e15d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc60dc1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc60e15b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc60dc7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc60dc7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc0a8df7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc0a89c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc0a8ab378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc0a89c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc0a888950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc0a884f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc0a831840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc0a820c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbe4103620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbe40b2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbe40b2400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbe406fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbe40169d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbe403d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbe403d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbdc280730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbdc2199d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbdc2460d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbdc246400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbdc1f5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbdc1960d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdbdc167510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.09461202
test_loss: 0.09103808
train_loss: 0.07626669
test_loss: 0.07456949
train_loss: 0.061507434
test_loss: 0.06096034
train_loss: 0.049827863
test_loss: 0.050562903
train_loss: 0.04222723
test_loss: 0.043183725
train_loss: 0.03515896
test_loss: 0.036211114
train_loss: 0.027909076
test_loss: 0.030543098
train_loss: 0.023754774
test_loss: 0.02604682
train_loss: 0.020321568
test_loss: 0.022875035
train_loss: 0.017669305
test_loss: 0.020073688
train_loss: 0.015071735
test_loss: 0.018053563
train_loss: 0.013260876
test_loss: 0.016439386
train_loss: 0.012017583
test_loss: 0.015430172
train_loss: 0.010834098
test_loss: 0.014411935
train_loss: 0.010231387
test_loss: 0.013440992
train_loss: 0.009398018
test_loss: 0.0127582615
train_loss: 0.008912701
test_loss: 0.012135652
train_loss: 0.007778541
test_loss: 0.011416657
train_loss: 0.007299607
test_loss: 0.010901516
train_loss: 0.0069905035
test_loss: 0.010480935
train_loss: 0.0065351655
test_loss: 0.01008704
train_loss: 0.0063177273
test_loss: 0.009695843
train_loss: 0.0058891205
test_loss: 0.009596187
train_loss: 0.0056777094
test_loss: 0.009197352
train_loss: 0.0058217114
test_loss: 0.00933649
train_loss: 0.00612709
test_loss: 0.00922243
train_loss: 0.0059478628
test_loss: 0.008833908
train_loss: 0.0057376046
test_loss: 0.008719342
train_loss: 0.0051574847
test_loss: 0.008866992
train_loss: 0.0051327143
test_loss: 0.008682575
train_loss: 0.0052878875
test_loss: 0.008358285
train_loss: 0.004886437
test_loss: 0.0084782215
train_loss: 0.00537886
test_loss: 0.008332515
train_loss: 0.0048565087
test_loss: 0.008218779
train_loss: 0.0052282205
test_loss: 0.008153208
train_loss: 0.0047914726
test_loss: 0.008135108
train_loss: 0.004665465
test_loss: 0.007971776
train_loss: 0.0047565163
test_loss: 0.0079406295
train_loss: 0.0043819025
test_loss: 0.007810329
train_loss: 0.0043981047
test_loss: 0.007968985
train_loss: 0.0044742394
test_loss: 0.0077003036
train_loss: 0.004249747
test_loss: 0.0077523254
train_loss: 0.0041595716
test_loss: 0.007568845
train_loss: 0.0039943126
test_loss: 0.007471642
train_loss: 0.004332202
test_loss: 0.007446254
train_loss: 0.0040648216
test_loss: 0.0074228942
train_loss: 0.0040121237
test_loss: 0.007363898
train_loss: 0.003812602
test_loss: 0.0074538505
train_loss: 0.0038591896
test_loss: 0.0074529164
train_loss: 0.003937046
test_loss: 0.007408025
train_loss: 0.0039103106
test_loss: 0.0073718303
train_loss: 0.0041819937
test_loss: 0.007470287
train_loss: 0.0038500864
test_loss: 0.0073021613
train_loss: 0.0035648602
test_loss: 0.0072998297
train_loss: 0.0034957323
test_loss: 0.0071788807
train_loss: 0.003686165
test_loss: 0.0072450656
train_loss: 0.0037879432
test_loss: 0.0071488535
train_loss: 0.0036646826
test_loss: 0.0072717066
train_loss: 0.0039228536
test_loss: 0.007102058
train_loss: 0.0036866735
test_loss: 0.007056605
train_loss: 0.003439641
test_loss: 0.0069884635
train_loss: 0.003640884
test_loss: 0.006974384
train_loss: 0.0035376681
test_loss: 0.00697886
train_loss: 0.0034374977
test_loss: 0.0069009806
train_loss: 0.0033644391
test_loss: 0.006790577
train_loss: 0.003443805
test_loss: 0.006924475
train_loss: 0.0033345683
test_loss: 0.0068074134
train_loss: 0.0032995932
test_loss: 0.0067915176
train_loss: 0.0035072821
test_loss: 0.006858169
train_loss: 0.0032701488
test_loss: 0.006955663
train_loss: 0.0033591227
test_loss: 0.006743165
train_loss: 0.0033778027
test_loss: 0.0068703983
train_loss: 0.003285691
test_loss: 0.0067061177
train_loss: 0.0034664334
test_loss: 0.0067138495
train_loss: 0.0031367557
test_loss: 0.006672646
train_loss: 0.0034109945
test_loss: 0.0067538638
train_loss: 0.0033108178
test_loss: 0.006704843
train_loss: 0.003561783
test_loss: 0.0068355463
train_loss: 0.0032751262
test_loss: 0.0065797116
train_loss: 0.0031459427
test_loss: 0.0065430766
train_loss: 0.0031788284
test_loss: 0.0065353573
train_loss: 0.003112857
test_loss: 0.006564551
train_loss: 0.0030058376
test_loss: 0.0066259713
train_loss: 0.0033033537
test_loss: 0.0066406596
train_loss: 0.003244524
test_loss: 0.006744809
train_loss: 0.0031230035
test_loss: 0.006633406
train_loss: 0.0033664044
test_loss: 0.0068373317
train_loss: 0.0031926003
test_loss: 0.006658226
train_loss: 0.0030854964
test_loss: 0.006504208
train_loss: 0.0031482866
test_loss: 0.0065276935
train_loss: 0.0033052077
test_loss: 0.006662645
train_loss: 0.0031124596
test_loss: 0.0065349154
train_loss: 0.0032146934
test_loss: 0.0065929824
train_loss: 0.003332322
test_loss: 0.0066276095
train_loss: 0.0030085105
test_loss: 0.0066326084
train_loss: 0.003155821
test_loss: 0.0064538755
train_loss: 0.003381223
test_loss: 0.006518268
train_loss: 0.002982357
test_loss: 0.0064232894
train_loss: 0.0031524766
test_loss: 0.0063634543
train_loss: 0.0030910089
test_loss: 0.0063903793
train_loss: 0.0031411275
test_loss: 0.006278051
train_loss: 0.0027374486
test_loss: 0.006530021
train_loss: 0.0029953634
test_loss: 0.006398772
train_loss: 0.0030048857
test_loss: 0.0063206763
train_loss: 0.0030133906
test_loss: 0.0063614473
train_loss: 0.0029842386
test_loss: 0.006452995
train_loss: 0.0029894086
test_loss: 0.0063285725
train_loss: 0.0028735949
test_loss: 0.006319366
train_loss: 0.0029303678
test_loss: 0.0063931737
train_loss: 0.002923159
test_loss: 0.006301687
train_loss: 0.00287101
test_loss: 0.006362979
train_loss: 0.002755212
test_loss: 0.0061965557
train_loss: 0.0028752075
test_loss: 0.006305903
train_loss: 0.0028822555
test_loss: 0.0062742466
train_loss: 0.0027984232
test_loss: 0.0062861387
train_loss: 0.0027991645
test_loss: 0.0061794207
train_loss: 0.0027799641
test_loss: 0.006252189
train_loss: 0.003028649
test_loss: 0.006204581
train_loss: 0.002746321
test_loss: 0.006328919
train_loss: 0.002890172
test_loss: 0.0063170665
train_loss: 0.0027996332
test_loss: 0.0065079806
train_loss: 0.002792494
test_loss: 0.006316465
train_loss: 0.002775832
test_loss: 0.0062368833
train_loss: 0.002807222
test_loss: 0.0062028817
train_loss: 0.0029006596
test_loss: 0.006169452
train_loss: 0.0028750598
test_loss: 0.0061927256
train_loss: 0.003009298
test_loss: 0.006225737
train_loss: 0.0029755596
test_loss: 0.0062847976
train_loss: 0.0029116136
test_loss: 0.00626626
train_loss: 0.002651059
test_loss: 0.0061449287
train_loss: 0.0027492933
test_loss: 0.006181791
train_loss: 0.0026602529
test_loss: 0.006167846
train_loss: 0.0027422947
test_loss: 0.0061325994
train_loss: 0.002839872
test_loss: 0.006269115
train_loss: 0.002762909
test_loss: 0.006443279
train_loss: 0.0033435621
test_loss: 0.006348807
train_loss: 0.003198884
test_loss: 0.0061967606
train_loss: 0.0029255692
test_loss: 0.0064904294
train_loss: 0.0032509048
test_loss: 0.0062529133
train_loss: 0.0032597207
test_loss: 0.006529336
train_loss: 0.0029986347
test_loss: 0.0063583213
train_loss: 0.0030280736
test_loss: 0.0062290463
train_loss: 0.0028893175
test_loss: 0.0062546125
train_loss: 0.0028542476
test_loss: 0.006199562
train_loss: 0.002720088
test_loss: 0.006116593
train_loss: 0.0026301949
test_loss: 0.006240085
train_loss: 0.0029093903
test_loss: 0.006167814
train_loss: 0.0026125372
test_loss: 0.0061900807
train_loss: 0.0027129166
test_loss: 0.0061252145
train_loss: 0.0024897777
test_loss: 0.00607554
train_loss: 0.002652424
test_loss: 0.0060841306
train_loss: 0.0026222179
test_loss: 0.0060103796
train_loss: 0.0024662276
test_loss: 0.0060977587
train_loss: 0.0025343285
test_loss: 0.006102558
train_loss: 0.002436121
test_loss: 0.0059936796
train_loss: 0.0024062889
test_loss: 0.006077566
train_loss: 0.0027352185
test_loss: 0.0060274513
train_loss: 0.0030448423
test_loss: 0.0060785757
train_loss: 0.002944312
test_loss: 0.0060497397
train_loss: 0.0028702016
test_loss: 0.0060557644
train_loss: 0.002852858
test_loss: 0.0059770793
train_loss: 0.0025601874
test_loss: 0.006012593
train_loss: 0.0025061492
test_loss: 0.0059713935
train_loss: 0.0023806407
test_loss: 0.006016166
train_loss: 0.0024777981
test_loss: 0.0059334044
train_loss: 0.0023496873
test_loss: 0.0059624314
train_loss: 0.002597754
test_loss: 0.005996427
train_loss: 0.0023108632
test_loss: 0.0059493575
train_loss: 0.0024662938
test_loss: 0.005980176
train_loss: 0.0023669878
test_loss: 0.0059674815
train_loss: 0.002528324
test_loss: 0.0059748525
train_loss: 0.002480635
test_loss: 0.0060308664
train_loss: 0.0028631007
test_loss: 0.006068849
train_loss: 0.0026144676
test_loss: 0.0060238214
train_loss: 0.0024688356
test_loss: 0.005950021
train_loss: 0.0026010452
test_loss: 0.0059234905
train_loss: 0.0024937713
test_loss: 0.0059684105
train_loss: 0.0029774564
test_loss: 0.0059743687
train_loss: 0.0027907726
test_loss: 0.0059308
train_loss: 0.0024778422
test_loss: 0.0059457785
train_loss: 0.0024524548
test_loss: 0.0060918573
train_loss: 0.002472844
test_loss: 0.0059339497
train_loss: 0.0024016963
test_loss: 0.0059482567
train_loss: 0.0025015096
test_loss: 0.0059840637
train_loss: 0.0025869317
test_loss: 0.0059956163
train_loss: 0.002891935
test_loss: 0.006087763
train_loss: 0.0025700033
test_loss: 0.0059805014
train_loss: 0.0025054966
test_loss: 0.005947901
train_loss: 0.0023723126
test_loss: 0.006000305
train_loss: 0.0023980476
test_loss: 0.0060745073
train_loss: 0.0026328946
test_loss: 0.0060410034
train_loss: 0.0026694804
test_loss: 0.00598008
train_loss: 0.0027961968
test_loss: 0.006062526
train_loss: 0.0026514959
test_loss: 0.0059819985
train_loss: 0.0027804787
test_loss: 0.005952742
train_loss: 0.0028698393
test_loss: 0.005981638
train_loss: 0.0024700258
test_loss: 0.0060063875
train_loss: 0.0025951983
test_loss: 0.0059398473
train_loss: 0.0024626642
test_loss: 0.00596123
train_loss: 0.0024205963
test_loss: 0.0059074326
train_loss: 0.002458545
test_loss: 0.005942107
train_loss: 0.0026507922
test_loss: 0.0059245983
train_loss: 0.002529951
test_loss: 0.005887022
train_loss: 0.0026111144
test_loss: 0.005993545
train_loss: 0.0026371519
test_loss: 0.005890105
train_loss: 0.002652854
test_loss: 0.0059694173
train_loss: 0.002527066
test_loss: 0.005998219
train_loss: 0.0024414917
test_loss: 0.005901902
train_loss: 0.002444807
test_loss: 0.006015335
train_loss: 0.0026212363
test_loss: 0.006031593
train_loss: 0.0030209452
test_loss: 0.0060173566
train_loss: 0.0025061236
test_loss: 0.0058346684
train_loss: 0.002522538
test_loss: 0.0060099424
train_loss: 0.0025586607
test_loss: 0.0061557493
train_loss: 0.0033565175
test_loss: 0.0062105968
train_loss: 0.0025620153
test_loss: 0.0059089186
train_loss: 0.0027486836
test_loss: 0.0060818763
train_loss: 0.002540758
test_loss: 0.005973262
train_loss: 0.0028722682
test_loss: 0.006029504
train_loss: 0.0025331336
test_loss: 0.0059494413
train_loss: 0.0023266012
test_loss: 0.0059946696
train_loss: 0.0022926698
test_loss: 0.0059337965
train_loss: 0.002474735
test_loss: 0.006013504
train_loss: 0.0023538023
test_loss: 0.0059422804
train_loss: 0.0024067117
test_loss: 0.0057986784
train_loss: 0.002239871
test_loss: 0.0058548064
train_loss: 0.0024839877
test_loss: 0.0059103845
train_loss: 0.0024342618
test_loss: 0.0057647014
train_loss: 0.0023070092
test_loss: 0.0058537778
train_loss: 0.0023028643
test_loss: 0.0059660226
train_loss: 0.0024076519
test_loss: 0.005894589
train_loss: 0.0023521218
test_loss: 0.005870611
train_loss: 0.0022951895
test_loss: 0.0058158664
train_loss: 0.0024167048
test_loss: 0.005813973
train_loss: 0.0025358696
test_loss: 0.005879775
train_loss: 0.0023518677
test_loss: 0.0058991597
train_loss: 0.0024559405
test_loss: 0.0059500793
train_loss: 0.0023544163
test_loss: 0.005865221
train_loss: 0.0023320913
test_loss: 0.005882282
train_loss: 0.0022324827
test_loss: 0.0058026826
train_loss: 0.0023221176
test_loss: 0.0058375085
train_loss: 0.002265558
test_loss: 0.005884761
train_loss: 0.0022836532
test_loss: 0.005812487
train_loss: 0.002547884
test_loss: 0.0057945494
train_loss: 0.0022735125
test_loss: 0.0057450808
train_loss: 0.002175827
test_loss: 0.005847475
train_loss: 0.0023534931
test_loss: 0.005755676
train_loss: 0.0023988355
test_loss: 0.0057134153
train_loss: 0.0023791187
test_loss: 0.0059102755
train_loss: 0.0023839828
test_loss: 0.0058382316
train_loss: 0.002274572
test_loss: 0.0058120033
train_loss: 0.0023547474
test_loss: 0.0059161405
train_loss: 0.002333107
test_loss: 0.005776235
train_loss: 0.002273987
test_loss: 0.0059101935
train_loss: 0.0024316628
test_loss: 0.0058322414
train_loss: 0.0026381637
test_loss: 0.0059651276
train_loss: 0.002470782
test_loss: 0.005963926
train_loss: 0.0022415477
test_loss: 0.005892125
train_loss: 0.0022568302
test_loss: 0.005880871
train_loss: 0.0022198851
test_loss: 0.005867192
train_loss: 0.0020907256
test_loss: 0.00580919
train_loss: 0.0021409718
test_loss: 0.0058529996
train_loss: 0.002198773
test_loss: 0.005723215
train_loss: 0.0022464693
test_loss: 0.0058199735
train_loss: 0.0022326168
test_loss: 0.0058363457
train_loss: 0.0020746365
test_loss: 0.0057949354
train_loss: 0.002145361
test_loss: 0.005819638
train_loss: 0.0021774536
test_loss: 0.0058023287
train_loss: 0.0022051067
test_loss: 0.0058331937
train_loss: 0.0023119613
test_loss: 0.005776676
train_loss: 0.0023500903
test_loss: 0.0058064647
train_loss: 0.0023836032
test_loss: 0.005829557
train_loss: 0.0023890007
test_loss: 0.005770034
train_loss: 0.0021546143
test_loss: 0.0059319367
train_loss: 0.0025087674
test_loss: 0.0058033606
train_loss: 0.002313755
test_loss: 0.0058846143
train_loss: 0.0024493083
test_loss: 0.0057708034
train_loss: 0.0022270982
test_loss: 0.005831166
train_loss: 0.0023260918
test_loss: 0.005742349
train_loss: 0.002271438
test_loss: 0.0059040086
train_loss: 0.002331518
test_loss: 0.0057524536
train_loss: 0.0021815232
test_loss: 0.005861777
train_loss: 0.0023847916
test_loss: 0.0058245026
train_loss: 0.0024274613
test_loss: 0.00582834
train_loss: 0.0021738457
test_loss: 0.005883844
train_loss: 0.002387446
test_loss: 0.005823457
train_loss: 0.0024891444
test_loss: 0.0057924814
train_loss: 0.0021444028
test_loss: 0.0058413707
train_loss: 0.0023553828
test_loss: 0.005857062
train_loss: 0.002162868
test_loss: 0.005758064
train_loss: 0.002242595
test_loss: 0.005788345
train_loss: 0.0022162693
test_loss: 0.005906033
train_loss: 0.0022785293
test_loss: 0.005827205
train_loss: 0.0023468812
test_loss: 0.0058101104
train_loss: 0.002517824
test_loss: 0.005866754
train_loss: 0.0023146207
test_loss: 0.0057657207
train_loss: 0.002341693
test_loss: 0.0059419903
train_loss: 0.0024138512
test_loss: 0.0058296914
train_loss: 0.0022645975
test_loss: 0.005797817
train_loss: 0.0022768467
test_loss: 0.005923593
train_loss: 0.0024107005
test_loss: 0.006117963
train_loss: 0.0028411378
test_loss: 0.005911457
train_loss: 0.0023306485
test_loss: 0.0059179524
train_loss: 0.002612543
test_loss: 0.005874067
train_loss: 0.0023863683
test_loss: 0.005803883
train_loss: 0.0021979187
test_loss: 0.005825491
train_loss: 0.0021404482
test_loss: 0.00583748
train_loss: 0.0021530103
test_loss: 0.0057842373
train_loss: 0.002235555
test_loss: 0.0057480074
train_loss: 0.0024014637
test_loss: 0.0057694726
train_loss: 0.0024316765
test_loss: 0.005806746
train_loss: 0.0022927062
test_loss: 0.0057996423
train_loss: 0.00257074
test_loss: 0.0058614616
train_loss: 0.0022526225
test_loss: 0.0058245105
train_loss: 0.002308164
test_loss: 0.0057761553
train_loss: 0.0022881138
test_loss: 0.0058951457
train_loss: 0.0022061574
test_loss: 0.0057898615
train_loss: 0.0022283958
test_loss: 0.005767107
train_loss: 0.0023015628
test_loss: 0.005966141
train_loss: 0.0024530117
test_loss: 0.005961267
train_loss: 0.0026068448
test_loss: 0.005867435
train_loss: 0.002485237
test_loss: 0.0058521833
train_loss: 0.002400111
test_loss: 0.0058399406
train_loss: 0.0025571545
test_loss: 0.0057413997
train_loss: 0.0022014817
test_loss: 0.0058250558
train_loss: 0.002551914
test_loss: 0.005834471
train_loss: 0.0022021863
test_loss: 0.0059757605
train_loss: 0.0024344204
test_loss: 0.0058013704
train_loss: 0.0023405142
test_loss: 0.0058867075
train_loss: 0.002356641
test_loss: 0.005979129
train_loss: 0.0023118553
test_loss: 0.0058573736
train_loss: 0.0022066603
test_loss: 0.005835214
train_loss: 0.002128793
test_loss: 0.0057888236
train_loss: 0.002150034
test_loss: 0.005820301
train_loss: 0.002410789
test_loss: 0.0057688607
train_loss: 0.002223052
test_loss: 0.005724638
train_loss: 0.0021385094
test_loss: 0.005802345
train_loss: 0.0020596276
test_loss: 0.0058000013
train_loss: 0.002240418
test_loss: 0.0058643213
train_loss: 0.0025257084
test_loss: 0.005792637
train_loss: 0.0021404037
test_loss: 0.0058392035
train_loss: 0.0021837202
test_loss: 0.005883174
train_loss: 0.0022198125
test_loss: 0.0057062567
train_loss: 0.0021134724
test_loss: 0.0059776674
train_loss: 0.0023091154
test_loss: 0.0058370433
train_loss: 0.0023632643
test_loss: 0.00582315
train_loss: 0.0022463566
test_loss: 0.005795016
train_loss: 0.0024131802
test_loss: 0.0058586327
train_loss: 0.0021477896
test_loss: 0.0057812715
train_loss: 0.0022790427
test_loss: 0.005828464
train_loss: 0.002199257
test_loss: 0.0057526585
train_loss: 0.002325481
test_loss: 0.0059513533
train_loss: 0.0024169562
test_loss: 0.005765516
train_loss: 0.0020673515
test_loss: 0.005773666
train_loss: 0.0020830964
test_loss: 0.0059564076
train_loss: 0.0022830968
test_loss: 0.00582141
train_loss: 0.0024872613
test_loss: 0.005934548
train_loss: 0.0024968742
test_loss: 0.0057858033
train_loss: 0.002254796
test_loss: 0.0059183133
train_loss: 0.0022862842
test_loss: 0.005822167
train_loss: 0.0024113
test_loss: 0.0058919177
train_loss: 0.0025635902
test_loss: 0.0058476254
train_loss: 0.0023294503
test_loss: 0.005877614
train_loss: 0.0022235669
test_loss: 0.0058362507
train_loss: 0.0022326442
test_loss: 0.00586873
train_loss: 0.002274444
test_loss: 0.005727798
train_loss: 0.002423947
test_loss: 0.005724204
train_loss: 0.0020594366
test_loss: 0.0058140717
train_loss: 0.002192594
test_loss: 0.0058039804
train_loss: 0.0024977825
test_loss: 0.005894773
train_loss: 0.0023972555
test_loss: 0.005814158
train_loss: 0.002182382
test_loss: 0.0059056757
train_loss: 0.0022374599
test_loss: 0.005679106
train_loss: 0.0021485612
test_loss: 0.005660249
train_loss: 0.002363063
test_loss: 0.0057875165
train_loss: 0.0024862355
test_loss: 0.005752551
train_loss: 0.0022337986
test_loss: 0.0056910496
train_loss: 0.0021959727
test_loss: 0.005750567
train_loss: 0.0022192048
test_loss: 0.0057824114
train_loss: 0.0022200677
test_loss: 0.0059028543
train_loss: 0.002391804
test_loss: 0.0057872366
train_loss: 0.0021426643
test_loss: 0.0057323314
train_loss: 0.0021888907
test_loss: 0.0058149626
train_loss: 0.0020753723
test_loss: 0.005827027
train_loss: 0.002435658
test_loss: 0.0059488784
train_loss: 0.0022869257
test_loss: 0.0058356696
train_loss: 0.002098301
test_loss: 0.005732246
train_loss: 0.0024268827
test_loss: 0.0058687176
train_loss: 0.0022196025
test_loss: 0.005839
train_loss: 0.0022931013
test_loss: 0.0057051373
train_loss: 0.0021102906
test_loss: 0.0057570133
train_loss: 0.002159242
test_loss: 0.005781279
train_loss: 0.002100139
test_loss: 0.0058138343
train_loss: 0.0022034142
test_loss: 0.005808993
train_loss: 0.0021827214
test_loss: 0.005836879
train_loss: 0.0025687395
test_loss: 0.00586568
train_loss: 0.002134493
test_loss: 0.005764751
train_loss: 0.0022352096
test_loss: 0.0058546592
train_loss: 0.0023112171
test_loss: 0.00580426
train_loss: 0.0023791736
test_loss: 0.0057271603
train_loss: 0.0021191214
test_loss: 0.0057560313
train_loss: 0.0020645715
test_loss: 0.0057280688
train_loss: 0.0022212942
test_loss: 0.005683633
train_loss: 0.002051572
test_loss: 0.005827202
train_loss: 0.0024680267
test_loss: 0.005801229
train_loss: 0.0022768527
test_loss: 0.005842642
train_loss: 0.0022271518
test_loss: 0.005746885
train_loss: 0.002200263
test_loss: 0.0057657296
train_loss: 0.0022800025
test_loss: 0.005793296
train_loss: 0.0023499979
test_loss: 0.0056609977
train_loss: 0.0022394906
test_loss: 0.0059414823
train_loss: 0.0023520004
test_loss: 0.005799742
train_loss: 0.0022980294
test_loss: 0.005856756
train_loss: 0.002307102
test_loss: 0.0057305037
train_loss: 0.00206237
test_loss: 0.0057778535
train_loss: 0.001988343
test_loss: 0.005765212
train_loss: 0.002328225
test_loss: 0.0057377936
train_loss: 0.0021192469
test_loss: 0.0057514384
train_loss: 0.0020863966
test_loss: 0.005841821
train_loss: 0.0021008407
test_loss: 0.0057445643
train_loss: 0.0021090142
test_loss: 0.005792494
train_loss: 0.002150197
test_loss: 0.005718372
train_loss: 0.0021591715
test_loss: 0.005702056
train_loss: 0.0020788563
test_loss: 0.0056909383
train_loss: 0.0019788358
test_loss: 0.005685688
train_loss: 0.0019393817
test_loss: 0.0056919223
train_loss: 0.002088313
test_loss: 0.0057031116
train_loss: 0.0018571311
test_loss: 0.005806493
train_loss: 0.0022910358
test_loss: 0.0058052526
train_loss: 0.001985303
test_loss: 0.0057212575
train_loss: 0.0021600449
test_loss: 0.0057651354
train_loss: 0.0023115075
test_loss: 0.0057854205
train_loss: 0.0021876027
test_loss: 0.005779604
train_loss: 0.00244205
test_loss: 0.0061453544
train_loss: 0.0026464611
test_loss: 0.005846889
train_loss: 0.0024794783
test_loss: 0.0059060184
train_loss: 0.0024070765
test_loss: 0.0060433033
train_loss: 0.0027824172
test_loss: 0.005786814
train_loss: 0.002114985
test_loss: 0.0058141868
train_loss: 0.0021714997
test_loss: 0.005749997
train_loss: 0.0021027108
test_loss: 0.0057922
train_loss: 0.0020844191
test_loss: 0.0057649314
train_loss: 0.0021157428
test_loss: 0.0057770866
train_loss: 0.0021070344
test_loss: 0.0057778643
train_loss: 0.0020230864
test_loss: 0.005746225
train_loss: 0.002049069
test_loss: 0.005788967
train_loss: 0.0023192419
test_loss: 0.005842297
train_loss: 0.0021477195
test_loss: 0.0057650013
train_loss: 0.0024472624
test_loss: 0.0058411127
train_loss: 0.002358382
test_loss: 0.005742112
train_loss: 0.0019999165
test_loss: 0.0057911556
train_loss: 0.0020522026
test_loss: 0.0057700765
train_loss: 0.0020548962
test_loss: 0.0057756654
train_loss: 0.0022458017
test_loss: 0.0058155027
train_loss: 0.0021698219
test_loss: 0.0057166023
train_loss: 0.00201647
test_loss: 0.0057632616
train_loss: 0.002125966
test_loss: 0.0057684337
train_loss: 0.0021909303
test_loss: 0.0057265675
train_loss: 0.0023770758
test_loss: 0.0057421545
train_loss: 0.0021327387
test_loss: 0.0058747507
train_loss: 0.0022644717
test_loss: 0.0058523915
train_loss: 0.0020201243
test_loss: 0.005791388
train_loss: 0.0020511898
test_loss: 0.0057105892
train_loss: 0.0020689678
test_loss: 0.0057889326
train_loss: 0.0022701707
test_loss: 0.005778251
train_loss: 0.0022409332
test_loss: 0.005684025
train_loss: 0.0021473425
test_loss: 0.0057425373
train_loss: 0.0022585487
test_loss: 0.005760256
train_loss: 0.0022185273
test_loss: 0.005829225
train_loss: 0.0022230737
test_loss: 0.0057871835
train_loss: 0.0020862832
test_loss: 0.005842918
train_loss: 0.0023358269
test_loss: 0.0058911135
train_loss: 0.0022057302
test_loss: 0.0057994304
train_loss: 0.0020838047
test_loss: 0.0057722335
train_loss: 0.0021514301
test_loss: 0.0057774177
train_loss: 0.0021364363
test_loss: 0.0057267887
train_loss: 0.0023734155
test_loss: 0.0058158794
train_loss: 0.0023240843
test_loss: 0.005810891
train_loss: 0.00228172
test_loss: 0.005830983
train_loss: 0.002412366
test_loss: 0.005769871
train_loss: 0.0023717156
test_loss: 0.0058662165
train_loss: 0.002284208
test_loss: 0.0057704244
train_loss: 0.0021253142
test_loss: 0.005780417
train_loss: 0.0020450158
test_loss: 0.005692156
train_loss: 0.0020244878
test_loss: 0.005803337
train_loss: 0.0019346586
test_loss: 0.005723435
train_loss: 0.0021629108
test_loss: 0.0057955296
train_loss: 0.0021973758
test_loss: 0.0056806337
train_loss: 0.002210015
test_loss: 0.005881999
train_loss: 0.0021163684
test_loss: 0.0058742906
train_loss: 0.0023432486
test_loss: 0.0058345753
train_loss: 0.0024060616
test_loss: 0.005931413
train_loss: 0.0023477348
test_loss: 0.0058685397
train_loss: 0.0022190607
test_loss: 0.0057361
train_loss: 0.002066769
test_loss: 0.0057287617
train_loss: 0.0021216588
test_loss: 0.005801921
train_loss: 0.0023349759
test_loss: 0.0057601016
train_loss: 0.0021133781
test_loss: 0.0058696144
train_loss: 0.0021796878
test_loss: 0.005765981
train_loss: 0.0020635924
test_loss: 0.005754715
train_loss: 0.0020991194
test_loss: 0.005715783
train_loss: 0.0022070338
test_loss: 0.005765233
train_loss: 0.0022181452
test_loss: 0.00579465
train_loss: 0.002201464
test_loss: 0.005857252
train_loss: 0.0022821522
test_loss: 0.0058386
train_loss: 0.0019852521
test_loss: 0.005729324
train_loss: 0.0020181518
test_loss: 0.0057983715
train_loss: 0.0020993932
test_loss: 0.005752552
train_loss: 0.002290034
test_loss: 0.0058003757
train_loss: 0.0021424456
test_loss: 0.005819703
train_loss: 0.0020591007
test_loss: 0.005780997
train_loss: 0.0020238694
test_loss: 0.005753571
train_loss: 0.0019881048
test_loss: 0.005802742
train_loss: 0.0019982082
test_loss: 0.005677488
train_loss: 0.002199691
test_loss: 0.0057289135
train_loss: 0.0020497646
test_loss: 0.005689878
train_loss: 0.002168392
test_loss: 0.005768385
train_loss: 0.0022113689
test_loss: 0.0057790396
train_loss: 0.0020025128
test_loss: 0.005771685
train_loss: 0.0020651827
test_loss: 0.005760578
train_loss: 0.0020502876
test_loss: 0.0057239197
train_loss: 0.0020141082
test_loss: 0.005836053
train_loss: 0.0021299599
test_loss: 0.0058552134
train_loss: 0.0021010141
test_loss: 0.0057378043
train_loss: 0.0020806221
test_loss: 0.0058238055
train_loss: 0.0020296846
test_loss: 0.0057012043
train_loss: 0.002192662
test_loss: 0.0057712565
train_loss: 0.0019871613
test_loss: 0.0056919744
train_loss: 0.0019012836
test_loss: 0.0056933253
train_loss: 0.0020345077
test_loss: 0.0058342014
train_loss: 0.0020629822
test_loss: 0.005779057
train_loss: 0.0022787908
test_loss: 0.005917862
train_loss: 0.0023503075
test_loss: 0.005867726
train_loss: 0.0024076584
test_loss: 0.005869617
train_loss: 0.002479737
test_loss: 0.0057970267
train_loss: 0.002080334
test_loss: 0.0058027203
train_loss: 0.0021836853
test_loss: 0.005860034
train_loss: 0.0020947475
test_loss: 0.005720308
train_loss: 0.0020657438
test_loss: 0.0058842683
train_loss: 0.0020915545
test_loss: 0.005749238
train_loss: 0.0020276299
test_loss: 0.0057279738
train_loss: 0.0021140173
test_loss: 0.0057399543
train_loss: 0.0020739648
test_loss: 0.0058534048
train_loss: 0.0019743787
test_loss: 0.005798426
train_loss: 0.0019841401
test_loss: 0.005817575
train_loss: 0.0018826791
test_loss: 0.005739965
train_loss: 0.0020807087
test_loss: 0.005744793
train_loss: 0.0020301251
test_loss: 0.005660688
train_loss: 0.00222114
test_loss: 0.0057309517
train_loss: 0.0020561917
test_loss: 0.0058088126
train_loss: 0.0019201238
test_loss: 0.005807998
train_loss: 0.0023064262
test_loss: 0.0057680258
train_loss: 0.002077418
test_loss: 0.005828988
train_loss: 0.0020712737
test_loss: 0.0058281347
train_loss: 0.0021005077
test_loss: 0.0058089113
train_loss: 0.002028449
test_loss: 0.005748222
train_loss: 0.0021490385
test_loss: 0.0057261265
train_loss: 0.0022164802
test_loss: 0.005800144
train_loss: 0.0022263261
test_loss: 0.005806949
train_loss: 0.0022406084
test_loss: 0.0059071914
train_loss: 0.0022776804
test_loss: 0.0058500743
train_loss: 0.0020553144
test_loss: 0.0058225393
train_loss: 0.0021199982
test_loss: 0.00573037
train_loss: 0.0020306972
test_loss: 0.0056917807
train_loss: 0.0021679406
test_loss: 0.0058926167
train_loss: 0.0022319553
test_loss: 0.005919202
train_loss: 0.0021875498
test_loss: 0.005817396
train_loss: 0.002042002
test_loss: 0.005819055
train_loss: 0.0019766453
test_loss: 0.0057376064
train_loss: 0.0022299634
test_loss: 0.0056747305
train_loss: 0.0020726298
test_loss: 0.0058125304
train_loss: 0.002251639
test_loss: 0.0057811537
train_loss: 0.0020622849
test_loss: 0.00581359
train_loss: 0.0020807602
test_loss: 0.0058299312
train_loss: 0.0019546081
test_loss: 0.005902079
train_loss: 0.0020618916
test_loss: 0.0057159956
train_loss: 0.0022309392
test_loss: 0.0057430584
train_loss: 0.0021766091
test_loss: 0.0057148994
train_loss: 0.0024185188
test_loss: 0.005836514
train_loss: 0.002266116
test_loss: 0.0057853116
train_loss: 0.0021161453
test_loss: 0.0057740314
train_loss: 0.0020415448
test_loss: 0.0057668947
train_loss: 0.0019423739
test_loss: 0.0057931137
train_loss: 0.0019355979
test_loss: 0.0057432945
train_loss: 0.002084621
test_loss: 0.0057757306
train_loss: 0.0022607548
test_loss: 0.0058259624
train_loss: 0.0021792618
test_loss: 0.00590665
train_loss: 0.00245314
test_loss: 0.0058899065
train_loss: 0.0024315636
test_loss: 0.0058112005
train_loss: 0.0020662383
test_loss: 0.005860222
train_loss: 0.0022214944
test_loss: 0.0060803257
train_loss: 0.0022873878
test_loss: 0.005827847
train_loss: 0.00226204
test_loss: 0.005872119
train_loss: 0.0020290292
test_loss: 0.005724612
train_loss: 0.0019828836
test_loss: 0.005778247
train_loss: 0.001904401
test_loss: 0.0057701953
train_loss: 0.0020545726
test_loss: 0.005715738
train_loss: 0.0021157502
test_loss: 0.0057934304
train_loss: 0.002100084
test_loss: 0.005912712
train_loss: 0.0021444196
test_loss: 0.005801108
train_loss: 0.0020159597
test_loss: 0.0057375114
train_loss: 0.001935194
test_loss: 0.005769533
train_loss: 0.0021341657
test_loss: 0.0057533733
train_loss: 0.0022003877
test_loss: 0.0057638483
train_loss: 0.0021388163
test_loss: 0.0058112266
train_loss: 0.0021319469
test_loss: 0.005793485
train_loss: 0.0019134744
test_loss: 0.0057482175
train_loss: 0.0021592756
test_loss: 0.0058223354
train_loss: 0.0020961456
test_loss: 0.0057992884
train_loss: 0.0019522433
test_loss: 0.005753371
train_loss: 0.0019914396
test_loss: 0.0057417313
train_loss: 0.0020624849
test_loss: 0.005871651
train_loss: 0.0023454982
test_loss: 0.0058830497
train_loss: 0.0021443842
test_loss: 0.0058272337
train_loss: 0.002069109
test_loss: 0.005869381
train_loss: 0.001966323
test_loss: 0.0058354037
train_loss: 0.001921114
test_loss: 0.0058400827
train_loss: 0.0021280681
test_loss: 0.005721619
train_loss: 0.001968492
test_loss: 0.0057596313
train_loss: 0.001989824
test_loss: 0.0057679177
train_loss: 0.0018912388
test_loss: 0.005716886
train_loss: 0.0019485264
test_loss: 0.005762247
train_loss: 0.00202882
test_loss: 0.0059257043
train_loss: 0.0021933264
test_loss: 0.0057586352
train_loss: 0.002032369
test_loss: 0.005961976
train_loss: 0.0023099151
test_loss: 0.005855582
train_loss: 0.0023143291
test_loss: 0.0057727774
train_loss: 0.002175529
test_loss: 0.005855945
train_loss: 0.0018399323
test_loss: 0.005849092
train_loss: 0.0021872635
test_loss: 0.0059241634
train_loss: 0.0023580166
test_loss: 0.005979277
train_loss: 0.0023882722
test_loss: 0.0058459374
train_loss: 0.0021285242
test_loss: 0.005730478
train_loss: 0.0021683588
test_loss: 0.005820072
train_loss: 0.001987697
test_loss: 0.0057833823
train_loss: 0.0019577504
test_loss: 0.0057730996
train_loss: 0.001844327
test_loss: 0.005742649
train_loss: 0.0019010165
test_loss: 0.0058684335
train_loss: 0.0023608527
test_loss: 0.0060820593
train_loss: 0.00251672
test_loss: 0.0057558897
train_loss: 0.0020598548
test_loss: 0.005771379
train_loss: 0.0021817095
test_loss: 0.0059378403
train_loss: 0.0022268654
test_loss: 0.0058095544
train_loss: 0.0020042574
test_loss: 0.005827381
train_loss: 0.0020263335
test_loss: 0.005766908
train_loss: 0.001888976
test_loss: 0.0057554566
train_loss: 0.0019515931
test_loss: 0.0057763807
train_loss: 0.0020214966
test_loss: 0.0058760233
train_loss: 0.002379253
test_loss: 0.005839766
train_loss: 0.0023927977
test_loss: 0.0060795974
train_loss: 0.0022521876
test_loss: 0.0058895606
train_loss: 0.0021820543
test_loss: 0.0058244695
train_loss: 0.002155485
test_loss: 0.005814566
train_loss: 0.0020544236
test_loss: 0.005840866
train_loss: 0.0024524466
test_loss: 0.00609882
train_loss: 0.002362464
test_loss: 0.0057818904
train_loss: 0.00212707
test_loss: 0.005772226
train_loss: 0.0018920511
test_loss: 0.0058835205
train_loss: 0.0019266127
test_loss: 0.005808895
train_loss: 0.0019971211
test_loss: 0.0057206554
train_loss: 0.0020783204
test_loss: 0.0057258746
train_loss: 0.001899237
test_loss: 0.00579661
train_loss: 0.002001502
test_loss: 0.00588886
train_loss: 0.00195937
test_loss: 0.0058355844
train_loss: 0.0018926334
test_loss: 0.005689968
train_loss: 0.0018869475
test_loss: 0.0057173106
train_loss: 0.0018891203
test_loss: 0.0057375724
train_loss: 0.001984196
test_loss: 0.0057375445
train_loss: 0.0019387071
test_loss: 0.0057071354
train_loss: 0.001926268
test_loss: 0.0057552336
train_loss: 0.001911429
test_loss: 0.0059040952
train_loss: 0.0020733522
test_loss: 0.0058967653
train_loss: 0.0022481
test_loss: 0.0058477703
train_loss: 0.0021657846
test_loss: 0.005796926
train_loss: 0.0020455078
test_loss: 0.00583289
train_loss: 0.0023195094
test_loss: 0.0059060967
train_loss: 0.002092519
test_loss: 0.005812545
train_loss: 0.0019934757
test_loss: 0.0057696044
train_loss: 0.0021310472
test_loss: 0.005702942
train_loss: 0.002026701
test_loss: 0.0058357255
train_loss: 0.002017768
test_loss: 0.005857553
train_loss: 0.0022576666
test_loss: 0.005911311
train_loss: 0.0021020332
test_loss: 0.005931809
train_loss: 0.002303006
test_loss: 0.0058916775
train_loss: 0.002046519
test_loss: 0.0058383443
train_loss: 0.0020227847
test_loss: 0.0057587298
train_loss: 0.0020219926
test_loss: 0.0057929037
train_loss: 0.0020904336
test_loss: 0.0058143265
train_loss: 0.002148886
test_loss: 0.005880929
train_loss: 0.0021257892
test_loss: 0.005760494
train_loss: 0.0019444248
test_loss: 0.005851977
train_loss: 0.001988965
test_loss: 0.0057515176
train_loss: 0.0020921917
test_loss: 0.0058695823
train_loss: 0.0020893284
test_loss: 0.005841611
train_loss: 0.0019883255
test_loss: 0.0057542287
train_loss: 0.0019580547
test_loss: 0.0058841747
train_loss: 0.0023160614
test_loss: 0.0060026716
train_loss: 0.002166458
test_loss: 0.0058199163
train_loss: 0.0020791998
test_loss: 0.005835196
train_loss: 0.0019671388
test_loss: 0.005781833
train_loss: 0.0019570035
test_loss: 0.005806488
train_loss: 0.002126371
test_loss: 0.0058047455
train_loss: 0.0019299953
test_loss: 0.0058503575
train_loss: 0.0021112196
test_loss: 0.0058341613
train_loss: 0.0018961173
test_loss: 0.0059137344
train_loss: 0.0020774845
test_loss: 0.0057965363
train_loss: 0.001934384
test_loss: 0.0058097807
train_loss: 0.0021306383
test_loss: 0.005825493
train_loss: 0.0020270308
test_loss: 0.0057892906
train_loss: 0.0020157099
test_loss: 0.0058266274
train_loss: 0.0019344818
test_loss: 0.0057833153
train_loss: 0.0019702178
test_loss: 0.0057282667
train_loss: 0.0017627238
test_loss: 0.005742597
train_loss: 0.0019014822
test_loss: 0.0058458117
train_loss: 0.0017228203
test_loss: 0.0057653137
train_loss: 0.0018449225
test_loss: 0.005794702
train_loss: 0.0018583699
test_loss: 0.0057746973
train_loss: 0.0020344376
test_loss: 0.005842031
train_loss: 0.00204236
test_loss: 0.005818948
train_loss: 0.0021935736
test_loss: 0.005763399
train_loss: 0.002210528
test_loss: 0.0058434457
train_loss: 0.0020678341
test_loss: 0.0058463598
train_loss: 0.002194043
test_loss: 0.005817428
train_loss: 0.0019280248
test_loss: 0.005783603
train_loss: 0.0019871048
test_loss: 0.0059754923
train_loss: 0.0022347555
test_loss: 0.0058313156
train_loss: 0.001976465
test_loss: 0.0058207344
train_loss: 0.0018758192
test_loss: 0.005823701
train_loss: 0.0019404384
test_loss: 0.0058446275
train_loss: 0.0019489228
test_loss: 0.005785678
train_loss: 0.0020311824
test_loss: 0.005870642
train_loss: 0.0020276462
test_loss: 0.005762415
train_loss: 0.0019817178
test_loss: 0.00594118
train_loss: 0.002044266
test_loss: 0.0059634745
train_loss: 0.0021206883
test_loss: 0.0058944942
train_loss: 0.0023780854
test_loss: 0.005872374
train_loss: 0.001996385
test_loss: 0.0058144373
train_loss: 0.0019829336
test_loss: 0.0058565554
train_loss: 0.0020134915
test_loss: 0.005775314
train_loss: 0.0017936991
test_loss: 0.00577923
train_loss: 0.0021415087
test_loss: 0.0058468445
train_loss: 0.002188
test_loss: 0.00587921
train_loss: 0.0020589014
test_loss: 0.0057510217
train_loss: 0.0020894494
test_loss: 0.005889397
train_loss: 0.0022080995
test_loss: 0.0058580255
train_loss: 0.0019879993
test_loss: 0.0059838863
train_loss: 0.0025407253
test_loss: 0.0058623273
train_loss: 0.0021786904
test_loss: 0.0059022224
train_loss: 0.0021777372
test_loss: 0.0060292897
train_loss: 0.002431196
test_loss: 0.0059939246
train_loss: 0.002077978
test_loss: 0.0058971248
train_loss: 0.0022566305
test_loss: 0.005900806
train_loss: 0.0020675743
test_loss: 0.0058301245
train_loss: 0.001985362
test_loss: 0.0058815
train_loss: 0.0019487034
test_loss: 0.005853085
train_loss: 0.0018432655
test_loss: 0.0057666497
train_loss: 0.0019218579
test_loss: 0.0057356255
train_loss: 0.0019456773
test_loss: 0.005755192
train_loss: 0.0020596436
test_loss: 0.005819697
train_loss: 0.0019290311
test_loss: 0.005787312
train_loss: 0.0017979569
test_loss: 0.0057659
train_loss: 0.0018454491
test_loss: 0.0057737306
train_loss: 0.0018459433
test_loss: 0.005747717
train_loss: 0.0017982629
test_loss: 0.0058429586
train_loss: 0.0018778914
test_loss: 0.005803231
train_loss: 0.0020115608
test_loss: 0.005770258
train_loss: 0.0018910528
test_loss: 0.0058351224
train_loss: 0.0022414147
test_loss: 0.0058200657
train_loss: 0.0020822352
test_loss: 0.0059262505
train_loss: 0.0023887805
test_loss: 0.005923438
train_loss: 0.0023255327
test_loss: 0.00603939
train_loss: 0.0020865935
test_loss: 0.005871105
train_loss: 0.0020105704
test_loss: 0.0060765943
train_loss: 0.002579979
test_loss: 0.005891839
train_loss: 0.0020859907
test_loss: 0.0059591397
train_loss: 0.002659989
test_loss: 0.006012669
train_loss: 0.0022943139
test_loss: 0.006044326
train_loss: 0.0023694665
test_loss: 0.005999152
train_loss: 0.0022102783
test_loss: 0.006006176
train_loss: 0.002340977
test_loss: 0.0058406685
train_loss: 0.0021259715
test_loss: 0.0058538653
train_loss: 0.002039319
test_loss: 0.0058669634
train_loss: 0.0019619577
test_loss: 0.0058246884
train_loss: 0.0019689738
test_loss: 0.0058385585
train_loss: 0.0018540933
test_loss: 0.005790383
train_loss: 0.0019316238
test_loss: 0.0057748817
train_loss: 0.0018935933
test_loss: 0.005935532
train_loss: 0.0021578076
test_loss: 0.005786479
train_loss: 0.0021795956
test_loss: 0.005852255
train_loss: 0.002150382
test_loss: 0.0058450694
train_loss: 0.001893263
test_loss: 0.0058681984
train_loss: 0.0021057962
test_loss: 0.005932328
train_loss: 0.0020816869
test_loss: 0.005810713
train_loss: 0.0020930192
test_loss: 0.005867051
train_loss: 0.0020392844
test_loss: 0.0058596283
train_loss: 0.0018576975
test_loss: 0.0058381543
train_loss: 0.0019086951
test_loss: 0.0058729723
train_loss: 0.0018417214
test_loss: 0.00585094
train_loss: 0.0017727761
test_loss: 0.0057915347
train_loss: 0.0017864532
test_loss: 0.0057426044
train_loss: 0.0019702902
test_loss: 0.0058161635
train_loss: 0.0021458205
test_loss: 0.0058031776
train_loss: 0.002028644
test_loss: 0.0057694754
train_loss: 0.0017866904
test_loss: 0.0057942504
train_loss: 0.0017727535
test_loss: 0.0058139767
train_loss: 0.0019185173
test_loss: 0.0057516205
train_loss: 0.0019245015
test_loss: 0.005830874
train_loss: 0.001831645
test_loss: 0.005835944
train_loss: 0.0018916295
test_loss: 0.005975853
train_loss: 0.0021862325
test_loss: 0.005818937
train_loss: 0.0019091635
test_loss: 0.005857656
train_loss: 0.0019395383
test_loss: 0.00586158
train_loss: 0.00190034
test_loss: 0.00584865
train_loss: 0.0019897658
test_loss: 0.005941506
train_loss: 0.002091053
test_loss: 0.0058290567
train_loss: 0.0020085108
test_loss: 0.005823235
train_loss: 0.0019139433
test_loss: 0.005814537
train_loss: 0.0018845123
test_loss: 0.005887796
train_loss: 0.0020615975
test_loss: 0.005885834
train_loss: 0.0019255404
test_loss: 0.005893969
train_loss: 0.002064122
test_loss: 0.005914932
train_loss: 0.0020087005
test_loss: 0.0058729304
train_loss: 0.0018507624
test_loss: 0.005795056
train_loss: 0.0019317562
test_loss: 0.0058755684
train_loss: 0.0021083509
test_loss: 0.0059560295
train_loss: 0.0020543558
test_loss: 0.0058398736
train_loss: 0.0019137
test_loss: 0.005862347
train_loss: 0.0019748667
test_loss: 0.0058719832
train_loss: 0.0020396314
test_loss: 0.0058347373
train_loss: 0.0022093775
test_loss: 0.0058238627
train_loss: 0.002315442
test_loss: 0.0058285613
train_loss: 0.002025087
test_loss: 0.0058756513
train_loss: 0.0022362822
test_loss: 0.0058902735
train_loss: 0.0019154642
test_loss: 0.0058396976
train_loss: 0.0023556333
test_loss: 0.005952445
train_loss: 0.0019831094
test_loss: 0.0058855074
train_loss: 0.0017826536
test_loss: 0.0058681774
train_loss: 0.0018640279
test_loss: 0.005785868
train_loss: 0.0019531478
test_loss: 0.0059693237
train_loss: 0.0020805483
test_loss: 0.005884007
train_loss: 0.0019416872
test_loss: 0.0058559896
train_loss: 0.0020467169
test_loss: 0.0059507373
train_loss: 0.001997901
test_loss: 0.005872334
train_loss: 0.0019467999
test_loss: 0.005988897
train_loss: 0.0023093426
test_loss: 0.005898835
train_loss: 0.0021356174
test_loss: 0.005923697
train_loss: 0.0019186416
test_loss: 0.0058532953
train_loss: 0.0019892005
test_loss: 0.005927615
train_loss: 0.0020911815
test_loss: 0.0059087696
train_loss: 0.0020735455
test_loss: 0.005961749
train_loss: 0.0021522257/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0058923457
train_loss: 0.0019910235
test_loss: 0.0058612972
train_loss: 0.0021159297
test_loss: 0.005891979
train_loss: 0.002044686
test_loss: 0.0058731963
train_loss: 0.0019825215
test_loss: 0.0060048643
train_loss: 0.002132597
test_loss: 0.0059340405
train_loss: 0.0021534364
test_loss: 0.0059388923
train_loss: 0.002100079
test_loss: 0.0058955215
train_loss: 0.002029642
test_loss: 0.005918588
train_loss: 0.0019929383
test_loss: 0.0059370557
train_loss: 0.0018753281
test_loss: 0.005863736
train_loss: 0.0019492382
test_loss: 0.0058977157
train_loss: 0.00206829
test_loss: 0.0059082913
train_loss: 0.0019272242
test_loss: 0.005862758
train_loss: 0.0017960327
test_loss: 0.0058005466
train_loss: 0.0018927325
test_loss: 0.005866712
train_loss: 0.0018855798
test_loss: 0.0058917166
train_loss: 0.0019297823
test_loss: 0.0058562066
train_loss: 0.0018948675
test_loss: 0.005866394
train_loss: 0.0019086829
test_loss: 0.005889095
train_loss: 0.0019866284
test_loss: 0.00588815
train_loss: 0.002012488
test_loss: 0.0059088697
train_loss: 0.0020258543
test_loss: 0.0059046187
train_loss: 0.002213244
test_loss: 0.0058946116
train_loss: 0.0022465137
test_loss: 0.0058572534
train_loss: 0.002036258
test_loss: 0.006007225
train_loss: 0.0021211396
test_loss: 0.005889333
train_loss: 0.0017618409
test_loss: 0.0058693313
train_loss: 0.0018296149
test_loss: 0.0058450024
train_loss: 0.0018266864
test_loss: 0.005834883
train_loss: 0.0017865213
test_loss: 0.0059124483
train_loss: 0.0019878722
test_loss: 0.0059359875
train_loss: 0.0019112956
test_loss: 0.005868795
train_loss: 0.0019762968
test_loss: 0.005926507
train_loss: 0.001782066
test_loss: 0.005844449
train_loss: 0.0018428591
test_loss: 0.005965389
train_loss: 0.0020477355
test_loss: 0.005845887
train_loss: 0.0020267142
test_loss: 0.0059106536
train_loss: 0.0020158815
test_loss: 0.005948363
train_loss: 0.00235395
test_loss: 0.005956373
train_loss: 0.0021701152
test_loss: 0.005952555
train_loss: 0.0020627505
test_loss: 0.0058768797
train_loss: 0.0020746232
test_loss: 0.005880753
train_loss: 0.001995638
test_loss: 0.0059523163
train_loss: 0.0019909535
test_loss: 0.005972411
train_loss: 0.0020106297
test_loss: 0.005880655
train_loss: 0.0020746943
test_loss: 0.0058650463
train_loss: 0.0020166547
test_loss: 0.0060458817
train_loss: 0.00204374
test_loss: 0.005869598
train_loss: 0.0019412192
test_loss: 0.0060025747
train_loss: 0.0022211163
test_loss: 0.0058613294
train_loss: 0.0019552866
test_loss: 0.0059604715
train_loss: 0.0020067035
test_loss: 0.005866897
train_loss: 0.0020276108
test_loss: 0.006055673
train_loss: 0.002154006
test_loss: 0.005891109
train_loss: 0.0021483107
test_loss: 0.0059234975
train_loss: 0.0021394207
test_loss: 0.0059397477
train_loss: 0.002142566
test_loss: 0.005942983
train_loss: 0.0019515813
test_loss: 0.0059816805
train_loss: 0.0020471644
test_loss: 0.005895589
train_loss: 0.0019673216
test_loss: 0.005938947
train_loss: 0.001907696
test_loss: 0.0058904
train_loss: 0.0021669087
test_loss: 0.0059307935
train_loss: 0.0020175031
test_loss: 0.0059571597
train_loss: 0.0020351163
test_loss: 0.0059626536
train_loss: 0.002061217
test_loss: 0.0058671236
train_loss: 0.0019569602
test_loss: 0.0058988184
train_loss: 0.0019053547
test_loss: 0.0058716335
train_loss: 0.0019256476
test_loss: 0.005939847
train_loss: 0.0018954102
test_loss: 0.0059013865
train_loss: 0.0019041578
test_loss: 0.005834013
train_loss: 0.0018902557
test_loss: 0.005863332
train_loss: 0.0018986785
test_loss: 0.0059317453
train_loss: 0.0020786757
test_loss: 0.005879194
train_loss: 0.0018641573
test_loss: 0.005901203
train_loss: 0.0019278519
test_loss: 0.005922897
train_loss: 0.002021199
test_loss: 0.00599186
train_loss: 0.0019831862
test_loss: 0.0060019894
train_loss: 0.0020322548
test_loss: 0.0059517547
train_loss: 0.0020063021
test_loss: 0.0059293546
train_loss: 0.002207504
test_loss: 0.005877644
train_loss: 0.0020831742
test_loss: 0.0059150327
train_loss: 0.0019269106
test_loss: 0.005912094
train_loss: 0.0017710023
test_loss: 0.0058912886
train_loss: 0.0019671037
test_loss: 0.0058687404
train_loss: 0.0019728115
test_loss: 0.0058315415
train_loss: 0.0019029009
test_loss: 0.0059156087
train_loss: 0.0018598872
test_loss: 0.0058035413
train_loss: 0.0017190326
test_loss: 0.0058259154
train_loss: 0.0019637262
test_loss: 0.0058643036
train_loss: 0.0020506713
test_loss: 0.0058253068
train_loss: 0.0019714092
test_loss: 0.0059419903
train_loss: 0.0018826921
test_loss: 0.0059202975
train_loss: 0.0019437018
test_loss: 0.005879711
train_loss: 0.0022296263
test_loss: 0.00586033
train_loss: 0.0018657197
test_loss: 0.0058492697
train_loss: 0.0017680698
test_loss: 0.0058934717
train_loss: 0.0019440191
test_loss: 0.0059156376
train_loss: 0.002031075
test_loss: 0.0058984454
train_loss: 0.0020299132
test_loss: 0.0058710594
train_loss: 0.0018731491
test_loss: 0.0059753275
train_loss: 0.0020499218
test_loss: 0.0058252923
train_loss: 0.0020322613
test_loss: 0.005995809
train_loss: 0.0022286938
test_loss: 0.005938253
train_loss: 0.0020774724
test_loss: 0.0061089727
train_loss: 0.002558032
test_loss: 0.005855383
train_loss: 0.0019182889
test_loss: 0.00605421
train_loss: 0.002270392
test_loss: 0.0059427433
train_loss: 0.0020382656
test_loss: 0.006030151
train_loss: 0.002304605
test_loss: 0.005909859
train_loss: 0.0020934201
test_loss: 0.005891694
train_loss: 0.0020317123
test_loss: 0.00598832
train_loss: 0.002138136
test_loss: 0.005863273
train_loss: 0.0018907761
test_loss: 0.005916777
train_loss: 0.0019380568
test_loss: 0.005859417
train_loss: 0.0018707952
test_loss: 0.005951334
train_loss: 0.0020074572
test_loss: 0.005846491
train_loss: 0.0018426512
test_loss: 0.0058757947
train_loss: 0.0019306983
test_loss: 0.005917778
train_loss: 0.0021113497
test_loss: 0.0059715738
train_loss: 0.001986979
test_loss: 0.005904675
train_loss: 0.0019229685
test_loss: 0.005866954
train_loss: 0.001825541
test_loss: 0.0059220484
train_loss: 0.0020000678
test_loss: 0.005876643
train_loss: 0.0019933213
test_loss: 0.00584719
train_loss: 0.0019348981
test_loss: 0.0058663758
train_loss: 0.0018865723
test_loss: 0.0058302656
train_loss: 0.0018016798
test_loss: 0.0058657373
train_loss: 0.001910407
test_loss: 0.005899521
train_loss: 0.0017665074
test_loss: 0.005900799
train_loss: 0.0020524163
test_loss: 0.005956586
train_loss: 0.0019713717
test_loss: 0.005848782
train_loss: 0.0020940858
test_loss: 0.0059666815
train_loss: 0.0018143005
test_loss: 0.0059799817
train_loss: 0.0018486618
test_loss: 0.005972112
train_loss: 0.0020059617
test_loss: 0.005912908
train_loss: 0.0020411029
test_loss: 0.006041142
train_loss: 0.001993466
test_loss: 0.0059186006
train_loss: 0.002018247
test_loss: 0.0058710687
train_loss: 0.0019553064
test_loss: 0.0058301226
train_loss: 0.001978808
test_loss: 0.0060459883
train_loss: 0.0020400195
test_loss: 0.0059003555
train_loss: 0.0020161804
test_loss: 0.005983348
train_loss: 0.0018144844
test_loss: 0.005954384
train_loss: 0.0019859408
test_loss: 0.0059549
train_loss: 0.0019712788
test_loss: 0.0059477077
train_loss: 0.0020032823
test_loss: 0.0059950124
train_loss: 0.001964281
test_loss: 0.005949928
train_loss: 0.0018103657
test_loss: 0.0058811544
train_loss: 0.0018766694
test_loss: 0.0059433547
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1 --function f1 --psi -2 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 4000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d60389400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d602d06a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d60389598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d602d01e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d603192f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d60319840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d6022bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d60260620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d60203ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d601c16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d601c17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d6016bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d60186a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d601532f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d600e4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d600e4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d60115510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d600bed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d601536a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d60115400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d60020620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d60020d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d400ecc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d400e8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d40117378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d400c7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4007f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4009f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4009f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d40040950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d400402f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d045c9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d045e4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d046021e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d045aa7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d045aaae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.07097739
test_loss: 0.0638694
train_loss: 0.039985467
test_loss: 0.035986207
train_loss: 0.029548056
test_loss: 0.027115885
train_loss: 0.022161003
test_loss: 0.022319311
train_loss: 0.018499527
test_loss: 0.019654844
train_loss: 0.0154037
test_loss: 0.016395396
train_loss: 0.012431165
test_loss: 0.013969866
train_loss: 0.009857744
test_loss: 0.012227635
train_loss: 0.008542834
test_loss: 0.010875473
train_loss: 0.006909631
test_loss: 0.010088496
train_loss: 0.006134299
test_loss: 0.009380192
train_loss: 0.0055858837
test_loss: 0.008695052
train_loss: 0.0052705454
test_loss: 0.008430405
train_loss: 0.0046835844
test_loss: 0.008325997
train_loss: 0.0045166793
test_loss: 0.007904259
train_loss: 0.0041795904
test_loss: 0.008020901
train_loss: 0.0042362344
test_loss: 0.0076622996
train_loss: 0.0037520004
test_loss: 0.007608942
train_loss: 0.0037702464
test_loss: 0.007422171
train_loss: 0.0035666656
test_loss: 0.0074395584
train_loss: 0.0034122097
test_loss: 0.007363747
train_loss: 0.0033447584
test_loss: 0.0072408277
train_loss: 0.0033423777
test_loss: 0.0073978296
train_loss: 0.0034326846
test_loss: 0.007400406
train_loss: 0.0033840854
test_loss: 0.0071911495
train_loss: 0.003127634
test_loss: 0.0071877786
train_loss: 0.0031296462
test_loss: 0.0071557127
train_loss: 0.0030716683
test_loss: 0.0070266305
train_loss: 0.003166059
test_loss: 0.007112184
train_loss: 0.0028648023
test_loss: 0.006998088
train_loss: 0.0028436214
test_loss: 0.0069811577
train_loss: 0.0030746772
test_loss: 0.0070491685
train_loss: 0.002845157
test_loss: 0.0071185445
train_loss: 0.0027584527
test_loss: 0.00706197
train_loss: 0.003075115
test_loss: 0.0070483643
train_loss: 0.0029671402
test_loss: 0.0071231816
train_loss: 0.0035488794
test_loss: 0.0070098704
train_loss: 0.002852338
test_loss: 0.00704123
train_loss: 0.002822752
test_loss: 0.0070507894
train_loss: 0.002971625
test_loss: 0.0071937605
train_loss: 0.002847686
test_loss: 0.0070284763
train_loss: 0.0029094727
test_loss: 0.0069555277
train_loss: 0.0027366495
test_loss: 0.0071484763
train_loss: 0.0030639092
test_loss: 0.0069621042
train_loss: 0.0026540956
test_loss: 0.007019075
train_loss: 0.002701929
test_loss: 0.00694986
train_loss: 0.0027266084
test_loss: 0.007187684
train_loss: 0.0030219234
test_loss: 0.0070988922
train_loss: 0.0028881072
test_loss: 0.007037827
train_loss: 0.0024668262
test_loss: 0.0070303767
train_loss: 0.002882883
test_loss: 0.0070544262
train_loss: 0.0026378962
test_loss: 0.0070354296
train_loss: 0.0026972145
test_loss: 0.0069527454
train_loss: 0.0027792663
test_loss: 0.006991283
train_loss: 0.0029010344
test_loss: 0.007022288
train_loss: 0.0029233582
test_loss: 0.0069839945
train_loss: 0.0027606464
test_loss: 0.0070805
train_loss: 0.0026798213
test_loss: 0.0070172814
train_loss: 0.0026624158
test_loss: 0.0070164413
train_loss: 0.0031487911
test_loss: 0.007246573
train_loss: 0.002925986
test_loss: 0.0071162684
train_loss: 0.0028244108
test_loss: 0.0069937115
train_loss: 0.0027455497
test_loss: 0.00716073
train_loss: 0.002633271
test_loss: 0.006863583
train_loss: 0.0027725773
test_loss: 0.0069973315
train_loss: 0.0024734086
test_loss: 0.006891442
train_loss: 0.0027567644
test_loss: 0.0069498057
train_loss: 0.0027884594
test_loss: 0.0069882106
train_loss: 0.0026185422
test_loss: 0.0069348575
train_loss: 0.0026931306
test_loss: 0.0069999257
train_loss: 0.00287699
test_loss: 0.0069846907
train_loss: 0.0028841752
test_loss: 0.0069526695
train_loss: 0.002675186
test_loss: 0.0071547036
train_loss: 0.0027166002
test_loss: 0.007038053
train_loss: 0.0026979186
test_loss: 0.007064618
train_loss: 0.0024692444
test_loss: 0.0068718567
train_loss: 0.0026013814
test_loss: 0.0068796263
train_loss: 0.0024743604
test_loss: 0.0069352305
train_loss: 0.0025140105
test_loss: 0.0069326037
train_loss: 0.0024952036
test_loss: 0.0069440287
train_loss: 0.0025701923
test_loss: 0.0069453893
train_loss: 0.0025382894
test_loss: 0.0069193216
train_loss: 0.0025047504
test_loss: 0.006941886
train_loss: 0.002519745
test_loss: 0.0068197837
train_loss: 0.002559835
test_loss: 0.0068878504
train_loss: 0.002521495
test_loss: 0.006922652
train_loss: 0.0025125071
test_loss: 0.006904127
train_loss: 0.0025457789
test_loss: 0.006884434
train_loss: 0.0025316356
test_loss: 0.0068539293
train_loss: 0.0022659993
test_loss: 0.006955075
train_loss: 0.002466434
test_loss: 0.006937894
train_loss: 0.002612704
test_loss: 0.0068378663
train_loss: 0.0024841682
test_loss: 0.0069579906
train_loss: 0.002610463
test_loss: 0.0069457013
train_loss: 0.0028243058
test_loss: 0.00688094
train_loss: 0.0025090603
test_loss: 0.0071042874
train_loss: 0.0028368328
test_loss: 0.007076502
train_loss: 0.002759849
test_loss: 0.007104744
train_loss: 0.0025557694
test_loss: 0.0070481594
train_loss: 0.0025624603
test_loss: 0.0070264214
train_loss: 0.0027182507
test_loss: 0.0069171316
train_loss: 0.0024134456
test_loss: 0.006984434
train_loss: 0.0023847586
test_loss: 0.0069996463
train_loss: 0.0025290344
test_loss: 0.006930101
train_loss: 0.002467974
test_loss: 0.007063726
train_loss: 0.0030754944
test_loss: 0.00703365
train_loss: 0.0026120462
test_loss: 0.0070574163
train_loss: 0.0025665897
test_loss: 0.0070468364
train_loss: 0.0025329287
test_loss: 0.006918504
train_loss: 0.0023437848
test_loss: 0.006952818
train_loss: 0.0023536675
test_loss: 0.006911833
train_loss: 0.0026346422
test_loss: 0.0069939527
train_loss: 0.0025611536
test_loss: 0.0070132883
train_loss: 0.0024846476
test_loss: 0.0069045685
train_loss: 0.0023247628
test_loss: 0.0069370507
train_loss: 0.0024048896
test_loss: 0.006991876
train_loss: 0.0025124368
test_loss: 0.0069578188
train_loss: 0.0024437995
test_loss: 0.0069523905
train_loss: 0.0024907014
test_loss: 0.00692534
train_loss: 0.002621248
test_loss: 0.007129929
train_loss: 0.0023322713
test_loss: 0.0068618455
train_loss: 0.0023783534
test_loss: 0.006906773
train_loss: 0.0023325507
test_loss: 0.007038971
train_loss: 0.002408509
test_loss: 0.006942366
train_loss: 0.0023559039
test_loss: 0.0069180327
train_loss: 0.0025349725
test_loss: 0.006935206
train_loss: 0.0024389024
test_loss: 0.007058498
train_loss: 0.0024951326
test_loss: 0.0068444517
train_loss: 0.002508144
test_loss: 0.006992797
train_loss: 0.0022646948
test_loss: 0.0069435793
train_loss: 0.0023529057
test_loss: 0.0069264323
train_loss: 0.0023829355
test_loss: 0.0068351002
train_loss: 0.002248377
test_loss: 0.006948444
train_loss: 0.0022171214
test_loss: 0.006881917
train_loss: 0.002355758
test_loss: 0.0069906064
train_loss: 0.0022634272
test_loss: 0.006868813
train_loss: 0.0021473165
test_loss: 0.0069244304
train_loss: 0.0022360825
test_loss: 0.0068523595
train_loss: 0.0023203637
test_loss: 0.006928142
train_loss: 0.0025053548
test_loss: 0.0069535933
train_loss: 0.0024438994
test_loss: 0.007156965
train_loss: 0.002649374
test_loss: 0.0069438163
train_loss: 0.002624391
test_loss: 0.006916628
train_loss: 0.002397561
test_loss: 0.0069296584
train_loss: 0.0023329405
test_loss: 0.006964224
train_loss: 0.002528784
test_loss: 0.007041642
train_loss: 0.0024797048
test_loss: 0.00687734
train_loss: 0.0021938712
test_loss: 0.007067449
train_loss: 0.0025939527
test_loss: 0.0070798653
train_loss: 0.0028709401
test_loss: 0.0069941836
train_loss: 0.0023864368
test_loss: 0.007061108
train_loss: 0.0025788862
test_loss: 0.0069849715
train_loss: 0.0024660418
test_loss: 0.007023887
train_loss: 0.0024386775
test_loss: 0.007005063
train_loss: 0.0024983769
test_loss: 0.006962356
train_loss: 0.0023560186
test_loss: 0.0070164017
train_loss: 0.0024910886
test_loss: 0.0069022803
train_loss: 0.002514945
test_loss: 0.006938747
train_loss: 0.0022636394
test_loss: 0.0068559647
train_loss: 0.0022070843
test_loss: 0.0068909153
train_loss: 0.0023021856
test_loss: 0.006919822
train_loss: 0.002397682
test_loss: 0.006950569
train_loss: 0.0021440494
test_loss: 0.0068804887
train_loss: 0.0020999813
test_loss: 0.00688712
train_loss: 0.0022389307
test_loss: 0.006942298
train_loss: 0.002317655
test_loss: 0.006879836
train_loss: 0.0022231159
test_loss: 0.006980937
train_loss: 0.0024400917
test_loss: 0.0068615056
train_loss: 0.0021864902
test_loss: 0.0069549177
train_loss: 0.0021551903
test_loss: 0.006927076
train_loss: 0.0021922982
test_loss: 0.006894186
train_loss: 0.002327263
test_loss: 0.006851053
train_loss: 0.0024838543
test_loss: 0.0068836105
train_loss: 0.0022393954
test_loss: 0.0068877153
train_loss: 0.002035746
test_loss: 0.0069035096
train_loss: 0.0021904567
test_loss: 0.0069629713
train_loss: 0.00221663
test_loss: 0.0069282292
train_loss: 0.002157985
test_loss: 0.006920889
train_loss: 0.0022367206
test_loss: 0.006878169
train_loss: 0.0025035187
test_loss: 0.0068903463
train_loss: 0.0021770075
test_loss: 0.0068648644
train_loss: 0.002227181
test_loss: 0.0069419583
train_loss: 0.0020915675
test_loss: 0.006912774
train_loss: 0.0020573405
test_loss: 0.006872169
train_loss: 0.0021419576
test_loss: 0.0068551027
train_loss: 0.0023738614
test_loss: 0.006889786
train_loss: 0.0021174818
test_loss: 0.0069587603
train_loss: 0.002297318
test_loss: 0.006996984
train_loss: 0.0022385358
test_loss: 0.006966361
train_loss: 0.002236455
test_loss: 0.006977552
train_loss: 0.0020421552
test_loss: 0.0069261664
train_loss: 0.0020311289
test_loss: 0.0068719853
train_loss: 0.0022512705
test_loss: 0.0070301127
train_loss: 0.0022072182
test_loss: 0.0069624977
train_loss: 0.002513097
test_loss: 0.0070556365
train_loss: 0.0024021242
test_loss: 0.0070126066
train_loss: 0.0024815467
test_loss: 0.0070179245
train_loss: 0.0023716572
test_loss: 0.007110467
train_loss: 0.0028688451
test_loss: 0.00704304
train_loss: 0.0024073792
test_loss: 0.0070695705
train_loss: 0.0025039697
test_loss: 0.007085859
train_loss: 0.0026845685
test_loss: 0.007103941
train_loss: 0.0026473172
test_loss: 0.007017685
train_loss: 0.0026610203
test_loss: 0.0071895598
train_loss: 0.0024530469
test_loss: 0.007058124
train_loss: 0.0024397671
test_loss: 0.0069649094
train_loss: 0.0024336318
test_loss: 0.0069232965
train_loss: 0.0022738127
test_loss: 0.0068893493
train_loss: 0.0021195777
test_loss: 0.006921524
train_loss: 0.0022495135
test_loss: 0.007065639
train_loss: 0.0022669956
test_loss: 0.006998128
train_loss: 0.0022918244
test_loss: 0.006969277
train_loss: 0.002448313
test_loss: 0.0069970014
train_loss: 0.0025415295
test_loss: 0.0070908424
train_loss: 0.002580572
test_loss: 0.0069461507
train_loss: 0.0022668662
test_loss: 0.006962409
train_loss: 0.00222734
test_loss: 0.00702844
train_loss: 0.0021987972
test_loss: 0.0070491577
train_loss: 0.0023444358
test_loss: 0.0069975327
train_loss: 0.0024734638
test_loss: 0.00708652
train_loss: 0.002327154
test_loss: 0.0069743325
train_loss: 0.0022651698
test_loss: 0.007068315
train_loss: 0.0021630605
test_loss: 0.006931243
train_loss: 0.0022475182
test_loss: 0.0070208334
train_loss: 0.0021770361
test_loss: 0.006947229
train_loss: 0.0021884043
test_loss: 0.0070054624
train_loss: 0.0020321673
test_loss: 0.0069552884
train_loss: 0.0021974705
test_loss: 0.006985878
train_loss: 0.0022113542
test_loss: 0.0069411024
train_loss: 0.0023024415
test_loss: 0.0069069737
train_loss: 0.0020606804
test_loss: 0.0069164583
train_loss: 0.0021659227
test_loss: 0.0068652453
train_loss: 0.002078041
test_loss: 0.0071053454
train_loss: 0.0020149432
test_loss: 0.006966842
train_loss: 0.002149558
test_loss: 0.0069320924
train_loss: 0.0021765744
test_loss: 0.006914472
train_loss: 0.0021117069
test_loss: 0.0069388053
train_loss: 0.0023449003
test_loss: 0.0070990925
train_loss: 0.0026227427
test_loss: 0.0070132455
train_loss: 0.002379747
test_loss: 0.0069726203
train_loss: 0.0021315734
test_loss: 0.007044078
train_loss: 0.00216604
test_loss: 0.0069756885
train_loss: 0.0021325045
test_loss: 0.0070077605
train_loss: 0.002154693
test_loss: 0.0069597336
train_loss: 0.0021513442
test_loss: 0.0069344305
train_loss: 0.0021478378
test_loss: 0.0069277734
train_loss: 0.002056632
test_loss: 0.0069245915
train_loss: 0.0022595848
test_loss: 0.0071889902
train_loss: 0.0021938279
test_loss: 0.0069219517
train_loss: 0.0022657118
test_loss: 0.0069288984
train_loss: 0.0021661583
test_loss: 0.007082767
train_loss: 0.0022673532
test_loss: 0.007050136
train_loss: 0.0022356752
test_loss: 0.006913785
train_loss: 0.0022506027
test_loss: 0.007103259
train_loss: 0.0024028053
test_loss: 0.006966528
train_loss: 0.0022839257
test_loss: 0.0070741703
train_loss: 0.0022518598
test_loss: 0.0069848686
train_loss: 0.0021240495
test_loss: 0.006986752
train_loss: 0.002246015
test_loss: 0.0069582122
train_loss: 0.0022608014
test_loss: 0.0069187223
train_loss: 0.0022899753
test_loss: 0.0071073794
train_loss: 0.0025665448
test_loss: 0.0070718373
train_loss: 0.0025588702
test_loss: 0.007100869
train_loss: 0.002419773
test_loss: 0.00700694
train_loss: 0.0020522599
test_loss: 0.007087725
train_loss: 0.0022953828
test_loss: 0.00697589
train_loss: 0.0022558055
test_loss: 0.0070423344
train_loss: 0.0023107259
test_loss: 0.0069820057
train_loss: 0.0023350925
test_loss: 0.006980642
train_loss: 0.0021107297
test_loss: 0.0069042793
train_loss: 0.0022056543
test_loss: 0.0069508543
train_loss: 0.0019977018
test_loss: 0.0069458783
train_loss: 0.0021423844
test_loss: 0.0070053693
train_loss: 0.002106109
test_loss: 0.006954488
train_loss: 0.0024341678
test_loss: 0.007113248
train_loss: 0.002384117
test_loss: 0.0069815577
train_loss: 0.0021560634
test_loss: 0.0069924784
train_loss: 0.0021706563
test_loss: 0.006999263
train_loss: 0.0021914192
test_loss: 0.0069609215
train_loss: 0.0021698023
test_loss: 0.006950666
train_loss: 0.0021817659
test_loss: 0.0069517773
train_loss: 0.0022297353
test_loss: 0.006976612
train_loss: 0.0022666496
test_loss: 0.0069820783
train_loss: 0.0022855455
test_loss: 0.006992027
train_loss: 0.0024986197
test_loss: 0.0070924335
train_loss: 0.002566556
test_loss: 0.007007875
train_loss: 0.0021092435
test_loss: 0.0070462083
train_loss: 0.0022857282
test_loss: 0.00716382
train_loss: 0.0024033324
test_loss: 0.006979962
train_loss: 0.0022438224
test_loss: 0.007025157
train_loss: 0.002303939
test_loss: 0.0070167533
train_loss: 0.0024143145
test_loss: 0.0069973967
train_loss: 0.00220637
test_loss: 0.0070696753
train_loss: 0.0022252034
test_loss: 0.0069848555
train_loss: 0.0021597352
test_loss: 0.0070762048
train_loss: 0.0022086413
test_loss: 0.0069990545
train_loss: 0.002249962
test_loss: 0.0071405866
train_loss: 0.0022516665
test_loss: 0.0069025606
train_loss: 0.0020300476
test_loss: 0.0070650913
train_loss: 0.0024128524
test_loss: 0.0069952416
train_loss: 0.0023774242
test_loss: 0.007264917
train_loss: 0.002402804
test_loss: 0.0070162658
train_loss: 0.0021354486
test_loss: 0.0070468388
train_loss: 0.0021340568
test_loss: 0.0070296335
train_loss: 0.002167377
test_loss: 0.006964852
train_loss: 0.002292466
test_loss: 0.007043091
train_loss: 0.0021011557
test_loss: 0.0070452625
train_loss: 0.0023187278
test_loss: 0.007045687
train_loss: 0.0022476565
test_loss: 0.0070171203
train_loss: 0.0022623828
test_loss: 0.0070401677
train_loss: 0.0026481557
test_loss: 0.007054534
train_loss: 0.0023661887
test_loss: 0.007242011
train_loss: 0.0022538996
test_loss: 0.0071899104
train_loss: 0.0026154695
test_loss: 0.0070974617
train_loss: 0.0022655507
test_loss: 0.0070975767
train_loss: 0.0023134341
test_loss: 0.0070668976
train_loss: 0.0020186566
test_loss: 0.0070031798
train_loss: 0.0019790803
test_loss: 0.0070077777
train_loss: 0.0020532457
test_loss: 0.006990905
train_loss: 0.0021284325
test_loss: 0.0070625823
train_loss: 0.0020709566
test_loss: 0.007087404
train_loss: 0.0024959869
test_loss: 0.007155823
train_loss: 0.002453537
test_loss: 0.0070594014
train_loss: 0.0022288729
test_loss: 0.0070628296
train_loss: 0.0022456034
test_loss: 0.0070092077
train_loss: 0.0021889189
test_loss: 0.007046831
train_loss: 0.002142759
test_loss: 0.0069614127
train_loss: 0.0023467424
test_loss: 0.0070339157
train_loss: 0.002085987
test_loss: 0.007026684
train_loss: 0.0021630293
test_loss: 0.006980626
train_loss: 0.0022292677
test_loss: 0.007025558
train_loss: 0.0022261215
test_loss: 0.0070368857
train_loss: 0.002232067
test_loss: 0.00707781
train_loss: 0.0022224989
test_loss: 0.0070300465
train_loss: 0.001987355
test_loss: 0.0070222034
train_loss: 0.002334339
test_loss: 0.0069628516
train_loss: 0.002111132
test_loss: 0.007127005
train_loss: 0.0022927052
test_loss: 0.0069867685
train_loss: 0.0021206185
test_loss: 0.0070218
train_loss: 0.0022800802
test_loss: 0.0070174956
train_loss: 0.0021049953
test_loss: 0.0069515915
train_loss: 0.0020479152
test_loss: 0.006981388
train_loss: 0.0021815922
test_loss: 0.007008517
train_loss: 0.002169855
test_loss: 0.006976453
train_loss: 0.0021564025
test_loss: 0.007014176
train_loss: 0.0021209097
test_loss: 0.006997466
train_loss: 0.0020481017
test_loss: 0.0069943503
train_loss: 0.0021763167
test_loss: 0.007056686
train_loss: 0.002098802
test_loss: 0.007136674
train_loss: 0.0023617567
test_loss: 0.007092979
train_loss: 0.0023151606
test_loss: 0.007131471
train_loss: 0.0023301814
test_loss: 0.007025185
train_loss: 0.0022410706
test_loss: 0.007163585
train_loss: 0.0023884047
test_loss: 0.007006409
train_loss: 0.002192106
test_loss: 0.0070487224
train_loss: 0.002615575
test_loss: 0.007031205
train_loss: 0.002293795
test_loss: 0.0070623355
train_loss: 0.0022848563
test_loss: 0.0070984634
train_loss: 0.0024307503
test_loss: 0.007156316
train_loss: 0.0024578709
test_loss: 0.00718103
train_loss: 0.0023089563
test_loss: 0.0071228463
train_loss: 0.0022638994
test_loss: 0.0070164064
train_loss: 0.0022726902
test_loss: 0.007038212
train_loss: 0.002365727
test_loss: 0.006991045
train_loss: 0.0020225546
test_loss: 0.0070883655
train_loss: 0.0020844876
test_loss: 0.0069611426
train_loss: 0.0021721518
test_loss: 0.0070532835
train_loss: 0.0021198532
test_loss: 0.0069613387
train_loss: 0.001997747
test_loss: 0.0070154485
train_loss: 0.0020559605
test_loss: 0.007024748
train_loss: 0.002074819
test_loss: 0.0070178267
train_loss: 0.0019791168
test_loss: 0.007072624
train_loss: 0.0020163357
test_loss: 0.007070886
train_loss: 0.0023029288
test_loss: 0.0071514333
train_loss: 0.002173943
test_loss: 0.0071261134
train_loss: 0.0021349676
test_loss: 0.007110641
train_loss: 0.0021881305
test_loss: 0.0070598824
train_loss: 0.002376901
test_loss: 0.0071047656
train_loss: 0.0024091888
test_loss: 0.0070195035
train_loss: 0.0023396378
test_loss: 0.0070718327
train_loss: 0.0021819107
test_loss: 0.007061603
train_loss: 0.0024581882
test_loss: 0.0072670192
train_loss: 0.0026779927
test_loss: 0.0070301034
train_loss: 0.0022238707
test_loss: 0.0071686655
train_loss: 0.0023741445
test_loss: 0.007128136
train_loss: 0.0022031579
test_loss: 0.007035897
train_loss: 0.00229972
test_loss: 0.00715646
train_loss: 0.0022541066
test_loss: 0.007042566
train_loss: 0.0022747158
test_loss: 0.007030843
train_loss: 0.0022285643
test_loss: 0.007110016
train_loss: 0.002234316
test_loss: 0.0071283393
train_loss: 0.0023287735
test_loss: 0.007118161
train_loss: 0.0021663534
test_loss: 0.007064809
train_loss: 0.0022579366
test_loss: 0.0070209852
train_loss: 0.002009478
test_loss: 0.0070403595
train_loss: 0.002131852
test_loss: 0.007085728
train_loss: 0.0021641757
test_loss: 0.0071125277
train_loss: 0.002239748
test_loss: 0.0071529774
train_loss: 0.002377955
test_loss: 0.0071130223
train_loss: 0.002427359
test_loss: 0.007056744
train_loss: 0.002294072
test_loss: 0.0071040867
train_loss: 0.0023456207
test_loss: 0.0072268667
train_loss: 0.0025310297
test_loss: 0.0070945574
train_loss: 0.0021952079
test_loss: 0.007157105
train_loss: 0.002569942
test_loss: 0.007120459
train_loss: 0.002062242
test_loss: 0.007094686
train_loss: 0.0024003317
test_loss: 0.007126422
train_loss: 0.0022914826
test_loss: 0.007181572
train_loss: 0.0025035813
test_loss: 0.0070763677
train_loss: 0.002427861
test_loss: 0.007093065
train_loss: 0.0023105445
test_loss: 0.0071555777
train_loss: 0.0021815242
test_loss: 0.0071199797
train_loss: 0.0023463618
test_loss: 0.0071562934
train_loss: 0.0022274633
test_loss: 0.0070347586
train_loss: 0.0021572984
test_loss: 0.0070383344
train_loss: 0.0021562655
test_loss: 0.007118831
train_loss: 0.0020653084
test_loss: 0.0070515755
train_loss: 0.002040418
test_loss: 0.0070851436
train_loss: 0.0020312336
test_loss: 0.007006866
train_loss: 0.002189079
test_loss: 0.0072717164
train_loss: 0.002218712
test_loss: 0.0070736916
train_loss: 0.002079441
test_loss: 0.007077016
train_loss: 0.0021691592
test_loss: 0.0070511433
train_loss: 0.002040335
test_loss: 0.00714087
train_loss: 0.002202838
test_loss: 0.0071030925
train_loss: 0.002215061
test_loss: 0.0071125003
train_loss: 0.0024438677
test_loss: 0.0070810975
train_loss: 0.0020935682
test_loss: 0.007128016
train_loss: 0.0021426433
test_loss: 0.00713357
train_loss: 0.002143394
test_loss: 0.007078228
train_loss: 0.0021944891
test_loss: 0.007106218
train_loss: 0.0021990358
test_loss: 0.007106222
train_loss: 0.002030814
test_loss: 0.007095994
train_loss: 0.0022788798
test_loss: 0.0070666326
train_loss: 0.0023631344
test_loss: 0.0072039436
train_loss: 0.0022471515
test_loss: 0.0072538317
train_loss: 0.00222722
test_loss: 0.007144003
train_loss: 0.002156315
test_loss: 0.007066193
train_loss: 0.0020549917
test_loss: 0.0071461787
train_loss: 0.0021485935
test_loss: 0.007164548
train_loss: 0.002220726
test_loss: 0.0070299283
train_loss: 0.0019172505
test_loss: 0.0071511
train_loss: 0.002003085
test_loss: 0.0070604743
train_loss: 0.0024076086
test_loss: 0.0071313945
train_loss: 0.0022908496
test_loss: 0.0070833466
train_loss: 0.0021604665
test_loss: 0.0071328264
train_loss: 0.002098162
test_loss: 0.0071864664
train_loss: 0.002160656
test_loss: 0.0071194586
train_loss: 0.0022552516
test_loss: 0.007017862
train_loss: 0.0021697788
test_loss: 0.0070510563
train_loss: 0.0022066375
test_loss: 0.0071020173
train_loss: 0.0020134328
test_loss: 0.007095216
train_loss: 0.0021318556
test_loss: 0.0070248954
train_loss: 0.0021180245
test_loss: 0.0070725707
train_loss: 0.0021916872
test_loss: 0.007101032
train_loss: 0.0020500647
test_loss: 0.007139278
train_loss: 0.0020996444
test_loss: 0.007070034
train_loss: 0.0020646765
test_loss: 0.0070805163
train_loss: 0.0023230987
test_loss: 0.0071220025
train_loss: 0.0021524997
test_loss: 0.0072564436
train_loss: 0.0022659902
test_loss: 0.0071269097
train_loss: 0.0023533101
test_loss: 0.0072270813
train_loss: 0.00224991
test_loss: 0.007106591
train_loss: 0.0023713468
test_loss: 0.0071340324
train_loss: 0.0022559292
test_loss: 0.007215035
train_loss: 0.002465645
test_loss: 0.0071554123
train_loss: 0.002004004
test_loss: 0.0070930566
train_loss: 0.002042666
test_loss: 0.0070951995
train_loss: 0.002317848
test_loss: 0.0071400935
train_loss: 0.0021868101
test_loss: 0.0070414143
train_loss: 0.0021481568
test_loss: 0.007168343
train_loss: 0.0022431542
test_loss: 0.0070969076
train_loss: 0.0021074782
test_loss: 0.0070809294
train_loss: 0.0022076261
test_loss: 0.0071187005
train_loss: 0.0022651036
test_loss: 0.0070884814
train_loss: 0.0022622885
test_loss: 0.007195194
train_loss: 0.002245622
test_loss: 0.0071317162
train_loss: 0.0020698574
test_loss: 0.0070856195
train_loss: 0.0021767835
test_loss: 0.007059647
train_loss: 0.0020734377
test_loss: 0.0071653128
train_loss: 0.0020696514
test_loss: 0.007085478
train_loss: 0.0021463365
test_loss: 0.007152467
train_loss: 0.0021257089
test_loss: 0.007117084
train_loss: 0.002488991
test_loss: 0.0071338094
train_loss: 0.0022402501
test_loss: 0.007102148
train_loss: 0.0021911561
test_loss: 0.0070841843
train_loss: 0.0024073136
test_loss: 0.0072154254
train_loss: 0.0023101626
test_loss: 0.0071556577
train_loss: 0.0021590835
test_loss: 0.007115491
train_loss: 0.0020688483
test_loss: 0.0070798174
train_loss: 0.0023345419
test_loss: 0.007195052
train_loss: 0.0021253082
test_loss: 0.0070854267
train_loss: 0.0020485925
test_loss: 0.007024361
train_loss: 0.0019920017
test_loss: 0.007017745
train_loss: 0.0020396966
test_loss: 0.007111664
train_loss: 0.0019745878
test_loss: 0.00704438
train_loss: 0.0020618157
test_loss: 0.007103126
train_loss: 0.0020111438
test_loss: 0.0070724785
train_loss: 0.0021807356
test_loss: 0.007169713
train_loss: 0.0020097946
test_loss: 0.0071006375
train_loss: 0.0020870424
test_loss: 0.0072192727
train_loss: 0.0021484722
test_loss: 0.007194187
train_loss: 0.0023203334
test_loss: 0.007125962
train_loss: 0.0020086542
test_loss: 0.007103755
train_loss: 0.002177158
test_loss: 0.0071489597
train_loss: 0.002100063
test_loss: 0.007069945
train_loss: 0.0021636551
test_loss: 0.007021174
train_loss: 0.0021211826
test_loss: 0.0070466506
train_loss: 0.0020295335
test_loss: 0.0070983632
train_loss: 0.0020529532
test_loss: 0.0072741834
train_loss: 0.002748833
test_loss: 0.0071506025
train_loss: 0.0021339166
test_loss: 0.0071054935
train_loss: 0.0020827875
test_loss: 0.0072699976
train_loss: 0.0022060024
test_loss: 0.007140597
train_loss: 0.002164069
test_loss: 0.0071105007
train_loss: 0.0022405456
test_loss: 0.0071582943
train_loss: 0.002140892
test_loss: 0.0071349256
train_loss: 0.0020495842
test_loss: 0.0073504047
train_loss: 0.002585812
test_loss: 0.0072220764
train_loss: 0.0021363306
test_loss: 0.007250624
train_loss: 0.002864194
test_loss: 0.0071358625
train_loss: 0.0022169272
test_loss: 0.00742118
train_loss: 0.0024621154
test_loss: 0.007102978
train_loss: 0.0021464305
test_loss: 0.00716401
train_loss: 0.0022643309
test_loss: 0.007155407
train_loss: 0.002063285
test_loss: 0.0071156197
train_loss: 0.0020815476
test_loss: 0.0071167885
train_loss: 0.002258685
test_loss: 0.0070902086
train_loss: 0.002066148
test_loss: 0.0071736006
train_loss: 0.0021498646
test_loss: 0.007130281
train_loss: 0.0020297808
test_loss: 0.00713113
train_loss: 0.0020527502
test_loss: 0.0071557835
train_loss: 0.002054829
test_loss: 0.0071877823
train_loss: 0.0023393682
test_loss: 0.007107179
train_loss: 0.0023491243
test_loss: 0.007301439
train_loss: 0.002349443
test_loss: 0.0071309013
train_loss: 0.0021437856
test_loss: 0.0071623675
train_loss: 0.0022543576
test_loss: 0.0071417065
train_loss: 0.0021229503
test_loss: 0.0070850975
train_loss: 0.002191502
test_loss: 0.0072571766
train_loss: 0.0023000317
test_loss: 0.0071466947
train_loss: 0.0021768305
test_loss: 0.0071582654
train_loss: 0.0023782293
test_loss: 0.0071539893
train_loss: 0.0021884423
test_loss: 0.0071751517
train_loss: 0.0020659354
test_loss: 0.007126456
train_loss: 0.0020092742
test_loss: 0.0071336883
train_loss: 0.0019754064
test_loss: 0.00715654
train_loss: 0.0019890582
test_loss: 0.0071549853
train_loss: 0.0020182848
test_loss: 0.007127967
train_loss: 0.002054746
test_loss: 0.0070493454
train_loss: 0.0021336395
test_loss: 0.007162109
train_loss: 0.002260338
test_loss: 0.0071277716
train_loss: 0.0020622453
test_loss: 0.007090243
train_loss: 0.002033424
test_loss: 0.0071350723
train_loss: 0.0023066741
test_loss: 0.0071235923
train_loss: 0.0021641138
test_loss: 0.0072241495
train_loss: 0.002129859
test_loss: 0.0071507185
train_loss: 0.0019334835
test_loss: 0.007093405
train_loss: 0.002024416
test_loss: 0.0071861558
train_loss: 0.0019994352
test_loss: 0.007066057
train_loss: 0.002094823
test_loss: 0.0071070623
train_loss: 0.0020307174
test_loss: 0.007111639
train_loss: 0.0020705573
test_loss: 0.007122997
train_loss: 0.0020714542
test_loss: 0.007276909
train_loss: 0.0020662972
test_loss: 0.0071712835
train_loss: 0.001978565
test_loss: 0.007149361
train_loss: 0.002291465
test_loss: 0.007211582
train_loss: 0.0021550131
test_loss: 0.007158186
train_loss: 0.0021011343
test_loss: 0.007092848
train_loss: 0.0022048252
test_loss: 0.0071339966
train_loss: 0.002056391
test_loss: 0.0071121803
train_loss: 0.0019214342
test_loss: 0.007123454
train_loss: 0.0020460372
test_loss: 0.007151517
train_loss: 0.0019781217
test_loss: 0.0071281428
train_loss: 0.0020344814
test_loss: 0.0070925127
train_loss: 0.0019743699
test_loss: 0.007198343
train_loss: 0.002329349
test_loss: 0.0072807083
train_loss: 0.0021583722
test_loss: 0.007135912
train_loss: 0.0022078753
test_loss: 0.007176369
train_loss: 0.0021457844
test_loss: 0.007208061
train_loss: 0.002031535
test_loss: 0.0072177793
train_loss: 0.0020476303
test_loss: 0.0071570333
train_loss: 0.002157684
test_loss: 0.0071316245
train_loss: 0.0020300704
test_loss: 0.0071610473
train_loss: 0.0020699066
test_loss: 0.0072181993
train_loss: 0.0019462601
test_loss: 0.0072944625
train_loss: 0.0020657077
test_loss: 0.0071743713
train_loss: 0.0022193592
test_loss: 0.007123273
train_loss: 0.0022422138
test_loss: 0.0072593093
train_loss: 0.002096984
test_loss: 0.007164708
train_loss: 0.0019450439
test_loss: 0.007116957
train_loss: 0.0019844864
test_loss: 0.0071151974
train_loss: 0.0019286112
test_loss: 0.007192915
train_loss: 0.002002083
test_loss: 0.0072627757
train_loss: 0.0021990621
test_loss: 0.007195606
train_loss: 0.0020794058
test_loss: 0.007232042
train_loss: 0.0021166885
test_loss: 0.0071423575
train_loss: 0.0020672013
test_loss: 0.007094891
train_loss: 0.0020534839
test_loss: 0.007126053
train_loss: 0.0021323226
test_loss: 0.0070757195
train_loss: 0.0019441966
test_loss: 0.0071222545
train_loss: 0.0019507909
test_loss: 0.0071377675
train_loss: 0.0021230057
test_loss: 0.007180264
train_loss: 0.0018944325
test_loss: 0.0071224472
train_loss: 0.002048591
test_loss: 0.0071602133
train_loss: 0.0022338328
test_loss: 0.0072021587
train_loss: 0.002263866
test_loss: 0.007221297
train_loss: 0.0022408427
test_loss: 0.0071914517
train_loss: 0.0023654501
test_loss: 0.007253792
train_loss: 0.0021915003
test_loss: 0.0073068007
train_loss: 0.00229103
test_loss: 0.007282483
train_loss: 0.0021472222
test_loss: 0.007215863
train_loss: 0.0022087058
test_loss: 0.0071282997
train_loss: 0.0024535384
test_loss: 0.007219291
train_loss: 0.0021266674
test_loss: 0.0072397157
train_loss: 0.0021997557
test_loss: 0.0071359267
train_loss: 0.0022180583
test_loss: 0.0072395266
train_loss: 0.002088374
test_loss: 0.0071816146
train_loss: 0.0021033785
test_loss: 0.007187219
train_loss: 0.0020010425
test_loss: 0.0072505292
train_loss: 0.002056071
test_loss: 0.007130905
train_loss: 0.0022744092
test_loss: 0.007303606
train_loss: 0.0022784888
test_loss: 0.0071743517
train_loss: 0.0020261237
test_loss: 0.0072050234
train_loss: 0.0022020387
test_loss: 0.0073060356
train_loss: 0.0023951891
test_loss: 0.007209458
train_loss: 0.0022834162
test_loss: 0.007237536
train_loss: 0.0021354873
test_loss: 0.00725907
train_loss: 0.0022659504
test_loss: 0.0072206
train_loss: 0.0022140744
test_loss: 0.0072885817
train_loss: 0.002030501
test_loss: 0.0071584177
train_loss: 0.0020830594
test_loss: 0.007350527
train_loss: 0.0022961227
test_loss: 0.007300625
train_loss: 0.0020403862
test_loss: 0.007159477
train_loss: 0.0019201974
test_loss: 0.007192466
train_loss: 0.0018900774
test_loss: 0.007195909
train_loss: 0.002004737
test_loss: 0.0072893333
train_loss: 0.0018389614
test_loss: 0.007180102
train_loss: 0.0020196938
test_loss: 0.007169998
train_loss: 0.0018870435
test_loss: 0.007217893
train_loss: 0.0018795548
test_loss: 0.0071696388
train_loss: 0.0020712723
test_loss: 0.00714033
train_loss: 0.0021436235
test_loss: 0.00734341
train_loss: 0.002131773
test_loss: 0.007197139
train_loss: 0.0020063538
test_loss: 0.0073540704
train_loss: 0.0022811259
test_loss: 0.00716398
train_loss: 0.0022705751
test_loss: 0.007232499
train_loss: 0.0020834054
test_loss: 0.007332749
train_loss: 0.0021743714
test_loss: 0.0071958546
train_loss: 0.0020791802
test_loss: 0.0072033196
train_loss: 0.0021064607
test_loss: 0.007123715
train_loss: 0.0020171765
test_loss: 0.0072474177
train_loss: 0.001944858
test_loss: 0.0072272117
train_loss: 0.002040762
test_loss: 0.0072999354
train_loss: 0.0021186215
test_loss: 0.007245623
train_loss: 0.0020749073
test_loss: 0.0071856612
train_loss: 0.0020928935
test_loss: 0.007223889
train_loss: 0.0020672246
test_loss: 0.007224252
train_loss: 0.0020460624
test_loss: 0.007227407
train_loss: 0.0020636315
test_loss: 0.007171335
train_loss: 0.0019721752
test_loss: 0.007202269
train_loss: 0.0020263593
test_loss: 0.0072097816
train_loss: 0.0022469335
test_loss: 0.0071966113
train_loss: 0.0023208265
test_loss: 0.00723301
train_loss: 0.002090237
test_loss: 0.0071980525
train_loss: 0.002131528
test_loss: 0.0071891616
train_loss: 0.0020870615
test_loss: 0.0072167916
train_loss: 0.0020562354
test_loss: 0.007169661
train_loss: 0.002075818
test_loss: 0.00729097
train_loss: 0.0024173965
test_loss: 0.007173652
train_loss: 0.0021291976
test_loss: 0.007165571
train_loss: 0.0020732996
test_loss: 0.0072429203
train_loss: 0.002103462
test_loss: 0.0072679133
train_loss: 0.0020643603
test_loss: 0.007243958
train_loss: 0.0020669748
test_loss: 0.0072038965
train_loss: 0.0019871527
test_loss: 0.0072510443
train_loss: 0.0019654736
test_loss: 0.0072623338
train_loss: 0.002085813
test_loss: 0.0072764647
train_loss: 0.002199193
test_loss: 0.007243511
train_loss: 0.0022222889
test_loss: 0.0073113875
train_loss: 0.0024277773
test_loss: 0.0072380775
train_loss: 0.0022481212
test_loss: 0.0072349743
train_loss: 0.0023199087
test_loss: 0.0074260584
train_loss: 0.0021076922
test_loss: 0.007182194
train_loss: 0.0020818217
test_loss: 0.0072034793
train_loss: 0.0020678402
test_loss: 0.0072371443
train_loss: 0.0019265178
test_loss: 0.007145255
train_loss: 0.0020442102
test_loss: 0.007248279
train_loss: 0.0019271385
test_loss: 0.0072697457
train_loss: 0.0021411788
test_loss: 0.007151704
train_loss: 0.002097379
test_loss: 0.007183798
train_loss: 0.0019730953
test_loss: 0.007084021
train_loss: 0.0019531061
test_loss: 0.0072983694
train_loss: 0.0021545966
test_loss: 0.0072833947
train_loss: 0.0023901423
test_loss: 0.007301383
train_loss: 0.002321354
test_loss: 0.007302594
train_loss: 0.0023333554
test_loss: 0.007355014
train_loss: 0.002175313
test_loss: 0.0071738157
train_loss: 0.0020797895
test_loss: 0.0072085448
train_loss: 0.0021111052
test_loss: 0.007231927
train_loss: 0.002255057
test_loss: 0.00717529
train_loss: 0.0020797648
test_loss: 0.0072400765
train_loss: 0.0021989318
test_loss: 0.0072868406
train_loss: 0.0021684375
test_loss: 0.007199383
train_loss: 0.0020105308
test_loss: 0.0071971174
train_loss: 0.0019650843
test_loss: 0.0072659007
train_loss: 0.0019637286
test_loss: 0.007184926
train_loss: 0.0020026308
test_loss: 0.0073280223
train_loss: 0.0022977744
test_loss: 0.0072372165
train_loss: 0.0021415453
test_loss: 0.0073028146
train_loss: 0.0021930854
test_loss: 0.0073189605
train_loss: 0.0022859466
test_loss: 0.0073786313
train_loss: 0.002302195
test_loss: 0.0072239838
train_loss: 0.0022934913
test_loss: 0.0072646295
train_loss: 0.0020643317
test_loss: 0.007175098
train_loss: 0.0020855044
test_loss: 0.0072984947
train_loss: 0.0020399478
test_loss: 0.007372244
train_loss: 0.002216906
test_loss: 0.0072838073
train_loss: 0.0024737609
test_loss: 0.0073342966
train_loss: 0.0023725966
test_loss: 0.0072794273
train_loss: 0.0021009943
test_loss: 0.007279722
train_loss: 0.0020468363
test_loss: 0.0073763337
train_loss: 0.0023117424
test_loss: 0.007283403
train_loss: 0.0023292033
test_loss: 0.007338315
train_loss: 0.0021697537
test_loss: 0.0073060202
train_loss: 0.0020763455
test_loss: 0.007282706
train_loss: 0.0021611948
test_loss: 0.007316235
train_loss: 0.0020991594
test_loss: 0.0071920436
train_loss: 0.0022425475
test_loss: 0.0071859527
train_loss: 0.0021139923
test_loss: 0.0072729806
train_loss: 0.0021484555
test_loss: 0.007257574
train_loss: 0.002154923
test_loss: 0.0072204852
train_loss: 0.0021414743
test_loss: 0.0071814735
train_loss: 0.0020118055
test_loss: 0.0072409655
train_loss: 0.0020433231
test_loss: 0.0072451364
train_loss: 0.0021573443
test_loss: 0.0071810544
train_loss: 0.0019955686
test_loss: 0.0071789804
train_loss: 0.002059637
test_loss: 0.0073467987
train_loss: 0.002399617
test_loss: 0.0073231184
train_loss: 0.0022778725
test_loss: 0.007315702
train_loss: 0.002352242
test_loss: 0.0074181706
train_loss: 0.0022503226
test_loss: 0.007293185
train_loss: 0.0020077692
test_loss: 0.0074055945
train_loss: 0.0022232267
test_loss: 0.0074073183
train_loss: 0.002533242
test_loss: 0.007332147
train_loss: 0.00206119
test_loss: 0.0073611876
train_loss: 0.0024594879
test_loss: 0.0074256086
train_loss: 0.0023165366
test_loss: 0.007391296
train_loss: 0.0021001762
test_loss: 0.0072974223
train_loss: 0.0019551346
test_loss: 0.0073131565
train_loss: 0.0018914882
test_loss: 0.0072445124
train_loss: 0.0022612242
test_loss: 0.007325272
train_loss: 0.0020692344
test_loss: 0.0072873165
train_loss: 0.0020444049
test_loss: 0.007310597
train_loss: 0.002090197
test_loss: 0.0073770555
train_loss: 0.0020213358
test_loss: 0.0072720014
train_loss: 0.0018582416
test_loss: 0.0073258146
train_loss: 0.0020953543
test_loss: 0.0072361026
train_loss: 0.0022723917
test_loss: 0.007307575
train_loss: 0.002039056
test_loss: 0.0072908867
train_loss: 0.0021037126
test_loss: 0.0072914143
train_loss: 0.0021605361
test_loss: 0.0072993855
train_loss: 0.0019310362
test_loss: 0.00726344
train_loss: 0.00227184
test_loss: 0.0073691662
train_loss: 0.002083191
test_loss: 0.007394107
train_loss: 0.002068262
test_loss: 0.007236541
train_loss: 0.0020739338
test_loss: 0.0072904415
train_loss: 0.0021530315
test_loss: 0.0073303776
train_loss: 0.0021400175
test_loss: 0.007310277
train_loss: 0.0019542547
test_loss: 0.0072112815
train_loss: 0.001948657
test_loss: 0.0072778394
train_loss: 0.0020734482
test_loss: 0.0072400556
train_loss: 0.0020881665
test_loss: 0.007301842
train_loss: 0.0022187638
test_loss: 0.007342714
train_loss: 0.002255491
test_loss: 0.0073186485
train_loss: 0.0021474771
test_loss: 0.0072675617
train_loss: 0.0020886557
test_loss: 0.0072792047
train_loss: 0.0020938488
test_loss: 0.0073626908
train_loss: 0.0019080424
test_loss: 0.0073582614
train_loss: 0.0020091946
test_loss: 0.0073512774
train_loss: 0.0022382003
test_loss: 0.007288736
train_loss: 0.0019185515
test_loss: 0.0073120245
train_loss: 0.0021157428
test_loss: 0.007361838
train_loss: 0.002293616
test_loss: 0.0073098694
train_loss: 0.002012308
test_loss: 0.007251093
train_loss: 0.002085214
test_loss: 0.007251145
train_loss: 0.0019786018
test_loss: 0.0072263265
train_loss: 0.001972284
test_loss: 0.007355299
train_loss: 0.0020567041
test_loss: 0.0072189774
train_loss: 0.0019440391
test_loss: 0.007380242
train_loss: 0.0021475032
test_loss: 0.007393599
train_loss: 0.0023673554
test_loss: 0.0073785563
train_loss: 0.0021688184
test_loss: 0.0073129497
train_loss: 0.0020547176
test_loss: 0.0073331934
train_loss: 0.0021285815
test_loss: 0.0073537403
train_loss: 0.0022279725
test_loss: 0.0072859395
train_loss: 0.0020467062
test_loss: 0.007218183
train_loss: 0.0018515519
test_loss: 0.0072745383
train_loss: 0.001812542
test_loss: 0.007262328
train_loss: 0.0020268953
test_loss: 0.007416493
train_loss: 0.0019882447
test_loss: 0.0072332597
train_loss: 0.0020011442
test_loss: 0.0072944346
train_loss: 0.0021350663
test_loss: 0.007336593
train_loss: 0.0020387028
test_loss: 0.007368789
train_loss: 0.0022398934
test_loss: 0.007253335
train_loss: 0.0021822895
test_loss: 0.007309024
train_loss: 0.0024015396
test_loss: 0.007292617
train_loss: 0.0020694325
test_loss: 0.0073607787
train_loss: 0.0022067707
test_loss: 0.007308867
train_loss: 0.00217707
test_loss: 0.0073785232
train_loss: 0.0021408973
test_loss: 0.0074005337
train_loss: 0.0021762655
test_loss: 0.007358486
train_loss: 0.0021106675
test_loss: 0.007372463
train_loss: 0.0020944062
test_loss: 0.007287546
train_loss: 0.0022270626
test_loss: 0.007386523
train_loss: 0.0021237626
test_loss: 0.0073119947
train_loss: 0.0022810893
test_loss: 0.0073340405
train_loss: 0.002348029
test_loss: 0.007446329
train_loss: 0.002212964
test_loss: 0.007427743
train_loss: 0.0021551577
test_loss: 0.0072522494
train_loss: 0.0019156169
test_loss: 0.0073088612
train_loss: 0.0019335061
test_loss: 0.0073435535
train_loss: 0.0018867663
test_loss: 0.0073320996
train_loss: 0.0018785099
test_loss: 0.0073263366
train_loss: 0.0020552713
test_loss: 0.0073105246
train_loss: 0.0020560068
test_loss: 0.0073378514
train_loss: 0.0020340364
test_loss: 0.007265102
train_loss: 0.0019210279
test_loss: 0.007334734
train_loss: 0.002104356
test_loss: 0.007274579
train_loss: 0.0019172458
test_loss: 0.007370599
train_loss: 0.0018326166
test_loss: 0.007257875
train_loss: 0.0020344127
test_loss: 0.0072257835
train_loss: 0.0020221244
test_loss: 0.0072648088
train_loss: 0.0018472719
test_loss: 0.007305722
train_loss: 0.0019363819
test_loss: 0.007304826
train_loss: 0.0019983647
test_loss: 0.007372187
train_loss: 0.0021641804
test_loss: 0.007303578
train_loss: 0.0020032017
test_loss: 0.007312045
train_loss: 0.0020048954
test_loss: 0.0072896224
train_loss: 0.0020602678
test_loss: 0.007295649
train_loss: 0.0020519018
test_loss: 0.007350967
train_loss: 0.0019111335
test_loss: 0.0072941547
train_loss: 0.0021323091
test_loss: 0.007355981
train_loss: 0.0021978316
test_loss: 0.0073473756
train_loss: 0.0020716097
test_loss: 0.007333084
train_loss: 0.0021416403
test_loss: 0.007273342
train_loss: 0.002127202
test_loss: 0.007425006
train_loss: 0.0023500212
test_loss: 0.0073532695
train_loss: 0.0022079712
test_loss: 0.0073014237
train_loss: 0.0020799409/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.007379748
train_loss: 0.0021535002
test_loss: 0.0073075267
train_loss: 0.0024381161
test_loss: 0.0073574893
train_loss: 0.0023129985
test_loss: 0.0075346325
train_loss: 0.002548451
test_loss: 0.0073453365
train_loss: 0.0021594523
test_loss: 0.0073742433
train_loss: 0.0024037687
test_loss: 0.0073980493
train_loss: 0.001990924
test_loss: 0.0073678936
train_loss: 0.002297698
test_loss: 0.0073761353
train_loss: 0.0020170545
test_loss: 0.007328843
train_loss: 0.002001501
test_loss: 0.007343396
train_loss: 0.002040502
test_loss: 0.0074301027
train_loss: 0.0021147488
test_loss: 0.0073413416
train_loss: 0.0020251675
test_loss: 0.0073623527
train_loss: 0.002078866
test_loss: 0.0072938907
train_loss: 0.0019630715
test_loss: 0.007321925
train_loss: 0.0020592192
test_loss: 0.0073832464
train_loss: 0.0022187529
test_loss: 0.007612845
train_loss: 0.0023306806
test_loss: 0.0073589003
train_loss: 0.0019658944
test_loss: 0.007367476
train_loss: 0.0022415228
test_loss: 0.007426299
train_loss: 0.0020000068
test_loss: 0.007370834
train_loss: 0.002305298
test_loss: 0.0073561547
train_loss: 0.0019130397
test_loss: 0.007326827
train_loss: 0.0021580236
test_loss: 0.0073461784
train_loss: 0.0019305742
test_loss: 0.0073719393
train_loss: 0.0022396187
test_loss: 0.007446743
train_loss: 0.0019891497
test_loss: 0.0073726736
train_loss: 0.002255144
test_loss: 0.007394865
train_loss: 0.0021463803
test_loss: 0.007350971
train_loss: 0.0020212168
test_loss: 0.0074383123
train_loss: 0.002020191
test_loss: 0.0072929794
train_loss: 0.0020709983
test_loss: 0.007377298
train_loss: 0.002210826
test_loss: 0.007335317
train_loss: 0.002009348
test_loss: 0.0073112613
train_loss: 0.0019109959
test_loss: 0.0073421285
train_loss: 0.0020088935
test_loss: 0.007369483
train_loss: 0.0019936305
test_loss: 0.007338004
train_loss: 0.0021122405
test_loss: 0.007350973
train_loss: 0.002261795
test_loss: 0.0073445863
train_loss: 0.0019951675
test_loss: 0.007297613
train_loss: 0.0020112852
test_loss: 0.0073485575
train_loss: 0.0020521707
test_loss: 0.007356511
train_loss: 0.0021178336
test_loss: 0.0073250257
train_loss: 0.0019900298
test_loss: 0.0074366527
train_loss: 0.0022021916
test_loss: 0.0074294256
train_loss: 0.002185694
test_loss: 0.007456902
train_loss: 0.0022507003
test_loss: 0.007298591
train_loss: 0.0020804945
test_loss: 0.007363654
train_loss: 0.0022413111
test_loss: 0.0074162795
train_loss: 0.0020375135
test_loss: 0.0074762767
train_loss: 0.0022588589
test_loss: 0.007322063
train_loss: 0.002015635
test_loss: 0.0074197263
train_loss: 0.002049741
test_loss: 0.00740335
train_loss: 0.0020775767
test_loss: 0.007452332
train_loss: 0.002098708
test_loss: 0.0073085525
train_loss: 0.0019205357
test_loss: 0.007369543
train_loss: 0.001994126
test_loss: 0.007342657
train_loss: 0.0019503445
test_loss: 0.0074322517
train_loss: 0.0021553892
test_loss: 0.0073701004
train_loss: 0.0020150123
test_loss: 0.007408373
train_loss: 0.0022371605
test_loss: 0.007428638
train_loss: 0.0022875066
test_loss: 0.0073740194
train_loss: 0.002002549
test_loss: 0.0073662223
train_loss: 0.0020363063
test_loss: 0.0074484614
train_loss: 0.002078856
test_loss: 0.0073129316
train_loss: 0.0020503514
test_loss: 0.007344617
train_loss: 0.0020471446
test_loss: 0.0073486133
train_loss: 0.002065186
test_loss: 0.0073847175
train_loss: 0.0020953058
test_loss: 0.007451241
train_loss: 0.0021351548
test_loss: 0.007390776
train_loss: 0.0020768966
test_loss: 0.0073977076
train_loss: 0.0018633888
test_loss: 0.0073595233
train_loss: 0.0019024262
test_loss: 0.007370944
train_loss: 0.001914185
test_loss: 0.007377092
train_loss: 0.001964232
test_loss: 0.0073584435
train_loss: 0.0019171633
test_loss: 0.0074253567
train_loss: 0.0017967843
test_loss: 0.0073192026
train_loss: 0.002092924
test_loss: 0.007352342
train_loss: 0.0020640078
test_loss: 0.0073705134
train_loss: 0.0020055415
test_loss: 0.0074361465
train_loss: 0.0019812381
test_loss: 0.0073188297
train_loss: 0.0020540669
test_loss: 0.0074330275
train_loss: 0.0019401248
test_loss: 0.0074305823
train_loss: 0.0019866105
test_loss: 0.007415443
train_loss: 0.0021854802
test_loss: 0.007366532
train_loss: 0.0020564834
test_loss: 0.007448345
train_loss: 0.0019992588
test_loss: 0.0074663744
train_loss: 0.0020008832
test_loss: 0.007420684
train_loss: 0.0020105408
test_loss: 0.0073967725
train_loss: 0.0018843352
test_loss: 0.0073516355
train_loss: 0.0020629354
test_loss: 0.007406553
train_loss: 0.001936429
test_loss: 0.0074221143
train_loss: 0.0020081827
test_loss: 0.0073636253
train_loss: 0.0021436939
test_loss: 0.0074018887
train_loss: 0.0020371217
test_loss: 0.007435094
train_loss: 0.0021902327
test_loss: 0.0073924186
train_loss: 0.0021878271
test_loss: 0.007312335
train_loss: 0.0019891383
test_loss: 0.0073360484
train_loss: 0.001965382
test_loss: 0.007420132
train_loss: 0.0018912419
test_loss: 0.007439231
train_loss: 0.0019315326
test_loss: 0.007381237
train_loss: 0.002084105
test_loss: 0.007402152
train_loss: 0.002137932
test_loss: 0.0075156917
train_loss: 0.0022220798
test_loss: 0.0074112383
train_loss: 0.0021886046
test_loss: 0.0074730422
train_loss: 0.0022949583
test_loss: 0.007384688
train_loss: 0.00205195
test_loss: 0.0074488693
train_loss: 0.0020131243
test_loss: 0.0074662487
train_loss: 0.0020442915
test_loss: 0.0074815666
train_loss: 0.0021161037
test_loss: 0.0074098487
train_loss: 0.002346251
test_loss: 0.0074143885
train_loss: 0.002066626
test_loss: 0.0074779373
train_loss: 0.0020189208
test_loss: 0.0074229804
train_loss: 0.0021635415
test_loss: 0.0074796467
train_loss: 0.0019692623
test_loss: 0.007481972
train_loss: 0.0021009734
test_loss: 0.0074504847
train_loss: 0.0019257644
test_loss: 0.007337814
train_loss: 0.0019167309
test_loss: 0.007336386
train_loss: 0.0020823823
test_loss: 0.0074133105
train_loss: 0.002065238
test_loss: 0.007406629
train_loss: 0.0019943668
test_loss: 0.0073893038
train_loss: 0.0019937558
test_loss: 0.0073450548
train_loss: 0.0019061673
test_loss: 0.0073502106
train_loss: 0.0018647339
test_loss: 0.0073885
train_loss: 0.001975649
test_loss: 0.0073055397
train_loss: 0.00203832
test_loss: 0.0074259504
train_loss: 0.0019740928
test_loss: 0.007425257
train_loss: 0.0020349785
test_loss: 0.0074147987
train_loss: 0.002186703
test_loss: 0.007443187
train_loss: 0.0021926148
test_loss: 0.0074554123
train_loss: 0.0020020409
test_loss: 0.0074282642
train_loss: 0.0019191611
test_loss: 0.007478742
train_loss: 0.002030657
test_loss: 0.007581569
train_loss: 0.0023266335
test_loss: 0.007406245
train_loss: 0.0020062216
test_loss: 0.007496877
train_loss: 0.0021778436
test_loss: 0.0075081307
train_loss: 0.0021721446
test_loss: 0.0074629863
train_loss: 0.0020308718
test_loss: 0.00746517
train_loss: 0.0022081875
test_loss: 0.0074396497
train_loss: 0.0019406138
test_loss: 0.0075233234
train_loss: 0.0021259699
test_loss: 0.0073765987
train_loss: 0.0020930148
test_loss: 0.007480185
train_loss: 0.0021519524
test_loss: 0.007473116
train_loss: 0.002131515
test_loss: 0.0074000102
train_loss: 0.0020603463
test_loss: 0.0074428744
train_loss: 0.0020406381
test_loss: 0.007436629
train_loss: 0.002329844
test_loss: 0.007649028
train_loss: 0.002287817
test_loss: 0.0074380958
train_loss: 0.002175514
test_loss: 0.0074458523
train_loss: 0.002013026
test_loss: 0.0073951995
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1 --function f1 --psi -2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 4000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c63949f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c63a626a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c63a2cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c6399abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c639ab268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c639ab950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c638c8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c63898d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c63898ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c63853620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c638539d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c63812e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c6381c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c637babf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c63777bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c63777b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c637af510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c63777510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c6371eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c63720ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c636c9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c636c0b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c636456a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c6365a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c63655620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c6361a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c635c8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c635ea840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c59fec0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c5a015510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c59fb0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c59fde048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c59fdeae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c59f98598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c59f3bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c59f3bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.07383718
test_loss: 0.0661545
train_loss: 0.03818027
test_loss: 0.036664292
train_loss: 0.030514032
test_loss: 0.02799376
train_loss: 0.021404292
test_loss: 0.02325523
train_loss: 0.019065581
test_loss: 0.020643735
train_loss: 0.016239531
test_loss: 0.017691374
train_loss: 0.012511475
test_loss: 0.015190133
train_loss: 0.01020433
test_loss: 0.0133116795
train_loss: 0.008774354
test_loss: 0.012290828
train_loss: 0.0073152897
test_loss: 0.011527938
train_loss: 0.006685009
test_loss: 0.010840152
train_loss: 0.0060852338
test_loss: 0.0104194265
train_loss: 0.00542395
test_loss: 0.010034703
train_loss: 0.005010117
test_loss: 0.009845189
train_loss: 0.0048575923
test_loss: 0.009568298
train_loss: 0.004471129
test_loss: 0.009573312
train_loss: 0.004183594
test_loss: 0.009237601
train_loss: 0.0044002235
test_loss: 0.009251392
train_loss: 0.0041427235
test_loss: 0.009226107
train_loss: 0.0037754383
test_loss: 0.009144054
train_loss: 0.0038860128
test_loss: 0.009072622
train_loss: 0.0036818325
test_loss: 0.008898173
train_loss: 0.0035716337
test_loss: 0.008922784
train_loss: 0.0034636855
test_loss: 0.008836786
train_loss: 0.003573163
test_loss: 0.008993619
train_loss: 0.003558889
test_loss: 0.008941439
train_loss: 0.0035842068
test_loss: 0.008835936
train_loss: 0.0031836573
test_loss: 0.008759788
train_loss: 0.0034076406
test_loss: 0.008758321
train_loss: 0.0032458561
test_loss: 0.008805117
train_loss: 0.0030336343
test_loss: 0.008809535
train_loss: 0.0031553716
test_loss: 0.008743965
train_loss: 0.0031398032
test_loss: 0.008692935
train_loss: 0.0029924489
test_loss: 0.008760496
train_loss: 0.0032888672
test_loss: 0.008729925
train_loss: 0.0031526755
test_loss: 0.008725074
train_loss: 0.0030817064
test_loss: 0.008770148
train_loss: 0.0032595503
test_loss: 0.008752033
train_loss: 0.0031121625
test_loss: 0.0086991
train_loss: 0.003205194
test_loss: 0.008674381
train_loss: 0.0030162632
test_loss: 0.008711384
train_loss: 0.003081861
test_loss: 0.00856536
train_loss: 0.0029546192
test_loss: 0.008747064
train_loss: 0.0030455731
test_loss: 0.00882881
train_loss: 0.002942821
test_loss: 0.008591268
train_loss: 0.0028725993
test_loss: 0.008662125
train_loss: 0.002887422
test_loss: 0.008618594
train_loss: 0.0028935515
test_loss: 0.008610979
train_loss: 0.0026812754
test_loss: 0.008754042
train_loss: 0.002946233
test_loss: 0.008642828
train_loss: 0.0027688881
test_loss: 0.008674624
train_loss: 0.0027007922
test_loss: 0.008688403
train_loss: 0.0027393827
test_loss: 0.008587634
train_loss: 0.0029152264
test_loss: 0.0086315675
train_loss: 0.0028845514
test_loss: 0.008607706
train_loss: 0.0029343702
test_loss: 0.008589978
train_loss: 0.002732568
test_loss: 0.008539372
train_loss: 0.002688222
test_loss: 0.008636141
train_loss: 0.0025991057
test_loss: 0.0087402
train_loss: 0.002842944
test_loss: 0.008665721
train_loss: 0.0027160929
test_loss: 0.008681771
train_loss: 0.0026819115
test_loss: 0.008686504
train_loss: 0.002596193
test_loss: 0.008514998
train_loss: 0.0026369232
test_loss: 0.008728055
train_loss: 0.0027521502
test_loss: 0.008529922
train_loss: 0.0026020673
test_loss: 0.008668609
train_loss: 0.0026057765
test_loss: 0.008575837
train_loss: 0.0024496736
test_loss: 0.008625323
train_loss: 0.0028838853
test_loss: 0.00867637
train_loss: 0.0029202383
test_loss: 0.008611246
train_loss: 0.0027164721
test_loss: 0.008685371
train_loss: 0.0026973414
test_loss: 0.00859105
train_loss: 0.0029097663
test_loss: 0.008695388
train_loss: 0.0026744453
test_loss: 0.008648667
train_loss: 0.0024651894
test_loss: 0.008663801
train_loss: 0.0028182068
test_loss: 0.008645963
train_loss: 0.0027195248
test_loss: 0.008635895
train_loss: 0.002558082
test_loss: 0.008641887
train_loss: 0.0025194876
test_loss: 0.008549412
train_loss: 0.0025366032
test_loss: 0.00865185
train_loss: 0.002482798
test_loss: 0.00858151
train_loss: 0.0028455588
test_loss: 0.008586524
train_loss: 0.0027986686
test_loss: 0.00863426
train_loss: 0.0028965296
test_loss: 0.008662618
train_loss: 0.0027325863
test_loss: 0.008668478
train_loss: 0.0025605916
test_loss: 0.008652262
train_loss: 0.002833461
test_loss: 0.0087017715
train_loss: 0.0027614147
test_loss: 0.008666361
train_loss: 0.0027144002
test_loss: 0.008798311
train_loss: 0.0025955027
test_loss: 0.008604976
train_loss: 0.0025956044
test_loss: 0.008580732
train_loss: 0.0026430374
test_loss: 0.008709639
train_loss: 0.0027122227
test_loss: 0.008622202
train_loss: 0.0027008953
test_loss: 0.008639217
train_loss: 0.0026283432
test_loss: 0.008654867
train_loss: 0.0025819207
test_loss: 0.008659361
train_loss: 0.0025006514
test_loss: 0.008599233
train_loss: 0.002645173
test_loss: 0.008585071
train_loss: 0.002582021
test_loss: 0.008594447
train_loss: 0.0025377464
test_loss: 0.008711982
train_loss: 0.0025590563
test_loss: 0.008741934
train_loss: 0.0027130987
test_loss: 0.008621624
train_loss: 0.0024142028
test_loss: 0.008691333
train_loss: 0.002471078
test_loss: 0.008679469
train_loss: 0.002487169
test_loss: 0.008581124
train_loss: 0.0024585817
test_loss: 0.008762454
train_loss: 0.0025844206
test_loss: 0.008635748
train_loss: 0.002554761
test_loss: 0.008682609
train_loss: 0.0026238416
test_loss: 0.008588937
train_loss: 0.002635135
test_loss: 0.0086863255
train_loss: 0.002592029
test_loss: 0.008558927
train_loss: 0.002452894
test_loss: 0.008591524
train_loss: 0.002352099
test_loss: 0.008657889
train_loss: 0.002239169
test_loss: 0.008646567
train_loss: 0.0022692783
test_loss: 0.008595802
train_loss: 0.0023651018
test_loss: 0.008644384
train_loss: 0.002418366
test_loss: 0.008575354
train_loss: 0.0024591857
test_loss: 0.008722416
train_loss: 0.0027466225
test_loss: 0.00858184
train_loss: 0.0024524236
test_loss: 0.008650523
train_loss: 0.0024590774
test_loss: 0.008545575
train_loss: 0.0023723892
test_loss: 0.008681099
train_loss: 0.0025898016
test_loss: 0.008582793
train_loss: 0.0026599306
test_loss: 0.008646895
train_loss: 0.0025360668
test_loss: 0.008745678
train_loss: 0.002784248
test_loss: 0.008728294
train_loss: 0.002731679
test_loss: 0.00875086
train_loss: 0.0028224678
test_loss: 0.008660807
train_loss: 0.002918698
test_loss: 0.008673404
train_loss: 0.0025574365
test_loss: 0.008691891
train_loss: 0.002731763
test_loss: 0.008735796
train_loss: 0.0025977858
test_loss: 0.008685898
train_loss: 0.0025730089
test_loss: 0.0086110085
train_loss: 0.0024553433
test_loss: 0.008620854
train_loss: 0.0029253934
test_loss: 0.008709861
train_loss: 0.0024172328
test_loss: 0.008773571
train_loss: 0.0027202722
test_loss: 0.008638418
train_loss: 0.0025225244
test_loss: 0.008790702
train_loss: 0.0024873307
test_loss: 0.008759215
train_loss: 0.0026725081
test_loss: 0.008776617
train_loss: 0.0023260124
test_loss: 0.008746035
train_loss: 0.0025180462
test_loss: 0.008692938
train_loss: 0.0025989148
test_loss: 0.008631781
train_loss: 0.002466106
test_loss: 0.008733755
train_loss: 0.0027034585
test_loss: 0.008728336
train_loss: 0.0026677686
test_loss: 0.008704152
train_loss: 0.00267371
test_loss: 0.008796273
train_loss: 0.0027719704
test_loss: 0.008836707
train_loss: 0.00253257
test_loss: 0.008739553
train_loss: 0.002802872
test_loss: 0.008691422
train_loss: 0.002653976
test_loss: 0.008746245
train_loss: 0.0026209399
test_loss: 0.008737004
train_loss: 0.0027263088
test_loss: 0.008714081
train_loss: 0.0025776553
test_loss: 0.008734025
train_loss: 0.0024610246
test_loss: 0.008684465
train_loss: 0.0026220002
test_loss: 0.00880882
train_loss: 0.0025587443
test_loss: 0.0087652095
train_loss: 0.0026007956
test_loss: 0.008684747
train_loss: 0.0023865262
test_loss: 0.008696768
train_loss: 0.0025574155
test_loss: 0.008666704
train_loss: 0.0022473964
test_loss: 0.008731978
train_loss: 0.002575
test_loss: 0.008661076
train_loss: 0.002449385
test_loss: 0.008649223
train_loss: 0.002316745
test_loss: 0.008630639
train_loss: 0.002222589
test_loss: 0.008688774
train_loss: 0.0023221248
test_loss: 0.008713144
train_loss: 0.0024539232
test_loss: 0.008651026
train_loss: 0.0024159336
test_loss: 0.008758779
train_loss: 0.00245788
test_loss: 0.0086638145
train_loss: 0.0025729279
test_loss: 0.008698736
train_loss: 0.0024598148
test_loss: 0.008813035
train_loss: 0.0023995396
test_loss: 0.008640847
train_loss: 0.0025267962
test_loss: 0.0086631095
train_loss: 0.002407924
test_loss: 0.008667012
train_loss: 0.0024037831
test_loss: 0.008627008
train_loss: 0.0022137277
test_loss: 0.0086831255
train_loss: 0.0021119062
test_loss: 0.0086460095
train_loss: 0.0021899247
test_loss: 0.008698173
train_loss: 0.0022310307
test_loss: 0.008685046
train_loss: 0.0024898704
test_loss: 0.00856951
train_loss: 0.0023791348
test_loss: 0.008716172
train_loss: 0.002315992
test_loss: 0.008727546
train_loss: 0.0023655286
test_loss: 0.0088401595
train_loss: 0.0028335361
test_loss: 0.008659761
train_loss: 0.002572162
test_loss: 0.008710495
train_loss: 0.0025967178
test_loss: 0.008658567
train_loss: 0.0024487786
test_loss: 0.00864724
train_loss: 0.0024807956
test_loss: 0.008781018
train_loss: 0.002492605
test_loss: 0.008758411
train_loss: 0.0024601826
test_loss: 0.008691932
train_loss: 0.0023256044
test_loss: 0.008751605
train_loss: 0.0025115786
test_loss: 0.008616079
train_loss: 0.0023065007
test_loss: 0.008690185
train_loss: 0.0024174782
test_loss: 0.008704387
train_loss: 0.0024036032
test_loss: 0.008787232
train_loss: 0.0026149107
test_loss: 0.008686302
train_loss: 0.0023587793
test_loss: 0.008610834
train_loss: 0.0022165442
test_loss: 0.008793963
train_loss: 0.002418751
test_loss: 0.0086927535
train_loss: 0.0023825068
test_loss: 0.008688859
train_loss: 0.0024032504
test_loss: 0.008725507
train_loss: 0.0022393288
test_loss: 0.008798229
train_loss: 0.002705353
test_loss: 0.008828823
train_loss: 0.0025591757
test_loss: 0.00878929
train_loss: 0.0027787616
test_loss: 0.00869343
train_loss: 0.0023625484
test_loss: 0.008706053
train_loss: 0.0024182172
test_loss: 0.008809599
train_loss: 0.002670898
test_loss: 0.008905192
train_loss: 0.002884506
test_loss: 0.008780322
train_loss: 0.0025201603
test_loss: 0.008907572
train_loss: 0.0029906852
test_loss: 0.008800238
train_loss: 0.0026221834
test_loss: 0.008949813
train_loss: 0.0027029805
test_loss: 0.009010153
train_loss: 0.0029104548
test_loss: 0.008925831
train_loss: 0.0030784782
test_loss: 0.008890714
train_loss: 0.0029808953
test_loss: 0.008876948
train_loss: 0.0027711508
test_loss: 0.008810015
train_loss: 0.002605355
test_loss: 0.008725459
train_loss: 0.0025537515
test_loss: 0.008744675
train_loss: 0.0025419847
test_loss: 0.008753271
train_loss: 0.0023804705
test_loss: 0.008730111
train_loss: 0.0022956997
test_loss: 0.0088296505
train_loss: 0.0023517078
test_loss: 0.00870731
train_loss: 0.0023976755
test_loss: 0.008703572
train_loss: 0.0023660657
test_loss: 0.00868949
train_loss: 0.0025126643
test_loss: 0.008777781
train_loss: 0.0024863782
test_loss: 0.0087533165
train_loss: 0.0023976045
test_loss: 0.008893406
train_loss: 0.0024918318
test_loss: 0.008734714
train_loss: 0.002477802
test_loss: 0.008825794
train_loss: 0.0023746789
test_loss: 0.00877806
train_loss: 0.0022749088
test_loss: 0.008671432
train_loss: 0.0024127325
test_loss: 0.008743172
train_loss: 0.0024168964
test_loss: 0.008731246
train_loss: 0.0023437864
test_loss: 0.008758014
train_loss: 0.002212842
test_loss: 0.0087247295
train_loss: 0.0022291401
test_loss: 0.008811287
train_loss: 0.0023570769
test_loss: 0.008702672
train_loss: 0.0021671725
test_loss: 0.008822994
train_loss: 0.0022615548
test_loss: 0.00876499
train_loss: 0.0024008946
test_loss: 0.008891704
train_loss: 0.0023272936
test_loss: 0.008704392
train_loss: 0.0023443317
test_loss: 0.008736998
train_loss: 0.0021472657
test_loss: 0.008783705
train_loss: 0.0022966864
test_loss: 0.0087920735
train_loss: 0.002575404
test_loss: 0.008760409
train_loss: 0.0024612402
test_loss: 0.008769642
train_loss: 0.0025011122
test_loss: 0.008770509
train_loss: 0.0023063347
test_loss: 0.008855645
train_loss: 0.0024347315
test_loss: 0.008780164
train_loss: 0.0022156537
test_loss: 0.0087566925
train_loss: 0.0023563656
test_loss: 0.008867925
train_loss: 0.0024288564
test_loss: 0.00870417
train_loss: 0.0023673861
test_loss: 0.008799748
train_loss: 0.0022326345
test_loss: 0.0088231005
train_loss: 0.0023187408
test_loss: 0.008821401
train_loss: 0.0024937193
test_loss: 0.008841618
train_loss: 0.0024589647
test_loss: 0.008926367
train_loss: 0.0025087425
test_loss: 0.008818255
train_loss: 0.0024093124
test_loss: 0.00887436
train_loss: 0.0025225587
test_loss: 0.008808859
train_loss: 0.0023469166
test_loss: 0.008799198
train_loss: 0.0022070375
test_loss: 0.008806966
train_loss: 0.0021792175
test_loss: 0.0087069515
train_loss: 0.0022019276
test_loss: 0.008792461
train_loss: 0.0022022189
test_loss: 0.00880522
train_loss: 0.0023448714
test_loss: 0.00879859
train_loss: 0.0025271112
test_loss: 0.008910429
train_loss: 0.0024487267
test_loss: 0.008863109
train_loss: 0.0024169846
test_loss: 0.008785806
train_loss: 0.0025758687
test_loss: 0.008908929
train_loss: 0.0025946908
test_loss: 0.00886536
train_loss: 0.0026703118
test_loss: 0.008874047
train_loss: 0.0027012683
test_loss: 0.00904689
train_loss: 0.0029088422
test_loss: 0.008831906
train_loss: 0.0028445164
test_loss: 0.008970192
train_loss: 0.0026091288
test_loss: 0.009118015
train_loss: 0.0028693867
test_loss: 0.008886631
train_loss: 0.0027479064
test_loss: 0.008937261
train_loss: 0.002655921
test_loss: 0.008932864
train_loss: 0.0026735333
test_loss: 0.008921522
train_loss: 0.00240353
test_loss: 0.009006011
train_loss: 0.002512329
test_loss: 0.008828557
train_loss: 0.0022965262
test_loss: 0.008841503
train_loss: 0.0024868357
test_loss: 0.008972784
train_loss: 0.002625216
test_loss: 0.008866989
train_loss: 0.0024519116
test_loss: 0.00882883
train_loss: 0.0026408916
test_loss: 0.009024556
train_loss: 0.0024555554
test_loss: 0.008861291
train_loss: 0.002239869
test_loss: 0.008827379
train_loss: 0.0022961681
test_loss: 0.008743152
train_loss: 0.0023722944
test_loss: 0.008794133
train_loss: 0.0022631036
test_loss: 0.008859045
train_loss: 0.002437054
test_loss: 0.008796396
train_loss: 0.002175275
test_loss: 0.00879212
train_loss: 0.002223947
test_loss: 0.008830631
train_loss: 0.0023972315
test_loss: 0.008904336
train_loss: 0.0024490084
test_loss: 0.008858691
train_loss: 0.0023520293
test_loss: 0.008766611
train_loss: 0.0023596124
test_loss: 0.008828383
train_loss: 0.0023559306
test_loss: 0.008823829
train_loss: 0.002525485
test_loss: 0.008781402
train_loss: 0.0028142347
test_loss: 0.008939379
train_loss: 0.002479785
test_loss: 0.008938981
train_loss: 0.0025944049
test_loss: 0.008909559
train_loss: 0.0022577257
test_loss: 0.008868364
train_loss: 0.0022645858
test_loss: 0.008901343
train_loss: 0.0022978596
test_loss: 0.008843091
train_loss: 0.0022692126
test_loss: 0.008799575
train_loss: 0.0022185848
test_loss: 0.008746948
train_loss: 0.0024018637
test_loss: 0.008843648
train_loss: 0.0021001415
test_loss: 0.00889959
train_loss: 0.0027233341
test_loss: 0.008816121
train_loss: 0.0024197877
test_loss: 0.008890898
train_loss: 0.0027454407
test_loss: 0.008926948
train_loss: 0.0027115848
test_loss: 0.008872501
train_loss: 0.0024703399
test_loss: 0.009028878
train_loss: 0.0025779563
test_loss: 0.0089059835
train_loss: 0.0023682064
test_loss: 0.00902039
train_loss: 0.0025325113
test_loss: 0.008905371
train_loss: 0.0023195855
test_loss: 0.008792456
train_loss: 0.0022567636
test_loss: 0.008908719
train_loss: 0.0023141443
test_loss: 0.008794015
train_loss: 0.002277236
test_loss: 0.008921151
train_loss: 0.0024713022
test_loss: 0.00881339
train_loss: 0.0022214497
test_loss: 0.008903415
train_loss: 0.002213658
test_loss: 0.008854915
train_loss: 0.0022307422
test_loss: 0.008872983
train_loss: 0.0023681559
test_loss: 0.008830595
train_loss: 0.0021455528
test_loss: 0.008902294
train_loss: 0.0023539586
test_loss: 0.008956708
train_loss: 0.0025543256
test_loss: 0.008923477
train_loss: 0.0023849
test_loss: 0.008888777
train_loss: 0.00244864
test_loss: 0.008851514
train_loss: 0.0023116164
test_loss: 0.008906932
train_loss: 0.0024490906
test_loss: 0.008786359
train_loss: 0.0023754193
test_loss: 0.008906424
train_loss: 0.002370087
test_loss: 0.008854599
train_loss: 0.002180864
test_loss: 0.008859702
train_loss: 0.0024323598
test_loss: 0.009105338
train_loss: 0.002469912
test_loss: 0.008781538
train_loss: 0.0022337642
test_loss: 0.008774683
train_loss: 0.0023560163
test_loss: 0.008858297
train_loss: 0.0024150154
test_loss: 0.00897491
train_loss: 0.0024984805
test_loss: 0.0088881785
train_loss: 0.002379569
test_loss: 0.009019178
train_loss: 0.002392958
test_loss: 0.008901465
train_loss: 0.002436557
test_loss: 0.008948314
train_loss: 0.002418706
test_loss: 0.009081202
train_loss: 0.0026309807
test_loss: 0.008924543
train_loss: 0.0025212492
test_loss: 0.008863787
train_loss: 0.0023589288
test_loss: 0.009023707
train_loss: 0.002504822
test_loss: 0.008790433
train_loss: 0.0022143472
test_loss: 0.008862622
train_loss: 0.0024295056
test_loss: 0.008876113
train_loss: 0.0024511523
test_loss: 0.009015616
train_loss: 0.0024432072
test_loss: 0.008878206
train_loss: 0.0025559838
test_loss: 0.008938821
train_loss: 0.0024647485
test_loss: 0.008906693
train_loss: 0.0023511623
test_loss: 0.008932356
train_loss: 0.0021625706
test_loss: 0.008860524
train_loss: 0.0021804362
test_loss: 0.008888473
train_loss: 0.002383159
test_loss: 0.00893839
train_loss: 0.0024948511
test_loss: 0.008888245
train_loss: 0.0024565794
test_loss: 0.008861809
train_loss: 0.0025108312
test_loss: 0.008929335
train_loss: 0.0023185818
test_loss: 0.008961361
train_loss: 0.0024874853
test_loss: 0.008815498
train_loss: 0.0021886649
test_loss: 0.008911999
train_loss: 0.002138227
test_loss: 0.008888823
train_loss: 0.0022369244
test_loss: 0.008848218
train_loss: 0.002228382
test_loss: 0.008854647
train_loss: 0.0023030152
test_loss: 0.008966152
train_loss: 0.0023421198
test_loss: 0.008913987
train_loss: 0.0021016055
test_loss: 0.0089742895
train_loss: 0.0022224458
test_loss: 0.008993265
train_loss: 0.0023101277
test_loss: 0.00896511
train_loss: 0.0022389386
test_loss: 0.008988406
train_loss: 0.0022533245
test_loss: 0.008887037
train_loss: 0.0025016044
test_loss: 0.009075875
train_loss: 0.0026328715
test_loss: 0.009061432
train_loss: 0.0028651627
test_loss: 0.008975172
train_loss: 0.002604856
test_loss: 0.009112578
train_loss: 0.0029137025
test_loss: 0.008960472
train_loss: 0.0026203939
test_loss: 0.009132897
train_loss: 0.0025924947
test_loss: 0.009314474
train_loss: 0.0025827019
test_loss: 0.00900515
train_loss: 0.0025899168
test_loss: 0.009005474
train_loss: 0.0024684998
test_loss: 0.008948021
train_loss: 0.0024908755
test_loss: 0.008978938
train_loss: 0.0023365014
test_loss: 0.008935701
train_loss: 0.00268191
test_loss: 0.008923744
train_loss: 0.00283904
test_loss: 0.009112444
train_loss: 0.0027458048
test_loss: 0.009180626
train_loss: 0.002524717
test_loss: 0.009010959
train_loss: 0.002916998
test_loss: 0.008965644
train_loss: 0.0023310466
test_loss: 0.0090622315
train_loss: 0.0026330766
test_loss: 0.009075464
train_loss: 0.0027019484
test_loss: 0.009075981
train_loss: 0.0025436494
test_loss: 0.008971393
train_loss: 0.0023633419
test_loss: 0.008984055
train_loss: 0.0023780908
test_loss: 0.0089879
train_loss: 0.002382485
test_loss: 0.008906278
train_loss: 0.0023146549
test_loss: 0.009078227
train_loss: 0.0023923633
test_loss: 0.008903224
train_loss: 0.0023950837
test_loss: 0.008913082
train_loss: 0.0022931152
test_loss: 0.008970079
train_loss: 0.002311708
test_loss: 0.008850022
train_loss: 0.0021896611
test_loss: 0.008952107
train_loss: 0.0021489756
test_loss: 0.008911463
train_loss: 0.0021323103
test_loss: 0.008957982
train_loss: 0.0023296203
test_loss: 0.009024189
train_loss: 0.0025503242
test_loss: 0.008932542
train_loss: 0.0023601563
test_loss: 0.008966612
train_loss: 0.0024984009
test_loss: 0.008958677
train_loss: 0.002320086
test_loss: 0.00898681
train_loss: 0.0021654642
test_loss: 0.009002943
train_loss: 0.0022843145
test_loss: 0.008870425
train_loss: 0.0021417458
test_loss: 0.00893587
train_loss: 0.0022208658
test_loss: 0.009001757
train_loss: 0.0021519377
test_loss: 0.008910571
train_loss: 0.0019383337
test_loss: 0.00893451
train_loss: 0.0021463546
test_loss: 0.008947282
train_loss: 0.002186085
test_loss: 0.008897273
train_loss: 0.0022389153
test_loss: 0.008977615
train_loss: 0.0023466372
test_loss: 0.009033321
train_loss: 0.002331755
test_loss: 0.008950898
train_loss: 0.0023447836
test_loss: 0.00903972
train_loss: 0.0023128628
test_loss: 0.008891306
train_loss: 0.0024382377
test_loss: 0.00906679
train_loss: 0.0022068615
test_loss: 0.009065665
train_loss: 0.0025183065
test_loss: 0.009077945
train_loss: 0.002499708
test_loss: 0.008932234
train_loss: 0.0025187708
test_loss: 0.009068949
train_loss: 0.002391762
test_loss: 0.00905654
train_loss: 0.0023334888
test_loss: 0.009012582
train_loss: 0.0022466409
test_loss: 0.008914493
train_loss: 0.0024446803
test_loss: 0.009084143
train_loss: 0.0025643744
test_loss: 0.008993786
train_loss: 0.0024640567
test_loss: 0.009067675
train_loss: 0.0024478198
test_loss: 0.008971715
train_loss: 0.0023848317
test_loss: 0.009031774
train_loss: 0.0022542144
test_loss: 0.009014734
train_loss: 0.0021295506
test_loss: 0.008980838
train_loss: 0.0020298925
test_loss: 0.0089422
train_loss: 0.00208604
test_loss: 0.00891933
train_loss: 0.0022008368
test_loss: 0.008880308
train_loss: 0.0022881029
test_loss: 0.009019909
train_loss: 0.0022130085
test_loss: 0.009004016
train_loss: 0.002280355
test_loss: 0.0088262735
train_loss: 0.0021167349
test_loss: 0.009014765
train_loss: 0.002224095
test_loss: 0.009133512
train_loss: 0.0025281622
test_loss: 0.008923957
train_loss: 0.0023409054
test_loss: 0.008951637
train_loss: 0.0023447
test_loss: 0.008939617
train_loss: 0.0023337195
test_loss: 0.008969229
train_loss: 0.0022047125
test_loss: 0.009025565
train_loss: 0.0021940998
test_loss: 0.009100465
train_loss: 0.0021596053
test_loss: 0.00896561
train_loss: 0.002356896
test_loss: 0.00898855
train_loss: 0.0021117444
test_loss: 0.009007396
train_loss: 0.0022321313
test_loss: 0.008987516
train_loss: 0.0023289095
test_loss: 0.00898662
train_loss: 0.0021632747
test_loss: 0.0090459995
train_loss: 0.0022917378
test_loss: 0.0089954
train_loss: 0.0022727936
test_loss: 0.009070777
train_loss: 0.0022445312
test_loss: 0.009052743
train_loss: 0.0023927859
test_loss: 0.00915964
train_loss: 0.0024539726
test_loss: 0.009033913
train_loss: 0.002449009
test_loss: 0.009064573
train_loss: 0.002258927
test_loss: 0.009031434
train_loss: 0.0021744843
test_loss: 0.008983303
train_loss: 0.0022265983
test_loss: 0.009098053
train_loss: 0.0022390413
test_loss: 0.008928761
train_loss: 0.002159221
test_loss: 0.009001228
train_loss: 0.0019350369
test_loss: 0.008970404
train_loss: 0.0023617018
test_loss: 0.008923681
train_loss: 0.0022366843
test_loss: 0.008908038
train_loss: 0.0022097533
test_loss: 0.00904508
train_loss: 0.0022346107
test_loss: 0.00901256
train_loss: 0.0020628641
test_loss: 0.008972456
train_loss: 0.0020588627
test_loss: 0.0089407535
train_loss: 0.0021649697
test_loss: 0.009017308
train_loss: 0.0020124523
test_loss: 0.008953964
train_loss: 0.0024016798
test_loss: 0.009225515
train_loss: 0.0025720412
test_loss: 0.009034361
train_loss: 0.0023474055
test_loss: 0.009046364
train_loss: 0.0024217872
test_loss: 0.008986872
train_loss: 0.002462698
test_loss: 0.009044632
train_loss: 0.002566531
test_loss: 0.009052681
train_loss: 0.0023632487
test_loss: 0.009043169
train_loss: 0.0024661561
test_loss: 0.009143676
train_loss: 0.002470909
test_loss: 0.009051494
train_loss: 0.002458072
test_loss: 0.009056392
train_loss: 0.002294224
test_loss: 0.00899432
train_loss: 0.0023181215
test_loss: 0.009077582
train_loss: 0.002223522
test_loss: 0.008995331
train_loss: 0.0023899428
test_loss: 0.008998481
train_loss: 0.0023448125
test_loss: 0.00907365
train_loss: 0.0023636904
test_loss: 0.00905116
train_loss: 0.0024367622
test_loss: 0.009041853
train_loss: 0.0024265386
test_loss: 0.0090700565
train_loss: 0.0024214701
test_loss: 0.009093388
train_loss: 0.002203859
test_loss: 0.009094867
train_loss: 0.002322594
test_loss: 0.008974347
train_loss: 0.0024343038
test_loss: 0.009031025
train_loss: 0.0026759815
test_loss: 0.009012914
train_loss: 0.0025181384
test_loss: 0.009140521
train_loss: 0.0024392998
test_loss: 0.009126235
train_loss: 0.0024796778
test_loss: 0.009061623
train_loss: 0.0024300152
test_loss: 0.009096546
train_loss: 0.002237875
test_loss: 0.009072522
train_loss: 0.0023360704
test_loss: 0.009040586
train_loss: 0.0023343782
test_loss: 0.0090563465
train_loss: 0.0023192486
test_loss: 0.009099679
train_loss: 0.0022955162
test_loss: 0.009009644
train_loss: 0.0024408523
test_loss: 0.009085102
train_loss: 0.002417507
test_loss: 0.009080211
train_loss: 0.002300936
test_loss: 0.009097897
train_loss: 0.0022451165
test_loss: 0.009163377
train_loss: 0.0021033462
test_loss: 0.009101441
train_loss: 0.0023465601
test_loss: 0.009061466
train_loss: 0.0023400222
test_loss: 0.00906957
train_loss: 0.0024959533
test_loss: 0.00911906
train_loss: 0.0023453226
test_loss: 0.009182685
train_loss: 0.0023015235
test_loss: 0.009045731
train_loss: 0.002179188
test_loss: 0.009071192
train_loss: 0.002317085
test_loss: 0.008981879
train_loss: 0.0020494089
test_loss: 0.008968267
train_loss: 0.00216916
test_loss: 0.009106981
train_loss: 0.002075636
test_loss: 0.009025411
train_loss: 0.0021791707
test_loss: 0.009049731
train_loss: 0.0022227275
test_loss: 0.009044921
train_loss: 0.002228932
test_loss: 0.009084685
train_loss: 0.002143988
test_loss: 0.009189281
train_loss: 0.0021686857
test_loss: 0.009113439
train_loss: 0.0022441265
test_loss: 0.0090722
train_loss: 0.0023714849
test_loss: 0.008965966
train_loss: 0.002349056
test_loss: 0.008969961
train_loss: 0.002319208
test_loss: 0.009060136
train_loss: 0.0021989872
test_loss: 0.009109346
train_loss: 0.0021822539
test_loss: 0.009028047
train_loss: 0.0021754229
test_loss: 0.009162103
train_loss: 0.0023997435
test_loss: 0.009041586
train_loss: 0.002329807
test_loss: 0.00919001
train_loss: 0.0025890828
test_loss: 0.009055665
train_loss: 0.0023948797
test_loss: 0.009109913
train_loss: 0.0024449155
test_loss: 0.009117114
train_loss: 0.002391238
test_loss: 0.009094085
train_loss: 0.0020935829
test_loss: 0.009003748
train_loss: 0.0022889331
test_loss: 0.009007306
train_loss: 0.0022552416
test_loss: 0.009142092
train_loss: 0.0023652494
test_loss: 0.009073158
train_loss: 0.0027069526
test_loss: 0.00930096
train_loss: 0.0024397387
test_loss: 0.009277578
train_loss: 0.002394704
test_loss: 0.009094266
train_loss: 0.0025057404
test_loss: 0.009095086
train_loss: 0.0022481175
test_loss: 0.009114952
train_loss: 0.0024646427
test_loss: 0.009154558
train_loss: 0.0022305092
test_loss: 0.009058356
train_loss: 0.0022261718
test_loss: 0.009113089
train_loss: 0.0021041206
test_loss: 0.009069691
train_loss: 0.002469936
test_loss: 0.009035836
train_loss: 0.0022587369
test_loss: 0.009128515
train_loss: 0.0023095738
test_loss: 0.009233064
train_loss: 0.0023343675
test_loss: 0.009128111
train_loss: 0.002228599
test_loss: 0.009076341
train_loss: 0.0022446068
test_loss: 0.009215565
train_loss: 0.0021283668
test_loss: 0.00903225
train_loss: 0.0022735405
test_loss: 0.009131247
train_loss: 0.0022185044
test_loss: 0.009058927
train_loss: 0.0020709285
test_loss: 0.00914305
train_loss: 0.0022134858
test_loss: 0.009126673
train_loss: 0.0022110501
test_loss: 0.009045083
train_loss: 0.002269827
test_loss: 0.00918227
train_loss: 0.0023821348
test_loss: 0.00915851
train_loss: 0.0024977806
test_loss: 0.009009073
train_loss: 0.0020633703
test_loss: 0.009021448
train_loss: 0.0021871002
test_loss: 0.009217725
train_loss: 0.0022172197
test_loss: 0.009051958
train_loss: 0.0019890368
test_loss: 0.009092823
train_loss: 0.0021534818
test_loss: 0.008981464
train_loss: 0.0022119146
test_loss: 0.009178458
train_loss: 0.0023716798
test_loss: 0.00910528
train_loss: 0.0023490966
test_loss: 0.009172316
train_loss: 0.0021449793
test_loss: 0.009073464
train_loss: 0.002224834
test_loss: 0.009146136
train_loss: 0.002280233
test_loss: 0.009104408
train_loss: 0.0021570816
test_loss: 0.009187158
train_loss: 0.002203638
test_loss: 0.009121474
train_loss: 0.0022594258
test_loss: 0.009142029
train_loss: 0.002238817
test_loss: 0.009224615
train_loss: 0.0021393541
test_loss: 0.009150991
train_loss: 0.0022023257
test_loss: 0.009103291
train_loss: 0.0023485539
test_loss: 0.009084284
train_loss: 0.0021401271
test_loss: 0.009312167
train_loss: 0.002180545
test_loss: 0.009127452
train_loss: 0.002194229
test_loss: 0.009127264
train_loss: 0.0023540994
test_loss: 0.009160263
train_loss: 0.0021743572
test_loss: 0.009166322
train_loss: 0.002136273
test_loss: 0.009159537
train_loss: 0.0021134543
test_loss: 0.00918326
train_loss: 0.0023287581
test_loss: 0.009195715
train_loss: 0.0020299992
test_loss: 0.009106303
train_loss: 0.0021701758
test_loss: 0.009183874
train_loss: 0.002229306
test_loss: 0.00913173
train_loss: 0.002294606
test_loss: 0.00921014
train_loss: 0.0022765195
test_loss: 0.009171436
train_loss: 0.0023088385
test_loss: 0.009212295
train_loss: 0.0021920907
test_loss: 0.0092473095
train_loss: 0.0022253804
test_loss: 0.009249179
train_loss: 0.0023635062
test_loss: 0.009233171
train_loss: 0.0024432286
test_loss: 0.0091473935
train_loss: 0.0021605468
test_loss: 0.009252747
train_loss: 0.0023728919
test_loss: 0.009135749
train_loss: 0.0023620622
test_loss: 0.009177664
train_loss: 0.0022987756
test_loss: 0.009171704
train_loss: 0.0020838273
test_loss: 0.009128857
train_loss: 0.0023258107
test_loss: 0.009261983
train_loss: 0.0023702483
test_loss: 0.009165614
train_loss: 0.0022311881
test_loss: 0.00913079
train_loss: 0.0020940246
test_loss: 0.009169868
train_loss: 0.0024727115
test_loss: 0.009168259
train_loss: 0.0022838442
test_loss: 0.009175675
train_loss: 0.0025862209
test_loss: 0.009309555
train_loss: 0.0025560185
test_loss: 0.009146096
train_loss: 0.0024109096
test_loss: 0.009204792
train_loss: 0.002362554
test_loss: 0.009110283
train_loss: 0.0022191051
test_loss: 0.009131666
train_loss: 0.0021544797
test_loss: 0.009146769
train_loss: 0.0021307121
test_loss: 0.009083594
train_loss: 0.00204472
test_loss: 0.009139384
train_loss: 0.0021857182
test_loss: 0.009113528
train_loss: 0.0023457806
test_loss: 0.009127641
train_loss: 0.0023405263
test_loss: 0.009227193
train_loss: 0.0024975785
test_loss: 0.009237217
train_loss: 0.002390975
test_loss: 0.009162906
train_loss: 0.002309254
test_loss: 0.00917048
train_loss: 0.0023434872
test_loss: 0.00920961
train_loss: 0.0023075787
test_loss: 0.0092343725
train_loss: 0.0021841074
test_loss: 0.009254535
train_loss: 0.0020210496
test_loss: 0.009179234
train_loss: 0.0021885047
test_loss: 0.009189286
train_loss: 0.0020359405
test_loss: 0.009135065
train_loss: 0.0019600596
test_loss: 0.00908867
train_loss: 0.0020914061
test_loss: 0.009163279
train_loss: 0.0021887089
test_loss: 0.00922988
train_loss: 0.0022687963
test_loss: 0.009149348
train_loss: 0.0023178535
test_loss: 0.009288148
train_loss: 0.0021732775
test_loss: 0.009205506
train_loss: 0.0023137324
test_loss: 0.009303234
train_loss: 0.0021049809
test_loss: 0.009266467
train_loss: 0.0021050065
test_loss: 0.009141206
train_loss: 0.0021841573
test_loss: 0.009209674
train_loss: 0.002300081
test_loss: 0.009191264
train_loss: 0.0020742118
test_loss: 0.009155328
train_loss: 0.0022461736
test_loss: 0.00918664
train_loss: 0.0022420923
test_loss: 0.009276503
train_loss: 0.002238187
test_loss: 0.009219792
train_loss: 0.0025311147
test_loss: 0.009263133
train_loss: 0.0023332315
test_loss: 0.009281744
train_loss: 0.0025273033
test_loss: 0.009257406
train_loss: 0.0023432905
test_loss: 0.0091923885
train_loss: 0.0024094218
test_loss: 0.009389397
train_loss: 0.002478695
test_loss: 0.009172943
train_loss: 0.0025046046
test_loss: 0.009411271
train_loss: 0.002381078
test_loss: 0.009233134
train_loss: 0.0025788473
test_loss: 0.009270764
train_loss: 0.002368343
test_loss: 0.009322298
train_loss: 0.0022959705
test_loss: 0.009270942
train_loss: 0.0024054602
test_loss: 0.009225961
train_loss: 0.0023838796
test_loss: 0.009294486
train_loss: 0.0024916294
test_loss: 0.009223307
train_loss: 0.0023601213
test_loss: 0.009265562
train_loss: 0.002208054
test_loss: 0.009169782
train_loss: 0.0022065188
test_loss: 0.009257181
train_loss: 0.0022697025
test_loss: 0.009202106
train_loss: 0.0024534634
test_loss: 0.00926287
train_loss: 0.0022260519
test_loss: 0.009332486
train_loss: 0.002386312
test_loss: 0.009179147
train_loss: 0.0022737184
test_loss: 0.009218473
train_loss: 0.0023611593
test_loss: 0.009186919
train_loss: 0.002297365
test_loss: 0.0091937035
train_loss: 0.0023386593
test_loss: 0.009183855
train_loss: 0.0025134669
test_loss: 0.009384667
train_loss: 0.0026841464
test_loss: 0.009236937
train_loss: 0.00239365
test_loss: 0.0092949765
train_loss: 0.0024469995
test_loss: 0.009361387
train_loss: 0.002278925
test_loss: 0.009313346
train_loss: 0.0021482569
test_loss: 0.009155453
train_loss: 0.0020654555
test_loss: 0.00932963
train_loss: 0.0026843853
test_loss: 0.009280342
train_loss: 0.0021553577
test_loss: 0.009175837
train_loss: 0.0021245903
test_loss: 0.009233389
train_loss: 0.0022757575
test_loss: 0.009246926
train_loss: 0.0021955632
test_loss: 0.009257514
train_loss: 0.002254997
test_loss: 0.009175862
train_loss: 0.0024290998
test_loss: 0.00925064
train_loss: 0.0023019514
test_loss: 0.009378081
train_loss: 0.0023612627
test_loss: 0.009269857
train_loss: 0.0021560881
test_loss: 0.009295739
train_loss: 0.0026281655
test_loss: 0.009243793
train_loss: 0.0025184918
test_loss: 0.009414477
train_loss: 0.002561411
test_loss: 0.00935943
train_loss: 0.0024543137
test_loss: 0.009304122
train_loss: 0.0022102704
test_loss: 0.009408489
train_loss: 0.002257743
test_loss: 0.009236274
train_loss: 0.0022813566
test_loss: 0.009276984
train_loss: 0.0021005773
test_loss: 0.009361724
train_loss: 0.0021898504
test_loss: 0.009232095
train_loss: 0.0021561093
test_loss: 0.0092355115
train_loss: 0.0021525328
test_loss: 0.009256576
train_loss: 0.0021856995
test_loss: 0.009212323
train_loss: 0.002289017
test_loss: 0.009237786
train_loss: 0.0021405446
test_loss: 0.009285335
train_loss: 0.00225716
test_loss: 0.009305729
train_loss: 0.002240382
test_loss: 0.00922722
train_loss: 0.0020830955
test_loss: 0.0091980025
train_loss: 0.00210168
test_loss: 0.00934813
train_loss: 0.0022656831
test_loss: 0.009167553
train_loss: 0.00231296
test_loss: 0.009371415
train_loss: 0.0021680617
test_loss: 0.0091778645
train_loss: 0.0024175788
test_loss: 0.009391288
train_loss: 0.0023966383
test_loss: 0.009366309
train_loss: 0.002462281
test_loss: 0.009417326
train_loss: 0.0026765172
test_loss: 0.009347491
train_loss: 0.0027636564
test_loss: 0.009449429
train_loss: 0.0026290298
test_loss: 0.009349573
train_loss: 0.00230148
test_loss: 0.00933025
train_loss: 0.002284543
test_loss: 0.00932771
train_loss: 0.0021252814
test_loss: 0.0092282165
train_loss: 0.0021381055
test_loss: 0.009245242
train_loss: 0.0021811456
test_loss: 0.009224998
train_loss: 0.002136656
test_loss: 0.009235037
train_loss: 0.0021714622
test_loss: 0.009276494
train_loss: 0.002000016
test_loss: 0.009300127
train_loss: 0.0022196108
test_loss: 0.009244419
train_loss: 0.0024460233
test_loss: 0.009386351
train_loss: 0.0021584255
test_loss: 0.009356945
train_loss: 0.0022348734
test_loss: 0.009241469
train_loss: 0.002103886
test_loss: 0.009362229
train_loss: 0.0020406395
test_loss: 0.009259657
train_loss: 0.0022338962
test_loss: 0.009160456
train_loss: 0.001976104
test_loss: 0.009317782
train_loss: 0.0020818594
test_loss: 0.0092366515
train_loss: 0.0021352672
test_loss: 0.009277773
train_loss: 0.0020782126
test_loss: 0.009301824
train_loss: 0.002242716
test_loss: 0.009258538
train_loss: 0.0021583803
test_loss: 0.009336745
train_loss: 0.0021896604
test_loss: 0.009265655
train_loss: 0.0021430945
test_loss: 0.009310024
train_loss: 0.0026414061
test_loss: 0.009327758
train_loss: 0.0024029368
test_loss: 0.009351177
train_loss: 0.0022972801
test_loss: 0.009371512
train_loss: 0.0022191273
test_loss: 0.009324947
train_loss: 0.002413841
test_loss: 0.009297201
train_loss: 0.0022831354
test_loss: 0.009322482
train_loss: 0.0020998067
test_loss: 0.00931931
train_loss: 0.0022464897
test_loss: 0.009358147
train_loss: 0.0024583694
test_loss: 0.0093418155
train_loss: 0.0022039963
test_loss: 0.00932444
train_loss: 0.0023033433
test_loss: 0.0092762895
train_loss: 0.0022235322
test_loss: 0.009275548
train_loss: 0.0020663347
test_loss: 0.009260457
train_loss: 0.0022170136
test_loss: 0.009325999
train_loss: 0.0024211467
test_loss: 0.009307968
train_loss: 0.0021865051
test_loss: 0.00938638
train_loss: 0.0023108008
test_loss: 0.009286239
train_loss: 0.0020014013
test_loss: 0.00927899
train_loss: 0.0019897737
test_loss: 0.00924554
train_loss: 0.001993829
test_loss: 0.00940048
train_loss: 0.0023046758
test_loss: 0.0093104
train_loss: 0.0022403337
test_loss: 0.009348992
train_loss: 0.002145924
test_loss: 0.009284105
train_loss: 0.0021671166
test_loss: 0.009334484
train_loss: 0.0020054136
test_loss: 0.009267485
train_loss: 0.0022113859
test_loss: 0.009353482
train_loss: 0.0020961259
test_loss: 0.009232212
train_loss: 0.002208939
test_loss: 0.009369293
train_loss: 0.0021717923
test_loss: 0.009265063
train_loss: 0.0023286212
test_loss: 0.009417201
train_loss: 0.002211303
test_loss: 0.0092487335
train_loss: 0.002174545
test_loss: 0.009362639
train_loss: 0.002163531
test_loss: 0.009282629
train_loss: 0.002076343
test_loss: 0.009270985
train_loss: 0.0022751628
test_loss: 0.009435162
train_loss: 0.0022428366
test_loss: 0.0093167545
train_loss: 0.0022096278
test_loss: 0.009413005
train_loss: 0.0022702443
test_loss: 0.009293051
train_loss: 0.002198957
test_loss: 0.00928915
train_loss: 0.00205284
test_loss: 0.009335835
train_loss: 0.002159451
test_loss: 0.009342197
train_loss: 0.0021769865
test_loss: 0.009307208
train_loss: 0.0021620628
test_loss: 0.009322197
train_loss: 0.002166375
test_loss: 0.009402676
train_loss: 0.0025627702
test_loss: 0.00935477
train_loss: 0.0023514205
test_loss: 0.009446222
train_loss: 0.0024226154
test_loss: 0.00935473
train_loss: 0.0023711158
test_loss: 0.009525014
train_loss: 0.0024306742
test_loss: 0.009374162
train_loss: 0.002241835
test_loss: 0.00938544
train_loss: 0.0024010562
test_loss: 0.009317581
train_loss: 0.0023310033
test_loss: 0.009389596
train_loss: 0.002135013
test_loss: 0.009385871
train_loss: 0.0020131809
test_loss: 0.009345654
train_loss: 0.0020850182
test_loss: 0.009289784
train_loss: 0.0020524412
test_loss: 0.00940021
train_loss: 0.0021574055
test_loss: 0.009443408
train_loss: 0.0022265762
test_loss: 0.009280802
train_loss: 0.002122152
test_loss: 0.009327524
train_loss: 0.0022057407
test_loss: 0.009393343
train_loss: 0.0021322668
test_loss: 0.009283356
train_loss: 0.0021889028
test_loss: 0.009305878
train_loss: 0.0020714942
test_loss: 0.0094397785
train_loss: 0.002021779
test_loss: 0.009322783
train_loss: 0.002130004
test_loss: 0.009472463
train_loss: 0.0024010495
test_loss: 0.009326694
train_loss: 0.0023951647
test_loss: 0.009399651
train_loss: 0.0023728302
test_loss: 0.00953139
train_loss: 0.0023333016
test_loss: 0.009566708
train_loss: 0.0025526106
test_loss: 0.009419952
train_loss: 0.0023990266
test_loss: 0.009444289
train_loss: 0.002573626
test_loss: 0.009457742
train_loss: 0.00237868
test_loss: 0.00940651
train_loss: 0.0025346002
test_loss: 0.009486124
train_loss: 0.0023936643
test_loss: 0.009335126
train_loss: 0.002301545
test_loss: 0.009452256
train_loss: 0.0022182066
test_loss: 0.009286257
train_loss: 0.0021460121
test_loss: 0.009367435
train_loss: 0.002314838
test_loss: 0.009288694
train_loss: 0.002536572
test_loss: 0.009616378
train_loss: 0.0024431564
test_loss: 0.009285188
train_loss: 0.0021304942
test_loss: 0.009396968
train_loss: 0.0021628146
test_loss: 0.009397021
train_loss: 0.002094763
test_loss: 0.0094752805
train_loss: 0.0022391009
test_loss: 0.009329363
train_loss: 0.0023276135
test_loss: 0.009483746
train_loss: 0.002453669
test_loss: 0.009431956
train_loss: 0.0024382574
test_loss: 0.009491231
train_loss: 0.0023043193
test_loss: 0.009358568
train_loss: 0.002082984
test_loss: 0.009404592
train_loss: 0.0022364138
test_loss: 0.009349326
train_loss: 0.0020506624
test_loss: 0.009371989
train_loss: 0.0021941657
test_loss: 0.009353222
train_loss: 0.0021279887
test_loss: 0.0094594695
train_loss: 0.0022614512
test_loss: 0.009372294
train_loss: 0.002214172
test_loss: 0.009491061
train_loss: 0.0020780668
test_loss: 0.009396633
train_loss: 0.0021482739
test_loss: 0.009316103
train_loss: 0.0021392684
test_loss: 0.009367619
train_loss: 0.0021994656
test_loss: 0.009434008
train_loss: 0.0023029873
test_loss: 0.00948966
train_loss: 0.0022249701
test_loss: 0.009425969
train_loss: 0.002137949
test_loss: 0.009368407
train_loss: 0.0020976486
test_loss: 0.009409045
train_loss: 0.0020776636
test_loss: 0.009356471
train_loss: 0.002129069
test_loss: 0.009458713
train_loss: 0.0021669995
test_loss: 0.009414609
train_loss: 0.002291399
test_loss: 0.009472455
train_loss: 0.002146738
test_loss: 