+ RUN=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ LAYERS=500_500_500_500_1
+ case $RUN in
+ PSI='0 1'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output69
+ for fn in f2
+ case $fn in
+ OPT=--alpha
+ for psi in $PSI
+ for layers in $LAYERS
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi0
+ date
Sat Nov  7 13:17:11 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi0/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi0_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi0_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi0_8000/500_500_500_500_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi0_8000/500_500_500_500_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi0_8000/500_500_500_500_1
++ basename experiments.final/script73
+ '[' -r STOP.script73 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi0/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi0.4
+ date
Sat Nov  7 13:17:11 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi0.4/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi0.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi0.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi0.4_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi0.4/500_500_500_500_1 --optimizer lbfgs --function f2 --psi 0 --alpha 0.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi0.4_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf86d6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf86d6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf86d6158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf86d6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf8675840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf86536a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf85da598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf85da0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf85f9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf858f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf85afc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf8700488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf8537840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf85039d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf84f7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf8503158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf8496378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf84540d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf8476840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf8476598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf841c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf841cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf8405bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf8442ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf85f7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf83a6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf21d0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf83a6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf2172c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf21c7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf2172d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf210b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf20e0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf20d1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf20d1d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2cf2069488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.12306216e-05
Iter: 2 loss: 8.94015557e-06
Iter: 3 loss: 8.93883407e-06
Iter: 4 loss: 7.88770922e-06
Iter: 5 loss: 1.03685661e-05
Iter: 6 loss: 7.50433901e-06
Iter: 7 loss: 7.04186186e-06
Iter: 8 loss: 6.72985425e-06
Iter: 9 loss: 6.55820713e-06
Iter: 10 loss: 6.16297802e-06
Iter: 11 loss: 6.10830693e-06
Iter: 12 loss: 5.90828586e-06
Iter: 13 loss: 5.84546433e-06
Iter: 14 loss: 5.72790304e-06
Iter: 15 loss: 5.53620339e-06
Iter: 16 loss: 5.92437618e-06
Iter: 17 loss: 5.45836156e-06
Iter: 18 loss: 5.17418312e-06
Iter: 19 loss: 6.33408945e-06
Iter: 20 loss: 5.11159396e-06
Iter: 21 loss: 4.92216896e-06
Iter: 22 loss: 4.9169e-06
Iter: 23 loss: 4.76900095e-06
Iter: 24 loss: 4.6585742e-06
Iter: 25 loss: 4.63954029e-06
Iter: 26 loss: 4.54699875e-06
Iter: 27 loss: 4.35477705e-06
Iter: 28 loss: 7.66655194e-06
Iter: 29 loss: 4.35049833e-06
Iter: 30 loss: 4.22515814e-06
Iter: 31 loss: 4.26327688e-06
Iter: 32 loss: 4.13544876e-06
Iter: 33 loss: 3.99254e-06
Iter: 34 loss: 4.48580886e-06
Iter: 35 loss: 3.95485222e-06
Iter: 36 loss: 3.85413296e-06
Iter: 37 loss: 3.85401609e-06
Iter: 38 loss: 3.77967172e-06
Iter: 39 loss: 3.67175949e-06
Iter: 40 loss: 3.66855829e-06
Iter: 41 loss: 3.53142013e-06
Iter: 42 loss: 4.21725599e-06
Iter: 43 loss: 3.50843857e-06
Iter: 44 loss: 3.3671638e-06
Iter: 45 loss: 4.28061867e-06
Iter: 46 loss: 3.3518179e-06
Iter: 47 loss: 3.29259865e-06
Iter: 48 loss: 3.2549533e-06
Iter: 49 loss: 3.23167433e-06
Iter: 50 loss: 3.1611587e-06
Iter: 51 loss: 4.00523095e-06
Iter: 52 loss: 3.16022124e-06
Iter: 53 loss: 3.08817357e-06
Iter: 54 loss: 3.07521827e-06
Iter: 55 loss: 3.02657304e-06
Iter: 56 loss: 2.95740756e-06
Iter: 57 loss: 3.16708292e-06
Iter: 58 loss: 2.93687572e-06
Iter: 59 loss: 2.8596146e-06
Iter: 60 loss: 3.41911436e-06
Iter: 61 loss: 2.85297574e-06
Iter: 62 loss: 2.8067368e-06
Iter: 63 loss: 2.72779334e-06
Iter: 64 loss: 2.72773195e-06
Iter: 65 loss: 2.66339816e-06
Iter: 66 loss: 3.07920436e-06
Iter: 67 loss: 2.65636641e-06
Iter: 68 loss: 2.61667083e-06
Iter: 69 loss: 2.6165319e-06
Iter: 70 loss: 2.58433965e-06
Iter: 71 loss: 2.58651539e-06
Iter: 72 loss: 2.5592044e-06
Iter: 73 loss: 2.53164467e-06
Iter: 74 loss: 2.59489661e-06
Iter: 75 loss: 2.52140057e-06
Iter: 76 loss: 2.49327923e-06
Iter: 77 loss: 2.78233347e-06
Iter: 78 loss: 2.49257232e-06
Iter: 79 loss: 2.46708601e-06
Iter: 80 loss: 2.42178271e-06
Iter: 81 loss: 3.55056704e-06
Iter: 82 loss: 2.42179431e-06
Iter: 83 loss: 2.39029669e-06
Iter: 84 loss: 2.78218658e-06
Iter: 85 loss: 2.39001361e-06
Iter: 86 loss: 2.36235269e-06
Iter: 87 loss: 2.4853739e-06
Iter: 88 loss: 2.35682865e-06
Iter: 89 loss: 2.33253968e-06
Iter: 90 loss: 2.29754551e-06
Iter: 91 loss: 2.29650072e-06
Iter: 92 loss: 2.28900535e-06
Iter: 93 loss: 2.27796863e-06
Iter: 94 loss: 2.26584211e-06
Iter: 95 loss: 2.24456926e-06
Iter: 96 loss: 2.24452856e-06
Iter: 97 loss: 2.21913115e-06
Iter: 98 loss: 2.20688344e-06
Iter: 99 loss: 2.19468598e-06
Iter: 100 loss: 2.17817842e-06
Iter: 101 loss: 2.17484148e-06
Iter: 102 loss: 2.15550222e-06
Iter: 103 loss: 2.16369244e-06
Iter: 104 loss: 2.14221404e-06
Iter: 105 loss: 2.11903966e-06
Iter: 106 loss: 2.10208282e-06
Iter: 107 loss: 2.09422819e-06
Iter: 108 loss: 2.08501774e-06
Iter: 109 loss: 2.0785692e-06
Iter: 110 loss: 2.06688037e-06
Iter: 111 loss: 2.06624827e-06
Iter: 112 loss: 2.0572711e-06
Iter: 113 loss: 2.04424828e-06
Iter: 114 loss: 2.04106414e-06
Iter: 115 loss: 2.03284799e-06
Iter: 116 loss: 2.02051706e-06
Iter: 117 loss: 2.01998137e-06
Iter: 118 loss: 2.0108248e-06
Iter: 119 loss: 1.99762508e-06
Iter: 120 loss: 1.99722808e-06
Iter: 121 loss: 1.98121552e-06
Iter: 122 loss: 2.07120797e-06
Iter: 123 loss: 1.97896497e-06
Iter: 124 loss: 1.95916937e-06
Iter: 125 loss: 1.98476073e-06
Iter: 126 loss: 1.94912309e-06
Iter: 127 loss: 1.93600499e-06
Iter: 128 loss: 1.92946209e-06
Iter: 129 loss: 1.92328844e-06
Iter: 130 loss: 1.90654714e-06
Iter: 131 loss: 2.00940258e-06
Iter: 132 loss: 1.90453068e-06
Iter: 133 loss: 1.89168463e-06
Iter: 134 loss: 2.09396012e-06
Iter: 135 loss: 1.89168009e-06
Iter: 136 loss: 1.88514264e-06
Iter: 137 loss: 1.87366527e-06
Iter: 138 loss: 1.87366902e-06
Iter: 139 loss: 1.86182535e-06
Iter: 140 loss: 1.94921586e-06
Iter: 141 loss: 1.86083093e-06
Iter: 142 loss: 1.84738974e-06
Iter: 143 loss: 1.88281024e-06
Iter: 144 loss: 1.84282089e-06
Iter: 145 loss: 1.83263512e-06
Iter: 146 loss: 1.82767462e-06
Iter: 147 loss: 1.82283156e-06
Iter: 148 loss: 1.81167161e-06
Iter: 149 loss: 1.96930205e-06
Iter: 150 loss: 1.81167286e-06
Iter: 151 loss: 1.80112102e-06
Iter: 152 loss: 1.80742393e-06
Iter: 153 loss: 1.79427434e-06
Iter: 154 loss: 1.78662935e-06
Iter: 155 loss: 1.81220219e-06
Iter: 156 loss: 1.78446953e-06
Iter: 157 loss: 1.77659058e-06
Iter: 158 loss: 1.83314216e-06
Iter: 159 loss: 1.77589209e-06
Iter: 160 loss: 1.76970775e-06
Iter: 161 loss: 1.75537184e-06
Iter: 162 loss: 1.93375035e-06
Iter: 163 loss: 1.75435628e-06
Iter: 164 loss: 1.74085903e-06
Iter: 165 loss: 1.81337623e-06
Iter: 166 loss: 1.73889794e-06
Iter: 167 loss: 1.73020544e-06
Iter: 168 loss: 1.73001172e-06
Iter: 169 loss: 1.72197861e-06
Iter: 170 loss: 1.7081486e-06
Iter: 171 loss: 1.70813905e-06
Iter: 172 loss: 1.6984086e-06
Iter: 173 loss: 1.77325103e-06
Iter: 174 loss: 1.69765042e-06
Iter: 175 loss: 1.69077316e-06
Iter: 176 loss: 1.78679124e-06
Iter: 177 loss: 1.69076202e-06
Iter: 178 loss: 1.68620215e-06
Iter: 179 loss: 1.67715734e-06
Iter: 180 loss: 1.85684758e-06
Iter: 181 loss: 1.67705025e-06
Iter: 182 loss: 1.67013184e-06
Iter: 183 loss: 1.76847584e-06
Iter: 184 loss: 1.67010376e-06
Iter: 185 loss: 1.66432903e-06
Iter: 186 loss: 1.69007797e-06
Iter: 187 loss: 1.6631e-06
Iter: 188 loss: 1.65838264e-06
Iter: 189 loss: 1.65046674e-06
Iter: 190 loss: 1.65046174e-06
Iter: 191 loss: 1.64508492e-06
Iter: 192 loss: 1.64433732e-06
Iter: 193 loss: 1.64004302e-06
Iter: 194 loss: 1.63287245e-06
Iter: 195 loss: 1.6328363e-06
Iter: 196 loss: 1.62430365e-06
Iter: 197 loss: 1.62393894e-06
Iter: 198 loss: 1.61737466e-06
Iter: 199 loss: 1.61667742e-06
Iter: 200 loss: 1.61345963e-06
Iter: 201 loss: 1.60974423e-06
Iter: 202 loss: 1.60693946e-06
Iter: 203 loss: 1.60569562e-06
Iter: 204 loss: 1.59995795e-06
Iter: 205 loss: 1.59436763e-06
Iter: 206 loss: 1.59318267e-06
Iter: 207 loss: 1.59119122e-06
Iter: 208 loss: 1.58866305e-06
Iter: 209 loss: 1.58523449e-06
Iter: 210 loss: 1.57877173e-06
Iter: 211 loss: 1.72814907e-06
Iter: 212 loss: 1.57878105e-06
Iter: 213 loss: 1.57093382e-06
Iter: 214 loss: 1.5796013e-06
Iter: 215 loss: 1.56672581e-06
Iter: 216 loss: 1.55955411e-06
Iter: 217 loss: 1.55937187e-06
Iter: 218 loss: 1.5551193e-06
Iter: 219 loss: 1.55174735e-06
Iter: 220 loss: 1.55042744e-06
Iter: 221 loss: 1.54627912e-06
Iter: 222 loss: 1.60241143e-06
Iter: 223 loss: 1.54624672e-06
Iter: 224 loss: 1.54224756e-06
Iter: 225 loss: 1.5428343e-06
Iter: 226 loss: 1.53924225e-06
Iter: 227 loss: 1.5356e-06
Iter: 228 loss: 1.53386304e-06
Iter: 229 loss: 1.53208362e-06
Iter: 230 loss: 1.52632208e-06
Iter: 231 loss: 1.54214592e-06
Iter: 232 loss: 1.52446785e-06
Iter: 233 loss: 1.51835604e-06
Iter: 234 loss: 1.60162358e-06
Iter: 235 loss: 1.51828135e-06
Iter: 236 loss: 1.51533152e-06
Iter: 237 loss: 1.50924552e-06
Iter: 238 loss: 1.6171756e-06
Iter: 239 loss: 1.50914832e-06
Iter: 240 loss: 1.50396249e-06
Iter: 241 loss: 1.57988848e-06
Iter: 242 loss: 1.50394408e-06
Iter: 243 loss: 1.4988666e-06
Iter: 244 loss: 1.512273e-06
Iter: 245 loss: 1.49715254e-06
Iter: 246 loss: 1.49451159e-06
Iter: 247 loss: 1.49325922e-06
Iter: 248 loss: 1.49198775e-06
Iter: 249 loss: 1.48846e-06
Iter: 250 loss: 1.53158885e-06
Iter: 251 loss: 1.48841877e-06
Iter: 252 loss: 1.48481672e-06
Iter: 253 loss: 1.48114168e-06
Iter: 254 loss: 1.48041181e-06
Iter: 255 loss: 1.47632659e-06
Iter: 256 loss: 1.50666301e-06
Iter: 257 loss: 1.47602691e-06
Iter: 258 loss: 1.47238438e-06
Iter: 259 loss: 1.48923402e-06
Iter: 260 loss: 1.47164371e-06
Iter: 261 loss: 1.46870423e-06
Iter: 262 loss: 1.46300408e-06
Iter: 263 loss: 1.5794526e-06
Iter: 264 loss: 1.46295213e-06
Iter: 265 loss: 1.45799959e-06
Iter: 266 loss: 1.49706625e-06
Iter: 267 loss: 1.45764432e-06
Iter: 268 loss: 1.45544936e-06
Iter: 269 loss: 1.45529373e-06
Iter: 270 loss: 1.45329955e-06
Iter: 271 loss: 1.44917271e-06
Iter: 272 loss: 1.5212629e-06
Iter: 273 loss: 1.44905891e-06
Iter: 274 loss: 1.44558658e-06
Iter: 275 loss: 1.45946069e-06
Iter: 276 loss: 1.44478668e-06
Iter: 277 loss: 1.44065598e-06
Iter: 278 loss: 1.46969796e-06
Iter: 279 loss: 1.44028581e-06
Iter: 280 loss: 1.43732905e-06
Iter: 281 loss: 1.43337127e-06
Iter: 282 loss: 1.43314924e-06
Iter: 283 loss: 1.43062994e-06
Iter: 284 loss: 1.43060356e-06
Iter: 285 loss: 1.42788065e-06
Iter: 286 loss: 1.42858175e-06
Iter: 287 loss: 1.42593012e-06
Iter: 288 loss: 1.42279146e-06
Iter: 289 loss: 1.42614772e-06
Iter: 290 loss: 1.42106592e-06
Iter: 291 loss: 1.41905332e-06
Iter: 292 loss: 1.41893349e-06
Iter: 293 loss: 1.41753083e-06
Iter: 294 loss: 1.41442706e-06
Iter: 295 loss: 1.45462582e-06
Iter: 296 loss: 1.41420719e-06
Iter: 297 loss: 1.4101264e-06
Iter: 298 loss: 1.41462863e-06
Iter: 299 loss: 1.40792599e-06
Iter: 300 loss: 1.40589009e-06
Iter: 301 loss: 1.40551697e-06
Iter: 302 loss: 1.4031823e-06
Iter: 303 loss: 1.40208476e-06
Iter: 304 loss: 1.40094028e-06
Iter: 305 loss: 1.39812846e-06
Iter: 306 loss: 1.39701024e-06
Iter: 307 loss: 1.39545489e-06
Iter: 308 loss: 1.3948129e-06
Iter: 309 loss: 1.39364329e-06
Iter: 310 loss: 1.39237432e-06
Iter: 311 loss: 1.38980886e-06
Iter: 312 loss: 1.43748548e-06
Iter: 313 loss: 1.38975133e-06
Iter: 314 loss: 1.38730024e-06
Iter: 315 loss: 1.39782696e-06
Iter: 316 loss: 1.38677524e-06
Iter: 317 loss: 1.38460541e-06
Iter: 318 loss: 1.41223438e-06
Iter: 319 loss: 1.38460507e-06
Iter: 320 loss: 1.38327596e-06
Iter: 321 loss: 1.3807819e-06
Iter: 322 loss: 1.43708212e-06
Iter: 323 loss: 1.38081009e-06
Iter: 324 loss: 1.37830932e-06
Iter: 325 loss: 1.41643534e-06
Iter: 326 loss: 1.37827772e-06
Iter: 327 loss: 1.37602046e-06
Iter: 328 loss: 1.37569521e-06
Iter: 329 loss: 1.37413053e-06
Iter: 330 loss: 1.37195082e-06
Iter: 331 loss: 1.37156826e-06
Iter: 332 loss: 1.37011943e-06
Iter: 333 loss: 1.36732569e-06
Iter: 334 loss: 1.39189046e-06
Iter: 335 loss: 1.36719609e-06
Iter: 336 loss: 1.3647284e-06
Iter: 337 loss: 1.38406745e-06
Iter: 338 loss: 1.36459698e-06
Iter: 339 loss: 1.36349979e-06
Iter: 340 loss: 1.36105155e-06
Iter: 341 loss: 1.3966694e-06
Iter: 342 loss: 1.36094377e-06
Iter: 343 loss: 1.35874598e-06
Iter: 344 loss: 1.35875746e-06
Iter: 345 loss: 1.35644791e-06
Iter: 346 loss: 1.35696678e-06
Iter: 347 loss: 1.35477592e-06
Iter: 348 loss: 1.35292544e-06
Iter: 349 loss: 1.35169444e-06
Iter: 350 loss: 1.35106984e-06
Iter: 351 loss: 1.34880963e-06
Iter: 352 loss: 1.34870402e-06
Iter: 353 loss: 1.34707318e-06
Iter: 354 loss: 1.34590255e-06
Iter: 355 loss: 1.34538436e-06
Iter: 356 loss: 1.34398465e-06
Iter: 357 loss: 1.36103927e-06
Iter: 358 loss: 1.34397669e-06
Iter: 359 loss: 1.34263837e-06
Iter: 360 loss: 1.34263246e-06
Iter: 361 loss: 1.34153515e-06
Iter: 362 loss: 1.33978267e-06
Iter: 363 loss: 1.33877347e-06
Iter: 364 loss: 1.33802018e-06
Iter: 365 loss: 1.33571098e-06
Iter: 366 loss: 1.34698269e-06
Iter: 367 loss: 1.33530261e-06
Iter: 368 loss: 1.33319384e-06
Iter: 369 loss: 1.35718869e-06
Iter: 370 loss: 1.33316757e-06
Iter: 371 loss: 1.33174569e-06
Iter: 372 loss: 1.32939681e-06
Iter: 373 loss: 1.3293909e-06
Iter: 374 loss: 1.32757259e-06
Iter: 375 loss: 1.34451057e-06
Iter: 376 loss: 1.32753507e-06
Iter: 377 loss: 1.3255974e-06
Iter: 378 loss: 1.33303911e-06
Iter: 379 loss: 1.32520336e-06
Iter: 380 loss: 1.32395598e-06
Iter: 381 loss: 1.32228774e-06
Iter: 382 loss: 1.32226523e-06
Iter: 383 loss: 1.32132084e-06
Iter: 384 loss: 1.32111904e-06
Iter: 385 loss: 1.32000241e-06
Iter: 386 loss: 1.31839124e-06
Iter: 387 loss: 1.31831803e-06
Iter: 388 loss: 1.31643674e-06
Iter: 389 loss: 1.32377e-06
Iter: 390 loss: 1.31597244e-06
Iter: 391 loss: 1.31400952e-06
Iter: 392 loss: 1.32734021e-06
Iter: 393 loss: 1.31383501e-06
Iter: 394 loss: 1.31262686e-06
Iter: 395 loss: 1.31111165e-06
Iter: 396 loss: 1.31100956e-06
Iter: 397 loss: 1.309043e-06
Iter: 398 loss: 1.31735283e-06
Iter: 399 loss: 1.30862645e-06
Iter: 400 loss: 1.30768422e-06
Iter: 401 loss: 1.30757235e-06
Iter: 402 loss: 1.30681985e-06
Iter: 403 loss: 1.30525473e-06
Iter: 404 loss: 1.33503488e-06
Iter: 405 loss: 1.30527974e-06
Iter: 406 loss: 1.30347576e-06
Iter: 407 loss: 1.30698197e-06
Iter: 408 loss: 1.30271724e-06
Iter: 409 loss: 1.30138437e-06
Iter: 410 loss: 1.30129797e-06
Iter: 411 loss: 1.30051637e-06
Iter: 412 loss: 1.29842647e-06
Iter: 413 loss: 1.31318779e-06
Iter: 414 loss: 1.29795706e-06
Iter: 415 loss: 1.29583168e-06
Iter: 416 loss: 1.32573666e-06
Iter: 417 loss: 1.29579757e-06
Iter: 418 loss: 1.2938832e-06
Iter: 419 loss: 1.30475735e-06
Iter: 420 loss: 1.2936373e-06
Iter: 421 loss: 1.29281148e-06
Iter: 422 loss: 1.29226328e-06
Iter: 423 loss: 1.29193302e-06
Iter: 424 loss: 1.29036493e-06
Iter: 425 loss: 1.30228227e-06
Iter: 426 loss: 1.29030218e-06
Iter: 427 loss: 1.28916054e-06
Iter: 428 loss: 1.28830266e-06
Iter: 429 loss: 1.28793272e-06
Iter: 430 loss: 1.28662111e-06
Iter: 431 loss: 1.29037505e-06
Iter: 432 loss: 1.28616023e-06
Iter: 433 loss: 1.28509737e-06
Iter: 434 loss: 1.30074704e-06
Iter: 435 loss: 1.28507281e-06
Iter: 436 loss: 1.28395629e-06
Iter: 437 loss: 1.28271745e-06
Iter: 438 loss: 1.28254283e-06
Iter: 439 loss: 1.28105376e-06
Iter: 440 loss: 1.28311092e-06
Iter: 441 loss: 1.28034014e-06
Iter: 442 loss: 1.27941894e-06
Iter: 443 loss: 1.27932185e-06
Iter: 444 loss: 1.27857629e-06
Iter: 445 loss: 1.2769051e-06
Iter: 446 loss: 1.30375906e-06
Iter: 447 loss: 1.27685519e-06
Iter: 448 loss: 1.27551152e-06
Iter: 449 loss: 1.28264423e-06
Iter: 450 loss: 1.27526391e-06
Iter: 451 loss: 1.27404985e-06
Iter: 452 loss: 1.28984129e-06
Iter: 453 loss: 1.27404337e-06
Iter: 454 loss: 1.27332635e-06
Iter: 455 loss: 1.27207375e-06
Iter: 456 loss: 1.27208682e-06
Iter: 457 loss: 1.27116641e-06
Iter: 458 loss: 1.27112094e-06
Iter: 459 loss: 1.27036583e-06
Iter: 460 loss: 1.26897862e-06
Iter: 461 loss: 1.26896884e-06
Iter: 462 loss: 1.26744885e-06
Iter: 463 loss: 1.27016733e-06
Iter: 464 loss: 1.26675388e-06
Iter: 465 loss: 1.26602276e-06
Iter: 466 loss: 1.26588111e-06
Iter: 467 loss: 1.26511532e-06
Iter: 468 loss: 1.26501118e-06
Iter: 469 loss: 1.2644897e-06
Iter: 470 loss: 1.263478e-06
Iter: 471 loss: 1.2623475e-06
Iter: 472 loss: 1.26220334e-06
Iter: 473 loss: 1.26183295e-06
Iter: 474 loss: 1.26143937e-06
Iter: 475 loss: 1.26080113e-06
Iter: 476 loss: 1.25941017e-06
Iter: 477 loss: 1.28353213e-06
Iter: 478 loss: 1.25936731e-06
Iter: 479 loss: 1.25768702e-06
Iter: 480 loss: 1.25773397e-06
Iter: 481 loss: 1.25635074e-06
Iter: 482 loss: 1.2560879e-06
Iter: 483 loss: 1.25531938e-06
Iter: 484 loss: 1.25462498e-06
Iter: 485 loss: 1.2533535e-06
Iter: 486 loss: 1.27600958e-06
Iter: 487 loss: 1.25325016e-06
Iter: 488 loss: 1.25231918e-06
Iter: 489 loss: 1.25229735e-06
Iter: 490 loss: 1.25143993e-06
Iter: 491 loss: 1.2513475e-06
Iter: 492 loss: 1.25067868e-06
Iter: 493 loss: 1.24982876e-06
Iter: 494 loss: 1.24949622e-06
Iter: 495 loss: 1.24903136e-06
Iter: 496 loss: 1.24783446e-06
Iter: 497 loss: 1.25907877e-06
Iter: 498 loss: 1.24776159e-06
Iter: 499 loss: 1.24659209e-06
Iter: 500 loss: 1.25057136e-06
Iter: 501 loss: 1.24627729e-06
Iter: 502 loss: 1.24545284e-06
Iter: 503 loss: 1.24425571e-06
Iter: 504 loss: 1.24420535e-06
Iter: 505 loss: 1.2429839e-06
Iter: 506 loss: 1.25801148e-06
Iter: 507 loss: 1.24298253e-06
Iter: 508 loss: 1.24169992e-06
Iter: 509 loss: 1.24304484e-06
Iter: 510 loss: 1.2409804e-06
Iter: 511 loss: 1.24006124e-06
Iter: 512 loss: 1.23925565e-06
Iter: 513 loss: 1.23901873e-06
Iter: 514 loss: 1.23829523e-06
Iter: 515 loss: 1.23817654e-06
Iter: 516 loss: 1.23733548e-06
Iter: 517 loss: 1.2363048e-06
Iter: 518 loss: 1.23624886e-06
Iter: 519 loss: 1.23521454e-06
Iter: 520 loss: 1.24087046e-06
Iter: 521 loss: 1.23511518e-06
Iter: 522 loss: 1.23391965e-06
Iter: 523 loss: 1.23596749e-06
Iter: 524 loss: 1.23344989e-06
Iter: 525 loss: 1.23237817e-06
Iter: 526 loss: 1.23160532e-06
Iter: 527 loss: 1.23126733e-06
Iter: 528 loss: 1.23018731e-06
Iter: 529 loss: 1.24407984e-06
Iter: 530 loss: 1.2302097e-06
Iter: 531 loss: 1.22926622e-06
Iter: 532 loss: 1.23391874e-06
Iter: 533 loss: 1.22915947e-06
Iter: 534 loss: 1.22841016e-06
Iter: 535 loss: 1.22725567e-06
Iter: 536 loss: 1.22724828e-06
Iter: 537 loss: 1.22640074e-06
Iter: 538 loss: 1.23653695e-06
Iter: 539 loss: 1.22638482e-06
Iter: 540 loss: 1.22557583e-06
Iter: 541 loss: 1.22906806e-06
Iter: 542 loss: 1.22540052e-06
Iter: 543 loss: 1.22477206e-06
Iter: 544 loss: 1.22334836e-06
Iter: 545 loss: 1.24376777e-06
Iter: 546 loss: 1.22326298e-06
Iter: 547 loss: 1.2224757e-06
Iter: 548 loss: 1.22241363e-06
Iter: 549 loss: 1.22145207e-06
Iter: 550 loss: 1.22185315e-06
Iter: 551 loss: 1.22077404e-06
Iter: 552 loss: 1.21980042e-06
Iter: 553 loss: 1.22070264e-06
Iter: 554 loss: 1.21935068e-06
Iter: 555 loss: 1.21841845e-06
Iter: 556 loss: 1.21844914e-06
Iter: 557 loss: 1.21797302e-06
Iter: 558 loss: 1.21698963e-06
Iter: 559 loss: 1.23355699e-06
Iter: 560 loss: 1.21694325e-06
Iter: 561 loss: 1.21584515e-06
Iter: 562 loss: 1.22242068e-06
Iter: 563 loss: 1.2157426e-06
Iter: 564 loss: 1.21500284e-06
Iter: 565 loss: 1.22594145e-06
Iter: 566 loss: 1.21499068e-06
Iter: 567 loss: 1.21443645e-06
Iter: 568 loss: 1.21369362e-06
Iter: 569 loss: 1.21368953e-06
Iter: 570 loss: 1.21260405e-06
Iter: 571 loss: 1.21366247e-06
Iter: 572 loss: 1.21195876e-06
Iter: 573 loss: 1.21082348e-06
Iter: 574 loss: 1.21082644e-06
Iter: 575 loss: 1.21020025e-06
Iter: 576 loss: 1.20917696e-06
Iter: 577 loss: 1.20915797e-06
Iter: 578 loss: 1.20815048e-06
Iter: 579 loss: 1.21128232e-06
Iter: 580 loss: 1.20784034e-06
Iter: 581 loss: 1.20675304e-06
Iter: 582 loss: 1.22032725e-06
Iter: 583 loss: 1.20670677e-06
Iter: 584 loss: 1.20623008e-06
Iter: 585 loss: 1.20573281e-06
Iter: 586 loss: 1.20560799e-06
Iter: 587 loss: 1.20488926e-06
Iter: 588 loss: 1.20487562e-06
Iter: 589 loss: 1.20433288e-06
Iter: 590 loss: 1.20325149e-06
Iter: 591 loss: 1.22672986e-06
Iter: 592 loss: 1.20323944e-06
Iter: 593 loss: 1.20229788e-06
Iter: 594 loss: 1.20724962e-06
Iter: 595 loss: 1.20217226e-06
Iter: 596 loss: 1.20156335e-06
Iter: 597 loss: 1.20155335e-06
Iter: 598 loss: 1.20105642e-06
Iter: 599 loss: 1.20046809e-06
Iter: 600 loss: 1.2004034e-06
Iter: 601 loss: 1.19965068e-06
Iter: 602 loss: 1.20095945e-06
Iter: 603 loss: 1.19932474e-06
Iter: 604 loss: 1.19869947e-06
Iter: 605 loss: 1.19869947e-06
Iter: 606 loss: 1.19818105e-06
Iter: 607 loss: 1.19723131e-06
Iter: 608 loss: 1.21793619e-06
Iter: 609 loss: 1.19721744e-06
Iter: 610 loss: 1.19633e-06
Iter: 611 loss: 1.19787501e-06
Iter: 612 loss: 1.1959512e-06
Iter: 613 loss: 1.19522599e-06
Iter: 614 loss: 1.19517267e-06
Iter: 615 loss: 1.19464573e-06
Iter: 616 loss: 1.19369929e-06
Iter: 617 loss: 1.21731796e-06
Iter: 618 loss: 1.19370452e-06
Iter: 619 loss: 1.19332674e-06
Iter: 620 loss: 1.19319725e-06
Iter: 621 loss: 1.19276069e-06
Iter: 622 loss: 1.19200831e-06
Iter: 623 loss: 1.20975346e-06
Iter: 624 loss: 1.1919775e-06
Iter: 625 loss: 1.1911352e-06
Iter: 626 loss: 1.19145261e-06
Iter: 627 loss: 1.19045626e-06
Iter: 628 loss: 1.19008928e-06
Iter: 629 loss: 1.18991102e-06
Iter: 630 loss: 1.18942148e-06
Iter: 631 loss: 1.18892149e-06
Iter: 632 loss: 1.18882213e-06
Iter: 633 loss: 1.18794344e-06
Iter: 634 loss: 1.1875627e-06
Iter: 635 loss: 1.18715434e-06
Iter: 636 loss: 1.18656146e-06
Iter: 637 loss: 1.1864164e-06
Iter: 638 loss: 1.18582739e-06
Iter: 639 loss: 1.18512366e-06
Iter: 640 loss: 1.18505409e-06
Iter: 641 loss: 1.18418654e-06
Iter: 642 loss: 1.18355956e-06
Iter: 643 loss: 1.18325738e-06
Iter: 644 loss: 1.18358525e-06
Iter: 645 loss: 1.18272396e-06
Iter: 646 loss: 1.1823696e-06
Iter: 647 loss: 1.18184425e-06
Iter: 648 loss: 1.18186165e-06
Iter: 649 loss: 1.18132675e-06
Iter: 650 loss: 1.18517846e-06
Iter: 651 loss: 1.18126741e-06
Iter: 652 loss: 1.18062121e-06
Iter: 653 loss: 1.18035041e-06
Iter: 654 loss: 1.17998366e-06
Iter: 655 loss: 1.179229e-06
Iter: 656 loss: 1.17915192e-06
Iter: 657 loss: 1.17856189e-06
Iter: 658 loss: 1.17775357e-06
Iter: 659 loss: 1.18978073e-06
Iter: 660 loss: 1.17773175e-06
Iter: 661 loss: 1.17686704e-06
Iter: 662 loss: 1.17842262e-06
Iter: 663 loss: 1.17642958e-06
Iter: 664 loss: 1.17581692e-06
Iter: 665 loss: 1.17550553e-06
Iter: 666 loss: 1.17521722e-06
Iter: 667 loss: 1.17447121e-06
Iter: 668 loss: 1.17448508e-06
Iter: 669 loss: 1.17381205e-06
Iter: 670 loss: 1.17409468e-06
Iter: 671 loss: 1.17334014e-06
Iter: 672 loss: 1.1728149e-06
Iter: 673 loss: 1.1721969e-06
Iter: 674 loss: 1.17211971e-06
Iter: 675 loss: 1.17135869e-06
Iter: 676 loss: 1.17136449e-06
Iter: 677 loss: 1.17059278e-06
Iter: 678 loss: 1.17024842e-06
Iter: 679 loss: 1.16983142e-06
Iter: 680 loss: 1.16913827e-06
Iter: 681 loss: 1.17304626e-06
Iter: 682 loss: 1.16904471e-06
Iter: 683 loss: 1.16825538e-06
Iter: 684 loss: 1.16973411e-06
Iter: 685 loss: 1.16795297e-06
Iter: 686 loss: 1.16720662e-06
Iter: 687 loss: 1.16625131e-06
Iter: 688 loss: 1.16620231e-06
Iter: 689 loss: 1.16537535e-06
Iter: 690 loss: 1.1753325e-06
Iter: 691 loss: 1.16536125e-06
Iter: 692 loss: 1.16465048e-06
Iter: 693 loss: 1.1699326e-06
Iter: 694 loss: 1.16458568e-06
Iter: 695 loss: 1.16405772e-06
Iter: 696 loss: 1.16306796e-06
Iter: 697 loss: 1.18268406e-06
Iter: 698 loss: 1.16306614e-06
Iter: 699 loss: 1.1624287e-06
Iter: 700 loss: 1.16240699e-06
Iter: 701 loss: 1.16185947e-06
Iter: 702 loss: 1.16302272e-06
Iter: 703 loss: 1.16161982e-06
Iter: 704 loss: 1.16108174e-06
Iter: 705 loss: 1.16017031e-06
Iter: 706 loss: 1.16017827e-06
Iter: 707 loss: 1.15945363e-06
Iter: 708 loss: 1.16729848e-06
Iter: 709 loss: 1.15945977e-06
Iter: 710 loss: 1.15874673e-06
Iter: 711 loss: 1.16291756e-06
Iter: 712 loss: 1.15864145e-06
Iter: 713 loss: 1.15819353e-06
Iter: 714 loss: 1.15762919e-06
Iter: 715 loss: 1.15761168e-06
Iter: 716 loss: 1.15698992e-06
Iter: 717 loss: 1.15698583e-06
Iter: 718 loss: 1.15664648e-06
Iter: 719 loss: 1.15600119e-06
Iter: 720 loss: 1.16696333e-06
Iter: 721 loss: 1.15600176e-06
Iter: 722 loss: 1.1551715e-06
Iter: 723 loss: 1.1567821e-06
Iter: 724 loss: 1.15482931e-06
Iter: 725 loss: 1.15440594e-06
Iter: 726 loss: 1.15432931e-06
Iter: 727 loss: 1.15395869e-06
Iter: 728 loss: 1.15320017e-06
Iter: 729 loss: 1.16524234e-06
Iter: 730 loss: 1.15316323e-06
Iter: 731 loss: 1.15243438e-06
Iter: 732 loss: 1.15670332e-06
Iter: 733 loss: 1.15231114e-06
Iter: 734 loss: 1.15175567e-06
Iter: 735 loss: 1.1602117e-06
Iter: 736 loss: 1.15174089e-06
Iter: 737 loss: 1.15145281e-06
Iter: 738 loss: 1.15084356e-06
Iter: 739 loss: 1.16231649e-06
Iter: 740 loss: 1.15082412e-06
Iter: 741 loss: 1.1500515e-06
Iter: 742 loss: 1.15069508e-06
Iter: 743 loss: 1.14966576e-06
Iter: 744 loss: 1.14945624e-06
Iter: 745 loss: 1.14917555e-06
Iter: 746 loss: 1.14884926e-06
Iter: 747 loss: 1.14806676e-06
Iter: 748 loss: 1.15551711e-06
Iter: 749 loss: 1.14792124e-06
Iter: 750 loss: 1.14740351e-06
Iter: 751 loss: 1.14732768e-06
Iter: 752 loss: 1.14686816e-06
Iter: 753 loss: 1.14622731e-06
Iter: 754 loss: 1.14620116e-06
Iter: 755 loss: 1.14563363e-06
Iter: 756 loss: 1.14691568e-06
Iter: 757 loss: 1.14540171e-06
Iter: 758 loss: 1.14502234e-06
Iter: 759 loss: 1.145022e-06
Iter: 760 loss: 1.14458874e-06
Iter: 761 loss: 1.14432828e-06
Iter: 762 loss: 1.1441532e-06
Iter: 763 loss: 1.14379588e-06
Iter: 764 loss: 1.14368243e-06
Iter: 765 loss: 1.14344573e-06
Iter: 766 loss: 1.14287627e-06
Iter: 767 loss: 1.15069975e-06
Iter: 768 loss: 1.1428707e-06
Iter: 769 loss: 1.14234808e-06
Iter: 770 loss: 1.1417153e-06
Iter: 771 loss: 1.1416688e-06
Iter: 772 loss: 1.14096702e-06
Iter: 773 loss: 1.1415309e-06
Iter: 774 loss: 1.14057161e-06
Iter: 775 loss: 1.14013324e-06
Iter: 776 loss: 1.14004661e-06
Iter: 777 loss: 1.13959277e-06
Iter: 778 loss: 1.13958106e-06
Iter: 779 loss: 1.13916144e-06
Iter: 780 loss: 1.13880662e-06
Iter: 781 loss: 1.14049794e-06
Iter: 782 loss: 1.13876263e-06
Iter: 783 loss: 1.13828889e-06
Iter: 784 loss: 1.13798944e-06
Iter: 785 loss: 1.13780322e-06
Iter: 786 loss: 1.13722103e-06
Iter: 787 loss: 1.13731676e-06
Iter: 788 loss: 1.13682881e-06
Iter: 789 loss: 1.13614067e-06
Iter: 790 loss: 1.14097395e-06
Iter: 791 loss: 1.13607257e-06
Iter: 792 loss: 1.13526528e-06
Iter: 793 loss: 1.13830117e-06
Iter: 794 loss: 1.13509327e-06
Iter: 795 loss: 1.13467274e-06
Iter: 796 loss: 1.13442763e-06
Iter: 797 loss: 1.13425165e-06
Iter: 798 loss: 1.13390342e-06
Iter: 799 loss: 1.13390945e-06
Iter: 800 loss: 1.13351678e-06
Iter: 801 loss: 1.13326792e-06
Iter: 802 loss: 1.13313149e-06
Iter: 803 loss: 1.13264991e-06
Iter: 804 loss: 1.13291821e-06
Iter: 805 loss: 1.13233978e-06
Iter: 806 loss: 1.13187093e-06
Iter: 807 loss: 1.1328691e-06
Iter: 808 loss: 1.13166379e-06
Iter: 809 loss: 1.13101305e-06
Iter: 810 loss: 1.13641249e-06
Iter: 811 loss: 1.13097485e-06
Iter: 812 loss: 1.13062072e-06
Iter: 813 loss: 1.13040096e-06
Iter: 814 loss: 1.13023918e-06
Iter: 815 loss: 1.12979569e-06
Iter: 816 loss: 1.1298149e-06
Iter: 817 loss: 1.12955729e-06
Iter: 818 loss: 1.12884061e-06
Iter: 819 loss: 1.13374949e-06
Iter: 820 loss: 1.12868111e-06
Iter: 821 loss: 1.1280456e-06
Iter: 822 loss: 1.13595922e-06
Iter: 823 loss: 1.12806572e-06
Iter: 824 loss: 1.12773103e-06
Iter: 825 loss: 1.12775251e-06
Iter: 826 loss: 1.12753651e-06
Iter: 827 loss: 1.12693533e-06
Iter: 828 loss: 1.13348074e-06
Iter: 829 loss: 1.12690293e-06
Iter: 830 loss: 1.12637667e-06
Iter: 831 loss: 1.13003614e-06
Iter: 832 loss: 1.12632563e-06
Iter: 833 loss: 1.12583484e-06
Iter: 834 loss: 1.13054966e-06
Iter: 835 loss: 1.12584689e-06
Iter: 836 loss: 1.12545149e-06
Iter: 837 loss: 1.12475254e-06
Iter: 838 loss: 1.13875217e-06
Iter: 839 loss: 1.12475459e-06
Iter: 840 loss: 1.12420628e-06
Iter: 841 loss: 1.12623798e-06
Iter: 842 loss: 1.12409964e-06
Iter: 843 loss: 1.12375119e-06
Iter: 844 loss: 1.1237637e-06
Iter: 845 loss: 1.12340695e-06
Iter: 846 loss: 1.12320856e-06
Iter: 847 loss: 1.12308908e-06
Iter: 848 loss: 1.12282578e-06
Iter: 849 loss: 1.12632642e-06
Iter: 850 loss: 1.12284965e-06
Iter: 851 loss: 1.12260295e-06
Iter: 852 loss: 1.12221892e-06
Iter: 853 loss: 1.12218504e-06
Iter: 854 loss: 1.12169641e-06
Iter: 855 loss: 1.1211298e-06
Iter: 856 loss: 1.12103476e-06
Iter: 857 loss: 1.12111593e-06
Iter: 858 loss: 1.1207394e-06
Iter: 859 loss: 1.12043983e-06
Iter: 860 loss: 1.12019256e-06
Iter: 861 loss: 1.12012401e-06
Iter: 862 loss: 1.11972042e-06
Iter: 863 loss: 1.11969348e-06
Iter: 864 loss: 1.1193697e-06
Iter: 865 loss: 1.11926965e-06
Iter: 866 loss: 1.11913118e-06
Iter: 867 loss: 1.11900465e-06
Iter: 868 loss: 1.11878148e-06
Iter: 869 loss: 1.11875909e-06
Iter: 870 loss: 1.1184768e-06
Iter: 871 loss: 1.11792372e-06
Iter: 872 loss: 1.11789836e-06
Iter: 873 loss: 1.11738098e-06
Iter: 874 loss: 1.12308271e-06
Iter: 875 loss: 1.11737745e-06
Iter: 876 loss: 1.11701343e-06
Iter: 877 loss: 1.11701593e-06
Iter: 878 loss: 1.1167723e-06
Iter: 879 loss: 1.11633119e-06
Iter: 880 loss: 1.12574753e-06
Iter: 881 loss: 1.11632539e-06
Iter: 882 loss: 1.11601651e-06
Iter: 883 loss: 1.1159982e-06
Iter: 884 loss: 1.11581744e-06
Iter: 885 loss: 1.11546115e-06
Iter: 886 loss: 1.12154862e-06
Iter: 887 loss: 1.11545546e-06
Iter: 888 loss: 1.1150812e-06
Iter: 889 loss: 1.11499378e-06
Iter: 890 loss: 1.11473071e-06
Iter: 891 loss: 1.11478255e-06
Iter: 892 loss: 1.11453824e-06
Iter: 893 loss: 1.11430484e-06
Iter: 894 loss: 1.1138809e-06
Iter: 895 loss: 1.12187058e-06
Iter: 896 loss: 1.11385725e-06
Iter: 897 loss: 1.11333111e-06
Iter: 898 loss: 1.1134141e-06
Iter: 899 loss: 1.11292024e-06
Iter: 900 loss: 1.11292843e-06
Iter: 901 loss: 1.11273812e-06
Iter: 902 loss: 1.11252177e-06
Iter: 903 loss: 1.11199438e-06
Iter: 904 loss: 1.11855638e-06
Iter: 905 loss: 1.11197119e-06
Iter: 906 loss: 1.11155566e-06
Iter: 907 loss: 1.11336135e-06
Iter: 908 loss: 1.11144868e-06
Iter: 909 loss: 1.11118129e-06
Iter: 910 loss: 1.11269287e-06
Iter: 911 loss: 1.11115287e-06
Iter: 912 loss: 1.11073962e-06
Iter: 913 loss: 1.11176655e-06
Iter: 914 loss: 1.11059467e-06
Iter: 915 loss: 1.11032432e-06
Iter: 916 loss: 1.11055226e-06
Iter: 917 loss: 1.11020006e-06
Iter: 918 loss: 1.10980011e-06
Iter: 919 loss: 1.11228087e-06
Iter: 920 loss: 1.10978067e-06
Iter: 921 loss: 1.10943813e-06
Iter: 922 loss: 1.10882706e-06
Iter: 923 loss: 1.10882445e-06
Iter: 924 loss: 1.10838141e-06
Iter: 925 loss: 1.11087195e-06
Iter: 926 loss: 1.10830115e-06
Iter: 927 loss: 1.10805126e-06
Iter: 928 loss: 1.1080549e-06
Iter: 929 loss: 1.10782707e-06
Iter: 930 loss: 1.1074394e-06
Iter: 931 loss: 1.10744804e-06
Iter: 932 loss: 1.10709129e-06
Iter: 933 loss: 1.10733049e-06
Iter: 934 loss: 1.10692963e-06
Iter: 935 loss: 1.10657015e-06
Iter: 936 loss: 1.10932478e-06
Iter: 937 loss: 1.10653195e-06
Iter: 938 loss: 1.10607346e-06
Iter: 939 loss: 1.10646181e-06
Iter: 940 loss: 1.10581448e-06
Iter: 941 loss: 1.10550877e-06
Iter: 942 loss: 1.10534438e-06
Iter: 943 loss: 1.10524081e-06
Iter: 944 loss: 1.10484939e-06
Iter: 945 loss: 1.10735459e-06
Iter: 946 loss: 1.10478561e-06
Iter: 947 loss: 1.10441499e-06
Iter: 948 loss: 1.10889175e-06
Iter: 949 loss: 1.10443079e-06
Iter: 950 loss: 1.10426049e-06
Iter: 951 loss: 1.10397218e-06
Iter: 952 loss: 1.11127781e-06
Iter: 953 loss: 1.10395479e-06
Iter: 954 loss: 1.1036509e-06
Iter: 955 loss: 1.10366454e-06
Iter: 956 loss: 1.1033934e-06
Iter: 957 loss: 1.10301971e-06
Iter: 958 loss: 1.10300448e-06
Iter: 959 loss: 1.10260748e-06
Iter: 960 loss: 1.10298436e-06
Iter: 961 loss: 1.10234862e-06
Iter: 962 loss: 1.10191604e-06
Iter: 963 loss: 1.10847111e-06
Iter: 964 loss: 1.10191638e-06
Iter: 965 loss: 1.10151427e-06
Iter: 966 loss: 1.10234168e-06
Iter: 967 loss: 1.1013675e-06
Iter: 968 loss: 1.10117423e-06
Iter: 969 loss: 1.10078236e-06
Iter: 970 loss: 1.1098914e-06
Iter: 971 loss: 1.10078167e-06
Iter: 972 loss: 1.10042117e-06
Iter: 973 loss: 1.10533722e-06
Iter: 974 loss: 1.10038741e-06
Iter: 975 loss: 1.0999554e-06
Iter: 976 loss: 1.10121323e-06
Iter: 977 loss: 1.09983421e-06
Iter: 978 loss: 1.09955522e-06
Iter: 979 loss: 1.09897417e-06
Iter: 980 loss: 1.10680412e-06
Iter: 981 loss: 1.09891153e-06
Iter: 982 loss: 1.09842176e-06
Iter: 983 loss: 1.09840971e-06
Iter: 984 loss: 1.09806558e-06
Iter: 985 loss: 1.10309406e-06
Iter: 986 loss: 1.09804137e-06
Iter: 987 loss: 1.09783309e-06
Iter: 988 loss: 1.09730763e-06
Iter: 989 loss: 1.1048229e-06
Iter: 990 loss: 1.09730081e-06
Iter: 991 loss: 1.09717928e-06
Iter: 992 loss: 1.09705957e-06
Iter: 993 loss: 1.09686971e-06
Iter: 994 loss: 1.09659209e-06
Iter: 995 loss: 1.10295855e-06
Iter: 996 loss: 1.09657594e-06
Iter: 997 loss: 1.0961727e-06
Iter: 998 loss: 1.09589769e-06
Iter: 999 loss: 1.09574103e-06
Iter: 1000 loss: 1.09547966e-06
Iter: 1001 loss: 1.09535767e-06
Iter: 1002 loss: 1.09504367e-06
Iter: 1003 loss: 1.09503412e-06
Iter: 1004 loss: 1.09476377e-06
Iter: 1005 loss: 1.0943636e-06
Iter: 1006 loss: 1.0938129e-06
Iter: 1007 loss: 1.09379494e-06
Iter: 1008 loss: 1.09357e-06
Iter: 1009 loss: 1.09342091e-06
Iter: 1010 loss: 1.09314931e-06
Iter: 1011 loss: 1.09366874e-06
Iter: 1012 loss: 1.09300709e-06
Iter: 1013 loss: 1.09280518e-06
Iter: 1014 loss: 1.0923153e-06
Iter: 1015 loss: 1.09931057e-06
Iter: 1016 loss: 1.09227403e-06
Iter: 1017 loss: 1.09189591e-06
Iter: 1018 loss: 1.09188932e-06
Iter: 1019 loss: 1.09149391e-06
Iter: 1020 loss: 1.09228972e-06
Iter: 1021 loss: 1.09135624e-06
Iter: 1022 loss: 1.09100051e-06
Iter: 1023 loss: 1.09050029e-06
Iter: 1024 loss: 1.09050643e-06
Iter: 1025 loss: 1.09027565e-06
Iter: 1026 loss: 1.09015707e-06
Iter: 1027 loss: 1.08996448e-06
Iter: 1028 loss: 1.08948495e-06
Iter: 1029 loss: 1.09508665e-06
Iter: 1030 loss: 1.08948905e-06
Iter: 1031 loss: 1.08893482e-06
Iter: 1032 loss: 1.08979657e-06
Iter: 1033 loss: 1.08868244e-06
Iter: 1034 loss: 1.08866334e-06
Iter: 1035 loss: 1.08839231e-06
Iter: 1036 loss: 1.0882286e-06
Iter: 1037 loss: 1.08765744e-06
Iter: 1038 loss: 1.09068935e-06
Iter: 1039 loss: 1.08742609e-06
Iter: 1040 loss: 1.08663698e-06
Iter: 1041 loss: 1.08906806e-06
Iter: 1042 loss: 1.08639006e-06
Iter: 1043 loss: 1.08612176e-06
Iter: 1044 loss: 1.08599522e-06
Iter: 1045 loss: 1.08573545e-06
Iter: 1046 loss: 1.08521215e-06
Iter: 1047 loss: 1.0943487e-06
Iter: 1048 loss: 1.08520317e-06
Iter: 1049 loss: 1.08454037e-06
Iter: 1050 loss: 1.08548193e-06
Iter: 1051 loss: 1.08423239e-06
Iter: 1052 loss: 1.08399627e-06
Iter: 1053 loss: 1.0839542e-06
Iter: 1054 loss: 1.08367442e-06
Iter: 1055 loss: 1.08354038e-06
Iter: 1056 loss: 1.08338827e-06
Iter: 1057 loss: 1.08295194e-06
Iter: 1058 loss: 1.08209667e-06
Iter: 1059 loss: 1.10137694e-06
Iter: 1060 loss: 1.08211179e-06
Iter: 1061 loss: 1.08171776e-06
Iter: 1062 loss: 1.08153358e-06
Iter: 1063 loss: 1.08118934e-06
Iter: 1064 loss: 1.08059919e-06
Iter: 1065 loss: 1.08057611e-06
Iter: 1066 loss: 1.07996743e-06
Iter: 1067 loss: 1.07992787e-06
Iter: 1068 loss: 1.07945266e-06
Iter: 1069 loss: 1.07955384e-06
Iter: 1070 loss: 1.07918834e-06
Iter: 1071 loss: 1.07895926e-06
Iter: 1072 loss: 1.07875644e-06
Iter: 1073 loss: 1.0786564e-06
Iter: 1074 loss: 1.078276e-06
Iter: 1075 loss: 1.07830147e-06
Iter: 1076 loss: 1.0779836e-06
Iter: 1077 loss: 1.07726009e-06
Iter: 1078 loss: 1.08163294e-06
Iter: 1079 loss: 1.07720462e-06
Iter: 1080 loss: 1.076839e-06
Iter: 1081 loss: 1.07621167e-06
Iter: 1082 loss: 1.07622668e-06
Iter: 1083 loss: 1.0754643e-06
Iter: 1084 loss: 1.07969e-06
Iter: 1085 loss: 1.07537358e-06
Iter: 1086 loss: 1.07470532e-06
Iter: 1087 loss: 1.07931817e-06
Iter: 1088 loss: 1.07464427e-06
Iter: 1089 loss: 1.07424466e-06
Iter: 1090 loss: 1.0744161e-06
Iter: 1091 loss: 1.07393271e-06
Iter: 1092 loss: 1.07358471e-06
Iter: 1093 loss: 1.0744759e-06
Iter: 1094 loss: 1.0734426e-06
Iter: 1095 loss: 1.07289475e-06
Iter: 1096 loss: 1.07536925e-06
Iter: 1097 loss: 1.07278493e-06
Iter: 1098 loss: 1.07242e-06
Iter: 1099 loss: 1.07195024e-06
Iter: 1100 loss: 1.07192147e-06
Iter: 1101 loss: 1.07122355e-06
Iter: 1102 loss: 1.07219557e-06
Iter: 1103 loss: 1.07091955e-06
Iter: 1104 loss: 1.07030667e-06
Iter: 1105 loss: 1.07023311e-06
Iter: 1106 loss: 1.06997413e-06
Iter: 1107 loss: 1.06960988e-06
Iter: 1108 loss: 1.06956065e-06
Iter: 1109 loss: 1.06921209e-06
Iter: 1110 loss: 1.06922857e-06
Iter: 1111 loss: 1.06886512e-06
Iter: 1112 loss: 1.06844846e-06
Iter: 1113 loss: 1.0683998e-06
Iter: 1114 loss: 1.06801633e-06
Iter: 1115 loss: 1.06974153e-06
Iter: 1116 loss: 1.06793948e-06
Iter: 1117 loss: 1.06758057e-06
Iter: 1118 loss: 1.07154597e-06
Iter: 1119 loss: 1.06757341e-06
Iter: 1120 loss: 1.06725452e-06
Iter: 1121 loss: 1.06672587e-06
Iter: 1122 loss: 1.06669268e-06
Iter: 1123 loss: 1.06611481e-06
Iter: 1124 loss: 1.06832897e-06
Iter: 1125 loss: 1.06594894e-06
Iter: 1126 loss: 1.06558628e-06
Iter: 1127 loss: 1.07139306e-06
Iter: 1128 loss: 1.06555524e-06
Iter: 1129 loss: 1.06525306e-06
Iter: 1130 loss: 1.06447612e-06
Iter: 1131 loss: 1.07942947e-06
Iter: 1132 loss: 1.06447283e-06
Iter: 1133 loss: 1.06397908e-06
Iter: 1134 loss: 1.0669346e-06
Iter: 1135 loss: 1.06389587e-06
Iter: 1136 loss: 1.06363404e-06
Iter: 1137 loss: 1.06362859e-06
Iter: 1138 loss: 1.06333096e-06
Iter: 1139 loss: 1.06273126e-06
Iter: 1140 loss: 1.07411915e-06
Iter: 1141 loss: 1.06271568e-06
Iter: 1142 loss: 1.06238826e-06
Iter: 1143 loss: 1.06237667e-06
Iter: 1144 loss: 1.06203765e-06
Iter: 1145 loss: 1.06206141e-06
Iter: 1146 loss: 1.06176537e-06
Iter: 1147 loss: 1.06136463e-06
Iter: 1148 loss: 1.06094194e-06
Iter: 1149 loss: 1.0608228e-06
Iter: 1150 loss: 1.06055904e-06
Iter: 1151 loss: 1.06046753e-06
Iter: 1152 loss: 1.06016068e-06
Iter: 1153 loss: 1.06039874e-06
Iter: 1154 loss: 1.05996901e-06
Iter: 1155 loss: 1.05967956e-06
Iter: 1156 loss: 1.05959134e-06
Iter: 1157 loss: 1.05941967e-06
Iter: 1158 loss: 1.05905087e-06
Iter: 1159 loss: 1.06275934e-06
Iter: 1160 loss: 1.05902552e-06
Iter: 1161 loss: 1.05867662e-06
Iter: 1162 loss: 1.05904917e-06
Iter: 1163 loss: 1.05843219e-06
Iter: 1164 loss: 1.05810716e-06
Iter: 1165 loss: 1.0573475e-06
Iter: 1166 loss: 1.07420556e-06
Iter: 1167 loss: 1.05734262e-06
Iter: 1168 loss: 1.05696245e-06
Iter: 1169 loss: 1.05690651e-06
Iter: 1170 loss: 1.05652737e-06
Iter: 1171 loss: 1.05849654e-06
Iter: 1172 loss: 1.05649906e-06
Iter: 1173 loss: 1.05627578e-06
Iter: 1174 loss: 1.0558889e-06
Iter: 1175 loss: 1.05589652e-06
Iter: 1176 loss: 1.05546678e-06
Iter: 1177 loss: 1.05545757e-06
Iter: 1178 loss: 1.05528625e-06
Iter: 1179 loss: 1.05495928e-06
Iter: 1180 loss: 1.05495712e-06
Iter: 1181 loss: 1.05451431e-06
Iter: 1182 loss: 1.05684876e-06
Iter: 1183 loss: 1.05446588e-06
Iter: 1184 loss: 1.05398681e-06
Iter: 1185 loss: 1.05503909e-06
Iter: 1186 loss: 1.05377694e-06
Iter: 1187 loss: 1.05330537e-06
Iter: 1188 loss: 1.05354616e-06
Iter: 1189 loss: 1.05301729e-06
Iter: 1190 loss: 1.05258187e-06
Iter: 1191 loss: 1.05536446e-06
Iter: 1192 loss: 1.05250524e-06
Iter: 1193 loss: 1.05206436e-06
Iter: 1194 loss: 1.05490949e-06
Iter: 1195 loss: 1.05203821e-06
Iter: 1196 loss: 1.05174092e-06
Iter: 1197 loss: 1.05145239e-06
Iter: 1198 loss: 1.05143158e-06
Iter: 1199 loss: 1.05102276e-06
Iter: 1200 loss: 1.0511443e-06
Iter: 1201 loss: 1.05077265e-06
Iter: 1202 loss: 1.05035e-06
Iter: 1203 loss: 1.05032109e-06
Iter: 1204 loss: 1.05007643e-06
Iter: 1205 loss: 1.04968876e-06
Iter: 1206 loss: 1.06017433e-06
Iter: 1207 loss: 1.04969524e-06
Iter: 1208 loss: 1.049302e-06
Iter: 1209 loss: 1.04931803e-06
Iter: 1210 loss: 1.04897867e-06
Iter: 1211 loss: 1.04830065e-06
Iter: 1212 loss: 1.06272842e-06
Iter: 1213 loss: 1.04831531e-06
Iter: 1214 loss: 1.04780827e-06
Iter: 1215 loss: 1.05208801e-06
Iter: 1216 loss: 1.04780952e-06
Iter: 1217 loss: 1.04746528e-06
Iter: 1218 loss: 1.05067909e-06
Iter: 1219 loss: 1.04746687e-06
Iter: 1220 loss: 1.04722244e-06
Iter: 1221 loss: 1.04678065e-06
Iter: 1222 loss: 1.04677451e-06
Iter: 1223 loss: 1.04624428e-06
Iter: 1224 loss: 1.04839955e-06
Iter: 1225 loss: 1.04614151e-06
Iter: 1226 loss: 1.04574713e-06
Iter: 1227 loss: 1.05145125e-06
Iter: 1228 loss: 1.04576407e-06
Iter: 1229 loss: 1.04538367e-06
Iter: 1230 loss: 1.04473338e-06
Iter: 1231 loss: 1.05649178e-06
Iter: 1232 loss: 1.0447086e-06
Iter: 1233 loss: 1.04397361e-06
Iter: 1234 loss: 1.04667288e-06
Iter: 1235 loss: 1.04385845e-06
Iter: 1236 loss: 1.04363312e-06
Iter: 1237 loss: 1.04354456e-06
Iter: 1238 loss: 1.04328012e-06
Iter: 1239 loss: 1.04276648e-06
Iter: 1240 loss: 1.0427666e-06
Iter: 1241 loss: 1.04239416e-06
Iter: 1242 loss: 1.04692776e-06
Iter: 1243 loss: 1.04240564e-06
Iter: 1244 loss: 1.04209607e-06
Iter: 1245 loss: 1.04253922e-06
Iter: 1246 loss: 1.04188598e-06
Iter: 1247 loss: 1.0415954e-06
Iter: 1248 loss: 1.04101059e-06
Iter: 1249 loss: 1.04102605e-06
Iter: 1250 loss: 1.0404965e-06
Iter: 1251 loss: 1.04048047e-06
Iter: 1252 loss: 1.04012258e-06
Iter: 1253 loss: 1.03984485e-06
Iter: 1254 loss: 1.0396792e-06
Iter: 1255 loss: 1.03914624e-06
Iter: 1256 loss: 1.03952971e-06
Iter: 1257 loss: 1.03880143e-06
Iter: 1258 loss: 1.03835782e-06
Iter: 1259 loss: 1.03835475e-06
Iter: 1260 loss: 1.03797356e-06
Iter: 1261 loss: 1.03824937e-06
Iter: 1262 loss: 1.0377571e-06
Iter: 1263 loss: 1.03739944e-06
Iter: 1264 loss: 1.03683874e-06
Iter: 1265 loss: 1.03683749e-06
Iter: 1266 loss: 1.03612479e-06
Iter: 1267 loss: 1.04321464e-06
Iter: 1268 loss: 1.03612479e-06
Iter: 1269 loss: 1.03553521e-06
Iter: 1270 loss: 1.03948e-06
Iter: 1271 loss: 1.03549939e-06
Iter: 1272 loss: 1.03515674e-06
Iter: 1273 loss: 1.03445768e-06
Iter: 1274 loss: 1.04961373e-06
Iter: 1275 loss: 1.03444063e-06
Iter: 1276 loss: 1.03374009e-06
Iter: 1277 loss: 1.03376738e-06
Iter: 1278 loss: 1.03341824e-06
Iter: 1279 loss: 1.03312595e-06
Iter: 1280 loss: 1.0330225e-06
Iter: 1281 loss: 1.03275738e-06
Iter: 1282 loss: 1.03629804e-06
Iter: 1283 loss: 1.03275465e-06
Iter: 1284 loss: 1.0324361e-06
Iter: 1285 loss: 1.03230786e-06
Iter: 1286 loss: 1.03210823e-06
Iter: 1287 loss: 1.03168463e-06
Iter: 1288 loss: 1.03170441e-06
Iter: 1289 loss: 1.03130992e-06
Iter: 1290 loss: 1.03078537e-06
Iter: 1291 loss: 1.03293883e-06
Iter: 1292 loss: 1.03068555e-06
Iter: 1293 loss: 1.02993135e-06
Iter: 1294 loss: 1.03326533e-06
Iter: 1295 loss: 1.02987883e-06
Iter: 1296 loss: 1.02938509e-06
Iter: 1297 loss: 1.02912122e-06
Iter: 1298 loss: 1.02887282e-06
Iter: 1299 loss: 1.02842171e-06
Iter: 1300 loss: 1.02965305e-06
Iter: 1301 loss: 1.02822742e-06
Iter: 1302 loss: 1.02793251e-06
Iter: 1303 loss: 1.0279249e-06
Iter: 1304 loss: 1.0277106e-06
Iter: 1305 loss: 1.0272397e-06
Iter: 1306 loss: 1.02723789e-06
Iter: 1307 loss: 1.02686226e-06
Iter: 1308 loss: 1.03074763e-06
Iter: 1309 loss: 1.02685931e-06
Iter: 1310 loss: 1.02636454e-06
Iter: 1311 loss: 1.02547733e-06
Iter: 1312 loss: 1.02548165e-06
Iter: 1313 loss: 1.02488434e-06
Iter: 1314 loss: 1.03098091e-06
Iter: 1315 loss: 1.02483978e-06
Iter: 1316 loss: 1.02439321e-06
Iter: 1317 loss: 1.02790136e-06
Iter: 1318 loss: 1.02435683e-06
Iter: 1319 loss: 1.02403374e-06
Iter: 1320 loss: 1.02337106e-06
Iter: 1321 loss: 1.03491163e-06
Iter: 1322 loss: 1.02335628e-06
Iter: 1323 loss: 1.02270883e-06
Iter: 1324 loss: 1.02673994e-06
Iter: 1325 loss: 1.0225998e-06
Iter: 1326 loss: 1.02224487e-06
Iter: 1327 loss: 1.02225761e-06
Iter: 1328 loss: 1.02179297e-06
Iter: 1329 loss: 1.02092054e-06
Iter: 1330 loss: 1.03452192e-06
Iter: 1331 loss: 1.0209194e-06
Iter: 1332 loss: 1.02002275e-06
Iter: 1333 loss: 1.02485501e-06
Iter: 1334 loss: 1.0198828e-06
Iter: 1335 loss: 1.01941839e-06
Iter: 1336 loss: 1.02533465e-06
Iter: 1337 loss: 1.01942032e-06
Iter: 1338 loss: 1.01884564e-06
Iter: 1339 loss: 1.01914281e-06
Iter: 1340 loss: 1.01848741e-06
Iter: 1341 loss: 1.01811565e-06
Iter: 1342 loss: 1.01998285e-06
Iter: 1343 loss: 1.01807291e-06
Iter: 1344 loss: 1.01770615e-06
Iter: 1345 loss: 1.01991793e-06
Iter: 1346 loss: 1.01768671e-06
Iter: 1347 loss: 1.01744342e-06
Iter: 1348 loss: 1.01683509e-06
Iter: 1349 loss: 1.02278955e-06
Iter: 1350 loss: 1.01672333e-06
Iter: 1351 loss: 1.01637499e-06
Iter: 1352 loss: 1.01628325e-06
Iter: 1353 loss: 1.0159373e-06
Iter: 1354 loss: 1.01548198e-06
Iter: 1355 loss: 1.01543264e-06
Iter: 1356 loss: 1.01480782e-06
Iter: 1357 loss: 1.01452292e-06
Iter: 1358 loss: 1.01421665e-06
Iter: 1359 loss: 1.01359876e-06
Iter: 1360 loss: 1.02098488e-06
Iter: 1361 loss: 1.01358216e-06
Iter: 1362 loss: 1.01326145e-06
Iter: 1363 loss: 1.01326168e-06
Iter: 1364 loss: 1.01299759e-06
Iter: 1365 loss: 1.01246246e-06
Iter: 1366 loss: 1.01955993e-06
Iter: 1367 loss: 1.01244677e-06
Iter: 1368 loss: 1.01179944e-06
Iter: 1369 loss: 1.01339083e-06
Iter: 1370 loss: 1.01160231e-06
Iter: 1371 loss: 1.01104285e-06
Iter: 1372 loss: 1.01542275e-06
Iter: 1373 loss: 1.0109818e-06
Iter: 1374 loss: 1.01029991e-06
Iter: 1375 loss: 1.01153432e-06
Iter: 1376 loss: 1.01002217e-06
Iter: 1377 loss: 1.00963985e-06
Iter: 1378 loss: 1.01050205e-06
Iter: 1379 loss: 1.00948762e-06
Iter: 1380 loss: 1.00906425e-06
Iter: 1381 loss: 1.01166609e-06
Iter: 1382 loss: 1.00902912e-06
Iter: 1383 loss: 1.00870784e-06
Iter: 1384 loss: 1.00831039e-06
Iter: 1385 loss: 1.00828356e-06
Iter: 1386 loss: 1.00815384e-06
Iter: 1387 loss: 1.0080928e-06
Iter: 1388 loss: 1.00787452e-06
Iter: 1389 loss: 1.00737861e-06
Iter: 1390 loss: 1.01236219e-06
Iter: 1391 loss: 1.00734246e-06
Iter: 1392 loss: 1.00665329e-06
Iter: 1393 loss: 1.00733723e-06
Iter: 1394 loss: 1.00628881e-06
Iter: 1395 loss: 1.00570742e-06
Iter: 1396 loss: 1.01171793e-06
Iter: 1397 loss: 1.00570037e-06
Iter: 1398 loss: 1.00513626e-06
Iter: 1399 loss: 1.00738657e-06
Iter: 1400 loss: 1.00500142e-06
Iter: 1401 loss: 1.00465672e-06
Iter: 1402 loss: 1.00448108e-06
Iter: 1403 loss: 1.004311e-06
Iter: 1404 loss: 1.00385569e-06
Iter: 1405 loss: 1.00359966e-06
Iter: 1406 loss: 1.00343118e-06
Iter: 1407 loss: 1.00316345e-06
Iter: 1408 loss: 1.00301145e-06
Iter: 1409 loss: 1.00263037e-06
Iter: 1410 loss: 1.00332977e-06
Iter: 1411 loss: 1.00248053e-06
Iter: 1412 loss: 1.00210696e-06
Iter: 1413 loss: 1.00147099e-06
Iter: 1414 loss: 1.00144416e-06
Iter: 1415 loss: 1.00107764e-06
Iter: 1416 loss: 1.00096793e-06
Iter: 1417 loss: 1.00069269e-06
Iter: 1418 loss: 1.00029774e-06
Iter: 1419 loss: 1.00028615e-06
Iter: 1420 loss: 9.99978283e-07
Iter: 1421 loss: 9.99984e-07
Iter: 1422 loss: 9.9964484e-07
Iter: 1423 loss: 9.99214649e-07
Iter: 1424 loss: 9.99208851e-07
Iter: 1425 loss: 9.98838232e-07
Iter: 1426 loss: 9.99010126e-07
Iter: 1427 loss: 9.98570613e-07
Iter: 1428 loss: 9.97917141e-07
Iter: 1429 loss: 9.98004907e-07
Iter: 1430 loss: 9.97430107e-07
Iter: 1431 loss: 9.97351094e-07
Iter: 1432 loss: 9.97028792e-07
Iter: 1433 loss: 9.96778795e-07
Iter: 1434 loss: 9.96310291e-07
Iter: 1435 loss: 1.00792465e-06
Iter: 1436 loss: 9.96288236e-07
Iter: 1437 loss: 9.95825417e-07
Iter: 1438 loss: 9.96161589e-07
Iter: 1439 loss: 9.95538585e-07
Iter: 1440 loss: 9.95224582e-07
Iter: 1441 loss: 9.95194e-07
Iter: 1442 loss: 9.94997094e-07
Iter: 1443 loss: 9.94575657e-07
Iter: 1444 loss: 1.00290231e-06
Iter: 1445 loss: 9.94577e-07
Iter: 1446 loss: 9.940361e-07
Iter: 1447 loss: 9.94672746e-07
Iter: 1448 loss: 9.93747562e-07
Iter: 1449 loss: 9.93442e-07
Iter: 1450 loss: 9.93367621e-07
Iter: 1451 loss: 9.9305214e-07
Iter: 1452 loss: 9.92446758e-07
Iter: 1453 loss: 9.92459263e-07
Iter: 1454 loss: 9.91900379e-07
Iter: 1455 loss: 9.97873826e-07
Iter: 1456 loss: 9.91941079e-07
Iter: 1457 loss: 9.91492e-07
Iter: 1458 loss: 9.92497689e-07
Iter: 1459 loss: 9.9131023e-07
Iter: 1460 loss: 9.91107072e-07
Iter: 1461 loss: 9.90683247e-07
Iter: 1462 loss: 9.90679496e-07
Iter: 1463 loss: 9.90130729e-07
Iter: 1464 loss: 9.90477133e-07
Iter: 1465 loss: 9.89764544e-07
Iter: 1466 loss: 9.89250225e-07
Iter: 1467 loss: 9.94145239e-07
Iter: 1468 loss: 9.89210207e-07
Iter: 1469 loss: 9.88667239e-07
Iter: 1470 loss: 9.93518597e-07
Iter: 1471 loss: 9.8865894e-07
Iter: 1472 loss: 9.88329248e-07
Iter: 1473 loss: 9.87904286e-07
Iter: 1474 loss: 9.87887802e-07
Iter: 1475 loss: 9.87409521e-07
Iter: 1476 loss: 9.87828571e-07
Iter: 1477 loss: 9.87162821e-07
Iter: 1478 loss: 9.87032308e-07
Iter: 1479 loss: 9.86924e-07
Iter: 1480 loss: 9.86663508e-07
Iter: 1481 loss: 9.86590635e-07
Iter: 1482 loss: 9.8651094e-07
Iter: 1483 loss: 9.86189434e-07
Iter: 1484 loss: 9.85462862e-07
Iter: 1485 loss: 9.94118409e-07
Iter: 1486 loss: 9.8538635e-07
Iter: 1487 loss: 9.85062229e-07
Iter: 1488 loss: 9.84912504e-07
Iter: 1489 loss: 9.84546659e-07
Iter: 1490 loss: 9.85984116e-07
Iter: 1491 loss: 9.84464577e-07
Iter: 1492 loss: 9.84213102e-07
Iter: 1493 loss: 9.83614427e-07
Iter: 1494 loss: 9.93432877e-07
Iter: 1495 loss: 9.83605446e-07
Iter: 1496 loss: 9.83391828e-07
Iter: 1497 loss: 9.83310883e-07
Iter: 1498 loss: 9.82999e-07
Iter: 1499 loss: 9.8304281e-07
Iter: 1500 loss: 9.82792244e-07
Iter: 1501 loss: 9.82503479e-07
Iter: 1502 loss: 9.82051347e-07
Iter: 1503 loss: 9.82048391e-07
Iter: 1504 loss: 9.81847734e-07
Iter: 1505 loss: 9.81745188e-07
Iter: 1506 loss: 9.81479388e-07
Iter: 1507 loss: 9.81105359e-07
Iter: 1508 loss: 9.81104449e-07
Iter: 1509 loss: 9.80652203e-07
Iter: 1510 loss: 9.80265327e-07
Iter: 1511 loss: 9.80116738e-07
Iter: 1512 loss: 9.79627885e-07
Iter: 1513 loss: 9.83777682e-07
Iter: 1514 loss: 9.79629704e-07
Iter: 1515 loss: 9.79346396e-07
Iter: 1516 loss: 9.79329229e-07
Iter: 1517 loss: 9.79076731e-07
Iter: 1518 loss: 9.78641197e-07
Iter: 1519 loss: 9.78636308e-07
Iter: 1520 loss: 9.78296612e-07
Iter: 1521 loss: 9.78603111e-07
Iter: 1522 loss: 9.7809334e-07
Iter: 1523 loss: 9.77626769e-07
Iter: 1524 loss: 9.83100108e-07
Iter: 1525 loss: 9.77595164e-07
Iter: 1526 loss: 9.77311629e-07
Iter: 1527 loss: 9.76873935e-07
Iter: 1528 loss: 9.7683585e-07
Iter: 1529 loss: 9.76647925e-07
Iter: 1530 loss: 9.76552e-07
Iter: 1531 loss: 9.76358706e-07
Iter: 1532 loss: 9.75879175e-07
Iter: 1533 loss: 9.79582e-07
Iter: 1534 loss: 9.75779585e-07
Iter: 1535 loss: 9.75485705e-07
Iter: 1536 loss: 9.75432386e-07
Iter: 1537 loss: 9.75182502e-07
Iter: 1538 loss: 9.77079708e-07
Iter: 1539 loss: 9.75171361e-07
Iter: 1540 loss: 9.74971385e-07
Iter: 1541 loss: 9.74543582e-07
Iter: 1542 loss: 9.79835e-07
Iter: 1543 loss: 9.74472073e-07
Iter: 1544 loss: 9.7398e-07
Iter: 1545 loss: 9.75966259e-07
Iter: 1546 loss: 9.73904662e-07
Iter: 1547 loss: 9.73396e-07
Iter: 1548 loss: 9.7448526e-07
Iter: 1549 loss: 9.7324812e-07
Iter: 1550 loss: 9.72726411e-07
Iter: 1551 loss: 9.80895e-07
Iter: 1552 loss: 9.7273869e-07
Iter: 1553 loss: 9.72574526e-07
Iter: 1554 loss: 9.72267344e-07
Iter: 1555 loss: 9.72265298e-07
Iter: 1556 loss: 9.72011776e-07
Iter: 1557 loss: 9.72006e-07
Iter: 1558 loss: 9.71784402e-07
Iter: 1559 loss: 9.71414806e-07
Iter: 1560 loss: 9.71401732e-07
Iter: 1561 loss: 9.71155828e-07
Iter: 1562 loss: 9.74659201e-07
Iter: 1563 loss: 9.71120244e-07
Iter: 1564 loss: 9.70825681e-07
Iter: 1565 loss: 9.70906626e-07
Iter: 1566 loss: 9.70627298e-07
Iter: 1567 loss: 9.70262e-07
Iter: 1568 loss: 9.70266e-07
Iter: 1569 loss: 9.70014298e-07
Iter: 1570 loss: 9.69793e-07
Iter: 1571 loss: 9.69712801e-07
Iter: 1572 loss: 9.69503162e-07
Iter: 1573 loss: 9.6932763e-07
Iter: 1574 loss: 9.69282496e-07
Iter: 1575 loss: 9.68955305e-07
Iter: 1576 loss: 9.68832637e-07
Iter: 1577 loss: 9.68697577e-07
Iter: 1578 loss: 9.68319796e-07
Iter: 1579 loss: 9.69963367e-07
Iter: 1580 loss: 9.68221116e-07
Iter: 1581 loss: 9.67969072e-07
Iter: 1582 loss: 9.67961455e-07
Iter: 1583 loss: 9.67729875e-07
Iter: 1584 loss: 9.67176561e-07
Iter: 1585 loss: 9.77842546e-07
Iter: 1586 loss: 9.67213509e-07
Iter: 1587 loss: 9.66874495e-07
Iter: 1588 loss: 9.69391522e-07
Iter: 1589 loss: 9.66845391e-07
Iter: 1590 loss: 9.6653946e-07
Iter: 1591 loss: 9.6810686e-07
Iter: 1592 loss: 9.6647841e-07
Iter: 1593 loss: 9.66225684e-07
Iter: 1594 loss: 9.65919526e-07
Iter: 1595 loss: 9.65898607e-07
Iter: 1596 loss: 9.6588883e-07
Iter: 1597 loss: 9.65754793e-07
Iter: 1598 loss: 9.65639629e-07
Iter: 1599 loss: 9.65298682e-07
Iter: 1600 loss: 9.66419293e-07
Iter: 1601 loss: 9.65148e-07
Iter: 1602 loss: 9.64612809e-07
Iter: 1603 loss: 9.68137329e-07
Iter: 1604 loss: 9.64556193e-07
Iter: 1605 loss: 9.64262e-07
Iter: 1606 loss: 9.64239803e-07
Iter: 1607 loss: 9.64090304e-07
Iter: 1608 loss: 9.63744e-07
Iter: 1609 loss: 9.66890411e-07
Iter: 1610 loss: 9.63690127e-07
Iter: 1611 loss: 9.63316552e-07
Iter: 1612 loss: 9.65276854e-07
Iter: 1613 loss: 9.6324311e-07
Iter: 1614 loss: 9.63018e-07
Iter: 1615 loss: 9.64444098e-07
Iter: 1616 loss: 9.62990725e-07
Iter: 1617 loss: 9.62783e-07
Iter: 1618 loss: 9.65509116e-07
Iter: 1619 loss: 9.62778586e-07
Iter: 1620 loss: 9.62665808e-07
Iter: 1621 loss: 9.62424565e-07
Iter: 1622 loss: 9.67725214e-07
Iter: 1623 loss: 9.62429567e-07
Iter: 1624 loss: 9.62241074e-07
Iter: 1625 loss: 9.64426135e-07
Iter: 1626 loss: 9.62233344e-07
Iter: 1627 loss: 9.61974365e-07
Iter: 1628 loss: 9.61713226e-07
Iter: 1629 loss: 9.6163626e-07
Iter: 1630 loss: 9.61401724e-07
Iter: 1631 loss: 9.62510285e-07
Iter: 1632 loss: 9.61323394e-07
Iter: 1633 loss: 9.61112391e-07
Iter: 1634 loss: 9.61098749e-07
Iter: 1635 loss: 9.61021556e-07
Iter: 1636 loss: 9.60713919e-07
Iter: 1637 loss: 9.62432068e-07
Iter: 1638 loss: 9.60557941e-07
Iter: 1639 loss: 9.60324314e-07
Iter: 1640 loss: 9.62726517e-07
Iter: 1641 loss: 9.60308284e-07
Iter: 1642 loss: 9.60113e-07
Iter: 1643 loss: 9.60139914e-07
Iter: 1644 loss: 9.5992209e-07
Iter: 1645 loss: 9.59587624e-07
Iter: 1646 loss: 9.65560503e-07
Iter: 1647 loss: 9.59562612e-07
Iter: 1648 loss: 9.59201e-07
Iter: 1649 loss: 9.59319323e-07
Iter: 1650 loss: 9.58964847e-07
Iter: 1651 loss: 9.5859582e-07
Iter: 1652 loss: 9.6201336e-07
Iter: 1653 loss: 9.58605142e-07
Iter: 1654 loss: 9.58519195e-07
Iter: 1655 loss: 9.58481223e-07
Iter: 1656 loss: 9.58361738e-07
Iter: 1657 loss: 9.58151077e-07
Iter: 1658 loss: 9.63348612e-07
Iter: 1659 loss: 9.58116402e-07
Iter: 1660 loss: 9.57913699e-07
Iter: 1661 loss: 9.58529426e-07
Iter: 1662 loss: 9.5782e-07
Iter: 1663 loss: 9.57657676e-07
Iter: 1664 loss: 9.59589e-07
Iter: 1665 loss: 9.5765381e-07
Iter: 1666 loss: 9.57476232e-07
Iter: 1667 loss: 9.57273528e-07
Iter: 1668 loss: 9.57241127e-07
Iter: 1669 loss: 9.56914164e-07
Iter: 1670 loss: 9.57360271e-07
Iter: 1671 loss: 9.56777512e-07
Iter: 1672 loss: 9.56629378e-07
Iter: 1673 loss: 9.56650524e-07
Iter: 1674 loss: 9.56474423e-07
Iter: 1675 loss: 9.56337431e-07
Iter: 1676 loss: 9.56292183e-07
Iter: 1677 loss: 9.56134841e-07
Iter: 1678 loss: 9.56534222e-07
Iter: 1679 loss: 9.56083568e-07
Iter: 1680 loss: 9.55941914e-07
Iter: 1681 loss: 9.58051e-07
Iter: 1682 loss: 9.55925088e-07
Iter: 1683 loss: 9.55856422e-07
Iter: 1684 loss: 9.55552764e-07
Iter: 1685 loss: 9.57265456e-07
Iter: 1686 loss: 9.55482e-07
Iter: 1687 loss: 9.55141218e-07
Iter: 1688 loss: 9.5584187e-07
Iter: 1689 loss: 9.54998541e-07
Iter: 1690 loss: 9.54848133e-07
Iter: 1691 loss: 9.54804705e-07
Iter: 1692 loss: 9.5459518e-07
Iter: 1693 loss: 9.54431925e-07
Iter: 1694 loss: 9.5436144e-07
Iter: 1695 loss: 9.54182156e-07
Iter: 1696 loss: 9.5420944e-07
Iter: 1697 loss: 9.54054258e-07
Iter: 1698 loss: 9.53826088e-07
Iter: 1699 loss: 9.5384371e-07
Iter: 1700 loss: 9.53643678e-07
Iter: 1701 loss: 9.53468941e-07
Iter: 1702 loss: 9.53463427e-07
Iter: 1703 loss: 9.53229573e-07
Iter: 1704 loss: 9.53569497e-07
Iter: 1705 loss: 9.53120832e-07
Iter: 1706 loss: 9.52907101e-07
Iter: 1707 loss: 9.56305485e-07
Iter: 1708 loss: 9.52897835e-07
Iter: 1709 loss: 9.52746518e-07
Iter: 1710 loss: 9.52519258e-07
Iter: 1711 loss: 9.52489813e-07
Iter: 1712 loss: 9.52281141e-07
Iter: 1713 loss: 9.52670916e-07
Iter: 1714 loss: 9.52135281e-07
Iter: 1715 loss: 9.51918935e-07
Iter: 1716 loss: 9.51938546e-07
Iter: 1717 loss: 9.51801667e-07
Iter: 1718 loss: 9.51571337e-07
Iter: 1719 loss: 9.51566278e-07
Iter: 1720 loss: 9.51330605e-07
Iter: 1721 loss: 9.51245738e-07
Iter: 1722 loss: 9.51083507e-07
Iter: 1723 loss: 9.50966182e-07
Iter: 1724 loss: 9.50881372e-07
Iter: 1725 loss: 9.5075228e-07
Iter: 1726 loss: 9.50497736e-07
Iter: 1727 loss: 9.50475794e-07
Iter: 1728 loss: 9.50216361e-07
Iter: 1729 loss: 9.50141953e-07
Iter: 1730 loss: 9.50016556e-07
Iter: 1731 loss: 9.4988826e-07
Iter: 1732 loss: 9.4980453e-07
Iter: 1733 loss: 9.49684363e-07
Iter: 1734 loss: 9.49473588e-07
Iter: 1735 loss: 9.54247e-07
Iter: 1736 loss: 9.49456819e-07
Iter: 1737 loss: 9.49198693e-07
Iter: 1738 loss: 9.49216428e-07
Iter: 1739 loss: 9.49031801e-07
Iter: 1740 loss: 9.48946763e-07
Iter: 1741 loss: 9.48820343e-07
Iter: 1742 loss: 9.48680224e-07
Iter: 1743 loss: 9.48441141e-07
Iter: 1744 loss: 9.48452e-07
Iter: 1745 loss: 9.48210072e-07
Iter: 1746 loss: 9.48062677e-07
Iter: 1747 loss: 9.4797997e-07
Iter: 1748 loss: 9.47775618e-07
Iter: 1749 loss: 9.47752426e-07
Iter: 1750 loss: 9.4757678e-07
Iter: 1751 loss: 9.48297895e-07
Iter: 1752 loss: 9.4752761e-07
Iter: 1753 loss: 9.47344233e-07
Iter: 1754 loss: 9.47026933e-07
Iter: 1755 loss: 9.47026365e-07
Iter: 1756 loss: 9.4674192e-07
Iter: 1757 loss: 9.46735327e-07
Iter: 1758 loss: 9.46551268e-07
Iter: 1759 loss: 9.47029207e-07
Iter: 1760 loss: 9.46487148e-07
Iter: 1761 loss: 9.46255e-07
Iter: 1762 loss: 9.45888814e-07
Iter: 1763 loss: 9.54656343e-07
Iter: 1764 loss: 9.45887e-07
Iter: 1765 loss: 9.459024e-07
Iter: 1766 loss: 9.4574574e-07
Iter: 1767 loss: 9.45614e-07
Iter: 1768 loss: 9.45361194e-07
Iter: 1769 loss: 9.50561173e-07
Iter: 1770 loss: 9.45363354e-07
Iter: 1771 loss: 9.45097668e-07
Iter: 1772 loss: 9.45325269e-07
Iter: 1773 loss: 9.44917929e-07
Iter: 1774 loss: 9.44695671e-07
Iter: 1775 loss: 9.45820261e-07
Iter: 1776 loss: 9.44614726e-07
Iter: 1777 loss: 9.44244448e-07
Iter: 1778 loss: 9.46123691e-07
Iter: 1779 loss: 9.44182e-07
Iter: 1780 loss: 9.44006501e-07
Iter: 1781 loss: 9.43848136e-07
Iter: 1782 loss: 9.43794e-07
Iter: 1783 loss: 9.43540954e-07
Iter: 1784 loss: 9.44867736e-07
Iter: 1785 loss: 9.43581142e-07
Iter: 1786 loss: 9.43280611e-07
Iter: 1787 loss: 9.44560554e-07
Iter: 1788 loss: 9.43172836e-07
Iter: 1789 loss: 9.43053806e-07
Iter: 1790 loss: 9.42888e-07
Iter: 1791 loss: 9.42857412e-07
Iter: 1792 loss: 9.42563247e-07
Iter: 1793 loss: 9.42390216e-07
Iter: 1794 loss: 9.4225976e-07
Iter: 1795 loss: 9.41998849e-07
Iter: 1796 loss: 9.41964117e-07
Iter: 1797 loss: 9.41724068e-07
Iter: 1798 loss: 9.43141799e-07
Iter: 1799 loss: 9.41681208e-07
Iter: 1800 loss: 9.41514656e-07
Iter: 1801 loss: 9.41065423e-07
Iter: 1802 loss: 9.45013369e-07
Iter: 1803 loss: 9.40978907e-07
Iter: 1804 loss: 9.40649329e-07
Iter: 1805 loss: 9.40631139e-07
Iter: 1806 loss: 9.4040206e-07
Iter: 1807 loss: 9.42687393e-07
Iter: 1808 loss: 9.40379e-07
Iter: 1809 loss: 9.40235225e-07
Iter: 1810 loss: 9.39821632e-07
Iter: 1811 loss: 9.43632244e-07
Iter: 1812 loss: 9.39756035e-07
Iter: 1813 loss: 9.3958613e-07
Iter: 1814 loss: 9.39531674e-07
Iter: 1815 loss: 9.39287418e-07
Iter: 1816 loss: 9.39527808e-07
Iter: 1817 loss: 9.39156052e-07
Iter: 1818 loss: 9.38945618e-07
Iter: 1819 loss: 9.38653102e-07
Iter: 1820 loss: 9.38643041e-07
Iter: 1821 loss: 9.38351718e-07
Iter: 1822 loss: 9.40573159e-07
Iter: 1823 loss: 9.38329492e-07
Iter: 1824 loss: 9.38064204e-07
Iter: 1825 loss: 9.39539063e-07
Iter: 1826 loss: 9.37995367e-07
Iter: 1827 loss: 9.37749633e-07
Iter: 1828 loss: 9.37552045e-07
Iter: 1829 loss: 9.37528227e-07
Iter: 1830 loss: 9.37287837e-07
Iter: 1831 loss: 9.40256029e-07
Iter: 1832 loss: 9.37285222e-07
Iter: 1833 loss: 9.37043e-07
Iter: 1834 loss: 9.371e-07
Iter: 1835 loss: 9.36868105e-07
Iter: 1836 loss: 9.36578317e-07
Iter: 1837 loss: 9.36631864e-07
Iter: 1838 loss: 9.36370043e-07
Iter: 1839 loss: 9.36070705e-07
Iter: 1840 loss: 9.36866456e-07
Iter: 1841 loss: 9.3601551e-07
Iter: 1842 loss: 9.35751871e-07
Iter: 1843 loss: 9.35770345e-07
Iter: 1844 loss: 9.35604817e-07
Iter: 1845 loss: 9.35184e-07
Iter: 1846 loss: 9.41764483e-07
Iter: 1847 loss: 9.35185199e-07
Iter: 1848 loss: 9.34832201e-07
Iter: 1849 loss: 9.36864e-07
Iter: 1850 loss: 9.34777233e-07
Iter: 1851 loss: 9.34416221e-07
Iter: 1852 loss: 9.36852587e-07
Iter: 1853 loss: 9.34389504e-07
Iter: 1854 loss: 9.34151899e-07
Iter: 1855 loss: 9.33666e-07
Iter: 1856 loss: 9.4250845e-07
Iter: 1857 loss: 9.33689762e-07
Iter: 1858 loss: 9.33209776e-07
Iter: 1859 loss: 9.3437211e-07
Iter: 1860 loss: 9.33077956e-07
Iter: 1861 loss: 9.32833188e-07
Iter: 1862 loss: 9.32779869e-07
Iter: 1863 loss: 9.32640148e-07
Iter: 1864 loss: 9.32389241e-07
Iter: 1865 loss: 9.32394e-07
Iter: 1866 loss: 9.32104399e-07
Iter: 1867 loss: 9.32887133e-07
Iter: 1868 loss: 9.32052899e-07
Iter: 1869 loss: 9.31654e-07
Iter: 1870 loss: 9.3252271e-07
Iter: 1871 loss: 9.31440184e-07
Iter: 1872 loss: 9.31192176e-07
Iter: 1873 loss: 9.31138118e-07
Iter: 1874 loss: 9.30944964e-07
Iter: 1875 loss: 9.30559679e-07
Iter: 1876 loss: 9.30562067e-07
Iter: 1877 loss: 9.3024596e-07
Iter: 1878 loss: 9.30045e-07
Iter: 1879 loss: 9.29973567e-07
Iter: 1880 loss: 9.29746875e-07
Iter: 1881 loss: 9.30148644e-07
Iter: 1882 loss: 9.29640123e-07
Iter: 1883 loss: 9.29418e-07
Iter: 1884 loss: 9.289692e-07
Iter: 1885 loss: 9.36291883e-07
Iter: 1886 loss: 9.28950328e-07
Iter: 1887 loss: 9.28824534e-07
Iter: 1888 loss: 9.28678e-07
Iter: 1889 loss: 9.28431291e-07
Iter: 1890 loss: 9.28167879e-07
Iter: 1891 loss: 9.28126724e-07
Iter: 1892 loss: 9.27772362e-07
Iter: 1893 loss: 9.27464441e-07
Iter: 1894 loss: 9.27326e-07
Iter: 1895 loss: 9.27049257e-07
Iter: 1896 loss: 9.27060967e-07
Iter: 1897 loss: 9.267211e-07
Iter: 1898 loss: 9.27164422e-07
Iter: 1899 loss: 9.26529765e-07
Iter: 1900 loss: 9.2622389e-07
Iter: 1901 loss: 9.26012092e-07
Iter: 1902 loss: 9.25859297e-07
Iter: 1903 loss: 9.25500103e-07
Iter: 1904 loss: 9.26751056e-07
Iter: 1905 loss: 9.25379538e-07
Iter: 1906 loss: 9.24962e-07
Iter: 1907 loss: 9.30162173e-07
Iter: 1908 loss: 9.24944572e-07
Iter: 1909 loss: 9.24726e-07
Iter: 1910 loss: 9.2434658e-07
Iter: 1911 loss: 9.24325718e-07
Iter: 1912 loss: 9.24035589e-07
Iter: 1913 loss: 9.2456014e-07
Iter: 1914 loss: 9.23902235e-07
Iter: 1915 loss: 9.23619211e-07
Iter: 1916 loss: 9.23608241e-07
Iter: 1917 loss: 9.23398943e-07
Iter: 1918 loss: 9.23710331e-07
Iter: 1919 loss: 9.23243761e-07
Iter: 1920 loss: 9.23021503e-07
Iter: 1921 loss: 9.2274513e-07
Iter: 1922 loss: 9.22737968e-07
Iter: 1923 loss: 9.22467507e-07
Iter: 1924 loss: 9.22434765e-07
Iter: 1925 loss: 9.22262927e-07
Iter: 1926 loss: 9.21963817e-07
Iter: 1927 loss: 9.21959781e-07
Iter: 1928 loss: 9.21638502e-07
Iter: 1929 loss: 9.21289882e-07
Iter: 1930 loss: 9.21236733e-07
Iter: 1931 loss: 9.20963544e-07
Iter: 1932 loss: 9.20931825e-07
Iter: 1933 loss: 9.20677735e-07
Iter: 1934 loss: 9.22499169e-07
Iter: 1935 loss: 9.20642378e-07
Iter: 1936 loss: 9.20474406e-07
Iter: 1937 loss: 9.19943773e-07
Iter: 1938 loss: 9.22050617e-07
Iter: 1939 loss: 9.19759088e-07
Iter: 1940 loss: 9.19544732e-07
Iter: 1941 loss: 9.19463503e-07
Iter: 1942 loss: 9.19165132e-07
Iter: 1943 loss: 9.20454966e-07
Iter: 1944 loss: 9.19115109e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script73
+ '[' -r STOP.script73 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi0.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi0.8
+ date
Sat Nov  7 13:44:10 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi0.8/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi0.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi0.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi0.8_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi0.8/500_500_500_500_1 --optimizer lbfgs --function f2 --psi 0 --alpha 0.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi0.8_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355e3e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355e3e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355ddcae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355ddc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355d989d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355d4e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355d21158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355d21598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355d47598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355d00bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355d00c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355e47840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355ea37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355ca1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355c49620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355e3ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355be8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355b92268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355be52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355be59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0355ba67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f034abec9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f034ac1cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f034ac1c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f034abdb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f034abdb488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f034ab9bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f034abec510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03206ce268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03206f2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f032065f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03206606a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f032063f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0320613a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0320613ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0320613158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.76271148e-05
Iter: 2 loss: 2.23693496e-05
Iter: 3 loss: 9.99986296e-05
Iter: 4 loss: 2.23646093e-05
Iter: 5 loss: 2.01943803e-05
Iter: 6 loss: 2.10857979e-05
Iter: 7 loss: 1.87082405e-05
Iter: 8 loss: 1.68912229e-05
Iter: 9 loss: 1.59909614e-05
Iter: 10 loss: 1.51254426e-05
Iter: 11 loss: 1.44921269e-05
Iter: 12 loss: 1.39382046e-05
Iter: 13 loss: 1.3294848e-05
Iter: 14 loss: 1.2767392e-05
Iter: 15 loss: 1.2579495e-05
Iter: 16 loss: 1.18893222e-05
Iter: 17 loss: 1.92819443e-05
Iter: 18 loss: 1.18728949e-05
Iter: 19 loss: 1.12215157e-05
Iter: 20 loss: 1.12549296e-05
Iter: 21 loss: 1.07092392e-05
Iter: 22 loss: 9.98816904e-06
Iter: 23 loss: 1.38202258e-05
Iter: 24 loss: 9.87827934e-06
Iter: 25 loss: 9.41166218e-06
Iter: 26 loss: 1.23230275e-05
Iter: 27 loss: 9.35769185e-06
Iter: 28 loss: 8.94476307e-06
Iter: 29 loss: 8.47561751e-06
Iter: 30 loss: 8.41536712e-06
Iter: 31 loss: 7.92145511e-06
Iter: 32 loss: 8.71780139e-06
Iter: 33 loss: 7.69394228e-06
Iter: 34 loss: 7.1668137e-06
Iter: 35 loss: 9.06971218e-06
Iter: 36 loss: 7.03309706e-06
Iter: 37 loss: 6.71556791e-06
Iter: 38 loss: 6.71538828e-06
Iter: 39 loss: 6.46066883e-06
Iter: 40 loss: 7.64691777e-06
Iter: 41 loss: 6.41331508e-06
Iter: 42 loss: 6.24552558e-06
Iter: 43 loss: 6.04840898e-06
Iter: 44 loss: 6.02645787e-06
Iter: 45 loss: 5.93660116e-06
Iter: 46 loss: 5.90254649e-06
Iter: 47 loss: 5.79001062e-06
Iter: 48 loss: 5.62352125e-06
Iter: 49 loss: 5.61946945e-06
Iter: 50 loss: 5.45403691e-06
Iter: 51 loss: 6.77985463e-06
Iter: 52 loss: 5.44298746e-06
Iter: 53 loss: 5.27835e-06
Iter: 54 loss: 5.67654479e-06
Iter: 55 loss: 5.21932634e-06
Iter: 56 loss: 5.099178e-06
Iter: 57 loss: 5.37823053e-06
Iter: 58 loss: 5.05481739e-06
Iter: 59 loss: 4.93338803e-06
Iter: 60 loss: 5.53158452e-06
Iter: 61 loss: 4.91265655e-06
Iter: 62 loss: 4.8190027e-06
Iter: 63 loss: 4.96177e-06
Iter: 64 loss: 4.77449203e-06
Iter: 65 loss: 4.68151757e-06
Iter: 66 loss: 4.69032739e-06
Iter: 67 loss: 4.60973297e-06
Iter: 68 loss: 4.49276922e-06
Iter: 69 loss: 4.73338787e-06
Iter: 70 loss: 4.44608759e-06
Iter: 71 loss: 4.32870638e-06
Iter: 72 loss: 4.47108414e-06
Iter: 73 loss: 4.26716861e-06
Iter: 74 loss: 4.14033411e-06
Iter: 75 loss: 4.74559056e-06
Iter: 76 loss: 4.11742076e-06
Iter: 77 loss: 4.02313708e-06
Iter: 78 loss: 4.65813537e-06
Iter: 79 loss: 4.01373245e-06
Iter: 80 loss: 3.96056112e-06
Iter: 81 loss: 3.95956204e-06
Iter: 82 loss: 3.91560388e-06
Iter: 83 loss: 3.85191561e-06
Iter: 84 loss: 3.84996747e-06
Iter: 85 loss: 3.79295329e-06
Iter: 86 loss: 4.1634321e-06
Iter: 87 loss: 3.78683126e-06
Iter: 88 loss: 3.72719342e-06
Iter: 89 loss: 4.00492263e-06
Iter: 90 loss: 3.71608121e-06
Iter: 91 loss: 3.67681605e-06
Iter: 92 loss: 3.66161771e-06
Iter: 93 loss: 3.6404108e-06
Iter: 94 loss: 3.58927923e-06
Iter: 95 loss: 4.18867558e-06
Iter: 96 loss: 3.58850912e-06
Iter: 97 loss: 3.55480188e-06
Iter: 98 loss: 3.53657765e-06
Iter: 99 loss: 3.52164352e-06
Iter: 100 loss: 3.47788409e-06
Iter: 101 loss: 3.66639051e-06
Iter: 102 loss: 3.46879324e-06
Iter: 103 loss: 3.43384818e-06
Iter: 104 loss: 3.69377653e-06
Iter: 105 loss: 3.43116221e-06
Iter: 106 loss: 3.39810208e-06
Iter: 107 loss: 3.43413194e-06
Iter: 108 loss: 3.38012865e-06
Iter: 109 loss: 3.34847937e-06
Iter: 110 loss: 3.32072204e-06
Iter: 111 loss: 3.31251704e-06
Iter: 112 loss: 3.26453892e-06
Iter: 113 loss: 3.36425273e-06
Iter: 114 loss: 3.24548182e-06
Iter: 115 loss: 3.21439416e-06
Iter: 116 loss: 3.21305879e-06
Iter: 117 loss: 3.1816694e-06
Iter: 118 loss: 3.22080564e-06
Iter: 119 loss: 3.16542514e-06
Iter: 120 loss: 3.14091267e-06
Iter: 121 loss: 3.13593455e-06
Iter: 122 loss: 3.11986514e-06
Iter: 123 loss: 3.09108214e-06
Iter: 124 loss: 3.09093457e-06
Iter: 125 loss: 3.07140635e-06
Iter: 126 loss: 3.05866365e-06
Iter: 127 loss: 3.05105686e-06
Iter: 128 loss: 3.02952321e-06
Iter: 129 loss: 3.17576723e-06
Iter: 130 loss: 3.02721446e-06
Iter: 131 loss: 3.00242914e-06
Iter: 132 loss: 3.00001648e-06
Iter: 133 loss: 2.98190071e-06
Iter: 134 loss: 2.95527116e-06
Iter: 135 loss: 3.01918453e-06
Iter: 136 loss: 2.94568349e-06
Iter: 137 loss: 2.92576806e-06
Iter: 138 loss: 3.16754131e-06
Iter: 139 loss: 2.92566551e-06
Iter: 140 loss: 2.90793923e-06
Iter: 141 loss: 2.92466075e-06
Iter: 142 loss: 2.89785385e-06
Iter: 143 loss: 2.87824173e-06
Iter: 144 loss: 2.89149784e-06
Iter: 145 loss: 2.86589625e-06
Iter: 146 loss: 2.8439481e-06
Iter: 147 loss: 2.85742931e-06
Iter: 148 loss: 2.82977499e-06
Iter: 149 loss: 2.80918107e-06
Iter: 150 loss: 3.09956749e-06
Iter: 151 loss: 2.80911331e-06
Iter: 152 loss: 2.78732841e-06
Iter: 153 loss: 2.81587859e-06
Iter: 154 loss: 2.77634308e-06
Iter: 155 loss: 2.76093442e-06
Iter: 156 loss: 2.76312926e-06
Iter: 157 loss: 2.74920626e-06
Iter: 158 loss: 2.73410069e-06
Iter: 159 loss: 2.73371961e-06
Iter: 160 loss: 2.72419243e-06
Iter: 161 loss: 2.71002432e-06
Iter: 162 loss: 2.70961482e-06
Iter: 163 loss: 2.69482871e-06
Iter: 164 loss: 2.83989084e-06
Iter: 165 loss: 2.69434759e-06
Iter: 166 loss: 2.67981591e-06
Iter: 167 loss: 2.68084273e-06
Iter: 168 loss: 2.66842335e-06
Iter: 169 loss: 2.65176595e-06
Iter: 170 loss: 2.67244832e-06
Iter: 171 loss: 2.64317623e-06
Iter: 172 loss: 2.63042557e-06
Iter: 173 loss: 2.63035668e-06
Iter: 174 loss: 2.61910964e-06
Iter: 175 loss: 2.6139387e-06
Iter: 176 loss: 2.60838942e-06
Iter: 177 loss: 2.5933e-06
Iter: 178 loss: 2.6233854e-06
Iter: 179 loss: 2.58721047e-06
Iter: 180 loss: 2.57202601e-06
Iter: 181 loss: 2.59615763e-06
Iter: 182 loss: 2.56505086e-06
Iter: 183 loss: 2.55249597e-06
Iter: 184 loss: 2.74706167e-06
Iter: 185 loss: 2.55256282e-06
Iter: 186 loss: 2.53995904e-06
Iter: 187 loss: 2.53062285e-06
Iter: 188 loss: 2.52645e-06
Iter: 189 loss: 2.51368147e-06
Iter: 190 loss: 2.57368856e-06
Iter: 191 loss: 2.51145411e-06
Iter: 192 loss: 2.50033827e-06
Iter: 193 loss: 2.604271e-06
Iter: 194 loss: 2.49982713e-06
Iter: 195 loss: 2.49232016e-06
Iter: 196 loss: 2.47854291e-06
Iter: 197 loss: 2.80356949e-06
Iter: 198 loss: 2.4785536e-06
Iter: 199 loss: 2.47066646e-06
Iter: 200 loss: 2.46980289e-06
Iter: 201 loss: 2.46277204e-06
Iter: 202 loss: 2.45590763e-06
Iter: 203 loss: 2.45448564e-06
Iter: 204 loss: 2.44377975e-06
Iter: 205 loss: 2.45855426e-06
Iter: 206 loss: 2.43853219e-06
Iter: 207 loss: 2.4291835e-06
Iter: 208 loss: 2.42901933e-06
Iter: 209 loss: 2.42266674e-06
Iter: 210 loss: 2.41367798e-06
Iter: 211 loss: 2.41337239e-06
Iter: 212 loss: 2.40007193e-06
Iter: 213 loss: 2.42479109e-06
Iter: 214 loss: 2.39440669e-06
Iter: 215 loss: 2.38373059e-06
Iter: 216 loss: 2.46602303e-06
Iter: 217 loss: 2.38298253e-06
Iter: 218 loss: 2.37460654e-06
Iter: 219 loss: 2.43909e-06
Iter: 220 loss: 2.37400923e-06
Iter: 221 loss: 2.36684355e-06
Iter: 222 loss: 2.35731113e-06
Iter: 223 loss: 2.35676976e-06
Iter: 224 loss: 2.34913523e-06
Iter: 225 loss: 2.3491325e-06
Iter: 226 loss: 2.34199797e-06
Iter: 227 loss: 2.3495702e-06
Iter: 228 loss: 2.3381408e-06
Iter: 229 loss: 2.33009268e-06
Iter: 230 loss: 2.32095954e-06
Iter: 231 loss: 2.31988452e-06
Iter: 232 loss: 2.31529543e-06
Iter: 233 loss: 2.31295826e-06
Iter: 234 loss: 2.30910155e-06
Iter: 235 loss: 2.29974876e-06
Iter: 236 loss: 2.39438259e-06
Iter: 237 loss: 2.29844318e-06
Iter: 238 loss: 2.28800718e-06
Iter: 239 loss: 2.39981637e-06
Iter: 240 loss: 2.28779481e-06
Iter: 241 loss: 2.27912096e-06
Iter: 242 loss: 2.3309085e-06
Iter: 243 loss: 2.27798773e-06
Iter: 244 loss: 2.27282317e-06
Iter: 245 loss: 2.26597194e-06
Iter: 246 loss: 2.26552356e-06
Iter: 247 loss: 2.25634335e-06
Iter: 248 loss: 2.30939713e-06
Iter: 249 loss: 2.25506756e-06
Iter: 250 loss: 2.2479453e-06
Iter: 251 loss: 2.30017395e-06
Iter: 252 loss: 2.24742507e-06
Iter: 253 loss: 2.24091809e-06
Iter: 254 loss: 2.25719918e-06
Iter: 255 loss: 2.23858524e-06
Iter: 256 loss: 2.23383927e-06
Iter: 257 loss: 2.23221173e-06
Iter: 258 loss: 2.22942163e-06
Iter: 259 loss: 2.22331346e-06
Iter: 260 loss: 2.3151415e-06
Iter: 261 loss: 2.22334097e-06
Iter: 262 loss: 2.21867276e-06
Iter: 263 loss: 2.212872e-06
Iter: 264 loss: 2.21232949e-06
Iter: 265 loss: 2.20551874e-06
Iter: 266 loss: 2.23773168e-06
Iter: 267 loss: 2.20425909e-06
Iter: 268 loss: 2.19729054e-06
Iter: 269 loss: 2.23360689e-06
Iter: 270 loss: 2.19614913e-06
Iter: 271 loss: 2.19101344e-06
Iter: 272 loss: 2.18167747e-06
Iter: 273 loss: 2.40626196e-06
Iter: 274 loss: 2.18165883e-06
Iter: 275 loss: 2.1783776e-06
Iter: 276 loss: 2.17641787e-06
Iter: 277 loss: 2.1721396e-06
Iter: 278 loss: 2.16879221e-06
Iter: 279 loss: 2.16759372e-06
Iter: 280 loss: 2.16132366e-06
Iter: 281 loss: 2.16029798e-06
Iter: 282 loss: 2.15607542e-06
Iter: 283 loss: 2.1484816e-06
Iter: 284 loss: 2.23254915e-06
Iter: 285 loss: 2.14832494e-06
Iter: 286 loss: 2.14258944e-06
Iter: 287 loss: 2.18046262e-06
Iter: 288 loss: 2.14196825e-06
Iter: 289 loss: 2.13741669e-06
Iter: 290 loss: 2.13702265e-06
Iter: 291 loss: 2.13364115e-06
Iter: 292 loss: 2.12837972e-06
Iter: 293 loss: 2.14336569e-06
Iter: 294 loss: 2.126548e-06
Iter: 295 loss: 2.12002033e-06
Iter: 296 loss: 2.15623231e-06
Iter: 297 loss: 2.11911583e-06
Iter: 298 loss: 2.11533234e-06
Iter: 299 loss: 2.11133101e-06
Iter: 300 loss: 2.11066117e-06
Iter: 301 loss: 2.10533017e-06
Iter: 302 loss: 2.17231695e-06
Iter: 303 loss: 2.10526468e-06
Iter: 304 loss: 2.10053076e-06
Iter: 305 loss: 2.10061e-06
Iter: 306 loss: 2.09670679e-06
Iter: 307 loss: 2.09138398e-06
Iter: 308 loss: 2.09416726e-06
Iter: 309 loss: 2.08788742e-06
Iter: 310 loss: 2.08187544e-06
Iter: 311 loss: 2.08187021e-06
Iter: 312 loss: 2.07782341e-06
Iter: 313 loss: 2.07403582e-06
Iter: 314 loss: 2.07305038e-06
Iter: 315 loss: 2.06774575e-06
Iter: 316 loss: 2.0703535e-06
Iter: 317 loss: 2.06416235e-06
Iter: 318 loss: 2.05865331e-06
Iter: 319 loss: 2.05864762e-06
Iter: 320 loss: 2.05387414e-06
Iter: 321 loss: 2.06125833e-06
Iter: 322 loss: 2.05170772e-06
Iter: 323 loss: 2.04756088e-06
Iter: 324 loss: 2.05317906e-06
Iter: 325 loss: 2.04559637e-06
Iter: 326 loss: 2.0412931e-06
Iter: 327 loss: 2.06791788e-06
Iter: 328 loss: 2.04082244e-06
Iter: 329 loss: 2.03625041e-06
Iter: 330 loss: 2.03789341e-06
Iter: 331 loss: 2.033079e-06
Iter: 332 loss: 2.02855699e-06
Iter: 333 loss: 2.03141781e-06
Iter: 334 loss: 2.02579395e-06
Iter: 335 loss: 2.02147589e-06
Iter: 336 loss: 2.02151091e-06
Iter: 337 loss: 2.01828675e-06
Iter: 338 loss: 2.01304624e-06
Iter: 339 loss: 2.01299758e-06
Iter: 340 loss: 2.00865588e-06
Iter: 341 loss: 2.06304776e-06
Iter: 342 loss: 2.00858426e-06
Iter: 343 loss: 2.00437e-06
Iter: 344 loss: 2.0131547e-06
Iter: 345 loss: 2.00263912e-06
Iter: 346 loss: 1.99916371e-06
Iter: 347 loss: 1.99697365e-06
Iter: 348 loss: 1.99561282e-06
Iter: 349 loss: 1.99011652e-06
Iter: 350 loss: 2.00406885e-06
Iter: 351 loss: 1.98827365e-06
Iter: 352 loss: 1.98504495e-06
Iter: 353 loss: 1.98472071e-06
Iter: 354 loss: 1.98221892e-06
Iter: 355 loss: 1.97842837e-06
Iter: 356 loss: 1.97833742e-06
Iter: 357 loss: 1.97406348e-06
Iter: 358 loss: 2.0011496e-06
Iter: 359 loss: 1.97347867e-06
Iter: 360 loss: 1.96934343e-06
Iter: 361 loss: 1.98499379e-06
Iter: 362 loss: 1.96831706e-06
Iter: 363 loss: 1.96468932e-06
Iter: 364 loss: 1.96568521e-06
Iter: 365 loss: 1.96211477e-06
Iter: 366 loss: 1.95784469e-06
Iter: 367 loss: 1.96577821e-06
Iter: 368 loss: 1.95610937e-06
Iter: 369 loss: 1.95131202e-06
Iter: 370 loss: 1.99152828e-06
Iter: 371 loss: 1.95104099e-06
Iter: 372 loss: 1.94834456e-06
Iter: 373 loss: 1.94454697e-06
Iter: 374 loss: 1.94440827e-06
Iter: 375 loss: 1.94176982e-06
Iter: 376 loss: 1.94150425e-06
Iter: 377 loss: 1.93884534e-06
Iter: 378 loss: 1.93405094e-06
Iter: 379 loss: 2.04625553e-06
Iter: 380 loss: 1.93405344e-06
Iter: 381 loss: 1.92935386e-06
Iter: 382 loss: 1.94369682e-06
Iter: 383 loss: 1.92789821e-06
Iter: 384 loss: 1.92425705e-06
Iter: 385 loss: 1.95496295e-06
Iter: 386 loss: 1.92407606e-06
Iter: 387 loss: 1.92026437e-06
Iter: 388 loss: 1.93207916e-06
Iter: 389 loss: 1.91909157e-06
Iter: 390 loss: 1.91598383e-06
Iter: 391 loss: 1.91502113e-06
Iter: 392 loss: 1.9132317e-06
Iter: 393 loss: 1.91028221e-06
Iter: 394 loss: 1.91021172e-06
Iter: 395 loss: 1.907963e-06
Iter: 396 loss: 1.90698324e-06
Iter: 397 loss: 1.90581807e-06
Iter: 398 loss: 1.90258709e-06
Iter: 399 loss: 1.90694186e-06
Iter: 400 loss: 1.90093363e-06
Iter: 401 loss: 1.89786238e-06
Iter: 402 loss: 1.92373e-06
Iter: 403 loss: 1.89771367e-06
Iter: 404 loss: 1.89470472e-06
Iter: 405 loss: 1.89603065e-06
Iter: 406 loss: 1.89264165e-06
Iter: 407 loss: 1.88976992e-06
Iter: 408 loss: 1.89741615e-06
Iter: 409 loss: 1.88885781e-06
Iter: 410 loss: 1.88659499e-06
Iter: 411 loss: 1.9180477e-06
Iter: 412 loss: 1.8865793e-06
Iter: 413 loss: 1.88461217e-06
Iter: 414 loss: 1.88003423e-06
Iter: 415 loss: 1.9286731e-06
Iter: 416 loss: 1.87945307e-06
Iter: 417 loss: 1.87417027e-06
Iter: 418 loss: 1.895737e-06
Iter: 419 loss: 1.87300805e-06
Iter: 420 loss: 1.87150147e-06
Iter: 421 loss: 1.87081127e-06
Iter: 422 loss: 1.86873547e-06
Iter: 423 loss: 1.86526472e-06
Iter: 424 loss: 1.86525642e-06
Iter: 425 loss: 1.86202442e-06
Iter: 426 loss: 1.88726574e-06
Iter: 427 loss: 1.8618191e-06
Iter: 428 loss: 1.85929525e-06
Iter: 429 loss: 1.87761816e-06
Iter: 430 loss: 1.85914678e-06
Iter: 431 loss: 1.85732415e-06
Iter: 432 loss: 1.85448835e-06
Iter: 433 loss: 1.85445481e-06
Iter: 434 loss: 1.85078659e-06
Iter: 435 loss: 1.87267574e-06
Iter: 436 loss: 1.85031809e-06
Iter: 437 loss: 1.84786495e-06
Iter: 438 loss: 1.87009687e-06
Iter: 439 loss: 1.84776741e-06
Iter: 440 loss: 1.84576595e-06
Iter: 441 loss: 1.84252576e-06
Iter: 442 loss: 1.84252804e-06
Iter: 443 loss: 1.83876227e-06
Iter: 444 loss: 1.87020066e-06
Iter: 445 loss: 1.83855173e-06
Iter: 446 loss: 1.83534473e-06
Iter: 447 loss: 1.85225736e-06
Iter: 448 loss: 1.83483962e-06
Iter: 449 loss: 1.83294208e-06
Iter: 450 loss: 1.82927317e-06
Iter: 451 loss: 1.90600122e-06
Iter: 452 loss: 1.82919712e-06
Iter: 453 loss: 1.8247872e-06
Iter: 454 loss: 1.85085412e-06
Iter: 455 loss: 1.82419853e-06
Iter: 456 loss: 1.82194526e-06
Iter: 457 loss: 1.82184476e-06
Iter: 458 loss: 1.82017413e-06
Iter: 459 loss: 1.81679366e-06
Iter: 460 loss: 1.87674516e-06
Iter: 461 loss: 1.81664018e-06
Iter: 462 loss: 1.81437838e-06
Iter: 463 loss: 1.81431187e-06
Iter: 464 loss: 1.81200153e-06
Iter: 465 loss: 1.811738e-06
Iter: 466 loss: 1.809884e-06
Iter: 467 loss: 1.80738095e-06
Iter: 468 loss: 1.80911661e-06
Iter: 469 loss: 1.80571942e-06
Iter: 470 loss: 1.80282768e-06
Iter: 471 loss: 1.83093221e-06
Iter: 472 loss: 1.80272264e-06
Iter: 473 loss: 1.80028951e-06
Iter: 474 loss: 1.80394773e-06
Iter: 475 loss: 1.79925541e-06
Iter: 476 loss: 1.79654819e-06
Iter: 477 loss: 1.79825702e-06
Iter: 478 loss: 1.79485994e-06
Iter: 479 loss: 1.79241761e-06
Iter: 480 loss: 1.82438339e-06
Iter: 481 loss: 1.79243023e-06
Iter: 482 loss: 1.79026688e-06
Iter: 483 loss: 1.78967321e-06
Iter: 484 loss: 1.78841378e-06
Iter: 485 loss: 1.78584992e-06
Iter: 486 loss: 1.78388723e-06
Iter: 487 loss: 1.78308744e-06
Iter: 488 loss: 1.78013556e-06
Iter: 489 loss: 1.82761551e-06
Iter: 490 loss: 1.78013158e-06
Iter: 491 loss: 1.77710024e-06
Iter: 492 loss: 1.78682205e-06
Iter: 493 loss: 1.77626e-06
Iter: 494 loss: 1.77433958e-06
Iter: 495 loss: 1.7737184e-06
Iter: 496 loss: 1.77265883e-06
Iter: 497 loss: 1.77060667e-06
Iter: 498 loss: 1.77061713e-06
Iter: 499 loss: 1.76885987e-06
Iter: 500 loss: 1.76611513e-06
Iter: 501 loss: 1.76612389e-06
Iter: 502 loss: 1.76328717e-06
Iter: 503 loss: 1.7731353e-06
Iter: 504 loss: 1.7625257e-06
Iter: 505 loss: 1.75997184e-06
Iter: 506 loss: 1.78489961e-06
Iter: 507 loss: 1.75980722e-06
Iter: 508 loss: 1.75797095e-06
Iter: 509 loss: 1.75751018e-06
Iter: 510 loss: 1.75634307e-06
Iter: 511 loss: 1.7536853e-06
Iter: 512 loss: 1.76326284e-06
Iter: 513 loss: 1.75308173e-06
Iter: 514 loss: 1.75067953e-06
Iter: 515 loss: 1.76861067e-06
Iter: 516 loss: 1.75045989e-06
Iter: 517 loss: 1.74856348e-06
Iter: 518 loss: 1.74731576e-06
Iter: 519 loss: 1.74661e-06
Iter: 520 loss: 1.74369757e-06
Iter: 521 loss: 1.74277011e-06
Iter: 522 loss: 1.74114621e-06
Iter: 523 loss: 1.73934052e-06
Iter: 524 loss: 1.73891067e-06
Iter: 525 loss: 1.73695889e-06
Iter: 526 loss: 1.73783565e-06
Iter: 527 loss: 1.73565422e-06
Iter: 528 loss: 1.73360195e-06
Iter: 529 loss: 1.73275521e-06
Iter: 530 loss: 1.73172862e-06
Iter: 531 loss: 1.73008289e-06
Iter: 532 loss: 1.72985256e-06
Iter: 533 loss: 1.72886803e-06
Iter: 534 loss: 1.72618331e-06
Iter: 535 loss: 1.74013053e-06
Iter: 536 loss: 1.72532918e-06
Iter: 537 loss: 1.72287491e-06
Iter: 538 loss: 1.72281204e-06
Iter: 539 loss: 1.72066655e-06
Iter: 540 loss: 1.72533294e-06
Iter: 541 loss: 1.71989177e-06
Iter: 542 loss: 1.71805436e-06
Iter: 543 loss: 1.71862666e-06
Iter: 544 loss: 1.71662737e-06
Iter: 545 loss: 1.71384636e-06
Iter: 546 loss: 1.72443174e-06
Iter: 547 loss: 1.71325792e-06
Iter: 548 loss: 1.710579e-06
Iter: 549 loss: 1.72347291e-06
Iter: 550 loss: 1.71013698e-06
Iter: 551 loss: 1.70839348e-06
Iter: 552 loss: 1.70670603e-06
Iter: 553 loss: 1.70638054e-06
Iter: 554 loss: 1.7035469e-06
Iter: 555 loss: 1.71536169e-06
Iter: 556 loss: 1.70291094e-06
Iter: 557 loss: 1.70107262e-06
Iter: 558 loss: 1.70104158e-06
Iter: 559 loss: 1.6996338e-06
Iter: 560 loss: 1.69726241e-06
Iter: 561 loss: 1.69723728e-06
Iter: 562 loss: 1.69522332e-06
Iter: 563 loss: 1.71810063e-06
Iter: 564 loss: 1.69526811e-06
Iter: 565 loss: 1.69321277e-06
Iter: 566 loss: 1.69627276e-06
Iter: 567 loss: 1.6922852e-06
Iter: 568 loss: 1.69048315e-06
Iter: 569 loss: 1.68785e-06
Iter: 570 loss: 1.68778206e-06
Iter: 571 loss: 1.68598422e-06
Iter: 572 loss: 1.68574e-06
Iter: 573 loss: 1.68403267e-06
Iter: 574 loss: 1.68295946e-06
Iter: 575 loss: 1.68222357e-06
Iter: 576 loss: 1.68006738e-06
Iter: 577 loss: 1.68922088e-06
Iter: 578 loss: 1.6796115e-06
Iter: 579 loss: 1.67743656e-06
Iter: 580 loss: 1.69047291e-06
Iter: 581 loss: 1.67722487e-06
Iter: 582 loss: 1.67544408e-06
Iter: 583 loss: 1.6744915e-06
Iter: 584 loss: 1.67368444e-06
Iter: 585 loss: 1.67093663e-06
Iter: 586 loss: 1.67541054e-06
Iter: 587 loss: 1.66969721e-06
Iter: 588 loss: 1.66748634e-06
Iter: 589 loss: 1.68444205e-06
Iter: 590 loss: 1.66734981e-06
Iter: 591 loss: 1.66523273e-06
Iter: 592 loss: 1.67631811e-06
Iter: 593 loss: 1.66487735e-06
Iter: 594 loss: 1.66350787e-06
Iter: 595 loss: 1.66181758e-06
Iter: 596 loss: 1.66169082e-06
Iter: 597 loss: 1.66054497e-06
Iter: 598 loss: 1.66033647e-06
Iter: 599 loss: 1.65919482e-06
Iter: 600 loss: 1.65717415e-06
Iter: 601 loss: 1.70656847e-06
Iter: 602 loss: 1.65717313e-06
Iter: 603 loss: 1.65494475e-06
Iter: 604 loss: 1.6582344e-06
Iter: 605 loss: 1.65386291e-06
Iter: 606 loss: 1.65161e-06
Iter: 607 loss: 1.65163976e-06
Iter: 608 loss: 1.65016945e-06
Iter: 609 loss: 1.64972073e-06
Iter: 610 loss: 1.64881703e-06
Iter: 611 loss: 1.64712014e-06
Iter: 612 loss: 1.65560357e-06
Iter: 613 loss: 1.64680546e-06
Iter: 614 loss: 1.64456776e-06
Iter: 615 loss: 1.64661105e-06
Iter: 616 loss: 1.64328685e-06
Iter: 617 loss: 1.6410836e-06
Iter: 618 loss: 1.64418111e-06
Iter: 619 loss: 1.64003882e-06
Iter: 620 loss: 1.6380236e-06
Iter: 621 loss: 1.64598328e-06
Iter: 622 loss: 1.63761592e-06
Iter: 623 loss: 1.6360658e-06
Iter: 624 loss: 1.6512397e-06
Iter: 625 loss: 1.63598e-06
Iter: 626 loss: 1.63441416e-06
Iter: 627 loss: 1.63399818e-06
Iter: 628 loss: 1.6330473e-06
Iter: 629 loss: 1.63146854e-06
Iter: 630 loss: 1.6347368e-06
Iter: 631 loss: 1.63081268e-06
Iter: 632 loss: 1.62889739e-06
Iter: 633 loss: 1.64416622e-06
Iter: 634 loss: 1.6287745e-06
Iter: 635 loss: 1.62738957e-06
Iter: 636 loss: 1.62497736e-06
Iter: 637 loss: 1.62499975e-06
Iter: 638 loss: 1.6229626e-06
Iter: 639 loss: 1.63951654e-06
Iter: 640 loss: 1.62286267e-06
Iter: 641 loss: 1.62059314e-06
Iter: 642 loss: 1.62670563e-06
Iter: 643 loss: 1.61999719e-06
Iter: 644 loss: 1.61832895e-06
Iter: 645 loss: 1.61910771e-06
Iter: 646 loss: 1.61740059e-06
Iter: 647 loss: 1.61562389e-06
Iter: 648 loss: 1.6367494e-06
Iter: 649 loss: 1.61561059e-06
Iter: 650 loss: 1.61409889e-06
Iter: 651 loss: 1.61242883e-06
Iter: 652 loss: 1.61224e-06
Iter: 653 loss: 1.60993579e-06
Iter: 654 loss: 1.61752405e-06
Iter: 655 loss: 1.60935178e-06
Iter: 656 loss: 1.60757054e-06
Iter: 657 loss: 1.62075116e-06
Iter: 658 loss: 1.60738466e-06
Iter: 659 loss: 1.605855e-06
Iter: 660 loss: 1.61568244e-06
Iter: 661 loss: 1.6057304e-06
Iter: 662 loss: 1.60460058e-06
Iter: 663 loss: 1.60317381e-06
Iter: 664 loss: 1.60309128e-06
Iter: 665 loss: 1.60170453e-06
Iter: 666 loss: 1.60169498e-06
Iter: 667 loss: 1.60036461e-06
Iter: 668 loss: 1.5995663e-06
Iter: 669 loss: 1.59900776e-06
Iter: 670 loss: 1.5972995e-06
Iter: 671 loss: 1.59752096e-06
Iter: 672 loss: 1.59597653e-06
Iter: 673 loss: 1.5942129e-06
Iter: 674 loss: 1.61893968e-06
Iter: 675 loss: 1.59423655e-06
Iter: 676 loss: 1.59257752e-06
Iter: 677 loss: 1.59369574e-06
Iter: 678 loss: 1.59153228e-06
Iter: 679 loss: 1.58991418e-06
Iter: 680 loss: 1.59157548e-06
Iter: 681 loss: 1.58898899e-06
Iter: 682 loss: 1.58711555e-06
Iter: 683 loss: 1.60766513e-06
Iter: 684 loss: 1.58702164e-06
Iter: 685 loss: 1.58592093e-06
Iter: 686 loss: 1.58401735e-06
Iter: 687 loss: 1.58400474e-06
Iter: 688 loss: 1.58178034e-06
Iter: 689 loss: 1.59446893e-06
Iter: 690 loss: 1.58152761e-06
Iter: 691 loss: 1.57972204e-06
Iter: 692 loss: 1.5980288e-06
Iter: 693 loss: 1.57965587e-06
Iter: 694 loss: 1.57820386e-06
Iter: 695 loss: 1.57851503e-06
Iter: 696 loss: 1.57708496e-06
Iter: 697 loss: 1.57555178e-06
Iter: 698 loss: 1.57913587e-06
Iter: 699 loss: 1.57501336e-06
Iter: 700 loss: 1.57358488e-06
Iter: 701 loss: 1.5883287e-06
Iter: 702 loss: 1.57353099e-06
Iter: 703 loss: 1.57238856e-06
Iter: 704 loss: 1.57018349e-06
Iter: 705 loss: 1.62078629e-06
Iter: 706 loss: 1.57019599e-06
Iter: 707 loss: 1.56830924e-06
Iter: 708 loss: 1.57740556e-06
Iter: 709 loss: 1.56792726e-06
Iter: 710 loss: 1.56661156e-06
Iter: 711 loss: 1.56662691e-06
Iter: 712 loss: 1.56551187e-06
Iter: 713 loss: 1.56324177e-06
Iter: 714 loss: 1.60351158e-06
Iter: 715 loss: 1.56324393e-06
Iter: 716 loss: 1.56182386e-06
Iter: 717 loss: 1.56175497e-06
Iter: 718 loss: 1.56036947e-06
Iter: 719 loss: 1.55933662e-06
Iter: 720 loss: 1.55886585e-06
Iter: 721 loss: 1.55709313e-06
Iter: 722 loss: 1.55930093e-06
Iter: 723 loss: 1.55621024e-06
Iter: 724 loss: 1.55447515e-06
Iter: 725 loss: 1.57017405e-06
Iter: 726 loss: 1.55435839e-06
Iter: 727 loss: 1.55277621e-06
Iter: 728 loss: 1.55840019e-06
Iter: 729 loss: 1.55229395e-06
Iter: 730 loss: 1.55110638e-06
Iter: 731 loss: 1.55137104e-06
Iter: 732 loss: 1.55025111e-06
Iter: 733 loss: 1.54880104e-06
Iter: 734 loss: 1.55969497e-06
Iter: 735 loss: 1.54868735e-06
Iter: 736 loss: 1.54731572e-06
Iter: 737 loss: 1.54767963e-06
Iter: 738 loss: 1.54627674e-06
Iter: 739 loss: 1.54437134e-06
Iter: 740 loss: 1.54340319e-06
Iter: 741 loss: 1.5425486e-06
Iter: 742 loss: 1.54097779e-06
Iter: 743 loss: 1.56402677e-06
Iter: 744 loss: 1.54099223e-06
Iter: 745 loss: 1.53947906e-06
Iter: 746 loss: 1.54249119e-06
Iter: 747 loss: 1.53889391e-06
Iter: 748 loss: 1.53746259e-06
Iter: 749 loss: 1.53694805e-06
Iter: 750 loss: 1.53615019e-06
Iter: 751 loss: 1.5347714e-06
Iter: 752 loss: 1.53473673e-06
Iter: 753 loss: 1.53387441e-06
Iter: 754 loss: 1.53197902e-06
Iter: 755 loss: 1.55849114e-06
Iter: 756 loss: 1.53192593e-06
Iter: 757 loss: 1.52970051e-06
Iter: 758 loss: 1.54106692e-06
Iter: 759 loss: 1.5293856e-06
Iter: 760 loss: 1.52799714e-06
Iter: 761 loss: 1.52793496e-06
Iter: 762 loss: 1.52712278e-06
Iter: 763 loss: 1.52578764e-06
Iter: 764 loss: 1.52572977e-06
Iter: 765 loss: 1.52406028e-06
Iter: 766 loss: 1.53291637e-06
Iter: 767 loss: 1.52376674e-06
Iter: 768 loss: 1.52232144e-06
Iter: 769 loss: 1.53315432e-06
Iter: 770 loss: 1.5221874e-06
Iter: 771 loss: 1.52119696e-06
Iter: 772 loss: 1.52054986e-06
Iter: 773 loss: 1.52017492e-06
Iter: 774 loss: 1.5187901e-06
Iter: 775 loss: 1.51994618e-06
Iter: 776 loss: 1.51795723e-06
Iter: 777 loss: 1.51666848e-06
Iter: 778 loss: 1.51664563e-06
Iter: 779 loss: 1.51568054e-06
Iter: 780 loss: 1.51437803e-06
Iter: 781 loss: 1.51427616e-06
Iter: 782 loss: 1.51294569e-06
Iter: 783 loss: 1.52746543e-06
Iter: 784 loss: 1.51289089e-06
Iter: 785 loss: 1.51166489e-06
Iter: 786 loss: 1.51326105e-06
Iter: 787 loss: 1.51101858e-06
Iter: 788 loss: 1.50981043e-06
Iter: 789 loss: 1.50873575e-06
Iter: 790 loss: 1.50849655e-06
Iter: 791 loss: 1.50707569e-06
Iter: 792 loss: 1.50702556e-06
Iter: 793 loss: 1.50568792e-06
Iter: 794 loss: 1.50772235e-06
Iter: 795 loss: 1.50495862e-06
Iter: 796 loss: 1.50383164e-06
Iter: 797 loss: 1.50383278e-06
Iter: 798 loss: 1.5029583e-06
Iter: 799 loss: 1.50152607e-06
Iter: 800 loss: 1.51914492e-06
Iter: 801 loss: 1.5014989e-06
Iter: 802 loss: 1.50037795e-06
Iter: 803 loss: 1.5002604e-06
Iter: 804 loss: 1.49945549e-06
Iter: 805 loss: 1.49814468e-06
Iter: 806 loss: 1.50096821e-06
Iter: 807 loss: 1.49766106e-06
Iter: 808 loss: 1.49645598e-06
Iter: 809 loss: 1.49886637e-06
Iter: 810 loss: 1.49598645e-06
Iter: 811 loss: 1.49434447e-06
Iter: 812 loss: 1.50409767e-06
Iter: 813 loss: 1.49414734e-06
Iter: 814 loss: 1.49314019e-06
Iter: 815 loss: 1.49243829e-06
Iter: 816 loss: 1.49206664e-06
Iter: 817 loss: 1.49079619e-06
Iter: 818 loss: 1.51028485e-06
Iter: 819 loss: 1.49076641e-06
Iter: 820 loss: 1.48975e-06
Iter: 821 loss: 1.48759341e-06
Iter: 822 loss: 1.53110147e-06
Iter: 823 loss: 1.48757545e-06
Iter: 824 loss: 1.48614072e-06
Iter: 825 loss: 1.50363167e-06
Iter: 826 loss: 1.48615322e-06
Iter: 827 loss: 1.48501704e-06
Iter: 828 loss: 1.49586776e-06
Iter: 829 loss: 1.48498521e-06
Iter: 830 loss: 1.48406912e-06
Iter: 831 loss: 1.48246545e-06
Iter: 832 loss: 1.51729637e-06
Iter: 833 loss: 1.48241213e-06
Iter: 834 loss: 1.48125559e-06
Iter: 835 loss: 1.48121546e-06
Iter: 836 loss: 1.48021832e-06
Iter: 837 loss: 1.48142237e-06
Iter: 838 loss: 1.47965329e-06
Iter: 839 loss: 1.47854712e-06
Iter: 840 loss: 1.47775e-06
Iter: 841 loss: 1.47729895e-06
Iter: 842 loss: 1.47557034e-06
Iter: 843 loss: 1.48217634e-06
Iter: 844 loss: 1.47508536e-06
Iter: 845 loss: 1.47385595e-06
Iter: 846 loss: 1.49016387e-06
Iter: 847 loss: 1.47381411e-06
Iter: 848 loss: 1.47286926e-06
Iter: 849 loss: 1.47183664e-06
Iter: 850 loss: 1.47160699e-06
Iter: 851 loss: 1.47029937e-06
Iter: 852 loss: 1.48113168e-06
Iter: 853 loss: 1.47022138e-06
Iter: 854 loss: 1.46906291e-06
Iter: 855 loss: 1.4728854e-06
Iter: 856 loss: 1.4687073e-06
Iter: 857 loss: 1.46773164e-06
Iter: 858 loss: 1.46600792e-06
Iter: 859 loss: 1.50913809e-06
Iter: 860 loss: 1.46598347e-06
Iter: 861 loss: 1.46478624e-06
Iter: 862 loss: 1.46466778e-06
Iter: 863 loss: 1.46333105e-06
Iter: 864 loss: 1.46406433e-06
Iter: 865 loss: 1.4624834e-06
Iter: 866 loss: 1.46137234e-06
Iter: 867 loss: 1.4619302e-06
Iter: 868 loss: 1.46062825e-06
Iter: 869 loss: 1.45938577e-06
Iter: 870 loss: 1.47863398e-06
Iter: 871 loss: 1.45938691e-06
Iter: 872 loss: 1.45858257e-06
Iter: 873 loss: 1.45774595e-06
Iter: 874 loss: 1.4575462e-06
Iter: 875 loss: 1.45644276e-06
Iter: 876 loss: 1.45995091e-06
Iter: 877 loss: 1.45611887e-06
Iter: 878 loss: 1.454894e-06
Iter: 879 loss: 1.45860258e-06
Iter: 880 loss: 1.45454317e-06
Iter: 881 loss: 1.45310946e-06
Iter: 882 loss: 1.45922627e-06
Iter: 883 loss: 1.45281092e-06
Iter: 884 loss: 1.45171134e-06
Iter: 885 loss: 1.4517027e-06
Iter: 886 loss: 1.45080605e-06
Iter: 887 loss: 1.44974888e-06
Iter: 888 loss: 1.44975104e-06
Iter: 889 loss: 1.4489583e-06
Iter: 890 loss: 1.44721082e-06
Iter: 891 loss: 1.48216145e-06
Iter: 892 loss: 1.447199e-06
Iter: 893 loss: 1.44556634e-06
Iter: 894 loss: 1.45405045e-06
Iter: 895 loss: 1.44532919e-06
Iter: 896 loss: 1.44409194e-06
Iter: 897 loss: 1.4440908e-06
Iter: 898 loss: 1.44344847e-06
Iter: 899 loss: 1.44204819e-06
Iter: 900 loss: 1.46449293e-06
Iter: 901 loss: 1.442e-06
Iter: 902 loss: 1.44062528e-06
Iter: 903 loss: 1.46044613e-06
Iter: 904 loss: 1.44065507e-06
Iter: 905 loss: 1.43940815e-06
Iter: 906 loss: 1.44132855e-06
Iter: 907 loss: 1.43884006e-06
Iter: 908 loss: 1.43784405e-06
Iter: 909 loss: 1.43727743e-06
Iter: 910 loss: 1.43686043e-06
Iter: 911 loss: 1.43535e-06
Iter: 912 loss: 1.44373257e-06
Iter: 913 loss: 1.43516411e-06
Iter: 914 loss: 1.43394095e-06
Iter: 915 loss: 1.44386524e-06
Iter: 916 loss: 1.43382545e-06
Iter: 917 loss: 1.43297973e-06
Iter: 918 loss: 1.43332045e-06
Iter: 919 loss: 1.43236048e-06
Iter: 920 loss: 1.43122088e-06
Iter: 921 loss: 1.43325212e-06
Iter: 922 loss: 1.43080069e-06
Iter: 923 loss: 1.42915223e-06
Iter: 924 loss: 1.43322404e-06
Iter: 925 loss: 1.4285622e-06
Iter: 926 loss: 1.42759166e-06
Iter: 927 loss: 1.42847671e-06
Iter: 928 loss: 1.42705267e-06
Iter: 929 loss: 1.42609417e-06
Iter: 930 loss: 1.43519253e-06
Iter: 931 loss: 1.42604131e-06
Iter: 932 loss: 1.42497424e-06
Iter: 933 loss: 1.42407703e-06
Iter: 934 loss: 1.42376382e-06
Iter: 935 loss: 1.4225393e-06
Iter: 936 loss: 1.42574163e-06
Iter: 937 loss: 1.42215367e-06
Iter: 938 loss: 1.42107615e-06
Iter: 939 loss: 1.43644081e-06
Iter: 940 loss: 1.42108456e-06
Iter: 941 loss: 1.4203606e-06
Iter: 942 loss: 1.41888358e-06
Iter: 943 loss: 1.44488831e-06
Iter: 944 loss: 1.41886994e-06
Iter: 945 loss: 1.41736427e-06
Iter: 946 loss: 1.4250013e-06
Iter: 947 loss: 1.41712371e-06
Iter: 948 loss: 1.41593682e-06
Iter: 949 loss: 1.42867646e-06
Iter: 950 loss: 1.4158976e-06
Iter: 951 loss: 1.41514647e-06
Iter: 952 loss: 1.41702458e-06
Iter: 953 loss: 1.41482235e-06
Iter: 954 loss: 1.41398391e-06
Iter: 955 loss: 1.4136142e-06
Iter: 956 loss: 1.4132213e-06
Iter: 957 loss: 1.41191072e-06
Iter: 958 loss: 1.42456315e-06
Iter: 959 loss: 1.41182818e-06
Iter: 960 loss: 1.41095006e-06
Iter: 961 loss: 1.40999362e-06
Iter: 962 loss: 1.40982934e-06
Iter: 963 loss: 1.40853263e-06
Iter: 964 loss: 1.4148585e-06
Iter: 965 loss: 1.40834481e-06
Iter: 966 loss: 1.40700809e-06
Iter: 967 loss: 1.41557257e-06
Iter: 968 loss: 1.4069177e-06
Iter: 969 loss: 1.40608745e-06
Iter: 970 loss: 1.40491375e-06
Iter: 971 loss: 1.40484644e-06
Iter: 972 loss: 1.40396799e-06
Iter: 973 loss: 1.40392979e-06
Iter: 974 loss: 1.40312102e-06
Iter: 975 loss: 1.4027637e-06
Iter: 976 loss: 1.40234692e-06
Iter: 977 loss: 1.40133625e-06
Iter: 978 loss: 1.40072007e-06
Iter: 979 loss: 1.4002469e-06
Iter: 980 loss: 1.39908627e-06
Iter: 981 loss: 1.41593273e-06
Iter: 982 loss: 1.39910981e-06
Iter: 983 loss: 1.39808503e-06
Iter: 984 loss: 1.40024963e-06
Iter: 985 loss: 1.39773124e-06
Iter: 986 loss: 1.3966403e-06
Iter: 987 loss: 1.39758197e-06
Iter: 988 loss: 1.3959891e-06
Iter: 989 loss: 1.39481983e-06
Iter: 990 loss: 1.40237501e-06
Iter: 991 loss: 1.39471035e-06
Iter: 992 loss: 1.39360111e-06
Iter: 993 loss: 1.39526901e-06
Iter: 994 loss: 1.39311601e-06
Iter: 995 loss: 1.39216183e-06
Iter: 996 loss: 1.39219412e-06
Iter: 997 loss: 1.3913625e-06
Iter: 998 loss: 1.39079452e-06
Iter: 999 loss: 1.3906922e-06
Iter: 1000 loss: 1.39000872e-06
Iter: 1001 loss: 1.38879125e-06
Iter: 1002 loss: 1.41821647e-06
Iter: 1003 loss: 1.38879534e-06
Iter: 1004 loss: 1.38742303e-06
Iter: 1005 loss: 1.39129543e-06
Iter: 1006 loss: 1.38697271e-06
Iter: 1007 loss: 1.38592281e-06
Iter: 1008 loss: 1.38594544e-06
Iter: 1009 loss: 1.38534779e-06
Iter: 1010 loss: 1.38426617e-06
Iter: 1011 loss: 1.40819873e-06
Iter: 1012 loss: 1.38425582e-06
Iter: 1013 loss: 1.38297798e-06
Iter: 1014 loss: 1.38621635e-06
Iter: 1015 loss: 1.38255587e-06
Iter: 1016 loss: 1.38153712e-06
Iter: 1017 loss: 1.38155826e-06
Iter: 1018 loss: 1.38083828e-06
Iter: 1019 loss: 1.38052155e-06
Iter: 1020 loss: 1.38015321e-06
Iter: 1021 loss: 1.37902157e-06
Iter: 1022 loss: 1.38174983e-06
Iter: 1023 loss: 1.37862094e-06
Iter: 1024 loss: 1.37747872e-06
Iter: 1025 loss: 1.38640223e-06
Iter: 1026 loss: 1.37737197e-06
Iter: 1027 loss: 1.37660072e-06
Iter: 1028 loss: 1.37585278e-06
Iter: 1029 loss: 1.37564598e-06
Iter: 1030 loss: 1.37453185e-06
Iter: 1031 loss: 1.38033965e-06
Iter: 1032 loss: 1.37428299e-06
Iter: 1033 loss: 1.37320239e-06
Iter: 1034 loss: 1.38205542e-06
Iter: 1035 loss: 1.37313725e-06
Iter: 1036 loss: 1.3724906e-06
Iter: 1037 loss: 1.37167706e-06
Iter: 1038 loss: 1.37161351e-06
Iter: 1039 loss: 1.37064796e-06
Iter: 1040 loss: 1.38000905e-06
Iter: 1041 loss: 1.37059692e-06
Iter: 1042 loss: 1.36958454e-06
Iter: 1043 loss: 1.37020129e-06
Iter: 1044 loss: 1.36889855e-06
Iter: 1045 loss: 1.36809479e-06
Iter: 1046 loss: 1.36753931e-06
Iter: 1047 loss: 1.3672543e-06
Iter: 1048 loss: 1.36613926e-06
Iter: 1049 loss: 1.37688426e-06
Iter: 1050 loss: 1.36604581e-06
Iter: 1051 loss: 1.36491394e-06
Iter: 1052 loss: 1.36925678e-06
Iter: 1053 loss: 1.36469635e-06
Iter: 1054 loss: 1.36386757e-06
Iter: 1055 loss: 1.36412086e-06
Iter: 1056 loss: 1.36330164e-06
Iter: 1057 loss: 1.36231415e-06
Iter: 1058 loss: 1.3699248e-06
Iter: 1059 loss: 1.3621792e-06
Iter: 1060 loss: 1.36130711e-06
Iter: 1061 loss: 1.3614457e-06
Iter: 1062 loss: 1.36062192e-06
Iter: 1063 loss: 1.35946186e-06
Iter: 1064 loss: 1.36138738e-06
Iter: 1065 loss: 1.35901e-06
Iter: 1066 loss: 1.35803589e-06
Iter: 1067 loss: 1.36946539e-06
Iter: 1068 loss: 1.35807068e-06
Iter: 1069 loss: 1.35712185e-06
Iter: 1070 loss: 1.35656751e-06
Iter: 1071 loss: 1.35623463e-06
Iter: 1072 loss: 1.35525715e-06
Iter: 1073 loss: 1.35718392e-06
Iter: 1074 loss: 1.35488654e-06
Iter: 1075 loss: 1.35402934e-06
Iter: 1076 loss: 1.35402854e-06
Iter: 1077 loss: 1.3534634e-06
Iter: 1078 loss: 1.35222604e-06
Iter: 1079 loss: 1.37087159e-06
Iter: 1080 loss: 1.35222967e-06
Iter: 1081 loss: 1.35098617e-06
Iter: 1082 loss: 1.35578557e-06
Iter: 1083 loss: 1.35075925e-06
Iter: 1084 loss: 1.34988898e-06
Iter: 1085 loss: 1.36340873e-06
Iter: 1086 loss: 1.34984862e-06
Iter: 1087 loss: 1.34910624e-06
Iter: 1088 loss: 1.34881043e-06
Iter: 1089 loss: 1.3483666e-06
Iter: 1090 loss: 1.3474048e-06
Iter: 1091 loss: 1.35002551e-06
Iter: 1092 loss: 1.34715413e-06
Iter: 1093 loss: 1.34595484e-06
Iter: 1094 loss: 1.35159394e-06
Iter: 1095 loss: 1.34574839e-06
Iter: 1096 loss: 1.34486e-06
Iter: 1097 loss: 1.34526749e-06
Iter: 1098 loss: 1.34424761e-06
Iter: 1099 loss: 1.34336369e-06
Iter: 1100 loss: 1.34770107e-06
Iter: 1101 loss: 1.34321817e-06
Iter: 1102 loss: 1.34230925e-06
Iter: 1103 loss: 1.34767561e-06
Iter: 1104 loss: 1.34226116e-06
Iter: 1105 loss: 1.34147683e-06
Iter: 1106 loss: 1.34055085e-06
Iter: 1107 loss: 1.34043557e-06
Iter: 1108 loss: 1.33963454e-06
Iter: 1109 loss: 1.34926461e-06
Iter: 1110 loss: 1.33964363e-06
Iter: 1111 loss: 1.33886078e-06
Iter: 1112 loss: 1.3406725e-06
Iter: 1113 loss: 1.33862966e-06
Iter: 1114 loss: 1.33786455e-06
Iter: 1115 loss: 1.33661e-06
Iter: 1116 loss: 1.33665151e-06
Iter: 1117 loss: 1.33546746e-06
Iter: 1118 loss: 1.342519e-06
Iter: 1119 loss: 1.33530102e-06
Iter: 1120 loss: 1.33432877e-06
Iter: 1121 loss: 1.34484048e-06
Iter: 1122 loss: 1.33427443e-06
Iter: 1123 loss: 1.3335e-06
Iter: 1124 loss: 1.33266701e-06
Iter: 1125 loss: 1.3324634e-06
Iter: 1126 loss: 1.33179196e-06
Iter: 1127 loss: 1.33176582e-06
Iter: 1128 loss: 1.33114224e-06
Iter: 1129 loss: 1.33119215e-06
Iter: 1130 loss: 1.33062804e-06
Iter: 1131 loss: 1.32977573e-06
Iter: 1132 loss: 1.32975288e-06
Iter: 1133 loss: 1.32906496e-06
Iter: 1134 loss: 1.32812033e-06
Iter: 1135 loss: 1.32810806e-06
Iter: 1136 loss: 1.32740911e-06
Iter: 1137 loss: 1.32909497e-06
Iter: 1138 loss: 1.32713546e-06
Iter: 1139 loss: 1.32655214e-06
Iter: 1140 loss: 1.32551213e-06
Iter: 1141 loss: 1.32550963e-06
Iter: 1142 loss: 1.32448918e-06
Iter: 1143 loss: 1.32445939e-06
Iter: 1144 loss: 1.323731e-06
Iter: 1145 loss: 1.32351192e-06
Iter: 1146 loss: 1.32303217e-06
Iter: 1147 loss: 1.32225387e-06
Iter: 1148 loss: 1.32192486e-06
Iter: 1149 loss: 1.3214601e-06
Iter: 1150 loss: 1.32033733e-06
Iter: 1151 loss: 1.33195408e-06
Iter: 1152 loss: 1.32034017e-06
Iter: 1153 loss: 1.319391e-06
Iter: 1154 loss: 1.32396872e-06
Iter: 1155 loss: 1.31922957e-06
Iter: 1156 loss: 1.31862271e-06
Iter: 1157 loss: 1.31810839e-06
Iter: 1158 loss: 1.31793365e-06
Iter: 1159 loss: 1.31704451e-06
Iter: 1160 loss: 1.32910725e-06
Iter: 1161 loss: 1.31700631e-06
Iter: 1162 loss: 1.31635181e-06
Iter: 1163 loss: 1.31564457e-06
Iter: 1164 loss: 1.31553941e-06
Iter: 1165 loss: 1.31473575e-06
Iter: 1166 loss: 1.32126468e-06
Iter: 1167 loss: 1.31465617e-06
Iter: 1168 loss: 1.31391369e-06
Iter: 1169 loss: 1.31773845e-06
Iter: 1170 loss: 1.31384309e-06
Iter: 1171 loss: 1.31324191e-06
Iter: 1172 loss: 1.3129079e-06
Iter: 1173 loss: 1.31260992e-06
Iter: 1174 loss: 1.31186766e-06
Iter: 1175 loss: 1.31544323e-06
Iter: 1176 loss: 1.31167963e-06
Iter: 1177 loss: 1.31089882e-06
Iter: 1178 loss: 1.31490231e-06
Iter: 1179 loss: 1.31083345e-06
Iter: 1180 loss: 1.31023535e-06
Iter: 1181 loss: 1.30959597e-06
Iter: 1182 loss: 1.30951969e-06
Iter: 1183 loss: 1.30849958e-06
Iter: 1184 loss: 1.3086883e-06
Iter: 1185 loss: 1.3078236e-06
Iter: 1186 loss: 1.30723038e-06
Iter: 1187 loss: 1.30708008e-06
Iter: 1188 loss: 1.30648937e-06
Iter: 1189 loss: 1.30572732e-06
Iter: 1190 loss: 1.30570118e-06
Iter: 1191 loss: 1.30486978e-06
Iter: 1192 loss: 1.31122454e-06
Iter: 1193 loss: 1.30480373e-06
Iter: 1194 loss: 1.30401281e-06
Iter: 1195 loss: 1.30508931e-06
Iter: 1196 loss: 1.30363276e-06
Iter: 1197 loss: 1.30285753e-06
Iter: 1198 loss: 1.30350793e-06
Iter: 1199 loss: 1.30243325e-06
Iter: 1200 loss: 1.30174294e-06
Iter: 1201 loss: 1.31095862e-06
Iter: 1202 loss: 1.30171577e-06
Iter: 1203 loss: 1.30107378e-06
Iter: 1204 loss: 1.3008804e-06
Iter: 1205 loss: 1.30050466e-06
Iter: 1206 loss: 1.29978025e-06
Iter: 1207 loss: 1.30151716e-06
Iter: 1208 loss: 1.29955424e-06
Iter: 1209 loss: 1.29889168e-06
Iter: 1210 loss: 1.30584885e-06
Iter: 1211 loss: 1.29888736e-06
Iter: 1212 loss: 1.29835644e-06
Iter: 1213 loss: 1.29802334e-06
Iter: 1214 loss: 1.29781915e-06
Iter: 1215 loss: 1.29698196e-06
Iter: 1216 loss: 1.296665e-06
Iter: 1217 loss: 1.29622549e-06
Iter: 1218 loss: 1.29534817e-06
Iter: 1219 loss: 1.30401497e-06
Iter: 1220 loss: 1.2953551e-06
Iter: 1221 loss: 1.29445948e-06
Iter: 1222 loss: 1.29775754e-06
Iter: 1223 loss: 1.29424177e-06
Iter: 1224 loss: 1.29361115e-06
Iter: 1225 loss: 1.29328032e-06
Iter: 1226 loss: 1.2930027e-06
Iter: 1227 loss: 1.29222872e-06
Iter: 1228 loss: 1.2922419e-06
Iter: 1229 loss: 1.29166688e-06
Iter: 1230 loss: 1.29075079e-06
Iter: 1231 loss: 1.29073976e-06
Iter: 1232 loss: 1.28996089e-06
Iter: 1233 loss: 1.30087369e-06
Iter: 1234 loss: 1.28994941e-06
Iter: 1235 loss: 1.28932834e-06
Iter: 1236 loss: 1.29190698e-06
Iter: 1237 loss: 1.28918236e-06
Iter: 1238 loss: 1.2885954e-06
Iter: 1239 loss: 1.28806266e-06
Iter: 1240 loss: 1.28795818e-06
Iter: 1241 loss: 1.28716783e-06
Iter: 1242 loss: 1.29475166e-06
Iter: 1243 loss: 1.28714078e-06
Iter: 1244 loss: 1.28646718e-06
Iter: 1245 loss: 1.28847182e-06
Iter: 1246 loss: 1.28632632e-06
Iter: 1247 loss: 1.28573788e-06
Iter: 1248 loss: 1.28513193e-06
Iter: 1249 loss: 1.28501745e-06
Iter: 1250 loss: 1.28405645e-06
Iter: 1251 loss: 1.28667239e-06
Iter: 1252 loss: 1.28378656e-06
Iter: 1253 loss: 1.28315639e-06
Iter: 1254 loss: 1.28310739e-06
Iter: 1255 loss: 1.28257079e-06
Iter: 1256 loss: 1.28170882e-06
Iter: 1257 loss: 1.2817161e-06
Iter: 1258 loss: 1.28091233e-06
Iter: 1259 loss: 1.28674128e-06
Iter: 1260 loss: 1.28086572e-06
Iter: 1261 loss: 1.27998726e-06
Iter: 1262 loss: 1.28236138e-06
Iter: 1263 loss: 1.27975022e-06
Iter: 1264 loss: 1.27908129e-06
Iter: 1265 loss: 1.27879593e-06
Iter: 1266 loss: 1.278441e-06
Iter: 1267 loss: 1.27790383e-06
Iter: 1268 loss: 1.2778612e-06
Iter: 1269 loss: 1.27736871e-06
Iter: 1270 loss: 1.27663861e-06
Iter: 1271 loss: 1.27661247e-06
Iter: 1272 loss: 1.27575959e-06
Iter: 1273 loss: 1.27918645e-06
Iter: 1274 loss: 1.27560372e-06
Iter: 1275 loss: 1.27473277e-06
Iter: 1276 loss: 1.28033662e-06
Iter: 1277 loss: 1.27462749e-06
Iter: 1278 loss: 1.27398971e-06
Iter: 1279 loss: 1.27399483e-06
Iter: 1280 loss: 1.2734223e-06
Iter: 1281 loss: 1.27261899e-06
Iter: 1282 loss: 1.27326712e-06
Iter: 1283 loss: 1.27214855e-06
Iter: 1284 loss: 1.27125782e-06
Iter: 1285 loss: 1.27594603e-06
Iter: 1286 loss: 1.27109115e-06
Iter: 1287 loss: 1.27030887e-06
Iter: 1288 loss: 1.27655096e-06
Iter: 1289 loss: 1.27021076e-06
Iter: 1290 loss: 1.26979558e-06
Iter: 1291 loss: 1.26927637e-06
Iter: 1292 loss: 1.26920872e-06
Iter: 1293 loss: 1.26829241e-06
Iter: 1294 loss: 1.27589954e-06
Iter: 1295 loss: 1.26827445e-06
Iter: 1296 loss: 1.26753821e-06
Iter: 1297 loss: 1.267524e-06
Iter: 1298 loss: 1.26698274e-06
Iter: 1299 loss: 1.26629368e-06
Iter: 1300 loss: 1.26920486e-06
Iter: 1301 loss: 1.26610485e-06
Iter: 1302 loss: 1.26523321e-06
Iter: 1303 loss: 1.26963482e-06
Iter: 1304 loss: 1.26512214e-06
Iter: 1305 loss: 1.26454893e-06
Iter: 1306 loss: 1.26383316e-06
Iter: 1307 loss: 1.26379973e-06
Iter: 1308 loss: 1.26312227e-06
Iter: 1309 loss: 1.26312216e-06
Iter: 1310 loss: 1.26259317e-06
Iter: 1311 loss: 1.26217901e-06
Iter: 1312 loss: 1.26198643e-06
Iter: 1313 loss: 1.26112855e-06
Iter: 1314 loss: 1.26281407e-06
Iter: 1315 loss: 1.26082216e-06
Iter: 1316 loss: 1.26003727e-06
Iter: 1317 loss: 1.26147324e-06
Iter: 1318 loss: 1.25973054e-06
Iter: 1319 loss: 1.2590524e-06
Iter: 1320 loss: 1.26933628e-06
Iter: 1321 loss: 1.25905478e-06
Iter: 1322 loss: 1.25845077e-06
Iter: 1323 loss: 1.25771476e-06
Iter: 1324 loss: 1.25765973e-06
Iter: 1325 loss: 1.25688609e-06
Iter: 1326 loss: 1.26427494e-06
Iter: 1327 loss: 1.25688712e-06
Iter: 1328 loss: 1.25621898e-06
Iter: 1329 loss: 1.25741258e-06
Iter: 1330 loss: 1.25590191e-06
Iter: 1331 loss: 1.25537701e-06
Iter: 1332 loss: 1.25522035e-06
Iter: 1333 loss: 1.25487e-06
Iter: 1334 loss: 1.25427835e-06
Iter: 1335 loss: 1.25423901e-06
Iter: 1336 loss: 1.25375902e-06
Iter: 1337 loss: 1.25297379e-06
Iter: 1338 loss: 1.25294082e-06
Iter: 1339 loss: 1.25203701e-06
Iter: 1340 loss: 1.25602605e-06
Iter: 1341 loss: 1.25180804e-06
Iter: 1342 loss: 1.25095164e-06
Iter: 1343 loss: 1.25651536e-06
Iter: 1344 loss: 1.25083182e-06
Iter: 1345 loss: 1.25019494e-06
Iter: 1346 loss: 1.24973303e-06
Iter: 1347 loss: 1.2495392e-06
Iter: 1348 loss: 1.24856763e-06
Iter: 1349 loss: 1.25193867e-06
Iter: 1350 loss: 1.24834582e-06
Iter: 1351 loss: 1.24762312e-06
Iter: 1352 loss: 1.25300517e-06
Iter: 1353 loss: 1.24755354e-06
Iter: 1354 loss: 1.2469327e-06
Iter: 1355 loss: 1.24925862e-06
Iter: 1356 loss: 1.2467699e-06
Iter: 1357 loss: 1.2461843e-06
Iter: 1358 loss: 1.24566645e-06
Iter: 1359 loss: 1.24552923e-06
Iter: 1360 loss: 1.24483506e-06
Iter: 1361 loss: 1.24486132e-06
Iter: 1362 loss: 1.24435178e-06
Iter: 1363 loss: 1.24364556e-06
Iter: 1364 loss: 1.24364215e-06
Iter: 1365 loss: 1.24300334e-06
Iter: 1366 loss: 1.25043641e-06
Iter: 1367 loss: 1.24295354e-06
Iter: 1368 loss: 1.24228552e-06
Iter: 1369 loss: 1.24380449e-06
Iter: 1370 loss: 1.24205621e-06
Iter: 1371 loss: 1.24155451e-06
Iter: 1372 loss: 1.24138683e-06
Iter: 1373 loss: 1.24108556e-06
Iter: 1374 loss: 1.24042242e-06
Iter: 1375 loss: 1.24852363e-06
Iter: 1376 loss: 1.24035751e-06
Iter: 1377 loss: 1.23985706e-06
Iter: 1378 loss: 1.23946324e-06
Iter: 1379 loss: 1.23928271e-06
Iter: 1380 loss: 1.23846462e-06
Iter: 1381 loss: 1.23926088e-06
Iter: 1382 loss: 1.23800271e-06
Iter: 1383 loss: 1.23709151e-06
Iter: 1384 loss: 1.24201699e-06
Iter: 1385 loss: 1.23689233e-06
Iter: 1386 loss: 1.23622021e-06
Iter: 1387 loss: 1.24463713e-06
Iter: 1388 loss: 1.23622556e-06
Iter: 1389 loss: 1.23572954e-06
Iter: 1390 loss: 1.23526206e-06
Iter: 1391 loss: 1.23510654e-06
Iter: 1392 loss: 1.23440896e-06
Iter: 1393 loss: 1.23748487e-06
Iter: 1394 loss: 1.23423797e-06
Iter: 1395 loss: 1.23349946e-06
Iter: 1396 loss: 1.23776567e-06
Iter: 1397 loss: 1.23341124e-06
Iter: 1398 loss: 1.23288e-06
Iter: 1399 loss: 1.23229768e-06
Iter: 1400 loss: 1.23221162e-06
Iter: 1401 loss: 1.23169559e-06
Iter: 1402 loss: 1.23164773e-06
Iter: 1403 loss: 1.23124437e-06
Iter: 1404 loss: 1.23057862e-06
Iter: 1405 loss: 1.23059749e-06
Iter: 1406 loss: 1.22990639e-06
Iter: 1407 loss: 1.23474115e-06
Iter: 1408 loss: 1.22989832e-06
Iter: 1409 loss: 1.22919982e-06
Iter: 1410 loss: 1.23100779e-06
Iter: 1411 loss: 1.22895983e-06
Iter: 1412 loss: 1.2284288e-06
Iter: 1413 loss: 1.22798644e-06
Iter: 1414 loss: 1.2278291e-06
Iter: 1415 loss: 1.22694485e-06
Iter: 1416 loss: 1.22972972e-06
Iter: 1417 loss: 1.22672532e-06
Iter: 1418 loss: 1.22594531e-06
Iter: 1419 loss: 1.23132327e-06
Iter: 1420 loss: 1.22587335e-06
Iter: 1421 loss: 1.22511437e-06
Iter: 1422 loss: 1.22771553e-06
Iter: 1423 loss: 1.22490235e-06
Iter: 1424 loss: 1.22437837e-06
Iter: 1425 loss: 1.2238869e-06
Iter: 1426 loss: 1.22370045e-06
Iter: 1427 loss: 1.22305835e-06
Iter: 1428 loss: 1.22307085e-06
Iter: 1429 loss: 1.22253334e-06
Iter: 1430 loss: 1.22239862e-06
Iter: 1431 loss: 1.2220703e-06
Iter: 1432 loss: 1.22149777e-06
Iter: 1433 loss: 1.22397887e-06
Iter: 1434 loss: 1.22134907e-06
Iter: 1435 loss: 1.22067468e-06
Iter: 1436 loss: 1.22325741e-06
Iter: 1437 loss: 1.22047709e-06
Iter: 1438 loss: 1.21992639e-06
Iter: 1439 loss: 1.21944765e-06
Iter: 1440 loss: 1.21929133e-06
Iter: 1441 loss: 1.21885853e-06
Iter: 1442 loss: 1.21880385e-06
Iter: 1443 loss: 1.21837343e-06
Iter: 1444 loss: 1.2175426e-06
Iter: 1445 loss: 1.23614711e-06
Iter: 1446 loss: 1.21757068e-06
Iter: 1447 loss: 1.21673406e-06
Iter: 1448 loss: 1.21923927e-06
Iter: 1449 loss: 1.2165001e-06
Iter: 1450 loss: 1.21580547e-06
Iter: 1451 loss: 1.21915946e-06
Iter: 1452 loss: 1.2156986e-06
Iter: 1453 loss: 1.21503433e-06
Iter: 1454 loss: 1.21903872e-06
Iter: 1455 loss: 1.21497487e-06
Iter: 1456 loss: 1.2143048e-06
Iter: 1457 loss: 1.21418907e-06
Iter: 1458 loss: 1.21373182e-06
Iter: 1459 loss: 1.21310177e-06
Iter: 1460 loss: 1.21614266e-06
Iter: 1461 loss: 1.21296262e-06
Iter: 1462 loss: 1.21239464e-06
Iter: 1463 loss: 1.21674509e-06
Iter: 1464 loss: 1.21235166e-06
Iter: 1465 loss: 1.21188282e-06
Iter: 1466 loss: 1.21124697e-06
Iter: 1467 loss: 1.21124265e-06
Iter: 1468 loss: 1.21072765e-06
Iter: 1469 loss: 1.2107264e-06
Iter: 1470 loss: 1.21019696e-06
Iter: 1471 loss: 1.20943309e-06
Iter: 1472 loss: 1.20943378e-06
Iter: 1473 loss: 1.20876462e-06
Iter: 1474 loss: 1.21240805e-06
Iter: 1475 loss: 1.20867151e-06
Iter: 1476 loss: 1.20802565e-06
Iter: 1477 loss: 1.21178471e-06
Iter: 1478 loss: 1.20790412e-06
Iter: 1479 loss: 1.20746154e-06
Iter: 1480 loss: 1.20683899e-06
Iter: 1481 loss: 1.20679579e-06
Iter: 1482 loss: 1.20600055e-06
Iter: 1483 loss: 1.21000073e-06
Iter: 1484 loss: 1.20590312e-06
Iter: 1485 loss: 1.20526829e-06
Iter: 1486 loss: 1.20868526e-06
Iter: 1487 loss: 1.20518894e-06
Iter: 1488 loss: 1.20454888e-06
Iter: 1489 loss: 1.20635559e-06
Iter: 1490 loss: 1.20427296e-06
Iter: 1491 loss: 1.20370487e-06
Iter: 1492 loss: 1.20405912e-06
Iter: 1493 loss: 1.20330776e-06
Iter: 1494 loss: 1.20278571e-06
Iter: 1495 loss: 1.2069296e-06
Iter: 1496 loss: 1.20268351e-06
Iter: 1497 loss: 1.20214327e-06
Iter: 1498 loss: 1.20334926e-06
Iter: 1499 loss: 1.20192169e-06
Iter: 1500 loss: 1.20141465e-06
Iter: 1501 loss: 1.20155414e-06
Iter: 1502 loss: 1.20109314e-06
Iter: 1503 loss: 1.20053323e-06
Iter: 1504 loss: 1.20053392e-06
Iter: 1505 loss: 1.20023151e-06
Iter: 1506 loss: 1.19947549e-06
Iter: 1507 loss: 1.20936897e-06
Iter: 1508 loss: 1.19949652e-06
Iter: 1509 loss: 1.19888921e-06
Iter: 1510 loss: 1.19889012e-06
Iter: 1511 loss: 1.19834908e-06
Iter: 1512 loss: 1.19880178e-06
Iter: 1513 loss: 1.19807351e-06
Iter: 1514 loss: 1.19756896e-06
Iter: 1515 loss: 1.19695653e-06
Iter: 1516 loss: 1.19690344e-06
Iter: 1517 loss: 1.19612696e-06
Iter: 1518 loss: 1.2041487e-06
Iter: 1519 loss: 1.19611673e-06
Iter: 1520 loss: 1.19558763e-06
Iter: 1521 loss: 1.19954871e-06
Iter: 1522 loss: 1.1956e-06
Iter: 1523 loss: 1.19510491e-06
Iter: 1524 loss: 1.19496508e-06
Iter: 1525 loss: 1.19468677e-06
Iter: 1526 loss: 1.19401e-06
Iter: 1527 loss: 1.19480273e-06
Iter: 1528 loss: 1.19365268e-06
Iter: 1529 loss: 1.19311596e-06
Iter: 1530 loss: 1.19311221e-06
Iter: 1531 loss: 1.19271067e-06
Iter: 1532 loss: 1.19240917e-06
Iter: 1533 loss: 1.19226615e-06
Iter: 1534 loss: 1.19171136e-06
Iter: 1535 loss: 1.19471e-06
Iter: 1536 loss: 1.1916527e-06
Iter: 1537 loss: 1.19101696e-06
Iter: 1538 loss: 1.19216452e-06
Iter: 1539 loss: 1.19070728e-06
Iter: 1540 loss: 1.19029232e-06
Iter: 1541 loss: 1.1899981e-06
Iter: 1542 loss: 1.18977891e-06
Iter: 1543 loss: 1.18923981e-06
Iter: 1544 loss: 1.18928131e-06
Iter: 1545 loss: 1.18885032e-06
Iter: 1546 loss: 1.18816388e-06
Iter: 1547 loss: 1.18818048e-06
Iter: 1548 loss: 1.18753314e-06
Iter: 1549 loss: 1.18896094e-06
Iter: 1550 loss: 1.18726484e-06
Iter: 1551 loss: 1.18669823e-06
Iter: 1552 loss: 1.19292076e-06
Iter: 1553 loss: 1.18666458e-06
Iter: 1554 loss: 1.18612616e-06
Iter: 1555 loss: 1.18739399e-06
Iter: 1556 loss: 1.18595744e-06
Iter: 1557 loss: 1.18548689e-06
Iter: 1558 loss: 1.18581863e-06
Iter: 1559 loss: 1.18515027e-06
Iter: 1560 loss: 1.18460093e-06
Iter: 1561 loss: 1.18629805e-06
Iter: 1562 loss: 1.18440653e-06
Iter: 1563 loss: 1.18378841e-06
Iter: 1564 loss: 1.1866905e-06
Iter: 1565 loss: 1.18368757e-06
Iter: 1566 loss: 1.1832044e-06
Iter: 1567 loss: 1.18344565e-06
Iter: 1568 loss: 1.18286493e-06
Iter: 1569 loss: 1.18237688e-06
Iter: 1570 loss: 1.18892399e-06
Iter: 1571 loss: 1.18239018e-06
Iter: 1572 loss: 1.1820182e-06
Iter: 1573 loss: 1.18127252e-06
Iter: 1574 loss: 1.19621109e-06
Iter: 1575 loss: 1.18127207e-06
Iter: 1576 loss: 1.18055641e-06
Iter: 1577 loss: 1.18603043e-06
Iter: 1578 loss: 1.18053936e-06
Iter: 1579 loss: 1.17982199e-06
Iter: 1580 loss: 1.18260471e-06
Iter: 1581 loss: 1.17968307e-06
Iter: 1582 loss: 1.17923241e-06
Iter: 1583 loss: 1.17898094e-06
Iter: 1584 loss: 1.17879131e-06
Iter: 1585 loss: 1.17808941e-06
Iter: 1586 loss: 1.17937361e-06
Iter: 1587 loss: 1.17782702e-06
Iter: 1588 loss: 1.17708464e-06
Iter: 1589 loss: 1.18514947e-06
Iter: 1590 loss: 1.17709078e-06
Iter: 1591 loss: 1.17658465e-06
Iter: 1592 loss: 1.17703621e-06
Iter: 1593 loss: 1.17633886e-06
Iter: 1594 loss: 1.17583409e-06
Iter: 1595 loss: 1.17654486e-06
Iter: 1596 loss: 1.17564605e-06
Iter: 1597 loss: 1.17509512e-06
Iter: 1598 loss: 1.17858849e-06
Iter: 1599 loss: 1.17502668e-06
Iter: 1600 loss: 1.17448178e-06
Iter: 1601 loss: 1.17457625e-06
Iter: 1602 loss: 1.17409945e-06
Iter: 1603 loss: 1.17362163e-06
Iter: 1604 loss: 1.17822287e-06
Iter: 1605 loss: 1.1735882e-06
Iter: 1606 loss: 1.17307604e-06
Iter: 1607 loss: 1.17365857e-06
Iter: 1608 loss: 1.17282161e-06
Iter: 1609 loss: 1.17236721e-06
Iter: 1610 loss: 1.17176251e-06
Iter: 1611 loss: 1.17172851e-06
Iter: 1612 loss: 1.17132527e-06
Iter: 1613 loss: 1.17121112e-06
Iter: 1614 loss: 1.17082493e-06
Iter: 1615 loss: 1.17018112e-06
Iter: 1616 loss: 1.18618834e-06
Iter: 1617 loss: 1.17017362e-06
Iter: 1618 loss: 1.16943306e-06
Iter: 1619 loss: 1.17100194e-06
Iter: 1620 loss: 1.16923025e-06
Iter: 1621 loss: 1.16868182e-06
Iter: 1622 loss: 1.17602394e-06
Iter: 1623 loss: 1.16867932e-06
Iter: 1624 loss: 1.16811e-06
Iter: 1625 loss: 1.16899855e-06
Iter: 1626 loss: 1.16787373e-06
Iter: 1627 loss: 1.16736408e-06
Iter: 1628 loss: 1.16759202e-06
Iter: 1629 loss: 1.16703882e-06
Iter: 1630 loss: 1.16641047e-06
Iter: 1631 loss: 1.17014065e-06
Iter: 1632 loss: 1.16633146e-06
Iter: 1633 loss: 1.16573528e-06
Iter: 1634 loss: 1.16719252e-06
Iter: 1635 loss: 1.16549541e-06
Iter: 1636 loss: 1.16492913e-06
Iter: 1637 loss: 1.16568663e-06
Iter: 1638 loss: 1.16463616e-06
Iter: 1639 loss: 1.16414094e-06
Iter: 1640 loss: 1.17139928e-06
Iter: 1641 loss: 1.16416754e-06
Iter: 1642 loss: 1.16384103e-06
Iter: 1643 loss: 1.16325202e-06
Iter: 1644 loss: 1.17739398e-06
Iter: 1645 loss: 1.1632269e-06
Iter: 1646 loss: 1.16257593e-06
Iter: 1647 loss: 1.16621857e-06
Iter: 1648 loss: 1.16244632e-06
Iter: 1649 loss: 1.1618697e-06
Iter: 1650 loss: 1.16662045e-06
Iter: 1651 loss: 1.1617966e-06
Iter: 1652 loss: 1.16146771e-06
Iter: 1653 loss: 1.16076296e-06
Iter: 1654 loss: 1.17237528e-06
Iter: 1655 loss: 1.16080366e-06
Iter: 1656 loss: 1.16001752e-06
Iter: 1657 loss: 1.16507852e-06
Iter: 1658 loss: 1.15992157e-06
Iter: 1659 loss: 1.15937848e-06
Iter: 1660 loss: 1.16685055e-06
Iter: 1661 loss: 1.1593678e-06
Iter: 1662 loss: 1.1589641e-06
Iter: 1663 loss: 1.15843045e-06
Iter: 1664 loss: 1.15834666e-06
Iter: 1665 loss: 1.15768739e-06
Iter: 1666 loss: 1.16208037e-06
Iter: 1667 loss: 1.15761486e-06
Iter: 1668 loss: 1.15711259e-06
Iter: 1669 loss: 1.1602433e-06
Iter: 1670 loss: 1.1570346e-06
Iter: 1671 loss: 1.15662237e-06
Iter: 1672 loss: 1.15681e-06
Iter: 1673 loss: 1.1563227e-06
Iter: 1674 loss: 1.1559091e-06
Iter: 1675 loss: 1.15923e-06
Iter: 1676 loss: 1.15584828e-06
Iter: 1677 loss: 1.15537819e-06
Iter: 1678 loss: 1.15548823e-06
Iter: 1679 loss: 1.15509079e-06
Iter: 1680 loss: 1.15451803e-06
Iter: 1681 loss: 1.15441355e-06
Iter: 1682 loss: 1.15402474e-06
Iter: 1683 loss: 1.1536174e-06
Iter: 1684 loss: 1.15357102e-06
Iter: 1685 loss: 1.15319096e-06
Iter: 1686 loss: 1.15276498e-06
Iter: 1687 loss: 1.15268574e-06
Iter: 1688 loss: 1.15213652e-06
Iter: 1689 loss: 1.15240778e-06
Iter: 1690 loss: 1.15181592e-06
Iter: 1691 loss: 1.15123646e-06
Iter: 1692 loss: 1.1583877e-06
Iter: 1693 loss: 1.1512476e-06
Iter: 1694 loss: 1.15071191e-06
Iter: 1695 loss: 1.15194541e-06
Iter: 1696 loss: 1.15050602e-06
Iter: 1697 loss: 1.15011221e-06
Iter: 1698 loss: 1.14985551e-06
Iter: 1699 loss: 1.14966656e-06
Iter: 1700 loss: 1.14892407e-06
Iter: 1701 loss: 1.15380362e-06
Iter: 1702 loss: 1.14879595e-06
Iter: 1703 loss: 1.14814429e-06
Iter: 1704 loss: 1.15055354e-06
Iter: 1705 loss: 1.1479442e-06
Iter: 1706 loss: 1.14754243e-06
Iter: 1707 loss: 1.14905322e-06
Iter: 1708 loss: 1.14743261e-06
Iter: 1709 loss: 1.14701538e-06
Iter: 1710 loss: 1.14902787e-06
Iter: 1711 loss: 1.14695285e-06
Iter: 1712 loss: 1.14657041e-06
Iter: 1713 loss: 1.14590455e-06
Iter: 1714 loss: 1.14591739e-06
Iter: 1715 loss: 1.14526199e-06
Iter: 1716 loss: 1.15070816e-06
Iter: 1717 loss: 1.14526222e-06
Iter: 1718 loss: 1.14463319e-06
Iter: 1719 loss: 1.14734826e-06
Iter: 1720 loss: 1.14452564e-06
Iter: 1721 loss: 1.14406657e-06
Iter: 1722 loss: 1.14363581e-06
Iter: 1723 loss: 1.14352861e-06
Iter: 1724 loss: 1.14293039e-06
Iter: 1725 loss: 1.14526313e-06
Iter: 1726 loss: 1.14280124e-06
Iter: 1727 loss: 1.14226123e-06
Iter: 1728 loss: 1.14882801e-06
Iter: 1729 loss: 1.14226532e-06
Iter: 1730 loss: 1.14187571e-06
Iter: 1731 loss: 1.14129148e-06
Iter: 1732 loss: 1.14127943e-06
Iter: 1733 loss: 1.14071838e-06
Iter: 1734 loss: 1.14473391e-06
Iter: 1735 loss: 1.14068598e-06
Iter: 1736 loss: 1.14009481e-06
Iter: 1737 loss: 1.14261582e-06
Iter: 1738 loss: 1.13999033e-06
Iter: 1739 loss: 1.13956344e-06
Iter: 1740 loss: 1.13995077e-06
Iter: 1741 loss: 1.13927933e-06
Iter: 1742 loss: 1.13880674e-06
Iter: 1743 loss: 1.14271916e-06
Iter: 1744 loss: 1.1387358e-06
Iter: 1745 loss: 1.13829969e-06
Iter: 1746 loss: 1.13850444e-06
Iter: 1747 loss: 1.1380356e-06
Iter: 1748 loss: 1.13754595e-06
Iter: 1749 loss: 1.13745455e-06
Iter: 1750 loss: 1.13717215e-06
Iter: 1751 loss: 1.13675378e-06
Iter: 1752 loss: 1.13673457e-06
Iter: 1753 loss: 1.13638987e-06
Iter: 1754 loss: 1.13576175e-06
Iter: 1755 loss: 1.1357406e-06
Iter: 1756 loss: 1.13510009e-06
Iter: 1757 loss: 1.13636156e-06
Iter: 1758 loss: 1.13480337e-06
Iter: 1759 loss: 1.13436488e-06
Iter: 1760 loss: 1.13437204e-06
Iter: 1761 loss: 1.13395822e-06
Iter: 1762 loss: 1.13419105e-06
Iter: 1763 loss: 1.13368287e-06
Iter: 1764 loss: 1.1332329e-06
Iter: 1765 loss: 1.13291503e-06
Iter: 1766 loss: 1.13268493e-06
Iter: 1767 loss: 1.13228896e-06
Iter: 1768 loss: 1.13225087e-06
Iter: 1769 loss: 1.13189469e-06
Iter: 1770 loss: 1.13164333e-06
Iter: 1771 loss: 1.13152737e-06
Iter: 1772 loss: 1.13095518e-06
Iter: 1773 loss: 1.13414126e-06
Iter: 1774 loss: 1.13087253e-06
Iter: 1775 loss: 1.13033855e-06
Iter: 1776 loss: 1.13173724e-06
Iter: 1777 loss: 1.13013994e-06
Iter: 1778 loss: 1.12965245e-06
Iter: 1779 loss: 1.1295781e-06
Iter: 1780 loss: 1.12921566e-06
Iter: 1781 loss: 1.12860403e-06
Iter: 1782 loss: 1.13089368e-06
Iter: 1783 loss: 1.12848954e-06
Iter: 1784 loss: 1.12775058e-06
Iter: 1785 loss: 1.13134206e-06
Iter: 1786 loss: 1.12759471e-06
Iter: 1787 loss: 1.12716498e-06
Iter: 1788 loss: 1.12661519e-06
Iter: 1789 loss: 1.12661223e-06
Iter: 1790 loss: 1.1259765e-06
Iter: 1791 loss: 1.132943e-06
Iter: 1792 loss: 1.12594819e-06
Iter: 1793 loss: 1.12547468e-06
Iter: 1794 loss: 1.12867156e-06
Iter: 1795 loss: 1.12539033e-06
Iter: 1796 loss: 1.12502084e-06
Iter: 1797 loss: 1.1245761e-06
Iter: 1798 loss: 1.12451767e-06
Iter: 1799 loss: 1.12393457e-06
Iter: 1800 loss: 1.12752718e-06
Iter: 1801 loss: 1.12383191e-06
Iter: 1802 loss: 1.1232255e-06
Iter: 1803 loss: 1.12595581e-06
Iter: 1804 loss: 1.12312898e-06
Iter: 1805 loss: 1.12273324e-06
Iter: 1806 loss: 1.12358043e-06
Iter: 1807 loss: 1.12255873e-06
Iter: 1808 loss: 1.1221241e-06
Iter: 1809 loss: 1.12422492e-06
Iter: 1810 loss: 1.12203566e-06
Iter: 1811 loss: 1.12161354e-06
Iter: 1812 loss: 1.12131045e-06
Iter: 1813 loss: 1.12118516e-06
Iter: 1814 loss: 1.12050361e-06
Iter: 1815 loss: 1.12210046e-06
Iter: 1816 loss: 1.12028158e-06
Iter: 1817 loss: 1.11986947e-06
Iter: 1818 loss: 1.11987879e-06
Iter: 1819 loss: 1.11951351e-06
Iter: 1820 loss: 1.11886357e-06
Iter: 1821 loss: 1.13372994e-06
Iter: 1822 loss: 1.11887653e-06
Iter: 1823 loss: 1.11825557e-06
Iter: 1824 loss: 1.12002351e-06
Iter: 1825 loss: 1.11809402e-06
Iter: 1826 loss: 1.11763177e-06
Iter: 1827 loss: 1.11763075e-06
Iter: 1828 loss: 1.11720647e-06
Iter: 1829 loss: 1.11678082e-06
Iter: 1830 loss: 1.11668373e-06
Iter: 1831 loss: 1.11613781e-06
Iter: 1832 loss: 1.117431e-06
Iter: 1833 loss: 1.11594795e-06
Iter: 1834 loss: 1.11538839e-06
Iter: 1835 loss: 1.12193459e-06
Iter: 1836 loss: 1.11537133e-06
Iter: 1837 loss: 1.11493443e-06
Iter: 1838 loss: 1.11491158e-06
Iter: 1839 loss: 1.11459792e-06
Iter: 1840 loss: 1.11414352e-06
Iter: 1841 loss: 1.11926431e-06
Iter: 1842 loss: 1.11415602e-06
Iter: 1843 loss: 1.11384e-06
Iter: 1844 loss: 1.11361851e-06
Iter: 1845 loss: 1.11347617e-06
Iter: 1846 loss: 1.11292218e-06
Iter: 1847 loss: 1.11327893e-06
Iter: 1848 loss: 1.11256691e-06
Iter: 1849 loss: 1.11206191e-06
Iter: 1850 loss: 1.11804582e-06
Iter: 1851 loss: 1.11205213e-06
Iter: 1852 loss: 1.11161501e-06
Iter: 1853 loss: 1.1120967e-06
Iter: 1854 loss: 1.11135887e-06
Iter: 1855 loss: 1.11087957e-06
Iter: 1856 loss: 1.11043755e-06
Iter: 1857 loss: 1.11033819e-06
Iter: 1858 loss: 1.10964618e-06
Iter: 1859 loss: 1.11567238e-06
Iter: 1860 loss: 1.10963947e-06
Iter: 1861 loss: 1.10904944e-06
Iter: 1862 loss: 1.11217878e-06
Iter: 1863 loss: 1.10896144e-06
Iter: 1864 loss: 1.10858048e-06
Iter: 1865 loss: 1.10820872e-06
Iter: 1866 loss: 1.10810879e-06
Iter: 1867 loss: 1.10762039e-06
Iter: 1868 loss: 1.11335953e-06
Iter: 1869 loss: 1.10759163e-06
Iter: 1870 loss: 1.10712926e-06
Iter: 1871 loss: 1.10778251e-06
Iter: 1872 loss: 1.10690166e-06
Iter: 1873 loss: 1.10647773e-06
Iter: 1874 loss: 1.10784276e-06
Iter: 1875 loss: 1.10636574e-06
Iter: 1876 loss: 1.10584983e-06
Iter: 1877 loss: 1.10738222e-06
Iter: 1878 loss: 1.1057241e-06
Iter: 1879 loss: 1.10529254e-06
Iter: 1880 loss: 1.10514804e-06
Iter: 1881 loss: 1.1048578e-06
Iter: 1882 loss: 1.10432461e-06
Iter: 1883 loss: 1.10668725e-06
Iter: 1884 loss: 1.1041991e-06
Iter: 1885 loss: 1.10370365e-06
Iter: 1886 loss: 1.10790643e-06
Iter: 1887 loss: 1.10367773e-06
Iter: 1888 loss: 1.10328733e-06
Iter: 1889 loss: 1.1028651e-06
Iter: 1890 loss: 1.10284645e-06
Iter: 1891 loss: 1.10219662e-06
Iter: 1892 loss: 1.10317796e-06
Iter: 1893 loss: 1.1018551e-06
Iter: 1894 loss: 1.10150893e-06
Iter: 1895 loss: 1.10148119e-06
Iter: 1896 loss: 1.10111819e-06
Iter: 1897 loss: 1.1005684e-06
Iter: 1898 loss: 1.10054e-06
Iter: 1899 loss: 1.09989378e-06
Iter: 1900 loss: 1.10146698e-06
Iter: 1901 loss: 1.09967016e-06
Iter: 1902 loss: 1.09913242e-06
Iter: 1903 loss: 1.09912264e-06
Iter: 1904 loss: 1.09883706e-06
Iter: 1905 loss: 1.0984752e-06
Iter: 1906 loss: 1.09841733e-06
Iter: 1907 loss: 1.09790267e-06
Iter: 1908 loss: 1.1019398e-06
Iter: 1909 loss: 1.09790756e-06
Iter: 1910 loss: 1.09738789e-06
Iter: 1911 loss: 1.09736186e-06
Iter: 1912 loss: 1.09706798e-06
Iter: 1913 loss: 1.09651876e-06
Iter: 1914 loss: 1.09826919e-06
Iter: 1915 loss: 1.09642519e-06
Iter: 1916 loss: 1.09596704e-06
Iter: 1917 loss: 1.09879625e-06
Iter: 1918 loss: 1.09590178e-06
Iter: 1919 loss: 1.09541861e-06
Iter: 1920 loss: 1.09529992e-06
Iter: 1921 loss: 1.09496909e-06
Iter: 1922 loss: 1.09448479e-06
Iter: 1923 loss: 1.09470011e-06
Iter: 1924 loss: 1.09410632e-06
Iter: 1925 loss: 1.09350276e-06
Iter: 1926 loss: 1.09833354e-06
Iter: 1927 loss: 1.09343796e-06
Iter: 1928 loss: 1.09287191e-06
Iter: 1929 loss: 1.09500536e-06
Iter: 1930 loss: 1.09272696e-06
Iter: 1931 loss: 1.0922588e-06
Iter: 1932 loss: 1.0919639e-06
Iter: 1933 loss: 1.09181849e-06
Iter: 1934 loss: 1.09131224e-06
Iter: 1935 loss: 1.09807252e-06
Iter: 1936 loss: 1.09129883e-06
Iter: 1937 loss: 1.09082191e-06
Iter: 1938 loss: 1.09109737e-06
Iter: 1939 loss: 1.0905103e-06
Iter: 1940 loss: 1.09010296e-06
Iter: 1941 loss: 1.09285588e-06
Iter: 1942 loss: 1.09005248e-06
Iter: 1943 loss: 1.089644e-06
Iter: 1944 loss: 1.08997324e-06
Iter: 1945 loss: 1.08936342e-06
Iter: 1946 loss: 1.08893471e-06
Iter: 1947 loss: 1.08925917e-06
Iter: 1948 loss: 1.08869085e-06
Iter: 1949 loss: 1.08820848e-06
Iter: 1950 loss: 1.09120219e-06
Iter: 1951 loss: 1.08813583e-06
Iter: 1952 loss: 1.08771292e-06
Iter: 1953 loss: 1.0893533e-06
Iter: 1954 loss: 1.08760355e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script73
+ '[' -r STOP.script73 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi0.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi1.2
+ date
Sat Nov  7 14:13:51 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi1.2/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi1.2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi1.2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi1.2_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi1.2/500_500_500_500_1 --optimizer lbfgs --function f2 --psi 0 --alpha 1.2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi1.2_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496d37510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496d37598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496cf7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496cf78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496cf7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496cbf9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496c29158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496cc8bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496bd1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496c05d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496c05488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496c05378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496b8a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496b84400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496b49488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496b498c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496ad4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496ad4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496af1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496ad4268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496ac9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4645dd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4645bcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4645dd400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd464589268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd464585ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd464555ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4644fa2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4644faea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4644ce2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3e4628620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3e4653620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3e4653378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3e4618840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3e46189d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd3e46188c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.19734133e-05
Iter: 2 loss: 3.5045392e-05
Iter: 3 loss: 6.91906e-05
Iter: 4 loss: 3.38735408e-05
Iter: 5 loss: 2.95677473e-05
Iter: 6 loss: 3.70307971e-05
Iter: 7 loss: 2.76663068e-05
Iter: 8 loss: 2.43494324e-05
Iter: 9 loss: 3.17225094e-05
Iter: 10 loss: 2.30598107e-05
Iter: 11 loss: 2.03737072e-05
Iter: 12 loss: 4.09227941e-05
Iter: 13 loss: 2.01662442e-05
Iter: 14 loss: 1.87124751e-05
Iter: 15 loss: 2.45794217e-05
Iter: 16 loss: 1.83898392e-05
Iter: 17 loss: 1.70725725e-05
Iter: 18 loss: 1.70542517e-05
Iter: 19 loss: 1.60134496e-05
Iter: 20 loss: 1.44564674e-05
Iter: 21 loss: 2.94262081e-05
Iter: 22 loss: 1.43976213e-05
Iter: 23 loss: 1.33722988e-05
Iter: 24 loss: 1.48014096e-05
Iter: 25 loss: 1.28654074e-05
Iter: 26 loss: 1.19986753e-05
Iter: 27 loss: 1.29238724e-05
Iter: 28 loss: 1.15221865e-05
Iter: 29 loss: 1.10071041e-05
Iter: 30 loss: 1.09366019e-05
Iter: 31 loss: 1.05430317e-05
Iter: 32 loss: 9.72540874e-06
Iter: 33 loss: 2.37694185e-05
Iter: 34 loss: 9.70667861e-06
Iter: 35 loss: 9.03286673e-06
Iter: 36 loss: 1.1965788e-05
Iter: 37 loss: 8.8954439e-06
Iter: 38 loss: 8.37335483e-06
Iter: 39 loss: 1.08221448e-05
Iter: 40 loss: 8.27751e-06
Iter: 41 loss: 7.97471e-06
Iter: 42 loss: 7.96331824e-06
Iter: 43 loss: 7.76506386e-06
Iter: 44 loss: 7.68301288e-06
Iter: 45 loss: 7.57854e-06
Iter: 46 loss: 7.32645913e-06
Iter: 47 loss: 8.95284393e-06
Iter: 48 loss: 7.29888598e-06
Iter: 49 loss: 7.09005417e-06
Iter: 50 loss: 7.29302428e-06
Iter: 51 loss: 6.97143787e-06
Iter: 52 loss: 6.77062371e-06
Iter: 53 loss: 7.5880821e-06
Iter: 54 loss: 6.7261476e-06
Iter: 55 loss: 6.52199469e-06
Iter: 56 loss: 6.85830037e-06
Iter: 57 loss: 6.42890336e-06
Iter: 58 loss: 6.22454036e-06
Iter: 59 loss: 7.32461604e-06
Iter: 60 loss: 6.19399361e-06
Iter: 61 loss: 6.0483826e-06
Iter: 62 loss: 6.20137962e-06
Iter: 63 loss: 5.96797054e-06
Iter: 64 loss: 5.83230076e-06
Iter: 65 loss: 6.63598439e-06
Iter: 66 loss: 5.81522181e-06
Iter: 67 loss: 5.66014296e-06
Iter: 68 loss: 5.92482138e-06
Iter: 69 loss: 5.59087175e-06
Iter: 70 loss: 5.46926913e-06
Iter: 71 loss: 5.54161761e-06
Iter: 72 loss: 5.39074335e-06
Iter: 73 loss: 5.25786936e-06
Iter: 74 loss: 5.36909056e-06
Iter: 75 loss: 5.178832e-06
Iter: 76 loss: 5.06428478e-06
Iter: 77 loss: 5.06419201e-06
Iter: 78 loss: 4.96068697e-06
Iter: 79 loss: 5.35523077e-06
Iter: 80 loss: 4.936197e-06
Iter: 81 loss: 4.86515364e-06
Iter: 82 loss: 4.84184466e-06
Iter: 83 loss: 4.80060453e-06
Iter: 84 loss: 4.72290685e-06
Iter: 85 loss: 5.82926623e-06
Iter: 86 loss: 4.72286547e-06
Iter: 87 loss: 4.66280653e-06
Iter: 88 loss: 4.61594118e-06
Iter: 89 loss: 4.59713829e-06
Iter: 90 loss: 4.53182429e-06
Iter: 91 loss: 5.11313374e-06
Iter: 92 loss: 4.52856239e-06
Iter: 93 loss: 4.46213653e-06
Iter: 94 loss: 4.46503873e-06
Iter: 95 loss: 4.4099047e-06
Iter: 96 loss: 4.34304411e-06
Iter: 97 loss: 4.7644462e-06
Iter: 98 loss: 4.33557307e-06
Iter: 99 loss: 4.27869554e-06
Iter: 100 loss: 4.36472374e-06
Iter: 101 loss: 4.25197732e-06
Iter: 102 loss: 4.20100559e-06
Iter: 103 loss: 4.75536081e-06
Iter: 104 loss: 4.19987555e-06
Iter: 105 loss: 4.15787235e-06
Iter: 106 loss: 4.11545716e-06
Iter: 107 loss: 4.1072617e-06
Iter: 108 loss: 4.04838693e-06
Iter: 109 loss: 4.08541246e-06
Iter: 110 loss: 4.01111811e-06
Iter: 111 loss: 3.94513154e-06
Iter: 112 loss: 4.16619241e-06
Iter: 113 loss: 3.92719403e-06
Iter: 114 loss: 3.90024343e-06
Iter: 115 loss: 3.89018e-06
Iter: 116 loss: 3.86543525e-06
Iter: 117 loss: 3.81948348e-06
Iter: 118 loss: 4.86079e-06
Iter: 119 loss: 3.81940345e-06
Iter: 120 loss: 3.7802256e-06
Iter: 121 loss: 4.19980643e-06
Iter: 122 loss: 3.77922606e-06
Iter: 123 loss: 3.7465129e-06
Iter: 124 loss: 3.84099167e-06
Iter: 125 loss: 3.73632247e-06
Iter: 126 loss: 3.70325756e-06
Iter: 127 loss: 3.67647112e-06
Iter: 128 loss: 3.6666861e-06
Iter: 129 loss: 3.62812807e-06
Iter: 130 loss: 4.21496588e-06
Iter: 131 loss: 3.62809897e-06
Iter: 132 loss: 3.59797286e-06
Iter: 133 loss: 3.60834883e-06
Iter: 134 loss: 3.57661838e-06
Iter: 135 loss: 3.54828444e-06
Iter: 136 loss: 3.65531469e-06
Iter: 137 loss: 3.54154258e-06
Iter: 138 loss: 3.51053404e-06
Iter: 139 loss: 3.62769333e-06
Iter: 140 loss: 3.50315122e-06
Iter: 141 loss: 3.47536593e-06
Iter: 142 loss: 3.57123668e-06
Iter: 143 loss: 3.46806701e-06
Iter: 144 loss: 3.44241539e-06
Iter: 145 loss: 3.42579278e-06
Iter: 146 loss: 3.4159516e-06
Iter: 147 loss: 3.3812662e-06
Iter: 148 loss: 3.50251207e-06
Iter: 149 loss: 3.37225265e-06
Iter: 150 loss: 3.34150218e-06
Iter: 151 loss: 3.44583668e-06
Iter: 152 loss: 3.33325124e-06
Iter: 153 loss: 3.30394096e-06
Iter: 154 loss: 3.67838038e-06
Iter: 155 loss: 3.30369539e-06
Iter: 156 loss: 3.2862738e-06
Iter: 157 loss: 3.26460486e-06
Iter: 158 loss: 3.26275199e-06
Iter: 159 loss: 3.23989548e-06
Iter: 160 loss: 3.36377593e-06
Iter: 161 loss: 3.23660424e-06
Iter: 162 loss: 3.20981599e-06
Iter: 163 loss: 3.29321301e-06
Iter: 164 loss: 3.20199206e-06
Iter: 165 loss: 3.18430466e-06
Iter: 166 loss: 3.17232616e-06
Iter: 167 loss: 3.16578416e-06
Iter: 168 loss: 3.14182489e-06
Iter: 169 loss: 3.48792e-06
Iter: 170 loss: 3.1418349e-06
Iter: 171 loss: 3.12342036e-06
Iter: 172 loss: 3.10379778e-06
Iter: 173 loss: 3.10067162e-06
Iter: 174 loss: 3.07653886e-06
Iter: 175 loss: 3.2967298e-06
Iter: 176 loss: 3.07549158e-06
Iter: 177 loss: 3.05622757e-06
Iter: 178 loss: 3.16891192e-06
Iter: 179 loss: 3.05372396e-06
Iter: 180 loss: 3.03955539e-06
Iter: 181 loss: 3.02774743e-06
Iter: 182 loss: 3.02374019e-06
Iter: 183 loss: 3.00018155e-06
Iter: 184 loss: 3.10700193e-06
Iter: 185 loss: 2.99563908e-06
Iter: 186 loss: 2.9781329e-06
Iter: 187 loss: 2.98702889e-06
Iter: 188 loss: 2.96633857e-06
Iter: 189 loss: 2.95061045e-06
Iter: 190 loss: 2.9505436e-06
Iter: 191 loss: 2.93479343e-06
Iter: 192 loss: 2.93670269e-06
Iter: 193 loss: 2.92262712e-06
Iter: 194 loss: 2.90813591e-06
Iter: 195 loss: 2.90927528e-06
Iter: 196 loss: 2.8969348e-06
Iter: 197 loss: 2.88070532e-06
Iter: 198 loss: 3.09380744e-06
Iter: 199 loss: 2.88058573e-06
Iter: 200 loss: 2.86541808e-06
Iter: 201 loss: 2.8660952e-06
Iter: 202 loss: 2.85340093e-06
Iter: 203 loss: 2.8399952e-06
Iter: 204 loss: 2.88064757e-06
Iter: 205 loss: 2.83587474e-06
Iter: 206 loss: 2.82160704e-06
Iter: 207 loss: 2.88470665e-06
Iter: 208 loss: 2.81864436e-06
Iter: 209 loss: 2.80289487e-06
Iter: 210 loss: 2.80596055e-06
Iter: 211 loss: 2.79112692e-06
Iter: 212 loss: 2.77946287e-06
Iter: 213 loss: 2.91364654e-06
Iter: 214 loss: 2.77921572e-06
Iter: 215 loss: 2.76815467e-06
Iter: 216 loss: 2.78290918e-06
Iter: 217 loss: 2.76260243e-06
Iter: 218 loss: 2.75084221e-06
Iter: 219 loss: 2.74664581e-06
Iter: 220 loss: 2.73989349e-06
Iter: 221 loss: 2.72719626e-06
Iter: 222 loss: 2.80578502e-06
Iter: 223 loss: 2.72561442e-06
Iter: 224 loss: 2.71288695e-06
Iter: 225 loss: 2.73736714e-06
Iter: 226 loss: 2.70759938e-06
Iter: 227 loss: 2.69740121e-06
Iter: 228 loss: 2.84856105e-06
Iter: 229 loss: 2.69738302e-06
Iter: 230 loss: 2.68875237e-06
Iter: 231 loss: 2.6813716e-06
Iter: 232 loss: 2.67901078e-06
Iter: 233 loss: 2.66738084e-06
Iter: 234 loss: 2.67585756e-06
Iter: 235 loss: 2.66029201e-06
Iter: 236 loss: 2.6510063e-06
Iter: 237 loss: 2.65101471e-06
Iter: 238 loss: 2.64083519e-06
Iter: 239 loss: 2.63119637e-06
Iter: 240 loss: 2.62882054e-06
Iter: 241 loss: 2.61590958e-06
Iter: 242 loss: 2.65527046e-06
Iter: 243 loss: 2.61199966e-06
Iter: 244 loss: 2.60077741e-06
Iter: 245 loss: 2.70231476e-06
Iter: 246 loss: 2.60024808e-06
Iter: 247 loss: 2.59087e-06
Iter: 248 loss: 2.6022683e-06
Iter: 249 loss: 2.58601358e-06
Iter: 250 loss: 2.57795637e-06
Iter: 251 loss: 2.60933325e-06
Iter: 252 loss: 2.57604529e-06
Iter: 253 loss: 2.56626709e-06
Iter: 254 loss: 2.58758382e-06
Iter: 255 loss: 2.56246062e-06
Iter: 256 loss: 2.55541408e-06
Iter: 257 loss: 2.545396e-06
Iter: 258 loss: 2.5452141e-06
Iter: 259 loss: 2.53205735e-06
Iter: 260 loss: 2.61739888e-06
Iter: 261 loss: 2.5307404e-06
Iter: 262 loss: 2.5207512e-06
Iter: 263 loss: 2.63203219e-06
Iter: 264 loss: 2.52052632e-06
Iter: 265 loss: 2.51353731e-06
Iter: 266 loss: 2.53758253e-06
Iter: 267 loss: 2.51172082e-06
Iter: 268 loss: 2.50480366e-06
Iter: 269 loss: 2.49631876e-06
Iter: 270 loss: 2.49545155e-06
Iter: 271 loss: 2.48569677e-06
Iter: 272 loss: 2.51332949e-06
Iter: 273 loss: 2.48256151e-06
Iter: 274 loss: 2.47458979e-06
Iter: 275 loss: 2.47456933e-06
Iter: 276 loss: 2.46837135e-06
Iter: 277 loss: 2.46383684e-06
Iter: 278 loss: 2.46171339e-06
Iter: 279 loss: 2.45230876e-06
Iter: 280 loss: 2.46799573e-06
Iter: 281 loss: 2.44799412e-06
Iter: 282 loss: 2.43988e-06
Iter: 283 loss: 2.52658651e-06
Iter: 284 loss: 2.43968111e-06
Iter: 285 loss: 2.43185696e-06
Iter: 286 loss: 2.43292106e-06
Iter: 287 loss: 2.42585566e-06
Iter: 288 loss: 2.41969428e-06
Iter: 289 loss: 2.48130209e-06
Iter: 290 loss: 2.41943826e-06
Iter: 291 loss: 2.41299631e-06
Iter: 292 loss: 2.40976533e-06
Iter: 293 loss: 2.40675536e-06
Iter: 294 loss: 2.39967858e-06
Iter: 295 loss: 2.40221539e-06
Iter: 296 loss: 2.3947382e-06
Iter: 297 loss: 2.38597227e-06
Iter: 298 loss: 2.44194166e-06
Iter: 299 loss: 2.38502662e-06
Iter: 300 loss: 2.37894278e-06
Iter: 301 loss: 2.47386606e-06
Iter: 302 loss: 2.37897075e-06
Iter: 303 loss: 2.37470931e-06
Iter: 304 loss: 2.36811388e-06
Iter: 305 loss: 2.36801975e-06
Iter: 306 loss: 2.35922244e-06
Iter: 307 loss: 2.39286646e-06
Iter: 308 loss: 2.35711741e-06
Iter: 309 loss: 2.35011021e-06
Iter: 310 loss: 2.36384517e-06
Iter: 311 loss: 2.34714639e-06
Iter: 312 loss: 2.34145091e-06
Iter: 313 loss: 2.41774592e-06
Iter: 314 loss: 2.34142658e-06
Iter: 315 loss: 2.33620494e-06
Iter: 316 loss: 2.33042147e-06
Iter: 317 loss: 2.32958928e-06
Iter: 318 loss: 2.32322054e-06
Iter: 319 loss: 2.34955041e-06
Iter: 320 loss: 2.32191269e-06
Iter: 321 loss: 2.31676e-06
Iter: 322 loss: 2.36221649e-06
Iter: 323 loss: 2.3165303e-06
Iter: 324 loss: 2.31128934e-06
Iter: 325 loss: 2.30708383e-06
Iter: 326 loss: 2.30548198e-06
Iter: 327 loss: 2.29988427e-06
Iter: 328 loss: 2.37318477e-06
Iter: 329 loss: 2.29979878e-06
Iter: 330 loss: 2.29492844e-06
Iter: 331 loss: 2.29600823e-06
Iter: 332 loss: 2.29130251e-06
Iter: 333 loss: 2.28540011e-06
Iter: 334 loss: 2.28287536e-06
Iter: 335 loss: 2.27984606e-06
Iter: 336 loss: 2.2742795e-06
Iter: 337 loss: 2.34484787e-06
Iter: 338 loss: 2.27416467e-06
Iter: 339 loss: 2.26890893e-06
Iter: 340 loss: 2.29412467e-06
Iter: 341 loss: 2.26789848e-06
Iter: 342 loss: 2.26339671e-06
Iter: 343 loss: 2.25942154e-06
Iter: 344 loss: 2.25832309e-06
Iter: 345 loss: 2.25267149e-06
Iter: 346 loss: 2.273463e-06
Iter: 347 loss: 2.25134e-06
Iter: 348 loss: 2.24492032e-06
Iter: 349 loss: 2.26407474e-06
Iter: 350 loss: 2.24303312e-06
Iter: 351 loss: 2.23725192e-06
Iter: 352 loss: 2.2780273e-06
Iter: 353 loss: 2.23661823e-06
Iter: 354 loss: 2.23202437e-06
Iter: 355 loss: 2.23264556e-06
Iter: 356 loss: 2.22846211e-06
Iter: 357 loss: 2.22273866e-06
Iter: 358 loss: 2.22903827e-06
Iter: 359 loss: 2.21970663e-06
Iter: 360 loss: 2.21529308e-06
Iter: 361 loss: 2.21531468e-06
Iter: 362 loss: 2.21147229e-06
Iter: 363 loss: 2.21028859e-06
Iter: 364 loss: 2.20788866e-06
Iter: 365 loss: 2.20231618e-06
Iter: 366 loss: 2.21918071e-06
Iter: 367 loss: 2.20067318e-06
Iter: 368 loss: 2.19535423e-06
Iter: 369 loss: 2.23022107e-06
Iter: 370 loss: 2.19477124e-06
Iter: 371 loss: 2.19150934e-06
Iter: 372 loss: 2.18434457e-06
Iter: 373 loss: 2.30189926e-06
Iter: 374 loss: 2.18415494e-06
Iter: 375 loss: 2.17682054e-06
Iter: 376 loss: 2.25038184e-06
Iter: 377 loss: 2.17659635e-06
Iter: 378 loss: 2.17273418e-06
Iter: 379 loss: 2.17256866e-06
Iter: 380 loss: 2.17027309e-06
Iter: 381 loss: 2.16376475e-06
Iter: 382 loss: 2.19290655e-06
Iter: 383 loss: 2.16125136e-06
Iter: 384 loss: 2.15405817e-06
Iter: 385 loss: 2.22839071e-06
Iter: 386 loss: 2.15382988e-06
Iter: 387 loss: 2.14931242e-06
Iter: 388 loss: 2.21033952e-06
Iter: 389 loss: 2.14936404e-06
Iter: 390 loss: 2.14566e-06
Iter: 391 loss: 2.14512238e-06
Iter: 392 loss: 2.14246393e-06
Iter: 393 loss: 2.13704311e-06
Iter: 394 loss: 2.15437171e-06
Iter: 395 loss: 2.13548947e-06
Iter: 396 loss: 2.13110752e-06
Iter: 397 loss: 2.13450244e-06
Iter: 398 loss: 2.12845339e-06
Iter: 399 loss: 2.12461646e-06
Iter: 400 loss: 2.12462191e-06
Iter: 401 loss: 2.12149121e-06
Iter: 402 loss: 2.11802808e-06
Iter: 403 loss: 2.11751876e-06
Iter: 404 loss: 2.1136043e-06
Iter: 405 loss: 2.15810769e-06
Iter: 406 loss: 2.11353881e-06
Iter: 407 loss: 2.10997769e-06
Iter: 408 loss: 2.10909593e-06
Iter: 409 loss: 2.1068422e-06
Iter: 410 loss: 2.10206326e-06
Iter: 411 loss: 2.1082808e-06
Iter: 412 loss: 2.099536e-06
Iter: 413 loss: 2.09532959e-06
Iter: 414 loss: 2.12429518e-06
Iter: 415 loss: 2.09486461e-06
Iter: 416 loss: 2.09071504e-06
Iter: 417 loss: 2.11734891e-06
Iter: 418 loss: 2.09014934e-06
Iter: 419 loss: 2.0876555e-06
Iter: 420 loss: 2.08140318e-06
Iter: 421 loss: 2.14101601e-06
Iter: 422 loss: 2.08063852e-06
Iter: 423 loss: 2.07568564e-06
Iter: 424 loss: 2.07568019e-06
Iter: 425 loss: 2.07205949e-06
Iter: 426 loss: 2.09090308e-06
Iter: 427 loss: 2.07156972e-06
Iter: 428 loss: 2.06751974e-06
Iter: 429 loss: 2.06638038e-06
Iter: 430 loss: 2.06392e-06
Iter: 431 loss: 2.06024606e-06
Iter: 432 loss: 2.08044548e-06
Iter: 433 loss: 2.05969491e-06
Iter: 434 loss: 2.05631682e-06
Iter: 435 loss: 2.06574896e-06
Iter: 436 loss: 2.05524111e-06
Iter: 437 loss: 2.05151036e-06
Iter: 438 loss: 2.06510458e-06
Iter: 439 loss: 2.0506709e-06
Iter: 440 loss: 2.04682283e-06
Iter: 441 loss: 2.04909702e-06
Iter: 442 loss: 2.04446519e-06
Iter: 443 loss: 2.04062235e-06
Iter: 444 loss: 2.05804827e-06
Iter: 445 loss: 2.03983518e-06
Iter: 446 loss: 2.03598211e-06
Iter: 447 loss: 2.04442358e-06
Iter: 448 loss: 2.03442141e-06
Iter: 449 loss: 2.03037689e-06
Iter: 450 loss: 2.027547e-06
Iter: 451 loss: 2.02614501e-06
Iter: 452 loss: 2.02382898e-06
Iter: 453 loss: 2.02322872e-06
Iter: 454 loss: 2.02048295e-06
Iter: 455 loss: 2.01882403e-06
Iter: 456 loss: 2.0177315e-06
Iter: 457 loss: 2.01414787e-06
Iter: 458 loss: 2.00967224e-06
Iter: 459 loss: 2.00935801e-06
Iter: 460 loss: 2.00536692e-06
Iter: 461 loss: 2.00530894e-06
Iter: 462 loss: 2.00183672e-06
Iter: 463 loss: 2.01995908e-06
Iter: 464 loss: 2.00127579e-06
Iter: 465 loss: 1.99885585e-06
Iter: 466 loss: 1.99523174e-06
Iter: 467 loss: 1.99517353e-06
Iter: 468 loss: 1.99087117e-06
Iter: 469 loss: 2.03532113e-06
Iter: 470 loss: 1.99083934e-06
Iter: 471 loss: 1.98770863e-06
Iter: 472 loss: 2.00296677e-06
Iter: 473 loss: 1.98711541e-06
Iter: 474 loss: 1.98422481e-06
Iter: 475 loss: 1.98498719e-06
Iter: 476 loss: 1.98214548e-06
Iter: 477 loss: 1.97810277e-06
Iter: 478 loss: 1.98667885e-06
Iter: 479 loss: 1.97642044e-06
Iter: 480 loss: 1.97267082e-06
Iter: 481 loss: 1.99361898e-06
Iter: 482 loss: 1.97208828e-06
Iter: 483 loss: 1.96850192e-06
Iter: 484 loss: 1.97094869e-06
Iter: 485 loss: 1.96627934e-06
Iter: 486 loss: 1.96292149e-06
Iter: 487 loss: 1.97429472e-06
Iter: 488 loss: 1.96192264e-06
Iter: 489 loss: 1.95907228e-06
Iter: 490 loss: 1.99974875e-06
Iter: 491 loss: 1.95904749e-06
Iter: 492 loss: 1.95714506e-06
Iter: 493 loss: 1.95240295e-06
Iter: 494 loss: 1.99378132e-06
Iter: 495 loss: 1.95156781e-06
Iter: 496 loss: 1.94720269e-06
Iter: 497 loss: 1.98683961e-06
Iter: 498 loss: 1.94697782e-06
Iter: 499 loss: 1.94407494e-06
Iter: 500 loss: 1.96744804e-06
Iter: 501 loss: 1.94388554e-06
Iter: 502 loss: 1.94064592e-06
Iter: 503 loss: 1.94212544e-06
Iter: 504 loss: 1.93839151e-06
Iter: 505 loss: 1.93482128e-06
Iter: 506 loss: 1.93665392e-06
Iter: 507 loss: 1.93245728e-06
Iter: 508 loss: 1.92930111e-06
Iter: 509 loss: 1.95417465e-06
Iter: 510 loss: 1.92912103e-06
Iter: 511 loss: 1.92565039e-06
Iter: 512 loss: 1.9339e-06
Iter: 513 loss: 1.92434936e-06
Iter: 514 loss: 1.92130551e-06
Iter: 515 loss: 1.92575817e-06
Iter: 516 loss: 1.91986601e-06
Iter: 517 loss: 1.91664731e-06
Iter: 518 loss: 1.92952666e-06
Iter: 519 loss: 1.91595495e-06
Iter: 520 loss: 1.91299341e-06
Iter: 521 loss: 1.92004882e-06
Iter: 522 loss: 1.91183608e-06
Iter: 523 loss: 1.90871106e-06
Iter: 524 loss: 1.91407435e-06
Iter: 525 loss: 1.90737524e-06
Iter: 526 loss: 1.90411015e-06
Iter: 527 loss: 1.91895469e-06
Iter: 528 loss: 1.90352057e-06
Iter: 529 loss: 1.90098558e-06
Iter: 530 loss: 1.92317066e-06
Iter: 531 loss: 1.900847e-06
Iter: 532 loss: 1.89896809e-06
Iter: 533 loss: 1.89514128e-06
Iter: 534 loss: 1.96592737e-06
Iter: 535 loss: 1.8950708e-06
Iter: 536 loss: 1.89140542e-06
Iter: 537 loss: 1.90758556e-06
Iter: 538 loss: 1.89064872e-06
Iter: 539 loss: 1.88747083e-06
Iter: 540 loss: 1.90128083e-06
Iter: 541 loss: 1.88680735e-06
Iter: 542 loss: 1.88278977e-06
Iter: 543 loss: 1.8988751e-06
Iter: 544 loss: 1.8819012e-06
Iter: 545 loss: 1.87994669e-06
Iter: 546 loss: 1.87632e-06
Iter: 547 loss: 1.87631144e-06
Iter: 548 loss: 1.87324417e-06
Iter: 549 loss: 1.8731273e-06
Iter: 550 loss: 1.87053479e-06
Iter: 551 loss: 1.87769706e-06
Iter: 552 loss: 1.86969942e-06
Iter: 553 loss: 1.86781881e-06
Iter: 554 loss: 1.86669104e-06
Iter: 555 loss: 1.86589648e-06
Iter: 556 loss: 1.86242789e-06
Iter: 557 loss: 1.87962178e-06
Iter: 558 loss: 1.86179182e-06
Iter: 559 loss: 1.85890303e-06
Iter: 560 loss: 1.86585726e-06
Iter: 561 loss: 1.85788679e-06
Iter: 562 loss: 1.85533747e-06
Iter: 563 loss: 1.86562352e-06
Iter: 564 loss: 1.8549224e-06
Iter: 565 loss: 1.85263229e-06
Iter: 566 loss: 1.86134196e-06
Iter: 567 loss: 1.85202407e-06
Iter: 568 loss: 1.84951512e-06
Iter: 569 loss: 1.85295482e-06
Iter: 570 loss: 1.84826831e-06
Iter: 571 loss: 1.84572605e-06
Iter: 572 loss: 1.84345686e-06
Iter: 573 loss: 1.84282419e-06
Iter: 574 loss: 1.83927978e-06
Iter: 575 loss: 1.85925796e-06
Iter: 576 loss: 1.83874772e-06
Iter: 577 loss: 1.83621262e-06
Iter: 578 loss: 1.84485771e-06
Iter: 579 loss: 1.83544626e-06
Iter: 580 loss: 1.83167867e-06
Iter: 581 loss: 1.84276007e-06
Iter: 582 loss: 1.83059956e-06
Iter: 583 loss: 1.82805786e-06
Iter: 584 loss: 1.82826545e-06
Iter: 585 loss: 1.82614554e-06
Iter: 586 loss: 1.82314466e-06
Iter: 587 loss: 1.83345946e-06
Iter: 588 loss: 1.82237352e-06
Iter: 589 loss: 1.81921371e-06
Iter: 590 loss: 1.84635394e-06
Iter: 591 loss: 1.81905671e-06
Iter: 592 loss: 1.81672749e-06
Iter: 593 loss: 1.81500445e-06
Iter: 594 loss: 1.81423229e-06
Iter: 595 loss: 1.81143537e-06
Iter: 596 loss: 1.82736096e-06
Iter: 597 loss: 1.81099631e-06
Iter: 598 loss: 1.8085052e-06
Iter: 599 loss: 1.82207077e-06
Iter: 600 loss: 1.80813311e-06
Iter: 601 loss: 1.80611369e-06
Iter: 602 loss: 1.80451229e-06
Iter: 603 loss: 1.80394841e-06
Iter: 604 loss: 1.80197958e-06
Iter: 605 loss: 1.80182906e-06
Iter: 606 loss: 1.80047994e-06
Iter: 607 loss: 1.79926292e-06
Iter: 608 loss: 1.79895756e-06
Iter: 609 loss: 1.79619087e-06
Iter: 610 loss: 1.79746451e-06
Iter: 611 loss: 1.79437927e-06
Iter: 612 loss: 1.79158565e-06
Iter: 613 loss: 1.79437336e-06
Iter: 614 loss: 1.79002893e-06
Iter: 615 loss: 1.78674293e-06
Iter: 616 loss: 1.80575876e-06
Iter: 617 loss: 1.78640266e-06
Iter: 618 loss: 1.78338405e-06
Iter: 619 loss: 1.80865447e-06
Iter: 620 loss: 1.78317566e-06
Iter: 621 loss: 1.78142136e-06
Iter: 622 loss: 1.77965808e-06
Iter: 623 loss: 1.77923994e-06
Iter: 624 loss: 1.77647496e-06
Iter: 625 loss: 1.78117341e-06
Iter: 626 loss: 1.77525567e-06
Iter: 627 loss: 1.77293055e-06
Iter: 628 loss: 1.77286813e-06
Iter: 629 loss: 1.77113793e-06
Iter: 630 loss: 1.76832407e-06
Iter: 631 loss: 1.76834305e-06
Iter: 632 loss: 1.76578783e-06
Iter: 633 loss: 1.78157006e-06
Iter: 634 loss: 1.7653889e-06
Iter: 635 loss: 1.76266929e-06
Iter: 636 loss: 1.77413301e-06
Iter: 637 loss: 1.76206913e-06
Iter: 638 loss: 1.75982939e-06
Iter: 639 loss: 1.76103515e-06
Iter: 640 loss: 1.75826278e-06
Iter: 641 loss: 1.75613445e-06
Iter: 642 loss: 1.78982691e-06
Iter: 643 loss: 1.75612524e-06
Iter: 644 loss: 1.75446701e-06
Iter: 645 loss: 1.75311106e-06
Iter: 646 loss: 1.75258037e-06
Iter: 647 loss: 1.74961906e-06
Iter: 648 loss: 1.7515124e-06
Iter: 649 loss: 1.7476973e-06
Iter: 650 loss: 1.74455772e-06
Iter: 651 loss: 1.7620755e-06
Iter: 652 loss: 1.74406455e-06
Iter: 653 loss: 1.74176853e-06
Iter: 654 loss: 1.74207833e-06
Iter: 655 loss: 1.73989042e-06
Iter: 656 loss: 1.73792341e-06
Iter: 657 loss: 1.73782496e-06
Iter: 658 loss: 1.73624119e-06
Iter: 659 loss: 1.73561034e-06
Iter: 660 loss: 1.73474336e-06
Iter: 661 loss: 1.73243336e-06
Iter: 662 loss: 1.72989598e-06
Iter: 663 loss: 1.72958744e-06
Iter: 664 loss: 1.72752652e-06
Iter: 665 loss: 1.727224e-06
Iter: 666 loss: 1.72518799e-06
Iter: 667 loss: 1.72681348e-06
Iter: 668 loss: 1.72398779e-06
Iter: 669 loss: 1.72192881e-06
Iter: 670 loss: 1.72114233e-06
Iter: 671 loss: 1.72008663e-06
Iter: 672 loss: 1.71782551e-06
Iter: 673 loss: 1.74046954e-06
Iter: 674 loss: 1.71772467e-06
Iter: 675 loss: 1.71550982e-06
Iter: 676 loss: 1.72081332e-06
Iter: 677 loss: 1.71466445e-06
Iter: 678 loss: 1.71265208e-06
Iter: 679 loss: 1.71564488e-06
Iter: 680 loss: 1.71175452e-06
Iter: 681 loss: 1.7091179e-06
Iter: 682 loss: 1.72587102e-06
Iter: 683 loss: 1.70882049e-06
Iter: 684 loss: 1.70722433e-06
Iter: 685 loss: 1.70470662e-06
Iter: 686 loss: 1.70464602e-06
Iter: 687 loss: 1.70154294e-06
Iter: 688 loss: 1.71535464e-06
Iter: 689 loss: 1.7009371e-06
Iter: 690 loss: 1.69804161e-06
Iter: 691 loss: 1.7090515e-06
Iter: 692 loss: 1.6973587e-06
Iter: 693 loss: 1.69515874e-06
Iter: 694 loss: 1.70664953e-06
Iter: 695 loss: 1.69483633e-06
Iter: 696 loss: 1.6925834e-06
Iter: 697 loss: 1.70080511e-06
Iter: 698 loss: 1.69203906e-06
Iter: 699 loss: 1.69062514e-06
Iter: 700 loss: 1.68822373e-06
Iter: 701 loss: 1.74888078e-06
Iter: 702 loss: 1.68825545e-06
Iter: 703 loss: 1.68639758e-06
Iter: 704 loss: 1.6862748e-06
Iter: 705 loss: 1.68434985e-06
Iter: 706 loss: 1.68501276e-06
Iter: 707 loss: 1.68308475e-06
Iter: 708 loss: 1.68123574e-06
Iter: 709 loss: 1.68135364e-06
Iter: 710 loss: 1.67979556e-06
Iter: 711 loss: 1.6778215e-06
Iter: 712 loss: 1.70056512e-06
Iter: 713 loss: 1.67777546e-06
Iter: 714 loss: 1.6756851e-06
Iter: 715 loss: 1.67584471e-06
Iter: 716 loss: 1.67411588e-06
Iter: 717 loss: 1.67178507e-06
Iter: 718 loss: 1.69184727e-06
Iter: 719 loss: 1.67166877e-06
Iter: 720 loss: 1.66980612e-06
Iter: 721 loss: 1.67399185e-06
Iter: 722 loss: 1.66907944e-06
Iter: 723 loss: 1.66753216e-06
Iter: 724 loss: 1.66445216e-06
Iter: 725 loss: 1.7308588e-06
Iter: 726 loss: 1.66447194e-06
Iter: 727 loss: 1.66226164e-06
Iter: 728 loss: 1.66225118e-06
Iter: 729 loss: 1.66034295e-06
Iter: 730 loss: 1.66385985e-06
Iter: 731 loss: 1.65954339e-06
Iter: 732 loss: 1.65758331e-06
Iter: 733 loss: 1.66792108e-06
Iter: 734 loss: 1.65724043e-06
Iter: 735 loss: 1.65515939e-06
Iter: 736 loss: 1.65754068e-06
Iter: 737 loss: 1.65404504e-06
Iter: 738 loss: 1.65218239e-06
Iter: 739 loss: 1.65251481e-06
Iter: 740 loss: 1.65086294e-06
Iter: 741 loss: 1.64862558e-06
Iter: 742 loss: 1.67472399e-06
Iter: 743 loss: 1.64860694e-06
Iter: 744 loss: 1.64665971e-06
Iter: 745 loss: 1.64877565e-06
Iter: 746 loss: 1.64562152e-06
Iter: 747 loss: 1.64415042e-06
Iter: 748 loss: 1.64265316e-06
Iter: 749 loss: 1.64243465e-06
Iter: 750 loss: 1.64044661e-06
Iter: 751 loss: 1.64037056e-06
Iter: 752 loss: 1.63861455e-06
Iter: 753 loss: 1.63947379e-06
Iter: 754 loss: 1.63747984e-06
Iter: 755 loss: 1.6360143e-06
Iter: 756 loss: 1.65437e-06
Iter: 757 loss: 1.63601032e-06
Iter: 758 loss: 1.63470304e-06
Iter: 759 loss: 1.63164805e-06
Iter: 760 loss: 1.67025962e-06
Iter: 761 loss: 1.631396e-06
Iter: 762 loss: 1.62855827e-06
Iter: 763 loss: 1.64686548e-06
Iter: 764 loss: 1.62820083e-06
Iter: 765 loss: 1.62609297e-06
Iter: 766 loss: 1.62741208e-06
Iter: 767 loss: 1.6247202e-06
Iter: 768 loss: 1.62226092e-06
Iter: 769 loss: 1.64997675e-06
Iter: 770 loss: 1.62221431e-06
Iter: 771 loss: 1.62049605e-06
Iter: 772 loss: 1.62987953e-06
Iter: 773 loss: 1.62025049e-06
Iter: 774 loss: 1.61877801e-06
Iter: 775 loss: 1.62066601e-06
Iter: 776 loss: 1.61800983e-06
Iter: 777 loss: 1.61614071e-06
Iter: 778 loss: 1.61532932e-06
Iter: 779 loss: 1.61434741e-06
Iter: 780 loss: 1.61238506e-06
Iter: 781 loss: 1.63612378e-06
Iter: 782 loss: 1.6123746e-06
Iter: 783 loss: 1.61058051e-06
Iter: 784 loss: 1.61441585e-06
Iter: 785 loss: 1.60984166e-06
Iter: 786 loss: 1.60813988e-06
Iter: 787 loss: 1.60678678e-06
Iter: 788 loss: 1.60628383e-06
Iter: 789 loss: 1.60421769e-06
Iter: 790 loss: 1.61604942e-06
Iter: 791 loss: 1.60395336e-06
Iter: 792 loss: 1.60192053e-06
Iter: 793 loss: 1.61817024e-06
Iter: 794 loss: 1.60179911e-06
Iter: 795 loss: 1.60054367e-06
Iter: 796 loss: 1.60163927e-06
Iter: 797 loss: 1.59976832e-06
Iter: 798 loss: 1.59783781e-06
Iter: 799 loss: 1.60147601e-06
Iter: 800 loss: 1.59700357e-06
Iter: 801 loss: 1.59537535e-06
Iter: 802 loss: 1.59619935e-06
Iter: 803 loss: 1.59428237e-06
Iter: 804 loss: 1.59243268e-06
Iter: 805 loss: 1.59328988e-06
Iter: 806 loss: 1.5911113e-06
Iter: 807 loss: 1.58856699e-06
Iter: 808 loss: 1.59855176e-06
Iter: 809 loss: 1.58797093e-06
Iter: 810 loss: 1.58609771e-06
Iter: 811 loss: 1.60041566e-06
Iter: 812 loss: 1.58598709e-06
Iter: 813 loss: 1.58413513e-06
Iter: 814 loss: 1.59168394e-06
Iter: 815 loss: 1.58372904e-06
Iter: 816 loss: 1.58219154e-06
Iter: 817 loss: 1.58359626e-06
Iter: 818 loss: 1.58139983e-06
Iter: 819 loss: 1.57963439e-06
Iter: 820 loss: 1.58409466e-06
Iter: 821 loss: 1.5789501e-06
Iter: 822 loss: 1.57743796e-06
Iter: 823 loss: 1.58464286e-06
Iter: 824 loss: 1.57719796e-06
Iter: 825 loss: 1.57544173e-06
Iter: 826 loss: 1.57828333e-06
Iter: 827 loss: 1.57468776e-06
Iter: 828 loss: 1.57315367e-06
Iter: 829 loss: 1.57230056e-06
Iter: 830 loss: 1.57163515e-06
Iter: 831 loss: 1.56962233e-06
Iter: 832 loss: 1.58527132e-06
Iter: 833 loss: 1.56952672e-06
Iter: 834 loss: 1.56767237e-06
Iter: 835 loss: 1.57940144e-06
Iter: 836 loss: 1.56754902e-06
Iter: 837 loss: 1.56632359e-06
Iter: 838 loss: 1.56709029e-06
Iter: 839 loss: 1.56550846e-06
Iter: 840 loss: 1.56376223e-06
Iter: 841 loss: 1.57075181e-06
Iter: 842 loss: 1.56343162e-06
Iter: 843 loss: 1.56216436e-06
Iter: 844 loss: 1.56053693e-06
Iter: 845 loss: 1.56045508e-06
Iter: 846 loss: 1.55842076e-06
Iter: 847 loss: 1.56459e-06
Iter: 848 loss: 1.55775274e-06
Iter: 849 loss: 1.55569228e-06
Iter: 850 loss: 1.56190481e-06
Iter: 851 loss: 1.55505336e-06
Iter: 852 loss: 1.55349449e-06
Iter: 853 loss: 1.55355019e-06
Iter: 854 loss: 1.5523965e-06
Iter: 855 loss: 1.55170164e-06
Iter: 856 loss: 1.55120165e-06
Iter: 857 loss: 1.54945417e-06
Iter: 858 loss: 1.55166163e-06
Iter: 859 loss: 1.54853478e-06
Iter: 860 loss: 1.54653219e-06
Iter: 861 loss: 1.56078909e-06
Iter: 862 loss: 1.54630561e-06
Iter: 863 loss: 1.54469672e-06
Iter: 864 loss: 1.55118494e-06
Iter: 865 loss: 1.544322e-06
Iter: 866 loss: 1.54289523e-06
Iter: 867 loss: 1.54387385e-06
Iter: 868 loss: 1.54189524e-06
Iter: 869 loss: 1.54043505e-06
Iter: 870 loss: 1.539305e-06
Iter: 871 loss: 1.53880421e-06
Iter: 872 loss: 1.53811516e-06
Iter: 873 loss: 1.53762858e-06
Iter: 874 loss: 1.53661881e-06
Iter: 875 loss: 1.53537303e-06
Iter: 876 loss: 1.53532199e-06
Iter: 877 loss: 1.53371582e-06
Iter: 878 loss: 1.5492202e-06
Iter: 879 loss: 1.53371161e-06
Iter: 880 loss: 1.53254086e-06
Iter: 881 loss: 1.53175279e-06
Iter: 882 loss: 1.53132987e-06
Iter: 883 loss: 1.52935229e-06
Iter: 884 loss: 1.53006954e-06
Iter: 885 loss: 1.52792904e-06
Iter: 886 loss: 1.5261528e-06
Iter: 887 loss: 1.5347382e-06
Iter: 888 loss: 1.52589462e-06
Iter: 889 loss: 1.52440623e-06
Iter: 890 loss: 1.54147972e-06
Iter: 891 loss: 1.52441737e-06
Iter: 892 loss: 1.52306006e-06
Iter: 893 loss: 1.52371172e-06
Iter: 894 loss: 1.52210737e-06
Iter: 895 loss: 1.52079247e-06
Iter: 896 loss: 1.52002008e-06
Iter: 897 loss: 1.51950246e-06
Iter: 898 loss: 1.51771019e-06
Iter: 899 loss: 1.54240888e-06
Iter: 900 loss: 1.51773054e-06
Iter: 901 loss: 1.51623681e-06
Iter: 902 loss: 1.51976872e-06
Iter: 903 loss: 1.515794e-06
Iter: 904 loss: 1.5144517e-06
Iter: 905 loss: 1.51700306e-06
Iter: 906 loss: 1.5138952e-06
Iter: 907 loss: 1.51244831e-06
Iter: 908 loss: 1.51120639e-06
Iter: 909 loss: 1.51073436e-06
Iter: 910 loss: 1.50939559e-06
Iter: 911 loss: 1.5093475e-06
Iter: 912 loss: 1.50800656e-06
Iter: 913 loss: 1.51161896e-06
Iter: 914 loss: 1.50758819e-06
Iter: 915 loss: 1.50651124e-06
Iter: 916 loss: 1.50794506e-06
Iter: 917 loss: 1.50585606e-06
Iter: 918 loss: 1.50449182e-06
Iter: 919 loss: 1.50908068e-06
Iter: 920 loss: 1.50412802e-06
Iter: 921 loss: 1.50287906e-06
Iter: 922 loss: 1.50176766e-06
Iter: 923 loss: 1.50142728e-06
Iter: 924 loss: 1.49942412e-06
Iter: 925 loss: 1.50258552e-06
Iter: 926 loss: 1.49838934e-06
Iter: 927 loss: 1.49719062e-06
Iter: 928 loss: 1.49713605e-06
Iter: 929 loss: 1.49593461e-06
Iter: 930 loss: 1.49858852e-06
Iter: 931 loss: 1.49544576e-06
Iter: 932 loss: 1.49438438e-06
Iter: 933 loss: 1.49274683e-06
Iter: 934 loss: 1.49274103e-06
Iter: 935 loss: 1.49121092e-06
Iter: 936 loss: 1.49121934e-06
Iter: 937 loss: 1.49006485e-06
Iter: 938 loss: 1.49594882e-06
Iter: 939 loss: 1.48985339e-06
Iter: 940 loss: 1.48891229e-06
Iter: 941 loss: 1.48808704e-06
Iter: 942 loss: 1.48783988e-06
Iter: 943 loss: 1.4860841e-06
Iter: 944 loss: 1.4918802e-06
Iter: 945 loss: 1.48563333e-06
Iter: 946 loss: 1.48427989e-06
Iter: 947 loss: 1.48839968e-06
Iter: 948 loss: 1.48387119e-06
Iter: 949 loss: 1.48257311e-06
Iter: 950 loss: 1.49554671e-06
Iter: 951 loss: 1.48255606e-06
Iter: 952 loss: 1.48151457e-06
Iter: 953 loss: 1.48030949e-06
Iter: 954 loss: 1.48016238e-06
Iter: 955 loss: 1.4790013e-06
Iter: 956 loss: 1.47900937e-06
Iter: 957 loss: 1.47817445e-06
Iter: 958 loss: 1.47639616e-06
Iter: 959 loss: 1.5106898e-06
Iter: 960 loss: 1.47637502e-06
Iter: 961 loss: 1.47440232e-06
Iter: 962 loss: 1.48136439e-06
Iter: 963 loss: 1.47396474e-06
Iter: 964 loss: 1.47237256e-06
Iter: 965 loss: 1.48029096e-06
Iter: 966 loss: 1.47216031e-06
Iter: 967 loss: 1.47070477e-06
Iter: 968 loss: 1.48260915e-06
Iter: 969 loss: 1.47062258e-06
Iter: 970 loss: 1.46935531e-06
Iter: 971 loss: 1.46936759e-06
Iter: 972 loss: 1.46836101e-06
Iter: 973 loss: 1.46710158e-06
Iter: 974 loss: 1.46977686e-06
Iter: 975 loss: 1.46669277e-06
Iter: 976 loss: 1.46525895e-06
Iter: 977 loss: 1.47701394e-06
Iter: 978 loss: 1.46518448e-06
Iter: 979 loss: 1.46422326e-06
Iter: 980 loss: 1.46332684e-06
Iter: 981 loss: 1.46308207e-06
Iter: 982 loss: 1.46183766e-06
Iter: 983 loss: 1.4690595e-06
Iter: 984 loss: 1.4617101e-06
Iter: 985 loss: 1.46048671e-06
Iter: 986 loss: 1.46143736e-06
Iter: 987 loss: 1.45975287e-06
Iter: 988 loss: 1.45845218e-06
Iter: 989 loss: 1.47384367e-06
Iter: 990 loss: 1.45842159e-06
Iter: 991 loss: 1.45723334e-06
Iter: 992 loss: 1.4568044e-06
Iter: 993 loss: 1.4561474e-06
Iter: 994 loss: 1.45485592e-06
Iter: 995 loss: 1.46144498e-06
Iter: 996 loss: 1.45470517e-06
Iter: 997 loss: 1.45343347e-06
Iter: 998 loss: 1.45517311e-06
Iter: 999 loss: 1.45279409e-06
Iter: 1000 loss: 1.45136221e-06
Iter: 1001 loss: 1.45191848e-06
Iter: 1002 loss: 1.45034903e-06
Iter: 1003 loss: 1.44894977e-06
Iter: 1004 loss: 1.45255683e-06
Iter: 1005 loss: 1.44838123e-06
Iter: 1006 loss: 1.44754404e-06
Iter: 1007 loss: 1.44748469e-06
Iter: 1008 loss: 1.44662067e-06
Iter: 1009 loss: 1.44477713e-06
Iter: 1010 loss: 1.47565765e-06
Iter: 1011 loss: 1.44475791e-06
Iter: 1012 loss: 1.44299929e-06
Iter: 1013 loss: 1.45139404e-06
Iter: 1014 loss: 1.44277965e-06
Iter: 1015 loss: 1.44181172e-06
Iter: 1016 loss: 1.44180763e-06
Iter: 1017 loss: 1.44106684e-06
Iter: 1018 loss: 1.43959983e-06
Iter: 1019 loss: 1.47032574e-06
Iter: 1020 loss: 1.4395639e-06
Iter: 1021 loss: 1.43792488e-06
Iter: 1022 loss: 1.44169155e-06
Iter: 1023 loss: 1.43726425e-06
Iter: 1024 loss: 1.43647299e-06
Iter: 1025 loss: 1.43637158e-06
Iter: 1026 loss: 1.43546708e-06
Iter: 1027 loss: 1.4344829e-06
Iter: 1028 loss: 1.43433294e-06
Iter: 1029 loss: 1.43287775e-06
Iter: 1030 loss: 1.43696809e-06
Iter: 1031 loss: 1.43246757e-06
Iter: 1032 loss: 1.43127636e-06
Iter: 1033 loss: 1.43944703e-06
Iter: 1034 loss: 1.43114744e-06
Iter: 1035 loss: 1.43021953e-06
Iter: 1036 loss: 1.43133e-06
Iter: 1037 loss: 1.42970839e-06
Iter: 1038 loss: 1.42847989e-06
Iter: 1039 loss: 1.42957242e-06
Iter: 1040 loss: 1.42776048e-06
Iter: 1041 loss: 1.42630063e-06
Iter: 1042 loss: 1.42814474e-06
Iter: 1043 loss: 1.42555268e-06
Iter: 1044 loss: 1.42447743e-06
Iter: 1045 loss: 1.42445697e-06
Iter: 1046 loss: 1.42343993e-06
Iter: 1047 loss: 1.42367355e-06
Iter: 1048 loss: 1.42267891e-06
Iter: 1049 loss: 1.42138219e-06
Iter: 1050 loss: 1.42103499e-06
Iter: 1051 loss: 1.42022964e-06
Iter: 1052 loss: 1.419044e-06
Iter: 1053 loss: 1.43742909e-06
Iter: 1054 loss: 1.41905184e-06
Iter: 1055 loss: 1.41784221e-06
Iter: 1056 loss: 1.41908833e-06
Iter: 1057 loss: 1.41715748e-06
Iter: 1058 loss: 1.41600572e-06
Iter: 1059 loss: 1.41587395e-06
Iter: 1060 loss: 1.41513931e-06
Iter: 1061 loss: 1.41410351e-06
Iter: 1062 loss: 1.41413693e-06
Iter: 1063 loss: 1.41319197e-06
Iter: 1064 loss: 1.4134888e-06
Iter: 1065 loss: 1.41252826e-06
Iter: 1066 loss: 1.41152e-06
Iter: 1067 loss: 1.41034116e-06
Iter: 1068 loss: 1.41027181e-06
Iter: 1069 loss: 1.4086047e-06
Iter: 1070 loss: 1.42445947e-06
Iter: 1071 loss: 1.40853376e-06
Iter: 1072 loss: 1.40736017e-06
Iter: 1073 loss: 1.41454211e-06
Iter: 1074 loss: 1.40723171e-06
Iter: 1075 loss: 1.40617158e-06
Iter: 1076 loss: 1.40519296e-06
Iter: 1077 loss: 1.40488214e-06
Iter: 1078 loss: 1.40376483e-06
Iter: 1079 loss: 1.4161227e-06
Iter: 1080 loss: 1.40371912e-06
Iter: 1081 loss: 1.40275688e-06
Iter: 1082 loss: 1.40604016e-06
Iter: 1083 loss: 1.40253042e-06
Iter: 1084 loss: 1.40147176e-06
Iter: 1085 loss: 1.40236682e-06
Iter: 1086 loss: 1.40081261e-06
Iter: 1087 loss: 1.39963129e-06
Iter: 1088 loss: 1.40329769e-06
Iter: 1089 loss: 1.39921372e-06
Iter: 1090 loss: 1.39834799e-06
Iter: 1091 loss: 1.40086047e-06
Iter: 1092 loss: 1.39803819e-06
Iter: 1093 loss: 1.39679457e-06
Iter: 1094 loss: 1.39941301e-06
Iter: 1095 loss: 1.39624353e-06
Iter: 1096 loss: 1.39520535e-06
Iter: 1097 loss: 1.3954234e-06
Iter: 1098 loss: 1.39441795e-06
Iter: 1099 loss: 1.39371366e-06
Iter: 1100 loss: 1.39371741e-06
Iter: 1101 loss: 1.3929573e-06
Iter: 1102 loss: 1.39151109e-06
Iter: 1103 loss: 1.42449812e-06
Iter: 1104 loss: 1.39152064e-06
Iter: 1105 loss: 1.39026815e-06
Iter: 1106 loss: 1.39299618e-06
Iter: 1107 loss: 1.38971029e-06
Iter: 1108 loss: 1.38828682e-06
Iter: 1109 loss: 1.3911507e-06
Iter: 1110 loss: 1.38771316e-06
Iter: 1111 loss: 1.38646931e-06
Iter: 1112 loss: 1.39896565e-06
Iter: 1113 loss: 1.38643327e-06
Iter: 1114 loss: 1.3851527e-06
Iter: 1115 loss: 1.38748533e-06
Iter: 1116 loss: 1.38461178e-06
Iter: 1117 loss: 1.38354255e-06
Iter: 1118 loss: 1.38471762e-06
Iter: 1119 loss: 1.38294604e-06
Iter: 1120 loss: 1.3817488e-06
Iter: 1121 loss: 1.39454369e-06
Iter: 1122 loss: 1.38164796e-06
Iter: 1123 loss: 1.38089968e-06
Iter: 1124 loss: 1.37971779e-06
Iter: 1125 loss: 1.37963275e-06
Iter: 1126 loss: 1.37865686e-06
Iter: 1127 loss: 1.37865823e-06
Iter: 1128 loss: 1.37783638e-06
Iter: 1129 loss: 1.37938264e-06
Iter: 1130 loss: 1.37755774e-06
Iter: 1131 loss: 1.37660027e-06
Iter: 1132 loss: 1.37720417e-06
Iter: 1133 loss: 1.37600898e-06
Iter: 1134 loss: 1.3748554e-06
Iter: 1135 loss: 1.37815459e-06
Iter: 1136 loss: 1.37459494e-06
Iter: 1137 loss: 1.37360189e-06
Iter: 1138 loss: 1.38351697e-06
Iter: 1139 loss: 1.37351412e-06
Iter: 1140 loss: 1.37282018e-06
Iter: 1141 loss: 1.37184099e-06
Iter: 1142 loss: 1.37172333e-06
Iter: 1143 loss: 1.37055258e-06
Iter: 1144 loss: 1.3704888e-06
Iter: 1145 loss: 1.36959216e-06
Iter: 1146 loss: 1.36807716e-06
Iter: 1147 loss: 1.37486086e-06
Iter: 1148 loss: 1.36772314e-06
Iter: 1149 loss: 1.36666472e-06
Iter: 1150 loss: 1.36666449e-06
Iter: 1151 loss: 1.36576659e-06
Iter: 1152 loss: 1.36553319e-06
Iter: 1153 loss: 1.36501944e-06
Iter: 1154 loss: 1.36402127e-06
Iter: 1155 loss: 1.37095e-06
Iter: 1156 loss: 1.36397534e-06
Iter: 1157 loss: 1.36309154e-06
Iter: 1158 loss: 1.36582696e-06
Iter: 1159 loss: 1.3628196e-06
Iter: 1160 loss: 1.36207973e-06
Iter: 1161 loss: 1.36109043e-06
Iter: 1162 loss: 1.36098822e-06
Iter: 1163 loss: 1.36000199e-06
Iter: 1164 loss: 1.36000358e-06
Iter: 1165 loss: 1.35918208e-06
Iter: 1166 loss: 1.35942491e-06
Iter: 1167 loss: 1.35856885e-06
Iter: 1168 loss: 1.35769517e-06
Iter: 1169 loss: 1.35804521e-06
Iter: 1170 loss: 1.35711343e-06
Iter: 1171 loss: 1.35589733e-06
Iter: 1172 loss: 1.36958715e-06
Iter: 1173 loss: 1.35592234e-06
Iter: 1174 loss: 1.35523555e-06
Iter: 1175 loss: 1.35422761e-06
Iter: 1176 loss: 1.35425034e-06
Iter: 1177 loss: 1.35291884e-06
Iter: 1178 loss: 1.35433993e-06
Iter: 1179 loss: 1.35229561e-06
Iter: 1180 loss: 1.35076209e-06
Iter: 1181 loss: 1.35721211e-06
Iter: 1182 loss: 1.35047208e-06
Iter: 1183 loss: 1.34950528e-06
Iter: 1184 loss: 1.36205506e-06
Iter: 1185 loss: 1.34947516e-06
Iter: 1186 loss: 1.34861398e-06
Iter: 1187 loss: 1.34944105e-06
Iter: 1188 loss: 1.34804418e-06
Iter: 1189 loss: 1.34726292e-06
Iter: 1190 loss: 1.34986169e-06
Iter: 1191 loss: 1.34699758e-06
Iter: 1192 loss: 1.34599816e-06
Iter: 1193 loss: 1.34858544e-06
Iter: 1194 loss: 1.34551703e-06
Iter: 1195 loss: 1.34464017e-06
Iter: 1196 loss: 1.34481661e-06
Iter: 1197 loss: 1.34395032e-06
Iter: 1198 loss: 1.34296783e-06
Iter: 1199 loss: 1.35654273e-06
Iter: 1200 loss: 1.34296386e-06
Iter: 1201 loss: 1.34219135e-06
Iter: 1202 loss: 1.34206618e-06
Iter: 1203 loss: 1.34159848e-06
Iter: 1204 loss: 1.34045058e-06
Iter: 1205 loss: 1.34409947e-06
Iter: 1206 loss: 1.34012146e-06
Iter: 1207 loss: 1.33924527e-06
Iter: 1208 loss: 1.34466154e-06
Iter: 1209 loss: 1.33909737e-06
Iter: 1210 loss: 1.33822812e-06
Iter: 1211 loss: 1.33921321e-06
Iter: 1212 loss: 1.33771869e-06
Iter: 1213 loss: 1.33687945e-06
Iter: 1214 loss: 1.33554272e-06
Iter: 1215 loss: 1.33553635e-06
Iter: 1216 loss: 1.33377603e-06
Iter: 1217 loss: 1.34147297e-06
Iter: 1218 loss: 1.33349386e-06
Iter: 1219 loss: 1.33216577e-06
Iter: 1220 loss: 1.33702019e-06
Iter: 1221 loss: 1.33185358e-06
Iter: 1222 loss: 1.33062474e-06
Iter: 1223 loss: 1.33399e-06
Iter: 1224 loss: 1.33022468e-06
Iter: 1225 loss: 1.32920741e-06
Iter: 1226 loss: 1.34094444e-06
Iter: 1227 loss: 1.32917478e-06
Iter: 1228 loss: 1.32816865e-06
Iter: 1229 loss: 1.3288834e-06
Iter: 1230 loss: 1.32755008e-06
Iter: 1231 loss: 1.32656385e-06
Iter: 1232 loss: 1.33133847e-06
Iter: 1233 loss: 1.32639093e-06
Iter: 1234 loss: 1.32543141e-06
Iter: 1235 loss: 1.32745231e-06
Iter: 1236 loss: 1.32505227e-06
Iter: 1237 loss: 1.32421985e-06
Iter: 1238 loss: 1.32586649e-06
Iter: 1239 loss: 1.32393745e-06
Iter: 1240 loss: 1.32299738e-06
Iter: 1241 loss: 1.32827586e-06
Iter: 1242 loss: 1.32290211e-06
Iter: 1243 loss: 1.32211198e-06
Iter: 1244 loss: 1.32406262e-06
Iter: 1245 loss: 1.32185073e-06
Iter: 1246 loss: 1.32109653e-06
Iter: 1247 loss: 1.32134392e-06
Iter: 1248 loss: 1.32057789e-06
Iter: 1249 loss: 1.31968909e-06
Iter: 1250 loss: 1.32481068e-06
Iter: 1251 loss: 1.319494e-06
Iter: 1252 loss: 1.31854858e-06
Iter: 1253 loss: 1.31962884e-06
Iter: 1254 loss: 1.31806382e-06
Iter: 1255 loss: 1.31709669e-06
Iter: 1256 loss: 1.3165029e-06
Iter: 1257 loss: 1.31617662e-06
Iter: 1258 loss: 1.3147876e-06
Iter: 1259 loss: 1.31910451e-06
Iter: 1260 loss: 1.31433785e-06
Iter: 1261 loss: 1.31305922e-06
Iter: 1262 loss: 1.31864647e-06
Iter: 1263 loss: 1.31279751e-06
Iter: 1264 loss: 1.31170327e-06
Iter: 1265 loss: 1.31766546e-06
Iter: 1266 loss: 1.31155832e-06
Iter: 1267 loss: 1.31047932e-06
Iter: 1268 loss: 1.31777097e-06
Iter: 1269 loss: 1.31038394e-06
Iter: 1270 loss: 1.30975548e-06
Iter: 1271 loss: 1.30872183e-06
Iter: 1272 loss: 1.30868875e-06
Iter: 1273 loss: 1.30780381e-06
Iter: 1274 loss: 1.3077738e-06
Iter: 1275 loss: 1.30717672e-06
Iter: 1276 loss: 1.30639864e-06
Iter: 1277 loss: 1.3063526e-06
Iter: 1278 loss: 1.30540207e-06
Iter: 1279 loss: 1.32015225e-06
Iter: 1280 loss: 1.30541e-06
Iter: 1281 loss: 1.30483659e-06
Iter: 1282 loss: 1.30377373e-06
Iter: 1283 loss: 1.30380363e-06
Iter: 1284 loss: 1.30281376e-06
Iter: 1285 loss: 1.31349123e-06
Iter: 1286 loss: 1.30278852e-06
Iter: 1287 loss: 1.3019262e-06
Iter: 1288 loss: 1.30336207e-06
Iter: 1289 loss: 1.30152478e-06
Iter: 1290 loss: 1.3007284e-06
Iter: 1291 loss: 1.30219678e-06
Iter: 1292 loss: 1.30042167e-06
Iter: 1293 loss: 1.29946784e-06
Iter: 1294 loss: 1.30005969e-06
Iter: 1295 loss: 1.29889645e-06
Iter: 1296 loss: 1.29767704e-06
Iter: 1297 loss: 1.29986176e-06
Iter: 1298 loss: 1.29718251e-06
Iter: 1299 loss: 1.29617092e-06
Iter: 1300 loss: 1.30492685e-06
Iter: 1301 loss: 1.29614523e-06
Iter: 1302 loss: 1.29522311e-06
Iter: 1303 loss: 1.29855937e-06
Iter: 1304 loss: 1.2950311e-06
Iter: 1305 loss: 1.29421255e-06
Iter: 1306 loss: 1.29388695e-06
Iter: 1307 loss: 1.29351133e-06
Iter: 1308 loss: 1.29266255e-06
Iter: 1309 loss: 1.29274827e-06
Iter: 1310 loss: 1.29200407e-06
Iter: 1311 loss: 1.29109048e-06
Iter: 1312 loss: 1.29102318e-06
Iter: 1313 loss: 1.29005525e-06
Iter: 1314 loss: 1.29003968e-06
Iter: 1315 loss: 1.28935392e-06
Iter: 1316 loss: 1.28880924e-06
Iter: 1317 loss: 1.28861882e-06
Iter: 1318 loss: 1.28781539e-06
Iter: 1319 loss: 1.29145303e-06
Iter: 1320 loss: 1.28759973e-06
Iter: 1321 loss: 1.28664851e-06
Iter: 1322 loss: 1.2902168e-06
Iter: 1323 loss: 1.286415e-06
Iter: 1324 loss: 1.2856849e-06
Iter: 1325 loss: 1.28559441e-06
Iter: 1326 loss: 1.28498982e-06
Iter: 1327 loss: 1.28408533e-06
Iter: 1328 loss: 1.28667921e-06
Iter: 1329 loss: 1.28381191e-06
Iter: 1330 loss: 1.28267084e-06
Iter: 1331 loss: 1.2848277e-06
Iter: 1332 loss: 1.28220631e-06
Iter: 1333 loss: 1.28111333e-06
Iter: 1334 loss: 1.28541967e-06
Iter: 1335 loss: 1.28091483e-06
Iter: 1336 loss: 1.280064e-06
Iter: 1337 loss: 1.29123941e-06
Iter: 1338 loss: 1.28008571e-06
Iter: 1339 loss: 1.27948442e-06
Iter: 1340 loss: 1.27851945e-06
Iter: 1341 loss: 1.27850308e-06
Iter: 1342 loss: 1.27742726e-06
Iter: 1343 loss: 1.2867514e-06
Iter: 1344 loss: 1.27741691e-06
Iter: 1345 loss: 1.27648036e-06
Iter: 1346 loss: 1.27936369e-06
Iter: 1347 loss: 1.27616647e-06
Iter: 1348 loss: 1.27544047e-06
Iter: 1349 loss: 1.2770214e-06
Iter: 1350 loss: 1.27520116e-06
Iter: 1351 loss: 1.27421958e-06
Iter: 1352 loss: 1.27613953e-06
Iter: 1353 loss: 1.27385351e-06
Iter: 1354 loss: 1.27303929e-06
Iter: 1355 loss: 1.27313626e-06
Iter: 1356 loss: 1.27239923e-06
Iter: 1357 loss: 1.27179555e-06
Iter: 1358 loss: 1.27173644e-06
Iter: 1359 loss: 1.27126009e-06
Iter: 1360 loss: 1.2702036e-06
Iter: 1361 loss: 1.2880206e-06
Iter: 1362 loss: 1.27017734e-06
Iter: 1363 loss: 1.26896748e-06
Iter: 1364 loss: 1.2719388e-06
Iter: 1365 loss: 1.26860641e-06
Iter: 1366 loss: 1.26754185e-06
Iter: 1367 loss: 1.27161559e-06
Iter: 1368 loss: 1.26726991e-06
Iter: 1369 loss: 1.26614816e-06
Iter: 1370 loss: 1.27394446e-06
Iter: 1371 loss: 1.26607551e-06
Iter: 1372 loss: 1.26505506e-06
Iter: 1373 loss: 1.26835948e-06
Iter: 1374 loss: 1.26474538e-06
Iter: 1375 loss: 1.26395844e-06
Iter: 1376 loss: 1.26535861e-06
Iter: 1377 loss: 1.26357554e-06
Iter: 1378 loss: 1.26273414e-06
Iter: 1379 loss: 1.26447685e-06
Iter: 1380 loss: 1.26239127e-06
Iter: 1381 loss: 1.26150758e-06
Iter: 1382 loss: 1.26986231e-06
Iter: 1383 loss: 1.26142436e-06
Iter: 1384 loss: 1.26095529e-06
Iter: 1385 loss: 1.26070177e-06
Iter: 1386 loss: 1.2604313e-06
Iter: 1387 loss: 1.25956603e-06
Iter: 1388 loss: 1.26344969e-06
Iter: 1389 loss: 1.25943097e-06
Iter: 1390 loss: 1.25862414e-06
Iter: 1391 loss: 1.25861766e-06
Iter: 1392 loss: 1.2579959e-06
Iter: 1393 loss: 1.25708959e-06
Iter: 1394 loss: 1.26374846e-06
Iter: 1395 loss: 1.25705219e-06
Iter: 1396 loss: 1.25613246e-06
Iter: 1397 loss: 1.25675297e-06
Iter: 1398 loss: 1.25561e-06
Iter: 1399 loss: 1.25475071e-06
Iter: 1400 loss: 1.25382394e-06
Iter: 1401 loss: 1.25374049e-06
Iter: 1402 loss: 1.25236011e-06
Iter: 1403 loss: 1.2611456e-06
Iter: 1404 loss: 1.25223119e-06
Iter: 1405 loss: 1.25153588e-06
Iter: 1406 loss: 1.25149518e-06
Iter: 1407 loss: 1.25082909e-06
Iter: 1408 loss: 1.25103179e-06
Iter: 1409 loss: 1.25033853e-06
Iter: 1410 loss: 1.24953146e-06
Iter: 1411 loss: 1.2507744e-06
Iter: 1412 loss: 1.24918552e-06
Iter: 1413 loss: 1.24840813e-06
Iter: 1414 loss: 1.2543436e-06
Iter: 1415 loss: 1.24833377e-06
Iter: 1416 loss: 1.24766882e-06
Iter: 1417 loss: 1.24917847e-06
Iter: 1418 loss: 1.24743212e-06
Iter: 1419 loss: 1.24676058e-06
Iter: 1420 loss: 1.2472849e-06
Iter: 1421 loss: 1.24633527e-06
Iter: 1422 loss: 1.24546523e-06
Iter: 1423 loss: 1.24789972e-06
Iter: 1424 loss: 1.24521341e-06
Iter: 1425 loss: 1.24429357e-06
Iter: 1426 loss: 1.24701069e-06
Iter: 1427 loss: 1.24401777e-06
Iter: 1428 loss: 1.24340033e-06
Iter: 1429 loss: 1.24531948e-06
Iter: 1430 loss: 1.24325379e-06
Iter: 1431 loss: 1.24250118e-06
Iter: 1432 loss: 1.2441534e-06
Iter: 1433 loss: 1.24222151e-06
Iter: 1434 loss: 1.2414738e-06
Iter: 1435 loss: 1.24104497e-06
Iter: 1436 loss: 1.24068174e-06
Iter: 1437 loss: 1.23976054e-06
Iter: 1438 loss: 1.24197925e-06
Iter: 1439 loss: 1.23938435e-06
Iter: 1440 loss: 1.23836344e-06
Iter: 1441 loss: 1.24439805e-06
Iter: 1442 loss: 1.23824532e-06
Iter: 1443 loss: 1.23739505e-06
Iter: 1444 loss: 1.24600945e-06
Iter: 1445 loss: 1.23732275e-06
Iter: 1446 loss: 1.23685595e-06
Iter: 1447 loss: 1.23607435e-06
Iter: 1448 loss: 1.23608379e-06
Iter: 1449 loss: 1.23537154e-06
Iter: 1450 loss: 1.23542725e-06
Iter: 1451 loss: 1.23463155e-06
Iter: 1452 loss: 1.23416885e-06
Iter: 1453 loss: 1.23384837e-06
Iter: 1454 loss: 1.23310622e-06
Iter: 1455 loss: 1.23877771e-06
Iter: 1456 loss: 1.23300902e-06
Iter: 1457 loss: 1.23227369e-06
Iter: 1458 loss: 1.23362815e-06
Iter: 1459 loss: 1.23199425e-06
Iter: 1460 loss: 1.23125687e-06
Iter: 1461 loss: 1.23275709e-06
Iter: 1462 loss: 1.23094401e-06
Iter: 1463 loss: 1.23026553e-06
Iter: 1464 loss: 1.23362224e-06
Iter: 1465 loss: 1.23014024e-06
Iter: 1466 loss: 1.22954975e-06
Iter: 1467 loss: 1.23042787e-06
Iter: 1468 loss: 1.22915e-06
Iter: 1469 loss: 1.22844494e-06
Iter: 1470 loss: 1.22953168e-06
Iter: 1471 loss: 1.22804624e-06
Iter: 1472 loss: 1.22729841e-06
Iter: 1473 loss: 1.22761264e-06
Iter: 1474 loss: 1.22670372e-06
Iter: 1475 loss: 1.2256678e-06
Iter: 1476 loss: 1.228565e-06
Iter: 1477 loss: 1.22538768e-06
Iter: 1478 loss: 1.22431686e-06
Iter: 1479 loss: 1.23764039e-06
Iter: 1480 loss: 1.22433016e-06
Iter: 1481 loss: 1.22382153e-06
Iter: 1482 loss: 1.22324536e-06
Iter: 1483 loss: 1.22319614e-06
Iter: 1484 loss: 1.22252595e-06
Iter: 1485 loss: 1.23212294e-06
Iter: 1486 loss: 1.22249526e-06
Iter: 1487 loss: 1.22182257e-06
Iter: 1488 loss: 1.22175948e-06
Iter: 1489 loss: 1.2212796e-06
Iter: 1490 loss: 1.22056656e-06
Iter: 1491 loss: 1.2218369e-06
Iter: 1492 loss: 1.220287e-06
Iter: 1493 loss: 1.21958192e-06
Iter: 1494 loss: 1.22601136e-06
Iter: 1495 loss: 1.21957987e-06
Iter: 1496 loss: 1.21905123e-06
Iter: 1497 loss: 1.21853964e-06
Iter: 1498 loss: 1.21840708e-06
Iter: 1499 loss: 1.21757216e-06
Iter: 1500 loss: 1.22430481e-06
Iter: 1501 loss: 1.21754238e-06
Iter: 1502 loss: 1.21683115e-06
Iter: 1503 loss: 1.21796825e-06
Iter: 1504 loss: 1.2164769e-06
Iter: 1505 loss: 1.2157692e-06
Iter: 1506 loss: 1.21623816e-06
Iter: 1507 loss: 1.21533799e-06
Iter: 1508 loss: 1.21435778e-06
Iter: 1509 loss: 1.2169105e-06
Iter: 1510 loss: 1.21403718e-06
Iter: 1511 loss: 1.21313985e-06
Iter: 1512 loss: 1.21543542e-06
Iter: 1513 loss: 1.21285666e-06
Iter: 1514 loss: 1.21225412e-06
Iter: 1515 loss: 1.21220296e-06
Iter: 1516 loss: 1.21165453e-06
Iter: 1517 loss: 1.21073492e-06
Iter: 1518 loss: 1.21076823e-06
Iter: 1519 loss: 1.21021253e-06
Iter: 1520 loss: 1.21020264e-06
Iter: 1521 loss: 1.20962636e-06
Iter: 1522 loss: 1.20996106e-06
Iter: 1523 loss: 1.20926177e-06
Iter: 1524 loss: 1.20857476e-06
Iter: 1525 loss: 1.20854156e-06
Iter: 1526 loss: 1.20808556e-06
Iter: 1527 loss: 1.20744312e-06
Iter: 1528 loss: 1.20743107e-06
Iter: 1529 loss: 1.20690424e-06
Iter: 1530 loss: 1.20647951e-06
Iter: 1531 loss: 1.20632831e-06
Iter: 1532 loss: 1.20555137e-06
Iter: 1533 loss: 1.20879213e-06
Iter: 1534 loss: 1.20536606e-06
Iter: 1535 loss: 1.20450147e-06
Iter: 1536 loss: 1.20732773e-06
Iter: 1537 loss: 1.20431514e-06
Iter: 1538 loss: 1.20365428e-06
Iter: 1539 loss: 1.20331663e-06
Iter: 1540 loss: 1.20296909e-06
Iter: 1541 loss: 1.20207505e-06
Iter: 1542 loss: 1.20638526e-06
Iter: 1543 loss: 1.20193454e-06
Iter: 1544 loss: 1.20100594e-06
Iter: 1545 loss: 1.20447817e-06
Iter: 1546 loss: 1.20077073e-06
Iter: 1547 loss: 1.20002e-06
Iter: 1548 loss: 1.20578125e-06
Iter: 1549 loss: 1.20000868e-06
Iter: 1550 loss: 1.19932383e-06
Iter: 1551 loss: 1.19936931e-06
Iter: 1552 loss: 1.19879985e-06
Iter: 1553 loss: 1.19814331e-06
Iter: 1554 loss: 1.20054858e-06
Iter: 1555 loss: 1.1979505e-06
Iter: 1556 loss: 1.19722438e-06
Iter: 1557 loss: 1.20136e-06
Iter: 1558 loss: 1.19709944e-06
Iter: 1559 loss: 1.1966174e-06
Iter: 1560 loss: 1.19616925e-06
Iter: 1561 loss: 1.19602055e-06
Iter: 1562 loss: 1.19536844e-06
Iter: 1563 loss: 1.19949323e-06
Iter: 1564 loss: 1.19531478e-06
Iter: 1565 loss: 1.19448623e-06
Iter: 1566 loss: 1.19612082e-06
Iter: 1567 loss: 1.19415245e-06
Iter: 1568 loss: 1.1934967e-06
Iter: 1569 loss: 1.19401057e-06
Iter: 1570 loss: 1.19308459e-06
Iter: 1571 loss: 1.19237404e-06
Iter: 1572 loss: 1.20018353e-06
Iter: 1573 loss: 1.19238121e-06
Iter: 1574 loss: 1.19180856e-06
Iter: 1575 loss: 1.19119568e-06
Iter: 1576 loss: 1.19109075e-06
Iter: 1577 loss: 1.19020183e-06
Iter: 1578 loss: 1.19213587e-06
Iter: 1579 loss: 1.18982575e-06
Iter: 1580 loss: 1.1890653e-06
Iter: 1581 loss: 1.19525043e-06
Iter: 1582 loss: 1.18900607e-06
Iter: 1583 loss: 1.18823743e-06
Iter: 1584 loss: 1.19199308e-06
Iter: 1585 loss: 1.18810965e-06
Iter: 1586 loss: 1.18756236e-06
Iter: 1587 loss: 1.18704884e-06
Iter: 1588 loss: 1.18688354e-06
Iter: 1589 loss: 1.18595301e-06
Iter: 1590 loss: 1.1944951e-06
Iter: 1591 loss: 1.18595926e-06
Iter: 1592 loss: 1.18542539e-06
Iter: 1593 loss: 1.18857315e-06
Iter: 1594 loss: 1.18532466e-06
Iter: 1595 loss: 1.18481773e-06
Iter: 1596 loss: 1.18419484e-06
Iter: 1597 loss: 1.18416892e-06
Iter: 1598 loss: 1.18339256e-06
Iter: 1599 loss: 1.18697938e-06
Iter: 1600 loss: 1.18322464e-06
Iter: 1601 loss: 1.18257981e-06
Iter: 1602 loss: 1.18889056e-06
Iter: 1603 loss: 1.18252649e-06
Iter: 1604 loss: 1.18202911e-06
Iter: 1605 loss: 1.18147886e-06
Iter: 1606 loss: 1.1814036e-06
Iter: 1607 loss: 1.18067567e-06
Iter: 1608 loss: 1.18612468e-06
Iter: 1609 loss: 1.18062462e-06
Iter: 1610 loss: 1.17991476e-06
Iter: 1611 loss: 1.18090156e-06
Iter: 1612 loss: 1.17953869e-06
Iter: 1613 loss: 1.17888419e-06
Iter: 1614 loss: 1.17878449e-06
Iter: 1615 loss: 1.17831485e-06
Iter: 1616 loss: 1.17737193e-06
Iter: 1617 loss: 1.18252217e-06
Iter: 1618 loss: 1.17726427e-06
Iter: 1619 loss: 1.17653803e-06
Iter: 1620 loss: 1.18640867e-06
Iter: 1621 loss: 1.17653394e-06
Iter: 1622 loss: 1.17612626e-06
Iter: 1623 loss: 1.17546165e-06
Iter: 1624 loss: 1.17543777e-06
Iter: 1625 loss: 1.17480158e-06
Iter: 1626 loss: 1.1845782e-06
Iter: 1627 loss: 1.17479988e-06
Iter: 1628 loss: 1.17421541e-06
Iter: 1629 loss: 1.17413038e-06
Iter: 1630 loss: 1.17373088e-06
Iter: 1631 loss: 1.17306081e-06
Iter: 1632 loss: 1.17496438e-06
Iter: 1633 loss: 1.17288039e-06
Iter: 1634 loss: 1.1721994e-06
Iter: 1635 loss: 1.17322611e-06
Iter: 1636 loss: 1.17183822e-06
Iter: 1637 loss: 1.17122033e-06
Iter: 1638 loss: 1.17718582e-06
Iter: 1639 loss: 1.17120703e-06
Iter: 1640 loss: 1.1706536e-06
Iter: 1641 loss: 1.17061768e-06
Iter: 1642 loss: 1.17018521e-06
Iter: 1643 loss: 1.16951571e-06
Iter: 1644 loss: 1.17104184e-06
Iter: 1645 loss: 1.16927583e-06
Iter: 1646 loss: 1.16853221e-06
Iter: 1647 loss: 1.17227285e-06
Iter: 1648 loss: 1.16840965e-06
Iter: 1649 loss: 1.16774879e-06
Iter: 1650 loss: 1.16777187e-06
Iter: 1651 loss: 1.16717661e-06
Iter: 1652 loss: 1.16639626e-06
Iter: 1653 loss: 1.16705064e-06
Iter: 1654 loss: 1.16580679e-06
Iter: 1655 loss: 1.16542355e-06
Iter: 1656 loss: 1.16526849e-06
Iter: 1657 loss: 1.16490537e-06
Iter: 1658 loss: 1.16442129e-06
Iter: 1659 loss: 1.16434342e-06
Iter: 1660 loss: 1.16378374e-06
Iter: 1661 loss: 1.16665262e-06
Iter: 1662 loss: 1.1636472e-06
Iter: 1663 loss: 1.16292313e-06
Iter: 1664 loss: 1.16384388e-06
Iter: 1665 loss: 1.16253341e-06
Iter: 1666 loss: 1.16184299e-06
Iter: 1667 loss: 1.1617592e-06
Iter: 1668 loss: 1.1612243e-06
Iter: 1669 loss: 1.16045e-06
Iter: 1670 loss: 1.16564763e-06
Iter: 1671 loss: 1.16034698e-06
Iter: 1672 loss: 1.15959824e-06
Iter: 1673 loss: 1.1643898e-06
Iter: 1674 loss: 1.15952162e-06
Iter: 1675 loss: 1.15906698e-06
Iter: 1676 loss: 1.15961484e-06
Iter: 1677 loss: 1.15885473e-06
Iter: 1678 loss: 1.15823491e-06
Iter: 1679 loss: 1.15855426e-06
Iter: 1680 loss: 1.15777789e-06
Iter: 1681 loss: 1.15701835e-06
Iter: 1682 loss: 1.16103581e-06
Iter: 1683 loss: 1.1569266e-06
Iter: 1684 loss: 1.1561965e-06
Iter: 1685 loss: 1.15813737e-06
Iter: 1686 loss: 1.15601631e-06
Iter: 1687 loss: 1.15541752e-06
Iter: 1688 loss: 1.15515058e-06
Iter: 1689 loss: 1.15485489e-06
Iter: 1690 loss: 1.15419675e-06
Iter: 1691 loss: 1.15421074e-06
Iter: 1692 loss: 1.1536157e-06
Iter: 1693 loss: 1.15360467e-06
Iter: 1694 loss: 1.15319062e-06
Iter: 1695 loss: 1.15256216e-06
Iter: 1696 loss: 1.15369494e-06
Iter: 1697 loss: 1.15232842e-06
Iter: 1698 loss: 1.15161515e-06
Iter: 1699 loss: 1.15799446e-06
Iter: 1700 loss: 1.15161765e-06
Iter: 1701 loss: 1.15118587e-06
Iter: 1702 loss: 1.1503995e-06
Iter: 1703 loss: 1.15036642e-06
Iter: 1704 loss: 1.14954298e-06
Iter: 1705 loss: 1.15271473e-06
Iter: 1706 loss: 1.14932345e-06
Iter: 1707 loss: 1.14845534e-06
Iter: 1708 loss: 1.15740022e-06
Iter: 1709 loss: 1.14845238e-06
Iter: 1710 loss: 1.14799695e-06
Iter: 1711 loss: 1.14787008e-06
Iter: 1712 loss: 1.14758132e-06
Iter: 1713 loss: 1.14691261e-06
Iter: 1714 loss: 1.1493554e-06
Iter: 1715 loss: 1.14677607e-06
Iter: 1716 loss: 1.14603836e-06
Iter: 1717 loss: 1.14764794e-06
Iter: 1718 loss: 1.14583304e-06
Iter: 1719 loss: 1.14518116e-06
Iter: 1720 loss: 1.14756585e-06
Iter: 1721 loss: 1.14501e-06
Iter: 1722 loss: 1.14435147e-06
Iter: 1723 loss: 1.14491252e-06
Iter: 1724 loss: 1.14399791e-06
Iter: 1725 loss: 1.14340696e-06
Iter: 1726 loss: 1.14991872e-06
Iter: 1727 loss: 1.14340276e-06
Iter: 1728 loss: 1.14283273e-06
Iter: 1729 loss: 1.14321642e-06
Iter: 1730 loss: 1.14246416e-06
Iter: 1731 loss: 1.14188686e-06
Iter: 1732 loss: 1.14194086e-06
Iter: 1733 loss: 1.14146235e-06
Iter: 1734 loss: 1.14066381e-06
Iter: 1735 loss: 1.14955674e-06
Iter: 1736 loss: 1.14062937e-06
Iter: 1737 loss: 1.14014074e-06
Iter: 1738 loss: 1.13935516e-06
Iter: 1739 loss: 1.13939973e-06
Iter: 1740 loss: 1.13864132e-06
Iter: 1741 loss: 1.1415558e-06
Iter: 1742 loss: 1.13844567e-06
Iter: 1743 loss: 1.13783358e-06
Iter: 1744 loss: 1.1467373e-06
Iter: 1745 loss: 1.13782608e-06
Iter: 1746 loss: 1.13738838e-06
Iter: 1747 loss: 1.13676856e-06
Iter: 1748 loss: 1.13674e-06
Iter: 1749 loss: 1.13609121e-06
Iter: 1750 loss: 1.13956037e-06
Iter: 1751 loss: 1.13597559e-06
Iter: 1752 loss: 1.13536157e-06
Iter: 1753 loss: 1.13819215e-06
Iter: 1754 loss: 1.13522719e-06
Iter: 1755 loss: 1.13465092e-06
Iter: 1756 loss: 1.13453189e-06
Iter: 1757 loss: 1.13419696e-06
Iter: 1758 loss: 1.13347551e-06
Iter: 1759 loss: 1.13888802e-06
Iter: 1760 loss: 1.13347915e-06
Iter: 1761 loss: 1.1328807e-06
Iter: 1762 loss: 1.13478291e-06
Iter: 1763 loss: 1.13273313e-06
Iter: 1764 loss: 1.13207977e-06
Iter: 1765 loss: 1.13259728e-06
Iter: 1766 loss: 1.13164754e-06
Iter: 1767 loss: 1.13104852e-06
Iter: 1768 loss: 1.13333181e-06
Iter: 1769 loss: 1.13087765e-06
Iter: 1770 loss: 1.13036458e-06
Iter: 1771 loss: 1.13431838e-06
Iter: 1772 loss: 1.13032252e-06
Iter: 1773 loss: 1.12980695e-06
Iter: 1774 loss: 1.12898579e-06
Iter: 1775 loss: 1.15014586e-06
Iter: 1776 loss: 1.12895577e-06
Iter: 1777 loss: 1.12804628e-06
Iter: 1778 loss: 1.13076248e-06
Iter: 1779 loss: 1.12786006e-06
Iter: 1780 loss: 1.12725843e-06
Iter: 1781 loss: 1.12724365e-06
Iter: 1782 loss: 1.12668704e-06
Iter: 1783 loss: 1.12603448e-06
Iter: 1784 loss: 1.1259408e-06
Iter: 1785 loss: 1.12521593e-06
Iter: 1786 loss: 1.12743498e-06
Iter: 1787 loss: 1.12503176e-06
Iter: 1788 loss: 1.12435396e-06
Iter: 1789 loss: 1.13039926e-06
Iter: 1790 loss: 1.12436305e-06
Iter: 1791 loss: 1.12383827e-06
Iter: 1792 loss: 1.12407508e-06
Iter: 1793 loss: 1.12345128e-06
Iter: 1794 loss: 1.12281623e-06
Iter: 1795 loss: 1.12367979e-06
Iter: 1796 loss: 1.12249654e-06
Iter: 1797 loss: 1.12181976e-06
Iter: 1798 loss: 1.13106273e-06
Iter: 1799 loss: 1.12181579e-06
Iter: 1800 loss: 1.12136581e-06
Iter: 1801 loss: 1.12092539e-06
Iter: 1802 loss: 1.12080033e-06
Iter: 1803 loss: 1.12007183e-06
Iter: 1804 loss: 1.12472276e-06
Iter: 1805 loss: 1.12001635e-06
Iter: 1806 loss: 1.11952147e-06
Iter: 1807 loss: 1.12256737e-06
Iter: 1808 loss: 1.11948702e-06
Iter: 1809 loss: 1.11898521e-06
Iter: 1810 loss: 1.11864517e-06
Iter: 1811 loss: 1.11845338e-06
Iter: 1812 loss: 1.11782776e-06
Iter: 1813 loss: 1.11808959e-06
Iter: 1814 loss: 1.11736699e-06
Iter: 1815 loss: 1.11674535e-06
Iter: 1816 loss: 1.12550811e-06
Iter: 1817 loss: 1.11676081e-06
Iter: 1818 loss: 1.11610905e-06
Iter: 1819 loss: 1.11687837e-06
Iter: 1820 loss: 1.11577742e-06
Iter: 1821 loss: 1.11518727e-06
Iter: 1822 loss: 1.11512122e-06
Iter: 1823 loss: 1.11475492e-06
Iter: 1824 loss: 1.11396275e-06
Iter: 1825 loss: 1.11718657e-06
Iter: 1826 loss: 1.11382496e-06
Iter: 1827 loss: 1.11301779e-06
Iter: 1828 loss: 1.11871896e-06
Iter: 1829 loss: 1.11297663e-06
Iter: 1830 loss: 1.11248596e-06
Iter: 1831 loss: 1.11234954e-06
Iter: 1832 loss: 1.11202962e-06
Iter: 1833 loss: 1.11147915e-06
Iter: 1834 loss: 1.12004977e-06
Iter: 1835 loss: 1.11148017e-06
Iter: 1836 loss: 1.11103031e-06
Iter: 1837 loss: 1.11043505e-06
Iter: 1838 loss: 1.11035092e-06
Iter: 1839 loss: 1.10987685e-06
Iter: 1840 loss: 1.11598661e-06
Iter: 1841 loss: 1.10980545e-06
Iter: 1842 loss: 1.10929091e-06
Iter: 1843 loss: 1.11029635e-06
Iter: 1844 loss: 1.10910025e-06
Iter: 1845 loss: 1.10856899e-06
Iter: 1846 loss: 1.10883661e-06
Iter: 1847 loss: 1.10827784e-06
Iter: 1848 loss: 1.10765416e-06
Iter: 1849 loss: 1.10955591e-06
Iter: 1850 loss: 1.10749e-06
Iter: 1851 loss: 1.10685369e-06
Iter: 1852 loss: 1.10786027e-06
Iter: 1853 loss: 1.10658e-06
Iter: 1854 loss: 1.10601923e-06
Iter: 1855 loss: 1.11222289e-06
Iter: 1856 loss: 1.10596989e-06
Iter: 1857 loss: 1.10548297e-06
Iter: 1858 loss: 1.10485098e-06
Iter: 1859 loss: 1.10482142e-06
Iter: 1860 loss: 1.10406108e-06
Iter: 1861 loss: 1.10446263e-06
Iter: 1862 loss: 1.1035296e-06
Iter: 1863 loss: 1.10300505e-06
Iter: 1864 loss: 1.10293252e-06
Iter: 1865 loss: 1.10244764e-06
Iter: 1866 loss: 1.10295798e-06
Iter: 1867 loss: 1.10219969e-06
Iter: 1868 loss: 1.10174733e-06
Iter: 1869 loss: 1.10426163e-06
Iter: 1870 loss: 1.10169435e-06
Iter: 1871 loss: 1.10121027e-06
Iter: 1872 loss: 1.10117e-06
Iter: 1873 loss: 1.10076087e-06
Iter: 1874 loss: 1.10020414e-06
Iter: 1875 loss: 1.10155383e-06
Iter: 1876 loss: 1.10004669e-06
Iter: 1877 loss: 1.09954362e-06
Iter: 1878 loss: 1.10557971e-06
Iter: 1879 loss: 1.09952464e-06
Iter: 1880 loss: 1.0991273e-06
Iter: 1881 loss: 1.09837788e-06
Iter: 1882 loss: 1.11392433e-06
Iter: 1883 loss: 1.09840175e-06
Iter: 1884 loss: 1.09756797e-06
Iter: 1885 loss: 1.10046369e-06
Iter: 1886 loss: 1.09735242e-06
Iter: 1887 loss: 1.09661471e-06
Iter: 1888 loss: 1.10368615e-06
Iter: 1889 loss: 1.09663347e-06
Iter: 1890 loss: 1.09613779e-06
Iter: 1891 loss: 1.09772873e-06
Iter: 1892 loss: 1.09595089e-06
Iter: 1893 loss: 1.09540963e-06
Iter: 1894 loss: 1.09556561e-06
Iter: 1895 loss: 1.09506414e-06
Iter: 1896 loss: 1.0943711e-06
Iter: 1897 loss: 1.09477594e-06
Iter: 1898 loss: 1.09394819e-06
Iter: 1899 loss: 1.09330881e-06
Iter: 1900 loss: 1.09901816e-06
Iter: 1901 loss: 1.09326845e-06
Iter: 1902 loss: 1.09266171e-06
Iter: 1903 loss: 1.09527468e-06
Iter: 1904 loss: 1.09257314e-06
Iter: 1905 loss: 1.09204962e-06
Iter: 1906 loss: 1.09245e-06
Iter: 1907 loss: 1.09178859e-06
Iter: 1908 loss: 1.0911524e-06
Iter: 1909 loss: 1.09618168e-06
Iter: 1910 loss: 1.09113353e-06
Iter: 1911 loss: 1.09074449e-06
Iter: 1912 loss: 1.090252e-06
Iter: 1913 loss: 1.09022858e-06
Iter: 1914 loss: 1.08965571e-06
Iter: 1915 loss: 1.09797088e-06
Iter: 1916 loss: 1.08966947e-06
Iter: 1917 loss: 1.08916788e-06
Iter: 1918 loss: 1.08877384e-06
Iter: 1919 loss: 1.08865095e-06
Iter: 1920 loss: 1.08795371e-06
Iter: 1921 loss: 1.08813538e-06
Iter: 1922 loss: 1.0874046e-06
Iter: 1923 loss: 1.08670952e-06
Iter: 1924 loss: 1.09319569e-06
Iter: 1925 loss: 1.08670872e-06
Iter: 1926 loss: 1.08626909e-06
Iter: 1927 loss: 1.09197504e-06
Iter: 1928 loss: 1.0862401e-06
Iter: 1929 loss: 1.08589882e-06
Iter: 1930 loss: 1.08525899e-06
Iter: 1931 loss: 1.0852666e-06
Iter: 1932 loss: 1.08456095e-06
Iter: 1933 loss: 1.090664e-06
Iter: 1934 loss: 1.084563e-06
Iter: 1935 loss: 1.08409881e-06
Iter: 1936 loss: 1.08452923e-06
Iter: 1937 loss: 1.08383654e-06
Iter: 1938 loss: 1.08323661e-06
Iter: 1939 loss: 1.08651682e-06
Iter: 1940 loss: 1.08314498e-06
Iter: 1941 loss: 1.08251379e-06
Iter: 1942 loss: 1.08316533e-06
Iter: 1943 loss: 1.08217068e-06
Iter: 1944 loss: 1.08166773e-06
Iter: 1945 loss: 1.0858829e-06
Iter: 1946 loss: 1.0816575e-06
Iter: 1947 loss: 1.08124027e-06
Iter: 1948 loss: 1.080939e-06
Iter: 1949 loss: 1.08080735e-06
Iter: 1950 loss: 1.08015502e-06
Iter: 1951 loss: 1.08339486e-06
Iter: 1952 loss: 1.08008726e-06
Iter: 1953 loss: 1.07945971e-06
Iter: 1954 loss: 1.08143229e-06
Iter: 1955 loss: 1.07924632e-06
Iter: 1956 loss: 1.07872506e-06
Iter: 1957 loss: 1.07820222e-06
Iter: 1958 loss: 1.07810524e-06
Iter: 1959 loss: 1.07732751e-06
Iter: 1960 loss: 1.07965525e-06
Iter: 1961 loss: 1.07707183e-06
Iter: 1962 loss: 1.07649191e-06
Iter: 1963 loss: 1.07650328e-06
Iter: 1964 loss: 1.07599965e-06
Iter: 1965 loss: 1.07654591e-06
Iter: 1966 loss: 1.07574101e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script73
+ '[' -r STOP.script73 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi1.2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi1.6
+ date
Sat Nov  7 14:41:08 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi1.6/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi1.6_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi1.6_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi1.6_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi1.6/500_500_500_500_1 --optimizer lbfgs --function f2 --psi 0 --alpha 1.6 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi1.6_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b798158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b7b5e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b7f7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b7d5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b7f7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b77c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b6d7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b6d7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b775840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b6b1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b691840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b6b18c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b6638c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b613488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b5d66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b6131e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b5d6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b5907b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b578158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b54cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b5290d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b4d5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b4e8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b4e8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b4e8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b45b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b45bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b45b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc8da60a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc9b45b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc8da606a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc8da18378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc8da60620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc8d9c9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc8d9c9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc8d9c96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.02288606e-05
Iter: 2 loss: 3.19871397e-05
Iter: 3 loss: 7.00177334e-05
Iter: 4 loss: 3.04113746e-05
Iter: 5 loss: 2.60238467e-05
Iter: 6 loss: 3.5605055e-05
Iter: 7 loss: 2.42743699e-05
Iter: 8 loss: 2.1115e-05
Iter: 9 loss: 2.54694423e-05
Iter: 10 loss: 1.95537978e-05
Iter: 11 loss: 1.68869665e-05
Iter: 12 loss: 3.49837501e-05
Iter: 13 loss: 1.66211121e-05
Iter: 14 loss: 1.52465618e-05
Iter: 15 loss: 2.86844e-05
Iter: 16 loss: 1.51982358e-05
Iter: 17 loss: 1.43007865e-05
Iter: 18 loss: 1.44118931e-05
Iter: 19 loss: 1.36158105e-05
Iter: 20 loss: 1.25781498e-05
Iter: 21 loss: 1.56249916e-05
Iter: 22 loss: 1.22585498e-05
Iter: 23 loss: 1.13225487e-05
Iter: 24 loss: 1.91856743e-05
Iter: 25 loss: 1.12678445e-05
Iter: 26 loss: 1.06606685e-05
Iter: 27 loss: 1.09261246e-05
Iter: 28 loss: 1.02463764e-05
Iter: 29 loss: 9.59111549e-06
Iter: 30 loss: 1.45889971e-05
Iter: 31 loss: 9.54134521e-06
Iter: 32 loss: 9.05818888e-06
Iter: 33 loss: 1.0086359e-05
Iter: 34 loss: 8.86864837e-06
Iter: 35 loss: 8.434763e-06
Iter: 36 loss: 8.17566433e-06
Iter: 37 loss: 7.99590362e-06
Iter: 38 loss: 7.46394289e-06
Iter: 39 loss: 1.1703135e-05
Iter: 40 loss: 7.42822976e-06
Iter: 41 loss: 7.10728727e-06
Iter: 42 loss: 9.12028736e-06
Iter: 43 loss: 7.07048594e-06
Iter: 44 loss: 6.76625677e-06
Iter: 45 loss: 8.79761865e-06
Iter: 46 loss: 6.73519253e-06
Iter: 47 loss: 6.54119322e-06
Iter: 48 loss: 6.37545236e-06
Iter: 49 loss: 6.32199499e-06
Iter: 50 loss: 6.10064581e-06
Iter: 51 loss: 7.15991155e-06
Iter: 52 loss: 6.06104459e-06
Iter: 53 loss: 5.85565385e-06
Iter: 54 loss: 7.44371255e-06
Iter: 55 loss: 5.84060263e-06
Iter: 56 loss: 5.69188023e-06
Iter: 57 loss: 5.73291572e-06
Iter: 58 loss: 5.58470629e-06
Iter: 59 loss: 5.41539885e-06
Iter: 60 loss: 6.43680232e-06
Iter: 61 loss: 5.39424082e-06
Iter: 62 loss: 5.26327494e-06
Iter: 63 loss: 5.34822766e-06
Iter: 64 loss: 5.18015622e-06
Iter: 65 loss: 5.02576813e-06
Iter: 66 loss: 6.49546246e-06
Iter: 67 loss: 5.01970499e-06
Iter: 68 loss: 4.93180642e-06
Iter: 69 loss: 4.90135153e-06
Iter: 70 loss: 4.85130295e-06
Iter: 71 loss: 4.72849661e-06
Iter: 72 loss: 5.75950799e-06
Iter: 73 loss: 4.72120291e-06
Iter: 74 loss: 4.63096421e-06
Iter: 75 loss: 4.66235542e-06
Iter: 76 loss: 4.56745238e-06
Iter: 77 loss: 4.47049752e-06
Iter: 78 loss: 4.66798201e-06
Iter: 79 loss: 4.43134422e-06
Iter: 80 loss: 4.35780839e-06
Iter: 81 loss: 4.35651145e-06
Iter: 82 loss: 4.29803413e-06
Iter: 83 loss: 4.24521158e-06
Iter: 84 loss: 4.23058e-06
Iter: 85 loss: 4.14355e-06
Iter: 86 loss: 4.22686162e-06
Iter: 87 loss: 4.09392851e-06
Iter: 88 loss: 4.01481202e-06
Iter: 89 loss: 4.74314311e-06
Iter: 90 loss: 4.01135912e-06
Iter: 91 loss: 3.95272764e-06
Iter: 92 loss: 4.35448965e-06
Iter: 93 loss: 3.94690687e-06
Iter: 94 loss: 3.90515e-06
Iter: 95 loss: 3.84478062e-06
Iter: 96 loss: 3.84293162e-06
Iter: 97 loss: 3.77113656e-06
Iter: 98 loss: 4.51354708e-06
Iter: 99 loss: 3.76913385e-06
Iter: 100 loss: 3.71433225e-06
Iter: 101 loss: 3.83145198e-06
Iter: 102 loss: 3.69282657e-06
Iter: 103 loss: 3.64219773e-06
Iter: 104 loss: 3.88738e-06
Iter: 105 loss: 3.63334721e-06
Iter: 106 loss: 3.59070873e-06
Iter: 107 loss: 3.6216486e-06
Iter: 108 loss: 3.56447322e-06
Iter: 109 loss: 3.51242397e-06
Iter: 110 loss: 3.69823829e-06
Iter: 111 loss: 3.49920629e-06
Iter: 112 loss: 3.44593127e-06
Iter: 113 loss: 3.57942e-06
Iter: 114 loss: 3.42731073e-06
Iter: 115 loss: 3.38851032e-06
Iter: 116 loss: 3.47878222e-06
Iter: 117 loss: 3.37423535e-06
Iter: 118 loss: 3.33317371e-06
Iter: 119 loss: 3.75102e-06
Iter: 120 loss: 3.33193771e-06
Iter: 121 loss: 3.30213788e-06
Iter: 122 loss: 3.2814628e-06
Iter: 123 loss: 3.27085627e-06
Iter: 124 loss: 3.23612721e-06
Iter: 125 loss: 3.23593122e-06
Iter: 126 loss: 3.2083833e-06
Iter: 127 loss: 3.17851e-06
Iter: 128 loss: 3.17715649e-06
Iter: 129 loss: 3.15009947e-06
Iter: 130 loss: 3.1584766e-06
Iter: 131 loss: 3.13078408e-06
Iter: 132 loss: 3.10126484e-06
Iter: 133 loss: 3.1289353e-06
Iter: 134 loss: 3.08423455e-06
Iter: 135 loss: 3.04839705e-06
Iter: 136 loss: 3.27267117e-06
Iter: 137 loss: 3.04433593e-06
Iter: 138 loss: 3.01612818e-06
Iter: 139 loss: 3.13666396e-06
Iter: 140 loss: 3.01031287e-06
Iter: 141 loss: 2.98740565e-06
Iter: 142 loss: 3.00837451e-06
Iter: 143 loss: 2.97414181e-06
Iter: 144 loss: 2.94625261e-06
Iter: 145 loss: 3.04118339e-06
Iter: 146 loss: 2.93867743e-06
Iter: 147 loss: 2.91037395e-06
Iter: 148 loss: 2.95409336e-06
Iter: 149 loss: 2.89711625e-06
Iter: 150 loss: 2.8714494e-06
Iter: 151 loss: 3.04724972e-06
Iter: 152 loss: 2.86895647e-06
Iter: 153 loss: 2.84929592e-06
Iter: 154 loss: 2.89432751e-06
Iter: 155 loss: 2.84190628e-06
Iter: 156 loss: 2.81860412e-06
Iter: 157 loss: 2.93347216e-06
Iter: 158 loss: 2.814543e-06
Iter: 159 loss: 2.79864935e-06
Iter: 160 loss: 2.78044558e-06
Iter: 161 loss: 2.77811796e-06
Iter: 162 loss: 2.75210527e-06
Iter: 163 loss: 2.83645454e-06
Iter: 164 loss: 2.7448541e-06
Iter: 165 loss: 2.72478019e-06
Iter: 166 loss: 2.93608696e-06
Iter: 167 loss: 2.72427542e-06
Iter: 168 loss: 2.70495775e-06
Iter: 169 loss: 2.72036868e-06
Iter: 170 loss: 2.69335578e-06
Iter: 171 loss: 2.67734436e-06
Iter: 172 loss: 2.6927e-06
Iter: 173 loss: 2.66822371e-06
Iter: 174 loss: 2.65086146e-06
Iter: 175 loss: 2.79634787e-06
Iter: 176 loss: 2.64984874e-06
Iter: 177 loss: 2.63247375e-06
Iter: 178 loss: 2.64179198e-06
Iter: 179 loss: 2.62099502e-06
Iter: 180 loss: 2.60596198e-06
Iter: 181 loss: 2.67277346e-06
Iter: 182 loss: 2.60298521e-06
Iter: 183 loss: 2.5881377e-06
Iter: 184 loss: 2.62741787e-06
Iter: 185 loss: 2.58308432e-06
Iter: 186 loss: 2.5691229e-06
Iter: 187 loss: 2.59231047e-06
Iter: 188 loss: 2.56279532e-06
Iter: 189 loss: 2.54737097e-06
Iter: 190 loss: 2.63433753e-06
Iter: 191 loss: 2.5452639e-06
Iter: 192 loss: 2.53226858e-06
Iter: 193 loss: 2.58965156e-06
Iter: 194 loss: 2.52964742e-06
Iter: 195 loss: 2.51745e-06
Iter: 196 loss: 2.52400241e-06
Iter: 197 loss: 2.50939047e-06
Iter: 198 loss: 2.49762024e-06
Iter: 199 loss: 2.49704794e-06
Iter: 200 loss: 2.48814013e-06
Iter: 201 loss: 2.47042499e-06
Iter: 202 loss: 2.55868e-06
Iter: 203 loss: 2.46749187e-06
Iter: 204 loss: 2.45822821e-06
Iter: 205 loss: 2.60494835e-06
Iter: 206 loss: 2.4581791e-06
Iter: 207 loss: 2.44983335e-06
Iter: 208 loss: 2.43457544e-06
Iter: 209 loss: 2.7921833e-06
Iter: 210 loss: 2.43458908e-06
Iter: 211 loss: 2.41920861e-06
Iter: 212 loss: 2.4878218e-06
Iter: 213 loss: 2.41618545e-06
Iter: 214 loss: 2.40484201e-06
Iter: 215 loss: 2.55980513e-06
Iter: 216 loss: 2.40477061e-06
Iter: 217 loss: 2.39571432e-06
Iter: 218 loss: 2.38601388e-06
Iter: 219 loss: 2.38439452e-06
Iter: 220 loss: 2.37133554e-06
Iter: 221 loss: 2.44237708e-06
Iter: 222 loss: 2.36938013e-06
Iter: 223 loss: 2.35752441e-06
Iter: 224 loss: 2.41032308e-06
Iter: 225 loss: 2.35511902e-06
Iter: 226 loss: 2.34600952e-06
Iter: 227 loss: 2.36201686e-06
Iter: 228 loss: 2.34202116e-06
Iter: 229 loss: 2.33331684e-06
Iter: 230 loss: 2.43000522e-06
Iter: 231 loss: 2.33306673e-06
Iter: 232 loss: 2.32668481e-06
Iter: 233 loss: 2.32381967e-06
Iter: 234 loss: 2.32065145e-06
Iter: 235 loss: 2.31061745e-06
Iter: 236 loss: 2.33162382e-06
Iter: 237 loss: 2.30668434e-06
Iter: 238 loss: 2.29674038e-06
Iter: 239 loss: 2.29360489e-06
Iter: 240 loss: 2.28770864e-06
Iter: 241 loss: 2.27811483e-06
Iter: 242 loss: 2.27799251e-06
Iter: 243 loss: 2.27052305e-06
Iter: 244 loss: 2.28372164e-06
Iter: 245 loss: 2.26717111e-06
Iter: 246 loss: 2.2582326e-06
Iter: 247 loss: 2.25366e-06
Iter: 248 loss: 2.2496256e-06
Iter: 249 loss: 2.24140308e-06
Iter: 250 loss: 2.31845979e-06
Iter: 251 loss: 2.24105042e-06
Iter: 252 loss: 2.23282632e-06
Iter: 253 loss: 2.25029453e-06
Iter: 254 loss: 2.22954645e-06
Iter: 255 loss: 2.22099015e-06
Iter: 256 loss: 2.22701965e-06
Iter: 257 loss: 2.21552341e-06
Iter: 258 loss: 2.20738707e-06
Iter: 259 loss: 2.24935775e-06
Iter: 260 loss: 2.20606603e-06
Iter: 261 loss: 2.19860658e-06
Iter: 262 loss: 2.23357347e-06
Iter: 263 loss: 2.19729372e-06
Iter: 264 loss: 2.1903013e-06
Iter: 265 loss: 2.19841718e-06
Iter: 266 loss: 2.18652963e-06
Iter: 267 loss: 2.1772978e-06
Iter: 268 loss: 2.22781341e-06
Iter: 269 loss: 2.17592469e-06
Iter: 270 loss: 2.17061142e-06
Iter: 271 loss: 2.16744456e-06
Iter: 272 loss: 2.16513854e-06
Iter: 273 loss: 2.15619684e-06
Iter: 274 loss: 2.17335719e-06
Iter: 275 loss: 2.1524329e-06
Iter: 276 loss: 2.14341844e-06
Iter: 277 loss: 2.20176298e-06
Iter: 278 loss: 2.14247689e-06
Iter: 279 loss: 2.13646445e-06
Iter: 280 loss: 2.15599584e-06
Iter: 281 loss: 2.1349465e-06
Iter: 282 loss: 2.12692294e-06
Iter: 283 loss: 2.1325759e-06
Iter: 284 loss: 2.12192435e-06
Iter: 285 loss: 2.11592169e-06
Iter: 286 loss: 2.12355644e-06
Iter: 287 loss: 2.11282259e-06
Iter: 288 loss: 2.10575263e-06
Iter: 289 loss: 2.14593274e-06
Iter: 290 loss: 2.10476605e-06
Iter: 291 loss: 2.09734935e-06
Iter: 292 loss: 2.11694396e-06
Iter: 293 loss: 2.09491327e-06
Iter: 294 loss: 2.08993e-06
Iter: 295 loss: 2.0932016e-06
Iter: 296 loss: 2.08683809e-06
Iter: 297 loss: 2.08043502e-06
Iter: 298 loss: 2.1062333e-06
Iter: 299 loss: 2.07885364e-06
Iter: 300 loss: 2.0717257e-06
Iter: 301 loss: 2.0979644e-06
Iter: 302 loss: 2.06989012e-06
Iter: 303 loss: 2.06474579e-06
Iter: 304 loss: 2.10205803e-06
Iter: 305 loss: 2.06430559e-06
Iter: 306 loss: 2.06045206e-06
Iter: 307 loss: 2.06163918e-06
Iter: 308 loss: 2.05773313e-06
Iter: 309 loss: 2.05228162e-06
Iter: 310 loss: 2.04497769e-06
Iter: 311 loss: 2.04459661e-06
Iter: 312 loss: 2.03729542e-06
Iter: 313 loss: 2.14985903e-06
Iter: 314 loss: 2.03729883e-06
Iter: 315 loss: 2.03215768e-06
Iter: 316 loss: 2.04439539e-06
Iter: 317 loss: 2.03036234e-06
Iter: 318 loss: 2.02410433e-06
Iter: 319 loss: 2.04151888e-06
Iter: 320 loss: 2.02204592e-06
Iter: 321 loss: 2.01562193e-06
Iter: 322 loss: 2.02353544e-06
Iter: 323 loss: 2.01229204e-06
Iter: 324 loss: 2.00627983e-06
Iter: 325 loss: 2.02638148e-06
Iter: 326 loss: 2.00451268e-06
Iter: 327 loss: 1.99998362e-06
Iter: 328 loss: 2.02235424e-06
Iter: 329 loss: 1.99913075e-06
Iter: 330 loss: 1.99361102e-06
Iter: 331 loss: 1.9998688e-06
Iter: 332 loss: 1.99065721e-06
Iter: 333 loss: 1.98561588e-06
Iter: 334 loss: 1.98319367e-06
Iter: 335 loss: 1.98077896e-06
Iter: 336 loss: 1.97801705e-06
Iter: 337 loss: 1.97700069e-06
Iter: 338 loss: 1.97378313e-06
Iter: 339 loss: 1.97052213e-06
Iter: 340 loss: 1.96984979e-06
Iter: 341 loss: 1.96445399e-06
Iter: 342 loss: 1.99810847e-06
Iter: 343 loss: 1.96379278e-06
Iter: 344 loss: 1.95951179e-06
Iter: 345 loss: 1.95643543e-06
Iter: 346 loss: 1.9550173e-06
Iter: 347 loss: 1.94938e-06
Iter: 348 loss: 1.96669271e-06
Iter: 349 loss: 1.9477111e-06
Iter: 350 loss: 1.94197787e-06
Iter: 351 loss: 1.97501981e-06
Iter: 352 loss: 1.94120685e-06
Iter: 353 loss: 1.93685969e-06
Iter: 354 loss: 1.96299925e-06
Iter: 355 loss: 1.93623555e-06
Iter: 356 loss: 1.9322556e-06
Iter: 357 loss: 1.93142455e-06
Iter: 358 loss: 1.92885591e-06
Iter: 359 loss: 1.92370567e-06
Iter: 360 loss: 1.94672657e-06
Iter: 361 loss: 1.9227441e-06
Iter: 362 loss: 1.91851154e-06
Iter: 363 loss: 1.92508742e-06
Iter: 364 loss: 1.91647473e-06
Iter: 365 loss: 1.91151958e-06
Iter: 366 loss: 1.9422173e-06
Iter: 367 loss: 1.91087747e-06
Iter: 368 loss: 1.90647665e-06
Iter: 369 loss: 1.90517426e-06
Iter: 370 loss: 1.90246624e-06
Iter: 371 loss: 1.89753177e-06
Iter: 372 loss: 1.91874551e-06
Iter: 373 loss: 1.89658158e-06
Iter: 374 loss: 1.89195111e-06
Iter: 375 loss: 1.93644246e-06
Iter: 376 loss: 1.8917483e-06
Iter: 377 loss: 1.88821298e-06
Iter: 378 loss: 1.88435411e-06
Iter: 379 loss: 1.8836331e-06
Iter: 380 loss: 1.87964474e-06
Iter: 381 loss: 1.93666347e-06
Iter: 382 loss: 1.87966884e-06
Iter: 383 loss: 1.87671276e-06
Iter: 384 loss: 1.87279898e-06
Iter: 385 loss: 1.8726289e-06
Iter: 386 loss: 1.86668717e-06
Iter: 387 loss: 1.88470176e-06
Iter: 388 loss: 1.864875e-06
Iter: 389 loss: 1.86089869e-06
Iter: 390 loss: 1.89675734e-06
Iter: 391 loss: 1.86072384e-06
Iter: 392 loss: 1.85636952e-06
Iter: 393 loss: 1.86012267e-06
Iter: 394 loss: 1.85387523e-06
Iter: 395 loss: 1.85005899e-06
Iter: 396 loss: 1.85667159e-06
Iter: 397 loss: 1.8483961e-06
Iter: 398 loss: 1.84380451e-06
Iter: 399 loss: 1.85930276e-06
Iter: 400 loss: 1.84258374e-06
Iter: 401 loss: 1.83867564e-06
Iter: 402 loss: 1.85243698e-06
Iter: 403 loss: 1.83774046e-06
Iter: 404 loss: 1.83362147e-06
Iter: 405 loss: 1.84934186e-06
Iter: 406 loss: 1.83276552e-06
Iter: 407 loss: 1.82934821e-06
Iter: 408 loss: 1.82744884e-06
Iter: 409 loss: 1.82584017e-06
Iter: 410 loss: 1.82235817e-06
Iter: 411 loss: 1.82232009e-06
Iter: 412 loss: 1.8192959e-06
Iter: 413 loss: 1.82470444e-06
Iter: 414 loss: 1.81792382e-06
Iter: 415 loss: 1.81526082e-06
Iter: 416 loss: 1.8143636e-06
Iter: 417 loss: 1.81282371e-06
Iter: 418 loss: 1.80852101e-06
Iter: 419 loss: 1.82759925e-06
Iter: 420 loss: 1.80755467e-06
Iter: 421 loss: 1.80383108e-06
Iter: 422 loss: 1.81475332e-06
Iter: 423 loss: 1.80271434e-06
Iter: 424 loss: 1.79949984e-06
Iter: 425 loss: 1.80033317e-06
Iter: 426 loss: 1.79721019e-06
Iter: 427 loss: 1.79369977e-06
Iter: 428 loss: 1.83775751e-06
Iter: 429 loss: 1.79363235e-06
Iter: 430 loss: 1.79029621e-06
Iter: 431 loss: 1.79026415e-06
Iter: 432 loss: 1.78761e-06
Iter: 433 loss: 1.78454286e-06
Iter: 434 loss: 1.78769574e-06
Iter: 435 loss: 1.78288394e-06
Iter: 436 loss: 1.77916e-06
Iter: 437 loss: 1.80494249e-06
Iter: 438 loss: 1.77874188e-06
Iter: 439 loss: 1.77497691e-06
Iter: 440 loss: 1.77963909e-06
Iter: 441 loss: 1.77314212e-06
Iter: 442 loss: 1.76967205e-06
Iter: 443 loss: 1.79145013e-06
Iter: 444 loss: 1.76929188e-06
Iter: 445 loss: 1.76642584e-06
Iter: 446 loss: 1.76517574e-06
Iter: 447 loss: 1.76365961e-06
Iter: 448 loss: 1.76000208e-06
Iter: 449 loss: 1.76004687e-06
Iter: 450 loss: 1.75792979e-06
Iter: 451 loss: 1.75480409e-06
Iter: 452 loss: 1.75471587e-06
Iter: 453 loss: 1.75135e-06
Iter: 454 loss: 1.76933634e-06
Iter: 455 loss: 1.75085165e-06
Iter: 456 loss: 1.74718207e-06
Iter: 457 loss: 1.75219964e-06
Iter: 458 loss: 1.74524143e-06
Iter: 459 loss: 1.74187926e-06
Iter: 460 loss: 1.75055629e-06
Iter: 461 loss: 1.74071329e-06
Iter: 462 loss: 1.73727688e-06
Iter: 463 loss: 1.74686579e-06
Iter: 464 loss: 1.73610306e-06
Iter: 465 loss: 1.73204444e-06
Iter: 466 loss: 1.75482955e-06
Iter: 467 loss: 1.73158173e-06
Iter: 468 loss: 1.72881e-06
Iter: 469 loss: 1.72824321e-06
Iter: 470 loss: 1.72648197e-06
Iter: 471 loss: 1.7225334e-06
Iter: 472 loss: 1.72770979e-06
Iter: 473 loss: 1.72052955e-06
Iter: 474 loss: 1.71688214e-06
Iter: 475 loss: 1.71687225e-06
Iter: 476 loss: 1.71453e-06
Iter: 477 loss: 1.71423176e-06
Iter: 478 loss: 1.71258034e-06
Iter: 479 loss: 1.70945373e-06
Iter: 480 loss: 1.72484079e-06
Iter: 481 loss: 1.70898261e-06
Iter: 482 loss: 1.70611634e-06
Iter: 483 loss: 1.71906856e-06
Iter: 484 loss: 1.70567421e-06
Iter: 485 loss: 1.70289763e-06
Iter: 486 loss: 1.70435067e-06
Iter: 487 loss: 1.70112219e-06
Iter: 488 loss: 1.69830139e-06
Iter: 489 loss: 1.70304816e-06
Iter: 490 loss: 1.69706664e-06
Iter: 491 loss: 1.69384441e-06
Iter: 492 loss: 1.69732868e-06
Iter: 493 loss: 1.69215264e-06
Iter: 494 loss: 1.68818906e-06
Iter: 495 loss: 1.71728334e-06
Iter: 496 loss: 1.68783652e-06
Iter: 497 loss: 1.68525833e-06
Iter: 498 loss: 1.68425277e-06
Iter: 499 loss: 1.68282895e-06
Iter: 500 loss: 1.67929852e-06
Iter: 501 loss: 1.71346028e-06
Iter: 502 loss: 1.67917813e-06
Iter: 503 loss: 1.67608e-06
Iter: 504 loss: 1.68092754e-06
Iter: 505 loss: 1.67462224e-06
Iter: 506 loss: 1.67188398e-06
Iter: 507 loss: 1.66834673e-06
Iter: 508 loss: 1.66814345e-06
Iter: 509 loss: 1.66550694e-06
Iter: 510 loss: 1.66531095e-06
Iter: 511 loss: 1.66242319e-06
Iter: 512 loss: 1.66088387e-06
Iter: 513 loss: 1.65958818e-06
Iter: 514 loss: 1.65653921e-06
Iter: 515 loss: 1.68312022e-06
Iter: 516 loss: 1.65633787e-06
Iter: 517 loss: 1.65389963e-06
Iter: 518 loss: 1.66174686e-06
Iter: 519 loss: 1.65322444e-06
Iter: 520 loss: 1.6502172e-06
Iter: 521 loss: 1.65086794e-06
Iter: 522 loss: 1.64807523e-06
Iter: 523 loss: 1.64536e-06
Iter: 524 loss: 1.65463234e-06
Iter: 525 loss: 1.64467178e-06
Iter: 526 loss: 1.64199446e-06
Iter: 527 loss: 1.6448123e-06
Iter: 528 loss: 1.64049379e-06
Iter: 529 loss: 1.63744267e-06
Iter: 530 loss: 1.65186975e-06
Iter: 531 loss: 1.63679101e-06
Iter: 532 loss: 1.63410073e-06
Iter: 533 loss: 1.6445822e-06
Iter: 534 loss: 1.63342634e-06
Iter: 535 loss: 1.63109371e-06
Iter: 536 loss: 1.63095854e-06
Iter: 537 loss: 1.62920674e-06
Iter: 538 loss: 1.6255774e-06
Iter: 539 loss: 1.6544318e-06
Iter: 540 loss: 1.62537719e-06
Iter: 541 loss: 1.62308049e-06
Iter: 542 loss: 1.62081506e-06
Iter: 543 loss: 1.62035985e-06
Iter: 544 loss: 1.61665366e-06
Iter: 545 loss: 1.62562196e-06
Iter: 546 loss: 1.61536354e-06
Iter: 547 loss: 1.61266291e-06
Iter: 548 loss: 1.61268122e-06
Iter: 549 loss: 1.61070352e-06
Iter: 550 loss: 1.60914828e-06
Iter: 551 loss: 1.60856098e-06
Iter: 552 loss: 1.6063625e-06
Iter: 553 loss: 1.63869163e-06
Iter: 554 loss: 1.60633908e-06
Iter: 555 loss: 1.60443437e-06
Iter: 556 loss: 1.6038141e-06
Iter: 557 loss: 1.60269622e-06
Iter: 558 loss: 1.59992089e-06
Iter: 559 loss: 1.60059744e-06
Iter: 560 loss: 1.59792353e-06
Iter: 561 loss: 1.595e-06
Iter: 562 loss: 1.62049469e-06
Iter: 563 loss: 1.59485387e-06
Iter: 564 loss: 1.59255342e-06
Iter: 565 loss: 1.59358024e-06
Iter: 566 loss: 1.59103729e-06
Iter: 567 loss: 1.58793512e-06
Iter: 568 loss: 1.60168258e-06
Iter: 569 loss: 1.58721537e-06
Iter: 570 loss: 1.5840501e-06
Iter: 571 loss: 1.58868897e-06
Iter: 572 loss: 1.58252158e-06
Iter: 573 loss: 1.58019338e-06
Iter: 574 loss: 1.6065402e-06
Iter: 575 loss: 1.58013154e-06
Iter: 576 loss: 1.5781601e-06
Iter: 577 loss: 1.57866907e-06
Iter: 578 loss: 1.57679256e-06
Iter: 579 loss: 1.57438433e-06
Iter: 580 loss: 1.57508418e-06
Iter: 581 loss: 1.57264265e-06
Iter: 582 loss: 1.56936505e-06
Iter: 583 loss: 1.58298326e-06
Iter: 584 loss: 1.5687026e-06
Iter: 585 loss: 1.56653869e-06
Iter: 586 loss: 1.59903902e-06
Iter: 587 loss: 1.56655028e-06
Iter: 588 loss: 1.56494855e-06
Iter: 589 loss: 1.56287206e-06
Iter: 590 loss: 1.56273677e-06
Iter: 591 loss: 1.56047724e-06
Iter: 592 loss: 1.560501e-06
Iter: 593 loss: 1.55868315e-06
Iter: 594 loss: 1.5557398e-06
Iter: 595 loss: 1.555739e-06
Iter: 596 loss: 1.55297664e-06
Iter: 597 loss: 1.56145938e-06
Iter: 598 loss: 1.55209977e-06
Iter: 599 loss: 1.54893905e-06
Iter: 600 loss: 1.55987084e-06
Iter: 601 loss: 1.54807162e-06
Iter: 602 loss: 1.54536065e-06
Iter: 603 loss: 1.56182796e-06
Iter: 604 loss: 1.54507507e-06
Iter: 605 loss: 1.54319298e-06
Iter: 606 loss: 1.54657414e-06
Iter: 607 loss: 1.54236477e-06
Iter: 608 loss: 1.53966994e-06
Iter: 609 loss: 1.544209e-06
Iter: 610 loss: 1.53854103e-06
Iter: 611 loss: 1.53578162e-06
Iter: 612 loss: 1.54728298e-06
Iter: 613 loss: 1.53512656e-06
Iter: 614 loss: 1.53284395e-06
Iter: 615 loss: 1.53865585e-06
Iter: 616 loss: 1.53193696e-06
Iter: 617 loss: 1.52975394e-06
Iter: 618 loss: 1.52793768e-06
Iter: 619 loss: 1.52729513e-06
Iter: 620 loss: 1.52475604e-06
Iter: 621 loss: 1.52477776e-06
Iter: 622 loss: 1.52242046e-06
Iter: 623 loss: 1.52597966e-06
Iter: 624 loss: 1.52133589e-06
Iter: 625 loss: 1.51940822e-06
Iter: 626 loss: 1.52648101e-06
Iter: 627 loss: 1.51897791e-06
Iter: 628 loss: 1.51665165e-06
Iter: 629 loss: 1.5202927e-06
Iter: 630 loss: 1.51555219e-06
Iter: 631 loss: 1.51373627e-06
Iter: 632 loss: 1.51103723e-06
Iter: 633 loss: 1.51097152e-06
Iter: 634 loss: 1.50751384e-06
Iter: 635 loss: 1.53344445e-06
Iter: 636 loss: 1.50720484e-06
Iter: 637 loss: 1.5048563e-06
Iter: 638 loss: 1.52122584e-06
Iter: 639 loss: 1.50461915e-06
Iter: 640 loss: 1.50245808e-06
Iter: 641 loss: 1.50495839e-06
Iter: 642 loss: 1.50132701e-06
Iter: 643 loss: 1.49871175e-06
Iter: 644 loss: 1.50724941e-06
Iter: 645 loss: 1.49794869e-06
Iter: 646 loss: 1.49564517e-06
Iter: 647 loss: 1.50855453e-06
Iter: 648 loss: 1.49533435e-06
Iter: 649 loss: 1.49368702e-06
Iter: 650 loss: 1.4950183e-06
Iter: 651 loss: 1.49264497e-06
Iter: 652 loss: 1.49052335e-06
Iter: 653 loss: 1.49352422e-06
Iter: 654 loss: 1.48945367e-06
Iter: 655 loss: 1.48670688e-06
Iter: 656 loss: 1.49483867e-06
Iter: 657 loss: 1.48588731e-06
Iter: 658 loss: 1.48415211e-06
Iter: 659 loss: 1.50853157e-06
Iter: 660 loss: 1.48417439e-06
Iter: 661 loss: 1.48260688e-06
Iter: 662 loss: 1.48123013e-06
Iter: 663 loss: 1.48076458e-06
Iter: 664 loss: 1.47869105e-06
Iter: 665 loss: 1.4965234e-06
Iter: 666 loss: 1.47856326e-06
Iter: 667 loss: 1.47650371e-06
Iter: 668 loss: 1.4752917e-06
Iter: 669 loss: 1.47439403e-06
Iter: 670 loss: 1.47238097e-06
Iter: 671 loss: 1.47088531e-06
Iter: 672 loss: 1.47011247e-06
Iter: 673 loss: 1.46696107e-06
Iter: 674 loss: 1.49893231e-06
Iter: 675 loss: 1.46685738e-06
Iter: 676 loss: 1.46499065e-06
Iter: 677 loss: 1.48318259e-06
Iter: 678 loss: 1.46490163e-06
Iter: 679 loss: 1.46319428e-06
Iter: 680 loss: 1.46262892e-06
Iter: 681 loss: 1.46160767e-06
Iter: 682 loss: 1.45927822e-06
Iter: 683 loss: 1.47513163e-06
Iter: 684 loss: 1.45905392e-06
Iter: 685 loss: 1.45702018e-06
Iter: 686 loss: 1.45970364e-06
Iter: 687 loss: 1.45594e-06
Iter: 688 loss: 1.45411127e-06
Iter: 689 loss: 1.45537854e-06
Iter: 690 loss: 1.45295803e-06
Iter: 691 loss: 1.4505456e-06
Iter: 692 loss: 1.46574371e-06
Iter: 693 loss: 1.45018896e-06
Iter: 694 loss: 1.44842966e-06
Iter: 695 loss: 1.45894933e-06
Iter: 696 loss: 1.44818193e-06
Iter: 697 loss: 1.44661317e-06
Iter: 698 loss: 1.44916612e-06
Iter: 699 loss: 1.44585692e-06
Iter: 700 loss: 1.44427349e-06
Iter: 701 loss: 1.44828823e-06
Iter: 702 loss: 1.44376554e-06
Iter: 703 loss: 1.44201476e-06
Iter: 704 loss: 1.44696014e-06
Iter: 705 loss: 1.4414602e-06
Iter: 706 loss: 1.43968145e-06
Iter: 707 loss: 1.43766397e-06
Iter: 708 loss: 1.43738464e-06
Iter: 709 loss: 1.43491275e-06
Iter: 710 loss: 1.44501655e-06
Iter: 711 loss: 1.4343027e-06
Iter: 712 loss: 1.43237526e-06
Iter: 713 loss: 1.44681019e-06
Iter: 714 loss: 1.43221098e-06
Iter: 715 loss: 1.43054433e-06
Iter: 716 loss: 1.43566604e-06
Iter: 717 loss: 1.43000238e-06
Iter: 718 loss: 1.42794147e-06
Iter: 719 loss: 1.43244324e-06
Iter: 720 loss: 1.42709871e-06
Iter: 721 loss: 1.42567251e-06
Iter: 722 loss: 1.43060902e-06
Iter: 723 loss: 1.42528984e-06
Iter: 724 loss: 1.42338968e-06
Iter: 725 loss: 1.42336046e-06
Iter: 726 loss: 1.42179022e-06
Iter: 727 loss: 1.4195524e-06
Iter: 728 loss: 1.4256542e-06
Iter: 729 loss: 1.41889291e-06
Iter: 730 loss: 1.41713565e-06
Iter: 731 loss: 1.43947204e-06
Iter: 732 loss: 1.41715964e-06
Iter: 733 loss: 1.41556632e-06
Iter: 734 loss: 1.41511123e-06
Iter: 735 loss: 1.41416274e-06
Iter: 736 loss: 1.41252849e-06
Iter: 737 loss: 1.42749616e-06
Iter: 738 loss: 1.41246983e-06
Iter: 739 loss: 1.41124383e-06
Iter: 740 loss: 1.41207454e-06
Iter: 741 loss: 1.41052544e-06
Iter: 742 loss: 1.40891552e-06
Iter: 743 loss: 1.41156215e-06
Iter: 744 loss: 1.40816519e-06
Iter: 745 loss: 1.40620193e-06
Iter: 746 loss: 1.40487714e-06
Iter: 747 loss: 1.40408395e-06
Iter: 748 loss: 1.40155169e-06
Iter: 749 loss: 1.41632017e-06
Iter: 750 loss: 1.40121733e-06
Iter: 751 loss: 1.39925203e-06
Iter: 752 loss: 1.40686188e-06
Iter: 753 loss: 1.3987983e-06
Iter: 754 loss: 1.39667509e-06
Iter: 755 loss: 1.41018472e-06
Iter: 756 loss: 1.39646454e-06
Iter: 757 loss: 1.39483836e-06
Iter: 758 loss: 1.39419399e-06
Iter: 759 loss: 1.3933992e-06
Iter: 760 loss: 1.39181111e-06
Iter: 761 loss: 1.41451392e-06
Iter: 762 loss: 1.39184931e-06
Iter: 763 loss: 1.39059352e-06
Iter: 764 loss: 1.38956102e-06
Iter: 765 loss: 1.38925861e-06
Iter: 766 loss: 1.38726e-06
Iter: 767 loss: 1.39533222e-06
Iter: 768 loss: 1.38693599e-06
Iter: 769 loss: 1.38503185e-06
Iter: 770 loss: 1.40100508e-06
Iter: 771 loss: 1.38493135e-06
Iter: 772 loss: 1.38382495e-06
Iter: 773 loss: 1.38316818e-06
Iter: 774 loss: 1.38266944e-06
Iter: 775 loss: 1.38089342e-06
Iter: 776 loss: 1.38919904e-06
Iter: 777 loss: 1.38046437e-06
Iter: 778 loss: 1.37888958e-06
Iter: 779 loss: 1.38346695e-06
Iter: 780 loss: 1.3783922e-06
Iter: 781 loss: 1.37690608e-06
Iter: 782 loss: 1.37572601e-06
Iter: 783 loss: 1.37525308e-06
Iter: 784 loss: 1.37293512e-06
Iter: 785 loss: 1.39118777e-06
Iter: 786 loss: 1.37275231e-06
Iter: 787 loss: 1.3713219e-06
Iter: 788 loss: 1.37323309e-06
Iter: 789 loss: 1.37043912e-06
Iter: 790 loss: 1.36878793e-06
Iter: 791 loss: 1.37496295e-06
Iter: 792 loss: 1.36846791e-06
Iter: 793 loss: 1.36659492e-06
Iter: 794 loss: 1.37569282e-06
Iter: 795 loss: 1.36623908e-06
Iter: 796 loss: 1.36494714e-06
Iter: 797 loss: 1.36390111e-06
Iter: 798 loss: 1.36356891e-06
Iter: 799 loss: 1.36172991e-06
Iter: 800 loss: 1.38162068e-06
Iter: 801 loss: 1.36174253e-06
Iter: 802 loss: 1.36018309e-06
Iter: 803 loss: 1.35983305e-06
Iter: 804 loss: 1.35884784e-06
Iter: 805 loss: 1.35744949e-06
Iter: 806 loss: 1.3574379e-06
Iter: 807 loss: 1.35625646e-06
Iter: 808 loss: 1.35665164e-06
Iter: 809 loss: 1.3554295e-06
Iter: 810 loss: 1.35418247e-06
Iter: 811 loss: 1.35492371e-06
Iter: 812 loss: 1.35338405e-06
Iter: 813 loss: 1.35169103e-06
Iter: 814 loss: 1.36214226e-06
Iter: 815 loss: 1.35141647e-06
Iter: 816 loss: 1.34997663e-06
Iter: 817 loss: 1.34977745e-06
Iter: 818 loss: 1.34871436e-06
Iter: 819 loss: 1.34703077e-06
Iter: 820 loss: 1.34966888e-06
Iter: 821 loss: 1.34622564e-06
Iter: 822 loss: 1.34424454e-06
Iter: 823 loss: 1.358492e-06
Iter: 824 loss: 1.34412358e-06
Iter: 825 loss: 1.34266463e-06
Iter: 826 loss: 1.34234483e-06
Iter: 827 loss: 1.34138168e-06
Iter: 828 loss: 1.33987714e-06
Iter: 829 loss: 1.33986259e-06
Iter: 830 loss: 1.33883736e-06
Iter: 831 loss: 1.33862886e-06
Iter: 832 loss: 1.33805645e-06
Iter: 833 loss: 1.33647768e-06
Iter: 834 loss: 1.33755714e-06
Iter: 835 loss: 1.33548895e-06
Iter: 836 loss: 1.33397634e-06
Iter: 837 loss: 1.35181926e-06
Iter: 838 loss: 1.3339602e-06
Iter: 839 loss: 1.33280821e-06
Iter: 840 loss: 1.33481308e-06
Iter: 841 loss: 1.33231106e-06
Iter: 842 loss: 1.3311452e-06
Iter: 843 loss: 1.33811795e-06
Iter: 844 loss: 1.33097421e-06
Iter: 845 loss: 1.32989499e-06
Iter: 846 loss: 1.32903961e-06
Iter: 847 loss: 1.32869309e-06
Iter: 848 loss: 1.32697483e-06
Iter: 849 loss: 1.3307515e-06
Iter: 850 loss: 1.32622176e-06
Iter: 851 loss: 1.3247161e-06
Iter: 852 loss: 1.3448938e-06
Iter: 853 loss: 1.3247361e-06
Iter: 854 loss: 1.32374453e-06
Iter: 855 loss: 1.32182754e-06
Iter: 856 loss: 1.36619542e-06
Iter: 857 loss: 1.32183879e-06
Iter: 858 loss: 1.31987827e-06
Iter: 859 loss: 1.32638331e-06
Iter: 860 loss: 1.31935e-06
Iter: 861 loss: 1.31741353e-06
Iter: 862 loss: 1.32840796e-06
Iter: 863 loss: 1.31715274e-06
Iter: 864 loss: 1.31541924e-06
Iter: 865 loss: 1.32429875e-06
Iter: 866 loss: 1.31509375e-06
Iter: 867 loss: 1.31394177e-06
Iter: 868 loss: 1.3208155e-06
Iter: 869 loss: 1.31379841e-06
Iter: 870 loss: 1.31265494e-06
Iter: 871 loss: 1.31101251e-06
Iter: 872 loss: 1.31102274e-06
Iter: 873 loss: 1.30903732e-06
Iter: 874 loss: 1.32671084e-06
Iter: 875 loss: 1.30898047e-06
Iter: 876 loss: 1.30790158e-06
Iter: 877 loss: 1.31885213e-06
Iter: 878 loss: 1.3078967e-06
Iter: 879 loss: 1.30691456e-06
Iter: 880 loss: 1.30606463e-06
Iter: 881 loss: 1.30577746e-06
Iter: 882 loss: 1.30404646e-06
Iter: 883 loss: 1.31371917e-06
Iter: 884 loss: 1.30377907e-06
Iter: 885 loss: 1.30269905e-06
Iter: 886 loss: 1.30220178e-06
Iter: 887 loss: 1.30170929e-06
Iter: 888 loss: 1.30036869e-06
Iter: 889 loss: 1.31549245e-06
Iter: 890 loss: 1.30034368e-06
Iter: 891 loss: 1.29912314e-06
Iter: 892 loss: 1.29828231e-06
Iter: 893 loss: 1.29785735e-06
Iter: 894 loss: 1.29632417e-06
Iter: 895 loss: 1.30165631e-06
Iter: 896 loss: 1.29599096e-06
Iter: 897 loss: 1.29458874e-06
Iter: 898 loss: 1.29557e-06
Iter: 899 loss: 1.29365606e-06
Iter: 900 loss: 1.29206626e-06
Iter: 901 loss: 1.30681428e-06
Iter: 902 loss: 1.29201351e-06
Iter: 903 loss: 1.29085379e-06
Iter: 904 loss: 1.29799901e-06
Iter: 905 loss: 1.29066325e-06
Iter: 906 loss: 1.28968941e-06
Iter: 907 loss: 1.28870704e-06
Iter: 908 loss: 1.28857494e-06
Iter: 909 loss: 1.28682893e-06
Iter: 910 loss: 1.29792295e-06
Iter: 911 loss: 1.28673082e-06
Iter: 912 loss: 1.2855993e-06
Iter: 913 loss: 1.29142973e-06
Iter: 914 loss: 1.28541956e-06
Iter: 915 loss: 1.28437728e-06
Iter: 916 loss: 1.28779334e-06
Iter: 917 loss: 1.2841673e-06
Iter: 918 loss: 1.28315571e-06
Iter: 919 loss: 1.28259148e-06
Iter: 920 loss: 1.28213787e-06
Iter: 921 loss: 1.28073111e-06
Iter: 922 loss: 1.29223258e-06
Iter: 923 loss: 1.28064698e-06
Iter: 924 loss: 1.27965677e-06
Iter: 925 loss: 1.27845863e-06
Iter: 926 loss: 1.27834e-06
Iter: 927 loss: 1.27716407e-06
Iter: 928 loss: 1.2771734e-06
Iter: 929 loss: 1.27623628e-06
Iter: 930 loss: 1.27476051e-06
Iter: 931 loss: 1.27476289e-06
Iter: 932 loss: 1.27312e-06
Iter: 933 loss: 1.2746325e-06
Iter: 934 loss: 1.27220426e-06
Iter: 935 loss: 1.2700857e-06
Iter: 936 loss: 1.27829628e-06
Iter: 937 loss: 1.26961038e-06
Iter: 938 loss: 1.26786915e-06
Iter: 939 loss: 1.27771693e-06
Iter: 940 loss: 1.267568e-06
Iter: 941 loss: 1.26611894e-06
Iter: 942 loss: 1.26681857e-06
Iter: 943 loss: 1.26511782e-06
Iter: 944 loss: 1.2638443e-06
Iter: 945 loss: 1.26376756e-06
Iter: 946 loss: 1.26286352e-06
Iter: 947 loss: 1.2648328e-06
Iter: 948 loss: 1.26250154e-06
Iter: 949 loss: 1.26141595e-06
Iter: 950 loss: 1.26161513e-06
Iter: 951 loss: 1.2606788e-06
Iter: 952 loss: 1.25933752e-06
Iter: 953 loss: 1.27273222e-06
Iter: 954 loss: 1.25927568e-06
Iter: 955 loss: 1.25837664e-06
Iter: 956 loss: 1.25886822e-06
Iter: 957 loss: 1.25778604e-06
Iter: 958 loss: 1.25663371e-06
Iter: 959 loss: 1.25725933e-06
Iter: 960 loss: 1.25581823e-06
Iter: 961 loss: 1.25443466e-06
Iter: 962 loss: 1.259952e-06
Iter: 963 loss: 1.25412078e-06
Iter: 964 loss: 1.25279348e-06
Iter: 965 loss: 1.25713245e-06
Iter: 966 loss: 1.25241161e-06
Iter: 967 loss: 1.25105021e-06
Iter: 968 loss: 1.25320673e-06
Iter: 969 loss: 1.25039878e-06
Iter: 970 loss: 1.24934786e-06
Iter: 971 loss: 1.25627366e-06
Iter: 972 loss: 1.24919757e-06
Iter: 973 loss: 1.24800181e-06
Iter: 974 loss: 1.2486812e-06
Iter: 975 loss: 1.24718849e-06
Iter: 976 loss: 1.24586859e-06
Iter: 977 loss: 1.24536564e-06
Iter: 978 loss: 1.24472365e-06
Iter: 979 loss: 1.24304802e-06
Iter: 980 loss: 1.24975838e-06
Iter: 981 loss: 1.24264727e-06
Iter: 982 loss: 1.24116264e-06
Iter: 983 loss: 1.24734356e-06
Iter: 984 loss: 1.24089229e-06
Iter: 985 loss: 1.23945551e-06
Iter: 986 loss: 1.24301903e-06
Iter: 987 loss: 1.23893278e-06
Iter: 988 loss: 1.23786867e-06
Iter: 989 loss: 1.23786265e-06
Iter: 990 loss: 1.2370333e-06
Iter: 991 loss: 1.23677432e-06
Iter: 992 loss: 1.23634572e-06
Iter: 993 loss: 1.23520363e-06
Iter: 994 loss: 1.24093265e-06
Iter: 995 loss: 1.23501979e-06
Iter: 996 loss: 1.23397717e-06
Iter: 997 loss: 1.23715313e-06
Iter: 998 loss: 1.23365544e-06
Iter: 999 loss: 1.23255427e-06
Iter: 1000 loss: 1.23392306e-06
Iter: 1001 loss: 1.23193377e-06
Iter: 1002 loss: 1.23087921e-06
Iter: 1003 loss: 1.23207121e-06
Iter: 1004 loss: 1.2302753e-06
Iter: 1005 loss: 1.22892607e-06
Iter: 1006 loss: 1.23448399e-06
Iter: 1007 loss: 1.22858069e-06
Iter: 1008 loss: 1.22751567e-06
Iter: 1009 loss: 1.23116183e-06
Iter: 1010 loss: 1.22725737e-06
Iter: 1011 loss: 1.22623203e-06
Iter: 1012 loss: 1.22724782e-06
Iter: 1013 loss: 1.22563915e-06
Iter: 1014 loss: 1.22448182e-06
Iter: 1015 loss: 1.23310713e-06
Iter: 1016 loss: 1.22436336e-06
Iter: 1017 loss: 1.22329959e-06
Iter: 1018 loss: 1.2227456e-06
Iter: 1019 loss: 1.22226356e-06
Iter: 1020 loss: 1.22090239e-06
Iter: 1021 loss: 1.22136294e-06
Iter: 1022 loss: 1.21993855e-06
Iter: 1023 loss: 1.21850508e-06
Iter: 1024 loss: 1.22795859e-06
Iter: 1025 loss: 1.21821699e-06
Iter: 1026 loss: 1.21693006e-06
Iter: 1027 loss: 1.22263498e-06
Iter: 1028 loss: 1.21663618e-06
Iter: 1029 loss: 1.21519679e-06
Iter: 1030 loss: 1.22537335e-06
Iter: 1031 loss: 1.21505627e-06
Iter: 1032 loss: 1.21434846e-06
Iter: 1033 loss: 1.21477228e-06
Iter: 1034 loss: 1.2139102e-06
Iter: 1035 loss: 1.21297739e-06
Iter: 1036 loss: 1.21890616e-06
Iter: 1037 loss: 1.21283472e-06
Iter: 1038 loss: 1.21210303e-06
Iter: 1039 loss: 1.21093899e-06
Iter: 1040 loss: 1.21092307e-06
Iter: 1041 loss: 1.20953246e-06
Iter: 1042 loss: 1.21913126e-06
Iter: 1043 loss: 1.20941172e-06
Iter: 1044 loss: 1.20854259e-06
Iter: 1045 loss: 1.21433823e-06
Iter: 1046 loss: 1.208399e-06
Iter: 1047 loss: 1.20749735e-06
Iter: 1048 loss: 1.20683865e-06
Iter: 1049 loss: 1.20650839e-06
Iter: 1050 loss: 1.20538846e-06
Iter: 1051 loss: 1.21756989e-06
Iter: 1052 loss: 1.20537106e-06
Iter: 1053 loss: 1.20439597e-06
Iter: 1054 loss: 1.20542768e-06
Iter: 1055 loss: 1.2038206e-06
Iter: 1056 loss: 1.2027391e-06
Iter: 1057 loss: 1.2045914e-06
Iter: 1058 loss: 1.20229333e-06
Iter: 1059 loss: 1.20110644e-06
Iter: 1060 loss: 1.20403797e-06
Iter: 1061 loss: 1.20071934e-06
Iter: 1062 loss: 1.19954075e-06
Iter: 1063 loss: 1.20169864e-06
Iter: 1064 loss: 1.19900665e-06
Iter: 1065 loss: 1.19775336e-06
Iter: 1066 loss: 1.19948345e-06
Iter: 1067 loss: 1.19718834e-06
Iter: 1068 loss: 1.19611445e-06
Iter: 1069 loss: 1.20789207e-06
Iter: 1070 loss: 1.19607216e-06
Iter: 1071 loss: 1.19511594e-06
Iter: 1072 loss: 1.19997026e-06
Iter: 1073 loss: 1.19497088e-06
Iter: 1074 loss: 1.19430513e-06
Iter: 1075 loss: 1.19354638e-06
Iter: 1076 loss: 1.1933962e-06
Iter: 1077 loss: 1.19235347e-06
Iter: 1078 loss: 1.20736411e-06
Iter: 1079 loss: 1.19238223e-06
Iter: 1080 loss: 1.19167305e-06
Iter: 1081 loss: 1.19083074e-06
Iter: 1082 loss: 1.19077413e-06
Iter: 1083 loss: 1.189621e-06
Iter: 1084 loss: 1.18976993e-06
Iter: 1085 loss: 1.18870821e-06
Iter: 1086 loss: 1.18781986e-06
Iter: 1087 loss: 1.18771845e-06
Iter: 1088 loss: 1.18704099e-06
Iter: 1089 loss: 1.18765377e-06
Iter: 1090 loss: 1.1866008e-06
Iter: 1091 loss: 1.18562548e-06
Iter: 1092 loss: 1.1855202e-06
Iter: 1093 loss: 1.1848316e-06
Iter: 1094 loss: 1.18370531e-06
Iter: 1095 loss: 1.19392826e-06
Iter: 1096 loss: 1.1836039e-06
Iter: 1097 loss: 1.18271214e-06
Iter: 1098 loss: 1.18389698e-06
Iter: 1099 loss: 1.18225489e-06
Iter: 1100 loss: 1.18119874e-06
Iter: 1101 loss: 1.18204366e-06
Iter: 1102 loss: 1.18058813e-06
Iter: 1103 loss: 1.17941909e-06
Iter: 1104 loss: 1.18551782e-06
Iter: 1105 loss: 1.17921581e-06
Iter: 1106 loss: 1.17820309e-06
Iter: 1107 loss: 1.1812657e-06
Iter: 1108 loss: 1.17798413e-06
Iter: 1109 loss: 1.17705474e-06
Iter: 1110 loss: 1.1846621e-06
Iter: 1111 loss: 1.17700392e-06
Iter: 1112 loss: 1.17620948e-06
Iter: 1113 loss: 1.17620039e-06
Iter: 1114 loss: 1.17560148e-06
Iter: 1115 loss: 1.17472587e-06
Iter: 1116 loss: 1.17598188e-06
Iter: 1117 loss: 1.17430068e-06
Iter: 1118 loss: 1.17326488e-06
Iter: 1119 loss: 1.18159323e-06
Iter: 1120 loss: 1.17321758e-06
Iter: 1121 loss: 1.1723472e-06
Iter: 1122 loss: 1.17230616e-06
Iter: 1123 loss: 1.17170771e-06
Iter: 1124 loss: 1.17078218e-06
Iter: 1125 loss: 1.16998581e-06
Iter: 1126 loss: 1.16972501e-06
Iter: 1127 loss: 1.16814749e-06
Iter: 1128 loss: 1.17767377e-06
Iter: 1129 loss: 1.16796423e-06
Iter: 1130 loss: 1.16701585e-06
Iter: 1131 loss: 1.17964032e-06
Iter: 1132 loss: 1.16700426e-06
Iter: 1133 loss: 1.16609772e-06
Iter: 1134 loss: 1.16616206e-06
Iter: 1135 loss: 1.16538945e-06
Iter: 1136 loss: 1.1643308e-06
Iter: 1137 loss: 1.16720105e-06
Iter: 1138 loss: 1.16406977e-06
Iter: 1139 loss: 1.16304591e-06
Iter: 1140 loss: 1.16944818e-06
Iter: 1141 loss: 1.16295553e-06
Iter: 1142 loss: 1.16213778e-06
Iter: 1143 loss: 1.1625666e-06
Iter: 1144 loss: 1.16165643e-06
Iter: 1145 loss: 1.16067622e-06
Iter: 1146 loss: 1.1618697e-06
Iter: 1147 loss: 1.16013234e-06
Iter: 1148 loss: 1.15892362e-06
Iter: 1149 loss: 1.16326953e-06
Iter: 1150 loss: 1.15855255e-06
Iter: 1151 loss: 1.15760804e-06
Iter: 1152 loss: 1.16878903e-06
Iter: 1153 loss: 1.15761407e-06
Iter: 1154 loss: 1.15687169e-06
Iter: 1155 loss: 1.15854129e-06
Iter: 1156 loss: 1.15660987e-06
Iter: 1157 loss: 1.15597311e-06
Iter: 1158 loss: 1.15514422e-06
Iter: 1159 loss: 1.15502621e-06
Iter: 1160 loss: 1.15391754e-06
Iter: 1161 loss: 1.16466197e-06
Iter: 1162 loss: 1.15388298e-06
Iter: 1163 loss: 1.15289231e-06
Iter: 1164 loss: 1.15534249e-06
Iter: 1165 loss: 1.15253806e-06
Iter: 1166 loss: 1.15177374e-06
Iter: 1167 loss: 1.15137846e-06
Iter: 1168 loss: 1.15104331e-06
Iter: 1169 loss: 1.14967975e-06
Iter: 1170 loss: 1.15044008e-06
Iter: 1171 loss: 1.14878389e-06
Iter: 1172 loss: 1.14751356e-06
Iter: 1173 loss: 1.16413219e-06
Iter: 1174 loss: 1.14755665e-06
Iter: 1175 loss: 1.14679278e-06
Iter: 1176 loss: 1.15476143e-06
Iter: 1177 loss: 1.14679278e-06
Iter: 1178 loss: 1.14612749e-06
Iter: 1179 loss: 1.14468594e-06
Iter: 1180 loss: 1.16403578e-06
Iter: 1181 loss: 1.14455565e-06
Iter: 1182 loss: 1.14321244e-06
Iter: 1183 loss: 1.1569374e-06
Iter: 1184 loss: 1.14318868e-06
Iter: 1185 loss: 1.1422643e-06
Iter: 1186 loss: 1.14994441e-06
Iter: 1187 loss: 1.14223099e-06
Iter: 1188 loss: 1.1414794e-06
Iter: 1189 loss: 1.14062357e-06
Iter: 1190 loss: 1.14049442e-06
Iter: 1191 loss: 1.1396977e-06
Iter: 1192 loss: 1.13970032e-06
Iter: 1193 loss: 1.1390664e-06
Iter: 1194 loss: 1.13951114e-06
Iter: 1195 loss: 1.13866349e-06
Iter: 1196 loss: 1.13799081e-06
Iter: 1197 loss: 1.13683518e-06
Iter: 1198 loss: 1.13678288e-06
Iter: 1199 loss: 1.13571514e-06
Iter: 1200 loss: 1.13566193e-06
Iter: 1201 loss: 1.13493866e-06
Iter: 1202 loss: 1.13703175e-06
Iter: 1203 loss: 1.13473698e-06
Iter: 1204 loss: 1.13397027e-06
Iter: 1205 loss: 1.13371448e-06
Iter: 1206 loss: 1.13329577e-06
Iter: 1207 loss: 1.13222552e-06
Iter: 1208 loss: 1.13619581e-06
Iter: 1209 loss: 1.13186445e-06
Iter: 1210 loss: 1.13101737e-06
Iter: 1211 loss: 1.13241992e-06
Iter: 1212 loss: 1.13055376e-06
Iter: 1213 loss: 1.12961288e-06
Iter: 1214 loss: 1.13323279e-06
Iter: 1215 loss: 1.12935754e-06
Iter: 1216 loss: 1.12838018e-06
Iter: 1217 loss: 1.13501392e-06
Iter: 1218 loss: 1.12824785e-06
Iter: 1219 loss: 1.12751968e-06
Iter: 1220 loss: 1.12770635e-06
Iter: 1221 loss: 1.12700218e-06
Iter: 1222 loss: 1.12600253e-06
Iter: 1223 loss: 1.12603357e-06
Iter: 1224 loss: 1.12519706e-06
Iter: 1225 loss: 1.1243244e-06
Iter: 1226 loss: 1.13762439e-06
Iter: 1227 loss: 1.12434736e-06
Iter: 1228 loss: 1.12358839e-06
Iter: 1229 loss: 1.12751968e-06
Iter: 1230 loss: 1.12345947e-06
Iter: 1231 loss: 1.12285795e-06
Iter: 1232 loss: 1.12145835e-06
Iter: 1233 loss: 1.1385614e-06
Iter: 1234 loss: 1.12132216e-06
Iter: 1235 loss: 1.12069119e-06
Iter: 1236 loss: 1.12056625e-06
Iter: 1237 loss: 1.11983161e-06
Iter: 1238 loss: 1.11987197e-06
Iter: 1239 loss: 1.11929171e-06
Iter: 1240 loss: 1.1182957e-06
Iter: 1241 loss: 1.12138378e-06
Iter: 1242 loss: 1.11797067e-06
Iter: 1243 loss: 1.11711302e-06
Iter: 1244 loss: 1.12035252e-06
Iter: 1245 loss: 1.11692611e-06
Iter: 1246 loss: 1.11611143e-06
Iter: 1247 loss: 1.1177716e-06
Iter: 1248 loss: 1.11584063e-06
Iter: 1249 loss: 1.11499344e-06
Iter: 1250 loss: 1.11630447e-06
Iter: 1251 loss: 1.11461122e-06
Iter: 1252 loss: 1.11369559e-06
Iter: 1253 loss: 1.11570637e-06
Iter: 1254 loss: 1.11325153e-06
Iter: 1255 loss: 1.11225245e-06
Iter: 1256 loss: 1.11429836e-06
Iter: 1257 loss: 1.11187956e-06
Iter: 1258 loss: 1.11106544e-06
Iter: 1259 loss: 1.12155647e-06
Iter: 1260 loss: 1.11105192e-06
Iter: 1261 loss: 1.11036366e-06
Iter: 1262 loss: 1.11003635e-06
Iter: 1263 loss: 1.10964993e-06
Iter: 1264 loss: 1.10879387e-06
Iter: 1265 loss: 1.11165582e-06
Iter: 1266 loss: 1.1084428e-06
Iter: 1267 loss: 1.10781025e-06
Iter: 1268 loss: 1.1078281e-06
Iter: 1269 loss: 1.10722522e-06
Iter: 1270 loss: 1.10648921e-06
Iter: 1271 loss: 1.10641429e-06
Iter: 1272 loss: 1.10556948e-06
Iter: 1273 loss: 1.10626183e-06
Iter: 1274 loss: 1.1050746e-06
Iter: 1275 loss: 1.10402141e-06
Iter: 1276 loss: 1.10845053e-06
Iter: 1277 loss: 1.10379597e-06
Iter: 1278 loss: 1.10314e-06
Iter: 1279 loss: 1.10311635e-06
Iter: 1280 loss: 1.10264853e-06
Iter: 1281 loss: 1.10153496e-06
Iter: 1282 loss: 1.11879422e-06
Iter: 1283 loss: 1.10157021e-06
Iter: 1284 loss: 1.10039286e-06
Iter: 1285 loss: 1.10648739e-06
Iter: 1286 loss: 1.10022143e-06
Iter: 1287 loss: 1.09921302e-06
Iter: 1288 loss: 1.10393307e-06
Iter: 1289 loss: 1.09905045e-06
Iter: 1290 loss: 1.09818836e-06
Iter: 1291 loss: 1.09889731e-06
Iter: 1292 loss: 1.09767632e-06
Iter: 1293 loss: 1.09667508e-06
Iter: 1294 loss: 1.10252404e-06
Iter: 1295 loss: 1.09650887e-06
Iter: 1296 loss: 1.0956112e-06
Iter: 1297 loss: 1.10044266e-06
Iter: 1298 loss: 1.09552047e-06
Iter: 1299 loss: 1.09484347e-06
Iter: 1300 loss: 1.09426719e-06
Iter: 1301 loss: 1.0941518e-06
Iter: 1302 loss: 1.09319558e-06
Iter: 1303 loss: 1.10224312e-06
Iter: 1304 loss: 1.09316511e-06
Iter: 1305 loss: 1.09234475e-06
Iter: 1306 loss: 1.09458983e-06
Iter: 1307 loss: 1.09208713e-06
Iter: 1308 loss: 1.09145253e-06
Iter: 1309 loss: 1.09154257e-06
Iter: 1310 loss: 1.09094628e-06
Iter: 1311 loss: 1.09016378e-06
Iter: 1312 loss: 1.0945646e-06
Iter: 1313 loss: 1.09001007e-06
Iter: 1314 loss: 1.0892993e-06
Iter: 1315 loss: 1.08947961e-06
Iter: 1316 loss: 1.08867857e-06
Iter: 1317 loss: 1.0877086e-06
Iter: 1318 loss: 1.08974382e-06
Iter: 1319 loss: 1.08736856e-06
Iter: 1320 loss: 1.08667575e-06
Iter: 1321 loss: 1.08666939e-06
Iter: 1322 loss: 1.08614267e-06
Iter: 1323 loss: 1.08534095e-06
Iter: 1324 loss: 1.08534323e-06
Iter: 1325 loss: 1.08407642e-06
Iter: 1326 loss: 1.08529946e-06
Iter: 1327 loss: 1.08349604e-06
Iter: 1328 loss: 1.08234497e-06
Iter: 1329 loss: 1.08537847e-06
Iter: 1330 loss: 1.08196468e-06
Iter: 1331 loss: 1.08114273e-06
Iter: 1332 loss: 1.09204427e-06
Iter: 1333 loss: 1.08109475e-06
Iter: 1334 loss: 1.08035806e-06
Iter: 1335 loss: 1.08184531e-06
Iter: 1336 loss: 1.08001223e-06
Iter: 1337 loss: 1.07923552e-06
Iter: 1338 loss: 1.08319136e-06
Iter: 1339 loss: 1.07911671e-06
Iter: 1340 loss: 1.07845733e-06
Iter: 1341 loss: 1.07934568e-06
Iter: 1342 loss: 1.07811593e-06
Iter: 1343 loss: 1.07747724e-06
Iter: 1344 loss: 1.08312975e-06
Iter: 1345 loss: 1.07740016e-06
Iter: 1346 loss: 1.07683536e-06
Iter: 1347 loss: 1.07583969e-06
Iter: 1348 loss: 1.07583685e-06
Iter: 1349 loss: 1.07486335e-06
Iter: 1350 loss: 1.07989945e-06
Iter: 1351 loss: 1.07472306e-06
Iter: 1352 loss: 1.0738944e-06
Iter: 1353 loss: 1.07357641e-06
Iter: 1354 loss: 1.07313917e-06
Iter: 1355 loss: 1.0722606e-06
Iter: 1356 loss: 1.07221831e-06
Iter: 1357 loss: 1.07159042e-06
Iter: 1358 loss: 1.07272638e-06
Iter: 1359 loss: 1.07127653e-06
Iter: 1360 loss: 1.07059407e-06
Iter: 1361 loss: 1.07110441e-06
Iter: 1362 loss: 1.07015921e-06
Iter: 1363 loss: 1.06923153e-06
Iter: 1364 loss: 1.07451979e-06
Iter: 1365 loss: 1.06904668e-06
Iter: 1366 loss: 1.06832579e-06
Iter: 1367 loss: 1.06837194e-06
Iter: 1368 loss: 1.06774542e-06
Iter: 1369 loss: 1.06696757e-06
Iter: 1370 loss: 1.06699349e-06
Iter: 1371 loss: 1.06629136e-06
Iter: 1372 loss: 1.0649826e-06
Iter: 1373 loss: 1.07060578e-06
Iter: 1374 loss: 1.06469565e-06
Iter: 1375 loss: 1.06374e-06
Iter: 1376 loss: 1.06997163e-06
Iter: 1377 loss: 1.06367088e-06
Iter: 1378 loss: 1.06295397e-06
Iter: 1379 loss: 1.06932271e-06
Iter: 1380 loss: 1.06290156e-06
Iter: 1381 loss: 1.06231244e-06
Iter: 1382 loss: 1.06328866e-06
Iter: 1383 loss: 1.06210484e-06
Iter: 1384 loss: 1.06141897e-06
Iter: 1385 loss: 1.0612132e-06
Iter: 1386 loss: 1.06088737e-06
Iter: 1387 loss: 1.06015364e-06
Iter: 1388 loss: 1.06679681e-06
Iter: 1389 loss: 1.06010964e-06
Iter: 1390 loss: 1.05939262e-06
Iter: 1391 loss: 1.05907179e-06
Iter: 1392 loss: 1.05871982e-06
Iter: 1393 loss: 1.05788422e-06
Iter: 1394 loss: 1.05961954e-06
Iter: 1395 loss: 1.05750007e-06
Iter: 1396 loss: 1.05654226e-06
Iter: 1397 loss: 1.05875642e-06
Iter: 1398 loss: 1.05622985e-06
Iter: 1399 loss: 1.05532013e-06
Iter: 1400 loss: 1.06183177e-06
Iter: 1401 loss: 1.05529546e-06
Iter: 1402 loss: 1.05457775e-06
Iter: 1403 loss: 1.05697541e-06
Iter: 1404 loss: 1.0543788e-06
Iter: 1405 loss: 1.0537226e-06
Iter: 1406 loss: 1.05325512e-06
Iter: 1407 loss: 1.05301842e-06
Iter: 1408 loss: 1.05221329e-06
Iter: 1409 loss: 1.05222557e-06
Iter: 1410 loss: 1.05165464e-06
Iter: 1411 loss: 1.05190247e-06
Iter: 1412 loss: 1.05128538e-06
Iter: 1413 loss: 1.05059382e-06
Iter: 1414 loss: 1.04975152e-06
Iter: 1415 loss: 1.04965841e-06
Iter: 1416 loss: 1.04855235e-06
Iter: 1417 loss: 1.05746369e-06
Iter: 1418 loss: 1.04846822e-06
Iter: 1419 loss: 1.04796061e-06
Iter: 1420 loss: 1.04791434e-06
Iter: 1421 loss: 1.04756077e-06
Iter: 1422 loss: 1.04672733e-06
Iter: 1423 loss: 1.06031223e-06
Iter: 1424 loss: 1.04672802e-06
Iter: 1425 loss: 1.04580204e-06
Iter: 1426 loss: 1.05207789e-06
Iter: 1427 loss: 1.04566629e-06
Iter: 1428 loss: 1.04500691e-06
Iter: 1429 loss: 1.0468525e-06
Iter: 1430 loss: 1.0447161e-06
Iter: 1431 loss: 1.04411458e-06
Iter: 1432 loss: 1.04774347e-06
Iter: 1433 loss: 1.04396941e-06
Iter: 1434 loss: 1.04332673e-06
Iter: 1435 loss: 1.04287142e-06
Iter: 1436 loss: 1.04261539e-06
Iter: 1437 loss: 1.04178093e-06
Iter: 1438 loss: 1.04410913e-06
Iter: 1439 loss: 1.0414991e-06
Iter: 1440 loss: 1.04057744e-06
Iter: 1441 loss: 1.04267849e-06
Iter: 1442 loss: 1.04021797e-06
Iter: 1443 loss: 1.03929278e-06
Iter: 1444 loss: 1.04970263e-06
Iter: 1445 loss: 1.0393328e-06
Iter: 1446 loss: 1.03876971e-06
Iter: 1447 loss: 1.03930984e-06
Iter: 1448 loss: 1.03848174e-06
Iter: 1449 loss: 1.0378194e-06
Iter: 1450 loss: 1.03967511e-06
Iter: 1451 loss: 1.03765e-06
Iter: 1452 loss: 1.03687557e-06
Iter: 1453 loss: 1.03974025e-06
Iter: 1454 loss: 1.03672119e-06
Iter: 1455 loss: 1.03618152e-06
Iter: 1456 loss: 1.03575212e-06
Iter: 1457 loss: 1.03554089e-06
Iter: 1458 loss: 1.03459763e-06
Iter: 1459 loss: 1.03577645e-06
Iter: 1460 loss: 1.03407604e-06
Iter: 1461 loss: 1.03310299e-06
Iter: 1462 loss: 1.03815205e-06
Iter: 1463 loss: 1.03291836e-06
Iter: 1464 loss: 1.03232105e-06
Iter: 1465 loss: 1.03230389e-06
Iter: 1466 loss: 1.03179946e-06
Iter: 1467 loss: 1.03182811e-06
Iter: 1468 loss: 1.03139507e-06
Iter: 1469 loss: 1.03071795e-06
Iter: 1470 loss: 1.03195441e-06
Iter: 1471 loss: 1.03042203e-06
Iter: 1472 loss: 1.02977401e-06
Iter: 1473 loss: 1.03298578e-06
Iter: 1474 loss: 1.02962156e-06
Iter: 1475 loss: 1.02904539e-06
Iter: 1476 loss: 1.02996921e-06
Iter: 1477 loss: 1.02874355e-06
Iter: 1478 loss: 1.02805791e-06
Iter: 1479 loss: 1.02822025e-06
Iter: 1480 loss: 1.02753415e-06
Iter: 1481 loss: 1.02661352e-06
Iter: 1482 loss: 1.02668184e-06
Iter: 1483 loss: 1.02593856e-06
Iter: 1484 loss: 1.02497779e-06
Iter: 1485 loss: 1.03542277e-06
Iter: 1486 loss: 1.02494459e-06
Iter: 1487 loss: 1.02428589e-06
Iter: 1488 loss: 1.03104071e-06
Iter: 1489 loss: 1.02420086e-06
Iter: 1490 loss: 1.02375543e-06
Iter: 1491 loss: 1.02367267e-06
Iter: 1492 loss: 1.02335355e-06
Iter: 1493 loss: 1.02260071e-06
Iter: 1494 loss: 1.0250767e-06
Iter: 1495 loss: 1.02236402e-06
Iter: 1496 loss: 1.02166018e-06
Iter: 1497 loss: 1.02353192e-06
Iter: 1498 loss: 1.0214502e-06
Iter: 1499 loss: 1.02076797e-06
Iter: 1500 loss: 1.02056379e-06
Iter: 1501 loss: 1.02021522e-06
Iter: 1502 loss: 1.0193221e-06
Iter: 1503 loss: 1.02230081e-06
Iter: 1504 loss: 1.01907676e-06
Iter: 1505 loss: 1.01860064e-06
Iter: 1506 loss: 1.01854016e-06
Iter: 1507 loss: 1.01812589e-06
Iter: 1508 loss: 1.01768478e-06
Iter: 1509 loss: 1.01761486e-06
Iter: 1510 loss: 1.01692626e-06
Iter: 1511 loss: 1.01772923e-06
Iter: 1512 loss: 1.01653814e-06
Iter: 1513 loss: 1.01579064e-06
Iter: 1514 loss: 1.02002014e-06
Iter: 1515 loss: 1.0157105e-06
Iter: 1516 loss: 1.01501337e-06
Iter: 1517 loss: 1.01594992e-06
Iter: 1518 loss: 1.01479816e-06
Iter: 1519 loss: 1.0139288e-06
Iter: 1520 loss: 1.01892306e-06
Iter: 1521 loss: 1.0138881e-06
Iter: 1522 loss: 1.01326191e-06
Iter: 1523 loss: 1.01333944e-06
Iter: 1524 loss: 1.01281239e-06
Iter: 1525 loss: 1.01206183e-06
Iter: 1526 loss: 1.01164039e-06
Iter: 1527 loss: 1.01128421e-06
Iter: 1528 loss: 1.01052365e-06
Iter: 1529 loss: 1.0104967e-06
Iter: 1530 loss: 1.00975649e-06
Iter: 1531 loss: 1.01186492e-06
Iter: 1532 loss: 1.00950524e-06
Iter: 1533 loss: 1.008862e-06
Iter: 1534 loss: 1.009784e-06
Iter: 1535 loss: 1.0084957e-06
Iter: 1536 loss: 1.00783177e-06
Iter: 1537 loss: 1.01273008e-06
Iter: 1538 loss: 1.00776674e-06
Iter: 1539 loss: 1.00718637e-06
Iter: 1540 loss: 1.00677516e-06
Iter: 1541 loss: 1.00657496e-06
Iter: 1542 loss: 1.00582724e-06
Iter: 1543 loss: 1.00947818e-06
Iter: 1544 loss: 1.00571992e-06
Iter: 1545 loss: 1.00509578e-06
Iter: 1546 loss: 1.00794705e-06
Iter: 1547 loss: 1.00504917e-06
Iter: 1548 loss: 1.00435489e-06
Iter: 1549 loss: 1.00618558e-06
Iter: 1550 loss: 1.00413945e-06
Iter: 1551 loss: 1.00367288e-06
Iter: 1552 loss: 1.00301691e-06
Iter: 1553 loss: 1.00293505e-06
Iter: 1554 loss: 1.00217471e-06
Iter: 1555 loss: 1.00970874e-06
Iter: 1556 loss: 1.00213174e-06
Iter: 1557 loss: 1.00148486e-06
Iter: 1558 loss: 1.00321972e-06
Iter: 1559 loss: 1.00130308e-06
Iter: 1560 loss: 1.00066393e-06
Iter: 1561 loss: 1.00324849e-06
Iter: 1562 loss: 1.00054376e-06
Iter: 1563 loss: 9.99988e-07
Iter: 1564 loss: 9.99572649e-07
Iter: 1565 loss: 9.99313443e-07
Iter: 1566 loss: 9.98477617e-07
Iter: 1567 loss: 1.00134957e-06
Iter: 1568 loss: 9.98324367e-07
Iter: 1569 loss: 9.97474444e-07
Iter: 1570 loss: 9.97448524e-07
Iter: 1571 loss: 9.9686e-07
Iter: 1572 loss: 9.96167614e-07
Iter: 1573 loss: 9.9613635e-07
Iter: 1574 loss: 9.95606797e-07
Iter: 1575 loss: 9.9693375e-07
Iter: 1576 loss: 9.95401251e-07
Iter: 1577 loss: 9.94902166e-07
Iter: 1578 loss: 9.94481411e-07
Iter: 1579 loss: 9.94347829e-07
Iter: 1580 loss: 9.93497e-07
Iter: 1581 loss: 1.00163606e-06
Iter: 1582 loss: 9.93459594e-07
Iter: 1583 loss: 9.92928904e-07
Iter: 1584 loss: 9.93344429e-07
Iter: 1585 loss: 9.92629339e-07
Iter: 1586 loss: 9.92074547e-07
Iter: 1587 loss: 9.94749144e-07
Iter: 1588 loss: 9.91997e-07
Iter: 1589 loss: 9.91326942e-07
Iter: 1590 loss: 9.92129685e-07
Iter: 1591 loss: 9.90973263e-07
Iter: 1592 loss: 9.90356511e-07
Iter: 1593 loss: 9.9086958e-07
Iter: 1594 loss: 9.89992714e-07
Iter: 1595 loss: 9.89293426e-07
Iter: 1596 loss: 9.89481805e-07
Iter: 1597 loss: 9.88777629e-07
Iter: 1598 loss: 9.8783994e-07
Iter: 1599 loss: 9.93848744e-07
Iter: 1600 loss: 9.87797421e-07
Iter: 1601 loss: 9.86970235e-07
Iter: 1602 loss: 9.91585239e-07
Iter: 1603 loss: 9.86868713e-07
Iter: 1604 loss: 9.86360419e-07
Iter: 1605 loss: 9.86824261e-07
Iter: 1606 loss: 9.86042778e-07
Iter: 1607 loss: 9.8542489e-07
Iter: 1608 loss: 9.86734676e-07
Iter: 1609 loss: 9.85160568e-07
Iter: 1610 loss: 9.84554276e-07
Iter: 1611 loss: 9.8528858e-07
Iter: 1612 loss: 9.84214694e-07
Iter: 1613 loss: 9.83448e-07
Iter: 1614 loss: 9.83824407e-07
Iter: 1615 loss: 9.83036102e-07
Iter: 1616 loss: 9.82171741e-07
Iter: 1617 loss: 9.91889465e-07
Iter: 1618 loss: 9.82165602e-07
Iter: 1619 loss: 9.81449602e-07
Iter: 1620 loss: 9.8441933e-07
Iter: 1621 loss: 9.8134069e-07
Iter: 1622 loss: 9.80735877e-07
Iter: 1623 loss: 9.80594e-07
Iter: 1624 loss: 9.80241566e-07
Iter: 1625 loss: 9.79592414e-07
Iter: 1626 loss: 9.85688e-07
Iter: 1627 loss: 9.79596734e-07
Iter: 1628 loss: 9.7909151e-07
Iter: 1629 loss: 9.79363108e-07
Iter: 1630 loss: 9.7877e-07
Iter: 1631 loss: 9.78106868e-07
Iter: 1632 loss: 9.78655862e-07
Iter: 1633 loss: 9.77800823e-07
Iter: 1634 loss: 9.77289119e-07
Iter: 1635 loss: 9.77271384e-07
Iter: 1636 loss: 9.76824595e-07
Iter: 1637 loss: 9.77360401e-07
Iter: 1638 loss: 9.76565389e-07
Iter: 1639 loss: 9.76092224e-07
Iter: 1640 loss: 9.75482635e-07
Iter: 1641 loss: 9.75369176e-07
Iter: 1642 loss: 9.74441491e-07
Iter: 1643 loss: 9.77942818e-07
Iter: 1644 loss: 9.74201157e-07
Iter: 1645 loss: 9.73505848e-07
Iter: 1646 loss: 9.76315391e-07
Iter: 1647 loss: 9.73339411e-07
Iter: 1648 loss: 9.72707312e-07
Iter: 1649 loss: 9.75086664e-07
Iter: 1650 loss: 9.7253178e-07
Iter: 1651 loss: 9.71906502e-07
Iter: 1652 loss: 9.73478e-07
Iter: 1653 loss: 9.71659233e-07
Iter: 1654 loss: 9.71075679e-07
Iter: 1655 loss: 9.74793579e-07
Iter: 1656 loss: 9.70993142e-07
Iter: 1657 loss: 9.70476776e-07
Iter: 1658 loss: 9.71039299e-07
Iter: 1659 loss: 9.70104793e-07
Iter: 1660 loss: 9.69647658e-07
Iter: 1661 loss: 9.68955419e-07
Iter: 1662 loss: 9.68935296e-07
Iter: 1663 loss: 9.6800386e-07
Iter: 1664 loss: 9.76285492e-07
Iter: 1665 loss: 9.67965207e-07
Iter: 1666 loss: 9.67356073e-07
Iter: 1667 loss: 9.7608438e-07
Iter: 1668 loss: 9.67337655e-07
Iter: 1669 loss: 9.66966127e-07
Iter: 1670 loss: 9.66465223e-07
Iter: 1671 loss: 9.66392463e-07
Iter: 1672 loss: 9.65781283e-07
Iter: 1673 loss: 9.70583756e-07
Iter: 1674 loss: 9.65723871e-07
Iter: 1675 loss: 9.65136223e-07
Iter: 1676 loss: 9.68500331e-07
Iter: 1677 loss: 9.65029699e-07
Iter: 1678 loss: 9.64722858e-07
Iter: 1679 loss: 9.64359288e-07
Iter: 1680 loss: 9.64297669e-07
Iter: 1681 loss: 9.63622369e-07
Iter: 1682 loss: 9.67062761e-07
Iter: 1683 loss: 9.63538241e-07
Iter: 1684 loss: 9.6296435e-07
Iter: 1685 loss: 9.63010848e-07
Iter: 1686 loss: 9.62577246e-07
Iter: 1687 loss: 9.61895921e-07
Iter: 1688 loss: 9.63336902e-07
Iter: 1689 loss: 9.61658884e-07
Iter: 1690 loss: 9.60823e-07
Iter: 1691 loss: 9.6179474e-07
Iter: 1692 loss: 9.60353e-07
Iter: 1693 loss: 9.59577847e-07
Iter: 1694 loss: 9.635678e-07
Iter: 1695 loss: 9.59482463e-07
Iter: 1696 loss: 9.58669148e-07
Iter: 1697 loss: 9.63444677e-07
Iter: 1698 loss: 9.58600822e-07
Iter: 1699 loss: 9.58051487e-07
Iter: 1700 loss: 9.59917429e-07
Iter: 1701 loss: 9.57875841e-07
Iter: 1702 loss: 9.57424277e-07
Iter: 1703 loss: 9.57985435e-07
Iter: 1704 loss: 9.57186899e-07
Iter: 1705 loss: 9.56559575e-07
Iter: 1706 loss: 9.56276835e-07
Iter: 1707 loss: 9.55931e-07
Iter: 1708 loss: 9.55408e-07
Iter: 1709 loss: 9.55393716e-07
Iter: 1710 loss: 9.549683e-07
Iter: 1711 loss: 9.55320729e-07
Iter: 1712 loss: 9.54638608e-07
Iter: 1713 loss: 9.54080519e-07
Iter: 1714 loss: 9.54511506e-07
Iter: 1715 loss: 9.5377527e-07
Iter: 1716 loss: 9.5302704e-07
Iter: 1717 loss: 9.60230068e-07
Iter: 1718 loss: 9.53035169e-07
Iter: 1719 loss: 9.52661424e-07
Iter: 1720 loss: 9.5229791e-07
Iter: 1721 loss: 9.52158189e-07
Iter: 1722 loss: 9.51591687e-07
Iter: 1723 loss: 9.53161e-07
Iter: 1724 loss: 9.51404274e-07
Iter: 1725 loss: 9.5068674e-07
Iter: 1726 loss: 9.533602e-07
Iter: 1727 loss: 9.50492733e-07
Iter: 1728 loss: 9.49847959e-07
Iter: 1729 loss: 9.49825e-07
Iter: 1730 loss: 9.49366665e-07
Iter: 1731 loss: 9.48665217e-07
Iter: 1732 loss: 9.50690207e-07
Iter: 1733 loss: 9.48414197e-07
Iter: 1734 loss: 9.47677677e-07
Iter: 1735 loss: 9.50405251e-07
Iter: 1736 loss: 9.47487138e-07
Iter: 1737 loss: 9.4698612e-07
Iter: 1738 loss: 9.5144884e-07
Iter: 1739 loss: 9.46897785e-07
Iter: 1740 loss: 9.46340265e-07
Iter: 1741 loss: 9.46845091e-07
Iter: 1742 loss: 9.46014552e-07
Iter: 1743 loss: 9.45416502e-07
Iter: 1744 loss: 9.46570935e-07
Iter: 1745 loss: 9.45100794e-07
Iter: 1746 loss: 9.44462215e-07
Iter: 1747 loss: 9.46586226e-07
Iter: 1748 loss: 9.44330168e-07
Iter: 1749 loss: 9.43732175e-07
Iter: 1750 loss: 9.44555609e-07
Iter: 1751 loss: 9.43415444e-07
Iter: 1752 loss: 9.42828308e-07
Iter: 1753 loss: 9.50330445e-07
Iter: 1754 loss: 9.42832855e-07
Iter: 1755 loss: 9.42350425e-07
Iter: 1756 loss: 9.42092299e-07
Iter: 1757 loss: 9.41912447e-07
Iter: 1758 loss: 9.41395513e-07
Iter: 1759 loss: 9.47797048e-07
Iter: 1760 loss: 9.41396763e-07
Iter: 1761 loss: 9.40973052e-07
Iter: 1762 loss: 9.40796497e-07
Iter: 1763 loss: 9.405789e-07
Iter: 1764 loss: 9.40035079e-07
Iter: 1765 loss: 9.40167467e-07
Iter: 1766 loss: 9.39637687e-07
Iter: 1767 loss: 9.3897188e-07
Iter: 1768 loss: 9.44677367e-07
Iter: 1769 loss: 9.38885137e-07
Iter: 1770 loss: 9.38324263e-07
Iter: 1771 loss: 9.39043844e-07
Iter: 1772 loss: 9.37987e-07
Iter: 1773 loss: 9.37457457e-07
Iter: 1774 loss: 9.3790851e-07
Iter: 1775 loss: 9.37119466e-07
Iter: 1776 loss: 9.36337585e-07
Iter: 1777 loss: 9.37432276e-07
Iter: 1778 loss: 9.35928e-07
Iter: 1779 loss: 9.35424339e-07
Iter: 1780 loss: 9.35422236e-07
Iter: 1781 loss: 9.34917125e-07
Iter: 1782 loss: 9.34952368e-07
Iter: 1783 loss: 9.34561172e-07
Iter: 1784 loss: 9.33978754e-07
Iter: 1785 loss: 9.34622e-07
Iter: 1786 loss: 9.3362172e-07
Iter: 1787 loss: 9.33070055e-07
Iter: 1788 loss: 9.35213393e-07
Iter: 1789 loss: 9.32881733e-07
Iter: 1790 loss: 9.32398052e-07
Iter: 1791 loss: 9.39191864e-07
Iter: 1792 loss: 9.32409648e-07
Iter: 1793 loss: 9.31979116e-07
Iter: 1794 loss: 9.3154307e-07
Iter: 1795 loss: 9.3148924e-07
Iter: 1796 loss: 9.30962642e-07
Iter: 1797 loss: 9.38865242e-07
Iter: 1798 loss: 9.30973329e-07
Iter: 1799 loss: 9.30508861e-07
Iter: 1800 loss: 9.30026317e-07
Iter: 1801 loss: 9.29941848e-07
Iter: 1802 loss: 9.29338967e-07
Iter: 1803 loss: 9.30543e-07
Iter: 1804 loss: 9.29147689e-07
Iter: 1805 loss: 9.28575673e-07
Iter: 1806 loss: 9.34352101e-07
Iter: 1807 loss: 9.28607733e-07
Iter: 1808 loss: 9.28122859e-07
Iter: 1809 loss: 9.28022416e-07
Iter: 1810 loss: 9.27791746e-07
Iter: 1811 loss: 9.27167434e-07
Iter: 1812 loss: 9.27945109e-07
Iter: 1813 loss: 9.2683922e-07
Iter: 1814 loss: 9.26197458e-07
Iter: 1815 loss: 9.30339525e-07
Iter: 1816 loss: 9.26116513e-07
Iter: 1817 loss: 9.25578e-07
Iter: 1818 loss: 9.28494046e-07
Iter: 1819 loss: 9.25564223e-07
Iter: 1820 loss: 9.25117547e-07
Iter: 1821 loss: 9.2569951e-07
Iter: 1822 loss: 9.24835376e-07
Iter: 1823 loss: 9.24289964e-07
Iter: 1824 loss: 9.24854078e-07
Iter: 1825 loss: 9.23923608e-07
Iter: 1826 loss: 9.23327e-07
Iter: 1827 loss: 9.24409676e-07
Iter: 1828 loss: 9.23048901e-07
Iter: 1829 loss: 9.22608137e-07
Iter: 1830 loss: 9.29245402e-07
Iter: 1831 loss: 9.22617232e-07
Iter: 1832 loss: 9.22116158e-07
Iter: 1833 loss: 9.21793514e-07
Iter: 1834 loss: 9.21601213e-07
Iter: 1835 loss: 9.21027549e-07
Iter: 1836 loss: 9.24694689e-07
Iter: 1837 loss: 9.2093228e-07
Iter: 1838 loss: 9.20499588e-07
Iter: 1839 loss: 9.22716254e-07
Iter: 1840 loss: 9.20464e-07
Iter: 1841 loss: 9.20040179e-07
Iter: 1842 loss: 9.195046e-07
Iter: 1843 loss: 9.19501133e-07
Iter: 1844 loss: 9.1883237e-07
Iter: 1845 loss: 9.21191486e-07
Iter: 1846 loss: 9.1869515e-07
Iter: 1847 loss: 9.18208343e-07
Iter: 1848 loss: 9.20707521e-07
Iter: 1849 loss: 9.18093292e-07
Iter: 1850 loss: 9.17453e-07
Iter: 1851 loss: 9.19147794e-07
Iter: 1852 loss: 9.17244e-07
Iter: 1853 loss: 9.16764179e-07
Iter: 1854 loss: 9.1672689e-07
Iter: 1855 loss: 9.16376791e-07
Iter: 1856 loss: 9.15737132e-07
Iter: 1857 loss: 9.20703826e-07
Iter: 1858 loss: 9.157161e-07
Iter: 1859 loss: 9.15102873e-07
Iter: 1860 loss: 9.17039131e-07
Iter: 1861 loss: 9.14932912e-07
Iter: 1862 loss: 9.14467421e-07
Iter: 1863 loss: 9.14632494e-07
Iter: 1864 loss: 9.14100497e-07
Iter: 1865 loss: 9.13549343e-07
Iter: 1866 loss: 9.14454631e-07
Iter: 1867 loss: 9.13307701e-07
Iter: 1868 loss: 9.12804637e-07
Iter: 1869 loss: 9.20717582e-07
Iter: 1870 loss: 9.12792473e-07
Iter: 1871 loss: 9.12342443e-07
Iter: 1872 loss: 9.12530936e-07
Iter: 1873 loss: 9.12016787e-07
Iter: 1874 loss: 9.11548341e-07
Iter: 1875 loss: 9.1299e-07
Iter: 1876 loss: 9.11406801e-07
Iter: 1877 loss: 9.10885092e-07
Iter: 1878 loss: 9.11979328e-07
Iter: 1879 loss: 9.10669826e-07
Iter: 1880 loss: 9.10183189e-07
Iter: 1881 loss: 9.1111724e-07
Iter: 1882 loss: 9.0992387e-07
Iter: 1883 loss: 9.09399603e-07
Iter: 1884 loss: 9.10111112e-07
Iter: 1885 loss: 9.09132439e-07
Iter: 1886 loss: 9.08615448e-07
Iter: 1887 loss: 9.09089749e-07
Iter: 1888 loss: 9.08227662e-07
Iter: 1889 loss: 9.07504273e-07
Iter: 1890 loss: 9.09568826e-07
Iter: 1891 loss: 9.0728571e-07
Iter: 1892 loss: 9.06826074e-07
Iter: 1893 loss: 9.11992629e-07
Iter: 1894 loss: 9.06826699e-07
Iter: 1895 loss: 9.06334492e-07
Iter: 1896 loss: 9.06554931e-07
Iter: 1897 loss: 9.06017476e-07
Iter: 1898 loss: 9.05463423e-07
Iter: 1899 loss: 9.06273385e-07
Iter: 1900 loss: 9.05131742e-07
Iter: 1901 loss: 9.04736225e-07
Iter: 1902 loss: 9.10216386e-07
Iter: 1903 loss: 9.04737306e-07
Iter: 1904 loss: 9.04276476e-07
Iter: 1905 loss: 9.04022443e-07
Iter: 1906 loss: 9.03852651e-07
Iter: 1907 loss: 9.03258069e-07
Iter: 1908 loss: 9.04241176e-07
Iter: 1909 loss: 9.03015462e-07
Iter: 1910 loss: 9.02502052e-07
Iter: 1911 loss: 9.09253856e-07
Iter: 1912 loss: 9.02529962e-07
Iter: 1913 loss: 9.02075726e-07
Iter: 1914 loss: 9.02859028e-07
Iter: 1915 loss: 9.0182283e-07
Iter: 1916 loss: 9.01429644e-07
Iter: 1917 loss: 9.01549072e-07
Iter: 1918 loss: 9.01148212e-07
Iter: 1919 loss: 9.0059882e-07
Iter: 1920 loss: 9.04693081e-07
Iter: 1921 loss: 9.00531745e-07
Iter: 1922 loss: 9.00034365e-07
Iter: 1923 loss: 8.99522433e-07
Iter: 1924 loss: 8.99419888e-07
Iter: 1925 loss: 8.98863505e-07
Iter: 1926 loss: 9.00667487e-07
Iter: 1927 loss: 8.98714632e-07
Iter: 1928 loss: 8.98073949e-07
Iter: 1929 loss: 8.99166e-07
Iter: 1930 loss: 8.97791892e-07
Iter: 1931 loss: 8.97271661e-07
Iter: 1932 loss: 9.02727152e-07
Iter: 1933 loss: 8.97224254e-07
Iter: 1934 loss: 8.96829647e-07
Iter: 1935 loss: 8.9761545e-07
Iter: 1936 loss: 8.96616825e-07
Iter: 1937 loss: 8.96105064e-07
Iter: 1938 loss: 8.96699078e-07
Iter: 1939 loss: 8.95835456e-07
Iter: 1940 loss: 8.95277878e-07
Iter: 1941 loss: 8.9808384e-07
Iter: 1942 loss: 8.95218704e-07
Iter: 1943 loss: 8.94697905e-07
Iter: 1944 loss: 8.95469498e-07
Iter: 1945 loss: 8.94483946e-07
Iter: 1946 loss: 8.93962238e-07
Iter: 1947 loss: 8.97093287e-07
Iter: 1948 loss: 8.93888796e-07
Iter: 1949 loss: 8.93395907e-07
Iter: 1950 loss: 8.93407673e-07
Iter: 1951 loss: 8.92997775e-07
Iter: 1952 loss: 8.92458445e-07
Iter: 1953 loss: 8.92981063e-07
Iter: 1954 loss: 8.9218679e-07
Iter: 1955 loss: 8.9175478e-07
Iter: 1956 loss: 8.91722266e-07
Iter: 1957 loss: 8.91445552e-07
Iter: 1958 loss: 8.91183788e-07
Iter: 1959 loss: 8.91099774e-07
Iter: 1960 loss: 8.90657e-07
Iter: 1961 loss: 8.92775688e-07
Iter: 1962 loss: 8.90567776e-07
Iter: 1963 loss: 8.90139518e-07
Iter: 1964 loss: 8.91665422e-07
Iter: 1965 loss: 8.90007925e-07
Iter: 1966 loss: 8.89673572e-07
Iter: 1967 loss: 8.89183184e-07
Iter: 1968 loss: 8.89167666e-07
Iter: 1969 loss: 8.88512091e-07
Iter: 1970 loss: 8.90100409e-07
Iter: 1971 loss: 8.88298473e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script73
+ '[' -r STOP.script73 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi1.6/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi2
+ date
Sat Nov  7 15:11:31 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi2/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi2_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi2/500_500_500_500_1 --optimizer lbfgs --function f2 --psi 0 --alpha 2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi2_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60ea6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60ea6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60ea6048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60ea62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60e16bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60e16c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60dd3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60dd3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60dd3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60d61620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60ceb510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60d1ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60e8bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60d1aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60c722f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60cdaae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60c72488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60c306a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60bfd730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60bfd7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60bc99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60b7b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c60b88ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c58943620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c589430d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c5894a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c5896f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c5896fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c589bb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c589027b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c588e6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c588e6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c588fff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c588a18c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c5887d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6c5887d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.42930289e-05
Iter: 2 loss: 3.01226755e-05
Iter: 3 loss: 3.367912e-05
Iter: 4 loss: 2.7656617e-05
Iter: 5 loss: 2.39057517e-05
Iter: 6 loss: 3.56086093e-05
Iter: 7 loss: 2.28233603e-05
Iter: 8 loss: 1.97936279e-05
Iter: 9 loss: 2.34802828e-05
Iter: 10 loss: 1.82015683e-05
Iter: 11 loss: 1.57610957e-05
Iter: 12 loss: 3.51376439e-05
Iter: 13 loss: 1.55953749e-05
Iter: 14 loss: 1.42975632e-05
Iter: 15 loss: 1.94962231e-05
Iter: 16 loss: 1.40072061e-05
Iter: 17 loss: 1.30433746e-05
Iter: 18 loss: 1.34440406e-05
Iter: 19 loss: 1.23795417e-05
Iter: 20 loss: 1.12249099e-05
Iter: 21 loss: 1.81056075e-05
Iter: 22 loss: 1.10786623e-05
Iter: 23 loss: 1.03581042e-05
Iter: 24 loss: 1.43175839e-05
Iter: 25 loss: 1.02532786e-05
Iter: 26 loss: 9.65085565e-06
Iter: 27 loss: 1.08563945e-05
Iter: 28 loss: 9.40519203e-06
Iter: 29 loss: 8.78775154e-06
Iter: 30 loss: 1.0425465e-05
Iter: 31 loss: 8.58062776e-06
Iter: 32 loss: 8.06988282e-06
Iter: 33 loss: 1.00305297e-05
Iter: 34 loss: 7.95032247e-06
Iter: 35 loss: 7.54069151e-06
Iter: 36 loss: 8.2662e-06
Iter: 37 loss: 7.36092943e-06
Iter: 38 loss: 6.9316543e-06
Iter: 39 loss: 7.63453681e-06
Iter: 40 loss: 6.73551858e-06
Iter: 41 loss: 6.51461869e-06
Iter: 42 loss: 6.50350876e-06
Iter: 43 loss: 6.29357783e-06
Iter: 44 loss: 6.63076798e-06
Iter: 45 loss: 6.19691809e-06
Iter: 46 loss: 6.01040847e-06
Iter: 47 loss: 5.99270606e-06
Iter: 48 loss: 5.85563e-06
Iter: 49 loss: 5.63444746e-06
Iter: 50 loss: 8.22102811e-06
Iter: 51 loss: 5.63110461e-06
Iter: 52 loss: 5.48010576e-06
Iter: 53 loss: 5.3716567e-06
Iter: 54 loss: 5.31947217e-06
Iter: 55 loss: 5.11490407e-06
Iter: 56 loss: 6.41796396e-06
Iter: 57 loss: 5.09201391e-06
Iter: 58 loss: 4.94066126e-06
Iter: 59 loss: 5.64452785e-06
Iter: 60 loss: 4.91263881e-06
Iter: 61 loss: 4.80141716e-06
Iter: 62 loss: 5.19497053e-06
Iter: 63 loss: 4.77286812e-06
Iter: 64 loss: 4.65462108e-06
Iter: 65 loss: 4.78535549e-06
Iter: 66 loss: 4.59057037e-06
Iter: 67 loss: 4.45599653e-06
Iter: 68 loss: 4.81971438e-06
Iter: 69 loss: 4.41158863e-06
Iter: 70 loss: 4.30920863e-06
Iter: 71 loss: 4.85947203e-06
Iter: 72 loss: 4.29357169e-06
Iter: 73 loss: 4.2039519e-06
Iter: 74 loss: 4.19290291e-06
Iter: 75 loss: 4.12859026e-06
Iter: 76 loss: 4.02802743e-06
Iter: 77 loss: 4.84779775e-06
Iter: 78 loss: 4.02145815e-06
Iter: 79 loss: 3.95314328e-06
Iter: 80 loss: 4.7284957e-06
Iter: 81 loss: 3.95185634e-06
Iter: 82 loss: 3.89483739e-06
Iter: 83 loss: 3.85875956e-06
Iter: 84 loss: 3.83620591e-06
Iter: 85 loss: 3.76410958e-06
Iter: 86 loss: 3.90750756e-06
Iter: 87 loss: 3.73466219e-06
Iter: 88 loss: 3.66795575e-06
Iter: 89 loss: 4.35951688e-06
Iter: 90 loss: 3.6661454e-06
Iter: 91 loss: 3.61492084e-06
Iter: 92 loss: 3.57946374e-06
Iter: 93 loss: 3.56089572e-06
Iter: 94 loss: 3.49074571e-06
Iter: 95 loss: 3.81123664e-06
Iter: 96 loss: 3.47734408e-06
Iter: 97 loss: 3.42158251e-06
Iter: 98 loss: 3.74887e-06
Iter: 99 loss: 3.41424607e-06
Iter: 100 loss: 3.36317589e-06
Iter: 101 loss: 3.44614477e-06
Iter: 102 loss: 3.33953449e-06
Iter: 103 loss: 3.2845046e-06
Iter: 104 loss: 3.40865677e-06
Iter: 105 loss: 3.26373379e-06
Iter: 106 loss: 3.2191931e-06
Iter: 107 loss: 3.5003286e-06
Iter: 108 loss: 3.21409e-06
Iter: 109 loss: 3.17605645e-06
Iter: 110 loss: 3.13363489e-06
Iter: 111 loss: 3.12766861e-06
Iter: 112 loss: 3.07231085e-06
Iter: 113 loss: 3.63279719e-06
Iter: 114 loss: 3.07073924e-06
Iter: 115 loss: 3.03259139e-06
Iter: 116 loss: 3.23444851e-06
Iter: 117 loss: 3.02682884e-06
Iter: 118 loss: 2.99060503e-06
Iter: 119 loss: 3.13558849e-06
Iter: 120 loss: 2.98230748e-06
Iter: 121 loss: 2.94981055e-06
Iter: 122 loss: 2.94411757e-06
Iter: 123 loss: 2.92195818e-06
Iter: 124 loss: 2.88968886e-06
Iter: 125 loss: 2.9799894e-06
Iter: 126 loss: 2.8793429e-06
Iter: 127 loss: 2.84716725e-06
Iter: 128 loss: 3.1250147e-06
Iter: 129 loss: 2.84559974e-06
Iter: 130 loss: 2.82062047e-06
Iter: 131 loss: 2.80610857e-06
Iter: 132 loss: 2.79562892e-06
Iter: 133 loss: 2.76479705e-06
Iter: 134 loss: 2.82979317e-06
Iter: 135 loss: 2.75272464e-06
Iter: 136 loss: 2.71935642e-06
Iter: 137 loss: 3.08258905e-06
Iter: 138 loss: 2.71853128e-06
Iter: 139 loss: 2.70007286e-06
Iter: 140 loss: 2.70568239e-06
Iter: 141 loss: 2.6867383e-06
Iter: 142 loss: 2.65828476e-06
Iter: 143 loss: 2.71231988e-06
Iter: 144 loss: 2.64630762e-06
Iter: 145 loss: 2.62019785e-06
Iter: 146 loss: 2.73559363e-06
Iter: 147 loss: 2.61502964e-06
Iter: 148 loss: 2.59342755e-06
Iter: 149 loss: 2.63907373e-06
Iter: 150 loss: 2.58481191e-06
Iter: 151 loss: 2.56381736e-06
Iter: 152 loss: 2.62879621e-06
Iter: 153 loss: 2.55761e-06
Iter: 154 loss: 2.53646249e-06
Iter: 155 loss: 2.73495493e-06
Iter: 156 loss: 2.5355705e-06
Iter: 157 loss: 2.52099449e-06
Iter: 158 loss: 2.52247173e-06
Iter: 159 loss: 2.5097861e-06
Iter: 160 loss: 2.49257096e-06
Iter: 161 loss: 2.51269967e-06
Iter: 162 loss: 2.48344372e-06
Iter: 163 loss: 2.4592207e-06
Iter: 164 loss: 2.49647496e-06
Iter: 165 loss: 2.44791545e-06
Iter: 166 loss: 2.42866145e-06
Iter: 167 loss: 2.42862552e-06
Iter: 168 loss: 2.41846965e-06
Iter: 169 loss: 2.40394274e-06
Iter: 170 loss: 2.40340705e-06
Iter: 171 loss: 2.3814855e-06
Iter: 172 loss: 2.46002e-06
Iter: 173 loss: 2.37593258e-06
Iter: 174 loss: 2.36188089e-06
Iter: 175 loss: 2.57239208e-06
Iter: 176 loss: 2.36183746e-06
Iter: 177 loss: 2.35003927e-06
Iter: 178 loss: 2.32681805e-06
Iter: 179 loss: 2.79251867e-06
Iter: 180 loss: 2.32668322e-06
Iter: 181 loss: 2.30798082e-06
Iter: 182 loss: 2.56659587e-06
Iter: 183 loss: 2.30801902e-06
Iter: 184 loss: 2.29338e-06
Iter: 185 loss: 2.31262629e-06
Iter: 186 loss: 2.28580211e-06
Iter: 187 loss: 2.27003056e-06
Iter: 188 loss: 2.32504044e-06
Iter: 189 loss: 2.26594102e-06
Iter: 190 loss: 2.25547728e-06
Iter: 191 loss: 2.38058283e-06
Iter: 192 loss: 2.25538975e-06
Iter: 193 loss: 2.24500582e-06
Iter: 194 loss: 2.23795951e-06
Iter: 195 loss: 2.23393067e-06
Iter: 196 loss: 2.22051494e-06
Iter: 197 loss: 2.25926578e-06
Iter: 198 loss: 2.21622031e-06
Iter: 199 loss: 2.20422953e-06
Iter: 200 loss: 2.2333e-06
Iter: 201 loss: 2.19991284e-06
Iter: 202 loss: 2.18645573e-06
Iter: 203 loss: 2.23599091e-06
Iter: 204 loss: 2.18312448e-06
Iter: 205 loss: 2.17020283e-06
Iter: 206 loss: 2.21282335e-06
Iter: 207 loss: 2.16654917e-06
Iter: 208 loss: 2.15601312e-06
Iter: 209 loss: 2.16710714e-06
Iter: 210 loss: 2.15020782e-06
Iter: 211 loss: 2.13885096e-06
Iter: 212 loss: 2.16940816e-06
Iter: 213 loss: 2.13511794e-06
Iter: 214 loss: 2.12298846e-06
Iter: 215 loss: 2.21176401e-06
Iter: 216 loss: 2.1220435e-06
Iter: 217 loss: 2.1144358e-06
Iter: 218 loss: 2.12216128e-06
Iter: 219 loss: 2.11019551e-06
Iter: 220 loss: 2.10084704e-06
Iter: 221 loss: 2.09693508e-06
Iter: 222 loss: 2.09195559e-06
Iter: 223 loss: 2.07999665e-06
Iter: 224 loss: 2.22692165e-06
Iter: 225 loss: 2.07980816e-06
Iter: 226 loss: 2.07134963e-06
Iter: 227 loss: 2.08810388e-06
Iter: 228 loss: 2.06797813e-06
Iter: 229 loss: 2.0583575e-06
Iter: 230 loss: 2.12173063e-06
Iter: 231 loss: 2.05738638e-06
Iter: 232 loss: 2.0502996e-06
Iter: 233 loss: 2.05656102e-06
Iter: 234 loss: 2.04619255e-06
Iter: 235 loss: 2.03834543e-06
Iter: 236 loss: 2.0344205e-06
Iter: 237 loss: 2.03070931e-06
Iter: 238 loss: 2.02071806e-06
Iter: 239 loss: 2.0791656e-06
Iter: 240 loss: 2.01933926e-06
Iter: 241 loss: 2.00976547e-06
Iter: 242 loss: 2.07428207e-06
Iter: 243 loss: 2.00883142e-06
Iter: 244 loss: 2.00272711e-06
Iter: 245 loss: 2.0005773e-06
Iter: 246 loss: 1.99714532e-06
Iter: 247 loss: 1.98723092e-06
Iter: 248 loss: 2.02829779e-06
Iter: 249 loss: 1.98506041e-06
Iter: 250 loss: 1.97702548e-06
Iter: 251 loss: 1.99048623e-06
Iter: 252 loss: 1.97338295e-06
Iter: 253 loss: 1.96470592e-06
Iter: 254 loss: 2.04283288e-06
Iter: 255 loss: 1.96424207e-06
Iter: 256 loss: 1.95897746e-06
Iter: 257 loss: 1.95041e-06
Iter: 258 loss: 1.95038456e-06
Iter: 259 loss: 1.9426e-06
Iter: 260 loss: 2.03382706e-06
Iter: 261 loss: 1.942479e-06
Iter: 262 loss: 1.93561868e-06
Iter: 263 loss: 1.95510916e-06
Iter: 264 loss: 1.93341793e-06
Iter: 265 loss: 1.92690436e-06
Iter: 266 loss: 1.97255849e-06
Iter: 267 loss: 1.92622201e-06
Iter: 268 loss: 1.92114749e-06
Iter: 269 loss: 1.92514426e-06
Iter: 270 loss: 1.91811114e-06
Iter: 271 loss: 1.91216191e-06
Iter: 272 loss: 1.91278718e-06
Iter: 273 loss: 1.90766468e-06
Iter: 274 loss: 1.90021501e-06
Iter: 275 loss: 1.91916888e-06
Iter: 276 loss: 1.89760817e-06
Iter: 277 loss: 1.88916647e-06
Iter: 278 loss: 1.93522874e-06
Iter: 279 loss: 1.88792501e-06
Iter: 280 loss: 1.88084027e-06
Iter: 281 loss: 1.90955279e-06
Iter: 282 loss: 1.87928231e-06
Iter: 283 loss: 1.87420687e-06
Iter: 284 loss: 1.87128387e-06
Iter: 285 loss: 1.86907289e-06
Iter: 286 loss: 1.86095076e-06
Iter: 287 loss: 1.91201616e-06
Iter: 288 loss: 1.86000807e-06
Iter: 289 loss: 1.85393981e-06
Iter: 290 loss: 1.88917454e-06
Iter: 291 loss: 1.85302929e-06
Iter: 292 loss: 1.84819373e-06
Iter: 293 loss: 1.85272006e-06
Iter: 294 loss: 1.84542205e-06
Iter: 295 loss: 1.83877205e-06
Iter: 296 loss: 1.84250564e-06
Iter: 297 loss: 1.83449015e-06
Iter: 298 loss: 1.8282949e-06
Iter: 299 loss: 1.86049442e-06
Iter: 300 loss: 1.82733106e-06
Iter: 301 loss: 1.82155782e-06
Iter: 302 loss: 1.86579848e-06
Iter: 303 loss: 1.82107226e-06
Iter: 304 loss: 1.81598523e-06
Iter: 305 loss: 1.81773714e-06
Iter: 306 loss: 1.81246605e-06
Iter: 307 loss: 1.80690483e-06
Iter: 308 loss: 1.8215211e-06
Iter: 309 loss: 1.80505072e-06
Iter: 310 loss: 1.79968333e-06
Iter: 311 loss: 1.8123975e-06
Iter: 312 loss: 1.79768335e-06
Iter: 313 loss: 1.79237827e-06
Iter: 314 loss: 1.79262975e-06
Iter: 315 loss: 1.78817254e-06
Iter: 316 loss: 1.78294158e-06
Iter: 317 loss: 1.7829027e-06
Iter: 318 loss: 1.77891343e-06
Iter: 319 loss: 1.77601385e-06
Iter: 320 loss: 1.77470315e-06
Iter: 321 loss: 1.76904132e-06
Iter: 322 loss: 1.78388518e-06
Iter: 323 loss: 1.7671947e-06
Iter: 324 loss: 1.76229059e-06
Iter: 325 loss: 1.80315419e-06
Iter: 326 loss: 1.76192816e-06
Iter: 327 loss: 1.75773857e-06
Iter: 328 loss: 1.76393746e-06
Iter: 329 loss: 1.75568323e-06
Iter: 330 loss: 1.75070602e-06
Iter: 331 loss: 1.75030107e-06
Iter: 332 loss: 1.74656668e-06
Iter: 333 loss: 1.74007664e-06
Iter: 334 loss: 1.78689197e-06
Iter: 335 loss: 1.73948297e-06
Iter: 336 loss: 1.73587955e-06
Iter: 337 loss: 1.76251729e-06
Iter: 338 loss: 1.73562012e-06
Iter: 339 loss: 1.73190483e-06
Iter: 340 loss: 1.7363185e-06
Iter: 341 loss: 1.7299601e-06
Iter: 342 loss: 1.72584282e-06
Iter: 343 loss: 1.72329169e-06
Iter: 344 loss: 1.7216928e-06
Iter: 345 loss: 1.7169906e-06
Iter: 346 loss: 1.77169181e-06
Iter: 347 loss: 1.71695365e-06
Iter: 348 loss: 1.71339229e-06
Iter: 349 loss: 1.71206977e-06
Iter: 350 loss: 1.71014176e-06
Iter: 351 loss: 1.70448106e-06
Iter: 352 loss: 1.723369e-06
Iter: 353 loss: 1.70291855e-06
Iter: 354 loss: 1.69755265e-06
Iter: 355 loss: 1.73945739e-06
Iter: 356 loss: 1.69699842e-06
Iter: 357 loss: 1.69420468e-06
Iter: 358 loss: 1.69780174e-06
Iter: 359 loss: 1.69272948e-06
Iter: 360 loss: 1.68857775e-06
Iter: 361 loss: 1.68397048e-06
Iter: 362 loss: 1.68342058e-06
Iter: 363 loss: 1.68166753e-06
Iter: 364 loss: 1.68039981e-06
Iter: 365 loss: 1.67799669e-06
Iter: 366 loss: 1.67273629e-06
Iter: 367 loss: 1.7439736e-06
Iter: 368 loss: 1.67240353e-06
Iter: 369 loss: 1.66706491e-06
Iter: 370 loss: 1.72238231e-06
Iter: 371 loss: 1.66687528e-06
Iter: 372 loss: 1.66374832e-06
Iter: 373 loss: 1.69296436e-06
Iter: 374 loss: 1.66363384e-06
Iter: 375 loss: 1.66035352e-06
Iter: 376 loss: 1.65822439e-06
Iter: 377 loss: 1.65697384e-06
Iter: 378 loss: 1.65188828e-06
Iter: 379 loss: 1.66963491e-06
Iter: 380 loss: 1.65057099e-06
Iter: 381 loss: 1.64700327e-06
Iter: 382 loss: 1.64717721e-06
Iter: 383 loss: 1.6443048e-06
Iter: 384 loss: 1.63958e-06
Iter: 385 loss: 1.67928499e-06
Iter: 386 loss: 1.63929474e-06
Iter: 387 loss: 1.63554057e-06
Iter: 388 loss: 1.63742811e-06
Iter: 389 loss: 1.63290542e-06
Iter: 390 loss: 1.62915626e-06
Iter: 391 loss: 1.66893199e-06
Iter: 392 loss: 1.62900278e-06
Iter: 393 loss: 1.62587185e-06
Iter: 394 loss: 1.62466972e-06
Iter: 395 loss: 1.62285437e-06
Iter: 396 loss: 1.61809612e-06
Iter: 397 loss: 1.62722949e-06
Iter: 398 loss: 1.61619096e-06
Iter: 399 loss: 1.61294383e-06
Iter: 400 loss: 1.65362644e-06
Iter: 401 loss: 1.6129552e-06
Iter: 402 loss: 1.61002708e-06
Iter: 403 loss: 1.61216917e-06
Iter: 404 loss: 1.60819729e-06
Iter: 405 loss: 1.60472473e-06
Iter: 406 loss: 1.60805052e-06
Iter: 407 loss: 1.60274544e-06
Iter: 408 loss: 1.59947967e-06
Iter: 409 loss: 1.63188145e-06
Iter: 410 loss: 1.5994143e-06
Iter: 411 loss: 1.59595129e-06
Iter: 412 loss: 1.59809974e-06
Iter: 413 loss: 1.59372826e-06
Iter: 414 loss: 1.59029639e-06
Iter: 415 loss: 1.59644424e-06
Iter: 416 loss: 1.58881244e-06
Iter: 417 loss: 1.58538739e-06
Iter: 418 loss: 1.5905822e-06
Iter: 419 loss: 1.58376054e-06
Iter: 420 loss: 1.57967975e-06
Iter: 421 loss: 1.59225647e-06
Iter: 422 loss: 1.57850582e-06
Iter: 423 loss: 1.5745253e-06
Iter: 424 loss: 1.58823798e-06
Iter: 425 loss: 1.57351099e-06
Iter: 426 loss: 1.57016837e-06
Iter: 427 loss: 1.58818909e-06
Iter: 428 loss: 1.56973897e-06
Iter: 429 loss: 1.56655199e-06
Iter: 430 loss: 1.56795443e-06
Iter: 431 loss: 1.56447993e-06
Iter: 432 loss: 1.56011515e-06
Iter: 433 loss: 1.57383636e-06
Iter: 434 loss: 1.5589203e-06
Iter: 435 loss: 1.55564931e-06
Iter: 436 loss: 1.56324984e-06
Iter: 437 loss: 1.55441523e-06
Iter: 438 loss: 1.55094324e-06
Iter: 439 loss: 1.5654889e-06
Iter: 440 loss: 1.55022008e-06
Iter: 441 loss: 1.54681493e-06
Iter: 442 loss: 1.55735074e-06
Iter: 443 loss: 1.54574332e-06
Iter: 444 loss: 1.54280315e-06
Iter: 445 loss: 1.54372196e-06
Iter: 446 loss: 1.54066606e-06
Iter: 447 loss: 1.53784254e-06
Iter: 448 loss: 1.53777876e-06
Iter: 449 loss: 1.53560597e-06
Iter: 450 loss: 1.533057e-06
Iter: 451 loss: 1.53273356e-06
Iter: 452 loss: 1.5290625e-06
Iter: 453 loss: 1.52923099e-06
Iter: 454 loss: 1.52622795e-06
Iter: 455 loss: 1.52240045e-06
Iter: 456 loss: 1.56108808e-06
Iter: 457 loss: 1.52226539e-06
Iter: 458 loss: 1.51908353e-06
Iter: 459 loss: 1.52985967e-06
Iter: 460 loss: 1.51838071e-06
Iter: 461 loss: 1.51545282e-06
Iter: 462 loss: 1.51955032e-06
Iter: 463 loss: 1.51397762e-06
Iter: 464 loss: 1.51066888e-06
Iter: 465 loss: 1.5328053e-06
Iter: 466 loss: 1.51034908e-06
Iter: 467 loss: 1.50805658e-06
Iter: 468 loss: 1.5085194e-06
Iter: 469 loss: 1.5063099e-06
Iter: 470 loss: 1.50319124e-06
Iter: 471 loss: 1.51394079e-06
Iter: 472 loss: 1.50238952e-06
Iter: 473 loss: 1.49939819e-06
Iter: 474 loss: 1.50800884e-06
Iter: 475 loss: 1.49838775e-06
Iter: 476 loss: 1.4957651e-06
Iter: 477 loss: 1.51066843e-06
Iter: 478 loss: 1.49541609e-06
Iter: 479 loss: 1.49277264e-06
Iter: 480 loss: 1.49355014e-06
Iter: 481 loss: 1.49093489e-06
Iter: 482 loss: 1.48862512e-06
Iter: 483 loss: 1.51395966e-06
Iter: 484 loss: 1.48855906e-06
Iter: 485 loss: 1.48601e-06
Iter: 486 loss: 1.48352126e-06
Iter: 487 loss: 1.4830116e-06
Iter: 488 loss: 1.47996e-06
Iter: 489 loss: 1.48794197e-06
Iter: 490 loss: 1.47895923e-06
Iter: 491 loss: 1.47564583e-06
Iter: 492 loss: 1.47917763e-06
Iter: 493 loss: 1.47378273e-06
Iter: 494 loss: 1.47035985e-06
Iter: 495 loss: 1.48572622e-06
Iter: 496 loss: 1.46970569e-06
Iter: 497 loss: 1.46681089e-06
Iter: 498 loss: 1.49034656e-06
Iter: 499 loss: 1.46663501e-06
Iter: 500 loss: 1.46425941e-06
Iter: 501 loss: 1.46352431e-06
Iter: 502 loss: 1.46201137e-06
Iter: 503 loss: 1.45903255e-06
Iter: 504 loss: 1.48897243e-06
Iter: 505 loss: 1.45893523e-06
Iter: 506 loss: 1.45673653e-06
Iter: 507 loss: 1.45465197e-06
Iter: 508 loss: 1.45417334e-06
Iter: 509 loss: 1.4513796e-06
Iter: 510 loss: 1.47969695e-06
Iter: 511 loss: 1.45131776e-06
Iter: 512 loss: 1.44869784e-06
Iter: 513 loss: 1.45118725e-06
Iter: 514 loss: 1.44717728e-06
Iter: 515 loss: 1.44453747e-06
Iter: 516 loss: 1.46044749e-06
Iter: 517 loss: 1.44423825e-06
Iter: 518 loss: 1.441878e-06
Iter: 519 loss: 1.44411479e-06
Iter: 520 loss: 1.44051205e-06
Iter: 521 loss: 1.43745183e-06
Iter: 522 loss: 1.45759884e-06
Iter: 523 loss: 1.43706802e-06
Iter: 524 loss: 1.43519333e-06
Iter: 525 loss: 1.43346324e-06
Iter: 526 loss: 1.43293346e-06
Iter: 527 loss: 1.42979e-06
Iter: 528 loss: 1.4325251e-06
Iter: 529 loss: 1.42787303e-06
Iter: 530 loss: 1.42403371e-06
Iter: 531 loss: 1.43766397e-06
Iter: 532 loss: 1.42317231e-06
Iter: 533 loss: 1.4201737e-06
Iter: 534 loss: 1.44661658e-06
Iter: 535 loss: 1.42004592e-06
Iter: 536 loss: 1.41734301e-06
Iter: 537 loss: 1.42300576e-06
Iter: 538 loss: 1.4162099e-06
Iter: 539 loss: 1.41375517e-06
Iter: 540 loss: 1.42039812e-06
Iter: 541 loss: 1.41297505e-06
Iter: 542 loss: 1.41011321e-06
Iter: 543 loss: 1.41405849e-06
Iter: 544 loss: 1.40875e-06
Iter: 545 loss: 1.40621944e-06
Iter: 546 loss: 1.4194884e-06
Iter: 547 loss: 1.40585507e-06
Iter: 548 loss: 1.40360908e-06
Iter: 549 loss: 1.40664406e-06
Iter: 550 loss: 1.40252678e-06
Iter: 551 loss: 1.39979261e-06
Iter: 552 loss: 1.41311216e-06
Iter: 553 loss: 1.39928727e-06
Iter: 554 loss: 1.39713632e-06
Iter: 555 loss: 1.40151553e-06
Iter: 556 loss: 1.39628446e-06
Iter: 557 loss: 1.39407553e-06
Iter: 558 loss: 1.40917882e-06
Iter: 559 loss: 1.3938452e-06
Iter: 560 loss: 1.39208441e-06
Iter: 561 loss: 1.39139343e-06
Iter: 562 loss: 1.3904961e-06
Iter: 563 loss: 1.38820837e-06
Iter: 564 loss: 1.39165081e-06
Iter: 565 loss: 1.38715177e-06
Iter: 566 loss: 1.3838104e-06
Iter: 567 loss: 1.38685527e-06
Iter: 568 loss: 1.38192115e-06
Iter: 569 loss: 1.37926304e-06
Iter: 570 loss: 1.38665769e-06
Iter: 571 loss: 1.37838106e-06
Iter: 572 loss: 1.37539382e-06
Iter: 573 loss: 1.39803649e-06
Iter: 574 loss: 1.37512109e-06
Iter: 575 loss: 1.37265e-06
Iter: 576 loss: 1.37888185e-06
Iter: 577 loss: 1.37180882e-06
Iter: 578 loss: 1.36957874e-06
Iter: 579 loss: 1.37150164e-06
Iter: 580 loss: 1.36842e-06
Iter: 581 loss: 1.36575807e-06
Iter: 582 loss: 1.37923826e-06
Iter: 583 loss: 1.36535937e-06
Iter: 584 loss: 1.3629874e-06
Iter: 585 loss: 1.36616711e-06
Iter: 586 loss: 1.36180938e-06
Iter: 587 loss: 1.35957703e-06
Iter: 588 loss: 1.37750249e-06
Iter: 589 loss: 1.35949404e-06
Iter: 590 loss: 1.35760035e-06
Iter: 591 loss: 1.35953246e-06
Iter: 592 loss: 1.35660378e-06
Iter: 593 loss: 1.3544884e-06
Iter: 594 loss: 1.36706637e-06
Iter: 595 loss: 1.3542483e-06
Iter: 596 loss: 1.35244113e-06
Iter: 597 loss: 1.35359505e-06
Iter: 598 loss: 1.35132859e-06
Iter: 599 loss: 1.34925028e-06
Iter: 600 loss: 1.34677862e-06
Iter: 601 loss: 1.34653874e-06
Iter: 602 loss: 1.34405354e-06
Iter: 603 loss: 1.34407765e-06
Iter: 604 loss: 1.34212337e-06
Iter: 605 loss: 1.34034008e-06
Iter: 606 loss: 1.33978301e-06
Iter: 607 loss: 1.33693e-06
Iter: 608 loss: 1.35849882e-06
Iter: 609 loss: 1.33669528e-06
Iter: 610 loss: 1.33468598e-06
Iter: 611 loss: 1.35494906e-06
Iter: 612 loss: 1.33454171e-06
Iter: 613 loss: 1.33303593e-06
Iter: 614 loss: 1.33095705e-06
Iter: 615 loss: 1.33080812e-06
Iter: 616 loss: 1.32739683e-06
Iter: 617 loss: 1.3398128e-06
Iter: 618 loss: 1.32655623e-06
Iter: 619 loss: 1.32448099e-06
Iter: 620 loss: 1.34797972e-06
Iter: 621 loss: 1.32433752e-06
Iter: 622 loss: 1.322755e-06
Iter: 623 loss: 1.32252671e-06
Iter: 624 loss: 1.32132675e-06
Iter: 625 loss: 1.31863499e-06
Iter: 626 loss: 1.33128674e-06
Iter: 627 loss: 1.31815875e-06
Iter: 628 loss: 1.31621687e-06
Iter: 629 loss: 1.32686e-06
Iter: 630 loss: 1.31582919e-06
Iter: 631 loss: 1.31433308e-06
Iter: 632 loss: 1.3160244e-06
Iter: 633 loss: 1.3134412e-06
Iter: 634 loss: 1.31131389e-06
Iter: 635 loss: 1.31169293e-06
Iter: 636 loss: 1.30978958e-06
Iter: 637 loss: 1.30722538e-06
Iter: 638 loss: 1.30965918e-06
Iter: 639 loss: 1.3057703e-06
Iter: 640 loss: 1.3031754e-06
Iter: 641 loss: 1.3127086e-06
Iter: 642 loss: 1.30252943e-06
Iter: 643 loss: 1.29981504e-06
Iter: 644 loss: 1.31151614e-06
Iter: 645 loss: 1.29925593e-06
Iter: 646 loss: 1.29702744e-06
Iter: 647 loss: 1.30999319e-06
Iter: 648 loss: 1.2966259e-06
Iter: 649 loss: 1.29493367e-06
Iter: 650 loss: 1.30132696e-06
Iter: 651 loss: 1.29450132e-06
Iter: 652 loss: 1.29257296e-06
Iter: 653 loss: 1.29206501e-06
Iter: 654 loss: 1.2909295e-06
Iter: 655 loss: 1.28865383e-06
Iter: 656 loss: 1.29850059e-06
Iter: 657 loss: 1.28808824e-06
Iter: 658 loss: 1.2862414e-06
Iter: 659 loss: 1.30279454e-06
Iter: 660 loss: 1.28611441e-06
Iter: 661 loss: 1.28450324e-06
Iter: 662 loss: 1.28611873e-06
Iter: 663 loss: 1.28369561e-06
Iter: 664 loss: 1.28215208e-06
Iter: 665 loss: 1.29551177e-06
Iter: 666 loss: 1.28205147e-06
Iter: 667 loss: 1.28074532e-06
Iter: 668 loss: 1.27931696e-06
Iter: 669 loss: 1.27916883e-06
Iter: 670 loss: 1.27700082e-06
Iter: 671 loss: 1.29115176e-06
Iter: 672 loss: 1.27675719e-06
Iter: 673 loss: 1.27507633e-06
Iter: 674 loss: 1.27316321e-06
Iter: 675 loss: 1.27290309e-06
Iter: 676 loss: 1.27050771e-06
Iter: 677 loss: 1.28425802e-06
Iter: 678 loss: 1.27025521e-06
Iter: 679 loss: 1.26782675e-06
Iter: 680 loss: 1.2711123e-06
Iter: 681 loss: 1.26669238e-06
Iter: 682 loss: 1.26427244e-06
Iter: 683 loss: 1.2788604e-06
Iter: 684 loss: 1.26400505e-06
Iter: 685 loss: 1.26222812e-06
Iter: 686 loss: 1.27647718e-06
Iter: 687 loss: 1.2620643e-06
Iter: 688 loss: 1.26071507e-06
Iter: 689 loss: 1.26096108e-06
Iter: 690 loss: 1.25966324e-06
Iter: 691 loss: 1.25773511e-06
Iter: 692 loss: 1.26051532e-06
Iter: 693 loss: 1.25672648e-06
Iter: 694 loss: 1.25441898e-06
Iter: 695 loss: 1.26439625e-06
Iter: 696 loss: 1.2540022e-06
Iter: 697 loss: 1.25248334e-06
Iter: 698 loss: 1.2711314e-06
Iter: 699 loss: 1.25249107e-06
Iter: 700 loss: 1.25128463e-06
Iter: 701 loss: 1.25041652e-06
Iter: 702 loss: 1.25004203e-06
Iter: 703 loss: 1.24789972e-06
Iter: 704 loss: 1.25900624e-06
Iter: 705 loss: 1.24755422e-06
Iter: 706 loss: 1.24623614e-06
Iter: 707 loss: 1.24861072e-06
Iter: 708 loss: 1.24573512e-06
Iter: 709 loss: 1.24401367e-06
Iter: 710 loss: 1.2434e-06
Iter: 711 loss: 1.24250175e-06
Iter: 712 loss: 1.2402777e-06
Iter: 713 loss: 1.25779434e-06
Iter: 714 loss: 1.24005851e-06
Iter: 715 loss: 1.23824259e-06
Iter: 716 loss: 1.23678433e-06
Iter: 717 loss: 1.23630412e-06
Iter: 718 loss: 1.23385144e-06
Iter: 719 loss: 1.24793962e-06
Iter: 720 loss: 1.23348468e-06
Iter: 721 loss: 1.23146265e-06
Iter: 722 loss: 1.23800032e-06
Iter: 723 loss: 1.23093764e-06
Iter: 724 loss: 1.22865026e-06
Iter: 725 loss: 1.24257531e-06
Iter: 726 loss: 1.22834e-06
Iter: 727 loss: 1.22702511e-06
Iter: 728 loss: 1.22732661e-06
Iter: 729 loss: 1.22605275e-06
Iter: 730 loss: 1.22446e-06
Iter: 731 loss: 1.23093105e-06
Iter: 732 loss: 1.22406982e-06
Iter: 733 loss: 1.22242045e-06
Iter: 734 loss: 1.2306117e-06
Iter: 735 loss: 1.2221808e-06
Iter: 736 loss: 1.22072231e-06
Iter: 737 loss: 1.22547135e-06
Iter: 738 loss: 1.22033407e-06
Iter: 739 loss: 1.21881817e-06
Iter: 740 loss: 1.21990831e-06
Iter: 741 loss: 1.21802054e-06
Iter: 742 loss: 1.2162277e-06
Iter: 743 loss: 1.22325696e-06
Iter: 744 loss: 1.21577318e-06
Iter: 745 loss: 1.21430583e-06
Iter: 746 loss: 1.21446533e-06
Iter: 747 loss: 1.2131236e-06
Iter: 748 loss: 1.21130779e-06
Iter: 749 loss: 1.21958033e-06
Iter: 750 loss: 1.21092762e-06
Iter: 751 loss: 1.20913455e-06
Iter: 752 loss: 1.21195649e-06
Iter: 753 loss: 1.20824427e-06
Iter: 754 loss: 1.20645609e-06
Iter: 755 loss: 1.21026414e-06
Iter: 756 loss: 1.20570689e-06
Iter: 757 loss: 1.20362802e-06
Iter: 758 loss: 1.20824711e-06
Iter: 759 loss: 1.20292646e-06
Iter: 760 loss: 1.20114566e-06
Iter: 761 loss: 1.21179085e-06
Iter: 762 loss: 1.200886e-06
Iter: 763 loss: 1.19927233e-06
Iter: 764 loss: 1.20763707e-06
Iter: 765 loss: 1.19905826e-06
Iter: 766 loss: 1.1978193e-06
Iter: 767 loss: 1.19711046e-06
Iter: 768 loss: 1.19662309e-06
Iter: 769 loss: 1.19473316e-06
Iter: 770 loss: 1.20232698e-06
Iter: 771 loss: 1.19438994e-06
Iter: 772 loss: 1.19311176e-06
Iter: 773 loss: 1.19308243e-06
Iter: 774 loss: 1.19222625e-06
Iter: 775 loss: 1.19106107e-06
Iter: 776 loss: 1.19100991e-06
Iter: 777 loss: 1.18937271e-06
Iter: 778 loss: 1.20027221e-06
Iter: 779 loss: 1.18911703e-06
Iter: 780 loss: 1.1879207e-06
Iter: 781 loss: 1.18830485e-06
Iter: 782 loss: 1.18701428e-06
Iter: 783 loss: 1.18525691e-06
Iter: 784 loss: 1.18893377e-06
Iter: 785 loss: 1.1846605e-06
Iter: 786 loss: 1.18278263e-06
Iter: 787 loss: 1.18631499e-06
Iter: 788 loss: 1.18204684e-06
Iter: 789 loss: 1.18035246e-06
Iter: 790 loss: 1.1887181e-06
Iter: 791 loss: 1.18005869e-06
Iter: 792 loss: 1.17863874e-06
Iter: 793 loss: 1.18304342e-06
Iter: 794 loss: 1.1783327e-06
Iter: 795 loss: 1.17679156e-06
Iter: 796 loss: 1.1756274e-06
Iter: 797 loss: 1.1752287e-06
Iter: 798 loss: 1.1736206e-06
Iter: 799 loss: 1.17358854e-06
Iter: 800 loss: 1.17230695e-06
Iter: 801 loss: 1.17511195e-06
Iter: 802 loss: 1.17191917e-06
Iter: 803 loss: 1.17057448e-06
Iter: 804 loss: 1.17042532e-06
Iter: 805 loss: 1.16947831e-06
Iter: 806 loss: 1.1680047e-06
Iter: 807 loss: 1.18240519e-06
Iter: 808 loss: 1.16795229e-06
Iter: 809 loss: 1.16677779e-06
Iter: 810 loss: 1.17138802e-06
Iter: 811 loss: 1.16638398e-06
Iter: 812 loss: 1.16522324e-06
Iter: 813 loss: 1.16438537e-06
Iter: 814 loss: 1.16399883e-06
Iter: 815 loss: 1.16257866e-06
Iter: 816 loss: 1.180867e-06
Iter: 817 loss: 1.16259514e-06
Iter: 818 loss: 1.1616321e-06
Iter: 819 loss: 1.16118827e-06
Iter: 820 loss: 1.16073761e-06
Iter: 821 loss: 1.15912349e-06
Iter: 822 loss: 1.16026649e-06
Iter: 823 loss: 1.15816954e-06
Iter: 824 loss: 1.15624312e-06
Iter: 825 loss: 1.17057255e-06
Iter: 826 loss: 1.1561383e-06
Iter: 827 loss: 1.1550128e-06
Iter: 828 loss: 1.15778835e-06
Iter: 829 loss: 1.15460273e-06
Iter: 830 loss: 1.15312105e-06
Iter: 831 loss: 1.1526422e-06
Iter: 832 loss: 1.15187731e-06
Iter: 833 loss: 1.14987984e-06
Iter: 834 loss: 1.16335593e-06
Iter: 835 loss: 1.14976388e-06
Iter: 836 loss: 1.1484542e-06
Iter: 837 loss: 1.15590092e-06
Iter: 838 loss: 1.14826139e-06
Iter: 839 loss: 1.14715033e-06
Iter: 840 loss: 1.15070111e-06
Iter: 841 loss: 1.14682916e-06
Iter: 842 loss: 1.14568468e-06
Iter: 843 loss: 1.14691568e-06
Iter: 844 loss: 1.14500244e-06
Iter: 845 loss: 1.14383579e-06
Iter: 846 loss: 1.14636566e-06
Iter: 847 loss: 1.1433392e-06
Iter: 848 loss: 1.14173542e-06
Iter: 849 loss: 1.14935881e-06
Iter: 850 loss: 1.14134855e-06
Iter: 851 loss: 1.1405109e-06
Iter: 852 loss: 1.13958447e-06
Iter: 853 loss: 1.13948147e-06
Iter: 854 loss: 1.1379683e-06
Iter: 855 loss: 1.15160162e-06
Iter: 856 loss: 1.13793385e-06
Iter: 857 loss: 1.13668057e-06
Iter: 858 loss: 1.1366609e-06
Iter: 859 loss: 1.1357082e-06
Iter: 860 loss: 1.13437841e-06
Iter: 861 loss: 1.13610315e-06
Iter: 862 loss: 1.13378019e-06
Iter: 863 loss: 1.13192482e-06
Iter: 864 loss: 1.13646604e-06
Iter: 865 loss: 1.13125247e-06
Iter: 866 loss: 1.12977284e-06
Iter: 867 loss: 1.1398439e-06
Iter: 868 loss: 1.12958855e-06
Iter: 869 loss: 1.12843259e-06
Iter: 870 loss: 1.12935709e-06
Iter: 871 loss: 1.12775592e-06
Iter: 872 loss: 1.1264458e-06
Iter: 873 loss: 1.13096166e-06
Iter: 874 loss: 1.12604209e-06
Iter: 875 loss: 1.12480279e-06
Iter: 876 loss: 1.13123406e-06
Iter: 877 loss: 1.12454563e-06
Iter: 878 loss: 1.12330019e-06
Iter: 879 loss: 1.12646876e-06
Iter: 880 loss: 1.12285215e-06
Iter: 881 loss: 1.12183147e-06
Iter: 882 loss: 1.1236782e-06
Iter: 883 loss: 1.12133807e-06
Iter: 884 loss: 1.11999191e-06
Iter: 885 loss: 1.12683119e-06
Iter: 886 loss: 1.11981512e-06
Iter: 887 loss: 1.11855866e-06
Iter: 888 loss: 1.12088594e-06
Iter: 889 loss: 1.11811653e-06
Iter: 890 loss: 1.11712711e-06
Iter: 891 loss: 1.11684926e-06
Iter: 892 loss: 1.11617953e-06
Iter: 893 loss: 1.1149242e-06
Iter: 894 loss: 1.12623275e-06
Iter: 895 loss: 1.11487441e-06
Iter: 896 loss: 1.11352676e-06
Iter: 897 loss: 1.11276233e-06
Iter: 898 loss: 1.11217867e-06
Iter: 899 loss: 1.11091185e-06
Iter: 900 loss: 1.11328063e-06
Iter: 901 loss: 1.11030818e-06
Iter: 902 loss: 1.10871406e-06
Iter: 903 loss: 1.11609631e-06
Iter: 904 loss: 1.10847327e-06
Iter: 905 loss: 1.10699386e-06
Iter: 906 loss: 1.11304564e-06
Iter: 907 loss: 1.10664416e-06
Iter: 908 loss: 1.10547626e-06
Iter: 909 loss: 1.10761539e-06
Iter: 910 loss: 1.10485985e-06
Iter: 911 loss: 1.10357792e-06
Iter: 912 loss: 1.10693827e-06
Iter: 913 loss: 1.10319593e-06
Iter: 914 loss: 1.10194105e-06
Iter: 915 loss: 1.1125843e-06
Iter: 916 loss: 1.10188353e-06
Iter: 917 loss: 1.10098131e-06
Iter: 918 loss: 1.1019772e-06
Iter: 919 loss: 1.10047768e-06
Iter: 920 loss: 1.09938196e-06
Iter: 921 loss: 1.10218139e-06
Iter: 922 loss: 1.09896769e-06
Iter: 923 loss: 1.09761118e-06
Iter: 924 loss: 1.10376027e-06
Iter: 925 loss: 1.09737573e-06
Iter: 926 loss: 1.0965241e-06
Iter: 927 loss: 1.09778898e-06
Iter: 928 loss: 1.0960739e-06
Iter: 929 loss: 1.09504867e-06
Iter: 930 loss: 1.09438406e-06
Iter: 931 loss: 1.09402072e-06
Iter: 932 loss: 1.09255723e-06
Iter: 933 loss: 1.10813789e-06
Iter: 934 loss: 1.09254506e-06
Iter: 935 loss: 1.09145151e-06
Iter: 936 loss: 1.09360212e-06
Iter: 937 loss: 1.09105588e-06
Iter: 938 loss: 1.08997199e-06
Iter: 939 loss: 1.08976769e-06
Iter: 940 loss: 1.08901816e-06
Iter: 941 loss: 1.08781467e-06
Iter: 942 loss: 1.09460962e-06
Iter: 943 loss: 1.08762833e-06
Iter: 944 loss: 1.08631843e-06
Iter: 945 loss: 1.0864087e-06
Iter: 946 loss: 1.08532322e-06
Iter: 947 loss: 1.08418703e-06
Iter: 948 loss: 1.08418453e-06
Iter: 949 loss: 1.08347888e-06
Iter: 950 loss: 1.08275367e-06
Iter: 951 loss: 1.08253346e-06
Iter: 952 loss: 1.08135612e-06
Iter: 953 loss: 1.09357381e-06
Iter: 954 loss: 1.08130985e-06
Iter: 955 loss: 1.08022277e-06
Iter: 956 loss: 1.08032123e-06
Iter: 957 loss: 1.07942947e-06
Iter: 958 loss: 1.07846063e-06
Iter: 959 loss: 1.09279745e-06
Iter: 960 loss: 1.07848678e-06
Iter: 961 loss: 1.0776721e-06
Iter: 962 loss: 1.07720473e-06
Iter: 963 loss: 1.07684457e-06
Iter: 964 loss: 1.07572328e-06
Iter: 965 loss: 1.07637584e-06
Iter: 966 loss: 1.074951e-06
Iter: 967 loss: 1.07386518e-06
Iter: 968 loss: 1.08387701e-06
Iter: 969 loss: 1.07380947e-06
Iter: 970 loss: 1.07274e-06
Iter: 971 loss: 1.0739675e-06
Iter: 972 loss: 1.07213589e-06
Iter: 973 loss: 1.0710894e-06
Iter: 974 loss: 1.07674794e-06
Iter: 975 loss: 1.07087612e-06
Iter: 976 loss: 1.06994571e-06
Iter: 977 loss: 1.0689572e-06
Iter: 978 loss: 1.06879406e-06
Iter: 979 loss: 1.06724838e-06
Iter: 980 loss: 1.07312371e-06
Iter: 981 loss: 1.06682364e-06
Iter: 982 loss: 1.06570155e-06
Iter: 983 loss: 1.06928314e-06
Iter: 984 loss: 1.06538948e-06
Iter: 985 loss: 1.06406526e-06
Iter: 986 loss: 1.07170194e-06
Iter: 987 loss: 1.06389962e-06
Iter: 988 loss: 1.06307266e-06
Iter: 989 loss: 1.06529967e-06
Iter: 990 loss: 1.0627034e-06
Iter: 991 loss: 1.06166885e-06
Iter: 992 loss: 1.06551215e-06
Iter: 993 loss: 1.06147991e-06
Iter: 994 loss: 1.06040125e-06
Iter: 995 loss: 1.06108928e-06
Iter: 996 loss: 1.05974141e-06
Iter: 997 loss: 1.05849358e-06
Iter: 998 loss: 1.06815833e-06
Iter: 999 loss: 1.05842446e-06
Iter: 1000 loss: 1.05766355e-06
Iter: 1001 loss: 1.05734455e-06
Iter: 1002 loss: 1.05690924e-06
Iter: 1003 loss: 1.05590334e-06
Iter: 1004 loss: 1.05736535e-06
Iter: 1005 loss: 1.05539084e-06
Iter: 1006 loss: 1.05413937e-06
Iter: 1007 loss: 1.05961749e-06
Iter: 1008 loss: 1.05391268e-06
Iter: 1009 loss: 1.05297693e-06
Iter: 1010 loss: 1.06084985e-06
Iter: 1011 loss: 1.0529443e-06
Iter: 1012 loss: 1.05212371e-06
Iter: 1013 loss: 1.05138042e-06
Iter: 1014 loss: 1.05117022e-06
Iter: 1015 loss: 1.04989954e-06
Iter: 1016 loss: 1.05455842e-06
Iter: 1017 loss: 1.04955052e-06
Iter: 1018 loss: 1.048493e-06
Iter: 1019 loss: 1.05268145e-06
Iter: 1020 loss: 1.04829303e-06
Iter: 1021 loss: 1.04712944e-06
Iter: 1022 loss: 1.04723108e-06
Iter: 1023 loss: 1.04627372e-06
Iter: 1024 loss: 1.04511594e-06
Iter: 1025 loss: 1.05904269e-06
Iter: 1026 loss: 1.04505102e-06
Iter: 1027 loss: 1.04425328e-06
Iter: 1028 loss: 1.04814558e-06
Iter: 1029 loss: 1.04403102e-06
Iter: 1030 loss: 1.0432741e-06
Iter: 1031 loss: 1.04439357e-06
Iter: 1032 loss: 1.04281196e-06
Iter: 1033 loss: 1.04190724e-06
Iter: 1034 loss: 1.04580909e-06
Iter: 1035 loss: 1.04166861e-06
Iter: 1036 loss: 1.04082528e-06
Iter: 1037 loss: 1.04234687e-06
Iter: 1038 loss: 1.04047899e-06
Iter: 1039 loss: 1.03962407e-06
Iter: 1040 loss: 1.04104765e-06
Iter: 1041 loss: 1.03926777e-06
Iter: 1042 loss: 1.03823686e-06
Iter: 1043 loss: 1.03873322e-06
Iter: 1044 loss: 1.03762136e-06
Iter: 1045 loss: 1.03635625e-06
Iter: 1046 loss: 1.03789012e-06
Iter: 1047 loss: 1.03569198e-06
Iter: 1048 loss: 1.03483831e-06
Iter: 1049 loss: 1.03482478e-06
Iter: 1050 loss: 1.03403488e-06
Iter: 1051 loss: 1.03312959e-06
Iter: 1052 loss: 1.03305069e-06
Iter: 1053 loss: 1.03189427e-06
Iter: 1054 loss: 1.03487287e-06
Iter: 1055 loss: 1.03155321e-06
Iter: 1056 loss: 1.03036859e-06
Iter: 1057 loss: 1.0360875e-06
Iter: 1058 loss: 1.03014418e-06
Iter: 1059 loss: 1.02922434e-06
Iter: 1060 loss: 1.03209277e-06
Iter: 1061 loss: 1.02883041e-06
Iter: 1062 loss: 1.0278693e-06
Iter: 1063 loss: 1.02955642e-06
Iter: 1064 loss: 1.02742729e-06
Iter: 1065 loss: 1.02633953e-06
Iter: 1066 loss: 1.03812567e-06
Iter: 1067 loss: 1.02632498e-06
Iter: 1068 loss: 1.02557215e-06
Iter: 1069 loss: 1.02582862e-06
Iter: 1070 loss: 1.0250817e-06
Iter: 1071 loss: 1.02412059e-06
Iter: 1072 loss: 1.02929232e-06
Iter: 1073 loss: 1.02394142e-06
Iter: 1074 loss: 1.02315926e-06
Iter: 1075 loss: 1.02322429e-06
Iter: 1076 loss: 1.02248532e-06
Iter: 1077 loss: 1.02142849e-06
Iter: 1078 loss: 1.02526405e-06
Iter: 1079 loss: 1.02107902e-06
Iter: 1080 loss: 1.02021397e-06
Iter: 1081 loss: 1.02266813e-06
Iter: 1082 loss: 1.01996488e-06
Iter: 1083 loss: 1.0190206e-06
Iter: 1084 loss: 1.01941839e-06
Iter: 1085 loss: 1.01850037e-06
Iter: 1086 loss: 1.01740432e-06
Iter: 1087 loss: 1.02452896e-06
Iter: 1088 loss: 1.0172223e-06
Iter: 1089 loss: 1.01635396e-06
Iter: 1090 loss: 1.02113427e-06
Iter: 1091 loss: 1.01621504e-06
Iter: 1092 loss: 1.01552553e-06
Iter: 1093 loss: 1.01492128e-06
Iter: 1094 loss: 1.01468447e-06
Iter: 1095 loss: 1.01330261e-06
Iter: 1096 loss: 1.01478383e-06
Iter: 1097 loss: 1.01258e-06
Iter: 1098 loss: 1.01126898e-06
Iter: 1099 loss: 1.01664364e-06
Iter: 1100 loss: 1.01107537e-06
Iter: 1101 loss: 1.01018259e-06
Iter: 1102 loss: 1.02385889e-06
Iter: 1103 loss: 1.01012188e-06
Iter: 1104 loss: 1.00942202e-06
Iter: 1105 loss: 1.01135242e-06
Iter: 1106 loss: 1.00922921e-06
Iter: 1107 loss: 1.00846069e-06
Iter: 1108 loss: 1.00945908e-06
Iter: 1109 loss: 1.00809086e-06
Iter: 1110 loss: 1.00719444e-06
Iter: 1111 loss: 1.008754e-06
Iter: 1112 loss: 1.00682769e-06
Iter: 1113 loss: 1.00585589e-06
Iter: 1114 loss: 1.00961915e-06
Iter: 1115 loss: 1.00561851e-06
Iter: 1116 loss: 1.0048966e-06
Iter: 1117 loss: 1.0045901e-06
Iter: 1118 loss: 1.00429929e-06
Iter: 1119 loss: 1.00327043e-06
Iter: 1120 loss: 1.01121543e-06
Iter: 1121 loss: 1.00312809e-06
Iter: 1122 loss: 1.00229545e-06
Iter: 1123 loss: 1.00144734e-06
Iter: 1124 loss: 1.00124714e-06
Iter: 1125 loss: 1.00030047e-06
Iter: 1126 loss: 1.00030763e-06
Iter: 1127 loss: 9.99550252e-07
Iter: 1128 loss: 9.99721124e-07
Iter: 1129 loss: 9.98961468e-07
Iter: 1130 loss: 9.97975803e-07
Iter: 1131 loss: 9.99045255e-07
Iter: 1132 loss: 9.97438519e-07
Iter: 1133 loss: 9.96358e-07
Iter: 1134 loss: 9.99682e-07
Iter: 1135 loss: 9.96047902e-07
Iter: 1136 loss: 9.95044388e-07
Iter: 1137 loss: 9.97711368e-07
Iter: 1138 loss: 9.94730499e-07
Iter: 1139 loss: 9.93954245e-07
Iter: 1140 loss: 1.0038442e-06
Iter: 1141 loss: 9.93951403e-07
Iter: 1142 loss: 9.93282811e-07
Iter: 1143 loss: 9.93924e-07
Iter: 1144 loss: 9.92882e-07
Iter: 1145 loss: 9.92206e-07
Iter: 1146 loss: 9.94341463e-07
Iter: 1147 loss: 9.92015e-07
Iter: 1148 loss: 9.91230309e-07
Iter: 1149 loss: 9.92031232e-07
Iter: 1150 loss: 9.90821e-07
Iter: 1151 loss: 9.89922114e-07
Iter: 1152 loss: 9.91699e-07
Iter: 1153 loss: 9.89509658e-07
Iter: 1154 loss: 9.88664169e-07
Iter: 1155 loss: 9.91036586e-07
Iter: 1156 loss: 9.88356533e-07
Iter: 1157 loss: 9.87401677e-07
Iter: 1158 loss: 9.87828457e-07
Iter: 1159 loss: 9.86806754e-07
Iter: 1160 loss: 9.85900101e-07
Iter: 1161 loss: 9.97079383e-07
Iter: 1162 loss: 9.85858151e-07
Iter: 1163 loss: 9.85208658e-07
Iter: 1164 loss: 9.85021e-07
Iter: 1165 loss: 9.84655799e-07
Iter: 1166 loss: 9.83552582e-07
Iter: 1167 loss: 9.89935756e-07
Iter: 1168 loss: 9.83433e-07
Iter: 1169 loss: 9.82655138e-07
Iter: 1170 loss: 9.83188784e-07
Iter: 1171 loss: 9.82162192e-07
Iter: 1172 loss: 9.81316362e-07
Iter: 1173 loss: 9.82977895e-07
Iter: 1174 loss: 9.80864343e-07
Iter: 1175 loss: 9.79919264e-07
Iter: 1176 loss: 9.8098576e-07
Iter: 1177 loss: 9.79439847e-07
Iter: 1178 loss: 9.78645e-07
Iter: 1179 loss: 9.78625167e-07
Iter: 1180 loss: 9.7782754e-07
Iter: 1181 loss: 9.78874368e-07
Iter: 1182 loss: 9.77481363e-07
Iter: 1183 loss: 9.76802312e-07
Iter: 1184 loss: 9.77992386e-07
Iter: 1185 loss: 9.76482852e-07
Iter: 1186 loss: 9.75655666e-07
Iter: 1187 loss: 9.79006e-07
Iter: 1188 loss: 9.75493094e-07
Iter: 1189 loss: 9.74772888e-07
Iter: 1190 loss: 9.74377144e-07
Iter: 1191 loss: 9.74104296e-07
Iter: 1192 loss: 9.7309146e-07
Iter: 1193 loss: 9.79221909e-07
Iter: 1194 loss: 9.72947191e-07
Iter: 1195 loss: 9.7203224e-07
Iter: 1196 loss: 9.73612828e-07
Iter: 1197 loss: 9.71600684e-07
Iter: 1198 loss: 9.70609676e-07
Iter: 1199 loss: 9.73239253e-07
Iter: 1200 loss: 9.70287374e-07
Iter: 1201 loss: 9.69522716e-07
Iter: 1202 loss: 9.77392e-07
Iter: 1203 loss: 9.6951851e-07
Iter: 1204 loss: 9.68897439e-07
Iter: 1205 loss: 9.68450763e-07
Iter: 1206 loss: 9.68284212e-07
Iter: 1207 loss: 9.67304686e-07
Iter: 1208 loss: 9.73887609e-07
Iter: 1209 loss: 9.67174628e-07
Iter: 1210 loss: 9.66528432e-07
Iter: 1211 loss: 9.66759671e-07
Iter: 1212 loss: 9.66124503e-07
Iter: 1213 loss: 9.65163e-07
Iter: 1214 loss: 9.64657374e-07
Iter: 1215 loss: 9.64183528e-07
Iter: 1216 loss: 9.63352e-07
Iter: 1217 loss: 9.63348612e-07
Iter: 1218 loss: 9.62721856e-07
Iter: 1219 loss: 9.66659741e-07
Iter: 1220 loss: 9.62628178e-07
Iter: 1221 loss: 9.61881142e-07
Iter: 1222 loss: 9.61068736e-07
Iter: 1223 loss: 9.60949819e-07
Iter: 1224 loss: 9.60146394e-07
Iter: 1225 loss: 9.71181521e-07
Iter: 1226 loss: 9.60146e-07
Iter: 1227 loss: 9.59541921e-07
Iter: 1228 loss: 9.60079205e-07
Iter: 1229 loss: 9.59223371e-07
Iter: 1230 loss: 9.5834082e-07
Iter: 1231 loss: 9.58410737e-07
Iter: 1232 loss: 9.57714747e-07
Iter: 1233 loss: 9.56769668e-07
Iter: 1234 loss: 9.58897431e-07
Iter: 1235 loss: 9.56445888e-07
Iter: 1236 loss: 9.55363e-07
Iter: 1237 loss: 9.62608397e-07
Iter: 1238 loss: 9.55254563e-07
Iter: 1239 loss: 9.54553229e-07
Iter: 1240 loss: 9.5576479e-07
Iter: 1241 loss: 9.54216603e-07
Iter: 1242 loss: 9.53488666e-07
Iter: 1243 loss: 9.57098223e-07
Iter: 1244 loss: 9.53324445e-07
Iter: 1245 loss: 9.52606229e-07
Iter: 1246 loss: 9.52995322e-07
Iter: 1247 loss: 9.52079859e-07
Iter: 1248 loss: 9.51198729e-07
Iter: 1249 loss: 9.53768e-07
Iter: 1250 loss: 9.50852041e-07
Iter: 1251 loss: 9.50129788e-07
Iter: 1252 loss: 9.51721177e-07
Iter: 1253 loss: 9.49840398e-07
Iter: 1254 loss: 9.48837567e-07
Iter: 1255 loss: 9.50443905e-07
Iter: 1256 loss: 9.48403567e-07
Iter: 1257 loss: 9.47376691e-07
Iter: 1258 loss: 9.51225502e-07
Iter: 1259 loss: 9.47138233e-07
Iter: 1260 loss: 9.46531372e-07
Iter: 1261 loss: 9.46510681e-07
Iter: 1262 loss: 9.46087766e-07
Iter: 1263 loss: 9.45328168e-07
Iter: 1264 loss: 9.45293039e-07
Iter: 1265 loss: 9.44675264e-07
Iter: 1266 loss: 9.50602839e-07
Iter: 1267 loss: 9.44631154e-07
Iter: 1268 loss: 9.43958298e-07
Iter: 1269 loss: 9.44635815e-07
Iter: 1270 loss: 9.43603482e-07
Iter: 1271 loss: 9.42876e-07
Iter: 1272 loss: 9.42270276e-07
Iter: 1273 loss: 9.42062854e-07
Iter: 1274 loss: 9.41080771e-07
Iter: 1275 loss: 9.50874437e-07
Iter: 1276 loss: 9.41103679e-07
Iter: 1277 loss: 9.40285304e-07
Iter: 1278 loss: 9.41981966e-07
Iter: 1279 loss: 9.39857614e-07
Iter: 1280 loss: 9.39017866e-07
Iter: 1281 loss: 9.41279382e-07
Iter: 1282 loss: 9.38716084e-07
Iter: 1283 loss: 9.38051471e-07
Iter: 1284 loss: 9.43837165e-07
Iter: 1285 loss: 9.38034532e-07
Iter: 1286 loss: 9.37454274e-07
Iter: 1287 loss: 9.36459685e-07
Iter: 1288 loss: 9.36454626e-07
Iter: 1289 loss: 9.35485104e-07
Iter: 1290 loss: 9.45122906e-07
Iter: 1291 loss: 9.35443666e-07
Iter: 1292 loss: 9.34749721e-07
Iter: 1293 loss: 9.36160291e-07
Iter: 1294 loss: 9.34457262e-07
Iter: 1295 loss: 9.33674471e-07
Iter: 1296 loss: 9.36445133e-07
Iter: 1297 loss: 9.33445449e-07
Iter: 1298 loss: 9.3258609e-07
Iter: 1299 loss: 9.37167e-07
Iter: 1300 loss: 9.32463365e-07
Iter: 1301 loss: 9.31754357e-07
Iter: 1302 loss: 9.31849e-07
Iter: 1303 loss: 9.31262434e-07
Iter: 1304 loss: 9.30558485e-07
Iter: 1305 loss: 9.33390197e-07
Iter: 1306 loss: 9.30397789e-07
Iter: 1307 loss: 9.29656494e-07
Iter: 1308 loss: 9.3235144e-07
Iter: 1309 loss: 9.2947e-07
Iter: 1310 loss: 9.28829e-07
Iter: 1311 loss: 9.28223812e-07
Iter: 1312 loss: 9.28155941e-07
Iter: 1313 loss: 9.27121789e-07
Iter: 1314 loss: 9.30870726e-07
Iter: 1315 loss: 9.26879807e-07
Iter: 1316 loss: 9.26154712e-07
Iter: 1317 loss: 9.35292405e-07
Iter: 1318 loss: 9.26182338e-07
Iter: 1319 loss: 9.25609527e-07
Iter: 1320 loss: 9.25409381e-07
Iter: 1321 loss: 9.25105724e-07
Iter: 1322 loss: 9.24180767e-07
Iter: 1323 loss: 9.27504288e-07
Iter: 1324 loss: 9.23961522e-07
Iter: 1325 loss: 9.23190214e-07
Iter: 1326 loss: 9.26262146e-07
Iter: 1327 loss: 9.23032701e-07
Iter: 1328 loss: 9.22425215e-07
Iter: 1329 loss: 9.22171807e-07
Iter: 1330 loss: 9.21786e-07
Iter: 1331 loss: 9.20867649e-07
Iter: 1332 loss: 9.24633241e-07
Iter: 1333 loss: 9.20664775e-07
Iter: 1334 loss: 9.19891e-07
Iter: 1335 loss: 9.29281e-07
Iter: 1336 loss: 9.1990205e-07
Iter: 1337 loss: 9.1932e-07
Iter: 1338 loss: 9.20511866e-07
Iter: 1339 loss: 9.19074637e-07
Iter: 1340 loss: 9.18486421e-07
Iter: 1341 loss: 9.17889452e-07
Iter: 1342 loss: 9.1786552e-07
Iter: 1343 loss: 9.16908789e-07
Iter: 1344 loss: 9.25753284e-07
Iter: 1345 loss: 9.16909471e-07
Iter: 1346 loss: 9.16301758e-07
Iter: 1347 loss: 9.17279237e-07
Iter: 1348 loss: 9.15955354e-07
Iter: 1349 loss: 9.15251064e-07
Iter: 1350 loss: 9.1631e-07
Iter: 1351 loss: 9.14988561e-07
Iter: 1352 loss: 9.14138354e-07
Iter: 1353 loss: 9.1395e-07
Iter: 1354 loss: 9.1336085e-07
Iter: 1355 loss: 9.12372343e-07
Iter: 1356 loss: 9.19529725e-07
Iter: 1357 loss: 9.12295548e-07
Iter: 1358 loss: 9.11565394e-07
Iter: 1359 loss: 9.184227e-07
Iter: 1360 loss: 9.11569202e-07
Iter: 1361 loss: 9.11050336e-07
Iter: 1362 loss: 9.10521635e-07
Iter: 1363 loss: 9.10410847e-07
Iter: 1364 loss: 9.09572464e-07
Iter: 1365 loss: 9.13585723e-07
Iter: 1366 loss: 9.0936112e-07
Iter: 1367 loss: 9.08558036e-07
Iter: 1368 loss: 9.11686641e-07
Iter: 1369 loss: 9.0836329e-07
Iter: 1370 loss: 9.07663889e-07
Iter: 1371 loss: 9.07985452e-07
Iter: 1372 loss: 9.07205e-07
Iter: 1373 loss: 9.06527305e-07
Iter: 1374 loss: 9.06549e-07
Iter: 1375 loss: 9.0601435e-07
Iter: 1376 loss: 9.05543402e-07
Iter: 1377 loss: 9.05459842e-07
Iter: 1378 loss: 9.04729177e-07
Iter: 1379 loss: 9.06010769e-07
Iter: 1380 loss: 9.04390788e-07
Iter: 1381 loss: 9.03615728e-07
Iter: 1382 loss: 9.0945241e-07
Iter: 1383 loss: 9.03564796e-07
Iter: 1384 loss: 9.02896829e-07
Iter: 1385 loss: 9.02991644e-07
Iter: 1386 loss: 9.02305e-07
Iter: 1387 loss: 9.01466876e-07
Iter: 1388 loss: 9.04568367e-07
Iter: 1389 loss: 9.0135e-07
Iter: 1390 loss: 9.00704435e-07
Iter: 1391 loss: 9.01835563e-07
Iter: 1392 loss: 9.00357918e-07
Iter: 1393 loss: 8.99456495e-07
Iter: 1394 loss: 8.99537099e-07
Iter: 1395 loss: 8.9878074e-07
Iter: 1396 loss: 8.98187182e-07
Iter: 1397 loss: 8.98149892e-07
Iter: 1398 loss: 8.97620282e-07
Iter: 1399 loss: 8.96970619e-07
Iter: 1400 loss: 8.96952656e-07
Iter: 1401 loss: 8.96157701e-07
Iter: 1402 loss: 9.00447e-07
Iter: 1403 loss: 8.96000643e-07
Iter: 1404 loss: 8.95356152e-07
Iter: 1405 loss: 8.98481971e-07
Iter: 1406 loss: 8.95217283e-07
Iter: 1407 loss: 8.94597292e-07
Iter: 1408 loss: 8.96339884e-07
Iter: 1409 loss: 8.94388506e-07
Iter: 1410 loss: 8.93742651e-07
Iter: 1411 loss: 8.98046778e-07
Iter: 1412 loss: 8.93685183e-07
Iter: 1413 loss: 8.93255503e-07
Iter: 1414 loss: 8.92792457e-07
Iter: 1415 loss: 8.92685648e-07
Iter: 1416 loss: 8.91978118e-07
Iter: 1417 loss: 8.94044092e-07
Iter: 1418 loss: 8.91719651e-07
Iter: 1419 loss: 8.90960564e-07
Iter: 1420 loss: 8.93780225e-07
Iter: 1421 loss: 8.90763545e-07
Iter: 1422 loss: 8.89890373e-07
Iter: 1423 loss: 8.92773755e-07
Iter: 1424 loss: 8.89681587e-07
Iter: 1425 loss: 8.89124408e-07
Iter: 1426 loss: 8.88649367e-07
Iter: 1427 loss: 8.88449165e-07
Iter: 1428 loss: 8.87482884e-07
Iter: 1429 loss: 8.90245758e-07
Iter: 1430 loss: 8.87209808e-07
Iter: 1431 loss: 8.86294572e-07
Iter: 1432 loss: 8.95151288e-07
Iter: 1433 loss: 8.86276723e-07
Iter: 1434 loss: 8.85636268e-07
Iter: 1435 loss: 8.87521594e-07
Iter: 1436 loss: 8.85488362e-07
Iter: 1437 loss: 8.84927715e-07
Iter: 1438 loss: 8.86096188e-07
Iter: 1439 loss: 8.84663223e-07
Iter: 1440 loss: 8.83929602e-07
Iter: 1441 loss: 8.83728831e-07
Iter: 1442 loss: 8.83318933e-07
Iter: 1443 loss: 8.82564734e-07
Iter: 1444 loss: 8.87531201e-07
Iter: 1445 loss: 8.82429617e-07
Iter: 1446 loss: 8.818281e-07
Iter: 1447 loss: 8.85805605e-07
Iter: 1448 loss: 8.81736071e-07
Iter: 1449 loss: 8.81091296e-07
Iter: 1450 loss: 8.82462928e-07
Iter: 1451 loss: 8.80816742e-07
Iter: 1452 loss: 8.80197604e-07
Iter: 1453 loss: 8.81572646e-07
Iter: 1454 loss: 8.79955337e-07
Iter: 1455 loss: 8.793603e-07
Iter: 1456 loss: 8.79960623e-07
Iter: 1457 loss: 8.7905903e-07
Iter: 1458 loss: 8.78306821e-07
Iter: 1459 loss: 8.78724848e-07
Iter: 1460 loss: 8.77866626e-07
Iter: 1461 loss: 8.77075365e-07
Iter: 1462 loss: 8.89507248e-07
Iter: 1463 loss: 8.77073262e-07
Iter: 1464 loss: 8.76698e-07
Iter: 1465 loss: 8.75740739e-07
Iter: 1466 loss: 8.90698061e-07
Iter: 1467 loss: 8.75747901e-07
Iter: 1468 loss: 8.74825901e-07
Iter: 1469 loss: 8.82946779e-07
Iter: 1470 loss: 8.74818966e-07
Iter: 1471 loss: 8.74151056e-07
Iter: 1472 loss: 8.75510068e-07
Iter: 1473 loss: 8.73892077e-07
Iter: 1474 loss: 8.73183751e-07
Iter: 1475 loss: 8.78917206e-07
Iter: 1476 loss: 8.73125146e-07
Iter: 1477 loss: 8.72657e-07
Iter: 1478 loss: 8.73376052e-07
Iter: 1479 loss: 8.72410965e-07
Iter: 1480 loss: 8.71816781e-07
Iter: 1481 loss: 8.72207238e-07
Iter: 1482 loss: 8.71466511e-07
Iter: 1483 loss: 8.7076063e-07
Iter: 1484 loss: 8.72799319e-07
Iter: 1485 loss: 8.70530812e-07
Iter: 1486 loss: 8.69976589e-07
Iter: 1487 loss: 8.75387684e-07
Iter: 1488 loss: 8.70031613e-07
Iter: 1489 loss: 8.69434643e-07
Iter: 1490 loss: 8.69547762e-07
Iter: 1491 loss: 8.69008545e-07
Iter: 1492 loss: 8.68365873e-07
Iter: 1493 loss: 8.70378926e-07
Iter: 1494 loss: 8.68173288e-07
Iter: 1495 loss: 8.67568247e-07
Iter: 1496 loss: 8.68761049e-07
Iter: 1497 loss: 8.67377366e-07
Iter: 1498 loss: 8.66714572e-07
Iter: 1499 loss: 8.67771519e-07
Iter: 1500 loss: 8.66439677e-07
Iter: 1501 loss: 8.65807351e-07
Iter: 1502 loss: 8.69637802e-07
Iter: 1503 loss: 8.65707875e-07
Iter: 1504 loss: 8.65152401e-07
Iter: 1505 loss: 8.66600431e-07
Iter: 1506 loss: 8.64942535e-07
Iter: 1507 loss: 8.64474146e-07
Iter: 1508 loss: 8.64076526e-07
Iter: 1509 loss: 8.63928904e-07
Iter: 1510 loss: 8.63105811e-07
Iter: 1511 loss: 8.65545246e-07
Iter: 1512 loss: 8.62882473e-07
Iter: 1513 loss: 8.62181196e-07
Iter: 1514 loss: 8.64476817e-07
Iter: 1515 loss: 8.61981221e-07
Iter: 1516 loss: 8.61366175e-07
Iter: 1517 loss: 8.68809366e-07
Iter: 1518 loss: 8.61390788e-07
Iter: 1519 loss: 8.60925468e-07
Iter: 1520 loss: 8.61093326e-07
Iter: 1521 loss: 8.60578155e-07
Iter: 1522 loss: 8.60079581e-07
Iter: 1523 loss: 8.60601631e-07
Iter: 1524 loss: 8.59781494e-07
Iter: 1525 loss: 8.59081865e-07
Iter: 1526 loss: 8.63053344e-07
Iter: 1527 loss: 8.59044803e-07
Iter: 1528 loss: 8.584733e-07
Iter: 1529 loss: 8.61652836e-07
Iter: 1530 loss: 8.58347903e-07
Iter: 1531 loss: 8.57862915e-07
Iter: 1532 loss: 8.57749114e-07
Iter: 1533 loss: 8.57400323e-07
Iter: 1534 loss: 8.56647716e-07
Iter: 1535 loss: 8.57866894e-07
Iter: 1536 loss: 8.56336669e-07
Iter: 1537 loss: 8.55736857e-07
Iter: 1538 loss: 8.59458794e-07
Iter: 1539 loss: 8.55675921e-07
Iter: 1540 loss: 8.55154894e-07
Iter: 1541 loss: 8.5609787e-07
Iter: 1542 loss: 8.54835832e-07
Iter: 1543 loss: 8.5428951e-07
Iter: 1544 loss: 8.5652e-07
Iter: 1545 loss: 8.541e-07
Iter: 1546 loss: 8.53521328e-07
Iter: 1547 loss: 8.54067707e-07
Iter: 1548 loss: 8.53231199e-07
Iter: 1549 loss: 8.5256579e-07
Iter: 1550 loss: 8.52829373e-07
Iter: 1551 loss: 8.52085748e-07
Iter: 1552 loss: 8.51342e-07
Iter: 1553 loss: 8.53669235e-07
Iter: 1554 loss: 8.51131404e-07
Iter: 1555 loss: 8.50467245e-07
Iter: 1556 loss: 8.57270095e-07
Iter: 1557 loss: 8.50429274e-07
Iter: 1558 loss: 8.49799562e-07
Iter: 1559 loss: 8.5008503e-07
Iter: 1560 loss: 8.49352091e-07
Iter: 1561 loss: 8.48694924e-07
Iter: 1562 loss: 8.51071775e-07
Iter: 1563 loss: 8.48522404e-07
Iter: 1564 loss: 8.48041e-07
Iter: 1565 loss: 8.50115953e-07
Iter: 1566 loss: 8.47941578e-07
Iter: 1567 loss: 8.47416345e-07
Iter: 1568 loss: 8.50511583e-07
Iter: 1569 loss: 8.47314254e-07
Iter: 1570 loss: 8.46885882e-07
Iter: 1571 loss: 8.46704609e-07
Iter: 1572 loss: 8.46520038e-07
Iter: 1573 loss: 8.45860939e-07
Iter: 1574 loss: 8.46765886e-07
Iter: 1575 loss: 8.45513796e-07
Iter: 1576 loss: 8.44695307e-07
Iter: 1577 loss: 8.48977663e-07
Iter: 1578 loss: 8.44598333e-07
Iter: 1579 loss: 8.4412909e-07
Iter: 1580 loss: 8.45842e-07
Iter: 1581 loss: 8.44037743e-07
Iter: 1582 loss: 8.43453449e-07
Iter: 1583 loss: 8.43992552e-07
Iter: 1584 loss: 8.43112105e-07
Iter: 1585 loss: 8.42484951e-07
Iter: 1586 loss: 8.43969701e-07
Iter: 1587 loss: 8.42333463e-07
Iter: 1588 loss: 8.417137e-07
Iter: 1589 loss: 8.42502459e-07
Iter: 1590 loss: 8.41466e-07
Iter: 1591 loss: 8.40810856e-07
Iter: 1592 loss: 8.43770579e-07
Iter: 1593 loss: 8.40705866e-07
Iter: 1594 loss: 8.40143116e-07
Iter: 1595 loss: 8.41072e-07
Iter: 1596 loss: 8.39898064e-07
Iter: 1597 loss: 8.39261929e-07
Iter: 1598 loss: 8.44018359e-07
Iter: 1599 loss: 8.39209e-07
Iter: 1600 loss: 8.38739879e-07
Iter: 1601 loss: 8.39009772e-07
Iter: 1602 loss: 8.38403139e-07
Iter: 1603 loss: 8.37876087e-07
Iter: 1604 loss: 8.38878918e-07
Iter: 1605 loss: 8.37635753e-07
Iter: 1606 loss: 8.3695744e-07
Iter: 1607 loss: 8.44042745e-07
Iter: 1608 loss: 8.36945901e-07
Iter: 1609 loss: 8.36631102e-07
Iter: 1610 loss: 8.36104562e-07
Iter: 1611 loss: 8.36072104e-07
Iter: 1612 loss: 8.3533223e-07
Iter: 1613 loss: 8.37880634e-07
Iter: 1614 loss: 8.35159426e-07
Iter: 1615 loss: 8.34496632e-07
Iter: 1616 loss: 8.38654444e-07
Iter: 1617 loss: 8.34431148e-07
Iter: 1618 loss: 8.33894887e-07
Iter: 1619 loss: 8.351426e-07
Iter: 1620 loss: 8.33675e-07
Iter: 1621 loss: 8.33155525e-07
Iter: 1622 loss: 8.34337129e-07
Iter: 1623 loss: 8.32888077e-07
Iter: 1624 loss: 8.32286617e-07
Iter: 1625 loss: 8.33514378e-07
Iter: 1626 loss: 8.32061e-07
Iter: 1627 loss: 8.31476711e-07
Iter: 1628 loss: 8.31582611e-07
Iter: 1629 loss: 8.31099896e-07
Iter: 1630 loss: 8.30343936e-07
Iter: 1631 loss: 8.34492766e-07
Iter: 1632 loss: 8.30201088e-07
Iter: 1633 loss: 8.29544149e-07
Iter: 1634 loss: 8.3206271e-07
Iter: 1635 loss: 8.29397322e-07
Iter: 1636 loss: 8.28810357e-07
Iter: 1637 loss: 8.31826526e-07
Iter: 1638 loss: 8.28701673e-07
Iter: 1639 loss: 8.28201792e-07
Iter: 1640 loss: 8.27998292e-07
Iter: 1641 loss: 8.2775307e-07
Iter: 1642 loss: 8.27220617e-07
Iter: 1643 loss: 8.27231077e-07
Iter: 1644 loss: 8.26942596e-07
Iter: 1645 loss: 8.27406325e-07
Iter: 1646 loss: 8.267e-07
Iter: 1647 loss: 8.26342273e-07
Iter: 1648 loss: 8.25641962e-07
Iter: 1649 loss: 8.25655e-07
Iter: 1650 loss: 8.24890321e-07
Iter: 1651 loss: 8.28910515e-07
Iter: 1652 loss: 8.24732297e-07
Iter: 1653 loss: 8.24190295e-07
Iter: 1654 loss: 8.27978909e-07
Iter: 1655 loss: 8.24161248e-07
Iter: 1656 loss: 8.23637379e-07
Iter: 1657 loss: 8.24402832e-07
Iter: 1658 loss: 8.23405514e-07
Iter: 1659 loss: 8.22791321e-07
Iter: 1660 loss: 8.23485436e-07
Iter: 1661 loss: 8.22474135e-07
Iter: 1662 loss: 8.21815377e-07
Iter: 1663 loss: 8.24802385e-07
Iter: 1664 loss: 8.21686228e-07
Iter: 1665 loss: 8.21131835e-07
Iter: 1666 loss: 8.22364427e-07
Iter: 1667 loss: 8.20937487e-07
Iter: 1668 loss: 8.20345065e-07
Iter: 1669 loss: 8.20641844e-07
Iter: 1670 loss: 8.2000264e-07
Iter: 1671 loss: 8.193594e-07
Iter: 1672 loss: 8.2223562e-07
Iter: 1673 loss: 8.19243041e-07
Iter: 1674 loss: 8.18676313e-07
Iter: 1675 loss: 8.23305072e-07
Iter: 1676 loss: 8.18627882e-07
Iter: 1677 loss: 8.18154604e-07
Iter: 1678 loss: 8.17946443e-07
Iter: 1679 loss: 8.17699e-07
Iter: 1680 loss: 8.17249543e-07
Iter: 1681 loss: 8.17219075e-07
Iter: 1682 loss: 8.16886768e-07
Iter: 1683 loss: 8.16546503e-07
Iter: 1684 loss: 8.16444413e-07
Iter: 1685 loss: 8.15853866e-07
Iter: 1686 loss: 8.16563443e-07
Iter: 1687 loss: 8.15527187e-07
Iter: 1688 loss: 8.14882753e-07
Iter: 1689 loss: 8.17367209e-07
Iter: 1690 loss: 8.14752582e-07
Iter: 1691 loss: 8.14167265e-07
Iter: 1692 loss: 8.17043485e-07
Iter: 1693 loss: 8.14062673e-07
Iter: 1694 loss: 8.1348594e-07
Iter: 1695 loss: 8.15368e-07
Iter: 1696 loss: 8.1338726e-07
Iter: 1697 loss: 8.1294337e-07
Iter: 1698 loss: 8.13320241e-07
Iter: 1699 loss: 8.12697749e-07
Iter: 1700 loss: 8.1209339e-07
Iter: 1701 loss: 8.13679662e-07
Iter: 1702 loss: 8.11940311e-07
Iter: 1703 loss: 8.11412804e-07
Iter: 1704 loss: 8.12774317e-07
Iter: 1705 loss: 8.11159737e-07
Iter: 1706 loss: 8.10654797e-07
Iter: 1707 loss: 8.12783e-07
Iter: 1708 loss: 8.10565666e-07
Iter: 1709 loss: 8.10082099e-07
Iter: 1710 loss: 8.10578399e-07
Iter: 1711 loss: 8.09815333e-07
Iter: 1712 loss: 8.09211429e-07
Iter: 1713 loss: 8.12479357e-07
Iter: 1714 loss: 8.09160611e-07
Iter: 1715 loss: 8.08593143e-07
Iter: 1716 loss: 8.10935603e-07
Iter: 1717 loss: 8.08446316e-07
Iter: 1718 loss: 8.08046536e-07
Iter: 1719 loss: 8.09133553e-07
Iter: 1720 loss: 8.07951778e-07
Iter: 1721 loss: 8.07402557e-07
Iter: 1722 loss: 8.08722461e-07
Iter: 1723 loss: 8.07252945e-07
Iter: 1724 loss: 8.0688892e-07
Iter: 1725 loss: 8.06713956e-07
Iter: 1726 loss: 8.06472428e-07
Iter: 1727 loss: 8.05896207e-07
Iter: 1728 loss: 8.05707487e-07
Iter: 1729 loss: 8.05378477e-07
Iter: 1730 loss: 8.04725062e-07
Iter: 1731 loss: 8.13583597e-07
Iter: 1732 loss: 8.04706247e-07
Iter: 1733 loss: 8.04207104e-07
Iter: 1734 loss: 8.06891592e-07
Iter: 1735 loss: 8.04131787e-07
Iter: 1736 loss: 8.03616e-07
Iter: 1737 loss: 8.03976036e-07
Iter: 1738 loss: 8.0334587e-07
Iter: 1739 loss: 8.02843601e-07
Iter: 1740 loss: 8.02788804e-07
Iter: 1741 loss: 8.0237794e-07
Iter: 1742 loss: 8.01648753e-07
Iter: 1743 loss: 8.07228048e-07
Iter: 1744 loss: 8.01638521e-07
Iter: 1745 loss: 8.01193892e-07
Iter: 1746 loss: 8.02963086e-07
Iter: 1747 loss: 8.01155e-07
Iter: 1748 loss: 8.00667124e-07
Iter: 1749 loss: 8.00974249e-07
Iter: 1750 loss: 8.00368241e-07
Iter: 1751 loss: 7.9979651e-07
Iter: 1752 loss: 8.0128774e-07
Iter: 1753 loss: 7.99610575e-07
Iter: 1754 loss: 7.99081249e-07
Iter: 1755 loss: 8.043869e-07
Iter: 1756 loss: 7.99076133e-07
Iter: 1757 loss: 7.98690735e-07
Iter: 1758 loss: 7.99118766e-07
Iter: 1759 loss: 7.98515885e-07
Iter: 1760 loss: 7.98022711e-07
Iter: 1761 loss: 7.99619102e-07
Iter: 1762 loss: 7.97890323e-07
Iter: 1763 loss: 7.97396524e-07
Iter: 1764 loss: 7.9729557e-07
Iter: 1765 loss: 7.96965594e-07
Iter: 1766 loss: 7.96411598e-07
Iter: 1767 loss: 7.9632116e-07
Iter: 1768 loss: 7.95893811e-07
Iter: 1769 loss: 7.95203732e-07
Iter: 1770 loss: 8.01204692e-07
Iter: 1771 loss: 7.95187e-07
Iter: 1772 loss: 7.9474421e-07
Iter: 1773 loss: 7.9931533e-07
Iter: 1774 loss: 7.94681057e-07
Iter: 1775 loss: 7.94297819e-07
Iter: 1776 loss: 7.94945777e-07
Iter: 1777 loss: 7.94118478e-07
Iter: 1778 loss: 7.93725235e-07
Iter: 1779 loss: 7.93947606e-07
Iter: 1780 loss: 7.93435e-07
Iter: 1781 loss: 7.92876335e-07
Iter: 1782 loss: 7.93410436e-07
Iter: 1783 loss: 7.92511457e-07
Iter: 1784 loss: 7.92006e-07
Iter: 1785 loss: 7.94781045e-07
Iter: 1786 loss: 7.91891466e-07
Iter: 1787 loss: 7.91359525e-07
Iter: 1788 loss: 7.92423634e-07
Iter: 1789 loss: 7.91112768e-07
Iter: 1790 loss: 7.90462082e-07
Iter: 1791 loss: 7.93574372e-07
Iter: 1792 loss: 7.90313777e-07
Iter: 1793 loss: 7.89839646e-07
Iter: 1794 loss: 7.91981108e-07
Iter: 1795 loss: 7.89729256e-07
Iter: 1796 loss: 7.89335104e-07
Iter: 1797 loss: 7.91620437e-07
Iter: 1798 loss: 7.89288663e-07
Iter: 1799 loss: 7.88963121e-07
Iter: 1800 loss: 7.88812883e-07
Iter: 1801 loss: 7.88568514e-07
Iter: 1802 loss: 7.88060674e-07
Iter: 1803 loss: 7.91006869e-07
Iter: 1804 loss: 7.87926581e-07
Iter: 1805 loss: 7.87547833e-07
Iter: 1806 loss: 7.87458816e-07
Iter: 1807 loss: 7.8719097e-07
Iter: 1808 loss: 7.86692112e-07
Iter: 1809 loss: 7.88351258e-07
Iter: 1810 loss: 7.8651874e-07
Iter: 1811 loss: 7.85947122e-07
Iter: 1812 loss: 7.857318e-07
Iter: 1813 loss: 7.8544673e-07
Iter: 1814 loss: 7.8507361e-07
Iter: 1815 loss: 7.84971235e-07
Iter: 1816 loss: 7.8463529e-07
Iter: 1817 loss: 7.84681788e-07
Iter: 1818 loss: 7.8435869e-07
Iter: 1819 loss: 7.83925032e-07
Iter: 1820 loss: 7.84179235e-07
Iter: 1821 loss: 7.83603923e-07
Iter: 1822 loss: 7.83026735e-07
Iter: 1823 loss: 7.8401456e-07
Iter: 1824 loss: 7.82722168e-07
Iter: 1825 loss: 7.82184657e-07
Iter: 1826 loss: 7.86238957e-07
Iter: 1827 loss: 7.82150323e-07
Iter: 1828 loss: 7.81661583e-07
Iter: 1829 loss: 7.81773338e-07
Iter: 1830 loss: 7.81316e-07
Iter: 1831 loss: 7.80759592e-07
Iter: 1832 loss: 7.86171483e-07
Iter: 1833 loss: 7.80757e-07
Iter: 1834 loss: 7.80322921e-07
Iter: 1835 loss: 7.83004e-07
Iter: 1836 loss: 7.80254e-07
Iter: 1837 loss: 7.79954632e-07
Iter: 1838 loss: 7.80153641e-07
Iter: 1839 loss: 7.79753e-07
Iter: 1840 loss: 7.7933305e-07
Iter: 1841 loss: 7.79037123e-07
Iter: 1842 loss: 7.78857725e-07
Iter: 1843 loss: 7.78346418e-07
Iter: 1844 loss: 7.81531753e-07
Iter: 1845 loss: 7.78297533e-07
Iter: 1846 loss: 7.77926402e-07
Iter: 1847 loss: 7.80730488e-07
Iter: 1848 loss: 7.77881496e-07
Iter: 1849 loss: 7.7753441e-07
Iter: 1850 loss: 7.77146511e-07
Iter: 1851 loss: 7.77048513e-07
Iter: 1852 loss: 7.76496449e-07
Iter: 1853 loss: 7.78146102e-07
Iter: 1854 loss: 7.76345587e-07
Iter: 1855 loss: 7.75795e-07
Iter: 1856 loss: 7.76035392e-07
Iter: 1857 loss: 7.75368449e-07
Iter: 1858 loss: 7.74819739e-07
Iter: 1859 loss: 7.79551101e-07
Iter: 1860 loss: 7.7479018e-07
Iter: 1861 loss: 7.74316163e-07
Iter: 1862 loss: 7.75703484e-07
Iter: 1863 loss: 7.74157e-07
Iter: 1864 loss: 7.73662578e-07
Iter: 1865 loss: 7.76157549e-07
Iter: 1866 loss: 7.73566853e-07
Iter: 1867 loss: 7.73216925e-07
Iter: 1868 loss: 7.73979e-07
Iter: 1869 loss: 7.73034e-07
Iter: 1870 loss: 7.726685e-07
Iter: 1871 loss: 7.73386319e-07
Iter: 1872 loss: 7.72484782e-07
Iter: 1873 loss: 7.7210052e-07
Iter: 1874 loss: 7.75035517e-07
Iter: 1875 loss: 7.72021735e-07
Iter: 1876 loss: 7.71673854e-07
Iter: 1877 loss: 7.71847283e-07
Iter: 1878 loss: 7.71441137e-07
Iter: 1879 loss: 7.71043517e-07
Iter: 1880 loss: 7.72137128e-07
Iter: 1881 loss: 7.70885663e-07
Iter: 1882 loss: 7.70502822e-07
Iter: 1883 loss: 7.7067358e-07
Iter: 1884 loss: 7.70219458e-07
Iter: 1885 loss: 7.69689336e-07
Iter: 1886 loss: 7.7123957e-07
Iter: 1887 loss: 7.69511303e-07
Iter: 1888 loss: 7.68944915e-07
Iter: 1889 loss: 7.6984918e-07
Iter: 1890 loss: 7.68667519e-07
Iter: 1891 loss: 7.68197424e-07
Iter: 1892 loss: 7.72413784e-07
Iter: 1893 loss: 7.68166501e-07
Iter: 1894 loss: 7.67666393e-07
Iter: 1895 loss: 7.68497216e-07
Iter: 1896 loss: 7.67444817e-07
Iter: 1897 loss: 7.67025767e-07
Iter: 1898 loss: 7.66979383e-07
Iter: 1899 loss: 7.66671405e-07
Iter: 1900 loss: 7.66094672e-07
Iter: 1901 loss: 7.67446409e-07
Iter: 1902 loss: 7.65938694e-07
Iter: 1903 loss: 7.65299717e-07
Iter: 1904 loss: 7.68516713e-07
Iter: 1905 loss: 7.65219283e-07
Iter: 1906 loss: 7.64761808e-07
Iter: 1907 loss: 7.6785193e-07
Iter: 1908 loss: 7.64741458e-07
Iter: 1909 loss: 7.643049e-07
Iter: 1910 loss: 7.65054097e-07
Iter: 1911 loss: 7.64147e-07
Iter: 1912 loss: 7.63741355e-07
Iter: 1913 loss: 7.65794141e-07
Iter: 1914 loss: 7.63661092e-07
Iter: 1915 loss: 7.63302296e-07
Iter: 1916 loss: 7.63815365e-07
Iter: 1917 loss: 7.63138416e-07
Iter: 1918 loss: 7.62614661e-07
Iter: 1919 loss: 7.63763751e-07
Iter: 1920 loss: 7.6251e-07
Iter: 1921 loss: 7.62166337e-07
Iter: 1922 loss: 7.62593345e-07
Iter: 1923 loss: 7.61954766e-07
Iter: 1924 loss: 7.61529122e-07
Iter: 1925 loss: 7.62891204e-07
Iter: 1926 loss: 7.61458693e-07
Iter: 1927 loss: 7.61021738e-07
Iter: 1928 loss: 7.61182605e-07
Iter: 1929 loss: 7.6074241e-07
Iter: 1930 loss: 7.60261059e-07
Iter: 1931 loss: 7.62169293e-07
Iter: 1932 loss: 7.60119121e-07
Iter: 1933 loss: 7.59773116e-07
Iter: 1934 loss: 7.60940907e-07
Iter: 1935 loss: 7.59634418e-07
Iter: 1936 loss: 7.59190812e-07
Iter: 1937 loss: 7.61655826e-07
Iter: 1938 loss: 7.59119644e-07
Iter: 1939 loss: 7.58714634e-07
Iter: 1940 loss: 7.58889371e-07
Iter: 1941 loss: 7.58476631e-07
Iter: 1942 loss: 7.58047804e-07
Iter: 1943 loss: 7.58670637e-07
Iter: 1944 loss: 7.57799398e-07
Iter: 1945 loss: 7.57270698e-07
Iter: 1946 loss: 7.58431952e-07
Iter: 1947 loss: 7.57096132e-07
Iter: 1948 loss: 7.56591078e-07
Iter: 1949 loss: 7.61625415e-07
Iter: 1950 loss: 7.56568056e-07
Iter: 1951 loss: 7.56152758e-07
Iter: 1952 loss: 7.57105681e-07
Iter: 1953 loss: 7.56044358e-07
Iter: 1954 loss: 7.5570631e-07
Iter: 1955 loss: 7.57380235e-07
Iter: 1956 loss: 7.556593e-07
Iter: 1957 loss: 7.5537082e-07
Iter: 1958 loss: 7.56048223e-07
Iter: 1959 loss: 7.55221436e-07
Iter: 1960 loss: 7.54923576e-07
Iter: 1961 loss: 7.54874463e-07
Iter: 1962 loss: 7.54656526e-07
Iter: 1963 loss: 7.54137886e-07
Iter: 1964 loss: 7.55376504e-07
Iter: 1965 loss: 7.53963263e-07
Iter: 1966 loss: 7.53444283e-07
Iter: 1967 loss: 7.54903112e-07
Iter: 1968 loss: 7.53306836e-07
Iter: 1969 loss: 7.52906885e-07
Iter: 1970 loss: 7.53508573e-07
Iter: 1971 loss: 7.5275625e-07
Iter: 1972 loss: 7.52195604e-07
Iter: 1973 loss: 7.5414755e-07
Iter: 1974 loss: 7.52074925e-07
Iter: 1975 loss: 7.51600169e-07
Iter: 1976 loss: 7.51700952e-07
Iter: 1977 loss: 7.51256152e-07
Iter: 1978 loss: 7.50911909e-07
Iter: 1979 loss: 7.50884851e-07
Iter: 1980 loss: 7.50639174e-07
Iter: 1981 loss: 7.50303911e-07
Iter: 1982 loss: 7.50292656e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script73
+ '[' -r STOP.script73 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi2.4
+ date
Sat Nov  7 15:41:01 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi2.4/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi2.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi2.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi2.4_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi2.4/500_500_500_500_1 --optimizer lbfgs --function f2 --psi 0 --alpha 2.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi2.4_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2040912378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2040908ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20409128c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20409129d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2040912ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20408aaa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2040816158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20408161e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f204085d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f204085dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20407da9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20407dab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2040791840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2040791b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2040739488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f204075c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20406de378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f204064c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f204067a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f204067a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20406b5e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2040611730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2040636950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2040636840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20405ed378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20405eb0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f204058c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f203c3b51e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f203c3b5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f203c3d12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f203c3b5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f203c337730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f203c337378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f203c3198c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f203c319d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f203c286d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.52831776e-05
Iter: 2 loss: 3.02286389e-05
Iter: 3 loss: 3.24615976e-05
Iter: 4 loss: 2.67632931e-05
Iter: 5 loss: 2.32254661e-05
Iter: 6 loss: 2.65327e-05
Iter: 7 loss: 2.11511051e-05
Iter: 8 loss: 1.76775648e-05
Iter: 9 loss: 4.02735168e-05
Iter: 10 loss: 1.73270164e-05
Iter: 11 loss: 1.52843368e-05
Iter: 12 loss: 3.29349677e-05
Iter: 13 loss: 1.51742934e-05
Iter: 14 loss: 1.39684644e-05
Iter: 15 loss: 1.72563596e-05
Iter: 16 loss: 1.35730515e-05
Iter: 17 loss: 1.24688895e-05
Iter: 18 loss: 1.24005701e-05
Iter: 19 loss: 1.15653011e-05
Iter: 20 loss: 1.0526599e-05
Iter: 21 loss: 1.85175049e-05
Iter: 22 loss: 1.04504379e-05
Iter: 23 loss: 9.79242122e-06
Iter: 24 loss: 1.26709501e-05
Iter: 25 loss: 9.65991057e-06
Iter: 26 loss: 9.11380084e-06
Iter: 27 loss: 1.05445415e-05
Iter: 28 loss: 8.92891e-06
Iter: 29 loss: 8.36623894e-06
Iter: 30 loss: 9.5058e-06
Iter: 31 loss: 8.13824954e-06
Iter: 32 loss: 7.65866571e-06
Iter: 33 loss: 8.76564445e-06
Iter: 34 loss: 7.48033563e-06
Iter: 35 loss: 7.0483411e-06
Iter: 36 loss: 7.9103047e-06
Iter: 37 loss: 6.8718291e-06
Iter: 38 loss: 6.43966177e-06
Iter: 39 loss: 8.17826276e-06
Iter: 40 loss: 6.34339858e-06
Iter: 41 loss: 5.98707538e-06
Iter: 42 loss: 7.00286273e-06
Iter: 43 loss: 5.87470595e-06
Iter: 44 loss: 5.59173e-06
Iter: 45 loss: 6.85015493e-06
Iter: 46 loss: 5.53591553e-06
Iter: 47 loss: 5.29185036e-06
Iter: 48 loss: 6.21392064e-06
Iter: 49 loss: 5.23338713e-06
Iter: 50 loss: 5.0539079e-06
Iter: 51 loss: 5.0538365e-06
Iter: 52 loss: 4.95138e-06
Iter: 53 loss: 5.08567155e-06
Iter: 54 loss: 4.89922286e-06
Iter: 55 loss: 4.76413243e-06
Iter: 56 loss: 5.08106905e-06
Iter: 57 loss: 4.7144058e-06
Iter: 58 loss: 4.60277897e-06
Iter: 59 loss: 4.66951815e-06
Iter: 60 loss: 4.53078428e-06
Iter: 61 loss: 4.3989985e-06
Iter: 62 loss: 5.0160138e-06
Iter: 63 loss: 4.37468316e-06
Iter: 64 loss: 4.25283633e-06
Iter: 65 loss: 4.34573576e-06
Iter: 66 loss: 4.17835963e-06
Iter: 67 loss: 4.06861363e-06
Iter: 68 loss: 5.07038567e-06
Iter: 69 loss: 4.06364552e-06
Iter: 70 loss: 3.97136455e-06
Iter: 71 loss: 4.10725761e-06
Iter: 72 loss: 3.92677339e-06
Iter: 73 loss: 3.83534825e-06
Iter: 74 loss: 3.95077268e-06
Iter: 75 loss: 3.78817276e-06
Iter: 76 loss: 3.69296754e-06
Iter: 77 loss: 4.10132543e-06
Iter: 78 loss: 3.67321059e-06
Iter: 79 loss: 3.58952866e-06
Iter: 80 loss: 3.72411864e-06
Iter: 81 loss: 3.551085e-06
Iter: 82 loss: 3.4617683e-06
Iter: 83 loss: 3.79249718e-06
Iter: 84 loss: 3.43988063e-06
Iter: 85 loss: 3.3661679e-06
Iter: 86 loss: 3.4731238e-06
Iter: 87 loss: 3.33030107e-06
Iter: 88 loss: 3.26309873e-06
Iter: 89 loss: 3.85875774e-06
Iter: 90 loss: 3.25994688e-06
Iter: 91 loss: 3.20103391e-06
Iter: 92 loss: 3.67340226e-06
Iter: 93 loss: 3.19734704e-06
Iter: 94 loss: 3.16633759e-06
Iter: 95 loss: 3.15346688e-06
Iter: 96 loss: 3.13717783e-06
Iter: 97 loss: 3.08311019e-06
Iter: 98 loss: 3.26248255e-06
Iter: 99 loss: 3.06830134e-06
Iter: 100 loss: 3.01642785e-06
Iter: 101 loss: 3.03541628e-06
Iter: 102 loss: 2.98019654e-06
Iter: 103 loss: 2.93533e-06
Iter: 104 loss: 3.15399711e-06
Iter: 105 loss: 2.9275368e-06
Iter: 106 loss: 2.87718558e-06
Iter: 107 loss: 2.97571637e-06
Iter: 108 loss: 2.85617489e-06
Iter: 109 loss: 2.81635857e-06
Iter: 110 loss: 3.11119e-06
Iter: 111 loss: 2.81304892e-06
Iter: 112 loss: 2.78225752e-06
Iter: 113 loss: 2.80403606e-06
Iter: 114 loss: 2.76302058e-06
Iter: 115 loss: 2.72410284e-06
Iter: 116 loss: 2.85764781e-06
Iter: 117 loss: 2.71380122e-06
Iter: 118 loss: 2.6789844e-06
Iter: 119 loss: 2.71668659e-06
Iter: 120 loss: 2.66006236e-06
Iter: 121 loss: 2.62453386e-06
Iter: 122 loss: 2.70231112e-06
Iter: 123 loss: 2.61072773e-06
Iter: 124 loss: 2.57088504e-06
Iter: 125 loss: 2.69821658e-06
Iter: 126 loss: 2.55956729e-06
Iter: 127 loss: 2.53200369e-06
Iter: 128 loss: 2.74868626e-06
Iter: 129 loss: 2.53012968e-06
Iter: 130 loss: 2.50635185e-06
Iter: 131 loss: 2.62395679e-06
Iter: 132 loss: 2.50218704e-06
Iter: 133 loss: 2.47912317e-06
Iter: 134 loss: 2.55495797e-06
Iter: 135 loss: 2.47264597e-06
Iter: 136 loss: 2.45551473e-06
Iter: 137 loss: 2.42935971e-06
Iter: 138 loss: 2.42888746e-06
Iter: 139 loss: 2.4042497e-06
Iter: 140 loss: 2.40422901e-06
Iter: 141 loss: 2.38354869e-06
Iter: 142 loss: 2.38670191e-06
Iter: 143 loss: 2.367874e-06
Iter: 144 loss: 2.34640947e-06
Iter: 145 loss: 2.38200073e-06
Iter: 146 loss: 2.33647233e-06
Iter: 147 loss: 2.31072386e-06
Iter: 148 loss: 2.39326437e-06
Iter: 149 loss: 2.30334103e-06
Iter: 150 loss: 2.28446379e-06
Iter: 151 loss: 2.54735414e-06
Iter: 152 loss: 2.28447129e-06
Iter: 153 loss: 2.27297187e-06
Iter: 154 loss: 2.2575407e-06
Iter: 155 loss: 2.256691e-06
Iter: 156 loss: 2.23533834e-06
Iter: 157 loss: 2.34812296e-06
Iter: 158 loss: 2.2320196e-06
Iter: 159 loss: 2.21041682e-06
Iter: 160 loss: 2.25287818e-06
Iter: 161 loss: 2.20130232e-06
Iter: 162 loss: 2.18305468e-06
Iter: 163 loss: 2.22882954e-06
Iter: 164 loss: 2.17658749e-06
Iter: 165 loss: 2.15928617e-06
Iter: 166 loss: 2.18931473e-06
Iter: 167 loss: 2.15171804e-06
Iter: 168 loss: 2.13617204e-06
Iter: 169 loss: 2.13603448e-06
Iter: 170 loss: 2.12565078e-06
Iter: 171 loss: 2.12641157e-06
Iter: 172 loss: 2.11751785e-06
Iter: 173 loss: 2.10554867e-06
Iter: 174 loss: 2.09824475e-06
Iter: 175 loss: 2.09335258e-06
Iter: 176 loss: 2.07755556e-06
Iter: 177 loss: 2.28874887e-06
Iter: 178 loss: 2.07746598e-06
Iter: 179 loss: 2.06653908e-06
Iter: 180 loss: 2.07648077e-06
Iter: 181 loss: 2.06015943e-06
Iter: 182 loss: 2.0457469e-06
Iter: 183 loss: 2.08447409e-06
Iter: 184 loss: 2.04087291e-06
Iter: 185 loss: 2.02850833e-06
Iter: 186 loss: 2.02763499e-06
Iter: 187 loss: 2.01832495e-06
Iter: 188 loss: 2.00285149e-06
Iter: 189 loss: 2.13262661e-06
Iter: 190 loss: 2.00193153e-06
Iter: 191 loss: 1.98938142e-06
Iter: 192 loss: 2.05240394e-06
Iter: 193 loss: 1.98732164e-06
Iter: 194 loss: 1.97784925e-06
Iter: 195 loss: 1.97173244e-06
Iter: 196 loss: 1.96803694e-06
Iter: 197 loss: 1.95325742e-06
Iter: 198 loss: 1.99554324e-06
Iter: 199 loss: 1.9486256e-06
Iter: 200 loss: 1.93493725e-06
Iter: 201 loss: 2.04000889e-06
Iter: 202 loss: 1.93394726e-06
Iter: 203 loss: 1.92432412e-06
Iter: 204 loss: 1.93859955e-06
Iter: 205 loss: 1.91966683e-06
Iter: 206 loss: 1.9087056e-06
Iter: 207 loss: 1.99430701e-06
Iter: 208 loss: 1.90792139e-06
Iter: 209 loss: 1.89797163e-06
Iter: 210 loss: 1.91316076e-06
Iter: 211 loss: 1.89334094e-06
Iter: 212 loss: 1.88491413e-06
Iter: 213 loss: 1.88604861e-06
Iter: 214 loss: 1.87848855e-06
Iter: 215 loss: 1.86791453e-06
Iter: 216 loss: 1.8822268e-06
Iter: 217 loss: 1.86262673e-06
Iter: 218 loss: 1.85100475e-06
Iter: 219 loss: 1.94706399e-06
Iter: 220 loss: 1.8502368e-06
Iter: 221 loss: 1.83977477e-06
Iter: 222 loss: 1.86689851e-06
Iter: 223 loss: 1.83628913e-06
Iter: 224 loss: 1.82980057e-06
Iter: 225 loss: 1.82143935e-06
Iter: 226 loss: 1.82102519e-06
Iter: 227 loss: 1.80839584e-06
Iter: 228 loss: 1.93924234e-06
Iter: 229 loss: 1.8081339e-06
Iter: 230 loss: 1.80068332e-06
Iter: 231 loss: 1.83303871e-06
Iter: 232 loss: 1.79914969e-06
Iter: 233 loss: 1.79151994e-06
Iter: 234 loss: 1.7959992e-06
Iter: 235 loss: 1.78665118e-06
Iter: 236 loss: 1.77683523e-06
Iter: 237 loss: 1.79848871e-06
Iter: 238 loss: 1.77302934e-06
Iter: 239 loss: 1.76454955e-06
Iter: 240 loss: 1.79146491e-06
Iter: 241 loss: 1.76203059e-06
Iter: 242 loss: 1.75485957e-06
Iter: 243 loss: 1.78317305e-06
Iter: 244 loss: 1.75321952e-06
Iter: 245 loss: 1.74563615e-06
Iter: 246 loss: 1.80604422e-06
Iter: 247 loss: 1.74510433e-06
Iter: 248 loss: 1.73998649e-06
Iter: 249 loss: 1.73997284e-06
Iter: 250 loss: 1.73583203e-06
Iter: 251 loss: 1.72820205e-06
Iter: 252 loss: 1.73636363e-06
Iter: 253 loss: 1.72402315e-06
Iter: 254 loss: 1.71603938e-06
Iter: 255 loss: 1.73950662e-06
Iter: 256 loss: 1.71343549e-06
Iter: 257 loss: 1.70629482e-06
Iter: 258 loss: 1.71383294e-06
Iter: 259 loss: 1.70231965e-06
Iter: 260 loss: 1.69593591e-06
Iter: 261 loss: 1.69589043e-06
Iter: 262 loss: 1.69138207e-06
Iter: 263 loss: 1.68371412e-06
Iter: 264 loss: 1.68372048e-06
Iter: 265 loss: 1.6740654e-06
Iter: 266 loss: 1.72242744e-06
Iter: 267 loss: 1.67246424e-06
Iter: 268 loss: 1.66645827e-06
Iter: 269 loss: 1.68474753e-06
Iter: 270 loss: 1.66477128e-06
Iter: 271 loss: 1.65694269e-06
Iter: 272 loss: 1.68304302e-06
Iter: 273 loss: 1.65473887e-06
Iter: 274 loss: 1.64899552e-06
Iter: 275 loss: 1.65521089e-06
Iter: 276 loss: 1.64569769e-06
Iter: 277 loss: 1.63896175e-06
Iter: 278 loss: 1.65432448e-06
Iter: 279 loss: 1.63641312e-06
Iter: 280 loss: 1.63051868e-06
Iter: 281 loss: 1.67846065e-06
Iter: 282 loss: 1.63004734e-06
Iter: 283 loss: 1.62621859e-06
Iter: 284 loss: 1.66446557e-06
Iter: 285 loss: 1.6261265e-06
Iter: 286 loss: 1.62274023e-06
Iter: 287 loss: 1.6174065e-06
Iter: 288 loss: 1.61734113e-06
Iter: 289 loss: 1.61025037e-06
Iter: 290 loss: 1.6238472e-06
Iter: 291 loss: 1.60722038e-06
Iter: 292 loss: 1.60082982e-06
Iter: 293 loss: 1.63551522e-06
Iter: 294 loss: 1.59994511e-06
Iter: 295 loss: 1.5937494e-06
Iter: 296 loss: 1.59830199e-06
Iter: 297 loss: 1.58998193e-06
Iter: 298 loss: 1.58262424e-06
Iter: 299 loss: 1.62104197e-06
Iter: 300 loss: 1.58145497e-06
Iter: 301 loss: 1.57694967e-06
Iter: 302 loss: 1.61700416e-06
Iter: 303 loss: 1.57670138e-06
Iter: 304 loss: 1.57315549e-06
Iter: 305 loss: 1.57207114e-06
Iter: 306 loss: 1.56999931e-06
Iter: 307 loss: 1.56371652e-06
Iter: 308 loss: 1.57019315e-06
Iter: 309 loss: 1.56020815e-06
Iter: 310 loss: 1.55546104e-06
Iter: 311 loss: 1.56517137e-06
Iter: 312 loss: 1.55361158e-06
Iter: 313 loss: 1.5474443e-06
Iter: 314 loss: 1.59369415e-06
Iter: 315 loss: 1.54700206e-06
Iter: 316 loss: 1.54255122e-06
Iter: 317 loss: 1.54025099e-06
Iter: 318 loss: 1.53821009e-06
Iter: 319 loss: 1.53290625e-06
Iter: 320 loss: 1.55879e-06
Iter: 321 loss: 1.53195447e-06
Iter: 322 loss: 1.52669259e-06
Iter: 323 loss: 1.57934824e-06
Iter: 324 loss: 1.52647931e-06
Iter: 325 loss: 1.52312191e-06
Iter: 326 loss: 1.51943482e-06
Iter: 327 loss: 1.51893983e-06
Iter: 328 loss: 1.51460881e-06
Iter: 329 loss: 1.53265955e-06
Iter: 330 loss: 1.5137656e-06
Iter: 331 loss: 1.50892049e-06
Iter: 332 loss: 1.5103019e-06
Iter: 333 loss: 1.50563255e-06
Iter: 334 loss: 1.49962239e-06
Iter: 335 loss: 1.54478607e-06
Iter: 336 loss: 1.4992446e-06
Iter: 337 loss: 1.49569928e-06
Iter: 338 loss: 1.5049402e-06
Iter: 339 loss: 1.49444747e-06
Iter: 340 loss: 1.49014352e-06
Iter: 341 loss: 1.49987568e-06
Iter: 342 loss: 1.48853985e-06
Iter: 343 loss: 1.48409094e-06
Iter: 344 loss: 1.4950383e-06
Iter: 345 loss: 1.48251365e-06
Iter: 346 loss: 1.47853871e-06
Iter: 347 loss: 1.49099901e-06
Iter: 348 loss: 1.47739024e-06
Iter: 349 loss: 1.47297703e-06
Iter: 350 loss: 1.47040487e-06
Iter: 351 loss: 1.46853949e-06
Iter: 352 loss: 1.46396337e-06
Iter: 353 loss: 1.52424093e-06
Iter: 354 loss: 1.46392063e-06
Iter: 355 loss: 1.46009e-06
Iter: 356 loss: 1.47103265e-06
Iter: 357 loss: 1.45890158e-06
Iter: 358 loss: 1.45477725e-06
Iter: 359 loss: 1.45706667e-06
Iter: 360 loss: 1.45216643e-06
Iter: 361 loss: 1.449205e-06
Iter: 362 loss: 1.44918624e-06
Iter: 363 loss: 1.44612704e-06
Iter: 364 loss: 1.44170508e-06
Iter: 365 loss: 1.44140915e-06
Iter: 366 loss: 1.43774378e-06
Iter: 367 loss: 1.44026671e-06
Iter: 368 loss: 1.43537602e-06
Iter: 369 loss: 1.4298904e-06
Iter: 370 loss: 1.44858393e-06
Iter: 371 loss: 1.4284733e-06
Iter: 372 loss: 1.42466774e-06
Iter: 373 loss: 1.44331489e-06
Iter: 374 loss: 1.42396436e-06
Iter: 375 loss: 1.41961436e-06
Iter: 376 loss: 1.43121883e-06
Iter: 377 loss: 1.41825308e-06
Iter: 378 loss: 1.41470832e-06
Iter: 379 loss: 1.42718727e-06
Iter: 380 loss: 1.41377939e-06
Iter: 381 loss: 1.41017699e-06
Iter: 382 loss: 1.42469412e-06
Iter: 383 loss: 1.40939346e-06
Iter: 384 loss: 1.40653833e-06
Iter: 385 loss: 1.40815075e-06
Iter: 386 loss: 1.40458565e-06
Iter: 387 loss: 1.40103316e-06
Iter: 388 loss: 1.41006331e-06
Iter: 389 loss: 1.39970575e-06
Iter: 390 loss: 1.3956269e-06
Iter: 391 loss: 1.40604618e-06
Iter: 392 loss: 1.39419944e-06
Iter: 393 loss: 1.39096517e-06
Iter: 394 loss: 1.41080307e-06
Iter: 395 loss: 1.39063673e-06
Iter: 396 loss: 1.38762641e-06
Iter: 397 loss: 1.39558892e-06
Iter: 398 loss: 1.38655412e-06
Iter: 399 loss: 1.38345445e-06
Iter: 400 loss: 1.39135545e-06
Iter: 401 loss: 1.38241421e-06
Iter: 402 loss: 1.3787436e-06
Iter: 403 loss: 1.39261374e-06
Iter: 404 loss: 1.37782411e-06
Iter: 405 loss: 1.37542088e-06
Iter: 406 loss: 1.37566849e-06
Iter: 407 loss: 1.37347433e-06
Iter: 408 loss: 1.37023437e-06
Iter: 409 loss: 1.37119764e-06
Iter: 410 loss: 1.36791834e-06
Iter: 411 loss: 1.3634467e-06
Iter: 412 loss: 1.37924599e-06
Iter: 413 loss: 1.36238441e-06
Iter: 414 loss: 1.35873267e-06
Iter: 415 loss: 1.36892902e-06
Iter: 416 loss: 1.35760502e-06
Iter: 417 loss: 1.35417952e-06
Iter: 418 loss: 1.3747424e-06
Iter: 419 loss: 1.35378923e-06
Iter: 420 loss: 1.35015966e-06
Iter: 421 loss: 1.35662526e-06
Iter: 422 loss: 1.34850984e-06
Iter: 423 loss: 1.34520178e-06
Iter: 424 loss: 1.35741971e-06
Iter: 425 loss: 1.3443306e-06
Iter: 426 loss: 1.34154277e-06
Iter: 427 loss: 1.3475277e-06
Iter: 428 loss: 1.34045297e-06
Iter: 429 loss: 1.33702724e-06
Iter: 430 loss: 1.34198331e-06
Iter: 431 loss: 1.33539697e-06
Iter: 432 loss: 1.33204264e-06
Iter: 433 loss: 1.33919343e-06
Iter: 434 loss: 1.33081153e-06
Iter: 435 loss: 1.32747255e-06
Iter: 436 loss: 1.35930554e-06
Iter: 437 loss: 1.32735533e-06
Iter: 438 loss: 1.32490777e-06
Iter: 439 loss: 1.33209414e-06
Iter: 440 loss: 1.32416471e-06
Iter: 441 loss: 1.3218496e-06
Iter: 442 loss: 1.33057756e-06
Iter: 443 loss: 1.32130151e-06
Iter: 444 loss: 1.31901947e-06
Iter: 445 loss: 1.31872093e-06
Iter: 446 loss: 1.31708543e-06
Iter: 447 loss: 1.31423076e-06
Iter: 448 loss: 1.31749255e-06
Iter: 449 loss: 1.3126471e-06
Iter: 450 loss: 1.30948956e-06
Iter: 451 loss: 1.32686318e-06
Iter: 452 loss: 1.30899298e-06
Iter: 453 loss: 1.30612625e-06
Iter: 454 loss: 1.30294302e-06
Iter: 455 loss: 1.30245189e-06
Iter: 456 loss: 1.29840907e-06
Iter: 457 loss: 1.31846582e-06
Iter: 458 loss: 1.29770342e-06
Iter: 459 loss: 1.29475507e-06
Iter: 460 loss: 1.33499293e-06
Iter: 461 loss: 1.29470095e-06
Iter: 462 loss: 1.29179784e-06
Iter: 463 loss: 1.29365208e-06
Iter: 464 loss: 1.28998909e-06
Iter: 465 loss: 1.28712156e-06
Iter: 466 loss: 1.29248292e-06
Iter: 467 loss: 1.28579609e-06
Iter: 468 loss: 1.28293016e-06
Iter: 469 loss: 1.29878185e-06
Iter: 470 loss: 1.28243437e-06
Iter: 471 loss: 1.27978274e-06
Iter: 472 loss: 1.28264787e-06
Iter: 473 loss: 1.27829117e-06
Iter: 474 loss: 1.2755338e-06
Iter: 475 loss: 1.28950887e-06
Iter: 476 loss: 1.2750827e-06
Iter: 477 loss: 1.27279054e-06
Iter: 478 loss: 1.29140517e-06
Iter: 479 loss: 1.27258772e-06
Iter: 480 loss: 1.27064504e-06
Iter: 481 loss: 1.27048042e-06
Iter: 482 loss: 1.26905e-06
Iter: 483 loss: 1.2660912e-06
Iter: 484 loss: 1.27728549e-06
Iter: 485 loss: 1.26542272e-06
Iter: 486 loss: 1.26292434e-06
Iter: 487 loss: 1.26250461e-06
Iter: 488 loss: 1.26085399e-06
Iter: 489 loss: 1.25757617e-06
Iter: 490 loss: 1.26412897e-06
Iter: 491 loss: 1.25626116e-06
Iter: 492 loss: 1.25297765e-06
Iter: 493 loss: 1.26991426e-06
Iter: 494 loss: 1.25246459e-06
Iter: 495 loss: 1.24988583e-06
Iter: 496 loss: 1.25382689e-06
Iter: 497 loss: 1.2486056e-06
Iter: 498 loss: 1.24550456e-06
Iter: 499 loss: 1.25426038e-06
Iter: 500 loss: 1.24457847e-06
Iter: 501 loss: 1.24182338e-06
Iter: 502 loss: 1.25818474e-06
Iter: 503 loss: 1.24154099e-06
Iter: 504 loss: 1.23912173e-06
Iter: 505 loss: 1.24855649e-06
Iter: 506 loss: 1.23858013e-06
Iter: 507 loss: 1.23633231e-06
Iter: 508 loss: 1.23576831e-06
Iter: 509 loss: 1.23434472e-06
Iter: 510 loss: 1.23140626e-06
Iter: 511 loss: 1.24104781e-06
Iter: 512 loss: 1.23054019e-06
Iter: 513 loss: 1.22791243e-06
Iter: 514 loss: 1.24209964e-06
Iter: 515 loss: 1.22755966e-06
Iter: 516 loss: 1.22508925e-06
Iter: 517 loss: 1.24021426e-06
Iter: 518 loss: 1.22475171e-06
Iter: 519 loss: 1.22298582e-06
Iter: 520 loss: 1.22821984e-06
Iter: 521 loss: 1.22251936e-06
Iter: 522 loss: 1.22063329e-06
Iter: 523 loss: 1.21964536e-06
Iter: 524 loss: 1.21881385e-06
Iter: 525 loss: 1.21579023e-06
Iter: 526 loss: 1.22547715e-06
Iter: 527 loss: 1.21496169e-06
Iter: 528 loss: 1.21228379e-06
Iter: 529 loss: 1.22459323e-06
Iter: 530 loss: 1.211822e-06
Iter: 531 loss: 1.20963102e-06
Iter: 532 loss: 1.20817e-06
Iter: 533 loss: 1.20743493e-06
Iter: 534 loss: 1.20367804e-06
Iter: 535 loss: 1.21403605e-06
Iter: 536 loss: 1.20247637e-06
Iter: 537 loss: 1.19951414e-06
Iter: 538 loss: 1.20862e-06
Iter: 539 loss: 1.19853155e-06
Iter: 540 loss: 1.19573269e-06
Iter: 541 loss: 1.20661048e-06
Iter: 542 loss: 1.19490187e-06
Iter: 543 loss: 1.1921336e-06
Iter: 544 loss: 1.21711651e-06
Iter: 545 loss: 1.19198251e-06
Iter: 546 loss: 1.19030744e-06
Iter: 547 loss: 1.19264143e-06
Iter: 548 loss: 1.18951198e-06
Iter: 549 loss: 1.1871432e-06
Iter: 550 loss: 1.18655112e-06
Iter: 551 loss: 1.18506136e-06
Iter: 552 loss: 1.18218281e-06
Iter: 553 loss: 1.20327854e-06
Iter: 554 loss: 1.18193452e-06
Iter: 555 loss: 1.18046626e-06
Iter: 556 loss: 1.20073719e-06
Iter: 557 loss: 1.18046773e-06
Iter: 558 loss: 1.17895524e-06
Iter: 559 loss: 1.1760535e-06
Iter: 560 loss: 1.23915765e-06
Iter: 561 loss: 1.17604793e-06
Iter: 562 loss: 1.17383354e-06
Iter: 563 loss: 1.20500135e-06
Iter: 564 loss: 1.17382024e-06
Iter: 565 loss: 1.17217655e-06
Iter: 566 loss: 1.17297395e-06
Iter: 567 loss: 1.1710797e-06
Iter: 568 loss: 1.16911497e-06
Iter: 569 loss: 1.17243144e-06
Iter: 570 loss: 1.16825686e-06
Iter: 571 loss: 1.16565457e-06
Iter: 572 loss: 1.17241632e-06
Iter: 573 loss: 1.16483909e-06
Iter: 574 loss: 1.16278238e-06
Iter: 575 loss: 1.16808349e-06
Iter: 576 loss: 1.16199055e-06
Iter: 577 loss: 1.15964212e-06
Iter: 578 loss: 1.15800833e-06
Iter: 579 loss: 1.15720673e-06
Iter: 580 loss: 1.15454372e-06
Iter: 581 loss: 1.19223216e-06
Iter: 582 loss: 1.15454907e-06
Iter: 583 loss: 1.15250282e-06
Iter: 584 loss: 1.1531007e-06
Iter: 585 loss: 1.15101454e-06
Iter: 586 loss: 1.14884494e-06
Iter: 587 loss: 1.1652055e-06
Iter: 588 loss: 1.14864542e-06
Iter: 589 loss: 1.14630052e-06
Iter: 590 loss: 1.15002831e-06
Iter: 591 loss: 1.14518343e-06
Iter: 592 loss: 1.14346267e-06
Iter: 593 loss: 1.14613044e-06
Iter: 594 loss: 1.14268732e-06
Iter: 595 loss: 1.14031786e-06
Iter: 596 loss: 1.14256977e-06
Iter: 597 loss: 1.13894498e-06
Iter: 598 loss: 1.13686622e-06
Iter: 599 loss: 1.13681949e-06
Iter: 600 loss: 1.13567512e-06
Iter: 601 loss: 1.13526448e-06
Iter: 602 loss: 1.13466285e-06
Iter: 603 loss: 1.13310591e-06
Iter: 604 loss: 1.13739884e-06
Iter: 605 loss: 1.13263286e-06
Iter: 606 loss: 1.1306654e-06
Iter: 607 loss: 1.13078158e-06
Iter: 608 loss: 1.12913597e-06
Iter: 609 loss: 1.12721773e-06
Iter: 610 loss: 1.15041871e-06
Iter: 611 loss: 1.12718681e-06
Iter: 612 loss: 1.12595194e-06
Iter: 613 loss: 1.12316434e-06
Iter: 614 loss: 1.16592651e-06
Iter: 615 loss: 1.12307657e-06
Iter: 616 loss: 1.12052373e-06
Iter: 617 loss: 1.12043949e-06
Iter: 618 loss: 1.1188514e-06
Iter: 619 loss: 1.12053294e-06
Iter: 620 loss: 1.11789586e-06
Iter: 621 loss: 1.11593511e-06
Iter: 622 loss: 1.11752502e-06
Iter: 623 loss: 1.11469024e-06
Iter: 624 loss: 1.11246084e-06
Iter: 625 loss: 1.11594272e-06
Iter: 626 loss: 1.11136546e-06
Iter: 627 loss: 1.10864357e-06
Iter: 628 loss: 1.12325665e-06
Iter: 629 loss: 1.10819508e-06
Iter: 630 loss: 1.10635744e-06
Iter: 631 loss: 1.13323176e-06
Iter: 632 loss: 1.10637814e-06
Iter: 633 loss: 1.10514713e-06
Iter: 634 loss: 1.10350322e-06
Iter: 635 loss: 1.10341784e-06
Iter: 636 loss: 1.10186124e-06
Iter: 637 loss: 1.10186477e-06
Iter: 638 loss: 1.10038832e-06
Iter: 639 loss: 1.10051587e-06
Iter: 640 loss: 1.09917437e-06
Iter: 641 loss: 1.09774328e-06
Iter: 642 loss: 1.09853715e-06
Iter: 643 loss: 1.09686471e-06
Iter: 644 loss: 1.0950157e-06
Iter: 645 loss: 1.10415408e-06
Iter: 646 loss: 1.09469443e-06
Iter: 647 loss: 1.09274913e-06
Iter: 648 loss: 1.09631628e-06
Iter: 649 loss: 1.09186635e-06
Iter: 650 loss: 1.09010261e-06
Iter: 651 loss: 1.09223811e-06
Iter: 652 loss: 1.08915879e-06
Iter: 653 loss: 1.08729273e-06
Iter: 654 loss: 1.09595203e-06
Iter: 655 loss: 1.08696509e-06
Iter: 656 loss: 1.08508459e-06
Iter: 657 loss: 1.08697463e-06
Iter: 658 loss: 1.08402378e-06
Iter: 659 loss: 1.0821791e-06
Iter: 660 loss: 1.09281279e-06
Iter: 661 loss: 1.0819133e-06
Iter: 662 loss: 1.08024528e-06
Iter: 663 loss: 1.08048823e-06
Iter: 664 loss: 1.07885762e-06
Iter: 665 loss: 1.07653898e-06
Iter: 666 loss: 1.08670031e-06
Iter: 667 loss: 1.0760142e-06
Iter: 668 loss: 1.0743521e-06
Iter: 669 loss: 1.08283871e-06
Iter: 670 loss: 1.07413007e-06
Iter: 671 loss: 1.07266146e-06
Iter: 672 loss: 1.077658e-06
Iter: 673 loss: 1.07234246e-06
Iter: 674 loss: 1.07056951e-06
Iter: 675 loss: 1.07439587e-06
Iter: 676 loss: 1.06991934e-06
Iter: 677 loss: 1.06846949e-06
Iter: 678 loss: 1.07779715e-06
Iter: 679 loss: 1.06826758e-06
Iter: 680 loss: 1.06699281e-06
Iter: 681 loss: 1.07034066e-06
Iter: 682 loss: 1.06659331e-06
Iter: 683 loss: 1.06551374e-06
Iter: 684 loss: 1.06320954e-06
Iter: 685 loss: 1.1014863e-06
Iter: 686 loss: 1.06310222e-06
Iter: 687 loss: 1.06077937e-06
Iter: 688 loss: 1.07450717e-06
Iter: 689 loss: 1.06053301e-06
Iter: 690 loss: 1.05832851e-06
Iter: 691 loss: 1.06771677e-06
Iter: 692 loss: 1.05791491e-06
Iter: 693 loss: 1.05587924e-06
Iter: 694 loss: 1.07107633e-06
Iter: 695 loss: 1.05573167e-06
Iter: 696 loss: 1.05462482e-06
Iter: 697 loss: 1.05339905e-06
Iter: 698 loss: 1.05311722e-06
Iter: 699 loss: 1.05109086e-06
Iter: 700 loss: 1.05418951e-06
Iter: 701 loss: 1.05007678e-06
Iter: 702 loss: 1.04800438e-06
Iter: 703 loss: 1.06573316e-06
Iter: 704 loss: 1.04786977e-06
Iter: 705 loss: 1.0464986e-06
Iter: 706 loss: 1.05156982e-06
Iter: 707 loss: 1.04613832e-06
Iter: 708 loss: 1.04453716e-06
Iter: 709 loss: 1.04537173e-06
Iter: 710 loss: 1.04339949e-06
Iter: 711 loss: 1.04155106e-06
Iter: 712 loss: 1.05020615e-06
Iter: 713 loss: 1.04127753e-06
Iter: 714 loss: 1.03983052e-06
Iter: 715 loss: 1.04562355e-06
Iter: 716 loss: 1.03955222e-06
Iter: 717 loss: 1.03800937e-06
Iter: 718 loss: 1.04542528e-06
Iter: 719 loss: 1.03772129e-06
Iter: 720 loss: 1.03655111e-06
Iter: 721 loss: 1.03905859e-06
Iter: 722 loss: 1.03613343e-06
Iter: 723 loss: 1.03462753e-06
Iter: 724 loss: 1.035639e-06
Iter: 725 loss: 1.03362208e-06
Iter: 726 loss: 1.03220623e-06
Iter: 727 loss: 1.03435457e-06
Iter: 728 loss: 1.03151808e-06
Iter: 729 loss: 1.02993658e-06
Iter: 730 loss: 1.0356714e-06
Iter: 731 loss: 1.02959302e-06
Iter: 732 loss: 1.02819558e-06
Iter: 733 loss: 1.02887918e-06
Iter: 734 loss: 1.0272845e-06
Iter: 735 loss: 1.02577656e-06
Iter: 736 loss: 1.04506307e-06
Iter: 737 loss: 1.02576359e-06
Iter: 738 loss: 1.02462468e-06
Iter: 739 loss: 1.02455158e-06
Iter: 740 loss: 1.02373792e-06
Iter: 741 loss: 1.02217155e-06
Iter: 742 loss: 1.02065349e-06
Iter: 743 loss: 1.02033709e-06
Iter: 744 loss: 1.01818864e-06
Iter: 745 loss: 1.05200706e-06
Iter: 746 loss: 1.0182107e-06
Iter: 747 loss: 1.01698106e-06
Iter: 748 loss: 1.01685441e-06
Iter: 749 loss: 1.01591445e-06
Iter: 750 loss: 1.01422211e-06
Iter: 751 loss: 1.02657896e-06
Iter: 752 loss: 1.01404487e-06
Iter: 753 loss: 1.01278886e-06
Iter: 754 loss: 1.02048125e-06
Iter: 755 loss: 1.01260048e-06
Iter: 756 loss: 1.01164574e-06
Iter: 757 loss: 1.01641695e-06
Iter: 758 loss: 1.01140222e-06
Iter: 759 loss: 1.01036699e-06
Iter: 760 loss: 1.01088665e-06
Iter: 761 loss: 1.00967543e-06
Iter: 762 loss: 1.00831585e-06
Iter: 763 loss: 1.01040177e-06
Iter: 764 loss: 1.0076626e-06
Iter: 765 loss: 1.00629654e-06
Iter: 766 loss: 1.01421824e-06
Iter: 767 loss: 1.00614511e-06
Iter: 768 loss: 1.00507032e-06
Iter: 769 loss: 1.00369562e-06
Iter: 770 loss: 1.00359307e-06
Iter: 771 loss: 1.0020018e-06
Iter: 772 loss: 1.00768818e-06
Iter: 773 loss: 1.00164903e-06
Iter: 774 loss: 1.00006923e-06
Iter: 775 loss: 1.00742341e-06
Iter: 776 loss: 9.99727263e-07
Iter: 777 loss: 9.98423502e-07
Iter: 778 loss: 1.00920374e-06
Iter: 779 loss: 9.98306405e-07
Iter: 780 loss: 9.97131565e-07
Iter: 781 loss: 9.96758445e-07
Iter: 782 loss: 9.96074732e-07
Iter: 783 loss: 9.94587481e-07
Iter: 784 loss: 9.94874199e-07
Iter: 785 loss: 9.93424692e-07
Iter: 786 loss: 9.91901061e-07
Iter: 787 loss: 1.01609567e-06
Iter: 788 loss: 9.91922661e-07
Iter: 789 loss: 9.90760668e-07
Iter: 790 loss: 9.89920522e-07
Iter: 791 loss: 9.89485e-07
Iter: 792 loss: 9.87953399e-07
Iter: 793 loss: 1.00213526e-06
Iter: 794 loss: 9.8795681e-07
Iter: 795 loss: 9.86803116e-07
Iter: 796 loss: 9.9958379e-07
Iter: 797 loss: 9.868196e-07
Iter: 798 loss: 9.8599e-07
Iter: 799 loss: 9.8480632e-07
Iter: 800 loss: 9.8479e-07
Iter: 801 loss: 9.83202426e-07
Iter: 802 loss: 9.95045866e-07
Iter: 803 loss: 9.83140353e-07
Iter: 804 loss: 9.81912535e-07
Iter: 805 loss: 9.82582264e-07
Iter: 806 loss: 9.81077392e-07
Iter: 807 loss: 9.79986453e-07
Iter: 808 loss: 9.85695578e-07
Iter: 809 loss: 9.79818878e-07
Iter: 810 loss: 9.78546495e-07
Iter: 811 loss: 9.78660182e-07
Iter: 812 loss: 9.77659283e-07
Iter: 813 loss: 9.76111096e-07
Iter: 814 loss: 9.81181529e-07
Iter: 815 loss: 9.75794251e-07
Iter: 816 loss: 9.74163299e-07
Iter: 817 loss: 9.76174647e-07
Iter: 818 loss: 9.73386477e-07
Iter: 819 loss: 9.72175712e-07
Iter: 820 loss: 9.86340865e-07
Iter: 821 loss: 9.72176622e-07
Iter: 822 loss: 9.71029067e-07
Iter: 823 loss: 9.71338295e-07
Iter: 824 loss: 9.70148335e-07
Iter: 825 loss: 9.68906e-07
Iter: 826 loss: 9.71421059e-07
Iter: 827 loss: 9.68440418e-07
Iter: 828 loss: 9.671719e-07
Iter: 829 loss: 9.70973815e-07
Iter: 830 loss: 9.6688e-07
Iter: 831 loss: 9.65585059e-07
Iter: 832 loss: 9.72397743e-07
Iter: 833 loss: 9.65395657e-07
Iter: 834 loss: 9.64015499e-07
Iter: 835 loss: 9.68874701e-07
Iter: 836 loss: 9.63604293e-07
Iter: 837 loss: 9.62835429e-07
Iter: 838 loss: 9.6216354e-07
Iter: 839 loss: 9.61971637e-07
Iter: 840 loss: 9.60836587e-07
Iter: 841 loss: 9.7309271e-07
Iter: 842 loss: 9.60821694e-07
Iter: 843 loss: 9.59883891e-07
Iter: 844 loss: 9.59869112e-07
Iter: 845 loss: 9.59152885e-07
Iter: 846 loss: 9.57848215e-07
Iter: 847 loss: 9.57883231e-07
Iter: 848 loss: 9.56818667e-07
Iter: 849 loss: 9.55700784e-07
Iter: 850 loss: 9.55655878e-07
Iter: 851 loss: 9.54821871e-07
Iter: 852 loss: 9.53763674e-07
Iter: 853 loss: 9.5368e-07
Iter: 854 loss: 9.52051778e-07
Iter: 855 loss: 9.59282261e-07
Iter: 856 loss: 9.5163756e-07
Iter: 857 loss: 9.50649792e-07
Iter: 858 loss: 9.59375939e-07
Iter: 859 loss: 9.50638935e-07
Iter: 860 loss: 9.49542255e-07
Iter: 861 loss: 9.48948127e-07
Iter: 862 loss: 9.48431534e-07
Iter: 863 loss: 9.47366289e-07
Iter: 864 loss: 9.56231588e-07
Iter: 865 loss: 9.4727551e-07
Iter: 866 loss: 9.46286718e-07
Iter: 867 loss: 9.46091177e-07
Iter: 868 loss: 9.45503132e-07
Iter: 869 loss: 9.44471253e-07
Iter: 870 loss: 9.44467729e-07
Iter: 871 loss: 9.43481837e-07
Iter: 872 loss: 9.43928171e-07
Iter: 873 loss: 9.42850363e-07
Iter: 874 loss: 9.42016129e-07
Iter: 875 loss: 9.42216161e-07
Iter: 876 loss: 9.41330768e-07
Iter: 877 loss: 9.40159907e-07
Iter: 878 loss: 9.4285781e-07
Iter: 879 loss: 9.39662812e-07
Iter: 880 loss: 9.38077164e-07
Iter: 881 loss: 9.46343221e-07
Iter: 882 loss: 9.37843538e-07
Iter: 883 loss: 9.36806828e-07
Iter: 884 loss: 9.37376853e-07
Iter: 885 loss: 9.36176036e-07
Iter: 886 loss: 9.349435e-07
Iter: 887 loss: 9.35795811e-07
Iter: 888 loss: 9.34136381e-07
Iter: 889 loss: 9.32868147e-07
Iter: 890 loss: 9.40422524e-07
Iter: 891 loss: 9.32701369e-07
Iter: 892 loss: 9.31485e-07
Iter: 893 loss: 9.36351626e-07
Iter: 894 loss: 9.31234695e-07
Iter: 895 loss: 9.30042575e-07
Iter: 896 loss: 9.32370426e-07
Iter: 897 loss: 9.29536e-07
Iter: 898 loss: 9.28440897e-07
Iter: 899 loss: 9.33890419e-07
Iter: 900 loss: 9.28269117e-07
Iter: 901 loss: 9.27286521e-07
Iter: 902 loss: 9.29352495e-07
Iter: 903 loss: 9.26875373e-07
Iter: 904 loss: 9.25746917e-07
Iter: 905 loss: 9.25905169e-07
Iter: 906 loss: 9.24924791e-07
Iter: 907 loss: 9.24067535e-07
Iter: 908 loss: 9.24001824e-07
Iter: 909 loss: 9.23489949e-07
Iter: 910 loss: 9.22855236e-07
Iter: 911 loss: 9.22808169e-07
Iter: 912 loss: 9.21763672e-07
Iter: 913 loss: 9.21650212e-07
Iter: 914 loss: 9.20941943e-07
Iter: 915 loss: 9.19660806e-07
Iter: 916 loss: 9.25880045e-07
Iter: 917 loss: 9.19368631e-07
Iter: 918 loss: 9.18243927e-07
Iter: 919 loss: 9.27172607e-07
Iter: 920 loss: 9.18116939e-07
Iter: 921 loss: 9.17157877e-07
Iter: 922 loss: 9.16468707e-07
Iter: 923 loss: 9.16234399e-07
Iter: 924 loss: 9.15168869e-07
Iter: 925 loss: 9.21058586e-07
Iter: 926 loss: 9.14979182e-07
Iter: 927 loss: 9.13914e-07
Iter: 928 loss: 9.14709631e-07
Iter: 929 loss: 9.13244548e-07
Iter: 930 loss: 9.11989275e-07
Iter: 931 loss: 9.18528485e-07
Iter: 932 loss: 9.11771565e-07
Iter: 933 loss: 9.10751851e-07
Iter: 934 loss: 9.1193e-07
Iter: 935 loss: 9.10274e-07
Iter: 936 loss: 9.08721688e-07
Iter: 937 loss: 9.16043859e-07
Iter: 938 loss: 9.08435709e-07
Iter: 939 loss: 9.07594313e-07
Iter: 940 loss: 9.09601454e-07
Iter: 941 loss: 9.07300887e-07
Iter: 942 loss: 9.0646131e-07
Iter: 943 loss: 9.14122893e-07
Iter: 944 loss: 9.06463526e-07
Iter: 945 loss: 9.0572496e-07
Iter: 946 loss: 9.05738091e-07
Iter: 947 loss: 9.05194042e-07
Iter: 948 loss: 9.04113676e-07
Iter: 949 loss: 9.0536264e-07
Iter: 950 loss: 9.03554735e-07
Iter: 951 loss: 9.02503189e-07
Iter: 952 loss: 9.06531227e-07
Iter: 953 loss: 9.02302531e-07
Iter: 954 loss: 9.01417252e-07
Iter: 955 loss: 9.00731948e-07
Iter: 956 loss: 9.00469558e-07
Iter: 957 loss: 8.99201666e-07
Iter: 958 loss: 9.17091484e-07
Iter: 959 loss: 8.99264137e-07
Iter: 960 loss: 8.98264602e-07
Iter: 961 loss: 8.99329677e-07
Iter: 962 loss: 8.97783707e-07
Iter: 963 loss: 8.96857784e-07
Iter: 964 loss: 8.96504787e-07
Iter: 965 loss: 8.96073118e-07
Iter: 966 loss: 8.94568302e-07
Iter: 967 loss: 8.98613848e-07
Iter: 968 loss: 8.94163179e-07
Iter: 969 loss: 8.92803655e-07
Iter: 970 loss: 9.01121098e-07
Iter: 971 loss: 8.9254894e-07
Iter: 972 loss: 8.91746e-07
Iter: 973 loss: 8.98840312e-07
Iter: 974 loss: 8.91670311e-07
Iter: 975 loss: 8.9083585e-07
Iter: 976 loss: 8.91024229e-07
Iter: 977 loss: 8.90217279e-07
Iter: 978 loss: 8.89331773e-07
Iter: 979 loss: 8.96711754e-07
Iter: 980 loss: 8.89264584e-07
Iter: 981 loss: 8.88524e-07
Iter: 982 loss: 8.91218519e-07
Iter: 983 loss: 8.8838209e-07
Iter: 984 loss: 8.87581393e-07
Iter: 985 loss: 8.87372153e-07
Iter: 986 loss: 8.86922635e-07
Iter: 987 loss: 8.85996e-07
Iter: 988 loss: 8.87714918e-07
Iter: 989 loss: 8.85528721e-07
Iter: 990 loss: 8.84585802e-07
Iter: 991 loss: 8.88063e-07
Iter: 992 loss: 8.84363772e-07
Iter: 993 loss: 8.83386122e-07
Iter: 994 loss: 8.85071302e-07
Iter: 995 loss: 8.83005782e-07
Iter: 996 loss: 8.81875621e-07
Iter: 997 loss: 8.85391387e-07
Iter: 998 loss: 8.8159419e-07
Iter: 999 loss: 8.8063689e-07
Iter: 1000 loss: 8.86203338e-07
Iter: 1001 loss: 8.80495065e-07
Iter: 1002 loss: 8.79710683e-07
Iter: 1003 loss: 8.80435209e-07
Iter: 1004 loss: 8.79219897e-07
Iter: 1005 loss: 8.78364631e-07
Iter: 1006 loss: 8.79292031e-07
Iter: 1007 loss: 8.77831326e-07
Iter: 1008 loss: 8.76662625e-07
Iter: 1009 loss: 8.77699449e-07
Iter: 1010 loss: 8.75945943e-07
Iter: 1011 loss: 8.74693e-07
Iter: 1012 loss: 8.78494177e-07
Iter: 1013 loss: 8.74346597e-07
Iter: 1014 loss: 8.73652652e-07
Iter: 1015 loss: 8.73598367e-07
Iter: 1016 loss: 8.72977637e-07
Iter: 1017 loss: 8.7249839e-07
Iter: 1018 loss: 8.7229057e-07
Iter: 1019 loss: 8.71313773e-07
Iter: 1020 loss: 8.81078677e-07
Iter: 1021 loss: 8.71282509e-07
Iter: 1022 loss: 8.70740791e-07
Iter: 1023 loss: 8.70070267e-07
Iter: 1024 loss: 8.70028543e-07
Iter: 1025 loss: 8.69068913e-07
Iter: 1026 loss: 8.69862561e-07
Iter: 1027 loss: 8.68437155e-07
Iter: 1028 loss: 8.67247309e-07
Iter: 1029 loss: 8.77142156e-07
Iter: 1030 loss: 8.67186145e-07
Iter: 1031 loss: 8.66269716e-07
Iter: 1032 loss: 8.66787673e-07
Iter: 1033 loss: 8.65734762e-07
Iter: 1034 loss: 8.64659228e-07
Iter: 1035 loss: 8.72919543e-07
Iter: 1036 loss: 8.64613753e-07
Iter: 1037 loss: 8.63892e-07
Iter: 1038 loss: 8.65971117e-07
Iter: 1039 loss: 8.63690047e-07
Iter: 1040 loss: 8.62946308e-07
Iter: 1041 loss: 8.63024468e-07
Iter: 1042 loss: 8.62354511e-07
Iter: 1043 loss: 8.61372314e-07
Iter: 1044 loss: 8.64724484e-07
Iter: 1045 loss: 8.61143917e-07
Iter: 1046 loss: 8.60144382e-07
Iter: 1047 loss: 8.61189619e-07
Iter: 1048 loss: 8.59571514e-07
Iter: 1049 loss: 8.58482792e-07
Iter: 1050 loss: 8.60780688e-07
Iter: 1051 loss: 8.58061867e-07
Iter: 1052 loss: 8.57250825e-07
Iter: 1053 loss: 8.70891654e-07
Iter: 1054 loss: 8.57248892e-07
Iter: 1055 loss: 8.56445865e-07
Iter: 1056 loss: 8.57145324e-07
Iter: 1057 loss: 8.55901e-07
Iter: 1058 loss: 8.55193548e-07
Iter: 1059 loss: 8.58345743e-07
Iter: 1060 loss: 8.55026713e-07
Iter: 1061 loss: 8.54322309e-07
Iter: 1062 loss: 8.55011308e-07
Iter: 1063 loss: 8.53850452e-07
Iter: 1064 loss: 8.53044867e-07
Iter: 1065 loss: 8.52584094e-07
Iter: 1066 loss: 8.52250537e-07
Iter: 1067 loss: 8.51075e-07
Iter: 1068 loss: 8.59704755e-07
Iter: 1069 loss: 8.50990773e-07
Iter: 1070 loss: 8.50220317e-07
Iter: 1071 loss: 8.51868606e-07
Iter: 1072 loss: 8.49918081e-07
Iter: 1073 loss: 8.48984e-07
Iter: 1074 loss: 8.54146322e-07
Iter: 1075 loss: 8.48899504e-07
Iter: 1076 loss: 8.48178047e-07
Iter: 1077 loss: 8.49489084e-07
Iter: 1078 loss: 8.47860406e-07
Iter: 1079 loss: 8.47076194e-07
Iter: 1080 loss: 8.48163722e-07
Iter: 1081 loss: 8.46632815e-07
Iter: 1082 loss: 8.45625323e-07
Iter: 1083 loss: 8.49079072e-07
Iter: 1084 loss: 8.45396244e-07
Iter: 1085 loss: 8.44649264e-07
Iter: 1086 loss: 8.43840382e-07
Iter: 1087 loss: 8.43683665e-07
Iter: 1088 loss: 8.42635e-07
Iter: 1089 loss: 8.58309079e-07
Iter: 1090 loss: 8.4258545e-07
Iter: 1091 loss: 8.41908786e-07
Iter: 1092 loss: 8.47237629e-07
Iter: 1093 loss: 8.41871383e-07
Iter: 1094 loss: 8.41250483e-07
Iter: 1095 loss: 8.42912641e-07
Iter: 1096 loss: 8.41043175e-07
Iter: 1097 loss: 8.40506686e-07
Iter: 1098 loss: 8.3970815e-07
Iter: 1099 loss: 8.39622771e-07
Iter: 1100 loss: 8.38855499e-07
Iter: 1101 loss: 8.51127083e-07
Iter: 1102 loss: 8.38833671e-07
Iter: 1103 loss: 8.38242158e-07
Iter: 1104 loss: 8.38401547e-07
Iter: 1105 loss: 8.37816287e-07
Iter: 1106 loss: 8.37025368e-07
Iter: 1107 loss: 8.36997685e-07
Iter: 1108 loss: 8.36406741e-07
Iter: 1109 loss: 8.35255662e-07
Iter: 1110 loss: 8.38050653e-07
Iter: 1111 loss: 8.34819559e-07
Iter: 1112 loss: 8.3413363e-07
Iter: 1113 loss: 8.3410157e-07
Iter: 1114 loss: 8.33403078e-07
Iter: 1115 loss: 8.33188778e-07
Iter: 1116 loss: 8.32784792e-07
Iter: 1117 loss: 8.32003423e-07
Iter: 1118 loss: 8.36814422e-07
Iter: 1119 loss: 8.31872399e-07
Iter: 1120 loss: 8.31236207e-07
Iter: 1121 loss: 8.31472335e-07
Iter: 1122 loss: 8.30822557e-07
Iter: 1123 loss: 8.2978886e-07
Iter: 1124 loss: 8.32669571e-07
Iter: 1125 loss: 8.29473436e-07
Iter: 1126 loss: 8.2873089e-07
Iter: 1127 loss: 8.31686691e-07
Iter: 1128 loss: 8.28576447e-07
Iter: 1129 loss: 8.27941676e-07
Iter: 1130 loss: 8.35631226e-07
Iter: 1131 loss: 8.27896e-07
Iter: 1132 loss: 8.27375e-07
Iter: 1133 loss: 8.26418159e-07
Iter: 1134 loss: 8.26431915e-07
Iter: 1135 loss: 8.25524e-07
Iter: 1136 loss: 8.32607839e-07
Iter: 1137 loss: 8.25435222e-07
Iter: 1138 loss: 8.24791186e-07
Iter: 1139 loss: 8.26397468e-07
Iter: 1140 loss: 8.24543065e-07
Iter: 1141 loss: 8.2386282e-07
Iter: 1142 loss: 8.25931124e-07
Iter: 1143 loss: 8.23642381e-07
Iter: 1144 loss: 8.22993456e-07
Iter: 1145 loss: 8.2302779e-07
Iter: 1146 loss: 8.22555251e-07
Iter: 1147 loss: 8.21582489e-07
Iter: 1148 loss: 8.24031e-07
Iter: 1149 loss: 8.21255412e-07
Iter: 1150 loss: 8.20255423e-07
Iter: 1151 loss: 8.23627886e-07
Iter: 1152 loss: 8.19944603e-07
Iter: 1153 loss: 8.19274305e-07
Iter: 1154 loss: 8.24384642e-07
Iter: 1155 loss: 8.19164e-07
Iter: 1156 loss: 8.18382659e-07
Iter: 1157 loss: 8.18529088e-07
Iter: 1158 loss: 8.17815192e-07
Iter: 1159 loss: 8.16998181e-07
Iter: 1160 loss: 8.19160732e-07
Iter: 1161 loss: 8.16689067e-07
Iter: 1162 loss: 8.15922306e-07
Iter: 1163 loss: 8.1716405e-07
Iter: 1164 loss: 8.15581132e-07
Iter: 1165 loss: 8.14579607e-07
Iter: 1166 loss: 8.21447486e-07
Iter: 1167 loss: 8.14449322e-07
Iter: 1168 loss: 8.13832855e-07
Iter: 1169 loss: 8.19460752e-07
Iter: 1170 loss: 8.13801762e-07
Iter: 1171 loss: 8.13411361e-07
Iter: 1172 loss: 8.13025167e-07
Iter: 1173 loss: 8.12895223e-07
Iter: 1174 loss: 8.12234816e-07
Iter: 1175 loss: 8.11402856e-07
Iter: 1176 loss: 8.11309633e-07
Iter: 1177 loss: 8.10783661e-07
Iter: 1178 loss: 8.106461e-07
Iter: 1179 loss: 8.10180836e-07
Iter: 1180 loss: 8.10195615e-07
Iter: 1181 loss: 8.09773439e-07
Iter: 1182 loss: 8.09032485e-07
Iter: 1183 loss: 8.09320397e-07
Iter: 1184 loss: 8.08507139e-07
Iter: 1185 loss: 8.0758673e-07
Iter: 1186 loss: 8.10591132e-07
Iter: 1187 loss: 8.07347419e-07
Iter: 1188 loss: 8.06481125e-07
Iter: 1189 loss: 8.10370352e-07
Iter: 1190 loss: 8.06241303e-07
Iter: 1191 loss: 8.05524792e-07
Iter: 1192 loss: 8.08847176e-07
Iter: 1193 loss: 8.05370576e-07
Iter: 1194 loss: 8.04613819e-07
Iter: 1195 loss: 8.06240223e-07
Iter: 1196 loss: 8.04348417e-07
Iter: 1197 loss: 8.03716489e-07
Iter: 1198 loss: 8.04723641e-07
Iter: 1199 loss: 8.03434e-07
Iter: 1200 loss: 8.02728096e-07
Iter: 1201 loss: 8.03706143e-07
Iter: 1202 loss: 8.0233815e-07
Iter: 1203 loss: 8.01812746e-07
Iter: 1204 loss: 8.0174334e-07
Iter: 1205 loss: 8.01324575e-07
Iter: 1206 loss: 8.00626083e-07
Iter: 1207 loss: 8.00567818e-07
Iter: 1208 loss: 7.99742338e-07
Iter: 1209 loss: 8.02445925e-07
Iter: 1210 loss: 7.99526788e-07
Iter: 1211 loss: 7.988383e-07
Iter: 1212 loss: 8.01524379e-07
Iter: 1213 loss: 7.98736608e-07
Iter: 1214 loss: 7.9805875e-07
Iter: 1215 loss: 7.98095186e-07
Iter: 1216 loss: 7.97566145e-07
Iter: 1217 loss: 7.96606287e-07
Iter: 1218 loss: 8.01482031e-07
Iter: 1219 loss: 7.96449058e-07
Iter: 1220 loss: 7.95698384e-07
Iter: 1221 loss: 7.99877171e-07
Iter: 1222 loss: 7.95604308e-07
Iter: 1223 loss: 7.94998869e-07
Iter: 1224 loss: 7.9489314e-07
Iter: 1225 loss: 7.94536277e-07
Iter: 1226 loss: 7.93734159e-07
Iter: 1227 loss: 7.94093694e-07
Iter: 1228 loss: 7.93219158e-07
Iter: 1229 loss: 7.92375545e-07
Iter: 1230 loss: 8.00218686e-07
Iter: 1231 loss: 7.92291644e-07
Iter: 1232 loss: 7.91600883e-07
Iter: 1233 loss: 7.94941457e-07
Iter: 1234 loss: 7.91493278e-07
Iter: 1235 loss: 7.90756587e-07
Iter: 1236 loss: 7.91810635e-07
Iter: 1237 loss: 7.90419108e-07
Iter: 1238 loss: 7.89738635e-07
Iter: 1239 loss: 7.90344188e-07
Iter: 1240 loss: 7.89354544e-07
Iter: 1241 loss: 7.88665716e-07
Iter: 1242 loss: 7.8868004e-07
Iter: 1243 loss: 7.88177772e-07
Iter: 1244 loss: 7.88206421e-07
Iter: 1245 loss: 7.87752811e-07
Iter: 1246 loss: 7.87160502e-07
Iter: 1247 loss: 7.88027e-07
Iter: 1248 loss: 7.86787382e-07
Iter: 1249 loss: 7.8626465e-07
Iter: 1250 loss: 7.87545e-07
Iter: 1251 loss: 7.86058877e-07
Iter: 1252 loss: 7.85367831e-07
Iter: 1253 loss: 7.86487817e-07
Iter: 1254 loss: 7.85081056e-07
Iter: 1255 loss: 7.84265922e-07
Iter: 1256 loss: 7.8818681e-07
Iter: 1257 loss: 7.84112444e-07
Iter: 1258 loss: 7.83534063e-07
Iter: 1259 loss: 7.85333611e-07
Iter: 1260 loss: 7.83363703e-07
Iter: 1261 loss: 7.82777818e-07
Iter: 1262 loss: 7.8372085e-07
Iter: 1263 loss: 7.82526399e-07
Iter: 1264 loss: 7.81772542e-07
Iter: 1265 loss: 7.82585062e-07
Iter: 1266 loss: 7.81361564e-07
Iter: 1267 loss: 7.80679898e-07
Iter: 1268 loss: 7.83176574e-07
Iter: 1269 loss: 7.80438313e-07
Iter: 1270 loss: 7.79660581e-07
Iter: 1271 loss: 7.80164896e-07
Iter: 1272 loss: 7.7921834e-07
Iter: 1273 loss: 7.78435094e-07
Iter: 1274 loss: 7.88265197e-07
Iter: 1275 loss: 7.78463232e-07
Iter: 1276 loss: 7.77872515e-07
Iter: 1277 loss: 7.7937284e-07
Iter: 1278 loss: 7.77664e-07
Iter: 1279 loss: 7.77233709e-07
Iter: 1280 loss: 7.79477659e-07
Iter: 1281 loss: 7.7721495e-07
Iter: 1282 loss: 7.76692104e-07
Iter: 1283 loss: 7.76849902e-07
Iter: 1284 loss: 7.76294e-07
Iter: 1285 loss: 7.7572804e-07
Iter: 1286 loss: 7.75044782e-07
Iter: 1287 loss: 7.74953833e-07
Iter: 1288 loss: 7.73980901e-07
Iter: 1289 loss: 7.76010438e-07
Iter: 1290 loss: 7.73630347e-07
Iter: 1291 loss: 7.72593069e-07
Iter: 1292 loss: 7.81628e-07
Iter: 1293 loss: 7.72582666e-07
Iter: 1294 loss: 7.72106e-07
Iter: 1295 loss: 7.76087177e-07
Iter: 1296 loss: 7.72028329e-07
Iter: 1297 loss: 7.71539362e-07
Iter: 1298 loss: 7.71436476e-07
Iter: 1299 loss: 7.71072223e-07
Iter: 1300 loss: 7.70406e-07
Iter: 1301 loss: 7.71835744e-07
Iter: 1302 loss: 7.70146812e-07
Iter: 1303 loss: 7.69466965e-07
Iter: 1304 loss: 7.71879627e-07
Iter: 1305 loss: 7.69192411e-07
Iter: 1306 loss: 7.68500968e-07
Iter: 1307 loss: 7.69494648e-07
Iter: 1308 loss: 7.68195264e-07
Iter: 1309 loss: 7.67482902e-07
Iter: 1310 loss: 7.69948542e-07
Iter: 1311 loss: 7.67313e-07
Iter: 1312 loss: 7.66584265e-07
Iter: 1313 loss: 7.72749445e-07
Iter: 1314 loss: 7.66577386e-07
Iter: 1315 loss: 7.66072958e-07
Iter: 1316 loss: 7.66882636e-07
Iter: 1317 loss: 7.6586e-07
Iter: 1318 loss: 7.65300911e-07
Iter: 1319 loss: 7.68071914e-07
Iter: 1320 loss: 7.65207e-07
Iter: 1321 loss: 7.64760046e-07
Iter: 1322 loss: 7.64446327e-07
Iter: 1323 loss: 7.64313029e-07
Iter: 1324 loss: 7.63639434e-07
Iter: 1325 loss: 7.6806e-07
Iter: 1326 loss: 7.63590492e-07
Iter: 1327 loss: 7.63074922e-07
Iter: 1328 loss: 7.62893e-07
Iter: 1329 loss: 7.62686682e-07
Iter: 1330 loss: 7.61895762e-07
Iter: 1331 loss: 7.64845822e-07
Iter: 1332 loss: 7.61691126e-07
Iter: 1333 loss: 7.61016508e-07
Iter: 1334 loss: 7.63212483e-07
Iter: 1335 loss: 7.60829323e-07
Iter: 1336 loss: 7.60249577e-07
Iter: 1337 loss: 7.64410231e-07
Iter: 1338 loss: 7.60152602e-07
Iter: 1339 loss: 7.59623e-07
Iter: 1340 loss: 7.59904822e-07
Iter: 1341 loss: 7.59337865e-07
Iter: 1342 loss: 7.58616238e-07
Iter: 1343 loss: 7.58456167e-07
Iter: 1344 loss: 7.58019155e-07
Iter: 1345 loss: 7.57283658e-07
Iter: 1346 loss: 7.64115214e-07
Iter: 1347 loss: 7.57310545e-07
Iter: 1348 loss: 7.56820896e-07
Iter: 1349 loss: 7.59420232e-07
Iter: 1350 loss: 7.56791223e-07
Iter: 1351 loss: 7.56174188e-07
Iter: 1352 loss: 7.56606937e-07
Iter: 1353 loss: 7.55806752e-07
Iter: 1354 loss: 7.55241729e-07
Iter: 1355 loss: 7.60122305e-07
Iter: 1356 loss: 7.55241501e-07
Iter: 1357 loss: 7.54816256e-07
Iter: 1358 loss: 7.5485741e-07
Iter: 1359 loss: 7.54464622e-07
Iter: 1360 loss: 7.53930067e-07
Iter: 1361 loss: 7.55030442e-07
Iter: 1362 loss: 7.53769598e-07
Iter: 1363 loss: 7.53125278e-07
Iter: 1364 loss: 7.53591735e-07
Iter: 1365 loss: 7.52708615e-07
Iter: 1366 loss: 7.52020924e-07
Iter: 1367 loss: 7.55045392e-07
Iter: 1368 loss: 7.51866537e-07
Iter: 1369 loss: 7.51167477e-07
Iter: 1370 loss: 7.5240041e-07
Iter: 1371 loss: 7.50924642e-07
Iter: 1372 loss: 7.50264348e-07
Iter: 1373 loss: 7.54477696e-07
Iter: 1374 loss: 7.50258778e-07
Iter: 1375 loss: 7.49705578e-07
Iter: 1376 loss: 7.49663286e-07
Iter: 1377 loss: 7.4925515e-07
Iter: 1378 loss: 7.48525451e-07
Iter: 1379 loss: 7.55311248e-07
Iter: 1380 loss: 7.48504931e-07
Iter: 1381 loss: 7.48095658e-07
Iter: 1382 loss: 7.47456909e-07
Iter: 1383 loss: 7.47462877e-07
Iter: 1384 loss: 7.4652695e-07
Iter: 1385 loss: 7.50042773e-07
Iter: 1386 loss: 7.46317937e-07
Iter: 1387 loss: 7.45880243e-07
Iter: 1388 loss: 7.45828743e-07
Iter: 1389 loss: 7.45396505e-07
Iter: 1390 loss: 7.45036061e-07
Iter: 1391 loss: 7.44968759e-07
Iter: 1392 loss: 7.44433237e-07
Iter: 1393 loss: 7.45890702e-07
Iter: 1394 loss: 7.44202225e-07
Iter: 1395 loss: 7.43507087e-07
Iter: 1396 loss: 7.45707098e-07
Iter: 1397 loss: 7.4328824e-07
Iter: 1398 loss: 7.42801433e-07
Iter: 1399 loss: 7.4292052e-07
Iter: 1400 loss: 7.42499537e-07
Iter: 1401 loss: 7.41822305e-07
Iter: 1402 loss: 7.43827513e-07
Iter: 1403 loss: 7.41673091e-07
Iter: 1404 loss: 7.41111535e-07
Iter: 1405 loss: 7.44031581e-07
Iter: 1406 loss: 7.40994665e-07
Iter: 1407 loss: 7.40512178e-07
Iter: 1408 loss: 7.41561621e-07
Iter: 1409 loss: 7.40315841e-07
Iter: 1410 loss: 7.39643724e-07
Iter: 1411 loss: 7.39792767e-07
Iter: 1412 loss: 7.39166808e-07
Iter: 1413 loss: 7.38442168e-07
Iter: 1414 loss: 7.44118438e-07
Iter: 1415 loss: 7.3838e-07
Iter: 1416 loss: 7.37827e-07
Iter: 1417 loss: 7.40521045e-07
Iter: 1418 loss: 7.37758512e-07
Iter: 1419 loss: 7.37345545e-07
Iter: 1420 loss: 7.3716285e-07
Iter: 1421 loss: 7.369336e-07
Iter: 1422 loss: 7.36255117e-07
Iter: 1423 loss: 7.39140773e-07
Iter: 1424 loss: 7.36097036e-07
Iter: 1425 loss: 7.35603635e-07
Iter: 1426 loss: 7.39358484e-07
Iter: 1427 loss: 7.35561798e-07
Iter: 1428 loss: 7.35018261e-07
Iter: 1429 loss: 7.36198331e-07
Iter: 1430 loss: 7.34802597e-07
Iter: 1431 loss: 7.34350465e-07
Iter: 1432 loss: 7.34466767e-07
Iter: 1433 loss: 7.34114451e-07
Iter: 1434 loss: 7.33556533e-07
Iter: 1435 loss: 7.34964146e-07
Iter: 1436 loss: 7.33416414e-07
Iter: 1437 loss: 7.32895046e-07
Iter: 1438 loss: 7.37058485e-07
Iter: 1439 loss: 7.3289641e-07
Iter: 1440 loss: 7.32499245e-07
Iter: 1441 loss: 7.31927059e-07
Iter: 1442 loss: 7.3194326e-07
Iter: 1443 loss: 7.31142791e-07
Iter: 1444 loss: 7.33753268e-07
Iter: 1445 loss: 7.30934516e-07
Iter: 1446 loss: 7.30345505e-07
Iter: 1447 loss: 7.31307864e-07
Iter: 1448 loss: 7.30007741e-07
Iter: 1449 loss: 7.29308908e-07
Iter: 1450 loss: 7.37051209e-07
Iter: 1451 loss: 7.29331191e-07
Iter: 1452 loss: 7.28847226e-07
Iter: 1453 loss: 7.29676515e-07
Iter: 1454 loss: 7.28651344e-07
Iter: 1455 loss: 7.28092687e-07
Iter: 1456 loss: 7.29691408e-07
Iter: 1457 loss: 7.28014413e-07
Iter: 1458 loss: 7.27458087e-07
Iter: 1459 loss: 7.28833641e-07
Iter: 1460 loss: 7.27304382e-07
Iter: 1461 loss: 7.26834571e-07
Iter: 1462 loss: 7.28188525e-07
Iter: 1463 loss: 7.26664041e-07
Iter: 1464 loss: 7.26277733e-07
Iter: 1465 loss: 7.2997284e-07
Iter: 1466 loss: 7.26248345e-07
Iter: 1467 loss: 7.2588665e-07
Iter: 1468 loss: 7.25288146e-07
Iter: 1469 loss: 7.25290136e-07
Iter: 1470 loss: 7.24764107e-07
Iter: 1471 loss: 7.2555008e-07
Iter: 1472 loss: 7.24539291e-07
Iter: 1473 loss: 7.23890935e-07
Iter: 1474 loss: 7.27551253e-07
Iter: 1475 loss: 7.23781852e-07
Iter: 1476 loss: 7.23195399e-07
Iter: 1477 loss: 7.24968686e-07
Iter: 1478 loss: 7.23096719e-07
Iter: 1479 loss: 7.22668858e-07
Iter: 1480 loss: 7.22853656e-07
Iter: 1481 loss: 7.22371169e-07
Iter: 1482 loss: 7.21791821e-07
Iter: 1483 loss: 7.23101152e-07
Iter: 1484 loss: 7.2155342e-07
Iter: 1485 loss: 7.20948947e-07
Iter: 1486 loss: 7.2258797e-07
Iter: 1487 loss: 7.20763694e-07
Iter: 1488 loss: 7.20237153e-07
Iter: 1489 loss: 7.21616289e-07
Iter: 1490 loss: 7.20015578e-07
Iter: 1491 loss: 7.1943839e-07
Iter: 1492 loss: 7.23340236e-07
Iter: 1493 loss: 7.19386719e-07
Iter: 1494 loss: 7.18929698e-07
Iter: 1495 loss: 7.19380807e-07
Iter: 1496 loss: 7.18570163e-07
Iter: 1497 loss: 7.18094213e-07
Iter: 1498 loss: 7.19632794e-07
Iter: 1499 loss: 7.17950286e-07
Iter: 1500 loss: 7.17360933e-07
Iter: 1501 loss: 7.20568e-07
Iter: 1502 loss: 7.17313e-07
Iter: 1503 loss: 7.16762543e-07
Iter: 1504 loss: 7.18258093e-07
Iter: 1505 loss: 7.16698537e-07
Iter: 1506 loss: 7.16213663e-07
Iter: 1507 loss: 7.16481736e-07
Iter: 1508 loss: 7.15925808e-07
Iter: 1509 loss: 7.15456167e-07
Iter: 1510 loss: 7.15985152e-07
Iter: 1511 loss: 7.15169506e-07
Iter: 1512 loss: 7.14522116e-07
Iter: 1513 loss: 7.15786598e-07
Iter: 1514 loss: 7.14252735e-07
Iter: 1515 loss: 7.13877967e-07
Iter: 1516 loss: 7.1385989e-07
Iter: 1517 loss: 7.13570785e-07
Iter: 1518 loss: 7.12944939e-07
Iter: 1519 loss: 7.22607183e-07
Iter: 1520 loss: 7.12827955e-07
Iter: 1521 loss: 7.12146402e-07
Iter: 1522 loss: 7.18874844e-07
Iter: 1523 loss: 7.12146175e-07
Iter: 1524 loss: 7.11675057e-07
Iter: 1525 loss: 7.11409257e-07
Iter: 1526 loss: 7.11156417e-07
Iter: 1527 loss: 7.10569623e-07
Iter: 1528 loss: 7.10576728e-07
Iter: 1529 loss: 7.10219638e-07
Iter: 1530 loss: 7.10622089e-07
Iter: 1531 loss: 7.10004031e-07
Iter: 1532 loss: 7.0948056e-07
Iter: 1533 loss: 7.10618622e-07
Iter: 1534 loss: 7.09261144e-07
Iter: 1535 loss: 7.08741823e-07
Iter: 1536 loss: 7.10709458e-07
Iter: 1537 loss: 7.08598805e-07
Iter: 1538 loss: 7.08202265e-07
Iter: 1539 loss: 7.12035217e-07
Iter: 1540 loss: 7.08166056e-07
Iter: 1541 loss: 7.07874e-07
Iter: 1542 loss: 7.07571758e-07
Iter: 1543 loss: 7.07491381e-07
Iter: 1544 loss: 7.06961487e-07
Iter: 1545 loss: 7.07974095e-07
Iter: 1546 loss: 7.06696255e-07
Iter: 1547 loss: 7.06268e-07
Iter: 1548 loss: 7.10154382e-07
Iter: 1549 loss: 7.0622059e-07
Iter: 1550 loss: 7.05824732e-07
Iter: 1551 loss: 7.052206e-07
Iter: 1552 loss: 7.05255843e-07
Iter: 1553 loss: 7.04605e-07
Iter: 1554 loss: 7.04579747e-07
Iter: 1555 loss: 7.04208048e-07
Iter: 1556 loss: 7.04388469e-07
Iter: 1557 loss: 7.03989826e-07
Iter: 1558 loss: 7.03450269e-07
Iter: 1559 loss: 7.02852731e-07
Iter: 1560 loss: 7.02787361e-07
Iter: 1561 loss: 7.02155376e-07
Iter: 1562 loss: 7.09118126e-07
Iter: 1563 loss: 7.02115301e-07
Iter: 1564 loss: 7.01628665e-07
Iter: 1565 loss: 7.02042485e-07
Iter: 1566 loss: 7.01350928e-07
Iter: 1567 loss: 7.00701719e-07
Iter: 1568 loss: 7.04843558e-07
Iter: 1569 loss: 7.00647149e-07
Iter: 1570 loss: 7.00084513e-07
Iter: 1571 loss: 7.03558726e-07
Iter: 1572 loss: 7.00040403e-07
Iter: 1573 loss: 6.99747716e-07
Iter: 1574 loss: 7.00741907e-07
Iter: 1575 loss: 6.99654834e-07
Iter: 1576 loss: 6.99204747e-07
Iter: 1577 loss: 6.99543364e-07
Iter: 1578 loss: 6.99037628e-07
Iter: 1579 loss: 6.98538656e-07
Iter: 1580 loss: 6.98805138e-07
Iter: 1581 loss: 6.98249607e-07
Iter: 1582 loss: 6.97720907e-07
Iter: 1583 loss: 6.99429222e-07
Iter: 1584 loss: 6.97563905e-07
Iter: 1585 loss: 6.9698666e-07
Iter: 1586 loss: 6.98095107e-07
Iter: 1587 loss: 6.96790721e-07
Iter: 1588 loss: 6.96318e-07
Iter: 1589 loss: 6.99378688e-07
Iter: 1590 loss: 6.96246843e-07
Iter: 1591 loss: 6.95841322e-07
Iter: 1592 loss: 6.96367579e-07
Iter: 1593 loss: 6.95611675e-07
Iter: 1594 loss: 6.95080359e-07
Iter: 1595 loss: 6.96915095e-07
Iter: 1596 loss: 6.94939104e-07
Iter: 1597 loss: 6.94493963e-07
Iter: 1598 loss: 6.94873961e-07
Iter: 1599 loss: 6.94194114e-07
Iter: 1600 loss: 6.93678089e-07
Iter: 1601 loss: 6.96194206e-07
Iter: 1602 loss: 6.93642107e-07
Iter: 1603 loss: 6.93144671e-07
Iter: 1604 loss: 6.93155243e-07
Iter: 1605 loss: 6.92799063e-07
Iter: 1606 loss: 6.92312312e-07
Iter: 1607 loss: 6.95532094e-07
Iter: 1608 loss: 6.92215167e-07
Iter: 1609 loss: 6.91772584e-07
Iter: 1610 loss: 6.95611106e-07
Iter: 1611 loss: 6.91731316e-07
Iter: 1612 loss: 6.91357855e-07
Iter: 1613 loss: 6.91713296e-07
Iter: 1614 loss: 6.91123887e-07
Iter: 1615 loss: 6.9068426e-07
Iter: 1616 loss: 6.92829644e-07
Iter: 1617 loss: 6.90623949e-07
Iter: 1618 loss: 6.90259185e-07
Iter: 1619 loss: 6.90154707e-07
Iter: 1620 loss: 6.89901867e-07
Iter: 1621 loss: 6.89421086e-07
Iter: 1622 loss: 6.89210651e-07
Iter: 1623 loss: 6.88911541e-07
Iter: 1624 loss: 6.88434341e-07
Iter: 1625 loss: 6.8841257e-07
Iter: 1626 loss: 6.88027569e-07
Iter: 1627 loss: 6.88739306e-07
Iter: 1628 loss: 6.87885631e-07
Iter: 1629 loss: 6.87462148e-07
Iter: 1630 loss: 6.88088335e-07
Iter: 1631 loss: 6.87237048e-07
Iter: 1632 loss: 6.86779117e-07
Iter: 1633 loss: 6.88747946e-07
Iter: 1634 loss: 6.86671626e-07
Iter: 1635 loss: 6.86215856e-07
Iter: 1636 loss: 6.86316639e-07
Iter: 1637 loss: 6.85850523e-07
Iter: 1638 loss: 6.85370082e-07
Iter: 1639 loss: 6.86165663e-07
Iter: 1640 loss: 6.85091265e-07
Iter: 1641 loss: 6.84436714e-07
Iter: 1642 loss: 6.8842769e-07
Iter: 1643 loss: 6.84325585e-07
Iter: 1644 loss: 6.83898577e-07
Iter: 1645 loss: 6.86667363e-07
Iter: 1646 loss: 6.83844405e-07
Iter: 1647 loss: 6.83484586e-07
Iter: 1648 loss: 6.85058581e-07
Iter: 1649 loss: 6.83384883e-07
Iter: 1650 loss: 6.83044505e-07
Iter: 1651 loss: 6.82995903e-07
Iter: 1652 loss: 6.82720952e-07
Iter: 1653 loss: 6.82357552e-07
Iter: 1654 loss: 6.85145e-07
Iter: 1655 loss: 6.822724e-07
Iter: 1656 loss: 6.8192378e-07
Iter: 1657 loss: 6.81545657e-07
Iter: 1658 loss: 6.81436575e-07
Iter: 1659 loss: 6.80879225e-07
Iter: 1660 loss: 6.82225e-07
Iter: 1661 loss: 6.80689197e-07
Iter: 1662 loss: 6.80161e-07
Iter: 1663 loss: 6.82841346e-07
Iter: 1664 loss: 6.80041808e-07
Iter: 1665 loss: 6.79607069e-07
Iter: 1666 loss: 6.81120298e-07
Iter: 1667 loss: 6.79444383e-07
Iter: 1668 loss: 6.78952347e-07
Iter: 1669 loss: 6.80822609e-07
Iter: 1670 loss: 6.78836841e-07
Iter: 1671 loss: 6.78425749e-07
Iter: 1672 loss: 6.78368906e-07
Iter: 1673 loss: 6.78123342e-07
Iter: 1674 loss: 6.77459184e-07
Iter: 1675 loss: 6.79947846e-07
Iter: 1676 loss: 6.77381081e-07
Iter: 1677 loss: 6.76826858e-07
Iter: 1678 loss: 6.78512777e-07
Iter: 1679 loss: 6.76671107e-07
Iter: 1680 loss: 6.76183902e-07
Iter: 1681 loss: 6.7760709e-07
Iter: 1682 loss: 6.76054697e-07
Iter: 1683 loss: 6.7565594e-07
Iter: 1684 loss: 6.80679705e-07
Iter: 1685 loss: 6.75639285e-07
Iter: 1686 loss: 6.75319939e-07
Iter: 1687 loss: 6.75209549e-07
Iter: 1688 loss: 6.75031401e-07
Iter: 1689 loss: 6.74624289e-07
Iter: 1690 loss: 6.7499235e-07
Iter: 1691 loss: 6.74411694e-07
Iter: 1692 loss: 6.73820068e-07
Iter: 1693 loss: 6.76650473e-07
Iter: 1694 loss: 6.73730142e-07
Iter: 1695 loss: 6.73398517e-07
Iter: 1696 loss: 6.73660168e-07
Iter: 1697 loss: 6.73103443e-07
Iter: 1698 loss: 6.72713838e-07
Iter: 1699 loss: 6.7429346e-07
Iter: 1700 loss: 6.72590431e-07
Iter: 1701 loss: 6.72092483e-07
Iter: 1702 loss: 6.72027795e-07
Iter: 1703 loss: 6.71670875e-07
Iter: 1704 loss: 6.71208113e-07
Iter: 1705 loss: 6.78362767e-07
Iter: 1706 loss: 6.71175712e-07
Iter: 1707 loss: 6.70757061e-07
Iter: 1708 loss: 6.71584246e-07
Iter: 1709 loss: 6.70612337e-07
Iter: 1710 loss: 6.70211e-07
Iter: 1711 loss: 6.69915835e-07
Iter: 1712 loss: 6.6983273e-07
Iter: 1713 loss: 6.69233714e-07
Iter: 1714 loss: 6.73855766e-07
Iter: 1715 loss: 6.69185624e-07
Iter: 1716 loss: 6.68834787e-07
Iter: 1717 loss: 6.70902864e-07
Iter: 1718 loss: 6.68763619e-07
Iter: 1719 loss: 6.68337066e-07
Iter: 1720 loss: 6.69402539e-07
Iter: 1721 loss: 6.68236453e-07
Iter: 1722 loss: 6.67819108e-07
Iter: 1723 loss: 6.689163e-07
Iter: 1724 loss: 6.67596396e-07
Iter: 1725 loss: 6.67245445e-07
Iter: 1726 loss: 6.67382153e-07
Iter: 1727 loss: 6.66970777e-07
Iter: 1728 loss: 6.66556843e-07
Iter: 1729 loss: 6.68636062e-07
Iter: 1730 loss: 6.66467884e-07
Iter: 1731 loss: 6.66091807e-07
Iter: 1732 loss: 6.66755511e-07
Iter: 1733 loss: 6.65922e-07
Iter: 1734 loss: 6.65476705e-07
Iter: 1735 loss: 6.65401387e-07
Iter: 1736 loss: 6.65099e-07
Iter: 1737 loss: 6.64613424e-07
Iter: 1738 loss: 6.69211772e-07
Iter: 1739 loss: 6.64616437e-07
Iter: 1740 loss: 6.64223876e-07
Iter: 1741 loss: 6.64762524e-07
Iter: 1742 loss: 6.63992353e-07
Iter: 1743 loss: 6.6350492e-07
Iter: 1744 loss: 6.65425887e-07
Iter: 1745 loss: 6.63383844e-07
Iter: 1746 loss: 6.62988668e-07
Iter: 1747 loss: 6.64692436e-07
Iter: 1748 loss: 6.62904199e-07
Iter: 1749 loss: 6.62569562e-07
Iter: 1750 loss: 6.62092475e-07
Iter: 1751 loss: 6.6204575e-07
Iter: 1752 loss: 6.61484648e-07
Iter: 1753 loss: 6.67228619e-07
Iter: 1754 loss: 6.61498177e-07
Iter: 1755 loss: 6.6116263e-07
Iter: 1756 loss: 6.65502512e-07
Iter: 1757 loss: 6.61167348e-07
Iter: 1758 loss: 6.60871763e-07
Iter: 1759 loss: 6.60753e-07
Iter: 1760 loss: 6.60600335e-07
Iter: 1761 loss: 6.60154342e-07
Iter: 1762 loss: 6.61397735e-07
Iter: 1763 loss: 6.60055093e-07
Iter: 1764 loss: 6.59742909e-07
Iter: 1765 loss: 6.60473688e-07
Iter: 1766 loss: 6.59647526e-07
Iter: 1767 loss: 6.5926838e-07
Iter: 1768 loss: 6.59946068e-07
Iter: 1769 loss: 6.59163334e-07
Iter: 1770 loss: 6.58652084e-07
Iter: 1771 loss: 6.59506895e-07
Iter: 1772 loss: 6.58412773e-07
Iter: 1773 loss: 6.58011231e-07
Iter: 1774 loss: 6.5843318e-07
Iter: 1775 loss: 6.57760268e-07
Iter: 1776 loss: 6.57260614e-07
Iter: 1777 loss: 6.58930219e-07
Iter: 1778 loss: 6.57069336e-07
Iter: 1779 loss: 6.56646478e-07
Iter: 1780 loss: 6.61180366e-07
Iter: 1781 loss: 6.56669101e-07
Iter: 1782 loss: 6.56385623e-07
Iter: 1783 loss: 6.56298653e-07
Iter: 1784 loss: 6.56100667e-07
Iter: 1785 loss: 6.55615452e-07
Iter: 1786 loss: 6.56337249e-07
Iter: 1787 loss: 6.55373697e-07
Iter: 1788 loss: 6.54882115e-07
Iter: 1789 loss: 6.56878115e-07
Iter: 1790 loss: 6.54717951e-07
Iter: 1791 loss: 6.54426572e-07
Iter: 1792 loss: 6.58003273e-07
Iter: 1793 loss: 6.54423161e-07
Iter: 1794 loss: 6.54078804e-07
Iter: 1795 loss: 6.54492851e-07
Iter: 1796 loss: 6.53858137e-07
Iter: 1797 loss: 6.53431e-07
Iter: 1798 loss: 6.53798793e-07
Iter: 1799 loss: 6.53204324e-07
Iter: 1800 loss: 6.52845188e-07
Iter: 1801 loss: 6.5352117e-07
Iter: 1802 loss: 6.52709673e-07
Iter: 1803 loss: 6.52228209e-07
Iter: 1804 loss: 6.53748373e-07
Iter: 1805 loss: 6.52090193e-07
Iter: 1806 loss: 6.51579967e-07
Iter: 1807 loss: 6.52686822e-07
Iter: 1808 loss: 6.51418759e-07
Iter: 1809 loss: 6.50983793e-07
Iter: 1810 loss: 6.52760036e-07
Iter: 1811 loss: 6.50888637e-07
Iter: 1812 loss: 6.50524157e-07
Iter: 1813 loss: 6.51006417e-07
Iter: 1814 loss: 6.50276775e-07
Iter: 1815 loss: 6.49857384e-07
Iter: 1816 loss: 6.50541722e-07
Iter: 1817 loss: 6.49618244e-07
Iter: 1818 loss: 6.49300546e-07
Iter: 1819 loss: 6.54001667e-07
Iter: 1820 loss: 6.49285425e-07
Iter: 1821 loss: 6.48944479e-07
Iter: 1822 loss: 6.48632181e-07
Iter: 1823 loss: 6.4858e-07
Iter: 1824 loss: 6.48045045e-07
Iter: 1825 loss: 6.49072035e-07
Iter: 1826 loss: 6.47790046e-07
Iter: 1827 loss: 6.47452168e-07
Iter: 1828 loss: 6.47456773e-07
Iter: 1829 loss: 6.47165791e-07
Iter: 1830 loss: 6.47279364e-07
Iter: 1831 loss: 6.46948138e-07
Iter: 1832 loss: 6.46496972e-07
Iter: 1833 loss: 6.4679557e-07
Iter: 1834 loss: 6.46235094e-07
Iter: 1835 loss: 6.45791e-07
Iter: 1836 loss: 6.47373099e-07
Iter: 1837 loss: 6.45651653e-07
Iter: 1838 loss: 6.45289788e-07
Iter: 1839 loss: 6.46682111e-07
Iter: 1840 loss: 6.45178943e-07
Iter: 1841 loss: 6.44756426e-07
Iter: 1842 loss: 6.45598902e-07
Iter: 1843 loss: 6.44631371e-07
Iter: 1844 loss: 6.44186571e-07
Iter: 1845 loss: 6.45975319e-07
Iter: 1846 loss: 6.43996145e-07
Iter: 1847 loss: 6.43721819e-07
Iter: 1848 loss: 6.43891497e-07
Iter: 1849 loss: 6.4350138e-07
Iter: 1850 loss: 6.43067949e-07
Iter: 1851 loss: 6.44976694e-07
Iter: 1852 loss: 6.42959094e-07
Iter: 1853 loss: 6.42555733e-07
Iter: 1854 loss: 6.43057717e-07
Iter: 1855 loss: 6.42400096e-07
Iter: 1856 loss: 6.42052896e-07
Iter: 1857 loss: 6.45872e-07
Iter: 1858 loss: 6.42056534e-07
Iter: 1859 loss: 6.4173048e-07
Iter: 1860 loss: 6.41476277e-07
Iter: 1861 loss: 6.41392717e-07
Iter: 1862 loss: 6.40932512e-07
Iter: 1863 loss: 6.43221711e-07
Iter: 1864 loss: 6.40862254e-07
Iter: 1865 loss: 6.40536314e-07
Iter: 1866 loss: 6.45165699e-07
Iter: 1867 loss: 6.40542623e-07
Iter: 1868 loss: 6.40273413e-07
Iter: 1869 loss: 6.39853113e-07
Iter: 1870 loss: 6.39840323e-07
Iter: 1871 loss: 6.39415646e-07
Iter: 1872 loss: 6.40603162e-07
Iter: 1873 loss: 6.39245968e-07
Iter: 1874 loss: 6.38806e-07
Iter: 1875 loss: 6.3982327e-07
Iter: 1876 loss: 6.38640699e-07
Iter: 1877 loss: 6.38170434e-07
Iter: 1878 loss: 6.41872589e-07
Iter: 1879 loss: 6.38164579e-07
Iter: 1880 loss: 6.3778657e-07
Iter: 1881 loss: 6.3771256e-07
Iter: 1882 loss: 6.37493656e-07
Iter: 1883 loss: 6.37039761e-07
Iter: 1884 loss: 6.39777738e-07
Iter: 1885 loss: 6.36989739e-07
Iter: 1886 loss: 6.36632251e-07
Iter: 1887 loss: 6.37104165e-07
Iter: 1888 loss: 6.36407435e-07
Iter: 1889 loss: 6.36002483e-07
Iter: 1890 loss: 6.37465291e-07
Iter: 1891 loss: 6.35893798e-07
Iter: 1892 loss: 6.35503739e-07
Iter: 1893 loss: 6.3699872e-07
Iter: 1894 loss: 6.35395679e-07
Iter: 1895 loss: 6.34998912e-07
Iter: 1896 loss: 6.35754191e-07
Iter: 1897 loss: 6.34811954e-07
Iter: 1898 loss: 6.344643e-07
Iter: 1899 loss: 6.35240326e-07
Iter: 1900 loss: 6.34346691e-07
Iter: 1901 loss: 6.33960212e-07
Iter: 1902 loss: 6.36904133e-07
Iter: 1903 loss: 6.33941227e-07
Iter: 1904 loss: 6.33667753e-07
Iter: 1905 loss: 6.33950492e-07
Iter: 1906 loss: 6.33479e-07
Iter: 1907 loss: 6.33126774e-07
Iter: 1908 loss: 6.32934814e-07
Iter: 1909 loss: 6.3275354e-07
Iter: 1910 loss: 6.32294586e-07
Iter: 1911 loss: 6.33983291e-07
Iter: 1912 loss: 6.32187721e-07
Iter: 1913 loss: 6.31713419e-07
Iter: 1914 loss: 6.32136903e-07
Iter: 1915 loss: 6.31396e-07
Iter: 1916 loss: 6.3103937e-07
Iter: 1917 loss: 6.31010323e-07
Iter: 1918 loss: 6.30765953e-07
Iter: 1919 loss: 6.30743614e-07
Iter: 1920 loss: 6.30539091e-07
Iter: 1921 loss: 6.30161253e-07
Iter: 1922 loss: 6.30110662e-07
Iter: 1923 loss: 6.29909323e-07
Iter: 1924 loss: 6.29385966e-07
Iter: 1925 loss: 6.33609716e-07
Iter: 1926 loss: 6.29334068e-07
Iter: 1927 loss: 6.29008809e-07
Iter: 1928 loss: 6.30395391e-07
Iter: 1929 loss: 6.28967825e-07
Iter: 1930 loss: 6.28619546e-07
Iter: 1931 loss: 6.29116698e-07
Iter: 1932 loss: 6.28416785e-07
Iter: 1933 loss: 6.2805e-07
Iter: 1934 loss: 6.29379201e-07
Iter: 1935 loss: 6.27953796e-07
Iter: 1936 loss: 6.27656618e-07
Iter: 1937 loss: 6.29547344e-07
Iter: 1938 loss: 6.27592215e-07
Iter: 1939 loss: 6.27336703e-07
Iter: 1940 loss: 6.28296448e-07
Iter: 1941 loss: 6.27278155e-07
Iter: 1942 loss: 6.27029351e-07
Iter: 1943 loss: 6.26909e-07
Iter: 1944 loss: 6.26788903e-07
Iter: 1945 loss: 6.26376618e-07
Iter: 1946 loss: 6.26393671e-07
Iter: 1947 loss: 6.26019073e-07
Iter: 1948 loss: 6.25537893e-07
Iter: 1949 loss: 6.27444592e-07
Iter: 1950 loss: 6.25419034e-07
Iter: 1951 loss: 6.25024654e-07
Iter: 1952 loss: 6.27619784e-07
Iter: 1953 loss: 6.24979293e-07
Iter: 1954 loss: 6.24614188e-07
Iter: 1955 loss: 6.26652763e-07
Iter: 1956 loss: 6.24569395e-07
Iter: 1957 loss: 6.24296717e-07
Iter: 1958 loss: 6.24391078e-07
Iter: 1959 loss: 6.24039103e-07
Iter: 1960 loss: 6.23737037e-07
Iter: 1961 loss: 6.24311838e-07
Iter: 1962 loss: 6.23590438e-07
Iter: 1963 loss: 6.23191454e-07
Iter: 1964 loss: 6.25261464e-07
Iter: 1965 loss: 6.23127221e-07
Iter: 1966 loss: 6.22802759e-07
Iter: 1967 loss: 6.23807182e-07
Iter: 1968 loss: 6.227e-07
Iter: 1969 loss: 6.22330163e-07
Iter: 1970 loss: 6.23378412e-07
Iter: 1971 loss: 6.22257858e-07
Iter: 1972 loss: 6.21897811e-07
Iter: 1973 loss: 6.22776327e-07
Iter: 1974 loss: 6.21805214e-07
Iter: 1975 loss: 6.2151048e-07
Iter: 1976 loss: 6.23976575e-07
Iter: 1977 loss: 6.21485583e-07
Iter: 1978 loss: 6.21276115e-07
Iter: 1979 loss: 6.20863148e-07
Iter: 1980 loss: 6.20859453e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script73
+ '[' -r STOP.script73 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi2.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi2.8
+ date
Sat Nov  7 16:08:18 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi2.8/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi2.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi2.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi2.8_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f2_psi0_phi2.8/500_500_500_500_1 --optimizer lbfgs --function f2 --psi 0 --alpha 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f2_psi0_phi2.8_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab4adb510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab4adb378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab4aa4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab4aa4ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab4aa42f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab4a6c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab49bf1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab49bf2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab4a57ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab4999ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab4999840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab4999a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab4918510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab49249d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab48b5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab48b58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab4872510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab484a048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab4873ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab480d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab480d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab47b97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab47dcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4ab47dc8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4a7f2e5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4a7f307ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4a7f2c3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4a7f307ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4a7f28e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4a7f21ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4a7f20e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4a7f20f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4a7f1f4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4a5814e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4a5814ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4a5814e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.03933323e-05
Iter: 2 loss: 1.89784987e-05
Iter: 3 loss: 1.81337491e-05
Iter: 4 loss: 1.75502646e-05
Iter: 5 loss: 1.57432714e-05
Iter: 6 loss: 2.06756231e-05
Iter: 7 loss: 1.51508493e-05
Iter: 8 loss: 1.35214514e-05
Iter: 9 loss: 1.55876205e-05
Iter: 10 loss: 1.26812411e-05
Iter: 11 loss: 1.13947572e-05
Iter: 12 loss: 1.95284229e-05
Iter: 13 loss: 1.12513462e-05
Iter: 14 loss: 1.04928904e-05
Iter: 15 loss: 1.25773731e-05
Iter: 16 loss: 1.02465929e-05
Iter: 17 loss: 9.41363669e-06
Iter: 18 loss: 1.04176825e-05
Iter: 19 loss: 8.97527934e-06
Iter: 20 loss: 8.28456177e-06
Iter: 21 loss: 1.13629885e-05
Iter: 22 loss: 8.14828763e-06
Iter: 23 loss: 7.55073597e-06
Iter: 24 loss: 9.65020809e-06
Iter: 25 loss: 7.3960764e-06
Iter: 26 loss: 6.9354187e-06
Iter: 27 loss: 8.50978358e-06
Iter: 28 loss: 6.81271513e-06
Iter: 29 loss: 6.35784272e-06
Iter: 30 loss: 8.45661634e-06
Iter: 31 loss: 6.27210329e-06
Iter: 32 loss: 5.9498434e-06
Iter: 33 loss: 6.18459535e-06
Iter: 34 loss: 5.75107e-06
Iter: 35 loss: 5.39853318e-06
Iter: 36 loss: 6.44937791e-06
Iter: 37 loss: 5.29128374e-06
Iter: 38 loss: 4.99696762e-06
Iter: 39 loss: 5.57718158e-06
Iter: 40 loss: 4.87556918e-06
Iter: 41 loss: 4.66754773e-06
Iter: 42 loss: 7.03854312e-06
Iter: 43 loss: 4.66386155e-06
Iter: 44 loss: 4.48859282e-06
Iter: 45 loss: 5.82964094e-06
Iter: 46 loss: 4.47527418e-06
Iter: 47 loss: 4.35226866e-06
Iter: 48 loss: 4.37187464e-06
Iter: 49 loss: 4.25946928e-06
Iter: 50 loss: 4.13015368e-06
Iter: 51 loss: 4.33705463e-06
Iter: 52 loss: 4.07015432e-06
Iter: 53 loss: 3.93925529e-06
Iter: 54 loss: 5.11433564e-06
Iter: 55 loss: 3.9330489e-06
Iter: 56 loss: 3.83947508e-06
Iter: 57 loss: 3.87526097e-06
Iter: 58 loss: 3.77456945e-06
Iter: 59 loss: 3.65736582e-06
Iter: 60 loss: 3.83072e-06
Iter: 61 loss: 3.60103e-06
Iter: 62 loss: 3.47900277e-06
Iter: 63 loss: 4.4331332e-06
Iter: 64 loss: 3.47020341e-06
Iter: 65 loss: 3.38853329e-06
Iter: 66 loss: 3.50186974e-06
Iter: 67 loss: 3.34791753e-06
Iter: 68 loss: 3.26229247e-06
Iter: 69 loss: 3.79118364e-06
Iter: 70 loss: 3.25227666e-06
Iter: 71 loss: 3.17939521e-06
Iter: 72 loss: 3.25700921e-06
Iter: 73 loss: 3.13943519e-06
Iter: 74 loss: 3.0657975e-06
Iter: 75 loss: 3.16447699e-06
Iter: 76 loss: 3.02888975e-06
Iter: 77 loss: 2.95468135e-06
Iter: 78 loss: 3.31254705e-06
Iter: 79 loss: 2.94152233e-06
Iter: 80 loss: 2.87990633e-06
Iter: 81 loss: 3.10495034e-06
Iter: 82 loss: 2.86461e-06
Iter: 83 loss: 2.81999087e-06
Iter: 84 loss: 3.41335499e-06
Iter: 85 loss: 2.81979987e-06
Iter: 86 loss: 2.77992513e-06
Iter: 87 loss: 2.77707022e-06
Iter: 88 loss: 2.74725335e-06
Iter: 89 loss: 2.70234e-06
Iter: 90 loss: 2.70973487e-06
Iter: 91 loss: 2.66856614e-06
Iter: 92 loss: 2.61259197e-06
Iter: 93 loss: 2.87321586e-06
Iter: 94 loss: 2.60200363e-06
Iter: 95 loss: 2.55713303e-06
Iter: 96 loss: 2.93418429e-06
Iter: 97 loss: 2.55459895e-06
Iter: 98 loss: 2.51834672e-06
Iter: 99 loss: 2.51577967e-06
Iter: 100 loss: 2.48873266e-06
Iter: 101 loss: 2.44940043e-06
Iter: 102 loss: 2.52279597e-06
Iter: 103 loss: 2.4326464e-06
Iter: 104 loss: 2.39018527e-06
Iter: 105 loss: 2.6788166e-06
Iter: 106 loss: 2.38602024e-06
Iter: 107 loss: 2.35388598e-06
Iter: 108 loss: 2.51608185e-06
Iter: 109 loss: 2.34843037e-06
Iter: 110 loss: 2.32462685e-06
Iter: 111 loss: 2.31471586e-06
Iter: 112 loss: 2.30243609e-06
Iter: 113 loss: 2.26187103e-06
Iter: 114 loss: 2.49726645e-06
Iter: 115 loss: 2.25644453e-06
Iter: 116 loss: 2.23168809e-06
Iter: 117 loss: 2.2258107e-06
Iter: 118 loss: 2.20991024e-06
Iter: 119 loss: 2.17470597e-06
Iter: 120 loss: 2.30149044e-06
Iter: 121 loss: 2.16563103e-06
Iter: 122 loss: 2.14566262e-06
Iter: 123 loss: 2.14323882e-06
Iter: 124 loss: 2.13224394e-06
Iter: 125 loss: 2.11319889e-06
Iter: 126 loss: 2.11318547e-06
Iter: 127 loss: 2.0854518e-06
Iter: 128 loss: 2.11083216e-06
Iter: 129 loss: 2.0692803e-06
Iter: 130 loss: 2.0421121e-06
Iter: 131 loss: 2.25319241e-06
Iter: 132 loss: 2.0401867e-06
Iter: 133 loss: 2.02606952e-06
Iter: 134 loss: 2.12735904e-06
Iter: 135 loss: 2.02481397e-06
Iter: 136 loss: 2.0090074e-06
Iter: 137 loss: 1.9918632e-06
Iter: 138 loss: 1.98933753e-06
Iter: 139 loss: 1.96725523e-06
Iter: 140 loss: 2.08275469e-06
Iter: 141 loss: 1.96374981e-06
Iter: 142 loss: 1.94545873e-06
Iter: 143 loss: 2.00564477e-06
Iter: 144 loss: 1.94046288e-06
Iter: 145 loss: 1.92481866e-06
Iter: 146 loss: 1.99733631e-06
Iter: 147 loss: 1.92164453e-06
Iter: 148 loss: 1.90398919e-06
Iter: 149 loss: 1.92064567e-06
Iter: 150 loss: 1.89386469e-06
Iter: 151 loss: 1.88064405e-06
Iter: 152 loss: 1.92374819e-06
Iter: 153 loss: 1.87705928e-06
Iter: 154 loss: 1.86148498e-06
Iter: 155 loss: 1.8762936e-06
Iter: 156 loss: 1.85260865e-06
Iter: 157 loss: 1.83231975e-06
Iter: 158 loss: 1.92309744e-06
Iter: 159 loss: 1.82839892e-06
Iter: 160 loss: 1.81851306e-06
Iter: 161 loss: 1.81834696e-06
Iter: 162 loss: 1.81087091e-06
Iter: 163 loss: 1.80226664e-06
Iter: 164 loss: 1.80116399e-06
Iter: 165 loss: 1.78716925e-06
Iter: 166 loss: 1.78575215e-06
Iter: 167 loss: 1.77532831e-06
Iter: 168 loss: 1.76037292e-06
Iter: 169 loss: 1.81666474e-06
Iter: 170 loss: 1.75674188e-06
Iter: 171 loss: 1.74435434e-06
Iter: 172 loss: 1.85408339e-06
Iter: 173 loss: 1.74359684e-06
Iter: 174 loss: 1.7301121e-06
Iter: 175 loss: 1.73616127e-06
Iter: 176 loss: 1.72096804e-06
Iter: 177 loss: 1.71067904e-06
Iter: 178 loss: 1.73686203e-06
Iter: 179 loss: 1.70708336e-06
Iter: 180 loss: 1.69444638e-06
Iter: 181 loss: 1.72042746e-06
Iter: 182 loss: 1.68950396e-06
Iter: 183 loss: 1.67658504e-06
Iter: 184 loss: 1.72909165e-06
Iter: 185 loss: 1.67384064e-06
Iter: 186 loss: 1.66339976e-06
Iter: 187 loss: 1.74411059e-06
Iter: 188 loss: 1.66267432e-06
Iter: 189 loss: 1.65644678e-06
Iter: 190 loss: 1.64928656e-06
Iter: 191 loss: 1.64838457e-06
Iter: 192 loss: 1.63595416e-06
Iter: 193 loss: 1.69223074e-06
Iter: 194 loss: 1.63351751e-06
Iter: 195 loss: 1.62511e-06
Iter: 196 loss: 1.71061538e-06
Iter: 197 loss: 1.62482399e-06
Iter: 198 loss: 1.61687194e-06
Iter: 199 loss: 1.62832441e-06
Iter: 200 loss: 1.61303456e-06
Iter: 201 loss: 1.60513355e-06
Iter: 202 loss: 1.62278627e-06
Iter: 203 loss: 1.60227319e-06
Iter: 204 loss: 1.59500189e-06
Iter: 205 loss: 1.59530566e-06
Iter: 206 loss: 1.58920579e-06
Iter: 207 loss: 1.57871773e-06
Iter: 208 loss: 1.58961723e-06
Iter: 209 loss: 1.57291117e-06
Iter: 210 loss: 1.56361921e-06
Iter: 211 loss: 1.65428969e-06
Iter: 212 loss: 1.56334954e-06
Iter: 213 loss: 1.55673899e-06
Iter: 214 loss: 1.60767195e-06
Iter: 215 loss: 1.55619796e-06
Iter: 216 loss: 1.55021098e-06
Iter: 217 loss: 1.53869746e-06
Iter: 218 loss: 1.78615e-06
Iter: 219 loss: 1.53862936e-06
Iter: 220 loss: 1.53033648e-06
Iter: 221 loss: 1.63398749e-06
Iter: 222 loss: 1.53028452e-06
Iter: 223 loss: 1.52332882e-06
Iter: 224 loss: 1.53867063e-06
Iter: 225 loss: 1.52068708e-06
Iter: 226 loss: 1.51299457e-06
Iter: 227 loss: 1.5486155e-06
Iter: 228 loss: 1.51152324e-06
Iter: 229 loss: 1.5042524e-06
Iter: 230 loss: 1.50315589e-06
Iter: 231 loss: 1.49807806e-06
Iter: 232 loss: 1.49124753e-06
Iter: 233 loss: 1.56200861e-06
Iter: 234 loss: 1.49100879e-06
Iter: 235 loss: 1.48669494e-06
Iter: 236 loss: 1.51791187e-06
Iter: 237 loss: 1.48637389e-06
Iter: 238 loss: 1.4811535e-06
Iter: 239 loss: 1.47512503e-06
Iter: 240 loss: 1.47447747e-06
Iter: 241 loss: 1.46786283e-06
Iter: 242 loss: 1.51582458e-06
Iter: 243 loss: 1.46728962e-06
Iter: 244 loss: 1.46247908e-06
Iter: 245 loss: 1.46220782e-06
Iter: 246 loss: 1.45855392e-06
Iter: 247 loss: 1.45131048e-06
Iter: 248 loss: 1.47165144e-06
Iter: 249 loss: 1.44903663e-06
Iter: 250 loss: 1.44165699e-06
Iter: 251 loss: 1.47085643e-06
Iter: 252 loss: 1.44004673e-06
Iter: 253 loss: 1.43498983e-06
Iter: 254 loss: 1.46162472e-06
Iter: 255 loss: 1.4342329e-06
Iter: 256 loss: 1.42916656e-06
Iter: 257 loss: 1.43880061e-06
Iter: 258 loss: 1.42695967e-06
Iter: 259 loss: 1.42132581e-06
Iter: 260 loss: 1.42906947e-06
Iter: 261 loss: 1.41848659e-06
Iter: 262 loss: 1.4129123e-06
Iter: 263 loss: 1.41207113e-06
Iter: 264 loss: 1.40818088e-06
Iter: 265 loss: 1.40428767e-06
Iter: 266 loss: 1.40390193e-06
Iter: 267 loss: 1.40010252e-06
Iter: 268 loss: 1.39653775e-06
Iter: 269 loss: 1.39568317e-06
Iter: 270 loss: 1.38818757e-06
Iter: 271 loss: 1.40662632e-06
Iter: 272 loss: 1.38552593e-06
Iter: 273 loss: 1.38340226e-06
Iter: 274 loss: 1.38277858e-06
Iter: 275 loss: 1.38022619e-06
Iter: 276 loss: 1.37353277e-06
Iter: 277 loss: 1.41841792e-06
Iter: 278 loss: 1.37187567e-06
Iter: 279 loss: 1.36567382e-06
Iter: 280 loss: 1.41456644e-06
Iter: 281 loss: 1.36526717e-06
Iter: 282 loss: 1.35980486e-06
Iter: 283 loss: 1.37929476e-06
Iter: 284 loss: 1.35843857e-06
Iter: 285 loss: 1.35313871e-06
Iter: 286 loss: 1.35952916e-06
Iter: 287 loss: 1.35028358e-06
Iter: 288 loss: 1.3451654e-06
Iter: 289 loss: 1.36397011e-06
Iter: 290 loss: 1.34397692e-06
Iter: 291 loss: 1.33935328e-06
Iter: 292 loss: 1.37675943e-06
Iter: 293 loss: 1.33908065e-06
Iter: 294 loss: 1.33580284e-06
Iter: 295 loss: 1.33860703e-06
Iter: 296 loss: 1.33392246e-06
Iter: 297 loss: 1.32894809e-06
Iter: 298 loss: 1.33145227e-06
Iter: 299 loss: 1.32552782e-06
Iter: 300 loss: 1.3204492e-06
Iter: 301 loss: 1.33240894e-06
Iter: 302 loss: 1.31864e-06
Iter: 303 loss: 1.31411457e-06
Iter: 304 loss: 1.34845141e-06
Iter: 305 loss: 1.31374122e-06
Iter: 306 loss: 1.30910598e-06
Iter: 307 loss: 1.31689944e-06
Iter: 308 loss: 1.30695867e-06
Iter: 309 loss: 1.30353646e-06
Iter: 310 loss: 1.31978072e-06
Iter: 311 loss: 1.30297246e-06
Iter: 312 loss: 1.29906175e-06
Iter: 313 loss: 1.31226102e-06
Iter: 314 loss: 1.29799309e-06
Iter: 315 loss: 1.29429304e-06
Iter: 316 loss: 1.29319756e-06
Iter: 317 loss: 1.29088914e-06
Iter: 318 loss: 1.28671127e-06
Iter: 319 loss: 1.28769807e-06
Iter: 320 loss: 1.28358795e-06
Iter: 321 loss: 1.27878184e-06
Iter: 322 loss: 1.31799243e-06
Iter: 323 loss: 1.27847716e-06
Iter: 324 loss: 1.27451858e-06
Iter: 325 loss: 1.28778527e-06
Iter: 326 loss: 1.27344333e-06
Iter: 327 loss: 1.26909083e-06
Iter: 328 loss: 1.27158251e-06
Iter: 329 loss: 1.2662606e-06
Iter: 330 loss: 1.2630544e-06
Iter: 331 loss: 1.26303371e-06
Iter: 332 loss: 1.26050577e-06
Iter: 333 loss: 1.2588157e-06
Iter: 334 loss: 1.25777126e-06
Iter: 335 loss: 1.25268923e-06
Iter: 336 loss: 1.26089265e-06
Iter: 337 loss: 1.25016959e-06
Iter: 338 loss: 1.2468372e-06
Iter: 339 loss: 1.25453528e-06
Iter: 340 loss: 1.24558e-06
Iter: 341 loss: 1.24138762e-06
Iter: 342 loss: 1.25477732e-06
Iter: 343 loss: 1.24011194e-06
Iter: 344 loss: 1.2361113e-06
Iter: 345 loss: 1.26453881e-06
Iter: 346 loss: 1.23577342e-06
Iter: 347 loss: 1.23321456e-06
Iter: 348 loss: 1.24414214e-06
Iter: 349 loss: 1.23266648e-06
Iter: 350 loss: 1.23016468e-06
Iter: 351 loss: 1.23673112e-06
Iter: 352 loss: 1.22945789e-06
Iter: 353 loss: 1.22660356e-06
Iter: 354 loss: 1.22207518e-06
Iter: 355 loss: 1.22201936e-06
Iter: 356 loss: 1.2177195e-06
Iter: 357 loss: 1.26186796e-06
Iter: 358 loss: 1.21755011e-06
Iter: 359 loss: 1.21459584e-06
Iter: 360 loss: 1.2108494e-06
Iter: 361 loss: 1.21051471e-06
Iter: 362 loss: 1.20545155e-06
Iter: 363 loss: 1.2419024e-06
Iter: 364 loss: 1.20495201e-06
Iter: 365 loss: 1.20142022e-06
Iter: 366 loss: 1.22665142e-06
Iter: 367 loss: 1.20107336e-06
Iter: 368 loss: 1.19813126e-06
Iter: 369 loss: 1.2062128e-06
Iter: 370 loss: 1.19713093e-06
Iter: 371 loss: 1.19380741e-06
Iter: 372 loss: 1.196928e-06
Iter: 373 loss: 1.19199512e-06
Iter: 374 loss: 1.18861249e-06
Iter: 375 loss: 1.21259018e-06
Iter: 376 loss: 1.18828825e-06
Iter: 377 loss: 1.18557728e-06
Iter: 378 loss: 1.18211096e-06
Iter: 379 loss: 1.18186267e-06
Iter: 380 loss: 1.17822765e-06
Iter: 381 loss: 1.2164594e-06
Iter: 382 loss: 1.17806383e-06
Iter: 383 loss: 1.1755485e-06
Iter: 384 loss: 1.20164032e-06
Iter: 385 loss: 1.17545744e-06
Iter: 386 loss: 1.17331251e-06
Iter: 387 loss: 1.17278751e-06
Iter: 388 loss: 1.17142667e-06
Iter: 389 loss: 1.16811657e-06
Iter: 390 loss: 1.18527305e-06
Iter: 391 loss: 1.16761134e-06
Iter: 392 loss: 1.1651116e-06
Iter: 393 loss: 1.16280694e-06
Iter: 394 loss: 1.16223157e-06
Iter: 395 loss: 1.15866419e-06
Iter: 396 loss: 1.16501337e-06
Iter: 397 loss: 1.15703074e-06
Iter: 398 loss: 1.15314913e-06
Iter: 399 loss: 1.17973127e-06
Iter: 400 loss: 1.15278692e-06
Iter: 401 loss: 1.14990871e-06
Iter: 402 loss: 1.15469447e-06
Iter: 403 loss: 1.14841987e-06
Iter: 404 loss: 1.14564386e-06
Iter: 405 loss: 1.16003685e-06
Iter: 406 loss: 1.1451566e-06
Iter: 407 loss: 1.14233035e-06
Iter: 408 loss: 1.15270018e-06
Iter: 409 loss: 1.14161321e-06
Iter: 410 loss: 1.13880071e-06
Iter: 411 loss: 1.14152726e-06
Iter: 412 loss: 1.13729493e-06
Iter: 413 loss: 1.133963e-06
Iter: 414 loss: 1.14334807e-06
Iter: 415 loss: 1.13285375e-06
Iter: 416 loss: 1.12965131e-06
Iter: 417 loss: 1.13561941e-06
Iter: 418 loss: 1.12831879e-06
Iter: 419 loss: 1.12523071e-06
Iter: 420 loss: 1.14854208e-06
Iter: 421 loss: 1.12499993e-06
Iter: 422 loss: 1.1228567e-06
Iter: 423 loss: 1.13486249e-06
Iter: 424 loss: 1.12252076e-06
Iter: 425 loss: 1.12026328e-06
Iter: 426 loss: 1.12081557e-06
Iter: 427 loss: 1.11861925e-06
Iter: 428 loss: 1.11627958e-06
Iter: 429 loss: 1.12375187e-06
Iter: 430 loss: 1.11552299e-06
Iter: 431 loss: 1.11300665e-06
Iter: 432 loss: 1.1187276e-06
Iter: 433 loss: 1.1120411e-06
Iter: 434 loss: 1.1094055e-06
Iter: 435 loss: 1.10685448e-06
Iter: 436 loss: 1.10631481e-06
Iter: 437 loss: 1.10279984e-06
Iter: 438 loss: 1.13671013e-06
Iter: 439 loss: 1.10270696e-06
Iter: 440 loss: 1.10029828e-06
Iter: 441 loss: 1.0983606e-06
Iter: 442 loss: 1.0976363e-06
Iter: 443 loss: 1.09393943e-06
Iter: 444 loss: 1.132394e-06
Iter: 445 loss: 1.0938237e-06
Iter: 446 loss: 1.09180087e-06
Iter: 447 loss: 1.11805525e-06
Iter: 448 loss: 1.09178131e-06
Iter: 449 loss: 1.09015241e-06
Iter: 450 loss: 1.08778363e-06
Iter: 451 loss: 1.08775544e-06
Iter: 452 loss: 1.08481186e-06
Iter: 453 loss: 1.1006523e-06
Iter: 454 loss: 1.08446818e-06
Iter: 455 loss: 1.08197719e-06
Iter: 456 loss: 1.0940214e-06
Iter: 457 loss: 1.08154586e-06
Iter: 458 loss: 1.07991355e-06
Iter: 459 loss: 1.0905319e-06
Iter: 460 loss: 1.07967344e-06
Iter: 461 loss: 1.0776871e-06
Iter: 462 loss: 1.07752089e-06
Iter: 463 loss: 1.07601591e-06
Iter: 464 loss: 1.0735024e-06
Iter: 465 loss: 1.08142615e-06
Iter: 466 loss: 1.0727656e-06
Iter: 467 loss: 1.07078858e-06
Iter: 468 loss: 1.07746655e-06
Iter: 469 loss: 1.07026528e-06
Iter: 470 loss: 1.06814139e-06
Iter: 471 loss: 1.07044866e-06
Iter: 472 loss: 1.06700804e-06
Iter: 473 loss: 1.06448306e-06
Iter: 474 loss: 1.06599259e-06
Iter: 475 loss: 1.06297432e-06
Iter: 476 loss: 1.06042319e-06
Iter: 477 loss: 1.08190761e-06
Iter: 478 loss: 1.06029211e-06
Iter: 479 loss: 1.05834386e-06
Iter: 480 loss: 1.05584775e-06
Iter: 481 loss: 1.05572371e-06
Iter: 482 loss: 1.05318145e-06
Iter: 483 loss: 1.05320942e-06
Iter: 484 loss: 1.05134461e-06
Iter: 485 loss: 1.05804531e-06
Iter: 486 loss: 1.05085473e-06
Iter: 487 loss: 1.04865535e-06
Iter: 488 loss: 1.05058916e-06
Iter: 489 loss: 1.04741423e-06
Iter: 490 loss: 1.04532751e-06
Iter: 491 loss: 1.04920321e-06
Iter: 492 loss: 1.04438686e-06
Iter: 493 loss: 1.04186722e-06
Iter: 494 loss: 1.0497954e-06
Iter: 495 loss: 1.04103196e-06
Iter: 496 loss: 1.03881166e-06
Iter: 497 loss: 1.06791822e-06
Iter: 498 loss: 1.03880313e-06
Iter: 499 loss: 1.03749107e-06
Iter: 500 loss: 1.03608454e-06
Iter: 501 loss: 1.03581351e-06
Iter: 502 loss: 1.03338152e-06
Iter: 503 loss: 1.04307333e-06
Iter: 504 loss: 1.03286197e-06
Iter: 505 loss: 1.03101547e-06
Iter: 506 loss: 1.03525008e-06
Iter: 507 loss: 1.03028628e-06
Iter: 508 loss: 1.0284316e-06
Iter: 509 loss: 1.03607715e-06
Iter: 510 loss: 1.02803733e-06
Iter: 511 loss: 1.02636136e-06
Iter: 512 loss: 1.02780109e-06
Iter: 513 loss: 1.02542617e-06
Iter: 514 loss: 1.02313993e-06
Iter: 515 loss: 1.02213107e-06
Iter: 516 loss: 1.02099818e-06
Iter: 517 loss: 1.018376e-06
Iter: 518 loss: 1.03656703e-06
Iter: 519 loss: 1.0181941e-06
Iter: 520 loss: 1.0157903e-06
Iter: 521 loss: 1.02528929e-06
Iter: 522 loss: 1.01524597e-06
Iter: 523 loss: 1.0131107e-06
Iter: 524 loss: 1.02014587e-06
Iter: 525 loss: 1.0126696e-06
Iter: 526 loss: 1.0104261e-06
Iter: 527 loss: 1.01934734e-06
Iter: 528 loss: 1.00993736e-06
Iter: 529 loss: 1.00842465e-06
Iter: 530 loss: 1.00716602e-06
Iter: 531 loss: 1.00666023e-06
Iter: 532 loss: 1.00486727e-06
Iter: 533 loss: 1.00484203e-06
Iter: 534 loss: 1.00327929e-06
Iter: 535 loss: 1.00406908e-06
Iter: 536 loss: 1.00235661e-06
Iter: 537 loss: 1.00060743e-06
Iter: 538 loss: 1.00067155e-06
Iter: 539 loss: 9.99201575e-07
Iter: 540 loss: 9.9748172e-07
Iter: 541 loss: 1.0172e-06
Iter: 542 loss: 9.97385087e-07
Iter: 543 loss: 9.95881e-07
Iter: 544 loss: 9.94923539e-07
Iter: 545 loss: 9.94379434e-07
Iter: 546 loss: 9.92411515e-07
Iter: 547 loss: 1.00785815e-06
Iter: 548 loss: 9.92225182e-07
Iter: 549 loss: 9.90670742e-07
Iter: 550 loss: 9.92773494e-07
Iter: 551 loss: 9.89877435e-07
Iter: 552 loss: 9.88102329e-07
Iter: 553 loss: 9.88202828e-07
Iter: 554 loss: 9.86706709e-07
Iter: 555 loss: 9.84238568e-07
Iter: 556 loss: 9.96170911e-07
Iter: 557 loss: 9.8380292e-07
Iter: 558 loss: 9.81766e-07
Iter: 559 loss: 9.86793339e-07
Iter: 560 loss: 9.81048061e-07
Iter: 561 loss: 9.7904956e-07
Iter: 562 loss: 9.91001343e-07
Iter: 563 loss: 9.78881e-07
Iter: 564 loss: 9.76965111e-07
Iter: 565 loss: 9.8137366e-07
Iter: 566 loss: 9.76282081e-07
Iter: 567 loss: 9.7451e-07
Iter: 568 loss: 9.77693389e-07
Iter: 569 loss: 9.73781312e-07
Iter: 570 loss: 9.7258e-07
Iter: 571 loss: 9.7255429e-07
Iter: 572 loss: 9.7165082e-07
Iter: 573 loss: 9.70290671e-07
Iter: 574 loss: 9.70229848e-07
Iter: 575 loss: 9.68250333e-07
Iter: 576 loss: 9.73091e-07
Iter: 577 loss: 9.67533879e-07
Iter: 578 loss: 9.65818e-07
Iter: 579 loss: 9.70537258e-07
Iter: 580 loss: 9.65269464e-07
Iter: 581 loss: 9.63489583e-07
Iter: 582 loss: 9.72029738e-07
Iter: 583 loss: 9.6313579e-07
Iter: 584 loss: 9.61366482e-07
Iter: 585 loss: 9.62554e-07
Iter: 586 loss: 9.60197212e-07
Iter: 587 loss: 9.58447231e-07
Iter: 588 loss: 9.62394211e-07
Iter: 589 loss: 9.57784096e-07
Iter: 590 loss: 9.55794121e-07
Iter: 591 loss: 9.66293101e-07
Iter: 592 loss: 9.55491487e-07
Iter: 593 loss: 9.54038683e-07
Iter: 594 loss: 9.54959887e-07
Iter: 595 loss: 9.53126573e-07
Iter: 596 loss: 9.51028198e-07
Iter: 597 loss: 9.55055384e-07
Iter: 598 loss: 9.50173103e-07
Iter: 599 loss: 9.48535956e-07
Iter: 600 loss: 9.58439387e-07
Iter: 601 loss: 9.4833382e-07
Iter: 602 loss: 9.46693433e-07
Iter: 603 loss: 9.54455913e-07
Iter: 604 loss: 9.46433488e-07
Iter: 605 loss: 9.45145302e-07
Iter: 606 loss: 9.46077648e-07
Iter: 607 loss: 9.44276394e-07
Iter: 608 loss: 9.42903966e-07
Iter: 609 loss: 9.62045306e-07
Iter: 610 loss: 9.42917154e-07
Iter: 611 loss: 9.42038355e-07
Iter: 612 loss: 9.40530072e-07
Iter: 613 loss: 9.76492856e-07
Iter: 614 loss: 9.40553093e-07
Iter: 615 loss: 9.38465178e-07
Iter: 616 loss: 9.41735266e-07
Iter: 617 loss: 9.37540051e-07
Iter: 618 loss: 9.35794446e-07
Iter: 619 loss: 9.52059906e-07
Iter: 620 loss: 9.35725325e-07
Iter: 621 loss: 9.34074649e-07
Iter: 622 loss: 9.37120035e-07
Iter: 623 loss: 9.33463866e-07
Iter: 624 loss: 9.31896807e-07
Iter: 625 loss: 9.37476e-07
Iter: 626 loss: 9.31516865e-07
Iter: 627 loss: 9.30081114e-07
Iter: 628 loss: 9.29726525e-07
Iter: 629 loss: 9.28730913e-07
Iter: 630 loss: 9.26882478e-07
Iter: 631 loss: 9.37427e-07
Iter: 632 loss: 9.2668563e-07
Iter: 633 loss: 9.2490103e-07
Iter: 634 loss: 9.28949476e-07
Iter: 635 loss: 9.24257961e-07
Iter: 636 loss: 9.22487288e-07
Iter: 637 loss: 9.25500103e-07
Iter: 638 loss: 9.21622814e-07
Iter: 639 loss: 9.20071273e-07
Iter: 640 loss: 9.31541422e-07
Iter: 641 loss: 9.19932177e-07
Iter: 642 loss: 9.18444471e-07
Iter: 643 loss: 9.2273342e-07
Iter: 644 loss: 9.18089768e-07
Iter: 645 loss: 9.16587794e-07
Iter: 646 loss: 9.21506285e-07
Iter: 647 loss: 9.16158513e-07
Iter: 648 loss: 9.14973612e-07
Iter: 649 loss: 9.2138913e-07
Iter: 650 loss: 9.14810187e-07
Iter: 651 loss: 9.13808321e-07
Iter: 652 loss: 9.13305485e-07
Iter: 653 loss: 9.12788607e-07
Iter: 654 loss: 9.11195116e-07
Iter: 655 loss: 9.08826848e-07
Iter: 656 loss: 9.0872561e-07
Iter: 657 loss: 9.07687422e-07
Iter: 658 loss: 9.07482e-07
Iter: 659 loss: 9.06347395e-07
Iter: 660 loss: 9.07892854e-07
Iter: 661 loss: 9.05764409e-07
Iter: 662 loss: 9.04092076e-07
Iter: 663 loss: 9.03643695e-07
Iter: 664 loss: 9.02625629e-07
Iter: 665 loss: 9.01051862e-07
Iter: 666 loss: 9.09753624e-07
Iter: 667 loss: 9.00863256e-07
Iter: 668 loss: 8.99357e-07
Iter: 669 loss: 9.00975351e-07
Iter: 670 loss: 8.9855871e-07
Iter: 671 loss: 8.96857046e-07
Iter: 672 loss: 9.08840491e-07
Iter: 673 loss: 8.96657298e-07
Iter: 674 loss: 8.9544119e-07
Iter: 675 loss: 8.96714482e-07
Iter: 676 loss: 8.94783398e-07
Iter: 677 loss: 8.933755e-07
Iter: 678 loss: 9.02616e-07
Iter: 679 loss: 8.93231231e-07
Iter: 680 loss: 8.91736818e-07
Iter: 681 loss: 8.94449386e-07
Iter: 682 loss: 8.91073569e-07
Iter: 683 loss: 8.89827561e-07
Iter: 684 loss: 8.98659891e-07
Iter: 685 loss: 8.89715238e-07
Iter: 686 loss: 8.88913291e-07
Iter: 687 loss: 8.8918415e-07
Iter: 688 loss: 8.88349518e-07
Iter: 689 loss: 8.87039391e-07
Iter: 690 loss: 8.87078102e-07
Iter: 691 loss: 8.86040311e-07
Iter: 692 loss: 8.84274641e-07
Iter: 693 loss: 8.87889541e-07
Iter: 694 loss: 8.83559551e-07
Iter: 695 loss: 8.82196844e-07
Iter: 696 loss: 8.87890906e-07
Iter: 697 loss: 8.81819687e-07
Iter: 698 loss: 8.80306743e-07
Iter: 699 loss: 8.88094633e-07
Iter: 700 loss: 8.80027756e-07
Iter: 701 loss: 8.78579613e-07
Iter: 702 loss: 8.79683796e-07
Iter: 703 loss: 8.77753507e-07
Iter: 704 loss: 8.76245053e-07
Iter: 705 loss: 8.78252081e-07
Iter: 706 loss: 8.75574074e-07
Iter: 707 loss: 8.73965178e-07
Iter: 708 loss: 8.81025073e-07
Iter: 709 loss: 8.73663566e-07
Iter: 710 loss: 8.72284943e-07
Iter: 711 loss: 8.73662032e-07
Iter: 712 loss: 8.7148328e-07
Iter: 713 loss: 8.69595056e-07
Iter: 714 loss: 8.82851623e-07
Iter: 715 loss: 8.69407813e-07
Iter: 716 loss: 8.6835837e-07
Iter: 717 loss: 8.78067112e-07
Iter: 718 loss: 8.68341203e-07
Iter: 719 loss: 8.67419772e-07
Iter: 720 loss: 8.66764537e-07
Iter: 721 loss: 8.66459e-07
Iter: 722 loss: 8.65182074e-07
Iter: 723 loss: 8.73007536e-07
Iter: 724 loss: 8.65064408e-07
Iter: 725 loss: 8.63939363e-07
Iter: 726 loss: 8.63912817e-07
Iter: 727 loss: 8.63026685e-07
Iter: 728 loss: 8.61368562e-07
Iter: 729 loss: 8.66148639e-07
Iter: 730 loss: 8.60913474e-07
Iter: 731 loss: 8.59702709e-07
Iter: 732 loss: 8.6098521e-07
Iter: 733 loss: 8.59002057e-07
Iter: 734 loss: 8.57485929e-07
Iter: 735 loss: 8.64083574e-07
Iter: 736 loss: 8.57217287e-07
Iter: 737 loss: 8.55729e-07
Iter: 738 loss: 8.63280377e-07
Iter: 739 loss: 8.55464577e-07
Iter: 740 loss: 8.54390919e-07
Iter: 741 loss: 8.5572276e-07
Iter: 742 loss: 8.53842494e-07
Iter: 743 loss: 8.52546805e-07
Iter: 744 loss: 8.54403538e-07
Iter: 745 loss: 8.51917434e-07
Iter: 746 loss: 8.50490949e-07
Iter: 747 loss: 8.52226549e-07
Iter: 748 loss: 8.49726916e-07
Iter: 749 loss: 8.4826894e-07
Iter: 750 loss: 8.51692619e-07
Iter: 751 loss: 8.47695446e-07
Iter: 752 loss: 8.46426474e-07
Iter: 753 loss: 8.46432272e-07
Iter: 754 loss: 8.45748389e-07
Iter: 755 loss: 8.46126454e-07
Iter: 756 loss: 8.45293073e-07
Iter: 757 loss: 8.44226349e-07
Iter: 758 loss: 8.44995213e-07
Iter: 759 loss: 8.43584189e-07
Iter: 760 loss: 8.42361374e-07
Iter: 761 loss: 8.46251623e-07
Iter: 762 loss: 8.42078407e-07
Iter: 763 loss: 8.40927441e-07
Iter: 764 loss: 8.43578619e-07
Iter: 765 loss: 8.40438133e-07
Iter: 766 loss: 8.39312747e-07
Iter: 767 loss: 8.40508392e-07
Iter: 768 loss: 8.38601068e-07
Iter: 769 loss: 8.37384277e-07
Iter: 770 loss: 8.39395796e-07
Iter: 771 loss: 8.36831646e-07
Iter: 772 loss: 8.35498042e-07
Iter: 773 loss: 8.48021614e-07
Iter: 774 loss: 8.35507e-07
Iter: 775 loss: 8.34457069e-07
Iter: 776 loss: 8.36683625e-07
Iter: 777 loss: 8.34005959e-07
Iter: 778 loss: 8.33092258e-07
Iter: 779 loss: 8.33884542e-07
Iter: 780 loss: 8.32551905e-07
Iter: 781 loss: 8.31349894e-07
Iter: 782 loss: 8.34076332e-07
Iter: 783 loss: 8.30822387e-07
Iter: 784 loss: 8.29488158e-07
Iter: 785 loss: 8.31386046e-07
Iter: 786 loss: 8.28811153e-07
Iter: 787 loss: 8.27830661e-07
Iter: 788 loss: 8.27791837e-07
Iter: 789 loss: 8.2698034e-07
Iter: 790 loss: 8.30031922e-07
Iter: 791 loss: 8.26810378e-07
