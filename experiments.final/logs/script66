+ RUN=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ PSI='0 1 -2'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output66
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output67
+ for fn in f1
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 2.8
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output66/f1_psi0_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output67/f1_psi0_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output66/f1_psi0_phi2.8 /home/mrdouglas/Manifold/experiments.final/output67/f1_psi0_phi2.8
+ date
Sat Oct 24 18:01:58 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output66/f1_psi0_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 2345 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 0 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output66/f1_psi0_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00928c46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0092803510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f009284a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f009284a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0092884620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00927c0e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0092777d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f009270c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f009270cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f009270c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00927371e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f009268d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0092670730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0092756730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00926222f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0092741488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0092650d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0092650bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0092650e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00925f1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0092605a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f009260df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00926058c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0071058840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00710221e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f004c0b6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f004c0b6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f004c092268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f004c0aa7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f004c03f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f004c011158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0071005f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f003073e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0070fe1488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f00306c7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0030692ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.6088115
test_loss: 0.6129165
train_loss: 0.3897854
test_loss: 0.3964641
train_loss: 0.27253777
test_loss: 0.28021154
train_loss: 0.21439844
test_loss: 0.2060667
train_loss: 0.16677868
test_loss: 0.16905515
train_loss: 0.13845883
test_loss: 0.13838702
train_loss: 0.12429428
test_loss: 0.11602685
train_loss: 0.08151149
test_loss: 0.0814676
train_loss: 0.06313086
test_loss: 0.06508139
train_loss: 0.049908858
test_loss: 0.050580867
train_loss: 0.045143493
test_loss: 0.043637663
train_loss: 0.039303504
test_loss: 0.038428087
train_loss: 0.033945467
test_loss: 0.034238387
train_loss: 0.03002773
test_loss: 0.031187333
train_loss: 0.02756243
test_loss: 0.029374853
train_loss: 0.024671819
test_loss: 0.026441507
train_loss: 0.023168445
test_loss: 0.023946168
train_loss: 0.022787694
test_loss: 0.023023333
train_loss: 0.020276904
test_loss: 0.022710519
train_loss: 0.020101707
test_loss: 0.0205766
train_loss: 0.017916348
test_loss: 0.020052008
train_loss: 0.018224426
test_loss: 0.018533329
train_loss: 0.017134532
test_loss: 0.018051408
train_loss: 0.016713
test_loss: 0.017037239
train_loss: 0.015317937
test_loss: 0.016855912
train_loss: 0.015405583
test_loss: 0.015982052
train_loss: 0.014445448
test_loss: 0.015367433
train_loss: 0.014029174
test_loss: 0.015344904
train_loss: 0.0125254495
test_loss: 0.014005225
train_loss: 0.01352411
test_loss: 0.014560428
train_loss: 0.013250629
test_loss: 0.014311001
train_loss: 0.011150366
test_loss: 0.013242334
train_loss: 0.011716236
test_loss: 0.012526359
train_loss: 0.011371195
test_loss: 0.013268952
train_loss: 0.009698621
test_loss: 0.01120929
train_loss: 0.010712963
test_loss: 0.010933509
train_loss: 0.010220666
test_loss: 0.010575492
train_loss: 0.009438565
test_loss: 0.01028728
train_loss: 0.0102895815
test_loss: 0.010185786
train_loss: 0.010143507
test_loss: 0.010365902
train_loss: 0.010049995
test_loss: 0.011353031
train_loss: 0.0089687295
test_loss: 0.009585623
train_loss: 0.009163059
test_loss: 0.009102695
train_loss: 0.008853383
test_loss: 0.009115881
train_loss: 0.008502884
test_loss: 0.010005132
train_loss: 0.009169061
test_loss: 0.009358887
train_loss: 0.010015142
test_loss: 0.010337748
train_loss: 0.009376856
test_loss: 0.009075824
train_loss: 0.008320965
test_loss: 0.009335678
train_loss: 0.008608041
test_loss: 0.00888206
train_loss: 0.008893039
test_loss: 0.009886286
train_loss: 0.007680063
test_loss: 0.0095388945
train_loss: 0.0077137905
test_loss: 0.008443035
train_loss: 0.007722604
test_loss: 0.008871667
train_loss: 0.007012024
test_loss: 0.0071867206
train_loss: 0.007891774
test_loss: 0.0076080114
train_loss: 0.006844757
test_loss: 0.00739849
train_loss: 0.00727049
test_loss: 0.007802777
train_loss: 0.0075956914
test_loss: 0.007579164
train_loss: 0.006860886
test_loss: 0.008142559
train_loss: 0.0071178973
test_loss: 0.007568717
train_loss: 0.0067918478
test_loss: 0.0068047913
train_loss: 0.006945534
test_loss: 0.0072383196
train_loss: 0.0067519275
test_loss: 0.0071831853
train_loss: 0.008022921
test_loss: 0.0069425893
train_loss: 0.006594051
test_loss: 0.009128953
train_loss: 0.0071395184
test_loss: 0.0104382895
train_loss: 0.0071637835
test_loss: 0.007168132
train_loss: 0.006716773
test_loss: 0.00681932
train_loss: 0.006572973
test_loss: 0.006917924
train_loss: 0.0063926904
test_loss: 0.0070239194
train_loss: 0.0059768064
test_loss: 0.0067026597
train_loss: 0.00688223
test_loss: 0.0064262324
train_loss: 0.0067664403
test_loss: 0.007023998
train_loss: 0.005868897
test_loss: 0.0060267095
train_loss: 0.0062369145
test_loss: 0.0072616814
train_loss: 0.009157908
test_loss: 0.0068958336
train_loss: 0.0065840366
test_loss: 0.0067008194
train_loss: 0.0059562344
test_loss: 0.0065868055
train_loss: 0.0052307807
test_loss: 0.005561705
train_loss: 0.005475137
test_loss: 0.00600926
train_loss: 0.0051381174
test_loss: 0.0059990454
train_loss: 0.0055514607
test_loss: 0.0057260892
train_loss: 0.005565202
test_loss: 0.0055070994
train_loss: 0.0058461167
test_loss: 0.005828948
train_loss: 0.0045531294
test_loss: 0.0051705968
train_loss: 0.0053459355
test_loss: 0.005039082
train_loss: 0.006274794
test_loss: 0.008371101
train_loss: 0.0048612505
test_loss: 0.005669169
train_loss: 0.005129175
test_loss: 0.0050298804
train_loss: 0.004343485
test_loss: 0.0048796297
train_loss: 0.0052703894
test_loss: 0.005400947
train_loss: 0.0050167735
test_loss: 0.005829808
train_loss: 0.0049270014
test_loss: 0.0049023814
train_loss: 0.0048657325
test_loss: 0.005594807
train_loss: 0.0058478033
test_loss: 0.004898745
train_loss: 0.005227279
test_loss: 0.0053836587
train_loss: 0.0054238485
test_loss: 0.005735101
train_loss: 0.0050161206
test_loss: 0.005209568
train_loss: 0.0052098534
test_loss: 0.0052283606
train_loss: 0.0064170463
test_loss: 0.0053396164
train_loss: 0.005129964
test_loss: 0.0047902702
train_loss: 0.005387369
test_loss: 0.0053073033
train_loss: 0.0052849944
test_loss: 0.00478207
train_loss: 0.004510017
test_loss: 0.004581354
train_loss: 0.0048497543
test_loss: 0.0046963547
train_loss: 0.0048140837
test_loss: 0.0042258548
train_loss: 0.0042143166
test_loss: 0.00561199
train_loss: 0.004892163
test_loss: 0.004438793
train_loss: 0.0054629976
test_loss: 0.0059979013
train_loss: 0.0042674015
test_loss: 0.004611176
train_loss: 0.003951012
test_loss: 0.004389126
train_loss: 0.004522392
test_loss: 0.0049285535
train_loss: 0.004014422
test_loss: 0.0041779764
train_loss: 0.0046384116
test_loss: 0.0052469354
train_loss: 0.0049822703
test_loss: 0.005029856
train_loss: 0.004161867
test_loss: 0.0040452788
train_loss: 0.0046000197
test_loss: 0.0050270627
train_loss: 0.0041941567
test_loss: 0.0058526485
train_loss: 0.0066720815
test_loss: 0.0054558585
train_loss: 0.0052381447
test_loss: 0.0053458437
train_loss: 0.0041251555
test_loss: 0.0043466324
train_loss: 0.004297056
test_loss: 0.0045446614
train_loss: 0.0041915225
test_loss: 0.004710717
train_loss: 0.0046711657
test_loss: 0.005191739
train_loss: 0.0040307464
test_loss: 0.004192573
train_loss: 0.00493098
test_loss: 0.0048195077
train_loss: 0.0048864344
test_loss: 0.00443886
train_loss: 0.003911944
test_loss: 0.004126172
train_loss: 0.0051890113
test_loss: 0.0045017507
train_loss: 0.004303286
test_loss: 0.0046077827
train_loss: 0.003998302
test_loss: 0.0040410645
train_loss: 0.004372417
test_loss: 0.004673698
train_loss: 0.005539589
test_loss: 0.0045259194
train_loss: 0.0050249263
test_loss: 0.004985484
train_loss: 0.0050172224
test_loss: 0.0046852455
train_loss: 0.0047394503
test_loss: 0.004462474
train_loss: 0.0046791676
test_loss: 0.004144135
train_loss: 0.004821693
test_loss: 0.00589025
train_loss: 0.006395467
test_loss: 0.005496262
train_loss: 0.005641131
test_loss: 0.0062561864
train_loss: 0.0053828256
test_loss: 0.006168772
train_loss: 0.0040061832
test_loss: 0.004216371
train_loss: 0.0039117346
test_loss: 0.004057654
train_loss: 0.003947991
test_loss: 0.004603036
train_loss: 0.0040425044
test_loss: 0.0043471907
train_loss: 0.0053029098
test_loss: 0.005413108
train_loss: 0.0048967423
test_loss: 0.005114691
train_loss: 0.003735649
test_loss: 0.004090855
train_loss: 0.004303298
test_loss: 0.0043594926
train_loss: 0.004970217
test_loss: 0.0045509287
train_loss: 0.0042317607
test_loss: 0.0036589291
train_loss: 0.003824632
test_loss: 0.0057590874
train_loss: 0.004272128
test_loss: 0.0043443074
train_loss: 0.0042098183
test_loss: 0.0043354165
train_loss: 0.0040651336
test_loss: 0.0042038704
train_loss: 0.0036374866
test_loss: 0.0041911844
train_loss: 0.0037326938
test_loss: 0.0036949373
train_loss: 0.0036784653
test_loss: 0.00381196
train_loss: 0.0043439325
test_loss: 0.004455509
train_loss: 0.004708603
test_loss: 0.0040820683
train_loss: 0.004160246
test_loss: 0.004120582
train_loss: 0.0048635495
test_loss: 0.004740452
train_loss: 0.0046104174
test_loss: 0.004953395
train_loss: 0.003953751
test_loss: 0.004346207
train_loss: 0.0038478568
test_loss: 0.0038274927
train_loss: 0.004390998
test_loss: 0.004296387
train_loss: 0.003953096
test_loss: 0.0040368275
train_loss: 0.0037342971
test_loss: 0.0036891135
train_loss: 0.003775057
test_loss: 0.004002207
train_loss: 0.004029811
test_loss: 0.004593925
train_loss: 0.0043351967
test_loss: 0.004138213
train_loss: 0.004451408
test_loss: 0.0041043814
train_loss: 0.0039769886
test_loss: 0.0040777684
train_loss: 0.0037395465
test_loss: 0.004142805
train_loss: 0.0046349387
test_loss: 0.004238488
train_loss: 0.004146109
test_loss: 0.0038360152
train_loss: 0.0036689874
test_loss: 0.003893943
train_loss: 0.004206944
test_loss: 0.0036812834
train_loss: 0.0039935494
test_loss: 0.0038112218
train_loss: 0.0033120632
test_loss: 0.0037186702
train_loss: 0.005055408
test_loss: 0.004041115
train_loss: 0.00442123
test_loss: 0.0037395013
train_loss: 0.004607899
test_loss: 0.0038444686
train_loss: 0.0041654375
test_loss: 0.004029388
train_loss: 0.0042416737
test_loss: 0.0037255697
train_loss: 0.00458512
test_loss: 0.0047589918
train_loss: 0.0035203672
test_loss: 0.0040949946
train_loss: 0.0038762614
test_loss: 0.0039373524
train_loss: 0.005117284
test_loss: 0.0045754095
train_loss: 0.004311371
test_loss: 0.0039355536
train_loss: 0.004458703
test_loss: 0.0040592887
train_loss: 0.0038768526
test_loss: 0.003922303
train_loss: 0.003377912
test_loss: 0.0037168534
train_loss: 0.0036888157
test_loss: 0.0034502386
train_loss: 0.0037198528
test_loss: 0.004035375
train_loss: 0.0039896546
test_loss: 0.0039694277
train_loss: 0.00359497
test_loss: 0.003705028
train_loss: 0.0033495706
test_loss: 0.0037095475
train_loss: 0.0035986374
test_loss: 0.0034181168
train_loss: 0.003668691
test_loss: 0.004037439
train_loss: 0.003628882
test_loss: 0.003947207
train_loss: 0.004517927
test_loss: 0.0045076297
train_loss: 0.004007453
test_loss: 0.0043017226
train_loss: 0.003828153
test_loss: 0.003891419
train_loss: 0.0036386456
test_loss: 0.0040346994
train_loss: 0.0040902393
test_loss: 0.004571098
train_loss: 0.003970603
test_loss: 0.0036953546
train_loss: 0.004627848
test_loss: 0.0039469567
train_loss: 0.0031778568
test_loss: 0.0033862838
train_loss: 0.003778125
test_loss: 0.004157118
train_loss: 0.0033649066
test_loss: 0.0038606774
train_loss: 0.0034483136
test_loss: 0.0037470364
train_loss: 0.0032894057
test_loss: 0.0037556845
train_loss: 0.0045810733
test_loss: 0.004524037
train_loss: 0.0038613067
test_loss: 0.004089974
train_loss: 0.0032047695
test_loss: 0.0030730353
train_loss: 0.003447135
test_loss: 0.0036346177
train_loss: 0.0037043078
test_loss: 0.0036947287
train_loss: 0.0032820853
test_loss: 0.0034840514
train_loss: 0.0042576557
test_loss: 0.0033973863
train_loss: 0.003189249
test_loss: 0.003401486
train_loss: 0.0038255418
test_loss: 0.003410804
train_loss: 0.0037802982
test_loss: 0.0037329434
train_loss: 0.0030435366
test_loss: 0.0030406364
train_loss: 0.004363635
test_loss: 0.004074235
train_loss: 0.0035479334
test_loss: 0.0035501376
train_loss: 0.003999934
test_loss: 0.0035208676
train_loss: 0.0036624514
test_loss: 0.0035471858
train_loss: 0.0032879296
test_loss: 0.003241914
train_loss: 0.003399703
test_loss: 0.0035035445
train_loss: 0.0039681643
test_loss: 0.0041721
train_loss: 0.0054614353
test_loss: 0.005485217
train_loss: 0.005434934
test_loss: 0.0049507655
train_loss: 0.00406116
test_loss: 0.00442759
train_loss: 0.003427065
test_loss: 0.0035350823
train_loss: 0.003100333
test_loss: 0.0035343796
train_loss: 0.003002358
test_loss: 0.0034327523
train_loss: 0.003413622
test_loss: 0.0036299778
train_loss: 0.0034640497
test_loss: 0.0043625203
train_loss: 0.0032255184
test_loss: 0.0041137813
train_loss: 0.0033228183
test_loss: 0.0036207365
train_loss: 0.0044356454
test_loss: 0.0038648255
train_loss: 0.0037354897
test_loss: 0.003755022
train_loss: 0.0037580791
test_loss: 0.0037011914
train_loss: 0.0033388569
test_loss: 0.0033347846
train_loss: 0.003786759
test_loss: 0.0039769034
train_loss: 0.0039000944
test_loss: 0.0038581912
train_loss: 0.0032041539
test_loss: 0.0031004832
train_loss: 0.0034113522
test_loss: 0.0036944833
train_loss: 0.0038866764
test_loss: 0.0038673216
train_loss: 0.003948858
test_loss: 0.004821999
train_loss: 0.0032361995
test_loss: 0.0038140418
train_loss: 0.0034383247
test_loss: 0.0037087735
train_loss: 0.0040676277
test_loss: 0.0045673577
train_loss: 0.003247247
test_loss: 0.0036714047
train_loss: 0.003736695
test_loss: 0.0040169535
train_loss: 0.0037650345
test_loss: 0.0036844567
train_loss: 0.004080867
test_loss: 0.0040065227
train_loss: 0.0034374064
test_loss: 0.0035126337
train_loss: 0.0032891426
test_loss: 0.00402089
train_loss: 0.003084832
test_loss: 0.0035766957
train_loss: 0.0033527422
test_loss: 0.002917414
train_loss: 0.003329144
test_loss: 0.004030899
train_loss: 0.0051982943
test_loss: 0.0050108237
train_loss: 0.0034473448
test_loss: 0.0032027997
train_loss: 0.0034709214
test_loss: 0.0039802203
train_loss: 0.0034353319
test_loss: 0.0037043905
train_loss: 0.0030976147
test_loss: 0.003521612
train_loss: 0.0033903539
test_loss: 0.004407399
train_loss: 0.0031976544
test_loss: 0.0035823723
train_loss: 0.0031939768
test_loss: 0.0029595331
train_loss: 0.00362633
test_loss: 0.0036291454
train_loss: 0.0028775465
test_loss: 0.0033417402
train_loss: 0.0033786548
test_loss: 0.0033641448
train_loss: 0.0037657772
test_loss: 0.0034380427
train_loss: 0.0032812357
test_loss: 0.0031055326
train_loss: 0.0031529549
test_loss: 0.0032059653
train_loss: 0.0029574365
test_loss: 0.0029471286
train_loss: 0.004184404
test_loss: 0.0037713402
train_loss: 0.0034525588
test_loss: 0.003221366
train_loss: 0.003266423
test_loss: 0.0038140905
train_loss: 0.0033753822
test_loss: 0.0035599794
train_loss: 0.003253223
test_loss: 0.003073066
train_loss: 0.0029559066
test_loss: 0.0035837602
train_loss: 0.0029976608
test_loss: 0.003140306
train_loss: 0.0034692488
test_loss: 0.0036123365
train_loss: 0.0031662355
test_loss: 0.003332579
train_loss: 0.0034513974
test_loss: 0.0035307445
train_loss: 0.003564301
test_loss: 0.0035576872
train_loss: 0.0036567312
test_loss: 0.005708139
train_loss: 0.003330008
test_loss: 0.005156806
train_loss: 0.0035753935
test_loss: 0.0037639008
train_loss: 0.0034023006
test_loss: 0.0035553125
train_loss: 0.0037368191
test_loss: 0.0037242488
train_loss: 0.003209769
test_loss: 0.0032068142
train_loss: 0.0033665574
test_loss: 0.004921625
train_loss: 0.004962518
test_loss: 0.003961126
train_loss: 0.0031394362
test_loss: 0.0036920847
train_loss: 0.003028716
test_loss: 0.0031952767
train_loss: 0.0033003115
test_loss: 0.0031806412
train_loss: 0.003644072
test_loss: 0.003388439
train_loss: 0.0031102374
test_loss: 0.003434045
train_loss: 0.0035830154
test_loss: 0.0034306543
train_loss: 0.0034756653
test_loss: 0.0033910216
train_loss: 0.0027540307
test_loss: 0.0032268108
train_loss: 0.0034336334
test_loss: 0.0031343505
train_loss: 0.0030489136
test_loss: 0.0033795848
train_loss: 0.0030695742
test_loss: 0.0032662652
train_loss: 0.0034498821
test_loss: 0.003007741
train_loss: 0.0031590406
test_loss: 0.003509686
train_loss: 0.0029783137
test_loss: 0.003738018
train_loss: 0.003411632
test_loss: 0.0033344065
train_loss: 0.0031097406
test_loss: 0.0033693404
train_loss: 0.003354359
test_loss: 0.003113727
train_loss: 0.002966777
test_loss: 0.0032701849
train_loss: 0.0029112177
test_loss: 0.003701796
train_loss: 0.0034726844
test_loss: 0.0035762521
train_loss: 0.0035234452
test_loss: 0.0031145592
train_loss: 0.002873592
test_loss: 0.003251696
train_loss: 0.0032524562
test_loss: 0.0034362003
train_loss: 0.0030297807
test_loss: 0.0042429687
train_loss: 0.0036754136
test_loss: 0.0038643104
train_loss: 0.002985483
test_loss: 0.0027583688
train_loss: 0.003568967
test_loss: 0.0032724573
train_loss: 0.0034050387
test_loss: 0.003015082
train_loss: 0.002828489
test_loss: 0.0029266037
train_loss: 0.0036093488
test_loss: 0.0031461786
train_loss: 0.002961915
test_loss: 0.00325798
train_loss: 0.0029568777
test_loss: 0.0031755632
train_loss: 0.0028075448
test_loss: 0.0028301727
train_loss: 0.003177208
test_loss: 0.0030469636
train_loss: 0.0033101267
test_loss: 0.0036622637
train_loss: 0.0027202668
test_loss: 0.002965134
train_loss: 0.0028779043
test_loss: 0.0033719742
train_loss: 0.0030535432
test_loss: 0.003297603
train_loss: 0.0032171202
test_loss: 0.0035920085
train_loss: 0.0055345064
test_loss: 0.005154688
train_loss: 0.004021465
test_loss: 0.0033270954
train_loss: 0.0034211474
test_loss: 0.0038286746
train_loss: 0.003738896
test_loss: 0.0044322144
train_loss: 0.004082384
test_loss: 0.0037496632
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0030476667
test_loss: 0.0028933457
train_loss: 0.0031087515
test_loss: 0.0029797636
train_loss: 0.002915987
test_loss: 0.0030423517
train_loss: 0.0030018976
test_loss: 0.0030694595
train_loss: 0.0032037138
test_loss: 0.003587592
train_loss: 0.0035535186
test_loss: 0.0032013047
train_loss: 0.0034965677
test_loss: 0.00339864
train_loss: 0.0029974799
test_loss: 0.0031803495
train_loss: 0.0030508256
test_loss: 0.0028312933
train_loss: 0.0037778637
test_loss: 0.0035703557
train_loss: 0.005065458
test_loss: 0.004705481
train_loss: 0.0028868015
test_loss: 0.0033387144
train_loss: 0.0030864063
test_loss: 0.0033396701
train_loss: 0.0035444386
test_loss: 0.0032610758
train_loss: 0.002890083
test_loss: 0.0031808964
train_loss: 0.0026415854
test_loss: 0.0027913149
train_loss: 0.0029622747
test_loss: 0.003811203
train_loss: 0.0028442468
test_loss: 0.0036890446
train_loss: 0.0034519383
test_loss: 0.0036340256
train_loss: 0.0029883373
test_loss: 0.00309048
train_loss: 0.0034175513
test_loss: 0.0034180393
train_loss: 0.0027385082
test_loss: 0.0033844446
train_loss: 0.0027666765
test_loss: 0.003156749
train_loss: 0.004014695
test_loss: 0.0037152646
train_loss: 0.00494958
test_loss: 0.0046536066
train_loss: 0.0040562535
test_loss: 0.0042675827
train_loss: 0.0037854451
test_loss: 0.003930242
train_loss: 0.0031717871
test_loss: 0.0036785142
train_loss: 0.003215055
test_loss: 0.0031622835
train_loss: 0.0030738786
test_loss: 0.0029547007
train_loss: 0.0028501314
test_loss: 0.0027618848
train_loss: 0.0026071067
test_loss: 0.002847534
train_loss: 0.0030684767
test_loss: 0.0030497136
train_loss: 0.0031684549
test_loss: 0.0032601228
train_loss: 0.0033132245
test_loss: 0.0037723687
train_loss: 0.0036568874
test_loss: 0.0032583869
train_loss: 0.0027451003
test_loss: 0.0029700832
train_loss: 0.0028053066
test_loss: 0.0035661529
train_loss: 0.0030562903
test_loss: 0.0032177765
train_loss: 0.002986999
test_loss: 0.0032666964
train_loss: 0.0032233875
test_loss: 0.003231957
train_loss: 0.0035863104
test_loss: 0.003256308
train_loss: 0.003335118
test_loss: 0.0036495023
train_loss: 0.0038642376
test_loss: 0.0031612357
train_loss: 0.004170288
test_loss: 0.0036495496
train_loss: 0.0028766776
test_loss: 0.0030851227
train_loss: 0.0029489382
test_loss: 0.0031395543
train_loss: 0.0032569007
test_loss: 0.0032723472
train_loss: 0.0029294826
test_loss: 0.0029947727
train_loss: 0.0030447594
test_loss: 0.0036978016
train_loss: 0.00307324
test_loss: 0.0033379486
train_loss: 0.0025835999
test_loss: 0.002922076
train_loss: 0.003996902
test_loss: 0.0033409088
train_loss: 0.0033862034
test_loss: 0.0032250118
train_loss: 0.0027906965
test_loss: 0.0029670417
train_loss: 0.0034710444
test_loss: 0.0029325366
train_loss: 0.0033309613
test_loss: 0.0030576475
train_loss: 0.0036606232
test_loss: 0.0031283856
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output67/f1_psi0_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output66/f1_psi0_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output67/f1_psi0_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf440510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf453a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf3ce9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf47d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf395f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf3952f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf3950d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf321048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf3b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf3b8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf2fb1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf2a1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf2ba9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf26b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf2baae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf234c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf2346a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf211a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf234ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf1437b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf16f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3fc40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81cf16f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3f9cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3f9ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3f9c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3f9c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3f1f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3f1fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3ecaf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3eda1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3eda7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3e30840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3e7f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3de2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f81c3e1ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.70580297e-05
Iter: 2 loss: 3.86148895e-05
Iter: 3 loss: 1.2279972e-05
Iter: 4 loss: 9.91016805e-06
Iter: 5 loss: 1.2356064e-05
Iter: 6 loss: 8.59178181e-06
Iter: 7 loss: 7.28694476e-06
Iter: 8 loss: 7.94363586e-06
Iter: 9 loss: 6.41622319e-06
Iter: 10 loss: 5.30064381e-06
Iter: 11 loss: 8.96100664e-06
Iter: 12 loss: 4.98982081e-06
Iter: 13 loss: 4.5361694e-06
Iter: 14 loss: 9.1774109e-06
Iter: 15 loss: 4.5229126e-06
Iter: 16 loss: 4.23505662e-06
Iter: 17 loss: 4.21933373e-06
Iter: 18 loss: 3.99967848e-06
Iter: 19 loss: 3.7779364e-06
Iter: 20 loss: 3.77565175e-06
Iter: 21 loss: 3.66382983e-06
Iter: 22 loss: 3.71323449e-06
Iter: 23 loss: 3.58768148e-06
Iter: 24 loss: 3.46574825e-06
Iter: 25 loss: 4.56311591e-06
Iter: 26 loss: 3.45991202e-06
Iter: 27 loss: 3.38032419e-06
Iter: 28 loss: 3.38445352e-06
Iter: 29 loss: 3.31770343e-06
Iter: 30 loss: 3.21257494e-06
Iter: 31 loss: 3.51143149e-06
Iter: 32 loss: 3.17928561e-06
Iter: 33 loss: 3.10911332e-06
Iter: 34 loss: 3.54710392e-06
Iter: 35 loss: 3.10085625e-06
Iter: 36 loss: 3.07291771e-06
Iter: 37 loss: 3.06758966e-06
Iter: 38 loss: 3.03236629e-06
Iter: 39 loss: 2.94960114e-06
Iter: 40 loss: 3.93409164e-06
Iter: 41 loss: 2.9425978e-06
Iter: 42 loss: 2.89223317e-06
Iter: 43 loss: 3.53689347e-06
Iter: 44 loss: 2.891858e-06
Iter: 45 loss: 2.85666465e-06
Iter: 46 loss: 2.83881354e-06
Iter: 47 loss: 2.82248243e-06
Iter: 48 loss: 2.76621222e-06
Iter: 49 loss: 2.91958804e-06
Iter: 50 loss: 2.74784452e-06
Iter: 51 loss: 2.70296414e-06
Iter: 52 loss: 2.96956841e-06
Iter: 53 loss: 2.69725297e-06
Iter: 54 loss: 2.65730887e-06
Iter: 55 loss: 2.67155929e-06
Iter: 56 loss: 2.62936032e-06
Iter: 57 loss: 2.58736145e-06
Iter: 58 loss: 2.99430371e-06
Iter: 59 loss: 2.58588102e-06
Iter: 60 loss: 2.56343469e-06
Iter: 61 loss: 2.65824656e-06
Iter: 62 loss: 2.55859618e-06
Iter: 63 loss: 2.53792155e-06
Iter: 64 loss: 2.54486167e-06
Iter: 65 loss: 2.52322116e-06
Iter: 66 loss: 2.49943787e-06
Iter: 67 loss: 2.66124744e-06
Iter: 68 loss: 2.49708091e-06
Iter: 69 loss: 2.48219294e-06
Iter: 70 loss: 2.46446962e-06
Iter: 71 loss: 2.46259447e-06
Iter: 72 loss: 2.4820315e-06
Iter: 73 loss: 2.45307206e-06
Iter: 74 loss: 2.44651642e-06
Iter: 75 loss: 2.43394516e-06
Iter: 76 loss: 2.69574139e-06
Iter: 77 loss: 2.43396244e-06
Iter: 78 loss: 2.42144938e-06
Iter: 79 loss: 2.41322186e-06
Iter: 80 loss: 2.40827285e-06
Iter: 81 loss: 2.38918528e-06
Iter: 82 loss: 2.57679812e-06
Iter: 83 loss: 2.38854545e-06
Iter: 84 loss: 2.37595827e-06
Iter: 85 loss: 2.37380755e-06
Iter: 86 loss: 2.3651578e-06
Iter: 87 loss: 2.34563845e-06
Iter: 88 loss: 2.39045539e-06
Iter: 89 loss: 2.33846663e-06
Iter: 90 loss: 2.32546154e-06
Iter: 91 loss: 2.46691707e-06
Iter: 92 loss: 2.32507091e-06
Iter: 93 loss: 2.31318973e-06
Iter: 94 loss: 2.30338424e-06
Iter: 95 loss: 2.2999543e-06
Iter: 96 loss: 2.28909425e-06
Iter: 97 loss: 2.28912586e-06
Iter: 98 loss: 2.2797808e-06
Iter: 99 loss: 2.2694137e-06
Iter: 100 loss: 2.26810289e-06
Iter: 101 loss: 2.25364329e-06
Iter: 102 loss: 2.37441145e-06
Iter: 103 loss: 2.25281951e-06
Iter: 104 loss: 2.24355745e-06
Iter: 105 loss: 2.25982103e-06
Iter: 106 loss: 2.23948973e-06
Iter: 107 loss: 2.22926974e-06
Iter: 108 loss: 2.2506656e-06
Iter: 109 loss: 2.22533458e-06
Iter: 110 loss: 2.22279118e-06
Iter: 111 loss: 2.21918663e-06
Iter: 112 loss: 2.2167626e-06
Iter: 113 loss: 2.2088243e-06
Iter: 114 loss: 2.22704352e-06
Iter: 115 loss: 2.20430456e-06
Iter: 116 loss: 2.19268804e-06
Iter: 117 loss: 2.25535518e-06
Iter: 118 loss: 2.19090543e-06
Iter: 119 loss: 2.18198102e-06
Iter: 120 loss: 2.23953589e-06
Iter: 121 loss: 2.18102377e-06
Iter: 122 loss: 2.17329898e-06
Iter: 123 loss: 2.17861634e-06
Iter: 124 loss: 2.16839157e-06
Iter: 125 loss: 2.16100625e-06
Iter: 126 loss: 2.1783128e-06
Iter: 127 loss: 2.15827981e-06
Iter: 128 loss: 2.14989132e-06
Iter: 129 loss: 2.17313573e-06
Iter: 130 loss: 2.14711326e-06
Iter: 131 loss: 2.13976136e-06
Iter: 132 loss: 2.18906621e-06
Iter: 133 loss: 2.13886278e-06
Iter: 134 loss: 2.13264866e-06
Iter: 135 loss: 2.13368321e-06
Iter: 136 loss: 2.12808209e-06
Iter: 137 loss: 2.1191895e-06
Iter: 138 loss: 2.15832029e-06
Iter: 139 loss: 2.11740303e-06
Iter: 140 loss: 2.11062525e-06
Iter: 141 loss: 2.14416468e-06
Iter: 142 loss: 2.1094188e-06
Iter: 143 loss: 2.10431517e-06
Iter: 144 loss: 2.09908558e-06
Iter: 145 loss: 2.09817586e-06
Iter: 146 loss: 2.09590621e-06
Iter: 147 loss: 2.09380801e-06
Iter: 148 loss: 2.09009454e-06
Iter: 149 loss: 2.10766689e-06
Iter: 150 loss: 2.08951e-06
Iter: 151 loss: 2.08756774e-06
Iter: 152 loss: 2.08191932e-06
Iter: 153 loss: 2.10613553e-06
Iter: 154 loss: 2.07979383e-06
Iter: 155 loss: 2.07106018e-06
Iter: 156 loss: 2.11019119e-06
Iter: 157 loss: 2.06933555e-06
Iter: 158 loss: 2.06421123e-06
Iter: 159 loss: 2.10455846e-06
Iter: 160 loss: 2.06389359e-06
Iter: 161 loss: 2.05786637e-06
Iter: 162 loss: 2.05789729e-06
Iter: 163 loss: 2.05313768e-06
Iter: 164 loss: 2.04728303e-06
Iter: 165 loss: 2.05573429e-06
Iter: 166 loss: 2.04447269e-06
Iter: 167 loss: 2.03645209e-06
Iter: 168 loss: 2.0667942e-06
Iter: 169 loss: 2.03457398e-06
Iter: 170 loss: 2.02864e-06
Iter: 171 loss: 2.05898914e-06
Iter: 172 loss: 2.02767956e-06
Iter: 173 loss: 2.02148567e-06
Iter: 174 loss: 2.02213482e-06
Iter: 175 loss: 2.01681632e-06
Iter: 176 loss: 2.01091757e-06
Iter: 177 loss: 2.07434528e-06
Iter: 178 loss: 2.01081139e-06
Iter: 179 loss: 2.00656791e-06
Iter: 180 loss: 2.00944919e-06
Iter: 181 loss: 2.0038317e-06
Iter: 182 loss: 1.99857118e-06
Iter: 183 loss: 2.02294609e-06
Iter: 184 loss: 1.99753731e-06
Iter: 185 loss: 1.99392798e-06
Iter: 186 loss: 1.99366423e-06
Iter: 187 loss: 1.99254555e-06
Iter: 188 loss: 1.98920407e-06
Iter: 189 loss: 2.01164266e-06
Iter: 190 loss: 1.98840462e-06
Iter: 191 loss: 1.98354883e-06
Iter: 192 loss: 1.99164219e-06
Iter: 193 loss: 1.98137695e-06
Iter: 194 loss: 1.97731924e-06
Iter: 195 loss: 1.99311626e-06
Iter: 196 loss: 1.97644e-06
Iter: 197 loss: 1.9718791e-06
Iter: 198 loss: 1.97832196e-06
Iter: 199 loss: 1.96960877e-06
Iter: 200 loss: 1.96413521e-06
Iter: 201 loss: 2.00096611e-06
Iter: 202 loss: 1.96360475e-06
Iter: 203 loss: 1.95928919e-06
Iter: 204 loss: 1.95600069e-06
Iter: 205 loss: 1.95464941e-06
Iter: 206 loss: 1.94999e-06
Iter: 207 loss: 1.98087423e-06
Iter: 208 loss: 1.94964218e-06
Iter: 209 loss: 1.94503173e-06
Iter: 210 loss: 1.95773509e-06
Iter: 211 loss: 1.94365862e-06
Iter: 212 loss: 1.93921528e-06
Iter: 213 loss: 1.95884013e-06
Iter: 214 loss: 1.93842538e-06
Iter: 215 loss: 1.93549795e-06
Iter: 216 loss: 1.94104632e-06
Iter: 217 loss: 1.93427149e-06
Iter: 218 loss: 1.93056485e-06
Iter: 219 loss: 1.94032236e-06
Iter: 220 loss: 1.92928337e-06
Iter: 221 loss: 1.92877e-06
Iter: 222 loss: 1.92777816e-06
Iter: 223 loss: 1.92670495e-06
Iter: 224 loss: 1.92445668e-06
Iter: 225 loss: 1.97174813e-06
Iter: 226 loss: 1.92442599e-06
Iter: 227 loss: 1.92209291e-06
Iter: 228 loss: 1.9198967e-06
Iter: 229 loss: 1.91943832e-06
Iter: 230 loss: 1.91539812e-06
Iter: 231 loss: 1.92758694e-06
Iter: 232 loss: 1.91419281e-06
Iter: 233 loss: 1.91075196e-06
Iter: 234 loss: 1.92176276e-06
Iter: 235 loss: 1.9097638e-06
Iter: 236 loss: 1.90598917e-06
Iter: 237 loss: 1.92091693e-06
Iter: 238 loss: 1.90519518e-06
Iter: 239 loss: 1.90238472e-06
Iter: 240 loss: 1.91367099e-06
Iter: 241 loss: 1.90172602e-06
Iter: 242 loss: 1.89826312e-06
Iter: 243 loss: 1.9022026e-06
Iter: 244 loss: 1.89644743e-06
Iter: 245 loss: 1.89413015e-06
Iter: 246 loss: 1.89745504e-06
Iter: 247 loss: 1.89298532e-06
Iter: 248 loss: 1.88968579e-06
Iter: 249 loss: 1.8956365e-06
Iter: 250 loss: 1.88820286e-06
Iter: 251 loss: 1.88588115e-06
Iter: 252 loss: 1.88577224e-06
Iter: 253 loss: 1.88438139e-06
Iter: 254 loss: 1.88232457e-06
Iter: 255 loss: 1.8822742e-06
Iter: 256 loss: 1.87908677e-06
Iter: 257 loss: 1.89861e-06
Iter: 258 loss: 1.87873093e-06
Iter: 259 loss: 1.876856e-06
Iter: 260 loss: 1.88932017e-06
Iter: 261 loss: 1.87662909e-06
Iter: 262 loss: 1.87529804e-06
Iter: 263 loss: 1.87519549e-06
Iter: 264 loss: 1.87434819e-06
Iter: 265 loss: 1.87182195e-06
Iter: 266 loss: 1.88935394e-06
Iter: 267 loss: 1.87131536e-06
Iter: 268 loss: 1.86889667e-06
Iter: 269 loss: 1.87558135e-06
Iter: 270 loss: 1.86809132e-06
Iter: 271 loss: 1.86534771e-06
Iter: 272 loss: 1.87028184e-06
Iter: 273 loss: 1.86414798e-06
Iter: 274 loss: 1.86184184e-06
Iter: 275 loss: 1.86967384e-06
Iter: 276 loss: 1.8611596e-06
Iter: 277 loss: 1.85858971e-06
Iter: 278 loss: 1.87229728e-06
Iter: 279 loss: 1.85804481e-06
Iter: 280 loss: 1.85602744e-06
Iter: 281 loss: 1.86366742e-06
Iter: 282 loss: 1.85558656e-06
Iter: 283 loss: 1.85349472e-06
Iter: 284 loss: 1.8539298e-06
Iter: 285 loss: 1.85211275e-06
Iter: 286 loss: 1.84970702e-06
Iter: 287 loss: 1.86080422e-06
Iter: 288 loss: 1.84940473e-06
Iter: 289 loss: 1.84761893e-06
Iter: 290 loss: 1.8469218e-06
Iter: 291 loss: 1.84591488e-06
Iter: 292 loss: 1.84323665e-06
Iter: 293 loss: 1.88232616e-06
Iter: 294 loss: 1.84328803e-06
Iter: 295 loss: 1.84181249e-06
Iter: 296 loss: 1.83904024e-06
Iter: 297 loss: 1.900342e-06
Iter: 298 loss: 1.83908173e-06
Iter: 299 loss: 1.83858481e-06
Iter: 300 loss: 1.8373313e-06
Iter: 301 loss: 1.83588691e-06
Iter: 302 loss: 1.83688769e-06
Iter: 303 loss: 1.83514487e-06
Iter: 304 loss: 1.83377904e-06
Iter: 305 loss: 1.83171892e-06
Iter: 306 loss: 1.83175007e-06
Iter: 307 loss: 1.82949032e-06
Iter: 308 loss: 1.84035866e-06
Iter: 309 loss: 1.82901397e-06
Iter: 310 loss: 1.82736221e-06
Iter: 311 loss: 1.82892677e-06
Iter: 312 loss: 1.82642896e-06
Iter: 313 loss: 1.82423378e-06
Iter: 314 loss: 1.83259647e-06
Iter: 315 loss: 1.82370354e-06
Iter: 316 loss: 1.82187091e-06
Iter: 317 loss: 1.824425e-06
Iter: 318 loss: 1.82089366e-06
Iter: 319 loss: 1.81859036e-06
Iter: 320 loss: 1.83329894e-06
Iter: 321 loss: 1.8182875e-06
Iter: 322 loss: 1.81647806e-06
Iter: 323 loss: 1.82474264e-06
Iter: 324 loss: 1.81608516e-06
Iter: 325 loss: 1.81474365e-06
Iter: 326 loss: 1.81474968e-06
Iter: 327 loss: 1.81366124e-06
Iter: 328 loss: 1.81146993e-06
Iter: 329 loss: 1.81622863e-06
Iter: 330 loss: 1.81062683e-06
Iter: 331 loss: 1.80882716e-06
Iter: 332 loss: 1.81561859e-06
Iter: 333 loss: 1.80843119e-06
Iter: 334 loss: 1.80657071e-06
Iter: 335 loss: 1.8141576e-06
Iter: 336 loss: 1.8061188e-06
Iter: 337 loss: 1.80460597e-06
Iter: 338 loss: 1.80466225e-06
Iter: 339 loss: 1.80349093e-06
Iter: 340 loss: 1.80318841e-06
Iter: 341 loss: 1.80247127e-06
Iter: 342 loss: 1.80140762e-06
Iter: 343 loss: 1.7995701e-06
Iter: 344 loss: 1.79954986e-06
Iter: 345 loss: 1.79754113e-06
Iter: 346 loss: 1.8132539e-06
Iter: 347 loss: 1.79745223e-06
Iter: 348 loss: 1.79554172e-06
Iter: 349 loss: 1.79467224e-06
Iter: 350 loss: 1.79361e-06
Iter: 351 loss: 1.79121389e-06
Iter: 352 loss: 1.79826134e-06
Iter: 353 loss: 1.79037067e-06
Iter: 354 loss: 1.7878923e-06
Iter: 355 loss: 1.78954895e-06
Iter: 356 loss: 1.78624066e-06
Iter: 357 loss: 1.78444157e-06
Iter: 358 loss: 1.78425319e-06
Iter: 359 loss: 1.78288542e-06
Iter: 360 loss: 1.78192658e-06
Iter: 361 loss: 1.78138487e-06
Iter: 362 loss: 1.77896027e-06
Iter: 363 loss: 1.7903584e-06
Iter: 364 loss: 1.77849347e-06
Iter: 365 loss: 1.77721688e-06
Iter: 366 loss: 1.77628169e-06
Iter: 367 loss: 1.77584229e-06
Iter: 368 loss: 1.77352661e-06
Iter: 369 loss: 1.7931618e-06
Iter: 370 loss: 1.77342451e-06
Iter: 371 loss: 1.77336369e-06
Iter: 372 loss: 1.7727275e-06
Iter: 373 loss: 1.77210325e-06
Iter: 374 loss: 1.77057268e-06
Iter: 375 loss: 1.778453e-06
Iter: 376 loss: 1.77000572e-06
Iter: 377 loss: 1.76834453e-06
Iter: 378 loss: 1.78733671e-06
Iter: 379 loss: 1.76828883e-06
Iter: 380 loss: 1.76717379e-06
Iter: 381 loss: 1.7659e-06
Iter: 382 loss: 1.76573838e-06
Iter: 383 loss: 1.7634e-06
Iter: 384 loss: 1.76890558e-06
Iter: 385 loss: 1.76255639e-06
Iter: 386 loss: 1.76096091e-06
Iter: 387 loss: 1.77124036e-06
Iter: 388 loss: 1.76071489e-06
Iter: 389 loss: 1.75883713e-06
Iter: 390 loss: 1.75833804e-06
Iter: 391 loss: 1.75712933e-06
Iter: 392 loss: 1.75496552e-06
Iter: 393 loss: 1.76735603e-06
Iter: 394 loss: 1.75456898e-06
Iter: 395 loss: 1.75302591e-06
Iter: 396 loss: 1.75610887e-06
Iter: 397 loss: 1.7524859e-06
Iter: 398 loss: 1.75072717e-06
Iter: 399 loss: 1.76453955e-06
Iter: 400 loss: 1.75064383e-06
Iter: 401 loss: 1.74940851e-06
Iter: 402 loss: 1.7497789e-06
Iter: 403 loss: 1.74861259e-06
Iter: 404 loss: 1.74707498e-06
Iter: 405 loss: 1.74967215e-06
Iter: 406 loss: 1.74645504e-06
Iter: 407 loss: 1.74587331e-06
Iter: 408 loss: 1.7455186e-06
Iter: 409 loss: 1.74492664e-06
Iter: 410 loss: 1.74340016e-06
Iter: 411 loss: 1.75873231e-06
Iter: 412 loss: 1.74324691e-06
Iter: 413 loss: 1.74162437e-06
Iter: 414 loss: 1.74425134e-06
Iter: 415 loss: 1.74088416e-06
Iter: 416 loss: 1.73933518e-06
Iter: 417 loss: 1.74921547e-06
Iter: 418 loss: 1.7392108e-06
Iter: 419 loss: 1.73754825e-06
Iter: 420 loss: 1.73588796e-06
Iter: 421 loss: 1.73551916e-06
Iter: 422 loss: 1.73359342e-06
Iter: 423 loss: 1.75581533e-06
Iter: 424 loss: 1.73354738e-06
Iter: 425 loss: 1.73212845e-06
Iter: 426 loss: 1.73142655e-06
Iter: 427 loss: 1.73073352e-06
Iter: 428 loss: 1.72872842e-06
Iter: 429 loss: 1.73878038e-06
Iter: 430 loss: 1.72841374e-06
Iter: 431 loss: 1.72672799e-06
Iter: 432 loss: 1.72689136e-06
Iter: 433 loss: 1.72551461e-06
Iter: 434 loss: 1.72355067e-06
Iter: 435 loss: 1.72350337e-06
Iter: 436 loss: 1.72236207e-06
Iter: 437 loss: 1.7230667e-06
Iter: 438 loss: 1.72162959e-06
Iter: 439 loss: 1.72005798e-06
Iter: 440 loss: 1.72271734e-06
Iter: 441 loss: 1.7193147e-06
Iter: 442 loss: 1.71795489e-06
Iter: 443 loss: 1.73169985e-06
Iter: 444 loss: 1.71791908e-06
Iter: 445 loss: 1.71715919e-06
Iter: 446 loss: 1.71721456e-06
Iter: 447 loss: 1.71649424e-06
Iter: 448 loss: 1.71472936e-06
Iter: 449 loss: 1.72631189e-06
Iter: 450 loss: 1.71423403e-06
Iter: 451 loss: 1.7129147e-06
Iter: 452 loss: 1.71533168e-06
Iter: 453 loss: 1.712369e-06
Iter: 454 loss: 1.71086617e-06
Iter: 455 loss: 1.72204591e-06
Iter: 456 loss: 1.71077284e-06
Iter: 457 loss: 1.70955877e-06
Iter: 458 loss: 1.71410784e-06
Iter: 459 loss: 1.70928092e-06
Iter: 460 loss: 1.70815463e-06
Iter: 461 loss: 1.70683984e-06
Iter: 462 loss: 1.70667647e-06
Iter: 463 loss: 1.70523595e-06
Iter: 464 loss: 1.72356408e-06
Iter: 465 loss: 1.70515693e-06
Iter: 466 loss: 1.70401927e-06
Iter: 467 loss: 1.70389717e-06
Iter: 468 loss: 1.70303213e-06
Iter: 469 loss: 1.70123633e-06
Iter: 470 loss: 1.70348369e-06
Iter: 471 loss: 1.70036674e-06
Iter: 472 loss: 1.69885652e-06
Iter: 473 loss: 1.69885402e-06
Iter: 474 loss: 1.6978297e-06
Iter: 475 loss: 1.69919065e-06
Iter: 476 loss: 1.69734926e-06
Iter: 477 loss: 1.6962058e-06
Iter: 478 loss: 1.69781856e-06
Iter: 479 loss: 1.69560485e-06
Iter: 480 loss: 1.69424436e-06
Iter: 481 loss: 1.69711302e-06
Iter: 482 loss: 1.69365353e-06
Iter: 483 loss: 1.69373834e-06
Iter: 484 loss: 1.69322743e-06
Iter: 485 loss: 1.69268253e-06
Iter: 486 loss: 1.69189798e-06
Iter: 487 loss: 1.71155739e-06
Iter: 488 loss: 1.69189298e-06
Iter: 489 loss: 1.69086388e-06
Iter: 490 loss: 1.69034615e-06
Iter: 491 loss: 1.68987981e-06
Iter: 492 loss: 1.68857161e-06
Iter: 493 loss: 1.69331418e-06
Iter: 494 loss: 1.68828319e-06
Iter: 495 loss: 1.68700558e-06
Iter: 496 loss: 1.68699808e-06
Iter: 497 loss: 1.68599399e-06
Iter: 498 loss: 1.68470865e-06
Iter: 499 loss: 1.68470933e-06
Iter: 500 loss: 1.68366091e-06
Iter: 501 loss: 1.68417512e-06
Iter: 502 loss: 1.68286238e-06
Iter: 503 loss: 1.68156146e-06
Iter: 504 loss: 1.68300903e-06
Iter: 505 loss: 1.68080692e-06
Iter: 506 loss: 1.6794263e-06
Iter: 507 loss: 1.68110921e-06
Iter: 508 loss: 1.67864357e-06
Iter: 509 loss: 1.67693133e-06
Iter: 510 loss: 1.68902955e-06
Iter: 511 loss: 1.67685664e-06
Iter: 512 loss: 1.67572932e-06
Iter: 513 loss: 1.67618691e-06
Iter: 514 loss: 1.67500946e-06
Iter: 515 loss: 1.6735936e-06
Iter: 516 loss: 1.68607301e-06
Iter: 517 loss: 1.67352073e-06
Iter: 518 loss: 1.67290659e-06
Iter: 519 loss: 1.67292546e-06
Iter: 520 loss: 1.67225028e-06
Iter: 521 loss: 1.67250892e-06
Iter: 522 loss: 1.67172482e-06
Iter: 523 loss: 1.67099336e-06
Iter: 524 loss: 1.67006965e-06
Iter: 525 loss: 1.67005271e-06
Iter: 526 loss: 1.66870473e-06
Iter: 527 loss: 1.67353232e-06
Iter: 528 loss: 1.66846644e-06
Iter: 529 loss: 1.66725761e-06
Iter: 530 loss: 1.66703489e-06
Iter: 531 loss: 1.66624181e-06
Iter: 532 loss: 1.66467271e-06
Iter: 533 loss: 1.67811e-06
Iter: 534 loss: 1.66460291e-06
Iter: 535 loss: 1.66347911e-06
Iter: 536 loss: 1.66304403e-06
Iter: 537 loss: 1.6624881e-06
Iter: 538 loss: 1.66109282e-06
Iter: 539 loss: 1.66482391e-06
Iter: 540 loss: 1.66057771e-06
Iter: 541 loss: 1.65911206e-06
Iter: 542 loss: 1.66704422e-06
Iter: 543 loss: 1.65898541e-06
Iter: 544 loss: 1.65757433e-06
Iter: 545 loss: 1.66475206e-06
Iter: 546 loss: 1.65742335e-06
Iter: 547 loss: 1.65620315e-06
Iter: 548 loss: 1.65656274e-06
Iter: 549 loss: 1.65534072e-06
Iter: 550 loss: 1.65378765e-06
Iter: 551 loss: 1.65803817e-06
Iter: 552 loss: 1.65334063e-06
Iter: 553 loss: 1.65376446e-06
Iter: 554 loss: 1.65279039e-06
Iter: 555 loss: 1.65243353e-06
Iter: 556 loss: 1.65162191e-06
Iter: 557 loss: 1.66148516e-06
Iter: 558 loss: 1.65153142e-06
Iter: 559 loss: 1.65035749e-06
Iter: 560 loss: 1.65040728e-06
Iter: 561 loss: 1.6495261e-06
Iter: 562 loss: 1.64799258e-06
Iter: 563 loss: 1.66150915e-06
Iter: 564 loss: 1.6480185e-06
Iter: 565 loss: 1.64718062e-06
Iter: 566 loss: 1.64684127e-06
Iter: 567 loss: 1.64652579e-06
Iter: 568 loss: 1.6451944e-06
Iter: 569 loss: 1.64820176e-06
Iter: 570 loss: 1.64470771e-06
Iter: 571 loss: 1.64341725e-06
Iter: 572 loss: 1.64795415e-06
Iter: 573 loss: 1.64299865e-06
Iter: 574 loss: 1.64169137e-06
Iter: 575 loss: 1.64267237e-06
Iter: 576 loss: 1.64084327e-06
Iter: 577 loss: 1.63939455e-06
Iter: 578 loss: 1.64537346e-06
Iter: 579 loss: 1.63899733e-06
Iter: 580 loss: 1.63756863e-06
Iter: 581 loss: 1.64783501e-06
Iter: 582 loss: 1.63742152e-06
Iter: 583 loss: 1.63645473e-06
Iter: 584 loss: 1.63605068e-06
Iter: 585 loss: 1.63547816e-06
Iter: 586 loss: 1.63435755e-06
Iter: 587 loss: 1.63433742e-06
Iter: 588 loss: 1.63342793e-06
Iter: 589 loss: 1.63940899e-06
Iter: 590 loss: 1.63329344e-06
Iter: 591 loss: 1.63277036e-06
Iter: 592 loss: 1.63180357e-06
Iter: 593 loss: 1.6317756e-06
Iter: 594 loss: 1.63083314e-06
Iter: 595 loss: 1.63138475e-06
Iter: 596 loss: 1.63035315e-06
Iter: 597 loss: 1.62927608e-06
Iter: 598 loss: 1.63732682e-06
Iter: 599 loss: 1.62921094e-06
Iter: 600 loss: 1.62840456e-06
Iter: 601 loss: 1.63089408e-06
Iter: 602 loss: 1.62812717e-06
Iter: 603 loss: 1.62728929e-06
Iter: 604 loss: 1.62802644e-06
Iter: 605 loss: 1.62679044e-06
Iter: 606 loss: 1.62579818e-06
Iter: 607 loss: 1.6264429e-06
Iter: 608 loss: 1.62515289e-06
Iter: 609 loss: 1.62386175e-06
Iter: 610 loss: 1.62814263e-06
Iter: 611 loss: 1.62350295e-06
Iter: 612 loss: 1.62232186e-06
Iter: 613 loss: 1.62838046e-06
Iter: 614 loss: 1.62224092e-06
Iter: 615 loss: 1.62118135e-06
Iter: 616 loss: 1.62028255e-06
Iter: 617 loss: 1.61998901e-06
Iter: 618 loss: 1.61870844e-06
Iter: 619 loss: 1.61866842e-06
Iter: 620 loss: 1.61793969e-06
Iter: 621 loss: 1.62655e-06
Iter: 622 loss: 1.61798403e-06
Iter: 623 loss: 1.61719095e-06
Iter: 624 loss: 1.61748744e-06
Iter: 625 loss: 1.61664616e-06
Iter: 626 loss: 1.61570938e-06
Iter: 627 loss: 1.61453681e-06
Iter: 628 loss: 1.61456296e-06
Iter: 629 loss: 1.61349612e-06
Iter: 630 loss: 1.61436992e-06
Iter: 631 loss: 1.61297521e-06
Iter: 632 loss: 1.61170306e-06
Iter: 633 loss: 1.62388062e-06
Iter: 634 loss: 1.61176672e-06
Iter: 635 loss: 1.61093135e-06
Iter: 636 loss: 1.61288312e-06
Iter: 637 loss: 1.61052662e-06
Iter: 638 loss: 1.60976447e-06
Iter: 639 loss: 1.61097478e-06
Iter: 640 loss: 1.60929176e-06
Iter: 641 loss: 1.60845252e-06
Iter: 642 loss: 1.61281037e-06
Iter: 643 loss: 1.60825152e-06
Iter: 644 loss: 1.60750756e-06
Iter: 645 loss: 1.60742889e-06
Iter: 646 loss: 1.60677655e-06
Iter: 647 loss: 1.6057196e-06
Iter: 648 loss: 1.60804677e-06
Iter: 649 loss: 1.60533659e-06
Iter: 650 loss: 1.60404488e-06
Iter: 651 loss: 1.60707191e-06
Iter: 652 loss: 1.60351442e-06
Iter: 653 loss: 1.60212153e-06
Iter: 654 loss: 1.60484274e-06
Iter: 655 loss: 1.60149739e-06
Iter: 656 loss: 1.60190302e-06
Iter: 657 loss: 1.60088257e-06
Iter: 658 loss: 1.6004451e-06
Iter: 659 loss: 1.59955846e-06
Iter: 660 loss: 1.61357093e-06
Iter: 661 loss: 1.59953606e-06
Iter: 662 loss: 1.59841375e-06
Iter: 663 loss: 1.59975684e-06
Iter: 664 loss: 1.59795127e-06
Iter: 665 loss: 1.59708338e-06
Iter: 666 loss: 1.60271611e-06
Iter: 667 loss: 1.59690603e-06
Iter: 668 loss: 1.59598335e-06
Iter: 669 loss: 1.59563342e-06
Iter: 670 loss: 1.59516367e-06
Iter: 671 loss: 1.59398132e-06
Iter: 672 loss: 1.59678757e-06
Iter: 673 loss: 1.59363481e-06
Iter: 674 loss: 1.59249907e-06
Iter: 675 loss: 1.59692149e-06
Iter: 676 loss: 1.59225647e-06
Iter: 677 loss: 1.59113142e-06
Iter: 678 loss: 1.60059517e-06
Iter: 679 loss: 1.59106798e-06
Iter: 680 loss: 1.59039087e-06
Iter: 681 loss: 1.58921841e-06
Iter: 682 loss: 1.58930652e-06
Iter: 683 loss: 1.58781813e-06
Iter: 684 loss: 1.60127388e-06
Iter: 685 loss: 1.58778903e-06
Iter: 686 loss: 1.58681087e-06
Iter: 687 loss: 1.58792818e-06
Iter: 688 loss: 1.58636703e-06
Iter: 689 loss: 1.58512012e-06
Iter: 690 loss: 1.58726414e-06
Iter: 691 loss: 1.58450666e-06
Iter: 692 loss: 1.58520879e-06
Iter: 693 loss: 1.58416356e-06
Iter: 694 loss: 1.58383591e-06
Iter: 695 loss: 1.58307921e-06
Iter: 696 loss: 1.58919067e-06
Iter: 697 loss: 1.58293733e-06
Iter: 698 loss: 1.58198122e-06
Iter: 699 loss: 1.58233411e-06
Iter: 700 loss: 1.5812343e-06
Iter: 701 loss: 1.58011858e-06
Iter: 702 loss: 1.58491014e-06
Iter: 703 loss: 1.57999466e-06
Iter: 704 loss: 1.57894351e-06
Iter: 705 loss: 1.5806786e-06
Iter: 706 loss: 1.57845193e-06
Iter: 707 loss: 1.5773835e-06
Iter: 708 loss: 1.5835999e-06
Iter: 709 loss: 1.57726254e-06
Iter: 710 loss: 1.57635668e-06
Iter: 711 loss: 1.58033026e-06
Iter: 712 loss: 1.57615636e-06
Iter: 713 loss: 1.57546128e-06
Iter: 714 loss: 1.57500597e-06
Iter: 715 loss: 1.57456282e-06
Iter: 716 loss: 1.57353725e-06
Iter: 717 loss: 1.57734326e-06
Iter: 718 loss: 1.57325428e-06
Iter: 719 loss: 1.57223167e-06
Iter: 720 loss: 1.57951933e-06
Iter: 721 loss: 1.57221587e-06
Iter: 722 loss: 1.57127658e-06
Iter: 723 loss: 1.57244745e-06
Iter: 724 loss: 1.57080285e-06
Iter: 725 loss: 1.57009799e-06
Iter: 726 loss: 1.57074783e-06
Iter: 727 loss: 1.56970168e-06
Iter: 728 loss: 1.56922226e-06
Iter: 729 loss: 1.56912199e-06
Iter: 730 loss: 1.56880219e-06
Iter: 731 loss: 1.56822625e-06
Iter: 732 loss: 1.58183764e-06
Iter: 733 loss: 1.56826547e-06
Iter: 734 loss: 1.56748274e-06
Iter: 735 loss: 1.56684109e-06
Iter: 736 loss: 1.56672229e-06
Iter: 737 loss: 1.56569831e-06
Iter: 738 loss: 1.57037755e-06
Iter: 739 loss: 1.56539988e-06
Iter: 740 loss: 1.56435158e-06
Iter: 741 loss: 1.56558747e-06
Iter: 742 loss: 1.56394185e-06
Iter: 743 loss: 1.56268266e-06
Iter: 744 loss: 1.5680971e-06
Iter: 745 loss: 1.562443e-06
Iter: 746 loss: 1.56146791e-06
Iter: 747 loss: 1.56255612e-06
Iter: 748 loss: 1.56103692e-06
Iter: 749 loss: 1.5599054e-06
Iter: 750 loss: 1.56634269e-06
Iter: 751 loss: 1.55976818e-06
Iter: 752 loss: 1.55897249e-06
Iter: 753 loss: 1.56084639e-06
Iter: 754 loss: 1.55861096e-06
Iter: 755 loss: 1.55774251e-06
Iter: 756 loss: 1.56319481e-06
Iter: 757 loss: 1.55763394e-06
Iter: 758 loss: 1.55708199e-06
Iter: 759 loss: 1.55724808e-06
Iter: 760 loss: 1.55670023e-06
Iter: 761 loss: 1.55621456e-06
Iter: 762 loss: 1.55622956e-06
Iter: 763 loss: 1.55569751e-06
Iter: 764 loss: 1.55567159e-06
Iter: 765 loss: 1.55521536e-06
Iter: 766 loss: 1.55469479e-06
Iter: 767 loss: 1.55589316e-06
Iter: 768 loss: 1.5544565e-06
Iter: 769 loss: 1.55387136e-06
Iter: 770 loss: 1.55281941e-06
Iter: 771 loss: 1.57694592e-06
Iter: 772 loss: 1.5528401e-06
Iter: 773 loss: 1.55145585e-06
Iter: 774 loss: 1.55744215e-06
Iter: 775 loss: 1.55125758e-06
Iter: 776 loss: 1.55031989e-06
Iter: 777 loss: 1.55830935e-06
Iter: 778 loss: 1.55029545e-06
Iter: 779 loss: 1.54947611e-06
Iter: 780 loss: 1.54871248e-06
Iter: 781 loss: 1.54849522e-06
Iter: 782 loss: 1.54722011e-06
Iter: 783 loss: 1.55693601e-06
Iter: 784 loss: 1.54720954e-06
Iter: 785 loss: 1.54646955e-06
Iter: 786 loss: 1.54783424e-06
Iter: 787 loss: 1.54615554e-06
Iter: 788 loss: 1.54536178e-06
Iter: 789 loss: 1.54768964e-06
Iter: 790 loss: 1.54504801e-06
Iter: 791 loss: 1.54440261e-06
Iter: 792 loss: 1.54575753e-06
Iter: 793 loss: 1.54413908e-06
Iter: 794 loss: 1.54343275e-06
Iter: 795 loss: 1.55153953e-06
Iter: 796 loss: 1.54343491e-06
Iter: 797 loss: 1.54312534e-06
Iter: 798 loss: 1.54811323e-06
Iter: 799 loss: 1.54310339e-06
Iter: 800 loss: 1.54275074e-06
Iter: 801 loss: 1.54234726e-06
Iter: 802 loss: 1.54220743e-06
Iter: 803 loss: 1.54162365e-06
Iter: 804 loss: 1.54140218e-06
Iter: 805 loss: 1.54108056e-06
Iter: 806 loss: 1.540418e-06
Iter: 807 loss: 1.54141514e-06
Iter: 808 loss: 1.54001668e-06
Iter: 809 loss: 1.53914539e-06
Iter: 810 loss: 1.54632608e-06
Iter: 811 loss: 1.53911822e-06
Iter: 812 loss: 1.53848032e-06
Iter: 813 loss: 1.53851272e-06
Iter: 814 loss: 1.53799056e-06
Iter: 815 loss: 1.53703184e-06
Iter: 816 loss: 1.53620795e-06
Iter: 817 loss: 1.53603594e-06
Iter: 818 loss: 1.53477845e-06
Iter: 819 loss: 1.54620761e-06
Iter: 820 loss: 1.53470774e-06
Iter: 821 loss: 1.53382121e-06
Iter: 822 loss: 1.53857104e-06
Iter: 823 loss: 1.53358371e-06
Iter: 824 loss: 1.53289784e-06
Iter: 825 loss: 1.53474559e-06
Iter: 826 loss: 1.5325877e-06
Iter: 827 loss: 1.53186875e-06
Iter: 828 loss: 1.53331405e-06
Iter: 829 loss: 1.53156861e-06
Iter: 830 loss: 1.53069e-06
Iter: 831 loss: 1.53791518e-06
Iter: 832 loss: 1.53062877e-06
Iter: 833 loss: 1.53e-06
Iter: 834 loss: 1.53988162e-06
Iter: 835 loss: 1.53000599e-06
Iter: 836 loss: 1.52971518e-06
Iter: 837 loss: 1.52910525e-06
Iter: 838 loss: 1.54118356e-06
Iter: 839 loss: 1.52911025e-06
Iter: 840 loss: 1.52842722e-06
Iter: 841 loss: 1.52910343e-06
Iter: 842 loss: 1.52805171e-06
Iter: 843 loss: 1.52722077e-06
Iter: 844 loss: 1.53128667e-06
Iter: 845 loss: 1.52709322e-06
Iter: 846 loss: 1.52635573e-06
Iter: 847 loss: 1.52616281e-06
Iter: 848 loss: 1.52583721e-06
Iter: 849 loss: 1.52469124e-06
Iter: 850 loss: 1.52929601e-06
Iter: 851 loss: 1.52447194e-06
Iter: 852 loss: 1.52361633e-06
Iter: 853 loss: 1.52919984e-06
Iter: 854 loss: 1.52349469e-06
Iter: 855 loss: 1.52265125e-06
Iter: 856 loss: 1.52221673e-06
Iter: 857 loss: 1.52192104e-06
Iter: 858 loss: 1.52084942e-06
Iter: 859 loss: 1.52521238e-06
Iter: 860 loss: 1.52072062e-06
Iter: 861 loss: 1.51968152e-06
Iter: 862 loss: 1.51956397e-06
Iter: 863 loss: 1.518874e-06
Iter: 864 loss: 1.51778954e-06
Iter: 865 loss: 1.52693042e-06
Iter: 866 loss: 1.51780273e-06
Iter: 867 loss: 1.51758525e-06
Iter: 868 loss: 1.51732809e-06
Iter: 869 loss: 1.51701693e-06
Iter: 870 loss: 1.51686822e-06
Iter: 871 loss: 1.51660231e-06
Iter: 872 loss: 1.51623419e-06
Iter: 873 loss: 1.51596828e-06
Iter: 874 loss: 1.51582867e-06
Iter: 875 loss: 1.51519271e-06
Iter: 876 loss: 1.51665586e-06
Iter: 877 loss: 1.51492395e-06
Iter: 878 loss: 1.51428276e-06
Iter: 879 loss: 1.51664403e-06
Iter: 880 loss: 1.51413678e-06
Iter: 881 loss: 1.51355027e-06
Iter: 882 loss: 1.51282609e-06
Iter: 883 loss: 1.51256825e-06
Iter: 884 loss: 1.5115188e-06
Iter: 885 loss: 1.5210951e-06
Iter: 886 loss: 1.51149266e-06
Iter: 887 loss: 1.51054883e-06
Iter: 888 loss: 1.51302947e-06
Iter: 889 loss: 1.51031873e-06
Iter: 890 loss: 1.50959988e-06
Iter: 891 loss: 1.51044435e-06
Iter: 892 loss: 1.50898882e-06
Iter: 893 loss: 1.50800531e-06
Iter: 894 loss: 1.50965161e-06
Iter: 895 loss: 1.50752646e-06
Iter: 896 loss: 1.50649498e-06
Iter: 897 loss: 1.50661447e-06
Iter: 898 loss: 1.50568485e-06
Iter: 899 loss: 1.50503138e-06
Iter: 900 loss: 1.504946e-06
Iter: 901 loss: 1.50442156e-06
Iter: 902 loss: 1.50437472e-06
Iter: 903 loss: 1.50413962e-06
Iter: 904 loss: 1.50352901e-06
Iter: 905 loss: 1.51588756e-06
Iter: 906 loss: 1.50349274e-06
Iter: 907 loss: 1.50293477e-06
Iter: 908 loss: 1.5050307e-06
Iter: 909 loss: 1.5027922e-06
Iter: 910 loss: 1.50213907e-06
Iter: 911 loss: 1.50211963e-06
Iter: 912 loss: 1.50173923e-06
Iter: 913 loss: 1.50067649e-06
Iter: 914 loss: 1.5048447e-06
Iter: 915 loss: 1.50047049e-06
Iter: 916 loss: 1.49984578e-06
Iter: 917 loss: 1.50167637e-06
Iter: 918 loss: 1.49969333e-06
Iter: 919 loss: 1.49893253e-06
Iter: 920 loss: 1.49914e-06
Iter: 921 loss: 1.49830157e-06
Iter: 922 loss: 1.49737e-06
Iter: 923 loss: 1.50268568e-06
Iter: 924 loss: 1.49726452e-06
Iter: 925 loss: 1.49657239e-06
Iter: 926 loss: 1.49879338e-06
Iter: 927 loss: 1.49636071e-06
Iter: 928 loss: 1.49565403e-06
Iter: 929 loss: 1.49554319e-06
Iter: 930 loss: 1.49499112e-06
Iter: 931 loss: 1.49392224e-06
Iter: 932 loss: 1.49551931e-06
Iter: 933 loss: 1.49346022e-06
Iter: 934 loss: 1.49285245e-06
Iter: 935 loss: 1.49278492e-06
Iter: 936 loss: 1.49227276e-06
Iter: 937 loss: 1.49620689e-06
Iter: 938 loss: 1.49210564e-06
Iter: 939 loss: 1.49175696e-06
Iter: 940 loss: 1.49086395e-06
Iter: 941 loss: 1.50000938e-06
Iter: 942 loss: 1.49080165e-06
Iter: 943 loss: 1.48983963e-06
Iter: 944 loss: 1.49620291e-06
Iter: 945 loss: 1.48978245e-06
Iter: 946 loss: 1.48915296e-06
Iter: 947 loss: 1.49287553e-06
Iter: 948 loss: 1.48902529e-06
Iter: 949 loss: 1.48838046e-06
Iter: 950 loss: 1.48743879e-06
Iter: 951 loss: 1.48746881e-06
Iter: 952 loss: 1.48643323e-06
Iter: 953 loss: 1.49368452e-06
Iter: 954 loss: 1.48634888e-06
Iter: 955 loss: 1.48554477e-06
Iter: 956 loss: 1.48960657e-06
Iter: 957 loss: 1.48538606e-06
Iter: 958 loss: 1.48461106e-06
Iter: 959 loss: 1.48481161e-06
Iter: 960 loss: 1.48398067e-06
Iter: 961 loss: 1.48335016e-06
Iter: 962 loss: 1.48940785e-06
Iter: 963 loss: 1.48320578e-06
Iter: 964 loss: 1.48255504e-06
Iter: 965 loss: 1.48200138e-06
Iter: 966 loss: 1.48172307e-06
Iter: 967 loss: 1.48085962e-06
Iter: 968 loss: 1.48846755e-06
Iter: 969 loss: 1.48081915e-06
Iter: 970 loss: 1.4803129e-06
Iter: 971 loss: 1.48676349e-06
Iter: 972 loss: 1.48037429e-06
Iter: 973 loss: 1.47971537e-06
Iter: 974 loss: 1.4797663e-06
Iter: 975 loss: 1.47919718e-06
Iter: 976 loss: 1.47875801e-06
Iter: 977 loss: 1.47861215e-06
Iter: 978 loss: 1.4783659e-06
Iter: 979 loss: 1.47759124e-06
Iter: 980 loss: 1.4771249e-06
Iter: 981 loss: 1.47695312e-06
Iter: 982 loss: 1.47602759e-06
Iter: 983 loss: 1.47603373e-06
Iter: 984 loss: 1.47531091e-06
Iter: 985 loss: 1.47605647e-06
Iter: 986 loss: 1.47490437e-06
Iter: 987 loss: 1.47417472e-06
Iter: 988 loss: 1.47662161e-06
Iter: 989 loss: 1.47393212e-06
Iter: 990 loss: 1.47329274e-06
Iter: 991 loss: 1.47444405e-06
Iter: 992 loss: 1.47305923e-06
Iter: 993 loss: 1.47239348e-06
Iter: 994 loss: 1.47638207e-06
Iter: 995 loss: 1.4723496e-06
Iter: 996 loss: 1.4717217e-06
Iter: 997 loss: 1.47233277e-06
Iter: 998 loss: 1.47140486e-06
Iter: 999 loss: 1.47067897e-06
Iter: 1000 loss: 1.47141725e-06
Iter: 1001 loss: 1.47024502e-06
Iter: 1002 loss: 1.46964567e-06
Iter: 1003 loss: 1.47583728e-06
Iter: 1004 loss: 1.46962429e-06
Iter: 1005 loss: 1.46917012e-06
Iter: 1006 loss: 1.4692107e-06
Iter: 1007 loss: 1.46893365e-06
Iter: 1008 loss: 1.46846241e-06
Iter: 1009 loss: 1.46849493e-06
Iter: 1010 loss: 1.46791967e-06
Iter: 1011 loss: 1.46728507e-06
Iter: 1012 loss: 1.46729462e-06
Iter: 1013 loss: 1.466399e-06
Iter: 1014 loss: 1.46842581e-06
Iter: 1015 loss: 1.46608443e-06
Iter: 1016 loss: 1.46527952e-06
Iter: 1017 loss: 1.47659887e-06
Iter: 1018 loss: 1.46534148e-06
Iter: 1019 loss: 1.46487741e-06
Iter: 1020 loss: 1.46561297e-06
Iter: 1021 loss: 1.46463412e-06
Iter: 1022 loss: 1.46400885e-06
Iter: 1023 loss: 1.46471848e-06
Iter: 1024 loss: 1.4636654e-06
Iter: 1025 loss: 1.46306297e-06
Iter: 1026 loss: 1.46541765e-06
Iter: 1027 loss: 1.46292541e-06
Iter: 1028 loss: 1.46229854e-06
Iter: 1029 loss: 1.46332593e-06
Iter: 1030 loss: 1.46203104e-06
Iter: 1031 loss: 1.46115462e-06
Iter: 1032 loss: 1.46402476e-06
Iter: 1033 loss: 1.46103605e-06
Iter: 1034 loss: 1.4604434e-06
Iter: 1035 loss: 1.461289e-06
Iter: 1036 loss: 1.46018249e-06
Iter: 1037 loss: 1.45953345e-06
Iter: 1038 loss: 1.46411207e-06
Iter: 1039 loss: 1.4594284e-06
Iter: 1040 loss: 1.45876174e-06
Iter: 1041 loss: 1.46255e-06
Iter: 1042 loss: 1.45862828e-06
Iter: 1043 loss: 1.45829006e-06
Iter: 1044 loss: 1.45762533e-06
Iter: 1045 loss: 1.46687114e-06
Iter: 1046 loss: 1.45765227e-06
Iter: 1047 loss: 1.4566574e-06
Iter: 1048 loss: 1.45904016e-06
Iter: 1049 loss: 1.45634908e-06
Iter: 1050 loss: 1.45547278e-06
Iter: 1051 loss: 1.46042407e-06
Iter: 1052 loss: 1.45539082e-06
Iter: 1053 loss: 1.45474041e-06
Iter: 1054 loss: 1.45511069e-06
Iter: 1055 loss: 1.45427487e-06
Iter: 1056 loss: 1.45366403e-06
Iter: 1057 loss: 1.46018715e-06
Iter: 1058 loss: 1.45368699e-06
Iter: 1059 loss: 1.45313561e-06
Iter: 1060 loss: 1.45425417e-06
Iter: 1061 loss: 1.45294359e-06
Iter: 1062 loss: 1.45247759e-06
Iter: 1063 loss: 1.45211425e-06
Iter: 1064 loss: 1.45192939e-06
Iter: 1065 loss: 1.4514153e-06
Iter: 1066 loss: 1.458616e-06
Iter: 1067 loss: 1.45140405e-06
Iter: 1068 loss: 1.45087904e-06
Iter: 1069 loss: 1.45191143e-06
Iter: 1070 loss: 1.45065246e-06
Iter: 1071 loss: 1.45029298e-06
Iter: 1072 loss: 1.4508862e-06
Iter: 1073 loss: 1.45001411e-06
Iter: 1074 loss: 1.45004424e-06
Iter: 1075 loss: 1.44984415e-06
Iter: 1076 loss: 1.44959665e-06
Iter: 1077 loss: 1.4491585e-06
Iter: 1078 loss: 1.45139836e-06
Iter: 1079 loss: 1.44896489e-06
Iter: 1080 loss: 1.4482323e-06
Iter: 1081 loss: 1.45010779e-06
Iter: 1082 loss: 1.44801618e-06
Iter: 1083 loss: 1.44738851e-06
Iter: 1084 loss: 1.44925741e-06
Iter: 1085 loss: 1.44717774e-06
Iter: 1086 loss: 1.44653256e-06
Iter: 1087 loss: 1.44902458e-06
Iter: 1088 loss: 1.4464166e-06
Iter: 1089 loss: 1.44580099e-06
Iter: 1090 loss: 1.44576359e-06
Iter: 1091 loss: 1.44538171e-06
Iter: 1092 loss: 1.44448506e-06
Iter: 1093 loss: 1.45176773e-06
Iter: 1094 loss: 1.44446744e-06
Iter: 1095 loss: 1.44392288e-06
Iter: 1096 loss: 1.44553883e-06
Iter: 1097 loss: 1.44381283e-06
Iter: 1098 loss: 1.44325077e-06
Iter: 1099 loss: 1.44266301e-06
Iter: 1100 loss: 1.44258365e-06
Iter: 1101 loss: 1.44181683e-06
Iter: 1102 loss: 1.44178841e-06
Iter: 1103 loss: 1.44143587e-06
Iter: 1104 loss: 1.44186731e-06
Iter: 1105 loss: 1.44115609e-06
Iter: 1106 loss: 1.44061983e-06
Iter: 1107 loss: 1.44405226e-06
Iter: 1108 loss: 1.44055093e-06
Iter: 1109 loss: 1.44011346e-06
Iter: 1110 loss: 1.44014939e-06
Iter: 1111 loss: 1.43992622e-06
Iter: 1112 loss: 1.43944544e-06
Iter: 1113 loss: 1.44405919e-06
Iter: 1114 loss: 1.43941668e-06
Iter: 1115 loss: 1.43886575e-06
Iter: 1116 loss: 1.43864213e-06
Iter: 1117 loss: 1.43833495e-06
Iter: 1118 loss: 1.43748048e-06
Iter: 1119 loss: 1.44212117e-06
Iter: 1120 loss: 1.43734633e-06
Iter: 1121 loss: 1.43642183e-06
Iter: 1122 loss: 1.44039359e-06
Iter: 1123 loss: 1.43632019e-06
Iter: 1124 loss: 1.43555667e-06
Iter: 1125 loss: 1.43618786e-06
Iter: 1126 loss: 1.4352263e-06
Iter: 1127 loss: 1.43444913e-06
Iter: 1128 loss: 1.44176738e-06
Iter: 1129 loss: 1.43440161e-06
Iter: 1130 loss: 1.43387535e-06
Iter: 1131 loss: 1.43374541e-06
Iter: 1132 loss: 1.43330965e-06
Iter: 1133 loss: 1.43270415e-06
Iter: 1134 loss: 1.43514421e-06
Iter: 1135 loss: 1.43256375e-06
Iter: 1136 loss: 1.43195712e-06
Iter: 1137 loss: 1.43405987e-06
Iter: 1138 loss: 1.43182842e-06
Iter: 1139 loss: 1.43136754e-06
Iter: 1140 loss: 1.43530679e-06
Iter: 1141 loss: 1.43137208e-06
Iter: 1142 loss: 1.43109037e-06
Iter: 1143 loss: 1.43378918e-06
Iter: 1144 loss: 1.43108048e-06
Iter: 1145 loss: 1.43079853e-06
Iter: 1146 loss: 1.43022726e-06
Iter: 1147 loss: 1.43026932e-06
Iter: 1148 loss: 1.42986482e-06
Iter: 1149 loss: 1.42993656e-06
Iter: 1150 loss: 1.4295108e-06
Iter: 1151 loss: 1.4288828e-06
Iter: 1152 loss: 1.42937461e-06
Iter: 1153 loss: 1.42853548e-06
Iter: 1154 loss: 1.42784086e-06
Iter: 1155 loss: 1.43241311e-06
Iter: 1156 loss: 1.4277532e-06
Iter: 1157 loss: 1.42719693e-06
Iter: 1158 loss: 1.42766305e-06
Iter: 1159 loss: 1.42686213e-06
Iter: 1160 loss: 1.42615318e-06
Iter: 1161 loss: 1.43178897e-06
Iter: 1162 loss: 1.42614476e-06
Iter: 1163 loss: 1.42572458e-06
Iter: 1164 loss: 1.42624822e-06
Iter: 1165 loss: 1.42549436e-06
Iter: 1166 loss: 1.42485567e-06
Iter: 1167 loss: 1.42623378e-06
Iter: 1168 loss: 1.42471265e-06
Iter: 1169 loss: 1.42423528e-06
Iter: 1170 loss: 1.42686531e-06
Iter: 1171 loss: 1.42410499e-06
Iter: 1172 loss: 1.42374165e-06
Iter: 1173 loss: 1.4234754e-06
Iter: 1174 loss: 1.42334386e-06
Iter: 1175 loss: 1.42316026e-06
Iter: 1176 loss: 1.42301792e-06
Iter: 1177 loss: 1.42276656e-06
Iter: 1178 loss: 1.42336023e-06
Iter: 1179 loss: 1.42264139e-06
Iter: 1180 loss: 1.422271e-06
Iter: 1181 loss: 1.42192539e-06
Iter: 1182 loss: 1.42186968e-06
Iter: 1183 loss: 1.42139356e-06
Iter: 1184 loss: 1.42117938e-06
Iter: 1185 loss: 1.42085923e-06
Iter: 1186 loss: 1.42024635e-06
Iter: 1187 loss: 1.42229146e-06
Iter: 1188 loss: 1.41998396e-06
Iter: 1189 loss: 1.41936755e-06
Iter: 1190 loss: 1.42200872e-06
Iter: 1191 loss: 1.41920248e-06
Iter: 1192 loss: 1.41859846e-06
Iter: 1193 loss: 1.42174872e-06
Iter: 1194 loss: 1.4185398e-06
Iter: 1195 loss: 1.41797273e-06
Iter: 1196 loss: 1.41910698e-06
Iter: 1197 loss: 1.41780197e-06
Iter: 1198 loss: 1.41724649e-06
Iter: 1199 loss: 1.4181162e-06
Iter: 1200 loss: 1.41693442e-06
Iter: 1201 loss: 1.41644023e-06
Iter: 1202 loss: 1.42082763e-06
Iter: 1203 loss: 1.41640498e-06
Iter: 1204 loss: 1.41601265e-06
Iter: 1205 loss: 1.41575583e-06
Iter: 1206 loss: 1.41555824e-06
Iter: 1207 loss: 1.41495548e-06
Iter: 1208 loss: 1.41736848e-06
Iter: 1209 loss: 1.41485498e-06
Iter: 1210 loss: 1.41456519e-06
Iter: 1211 loss: 1.41452801e-06
Iter: 1212 loss: 1.41416808e-06
Iter: 1213 loss: 1.4138194e-06
Iter: 1214 loss: 1.41376722e-06
Iter: 1215 loss: 1.41334931e-06
Iter: 1216 loss: 1.41476426e-06
Iter: 1217 loss: 1.41324244e-06
Iter: 1218 loss: 1.41284045e-06
Iter: 1219 loss: 1.41248233e-06
Iter: 1220 loss: 1.41237058e-06
Iter: 1221 loss: 1.41188059e-06
Iter: 1222 loss: 1.41686337e-06
Iter: 1223 loss: 1.41183443e-06
Iter: 1224 loss: 1.41142618e-06
Iter: 1225 loss: 1.41094938e-06
Iter: 1226 loss: 1.41090902e-06
Iter: 1227 loss: 1.41028136e-06
Iter: 1228 loss: 1.41614191e-06
Iter: 1229 loss: 1.4103066e-06
Iter: 1230 loss: 1.40976772e-06
Iter: 1231 loss: 1.41124565e-06
Iter: 1232 loss: 1.40955729e-06
Iter: 1233 loss: 1.40911789e-06
Iter: 1234 loss: 1.41074804e-06
Iter: 1235 loss: 1.40890916e-06
Iter: 1236 loss: 1.40857799e-06
Iter: 1237 loss: 1.41040789e-06
Iter: 1238 loss: 1.40848965e-06
Iter: 1239 loss: 1.40805173e-06
Iter: 1240 loss: 1.40748227e-06
Iter: 1241 loss: 1.40747306e-06
Iter: 1242 loss: 1.40716088e-06
Iter: 1243 loss: 1.40708096e-06
Iter: 1244 loss: 1.40674797e-06
Iter: 1245 loss: 1.40773454e-06
Iter: 1246 loss: 1.40658835e-06
Iter: 1247 loss: 1.40633892e-06
Iter: 1248 loss: 1.4057149e-06
Iter: 1249 loss: 1.41543e-06
Iter: 1250 loss: 1.40571251e-06
Iter: 1251 loss: 1.40500094e-06
Iter: 1252 loss: 1.40847419e-06
Iter: 1253 loss: 1.40494865e-06
Iter: 1254 loss: 1.40446809e-06
Iter: 1255 loss: 1.40562929e-06
Iter: 1256 loss: 1.40430234e-06
Iter: 1257 loss: 1.40385509e-06
Iter: 1258 loss: 1.4037671e-06
Iter: 1259 loss: 1.40345458e-06
Iter: 1260 loss: 1.40281691e-06
Iter: 1261 loss: 1.40571615e-06
Iter: 1262 loss: 1.40276916e-06
Iter: 1263 loss: 1.40221994e-06
Iter: 1264 loss: 1.40434645e-06
Iter: 1265 loss: 1.40213342e-06
Iter: 1266 loss: 1.40161819e-06
Iter: 1267 loss: 1.40275097e-06
Iter: 1268 loss: 1.40149211e-06
Iter: 1269 loss: 1.40092402e-06
Iter: 1270 loss: 1.40246925e-06
Iter: 1271 loss: 1.4008067e-06
Iter: 1272 loss: 1.40019074e-06
Iter: 1273 loss: 1.40092743e-06
Iter: 1274 loss: 1.39981989e-06
Iter: 1275 loss: 1.39926658e-06
Iter: 1276 loss: 1.40299153e-06
Iter: 1277 loss: 1.39918893e-06
Iter: 1278 loss: 1.3988099e-06
Iter: 1279 loss: 1.40401926e-06
Iter: 1280 loss: 1.39876147e-06
Iter: 1281 loss: 1.39841336e-06
Iter: 1282 loss: 1.39797066e-06
Iter: 1283 loss: 1.39788767e-06
Iter: 1284 loss: 1.3974784e-06
Iter: 1285 loss: 1.39750034e-06
Iter: 1286 loss: 1.39712233e-06
Iter: 1287 loss: 1.39654162e-06
Iter: 1288 loss: 1.39778149e-06
Iter: 1289 loss: 1.39634358e-06
Iter: 1290 loss: 1.39573649e-06
Iter: 1291 loss: 1.39971939e-06
Iter: 1292 loss: 1.39569238e-06
Iter: 1293 loss: 1.39522194e-06
Iter: 1294 loss: 1.39460144e-06
Iter: 1295 loss: 1.39452743e-06
Iter: 1296 loss: 1.393861e-06
Iter: 1297 loss: 1.39768554e-06
Iter: 1298 loss: 1.39377448e-06
Iter: 1299 loss: 1.39310328e-06
Iter: 1300 loss: 1.39625365e-06
Iter: 1301 loss: 1.3929141e-06
Iter: 1302 loss: 1.39251938e-06
Iter: 1303 loss: 1.39690781e-06
Iter: 1304 loss: 1.39245878e-06
Iter: 1305 loss: 1.39214058e-06
Iter: 1306 loss: 1.39202484e-06
Iter: 1307 loss: 1.39188762e-06
Iter: 1308 loss: 1.39131544e-06
Iter: 1309 loss: 1.39405438e-06
Iter: 1310 loss: 1.39122722e-06
Iter: 1311 loss: 1.3908674e-06
Iter: 1312 loss: 1.39388112e-06
Iter: 1313 loss: 1.39087592e-06
Iter: 1314 loss: 1.3905867e-06
Iter: 1315 loss: 1.39135011e-06
Iter: 1316 loss: 1.39037252e-06
Iter: 1317 loss: 1.39009785e-06
Iter: 1318 loss: 1.38959945e-06
Iter: 1319 loss: 1.38958046e-06
Iter: 1320 loss: 1.38912833e-06
Iter: 1321 loss: 1.3899022e-06
Iter: 1322 loss: 1.38881092e-06
Iter: 1323 loss: 1.38832547e-06
Iter: 1324 loss: 1.39086853e-06
Iter: 1325 loss: 1.38823316e-06
Iter: 1326 loss: 1.38769701e-06
Iter: 1327 loss: 1.38952964e-06
Iter: 1328 loss: 1.38760174e-06
Iter: 1329 loss: 1.38720247e-06
Iter: 1330 loss: 1.38794462e-06
Iter: 1331 loss: 1.38697089e-06
Iter: 1332 loss: 1.38655457e-06
Iter: 1333 loss: 1.38632379e-06
Iter: 1334 loss: 1.38606151e-06
Iter: 1335 loss: 1.3854517e-06
Iter: 1336 loss: 1.38803478e-06
Iter: 1337 loss: 1.38532459e-06
Iter: 1338 loss: 1.38479766e-06
Iter: 1339 loss: 1.3864767e-06
Iter: 1340 loss: 1.38460928e-06
Iter: 1341 loss: 1.38405426e-06
Iter: 1342 loss: 1.3883149e-06
Iter: 1343 loss: 1.38392852e-06
Iter: 1344 loss: 1.38359678e-06
Iter: 1345 loss: 1.38375697e-06
Iter: 1346 loss: 1.38340079e-06
Iter: 1347 loss: 1.38322514e-06
Iter: 1348 loss: 1.38306837e-06
Iter: 1349 loss: 1.38295218e-06
Iter: 1350 loss: 1.38266091e-06
Iter: 1351 loss: 1.38263272e-06
Iter: 1352 loss: 1.38235941e-06
Iter: 1353 loss: 1.38216308e-06
Iter: 1354 loss: 1.38205814e-06
Iter: 1355 loss: 1.38155713e-06
Iter: 1356 loss: 1.38203336e-06
Iter: 1357 loss: 1.38125586e-06
Iter: 1358 loss: 1.3808135e-06
Iter: 1359 loss: 1.38362532e-06
Iter: 1360 loss: 1.38068845e-06
Iter: 1361 loss: 1.38021778e-06
Iter: 1362 loss: 1.38129064e-06
Iter: 1363 loss: 1.37998802e-06
Iter: 1364 loss: 1.3795925e-06
Iter: 1365 loss: 1.3808052e-06
Iter: 1366 loss: 1.37944312e-06
Iter: 1367 loss: 1.37892368e-06
Iter: 1368 loss: 1.37887764e-06
Iter: 1369 loss: 1.37849645e-06
Iter: 1370 loss: 1.37800942e-06
Iter: 1371 loss: 1.38154701e-06
Iter: 1372 loss: 1.37799361e-06
Iter: 1373 loss: 1.37747816e-06
Iter: 1374 loss: 1.37755296e-06
Iter: 1375 loss: 1.37714756e-06
Iter: 1376 loss: 1.37678296e-06
Iter: 1377 loss: 1.37683332e-06
Iter: 1378 loss: 1.37643497e-06
Iter: 1379 loss: 1.3763173e-06
Iter: 1380 loss: 1.37613301e-06
Iter: 1381 loss: 1.3760731e-06
Iter: 1382 loss: 1.37583856e-06
Iter: 1383 loss: 1.37569555e-06
Iter: 1384 loss: 1.37532425e-06
Iter: 1385 loss: 1.37807706e-06
Iter: 1386 loss: 1.37531595e-06
Iter: 1387 loss: 1.37479083e-06
Iter: 1388 loss: 1.37554787e-06
Iter: 1389 loss: 1.37463098e-06
Iter: 1390 loss: 1.37398513e-06
Iter: 1391 loss: 1.37548625e-06
Iter: 1392 loss: 1.37382199e-06
Iter: 1393 loss: 1.37326924e-06
Iter: 1394 loss: 1.37536665e-06
Iter: 1395 loss: 1.37314578e-06
Iter: 1396 loss: 1.37268728e-06
Iter: 1397 loss: 1.37285588e-06
Iter: 1398 loss: 1.37227153e-06
Iter: 1399 loss: 1.37182133e-06
Iter: 1400 loss: 1.37826896e-06
Iter: 1401 loss: 1.37178824e-06
Iter: 1402 loss: 1.37139477e-06
Iter: 1403 loss: 1.37103279e-06
Iter: 1404 loss: 1.37087954e-06
Iter: 1405 loss: 1.37029144e-06
Iter: 1406 loss: 1.37249594e-06
Iter: 1407 loss: 1.37018117e-06
Iter: 1408 loss: 1.3697412e-06
Iter: 1409 loss: 1.37148879e-06
Iter: 1410 loss: 1.3696158e-06
Iter: 1411 loss: 1.36918993e-06
Iter: 1412 loss: 1.37293955e-06
Iter: 1413 loss: 1.36915776e-06
Iter: 1414 loss: 1.36890515e-06
Iter: 1415 loss: 1.3693807e-06
Iter: 1416 loss: 1.3687951e-06
Iter: 1417 loss: 1.36851361e-06
Iter: 1418 loss: 1.36846006e-06
Iter: 1419 loss: 1.3683175e-06
Iter: 1420 loss: 1.36803783e-06
Iter: 1421 loss: 1.37426946e-06
Iter: 1422 loss: 1.36793244e-06
Iter: 1423 loss: 1.36761264e-06
Iter: 1424 loss: 1.36788367e-06
Iter: 1425 loss: 1.36734332e-06
Iter: 1426 loss: 1.3668523e-06
Iter: 1427 loss: 1.3679653e-06
Iter: 1428 loss: 1.36669701e-06
Iter: 1429 loss: 1.36629023e-06
Iter: 1430 loss: 1.37034681e-06
Iter: 1431 loss: 1.36629728e-06
Iter: 1432 loss: 1.3659519e-06
Iter: 1433 loss: 1.36557605e-06
Iter: 1434 loss: 1.36555968e-06
Iter: 1435 loss: 1.36513017e-06
Iter: 1436 loss: 1.37124675e-06
Iter: 1437 loss: 1.36510323e-06
Iter: 1438 loss: 1.36472954e-06
Iter: 1439 loss: 1.36460358e-06
Iter: 1440 loss: 1.3644684e-06
Iter: 1441 loss: 1.36397728e-06
Iter: 1442 loss: 1.3650008e-06
Iter: 1443 loss: 1.36380083e-06
Iter: 1444 loss: 1.36341646e-06
Iter: 1445 loss: 1.36364974e-06
Iter: 1446 loss: 1.36316692e-06
Iter: 1447 loss: 1.36275651e-06
Iter: 1448 loss: 1.36274457e-06
Iter: 1449 loss: 1.36252788e-06
Iter: 1450 loss: 1.36394237e-06
Iter: 1451 loss: 1.36246604e-06
Iter: 1452 loss: 1.36223207e-06
Iter: 1453 loss: 1.36241556e-06
Iter: 1454 loss: 1.36206597e-06
Iter: 1455 loss: 1.36174424e-06
Iter: 1456 loss: 1.36146809e-06
Iter: 1457 loss: 1.36136771e-06
Iter: 1458 loss: 1.36096992e-06
Iter: 1459 loss: 1.36161611e-06
Iter: 1460 loss: 1.36076119e-06
Iter: 1461 loss: 1.3603053e-06
Iter: 1462 loss: 1.36174754e-06
Iter: 1463 loss: 1.36009544e-06
Iter: 1464 loss: 1.35965058e-06
Iter: 1465 loss: 1.36230062e-06
Iter: 1466 loss: 1.35960852e-06
Iter: 1467 loss: 1.35921209e-06
Iter: 1468 loss: 1.35966775e-06
Iter: 1469 loss: 1.3590145e-06
Iter: 1470 loss: 1.35858124e-06
Iter: 1471 loss: 1.36018866e-06
Iter: 1472 loss: 1.35848541e-06
Iter: 1473 loss: 1.35810637e-06
Iter: 1474 loss: 1.35945379e-06
Iter: 1475 loss: 1.35802793e-06
Iter: 1476 loss: 1.35770642e-06
Iter: 1477 loss: 1.35780351e-06
Iter: 1478 loss: 1.35741357e-06
Iter: 1479 loss: 1.35698144e-06
Iter: 1480 loss: 1.35884295e-06
Iter: 1481 loss: 1.35691937e-06
Iter: 1482 loss: 1.35647815e-06
Iter: 1483 loss: 1.35753203e-06
Iter: 1484 loss: 1.35633616e-06
Iter: 1485 loss: 1.35608548e-06
Iter: 1486 loss: 1.3560757e-06
Iter: 1487 loss: 1.35583866e-06
Iter: 1488 loss: 1.35556888e-06
Iter: 1489 loss: 1.35558264e-06
Iter: 1490 loss: 1.35509697e-06
Iter: 1491 loss: 1.35524851e-06
Iter: 1492 loss: 1.35482242e-06
Iter: 1493 loss: 1.35441621e-06
Iter: 1494 loss: 1.35494361e-06
Iter: 1495 loss: 1.35413234e-06
Iter: 1496 loss: 1.3537209e-06
Iter: 1497 loss: 1.35568894e-06
Iter: 1498 loss: 1.35360801e-06
Iter: 1499 loss: 1.3530564e-06
Iter: 1500 loss: 1.35473852e-06
Iter: 1501 loss: 1.35294363e-06
Iter: 1502 loss: 1.3525962e-06
Iter: 1503 loss: 1.35366668e-06
Iter: 1504 loss: 1.35247365e-06
Iter: 1505 loss: 1.35200412e-06
Iter: 1506 loss: 1.35265645e-06
Iter: 1507 loss: 1.35184359e-06
Iter: 1508 loss: 1.35135872e-06
Iter: 1509 loss: 1.3536868e-06
Iter: 1510 loss: 1.3513652e-06
Iter: 1511 loss: 1.35105813e-06
Iter: 1512 loss: 1.35097082e-06
Iter: 1513 loss: 1.3507547e-06
Iter: 1514 loss: 1.35022538e-06
Iter: 1515 loss: 1.35158393e-06
Iter: 1516 loss: 1.35001676e-06
Iter: 1517 loss: 1.34995639e-06
Iter: 1518 loss: 1.34981087e-06
Iter: 1519 loss: 1.34956463e-06
Iter: 1520 loss: 1.34944e-06
Iter: 1521 loss: 1.34940205e-06
Iter: 1522 loss: 1.3490461e-06
Iter: 1523 loss: 1.34912807e-06
Iter: 1524 loss: 1.34881725e-06
Iter: 1525 loss: 1.34840934e-06
Iter: 1526 loss: 1.34900324e-06
Iter: 1527 loss: 1.34819481e-06
Iter: 1528 loss: 1.34777861e-06
Iter: 1529 loss: 1.34813774e-06
Iter: 1530 loss: 1.34754146e-06
Iter: 1531 loss: 1.34702054e-06
Iter: 1532 loss: 1.34951313e-06
Iter: 1533 loss: 1.34700895e-06
Iter: 1534 loss: 1.34650475e-06
Iter: 1535 loss: 1.347629e-06
Iter: 1536 loss: 1.3463482e-06
Iter: 1537 loss: 1.34595678e-06
Iter: 1538 loss: 1.34770596e-06
Iter: 1539 loss: 1.34582342e-06
Iter: 1540 loss: 1.34543734e-06
Iter: 1541 loss: 1.34650077e-06
Iter: 1542 loss: 1.34536253e-06
Iter: 1543 loss: 1.345e-06
Iter: 1544 loss: 1.34542051e-06
Iter: 1545 loss: 1.3448398e-06
Iter: 1546 loss: 1.34440802e-06
Iter: 1547 loss: 1.34496099e-06
Iter: 1548 loss: 1.34419304e-06
Iter: 1549 loss: 1.34361881e-06
Iter: 1550 loss: 1.34489983e-06
Iter: 1551 loss: 1.34352808e-06
Iter: 1552 loss: 1.34353627e-06
Iter: 1553 loss: 1.34329184e-06
Iter: 1554 loss: 1.34314632e-06
Iter: 1555 loss: 1.34279014e-06
Iter: 1556 loss: 1.34571223e-06
Iter: 1557 loss: 1.34279662e-06
Iter: 1558 loss: 1.34236655e-06
Iter: 1559 loss: 1.34344816e-06
Iter: 1560 loss: 1.34222989e-06
Iter: 1561 loss: 1.34184586e-06
Iter: 1562 loss: 1.34253321e-06
Iter: 1563 loss: 1.34164736e-06
Iter: 1564 loss: 1.34128413e-06
Iter: 1565 loss: 1.34178526e-06
Iter: 1566 loss: 1.34107631e-06
Iter: 1567 loss: 1.34054608e-06
Iter: 1568 loss: 1.34133643e-06
Iter: 1569 loss: 1.34028949e-06
Iter: 1570 loss: 1.33987396e-06
Iter: 1571 loss: 1.34399636e-06
Iter: 1572 loss: 1.33985009e-06
Iter: 1573 loss: 1.33953904e-06
Iter: 1574 loss: 1.33991614e-06
Iter: 1575 loss: 1.33933258e-06
Iter: 1576 loss: 1.33888614e-06
Iter: 1577 loss: 1.33970104e-06
Iter: 1578 loss: 1.33869753e-06
Iter: 1579 loss: 1.33836124e-06
Iter: 1580 loss: 1.34100969e-06
Iter: 1581 loss: 1.33830588e-06
Iter: 1582 loss: 1.33803599e-06
Iter: 1583 loss: 1.33798267e-06
Iter: 1584 loss: 1.33779474e-06
Iter: 1585 loss: 1.33762808e-06
Iter: 1586 loss: 1.33760955e-06
Iter: 1587 loss: 1.33733795e-06
Iter: 1588 loss: 1.33733954e-06
Iter: 1589 loss: 1.33714093e-06
Iter: 1590 loss: 1.33693902e-06
Iter: 1591 loss: 1.33684637e-06
Iter: 1592 loss: 1.33676622e-06
Iter: 1593 loss: 1.33645096e-06
Iter: 1594 loss: 1.33806316e-06
Iter: 1595 loss: 1.33634353e-06
Iter: 1596 loss: 1.33604703e-06
Iter: 1597 loss: 1.33559683e-06
Iter: 1598 loss: 1.33558456e-06
Iter: 1599 loss: 1.33506774e-06
Iter: 1600 loss: 1.33792491e-06
Iter: 1601 loss: 1.33500089e-06
Iter: 1602 loss: 1.3345209e-06
Iter: 1603 loss: 1.33668198e-06
Iter: 1604 loss: 1.33446861e-06
Iter: 1605 loss: 1.33403239e-06
Iter: 1606 loss: 1.33535355e-06
Iter: 1607 loss: 1.33393314e-06
Iter: 1608 loss: 1.33361073e-06
Iter: 1609 loss: 1.33441881e-06
Iter: 1610 loss: 1.33342724e-06
Iter: 1611 loss: 1.33312437e-06
Iter: 1612 loss: 1.33402682e-06
Iter: 1613 loss: 1.33299523e-06
Iter: 1614 loss: 1.33263165e-06
Iter: 1615 loss: 1.33344224e-06
Iter: 1616 loss: 1.33252581e-06
Iter: 1617 loss: 1.33214371e-06
Iter: 1618 loss: 1.33348499e-06
Iter: 1619 loss: 1.3320946e-06
Iter: 1620 loss: 1.3318795e-06
Iter: 1621 loss: 1.33179151e-06
Iter: 1622 loss: 1.33167487e-06
Iter: 1623 loss: 1.33128367e-06
Iter: 1624 loss: 1.33412027e-06
Iter: 1625 loss: 1.33121625e-06
Iter: 1626 loss: 1.33074332e-06
Iter: 1627 loss: 1.33305559e-06
Iter: 1628 loss: 1.33058029e-06
Iter: 1629 loss: 1.33022854e-06
Iter: 1630 loss: 1.33171966e-06
Iter: 1631 loss: 1.33014157e-06
Iter: 1632 loss: 1.32980017e-06
Iter: 1633 loss: 1.3303536e-06
Iter: 1634 loss: 1.3296011e-06
Iter: 1635 loss: 1.32926527e-06
Iter: 1636 loss: 1.3293178e-06
Iter: 1637 loss: 1.32896855e-06
Iter: 1638 loss: 1.32854177e-06
Iter: 1639 loss: 1.33168965e-06
Iter: 1640 loss: 1.32850255e-06
Iter: 1641 loss: 1.32806656e-06
Iter: 1642 loss: 1.32961668e-06
Iter: 1643 loss: 1.327988e-06
Iter: 1644 loss: 1.32766877e-06
Iter: 1645 loss: 1.32768844e-06
Iter: 1646 loss: 1.3273592e-06
Iter: 1647 loss: 1.32704054e-06
Iter: 1648 loss: 1.33162314e-06
Iter: 1649 loss: 1.32700666e-06
Iter: 1650 loss: 1.3268093e-06
Iter: 1651 loss: 1.32678201e-06
Iter: 1652 loss: 1.32660443e-06
Iter: 1653 loss: 1.32657556e-06
Iter: 1654 loss: 1.32639775e-06
Iter: 1655 loss: 1.32627815e-06
Iter: 1656 loss: 1.32593061e-06
Iter: 1657 loss: 1.33139054e-06
Iter: 1658 loss: 1.32596097e-06
Iter: 1659 loss: 1.3256664e-06
Iter: 1660 loss: 1.32609011e-06
Iter: 1661 loss: 1.32551361e-06
Iter: 1662 loss: 1.3251921e-06
Iter: 1663 loss: 1.32676394e-06
Iter: 1664 loss: 1.32519335e-06
Iter: 1665 loss: 1.32490129e-06
Iter: 1666 loss: 1.32514276e-06
Iter: 1667 loss: 1.32476748e-06
Iter: 1668 loss: 1.32439538e-06
Iter: 1669 loss: 1.32503283e-06
Iter: 1670 loss: 1.32419621e-06
Iter: 1671 loss: 1.32383866e-06
Iter: 1672 loss: 1.3237609e-06
Iter: 1673 loss: 1.32354694e-06
Iter: 1674 loss: 1.32308651e-06
Iter: 1675 loss: 1.3277097e-06
Iter: 1676 loss: 1.32300386e-06
Iter: 1677 loss: 1.32269065e-06
Iter: 1678 loss: 1.32466448e-06
Iter: 1679 loss: 1.32262801e-06
Iter: 1680 loss: 1.32236278e-06
Iter: 1681 loss: 1.3221196e-06
Iter: 1682 loss: 1.32205514e-06
Iter: 1683 loss: 1.32168987e-06
Iter: 1684 loss: 1.32609591e-06
Iter: 1685 loss: 1.32163348e-06
Iter: 1686 loss: 1.3213064e-06
Iter: 1687 loss: 1.32239279e-06
Iter: 1688 loss: 1.32123512e-06
Iter: 1689 loss: 1.32092237e-06
Iter: 1690 loss: 1.32294349e-06
Iter: 1691 loss: 1.32086325e-06
Iter: 1692 loss: 1.32074649e-06
Iter: 1693 loss: 1.32040429e-06
Iter: 1694 loss: 1.32042589e-06
Iter: 1695 loss: 1.32014031e-06
Iter: 1696 loss: 1.32105e-06
Iter: 1697 loss: 1.32001594e-06
Iter: 1698 loss: 1.31972979e-06
Iter: 1699 loss: 1.32081811e-06
Iter: 1700 loss: 1.31964646e-06
Iter: 1701 loss: 1.31932632e-06
Iter: 1702 loss: 1.31998593e-06
Iter: 1703 loss: 1.31922752e-06
Iter: 1704 loss: 1.31890374e-06
Iter: 1705 loss: 1.31912066e-06
Iter: 1706 loss: 1.31874492e-06
Iter: 1707 loss: 1.31836487e-06
Iter: 1708 loss: 1.31957495e-06
Iter: 1709 loss: 1.31830166e-06
Iter: 1710 loss: 1.31800607e-06
Iter: 1711 loss: 1.31932984e-06
Iter: 1712 loss: 1.31791387e-06
Iter: 1713 loss: 1.31756974e-06
Iter: 1714 loss: 1.31803836e-06
Iter: 1715 loss: 1.31732918e-06
Iter: 1716 loss: 1.3170303e-06
Iter: 1717 loss: 1.31765933e-06
Iter: 1718 loss: 1.31688512e-06
Iter: 1719 loss: 1.31648187e-06
Iter: 1720 loss: 1.31852357e-06
Iter: 1721 loss: 1.31644015e-06
Iter: 1722 loss: 1.31614206e-06
Iter: 1723 loss: 1.31610261e-06
Iter: 1724 loss: 1.31593356e-06
Iter: 1725 loss: 1.31544448e-06
Iter: 1726 loss: 1.32504692e-06
Iter: 1727 loss: 1.31550314e-06
Iter: 1728 loss: 1.3151332e-06
Iter: 1729 loss: 1.31557033e-06
Iter: 1730 loss: 1.31494289e-06
Iter: 1731 loss: 1.31458341e-06
Iter: 1732 loss: 1.31623767e-06
Iter: 1733 loss: 1.31456659e-06
Iter: 1734 loss: 1.31424895e-06
Iter: 1735 loss: 1.31518436e-06
Iter: 1736 loss: 1.31414333e-06
Iter: 1737 loss: 1.31377692e-06
Iter: 1738 loss: 1.31433842e-06
Iter: 1739 loss: 1.31363458e-06
Iter: 1740 loss: 1.31330285e-06
Iter: 1741 loss: 1.31334741e-06
Iter: 1742 loss: 1.31301795e-06
Iter: 1743 loss: 1.31268121e-06
Iter: 1744 loss: 1.31613729e-06
Iter: 1745 loss: 1.31263107e-06
Iter: 1746 loss: 1.31230934e-06
Iter: 1747 loss: 1.31297554e-06
Iter: 1748 loss: 1.31221611e-06
Iter: 1749 loss: 1.31187016e-06
Iter: 1750 loss: 1.31270201e-06
Iter: 1751 loss: 1.31171214e-06
Iter: 1752 loss: 1.31129332e-06
Iter: 1753 loss: 1.31179138e-06
Iter: 1754 loss: 1.31112074e-06
Iter: 1755 loss: 1.31074671e-06
Iter: 1756 loss: 1.3129918e-06
Iter: 1757 loss: 1.31074012e-06
Iter: 1758 loss: 1.31037405e-06
Iter: 1759 loss: 1.31345644e-06
Iter: 1760 loss: 1.31038928e-06
Iter: 1761 loss: 1.31027593e-06
Iter: 1762 loss: 1.30990156e-06
Iter: 1763 loss: 1.31472007e-06
Iter: 1764 loss: 1.30982755e-06
Iter: 1765 loss: 1.30948638e-06
Iter: 1766 loss: 1.311456e-06
Iter: 1767 loss: 1.30943101e-06
Iter: 1768 loss: 1.30909439e-06
Iter: 1769 loss: 1.3094691e-06
Iter: 1770 loss: 1.30897763e-06
Iter: 1771 loss: 1.3085554e-06
Iter: 1772 loss: 1.30978503e-06
Iter: 1773 loss: 1.30846468e-06
Iter: 1774 loss: 1.30810338e-06
Iter: 1775 loss: 1.3103022e-06
Iter: 1776 loss: 1.30804767e-06
Iter: 1777 loss: 1.30770786e-06
Iter: 1778 loss: 1.30741296e-06
Iter: 1779 loss: 1.30737067e-06
Iter: 1780 loss: 1.30687749e-06
Iter: 1781 loss: 1.30959143e-06
Iter: 1782 loss: 1.30680723e-06
Iter: 1783 loss: 1.30640456e-06
Iter: 1784 loss: 1.30848241e-06
Iter: 1785 loss: 1.30634396e-06
Iter: 1786 loss: 1.30593298e-06
Iter: 1787 loss: 1.30746673e-06
Iter: 1788 loss: 1.30595447e-06
Iter: 1789 loss: 1.3055037e-06
Iter: 1790 loss: 1.30557862e-06
Iter: 1791 loss: 1.30527769e-06
Iter: 1792 loss: 1.30522494e-06
Iter: 1793 loss: 1.30506612e-06
Iter: 1794 loss: 1.30493891e-06
Iter: 1795 loss: 1.30463206e-06
Iter: 1796 loss: 1.31248873e-06
Iter: 1797 loss: 1.30459443e-06
Iter: 1798 loss: 1.30435296e-06
Iter: 1799 loss: 1.30462786e-06
Iter: 1800 loss: 1.30416765e-06
Iter: 1801 loss: 1.30384888e-06
Iter: 1802 loss: 1.30471199e-06
Iter: 1803 loss: 1.30376645e-06
Iter: 1804 loss: 1.30339799e-06
Iter: 1805 loss: 1.30446119e-06
Iter: 1806 loss: 1.30335604e-06
Iter: 1807 loss: 1.30306444e-06
Iter: 1808 loss: 1.30474052e-06
Iter: 1809 loss: 1.30302783e-06
Iter: 1810 loss: 1.30270541e-06
Iter: 1811 loss: 1.30257354e-06
Iter: 1812 loss: 1.30244644e-06
Iter: 1813 loss: 1.30201897e-06
Iter: 1814 loss: 1.30496403e-06
Iter: 1815 loss: 1.3020267e-06
Iter: 1816 loss: 1.30173646e-06
Iter: 1817 loss: 1.30223793e-06
Iter: 1818 loss: 1.30155524e-06
Iter: 1819 loss: 1.30115541e-06
Iter: 1820 loss: 1.30141575e-06
Iter: 1821 loss: 1.30101091e-06
Iter: 1822 loss: 1.30055946e-06
Iter: 1823 loss: 1.30437888e-06
Iter: 1824 loss: 1.30058129e-06
Iter: 1825 loss: 1.30027263e-06
Iter: 1826 loss: 1.30140165e-06
Iter: 1827 loss: 1.30019907e-06
Iter: 1828 loss: 1.30008948e-06
Iter: 1829 loss: 1.30007686e-06
Iter: 1830 loss: 1.29997602e-06
Iter: 1831 loss: 1.29969771e-06
Iter: 1832 loss: 1.30121543e-06
Iter: 1833 loss: 1.29959221e-06
Iter: 1834 loss: 1.29926502e-06
Iter: 1835 loss: 1.30057595e-06
Iter: 1836 loss: 1.29917782e-06
Iter: 1837 loss: 1.29889793e-06
Iter: 1838 loss: 1.29895716e-06
Iter: 1839 loss: 1.29871296e-06
Iter: 1840 loss: 1.29825889e-06
Iter: 1841 loss: 1.30032777e-06
Iter: 1842 loss: 1.29822297e-06
Iter: 1843 loss: 1.2979159e-06
Iter: 1844 loss: 1.30000171e-06
Iter: 1845 loss: 1.2978378e-06
Iter: 1846 loss: 1.29756972e-06
Iter: 1847 loss: 1.2974582e-06
Iter: 1848 loss: 1.29726345e-06
Iter: 1849 loss: 1.29676778e-06
Iter: 1850 loss: 1.29780062e-06
Iter: 1851 loss: 1.29659043e-06
Iter: 1852 loss: 1.29611226e-06
Iter: 1853 loss: 1.29874411e-06
Iter: 1854 loss: 1.29606065e-06
Iter: 1855 loss: 1.29573175e-06
Iter: 1856 loss: 1.29640239e-06
Iter: 1857 loss: 1.29558089e-06
Iter: 1858 loss: 1.29521754e-06
Iter: 1859 loss: 1.29693126e-06
Iter: 1860 loss: 1.29517343e-06
Iter: 1861 loss: 1.29490138e-06
Iter: 1862 loss: 1.29744626e-06
Iter: 1863 loss: 1.29497585e-06
Iter: 1864 loss: 1.29472312e-06
Iter: 1865 loss: 1.2951856e-06
Iter: 1866 loss: 1.29463126e-06
Iter: 1867 loss: 1.29442196e-06
Iter: 1868 loss: 1.29425541e-06
Iter: 1869 loss: 1.29420937e-06
Iter: 1870 loss: 1.29391037e-06
Iter: 1871 loss: 1.29414946e-06
Iter: 1872 loss: 1.29370187e-06
Iter: 1873 loss: 1.29347302e-06
Iter: 1874 loss: 1.2937943e-06
Iter: 1875 loss: 1.29324701e-06
Iter: 1876 loss: 1.29281102e-06
Iter: 1877 loss: 1.29331272e-06
Iter: 1878 loss: 1.29262685e-06
Iter: 1879 loss: 1.29212071e-06
Iter: 1880 loss: 1.29691989e-06
Iter: 1881 loss: 1.29214368e-06
Iter: 1882 loss: 1.29179705e-06
Iter: 1883 loss: 1.29257e-06
Iter: 1884 loss: 1.29162368e-06
Iter: 1885 loss: 1.2913481e-06
Iter: 1886 loss: 1.29140699e-06
Iter: 1887 loss: 1.29109515e-06
Iter: 1888 loss: 1.29058071e-06
Iter: 1889 loss: 1.29163459e-06
Iter: 1890 loss: 1.29041473e-06
Iter: 1891 loss: 1.29004229e-06
Iter: 1892 loss: 1.29368868e-06
Iter: 1893 loss: 1.28995509e-06
Iter: 1894 loss: 1.2896794e-06
Iter: 1895 loss: 1.29080331e-06
Iter: 1896 loss: 1.28960528e-06
Iter: 1897 loss: 1.2894501e-06
Iter: 1898 loss: 1.28941429e-06
Iter: 1899 loss: 1.2893247e-06
Iter: 1900 loss: 1.28900115e-06
Iter: 1901 loss: 1.29393459e-06
Iter: 1902 loss: 1.28899046e-06
Iter: 1903 loss: 1.28870909e-06
Iter: 1904 loss: 1.28985153e-06
Iter: 1905 loss: 1.28875217e-06
Iter: 1906 loss: 1.28851343e-06
Iter: 1907 loss: 1.28822558e-06
Iter: 1908 loss: 1.28820784e-06
Iter: 1909 loss: 1.28781176e-06
Iter: 1910 loss: 1.28855584e-06
Iter: 1911 loss: 1.28764964e-06
Iter: 1912 loss: 1.28722877e-06
Iter: 1913 loss: 1.28906811e-06
Iter: 1914 loss: 1.28717079e-06
Iter: 1915 loss: 1.28674264e-06
Iter: 1916 loss: 1.28828583e-06
Iter: 1917 loss: 1.28662e-06
Iter: 1918 loss: 1.28622082e-06
Iter: 1919 loss: 1.28760735e-06
Iter: 1920 loss: 1.28609918e-06
Iter: 1921 loss: 1.28578949e-06
Iter: 1922 loss: 1.28596616e-06
Iter: 1923 loss: 1.28551028e-06
Iter: 1924 loss: 1.28505167e-06
Iter: 1925 loss: 1.2865022e-06
Iter: 1926 loss: 1.28497152e-06
Iter: 1927 loss: 1.28464876e-06
Iter: 1928 loss: 1.28668216e-06
Iter: 1929 loss: 1.28457896e-06
Iter: 1930 loss: 1.28432839e-06
Iter: 1931 loss: 1.28615011e-06
Iter: 1932 loss: 1.28435136e-06
Iter: 1933 loss: 1.28411546e-06
Iter: 1934 loss: 1.28549925e-06
Iter: 1935 loss: 1.28413694e-06
Iter: 1936 loss: 1.28392787e-06
Iter: 1937 loss: 1.28372483e-06
Iter: 1938 loss: 1.28365878e-06
Iter: 1939 loss: 1.28347813e-06
Iter: 1940 loss: 1.28394527e-06
Iter: 1941 loss: 1.28340218e-06
Iter: 1942 loss: 1.2830759e-06
Iter: 1943 loss: 1.28323609e-06
Iter: 1944 loss: 1.28291765e-06
Iter: 1945 loss: 1.28257363e-06
Iter: 1946 loss: 1.28435101e-06
Iter: 1947 loss: 1.2825667e-06
Iter: 1948 loss: 1.28222155e-06
Iter: 1949 loss: 1.28262263e-06
Iter: 1950 loss: 1.28206398e-06
Iter: 1951 loss: 1.28173065e-06
Iter: 1952 loss: 1.28181796e-06
Iter: 1953 loss: 1.28145336e-06
Iter: 1954 loss: 1.28107627e-06
Iter: 1955 loss: 1.28112322e-06
Iter: 1956 loss: 1.28083821e-06
Iter: 1957 loss: 1.28085276e-06
Iter: 1958 loss: 1.28061208e-06
Iter: 1959 loss: 1.28028205e-06
Iter: 1960 loss: 1.28030456e-06
Iter: 1961 loss: 1.28000806e-06
Iter: 1962 loss: 1.27958879e-06
Iter: 1963 loss: 1.28214037e-06
Iter: 1964 loss: 1.27963256e-06
Iter: 1965 loss: 1.27956355e-06
Iter: 1966 loss: 1.27949784e-06
Iter: 1967 loss: 1.27932981e-06
Iter: 1968 loss: 1.27918452e-06
Iter: 1969 loss: 1.27913313e-06
Iter: 1970 loss: 1.27895464e-06
Iter: 1971 loss: 1.27898693e-06
Iter: 1972 loss: 1.27873352e-06
Iter: 1973 loss: 1.27846886e-06
Iter: 1974 loss: 1.27887506e-06
Iter: 1975 loss: 1.27831572e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script66
+ '[' -r STOP.script66 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output66/f1_psi0_phi2.8/300_300_300_1
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1
+ for phi in 2.8
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output66/f1_psi0_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output67/f1_psi0_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output66/f1_psi0_phi2.8 /home/mrdouglas/Manifold/experiments.final/output67/f1_psi0_phi2.8
+ date
Sat Oct 24 19:39:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output66/f1_psi0_phi2.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 2345 --load_model experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1 --function f1 --psi 0 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output66/f1_psi0_phi2.8/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49a1ed620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49a17b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49a1fb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49a1fb2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49a1b5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49a0c4e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49a101950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49a06f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49a06f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49a06fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49a0911e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd499fe69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd499ff56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49a0b47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd499fb3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49a09f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd499f80d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd499ec5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd499ec52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd499f3d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd499f538c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496cce0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd499f53950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496cca8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496c09488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496c7eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496c7e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496c682f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496c68598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496c2f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496c441e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496c598c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496b1b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496b7b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496a5cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd496a21f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 1.9894581
test_loss: 1.9909075
train_loss: 1.992348
test_loss: 2.4595044
train_loss: 0.7451458
test_loss: 0.8075995
train_loss: 0.79997134
test_loss: 0.80474883
train_loss: 0.7962554
test_loss: 0.79825467
train_loss: 0.78268147
test_loss: 0.7901598
train_loss: 0.7936852
test_loss: 0.7808147
train_loss: 0.7285445
test_loss: 0.7693603
train_loss: 0.7488346
test_loss: 0.7502551
train_loss: 0.69673336
test_loss: 0.7144109
train_loss: 0.6965434
test_loss: 0.6891661
train_loss: 0.63760966
test_loss: 0.6261077
train_loss: 0.50358444
test_loss: 0.5046147
train_loss: 0.37549537
test_loss: 0.36794856
train_loss: 1.9981647
test_loss: 1.9956042
train_loss: 1.99468
test_loss: 1.9931625
train_loss: 0.67984134
test_loss: 0.69012946
train_loss: 0.7368516
test_loss: 0.7122292
train_loss: 0.7178106
test_loss: 0.711969
train_loss: 0.71470433
test_loss: 0.70900434
train_loss: 0.68530464
test_loss: 0.70531404
train_loss: 0.7228806
test_loss: 0.7017426
train_loss: 0.6495808
test_loss: 0.6980086
train_loss: 0.67058074
test_loss: 0.6942518
train_loss: 0.6770694
test_loss: 0.6902511
train_loss: 0.6538589
test_loss: 0.6863165
train_loss: 0.6818315
test_loss: 0.68221307
train_loss: 0.6424129
test_loss: 0.6780756
train_loss: 0.64233387
test_loss: 0.67374825
train_loss: 0.638566
test_loss: 0.66968954
train_loss: 0.6478815
test_loss: 0.66505796
train_loss: 0.6624423
test_loss: 0.6605996
train_loss: 0.67423123
test_loss: 0.65584564
train_loss: 0.66588056
test_loss: 0.65083754
train_loss: 0.6416166
test_loss: 0.64522094
train_loss: 0.6044812
test_loss: 0.6376315
train_loss: 0.6122371
test_loss: 0.62412274
train_loss: 0.5843441
test_loss: 0.60107285
train_loss: 0.5705658
test_loss: 0.57853425
train_loss: 0.5503464
test_loss: 0.55405676
train_loss: 0.5332527
test_loss: 0.52609736
train_loss: 0.48231018
test_loss: 0.4972593
train_loss: 0.47593015
test_loss: 0.46880054
train_loss: 0.41940218
test_loss: 0.4425479
train_loss: 0.42676014
test_loss: 0.41950756
train_loss: 0.38856366
test_loss: 0.39857334
train_loss: 0.36675993
test_loss: 0.38000682
train_loss: 0.36500287
test_loss: 0.36330292
train_loss: 0.35903317
test_loss: 0.34864503
train_loss: 0.3261898
test_loss: 0.33564958
train_loss: 0.30877662
test_loss: 0.32421294
train_loss: 0.30114412
test_loss: 0.31392103
train_loss: 0.30999368
test_loss: 0.3053682
train_loss: 0.2895872
test_loss: 0.29759276
train_loss: 0.28910094
test_loss: 0.2903666
train_loss: 0.28137532
test_loss: 0.2835235
train_loss: 0.2577241
test_loss: 0.27589777
train_loss: 0.26102307
test_loss: 0.26789033
train_loss: 0.2567512
test_loss: 0.25871643
train_loss: 0.23860712
test_loss: 0.2467825
train_loss: 0.22727033
test_loss: 0.23213124
train_loss: 0.21079424
test_loss: 0.21395117
train_loss: 0.18687385
test_loss: 0.1941173
train_loss: 0.19332798
test_loss: 0.17725258
train_loss: 0.1701633
test_loss: 0.16276972
train_loss: 0.16102326
test_loss: 0.15038817
train_loss: 0.13546395
test_loss: 0.13983856
train_loss: 0.13128582
test_loss: 0.13069019
train_loss: 0.12294127
test_loss: 0.122649476
train_loss: 0.11193378
test_loss: 0.1152225
train_loss: 0.104861125
test_loss: 0.10871763
train_loss: 0.105642766
test_loss: 0.1019238
train_loss: 0.09274521
test_loss: 0.09608448
train_loss: 0.09382657
test_loss: 0.09221371
train_loss: 0.08259533
test_loss: 0.08620977
train_loss: 0.08080163
test_loss: 0.08231159
train_loss: 0.07890998
test_loss: 0.08053088
train_loss: 0.07530896
test_loss: 0.07693214
train_loss: 0.074878775
test_loss: 0.07551009
train_loss: 0.074292555
test_loss: 0.074212395
train_loss: 0.072087154
test_loss: 0.07271314
train_loss: 0.07523909
test_loss: 0.07203659
train_loss: 0.07378579
test_loss: 0.071509905
train_loss: 0.07363558
test_loss: 0.0708551
train_loss: 0.06871239
test_loss: 0.06998781
train_loss: 0.06829943
test_loss: 0.069323234
train_loss: 0.06674373
test_loss: 0.068760216
train_loss: 0.06690199
test_loss: 0.06822867
train_loss: 0.06589554
test_loss: 0.06778379
train_loss: 0.06688518
test_loss: 0.06743619
train_loss: 0.065815195
test_loss: 0.06731572
train_loss: 0.06566089
test_loss: 0.06703086
train_loss: 0.06551999
test_loss: 0.066048354
train_loss: 0.06265256
test_loss: 0.06569281
train_loss: 0.06436944
test_loss: 0.0654878
train_loss: 0.063505396
test_loss: 0.0650057
train_loss: 0.064606816
test_loss: 0.06453699
train_loss: 0.061090283
test_loss: 0.06396886
train_loss: 0.0625942
test_loss: 0.063491255
train_loss: 0.060716037
test_loss: 0.06294751
train_loss: 0.060042463
test_loss: 0.06257099
train_loss: 0.06213124
test_loss: 0.062474933
train_loss: 0.059962556
test_loss: 0.062042315
train_loss: 0.05906771
test_loss: 0.060876906
train_loss: 0.06004335
test_loss: 0.061446972
train_loss: 0.058366846
test_loss: 0.06046398
train_loss: 0.0596193
test_loss: 0.059809096
train_loss: 0.057182185
test_loss: 0.05897162
train_loss: 0.057617106
test_loss: 0.05854127
train_loss: 0.057512566
test_loss: 0.057951078
train_loss: 0.05700047
test_loss: 0.057484295
train_loss: 0.05559458
test_loss: 0.057372868
train_loss: 0.054053724
test_loss: 0.0569918
train_loss: 0.054859586
test_loss: 0.056371
train_loss: 0.05469726
test_loss: 0.057130434
train_loss: 0.056032468
test_loss: 0.056322653
train_loss: 0.05497025
test_loss: 0.056124307
train_loss: 0.054623466
test_loss: 0.05563012
train_loss: 0.05220169
test_loss: 0.054983266
train_loss: 0.05183357
test_loss: 0.054642666
train_loss: 0.05281367
test_loss: 0.05410431
train_loss: 0.052615777
test_loss: 0.05386708
train_loss: 0.05239834
test_loss: 0.053675864
train_loss: 0.05240138
test_loss: 0.053477027
train_loss: 0.052489154
test_loss: 0.053144265
train_loss: 0.05055795
test_loss: 0.052825153
train_loss: 0.049721442
test_loss: 0.052643955
train_loss: 0.049111906
test_loss: 0.051960833
train_loss: 0.050039183
test_loss: 0.051863395
train_loss: 0.052157603
test_loss: 0.051792875
train_loss: 0.050723277
test_loss: 0.051730353
train_loss: 0.052915752
test_loss: 0.05207163
train_loss: 0.047289215
test_loss: 0.050627705
train_loss: 0.04897444
test_loss: 0.050368793
train_loss: 0.047357254
test_loss: 0.05045897
train_loss: 0.048702106
test_loss: 0.049901407
train_loss: 0.04818478
test_loss: 0.049720958
train_loss: 0.04811956
test_loss: 0.05085241
train_loss: 0.049005836
test_loss: 0.049268708
train_loss: 0.046310887
test_loss: 0.04873015
train_loss: 0.04832837
test_loss: 0.04851821
train_loss: 0.046407916
test_loss: 0.04825625
train_loss: 0.04577146
test_loss: 0.047962096
train_loss: 0.04581385
test_loss: 0.048217677
train_loss: 0.04726082
test_loss: 0.047375415
train_loss: 0.045604043
test_loss: 0.048799507
train_loss: 0.04606563
test_loss: 0.047548734
train_loss: 0.046072245
test_loss: 0.046747696
train_loss: 0.047122158
test_loss: 0.046553154
train_loss: 0.04484587
test_loss: 0.046362158
train_loss: 0.044227276
test_loss: 0.045796636
train_loss: 0.0437
test_loss: 0.046009358
train_loss: 0.044839688
test_loss: 0.04524638
train_loss: 0.045039777
test_loss: 0.04504162
train_loss: 0.044598483
test_loss: 0.044907454
train_loss: 0.044362113
test_loss: 0.044517066
train_loss: 0.042685546
test_loss: 0.044367105
train_loss: 0.043547943
test_loss: 0.04449799
train_loss: 0.042799883
test_loss: 0.044061147
train_loss: 0.0425352
test_loss: 0.043186568
train_loss: 0.042010285
test_loss: 0.04302913
train_loss: 0.04327132
test_loss: 0.04288038
train_loss: 0.04180199
test_loss: 0.043852985
train_loss: 0.041513126
test_loss: 0.04247944
train_loss: 0.03954968
test_loss: 0.04202549
train_loss: 0.040495347
test_loss: 0.041970186
train_loss: 0.040208463
test_loss: 0.041207474
train_loss: 0.039673418
test_loss: 0.042191215
train_loss: 0.04003393
test_loss: 0.04089428
train_loss: 0.040872324
test_loss: 0.04097275
train_loss: 0.03683668
test_loss: 0.039923772
train_loss: 0.039146356
test_loss: 0.039751504
train_loss: 0.039544385
test_loss: 0.039313573
train_loss: 0.03919188
test_loss: 0.040209237
train_loss: 0.03818489
test_loss: 0.03861483
train_loss: 0.03847279
test_loss: 0.03813012
train_loss: 0.036931492
test_loss: 0.0378058
train_loss: 0.03689639
test_loss: 0.03752549
train_loss: 0.034623697
test_loss: 0.03704546
train_loss: 0.036067795
test_loss: 0.037388604
train_loss: 0.036067195
test_loss: 0.036680628
train_loss: 0.036091425
test_loss: 0.035856247
train_loss: 0.034473743
test_loss: 0.036037825
train_loss: 0.034309715
test_loss: 0.0349918
train_loss: 0.033702645
test_loss: 0.034451827
train_loss: 0.0328227
test_loss: 0.034537073
train_loss: 0.031189576
test_loss: 0.033881415
train_loss: 0.03229681
test_loss: 0.03405489
train_loss: 0.033027142
test_loss: 0.033219963
train_loss: 0.03241508
test_loss: 0.032258295
train_loss: 0.03148401
test_loss: 0.031853378
train_loss: 0.028755376
test_loss: 0.031638738
train_loss: 0.02983854
test_loss: 0.030937027
train_loss: 0.029199429
test_loss: 0.030007634
train_loss: 0.028052501
test_loss: 0.029610638
train_loss: 0.02827464
test_loss: 0.029205475
train_loss: 0.027475351
test_loss: 0.027860755
train_loss: 0.027315836
test_loss: 0.027399763
train_loss: 0.025810152
test_loss: 0.026934227
train_loss: 0.024096485
test_loss: 0.02630665
train_loss: 0.025403704
test_loss: 0.025718823
train_loss: 0.024549805
test_loss: 0.02511613
train_loss: 0.023857716
test_loss: 0.025056882
train_loss: 0.022473024
test_loss: 0.023679623
train_loss: 0.021318082
test_loss: 0.02323291
train_loss: 0.021446168
test_loss: 0.023302514
train_loss: 0.020391531
test_loss: 0.023059107
train_loss: 0.021690765
test_loss: 0.021058328
train_loss: 0.019143226
test_loss: 0.020689897
train_loss: 0.01997747
test_loss: 0.020787789
train_loss: 0.019125756
test_loss: 0.019781714
train_loss: 0.018621985
test_loss: 0.019479346
train_loss: 0.01831614
test_loss: 0.019066403
train_loss: 0.018744513
test_loss: 0.01890483
train_loss: 0.01715132
test_loss: 0.018429687
train_loss: 0.016608363
test_loss: 0.017991252
train_loss: 0.015813665
test_loss: 0.018350787
train_loss: 0.016971372
test_loss: 0.017105881
train_loss: 0.015919035
test_loss: 0.017209696
train_loss: 0.015542846
test_loss: 0.016615363
train_loss: 0.015009271
test_loss: 0.016248772
train_loss: 0.0147983525
test_loss: 0.016179249
train_loss: 0.0142506305
test_loss: 0.01586363
train_loss: 0.014438957
test_loss: 0.01575964
train_loss: 0.0144992955
test_loss: 0.015260716
train_loss: 0.014701824
test_loss: 0.015062781
train_loss: 0.013223939
test_loss: 0.015132742
train_loss: 0.013712178
test_loss: 0.014935972
train_loss: 0.013755696
test_loss: 0.014694182
train_loss: 0.0126398085
test_loss: 0.014305288
train_loss: 0.013403844
test_loss: 0.014016822
train_loss: 0.013287736
test_loss: 0.014123704
train_loss: 0.013778907
test_loss: 0.013517683
train_loss: 0.012685537
test_loss: 0.013307684
train_loss: 0.012339835
test_loss: 0.013409389
train_loss: 0.0137633635
test_loss: 0.0127642285
train_loss: 0.011975569
test_loss: 0.012877645
train_loss: 0.012112558
test_loss: 0.012335159
train_loss: 0.011103886
test_loss: 0.012314008
train_loss: 0.011713672
test_loss: 0.011966488
train_loss: 0.011129046
test_loss: 0.012331414
train_loss: 0.011781044
test_loss: 0.011690465
train_loss: 0.010520685
test_loss: 0.011652003
train_loss: 0.01035757
test_loss: 0.011746107
train_loss: 0.010153614
test_loss: 0.011478938
train_loss: 0.010208582
test_loss: 0.010819377
train_loss: 0.009874465
test_loss: 0.010969668
train_loss: 0.009873668
test_loss: 0.011020477
train_loss: 0.0095543265
test_loss: 0.010673492
train_loss: 0.009652057
test_loss: 0.010587795
train_loss: 0.009644048
test_loss: 0.010339727
train_loss: 0.010029638
test_loss: 0.010021914
train_loss: 0.010144397
test_loss: 0.01034282
train_loss: 0.009291432
test_loss: 0.009708652
train_loss: 0.009194987
test_loss: 0.009839687
train_loss: 0.008578807
test_loss: 0.009526017
train_loss: 0.008303747
test_loss: 0.009127482
train_loss: 0.008378941
test_loss: 0.00938376
train_loss: 0.008102605
test_loss: 0.009804833
train_loss: 0.008448379
test_loss: 0.011188941
train_loss: 0.00821968
test_loss: 0.009094602
train_loss: 0.008886784
test_loss: 0.008991468
train_loss: 0.008358804
test_loss: 0.008889389
train_loss: 0.00782802
test_loss: 0.009094961
train_loss: 0.007565283
test_loss: 0.010265571
train_loss: 0.008506027
test_loss: 0.008489538
train_loss: 0.007378076
test_loss: 0.008332555
train_loss: 0.009183207
test_loss: 0.0082752835
train_loss: 0.007506842
test_loss: 0.009341187
train_loss: 0.007151137
test_loss: 0.00808579
train_loss: 0.007775895
test_loss: 0.0080059245
train_loss: 0.0073358137
test_loss: 0.007819042
train_loss: 0.007022935
test_loss: 0.0077738417
train_loss: 0.007070898
test_loss: 0.008096682
train_loss: 0.0065548867
test_loss: 0.0076226355
train_loss: 0.006848162
test_loss: 0.0074920235
train_loss: 0.006774837
test_loss: 0.0074489727
train_loss: 0.0064364877
test_loss: 0.007566807
train_loss: 0.006465857
test_loss: 0.0071360855
train_loss: 0.006352417
test_loss: 0.006913377
train_loss: 0.0063917777
test_loss: 0.0069542928
train_loss: 0.006545072
test_loss: 0.006933335
train_loss: 0.0064507555
test_loss: 0.007614005
train_loss: 0.0066995993
test_loss: 0.007559639
train_loss: 0.006170811
test_loss: 0.0066934964
train_loss: 0.0066148136
test_loss: 0.008459461
train_loss: 0.008921878
test_loss: 0.007906631
train_loss: 0.0067435596
test_loss: 0.007455613
train_loss: 0.007025537
test_loss: 0.0069705495
train_loss: 0.006684674
test_loss: 0.0071678613
train_loss: 0.0066693174
test_loss: 0.0076041827
train_loss: 0.009107331
test_loss: 0.0069847195
train_loss: 0.009626527
test_loss: 0.0069599072
train_loss: 0.0068972446
test_loss: 0.006633126
train_loss: 0.006390603
test_loss: 0.0071821897
train_loss: 0.0055426504
test_loss: 0.006147164
train_loss: 0.0058007324
test_loss: 0.006142272
train_loss: 0.0054353396
test_loss: 0.006116524
train_loss: 0.0058492236
test_loss: 0.006382645
train_loss: 0.005758959
test_loss: 0.006235941
train_loss: 0.0059557334
test_loss: 0.0062186737
train_loss: 0.005876649
test_loss: 0.0067639556
train_loss: 0.0059381356
test_loss: 0.0060760635
train_loss: 0.005529181
test_loss: 0.006104002
train_loss: 0.005217224
test_loss: 0.0060262843
train_loss: 0.0054706726
test_loss: 0.005735546
train_loss: 0.0057764524
test_loss: 0.005932758
train_loss: 0.0056665605
test_loss: 0.005878251
train_loss: 0.0057714116
test_loss: 0.006299327
train_loss: 0.0054450035
test_loss: 0.0060470267
train_loss: 0.00528659
test_loss: 0.005531928
train_loss: 0.005876491
test_loss: 0.005918968
train_loss: 0.005451138
test_loss: 0.006674432
train_loss: 0.0055923415
test_loss: 0.006646214
train_loss: 0.0055482714
test_loss: 0.006342263
train_loss: 0.0055095395
test_loss: 0.005392831
train_loss: 0.0060379575
test_loss: 0.0060551697
train_loss: 0.006230018
test_loss: 0.005927051
train_loss: 0.0072473176
test_loss: 0.006450726
train_loss: 0.0056185005
test_loss: 0.005826474
train_loss: 0.0069628693
test_loss: 0.0055196574
train_loss: 0.006420943
test_loss: 0.005818081
train_loss: 0.0052940343
test_loss: 0.0055150418
train_loss: 0.005106679
test_loss: 0.005432522
train_loss: 0.005124426
test_loss: 0.005920664
train_loss: 0.0052475873
test_loss: 0.0055077486
train_loss: 0.0049669254
test_loss: 0.005324814
train_loss: 0.005902851
test_loss: 0.00590266
train_loss: 0.005155421
test_loss: 0.0052448954
train_loss: 0.005040526
test_loss: 0.005331285
train_loss: 0.005674053
test_loss: 0.0060597947
train_loss: 0.005739082
test_loss: 0.0060051205
train_loss: 0.005048822
test_loss: 0.005735288
train_loss: 0.0048376736
test_loss: 0.0054865493
train_loss: 0.0054367688
test_loss: 0.0053174607
train_loss: 0.005405644
test_loss: 0.0064040176
train_loss: 0.0056657875
test_loss: 0.0052818414
train_loss: 0.0051245145
test_loss: 0.0054040956
train_loss: 0.0052988753
test_loss: 0.005834869
train_loss: 0.0052336375
test_loss: 0.0052835564
train_loss: 0.0054811565
test_loss: 0.0055098576
train_loss: 0.004923634
test_loss: 0.005258951
train_loss: 0.004895434
test_loss: 0.0052732234
train_loss: 0.0049984017
test_loss: 0.004846755
train_loss: 0.0054342505
test_loss: 0.0050709634
train_loss: 0.005988134
test_loss: 0.0051241284
train_loss: 0.0053778286
test_loss: 0.0060811243
train_loss: 0.005310025
test_loss: 0.0054332265
train_loss: 0.005201728
test_loss: 0.005554672
train_loss: 0.0048767207
test_loss: 0.00584146
train_loss: 0.0048458967
test_loss: 0.004739738
train_loss: 0.0046311584
test_loss: 0.0051136687
train_loss: 0.00492533
test_loss: 0.0056414306
train_loss: 0.0048483405
test_loss: 0.0054199724
train_loss: 0.00517573/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0053821313
train_loss: 0.00487902
test_loss: 0.0048663504
train_loss: 0.004820885
test_loss: 0.0050185835
train_loss: 0.0054245163
test_loss: 0.005812664
train_loss: 0.0059062922
test_loss: 0.0055867927
train_loss: 0.004786322
test_loss: 0.0052730558
train_loss: 0.0048096557
test_loss: 0.0050644292
train_loss: 0.00450628
test_loss: 0.004656139
train_loss: 0.004867809
test_loss: 0.0055043926
train_loss: 0.0056648226
test_loss: 0.005740687
train_loss: 0.004845991
test_loss: 0.0050464002
train_loss: 0.00456011
test_loss: 0.0048945607
train_loss: 0.0046918183
test_loss: 0.00528724
train_loss: 0.0048454315
test_loss: 0.005653177
train_loss: 0.005495925
test_loss: 0.0055405875
train_loss: 0.0044529117
test_loss: 0.00490129
train_loss: 0.0055210097
test_loss: 0.005388699
train_loss: 0.004734844
test_loss: 0.0053569404
train_loss: 0.004582337
test_loss: 0.0048569674
train_loss: 0.004858833
test_loss: 0.0051353197
train_loss: 0.0048205843
test_loss: 0.0047671343
train_loss: 0.005357136
test_loss: 0.0049581276
train_loss: 0.0046448745
test_loss: 0.0046578003
train_loss: 0.0051774383
test_loss: 0.004774981
train_loss: 0.005167119
test_loss: 0.0057094125
train_loss: 0.004691779
test_loss: 0.004709169
train_loss: 0.0046859803
test_loss: 0.004927721
train_loss: 0.0043833465
test_loss: 0.004600142
train_loss: 0.0045985854
test_loss: 0.0048391516
train_loss: 0.0047337897
test_loss: 0.005284913
train_loss: 0.004255764
test_loss: 0.004645751
train_loss: 0.0050242837
test_loss: 0.0064414856
train_loss: 0.005146602
test_loss: 0.004894812
train_loss: 0.0043827053
test_loss: 0.005003994
train_loss: 0.0052461727
test_loss: 0.005508033
train_loss: 0.0045628976
test_loss: 0.004930368
train_loss: 0.004894172
test_loss: 0.0053934865
train_loss: 0.005210148
test_loss: 0.004531626
train_loss: 0.004635891
test_loss: 0.0057624895
train_loss: 0.0048785415
test_loss: 0.005920025
train_loss: 0.0046158563
test_loss: 0.0051222723
train_loss: 0.0043797926
test_loss: 0.004324108
train_loss: 0.0041587534
test_loss: 0.0048188544
train_loss: 0.004122654
test_loss: 0.004778314
train_loss: 0.0048442697
test_loss: 0.0048695747
train_loss: 0.004532202
test_loss: 0.004635474
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output67/f1_psi0_phi2.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output66/f1_psi0_phi2.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output67/f1_psi0_phi2.8/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff79c166268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff79c18da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff79c0f89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff79c1f3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff79c1f38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff79c0c96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff79c0a4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff79c052048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff79c0c6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff79c0a4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff79c0c6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff780760f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7807789d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7807252f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7806cae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff780778d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7807afb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7807afa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7807af378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff78060b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff78062b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7805de0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff78062b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7805bac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7805baa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7805ba510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7805ba950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff780537950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff780537e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7804e8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7804f91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7804f97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7804432f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7804a07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7803ffea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7803ccf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.45623082e-05
Iter: 2 loss: 8.13370425e-05
Iter: 3 loss: 3.83043225e-05
Iter: 4 loss: 3.55879274e-05
Iter: 5 loss: 3.55865704e-05
Iter: 6 loss: 3.3854405e-05
Iter: 7 loss: 3.42625935e-05
Iter: 8 loss: 3.25815345e-05
Iter: 9 loss: 3.07784467e-05
Iter: 10 loss: 2.98895084e-05
Iter: 11 loss: 2.90312073e-05
Iter: 12 loss: 2.74424528e-05
Iter: 13 loss: 3.04684199e-05
Iter: 14 loss: 2.67718369e-05
Iter: 15 loss: 2.51361653e-05
Iter: 16 loss: 3.19124374e-05
Iter: 17 loss: 2.47855642e-05
Iter: 18 loss: 2.36148262e-05
Iter: 19 loss: 2.98955929e-05
Iter: 20 loss: 2.34355139e-05
Iter: 21 loss: 2.27210076e-05
Iter: 22 loss: 2.55322284e-05
Iter: 23 loss: 2.25565782e-05
Iter: 24 loss: 2.20586226e-05
Iter: 25 loss: 2.2953e-05
Iter: 26 loss: 2.18424302e-05
Iter: 27 loss: 2.13976127e-05
Iter: 28 loss: 2.51629972e-05
Iter: 29 loss: 2.13726962e-05
Iter: 30 loss: 2.11586339e-05
Iter: 31 loss: 2.13619187e-05
Iter: 32 loss: 2.10354192e-05
Iter: 33 loss: 2.07925605e-05
Iter: 34 loss: 2.22001036e-05
Iter: 35 loss: 2.07604317e-05
Iter: 36 loss: 2.06262484e-05
Iter: 37 loss: 2.16629505e-05
Iter: 38 loss: 2.06164041e-05
Iter: 39 loss: 2.04659482e-05
Iter: 40 loss: 2.15923228e-05
Iter: 41 loss: 2.04535936e-05
Iter: 42 loss: 2.03608579e-05
Iter: 43 loss: 2.03444652e-05
Iter: 44 loss: 2.02814481e-05
Iter: 45 loss: 2.01711409e-05
Iter: 46 loss: 2.04328207e-05
Iter: 47 loss: 2.01304883e-05
Iter: 48 loss: 2.00433715e-05
Iter: 49 loss: 1.98428443e-05
Iter: 50 loss: 2.25157055e-05
Iter: 51 loss: 1.98300222e-05
Iter: 52 loss: 1.97199279e-05
Iter: 53 loss: 1.97116424e-05
Iter: 54 loss: 1.9593761e-05
Iter: 55 loss: 1.94381464e-05
Iter: 56 loss: 1.94282875e-05
Iter: 57 loss: 1.93119376e-05
Iter: 58 loss: 1.93095166e-05
Iter: 59 loss: 1.92230909e-05
Iter: 60 loss: 1.91691433e-05
Iter: 61 loss: 1.91347826e-05
Iter: 62 loss: 1.89992534e-05
Iter: 63 loss: 1.94521672e-05
Iter: 64 loss: 1.89617713e-05
Iter: 65 loss: 1.88254671e-05
Iter: 66 loss: 1.91930194e-05
Iter: 67 loss: 1.87779024e-05
Iter: 68 loss: 1.86787074e-05
Iter: 69 loss: 1.94900804e-05
Iter: 70 loss: 1.86723155e-05
Iter: 71 loss: 1.86481684e-05
Iter: 72 loss: 1.86365651e-05
Iter: 73 loss: 1.8596289e-05
Iter: 74 loss: 1.85646713e-05
Iter: 75 loss: 1.85519293e-05
Iter: 76 loss: 1.8521665e-05
Iter: 77 loss: 1.87064234e-05
Iter: 78 loss: 1.85162808e-05
Iter: 79 loss: 1.84903802e-05
Iter: 80 loss: 1.84464589e-05
Iter: 81 loss: 1.84462406e-05
Iter: 82 loss: 1.8392122e-05
Iter: 83 loss: 1.84571054e-05
Iter: 84 loss: 1.83640677e-05
Iter: 85 loss: 1.83055872e-05
Iter: 86 loss: 1.84132041e-05
Iter: 87 loss: 1.82803069e-05
Iter: 88 loss: 1.82086478e-05
Iter: 89 loss: 1.85473673e-05
Iter: 90 loss: 1.8196497e-05
Iter: 91 loss: 1.8128434e-05
Iter: 92 loss: 1.82109925e-05
Iter: 93 loss: 1.80922561e-05
Iter: 94 loss: 1.80244988e-05
Iter: 95 loss: 1.84291239e-05
Iter: 96 loss: 1.80163843e-05
Iter: 97 loss: 1.79600393e-05
Iter: 98 loss: 1.79053459e-05
Iter: 99 loss: 1.7893668e-05
Iter: 100 loss: 1.78308292e-05
Iter: 101 loss: 1.78299397e-05
Iter: 102 loss: 1.77956736e-05
Iter: 103 loss: 1.77455113e-05
Iter: 104 loss: 1.7744358e-05
Iter: 105 loss: 1.78520531e-05
Iter: 106 loss: 1.77276852e-05
Iter: 107 loss: 1.7714061e-05
Iter: 108 loss: 1.76951598e-05
Iter: 109 loss: 1.76936e-05
Iter: 110 loss: 1.76728463e-05
Iter: 111 loss: 1.76510985e-05
Iter: 112 loss: 1.76464873e-05
Iter: 113 loss: 1.75969872e-05
Iter: 114 loss: 1.7813627e-05
Iter: 115 loss: 1.75875284e-05
Iter: 116 loss: 1.7567083e-05
Iter: 117 loss: 1.75484347e-05
Iter: 118 loss: 1.75429414e-05
Iter: 119 loss: 1.74930792e-05
Iter: 120 loss: 1.75172827e-05
Iter: 121 loss: 1.74600591e-05
Iter: 122 loss: 1.74181532e-05
Iter: 123 loss: 1.77461734e-05
Iter: 124 loss: 1.7415332e-05
Iter: 125 loss: 1.73726075e-05
Iter: 126 loss: 1.74524503e-05
Iter: 127 loss: 1.73546887e-05
Iter: 128 loss: 1.73157314e-05
Iter: 129 loss: 1.74985034e-05
Iter: 130 loss: 1.73093904e-05
Iter: 131 loss: 1.72760392e-05
Iter: 132 loss: 1.72724485e-05
Iter: 133 loss: 1.72485834e-05
Iter: 134 loss: 1.72036e-05
Iter: 135 loss: 1.75405876e-05
Iter: 136 loss: 1.72009295e-05
Iter: 137 loss: 1.71750671e-05
Iter: 138 loss: 1.72246619e-05
Iter: 139 loss: 1.71635766e-05
Iter: 140 loss: 1.71529337e-05
Iter: 141 loss: 1.7144861e-05
Iter: 142 loss: 1.71363736e-05
Iter: 143 loss: 1.71115153e-05
Iter: 144 loss: 1.71939828e-05
Iter: 145 loss: 1.71000247e-05
Iter: 146 loss: 1.70686362e-05
Iter: 147 loss: 1.73795215e-05
Iter: 148 loss: 1.70663952e-05
Iter: 149 loss: 1.70442e-05
Iter: 150 loss: 1.71351803e-05
Iter: 151 loss: 1.70393796e-05
Iter: 152 loss: 1.70215335e-05
Iter: 153 loss: 1.69805862e-05
Iter: 154 loss: 1.75187088e-05
Iter: 155 loss: 1.69780033e-05
Iter: 156 loss: 1.69417963e-05
Iter: 157 loss: 1.74069664e-05
Iter: 158 loss: 1.69425803e-05
Iter: 159 loss: 1.69095911e-05
Iter: 160 loss: 1.68799834e-05
Iter: 161 loss: 1.68709485e-05
Iter: 162 loss: 1.68366987e-05
Iter: 163 loss: 1.68365787e-05
Iter: 164 loss: 1.68090119e-05
Iter: 165 loss: 1.68212828e-05
Iter: 166 loss: 1.67903308e-05
Iter: 167 loss: 1.67625949e-05
Iter: 168 loss: 1.68845399e-05
Iter: 169 loss: 1.67578655e-05
Iter: 170 loss: 1.67293565e-05
Iter: 171 loss: 1.67281396e-05
Iter: 172 loss: 1.6704751e-05
Iter: 173 loss: 1.6682865e-05
Iter: 174 loss: 1.66827049e-05
Iter: 175 loss: 1.66633472e-05
Iter: 176 loss: 1.68950846e-05
Iter: 177 loss: 1.66631162e-05
Iter: 178 loss: 1.66535065e-05
Iter: 179 loss: 1.66255504e-05
Iter: 180 loss: 1.66781356e-05
Iter: 181 loss: 1.66039936e-05
Iter: 182 loss: 1.65830861e-05
Iter: 183 loss: 1.65814472e-05
Iter: 184 loss: 1.65581441e-05
Iter: 185 loss: 1.65525398e-05
Iter: 186 loss: 1.65383026e-05
Iter: 187 loss: 1.65056408e-05
Iter: 188 loss: 1.65783458e-05
Iter: 189 loss: 1.64921839e-05
Iter: 190 loss: 1.64695666e-05
Iter: 191 loss: 1.64980192e-05
Iter: 192 loss: 1.64586017e-05
Iter: 193 loss: 1.64277189e-05
Iter: 194 loss: 1.6451484e-05
Iter: 195 loss: 1.64088488e-05
Iter: 196 loss: 1.63786317e-05
Iter: 197 loss: 1.65580168e-05
Iter: 198 loss: 1.6375976e-05
Iter: 199 loss: 1.63474033e-05
Iter: 200 loss: 1.64564299e-05
Iter: 201 loss: 1.63411969e-05
Iter: 202 loss: 1.63142104e-05
Iter: 203 loss: 1.6328866e-05
Iter: 204 loss: 1.62953875e-05
Iter: 205 loss: 1.62621291e-05
Iter: 206 loss: 1.64039702e-05
Iter: 207 loss: 1.62547876e-05
Iter: 208 loss: 1.62258675e-05
Iter: 209 loss: 1.62736742e-05
Iter: 210 loss: 1.62128708e-05
Iter: 211 loss: 1.61910593e-05
Iter: 212 loss: 1.61877942e-05
Iter: 213 loss: 1.61790431e-05
Iter: 214 loss: 1.61631888e-05
Iter: 215 loss: 1.64289158e-05
Iter: 216 loss: 1.61624594e-05
Iter: 217 loss: 1.61465341e-05
Iter: 218 loss: 1.61344597e-05
Iter: 219 loss: 1.61299013e-05
Iter: 220 loss: 1.61089902e-05
Iter: 221 loss: 1.61088901e-05
Iter: 222 loss: 1.60985983e-05
Iter: 223 loss: 1.60833424e-05
Iter: 224 loss: 1.60827731e-05
Iter: 225 loss: 1.60586405e-05
Iter: 226 loss: 1.6082915e-05
Iter: 227 loss: 1.60456912e-05
Iter: 228 loss: 1.60263953e-05
Iter: 229 loss: 1.60778054e-05
Iter: 230 loss: 1.60202089e-05
Iter: 231 loss: 1.59915635e-05
Iter: 232 loss: 1.60013897e-05
Iter: 233 loss: 1.5972555e-05
Iter: 234 loss: 1.59428291e-05
Iter: 235 loss: 1.607788e-05
Iter: 236 loss: 1.59358278e-05
Iter: 237 loss: 1.59068404e-05
Iter: 238 loss: 1.60378804e-05
Iter: 239 loss: 1.59014635e-05
Iter: 240 loss: 1.58779512e-05
Iter: 241 loss: 1.59589945e-05
Iter: 242 loss: 1.58714975e-05
Iter: 243 loss: 1.5854268e-05
Iter: 244 loss: 1.60422042e-05
Iter: 245 loss: 1.58529856e-05
Iter: 246 loss: 1.58330076e-05
Iter: 247 loss: 1.59151914e-05
Iter: 248 loss: 1.58289713e-05
Iter: 249 loss: 1.58206458e-05
Iter: 250 loss: 1.57981631e-05
Iter: 251 loss: 1.59450246e-05
Iter: 252 loss: 1.57927134e-05
Iter: 253 loss: 1.57674913e-05
Iter: 254 loss: 1.59206647e-05
Iter: 255 loss: 1.57646864e-05
Iter: 256 loss: 1.57466347e-05
Iter: 257 loss: 1.59288247e-05
Iter: 258 loss: 1.57459035e-05
Iter: 259 loss: 1.57279283e-05
Iter: 260 loss: 1.57216928e-05
Iter: 261 loss: 1.5710446e-05
Iter: 262 loss: 1.5692287e-05
Iter: 263 loss: 1.57345166e-05
Iter: 264 loss: 1.56855785e-05
Iter: 265 loss: 1.56662318e-05
Iter: 266 loss: 1.56797978e-05
Iter: 267 loss: 1.56550486e-05
Iter: 268 loss: 1.56337301e-05
Iter: 269 loss: 1.56680289e-05
Iter: 270 loss: 1.56235765e-05
Iter: 271 loss: 1.56045789e-05
Iter: 272 loss: 1.59030606e-05
Iter: 273 loss: 1.56046717e-05
Iter: 274 loss: 1.55898033e-05
Iter: 275 loss: 1.55588441e-05
Iter: 276 loss: 1.60881791e-05
Iter: 277 loss: 1.55575653e-05
Iter: 278 loss: 1.5553087e-05
Iter: 279 loss: 1.55414837e-05
Iter: 280 loss: 1.55356356e-05
Iter: 281 loss: 1.55346661e-05
Iter: 282 loss: 1.552922e-05
Iter: 283 loss: 1.55123053e-05
Iter: 284 loss: 1.55314428e-05
Iter: 285 loss: 1.54995141e-05
Iter: 286 loss: 1.54772206e-05
Iter: 287 loss: 1.56762217e-05
Iter: 288 loss: 1.54750687e-05
Iter: 289 loss: 1.54613681e-05
Iter: 290 loss: 1.54593072e-05
Iter: 291 loss: 1.54505578e-05
Iter: 292 loss: 1.54300942e-05
Iter: 293 loss: 1.57071627e-05
Iter: 294 loss: 1.54297959e-05
Iter: 295 loss: 1.54197696e-05
Iter: 296 loss: 1.54098689e-05
Iter: 297 loss: 1.54072695e-05
Iter: 298 loss: 1.53875044e-05
Iter: 299 loss: 1.53856345e-05
Iter: 300 loss: 1.53719793e-05
Iter: 301 loss: 1.53469173e-05
Iter: 302 loss: 1.54604131e-05
Iter: 303 loss: 1.53426154e-05
Iter: 304 loss: 1.53216733e-05
Iter: 305 loss: 1.54071058e-05
Iter: 306 loss: 1.53163746e-05
Iter: 307 loss: 1.52989469e-05
Iter: 308 loss: 1.53079054e-05
Iter: 309 loss: 1.52875837e-05
Iter: 310 loss: 1.52632347e-05
Iter: 311 loss: 1.537758e-05
Iter: 312 loss: 1.52578468e-05
Iter: 313 loss: 1.5241083e-05
Iter: 314 loss: 1.54428562e-05
Iter: 315 loss: 1.52409029e-05
Iter: 316 loss: 1.52222947e-05
Iter: 317 loss: 1.53098499e-05
Iter: 318 loss: 1.5218865e-05
Iter: 319 loss: 1.52092252e-05
Iter: 320 loss: 1.51975964e-05
Iter: 321 loss: 1.51974373e-05
Iter: 322 loss: 1.51846707e-05
Iter: 323 loss: 1.51824361e-05
Iter: 324 loss: 1.51747599e-05
Iter: 325 loss: 1.51516797e-05
Iter: 326 loss: 1.52102748e-05
Iter: 327 loss: 1.51445e-05
Iter: 328 loss: 1.51338436e-05
Iter: 329 loss: 1.51331969e-05
Iter: 330 loss: 1.51227468e-05
Iter: 331 loss: 1.51051472e-05
Iter: 332 loss: 1.51048662e-05
Iter: 333 loss: 1.50828291e-05
Iter: 334 loss: 1.51529102e-05
Iter: 335 loss: 1.5076801e-05
Iter: 336 loss: 1.50620017e-05
Iter: 337 loss: 1.50601336e-05
Iter: 338 loss: 1.50498527e-05
Iter: 339 loss: 1.50352598e-05
Iter: 340 loss: 1.52409557e-05
Iter: 341 loss: 1.50356364e-05
Iter: 342 loss: 1.50237775e-05
Iter: 343 loss: 1.50124797e-05
Iter: 344 loss: 1.50100595e-05
Iter: 345 loss: 1.49896159e-05
Iter: 346 loss: 1.50997894e-05
Iter: 347 loss: 1.49865382e-05
Iter: 348 loss: 1.49761263e-05
Iter: 349 loss: 1.49761672e-05
Iter: 350 loss: 1.49636953e-05
Iter: 351 loss: 1.5002629e-05
Iter: 352 loss: 1.49597181e-05
Iter: 353 loss: 1.49545813e-05
Iter: 354 loss: 1.49365369e-05
Iter: 355 loss: 1.49885545e-05
Iter: 356 loss: 1.49278403e-05
Iter: 357 loss: 1.49024681e-05
Iter: 358 loss: 1.51959794e-05
Iter: 359 loss: 1.49025709e-05
Iter: 360 loss: 1.48888812e-05
Iter: 361 loss: 1.49170992e-05
Iter: 362 loss: 1.48840563e-05
Iter: 363 loss: 1.48691e-05
Iter: 364 loss: 1.49413609e-05
Iter: 365 loss: 1.48679228e-05
Iter: 366 loss: 1.48549352e-05
Iter: 367 loss: 1.49251146e-05
Iter: 368 loss: 1.48518566e-05
Iter: 369 loss: 1.48422296e-05
Iter: 370 loss: 1.48283179e-05
Iter: 371 loss: 1.48273793e-05
Iter: 372 loss: 1.48113231e-05
Iter: 373 loss: 1.48959425e-05
Iter: 374 loss: 1.48079298e-05
Iter: 375 loss: 1.47925766e-05
Iter: 376 loss: 1.47873707e-05
Iter: 377 loss: 1.47776836e-05
Iter: 378 loss: 1.47551109e-05
Iter: 379 loss: 1.47633018e-05
Iter: 380 loss: 1.47392857e-05
Iter: 381 loss: 1.47178616e-05
Iter: 382 loss: 1.47178471e-05
Iter: 383 loss: 1.47072642e-05
Iter: 384 loss: 1.46903667e-05
Iter: 385 loss: 1.46905e-05
Iter: 386 loss: 1.4677933e-05
Iter: 387 loss: 1.46761586e-05
Iter: 388 loss: 1.46628909e-05
Iter: 389 loss: 1.47426108e-05
Iter: 390 loss: 1.46611374e-05
Iter: 391 loss: 1.46550628e-05
Iter: 392 loss: 1.46379489e-05
Iter: 393 loss: 1.46831262e-05
Iter: 394 loss: 1.46281973e-05
Iter: 395 loss: 1.46054581e-05
Iter: 396 loss: 1.48040481e-05
Iter: 397 loss: 1.46051116e-05
Iter: 398 loss: 1.45908489e-05
Iter: 399 loss: 1.46847688e-05
Iter: 400 loss: 1.458926e-05
Iter: 401 loss: 1.45741406e-05
Iter: 402 loss: 1.46185303e-05
Iter: 403 loss: 1.45707636e-05
Iter: 404 loss: 1.45560207e-05
Iter: 405 loss: 1.45960694e-05
Iter: 406 loss: 1.45529202e-05
Iter: 407 loss: 1.45378954e-05
Iter: 408 loss: 1.45255181e-05
Iter: 409 loss: 1.45210615e-05
Iter: 410 loss: 1.45031763e-05
Iter: 411 loss: 1.45945905e-05
Iter: 412 loss: 1.45005861e-05
Iter: 413 loss: 1.44830083e-05
Iter: 414 loss: 1.45329086e-05
Iter: 415 loss: 1.44773512e-05
Iter: 416 loss: 1.44639671e-05
Iter: 417 loss: 1.44592886e-05
Iter: 418 loss: 1.44519172e-05
Iter: 419 loss: 1.44314581e-05
Iter: 420 loss: 1.46115908e-05
Iter: 421 loss: 1.44296755e-05
Iter: 422 loss: 1.44155e-05
Iter: 423 loss: 1.44558035e-05
Iter: 424 loss: 1.44122805e-05
Iter: 425 loss: 1.44145415e-05
Iter: 426 loss: 1.44058158e-05
Iter: 427 loss: 1.44027108e-05
Iter: 428 loss: 1.43918714e-05
Iter: 429 loss: 1.44183859e-05
Iter: 430 loss: 1.43866237e-05
Iter: 431 loss: 1.43697634e-05
Iter: 432 loss: 1.43992747e-05
Iter: 433 loss: 1.43626212e-05
Iter: 434 loss: 1.43519428e-05
Iter: 435 loss: 1.44735814e-05
Iter: 436 loss: 1.43521102e-05
Iter: 437 loss: 1.43405887e-05
Iter: 438 loss: 1.43513507e-05
Iter: 439 loss: 1.43340212e-05
Iter: 440 loss: 1.43222405e-05
Iter: 441 loss: 1.43869165e-05
Iter: 442 loss: 1.43205198e-05
Iter: 443 loss: 1.43136494e-05
Iter: 444 loss: 1.43082989e-05
Iter: 445 loss: 1.43050565e-05
Iter: 446 loss: 1.4291446e-05
Iter: 447 loss: 1.43186653e-05
Iter: 448 loss: 1.42849485e-05
Iter: 449 loss: 1.42730714e-05
Iter: 450 loss: 1.42766321e-05
Iter: 451 loss: 1.42643203e-05
Iter: 452 loss: 1.42479703e-05
Iter: 453 loss: 1.43237767e-05
Iter: 454 loss: 1.42444142e-05
Iter: 455 loss: 1.42273693e-05
Iter: 456 loss: 1.42505251e-05
Iter: 457 loss: 1.42181289e-05
Iter: 458 loss: 1.42068811e-05
Iter: 459 loss: 1.42069039e-05
Iter: 460 loss: 1.41959526e-05
Iter: 461 loss: 1.42635936e-05
Iter: 462 loss: 1.41937826e-05
Iter: 463 loss: 1.41882783e-05
Iter: 464 loss: 1.41728869e-05
Iter: 465 loss: 1.42821318e-05
Iter: 466 loss: 1.41693927e-05
Iter: 467 loss: 1.41540122e-05
Iter: 468 loss: 1.41651099e-05
Iter: 469 loss: 1.41438659e-05
Iter: 470 loss: 1.41298297e-05
Iter: 471 loss: 1.41292867e-05
Iter: 472 loss: 1.41176215e-05
Iter: 473 loss: 1.41488163e-05
Iter: 474 loss: 1.4114019e-05
Iter: 475 loss: 1.41041755e-05
Iter: 476 loss: 1.41054834e-05
Iter: 477 loss: 1.40974407e-05
Iter: 478 loss: 1.40838874e-05
Iter: 479 loss: 1.41221281e-05
Iter: 480 loss: 1.40800494e-05
Iter: 481 loss: 1.40643297e-05
Iter: 482 loss: 1.40726224e-05
Iter: 483 loss: 1.40552111e-05
Iter: 484 loss: 1.40415978e-05
Iter: 485 loss: 1.41283117e-05
Iter: 486 loss: 1.40403918e-05
Iter: 487 loss: 1.40280908e-05
Iter: 488 loss: 1.40206821e-05
Iter: 489 loss: 1.40157235e-05
Iter: 490 loss: 1.40000038e-05
Iter: 491 loss: 1.41833898e-05
Iter: 492 loss: 1.40000457e-05
Iter: 493 loss: 1.39941294e-05
Iter: 494 loss: 1.39934118e-05
Iter: 495 loss: 1.39854392e-05
Iter: 496 loss: 1.39689555e-05
Iter: 497 loss: 1.42602958e-05
Iter: 498 loss: 1.3969111e-05
Iter: 499 loss: 1.39610756e-05
Iter: 500 loss: 1.39604317e-05
Iter: 501 loss: 1.39547319e-05
Iter: 502 loss: 1.39418471e-05
Iter: 503 loss: 1.3954239e-05
Iter: 504 loss: 1.39343929e-05
Iter: 505 loss: 1.39218228e-05
Iter: 506 loss: 1.39221211e-05
Iter: 507 loss: 1.39112353e-05
Iter: 508 loss: 1.39361546e-05
Iter: 509 loss: 1.39081694e-05
Iter: 510 loss: 1.3899823e-05
Iter: 511 loss: 1.38991845e-05
Iter: 512 loss: 1.3892999e-05
Iter: 513 loss: 1.38794849e-05
Iter: 514 loss: 1.3883564e-05
Iter: 515 loss: 1.38696805e-05
Iter: 516 loss: 1.38572132e-05
Iter: 517 loss: 1.40267257e-05
Iter: 518 loss: 1.3856672e-05
Iter: 519 loss: 1.38469886e-05
Iter: 520 loss: 1.38463165e-05
Iter: 521 loss: 1.38393e-05
Iter: 522 loss: 1.38277683e-05
Iter: 523 loss: 1.388081e-05
Iter: 524 loss: 1.38254236e-05
Iter: 525 loss: 1.38144478e-05
Iter: 526 loss: 1.38389823e-05
Iter: 527 loss: 1.38104624e-05
Iter: 528 loss: 1.38005898e-05
Iter: 529 loss: 1.38006189e-05
Iter: 530 loss: 1.3798096e-05
Iter: 531 loss: 1.37898332e-05
Iter: 532 loss: 1.3799905e-05
Iter: 533 loss: 1.37817296e-05
Iter: 534 loss: 1.37674051e-05
Iter: 535 loss: 1.38312262e-05
Iter: 536 loss: 1.37649622e-05
Iter: 537 loss: 1.37530515e-05
Iter: 538 loss: 1.37914194e-05
Iter: 539 loss: 1.37494517e-05
Iter: 540 loss: 1.37397556e-05
Iter: 541 loss: 1.38646237e-05
Iter: 542 loss: 1.37401403e-05
Iter: 543 loss: 1.37318657e-05
Iter: 544 loss: 1.3735591e-05
Iter: 545 loss: 1.37263687e-05
Iter: 546 loss: 1.37165898e-05
Iter: 547 loss: 1.37023362e-05
Iter: 548 loss: 1.37025745e-05
Iter: 549 loss: 1.36885756e-05
Iter: 550 loss: 1.36886883e-05
Iter: 551 loss: 1.36791987e-05
Iter: 552 loss: 1.36777235e-05
Iter: 553 loss: 1.36707667e-05
Iter: 554 loss: 1.36565395e-05
Iter: 555 loss: 1.37455709e-05
Iter: 556 loss: 1.36549988e-05
Iter: 557 loss: 1.36457347e-05
Iter: 558 loss: 1.36499693e-05
Iter: 559 loss: 1.36403578e-05
Iter: 560 loss: 1.36417893e-05
Iter: 561 loss: 1.36351518e-05
Iter: 562 loss: 1.36303233e-05
Iter: 563 loss: 1.36201688e-05
Iter: 564 loss: 1.37583365e-05
Iter: 565 loss: 1.36193939e-05
Iter: 566 loss: 1.36106546e-05
Iter: 567 loss: 1.36026283e-05
Iter: 568 loss: 1.36002745e-05
Iter: 569 loss: 1.35848204e-05
Iter: 570 loss: 1.35993132e-05
Iter: 571 loss: 1.35763275e-05
Iter: 572 loss: 1.35657247e-05
Iter: 573 loss: 1.35646978e-05
Iter: 574 loss: 1.35559585e-05
Iter: 575 loss: 1.35870578e-05
Iter: 576 loss: 1.35545733e-05
Iter: 577 loss: 1.3546005e-05
Iter: 578 loss: 1.35300434e-05
Iter: 579 loss: 1.38833602e-05
Iter: 580 loss: 1.35300115e-05
Iter: 581 loss: 1.35146074e-05
Iter: 582 loss: 1.36565668e-05
Iter: 583 loss: 1.35139071e-05
Iter: 584 loss: 1.3503538e-05
Iter: 585 loss: 1.35045211e-05
Iter: 586 loss: 1.34938691e-05
Iter: 587 loss: 1.34812626e-05
Iter: 588 loss: 1.36336039e-05
Iter: 589 loss: 1.34814018e-05
Iter: 590 loss: 1.34708016e-05
Iter: 591 loss: 1.34589354e-05
Iter: 592 loss: 1.34574666e-05
Iter: 593 loss: 1.34618185e-05
Iter: 594 loss: 1.34521e-05
Iter: 595 loss: 1.34444053e-05
Iter: 596 loss: 1.34365655e-05
Iter: 597 loss: 1.34354968e-05
Iter: 598 loss: 1.34263246e-05
Iter: 599 loss: 1.34124784e-05
Iter: 600 loss: 1.34126285e-05
Iter: 601 loss: 1.33956491e-05
Iter: 602 loss: 1.3419336e-05
Iter: 603 loss: 1.33862441e-05
Iter: 604 loss: 1.33691974e-05
Iter: 605 loss: 1.35915488e-05
Iter: 606 loss: 1.33698313e-05
Iter: 607 loss: 1.33589019e-05
Iter: 608 loss: 1.34519232e-05
Iter: 609 loss: 1.33591557e-05
Iter: 610 loss: 1.33504554e-05
Iter: 611 loss: 1.33437443e-05
Iter: 612 loss: 1.33414032e-05
Iter: 613 loss: 1.3329357e-05
Iter: 614 loss: 1.33398435e-05
Iter: 615 loss: 1.33220265e-05
Iter: 616 loss: 1.33064777e-05
Iter: 617 loss: 1.33549602e-05
Iter: 618 loss: 1.33014055e-05
Iter: 619 loss: 1.32893047e-05
Iter: 620 loss: 1.33777557e-05
Iter: 621 loss: 1.3287924e-05
Iter: 622 loss: 1.32764735e-05
Iter: 623 loss: 1.32861678e-05
Iter: 624 loss: 1.32701452e-05
Iter: 625 loss: 1.3261857e-05
Iter: 626 loss: 1.32617715e-05
Iter: 627 loss: 1.32542136e-05
Iter: 628 loss: 1.33183657e-05
Iter: 629 loss: 1.32534551e-05
Iter: 630 loss: 1.32498935e-05
Iter: 631 loss: 1.32370187e-05
Iter: 632 loss: 1.32576652e-05
Iter: 633 loss: 1.3229419e-05
Iter: 634 loss: 1.32137047e-05
Iter: 635 loss: 1.33467793e-05
Iter: 636 loss: 1.32135137e-05
Iter: 637 loss: 1.32010182e-05
Iter: 638 loss: 1.32151235e-05
Iter: 639 loss: 1.31946827e-05
Iter: 640 loss: 1.31880734e-05
Iter: 641 loss: 1.3187253e-05
Iter: 642 loss: 1.31806537e-05
Iter: 643 loss: 1.31747047e-05
Iter: 644 loss: 1.31729339e-05
Iter: 645 loss: 1.31609031e-05
Iter: 646 loss: 1.31704219e-05
Iter: 647 loss: 1.31541083e-05
Iter: 648 loss: 1.31409606e-05
Iter: 649 loss: 1.31490106e-05
Iter: 650 loss: 1.3133109e-05
Iter: 651 loss: 1.31180923e-05
Iter: 652 loss: 1.32037203e-05
Iter: 653 loss: 1.31164752e-05
Iter: 654 loss: 1.31007828e-05
Iter: 655 loss: 1.31317456e-05
Iter: 656 loss: 1.30946109e-05
Iter: 657 loss: 1.30855551e-05
Iter: 658 loss: 1.32279265e-05
Iter: 659 loss: 1.30850467e-05
Iter: 660 loss: 1.30792541e-05
Iter: 661 loss: 1.30793223e-05
Iter: 662 loss: 1.30744456e-05
Iter: 663 loss: 1.30624594e-05
Iter: 664 loss: 1.30877033e-05
Iter: 665 loss: 1.30548833e-05
Iter: 666 loss: 1.30428361e-05
Iter: 667 loss: 1.31475699e-05
Iter: 668 loss: 1.30424714e-05
Iter: 669 loss: 1.30319968e-05
Iter: 670 loss: 1.3035823e-05
Iter: 671 loss: 1.3025121e-05
Iter: 672 loss: 1.30150365e-05
Iter: 673 loss: 1.30906728e-05
Iter: 674 loss: 1.30146991e-05
Iter: 675 loss: 1.30046319e-05
Iter: 676 loss: 1.30393983e-05
Iter: 677 loss: 1.30011722e-05
Iter: 678 loss: 1.29919717e-05
Iter: 679 loss: 1.29930313e-05
Iter: 680 loss: 1.29853133e-05
Iter: 681 loss: 1.29741493e-05
Iter: 682 loss: 1.29784503e-05
Iter: 683 loss: 1.29658038e-05
Iter: 684 loss: 1.29553855e-05
Iter: 685 loss: 1.30284789e-05
Iter: 686 loss: 1.2953682e-05
Iter: 687 loss: 1.29425307e-05
Iter: 688 loss: 1.29758537e-05
Iter: 689 loss: 1.29400887e-05
Iter: 690 loss: 1.29305727e-05
Iter: 691 loss: 1.2952416e-05
Iter: 692 loss: 1.29277287e-05
Iter: 693 loss: 1.29248838e-05
Iter: 694 loss: 1.29213768e-05
Iter: 695 loss: 1.29176005e-05
Iter: 696 loss: 1.29079117e-05
Iter: 697 loss: 1.3094068e-05
Iter: 698 loss: 1.29077598e-05
Iter: 699 loss: 1.29015734e-05
Iter: 700 loss: 1.28994361e-05
Iter: 701 loss: 1.28955e-05
Iter: 702 loss: 1.28849206e-05
Iter: 703 loss: 1.28833108e-05
Iter: 704 loss: 1.28754345e-05
Iter: 705 loss: 1.28651409e-05
Iter: 706 loss: 1.30054559e-05
Iter: 707 loss: 1.28653965e-05
Iter: 708 loss: 1.28579813e-05
Iter: 709 loss: 1.28750153e-05
Iter: 710 loss: 1.28543088e-05
Iter: 711 loss: 1.28448055e-05
Iter: 712 loss: 1.29046421e-05
Iter: 713 loss: 1.28437368e-05
Iter: 714 loss: 1.28388383e-05
Iter: 715 loss: 1.28290631e-05
Iter: 716 loss: 1.28294423e-05
Iter: 717 loss: 1.28180591e-05
Iter: 718 loss: 1.28741431e-05
Iter: 719 loss: 1.28163265e-05
Iter: 720 loss: 1.28070151e-05
Iter: 721 loss: 1.28119191e-05
Iter: 722 loss: 1.2801278e-05
Iter: 723 loss: 1.27907415e-05
Iter: 724 loss: 1.28931906e-05
Iter: 725 loss: 1.27904759e-05
Iter: 726 loss: 1.27854237e-05
Iter: 727 loss: 1.27849344e-05
Iter: 728 loss: 1.27779522e-05
Iter: 729 loss: 1.27783778e-05
Iter: 730 loss: 1.27725143e-05
Iter: 731 loss: 1.27674994e-05
Iter: 732 loss: 1.27612229e-05
Iter: 733 loss: 1.27605463e-05
Iter: 734 loss: 1.27504991e-05
Iter: 735 loss: 1.27481599e-05
Iter: 736 loss: 1.2741687e-05
Iter: 737 loss: 1.27291605e-05
Iter: 738 loss: 1.27851399e-05
Iter: 739 loss: 1.27269759e-05
Iter: 740 loss: 1.27148223e-05
Iter: 741 loss: 1.27666735e-05
Iter: 742 loss: 1.27124995e-05
Iter: 743 loss: 1.27053245e-05
Iter: 744 loss: 1.27058156e-05
Iter: 745 loss: 1.26991163e-05
Iter: 746 loss: 1.26903196e-05
Iter: 747 loss: 1.26900304e-05
Iter: 748 loss: 1.26802215e-05
Iter: 749 loss: 1.27204385e-05
Iter: 750 loss: 1.26781833e-05
Iter: 751 loss: 1.26695777e-05
Iter: 752 loss: 1.26641935e-05
Iter: 753 loss: 1.26610812e-05
Iter: 754 loss: 1.26491705e-05
Iter: 755 loss: 1.2744692e-05
Iter: 756 loss: 1.26477807e-05
Iter: 757 loss: 1.26401028e-05
Iter: 758 loss: 1.26975947e-05
Iter: 759 loss: 1.26392497e-05
Iter: 760 loss: 1.26315917e-05
Iter: 761 loss: 1.27440535e-05
Iter: 762 loss: 1.26317937e-05
Iter: 763 loss: 1.26281539e-05
Iter: 764 loss: 1.26223058e-05
Iter: 765 loss: 1.27493095e-05
Iter: 766 loss: 1.26223204e-05
Iter: 767 loss: 1.26141585e-05
Iter: 768 loss: 1.26047507e-05
Iter: 769 loss: 1.26038331e-05
Iter: 770 loss: 1.25904862e-05
Iter: 771 loss: 1.269114e-05
Iter: 772 loss: 1.25903534e-05
Iter: 773 loss: 1.25815386e-05
Iter: 774 loss: 1.25847946e-05
Iter: 775 loss: 1.25752722e-05
Iter: 776 loss: 1.25659799e-05
Iter: 777 loss: 1.2711158e-05
Iter: 778 loss: 1.25659717e-05
Iter: 779 loss: 1.25584529e-05
Iter: 780 loss: 1.25845827e-05
Iter: 781 loss: 1.25561601e-05
Iter: 782 loss: 1.25492679e-05
Iter: 783 loss: 1.25453698e-05
Iter: 784 loss: 1.25417528e-05
Iter: 785 loss: 1.25325023e-05
Iter: 786 loss: 1.2535691e-05
Iter: 787 loss: 1.25261422e-05
Iter: 788 loss: 1.25147781e-05
Iter: 789 loss: 1.26068826e-05
Iter: 790 loss: 1.25140705e-05
Iter: 791 loss: 1.25040433e-05
Iter: 792 loss: 1.25036649e-05
Iter: 793 loss: 1.24951302e-05
Iter: 794 loss: 1.25053075e-05
Iter: 795 loss: 1.24902263e-05
Iter: 796 loss: 1.24871967e-05
Iter: 797 loss: 1.24804783e-05
Iter: 798 loss: 1.25921579e-05
Iter: 799 loss: 1.24797434e-05
Iter: 800 loss: 1.2474231e-05
Iter: 801 loss: 1.24705575e-05
Iter: 802 loss: 1.24679054e-05
Iter: 803 loss: 1.24562566e-05
Iter: 804 loss: 1.24835669e-05
Iter: 805 loss: 1.24511389e-05
Iter: 806 loss: 1.24412936e-05
Iter: 807 loss: 1.24370863e-05
Iter: 808 loss: 1.24309499e-05
Iter: 809 loss: 1.24204525e-05
Iter: 810 loss: 1.24196449e-05
Iter: 811 loss: 1.24129783e-05
Iter: 812 loss: 1.24387498e-05
Iter: 813 loss: 1.24105436e-05
Iter: 814 loss: 1.24022154e-05
Iter: 815 loss: 1.2407424e-05
Iter: 816 loss: 1.23971986e-05
Iter: 817 loss: 1.2385517e-05
Iter: 818 loss: 1.23808604e-05
Iter: 819 loss: 1.23753707e-05
Iter: 820 loss: 1.23602895e-05
Iter: 821 loss: 1.24508279e-05
Iter: 822 loss: 1.23582831e-05
Iter: 823 loss: 1.23477366e-05
Iter: 824 loss: 1.23643549e-05
Iter: 825 loss: 1.23417158e-05
Iter: 826 loss: 1.23395312e-05
Iter: 827 loss: 1.2335986e-05
Iter: 828 loss: 1.23294403e-05
Iter: 829 loss: 1.2323424e-05
Iter: 830 loss: 1.23217815e-05
Iter: 831 loss: 1.23134105e-05
Iter: 832 loss: 1.23017553e-05
Iter: 833 loss: 1.23013697e-05
Iter: 834 loss: 1.22885804e-05
Iter: 835 loss: 1.23742684e-05
Iter: 836 loss: 1.22855436e-05
Iter: 837 loss: 1.22768033e-05
Iter: 838 loss: 1.2276244e-05
Iter: 839 loss: 1.22693382e-05
Iter: 840 loss: 1.22537858e-05
Iter: 841 loss: 1.22988404e-05
Iter: 842 loss: 1.22491092e-05
Iter: 843 loss: 1.22378551e-05
Iter: 844 loss: 1.23105301e-05
Iter: 845 loss: 1.22358697e-05
Iter: 846 loss: 1.22230322e-05
Iter: 847 loss: 1.22616766e-05
Iter: 848 loss: 1.22196743e-05
Iter: 849 loss: 1.22096008e-05
Iter: 850 loss: 1.22149686e-05
Iter: 851 loss: 1.22038282e-05
Iter: 852 loss: 1.21908888e-05
Iter: 853 loss: 1.21961994e-05
Iter: 854 loss: 1.21830508e-05
Iter: 855 loss: 1.21698467e-05
Iter: 856 loss: 1.2262768e-05
Iter: 857 loss: 1.21688172e-05
Iter: 858 loss: 1.21580724e-05
Iter: 859 loss: 1.22034053e-05
Iter: 860 loss: 1.21566518e-05
Iter: 861 loss: 1.21470239e-05
Iter: 862 loss: 1.23037471e-05
Iter: 863 loss: 1.21475678e-05
Iter: 864 loss: 1.21436524e-05
Iter: 865 loss: 1.21349e-05
Iter: 866 loss: 1.22132969e-05
Iter: 867 loss: 1.21336006e-05
Iter: 868 loss: 1.2122533e-05
Iter: 869 loss: 1.21361936e-05
Iter: 870 loss: 1.21170742e-05
Iter: 871 loss: 1.21065332e-05
Iter: 872 loss: 1.2140169e-05
Iter: 873 loss: 1.21034464e-05
Iter: 874 loss: 1.20924215e-05
Iter: 875 loss: 1.21098565e-05
Iter: 876 loss: 1.20864697e-05
Iter: 877 loss: 1.20771456e-05
Iter: 878 loss: 1.20987879e-05
Iter: 879 loss: 1.20726118e-05
Iter: 880 loss: 1.20655768e-05
Iter: 881 loss: 1.20650366e-05
Iter: 882 loss: 1.20593295e-05
Iter: 883 loss: 1.20544482e-05
Iter: 884 loss: 1.2052441e-05
Iter: 885 loss: 1.20425175e-05
Iter: 886 loss: 1.20367104e-05
Iter: 887 loss: 1.2030896e-05
Iter: 888 loss: 1.20186933e-05
Iter: 889 loss: 1.20639343e-05
Iter: 890 loss: 1.20156656e-05
Iter: 891 loss: 1.20000495e-05
Iter: 892 loss: 1.20366349e-05
Iter: 893 loss: 1.1994659e-05
Iter: 894 loss: 1.20079167e-05
Iter: 895 loss: 1.19912747e-05
Iter: 896 loss: 1.19877477e-05
Iter: 897 loss: 1.1978359e-05
Iter: 898 loss: 1.2004748e-05
Iter: 899 loss: 1.19737988e-05
Iter: 900 loss: 1.19592369e-05
Iter: 901 loss: 1.20042632e-05
Iter: 902 loss: 1.19542583e-05
Iter: 903 loss: 1.19449433e-05
Iter: 904 loss: 1.19815413e-05
Iter: 905 loss: 1.19421893e-05
Iter: 906 loss: 1.19301367e-05
Iter: 907 loss: 1.19365777e-05
Iter: 908 loss: 1.19216038e-05
Iter: 909 loss: 1.19090619e-05
Iter: 910 loss: 1.19643801e-05
Iter: 911 loss: 1.19067154e-05
Iter: 912 loss: 1.18937569e-05
Iter: 913 loss: 1.19140977e-05
Iter: 914 loss: 1.18874359e-05
Iter: 915 loss: 1.18787548e-05
Iter: 916 loss: 1.187819e-05
Iter: 917 loss: 1.18712151e-05
Iter: 918 loss: 1.18607304e-05
Iter: 919 loss: 1.18603957e-05
Iter: 920 loss: 1.18487296e-05
Iter: 921 loss: 1.18696189e-05
Iter: 922 loss: 1.18434054e-05
Iter: 923 loss: 1.18282624e-05
Iter: 924 loss: 1.18513453e-05
Iter: 925 loss: 1.18216431e-05
Iter: 926 loss: 1.18094849e-05
Iter: 927 loss: 1.19230344e-05
Iter: 928 loss: 1.18089765e-05
Iter: 929 loss: 1.17949494e-05
Iter: 930 loss: 1.19123397e-05
Iter: 931 loss: 1.17947375e-05
Iter: 932 loss: 1.17873678e-05
Iter: 933 loss: 1.1775448e-05
Iter: 934 loss: 1.20577042e-05
Iter: 935 loss: 1.1775428e-05
Iter: 936 loss: 1.17646332e-05
Iter: 937 loss: 1.17791442e-05
Iter: 938 loss: 1.17597137e-05
Iter: 939 loss: 1.17464924e-05
Iter: 940 loss: 1.18412827e-05
Iter: 941 loss: 1.17454802e-05
Iter: 942 loss: 1.17356312e-05
Iter: 943 loss: 1.1749582e-05
Iter: 944 loss: 1.17307118e-05
Iter: 945 loss: 1.17188538e-05
Iter: 946 loss: 1.17361042e-05
Iter: 947 loss: 1.17132786e-05
Iter: 948 loss: 1.1706873e-05
Iter: 949 loss: 1.17058953e-05
Iter: 950 loss: 1.16999481e-05
Iter: 951 loss: 1.17007576e-05
Iter: 952 loss: 1.16958372e-05
Iter: 953 loss: 1.16857182e-05
Iter: 954 loss: 1.16786068e-05
Iter: 955 loss: 1.16752044e-05
Iter: 956 loss: 1.16624196e-05
Iter: 957 loss: 1.17099707e-05
Iter: 958 loss: 1.16590109e-05
Iter: 959 loss: 1.16473557e-05
Iter: 960 loss: 1.17023055e-05
Iter: 961 loss: 1.16455985e-05
Iter: 962 loss: 1.16508381e-05
Iter: 963 loss: 1.16434958e-05
Iter: 964 loss: 1.16404481e-05
Iter: 965 loss: 1.1633465e-05
Iter: 966 loss: 1.16662195e-05
Iter: 967 loss: 1.16311894e-05
Iter: 968 loss: 1.16223218e-05
Iter: 969 loss: 1.1641835e-05
Iter: 970 loss: 1.16191222e-05
Iter: 971 loss: 1.16108513e-05
Iter: 972 loss: 1.16142664e-05
Iter: 973 loss: 1.16046212e-05
Iter: 974 loss: 1.15951962e-05
Iter: 975 loss: 1.16959363e-05
Iter: 976 loss: 1.15945e-05
Iter: 977 loss: 1.15865323e-05
Iter: 978 loss: 1.15835974e-05
Iter: 979 loss: 1.15789271e-05
Iter: 980 loss: 1.1569181e-05
Iter: 981 loss: 1.16342362e-05
Iter: 982 loss: 1.15676157e-05
Iter: 983 loss: 1.15580388e-05
Iter: 984 loss: 1.15872663e-05
Iter: 985 loss: 1.15543944e-05
Iter: 986 loss: 1.15432e-05
Iter: 987 loss: 1.15736584e-05
Iter: 988 loss: 1.15406328e-05
Iter: 989 loss: 1.15338062e-05
Iter: 990 loss: 1.15257762e-05
Iter: 991 loss: 1.15255534e-05
Iter: 992 loss: 1.15127359e-05
Iter: 993 loss: 1.15966432e-05
Iter: 994 loss: 1.15112216e-05
Iter: 995 loss: 1.1506776e-05
Iter: 996 loss: 1.15062376e-05
Iter: 997 loss: 1.14988761e-05
Iter: 998 loss: 1.14926079e-05
Iter: 999 loss: 1.14908162e-05
Iter: 1000 loss: 1.14847371e-05
Iter: 1001 loss: 1.14829018e-05
Iter: 1002 loss: 1.14793311e-05
Iter: 1003 loss: 1.1470318e-05
Iter: 1004 loss: 1.14739032e-05
Iter: 1005 loss: 1.14641516e-05
Iter: 1006 loss: 1.14535724e-05
Iter: 1007 loss: 1.15086059e-05
Iter: 1008 loss: 1.1451506e-05
Iter: 1009 loss: 1.14397571e-05
Iter: 1010 loss: 1.14663435e-05
Iter: 1011 loss: 1.14363629e-05
Iter: 1012 loss: 1.14249433e-05
Iter: 1013 loss: 1.14519826e-05
Iter: 1014 loss: 1.14207087e-05
Iter: 1015 loss: 1.1411903e-05
Iter: 1016 loss: 1.14834174e-05
Iter: 1017 loss: 1.14116829e-05
Iter: 1018 loss: 1.14040849e-05
Iter: 1019 loss: 1.14468075e-05
Iter: 1020 loss: 1.14033828e-05
Iter: 1021 loss: 1.13966453e-05
Iter: 1022 loss: 1.1383424e-05
Iter: 1023 loss: 1.15839939e-05
Iter: 1024 loss: 1.13832721e-05
Iter: 1025 loss: 1.13681162e-05
Iter: 1026 loss: 1.14847026e-05
Iter: 1027 loss: 1.13670612e-05
Iter: 1028 loss: 1.13573024e-05
Iter: 1029 loss: 1.13668029e-05
Iter: 1030 loss: 1.13519609e-05
Iter: 1031 loss: 1.13528822e-05
Iter: 1032 loss: 1.13459264e-05
Iter: 1033 loss: 1.13436308e-05
Iter: 1034 loss: 1.13376536e-05
Iter: 1035 loss: 1.13853384e-05
Iter: 1036 loss: 1.13365386e-05
Iter: 1037 loss: 1.13298538e-05
Iter: 1038 loss: 1.13230708e-05
Iter: 1039 loss: 1.13208571e-05
Iter: 1040 loss: 1.13118831e-05
Iter: 1041 loss: 1.14098584e-05
Iter: 1042 loss: 1.13120441e-05
Iter: 1043 loss: 1.13038877e-05
Iter: 1044 loss: 1.13188371e-05
Iter: 1045 loss: 1.13006245e-05
Iter: 1046 loss: 1.12919843e-05
Iter: 1047 loss: 1.13393353e-05
Iter: 1048 loss: 1.12906655e-05
Iter: 1049 loss: 1.12839125e-05
Iter: 1050 loss: 1.1280441e-05
Iter: 1051 loss: 1.12770049e-05
Iter: 1052 loss: 1.12664711e-05
Iter: 1053 loss: 1.14004579e-05
Iter: 1054 loss: 1.12668149e-05
Iter: 1055 loss: 1.1260674e-05
Iter: 1056 loss: 1.12919179e-05
Iter: 1057 loss: 1.12595662e-05
Iter: 1058 loss: 1.12567741e-05
Iter: 1059 loss: 1.12468078e-05
Iter: 1060 loss: 1.13276401e-05
Iter: 1061 loss: 1.12452262e-05
Iter: 1062 loss: 1.12350772e-05
Iter: 1063 loss: 1.13649076e-05
Iter: 1064 loss: 1.12348625e-05
Iter: 1065 loss: 1.12296075e-05
Iter: 1066 loss: 1.12876778e-05
Iter: 1067 loss: 1.12297748e-05
Iter: 1068 loss: 1.1222086e-05
Iter: 1069 loss: 1.12296129e-05
Iter: 1070 loss: 1.12186481e-05
Iter: 1071 loss: 1.12143507e-05
Iter: 1072 loss: 1.12068701e-05
Iter: 1073 loss: 1.12072821e-05
Iter: 1074 loss: 1.11976642e-05
Iter: 1075 loss: 1.12079642e-05
Iter: 1076 loss: 1.11931822e-05
Iter: 1077 loss: 1.11824083e-05
Iter: 1078 loss: 1.12024545e-05
Iter: 1079 loss: 1.11768741e-05
Iter: 1080 loss: 1.11668814e-05
Iter: 1081 loss: 1.12686967e-05
Iter: 1082 loss: 1.11668614e-05
Iter: 1083 loss: 1.11594054e-05
Iter: 1084 loss: 1.11735753e-05
Iter: 1085 loss: 1.11560767e-05
Iter: 1086 loss: 1.11481831e-05
Iter: 1087 loss: 1.11760828e-05
Iter: 1088 loss: 1.11463614e-05
Iter: 1089 loss: 1.11417703e-05
Iter: 1090 loss: 1.12171965e-05
Iter: 1091 loss: 1.11422387e-05
Iter: 1092 loss: 1.11372847e-05
Iter: 1093 loss: 1.11303307e-05
Iter: 1094 loss: 1.11303398e-05
Iter: 1095 loss: 1.112065e-05
Iter: 1096 loss: 1.11517747e-05
Iter: 1097 loss: 1.11186646e-05
Iter: 1098 loss: 1.11106656e-05
Iter: 1099 loss: 1.11080972e-05
Iter: 1100 loss: 1.11029858e-05
Iter: 1101 loss: 1.11153649e-05
Iter: 1102 loss: 1.10982301e-05
Iter: 1103 loss: 1.10961992e-05
Iter: 1104 loss: 1.10882957e-05
Iter: 1105 loss: 1.11624058e-05
Iter: 1106 loss: 1.10882738e-05
Iter: 1107 loss: 1.10809551e-05
Iter: 1108 loss: 1.10688306e-05
Iter: 1109 loss: 1.1068536e-05
Iter: 1110 loss: 1.10563924e-05
Iter: 1111 loss: 1.12119624e-05
Iter: 1112 loss: 1.10562523e-05
Iter: 1113 loss: 1.10490564e-05
Iter: 1114 loss: 1.10664841e-05
Iter: 1115 loss: 1.10470901e-05
Iter: 1116 loss: 1.10380133e-05
Iter: 1117 loss: 1.10447236e-05
Iter: 1118 loss: 1.10321107e-05
Iter: 1119 loss: 1.10241062e-05
Iter: 1120 loss: 1.10269848e-05
Iter: 1121 loss: 1.10177789e-05
Iter: 1122 loss: 1.10124674e-05
Iter: 1123 loss: 1.1011809e-05
Iter: 1124 loss: 1.10077654e-05
Iter: 1125 loss: 1.1005528e-05
Iter: 1126 loss: 1.10027304e-05
Iter: 1127 loss: 1.09959728e-05
Iter: 1128 loss: 1.10036908e-05
Iter: 1129 loss: 1.09922012e-05
Iter: 1130 loss: 1.0984093e-05
Iter: 1131 loss: 1.09861612e-05
Iter: 1132 loss: 1.09786524e-05
Iter: 1133 loss: 1.09771117e-05
Iter: 1134 loss: 1.09742678e-05
Iter: 1135 loss: 1.0968859e-05
Iter: 1136 loss: 1.09654047e-05
Iter: 1137 loss: 1.09642033e-05
Iter: 1138 loss: 1.09584525e-05
Iter: 1139 loss: 1.09472057e-05
Iter: 1140 loss: 1.11614045e-05
Iter: 1141 loss: 1.09475586e-05
Iter: 1142 loss: 1.09398661e-05
Iter: 1143 loss: 1.10561432e-05
Iter: 1144 loss: 1.0939737e-05
Iter: 1145 loss: 1.09340745e-05
Iter: 1146 loss: 1.09284938e-05
Iter: 1147 loss: 1.0926804e-05
Iter: 1148 loss: 1.09191915e-05
Iter: 1149 loss: 1.09444445e-05
Iter: 1150 loss: 1.09175362e-05
Iter: 1151 loss: 1.09068023e-05
Iter: 1152 loss: 1.09582461e-05
Iter: 1153 loss: 1.09049488e-05
Iter: 1154 loss: 1.08992463e-05
Iter: 1155 loss: 1.09291514e-05
Iter: 1156 loss: 1.08981958e-05
Iter: 1157 loss: 1.08911963e-05
Iter: 1158 loss: 1.08922031e-05
Iter: 1159 loss: 1.08862496e-05
Iter: 1160 loss: 1.08790082e-05
Iter: 1161 loss: 1.09311113e-05
Iter: 1162 loss: 1.08788045e-05
Iter: 1163 loss: 1.08733084e-05
Iter: 1164 loss: 1.09155553e-05
Iter: 1165 loss: 1.08733902e-05
Iter: 1166 loss: 1.08688146e-05
Iter: 1167 loss: 1.08735048e-05
Iter: 1168 loss: 1.08662e-05
Iter: 1169 loss: 1.0863625e-05
Iter: 1170 loss: 1.08936174e-05
Iter: 1171 loss: 1.0863504e-05
Iter: 1172 loss: 1.0860258e-05
Iter: 1173 loss: 1.08545846e-05
Iter: 1174 loss: 1.08545428e-05
Iter: 1175 loss: 1.08497989e-05
Iter: 1176 loss: 1.08507811e-05
Iter: 1177 loss: 1.08463737e-05
Iter: 1178 loss: 1.08417316e-05
Iter: 1179 loss: 1.08445611e-05
Iter: 1180 loss: 1.08389022e-05
Iter: 1181 loss: 1.08329878e-05
Iter: 1182 loss: 1.08584927e-05
Iter: 1183 loss: 1.08307049e-05
Iter: 1184 loss: 1.08249551e-05
Iter: 1185 loss: 1.0820072e-05
Iter: 1186 loss: 1.08184458e-05
Iter: 1187 loss: 1.08073464e-05
Iter: 1188 loss: 1.08668928e-05
Iter: 1189 loss: 1.08056829e-05
Iter: 1190 loss: 1.07985161e-05
Iter: 1191 loss: 1.08257054e-05
Iter: 1192 loss: 1.07965479e-05
Iter: 1193 loss: 1.07888036e-05
Iter: 1194 loss: 1.08178183e-05
Iter: 1195 loss: 1.07859432e-05
Iter: 1196 loss: 1.07770993e-05
Iter: 1197 loss: 1.07913111e-05
Iter: 1198 loss: 1.0773515e-05
Iter: 1199 loss: 1.0768943e-05
Iter: 1200 loss: 1.0768933e-05
Iter: 1201 loss: 1.07651304e-05
Iter: 1202 loss: 1.07605156e-05
Iter: 1203 loss: 1.07598735e-05
Iter: 1204 loss: 1.07541855e-05
Iter: 1205 loss: 1.08034665e-05
Iter: 1206 loss: 1.075415e-05
Iter: 1207 loss: 1.07495853e-05
Iter: 1208 loss: 1.08071217e-05
Iter: 1209 loss: 1.07494379e-05
Iter: 1210 loss: 1.07468877e-05
Iter: 1211 loss: 1.07415599e-05
Iter: 1212 loss: 1.07438136e-05
Iter: 1213 loss: 1.07356191e-05
Iter: 1214 loss: 1.07284632e-05
Iter: 1215 loss: 1.07284186e-05
Iter: 1216 loss: 1.07231635e-05
Iter: 1217 loss: 1.07123415e-05
Iter: 1218 loss: 1.09569883e-05
Iter: 1219 loss: 1.07121396e-05
Iter: 1220 loss: 1.07004571e-05
Iter: 1221 loss: 1.08497397e-05
Iter: 1222 loss: 1.07003525e-05
Iter: 1223 loss: 1.06923435e-05
Iter: 1224 loss: 1.06889993e-05
Iter: 1225 loss: 1.06842253e-05
Iter: 1226 loss: 1.06721309e-05
Iter: 1227 loss: 1.07929181e-05
Iter: 1228 loss: 1.06716907e-05
Iter: 1229 loss: 1.06635162e-05
Iter: 1230 loss: 1.06887601e-05
Iter: 1231 loss: 1.06614007e-05
Iter: 1232 loss: 1.06543976e-05
Iter: 1233 loss: 1.07237074e-05
Iter: 1234 loss: 1.06544958e-05
Iter: 1235 loss: 1.06486623e-05
Iter: 1236 loss: 1.06429889e-05
Iter: 1237 loss: 1.06404823e-05
Iter: 1238 loss: 1.06347197e-05
Iter: 1239 loss: 1.06342959e-05
Iter: 1240 loss: 1.06300722e-05
Iter: 1241 loss: 1.06762363e-05
Iter: 1242 loss: 1.06297557e-05
Iter: 1243 loss: 1.06259295e-05
Iter: 1244 loss: 1.06211373e-05
Iter: 1245 loss: 1.0620638e-05
Iter: 1246 loss: 1.06141269e-05
Iter: 1247 loss: 1.06114867e-05
Iter: 1248 loss: 1.06079515e-05
Iter: 1249 loss: 1.05982053e-05
Iter: 1250 loss: 1.05926702e-05
Iter: 1251 loss: 1.05884556e-05
Iter: 1252 loss: 1.05786712e-05
Iter: 1253 loss: 1.06580355e-05
Iter: 1254 loss: 1.05775453e-05
Iter: 1255 loss: 1.05683812e-05
Iter: 1256 loss: 1.06064199e-05
Iter: 1257 loss: 1.05663548e-05
Iter: 1258 loss: 1.05593954e-05
Iter: 1259 loss: 1.05632653e-05
Iter: 1260 loss: 1.05543586e-05
Iter: 1261 loss: 1.05436575e-05
Iter: 1262 loss: 1.05824574e-05
Iter: 1263 loss: 1.05410691e-05
Iter: 1264 loss: 1.0533995e-05
Iter: 1265 loss: 1.05387262e-05
Iter: 1266 loss: 1.05301369e-05
Iter: 1267 loss: 1.05204745e-05
Iter: 1268 loss: 1.05737345e-05
Iter: 1269 loss: 1.05190511e-05
Iter: 1270 loss: 1.05118797e-05
Iter: 1271 loss: 1.05744821e-05
Iter: 1272 loss: 1.05112777e-05
Iter: 1273 loss: 1.05058043e-05
Iter: 1274 loss: 1.05408435e-05
Iter: 1275 loss: 1.05050995e-05
Iter: 1276 loss: 1.04976025e-05
Iter: 1277 loss: 1.05027175e-05
Iter: 1278 loss: 1.04936025e-05
Iter: 1279 loss: 1.04881638e-05
Iter: 1280 loss: 1.04967494e-05
Iter: 1281 loss: 1.04851542e-05
Iter: 1282 loss: 1.04792516e-05
Iter: 1283 loss: 1.04728251e-05
Iter: 1284 loss: 1.04725477e-05
Iter: 1285 loss: 1.0460898e-05
Iter: 1286 loss: 1.04836417e-05
Iter: 1287 loss: 1.04563569e-05
Iter: 1288 loss: 1.04473329e-05
Iter: 1289 loss: 1.04868423e-05
Iter: 1290 loss: 1.04462088e-05
Iter: 1291 loss: 1.04373012e-05
Iter: 1292 loss: 1.0453422e-05
Iter: 1293 loss: 1.04346964e-05
Iter: 1294 loss: 1.04257879e-05
Iter: 1295 loss: 1.04518558e-05
Iter: 1296 loss: 1.04236578e-05
Iter: 1297 loss: 1.04148294e-05
Iter: 1298 loss: 1.04461842e-05
Iter: 1299 loss: 1.04126575e-05
Iter: 1300 loss: 1.04046667e-05
Iter: 1301 loss: 1.04054161e-05
Iter: 1302 loss: 1.03988896e-05
Iter: 1303 loss: 1.03880548e-05
Iter: 1304 loss: 1.04812316e-05
Iter: 1305 loss: 1.03875846e-05
Iter: 1306 loss: 1.03819812e-05
Iter: 1307 loss: 1.04530491e-05
Iter: 1308 loss: 1.03816565e-05
Iter: 1309 loss: 1.03772891e-05
Iter: 1310 loss: 1.04166984e-05
Iter: 1311 loss: 1.03767252e-05
Iter: 1312 loss: 1.03719449e-05
Iter: 1313 loss: 1.03718412e-05
Iter: 1314 loss: 1.03685143e-05
Iter: 1315 loss: 1.03646144e-05
Iter: 1316 loss: 1.03610846e-05
Iter: 1317 loss: 1.03602606e-05
Iter: 1318 loss: 1.03512339e-05
Iter: 1319 loss: 1.03738548e-05
Iter: 1320 loss: 1.03486063e-05
Iter: 1321 loss: 1.03427756e-05
Iter: 1322 loss: 1.0348871e-05
Iter: 1323 loss: 1.03386783e-05
Iter: 1324 loss: 1.03306184e-05
Iter: 1325 loss: 1.0345324e-05
Iter: 1326 loss: 1.03270158e-05
Iter: 1327 loss: 1.03188067e-05
Iter: 1328 loss: 1.03212078e-05
Iter: 1329 loss: 1.03125958e-05
Iter: 1330 loss: 1.02998592e-05
Iter: 1331 loss: 1.03884095e-05
Iter: 1332 loss: 1.0298706e-05
Iter: 1333 loss: 1.02924369e-05
Iter: 1334 loss: 1.02990343e-05
Iter: 1335 loss: 1.02880076e-05
Iter: 1336 loss: 1.02788563e-05
Iter: 1337 loss: 1.03215061e-05
Iter: 1338 loss: 1.02769181e-05
Iter: 1339 loss: 1.0272026e-05
Iter: 1340 loss: 1.03227549e-05
Iter: 1341 loss: 1.02720787e-05
Iter: 1342 loss: 1.02669201e-05
Iter: 1343 loss: 1.03001566e-05
Iter: 1344 loss: 1.02665654e-05
Iter: 1345 loss: 1.0261223e-05
Iter: 1346 loss: 1.02612757e-05
Iter: 1347 loss: 1.02574768e-05
Iter: 1348 loss: 1.02516588e-05
Iter: 1349 loss: 1.02522899e-05
Iter: 1350 loss: 1.02476351e-05
Iter: 1351 loss: 1.02418689e-05
Iter: 1352 loss: 1.0248059e-05
Iter: 1353 loss: 1.02386748e-05
Iter: 1354 loss: 1.02306867e-05
Iter: 1355 loss: 1.02464146e-05
Iter: 1356 loss: 1.02274626e-05
Iter: 1357 loss: 1.02186877e-05
Iter: 1358 loss: 1.02284375e-05
Iter: 1359 loss: 1.021363e-05
Iter: 1360 loss: 1.02045442e-05
Iter: 1361 loss: 1.02300564e-05
Iter: 1362 loss: 1.02018148e-05
Iter: 1363 loss: 1.01929445e-05
Iter: 1364 loss: 1.02256145e-05
Iter: 1365 loss: 1.01906689e-05
Iter: 1366 loss: 1.01838832e-05
Iter: 1367 loss: 1.02045924e-05
Iter: 1368 loss: 1.01817768e-05
Iter: 1369 loss: 1.01712194e-05
Iter: 1370 loss: 1.01715668e-05
Iter: 1371 loss: 1.01629848e-05
Iter: 1372 loss: 1.01548358e-05
Iter: 1373 loss: 1.01549049e-05
Iter: 1374 loss: 1.015208e-05
Iter: 1375 loss: 1.01508413e-05
Iter: 1376 loss: 1.01483565e-05
Iter: 1377 loss: 1.01464084e-05
Iter: 1378 loss: 1.01447795e-05
Iter: 1379 loss: 1.01390387e-05
Iter: 1380 loss: 1.01434125e-05
Iter: 1381 loss: 1.01352607e-05
Iter: 1382 loss: 1.01315618e-05
Iter: 1383 loss: 1.0131731e-05
Iter: 1384 loss: 1.0127821e-05
Iter: 1385 loss: 1.01201958e-05
Iter: 1386 loss: 1.01239993e-05
Iter: 1387 loss: 1.01147107e-05
Iter: 1388 loss: 1.01088126e-05
Iter: 1389 loss: 1.01691039e-05
Iter: 1390 loss: 1.0108467e-05
Iter: 1391 loss: 1.01030246e-05
Iter: 1392 loss: 1.00928783e-05
Iter: 1393 loss: 1.00935249e-05
Iter: 1394 loss: 1.00833295e-05
Iter: 1395 loss: 1.02022386e-05
Iter: 1396 loss: 1.00836278e-05
Iter: 1397 loss: 1.00765483e-05
Iter: 1398 loss: 1.0076008e-05
Iter: 1399 loss: 1.00698326e-05
Iter: 1400 loss: 1.0060523e-05
Iter: 1401 loss: 1.01356427e-05
Iter: 1402 loss: 1.00601519e-05
Iter: 1403 loss: 1.00516445e-05
Iter: 1404 loss: 1.00621537e-05
Iter: 1405 loss: 1.004763e-05
Iter: 1406 loss: 1.00461411e-05
Iter: 1407 loss: 1.00435664e-05
Iter: 1408 loss: 1.00400521e-05
Iter: 1409 loss: 1.00429652e-05
Iter: 1410 loss: 1.00379984e-05
Iter: 1411 loss: 1.00330199e-05
Iter: 1412 loss: 1.00347652e-05
Iter: 1413 loss: 1.00302896e-05
Iter: 1414 loss: 1.00256948e-05
Iter: 1415 loss: 1.00231018e-05
Iter: 1416 loss: 1.0020839e-05
Iter: 1417 loss: 1.00134639e-05
Iter: 1418 loss: 1.00243178e-05
Iter: 1419 loss: 1.00104808e-05
Iter: 1420 loss: 1.00036013e-05
Iter: 1421 loss: 1.0052263e-05
Iter: 1422 loss: 1.00020279e-05
Iter: 1423 loss: 9.99678559e-06
Iter: 1424 loss: 9.98773e-06
Iter: 1425 loss: 9.98790711e-06
Iter: 1426 loss: 9.97773168e-06
Iter: 1427 loss: 1.00986726e-05
Iter: 1428 loss: 9.97698e-06
Iter: 1429 loss: 9.97069401e-06
Iter: 1430 loss: 9.97275674e-06
Iter: 1431 loss: 9.96609378e-06
Iter: 1432 loss: 9.95862865e-06
Iter: 1433 loss: 1.00099851e-05
Iter: 1434 loss: 9.95699702e-06
Iter: 1435 loss: 9.95134633e-06
Iter: 1436 loss: 9.96191375e-06
Iter: 1437 loss: 9.94936727e-06
Iter: 1438 loss: 9.94459879e-06
Iter: 1439 loss: 1.00167363e-05
Iter: 1440 loss: 9.94403126e-06
Iter: 1441 loss: 9.94108632e-06
Iter: 1442 loss: 9.98548603e-06
Iter: 1443 loss: 9.94116181e-06
Iter: 1444 loss: 9.93850699e-06
Iter: 1445 loss: 9.93711e-06
Iter: 1446 loss: 9.93643334e-06
Iter: 1447 loss: 9.93250251e-06
Iter: 1448 loss: 9.92705282e-06
Iter: 1449 loss: 9.92678179e-06
Iter: 1450 loss: 9.92105379e-06
Iter: 1451 loss: 9.94420589e-06
Iter: 1452 loss: 9.92012792e-06
Iter: 1453 loss: 9.91372417e-06
Iter: 1454 loss: 9.91806519e-06
Iter: 1455 loss: 9.91112574e-06
Iter: 1456 loss: 9.90366607e-06
Iter: 1457 loss: 9.9452518e-06
Iter: 1458 loss: 9.90291119e-06
Iter: 1459 loss: 9.89868e-06
Iter: 1460 loss: 9.89304317e-06
Iter: 1461 loss: 9.89293e-06
Iter: 1462 loss: 9.88598367e-06
Iter: 1463 loss: 9.96140261e-06
Iter: 1464 loss: 9.88603097e-06
Iter: 1465 loss: 9.88005377e-06
Iter: 1466 loss: 9.87364456e-06
Iter: 1467 loss: 9.87281601e-06
Iter: 1468 loss: 9.86597297e-06
Iter: 1469 loss: 9.86559917e-06
Iter: 1470 loss: 9.86132272e-06
Iter: 1471 loss: 9.88072225e-06
Iter: 1472 loss: 9.86095256e-06
Iter: 1473 loss: 9.86039595e-06
Iter: 1474 loss: 9.85862425e-06
Iter: 1475 loss: 9.85741099e-06
Iter: 1476 loss: 9.8544806e-06
Iter: 1477 loss: 9.91011257e-06
Iter: 1478 loss: 9.85482711e-06
Iter: 1479 loss: 9.85122279e-06
Iter: 1480 loss: 9.86078e-06
Iter: 1481 loss: 9.85051065e-06
Iter: 1482 loss: 9.84786857e-06
Iter: 1483 loss: 9.8428e-06
Iter: 1484 loss: 9.8422006e-06
Iter: 1485 loss: 9.83687551e-06
Iter: 1486 loss: 9.8567125e-06
Iter: 1487 loss: 9.83563405e-06
Iter: 1488 loss: 9.8303517e-06
Iter: 1489 loss: 9.8737728e-06
Iter: 1490 loss: 9.82988422e-06
Iter: 1491 loss: 9.82723486e-06
Iter: 1492 loss: 9.82332676e-06
Iter: 1493 loss: 9.82289203e-06
Iter: 1494 loss: 9.81563608e-06
Iter: 1495 loss: 9.83154678e-06
Iter: 1496 loss: 9.81361063e-06
Iter: 1497 loss: 9.80818459e-06
Iter: 1498 loss: 9.81679113e-06
Iter: 1499 loss: 9.80446839e-06
Iter: 1500 loss: 9.79692231e-06
Iter: 1501 loss: 9.83721566e-06
Iter: 1502 loss: 9.79521883e-06
Iter: 1503 loss: 9.79113429e-06
Iter: 1504 loss: 9.79067772e-06
Iter: 1505 loss: 9.78713797e-06
Iter: 1506 loss: 9.78614935e-06
Iter: 1507 loss: 9.783309e-06
Iter: 1508 loss: 9.78075695e-06
Iter: 1509 loss: 9.78332537e-06
Iter: 1510 loss: 9.77885884e-06
Iter: 1511 loss: 9.77660056e-06
Iter: 1512 loss: 9.77769196e-06
Iter: 1513 loss: 9.77527088e-06
Iter: 1514 loss: 9.77147283e-06
Iter: 1515 loss: 9.76637602e-06
Iter: 1516 loss: 9.76628e-06
Iter: 1517 loss: 9.75995863e-06
Iter: 1518 loss: 9.77906348e-06
Iter: 1519 loss: 9.75839885e-06
Iter: 1520 loss: 9.7529155e-06
Iter: 1521 loss: 9.75637158e-06
Iter: 1522 loss: 9.74915383e-06
Iter: 1523 loss: 9.7422726e-06
Iter: 1524 loss: 9.79923061e-06
Iter: 1525 loss: 9.74221257e-06
Iter: 1526 loss: 9.73656643e-06
Iter: 1527 loss: 9.72986527e-06
Iter: 1528 loss: 9.72955422e-06
Iter: 1529 loss: 9.72031285e-06
Iter: 1530 loss: 9.80177629e-06
Iter: 1531 loss: 9.71969712e-06
Iter: 1532 loss: 9.71301779e-06
Iter: 1533 loss: 9.71338e-06
Iter: 1534 loss: 9.70813198e-06
Iter: 1535 loss: 9.70024121e-06
Iter: 1536 loss: 9.77399759e-06
Iter: 1537 loss: 9.69995926e-06
Iter: 1538 loss: 9.69533176e-06
Iter: 1539 loss: 9.73058741e-06
Iter: 1540 loss: 9.69518e-06
Iter: 1541 loss: 9.69025677e-06
Iter: 1542 loss: 9.72810813e-06
Iter: 1543 loss: 9.6898475e-06
Iter: 1544 loss: 9.68748827e-06
Iter: 1545 loss: 9.68348559e-06
Iter: 1546 loss: 9.68363747e-06
Iter: 1547 loss: 9.67840151e-06
Iter: 1548 loss: 9.7043021e-06
Iter: 1549 loss: 9.67739834e-06
Iter: 1550 loss: 9.67292181e-06
Iter: 1551 loss: 9.66852167e-06
Iter: 1552 loss: 9.66778e-06
Iter: 1553 loss: 9.66169682e-06
Iter: 1554 loss: 9.69933717e-06
Iter: 1555 loss: 9.66132393e-06
Iter: 1556 loss: 9.65617e-06
Iter: 1557 loss: 9.66007519e-06
Iter: 1558 loss: 9.65304389e-06
Iter: 1559 loss: 9.64553419e-06
Iter: 1560 loss: 9.66452444e-06
Iter: 1561 loss: 9.64290575e-06
Iter: 1562 loss: 9.63761704e-06
Iter: 1563 loss: 9.63828734e-06
Iter: 1564 loss: 9.63290586e-06
Iter: 1565 loss: 9.62409649e-06
Iter: 1566 loss: 9.65728759e-06
Iter: 1567 loss: 9.62265858e-06
Iter: 1568 loss: 9.61420665e-06
Iter: 1569 loss: 9.63473667e-06
Iter: 1570 loss: 9.61259684e-06
Iter: 1571 loss: 9.60630859e-06
Iter: 1572 loss: 9.64812352e-06
Iter: 1573 loss: 9.60535635e-06
Iter: 1574 loss: 9.60242596e-06
Iter: 1575 loss: 9.60215675e-06
Iter: 1576 loss: 9.59989302e-06
Iter: 1577 loss: 9.59410227e-06
Iter: 1578 loss: 9.67331744e-06
Iter: 1579 loss: 9.59377849e-06
Iter: 1580 loss: 9.58907185e-06
Iter: 1581 loss: 9.63676212e-06
Iter: 1582 loss: 9.58865712e-06
Iter: 1583 loss: 9.58502096e-06
Iter: 1584 loss: 9.58063174e-06
Iter: 1585 loss: 9.57991688e-06
Iter: 1586 loss: 9.57267093e-06
Iter: 1587 loss: 9.58613e-06
Iter: 1588 loss: 9.56984e-06
Iter: 1589 loss: 9.5627538e-06
Iter: 1590 loss: 9.58295277e-06
Iter: 1591 loss: 9.56039548e-06
Iter: 1592 loss: 9.55382438e-06
Iter: 1593 loss: 9.58218334e-06
Iter: 1594 loss: 9.55179257e-06
Iter: 1595 loss: 9.54704046e-06
Iter: 1596 loss: 9.56794611e-06
Iter: 1597 loss: 9.54553252e-06
Iter: 1598 loss: 9.54103052e-06
Iter: 1599 loss: 9.53417202e-06
Iter: 1600 loss: 9.53346353e-06
Iter: 1601 loss: 9.52671235e-06
Iter: 1602 loss: 9.61703427e-06
Iter: 1603 loss: 9.52593746e-06
Iter: 1604 loss: 9.52066512e-06
Iter: 1605 loss: 9.5171381e-06
Iter: 1606 loss: 9.51517814e-06
Iter: 1607 loss: 9.52627306e-06
Iter: 1608 loss: 9.51317543e-06
Iter: 1609 loss: 9.51093534e-06
Iter: 1610 loss: 9.50764479e-06
Iter: 1611 loss: 9.58683813e-06
Iter: 1612 loss: 9.50762114e-06
Iter: 1613 loss: 9.50324284e-06
Iter: 1614 loss: 9.50060166e-06
Iter: 1615 loss: 9.49898731e-06
Iter: 1616 loss: 9.49093555e-06
Iter: 1617 loss: 9.53368362e-06
Iter: 1618 loss: 9.49028799e-06
Iter: 1619 loss: 9.48505294e-06
Iter: 1620 loss: 9.48690649e-06
Iter: 1621 loss: 9.48248544e-06
Iter: 1622 loss: 9.4764e-06
Iter: 1623 loss: 9.48133311e-06
Iter: 1624 loss: 9.47209446e-06
Iter: 1625 loss: 9.46567707e-06
Iter: 1626 loss: 9.5224641e-06
Iter: 1627 loss: 9.465517e-06
Iter: 1628 loss: 9.45985084e-06
Iter: 1629 loss: 9.4640036e-06
Iter: 1630 loss: 9.45636111e-06
Iter: 1631 loss: 9.45052e-06
Iter: 1632 loss: 9.47467e-06
Iter: 1633 loss: 9.44880867e-06
Iter: 1634 loss: 9.44471867e-06
Iter: 1635 loss: 9.44400381e-06
Iter: 1636 loss: 9.44062776e-06
Iter: 1637 loss: 9.4328152e-06
Iter: 1638 loss: 9.46682849e-06
Iter: 1639 loss: 9.43141e-06
Iter: 1640 loss: 9.42677161e-06
Iter: 1641 loss: 9.46288583e-06
Iter: 1642 loss: 9.42614952e-06
Iter: 1643 loss: 9.42002953e-06
Iter: 1644 loss: 9.45458123e-06
Iter: 1645 loss: 9.41932558e-06
Iter: 1646 loss: 9.41714825e-06
Iter: 1647 loss: 9.41459166e-06
Iter: 1648 loss: 9.41414601e-06
Iter: 1649 loss: 9.41059079e-06
Iter: 1650 loss: 9.42460429e-06
Iter: 1651 loss: 9.40973587e-06
Iter: 1652 loss: 9.40627433e-06
Iter: 1653 loss: 9.40372684e-06
Iter: 1654 loss: 9.40214704e-06
Iter: 1655 loss: 9.39728488e-06
Iter: 1656 loss: 9.40475093e-06
Iter: 1657 loss: 9.39490565e-06
Iter: 1658 loss: 9.38957419e-06
Iter: 1659 loss: 9.40964492e-06
Iter: 1660 loss: 9.38819e-06
Iter: 1661 loss: 9.3831959e-06
Iter: 1662 loss: 9.39811616e-06
Iter: 1663 loss: 9.38185622e-06
Iter: 1664 loss: 9.37661207e-06
Iter: 1665 loss: 9.38814e-06
Iter: 1666 loss: 9.37450932e-06
Iter: 1667 loss: 9.36977267e-06
Iter: 1668 loss: 9.37629557e-06
Iter: 1669 loss: 9.36748893e-06
Iter: 1670 loss: 9.36162905e-06
Iter: 1671 loss: 9.37000095e-06
Iter: 1672 loss: 9.35893149e-06
Iter: 1673 loss: 9.35413846e-06
Iter: 1674 loss: 9.40425525e-06
Iter: 1675 loss: 9.35415483e-06
Iter: 1676 loss: 9.35154276e-06
Iter: 1677 loss: 9.35105436e-06
Iter: 1678 loss: 9.35016942e-06
Iter: 1679 loss: 9.34621858e-06
Iter: 1680 loss: 9.39452366e-06
Iter: 1681 loss: 9.34595846e-06
Iter: 1682 loss: 9.34379113e-06
Iter: 1683 loss: 9.36782e-06
Iter: 1684 loss: 9.34242235e-06
Iter: 1685 loss: 9.33944648e-06
Iter: 1686 loss: 9.33910542e-06
Iter: 1687 loss: 9.33734e-06
Iter: 1688 loss: 9.33211504e-06
Iter: 1689 loss: 9.33419324e-06
Iter: 1690 loss: 9.33018237e-06
Iter: 1691 loss: 9.32546754e-06
Iter: 1692 loss: 9.33980482e-06
Iter: 1693 loss: 9.32428338e-06
Iter: 1694 loss: 9.31908653e-06
Iter: 1695 loss: 9.33773481e-06
Iter: 1696 loss: 9.31826253e-06
Iter: 1697 loss: 9.31485738e-06
Iter: 1698 loss: 9.33346382e-06
Iter: 1699 loss: 9.31487739e-06
Iter: 1700 loss: 9.31118302e-06
Iter: 1701 loss: 9.30485294e-06
Iter: 1702 loss: 9.30419901e-06
Iter: 1703 loss: 9.29835915e-06
Iter: 1704 loss: 9.36838205e-06
Iter: 1705 loss: 9.29766338e-06
Iter: 1706 loss: 9.29425732e-06
Iter: 1707 loss: 9.29548241e-06
Iter: 1708 loss: 9.29076123e-06
Iter: 1709 loss: 9.29750058e-06
Iter: 1710 loss: 9.28900226e-06
Iter: 1711 loss: 9.2879427e-06
Iter: 1712 loss: 9.28407462e-06
Iter: 1713 loss: 9.30477381e-06
Iter: 1714 loss: 9.28303143e-06
Iter: 1715 loss: 9.27797828e-06
Iter: 1716 loss: 9.29841735e-06
Iter: 1717 loss: 9.27692508e-06
Iter: 1718 loss: 9.27222572e-06
Iter: 1719 loss: 9.28662121e-06
Iter: 1720 loss: 9.27149813e-06
Iter: 1721 loss: 9.26694702e-06
Iter: 1722 loss: 9.26624762e-06
Iter: 1723 loss: 9.26350549e-06
Iter: 1724 loss: 9.25725271e-06
Iter: 1725 loss: 9.27253404e-06
Iter: 1726 loss: 9.25573659e-06
Iter: 1727 loss: 9.25026325e-06
Iter: 1728 loss: 9.26602388e-06
Iter: 1729 loss: 9.24834239e-06
Iter: 1730 loss: 9.24298729e-06
Iter: 1731 loss: 9.26784469e-06
Iter: 1732 loss: 9.24191954e-06
Iter: 1733 loss: 9.23727839e-06
Iter: 1734 loss: 9.24384494e-06
Iter: 1735 loss: 9.23494099e-06
Iter: 1736 loss: 9.22887193e-06
Iter: 1737 loss: 9.23756306e-06
Iter: 1738 loss: 9.22622e-06
Iter: 1739 loss: 9.22037907e-06
Iter: 1740 loss: 9.23468906e-06
Iter: 1741 loss: 9.21869832e-06
Iter: 1742 loss: 9.21344144e-06
Iter: 1743 loss: 9.28120608e-06
Iter: 1744 loss: 9.21302126e-06
Iter: 1745 loss: 9.20745151e-06
Iter: 1746 loss: 9.23949847e-06
Iter: 1747 loss: 9.20677758e-06
Iter: 1748 loss: 9.20525144e-06
Iter: 1749 loss: 9.20177899e-06
Iter: 1750 loss: 9.26769917e-06
Iter: 1751 loss: 9.20184175e-06
Iter: 1752 loss: 9.19702325e-06
Iter: 1753 loss: 9.21926e-06
Iter: 1754 loss: 9.19639933e-06
Iter: 1755 loss: 9.1923539e-06
Iter: 1756 loss: 9.19969716e-06
Iter: 1757 loss: 9.19050854e-06
Iter: 1758 loss: 9.18687874e-06
Iter: 1759 loss: 9.18282149e-06
Iter: 1760 loss: 9.18193655e-06
Iter: 1761 loss: 9.17638499e-06
Iter: 1762 loss: 9.2291566e-06
Iter: 1763 loss: 9.17560465e-06
Iter: 1764 loss: 9.1712327e-06
Iter: 1765 loss: 9.17880425e-06
Iter: 1766 loss: 9.16900262e-06
Iter: 1767 loss: 9.16319823e-06
Iter: 1768 loss: 9.17235866e-06
Iter: 1769 loss: 9.16051522e-06
Iter: 1770 loss: 9.15581222e-06
Iter: 1771 loss: 9.17151374e-06
Iter: 1772 loss: 9.15390865e-06
Iter: 1773 loss: 9.14905831e-06
Iter: 1774 loss: 9.15530654e-06
Iter: 1775 loss: 9.14631e-06
Iter: 1776 loss: 9.14225893e-06
Iter: 1777 loss: 9.19564718e-06
Iter: 1778 loss: 9.14258271e-06
Iter: 1779 loss: 9.14060911e-06
Iter: 1780 loss: 9.14038174e-06
Iter: 1781 loss: 9.1390084e-06
Iter: 1782 loss: 9.13513577e-06
Iter: 1783 loss: 9.14184238e-06
Iter: 1784 loss: 9.13147414e-06
Iter: 1785 loss: 9.12790074e-06
Iter: 1786 loss: 9.1275333e-06
Iter: 1787 loss: 9.12469659e-06
Iter: 1788 loss: 9.12448e-06
Iter: 1789 loss: 9.12236465e-06
Iter: 1790 loss: 9.11778716e-06
Iter: 1791 loss: 9.11446477e-06
Iter: 1792 loss: 9.11171264e-06
Iter: 1793 loss: 9.10744e-06
Iter: 1794 loss: 9.14665179e-06
Iter: 1795 loss: 9.10666949e-06
Iter: 1796 loss: 9.10228937e-06
Iter: 1797 loss: 9.10575e-06
Iter: 1798 loss: 9.09972914e-06
Iter: 1799 loss: 9.09554183e-06
Iter: 1800 loss: 9.1261536e-06
Iter: 1801 loss: 9.09476694e-06
Iter: 1802 loss: 9.09031496e-06
Iter: 1803 loss: 9.08730544e-06
Iter: 1804 loss: 9.08529273e-06
Iter: 1805 loss: 9.07923095e-06
Iter: 1806 loss: 9.10713061e-06
Iter: 1807 loss: 9.07773665e-06
Iter: 1808 loss: 9.07154754e-06
Iter: 1809 loss: 9.08080619e-06
Iter: 1810 loss: 9.06788136e-06
Iter: 1811 loss: 9.07430876e-06
Iter: 1812 loss: 9.06632158e-06
Iter: 1813 loss: 9.06462719e-06
Iter: 1814 loss: 9.06127207e-06
Iter: 1815 loss: 9.09353548e-06
Iter: 1816 loss: 9.06091554e-06
Iter: 1817 loss: 9.05656634e-06
Iter: 1818 loss: 9.05349953e-06
Iter: 1819 loss: 9.05169e-06
Iter: 1820 loss: 9.04560693e-06
Iter: 1821 loss: 9.04548233e-06
Iter: 1822 loss: 9.04116132e-06
Iter: 1823 loss: 9.03923592e-06
Iter: 1824 loss: 9.03679393e-06
Iter: 1825 loss: 9.03132059e-06
Iter: 1826 loss: 9.03785167e-06
Iter: 1827 loss: 9.02940428e-06
Iter: 1828 loss: 9.02315e-06
Iter: 1829 loss: 9.04963599e-06
Iter: 1830 loss: 9.02301e-06
Iter: 1831 loss: 9.0171352e-06
Iter: 1832 loss: 9.03904e-06
Iter: 1833 loss: 9.01574913e-06
Iter: 1834 loss: 9.01201747e-06
Iter: 1835 loss: 9.01568092e-06
Iter: 1836 loss: 9.00967279e-06
Iter: 1837 loss: 9.00476516e-06
Iter: 1838 loss: 9.01105523e-06
Iter: 1839 loss: 9.00126815e-06
Iter: 1840 loss: 8.99546376e-06
Iter: 1841 loss: 9.00559098e-06
Iter: 1842 loss: 8.99317547e-06
Iter: 1843 loss: 8.98838061e-06
Iter: 1844 loss: 8.98866347e-06
Iter: 1845 loss: 8.98437247e-06
Iter: 1846 loss: 9.03152431e-06
Iter: 1847 loss: 8.98449434e-06
Iter: 1848 loss: 8.98230792e-06
Iter: 1849 loss: 8.97780137e-06
Iter: 1850 loss: 8.98184408e-06
Iter: 1851 loss: 8.9737805e-06
Iter: 1852 loss: 8.97017e-06
Iter: 1853 loss: 8.96994788e-06
Iter: 1854 loss: 8.96603e-06
Iter: 1855 loss: 8.97735845e-06
Iter: 1856 loss: 8.96563506e-06
Iter: 1857 loss: 8.96186884e-06
Iter: 1858 loss: 8.95920311e-06
Iter: 1859 loss: 8.95861467e-06
Iter: 1860 loss: 8.95455196e-06
Iter: 1861 loss: 8.97957216e-06
Iter: 1862 loss: 8.95376797e-06
Iter: 1863 loss: 8.95064295e-06
Iter: 1864 loss: 8.95057e-06
Iter: 1865 loss: 8.94734148e-06
Iter: 1866 loss: 8.94294408e-06
Iter: 1867 loss: 8.9843e-06
Iter: 1868 loss: 8.94260211e-06
Iter: 1869 loss: 8.939076e-06
Iter: 1870 loss: 8.9388268e-06
Iter: 1871 loss: 8.93712058e-06
Iter: 1872 loss: 8.9320456e-06
Iter: 1873 loss: 8.95029098e-06
Iter: 1874 loss: 8.93032e-06
Iter: 1875 loss: 8.92616299e-06
Iter: 1876 loss: 8.92848e-06
Iter: 1877 loss: 8.92368917e-06
Iter: 1878 loss: 8.92403659e-06
Iter: 1879 loss: 8.92122443e-06
Iter: 1880 loss: 8.91927812e-06
Iter: 1881 loss: 8.91581658e-06
Iter: 1882 loss: 8.91556e-06
Iter: 1883 loss: 8.91316085e-06
Iter: 1884 loss: 8.90759657e-06
Iter: 1885 loss: 8.99255338e-06
Iter: 1886 loss: 8.90702177e-06
Iter: 1887 loss: 8.90126285e-06
Iter: 1888 loss: 8.95475478e-06
Iter: 1889 loss: 8.90077536e-06
Iter: 1890 loss: 8.89739e-06
Iter: 1891 loss: 8.9524674e-06
Iter: 1892 loss: 8.89745934e-06
Iter: 1893 loss: 8.89312196e-06
Iter: 1894 loss: 8.8870429e-06
Iter: 1895 loss: 9.01081694e-06
Iter: 1896 loss: 8.88693285e-06
Iter: 1897 loss: 8.88152226e-06
Iter: 1898 loss: 8.90146748e-06
Iter: 1899 loss: 8.87975875e-06
Iter: 1900 loss: 8.87532951e-06
Iter: 1901 loss: 8.89204875e-06
Iter: 1902 loss: 8.87380156e-06
Iter: 1903 loss: 8.86975613e-06
Iter: 1904 loss: 8.87173792e-06
Iter: 1905 loss: 8.86673e-06
Iter: 1906 loss: 8.86063481e-06
Iter: 1907 loss: 8.86796715e-06
Iter: 1908 loss: 8.85714871e-06
Iter: 1909 loss: 8.85215923e-06
Iter: 1910 loss: 8.89645e-06
Iter: 1911 loss: 8.8521665e-06
Iter: 1912 loss: 8.84646761e-06
Iter: 1913 loss: 8.84275687e-06
Iter: 1914 loss: 8.84103247e-06
Iter: 1915 loss: 8.84999463e-06
Iter: 1916 loss: 8.83880239e-06
Iter: 1917 loss: 8.83713528e-06
Iter: 1918 loss: 8.83233133e-06
Iter: 1919 loss: 8.86974431e-06
Iter: 1920 loss: 8.83124449e-06
Iter: 1921 loss: 8.82555105e-06
Iter: 1922 loss: 8.82818313e-06
Iter: 1923 loss: 8.82187214e-06
Iter: 1924 loss: 8.81600135e-06
Iter: 1925 loss: 8.83769644e-06
Iter: 1926 loss: 8.81509732e-06
Iter: 1927 loss: 8.81050437e-06
Iter: 1928 loss: 8.86897487e-06
Iter: 1929 loss: 8.81078813e-06
Iter: 1930 loss: 8.80865355e-06
Iter: 1931 loss: 8.8060724e-06
Iter: 1932 loss: 8.80520929e-06
Iter: 1933 loss: 8.79878826e-06
Iter: 1934 loss: 8.80221342e-06
Iter: 1935 loss: 8.79587515e-06
Iter: 1936 loss: 8.79180152e-06
Iter: 1937 loss: 8.83546545e-06
Iter: 1938 loss: 8.79211348e-06
Iter: 1939 loss: 8.78817809e-06
Iter: 1940 loss: 8.78395531e-06
Iter: 1941 loss: 8.78339233e-06
Iter: 1942 loss: 8.77728598e-06
Iter: 1943 loss: 8.81096639e-06
Iter: 1944 loss: 8.77615457e-06
Iter: 1945 loss: 8.77102048e-06
Iter: 1946 loss: 8.77742059e-06
Iter: 1947 loss: 8.76798913e-06
Iter: 1948 loss: 8.76603553e-06
Iter: 1949 loss: 8.7652088e-06
Iter: 1950 loss: 8.76335253e-06
Iter: 1951 loss: 8.77952e-06
Iter: 1952 loss: 8.76294e-06
Iter: 1953 loss: 8.76121885e-06
Iter: 1954 loss: 8.75750084e-06
Iter: 1955 loss: 8.76845479e-06
Iter: 1956 loss: 8.75557726e-06
Iter: 1957 loss: 8.7505523e-06
Iter: 1958 loss: 8.78785249e-06
Iter: 1959 loss: 8.75025762e-06
Iter: 1960 loss: 8.74474699e-06
Iter: 1961 loss: 8.75841397e-06
Iter: 1962 loss: 8.7429e-06
Iter: 1963 loss: 8.73828867e-06
Iter: 1964 loss: 8.77552156e-06
Iter: 1965 loss: 8.73879708e-06
Iter: 1966 loss: 8.73533827e-06
Iter: 1967 loss: 8.73558565e-06
Iter: 1968 loss: 8.73279896e-06
Iter: 1969 loss: 8.72892088e-06
Iter: 1970 loss: 8.73353201e-06
Iter: 1971 loss: 8.72680175e-06
Iter: 1972 loss: 8.72268083e-06
Iter: 1973 loss: 8.75403475e-06
Iter: 1974 loss: 8.72276541e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script66
+ '[' -r STOP.script66 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output66/f1_psi0_phi2.8/500_500_500_500_1
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 2.8
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output66/f1_psi1_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output67/f1_psi1_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output66/f1_psi1_phi2.8 /home/mrdouglas/Manifold/experiments.final/output67/f1_psi1_phi2.8
+ date
Sat Oct 24 21:53:00 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output66/f1_psi1_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 2345 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 1 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output66/f1_psi1_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e08b2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e0912378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e090b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e0858a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e076c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e0858400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e06dca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e07b2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e07b2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e07d9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e06236a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e06388c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e0638730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e0674620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e05f02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e0614bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e060d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e0579bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e0614d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25bd0c3048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25bd0bd048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25e075aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25bd06c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25bd064378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25bd064730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25bcffd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2598071840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2598080400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f259800c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2598080488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25bd012510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25bcfc7158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25bcfc7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25bcfc7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f25800722f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2580087d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.6170625
test_loss: 0.5899815
train_loss: 0.382946
test_loss: 0.37661123
train_loss: 0.25234628
test_loss: 0.260113
train_loss: 0.29959527
test_loss: 0.29145885
train_loss: 0.2185238
test_loss: 0.21926953
train_loss: 0.18322417
test_loss: 0.18758734
train_loss: 0.15931301
test_loss: 0.16297443
train_loss: 0.13344814
test_loss: 0.13599196
train_loss: 0.100878775
test_loss: 0.10482966
train_loss: 0.09069902
test_loss: 0.086354926
train_loss: 0.07392384
test_loss: 0.07698265
train_loss: 0.064744115
test_loss: 0.0682904
train_loss: 0.06436456
test_loss: 0.06526761
train_loss: 0.0556487
test_loss: 0.06079017
train_loss: 0.05271087
test_loss: 0.055905
train_loss: 0.05152891
test_loss: 0.05415296
train_loss: 0.050518464
test_loss: 0.052225992
train_loss: 0.047512926
test_loss: 0.04991198
train_loss: 0.048437588
test_loss: 0.048260305
train_loss: 0.039107732
test_loss: 0.046325713
train_loss: 0.042097017
test_loss: 0.04577383
train_loss: 0.03954374
test_loss: 0.044133894
train_loss: 0.041535024
test_loss: 0.04306614
train_loss: 0.036511667
test_loss: 0.041680325
train_loss: 0.0396714
test_loss: 0.04051585
train_loss: 0.037716586
test_loss: 0.04052891
train_loss: 0.035764102
test_loss: 0.04092628
train_loss: 0.03706443
test_loss: 0.039200906
train_loss: 0.03337516
test_loss: 0.03938156
train_loss: 0.034527365
test_loss: 0.038715888
train_loss: 0.03479765
test_loss: 0.037372936
train_loss: 0.032732032
test_loss: 0.0370682
train_loss: 0.03390732
test_loss: 0.037134685
train_loss: 0.03252603
test_loss: 0.03597261
train_loss: 0.03356865
test_loss: 0.03654904
train_loss: 0.029659253
test_loss: 0.03646912
train_loss: 0.032377373
test_loss: 0.0349385
train_loss: 0.0327869
test_loss: 0.03421293
train_loss: 0.029280772
test_loss: 0.03276983
train_loss: 0.031313457
test_loss: 0.03384479
train_loss: 0.026686106
test_loss: 0.033848796
train_loss: 0.031121802
test_loss: 0.033509534
train_loss: 0.02859198
test_loss: 0.033630513
train_loss: 0.03160541
test_loss: 0.03210386
train_loss: 0.028370686
test_loss: 0.03132354
train_loss: 0.02861158
test_loss: 0.030543152
train_loss: 0.027845645
test_loss: 0.030764896
train_loss: 0.02620773
test_loss: 0.030379267
train_loss: 0.02684654
test_loss: 0.030584106
train_loss: 0.027742587
test_loss: 0.030670172
train_loss: 0.026791837
test_loss: 0.030711498
train_loss: 0.028829612
test_loss: 0.03019396
train_loss: 0.026359778
test_loss: 0.028302772
train_loss: 0.025301842
test_loss: 0.028085914
train_loss: 0.027767034
test_loss: 0.02936954
train_loss: 0.025872476
test_loss: 0.027911095
train_loss: 0.02406346
test_loss: 0.028864387
train_loss: 0.02387408
test_loss: 0.027674638
train_loss: 0.020942297
test_loss: 0.027232444
train_loss: 0.022119168
test_loss: 0.026458677
train_loss: 0.021460012
test_loss: 0.026243506
train_loss: 0.026449319
test_loss: 0.026285227
train_loss: 0.023873448
test_loss: 0.026533054
train_loss: 0.020573676
test_loss: 0.025719225
train_loss: 0.021853853
test_loss: 0.02492894
train_loss: 0.021329585
test_loss: 0.024264863
train_loss: 0.02087499
test_loss: 0.02449712
train_loss: 0.020307302
test_loss: 0.024332142
train_loss: 0.022800177
test_loss: 0.023846777
train_loss: 0.020914137
test_loss: 0.024130287
train_loss: 0.018989036
test_loss: 0.023638709
train_loss: 0.021951795
test_loss: 0.023230018
train_loss: 0.018666616
test_loss: 0.02308502
train_loss: 0.020013092
test_loss: 0.023141332
train_loss: 0.01792125
test_loss: 0.024005739
train_loss: 0.018912783
test_loss: 0.023200681
train_loss: 0.020940486
test_loss: 0.022660146
train_loss: 0.019521518
test_loss: 0.021366995
train_loss: 0.0207286
test_loss: 0.021801507
train_loss: 0.017085547
test_loss: 0.02179241
train_loss: 0.019880628
test_loss: 0.022330428
train_loss: 0.018292831
test_loss: 0.021620031
train_loss: 0.019794544
test_loss: 0.022221278
train_loss: 0.016976302
test_loss: 0.02059722
train_loss: 0.016796395
test_loss: 0.020956323
train_loss: 0.018167455
test_loss: 0.021731852
train_loss: 0.017973676
test_loss: 0.020237384
train_loss: 0.016285986
test_loss: 0.020365713
train_loss: 0.017576028
test_loss: 0.020512337
train_loss: 0.021385668
test_loss: 0.021081766
train_loss: 0.017045638
test_loss: 0.020060422
train_loss: 0.017992076
test_loss: 0.019937169
train_loss: 0.015722066
test_loss: 0.019538142
train_loss: 0.017677484
test_loss: 0.01915383
train_loss: 0.017494041
test_loss: 0.020761779
train_loss: 0.015870541
test_loss: 0.019841064
train_loss: 0.018082293
test_loss: 0.01891871
train_loss: 0.015868187
test_loss: 0.020863501
train_loss: 0.01709636
test_loss: 0.021058524
train_loss: 0.015712235
test_loss: 0.018659106
train_loss: 0.016943714
test_loss: 0.018138174
train_loss: 0.01668139
test_loss: 0.018931866
train_loss: 0.016015545
test_loss: 0.018678071
train_loss: 0.016124943
test_loss: 0.018151315
train_loss: 0.015576992
test_loss: 0.018062282
train_loss: 0.014432356
test_loss: 0.017754305
train_loss: 0.015132632
test_loss: 0.018026967
train_loss: 0.014208352
test_loss: 0.018004308
train_loss: 0.014306808
test_loss: 0.01838699
train_loss: 0.016307801
test_loss: 0.019091891
train_loss: 0.014391629
test_loss: 0.016909989
train_loss: 0.014226268
test_loss: 0.018272994
train_loss: 0.013762869
test_loss: 0.018347653
train_loss: 0.015912097
test_loss: 0.01768284
train_loss: 0.014184199
test_loss: 0.017515495
train_loss: 0.015734605
test_loss: 0.018401902
train_loss: 0.015238738
test_loss: 0.017705116
train_loss: 0.014348555
test_loss: 0.017199887
train_loss: 0.0162592
test_loss: 0.017440299
train_loss: 0.0143623
test_loss: 0.017124413
train_loss: 0.014995847
test_loss: 0.017028097
train_loss: 0.014979774
test_loss: 0.016477687
train_loss: 0.013546029
test_loss: 0.017087137
train_loss: 0.013251316
test_loss: 0.016594743
train_loss: 0.015181301
test_loss: 0.017360218
train_loss: 0.013107841
test_loss: 0.016220236
train_loss: 0.012980379
test_loss: 0.01588641
train_loss: 0.013503265
test_loss: 0.015912531
train_loss: 0.013505859
test_loss: 0.016419344
train_loss: 0.012626578
test_loss: 0.016134217
train_loss: 0.013671087
test_loss: 0.016697507
train_loss: 0.012913392
test_loss: 0.015732769
train_loss: 0.012410584
test_loss: 0.015864145
train_loss: 0.012523313
test_loss: 0.015296834
train_loss: 0.012980046
test_loss: 0.01621737
train_loss: 0.012158212
test_loss: 0.0154764075
train_loss: 0.0125101125
test_loss: 0.015340258
train_loss: 0.013127953
test_loss: 0.0148597155
train_loss: 0.013808391
test_loss: 0.015722696
train_loss: 0.012631338
test_loss: 0.016733788
train_loss: 0.013704475
test_loss: 0.016092516
train_loss: 0.013303968
test_loss: 0.01641935
train_loss: 0.012642326
test_loss: 0.015043323
train_loss: 0.012048163
test_loss: 0.015662456
train_loss: 0.013357697
test_loss: 0.015583912
train_loss: 0.013654986
test_loss: 0.015382105
train_loss: 0.011454709
test_loss: 0.015875429
train_loss: 0.013206601
test_loss: 0.014763881
train_loss: 0.013080284
test_loss: 0.015861541
train_loss: 0.012594739
test_loss: 0.0149343675
train_loss: 0.01238643
test_loss: 0.014775775
train_loss: 0.012071276
test_loss: 0.0150517905
train_loss: 0.011982068
test_loss: 0.015559133
train_loss: 0.0106344
test_loss: 0.01500516
train_loss: 0.012514094
test_loss: 0.014852203
train_loss: 0.011519824
test_loss: 0.014648097
train_loss: 0.012187965
test_loss: 0.015669867
train_loss: 0.012562978
test_loss: 0.014661548
train_loss: 0.0128332805
test_loss: 0.015526363
train_loss: 0.011925855
test_loss: 0.01437505
train_loss: 0.012191348
test_loss: 0.014890936
train_loss: 0.012528783
test_loss: 0.016081132
train_loss: 0.01300921
test_loss: 0.015210742
train_loss: 0.011663895
test_loss: 0.014508511
train_loss: 0.012497392
test_loss: 0.014274189
train_loss: 0.011975905
test_loss: 0.014785666
train_loss: 0.013104184
test_loss: 0.014867261
train_loss: 0.012151576
test_loss: 0.015053769
train_loss: 0.011582574
test_loss: 0.014163683
train_loss: 0.012774502
test_loss: 0.015338994
train_loss: 0.012935491
test_loss: 0.014436323
train_loss: 0.01066024
test_loss: 0.013999272
train_loss: 0.010407918
test_loss: 0.014198993
train_loss: 0.012124275
test_loss: 0.01411295
train_loss: 0.012511883
test_loss: 0.014308549
train_loss: 0.012274017
test_loss: 0.014477098
train_loss: 0.011568502
test_loss: 0.014044328
train_loss: 0.010941142
test_loss: 0.014379651
train_loss: 0.010359698
test_loss: 0.013528926
train_loss: 0.010613416
test_loss: 0.013937723
train_loss: 0.012320027
test_loss: 0.014222535
train_loss: 0.011246482
test_loss: 0.013675415
train_loss: 0.010780031
test_loss: 0.013784333
train_loss: 0.00961541
test_loss: 0.013905299
train_loss: 0.010482354
test_loss: 0.013724286
train_loss: 0.011458321
test_loss: 0.014396413
train_loss: 0.01231697
test_loss: 0.014606378
train_loss: 0.011711451
test_loss: 0.013660405
train_loss: 0.011561584
test_loss: 0.013722324
train_loss: 0.013168136
test_loss: 0.014471202
train_loss: 0.011069624
test_loss: 0.015010692
train_loss: 0.012793993
test_loss: 0.014626209
train_loss: 0.009801628
test_loss: 0.013623657
train_loss: 0.01093699
test_loss: 0.013680724
train_loss: 0.010161629
test_loss: 0.0140345795
train_loss: 0.010021228
test_loss: 0.014110529
train_loss: 0.011860235
test_loss: 0.015397607
train_loss: 0.011650534
test_loss: 0.013949724
train_loss: 0.011578614
test_loss: 0.013533002
train_loss: 0.009553792
test_loss: 0.012978452
train_loss: 0.009544751
test_loss: 0.0135417655
train_loss: 0.010110876
test_loss: 0.013233199
train_loss: 0.011869949
test_loss: 0.013449153
train_loss: 0.0128347315
test_loss: 0.014654119
train_loss: 0.010297325
test_loss: 0.013271743
train_loss: 0.009831152
test_loss: 0.012911729
train_loss: 0.011852844
test_loss: 0.013547255
train_loss: 0.010997666
test_loss: 0.014633916
train_loss: 0.011200695
test_loss: 0.01382268
train_loss: 0.010235309
test_loss: 0.013195165
train_loss: 0.010376516
test_loss: 0.01350502
train_loss: 0.01019506
test_loss: 0.012981624
train_loss: 0.010981303
test_loss: 0.013693941
train_loss: 0.011403223
test_loss: 0.013585773
train_loss: 0.011284724
test_loss: 0.0143948775
train_loss: 0.011220086
test_loss: 0.013665318
train_loss: 0.010046508
test_loss: 0.013152675
train_loss: 0.009782561
test_loss: 0.013244177
train_loss: 0.010478334
test_loss: 0.01303175
train_loss: 0.010367539
test_loss: 0.013019397
train_loss: 0.0104834875
test_loss: 0.012952838
train_loss: 0.010679762
test_loss: 0.013475532
train_loss: 0.010290557
test_loss: 0.013088386
train_loss: 0.0096887415
test_loss: 0.012520119
train_loss: 0.009934813
test_loss: 0.013206565
train_loss: 0.009240232
test_loss: 0.012946683
train_loss: 0.009858586
test_loss: 0.013009041
train_loss: 0.010161241
test_loss: 0.012479534
train_loss: 0.0101863
test_loss: 0.014228521
train_loss: 0.010436321
test_loss: 0.0129481405
train_loss: 0.010069835
test_loss: 0.01324719
train_loss: 0.010516765
test_loss: 0.013101907
train_loss: 0.009148741
test_loss: 0.012676389
train_loss: 0.009732419
test_loss: 0.012640816
train_loss: 0.009321908
test_loss: 0.012391377
train_loss: 0.0104415305
test_loss: 0.012991116
train_loss: 0.009679796
test_loss: 0.0124712195
train_loss: 0.009527319
test_loss: 0.0129487915
train_loss: 0.0104815
test_loss: 0.013147613
train_loss: 0.010667877
test_loss: 0.013589141
train_loss: 0.0097798165
test_loss: 0.012666304
train_loss: 0.0093310755
test_loss: 0.013606897
train_loss: 0.0102662
test_loss: 0.01230546
train_loss: 0.009850945
test_loss: 0.013423856
train_loss: 0.011222642
test_loss: 0.013805429
train_loss: 0.008796526
test_loss: 0.012882168
train_loss: 0.0095908465
test_loss: 0.012824137
train_loss: 0.009633343
test_loss: 0.012554621
train_loss: 0.010505507
test_loss: 0.012489054
train_loss: 0.009781287
test_loss: 0.012496693
train_loss: 0.010140858
test_loss: 0.013299554
train_loss: 0.009773228
test_loss: 0.012760345
train_loss: 0.009883144
test_loss: 0.0130217
train_loss: 0.009566527
test_loss: 0.013552082
train_loss: 0.010290479
test_loss: 0.01273215
train_loss: 0.010293356
test_loss: 0.013394407
train_loss: 0.010038658
test_loss: 0.012773501
train_loss: 0.009995567
test_loss: 0.012367447
train_loss: 0.01099102
test_loss: 0.012803577
train_loss: 0.010304196
test_loss: 0.013299683
train_loss: 0.008883834
test_loss: 0.012367383
train_loss: 0.009631836
test_loss: 0.012749934
train_loss: 0.010037612
test_loss: 0.012788232
train_loss: 0.010276381
test_loss: 0.013358037
train_loss: 0.009562558
test_loss: 0.012678535
train_loss: 0.009611021
test_loss: 0.012313752
train_loss: 0.008296574
test_loss: 0.012117466
train_loss: 0.009531289
test_loss: 0.012356249
train_loss: 0.009433938
test_loss: 0.012355094
train_loss: 0.009034259
test_loss: 0.012410123
train_loss: 0.010113908
test_loss: 0.012184878
train_loss: 0.008793186
test_loss: 0.012086884
train_loss: 0.008417162
test_loss: 0.012191352
train_loss: 0.008988056
test_loss: 0.012499606
train_loss: 0.009503842
test_loss: 0.01268154
train_loss: 0.009726776
test_loss: 0.012144114
train_loss: 0.00947115
test_loss: 0.012523128
train_loss: 0.00915205
test_loss: 0.012883645
train_loss: 0.009089107
test_loss: 0.011964439
train_loss: 0.009389429
test_loss: 0.012936734
train_loss: 0.009310972
test_loss: 0.0124513
train_loss: 0.009359356
test_loss: 0.012680898
train_loss: 0.009385137
test_loss: 0.012287503
train_loss: 0.009189827
test_loss: 0.012903908
train_loss: 0.009696788
test_loss: 0.012160406
train_loss: 0.009563803
test_loss: 0.012535969
train_loss: 0.009263576
test_loss: 0.011951177
train_loss: 0.008720692
test_loss: 0.012236186
train_loss: 0.009153554
test_loss: 0.012579932
train_loss: 0.01004256
test_loss: 0.0123241665
train_loss: 0.00939669
test_loss: 0.0120874625
train_loss: 0.0098044
test_loss: 0.011971239
train_loss: 0.009082002
test_loss: 0.012355642
train_loss: 0.009463832
test_loss: 0.0124603305
train_loss: 0.009322083
test_loss: 0.012059991
train_loss: 0.008821683
test_loss: 0.01212181
train_loss: 0.009050079
test_loss: 0.011423339
train_loss: 0.009053806
test_loss: 0.012097187
train_loss: 0.009323968
test_loss: 0.01251566
train_loss: 0.009786535
test_loss: 0.012239849
train_loss: 0.009011202
test_loss: 0.011579295
train_loss: 0.009101
test_loss: 0.012312914
train_loss: 0.009178347
test_loss: 0.012868078
train_loss: 0.0076522515
test_loss: 0.01165887
train_loss: 0.009073873
test_loss: 0.012806776
train_loss: 0.008232926
test_loss: 0.011860775
train_loss: 0.009072116
test_loss: 0.01197417
train_loss: 0.008244135
test_loss: 0.012058736
train_loss: 0.010335768
test_loss: 0.012613821
train_loss: 0.009552715
test_loss: 0.012008177
train_loss: 0.008923655
test_loss: 0.012378615
train_loss: 0.0099957585
test_loss: 0.011955207
train_loss: 0.008665827
test_loss: 0.0119790565
train_loss: 0.008951644
test_loss: 0.012344559
train_loss: 0.008887586
test_loss: 0.012097488
train_loss: 0.008896255
test_loss: 0.011528668
train_loss: 0.008738477
test_loss: 0.013451963
train_loss: 0.008639435
test_loss: 0.012074509
train_loss: 0.008423092
test_loss: 0.011697819
train_loss: 0.008351714
test_loss: 0.011579747
train_loss: 0.00882949
test_loss: 0.012513646
train_loss: 0.008523574
test_loss: 0.012409959
train_loss: 0.008916337
test_loss: 0.012272279
train_loss: 0.008628584
test_loss: 0.011895061
train_loss: 0.009775896
test_loss: 0.012236116
train_loss: 0.0083603375
test_loss: 0.011908857
train_loss: 0.009800917
test_loss: 0.012335029
train_loss: 0.008835505
test_loss: 0.0117178075
train_loss: 0.0078017246
test_loss: 0.011414687
train_loss: 0.0088273
test_loss: 0.0119361365
train_loss: 0.008098903
test_loss: 0.011702775
train_loss: 0.008642231
test_loss: 0.011560351
train_loss: 0.009229939
test_loss: 0.011976585
train_loss: 0.008432419
test_loss: 0.011789659
train_loss: 0.009442411
test_loss: 0.011832517
train_loss: 0.008200527
test_loss: 0.011755324
train_loss: 0.008810038
test_loss: 0.011486811
train_loss: 0.008872515
test_loss: 0.011653278
train_loss: 0.00857579
test_loss: 0.012340601
train_loss: 0.0074291844
test_loss: 0.011198732
train_loss: 0.0077550206
test_loss: 0.011376255
train_loss: 0.008073032
test_loss: 0.011708309
train_loss: 0.0088531235
test_loss: 0.012450314
train_loss: 0.009293788
test_loss: 0.011795479
train_loss: 0.008486818
test_loss: 0.011766711
train_loss: 0.009225911
test_loss: 0.011902827
train_loss: 0.00877057
test_loss: 0.011861638
train_loss: 0.008552076
test_loss: 0.0118110385
train_loss: 0.008510148
test_loss: 0.011575881
train_loss: 0.008002494
test_loss: 0.011513164
train_loss: 0.008751836
test_loss: 0.011785057
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.008097012
test_loss: 0.01198051
train_loss: 0.008945046
test_loss: 0.011672225
train_loss: 0.009586122
test_loss: 0.012344253
train_loss: 0.007683948
test_loss: 0.011429565
train_loss: 0.00786897
test_loss: 0.011360811
train_loss: 0.007973053
test_loss: 0.011404365
train_loss: 0.008076171
test_loss: 0.011598883
train_loss: 0.008505181
test_loss: 0.012194237
train_loss: 0.007969048
test_loss: 0.011713878
train_loss: 0.008894951
test_loss: 0.011755101
train_loss: 0.008065469
test_loss: 0.012098121
train_loss: 0.008440143
test_loss: 0.011923009
train_loss: 0.008569394
test_loss: 0.012226283
train_loss: 0.008487578
test_loss: 0.0116907125
train_loss: 0.008127583
test_loss: 0.012175435
train_loss: 0.008804464
test_loss: 0.01188763
train_loss: 0.008284641
test_loss: 0.011564781
train_loss: 0.0082042795
test_loss: 0.011735482
train_loss: 0.008599726
test_loss: 0.012049564
train_loss: 0.008988662
test_loss: 0.012661963
train_loss: 0.008223717
test_loss: 0.012147255
train_loss: 0.007796586
test_loss: 0.011058346
train_loss: 0.008692519
test_loss: 0.011509019
train_loss: 0.008725682
test_loss: 0.012758595
train_loss: 0.008730374
test_loss: 0.0123762395
train_loss: 0.008390278
test_loss: 0.011571671
train_loss: 0.008687478
test_loss: 0.011304025
train_loss: 0.008122897
test_loss: 0.011247188
train_loss: 0.008707965
test_loss: 0.011707263
train_loss: 0.00832846
test_loss: 0.011421954
train_loss: 0.008611403
test_loss: 0.0118452655
train_loss: 0.008083145
test_loss: 0.011257945
train_loss: 0.008510709
test_loss: 0.011810875
train_loss: 0.00789455
test_loss: 0.011344579
train_loss: 0.008508867
test_loss: 0.011416925
train_loss: 0.008520946
test_loss: 0.011682117
train_loss: 0.008254932
test_loss: 0.011679463
train_loss: 0.008199116
test_loss: 0.0119926585
train_loss: 0.0077550625
test_loss: 0.01119767
train_loss: 0.008864667
test_loss: 0.01152124
train_loss: 0.007630315
test_loss: 0.011317776
train_loss: 0.008156435
test_loss: 0.011843223
train_loss: 0.008089658
test_loss: 0.011903554
train_loss: 0.008309195
test_loss: 0.011352046
train_loss: 0.00865132
test_loss: 0.011696151
train_loss: 0.008770989
test_loss: 0.011606218
train_loss: 0.007761931
test_loss: 0.011355832
train_loss: 0.007480061
test_loss: 0.010928711
train_loss: 0.008009167
test_loss: 0.011141007
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output67/f1_psi1_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output66/f1_psi1_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 1 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output67/f1_psi1_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d5997b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d5d3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d5ca0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d53b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d49b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d53b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d4b62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d433598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d4332f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d3d0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d4337b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d3db158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d3998c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d35f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d35f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d313c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d2d0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d2e4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d3139d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d249488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b8d27a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74f0eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74ecd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74ee9400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74ee9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74e9a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74e5d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74e66620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74e07598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74e07ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74de1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74d8c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74de1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74d9fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74d72840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8b74d71b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000384780578
Iter: 2 loss: 0.000324270281
Iter: 3 loss: 0.00121985981
Iter: 4 loss: 0.000324002351
Iter: 5 loss: 0.00029073804
Iter: 6 loss: 0.00073377369
Iter: 7 loss: 0.000290594704
Iter: 8 loss: 0.000257760199
Iter: 9 loss: 0.000266370247
Iter: 10 loss: 0.000233927844
Iter: 11 loss: 0.00021517284
Iter: 12 loss: 0.000247487245
Iter: 13 loss: 0.000206678698
Iter: 14 loss: 0.000186655583
Iter: 15 loss: 0.000199596165
Iter: 16 loss: 0.000173992143
Iter: 17 loss: 0.000152905239
Iter: 18 loss: 0.000343856984
Iter: 19 loss: 0.000151898421
Iter: 20 loss: 0.000142330944
Iter: 21 loss: 0.000186132878
Iter: 22 loss: 0.000140517965
Iter: 23 loss: 0.000133328198
Iter: 24 loss: 0.000128659187
Iter: 25 loss: 0.000125899183
Iter: 26 loss: 0.000116826806
Iter: 27 loss: 0.000195049361
Iter: 28 loss: 0.000116340052
Iter: 29 loss: 0.000110705259
Iter: 30 loss: 0.000141405064
Iter: 31 loss: 0.000109865323
Iter: 32 loss: 0.0001048821
Iter: 33 loss: 0.000122879283
Iter: 34 loss: 0.000103627943
Iter: 35 loss: 9.96308809e-05
Iter: 36 loss: 0.000102819642
Iter: 37 loss: 9.72149792e-05
Iter: 38 loss: 9.20526218e-05
Iter: 39 loss: 0.000102013284
Iter: 40 loss: 8.98946746e-05
Iter: 41 loss: 9.19971499e-05
Iter: 42 loss: 8.85259506e-05
Iter: 43 loss: 8.70934527e-05
Iter: 44 loss: 8.4153944e-05
Iter: 45 loss: 0.000135749244
Iter: 46 loss: 8.40970752e-05
Iter: 47 loss: 8.13262741e-05
Iter: 48 loss: 8.76691192e-05
Iter: 49 loss: 8.02809082e-05
Iter: 50 loss: 7.75742228e-05
Iter: 51 loss: 8.70474832e-05
Iter: 52 loss: 7.68730824e-05
Iter: 53 loss: 7.44617719e-05
Iter: 54 loss: 7.72834755e-05
Iter: 55 loss: 7.31797918e-05
Iter: 56 loss: 7.07972431e-05
Iter: 57 loss: 9.38853627e-05
Iter: 58 loss: 7.0711365e-05
Iter: 59 loss: 6.94033879e-05
Iter: 60 loss: 6.75479678e-05
Iter: 61 loss: 6.74812e-05
Iter: 62 loss: 6.51200826e-05
Iter: 63 loss: 8.83111788e-05
Iter: 64 loss: 6.50392176e-05
Iter: 65 loss: 6.36197728e-05
Iter: 66 loss: 6.81280944e-05
Iter: 67 loss: 6.32134e-05
Iter: 68 loss: 6.17975311e-05
Iter: 69 loss: 6.50250295e-05
Iter: 70 loss: 6.12683871e-05
Iter: 71 loss: 5.97122307e-05
Iter: 72 loss: 6.76763229e-05
Iter: 73 loss: 5.94592311e-05
Iter: 74 loss: 5.84926202e-05
Iter: 75 loss: 6.02509208e-05
Iter: 76 loss: 5.80762862e-05
Iter: 77 loss: 5.66430608e-05
Iter: 78 loss: 6.80358644e-05
Iter: 79 loss: 5.65429e-05
Iter: 80 loss: 5.60609915e-05
Iter: 81 loss: 5.52502497e-05
Iter: 82 loss: 5.52484962e-05
Iter: 83 loss: 5.42193811e-05
Iter: 84 loss: 5.77875835e-05
Iter: 85 loss: 5.39475041e-05
Iter: 86 loss: 5.29493118e-05
Iter: 87 loss: 5.52510064e-05
Iter: 88 loss: 5.25797514e-05
Iter: 89 loss: 5.16503496e-05
Iter: 90 loss: 5.48010976e-05
Iter: 91 loss: 5.13996165e-05
Iter: 92 loss: 5.05218e-05
Iter: 93 loss: 5.31469523e-05
Iter: 94 loss: 5.02555049e-05
Iter: 95 loss: 4.9462531e-05
Iter: 96 loss: 5.12436091e-05
Iter: 97 loss: 4.91610117e-05
Iter: 98 loss: 4.84540578e-05
Iter: 99 loss: 4.86484096e-05
Iter: 100 loss: 4.79420341e-05
Iter: 101 loss: 4.69750667e-05
Iter: 102 loss: 5.40269975e-05
Iter: 103 loss: 4.68925136e-05
Iter: 104 loss: 4.62488097e-05
Iter: 105 loss: 5.06008437e-05
Iter: 106 loss: 4.6185105e-05
Iter: 107 loss: 4.55970439e-05
Iter: 108 loss: 4.55564696e-05
Iter: 109 loss: 4.51130254e-05
Iter: 110 loss: 4.54133042e-05
Iter: 111 loss: 4.4852859e-05
Iter: 112 loss: 4.46585909e-05
Iter: 113 loss: 4.41733864e-05
Iter: 114 loss: 4.88078949e-05
Iter: 115 loss: 4.41062803e-05
Iter: 116 loss: 4.35193724e-05
Iter: 117 loss: 4.42119563e-05
Iter: 118 loss: 4.32076959e-05
Iter: 119 loss: 4.27789055e-05
Iter: 120 loss: 4.60854062e-05
Iter: 121 loss: 4.27467239e-05
Iter: 122 loss: 4.23137171e-05
Iter: 123 loss: 4.28139101e-05
Iter: 124 loss: 4.2083273e-05
Iter: 125 loss: 4.16375296e-05
Iter: 126 loss: 4.26906954e-05
Iter: 127 loss: 4.14749e-05
Iter: 128 loss: 4.10229513e-05
Iter: 129 loss: 4.2674721e-05
Iter: 130 loss: 4.09101413e-05
Iter: 131 loss: 4.04404309e-05
Iter: 132 loss: 4.09870227e-05
Iter: 133 loss: 4.01904472e-05
Iter: 134 loss: 3.97509575e-05
Iter: 135 loss: 4.00456338e-05
Iter: 136 loss: 3.94745766e-05
Iter: 137 loss: 3.89999768e-05
Iter: 138 loss: 4.28517305e-05
Iter: 139 loss: 3.89697889e-05
Iter: 140 loss: 3.85729109e-05
Iter: 141 loss: 4.05163373e-05
Iter: 142 loss: 3.85050153e-05
Iter: 143 loss: 3.81997233e-05
Iter: 144 loss: 3.91909125e-05
Iter: 145 loss: 3.81139071e-05
Iter: 146 loss: 3.77536853e-05
Iter: 147 loss: 4.12125883e-05
Iter: 148 loss: 3.77403558e-05
Iter: 149 loss: 3.76148237e-05
Iter: 150 loss: 3.7362006e-05
Iter: 151 loss: 4.20421202e-05
Iter: 152 loss: 3.73579678e-05
Iter: 153 loss: 3.70363232e-05
Iter: 154 loss: 3.73382318e-05
Iter: 155 loss: 3.68517867e-05
Iter: 156 loss: 3.64849366e-05
Iter: 157 loss: 3.9326e-05
Iter: 158 loss: 3.64578555e-05
Iter: 159 loss: 3.61604834e-05
Iter: 160 loss: 3.63992076e-05
Iter: 161 loss: 3.59815822e-05
Iter: 162 loss: 3.56069213e-05
Iter: 163 loss: 3.64221705e-05
Iter: 164 loss: 3.54618242e-05
Iter: 165 loss: 3.51715607e-05
Iter: 166 loss: 3.73994626e-05
Iter: 167 loss: 3.51491726e-05
Iter: 168 loss: 3.49033653e-05
Iter: 169 loss: 3.481764e-05
Iter: 170 loss: 3.46786073e-05
Iter: 171 loss: 3.43093634e-05
Iter: 172 loss: 3.55346565e-05
Iter: 173 loss: 3.42087151e-05
Iter: 174 loss: 3.39139442e-05
Iter: 175 loss: 3.45169356e-05
Iter: 176 loss: 3.37953315e-05
Iter: 177 loss: 3.34565921e-05
Iter: 178 loss: 3.56800156e-05
Iter: 179 loss: 3.34210563e-05
Iter: 180 loss: 3.32774507e-05
Iter: 181 loss: 3.32682175e-05
Iter: 182 loss: 3.31189694e-05
Iter: 183 loss: 3.31055417e-05
Iter: 184 loss: 3.29953109e-05
Iter: 185 loss: 3.28093593e-05
Iter: 186 loss: 3.25377077e-05
Iter: 187 loss: 3.25309302e-05
Iter: 188 loss: 3.22524575e-05
Iter: 189 loss: 3.29775212e-05
Iter: 190 loss: 3.21579791e-05
Iter: 191 loss: 3.18905331e-05
Iter: 192 loss: 3.4703131e-05
Iter: 193 loss: 3.18832535e-05
Iter: 194 loss: 3.1678308e-05
Iter: 195 loss: 3.18190287e-05
Iter: 196 loss: 3.15497236e-05
Iter: 197 loss: 3.1337353e-05
Iter: 198 loss: 3.18418606e-05
Iter: 199 loss: 3.12596312e-05
Iter: 200 loss: 3.10508112e-05
Iter: 201 loss: 3.2262742e-05
Iter: 202 loss: 3.10226897e-05
Iter: 203 loss: 3.0838215e-05
Iter: 204 loss: 3.0656789e-05
Iter: 205 loss: 3.06178e-05
Iter: 206 loss: 3.03330889e-05
Iter: 207 loss: 3.30437651e-05
Iter: 208 loss: 3.03220841e-05
Iter: 209 loss: 3.01468026e-05
Iter: 210 loss: 3.04238838e-05
Iter: 211 loss: 3.00650681e-05
Iter: 212 loss: 2.99064759e-05
Iter: 213 loss: 3.12087068e-05
Iter: 214 loss: 2.98965133e-05
Iter: 215 loss: 2.97320221e-05
Iter: 216 loss: 3.11337135e-05
Iter: 217 loss: 2.97226306e-05
Iter: 218 loss: 2.96371509e-05
Iter: 219 loss: 2.96231156e-05
Iter: 220 loss: 2.95645914e-05
Iter: 221 loss: 2.9449453e-05
Iter: 222 loss: 2.92179466e-05
Iter: 223 loss: 3.35500154e-05
Iter: 224 loss: 2.9215109e-05
Iter: 225 loss: 2.89602067e-05
Iter: 226 loss: 3.10927098e-05
Iter: 227 loss: 2.89453619e-05
Iter: 228 loss: 2.8787892e-05
Iter: 229 loss: 2.98121849e-05
Iter: 230 loss: 2.87714283e-05
Iter: 231 loss: 2.86057475e-05
Iter: 232 loss: 2.86436298e-05
Iter: 233 loss: 2.84837079e-05
Iter: 234 loss: 2.83043519e-05
Iter: 235 loss: 2.87914918e-05
Iter: 236 loss: 2.82451874e-05
Iter: 237 loss: 2.80609092e-05
Iter: 238 loss: 2.86145059e-05
Iter: 239 loss: 2.80050845e-05
Iter: 240 loss: 2.7851318e-05
Iter: 241 loss: 2.86993018e-05
Iter: 242 loss: 2.78288353e-05
Iter: 243 loss: 2.76902683e-05
Iter: 244 loss: 2.76858646e-05
Iter: 245 loss: 2.75783568e-05
Iter: 246 loss: 2.74186423e-05
Iter: 247 loss: 2.79771884e-05
Iter: 248 loss: 2.73768346e-05
Iter: 249 loss: 2.73609403e-05
Iter: 250 loss: 2.73093865e-05
Iter: 251 loss: 2.72387369e-05
Iter: 252 loss: 2.70858091e-05
Iter: 253 loss: 2.94245619e-05
Iter: 254 loss: 2.70796863e-05
Iter: 255 loss: 2.69391294e-05
Iter: 256 loss: 2.76307601e-05
Iter: 257 loss: 2.69146312e-05
Iter: 258 loss: 2.68033364e-05
Iter: 259 loss: 2.67987816e-05
Iter: 260 loss: 2.67129126e-05
Iter: 261 loss: 2.65677827e-05
Iter: 262 loss: 2.66812967e-05
Iter: 263 loss: 2.64805785e-05
Iter: 264 loss: 2.63620241e-05
Iter: 265 loss: 2.63602269e-05
Iter: 266 loss: 2.62599551e-05
Iter: 267 loss: 2.6163103e-05
Iter: 268 loss: 2.61405967e-05
Iter: 269 loss: 2.60232373e-05
Iter: 270 loss: 2.67559662e-05
Iter: 271 loss: 2.60093711e-05
Iter: 272 loss: 2.5894944e-05
Iter: 273 loss: 2.59957451e-05
Iter: 274 loss: 2.58281616e-05
Iter: 275 loss: 2.57051179e-05
Iter: 276 loss: 2.65801846e-05
Iter: 277 loss: 2.5694264e-05
Iter: 278 loss: 2.55982577e-05
Iter: 279 loss: 2.55572741e-05
Iter: 280 loss: 2.55075429e-05
Iter: 281 loss: 2.54088955e-05
Iter: 282 loss: 2.54085426e-05
Iter: 283 loss: 2.53301369e-05
Iter: 284 loss: 2.61515634e-05
Iter: 285 loss: 2.53274884e-05
Iter: 286 loss: 2.52880982e-05
Iter: 287 loss: 2.51858837e-05
Iter: 288 loss: 2.60257548e-05
Iter: 289 loss: 2.51676884e-05
Iter: 290 loss: 2.50376179e-05
Iter: 291 loss: 2.5885256e-05
Iter: 292 loss: 2.502387e-05
Iter: 293 loss: 2.49353743e-05
Iter: 294 loss: 2.50345947e-05
Iter: 295 loss: 2.48878459e-05
Iter: 296 loss: 2.47828211e-05
Iter: 297 loss: 2.50100638e-05
Iter: 298 loss: 2.47424196e-05
Iter: 299 loss: 2.46356358e-05
Iter: 300 loss: 2.4967565e-05
Iter: 301 loss: 2.46043128e-05
Iter: 302 loss: 2.45055126e-05
Iter: 303 loss: 2.52785794e-05
Iter: 304 loss: 2.44985076e-05
Iter: 305 loss: 2.44224575e-05
Iter: 306 loss: 2.43843351e-05
Iter: 307 loss: 2.43483337e-05
Iter: 308 loss: 2.42409023e-05
Iter: 309 loss: 2.43747709e-05
Iter: 310 loss: 2.41850521e-05
Iter: 311 loss: 2.40464615e-05
Iter: 312 loss: 2.45016017e-05
Iter: 313 loss: 2.4008501e-05
Iter: 314 loss: 2.3908935e-05
Iter: 315 loss: 2.51400343e-05
Iter: 316 loss: 2.39080273e-05
Iter: 317 loss: 2.38485154e-05
Iter: 318 loss: 2.43907689e-05
Iter: 319 loss: 2.38458215e-05
Iter: 320 loss: 2.37722779e-05
Iter: 321 loss: 2.37542808e-05
Iter: 322 loss: 2.37078784e-05
Iter: 323 loss: 2.36557062e-05
Iter: 324 loss: 2.35761672e-05
Iter: 325 loss: 2.35750194e-05
Iter: 326 loss: 2.3462444e-05
Iter: 327 loss: 2.42024835e-05
Iter: 328 loss: 2.34507315e-05
Iter: 329 loss: 2.3370596e-05
Iter: 330 loss: 2.36092947e-05
Iter: 331 loss: 2.33464089e-05
Iter: 332 loss: 2.32627135e-05
Iter: 333 loss: 2.32381935e-05
Iter: 334 loss: 2.31873801e-05
Iter: 335 loss: 2.31032445e-05
Iter: 336 loss: 2.42132373e-05
Iter: 337 loss: 2.31032209e-05
Iter: 338 loss: 2.304191e-05
Iter: 339 loss: 2.32234925e-05
Iter: 340 loss: 2.30230398e-05
Iter: 341 loss: 2.2948785e-05
Iter: 342 loss: 2.29244433e-05
Iter: 343 loss: 2.28819299e-05
Iter: 344 loss: 2.27957062e-05
Iter: 345 loss: 2.29598809e-05
Iter: 346 loss: 2.27597357e-05
Iter: 347 loss: 2.26507454e-05
Iter: 348 loss: 2.29881025e-05
Iter: 349 loss: 2.26189295e-05
Iter: 350 loss: 2.25400472e-05
Iter: 351 loss: 2.34523177e-05
Iter: 352 loss: 2.25387848e-05
Iter: 353 loss: 2.24804135e-05
Iter: 354 loss: 2.33299652e-05
Iter: 355 loss: 2.24802461e-05
Iter: 356 loss: 2.24486321e-05
Iter: 357 loss: 2.23705083e-05
Iter: 358 loss: 2.31722406e-05
Iter: 359 loss: 2.23612951e-05
Iter: 360 loss: 2.22695817e-05
Iter: 361 loss: 2.26769025e-05
Iter: 362 loss: 2.22514973e-05
Iter: 363 loss: 2.21825139e-05
Iter: 364 loss: 2.21943264e-05
Iter: 365 loss: 2.21314422e-05
Iter: 366 loss: 2.20341353e-05
Iter: 367 loss: 2.24898376e-05
Iter: 368 loss: 2.20165712e-05
Iter: 369 loss: 2.19419453e-05
Iter: 370 loss: 2.25317126e-05
Iter: 371 loss: 2.19363828e-05
Iter: 372 loss: 2.18787718e-05
Iter: 373 loss: 2.18349051e-05
Iter: 374 loss: 2.18162259e-05
Iter: 375 loss: 2.17419274e-05
Iter: 376 loss: 2.2283446e-05
Iter: 377 loss: 2.17357228e-05
Iter: 378 loss: 2.16651752e-05
Iter: 379 loss: 2.18970417e-05
Iter: 380 loss: 2.16456701e-05
Iter: 381 loss: 2.15810287e-05
Iter: 382 loss: 2.16825847e-05
Iter: 383 loss: 2.15506807e-05
Iter: 384 loss: 2.14900701e-05
Iter: 385 loss: 2.14936517e-05
Iter: 386 loss: 2.1442951e-05
Iter: 387 loss: 2.13921467e-05
Iter: 388 loss: 2.13889871e-05
Iter: 389 loss: 2.13352505e-05
Iter: 390 loss: 2.15707478e-05
Iter: 391 loss: 2.13240965e-05
Iter: 392 loss: 2.12979485e-05
Iter: 393 loss: 2.12246487e-05
Iter: 394 loss: 2.16148037e-05
Iter: 395 loss: 2.1201522e-05
Iter: 396 loss: 2.1112699e-05
Iter: 397 loss: 2.20095244e-05
Iter: 398 loss: 2.11104707e-05
Iter: 399 loss: 2.10473772e-05
Iter: 400 loss: 2.11585902e-05
Iter: 401 loss: 2.10196631e-05
Iter: 402 loss: 2.09486279e-05
Iter: 403 loss: 2.12688719e-05
Iter: 404 loss: 2.09349109e-05
Iter: 405 loss: 2.08789079e-05
Iter: 406 loss: 2.10534636e-05
Iter: 407 loss: 2.08622241e-05
Iter: 408 loss: 2.08035708e-05
Iter: 409 loss: 2.08464753e-05
Iter: 410 loss: 2.07671346e-05
Iter: 411 loss: 2.06932673e-05
Iter: 412 loss: 2.12406794e-05
Iter: 413 loss: 2.0687201e-05
Iter: 414 loss: 2.06447876e-05
Iter: 415 loss: 2.06419954e-05
Iter: 416 loss: 2.06090263e-05
Iter: 417 loss: 2.0545709e-05
Iter: 418 loss: 2.09695136e-05
Iter: 419 loss: 2.05392935e-05
Iter: 420 loss: 2.04843764e-05
Iter: 421 loss: 2.05652177e-05
Iter: 422 loss: 2.0458283e-05
Iter: 423 loss: 2.04314747e-05
Iter: 424 loss: 2.04291955e-05
Iter: 425 loss: 2.03949039e-05
Iter: 426 loss: 2.03393683e-05
Iter: 427 loss: 2.03391028e-05
Iter: 428 loss: 2.02897718e-05
Iter: 429 loss: 2.0278083e-05
Iter: 430 loss: 2.02467199e-05
Iter: 431 loss: 2.01831081e-05
Iter: 432 loss: 2.02594056e-05
Iter: 433 loss: 2.01496e-05
Iter: 434 loss: 2.007699e-05
Iter: 435 loss: 2.04575954e-05
Iter: 436 loss: 2.00656723e-05
Iter: 437 loss: 2.00042086e-05
Iter: 438 loss: 2.01357034e-05
Iter: 439 loss: 1.99803035e-05
Iter: 440 loss: 1.99197548e-05
Iter: 441 loss: 2.05011438e-05
Iter: 442 loss: 1.9918e-05
Iter: 443 loss: 1.98783382e-05
Iter: 444 loss: 1.9888701e-05
Iter: 445 loss: 1.98501766e-05
Iter: 446 loss: 1.97872068e-05
Iter: 447 loss: 1.99061451e-05
Iter: 448 loss: 1.97606041e-05
Iter: 449 loss: 1.97034096e-05
Iter: 450 loss: 2.00498471e-05
Iter: 451 loss: 1.96962137e-05
Iter: 452 loss: 1.96449055e-05
Iter: 453 loss: 1.97156023e-05
Iter: 454 loss: 1.9619878e-05
Iter: 455 loss: 1.95699358e-05
Iter: 456 loss: 1.99115566e-05
Iter: 457 loss: 1.95650427e-05
Iter: 458 loss: 1.95442e-05
Iter: 459 loss: 1.95417088e-05
Iter: 460 loss: 1.9522704e-05
Iter: 461 loss: 1.94821e-05
Iter: 462 loss: 2.0122905e-05
Iter: 463 loss: 1.94806507e-05
Iter: 464 loss: 1.94314307e-05
Iter: 465 loss: 1.94843869e-05
Iter: 466 loss: 1.9404004e-05
Iter: 467 loss: 1.93572923e-05
Iter: 468 loss: 1.93109281e-05
Iter: 469 loss: 1.93010874e-05
Iter: 470 loss: 1.92219341e-05
Iter: 471 loss: 1.98764719e-05
Iter: 472 loss: 1.92172483e-05
Iter: 473 loss: 1.91633226e-05
Iter: 474 loss: 1.92302668e-05
Iter: 475 loss: 1.91357867e-05
Iter: 476 loss: 1.90670435e-05
Iter: 477 loss: 1.92728985e-05
Iter: 478 loss: 1.90460814e-05
Iter: 479 loss: 1.89947459e-05
Iter: 480 loss: 1.93520937e-05
Iter: 481 loss: 1.89898856e-05
Iter: 482 loss: 1.89419443e-05
Iter: 483 loss: 1.90770734e-05
Iter: 484 loss: 1.89268903e-05
Iter: 485 loss: 1.88765953e-05
Iter: 486 loss: 1.89894636e-05
Iter: 487 loss: 1.88577087e-05
Iter: 488 loss: 1.88127124e-05
Iter: 489 loss: 1.88949543e-05
Iter: 490 loss: 1.87927599e-05
Iter: 491 loss: 1.87458227e-05
Iter: 492 loss: 1.92811804e-05
Iter: 493 loss: 1.87448295e-05
Iter: 494 loss: 1.8708608e-05
Iter: 495 loss: 1.90319442e-05
Iter: 496 loss: 1.87069527e-05
Iter: 497 loss: 1.86890429e-05
Iter: 498 loss: 1.86645848e-05
Iter: 499 loss: 1.86636262e-05
Iter: 500 loss: 1.86263478e-05
Iter: 501 loss: 1.8578663e-05
Iter: 502 loss: 1.85752615e-05
Iter: 503 loss: 1.85140707e-05
Iter: 504 loss: 1.88653685e-05
Iter: 505 loss: 1.85061144e-05
Iter: 506 loss: 1.84549281e-05
Iter: 507 loss: 1.85443714e-05
Iter: 508 loss: 1.84325036e-05
Iter: 509 loss: 1.83893062e-05
Iter: 510 loss: 1.89394741e-05
Iter: 511 loss: 1.83890188e-05
Iter: 512 loss: 1.83574157e-05
Iter: 513 loss: 1.83094344e-05
Iter: 514 loss: 1.8308685e-05
Iter: 515 loss: 1.82553758e-05
Iter: 516 loss: 1.8689354e-05
Iter: 517 loss: 1.82521653e-05
Iter: 518 loss: 1.82062176e-05
Iter: 519 loss: 1.8223709e-05
Iter: 520 loss: 1.81744908e-05
Iter: 521 loss: 1.81227824e-05
Iter: 522 loss: 1.84427845e-05
Iter: 523 loss: 1.81166433e-05
Iter: 524 loss: 1.80726529e-05
Iter: 525 loss: 1.82911863e-05
Iter: 526 loss: 1.80651768e-05
Iter: 527 loss: 1.80326933e-05
Iter: 528 loss: 1.80323113e-05
Iter: 529 loss: 1.80069565e-05
Iter: 530 loss: 1.80306251e-05
Iter: 531 loss: 1.79920298e-05
Iter: 532 loss: 1.79693197e-05
Iter: 533 loss: 1.79263825e-05
Iter: 534 loss: 1.88759659e-05
Iter: 535 loss: 1.79264607e-05
Iter: 536 loss: 1.78742193e-05
Iter: 537 loss: 1.80865281e-05
Iter: 538 loss: 1.78625814e-05
Iter: 539 loss: 1.78252631e-05
Iter: 540 loss: 1.78951414e-05
Iter: 541 loss: 1.78093615e-05
Iter: 542 loss: 1.77632664e-05
Iter: 543 loss: 1.7875469e-05
Iter: 544 loss: 1.77469592e-05
Iter: 545 loss: 1.77026432e-05
Iter: 546 loss: 1.80762872e-05
Iter: 547 loss: 1.7700193e-05
Iter: 548 loss: 1.76718568e-05
Iter: 549 loss: 1.76513859e-05
Iter: 550 loss: 1.76416652e-05
Iter: 551 loss: 1.75925124e-05
Iter: 552 loss: 1.78469563e-05
Iter: 553 loss: 1.75847235e-05
Iter: 554 loss: 1.75527639e-05
Iter: 555 loss: 1.77825386e-05
Iter: 556 loss: 1.75498462e-05
Iter: 557 loss: 1.75219902e-05
Iter: 558 loss: 1.74674933e-05
Iter: 559 loss: 1.85530553e-05
Iter: 560 loss: 1.74668203e-05
Iter: 561 loss: 1.74164197e-05
Iter: 562 loss: 1.81141859e-05
Iter: 563 loss: 1.7416136e-05
Iter: 564 loss: 1.74027609e-05
Iter: 565 loss: 1.73974513e-05
Iter: 566 loss: 1.7377246e-05
Iter: 567 loss: 1.73362496e-05
Iter: 568 loss: 1.80731404e-05
Iter: 569 loss: 1.73354711e-05
Iter: 570 loss: 1.73041099e-05
Iter: 571 loss: 1.73405169e-05
Iter: 572 loss: 1.72874934e-05
Iter: 573 loss: 1.72515083e-05
Iter: 574 loss: 1.73637127e-05
Iter: 575 loss: 1.72411019e-05
Iter: 576 loss: 1.71998945e-05
Iter: 577 loss: 1.73650751e-05
Iter: 578 loss: 1.71905867e-05
Iter: 579 loss: 1.71615939e-05
Iter: 580 loss: 1.71795873e-05
Iter: 581 loss: 1.71430984e-05
Iter: 582 loss: 1.71009706e-05
Iter: 583 loss: 1.7177088e-05
Iter: 584 loss: 1.70827971e-05
Iter: 585 loss: 1.70508902e-05
Iter: 586 loss: 1.72073287e-05
Iter: 587 loss: 1.70452713e-05
Iter: 588 loss: 1.70099756e-05
Iter: 589 loss: 1.70901058e-05
Iter: 590 loss: 1.69965278e-05
Iter: 591 loss: 1.69648047e-05
Iter: 592 loss: 1.70640869e-05
Iter: 593 loss: 1.69556079e-05
Iter: 594 loss: 1.69259474e-05
Iter: 595 loss: 1.69444938e-05
Iter: 596 loss: 1.69071463e-05
Iter: 597 loss: 1.68664537e-05
Iter: 598 loss: 1.71513893e-05
Iter: 599 loss: 1.68628339e-05
Iter: 600 loss: 1.68468341e-05
Iter: 601 loss: 1.68465122e-05
Iter: 602 loss: 1.68262959e-05
Iter: 603 loss: 1.67899489e-05
Iter: 604 loss: 1.76812864e-05
Iter: 605 loss: 1.67897415e-05
Iter: 606 loss: 1.67630315e-05
Iter: 607 loss: 1.67852231e-05
Iter: 608 loss: 1.67473718e-05
Iter: 609 loss: 1.67126273e-05
Iter: 610 loss: 1.67193411e-05
Iter: 611 loss: 1.66870468e-05
Iter: 612 loss: 1.66491536e-05
Iter: 613 loss: 1.70258427e-05
Iter: 614 loss: 1.66478821e-05
Iter: 615 loss: 1.66182108e-05
Iter: 616 loss: 1.66957507e-05
Iter: 617 loss: 1.66083373e-05
Iter: 618 loss: 1.65705605e-05
Iter: 619 loss: 1.66229511e-05
Iter: 620 loss: 1.65518268e-05
Iter: 621 loss: 1.65210204e-05
Iter: 622 loss: 1.66124628e-05
Iter: 623 loss: 1.65116708e-05
Iter: 624 loss: 1.64794492e-05
Iter: 625 loss: 1.65056845e-05
Iter: 626 loss: 1.64607754e-05
Iter: 627 loss: 1.64275534e-05
Iter: 628 loss: 1.67047e-05
Iter: 629 loss: 1.64254197e-05
Iter: 630 loss: 1.63997902e-05
Iter: 631 loss: 1.64595695e-05
Iter: 632 loss: 1.63903605e-05
Iter: 633 loss: 1.63601853e-05
Iter: 634 loss: 1.63740806e-05
Iter: 635 loss: 1.63396144e-05
Iter: 636 loss: 1.63206187e-05
Iter: 637 loss: 1.63190743e-05
Iter: 638 loss: 1.62966535e-05
Iter: 639 loss: 1.63307e-05
Iter: 640 loss: 1.62858e-05
Iter: 641 loss: 1.62646975e-05
Iter: 642 loss: 1.62433789e-05
Iter: 643 loss: 1.62384822e-05
Iter: 644 loss: 1.62081888e-05
Iter: 645 loss: 1.6266491e-05
Iter: 646 loss: 1.61951648e-05
Iter: 647 loss: 1.61639109e-05
Iter: 648 loss: 1.61331227e-05
Iter: 649 loss: 1.61266762e-05
Iter: 650 loss: 1.60922154e-05
Iter: 651 loss: 1.60921518e-05
Iter: 652 loss: 1.60621603e-05
Iter: 653 loss: 1.60640502e-05
Iter: 654 loss: 1.60386517e-05
Iter: 655 loss: 1.60111667e-05
Iter: 656 loss: 1.60109503e-05
Iter: 657 loss: 1.59890806e-05
Iter: 658 loss: 1.60009986e-05
Iter: 659 loss: 1.5974525e-05
Iter: 660 loss: 1.59429201e-05
Iter: 661 loss: 1.5972495e-05
Iter: 662 loss: 1.59246119e-05
Iter: 663 loss: 1.58965013e-05
Iter: 664 loss: 1.60008021e-05
Iter: 665 loss: 1.58898474e-05
Iter: 666 loss: 1.58606381e-05
Iter: 667 loss: 1.59160354e-05
Iter: 668 loss: 1.58485473e-05
Iter: 669 loss: 1.58393741e-05
Iter: 670 loss: 1.58331677e-05
Iter: 671 loss: 1.58187177e-05
Iter: 672 loss: 1.58129587e-05
Iter: 673 loss: 1.58055409e-05
Iter: 674 loss: 1.57876857e-05
Iter: 675 loss: 1.57723334e-05
Iter: 676 loss: 1.57674112e-05
Iter: 677 loss: 1.57427785e-05
Iter: 678 loss: 1.58125258e-05
Iter: 679 loss: 1.57350732e-05
Iter: 680 loss: 1.57093564e-05
Iter: 681 loss: 1.57765026e-05
Iter: 682 loss: 1.57005779e-05
Iter: 683 loss: 1.56681563e-05
Iter: 684 loss: 1.57126178e-05
Iter: 685 loss: 1.56520873e-05
Iter: 686 loss: 1.56229289e-05
Iter: 687 loss: 1.56466594e-05
Iter: 688 loss: 1.56056085e-05
Iter: 689 loss: 1.55705329e-05
Iter: 690 loss: 1.57275299e-05
Iter: 691 loss: 1.556387e-05
Iter: 692 loss: 1.55320631e-05
Iter: 693 loss: 1.55820162e-05
Iter: 694 loss: 1.55172911e-05
Iter: 695 loss: 1.5478643e-05
Iter: 696 loss: 1.56316364e-05
Iter: 697 loss: 1.54696972e-05
Iter: 698 loss: 1.54441623e-05
Iter: 699 loss: 1.56065234e-05
Iter: 700 loss: 1.54413465e-05
Iter: 701 loss: 1.54149857e-05
Iter: 702 loss: 1.54688078e-05
Iter: 703 loss: 1.54044137e-05
Iter: 704 loss: 1.53847177e-05
Iter: 705 loss: 1.53845976e-05
Iter: 706 loss: 1.53652691e-05
Iter: 707 loss: 1.53669e-05
Iter: 708 loss: 1.53502424e-05
Iter: 709 loss: 1.53358214e-05
Iter: 710 loss: 1.53139608e-05
Iter: 711 loss: 1.53135879e-05
Iter: 712 loss: 1.52799075e-05
Iter: 713 loss: 1.53625697e-05
Iter: 714 loss: 1.52682278e-05
Iter: 715 loss: 1.52407174e-05
Iter: 716 loss: 1.54473819e-05
Iter: 717 loss: 1.52385091e-05
Iter: 718 loss: 1.52143539e-05
Iter: 719 loss: 1.51841878e-05
Iter: 720 loss: 1.51818349e-05
Iter: 721 loss: 1.51557424e-05
Iter: 722 loss: 1.51552467e-05
Iter: 723 loss: 1.51326285e-05
Iter: 724 loss: 1.51270015e-05
Iter: 725 loss: 1.51128024e-05
Iter: 726 loss: 1.50811638e-05
Iter: 727 loss: 1.51867125e-05
Iter: 728 loss: 1.50724563e-05
Iter: 729 loss: 1.50467695e-05
Iter: 730 loss: 1.50552878e-05
Iter: 731 loss: 1.5028736e-05
Iter: 732 loss: 1.49955258e-05
Iter: 733 loss: 1.52448592e-05
Iter: 734 loss: 1.49929565e-05
Iter: 735 loss: 1.49681218e-05
Iter: 736 loss: 1.50124097e-05
Iter: 737 loss: 1.49572734e-05
Iter: 738 loss: 1.49508323e-05
Iter: 739 loss: 1.49431507e-05
Iter: 740 loss: 1.49320658e-05
Iter: 741 loss: 1.49204307e-05
Iter: 742 loss: 1.49181442e-05
Iter: 743 loss: 1.49028847e-05
Iter: 744 loss: 1.49216557e-05
Iter: 745 loss: 1.48949712e-05
Iter: 746 loss: 1.48750642e-05
Iter: 747 loss: 1.48660683e-05
Iter: 748 loss: 1.48562704e-05
Iter: 749 loss: 1.48295139e-05
Iter: 750 loss: 1.49299485e-05
Iter: 751 loss: 1.48232675e-05
Iter: 752 loss: 1.47953833e-05
Iter: 753 loss: 1.48630288e-05
Iter: 754 loss: 1.47854262e-05
Iter: 755 loss: 1.47566916e-05
Iter: 756 loss: 1.48440886e-05
Iter: 757 loss: 1.47481351e-05
Iter: 758 loss: 1.47257833e-05
Iter: 759 loss: 1.47868259e-05
Iter: 760 loss: 1.4718632e-05
Iter: 761 loss: 1.46944712e-05
Iter: 762 loss: 1.47606706e-05
Iter: 763 loss: 1.46865577e-05
Iter: 764 loss: 1.46602124e-05
Iter: 765 loss: 1.47455748e-05
Iter: 766 loss: 1.46527054e-05
Iter: 767 loss: 1.4630059e-05
Iter: 768 loss: 1.4617015e-05
Iter: 769 loss: 1.46074635e-05
Iter: 770 loss: 1.4577714e-05
Iter: 771 loss: 1.47935289e-05
Iter: 772 loss: 1.45748554e-05
Iter: 773 loss: 1.45465383e-05
Iter: 774 loss: 1.45845088e-05
Iter: 775 loss: 1.4532312e-05
Iter: 776 loss: 1.45327303e-05
Iter: 777 loss: 1.45180711e-05
Iter: 778 loss: 1.45081822e-05
Iter: 779 loss: 1.44945188e-05
Iter: 780 loss: 1.44938322e-05
Iter: 781 loss: 1.44784071e-05
Iter: 782 loss: 1.44883234e-05
Iter: 783 loss: 1.44687283e-05
Iter: 784 loss: 1.44420101e-05
Iter: 785 loss: 1.44274709e-05
Iter: 786 loss: 1.44159021e-05
Iter: 787 loss: 1.43887946e-05
Iter: 788 loss: 1.45381673e-05
Iter: 789 loss: 1.43849393e-05
Iter: 790 loss: 1.43565048e-05
Iter: 791 loss: 1.44070846e-05
Iter: 792 loss: 1.43443485e-05
Iter: 793 loss: 1.43200214e-05
Iter: 794 loss: 1.44863679e-05
Iter: 795 loss: 1.43173575e-05
Iter: 796 loss: 1.42986046e-05
Iter: 797 loss: 1.42902882e-05
Iter: 798 loss: 1.42806757e-05
Iter: 799 loss: 1.42506424e-05
Iter: 800 loss: 1.44366177e-05
Iter: 801 loss: 1.42474955e-05
Iter: 802 loss: 1.4226569e-05
Iter: 803 loss: 1.43475017e-05
Iter: 804 loss: 1.42237841e-05
Iter: 805 loss: 1.42063463e-05
Iter: 806 loss: 1.41820983e-05
Iter: 807 loss: 1.41810642e-05
Iter: 808 loss: 1.41536657e-05
Iter: 809 loss: 1.43847119e-05
Iter: 810 loss: 1.41523442e-05
Iter: 811 loss: 1.4129635e-05
Iter: 812 loss: 1.41692954e-05
Iter: 813 loss: 1.41195742e-05
Iter: 814 loss: 1.41188393e-05
Iter: 815 loss: 1.41087876e-05
Iter: 816 loss: 1.40999164e-05
Iter: 817 loss: 1.40887223e-05
Iter: 818 loss: 1.408769e-05
Iter: 819 loss: 1.40740567e-05
Iter: 820 loss: 1.40835346e-05
Iter: 821 loss: 1.40656757e-05
Iter: 822 loss: 1.40456114e-05
Iter: 823 loss: 1.40613647e-05
Iter: 824 loss: 1.40338543e-05
Iter: 825 loss: 1.40110897e-05
Iter: 826 loss: 1.40314105e-05
Iter: 827 loss: 1.39978019e-05
Iter: 828 loss: 1.39713338e-05
Iter: 829 loss: 1.41038472e-05
Iter: 830 loss: 1.39669337e-05
Iter: 831 loss: 1.39443382e-05
Iter: 832 loss: 1.40522416e-05
Iter: 833 loss: 1.3940401e-05
Iter: 834 loss: 1.39227477e-05
Iter: 835 loss: 1.39180338e-05
Iter: 836 loss: 1.39070644e-05
Iter: 837 loss: 1.38826927e-05
Iter: 838 loss: 1.40602933e-05
Iter: 839 loss: 1.38805563e-05
Iter: 840 loss: 1.38607356e-05
Iter: 841 loss: 1.39036993e-05
Iter: 842 loss: 1.38529194e-05
Iter: 843 loss: 1.38283194e-05
Iter: 844 loss: 1.38539745e-05
Iter: 845 loss: 1.38146261e-05
Iter: 846 loss: 1.37945517e-05
Iter: 847 loss: 1.38557698e-05
Iter: 848 loss: 1.37886609e-05
Iter: 849 loss: 1.3766723e-05
Iter: 850 loss: 1.38321748e-05
Iter: 851 loss: 1.376008e-05
Iter: 852 loss: 1.37469633e-05
Iter: 853 loss: 1.37444895e-05
Iter: 854 loss: 1.37383404e-05
Iter: 855 loss: 1.37245152e-05
Iter: 856 loss: 1.39058557e-05
Iter: 857 loss: 1.37236711e-05
Iter: 858 loss: 1.37046191e-05
Iter: 859 loss: 1.37358893e-05
Iter: 860 loss: 1.36957387e-05
Iter: 861 loss: 1.36789185e-05
Iter: 862 loss: 1.37837178e-05
Iter: 863 loss: 1.36770177e-05
Iter: 864 loss: 1.36628205e-05
Iter: 865 loss: 1.3637984e-05
Iter: 866 loss: 1.42436511e-05
Iter: 867 loss: 1.3637994e-05
Iter: 868 loss: 1.36149674e-05
Iter: 869 loss: 1.39798594e-05
Iter: 870 loss: 1.3614801e-05
Iter: 871 loss: 1.35970295e-05
Iter: 872 loss: 1.36214976e-05
Iter: 873 loss: 1.35881564e-05
Iter: 874 loss: 1.35666733e-05
Iter: 875 loss: 1.35911978e-05
Iter: 876 loss: 1.35550363e-05
Iter: 877 loss: 1.35360151e-05
Iter: 878 loss: 1.36734598e-05
Iter: 879 loss: 1.35343134e-05
Iter: 880 loss: 1.35165183e-05
Iter: 881 loss: 1.3539222e-05
Iter: 882 loss: 1.35075625e-05
Iter: 883 loss: 1.34851916e-05
Iter: 884 loss: 1.35377986e-05
Iter: 885 loss: 1.34773427e-05
Iter: 886 loss: 1.34603706e-05
Iter: 887 loss: 1.35398413e-05
Iter: 888 loss: 1.34573347e-05
Iter: 889 loss: 1.34452039e-05
Iter: 890 loss: 1.34448755e-05
Iter: 891 loss: 1.34381089e-05
Iter: 892 loss: 1.34199818e-05
Iter: 893 loss: 1.3573419e-05
Iter: 894 loss: 1.34173679e-05
Iter: 895 loss: 1.33998856e-05
Iter: 896 loss: 1.34966094e-05
Iter: 897 loss: 1.33975036e-05
Iter: 898 loss: 1.33791873e-05
Iter: 899 loss: 1.34020611e-05
Iter: 900 loss: 1.33697304e-05
Iter: 901 loss: 1.33511348e-05
Iter: 902 loss: 1.34092661e-05
Iter: 903 loss: 1.33453104e-05
Iter: 904 loss: 1.33263729e-05
Iter: 905 loss: 1.33245949e-05
Iter: 906 loss: 1.3310455e-05
Iter: 907 loss: 1.3288075e-05
Iter: 908 loss: 1.3498774e-05
Iter: 909 loss: 1.32871282e-05
Iter: 910 loss: 1.3271303e-05
Iter: 911 loss: 1.3308726e-05
Iter: 912 loss: 1.32653158e-05
Iter: 913 loss: 1.32469168e-05
Iter: 914 loss: 1.32640253e-05
Iter: 915 loss: 1.32360956e-05
Iter: 916 loss: 1.32177111e-05
Iter: 917 loss: 1.3301913e-05
Iter: 918 loss: 1.32142932e-05
Iter: 919 loss: 1.31945544e-05
Iter: 920 loss: 1.32419118e-05
Iter: 921 loss: 1.31873021e-05
Iter: 922 loss: 1.31710449e-05
Iter: 923 loss: 1.32453933e-05
Iter: 924 loss: 1.31680627e-05
Iter: 925 loss: 1.31626866e-05
Iter: 926 loss: 1.31601946e-05
Iter: 927 loss: 1.31548495e-05
Iter: 928 loss: 1.31391844e-05
Iter: 929 loss: 1.31913566e-05
Iter: 930 loss: 1.31317838e-05
Iter: 931 loss: 1.31137258e-05
Iter: 932 loss: 1.32417845e-05
Iter: 933 loss: 1.31121496e-05
Iter: 934 loss: 1.30964936e-05
Iter: 935 loss: 1.31386114e-05
Iter: 936 loss: 1.30910903e-05
Iter: 937 loss: 1.30726457e-05
Iter: 938 loss: 1.31039487e-05
Iter: 939 loss: 1.3064323e-05
Iter: 940 loss: 1.30491853e-05
Iter: 941 loss: 1.30659291e-05
Iter: 942 loss: 1.30409117e-05
Iter: 943 loss: 1.30190747e-05
Iter: 944 loss: 1.3067387e-05
Iter: 945 loss: 1.30108965e-05
Iter: 946 loss: 1.2993034e-05
Iter: 947 loss: 1.31074066e-05
Iter: 948 loss: 1.29911396e-05
Iter: 949 loss: 1.29753607e-05
Iter: 950 loss: 1.29878317e-05
Iter: 951 loss: 1.29658201e-05
Iter: 952 loss: 1.29464615e-05
Iter: 953 loss: 1.3008892e-05
Iter: 954 loss: 1.29408572e-05
Iter: 955 loss: 1.29253176e-05
Iter: 956 loss: 1.29979144e-05
Iter: 957 loss: 1.29226082e-05
Iter: 958 loss: 1.29066375e-05
Iter: 959 loss: 1.2944638e-05
Iter: 960 loss: 1.29007385e-05
Iter: 961 loss: 1.28933607e-05
Iter: 962 loss: 1.28910851e-05
Iter: 963 loss: 1.28855736e-05
Iter: 964 loss: 1.28704532e-05
Iter: 965 loss: 1.29563032e-05
Iter: 966 loss: 1.28659376e-05
Iter: 967 loss: 1.28469164e-05
Iter: 968 loss: 1.28548727e-05
Iter: 969 loss: 1.28337342e-05
Iter: 970 loss: 1.28146785e-05
Iter: 971 loss: 1.30871504e-05
Iter: 972 loss: 1.28144766e-05
Iter: 973 loss: 1.27991061e-05
Iter: 974 loss: 1.28195097e-05
Iter: 975 loss: 1.27912954e-05
Iter: 976 loss: 1.27724861e-05
Iter: 977 loss: 1.27982e-05
Iter: 978 loss: 1.27630719e-05
Iter: 979 loss: 1.2748038e-05
Iter: 980 loss: 1.27935236e-05
Iter: 981 loss: 1.27434541e-05
Iter: 982 loss: 1.27254089e-05
Iter: 983 loss: 1.27538251e-05
Iter: 984 loss: 1.27167905e-05
Iter: 985 loss: 1.26989225e-05
Iter: 986 loss: 1.28007105e-05
Iter: 987 loss: 1.26964842e-05
Iter: 988 loss: 1.26788982e-05
Iter: 989 loss: 1.26831274e-05
Iter: 990 loss: 1.26663126e-05
Iter: 991 loss: 1.26477753e-05
Iter: 992 loss: 1.28017891e-05
Iter: 993 loss: 1.26468894e-05
Iter: 994 loss: 1.26338455e-05
Iter: 995 loss: 1.26820978e-05
Iter: 996 loss: 1.26306368e-05
Iter: 997 loss: 1.26178638e-05
Iter: 998 loss: 1.27855419e-05
Iter: 999 loss: 1.26178784e-05
Iter: 1000 loss: 1.26100495e-05
Iter: 1001 loss: 1.2601522e-05
Iter: 1002 loss: 1.26004779e-05
Iter: 1003 loss: 1.25889401e-05
Iter: 1004 loss: 1.25679817e-05
Iter: 1005 loss: 1.30721473e-05
Iter: 1006 loss: 1.25681036e-05
Iter: 1007 loss: 1.25480447e-05
Iter: 1008 loss: 1.28243064e-05
Iter: 1009 loss: 1.25480456e-05
Iter: 1010 loss: 1.25346578e-05
Iter: 1011 loss: 1.26025598e-05
Iter: 1012 loss: 1.25323368e-05
Iter: 1013 loss: 1.25179695e-05
Iter: 1014 loss: 1.25050974e-05
Iter: 1015 loss: 1.25014913e-05
Iter: 1016 loss: 1.24845847e-05
Iter: 1017 loss: 1.26106988e-05
Iter: 1018 loss: 1.24832823e-05
Iter: 1019 loss: 1.24690669e-05
Iter: 1020 loss: 1.24774469e-05
Iter: 1021 loss: 1.24599173e-05
Iter: 1022 loss: 1.24405888e-05
Iter: 1023 loss: 1.25256029e-05
Iter: 1024 loss: 1.24364879e-05
Iter: 1025 loss: 1.24195267e-05
Iter: 1026 loss: 1.24724365e-05
Iter: 1027 loss: 1.241461e-05
Iter: 1028 loss: 1.23970258e-05
Iter: 1029 loss: 1.24104263e-05
Iter: 1030 loss: 1.23864447e-05
Iter: 1031 loss: 1.23795226e-05
Iter: 1032 loss: 1.23766622e-05
Iter: 1033 loss: 1.23683531e-05
Iter: 1034 loss: 1.24011312e-05
Iter: 1035 loss: 1.2366525e-05
Iter: 1036 loss: 1.23601403e-05
Iter: 1037 loss: 1.23451364e-05
Iter: 1038 loss: 1.25083079e-05
Iter: 1039 loss: 1.23436166e-05
Iter: 1040 loss: 1.23258878e-05
Iter: 1041 loss: 1.24216531e-05
Iter: 1042 loss: 1.23231939e-05
Iter: 1043 loss: 1.23110767e-05
Iter: 1044 loss: 1.23082236e-05
Iter: 1045 loss: 1.23005784e-05
Iter: 1046 loss: 1.22838619e-05
Iter: 1047 loss: 1.24340868e-05
Iter: 1048 loss: 1.22833717e-05
Iter: 1049 loss: 1.22688198e-05
Iter: 1050 loss: 1.2310973e-05
Iter: 1051 loss: 1.22641395e-05
Iter: 1052 loss: 1.22505e-05
Iter: 1053 loss: 1.22462725e-05
Iter: 1054 loss: 1.22382698e-05
Iter: 1055 loss: 1.22238234e-05
Iter: 1056 loss: 1.23323898e-05
Iter: 1057 loss: 1.22228557e-05
Iter: 1058 loss: 1.22092697e-05
Iter: 1059 loss: 1.22115507e-05
Iter: 1060 loss: 1.21991043e-05
Iter: 1061 loss: 1.21810453e-05
Iter: 1062 loss: 1.23253367e-05
Iter: 1063 loss: 1.21797393e-05
Iter: 1064 loss: 1.21677485e-05
Iter: 1065 loss: 1.21796938e-05
Iter: 1066 loss: 1.21610265e-05
Iter: 1067 loss: 1.21487428e-05
Iter: 1068 loss: 1.2323153e-05
Iter: 1069 loss: 1.21487219e-05
Iter: 1070 loss: 1.21367875e-05
Iter: 1071 loss: 1.21780477e-05
Iter: 1072 loss: 1.21334815e-05
Iter: 1073 loss: 1.21274961e-05
Iter: 1074 loss: 1.21179837e-05
Iter: 1075 loss: 1.2117911e-05
Iter: 1076 loss: 1.21037574e-05
Iter: 1077 loss: 1.21305893e-05
Iter: 1078 loss: 1.20977229e-05
Iter: 1079 loss: 1.20846344e-05
Iter: 1080 loss: 1.2104636e-05
Iter: 1081 loss: 1.20783725e-05
Iter: 1082 loss: 1.20614568e-05
Iter: 1083 loss: 1.20935229e-05
Iter: 1084 loss: 1.20545119e-05
Iter: 1085 loss: 1.20428685e-05
Iter: 1086 loss: 1.20430986e-05
Iter: 1087 loss: 1.20337309e-05
Iter: 1088 loss: 1.20255045e-05
Iter: 1089 loss: 1.2023158e-05
Iter: 1090 loss: 1.20069199e-05
Iter: 1091 loss: 1.20393324e-05
Iter: 1092 loss: 1.20002205e-05
Iter: 1093 loss: 1.19852575e-05
Iter: 1094 loss: 1.20117411e-05
Iter: 1095 loss: 1.19787019e-05
Iter: 1096 loss: 1.19627894e-05
Iter: 1097 loss: 1.20829591e-05
Iter: 1098 loss: 1.19615452e-05
Iter: 1099 loss: 1.19486458e-05
Iter: 1100 loss: 1.19774304e-05
Iter: 1101 loss: 1.19437873e-05
Iter: 1102 loss: 1.1935108e-05
Iter: 1103 loss: 1.19350152e-05
Iter: 1104 loss: 1.1925431e-05
Iter: 1105 loss: 1.19293154e-05
Iter: 1106 loss: 1.19189544e-05
Iter: 1107 loss: 1.19095275e-05
Iter: 1108 loss: 1.19031592e-05
Iter: 1109 loss: 1.18994994e-05
Iter: 1110 loss: 1.1885304e-05
Iter: 1111 loss: 1.18885837e-05
Iter: 1112 loss: 1.18750395e-05
Iter: 1113 loss: 1.18547132e-05
Iter: 1114 loss: 1.19789092e-05
Iter: 1115 loss: 1.18518983e-05
Iter: 1116 loss: 1.18404023e-05
Iter: 1117 loss: 1.18436456e-05
Iter: 1118 loss: 1.18319731e-05
Iter: 1119 loss: 1.18139233e-05
Iter: 1120 loss: 1.18880016e-05
Iter: 1121 loss: 1.18098724e-05
Iter: 1122 loss: 1.17971258e-05
Iter: 1123 loss: 1.19187198e-05
Iter: 1124 loss: 1.17965064e-05
Iter: 1125 loss: 1.17865366e-05
Iter: 1126 loss: 1.17727741e-05
Iter: 1127 loss: 1.17721593e-05
Iter: 1128 loss: 1.17526051e-05
Iter: 1129 loss: 1.18263197e-05
Iter: 1130 loss: 1.1747863e-05
Iter: 1131 loss: 1.17318141e-05
Iter: 1132 loss: 1.1759168e-05
Iter: 1133 loss: 1.17244899e-05
Iter: 1134 loss: 1.17088039e-05
Iter: 1135 loss: 1.18893277e-05
Iter: 1136 loss: 1.17086392e-05
Iter: 1137 loss: 1.17007712e-05
Iter: 1138 loss: 1.18052067e-05
Iter: 1139 loss: 1.17007821e-05
Iter: 1140 loss: 1.16926167e-05
Iter: 1141 loss: 1.17019308e-05
Iter: 1142 loss: 1.16882929e-05
Iter: 1143 loss: 1.16811025e-05
Iter: 1144 loss: 1.1669792e-05
Iter: 1145 loss: 1.16697192e-05
Iter: 1146 loss: 1.1655362e-05
Iter: 1147 loss: 1.16982101e-05
Iter: 1148 loss: 1.165106e-05
Iter: 1149 loss: 1.16359088e-05
Iter: 1150 loss: 1.16748406e-05
Iter: 1151 loss: 1.16306474e-05
Iter: 1152 loss: 1.16193551e-05
Iter: 1153 loss: 1.16857191e-05
Iter: 1154 loss: 1.16176125e-05
Iter: 1155 loss: 1.16069587e-05
Iter: 1156 loss: 1.15957437e-05
Iter: 1157 loss: 1.15937128e-05
Iter: 1158 loss: 1.15770199e-05
Iter: 1159 loss: 1.17838918e-05
Iter: 1160 loss: 1.15769853e-05
Iter: 1161 loss: 1.15652083e-05
Iter: 1162 loss: 1.15944413e-05
Iter: 1163 loss: 1.1560961e-05
Iter: 1164 loss: 1.15476096e-05
Iter: 1165 loss: 1.15364219e-05
Iter: 1166 loss: 1.15327366e-05
Iter: 1167 loss: 1.15173716e-05
Iter: 1168 loss: 1.16245938e-05
Iter: 1169 loss: 1.151582e-05
Iter: 1170 loss: 1.15019111e-05
Iter: 1171 loss: 1.15300718e-05
Iter: 1172 loss: 1.14959867e-05
Iter: 1173 loss: 1.14914583e-05
Iter: 1174 loss: 1.14884906e-05
Iter: 1175 loss: 1.14814738e-05
Iter: 1176 loss: 1.14798622e-05
Iter: 1177 loss: 1.1475372e-05
Iter: 1178 loss: 1.14666609e-05
Iter: 1179 loss: 1.14554459e-05
Iter: 1180 loss: 1.14547565e-05
Iter: 1181 loss: 1.14378945e-05
Iter: 1182 loss: 1.14678141e-05
Iter: 1183 loss: 1.14306486e-05
Iter: 1184 loss: 1.1418324e-05
Iter: 1185 loss: 1.15412076e-05
Iter: 1186 loss: 1.1417882e-05
Iter: 1187 loss: 1.14069426e-05
Iter: 1188 loss: 1.13967872e-05
Iter: 1189 loss: 1.13941533e-05
Iter: 1190 loss: 1.13799015e-05
Iter: 1191 loss: 1.1546741e-05
Iter: 1192 loss: 1.13796368e-05
Iter: 1193 loss: 1.13698579e-05
Iter: 1194 loss: 1.13741662e-05
Iter: 1195 loss: 1.13631868e-05
Iter: 1196 loss: 1.13505648e-05
Iter: 1197 loss: 1.14517879e-05
Iter: 1198 loss: 1.1349598e-05
Iter: 1199 loss: 1.13411406e-05
Iter: 1200 loss: 1.13473452e-05
Iter: 1201 loss: 1.13357355e-05
Iter: 1202 loss: 1.13234582e-05
Iter: 1203 loss: 1.13117812e-05
Iter: 1204 loss: 1.13091201e-05
Iter: 1205 loss: 1.1294147e-05
Iter: 1206 loss: 1.15054536e-05
Iter: 1207 loss: 1.12939615e-05
Iter: 1208 loss: 1.12859107e-05
Iter: 1209 loss: 1.12853459e-05
Iter: 1210 loss: 1.12792759e-05
Iter: 1211 loss: 1.12700473e-05
Iter: 1212 loss: 1.12701455e-05
Iter: 1213 loss: 1.12614853e-05
Iter: 1214 loss: 1.12787784e-05
Iter: 1215 loss: 1.12577345e-05
Iter: 1216 loss: 1.12471871e-05
Iter: 1217 loss: 1.12444231e-05
Iter: 1218 loss: 1.12379939e-05
Iter: 1219 loss: 1.12256967e-05
Iter: 1220 loss: 1.13337574e-05
Iter: 1221 loss: 1.12250318e-05
Iter: 1222 loss: 1.12145917e-05
Iter: 1223 loss: 1.12197076e-05
Iter: 1224 loss: 1.1207675e-05
Iter: 1225 loss: 1.11947138e-05
Iter: 1226 loss: 1.12515609e-05
Iter: 1227 loss: 1.11921327e-05
Iter: 1228 loss: 1.11824065e-05
Iter: 1229 loss: 1.12067901e-05
Iter: 1230 loss: 1.11792069e-05
Iter: 1231 loss: 1.11672925e-05
Iter: 1232 loss: 1.11985009e-05
Iter: 1233 loss: 1.11630634e-05
Iter: 1234 loss: 1.11517138e-05
Iter: 1235 loss: 1.11984555e-05
Iter: 1236 loss: 1.11493082e-05
Iter: 1237 loss: 1.1139834e-05
Iter: 1238 loss: 1.11345416e-05
Iter: 1239 loss: 1.11304344e-05
Iter: 1240 loss: 1.11144791e-05
Iter: 1241 loss: 1.11715026e-05
Iter: 1242 loss: 1.11104455e-05
Iter: 1243 loss: 1.11156933e-05
Iter: 1244 loss: 1.1105175e-05
Iter: 1245 loss: 1.11017234e-05
Iter: 1246 loss: 1.1090955e-05
Iter: 1247 loss: 1.11199352e-05
Iter: 1248 loss: 1.10850951e-05
Iter: 1249 loss: 1.10707188e-05
Iter: 1250 loss: 1.11843201e-05
Iter: 1251 loss: 1.1069651e-05
Iter: 1252 loss: 1.10594283e-05
Iter: 1253 loss: 1.10761439e-05
Iter: 1254 loss: 1.10547644e-05
Iter: 1255 loss: 1.10440142e-05
Iter: 1256 loss: 1.10584378e-05
Iter: 1257 loss: 1.1038348e-05
Iter: 1258 loss: 1.10271039e-05
Iter: 1259 loss: 1.1096201e-05
Iter: 1260 loss: 1.1025757e-05
Iter: 1261 loss: 1.10156352e-05
Iter: 1262 loss: 1.10219662e-05
Iter: 1263 loss: 1.10094807e-05
Iter: 1264 loss: 1.09961693e-05
Iter: 1265 loss: 1.10461224e-05
Iter: 1266 loss: 1.09930688e-05
Iter: 1267 loss: 1.09837683e-05
Iter: 1268 loss: 1.10313404e-05
Iter: 1269 loss: 1.0982103e-05
Iter: 1270 loss: 1.09722569e-05
Iter: 1271 loss: 1.09853681e-05
Iter: 1272 loss: 1.09671155e-05
Iter: 1273 loss: 1.0956217e-05
Iter: 1274 loss: 1.09826969e-05
Iter: 1275 loss: 1.0952248e-05
Iter: 1276 loss: 1.0941787e-05
Iter: 1277 loss: 1.09510884e-05
Iter: 1278 loss: 1.09356251e-05
Iter: 1279 loss: 1.09310058e-05
Iter: 1280 loss: 1.09291759e-05
Iter: 1281 loss: 1.09218536e-05
Iter: 1282 loss: 1.09156117e-05
Iter: 1283 loss: 1.09135372e-05
Iter: 1284 loss: 1.09061257e-05
Iter: 1285 loss: 1.08908162e-05
Iter: 1286 loss: 1.11845875e-05
Iter: 1287 loss: 1.08907161e-05
Iter: 1288 loss: 1.0880738e-05
Iter: 1289 loss: 1.0880176e-05
Iter: 1290 loss: 1.08716722e-05
Iter: 1291 loss: 1.08692348e-05
Iter: 1292 loss: 1.0864107e-05
Iter: 1293 loss: 1.08517424e-05
Iter: 1294 loss: 1.09023458e-05
Iter: 1295 loss: 1.08490485e-05
Iter: 1296 loss: 1.08389686e-05
Iter: 1297 loss: 1.08495578e-05
Iter: 1298 loss: 1.08333e-05
Iter: 1299 loss: 1.08182739e-05
Iter: 1300 loss: 1.08838167e-05
Iter: 1301 loss: 1.08151362e-05
Iter: 1302 loss: 1.08056138e-05
Iter: 1303 loss: 1.08331797e-05
Iter: 1304 loss: 1.0802707e-05
Iter: 1305 loss: 1.07916476e-05
Iter: 1306 loss: 1.08194581e-05
Iter: 1307 loss: 1.07879478e-05
Iter: 1308 loss: 1.07772667e-05
Iter: 1309 loss: 1.08280547e-05
Iter: 1310 loss: 1.07754586e-05
Iter: 1311 loss: 1.07664473e-05
Iter: 1312 loss: 1.0766983e-05
Iter: 1313 loss: 1.07591313e-05
Iter: 1314 loss: 1.07508449e-05
Iter: 1315 loss: 1.07507758e-05
Iter: 1316 loss: 1.07420856e-05
Iter: 1317 loss: 1.07711257e-05
Iter: 1318 loss: 1.07396363e-05
Iter: 1319 loss: 1.07346714e-05
Iter: 1320 loss: 1.07236028e-05
Iter: 1321 loss: 1.08763361e-05
Iter: 1322 loss: 1.07229589e-05
Iter: 1323 loss: 1.07083015e-05
Iter: 1324 loss: 1.07367778e-05
Iter: 1325 loss: 1.07025089e-05
Iter: 1326 loss: 1.06908446e-05
Iter: 1327 loss: 1.07808955e-05
Iter: 1328 loss: 1.06900334e-05
Iter: 1329 loss: 1.06784228e-05
Iter: 1330 loss: 1.06992211e-05
Iter: 1331 loss: 1.0673526e-05
Iter: 1332 loss: 1.06623793e-05
Iter: 1333 loss: 1.06884418e-05
Iter: 1334 loss: 1.06583147e-05
Iter: 1335 loss: 1.06482294e-05
Iter: 1336 loss: 1.06582356e-05
Iter: 1337 loss: 1.06424977e-05
Iter: 1338 loss: 1.06287162e-05
Iter: 1339 loss: 1.07183914e-05
Iter: 1340 loss: 1.06272128e-05
Iter: 1341 loss: 1.06177358e-05
Iter: 1342 loss: 1.0620317e-05
Iter: 1343 loss: 1.06107882e-05
Iter: 1344 loss: 1.05999043e-05
Iter: 1345 loss: 1.07216274e-05
Iter: 1346 loss: 1.05996014e-05
Iter: 1347 loss: 1.05918052e-05
Iter: 1348 loss: 1.05942381e-05
Iter: 1349 loss: 1.05861118e-05
Iter: 1350 loss: 1.05761155e-05
Iter: 1351 loss: 1.06080288e-05
Iter: 1352 loss: 1.05732706e-05
Iter: 1353 loss: 1.05643048e-05
Iter: 1354 loss: 1.05643931e-05
Iter: 1355 loss: 1.05595982e-05
Iter: 1356 loss: 1.05495701e-05
Iter: 1357 loss: 1.07150572e-05
Iter: 1358 loss: 1.054927e-05
Iter: 1359 loss: 1.05389572e-05
Iter: 1360 loss: 1.05367953e-05
Iter: 1361 loss: 1.05302242e-05
Iter: 1362 loss: 1.05201907e-05
Iter: 1363 loss: 1.06572788e-05
Iter: 1364 loss: 1.05201025e-05
Iter: 1365 loss: 1.05117606e-05
Iter: 1366 loss: 1.05105828e-05
Iter: 1367 loss: 1.05049112e-05
Iter: 1368 loss: 1.04905776e-05
Iter: 1369 loss: 1.05585168e-05
Iter: 1370 loss: 1.0487729e-05
Iter: 1371 loss: 1.04785076e-05
Iter: 1372 loss: 1.04891687e-05
Iter: 1373 loss: 1.04736191e-05
Iter: 1374 loss: 1.04619558e-05
Iter: 1375 loss: 1.04964111e-05
Iter: 1376 loss: 1.04585015e-05
Iter: 1377 loss: 1.04501287e-05
Iter: 1378 loss: 1.05245726e-05
Iter: 1379 loss: 1.04497021e-05
Iter: 1380 loss: 1.04416977e-05
Iter: 1381 loss: 1.04301871e-05
Iter: 1382 loss: 1.04297796e-05
Iter: 1383 loss: 1.04195042e-05
Iter: 1384 loss: 1.0419365e-05
Iter: 1385 loss: 1.04125011e-05
Iter: 1386 loss: 1.04192786e-05
Iter: 1387 loss: 1.04084929e-05
Iter: 1388 loss: 1.03992797e-05
Iter: 1389 loss: 1.05063227e-05
Iter: 1390 loss: 1.03991924e-05
Iter: 1391 loss: 1.03951352e-05
Iter: 1392 loss: 1.0385158e-05
Iter: 1393 loss: 1.05025247e-05
Iter: 1394 loss: 1.03843659e-05
Iter: 1395 loss: 1.03744824e-05
Iter: 1396 loss: 1.03962102e-05
Iter: 1397 loss: 1.03707107e-05
Iter: 1398 loss: 1.0358106e-05
Iter: 1399 loss: 1.03768962e-05
Iter: 1400 loss: 1.03523016e-05
Iter: 1401 loss: 1.0338832e-05
Iter: 1402 loss: 1.03774746e-05
Iter: 1403 loss: 1.03348466e-05
Iter: 1404 loss: 1.03218972e-05
Iter: 1405 loss: 1.03606253e-05
Iter: 1406 loss: 1.03181364e-05
Iter: 1407 loss: 1.03085385e-05
Iter: 1408 loss: 1.04320334e-05
Iter: 1409 loss: 1.03085022e-05
Iter: 1410 loss: 1.03008288e-05
Iter: 1411 loss: 1.02933063e-05
Iter: 1412 loss: 1.02917047e-05
Iter: 1413 loss: 1.02797749e-05
Iter: 1414 loss: 1.03671582e-05
Iter: 1415 loss: 1.02786862e-05
Iter: 1416 loss: 1.02693739e-05
Iter: 1417 loss: 1.02759477e-05
Iter: 1418 loss: 1.02635549e-05
Iter: 1419 loss: 1.02520571e-05
Iter: 1420 loss: 1.03433922e-05
Iter: 1421 loss: 1.02515805e-05
Iter: 1422 loss: 1.02441909e-05
Iter: 1423 loss: 1.02565245e-05
Iter: 1424 loss: 1.02410886e-05
Iter: 1425 loss: 1.02325612e-05
Iter: 1426 loss: 1.03363782e-05
Iter: 1427 loss: 1.02325757e-05
Iter: 1428 loss: 1.02259173e-05
Iter: 1429 loss: 1.02272434e-05
Iter: 1430 loss: 1.0221037e-05
Iter: 1431 loss: 1.02161121e-05
Iter: 1432 loss: 1.02075574e-05
Iter: 1433 loss: 1.04173705e-05
Iter: 1434 loss: 1.0207601e-05
Iter: 1435 loss: 1.01949663e-05
Iter: 1436 loss: 1.02107697e-05
Iter: 1437 loss: 1.01883215e-05
Iter: 1438 loss: 1.01771875e-05
Iter: 1439 loss: 1.02634904e-05
Iter: 1440 loss: 1.01763653e-05
Iter: 1441 loss: 1.01646983e-05
Iter: 1442 loss: 1.01621827e-05
Iter: 1443 loss: 1.01545411e-05
Iter: 1444 loss: 1.01420173e-05
Iter: 1445 loss: 1.02257691e-05
Iter: 1446 loss: 1.01409642e-05
Iter: 1447 loss: 1.01289297e-05
Iter: 1448 loss: 1.01724227e-05
Iter: 1449 loss: 1.01259866e-05
Iter: 1450 loss: 1.01146788e-05
Iter: 1451 loss: 1.01437463e-05
Iter: 1452 loss: 1.01104879e-05
Iter: 1453 loss: 1.0101362e-05
Iter: 1454 loss: 1.01109017e-05
Iter: 1455 loss: 1.00964889e-05
Iter: 1456 loss: 1.00839279e-05
Iter: 1457 loss: 1.01403211e-05
Iter: 1458 loss: 1.00815778e-05
Iter: 1459 loss: 1.00730722e-05
Iter: 1460 loss: 1.01200412e-05
Iter: 1461 loss: 1.00719672e-05
Iter: 1462 loss: 1.00653724e-05
Iter: 1463 loss: 1.01144615e-05
Iter: 1464 loss: 1.00647385e-05
Iter: 1465 loss: 1.00569941e-05
Iter: 1466 loss: 1.0076149e-05
Iter: 1467 loss: 1.00543311e-05
Iter: 1468 loss: 1.00493544e-05
Iter: 1469 loss: 1.00470606e-05
Iter: 1470 loss: 1.0044525e-05
Iter: 1471 loss: 1.00362249e-05
Iter: 1472 loss: 1.00328934e-05
Iter: 1473 loss: 1.0028727e-05
Iter: 1474 loss: 1.00169e-05
Iter: 1475 loss: 1.00389789e-05
Iter: 1476 loss: 1.00120942e-05
Iter: 1477 loss: 1.000049e-05
Iter: 1478 loss: 1.00359339e-05
Iter: 1479 loss: 9.99707663e-06
Iter: 1480 loss: 9.98593714e-06
Iter: 1481 loss: 1.00459329e-05
Iter: 1482 loss: 9.98418454e-06
Iter: 1483 loss: 9.97495499e-06
Iter: 1484 loss: 9.98878386e-06
Iter: 1485 loss: 9.97060852e-06
Iter: 1486 loss: 9.96058952e-06
Iter: 1487 loss: 1.00202597e-05
Iter: 1488 loss: 9.95923074e-06
Iter: 1489 loss: 9.95184746e-06
Iter: 1490 loss: 9.98663927e-06
Iter: 1491 loss: 9.95029404e-06
Iter: 1492 loss: 9.94282163e-06
Iter: 1493 loss: 9.9338522e-06
Iter: 1494 loss: 9.93305366e-06
Iter: 1495 loss: 9.92423156e-06
Iter: 1496 loss: 9.92428e-06
Iter: 1497 loss: 9.91721572e-06
Iter: 1498 loss: 9.92463174e-06
Iter: 1499 loss: 9.91320667e-06
Iter: 1500 loss: 9.90650915e-06
Iter: 1501 loss: 9.90641183e-06
Iter: 1502 loss: 9.9008e-06
Iter: 1503 loss: 9.8959008e-06
Iter: 1504 loss: 9.89443197e-06
Iter: 1505 loss: 9.88849206e-06
Iter: 1506 loss: 9.89624459e-06
Iter: 1507 loss: 9.88541797e-06
Iter: 1508 loss: 9.87670137e-06
Iter: 1509 loss: 9.86599116e-06
Iter: 1510 loss: 9.86495525e-06
Iter: 1511 loss: 9.85220049e-06
Iter: 1512 loss: 9.95407572e-06
Iter: 1513 loss: 9.85138468e-06
Iter: 1514 loss: 9.84121561e-06
Iter: 1515 loss: 9.84235066e-06
Iter: 1516 loss: 9.83339305e-06
Iter: 1517 loss: 9.82269557e-06
Iter: 1518 loss: 9.89557793e-06
Iter: 1519 loss: 9.82166603e-06
Iter: 1520 loss: 9.81290395e-06
Iter: 1521 loss: 9.83348491e-06
Iter: 1522 loss: 9.8093924e-06
Iter: 1523 loss: 9.79989727e-06
Iter: 1524 loss: 9.85017687e-06
Iter: 1525 loss: 9.79839388e-06
Iter: 1526 loss: 9.7904458e-06
Iter: 1527 loss: 9.80824461e-06
Iter: 1528 loss: 9.78747357e-06
Iter: 1529 loss: 9.77827e-06
Iter: 1530 loss: 9.79117613e-06
Iter: 1531 loss: 9.77339914e-06
Iter: 1532 loss: 9.76491629e-06
Iter: 1533 loss: 9.78791286e-06
Iter: 1534 loss: 9.76220417e-06
Iter: 1535 loss: 9.75459625e-06
Iter: 1536 loss: 9.75451348e-06
Iter: 1537 loss: 9.74829254e-06
Iter: 1538 loss: 9.75490912e-06
Iter: 1539 loss: 9.74479917e-06
Iter: 1540 loss: 9.73949136e-06
Iter: 1541 loss: 9.73597071e-06
Iter: 1542 loss: 9.73415808e-06
Iter: 1543 loss: 9.72608e-06
Iter: 1544 loss: 9.74025806e-06
Iter: 1545 loss: 9.72253292e-06
Iter: 1546 loss: 9.71302234e-06
Iter: 1547 loss: 9.741746e-06
Iter: 1548 loss: 9.71012651e-06
Iter: 1549 loss: 9.70241399e-06
Iter: 1550 loss: 9.7003358e-06
Iter: 1551 loss: 9.69547727e-06
Iter: 1552 loss: 9.68343738e-06
Iter: 1553 loss: 9.74356863e-06
Iter: 1554 loss: 9.68138374e-06
Iter: 1555 loss: 9.67217784e-06
Iter: 1556 loss: 9.68913082e-06
Iter: 1557 loss: 9.66825792e-06
Iter: 1558 loss: 9.6582944e-06
Iter: 1559 loss: 9.69651319e-06
Iter: 1560 loss: 9.65590516e-06
Iter: 1561 loss: 9.64740684e-06
Iter: 1562 loss: 9.70342e-06
Iter: 1563 loss: 9.64662104e-06
Iter: 1564 loss: 9.63835555e-06
Iter: 1565 loss: 9.6384374e-06
Iter: 1566 loss: 9.63199818e-06
Iter: 1567 loss: 9.62164e-06
Iter: 1568 loss: 9.68900622e-06
Iter: 1569 loss: 9.6205531e-06
Iter: 1570 loss: 9.61612386e-06
Iter: 1571 loss: 9.61609294e-06
Iter: 1572 loss: 9.61097e-06
Iter: 1573 loss: 9.60943e-06
Iter: 1574 loss: 9.60634497e-06
Iter: 1575 loss: 9.60068792e-06
Iter: 1576 loss: 9.60786e-06
Iter: 1577 loss: 9.59743738e-06
Iter: 1578 loss: 9.59201134e-06
Iter: 1579 loss: 9.59958e-06
Iter: 1580 loss: 9.58914279e-06
Iter: 1581 loss: 9.5819e-06
Iter: 1582 loss: 9.58128658e-06
Iter: 1583 loss: 9.57573502e-06
Iter: 1584 loss: 9.5680989e-06
Iter: 1585 loss: 9.6665517e-06
Iter: 1586 loss: 9.56798613e-06
Iter: 1587 loss: 9.56249642e-06
Iter: 1588 loss: 9.54930147e-06
Iter: 1589 loss: 9.71532427e-06
Iter: 1590 loss: 9.54834377e-06
Iter: 1591 loss: 9.53813924e-06
Iter: 1592 loss: 9.53826657e-06
Iter: 1593 loss: 9.53009385e-06
Iter: 1594 loss: 9.53784183e-06
Iter: 1595 loss: 9.52528899e-06
Iter: 1596 loss: 9.51658e-06
Iter: 1597 loss: 9.58677629e-06
Iter: 1598 loss: 9.51587208e-06
Iter: 1599 loss: 9.50898811e-06
Iter: 1600 loss: 9.521018e-06
Iter: 1601 loss: 9.50566846e-06
Iter: 1602 loss: 9.49618152e-06
Iter: 1603 loss: 9.51223137e-06
Iter: 1604 loss: 9.49208697e-06
Iter: 1605 loss: 9.49179685e-06
Iter: 1606 loss: 9.48885827e-06
Iter: 1607 loss: 9.48553679e-06
Iter: 1608 loss: 9.47951139e-06
Iter: 1609 loss: 9.61271235e-06
Iter: 1610 loss: 9.47944136e-06
Iter: 1611 loss: 9.47252556e-06
Iter: 1612 loss: 9.47991794e-06
Iter: 1613 loss: 9.4685447e-06
Iter: 1614 loss: 9.46107866e-06
Iter: 1615 loss: 9.49155583e-06
Iter: 1616 loss: 9.45942702e-06
Iter: 1617 loss: 9.45246848e-06
Iter: 1618 loss: 9.45539796e-06
Iter: 1619 loss: 9.44771818e-06
Iter: 1620 loss: 9.43893792e-06
Iter: 1621 loss: 9.47415901e-06
Iter: 1622 loss: 9.43703344e-06
Iter: 1623 loss: 9.42916813e-06
Iter: 1624 loss: 9.44795283e-06
Iter: 1625 loss: 9.42647694e-06
Iter: 1626 loss: 9.41707e-06
Iter: 1627 loss: 9.41950111e-06
Iter: 1628 loss: 9.41044709e-06
Iter: 1629 loss: 9.40132395e-06
Iter: 1630 loss: 9.42397855e-06
Iter: 1631 loss: 9.39809252e-06
Iter: 1632 loss: 9.38834455e-06
Iter: 1633 loss: 9.45212832e-06
Iter: 1634 loss: 9.38744051e-06
Iter: 1635 loss: 9.37991445e-06
Iter: 1636 loss: 9.40974223e-06
Iter: 1637 loss: 9.37825826e-06
Iter: 1638 loss: 9.37062e-06
Iter: 1639 loss: 9.38706762e-06
Iter: 1640 loss: 9.36780816e-06
Iter: 1641 loss: 9.36554716e-06
Iter: 1642 loss: 9.3638846e-06
Iter: 1643 loss: 9.36097877e-06
Iter: 1644 loss: 9.35583e-06
Iter: 1645 loss: 9.35580829e-06
Iter: 1646 loss: 9.35028402e-06
Iter: 1647 loss: 9.34599e-06
Iter: 1648 loss: 9.34444324e-06
Iter: 1649 loss: 9.33513547e-06
Iter: 1650 loss: 9.40118298e-06
Iter: 1651 loss: 9.3344e-06
Iter: 1652 loss: 9.32781586e-06
Iter: 1653 loss: 9.34315358e-06
Iter: 1654 loss: 9.32528e-06
Iter: 1655 loss: 9.31822433e-06
Iter: 1656 loss: 9.31365594e-06
Iter: 1657 loss: 9.31077e-06
Iter: 1658 loss: 9.30105e-06
Iter: 1659 loss: 9.41604503e-06
Iter: 1660 loss: 9.30082842e-06
Iter: 1661 loss: 9.29487578e-06
Iter: 1662 loss: 9.29439739e-06
Iter: 1663 loss: 9.28993541e-06
Iter: 1664 loss: 9.28016379e-06
Iter: 1665 loss: 9.30564693e-06
Iter: 1666 loss: 9.2770515e-06
Iter: 1667 loss: 9.26872508e-06
Iter: 1668 loss: 9.27198835e-06
Iter: 1669 loss: 9.26304529e-06
Iter: 1670 loss: 9.25321456e-06
Iter: 1671 loss: 9.33792853e-06
Iter: 1672 loss: 9.25260247e-06
Iter: 1673 loss: 9.24541e-06
Iter: 1674 loss: 9.27665405e-06
Iter: 1675 loss: 9.24402775e-06
Iter: 1676 loss: 9.23689731e-06
Iter: 1677 loss: 9.28610461e-06
Iter: 1678 loss: 9.23632433e-06
Iter: 1679 loss: 9.22836443e-06
Iter: 1680 loss: 9.25304084e-06
Iter: 1681 loss: 9.22613708e-06
Iter: 1682 loss: 9.22255458e-06
Iter: 1683 loss: 9.21534229e-06
Iter: 1684 loss: 9.34034233e-06
Iter: 1685 loss: 9.21521678e-06
Iter: 1686 loss: 9.20446837e-06
Iter: 1687 loss: 9.23525931e-06
Iter: 1688 loss: 9.2012624e-06
Iter: 1689 loss: 9.19370905e-06
Iter: 1690 loss: 9.25987e-06
Iter: 1691 loss: 9.19328249e-06
Iter: 1692 loss: 9.18696605e-06
Iter: 1693 loss: 9.18722526e-06
Iter: 1694 loss: 9.18204387e-06
Iter: 1695 loss: 9.17441866e-06
Iter: 1696 loss: 9.19779086e-06
Iter: 1697 loss: 9.17221496e-06
Iter: 1698 loss: 9.16520548e-06
Iter: 1699 loss: 9.193e-06
Iter: 1700 loss: 9.1637221e-06
Iter: 1701 loss: 9.15628698e-06
Iter: 1702 loss: 9.16076897e-06
Iter: 1703 loss: 9.15158853e-06
Iter: 1704 loss: 9.14325938e-06
Iter: 1705 loss: 9.16146e-06
Iter: 1706 loss: 9.13993699e-06
Iter: 1707 loss: 9.13006079e-06
Iter: 1708 loss: 9.15876262e-06
Iter: 1709 loss: 9.12720498e-06
Iter: 1710 loss: 9.12003816e-06
Iter: 1711 loss: 9.1557722e-06
Iter: 1712 loss: 9.11895768e-06
Iter: 1713 loss: 9.11405823e-06
Iter: 1714 loss: 9.11398274e-06
Iter: 1715 loss: 9.10913877e-06
Iter: 1716 loss: 9.10802373e-06
Iter: 1717 loss: 9.10493145e-06
Iter: 1718 loss: 9.09940809e-06
Iter: 1719 loss: 9.09865412e-06
Iter: 1720 loss: 9.09446226e-06
Iter: 1721 loss: 9.08779293e-06
Iter: 1722 loss: 9.08534457e-06
Iter: 1723 loss: 9.08145375e-06
Iter: 1724 loss: 9.07165668e-06
Iter: 1725 loss: 9.18149453e-06
Iter: 1726 loss: 9.07153299e-06
Iter: 1727 loss: 9.0658e-06
Iter: 1728 loss: 9.08150741e-06
Iter: 1729 loss: 9.0639187e-06
Iter: 1730 loss: 9.05807246e-06
Iter: 1731 loss: 9.05853449e-06
Iter: 1732 loss: 9.0534395e-06
Iter: 1733 loss: 9.04517492e-06
Iter: 1734 loss: 9.0752992e-06
Iter: 1735 loss: 9.0432959e-06
Iter: 1736 loss: 9.03632099e-06
Iter: 1737 loss: 9.05054094e-06
Iter: 1738 loss: 9.03347427e-06
Iter: 1739 loss: 9.02494867e-06
Iter: 1740 loss: 9.0515623e-06
Iter: 1741 loss: 9.02255852e-06
Iter: 1742 loss: 9.01606836e-06
Iter: 1743 loss: 9.01819e-06
Iter: 1744 loss: 9.01127623e-06
Iter: 1745 loss: 9.00146279e-06
Iter: 1746 loss: 9.03978798e-06
Iter: 1747 loss: 8.99920633e-06
Iter: 1748 loss: 9.00256236e-06
Iter: 1749 loss: 8.99621136e-06
Iter: 1750 loss: 8.99410588e-06
Iter: 1751 loss: 8.98906183e-06
Iter: 1752 loss: 9.04398075e-06
Iter: 1753 loss: 8.98857888e-06
Iter: 1754 loss: 8.98089638e-06
Iter: 1755 loss: 8.98653434e-06
Iter: 1756 loss: 8.97652e-06
Iter: 1757 loss: 8.96947677e-06
Iter: 1758 loss: 8.9942223e-06
Iter: 1759 loss: 8.96746496e-06
Iter: 1760 loss: 8.96044e-06
Iter: 1761 loss: 8.96520669e-06
Iter: 1762 loss: 8.95589073e-06
Iter: 1763 loss: 8.94978166e-06
Iter: 1764 loss: 8.94973346e-06
Iter: 1765 loss: 8.94522145e-06
Iter: 1766 loss: 8.93932e-06
Iter: 1767 loss: 8.93889137e-06
Iter: 1768 loss: 8.93048e-06
Iter: 1769 loss: 8.97118662e-06
Iter: 1770 loss: 8.92919707e-06
Iter: 1771 loss: 8.92195749e-06
Iter: 1772 loss: 8.93553442e-06
Iter: 1773 loss: 8.91890704e-06
Iter: 1774 loss: 8.90998308e-06
Iter: 1775 loss: 8.9267669e-06
Iter: 1776 loss: 8.90619231e-06
Iter: 1777 loss: 8.89869352e-06
Iter: 1778 loss: 8.93969445e-06
Iter: 1779 loss: 8.89763669e-06
Iter: 1780 loss: 8.89011517e-06
Iter: 1781 loss: 8.89114744e-06
Iter: 1782 loss: 8.88447812e-06
Iter: 1783 loss: 8.88526665e-06
Iter: 1784 loss: 8.88120849e-06
Iter: 1785 loss: 8.87786e-06
Iter: 1786 loss: 8.87024089e-06
Iter: 1787 loss: 9.00467603e-06
Iter: 1788 loss: 8.87014539e-06
Iter: 1789 loss: 8.86366252e-06
Iter: 1790 loss: 8.86752059e-06
Iter: 1791 loss: 8.85924055e-06
Iter: 1792 loss: 8.85260215e-06
Iter: 1793 loss: 8.92727e-06
Iter: 1794 loss: 8.85247209e-06
Iter: 1795 loss: 8.84734163e-06
Iter: 1796 loss: 8.83842586e-06
Iter: 1797 loss: 8.83844496e-06
Iter: 1798 loss: 8.83097618e-06
Iter: 1799 loss: 8.83093708e-06
Iter: 1800 loss: 8.8255083e-06
Iter: 1801 loss: 8.83333541e-06
Iter: 1802 loss: 8.82296808e-06
Iter: 1803 loss: 8.81551205e-06
Iter: 1804 loss: 8.8190427e-06
Iter: 1805 loss: 8.81044798e-06
Iter: 1806 loss: 8.80366588e-06
Iter: 1807 loss: 8.83925895e-06
Iter: 1808 loss: 8.80254356e-06
Iter: 1809 loss: 8.79583604e-06
Iter: 1810 loss: 8.800248e-06
Iter: 1811 loss: 8.79164509e-06
Iter: 1812 loss: 8.78400533e-06
Iter: 1813 loss: 8.83658322e-06
Iter: 1814 loss: 8.78329138e-06
Iter: 1815 loss: 8.77695675e-06
Iter: 1816 loss: 8.77704133e-06
Iter: 1817 loss: 8.7720382e-06
Iter: 1818 loss: 8.76372724e-06
Iter: 1819 loss: 8.8219e-06
Iter: 1820 loss: 8.7630915e-06
Iter: 1821 loss: 8.76209469e-06
Iter: 1822 loss: 8.76018203e-06
Iter: 1823 loss: 8.7580147e-06
Iter: 1824 loss: 8.75222577e-06
Iter: 1825 loss: 8.79370691e-06
Iter: 1826 loss: 8.75092883e-06
Iter: 1827 loss: 8.74328543e-06
Iter: 1828 loss: 8.74484886e-06
Iter: 1829 loss: 8.73775389e-06
Iter: 1830 loss: 8.73283e-06
Iter: 1831 loss: 8.73272529e-06
Iter: 1832 loss: 8.72800501e-06
Iter: 1833 loss: 8.72162855e-06
Iter: 1834 loss: 8.72147939e-06
Iter: 1835 loss: 8.71254269e-06
Iter: 1836 loss: 8.73767385e-06
Iter: 1837 loss: 8.70970507e-06
Iter: 1838 loss: 8.70150507e-06
Iter: 1839 loss: 8.75060141e-06
Iter: 1840 loss: 8.70071381e-06
Iter: 1841 loss: 8.6926284e-06
Iter: 1842 loss: 8.71881639e-06
Iter: 1843 loss: 8.69040377e-06
Iter: 1844 loss: 8.68460847e-06
Iter: 1845 loss: 8.679036e-06
Iter: 1846 loss: 8.6777909e-06
Iter: 1847 loss: 8.6700984e-06
Iter: 1848 loss: 8.75576188e-06
Iter: 1849 loss: 8.66999835e-06
Iter: 1850 loss: 8.66394e-06
Iter: 1851 loss: 8.66574555e-06
Iter: 1852 loss: 8.65957463e-06
Iter: 1853 loss: 8.65097627e-06
Iter: 1854 loss: 8.69093947e-06
Iter: 1855 loss: 8.64936919e-06
Iter: 1856 loss: 8.64666526e-06
Iter: 1857 loss: 8.64598587e-06
Iter: 1858 loss: 8.64239155e-06
Iter: 1859 loss: 8.6403079e-06
Iter: 1860 loss: 8.63869718e-06
Iter: 1861 loss: 8.63368e-06
Iter: 1862 loss: 8.62559e-06
Iter: 1863 loss: 8.62558e-06
Iter: 1864 loss: 8.61759054e-06
Iter: 1865 loss: 8.65563743e-06
Iter: 1866 loss: 8.61608e-06
Iter: 1867 loss: 8.60800901e-06
Iter: 1868 loss: 8.61257649e-06
Iter: 1869 loss: 8.60272758e-06
Iter: 1870 loss: 8.59531428e-06
Iter: 1871 loss: 8.70044278e-06
Iter: 1872 loss: 8.59528245e-06
Iter: 1873 loss: 8.58925e-06
Iter: 1874 loss: 8.5919728e-06
Iter: 1875 loss: 8.58494059e-06
Iter: 1876 loss: 8.57689429e-06
Iter: 1877 loss: 8.60306409e-06
Iter: 1878 loss: 8.57471878e-06
Iter: 1879 loss: 8.56926636e-06
Iter: 1880 loss: 8.59341708e-06
Iter: 1881 loss: 8.56825409e-06
Iter: 1882 loss: 8.56139104e-06
Iter: 1883 loss: 8.56167208e-06
Iter: 1884 loss: 8.55607141e-06
Iter: 1885 loss: 8.54857353e-06
Iter: 1886 loss: 8.56474435e-06
Iter: 1887 loss: 8.54590326e-06
Iter: 1888 loss: 8.53788697e-06
Iter: 1889 loss: 8.5470765e-06
Iter: 1890 loss: 8.53349229e-06
Iter: 1891 loss: 8.53147321e-06
Iter: 1892 loss: 8.5295851e-06
Iter: 1893 loss: 8.52574e-06
Iter: 1894 loss: 8.53416714e-06
Iter: 1895 loss: 8.52413e-06
Iter: 1896 loss: 8.5206957e-06
Iter: 1897 loss: 8.51328696e-06
Iter: 1898 loss: 8.63795231e-06
Iter: 1899 loss: 8.51319874e-06
Iter: 1900 loss: 8.50574088e-06
Iter: 1901 loss: 8.53948e-06
Iter: 1902 loss: 8.50414835e-06
Iter: 1903 loss: 8.49663593e-06
Iter: 1904 loss: 8.50763718e-06
Iter: 1905 loss: 8.49299704e-06
Iter: 1906 loss: 8.48572563e-06
Iter: 1907 loss: 8.51060486e-06
Iter: 1908 loss: 8.4836e-06
Iter: 1909 loss: 8.47720185e-06
Iter: 1910 loss: 8.484305e-06
Iter: 1911 loss: 8.47364208e-06
Iter: 1912 loss: 8.46555486e-06
Iter: 1913 loss: 8.53244273e-06
Iter: 1914 loss: 8.46491e-06
Iter: 1915 loss: 8.45928389e-06
Iter: 1916 loss: 8.47301817e-06
Iter: 1917 loss: 8.45720842e-06
Iter: 1918 loss: 8.45135946e-06
Iter: 1919 loss: 8.45729119e-06
Iter: 1920 loss: 8.4481444e-06
Iter: 1921 loss: 8.44303668e-06
Iter: 1922 loss: 8.49653225e-06
Iter: 1923 loss: 8.44281567e-06
Iter: 1924 loss: 8.43829275e-06
Iter: 1925 loss: 8.43495309e-06
Iter: 1926 loss: 8.43326598e-06
Iter: 1927 loss: 8.42745612e-06
Iter: 1928 loss: 8.47550109e-06
Iter: 1929 loss: 8.42710779e-06
Iter: 1930 loss: 8.42280497e-06
Iter: 1931 loss: 8.42296413e-06
Iter: 1932 loss: 8.42133e-06
Iter: 1933 loss: 8.41554e-06
Iter: 1934 loss: 8.41621841e-06
Iter: 1935 loss: 8.40998291e-06
Iter: 1936 loss: 8.39997665e-06
Iter: 1937 loss: 8.47482079e-06
Iter: 1938 loss: 8.39919903e-06
Iter: 1939 loss: 8.39178119e-06
Iter: 1940 loss: 8.41292058e-06
Iter: 1941 loss: 8.38932283e-06
Iter: 1942 loss: 8.38181768e-06
Iter: 1943 loss: 8.41357269e-06
Iter: 1944 loss: 8.3802006e-06
Iter: 1945 loss: 8.37438e-06
Iter: 1946 loss: 8.38152118e-06
Iter: 1947 loss: 8.37142306e-06
Iter: 1948 loss: 8.36293657e-06
Iter: 1949 loss: 8.40081248e-06
Iter: 1950 loss: 8.361244e-06
Iter: 1951 loss: 8.35533865e-06
Iter: 1952 loss: 8.3631694e-06
Iter: 1953 loss: 8.35207e-06
Iter: 1954 loss: 8.34426191e-06
Iter: 1955 loss: 8.38013602e-06
Iter: 1956 loss: 8.34274397e-06
Iter: 1957 loss: 8.33661852e-06
Iter: 1958 loss: 8.36532e-06
Iter: 1959 loss: 8.33540525e-06
Iter: 1960 loss: 8.33021113e-06
Iter: 1961 loss: 8.32618298e-06
Iter: 1962 loss: 8.32464e-06
Iter: 1963 loss: 8.31921352e-06
Iter: 1964 loss: 8.31876241e-06
Iter: 1965 loss: 8.31385205e-06
Iter: 1966 loss: 8.332996e-06
Iter: 1967 loss: 8.31282796e-06
Iter: 1968 loss: 8.3101113e-06
Iter: 1969 loss: 8.30557747e-06
Iter: 1970 loss: 8.30554291e-06
Iter: 1971 loss: 8.29943e-06
Iter: 1972 loss: 8.29861347e-06
Iter: 1973 loss: 8.29406054e-06
Iter: 1974 loss: 8.28653356e-06
Iter: 1975 loss: 8.30593399e-06
Iter: 1976 loss: 8.28406701e-06
Iter: 1977 loss: 8.27585609e-06
Iter: 1978 loss: 8.29118471e-06
Iter: 1979 loss: 8.27235908e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script66
+ '[' -r STOP.script66 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output66/f1_psi1_phi2.8/300_300_300_1
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1
+ for phi in 2.8
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output66/f1_psi1_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output67/f1_psi1_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output66/f1_psi1_phi2.8 /home/mrdouglas/Manifold/experiments.final/output67/f1_psi1_phi2.8
+ date
Sat Oct 24 23:30:19 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output66/f1_psi1_phi2.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 2345 --load_model experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1 --function f1 --psi 1 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output66/f1_psi1_phi2.8/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70622e4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70622e5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f706241c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70623ddbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70623fc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7062266598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70622276a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70622b4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70622b4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70622c1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70622c1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f706214d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f706214da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7062208950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70622088c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f703dcbcb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f703dcb16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f703dc01f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f703dcb1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70621b3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70621b4268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f703dbe6d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f703dbec048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f703dbbd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f703dbbdb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7018408510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7018443598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f701843a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70183c8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f701843a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70182fc620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70182c5158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70182c5048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70182c52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70183918c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f70183a2b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 1.999187
test_loss: 1.9910262
train_loss: 1.986595
test_loss: 1.991176
train_loss: 1.9973552
test_loss: 1.9937323
train_loss: 1.9902953
test_loss: 1.9916534
train_loss: 1.9929519
test_loss: 1.9921964
train_loss: 1.9957423
test_loss: 1.9911716
train_loss: 1.971906
test_loss: 1.9941236
train_loss: 1.9952065
test_loss: 1.9903274
train_loss: 1.9879773
test_loss: 1.9906931
train_loss: 1.9922683
test_loss: 1.9914436
train_loss: 1.999526
test_loss: 1.988166
train_loss: 1.9957845
test_loss: 1.9910768
train_loss: 1.9988308
test_loss: 1.9903855
train_loss: 1.9966686
test_loss: 1.9921641
train_loss: 1.9961917
test_loss: 1.9911783
train_loss: 1.9992291
test_loss: 1.9918542
train_loss: 1.9829724
test_loss: 1.9950414
train_loss: 1.9932588
test_loss: 1.9918673
train_loss: 1.9983481
test_loss: 1.9895662
train_loss: 1.9713883
test_loss: 1.9899293
train_loss: 1.992134
test_loss: 1.991544
train_loss: 1.9919248
test_loss: 1.9899378
train_loss: 1.9902898
test_loss: 1.9948717
train_loss: 1.9985129
test_loss: 1.9917712
train_loss: 1.9707475
test_loss: 1.9899852
train_loss: 1.9938862
test_loss: 1.992266
train_loss: 1.9987273
test_loss: 1.9921376
train_loss: 1.9878511
test_loss: 1.9957112
train_loss: 1.9880786
test_loss: 1.9933152
train_loss: 1.9956799
test_loss: 1.9922764
train_loss: 1.9909372
test_loss: 1.9906399
train_loss: 1.9988172
test_loss: 1.9935453
train_loss: 1.9911422
test_loss: 1.9883482
train_loss: 1.9882551
test_loss: 1.9914373
train_loss: 1.9893003
test_loss: 1.9911467
train_loss: 1.9976754
test_loss: 1.9917358
train_loss: 1.9867336
test_loss: 1.9905143
train_loss: 1.99617
test_loss: 1.9927504
train_loss: 1.9887989
test_loss: 1.9905139
train_loss: 1.9959704
test_loss: 1.989354
train_loss: 1.998973
test_loss: 1.9911699
train_loss: 1.9929988
test_loss: 1.9917948
train_loss: 1.9970872
test_loss: 1.9913999
train_loss: 1.9984525
test_loss: 1.9904394
train_loss: 1.9872661
test_loss: 1.9916291
train_loss: 1.9994129
test_loss: 1.9908208
train_loss: 1.9929101
test_loss: 1.991423
train_loss: 1.9883064
test_loss: 1.9896121
train_loss: 1.9718282
test_loss: 1.986372
train_loss: 1.9981031
test_loss: 1.9918584
train_loss: 1.9668562
test_loss: 1.9914345
train_loss: 1.9803088
test_loss: 1.9936473
train_loss: 1.9977875
test_loss: 1.9912435
train_loss: 1.9973017
test_loss: 1.9922426
train_loss: 1.9964621
test_loss: 1.9933246
train_loss: 1.9803159
test_loss: 1.9891199
train_loss: 1.9904549
test_loss: 1.9883075
train_loss: 1.9957745
test_loss: 1.9925331
train_loss: 1.9926455
test_loss: 1.9928818
train_loss: 1.9938357
test_loss: 1.9917662
train_loss: 1.9977564
test_loss: 1.9919975
train_loss: 1.9968798
test_loss: 1.993877
train_loss: 1.9947269
test_loss: 1.990899
train_loss: 1.9842619
test_loss: 1.9912378
train_loss: 1.996554
test_loss: 1.9871056
train_loss: 1.9988981
test_loss: 1.9862157
train_loss: 1.988348
test_loss: 1.9890748
train_loss: 1.9962369
test_loss: 1.9898497
train_loss: 1.983941
test_loss: 1.9895172
train_loss: 1.9895165
test_loss: 1.986544
train_loss: 1.9770485
test_loss: 1.9888023
train_loss: 1.992423
test_loss: 1.9930469
train_loss: 1.9953774
test_loss: 1.9887727
train_loss: 1.9952326
test_loss: 1.9942347
train_loss: 1.9937239
test_loss: 1.9922892
train_loss: 1.9680985
test_loss: 1.9928269
train_loss: 1.9945557
test_loss: 1.9900008
train_loss: 1.9994333
test_loss: 1.9910405
train_loss: 1.9800258
test_loss: 1.9932433
train_loss: 1.9863278
test_loss: 1.9928968
train_loss: 1.9983113
test_loss: 1.9901416
train_loss: 1.9862465
test_loss: 1.9909633
train_loss: 1.9801338
test_loss: 1.9917873
train_loss: 1.9997178
test_loss: 1.990807
train_loss: 1.9982252
test_loss: 1.9912434
train_loss: 1.9911752
test_loss: 1.9896171
train_loss: 1.9894464
test_loss: 1.9872321
train_loss: 1.9998124
test_loss: 1.9929224
train_loss: 1.9968412
test_loss: 1.9942542
train_loss: 1.9915072
test_loss: 1.9905072
train_loss: 1.9915813
test_loss: 1.993066
train_loss: 1.998179
test_loss: 1.9895008
train_loss: 1.997268
test_loss: 1.9907783
train_loss: 1.9926999
test_loss: 1.9898762
train_loss: 1.9888165
test_loss: 1.9920386
train_loss: 1.9982775
test_loss: 1.9918281
train_loss: 1.9990896
test_loss: 1.990623
train_loss: 1.9955211
test_loss: 1.9913096
train_loss: 1.9934554
test_loss: 1.9924843
train_loss: 1.9959773
test_loss: 1.9901005
train_loss: 1.989859
test_loss: 1.9921795
train_loss: 1.9999169
test_loss: 1.996172
train_loss: 1.992249
test_loss: 1.9947894
train_loss: 1.9736097
test_loss: 1.991166
train_loss: 1.9985797
test_loss: 1.9916185
train_loss: 1.9995587
test_loss: 1.992442
train_loss: 1.9932749
test_loss: 1.9936992
train_loss: 1.9876571
test_loss: 1.992902
train_loss: 1.9934237
test_loss: 1.9876676
train_loss: 1.9972962
test_loss: 1.9898518
train_loss: 1.9918737
test_loss: 1.9918343
train_loss: 1.9966613
test_loss: 1.9921584
train_loss: 1.9922601
test_loss: 1.9916039
train_loss: 1.9870231
test_loss: 1.9927264
train_loss: 1.9782555
test_loss: 1.9959478
train_loss: 1.9951918
test_loss: 1.9932493
train_loss: 1.9966036
test_loss: 1.9920021
train_loss: 1.9902302
test_loss: 1.9932946
train_loss: 1.9971356
test_loss: 1.9947196
train_loss: 1.9996291
test_loss: 1.9948963
train_loss: 1.9977536
test_loss: 1.9947647
train_loss: 1.9861667
test_loss: 1.9939116
train_loss: 1.9985584
test_loss: 1.9933952
train_loss: 1.9963021
test_loss: 1.9971858
train_loss: 1.9992665
test_loss: 2.0148575
train_loss: 1.9908347
test_loss: 1.9898242
train_loss: 1.9838797
test_loss: 1.9911779
train_loss: 1.995018
test_loss: 1.9928591
train_loss: 1.9995967
test_loss: 1.993558
train_loss: 1.980113
test_loss: 1.9908639
train_loss: 1.9821762
test_loss: 1.9914285
train_loss: 1.9818181
test_loss: 1.9914951
train_loss: 1.9971446
test_loss: 1.9923005
train_loss: 1.9923596
test_loss: 1.9900291
train_loss: 1.9942623
test_loss: 1.9929891
train_loss: 1.9867641
test_loss: 1.9915577
train_loss: 1.992256
test_loss: 1.9918648
train_loss: 1.9949865
test_loss: 1.9954128
train_loss: 1.9996098
test_loss: 1.9923853
train_loss: 1.9999318
test_loss: 1.9921901
train_loss: 1.9917612
test_loss: 1.9911165
train_loss: 1.9957277
test_loss: 1.9955535
train_loss: 1.9948084
test_loss: 1.993577
train_loss: 1.987731
test_loss: 1.9920547
train_loss: 1.9909418
test_loss: 1.9933981
train_loss: 1.9908353
test_loss: 1.9936267
train_loss: 1.9718488
test_loss: 1.9905703
train_loss: 1.9980278
test_loss: 1.9931467
train_loss: 1.99265
test_loss: 1.98938
train_loss: 1.9967535
test_loss: 1.9925195
train_loss: 1.9906821
test_loss: 1.9935485
train_loss: 1.9929862
test_loss: 1.9890697
train_loss: 1.9980067
test_loss: 1.9918183
train_loss: 1.9944019
test_loss: 1.9933213
train_loss: 1.993206
test_loss: 1.9914334
train_loss: 1.9956638
test_loss: 1.9929565
train_loss: 1.990122
test_loss: 1.9937377
train_loss: 1.9909838
test_loss: 1.994019
train_loss: 1.994412
test_loss: 1.995239
train_loss: 1.986263
test_loss: 1.9946785
train_loss: 1.9956614
test_loss: 1.992173
train_loss: 1.9943407
test_loss: 1.993171
train_loss: 1.9988215
test_loss: 1.9907335
train_loss: 1.9940467
test_loss: 1.9933876
train_loss: 1.9991199
test_loss: 1.995381
train_loss: 1.9817159
test_loss: 1.9917742
train_loss: 1.9974885
test_loss: 1.9956253
train_loss: 1.9977633
test_loss: 1.9945406
train_loss: 1.9974327
test_loss: 1.9949186
train_loss: 1.9969211
test_loss: 1.9939569
train_loss: 1.03175
test_loss: 0.9716688
train_loss: 0.90868795
test_loss: 0.92279357
train_loss: 0.90224195
test_loss: 0.9247998
train_loss: 0.92739004
test_loss: 0.9247694
train_loss: 0.9481878
test_loss: 0.9244723
train_loss: 0.907084
test_loss: 0.92417485
train_loss: 0.9014805
test_loss: 0.923952
train_loss: 0.8860884
test_loss: 0.92358756
train_loss: 0.9218963
test_loss: 0.92323273
train_loss: 0.9338173
test_loss: 0.9231009
train_loss: 0.92163837
test_loss: 0.9226405
train_loss: 0.91067994
test_loss: 0.92235905
train_loss: 0.8961169
test_loss: 0.9220532
train_loss: 0.88650864
test_loss: 0.9216648
train_loss: 0.90801024
test_loss: 0.92122287
train_loss: 0.903409
test_loss: 0.92083263
train_loss: 0.9343792
test_loss: 0.92053163
train_loss: 0.9039403
test_loss: 0.9202255
train_loss: 0.90984154
test_loss: 0.9197554
train_loss: 0.9369256
test_loss: 0.91944313
train_loss: 0.898978
test_loss: 0.91911876
train_loss: 0.9394262
test_loss: 0.9185871
train_loss: 0.9118362
test_loss: 0.91808677
train_loss: 0.89586824
test_loss: 0.9176603
train_loss: 0.9265451
test_loss: 0.916984
train_loss: 0.888137
test_loss: 0.9162572
train_loss: 0.9239996
test_loss: 0.9150328
train_loss: 0.92827106
test_loss: 0.9127674
train_loss: 0.9087079
test_loss: 0.90893716
train_loss: 0.90530884
test_loss: 0.9018923
train_loss: 0.8868352
test_loss: 0.8919889
train_loss: 0.87334263
test_loss: 0.8814866
train_loss: 0.87978256
test_loss: 0.8723951
train_loss: 0.87981
test_loss: 0.8648681
train_loss: 0.8618531
test_loss: 0.858207
train_loss: 0.83475626
test_loss: 0.8519933
train_loss: 0.8636074
test_loss: 0.8461263
train_loss: 0.84925973
test_loss: 0.84025276
train_loss: 0.83833206
test_loss: 0.83417076
train_loss: 0.8269107
test_loss: 0.82822293
train_loss: 0.8033308
test_loss: 0.82209325
train_loss: 0.7913068
test_loss: 0.81572205
train_loss: 0.8063668
test_loss: 0.8093388
train_loss: 0.7975565
test_loss: 0.8026222
train_loss: 0.7864112
test_loss: 0.7963019
train_loss: 0.7740129
test_loss: 0.7895969
train_loss: 0.77293354
test_loss: 0.7830382
train_loss: 0.7683425
test_loss: 0.77644676
train_loss: 0.7780483
test_loss: 0.76962394
train_loss: 0.7435778
test_loss: 0.7630543
train_loss: 0.7553283
test_loss: 0.75637
train_loss: 0.72977006
test_loss: 0.7499248
train_loss: 0.7191883
test_loss: 0.74306977
train_loss: 0.7233888
test_loss: 0.73629856
train_loss: 0.7050299
test_loss: 0.7293252
train_loss: 0.7246574
test_loss: 0.7220782
train_loss: 0.6996467
test_loss: 0.71479005
train_loss: 0.6893029
test_loss: 0.707262
train_loss: 0.6562632
test_loss: 0.6992902
train_loss: 0.7008485
test_loss: 0.691045
train_loss: 0.70938575
test_loss: 0.6823433
train_loss: 0.658237
test_loss: 0.6731347
train_loss: 0.6431336
test_loss: 0.66310066
train_loss: 0.6529855
test_loss: 0.6525723
train_loss: 0.5998875
test_loss: 0.64112633
train_loss: 0.6241758
test_loss: 0.6288647
train_loss: 0.5970732
test_loss: 0.61574286
train_loss: 0.5756367
test_loss: 0.60159796
train_loss: 0.5872815
test_loss: 0.58635306
train_loss: 0.5508714
test_loss: 0.5698279
train_loss: 0.5618302
test_loss: 0.5520871
train_loss: 0.5025636
test_loss: 0.53250724
train_loss: 0.5048977
test_loss: 0.5115689
train_loss: 0.4658368
test_loss: 0.4890156
train_loss: 0.48670077
test_loss: 0.4654569
train_loss: 0.44696596
test_loss: 0.44092295
train_loss: 0.4160092
test_loss: 0.41647124
train_loss: 0.41347826
test_loss: 0.39234352
train_loss: 0.35359672
test_loss: 0.3695505
train_loss: 0.3216033
test_loss: 0.34833983
train_loss: 0.33173114
test_loss: 0.32803488
train_loss: 0.32027268
test_loss: 0.3098263
train_loss: 0.2967697
test_loss: 0.2928858
train_loss: 0.29616815
test_loss: 0.27785796
train_loss: 0.28919435
test_loss: 0.2644029
train_loss: 0.25261995
test_loss: 0.2524132
train_loss: 0.23043326
test_loss: 0.24194059
train_loss: 0.23982254
test_loss: 0.23214668
train_loss: 0.23540369
test_loss: 0.223824
train_loss: 0.22347276
test_loss: 0.21613857
train_loss: 0.21050231
test_loss: 0.20903234
train_loss: 0.20009243
test_loss: 0.20262635
train_loss: 0.19593437
test_loss: 0.19672768
train_loss: 0.18915597
test_loss: 0.19121921
train_loss: 0.17442065
test_loss: 0.1862144
train_loss: 0.18739545
test_loss: 0.18173882
train_loss: 0.16417356
test_loss: 0.17708063
train_loss: 0.16562928
test_loss: 0.17337431
train_loss: 0.16590288
test_loss: 0.16940993
train_loss: 0.1546863
test_loss: 0.16606624
train_loss: 0.18830101
test_loss: 0.16317578
train_loss: 0.17406684
test_loss: 0.15978314
train_loss: 0.16495138
test_loss: 0.15646005
train_loss: 0.15638205
test_loss: 0.15373465
train_loss: 0.14371406
test_loss: 0.15105075
train_loss: 0.16168517
test_loss: 0.14816752
train_loss: 0.13726288
test_loss: 0.14540015
train_loss: 0.13947266
test_loss: 0.14310518
train_loss: 0.14716385
test_loss: 0.14072089
train_loss: 0.13375796
test_loss: 0.13806464
train_loss: 0.1354852
test_loss: 0.13653165
train_loss: 0.13049151
test_loss: 0.13372394
train_loss: 0.13403806
test_loss: 0.13245188
train_loss: 0.13728377
test_loss: 0.12944794
train_loss: 0.13031587
test_loss: 0.12743624
train_loss: 0.12585446
test_loss: 0.12603717
train_loss: 0.117638476
test_loss: 0.12419535
train_loss: 0.11853772
test_loss: 0.12175367
train_loss: 0.11750937
test_loss: 0.12000382
train_loss: 0.12086273
test_loss: 0.11864957
train_loss: 0.114937246
test_loss: 0.11628685
train_loss: 0.10927172
test_loss: 0.114648536
train_loss: 0.11113934
test_loss: 0.112947516
train_loss: 0.11269569
test_loss: 0.111403584
train_loss: 0.106207356
test_loss: 0.10978222
train_loss: 0.11348753
test_loss: 0.10858255
train_loss: 0.1019437
test_loss: 0.1070948
train_loss: 0.10840635
test_loss: 0.1056097
train_loss: 0.11098389
test_loss: 0.104082495
train_loss: 0.10755505
test_loss: 0.10337409
train_loss: 0.104582
test_loss: 0.10144444
train_loss: 0.10406133
test_loss: 0.10042992
train_loss: 0.09992827
test_loss: 0.09940781
train_loss: 0.09310878
test_loss: 0.09855163
train_loss: 0.101218395
test_loss: 0.09745524
train_loss: 0.09144284
test_loss: 0.09653561
train_loss: 0.090923026
test_loss: 0.09579835
train_loss: 0.09786627
test_loss: 0.09475446
train_loss: 0.0940419
test_loss: 0.09416991
train_loss: 0.09672101
test_loss: 0.093156606
train_loss: 0.09422456
test_loss: 0.092322946
train_loss: 0.09676072
test_loss: 0.09181453
train_loss: 0.081662595
test_loss: 0.090884626
train_loss: 0.087390624
test_loss: 0.090248615
train_loss: 0.08971756
test_loss: 0.08990378
train_loss: 0.09007637
test_loss: 0.089614436
train_loss: 0.08594598
test_loss: 0.090139166
train_loss: 0.091510564
test_loss: 0.08878099
train_loss: 0.08338382
test_loss: 0.08840635
train_loss: 0.081664585
test_loss: 0.08848783
train_loss: 0.08598017
test_loss: 0.0884634
train_loss: 0.08145726
test_loss: 0.08801319
train_loss: 0.07968401
test_loss: 0.087399036
train_loss: 0.08267738
test_loss: 0.087166935
train_loss: 0.08360793
test_loss: 0.086819835
train_loss: 0.08398454
test_loss: 0.08726907
train_loss: 0.087945536
test_loss: 0.087389536
train_loss: 0.08211911
test_loss: 0.08641147
train_loss: 0.08772287
test_loss: 0.085950464
train_loss: 0.08936291
test_loss: 0.08603266
train_loss: 0.08504048
test_loss: 0.085492335
train_loss: 0.079202786
test_loss: 0.08558071
train_loss: 0.085127555
test_loss: 0.08569209
train_loss: 0.07859105
test_loss: 0.08594773
train_loss: 0.08172205
test_loss: 0.08510643
train_loss: 0.08262355
test_loss: 0.08506311
train_loss: 0.08606979
test_loss: 0.08471394
train_loss: 0.08496773
test_loss: 0.08460657
train_loss: 0.08667686
test_loss: 0.08439346
train_loss: 0.0797717
test_loss: 0.08477423
train_loss: 0.08039638
test_loss: 0.08389139
train_loss: 0.078851424
test_loss: 0.08395564
train_loss: 0.07879586
test_loss: 0.08411441
train_loss: 0.085096605
test_loss: 0.0840153
train_loss: 0.07943654
test_loss: 0.08375583
train_loss: 0.07866424
test_loss: 0.083848655
train_loss: 0.08583055
test_loss: 0.08289406
train_loss: 0.07613212
test_loss: 0.082743384
train_loss: 0.08039618
test_loss: 0.08270352
train_loss: 0.0884296
test_loss: 0.08339471
train_loss: 0.0834972
test_loss: 0.08306198
train_loss: 0.08096422
test_loss: 0.08352611
train_loss: 0.08261225
test_loss: 0.082570724
train_loss: 0.08623343
test_loss: 0.08224496
train_loss: 0.0811397
test_loss: 0.082245305
train_loss: 0.084543355
test_loss: 0.08210385
train_loss: 0.07868473
test_loss: 0.08275026
train_loss: 0.07659779
test_loss: 0.08206381
train_loss: 0.08698594
test_loss: 0.08160378
train_loss: 0.079534434
test_loss: 0.082222104
train_loss: 0.0816735
test_loss: 0.08202575
train_loss: 0.08413278
test_loss: 0.08155151
train_loss: 0.077760935
test_loss: 0.08129196
train_loss: 0.08420896
test_loss: 0.08141327
train_loss: 0.07958644
test_loss: 0.080933765
train_loss: 0.07714371
test_loss: 0.080664426
train_loss: 0.0821541
test_loss: 0.082728095
train_loss: 0.0822183
test_loss: 0.08072309
train_loss: 0.07596178
test_loss: 0.08050529
train_loss: 0.07863406
test_loss: 0.08063893
train_loss: 0.08021249
test_loss: 0.080408715
train_loss: 0.0762482
test_loss: 0.08030433
train_loss: 0.07566417
test_loss: 0.08039149
train_loss: 0.077562146
test_loss: 0.080019265
train_loss: 0.07819545
test_loss: 0.0800201
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.07663826
test_loss: 0.08074016
train_loss: 0.07889792
test_loss: 0.08058486
train_loss: 0.07898014
test_loss: 0.080103554
train_loss: 0.07401875
test_loss: 0.07964101
train_loss: 0.079683416
test_loss: 0.07966817
train_loss: 0.07210287
test_loss: 0.0792289
train_loss: 0.07995671
test_loss: 0.079032764
train_loss: 0.07554724
test_loss: 0.078922845
train_loss: 0.07401566
test_loss: 0.07950092
train_loss: 0.07521495
test_loss: 0.07904062
train_loss: 0.07066454
test_loss: 0.07868324
train_loss: 0.07575488
test_loss: 0.078527726
train_loss: 0.071882166
test_loss: 0.078509174
train_loss: 0.07794161
test_loss: 0.078536674
train_loss: 0.07781707
test_loss: 0.078697406
train_loss: 0.07311091
test_loss: 0.07832349
train_loss: 0.07588599
test_loss: 0.07807541
train_loss: 0.06994288
test_loss: 0.07800155
train_loss: 0.075981975
test_loss: 0.07809045
train_loss: 0.081693776
test_loss: 0.07781168
train_loss: 0.073887184
test_loss: 0.077835806
train_loss: 0.080646865
test_loss: 0.077751115
train_loss: 0.07705118
test_loss: 0.07757103
train_loss: 0.07463907
test_loss: 0.07807399
train_loss: 0.07495141
test_loss: 0.07726842
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output67/f1_psi1_phi2.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output66/f1_psi1_phi2.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 1 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output67/f1_psi1_phi2.8/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c23b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8353363a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8353363158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8353343510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c277488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c1dd268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c1bd8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c165598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c165bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c1656a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c107bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c0e6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c101840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c09d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c09d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c077bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c084400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f832c01a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83101b9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8310145268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83101761e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f831010e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83100e7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83100fa620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83100fabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8310084378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8310084840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8310064730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8310005620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8310005d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f82d05ba620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f82d056c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f82d05bac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f82d05a7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f82d054f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f82d054ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.0101814233
Iter: 2 loss: 0.0187765844
Iter: 3 loss: 0.010148447
Iter: 4 loss: 0.00980662927
Iter: 5 loss: 0.040009696
Iter: 6 loss: 0.00980634242
Iter: 7 loss: 0.00971028
Iter: 8 loss: 0.0135544371
Iter: 9 loss: 0.00970993564
Iter: 10 loss: 0.00961842574
Iter: 11 loss: 0.00968460366
Iter: 12 loss: 0.00956674758
Iter: 13 loss: 0.00945662
Iter: 14 loss: 0.00968580507
Iter: 15 loss: 0.00943465
Iter: 16 loss: 0.0093495762
Iter: 17 loss: 0.00990641862
Iter: 18 loss: 0.00933984108
Iter: 19 loss: 0.00928511
Iter: 20 loss: 0.00941715203
Iter: 21 loss: 0.0092691686
Iter: 22 loss: 0.00921846274
Iter: 23 loss: 0.0092531126
Iter: 24 loss: 0.00918687414
Iter: 25 loss: 0.00914677605
Iter: 26 loss: 0.00915970095
Iter: 27 loss: 0.00911849365
Iter: 28 loss: 0.00907817204
Iter: 29 loss: 0.00910883304
Iter: 30 loss: 0.00905350875
Iter: 31 loss: 0.00901242
Iter: 32 loss: 0.0091494862
Iter: 33 loss: 0.00900192
Iter: 34 loss: 0.00896887947
Iter: 35 loss: 0.00898145325
Iter: 36 loss: 0.00894612446
Iter: 37 loss: 0.00891159568
Iter: 38 loss: 0.00897759199
Iter: 39 loss: 0.00889742933
Iter: 40 loss: 0.00886810385
Iter: 41 loss: 0.00911381934
Iter: 42 loss: 0.00886694063
Iter: 43 loss: 0.00885103829
Iter: 44 loss: 0.0088716168
Iter: 45 loss: 0.0088429153
Iter: 46 loss: 0.00882474892
Iter: 47 loss: 0.00880172476
Iter: 48 loss: 0.00880002882
Iter: 49 loss: 0.00876816921
Iter: 50 loss: 0.00889604166
Iter: 51 loss: 0.00876111165
Iter: 52 loss: 0.00873544
Iter: 53 loss: 0.00897214189
Iter: 54 loss: 0.00873427186
Iter: 55 loss: 0.00871091522
Iter: 56 loss: 0.00887499098
Iter: 57 loss: 0.00870866
Iter: 58 loss: 0.008694686
Iter: 59 loss: 0.00867261551
Iter: 60 loss: 0.00867237709
Iter: 61 loss: 0.00865162164
Iter: 62 loss: 0.00890159048
Iter: 63 loss: 0.00865135808
Iter: 64 loss: 0.0086284373
Iter: 65 loss: 0.00871123467
Iter: 66 loss: 0.00862253457
Iter: 67 loss: 0.00860589091
Iter: 68 loss: 0.00859564822
Iter: 69 loss: 0.00858882908
Iter: 70 loss: 0.00856246613
Iter: 71 loss: 0.00870688725
Iter: 72 loss: 0.00855880883
Iter: 73 loss: 0.00853967667
Iter: 74 loss: 0.00861952081
Iter: 75 loss: 0.00853565
Iter: 76 loss: 0.00851691328
Iter: 77 loss: 0.00855315
Iter: 78 loss: 0.008508862
Iter: 79 loss: 0.00848742388
Iter: 80 loss: 0.00848914
Iter: 81 loss: 0.00847059581
Iter: 82 loss: 0.00844780263
Iter: 83 loss: 0.00848791841
Iter: 84 loss: 0.00843776576
Iter: 85 loss: 0.00841527246
Iter: 86 loss: 0.00844590738
Iter: 87 loss: 0.00840401091
Iter: 88 loss: 0.00837712549
Iter: 89 loss: 0.00839240663
Iter: 90 loss: 0.00835953653
Iter: 91 loss: 0.00835303217
Iter: 92 loss: 0.00834147353
Iter: 93 loss: 0.00832799263
Iter: 94 loss: 0.00830172934
Iter: 95 loss: 0.00889876951
Iter: 96 loss: 0.00830158312
Iter: 97 loss: 0.00827118
Iter: 98 loss: 0.00830883253
Iter: 99 loss: 0.00825460441
Iter: 100 loss: 0.0082291
Iter: 101 loss: 0.00836332329
Iter: 102 loss: 0.00822502747
Iter: 103 loss: 0.00819981471
Iter: 104 loss: 0.0083157355
Iter: 105 loss: 0.00819525
Iter: 106 loss: 0.0081806276
Iter: 107 loss: 0.00817843527
Iter: 108 loss: 0.00816808175
Iter: 109 loss: 0.00813753251
Iter: 110 loss: 0.00821348559
Iter: 111 loss: 0.0081268819
Iter: 112 loss: 0.00809919462
Iter: 113 loss: 0.00824294612
Iter: 114 loss: 0.00809448399
Iter: 115 loss: 0.00806529727
Iter: 116 loss: 0.00814979151
Iter: 117 loss: 0.00805621
Iter: 118 loss: 0.00802340079
Iter: 119 loss: 0.0080839647
Iter: 120 loss: 0.00800956879
Iter: 121 loss: 0.0079860948
Iter: 122 loss: 0.00822297391
Iter: 123 loss: 0.00798528455
Iter: 124 loss: 0.00796772446
Iter: 125 loss: 0.00802945346
Iter: 126 loss: 0.00796312466
Iter: 127 loss: 0.00794508494
Iter: 128 loss: 0.00800235663
Iter: 129 loss: 0.00793989748
Iter: 130 loss: 0.007917298
Iter: 131 loss: 0.00793215632
Iter: 132 loss: 0.00790291745
Iter: 133 loss: 0.00788304396
Iter: 134 loss: 0.00787925068
Iter: 135 loss: 0.0078658713
Iter: 136 loss: 0.00783011317
Iter: 137 loss: 0.00790046807
Iter: 138 loss: 0.00781389792
Iter: 139 loss: 0.00778525975
Iter: 140 loss: 0.00778524671
Iter: 141 loss: 0.0077632796
Iter: 142 loss: 0.00793498382
Iter: 143 loss: 0.0077617215
Iter: 144 loss: 0.00774367619
Iter: 145 loss: 0.00773705682
Iter: 146 loss: 0.00772600574
Iter: 147 loss: 0.0076807458
Iter: 148 loss: 0.0078752283
Iter: 149 loss: 0.00767152198
Iter: 150 loss: 0.00764035247
Iter: 151 loss: 0.00785155501
Iter: 152 loss: 0.00763673242
Iter: 153 loss: 0.00761175528
Iter: 154 loss: 0.00764637487
Iter: 155 loss: 0.00759929605
Iter: 156 loss: 0.00756807532
Iter: 157 loss: 0.00767061301
Iter: 158 loss: 0.00755895954
Iter: 159 loss: 0.0075448947
Iter: 160 loss: 0.007542721
Iter: 161 loss: 0.00752649177
Iter: 162 loss: 0.00754648214
Iter: 163 loss: 0.00751805212
Iter: 164 loss: 0.00749985827
Iter: 165 loss: 0.00748600671
Iter: 166 loss: 0.00748000201
Iter: 167 loss: 0.00745594827
Iter: 168 loss: 0.00781707279
Iter: 169 loss: 0.00745589053
Iter: 170 loss: 0.00743509503
Iter: 171 loss: 0.00761311
Iter: 172 loss: 0.00743405148
Iter: 173 loss: 0.00741285086
Iter: 174 loss: 0.00750282221
Iter: 175 loss: 0.00740731787
Iter: 176 loss: 0.00738137402
Iter: 177 loss: 0.00762554351
Iter: 178 loss: 0.00738030951
Iter: 179 loss: 0.00735325599
Iter: 180 loss: 0.00760274287
Iter: 181 loss: 0.00735136773
Iter: 182 loss: 0.00733082369
Iter: 183 loss: 0.00735511817
Iter: 184 loss: 0.00731991138
Iter: 185 loss: 0.00729469396
Iter: 186 loss: 0.00733154872
Iter: 187 loss: 0.00728249364
Iter: 188 loss: 0.00725508202
Iter: 189 loss: 0.00736051053
Iter: 190 loss: 0.00724777114
Iter: 191 loss: 0.00722471811
Iter: 192 loss: 0.00734914
Iter: 193 loss: 0.00722155767
Iter: 194 loss: 0.00720000546
Iter: 195 loss: 0.00733384816
Iter: 196 loss: 0.00719635608
Iter: 197 loss: 0.00718156528
Iter: 198 loss: 0.00730140693
Iter: 199 loss: 0.00718057249
Iter: 200 loss: 0.00716243824
Iter: 201 loss: 0.00724188285
Iter: 202 loss: 0.0071587367
Iter: 203 loss: 0.00714085158
Iter: 204 loss: 0.00717581855
Iter: 205 loss: 0.00713267969
Iter: 206 loss: 0.00711092
Iter: 207 loss: 0.00725109084
Iter: 208 loss: 0.00710850488
Iter: 209 loss: 0.00709080044
Iter: 210 loss: 0.0071003437
Iter: 211 loss: 0.00707920641
Iter: 212 loss: 0.00706058368
Iter: 213 loss: 0.00719363149
Iter: 214 loss: 0.0070582265
Iter: 215 loss: 0.00703908224
Iter: 216 loss: 0.00716995541
Iter: 217 loss: 0.00703721307
Iter: 218 loss: 0.00701897638
Iter: 219 loss: 0.00708798226
Iter: 220 loss: 0.00701310579
Iter: 221 loss: 0.00699520949
Iter: 222 loss: 0.00704097
Iter: 223 loss: 0.00698899245
Iter: 224 loss: 0.00697180163
Iter: 225 loss: 0.00709195528
Iter: 226 loss: 0.0069702277
Iter: 227 loss: 0.00695233513
Iter: 228 loss: 0.00696858857
Iter: 229 loss: 0.00694194343
Iter: 230 loss: 0.00692291372
Iter: 231 loss: 0.00694303587
Iter: 232 loss: 0.00691208523
Iter: 233 loss: 0.00689736195
Iter: 234 loss: 0.00689699408
Iter: 235 loss: 0.00687990244
Iter: 236 loss: 0.00687264
Iter: 237 loss: 0.00686368486
Iter: 238 loss: 0.00684130378
Iter: 239 loss: 0.00702398689
Iter: 240 loss: 0.0068397834
Iter: 241 loss: 0.00681849755
Iter: 242 loss: 0.00692859571
Iter: 243 loss: 0.00681498181
Iter: 244 loss: 0.00679840799
Iter: 245 loss: 0.00687170029
Iter: 246 loss: 0.00679440331
Iter: 247 loss: 0.00677293725
Iter: 248 loss: 0.00688739587
Iter: 249 loss: 0.00676966598
Iter: 250 loss: 0.00674907677
Iter: 251 loss: 0.00704831
Iter: 252 loss: 0.00674893335
Iter: 253 loss: 0.00672873
Iter: 254 loss: 0.00678059
Iter: 255 loss: 0.0067218258
Iter: 256 loss: 0.00670275744
Iter: 257 loss: 0.00668763183
Iter: 258 loss: 0.00668174401
Iter: 259 loss: 0.00664863409
Iter: 260 loss: 0.00669793366
Iter: 261 loss: 0.00663263444
Iter: 262 loss: 0.00660561
Iter: 263 loss: 0.00678580347
Iter: 264 loss: 0.00660255924
Iter: 265 loss: 0.00658278214
Iter: 266 loss: 0.00686673494
Iter: 267 loss: 0.00658275373
Iter: 268 loss: 0.00656284671
Iter: 269 loss: 0.00655377144
Iter: 270 loss: 0.00654341932
Iter: 271 loss: 0.0065197954
Iter: 272 loss: 0.00664693303
Iter: 273 loss: 0.00651612133
Iter: 274 loss: 0.00649439171
Iter: 275 loss: 0.00651008077
Iter: 276 loss: 0.0064805029
Iter: 277 loss: 0.00645043701
Iter: 278 loss: 0.00669156108
Iter: 279 loss: 0.00644845143
Iter: 280 loss: 0.00642738864
Iter: 281 loss: 0.00657913042
Iter: 282 loss: 0.00642559025
Iter: 283 loss: 0.00640412048
Iter: 284 loss: 0.00640960131
Iter: 285 loss: 0.00638842396
Iter: 286 loss: 0.00636246148
Iter: 287 loss: 0.00639273133
Iter: 288 loss: 0.00634855311
Iter: 289 loss: 0.00633603474
Iter: 290 loss: 0.0063338615
Iter: 291 loss: 0.0063200281
Iter: 292 loss: 0.00632393174
Iter: 293 loss: 0.00630975282
Iter: 294 loss: 0.00628308626
Iter: 295 loss: 0.0063299397
Iter: 296 loss: 0.00627128407
Iter: 297 loss: 0.00625283131
Iter: 298 loss: 0.00625005411
Iter: 299 loss: 0.00623265747
Iter: 300 loss: 0.00626477972
Iter: 301 loss: 0.00622514915
Iter: 302 loss: 0.00620720442
Iter: 303 loss: 0.00619024783
Iter: 304 loss: 0.00618609227
Iter: 305 loss: 0.00615788437
Iter: 306 loss: 0.00631719455
Iter: 307 loss: 0.00615377
Iter: 308 loss: 0.00612912793
Iter: 309 loss: 0.00620869827
Iter: 310 loss: 0.00612203544
Iter: 311 loss: 0.00609766599
Iter: 312 loss: 0.00618924387
Iter: 313 loss: 0.00609171763
Iter: 314 loss: 0.00607362
Iter: 315 loss: 0.00627372926
Iter: 316 loss: 0.00607321039
Iter: 317 loss: 0.00605633669
Iter: 318 loss: 0.00605122186
Iter: 319 loss: 0.00604037289
Iter: 320 loss: 0.00599262537
Iter: 321 loss: 0.0062345909
Iter: 322 loss: 0.00598465372
Iter: 323 loss: 0.00598607771
Iter: 324 loss: 0.00596541818
Iter: 325 loss: 0.00595162809
Iter: 326 loss: 0.00593225192
Iter: 327 loss: 0.00593147613
Iter: 328 loss: 0.00590669
Iter: 329 loss: 0.00597408228
Iter: 330 loss: 0.00589853711
Iter: 331 loss: 0.00587388687
Iter: 332 loss: 0.00626144465
Iter: 333 loss: 0.00587387756
Iter: 334 loss: 0.00585289113
Iter: 335 loss: 0.00584065355
Iter: 336 loss: 0.0058316458
Iter: 337 loss: 0.0057993317
Iter: 338 loss: 0.00582038239
Iter: 339 loss: 0.00577882025
Iter: 340 loss: 0.00574218761
Iter: 341 loss: 0.00610708259
Iter: 342 loss: 0.00574076269
Iter: 343 loss: 0.00571172871
Iter: 344 loss: 0.00585680315
Iter: 345 loss: 0.00570662227
Iter: 346 loss: 0.00568039343
Iter: 347 loss: 0.00580155291
Iter: 348 loss: 0.00567529304
Iter: 349 loss: 0.00565314665
Iter: 350 loss: 0.00577207189
Iter: 351 loss: 0.00564969
Iter: 352 loss: 0.00563115953
Iter: 353 loss: 0.00578712113
Iter: 354 loss: 0.00562989712
Iter: 355 loss: 0.00561335264
Iter: 356 loss: 0.00561716221
Iter: 357 loss: 0.0056012203
Iter: 358 loss: 0.00557101704
Iter: 359 loss: 0.00565231126
Iter: 360 loss: 0.00556048611
Iter: 361 loss: 0.00552571425
Iter: 362 loss: 0.00574591151
Iter: 363 loss: 0.00552162528
Iter: 364 loss: 0.00550427288
Iter: 365 loss: 0.00550341047
Iter: 366 loss: 0.0054876497
Iter: 367 loss: 0.00552247139
Iter: 368 loss: 0.00548157375
Iter: 369 loss: 0.00546345767
Iter: 370 loss: 0.00548751839
Iter: 371 loss: 0.00545428274
Iter: 372 loss: 0.00543656666
Iter: 373 loss: 0.00542067
Iter: 374 loss: 0.00541613065
Iter: 375 loss: 0.00538887177
Iter: 376 loss: 0.00547051802
Iter: 377 loss: 0.00538058579
Iter: 378 loss: 0.0053520184
Iter: 379 loss: 0.00539235
Iter: 380 loss: 0.00533768861
Iter: 381 loss: 0.00531057548
Iter: 382 loss: 0.00571936881
Iter: 383 loss: 0.00531056942
Iter: 384 loss: 0.00529022
Iter: 385 loss: 0.00541043235
Iter: 386 loss: 0.00528741814
Iter: 387 loss: 0.00526785571
Iter: 388 loss: 0.00532393437
Iter: 389 loss: 0.00526172761
Iter: 390 loss: 0.00524348347
Iter: 391 loss: 0.0052211117
Iter: 392 loss: 0.005219
Iter: 393 loss: 0.0051906053
Iter: 394 loss: 0.00531290658
Iter: 395 loss: 0.00518474542
Iter: 396 loss: 0.00516180461
Iter: 397 loss: 0.0053246757
Iter: 398 loss: 0.00515979622
Iter: 399 loss: 0.00514526
Iter: 400 loss: 0.00514525827
Iter: 401 loss: 0.00513253827
Iter: 402 loss: 0.00513906125
Iter: 403 loss: 0.00512409257
Iter: 404 loss: 0.00510792248
Iter: 405 loss: 0.00512849446
Iter: 406 loss: 0.00509963464
Iter: 407 loss: 0.00508312974
Iter: 408 loss: 0.00507849548
Iter: 409 loss: 0.00506818248
Iter: 410 loss: 0.00504224654
Iter: 411 loss: 0.00511940289
Iter: 412 loss: 0.0050342381
Iter: 413 loss: 0.0050093662
Iter: 414 loss: 0.00507332943
Iter: 415 loss: 0.0050006425
Iter: 416 loss: 0.00497629354
Iter: 417 loss: 0.00509128254
Iter: 418 loss: 0.00497148652
Iter: 419 loss: 0.0049586352
Iter: 420 loss: 0.00495623518
Iter: 421 loss: 0.00494566793
Iter: 422 loss: 0.0049239425
Iter: 423 loss: 0.00534239924
Iter: 424 loss: 0.00492354529
Iter: 425 loss: 0.00489332061
Iter: 426 loss: 0.00492555229
Iter: 427 loss: 0.0048766043
Iter: 428 loss: 0.00484843459
Iter: 429 loss: 0.0048484127
Iter: 430 loss: 0.00482573314
Iter: 431 loss: 0.00498532224
Iter: 432 loss: 0.00482328562
Iter: 433 loss: 0.00480681751
Iter: 434 loss: 0.00500349421
Iter: 435 loss: 0.00480660051
Iter: 436 loss: 0.00479045883
Iter: 437 loss: 0.00480613159
Iter: 438 loss: 0.00478123873
Iter: 439 loss: 0.00476417691
Iter: 440 loss: 0.00480372645
Iter: 441 loss: 0.0047574155
Iter: 442 loss: 0.00473599974
Iter: 443 loss: 0.00478391536
Iter: 444 loss: 0.00472789863
Iter: 445 loss: 0.00470492
Iter: 446 loss: 0.00497401785
Iter: 447 loss: 0.00470455829
Iter: 448 loss: 0.00468824059
Iter: 449 loss: 0.00471132156
Iter: 450 loss: 0.00468004402
Iter: 451 loss: 0.00466504414
Iter: 452 loss: 0.00476280134
Iter: 453 loss: 0.00466342643
Iter: 454 loss: 0.00464689266
Iter: 455 loss: 0.00464155944
Iter: 456 loss: 0.00463188812
Iter: 457 loss: 0.00461166259
Iter: 458 loss: 0.00466235075
Iter: 459 loss: 0.00460420735
Iter: 460 loss: 0.00458739651
Iter: 461 loss: 0.00458605774
Iter: 462 loss: 0.00457357708
Iter: 463 loss: 0.00454028789
Iter: 464 loss: 0.00467567192
Iter: 465 loss: 0.00453209691
Iter: 466 loss: 0.00450478401
Iter: 467 loss: 0.00491816
Iter: 468 loss: 0.00450474489
Iter: 469 loss: 0.00448503438
Iter: 470 loss: 0.00455241
Iter: 471 loss: 0.00447981432
Iter: 472 loss: 0.00446648523
Iter: 473 loss: 0.00446647778
Iter: 474 loss: 0.00445392914
Iter: 475 loss: 0.00448765792
Iter: 476 loss: 0.00444975216
Iter: 477 loss: 0.0044417344
Iter: 478 loss: 0.00443380652
Iter: 479 loss: 0.00443215715
Iter: 480 loss: 0.00441032834
Iter: 481 loss: 0.0044995
Iter: 482 loss: 0.00440500211
Iter: 483 loss: 0.00438845158
Iter: 484 loss: 0.00438843668
Iter: 485 loss: 0.004375902
Iter: 486 loss: 0.00450729253
Iter: 487 loss: 0.00437558815
Iter: 488 loss: 0.00436627958
Iter: 489 loss: 0.00435564946
Iter: 490 loss: 0.00435432559
Iter: 491 loss: 0.00433754316
Iter: 492 loss: 0.00438074861
Iter: 493 loss: 0.00433178898
Iter: 494 loss: 0.00431387033
Iter: 495 loss: 0.00437800586
Iter: 496 loss: 0.00430910056
Iter: 497 loss: 0.00429302128
Iter: 498 loss: 0.00430367235
Iter: 499 loss: 0.00428288057
Iter: 500 loss: 0.00426326972
Iter: 501 loss: 0.00431095948
Iter: 502 loss: 0.00425619865
Iter: 503 loss: 0.00424146745
Iter: 504 loss: 0.00445022853
Iter: 505 loss: 0.00424144
Iter: 506 loss: 0.00422679773
Iter: 507 loss: 0.00428483542
Iter: 508 loss: 0.00422345055
Iter: 509 loss: 0.00421037897
Iter: 510 loss: 0.0042544161
Iter: 511 loss: 0.00420688419
Iter: 512 loss: 0.00419637933
Iter: 513 loss: 0.00418074802
Iter: 514 loss: 0.00418034755
Iter: 515 loss: 0.00415776297
Iter: 516 loss: 0.00431452785
Iter: 517 loss: 0.00415563583
Iter: 518 loss: 0.00414215308
Iter: 519 loss: 0.00414212234
Iter: 520 loss: 0.00413126592
Iter: 521 loss: 0.00415304489
Iter: 522 loss: 0.00412684958
Iter: 523 loss: 0.00411565043
Iter: 524 loss: 0.00410577888
Iter: 525 loss: 0.00410283264
Iter: 526 loss: 0.00408599339
Iter: 527 loss: 0.00419151224
Iter: 528 loss: 0.00408404367
Iter: 529 loss: 0.00407071505
Iter: 530 loss: 0.0040758187
Iter: 531 loss: 0.00406137295
Iter: 532 loss: 0.00404358655
Iter: 533 loss: 0.00406596623
Iter: 534 loss: 0.00403426634
Iter: 535 loss: 0.00401422195
Iter: 536 loss: 0.00404875632
Iter: 537 loss: 0.00400522351
Iter: 538 loss: 0.00399302645
Iter: 539 loss: 0.00399179664
Iter: 540 loss: 0.00398144
Iter: 541 loss: 0.00403706916
Iter: 542 loss: 0.0039798608
Iter: 543 loss: 0.00397179509
Iter: 544 loss: 0.0039836904
Iter: 545 loss: 0.00396789098
Iter: 546 loss: 0.00395894051
Iter: 547 loss: 0.00395700289
Iter: 548 loss: 0.00395091809
Iter: 549 loss: 0.00393256545
Iter: 550 loss: 0.00395490229
Iter: 551 loss: 0.00392291788
Iter: 552 loss: 0.00390327978
Iter: 553 loss: 0.00390326791
Iter: 554 loss: 0.00389174232
Iter: 555 loss: 0.00389146782
Iter: 556 loss: 0.00388380163
Iter: 557 loss: 0.00387127185
Iter: 558 loss: 0.00387121364
Iter: 559 loss: 0.00385734765
Iter: 560 loss: 0.00391423889
Iter: 561 loss: 0.00385435345
Iter: 562 loss: 0.00384132355
Iter: 563 loss: 0.00392527832
Iter: 564 loss: 0.00383985601
Iter: 565 loss: 0.00382849411
Iter: 566 loss: 0.00382899446
Iter: 567 loss: 0.00381953828
Iter: 568 loss: 0.003802591
Iter: 569 loss: 0.00385209359
Iter: 570 loss: 0.00379733089
Iter: 571 loss: 0.00378300366
Iter: 572 loss: 0.00386999035
Iter: 573 loss: 0.00378059642
Iter: 574 loss: 0.00376327662
Iter: 575 loss: 0.00399033446
Iter: 576 loss: 0.00376313948
Iter: 577 loss: 0.00374798453
Iter: 578 loss: 0.0037568796
Iter: 579 loss: 0.00373789296
Iter: 580 loss: 0.00373067381
Iter: 581 loss: 0.00372962793
Iter: 582 loss: 0.00372097408
Iter: 583 loss: 0.00373269897
Iter: 584 loss: 0.00371670839
Iter: 585 loss: 0.00370473135
Iter: 586 loss: 0.0037229422
Iter: 587 loss: 0.00369872246
Iter: 588 loss: 0.00369146536
Iter: 589 loss: 0.00369044067
Iter: 590 loss: 0.00368278497
Iter: 591 loss: 0.00368785299
Iter: 592 loss: 0.00367794326
Iter: 593 loss: 0.00366647937
Iter: 594 loss: 0.00365892937
Iter: 595 loss: 0.00365441223
Iter: 596 loss: 0.00363858
Iter: 597 loss: 0.00378198619
Iter: 598 loss: 0.00363784237
Iter: 599 loss: 0.00362375379
Iter: 600 loss: 0.00370507548
Iter: 601 loss: 0.00362187135
Iter: 602 loss: 0.00360957952
Iter: 603 loss: 0.00360865844
Iter: 604 loss: 0.00359937083
Iter: 605 loss: 0.00358325383
Iter: 606 loss: 0.00361996051
Iter: 607 loss: 0.00357713317
Iter: 608 loss: 0.00356243784
Iter: 609 loss: 0.00363612641
Iter: 610 loss: 0.00356000941
Iter: 611 loss: 0.00354533317
Iter: 612 loss: 0.00364453439
Iter: 613 loss: 0.00354381278
Iter: 614 loss: 0.00353385787
Iter: 615 loss: 0.0036295834
Iter: 616 loss: 0.00353348372
Iter: 617 loss: 0.00352575956
Iter: 618 loss: 0.00352380797
Iter: 619 loss: 0.00351887313
Iter: 620 loss: 0.00351069099
Iter: 621 loss: 0.00357915531
Iter: 622 loss: 0.00351020927
Iter: 623 loss: 0.00350192958
Iter: 624 loss: 0.00351410359
Iter: 625 loss: 0.00349791651
Iter: 626 loss: 0.00349009014
Iter: 627 loss: 0.00349092693
Iter: 628 loss: 0.00348403351
Iter: 629 loss: 0.00347422156
Iter: 630 loss: 0.00346624898
Iter: 631 loss: 0.0034633521
Iter: 632 loss: 0.00344680669
Iter: 633 loss: 0.00352817122
Iter: 634 loss: 0.00344392704
Iter: 635 loss: 0.00343018654
Iter: 636 loss: 0.00354175665
Iter: 637 loss: 0.00342928478
Iter: 638 loss: 0.00341702765
Iter: 639 loss: 0.00342137273
Iter: 640 loss: 0.00340844062
Iter: 641 loss: 0.00339324283
Iter: 642 loss: 0.00340321613
Iter: 643 loss: 0.00338362809
Iter: 644 loss: 0.0033679537
Iter: 645 loss: 0.00340062962
Iter: 646 loss: 0.00336174225
Iter: 647 loss: 0.00335542252
Iter: 648 loss: 0.00335313939
Iter: 649 loss: 0.00334403268
Iter: 650 loss: 0.00334518868
Iter: 651 loss: 0.00333708175
Iter: 652 loss: 0.00332987076
Iter: 653 loss: 0.00335780112
Iter: 654 loss: 0.00332812266
Iter: 655 loss: 0.00331927487
Iter: 656 loss: 0.00333824521
Iter: 657 loss: 0.00331579987
Iter: 658 loss: 0.00330733974
Iter: 659 loss: 0.00334136887
Iter: 660 loss: 0.00330542726
Iter: 661 loss: 0.00329725049
Iter: 662 loss: 0.00328908907
Iter: 663 loss: 0.00328740501
Iter: 664 loss: 0.00326938089
Iter: 665 loss: 0.00331041426
Iter: 666 loss: 0.00326237874
Iter: 667 loss: 0.00324531621
Iter: 668 loss: 0.00330828875
Iter: 669 loss: 0.00324114296
Iter: 670 loss: 0.00322688557
Iter: 671 loss: 0.00333150197
Iter: 672 loss: 0.00322577567
Iter: 673 loss: 0.00321270898
Iter: 674 loss: 0.00324577861
Iter: 675 loss: 0.00320814247
Iter: 676 loss: 0.00319634029
Iter: 677 loss: 0.00319337752
Iter: 678 loss: 0.0031860061
Iter: 679 loss: 0.0031696693
Iter: 680 loss: 0.00324111083
Iter: 681 loss: 0.0031663063
Iter: 682 loss: 0.00316115562
Iter: 683 loss: 0.00315853069
Iter: 684 loss: 0.00315143773
Iter: 685 loss: 0.00313824438
Iter: 686 loss: 0.00343591231
Iter: 687 loss: 0.00313823135
Iter: 688 loss: 0.0031294371
Iter: 689 loss: 0.0031289719
Iter: 690 loss: 0.00312050851
Iter: 691 loss: 0.00316901086
Iter: 692 loss: 0.00311928289
Iter: 693 loss: 0.00311077712
Iter: 694 loss: 0.00310910819
Iter: 695 loss: 0.00310340314
Iter: 696 loss: 0.00309002702
Iter: 697 loss: 0.00309067965
Iter: 698 loss: 0.00307943532
Iter: 699 loss: 0.00306324335
Iter: 700 loss: 0.00309467549
Iter: 701 loss: 0.00305653759
Iter: 702 loss: 0.00303818122
Iter: 703 loss: 0.00314232847
Iter: 704 loss: 0.00303565455
Iter: 705 loss: 0.00302266376
Iter: 706 loss: 0.00305631431
Iter: 707 loss: 0.00301811681
Iter: 708 loss: 0.00300315279
Iter: 709 loss: 0.00307997083
Iter: 710 loss: 0.00300076907
Iter: 711 loss: 0.00298766047
Iter: 712 loss: 0.00302875391
Iter: 713 loss: 0.00298377988
Iter: 714 loss: 0.00297058048
Iter: 715 loss: 0.00303165661
Iter: 716 loss: 0.00296815182
Iter: 717 loss: 0.00295918807
Iter: 718 loss: 0.00306755956
Iter: 719 loss: 0.00295909075
Iter: 720 loss: 0.00295143668
Iter: 721 loss: 0.0029745677
Iter: 722 loss: 0.00294912932
Iter: 723 loss: 0.00294338726
Iter: 724 loss: 0.00293681258
Iter: 725 loss: 0.0029360007
Iter: 726 loss: 0.00293279882
Iter: 727 loss: 0.00293066166
Iter: 728 loss: 0.00292734196
Iter: 729 loss: 0.00292707
Iter: 730 loss: 0.00292461
Iter: 731 loss: 0.00291847158
Iter: 732 loss: 0.00291721127
Iter: 733 loss: 0.00291314069
Iter: 734 loss: 0.00290484843
Iter: 735 loss: 0.00290024141
Iter: 736 loss: 0.00289656292
Iter: 737 loss: 0.002882923
Iter: 738 loss: 0.00291526341
Iter: 739 loss: 0.00287792692
Iter: 740 loss: 0.00286437524
Iter: 741 loss: 0.0029212907
Iter: 742 loss: 0.00286149676
Iter: 743 loss: 0.00284819701
Iter: 744 loss: 0.00285697775
Iter: 745 loss: 0.00283985189
Iter: 746 loss: 0.00282295048
Iter: 747 loss: 0.00290511944
Iter: 748 loss: 0.00281976862
Iter: 749 loss: 0.00280534034
Iter: 750 loss: 0.0028681478
Iter: 751 loss: 0.002802453
Iter: 752 loss: 0.00279045803
Iter: 753 loss: 0.00285055884
Iter: 754 loss: 0.00278841378
Iter: 755 loss: 0.00277869706
Iter: 756 loss: 0.0028403257
Iter: 757 loss: 0.00277758902
Iter: 758 loss: 0.00276844273
Iter: 759 loss: 0.00279606692
Iter: 760 loss: 0.00276571652
Iter: 761 loss: 0.00275828829
Iter: 762 loss: 0.00276941899
Iter: 763 loss: 0.00275472132
Iter: 764 loss: 0.00274687051
Iter: 765 loss: 0.00279636867
Iter: 766 loss: 0.00274593523
Iter: 767 loss: 0.00273913867
Iter: 768 loss: 0.00276064919
Iter: 769 loss: 0.00273713935
Iter: 770 loss: 0.00272972
Iter: 771 loss: 0.00277812686
Iter: 772 loss: 0.00272893952
Iter: 773 loss: 0.00272448268
Iter: 774 loss: 0.00271575944
Iter: 775 loss: 0.00288899802
Iter: 776 loss: 0.00271568447
Iter: 777 loss: 0.00269907946
Iter: 778 loss: 0.0028020693
Iter: 779 loss: 0.00269703753
Iter: 780 loss: 0.0026850421
Iter: 781 loss: 0.00284149242
Iter: 782 loss: 0.00268494966
Iter: 783 loss: 0.00267705484
Iter: 784 loss: 0.00270981342
Iter: 785 loss: 0.0026753773
Iter: 786 loss: 0.00266813301
Iter: 787 loss: 0.00268539507
Iter: 788 loss: 0.00266549783
Iter: 789 loss: 0.00265867636
Iter: 790 loss: 0.00272414158
Iter: 791 loss: 0.00265841652
Iter: 792 loss: 0.00265271915
Iter: 793 loss: 0.00267433212
Iter: 794 loss: 0.00265137199
Iter: 795 loss: 0.00264599919
Iter: 796 loss: 0.00264302175
Iter: 797 loss: 0.0026406548
Iter: 798 loss: 0.00263352296
Iter: 799 loss: 0.00269645639
Iter: 800 loss: 0.00263313157
Iter: 801 loss: 0.00262663257
Iter: 802 loss: 0.00265476247
Iter: 803 loss: 0.00262526213
Iter: 804 loss: 0.00262063649
Iter: 805 loss: 0.00266532879
Iter: 806 loss: 0.00262047118
Iter: 807 loss: 0.00261588674
Iter: 808 loss: 0.00260972767
Iter: 809 loss: 0.00260939635
Iter: 810 loss: 0.0026019041
Iter: 811 loss: 0.00260337861
Iter: 812 loss: 0.002596315
Iter: 813 loss: 0.0025855829
Iter: 814 loss: 0.00261483132
Iter: 815 loss: 0.00258205738
Iter: 816 loss: 0.00256956927
Iter: 817 loss: 0.00259923469
Iter: 818 loss: 0.00256502768
Iter: 819 loss: 0.002555507
Iter: 820 loss: 0.00261974288
Iter: 821 loss: 0.00255457894
Iter: 822 loss: 0.0025458103
Iter: 823 loss: 0.00256819068
Iter: 824 loss: 0.00254274206
Iter: 825 loss: 0.00253505819
Iter: 826 loss: 0.00262070633
Iter: 827 loss: 0.00253492268
Iter: 828 loss: 0.00252928119
Iter: 829 loss: 0.00254485616
Iter: 830 loss: 0.00252744765
Iter: 831 loss: 0.00251998962
Iter: 832 loss: 0.0025301096
Iter: 833 loss: 0.00251626084
Iter: 834 loss: 0.00250846916
Iter: 835 loss: 0.00252469024
Iter: 836 loss: 0.00250534946
Iter: 837 loss: 0.00249904976
Iter: 838 loss: 0.00255326671
Iter: 839 loss: 0.00249872403
Iter: 840 loss: 0.00249265949
Iter: 841 loss: 0.00250168098
Iter: 842 loss: 0.00248972909
Iter: 843 loss: 0.00248422124
Iter: 844 loss: 0.0025673036
Iter: 845 loss: 0.00248422287
Iter: 846 loss: 0.00248063216
Iter: 847 loss: 0.00247217808
Iter: 848 loss: 0.00257325801
Iter: 849 loss: 0.00247146096
Iter: 850 loss: 0.00246314658
Iter: 851 loss: 0.00247612339
Iter: 852 loss: 0.00245925691
Iter: 853 loss: 0.00244954601
Iter: 854 loss: 0.0025182846
Iter: 855 loss: 0.00244866358
Iter: 856 loss: 0.00244065723
Iter: 857 loss: 0.00245272531
Iter: 858 loss: 0.00243685395
Iter: 859 loss: 0.00242786924
Iter: 860 loss: 0.00247391406
Iter: 861 loss: 0.00242644385
Iter: 862 loss: 0.00241827429
Iter: 863 loss: 0.00246737339
Iter: 864 loss: 0.00241723331
Iter: 865 loss: 0.00240926025
Iter: 866 loss: 0.00243684789
Iter: 867 loss: 0.00240718946
Iter: 868 loss: 0.00240006787
Iter: 869 loss: 0.0024174971
Iter: 870 loss: 0.00239753234
Iter: 871 loss: 0.00239060912
Iter: 872 loss: 0.00240265788
Iter: 873 loss: 0.00238754344
Iter: 874 loss: 0.00238316855
Iter: 875 loss: 0.00238307193
Iter: 876 loss: 0.00237956503
Iter: 877 loss: 0.00238165958
Iter: 878 loss: 0.00237728935
Iter: 879 loss: 0.00237262482
Iter: 880 loss: 0.00239851559
Iter: 881 loss: 0.00237195
Iter: 882 loss: 0.00236875284
Iter: 883 loss: 0.00236385735
Iter: 884 loss: 0.00236377958
Iter: 885 loss: 0.00235595275
Iter: 886 loss: 0.00236481428
Iter: 887 loss: 0.00235159323
Iter: 888 loss: 0.00234270329
Iter: 889 loss: 0.00240058685
Iter: 890 loss: 0.00234178617
Iter: 891 loss: 0.0023332797
Iter: 892 loss: 0.00234413915
Iter: 893 loss: 0.00232896511
Iter: 894 loss: 0.00232244888
Iter: 895 loss: 0.0023912848
Iter: 896 loss: 0.00232228194
Iter: 897 loss: 0.00231692
Iter: 898 loss: 0.00235039182
Iter: 899 loss: 0.00231630728
Iter: 900 loss: 0.00231198152
Iter: 901 loss: 0.0023223348
Iter: 902 loss: 0.00231041317
Iter: 903 loss: 0.00230553793
Iter: 904 loss: 0.00231010769
Iter: 905 loss: 0.00230272627
Iter: 906 loss: 0.00229770388
Iter: 907 loss: 0.0023022613
Iter: 908 loss: 0.00229478674
Iter: 909 loss: 0.00229316927
Iter: 910 loss: 0.00229191338
Iter: 911 loss: 0.00228939275
Iter: 912 loss: 0.00228664372
Iter: 913 loss: 0.00228624046
Iter: 914 loss: 0.00228116568
Iter: 915 loss: 0.00231799344
Iter: 916 loss: 0.00228070701
Iter: 917 loss: 0.00227726973
Iter: 918 loss: 0.00227735448
Iter: 919 loss: 0.00227445643
Iter: 920 loss: 0.00226697978
Iter: 921 loss: 0.00225935271
Iter: 922 loss: 0.00225789216
Iter: 923 loss: 0.00224676915
Iter: 924 loss: 0.0023442558
Iter: 925 loss: 0.00224611163
Iter: 926 loss: 0.00223866198
Iter: 927 loss: 0.00229529757
Iter: 928 loss: 0.00223810924
Iter: 929 loss: 0.00223241374
Iter: 930 loss: 0.00226221909
Iter: 931 loss: 0.00223155785
Iter: 932 loss: 0.00222531613
Iter: 933 loss: 0.00224524667
Iter: 934 loss: 0.00222352473
Iter: 935 loss: 0.00221820502
Iter: 936 loss: 0.0022311327
Iter: 937 loss: 0.00221628603
Iter: 938 loss: 0.00221118564
Iter: 939 loss: 0.00222013844
Iter: 940 loss: 0.00220894301
Iter: 941 loss: 0.00220431015
Iter: 942 loss: 0.00223232154
Iter: 943 loss: 0.00220370595
Iter: 944 loss: 0.00219753641
Iter: 945 loss: 0.00221623713
Iter: 946 loss: 0.0021956889
Iter: 947 loss: 0.00219184859
Iter: 948 loss: 0.00220493414
Iter: 949 loss: 0.00219079852
Iter: 950 loss: 0.00218697684
Iter: 951 loss: 0.00218844274
Iter: 952 loss: 0.00218431978
Iter: 953 loss: 0.0021793812
Iter: 954 loss: 0.00217454275
Iter: 955 loss: 0.00217349734
Iter: 956 loss: 0.00216508238
Iter: 957 loss: 0.00218319846
Iter: 958 loss: 0.00216182461
Iter: 959 loss: 0.00215211278
Iter: 960 loss: 0.00217145472
Iter: 961 loss: 0.00214814954
Iter: 962 loss: 0.00213952479
Iter: 963 loss: 0.00218986417
Iter: 964 loss: 0.00213839952
Iter: 965 loss: 0.00213206257
Iter: 966 loss: 0.00213126559
Iter: 967 loss: 0.00212672679
Iter: 968 loss: 0.00212293724
Iter: 969 loss: 0.00212209765
Iter: 970 loss: 0.00211788504
Iter: 971 loss: 0.00211988273
Iter: 972 loss: 0.00211503287
Iter: 973 loss: 0.00211116066
Iter: 974 loss: 0.00211105682
Iter: 975 loss: 0.00210779393
Iter: 976 loss: 0.00210938044
Iter: 977 loss: 0.00210561836
Iter: 978 loss: 0.00210072566
Iter: 979 loss: 0.00211914117
Iter: 980 loss: 0.00209954521
Iter: 981 loss: 0.00209580129
Iter: 982 loss: 0.00209553563
Iter: 983 loss: 0.00209272
Iter: 984 loss: 0.00208643451
Iter: 985 loss: 0.00209172955
Iter: 986 loss: 0.00208270829
Iter: 987 loss: 0.00207738765
Iter: 988 loss: 0.00207990361
Iter: 989 loss: 0.00207381393
Iter: 990 loss: 0.00206715311
Iter: 991 loss: 0.00209299708
Iter: 992 loss: 0.00206555403
Iter: 993 loss: 0.00205863174
Iter: 994 loss: 0.00207463349
Iter: 995 loss: 0.00205604639
Iter: 996 loss: 0.00204851385
Iter: 997 loss: 0.00207433663
Iter: 998 loss: 0.00204653176
Iter: 999 loss: 0.00204062182
Iter: 1000 loss: 0.00208532065
Iter: 1001 loss: 0.00204015058
Iter: 1002 loss: 0.00203608535
Iter: 1003 loss: 0.00208665431
Iter: 1004 loss: 0.00203604205
Iter: 1005 loss: 0.00203311536
Iter: 1006 loss: 0.00203031534
Iter: 1007 loss: 0.00202964759
Iter: 1008 loss: 0.00202620146
Iter: 1009 loss: 0.00202586362
Iter: 1010 loss: 0.00202366291
Iter: 1011 loss: 0.00202276814
Iter: 1012 loss: 0.00202160515
Iter: 1013 loss: 0.00201750454
Iter: 1014 loss: 0.00203322968
Iter: 1015 loss: 0.0020165341
Iter: 1016 loss: 0.00201305328
Iter: 1017 loss: 0.00201285863
Iter: 1018 loss: 0.0020102025
Iter: 1019 loss: 0.00200507534
Iter: 1020 loss: 0.00200261455
Iter: 1021 loss: 0.00200014515
Iter: 1022 loss: 0.00199355651
Iter: 1023 loss: 0.00200042664
Iter: 1024 loss: 0.00198990246
Iter: 1025 loss: 0.00198167702
Iter: 1026 loss: 0.00200907444
Iter: 1027 loss: 0.00197944674
Iter: 1028 loss: 0.00197232654
Iter: 1029 loss: 0.00201159366
Iter: 1030 loss: 0.00197130302
Iter: 1031 loss: 0.00196584035
Iter: 1032 loss: 0.00198585703
Iter: 1033 loss: 0.00196446199
Iter: 1034 loss: 0.00195951853
Iter: 1035 loss: 0.00199111202
Iter: 1036 loss: 0.00195898348
Iter: 1037 loss: 0.0019549015
Iter: 1038 loss: 0.00198200531
Iter: 1039 loss: 0.0019544838
Iter: 1040 loss: 0.00195219065
Iter: 1041 loss: 0.00196346408
Iter: 1042 loss: 0.0019517804
Iter: 1043 loss: 0.00194837467
Iter: 1044 loss: 0.00194580457
Iter: 1045 loss: 0.00194469083
Iter: 1046 loss: 0.00194182293
Iter: 1047 loss: 0.00198529451
Iter: 1048 loss: 0.00194181746
Iter: 1049 loss: 0.00193940056
Iter: 1050 loss: 0.00193917146
Iter: 1051 loss: 0.00193739648
Iter: 1052 loss: 0.00193366816
Iter: 1053 loss: 0.00193118141
Iter: 1054 loss: 0.00192977535
Iter: 1055 loss: 0.00192410219
Iter: 1056 loss: 0.00193819334
Iter: 1057 loss: 0.00192208297
Iter: 1058 loss: 0.00191662216
Iter: 1059 loss: 0.00192744634
Iter: 1060 loss: 0.00191438012
Iter: 1061 loss: 0.00190855714
Iter: 1062 loss: 0.00192329602
Iter: 1063 loss: 0.0019065364
Iter: 1064 loss: 0.00190046465
Iter: 1065 loss: 0.00191244413
Iter: 1066 loss: 0.00189795962
Iter: 1067 loss: 0.00189306075
Iter: 1068 loss: 0.00195145165
Iter: 1069 loss: 0.00189298601
Iter: 1070 loss: 0.00188947842
Iter: 1071 loss: 0.001901777
Iter: 1072 loss: 0.00188858213
Iter: 1073 loss: 0.00188493053
Iter: 1074 loss: 0.00189541536
Iter: 1075 loss: 0.00188378431
Iter: 1076 loss: 0.00188184343
Iter: 1077 loss: 0.00188167603
Iter: 1078 loss: 0.0018799135
Iter: 1079 loss: 0.00187578332
Iter: 1080 loss: 0.0019253823
Iter: 1081 loss: 0.00187544955
Iter: 1082 loss: 0.00187086524
Iter: 1083 loss: 0.00187634432
Iter: 1084 loss: 0.00186844636
Iter: 1085 loss: 0.00186949759
Iter: 1086 loss: 0.00186655228
Iter: 1087 loss: 0.00186488195
Iter: 1088 loss: 0.00186044793
Iter: 1089 loss: 0.00189159461
Iter: 1090 loss: 0.00185946678
Iter: 1091 loss: 0.00185300992
Iter: 1092 loss: 0.00187340123
Iter: 1093 loss: 0.00185103645
Iter: 1094 loss: 0.00184641429
Iter: 1095 loss: 0.00187730812
Iter: 1096 loss: 0.0018459463
Iter: 1097 loss: 0.00184174371
Iter: 1098 loss: 0.00184731814
Iter: 1099 loss: 0.00183960854
Iter: 1100 loss: 0.00183582061
Iter: 1101 loss: 0.00183476112
Iter: 1102 loss: 0.0018324391
Iter: 1103 loss: 0.00182719203
Iter: 1104 loss: 0.00184209482
Iter: 1105 loss: 0.00182552799
Iter: 1106 loss: 0.0018222651
Iter: 1107 loss: 0.00182207266
Iter: 1108 loss: 0.00181941502
Iter: 1109 loss: 0.00181596691
Iter: 1110 loss: 0.00181571301
Iter: 1111 loss: 0.00181290181
Iter: 1112 loss: 0.00181263906
Iter: 1113 loss: 0.00180971809
Iter: 1114 loss: 0.00181636272
Iter: 1115 loss: 0.00180861936
Iter: 1116 loss: 0.00180522737
Iter: 1117 loss: 0.00183427788
Iter: 1118 loss: 0.00180503528
Iter: 1119 loss: 0.00180088298
Iter: 1120 loss: 0.00180805847
Iter: 1121 loss: 0.00179906446
Iter: 1122 loss: 0.00179607759
Iter: 1123 loss: 0.00179451925
Iter: 1124 loss: 0.00179315964
Iter: 1125 loss: 0.00178962969
Iter: 1126 loss: 0.00179250212
Iter: 1127 loss: 0.00178748392
Iter: 1128 loss: 0.00178422511
Iter: 1129 loss: 0.00179051317
Iter: 1130 loss: 0.00178286596
Iter: 1131 loss: 0.00177971949
Iter: 1132 loss: 0.00178472977
Iter: 1133 loss: 0.00177825475
Iter: 1134 loss: 0.00177458883
Iter: 1135 loss: 0.00179168617
Iter: 1136 loss: 0.00177390594
Iter: 1137 loss: 0.00176991581
Iter: 1138 loss: 0.0017694619
Iter: 1139 loss: 0.00176657294
Iter: 1140 loss: 0.00176320947
Iter: 1141 loss: 0.00176303834
Iter: 1142 loss: 0.00176067394
Iter: 1143 loss: 0.00176729646
Iter: 1144 loss: 0.00175992539
Iter: 1145 loss: 0.00175740779
Iter: 1146 loss: 0.00176661182
Iter: 1147 loss: 0.00175678893
Iter: 1148 loss: 0.00175449532
Iter: 1149 loss: 0.00175596052
Iter: 1150 loss: 0.00175303826
Iter: 1151 loss: 0.00175023987
Iter: 1152 loss: 0.00176714337
Iter: 1153 loss: 0.00174990133
Iter: 1154 loss: 0.00174785568
Iter: 1155 loss: 0.00174633064
Iter: 1156 loss: 0.0017456502
Iter: 1157 loss: 0.00174186239
Iter: 1158 loss: 0.00174398674
Iter: 1159 loss: 0.00173942861
Iter: 1160 loss: 0.00173512311
Iter: 1161 loss: 0.00175443362
Iter: 1162 loss: 0.00173428096
Iter: 1163 loss: 0.00173137686
Iter: 1164 loss: 0.00172954029
Iter: 1165 loss: 0.00172840769
Iter: 1166 loss: 0.001724064
Iter: 1167 loss: 0.00173396908
Iter: 1168 loss: 0.0017224378
Iter: 1169 loss: 0.00171815208
Iter: 1170 loss: 0.00173408585
Iter: 1171 loss: 0.0017171033
Iter: 1172 loss: 0.00171318569
Iter: 1173 loss: 0.00172612537
Iter: 1174 loss: 0.00171210046
Iter: 1175 loss: 0.00170921709
Iter: 1176 loss: 0.0017295992
Iter: 1177 loss: 0.00170895422
Iter: 1178 loss: 0.0017066655
Iter: 1179 loss: 0.00172281452
Iter: 1180 loss: 0.00170645583
Iter: 1181 loss: 0.00170402019
Iter: 1182 loss: 0.00171692972
Iter: 1183 loss: 0.00170363672
Iter: 1184 loss: 0.00170200423
Iter: 1185 loss: 0.00170612743
Iter: 1186 loss: 0.00170143531
Iter: 1187 loss: 0.00169981096
Iter: 1188 loss: 0.00170017011
Iter: 1189 loss: 0.00169861014
Iter: 1190 loss: 0.00169650442
Iter: 1191 loss: 0.00170671614
Iter: 1192 loss: 0.00169613794
Iter: 1193 loss: 0.00169400393
Iter: 1194 loss: 0.00169456925
Iter: 1195 loss: 0.00169245957
Iter: 1196 loss: 0.00168982137
Iter: 1197 loss: 0.00169148226
Iter: 1198 loss: 0.00168813369
Iter: 1199 loss: 0.00168440246
Iter: 1200 loss: 0.00168539863
Iter: 1201 loss: 0.0016816922
Iter: 1202 loss: 0.00167682697
Iter: 1203 loss: 0.00169988279
Iter: 1204 loss: 0.00167593546
Iter: 1205 loss: 0.00167182065
Iter: 1206 loss: 0.00168197835
Iter: 1207 loss: 0.00167037197
Iter: 1208 loss: 0.00166614074
Iter: 1209 loss: 0.00168127415
Iter: 1210 loss: 0.00166505668
Iter: 1211 loss: 0.00166153163
Iter: 1212 loss: 0.00166432769
Iter: 1213 loss: 0.00165939564
Iter: 1214 loss: 0.00165535696
Iter: 1215 loss: 0.00168604287
Iter: 1216 loss: 0.00165504811
Iter: 1217 loss: 0.00165178708
Iter: 1218 loss: 0.00167263497
Iter: 1219 loss: 0.00165142422
Iter: 1220 loss: 0.00164792151
Iter: 1221 loss: 0.0016557842
Iter: 1222 loss: 0.00164659927
Iter: 1223 loss: 0.00164347619
Iter: 1224 loss: 0.00165255554
Iter: 1225 loss: 0.00164251169
Iter: 1226 loss: 0.00164053589
Iter: 1227 loss: 0.0016405161
Iter: 1228 loss: 0.00163916778
Iter: 1229 loss: 0.00164222112
Iter: 1230 loss: 0.00163867
Iter: 1231 loss: 0.00163690257
Iter: 1232 loss: 0.00163447496
Iter: 1233 loss: 0.0016343646
Iter: 1234 loss: 0.00163172116
Iter: 1235 loss: 0.0016334384
Iter: 1236 loss: 0.00163004315
Iter: 1237 loss: 0.0016270068
Iter: 1238 loss: 0.00163032452
Iter: 1239 loss: 0.00162534474
Iter: 1240 loss: 0.00162181107
Iter: 1241 loss: 0.00162839575
Iter: 1242 loss: 0.00162029138
Iter: 1243 loss: 0.00161689834
Iter: 1244 loss: 0.00162918156
Iter: 1245 loss: 0.00161604665
Iter: 1246 loss: 0.00161246071
Iter: 1247 loss: 0.00162229291
Iter: 1248 loss: 0.00161129213
Iter: 1249 loss: 0.00160817849
Iter: 1250 loss: 0.00162033865
Iter: 1251 loss: 0.00160745485
Iter: 1252 loss: 0.00160493702
Iter: 1253 loss: 0.00162278314
Iter: 1254 loss: 0.0016047192
Iter: 1255 loss: 0.00160295726
Iter: 1256 loss: 0.00162571366
Iter: 1257 loss: 0.00160294469
Iter: 1258 loss: 0.00160165783
Iter: 1259 loss: 0.00159983546
Iter: 1260 loss: 0.00159976888
Iter: 1261 loss: 0.00159734115
Iter: 1262 loss: 0.00163187366
Iter: 1263 loss: 0.00159733417
Iter: 1264 loss: 0.00159556489
Iter: 1265 loss: 0.00159892673
Iter: 1266 loss: 0.00159482006
Iter: 1267 loss: 0.00159289106
Iter: 1268 loss: 0.0015928332
Iter: 1269 loss: 0.00159132737
Iter: 1270 loss: 0.00158929289
Iter: 1271 loss: 0.00158610207
Iter: 1272 loss: 0.00158606726
Iter: 1273 loss: 0.00158209819
Iter: 1274 loss: 0.00160433119
Iter: 1275 loss: 0.00158152892
Iter: 1276 loss: 0.00157829351
Iter: 1277 loss: 0.00158449286
Iter: 1278 loss: 0.00157694216
Iter: 1279 loss: 0.00157349929
Iter: 1280 loss: 0.00157978944
Iter: 1281 loss: 0.00157201523
Iter: 1282 loss: 0.00156895572
Iter: 1283 loss: 0.00158625212
Iter: 1284 loss: 0.00156853045
Iter: 1285 loss: 0.0015655579
Iter: 1286 loss: 0.00156933104
Iter: 1287 loss: 0.00156403193
Iter: 1288 loss: 0.00156116392
Iter: 1289 loss: 0.00159235555
Iter: 1290 loss: 0.00156110199
Iter: 1291 loss: 0.00155852025
Iter: 1292 loss: 0.0015707023
Iter: 1293 loss: 0.0015580568
Iter: 1294 loss: 0.00155653362
Iter: 1295 loss: 0.00156094832
Iter: 1296 loss: 0.00155605469
Iter: 1297 loss: 0.00155422406
Iter: 1298 loss: 0.00156448293
Iter: 1299 loss: 0.00155396014
Iter: 1300 loss: 0.00155257247
Iter: 1301 loss: 0.00155404187
Iter: 1302 loss: 0.00155179831
Iter: 1303 loss: 0.0015499267
Iter: 1304 loss: 0.00154782785
Iter: 1305 loss: 0.00154754845
Iter: 1306 loss: 0.00154514774
Iter: 1307 loss: 0.00154747
Iter: 1308 loss: 0.00154378649
Iter: 1309 loss: 0.00154090347
Iter: 1310 loss: 0.00155210495
Iter: 1311 loss: 0.00154023117
Iter: 1312 loss: 0.00153804675
Iter: 1313 loss: 0.00154172094
Iter: 1314 loss: 0.00153706619
Iter: 1315 loss: 0.00153412553
Iter: 1316 loss: 0.00153737632
Iter: 1317 loss: 0.00153253251
Iter: 1318 loss: 0.00152935635
Iter: 1319 loss: 0.00154293864
Iter: 1320 loss: 0.00152870663
Iter: 1321 loss: 0.00152627565
Iter: 1322 loss: 0.00154891168
Iter: 1323 loss: 0.00152617751
Iter: 1324 loss: 0.00152477541
Iter: 1325 loss: 0.00154496124
Iter: 1326 loss: 0.00152477424
Iter: 1327 loss: 0.00152361719
Iter: 1328 loss: 0.00152188749
Iter: 1329 loss: 0.00152185024
Iter: 1330 loss: 0.00152163638
Iter: 1331 loss: 0.00152087351
Iter: 1332 loss: 0.00152016617
Iter: 1333 loss: 0.00152016606
Iter: 1334 loss: 0.00151960575
Iter: 1335 loss: 0.00151841156
Iter: 1336 loss: 0.00152013544
Iter: 1337 loss: 0.00151783356
Iter: 1338 loss: 0.00151620235
Iter: 1339 loss: 0.00151432084
Iter: 1340 loss: 0.00151409337
Iter: 1341 loss: 0.00151169952
Iter: 1342 loss: 0.0015203394
Iter: 1343 loss: 0.0015110923
Iter: 1344 loss: 0.00150880741
Iter: 1345 loss: 0.00151385239
Iter: 1346 loss: 0.00150792603
Iter: 1347 loss: 0.00150561973
Iter: 1348 loss: 0.00151065423
Iter: 1349 loss: 0.00150473858
Iter: 1350 loss: 0.00150189211
Iter: 1351 loss: 0.00151032396
Iter: 1352 loss: 0.00150102493
Iter: 1353 loss: 0.00149853609
Iter: 1354 loss: 0.00150459283
Iter: 1355 loss: 0.0014976582
Iter: 1356 loss: 0.00149545423
Iter: 1357 loss: 0.00152094266
Iter: 1358 loss: 0.00149542035
Iter: 1359 loss: 0.00149354618
Iter: 1360 loss: 0.00150244078
Iter: 1361 loss: 0.00149320601
Iter: 1362 loss: 0.0014917443
Iter: 1363 loss: 0.00149426213
Iter: 1364 loss: 0.00149109552
Iter: 1365 loss: 0.00148921681
Iter: 1366 loss: 0.00150385662
Iter: 1367 loss: 0.00148908468
Iter: 1368 loss: 0.00148809073
Iter: 1369 loss: 0.00148722634
Iter: 1370 loss: 0.00148696185
Iter: 1371 loss: 0.00148500176
Iter: 1372 loss: 0.00148778351
Iter: 1373 loss: 0.00148404017
Iter: 1374 loss: 0.00148209848
Iter: 1375 loss: 0.00148121105
Iter: 1376 loss: 0.00148024049
Iter: 1377 loss: 0.00147750857
Iter: 1378 loss: 0.0014826
Iter: 1379 loss: 0.00147633918
Iter: 1380 loss: 0.00147294509
Iter: 1381 loss: 0.00147547864
Iter: 1382 loss: 0.00147086778
Iter: 1383 loss: 0.001466783
Iter: 1384 loss: 0.00149312569
Iter: 1385 loss: 0.00146632711
Iter: 1386 loss: 0.00146355992
Iter: 1387 loss: 0.00147299527
Iter: 1388 loss: 0.00146282371
Iter: 1389 loss: 0.00146028283
Iter: 1390 loss: 0.00147287734
Iter: 1391 loss: 0.00145984045
Iter: 1392 loss: 0.0014577799
Iter: 1393 loss: 0.0014790881
Iter: 1394 loss: 0.00145772065
Iter: 1395 loss: 0.00145627628
Iter: 1396 loss: 0.0014577827
Iter: 1397 loss: 0.00145547558
Iter: 1398 loss: 0.00145424355
Iter: 1399 loss: 0.00145423599
Iter: 1400 loss: 0.0014532858
Iter: 1401 loss: 0.00145117391
Iter: 1402 loss: 0.00148220453
Iter: 1403 loss: 0.00145108206
Iter: 1404 loss: 0.00144892326
Iter: 1405 loss: 0.00147051
Iter: 1406 loss: 0.00144885096
Iter: 1407 loss: 0.00144723093
Iter: 1408 loss: 0.00144606188
Iter: 1409 loss: 0.00144549715
Iter: 1410 loss: 0.0014430756
Iter: 1411 loss: 0.00144461915
Iter: 1412 loss: 0.00144153473
Iter: 1413 loss: 0.00143877044
Iter: 1414 loss: 0.00144884305
Iter: 1415 loss: 0.00143808115
Iter: 1416 loss: 0.00143516413
Iter: 1417 loss: 0.00143801537
Iter: 1418 loss: 0.00143350079
Iter: 1419 loss: 0.00142972381
Iter: 1420 loss: 0.00144129177
Iter: 1421 loss: 0.00142860785
Iter: 1422 loss: 0.00142594019
Iter: 1423 loss: 0.00145658955
Iter: 1424 loss: 0.00142589293
Iter: 1425 loss: 0.00142423948
Iter: 1426 loss: 0.00143764657
Iter: 1427 loss: 0.00142412703
Iter: 1428 loss: 0.00142252096
Iter: 1429 loss: 0.00142130558
Iter: 1430 loss: 0.00142078334
Iter: 1431 loss: 0.00141923886
Iter: 1432 loss: 0.00141915865
Iter: 1433 loss: 0.00141771021
Iter: 1434 loss: 0.00141714211
Iter: 1435 loss: 0.0014163747
Iter: 1436 loss: 0.00141494209
Iter: 1437 loss: 0.00141956517
Iter: 1438 loss: 0.00141454069
Iter: 1439 loss: 0.00141297013
Iter: 1440 loss: 0.00141474092
Iter: 1441 loss: 0.001412121
Iter: 1442 loss: 0.00141031109
Iter: 1443 loss: 0.00140887848
Iter: 1444 loss: 0.00140832353
Iter: 1445 loss: 0.00140552211
Iter: 1446 loss: 0.00141312752
Iter: 1447 loss: 0.0014046001
Iter: 1448 loss: 0.00140195538
Iter: 1449 loss: 0.00140924577
Iter: 1450 loss: 0.00140109763
Iter: 1451 loss: 0.00139871042
Iter: 1452 loss: 0.0014067716
Iter: 1453 loss: 0.00139807118
Iter: 1454 loss: 0.00139561924
Iter: 1455 loss: 0.00140539557
Iter: 1456 loss: 0.00139506301
Iter: 1457 loss: 0.0013933325
Iter: 1458 loss: 0.00142089801
Iter: 1459 loss: 0.00139333506
Iter: 1460 loss: 0.00139193889
Iter: 1461 loss: 0.00139211514
Iter: 1462 loss: 0.00139087986
Iter: 1463 loss: 0.00138954201
Iter: 1464 loss: 0.00140303362
Iter: 1465 loss: 0.00138949964
Iter: 1466 loss: 0.00138812326
Iter: 1467 loss: 0.00139136473
Iter: 1468 loss: 0.00138762465
Iter: 1469 loss: 0.00138671626
Iter: 1470 loss: 0.00138644048
Iter: 1471 loss: 0.00138590776
Iter: 1472 loss: 0.00138444384
Iter: 1473 loss: 0.0013887682
Iter: 1474 loss: 0.00138400286
Iter: 1475 loss: 0.00138255465
Iter: 1476 loss: 0.00138200494
Iter: 1477 loss: 0.0013812033
Iter: 1478 loss: 0.00137956452
Iter: 1479 loss: 0.00138292252
Iter: 1480 loss: 0.00137890386
Iter: 1481 loss: 0.00137701316
Iter: 1482 loss: 0.00137842027
Iter: 1483 loss: 0.00137585
Iter: 1484 loss: 0.00137341477
Iter: 1485 loss: 0.00137743168
Iter: 1486 loss: 0.0013723002
Iter: 1487 loss: 0.00136972824
Iter: 1488 loss: 0.00139159686
Iter: 1489 loss: 0.0013695783
Iter: 1490 loss: 0.00136814709
Iter: 1491 loss: 0.00139010348
Iter: 1492 loss: 0.00136815163
Iter: 1493 loss: 0.00136678852
Iter: 1494 loss: 0.00136642461
Iter: 1495 loss: 0.00136558281
Iter: 1496 loss: 0.00136379362
Iter: 1497 loss: 0.00137003069
Iter: 1498 loss: 0.00136332272
Iter: 1499 loss: 0.00136178208
Iter: 1500 loss: 0.00136178616
Iter: 1501 loss: 0.00136108324
Iter: 1502 loss: 0.00135979755
Iter: 1503 loss: 0.00138940592
Iter: 1504 loss: 0.00135979056
Iter: 1505 loss: 0.00135824119
Iter: 1506 loss: 0.00136720121
Iter: 1507 loss: 0.00135803269
Iter: 1508 loss: 0.00135668041
Iter: 1509 loss: 0.00135636202
Iter: 1510 loss: 0.00135549926
Iter: 1511 loss: 0.00135373825
Iter: 1512 loss: 0.00135426957
Iter: 1513 loss: 0.00135247689
Iter: 1514 loss: 0.00135032507
Iter: 1515 loss: 0.00135814666
Iter: 1516 loss: 0.00134978304
Iter: 1517 loss: 0.00134790363
Iter: 1518 loss: 0.00135059259
Iter: 1519 loss: 0.00134699687
Iter: 1520 loss: 0.0013446419
Iter: 1521 loss: 0.00135085604
Iter: 1522 loss: 0.00134384655
Iter: 1523 loss: 0.00134197215
Iter: 1524 loss: 0.00134197623
Iter: 1525 loss: 0.00134025863
Iter: 1526 loss: 0.00134511362
Iter: 1527 loss: 0.00133972312
Iter: 1528 loss: 0.00133829541
Iter: 1529 loss: 0.00133855315
Iter: 1530 loss: 0.00133722357
Iter: 1531 loss: 0.00133652333
Iter: 1532 loss: 0.00133620342
Iter: 1533 loss: 0.00133552542
Iter: 1534 loss: 0.00133398105
Iter: 1535 loss: 0.00135373266
Iter: 1536 loss: 0.00133388513
Iter: 1537 loss: 0.00133220386
Iter: 1538 loss: 0.00134208938
Iter: 1539 loss: 0.001331985
Iter: 1540 loss: 0.00133030803
Iter: 1541 loss: 0.00133071514
Iter: 1542 loss: 0.00132907741
Iter: 1543 loss: 0.00132692931
Iter: 1544 loss: 0.00132664281
Iter: 1545 loss: 0.0013251279
Iter: 1546 loss: 0.00132255116
Iter: 1547 loss: 0.00133354834
Iter: 1548 loss: 0.00132201309
Iter: 1549 loss: 0.00131953764
Iter: 1550 loss: 0.00132111786
Iter: 1551 loss: 0.00131797139
Iter: 1552 loss: 0.00131492177
Iter: 1553 loss: 0.00133227254
Iter: 1554 loss: 0.00131450384
Iter: 1555 loss: 0.00131252385
Iter: 1556 loss: 0.00132256956
Iter: 1557 loss: 0.00131220266
Iter: 1558 loss: 0.00131040486
Iter: 1559 loss: 0.00133265695
Iter: 1560 loss: 0.00131038786
Iter: 1561 loss: 0.00130913523
Iter: 1562 loss: 0.00130872102
Iter: 1563 loss: 0.00130798749
Iter: 1564 loss: 0.00130721694
Iter: 1565 loss: 0.00130709598
Iter: 1566 loss: 0.0013061983
Iter: 1567 loss: 0.00130475615
Iter: 1568 loss: 0.00130474498
Iter: 1569 loss: 0.00130325812
Iter: 1570 loss: 0.0013077911
Iter: 1571 loss: 0.00130281947
Iter: 1572 loss: 0.001301251
Iter: 1573 loss: 0.00130566
Iter: 1574 loss: 0.00130075461
Iter: 1575 loss: 0.00129939686
Iter: 1576 loss: 0.00129909
Iter: 1577 loss: 0.0012982171
Iter: 1578 loss: 0.00129635399
Iter: 1579 loss: 0.00129660079
Iter: 1580 loss: 0.00129493931
Iter: 1581 loss: 0.0012924748
Iter: 1582 loss: 0.00130478595
Iter: 1583 loss: 0.00129205652
Iter: 1584 loss: 0.00129010202
Iter: 1585 loss: 0.00129770185
Iter: 1586 loss: 0.00128964474
Iter: 1587 loss: 0.00128789409
Iter: 1588 loss: 0.00129144127
Iter: 1589 loss: 0.00128718419
Iter: 1590 loss: 0.00128610223
Iter: 1591 loss: 0.00128599443
Iter: 1592 loss: 0.00128494
Iter: 1593 loss: 0.00128468196
Iter: 1594 loss: 0.00128400733
Iter: 1595 loss: 0.00128294062
Iter: 1596 loss: 0.00129546155
Iter: 1597 loss: 0.00128292455
Iter: 1598 loss: 0.00128178054
Iter: 1599 loss: 0.00128309487
Iter: 1600 loss: 0.00128117029
Iter: 1601 loss: 0.00128023513
Iter: 1602 loss: 0.00128008728
Iter: 1603 loss: 0.00127944374
Iter: 1604 loss: 0.00127821835
Iter: 1605 loss: 0.00128692086
Iter: 1606 loss: 0.00127810566
Iter: 1607 loss: 0.00127720856
Iter: 1608 loss: 0.0012764734
Iter: 1609 loss: 0.0012762181
Iter: 1610 loss: 0.00127468328
Iter: 1611 loss: 0.00127585512
Iter: 1612 loss: 0.00127375312
Iter: 1613 loss: 0.00127204822
Iter: 1614 loss: 0.00127595593
Iter: 1615 loss: 0.00127141783
Iter: 1616 loss: 0.00126964133
Iter: 1617 loss: 0.00127638585
Iter: 1618 loss: 0.00126921246
Iter: 1619 loss: 0.00126744504
Iter: 1620 loss: 0.00127145613
Iter: 1621 loss: 0.00126677961
Iter: 1622 loss: 0.00126552104
Iter: 1623 loss: 0.0012655115
Iter: 1624 loss: 0.00126426853
Iter: 1625 loss: 0.00126651034
Iter: 1626 loss: 0.00126372429
Iter: 1627 loss: 0.00126268086
Iter: 1628 loss: 0.00126729487
Iter: 1629 loss: 0.00126246666
Iter: 1630 loss: 0.00126131764
Iter: 1631 loss: 0.00126850687
Iter: 1632 loss: 0.00126118213
Iter: 1633 loss: 0.00126049027
Iter: 1634 loss: 0.00125946617
Iter: 1635 loss: 0.001259443
Iter: 1636 loss: 0.00125813088
Iter: 1637 loss: 0.00126644725
Iter: 1638 loss: 0.00125798304
Iter: 1639 loss: 0.00125680678
Iter: 1640 loss: 0.00125611899
Iter: 1641 loss: 0.00125562446
Iter: 1642 loss: 0.00125399255
Iter: 1643 loss: 0.00125773461
Iter: 1644 loss: 0.00125338649
Iter: 1645 loss: 0.00125177158
Iter: 1646 loss: 0.00125167216
Iter: 1647 loss: 0.00125044561
Iter: 1648 loss: 0.00124826538
Iter: 1649 loss: 0.00125953567
Iter: 1650 loss: 0.00124791986
Iter: 1651 loss: 0.00124603393
Iter: 1652 loss: 0.00125311967
Iter: 1653 loss: 0.00124557805
Iter: 1654 loss: 0.00124420086
Iter: 1655 loss: 0.00125793135
Iter: 1656 loss: 0.00124415895
Iter: 1657 loss: 0.00124288874
Iter: 1658 loss: 0.00124980556
Iter: 1659 loss: 0.00124270422
Iter: 1660 loss: 0.00124177244
Iter: 1661 loss: 0.00124243973
Iter: 1662 loss: 0.00124119758
Iter: 1663 loss: 0.00124017219
Iter: 1664 loss: 0.00124016963
Iter: 1665 loss: 0.00123954657
Iter: 1666 loss: 0.00123844389
Iter: 1667 loss: 0.00123843981
Iter: 1668 loss: 0.00123717869
Iter: 1669 loss: 0.00124502275
Iter: 1670 loss: 0.00123703224
Iter: 1671 loss: 0.00123596843
Iter: 1672 loss: 0.00123799487
Iter: 1673 loss: 0.00123551895
Iter: 1674 loss: 0.00123456703
Iter: 1675 loss: 0.00123480451
Iter: 1676 loss: 0.00123386737
Iter: 1677 loss: 0.00123219378
Iter: 1678 loss: 0.00123174931
Iter: 1679 loss: 0.00123070669
Iter: 1680 loss: 0.00122896861
Iter: 1681 loss: 0.0012421154
Iter: 1682 loss: 0.00122882985
Iter: 1683 loss: 0.00122740562
Iter: 1684 loss: 0.0012303174
Iter: 1685 loss: 0.00122683193
Iter: 1686 loss: 0.00122530619
Iter: 1687 loss: 0.00123384991
Iter: 1688 loss: 0.0012250921
Iter: 1689 loss: 0.00122399663
Iter: 1690 loss: 0.00124063902
Iter: 1691 loss: 0.00122399605
Iter: 1692 loss: 0.00122320163
Iter: 1693 loss: 0.00122228637
Iter: 1694 loss: 0.00122218032
Iter: 1695 loss: 0.00122142688
Iter: 1696 loss: 0.0012213313
Iter: 1697 loss: 0.00122068834
Iter: 1698 loss: 0.00121974899
Iter: 1699 loss: 0.00121972593
Iter: 1700 loss: 0.0012186002
Iter: 1701 loss: 0.00122239732
Iter: 1702 loss: 0.00121830101
Iter: 1703 loss: 0.001217401
Iter: 1704 loss: 0.00122225937
Iter: 1705 loss: 0.00121726166
Iter: 1706 loss: 0.0012165152
Iter: 1707 loss: 0.00121529726
Iter: 1708 loss: 0.00121528737
Iter: 1709 loss: 0.00121348817
Iter: 1710 loss: 0.00121960568
Iter: 1711 loss: 0.00121300272
Iter: 1712 loss: 0.00121152773
Iter: 1713 loss: 0.00121225941
Iter: 1714 loss: 0.00121055089
Iter: 1715 loss: 0.00120861037
Iter: 1716 loss: 0.00121590728
Iter: 1717 loss: 0.00120814866
Iter: 1718 loss: 0.00120686146
Iter: 1719 loss: 0.0012183066
Iter: 1720 loss: 0.0012067986
Iter: 1721 loss: 0.00120583025
Iter: 1722 loss: 0.00121711974
Iter: 1723 loss: 0.00120581873
Iter: 1724 loss: 0.0012049051
Iter: 1725 loss: 0.00120468286
Iter: 1726 loss: 0.00120410835
Iter: 1727 loss: 0.00120341522
Iter: 1728 loss: 0.0012034009
Iter: 1729 loss: 0.00120270904
Iter: 1730 loss: 0.00120152486
Iter: 1731 loss: 0.00120152941
Iter: 1732 loss: 0.00120001961
Iter: 1733 loss: 0.00120530115
Iter: 1734 loss: 0.00119962124
Iter: 1735 loss: 0.00119859725
Iter: 1736 loss: 0.0012025577
Iter: 1737 loss: 0.00119836023
Iter: 1738 loss: 0.00119725382
Iter: 1739 loss: 0.00119718793
Iter: 1740 loss: 0.00119635079
Iter: 1741 loss: 0.00119492714
Iter: 1742 loss: 0.00119880051
Iter: 1743 loss: 0.00119445776
Iter: 1744 loss: 0.00119300629
Iter: 1745 loss: 0.00119492738
Iter: 1746 loss: 0.00119228242
Iter: 1747 loss: 0.00119070627
Iter: 1748 loss: 0.00119426195
Iter: 1749 loss: 0.00119010545
Iter: 1750 loss: 0.001188569
Iter: 1751 loss: 0.00119340303
Iter: 1752 loss: 0.00118811452
Iter: 1753 loss: 0.00118683395
Iter: 1754 loss: 0.00120664691
Iter: 1755 loss: 0.00118683162
Iter: 1756 loss: 0.00118566467
Iter: 1757 loss: 0.00118759857
Iter: 1758 loss: 0.00118513696
Iter: 1759 loss: 0.00118434126
Iter: 1760 loss: 0.00119034899
Iter: 1761 loss: 0.00118427793
Iter: 1762 loss: 0.00118335849
Iter: 1763 loss: 0.00118366058
Iter: 1764 loss: 0.00118270365
Iter: 1765 loss: 0.0011818636
Iter: 1766 loss: 0.00118334685
Iter: 1767 loss: 0.00118149095
Iter: 1768 loss: 0.00118060899
Iter: 1769 loss: 0.00118174776
Iter: 1770 loss: 0.00118015741
Iter: 1771 loss: 0.00117899291
Iter: 1772 loss: 0.00118097209
Iter: 1773 loss: 0.00117846322
Iter: 1774 loss: 0.00117730536
Iter: 1775 loss: 0.00117807381
Iter: 1776 loss: 0.00117656379
Iter: 1777 loss: 0.00117524876
Iter: 1778 loss: 0.00118027721
Iter: 1779 loss: 0.00117493537
Iter: 1780 loss: 0.00117373234
Iter: 1781 loss: 0.0011747689
Iter: 1782 loss: 0.00117302034
Iter: 1783 loss: 0.00117151719
Iter: 1784 loss: 0.00117924367
Iter: 1785 loss: 0.00117127085
Iter: 1786 loss: 0.00117031136
Iter: 1787 loss: 0.00118204753
Iter: 1788 loss: 0.0011702975
Iter: 1789 loss: 0.00116937514
Iter: 1790 loss: 0.00117162301
Iter: 1791 loss: 0.0011690458
Iter: 1792 loss: 0.00116829423
Iter: 1793 loss: 0.0011702301
Iter: 1794 loss: 0.00116803485
Iter: 1795 loss: 0.00116700213
Iter: 1796 loss: 0.00117025722
Iter: 1797 loss: 0.00116669759
Iter: 1798 loss: 0.00116603414
Iter: 1799 loss: 0.00116546359
Iter: 1800 loss: 0.00116528198
Iter: 1801 loss: 0.00116404216
Iter: 1802 loss: 0.00116689701
Iter: 1803 loss: 0.0011635765
Iter: 1804 loss: 0.00116241095
Iter: 1805 loss: 0.00116533216
Iter: 1806 loss: 0.00116199511
Iter: 1807 loss: 0.00116086192
Iter: 1808 loss: 0.00116131292
Iter: 1809 loss: 0.00116008148
Iter: 1810 loss: 0.00115882454
Iter: 1811 loss: 0.00116331666
Iter: 1812 loss: 0.00115850754
Iter: 1813 loss: 0.00115725701
Iter: 1814 loss: 0.0011578839
Iter: 1815 loss: 0.0011564123
Iter: 1816 loss: 0.00115490356
Iter: 1817 loss: 0.00116675044
Iter: 1818 loss: 0.00115479936
Iter: 1819 loss: 0.00115388958
Iter: 1820 loss: 0.00116165134
Iter: 1821 loss: 0.00115383789
Iter: 1822 loss: 0.0011529281
Iter: 1823 loss: 0.00115595385
Iter: 1824 loss: 0.00115267304
Iter: 1825 loss: 0.00115190074
Iter: 1826 loss: 0.00115321111
Iter: 1827 loss: 0.00115155079
Iter: 1828 loss: 0.00115059537
Iter: 1829 loss: 0.00115802581
Iter: 1830 loss: 0.0011505275
Iter: 1831 loss: 0.00115003146
Iter: 1832 loss: 0.00114916568
Iter: 1833 loss: 0.00114916195
Iter: 1834 loss: 0.00114806136
Iter: 1835 loss: 0.00115429773
Iter: 1836 loss: 0.00114790408
Iter: 1837 loss: 0.00114696054
Iter: 1838 loss: 0.00114709476
Iter: 1839 loss: 0.00114624074
Iter: 1840 loss: 0.00114479102
Iter: 1841 loss: 0.00114566262
Iter: 1842 loss: 0.00114386389
Iter: 1843 loss: 0.00114238565
Iter: 1844 loss: 0.00114826416
Iter: 1845 loss: 0.00114204723
Iter: 1846 loss: 0.00114064338
Iter: 1847 loss: 0.00114285632
Iter: 1848 loss: 0.00113998738
Iter: 1849 loss: 0.00113856513
Iter: 1850 loss: 0.00114557939
Iter: 1851 loss: 0.00113831821
Iter: 1852 loss: 0.00113716768
Iter: 1853 loss: 0.00114522781
Iter: 1854 loss: 0.00113706244
Iter: 1855 loss: 0.00113601983
Iter: 1856 loss: 0.00114272418
Iter: 1857 loss: 0.00113590085
Iter: 1858 loss: 0.00113511365
Iter: 1859 loss: 0.00113610364
Iter: 1860 loss: 0.00113470212
Iter: 1861 loss: 0.00113386079
Iter: 1862 loss: 0.00114353723
Iter: 1863 loss: 0.00113384961
Iter: 1864 loss: 0.00113334402
Iter: 1865 loss: 0.00113225402
Iter: 1866 loss: 0.00114924402
Iter: 1867 loss: 0.00113221561
Iter: 1868 loss: 0.00113100256
Iter: 1869 loss: 0.00113958656
Iter: 1870 loss: 0.00113088731
Iter: 1871 loss: 0.00112985028
Iter: 1872 loss: 0.00113001605
Iter: 1873 loss: 0.00112906
Iter: 1874 loss: 0.00112761604
Iter: 1875 loss: 0.00113036926
Iter: 1876 loss: 0.00112700334
Iter: 1877 loss: 0.00112588576
Iter: 1878 loss: 0.00112940418
Iter: 1879 loss: 0.00112556224
Iter: 1880 loss: 0.00112437096
Iter: 1881 loss: 0.00112641463
Iter: 1882 loss: 0.00112383789
Iter: 1883 loss: 0.00112263788
Iter: 1884 loss: 0.00112724141
Iter: 1885 loss: 0.00112234906
Iter: 1886 loss: 0.00112135126
Iter: 1887 loss: 0.00113017403
Iter: 1888 loss: 0.00112130144
Iter: 1889 loss: 0.00112056406
Iter: 1890 loss: 0.00112705748
Iter: 1891 loss: 0.00112052285
Iter: 1892 loss: 0.00111995463
Iter: 1893 loss: 0.00112029503
Iter: 1894 loss: 0.00111958012
Iter: 1895 loss: 0.00111893308
Iter: 1896 loss: 0.00112668891
Iter: 1897 loss: 0.00111892587
Iter: 1898 loss: 0.0011184609
Iter: 1899 loss: 0.00111758662
Iter: 1900 loss: 0.00113722158
Iter: 1901 loss: 0.00111758441
Iter: 1902 loss: 0.0011167035
Iter: 1903 loss: 0.00112227385
Iter: 1904 loss: 0.00111660128
Iter: 1905 loss: 0.00111575215
Iter: 1906 loss: 0.00111602736
Iter: 1907 loss: 0.00111513911
Iter: 1908 loss: 0.00111400289
Iter: 1909 loss: 0.00111527857
Iter: 1910 loss: 0.00111339171
Iter: 1911 loss: 0.00111232116
Iter: 1912 loss: 0.00111367356
Iter: 1913 loss: 0.00111176376
Iter: 1914 loss: 0.00111040752
Iter: 1915 loss: 0.00111600349
Iter: 1916 loss: 0.00111011229
Iter: 1917 loss: 0.00110902474
Iter: 1918 loss: 0.00111230463
Iter: 1919 loss: 0.00110869552
Iter: 1920 loss: 0.00110775256
Iter: 1921 loss: 0.00111645786
Iter: 1922 loss: 0.00110771274
Iter: 1923 loss: 0.00110699236
Iter: 1924 loss: 0.00111181638
Iter: 1925 loss: 0.00110692112
Iter: 1926 loss: 0.00110623101
Iter: 1927 loss: 0.00110640645
Iter: 1928 loss: 0.00110572809
Iter: 1929 loss: 0.00110491947
Iter: 1930 loss: 0.00111495284
Iter: 1931 loss: 0.00110491063
Iter: 1932 loss: 0.00110430387
Iter: 1933 loss: 0.00110348512
Iter: 1934 loss: 0.00110344205
Iter: 1935 loss: 0.00110256
Iter: 1936 loss: 0.00110575231
Iter: 1937 loss: 0.00110233563
Iter: 1938 loss: 0.00110129605
Iter: 1939 loss: 0.00110211805
Iter: 1940 loss: 0.00110066682
Iter: 1941 loss: 0.0010994582
Iter: 1942 loss: 0.00110113446
Iter: 1943 loss: 0.00109887356
Iter: 1944 loss: 0.00109778228
Iter: 1945 loss: 0.00109886099
Iter: 1946 loss: 0.00109717413
Iter: 1947 loss: 0.00109592732
Iter: 1948 loss: 0.00110228045
Iter: 1949 loss: 0.00109572429
Iter: 1950 loss: 0.00109465036
Iter: 1951 loss: 0.0010960846
Iter: 1952 loss: 0.00109411008
Iter: 1953 loss: 0.00109292637
Iter: 1954 loss: 0.00110329827
Iter: 1955 loss: 0.00109286769
Iter: 1956 loss: 0.00109197444
Iter: 1957 loss: 0.00109900557
Iter: 1958 loss: 0.00109190843
Iter: 1959 loss: 0.00109112845
Iter: 1960 loss: 0.00109184952
Iter: 1961 loss: 0.0010906785
Iter: 1962 loss: 0.00108999072
Iter: 1963 loss: 0.00110029872
Iter: 1964 loss: 0.00108999398
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script66
+ '[' -r STOP.script66 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output66/f1_psi1_phi2.8/500_500_500_500_1
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 2.8
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output66/f1_psi-2_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output67/f1_psi-2_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output66/f1_psi-2_phi2.8 /home/mrdouglas/Manifold/experiments.final/output67/f1_psi-2_phi2.8
+ date
Sun Oct 25 01:43:01 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output66/f1_psi-2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 2345 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi -2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output66/f1_psi-2_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb344b3dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb344a76510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb344b5cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb344aa5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb344aa5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb344abbc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb34497a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3449847b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb344984158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3449851e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb344a289d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3448ac488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3448acc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb34493c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3448f6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3448f6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3448dc510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb34484dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb344830950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb344828f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb34479a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3447b4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb340d23620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb340d30598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb340d3a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3447f02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb340d0c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb340ced730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb340ced2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb340cd2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb340cd2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb340bec158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb340bec2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb340c05400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb344767a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb344767e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.5248781
test_loss: 0.5369017
train_loss: 3.0702412
test_loss: 1.2783664
train_loss: 0.48542958
test_loss: 0.489512
train_loss: 0.5178145
test_loss: 0.4909979
train_loss: 0.45323306
test_loss: 0.47989112
train_loss: 0.46091795
test_loss: 0.4668761
train_loss: 0.46194276
test_loss: 0.45341298
train_loss: 0.4200256
test_loss: 0.43953174
train_loss: 0.42713013
test_loss: 0.42507747
train_loss: 0.39390582
test_loss: 0.40956217
train_loss: 0.40361536
test_loss: 0.3918418
train_loss: 0.35392034
test_loss: 0.3714169
train_loss: 0.3439652
test_loss: 0.3459529
train_loss: 0.32002506
test_loss: 0.32016668
train_loss: 0.29514557
test_loss: 0.29607522
train_loss: 0.3296742
test_loss: 0.3378272
train_loss: 0.31965467
test_loss: 0.33581647
train_loss: 0.31864032
test_loss: 0.32242945
train_loss: 0.30223393
test_loss: 0.31004405
train_loss: 0.3037585
test_loss: 0.298209
train_loss: 0.2800236
test_loss: 0.2867624
train_loss: 0.27142224
test_loss: 0.27504763
train_loss: 0.25347802
test_loss: 0.26389414
train_loss: 0.26100445
test_loss: 0.25229487
train_loss: 0.23192467
test_loss: 0.24133515
train_loss: 0.22404453
test_loss: 0.23083954
train_loss: 0.21024187
test_loss: 0.22017118
train_loss: 0.2070198
test_loss: 0.20947199
train_loss: 0.19262913
test_loss: 0.19707866
train_loss: 0.18220071
test_loss: 0.18451193
train_loss: 0.18048754
test_loss: 0.17274512
train_loss: 0.16076228
test_loss: 0.15932497
train_loss: 0.14456944
test_loss: 0.14453685
train_loss: 0.1291575
test_loss: 0.1273851
train_loss: 0.11333917
test_loss: 3.8390543
train_loss: 0.10647124
test_loss: 0.101990275
train_loss: 0.09579934
test_loss: 0.0968506
train_loss: 0.09475637
test_loss: 0.094317235
train_loss: 0.08969944
test_loss: 0.09253691
train_loss: 0.090938844
test_loss: 0.090945765
train_loss: 0.08844717
test_loss: 0.09043902
train_loss: 0.084378555
test_loss: 0.08949453
train_loss: 0.09242372
test_loss: 0.08845164
train_loss: 0.08447552
test_loss: 0.08789019
train_loss: 0.09002629
test_loss: 0.08744308
train_loss: 0.08737747
test_loss: 0.08641331
train_loss: 0.08290614
test_loss: 0.08605237
train_loss: 0.084945746
test_loss: 0.08555026
train_loss: 0.083465695
test_loss: 0.08495806
train_loss: 0.08036286
test_loss: 0.083942324
train_loss: 0.08061954
test_loss: 0.08350549
train_loss: 0.080221176
test_loss: 0.0828297
train_loss: 0.08400687
test_loss: 0.08239705
train_loss: 0.07815109
test_loss: 0.08139348
train_loss: 0.07603959
test_loss: 0.08055431
train_loss: 0.074656084
test_loss: 0.07973634
train_loss: 0.07530717
test_loss: 0.07967145
train_loss: 0.07554264
test_loss: 0.07875094
train_loss: 0.076887205
test_loss: 0.07837588
train_loss: 0.073103674
test_loss: 0.077900335
train_loss: 0.07575415
test_loss: 0.07708561
train_loss: 0.07788126
test_loss: 0.07670951
train_loss: 0.07841662
test_loss: 0.0761906
train_loss: 0.0708316
test_loss: 0.0755985
train_loss: 0.06808091
test_loss: 0.07489829
train_loss: 0.071988806
test_loss: 0.074527
train_loss: 0.06944001
test_loss: 0.074075684
train_loss: 0.07556356
test_loss: 0.07353655
train_loss: 0.068980485
test_loss: 0.07289125
train_loss: 0.069013305
test_loss: 0.072545655
train_loss: 0.06349305
test_loss: 0.07171522
train_loss: 0.06460365
test_loss: 0.0716023
train_loss: 0.06654216
test_loss: 0.07080285
train_loss: 0.06830257
test_loss: 0.07077981
train_loss: 0.06562722
test_loss: 0.07016882
train_loss: 0.06664794
test_loss: 0.07023337
train_loss: 0.066118695
test_loss: 0.0693793
train_loss: 0.06798047
test_loss: 0.06947746
train_loss: 0.06426896
test_loss: 0.068797044
train_loss: 0.06556174
test_loss: 0.06827019
train_loss: 0.06404964
test_loss: 0.067547806
train_loss: 0.061041262
test_loss: 0.0672108
train_loss: 0.060189176
test_loss: 0.066857815
train_loss: 0.06813812
test_loss: 0.06648774
train_loss: 0.06112981
test_loss: 0.06616086
train_loss: 0.05966402
test_loss: 0.06568103
train_loss: 0.060107425
test_loss: 0.06488843
train_loss: 0.06158546
test_loss: 0.06494145
train_loss: 0.05947339
test_loss: 0.06480542
train_loss: 0.059172783
test_loss: 0.064542316
train_loss: 0.05652708
test_loss: 0.06355612
train_loss: 0.05583006
test_loss: 0.0631278
train_loss: 0.060848944
test_loss: 0.06280705
train_loss: 0.057808593
test_loss: 0.06290038
train_loss: 0.06116973
test_loss: 0.06263237
train_loss: 0.059380226
test_loss: 0.062232804
train_loss: 0.05567898
test_loss: 0.061813977
train_loss: 0.056005865
test_loss: 0.061840046
train_loss: 0.057572193
test_loss: 0.061763514
train_loss: 0.054512355
test_loss: 0.060897075
train_loss: 0.05396136
test_loss: 0.060517535
train_loss: 0.055541486
test_loss: 0.06093411
train_loss: 0.055173807
test_loss: 0.060573373
train_loss: 0.05320491
test_loss: 0.059862033
train_loss: 0.056393344
test_loss: 0.059551775
train_loss: 0.05391689
test_loss: 0.059410598
train_loss: 0.05479222
test_loss: 0.05939031
train_loss: 0.053574704
test_loss: 0.058802687
train_loss: 0.053231
test_loss: 0.059203986
train_loss: 0.05414549
test_loss: 0.05855519
train_loss: 0.052633613
test_loss: 0.05814907
train_loss: 0.053603128
test_loss: 0.057938404
train_loss: 0.051647864
test_loss: 0.057864696
train_loss: 0.050769977
test_loss: 0.057552867
train_loss: 0.053558063
test_loss: 0.057539493
train_loss: 0.053461045
test_loss: 0.057489112
train_loss: 0.05041859
test_loss: 0.05737355
train_loss: 0.050900884
test_loss: 0.056625016
train_loss: 0.052126765
test_loss: 0.05665524
train_loss: 0.05180575
test_loss: 0.056377895
train_loss: 0.049582038
test_loss: 0.056193873
train_loss: 0.05150047
test_loss: 0.056184884
train_loss: 0.050853066
test_loss: 0.055516448
train_loss: 0.052349858
test_loss: 0.055423904
train_loss: 0.04833152
test_loss: 0.055042386
train_loss: 0.052419804
test_loss: 0.05516902
train_loss: 0.048443343
test_loss: 0.05502204
train_loss: 0.04934479
test_loss: 0.054308906
train_loss: 0.049872067
test_loss: 0.054967824
train_loss: 0.04609661
test_loss: 0.054672245
train_loss: 0.046872754
test_loss: 0.054579575
train_loss: 0.04760757
test_loss: 0.053795833
train_loss: 0.049363643
test_loss: 0.054028843
train_loss: 0.048328232
test_loss: 0.0533086
train_loss: 0.045479625
test_loss: 0.053240452
train_loss: 0.04639612
test_loss: 0.052955505
train_loss: 0.044961628
test_loss: 0.052595846
train_loss: 0.046275802
test_loss: 0.05240313
train_loss: 0.045931727
test_loss: 0.052018274
train_loss: 0.04438477
test_loss: 0.052228224
train_loss: 0.04393995
test_loss: 0.05220949
train_loss: 0.04633228
test_loss: 0.051845744
train_loss: 0.04451549
test_loss: 0.05137334
train_loss: 0.04414412
test_loss: 0.05123823
train_loss: 0.044045907
test_loss: 0.05073431
train_loss: 0.044590272
test_loss: 0.05089299
train_loss: 0.04583125
test_loss: 0.050703947
train_loss: 0.044142883
test_loss: 0.050470613
train_loss: 0.04362227
test_loss: 0.05016347
train_loss: 0.045926686
test_loss: 0.04989582
train_loss: 0.043323345
test_loss: 0.049875237
train_loss: 0.043565363
test_loss: 0.049474925
train_loss: 0.04460143
test_loss: 0.049692485
train_loss: 0.042295203
test_loss: 0.04961275
train_loss: 0.04693643
test_loss: 0.049462605
train_loss: 0.043873783
test_loss: 0.048857454
train_loss: 0.04286266
test_loss: 0.048898734
train_loss: 0.046809405
test_loss: 0.049092196
train_loss: 0.043675475
test_loss: 0.0490294
train_loss: 0.041449964
test_loss: 0.048381243
train_loss: 0.04247403
test_loss: 0.0479223
train_loss: 0.04432171
test_loss: 0.048201915
train_loss: 0.04024725
test_loss: 0.04795167
train_loss: 0.043599583
test_loss: 0.047500476
train_loss: 0.03897253
test_loss: 0.04757556
train_loss: 0.039423943
test_loss: 0.04719433
train_loss: 0.039583832
test_loss: 0.04765121
train_loss: 0.040407248
test_loss: 0.04742158
train_loss: 0.03929263
test_loss: 0.04696714
train_loss: 0.040016197
test_loss: 0.046596427
train_loss: 0.041777395
test_loss: 0.046684947
train_loss: 0.041794993
test_loss: 0.04657429
train_loss: 0.040248394
test_loss: 0.046557255
train_loss: 0.040581368
test_loss: 0.046053667
train_loss: 0.040664114
test_loss: 0.046560686
train_loss: 0.03723447
test_loss: 0.04605536
train_loss: 0.03803916
test_loss: 0.04569509
train_loss: 0.037827037
test_loss: 0.045730047
train_loss: 0.039805647
test_loss: 0.04578479
train_loss: 0.04065945
test_loss: 0.045505222
train_loss: 0.03952407
test_loss: 0.045644708
train_loss: 0.03785397
test_loss: 0.045295462
train_loss: 0.03911744
test_loss: 0.045873314
train_loss: 0.03839869
test_loss: 0.045925193
train_loss: 0.037168264
test_loss: 0.045186825
train_loss: 0.0399247
test_loss: 0.044924863
train_loss: 0.03934153
test_loss: 0.04451548
train_loss: 0.036677487
test_loss: 0.044563133
train_loss: 0.03796424
test_loss: 0.04436075
train_loss: 0.038511477
test_loss: 0.04417469
train_loss: 0.040049393
test_loss: 0.04392581
train_loss: 0.036494024
test_loss: 0.0443504
train_loss: 0.036347948
test_loss: 0.044100795
train_loss: 0.03766893
test_loss: 0.0443264
train_loss: 0.039884266
test_loss: 0.04413812
train_loss: 0.035240933
test_loss: 0.044140723
train_loss: 0.037200212
test_loss: 0.043323852
train_loss: 0.037291653
test_loss: 0.04384008
train_loss: 0.037744064
test_loss: 0.04349816
train_loss: 0.03826528
test_loss: 0.04342149
train_loss: 0.038197774
test_loss: 0.0434497
train_loss: 0.035157844
test_loss: 0.04353166
train_loss: 0.037073854
test_loss: 0.04339583
train_loss: 0.038363747
test_loss: 0.043549683
train_loss: 0.037467677
test_loss: 0.04277403
train_loss: 0.03534434
test_loss: 0.04274797
train_loss: 0.036224835
test_loss: 0.042875472
train_loss: 0.03863231
test_loss: 0.043068852
train_loss: 0.03602372
test_loss: 0.04222162
train_loss: 0.03851249
test_loss: 0.04215331
train_loss: 0.034480482
test_loss: 0.042238057
train_loss: 0.03534257
test_loss: 0.04199206
train_loss: 0.035717446
test_loss: 0.041945204
train_loss: 0.03504031
test_loss: 0.042022806
train_loss: 0.035995547
test_loss: 0.042033438
train_loss: 0.036950268
test_loss: 0.04153168
train_loss: 0.03574051
test_loss: 0.041008364
train_loss: 0.033884645
test_loss: 0.041200165
train_loss: 0.035068955
test_loss: 0.041056216
train_loss: 0.033296555
test_loss: 0.040865045
train_loss: 0.034472313
test_loss: 0.04072102
train_loss: 0.032907054
test_loss: 0.040527027
train_loss: 0.034910634
test_loss: 0.040473145
train_loss: 0.03384003
test_loss: 0.04038498
train_loss: 0.03186943
test_loss: 0.040749814
train_loss: 0.031195905
test_loss: 0.040688742
train_loss: 0.033951107
test_loss: 0.03964967
train_loss: 0.033141363
test_loss: 0.03968756
train_loss: 0.032888718
test_loss: 0.040076844
train_loss: 0.03139029
test_loss: 0.03968497
train_loss: 0.032926857
test_loss: 0.039897427
train_loss: 0.031603012
test_loss: 0.03922924
train_loss: 0.034163054
test_loss: 0.039320976
train_loss: 0.029698107
test_loss: 0.039047662
train_loss: 0.03184125
test_loss: 0.038763467
train_loss: 0.030023832
test_loss: 0.0383372
train_loss: 0.03003136
test_loss: 0.03831489
train_loss: 0.033831768
test_loss: 0.038966626
train_loss: 0.03336206
test_loss: 0.03813854
train_loss: 0.03128908
test_loss: 0.038118437
train_loss: 0.03063304
test_loss: 0.037762504
train_loss: 0.03242391
test_loss: 0.037958134
train_loss: 0.031796414
test_loss: 0.037677586
train_loss: 0.031426758
test_loss: 0.03737038
train_loss: 0.030906595
test_loss: 0.03760058
train_loss: 0.030019179
test_loss: 0.037095197
train_loss: 0.03076264
test_loss: 0.037121847
train_loss: 0.03192481
test_loss: 0.036593225
train_loss: 0.030557565
test_loss: 0.036661107
train_loss: 0.029436836
test_loss: 0.036323562
train_loss: 0.029433044
test_loss: 0.036676634
train_loss: 0.03091028
test_loss: 0.03609449
train_loss: 0.029686041
test_loss: 0.035670407
train_loss: 0.029565055
test_loss: 0.03612095
train_loss: 0.028370192
test_loss: 0.035209566
train_loss: 0.027923983
test_loss: 0.034901273
train_loss: 0.027024254
test_loss: 0.034436837
train_loss: 0.029999245
test_loss: 0.034376618
train_loss: 0.028401865
test_loss: 0.034402754
train_loss: 0.02863197
test_loss: 0.03439902
train_loss: 0.028382841
test_loss: 0.03355772
train_loss: 0.029391516
test_loss: 0.033566255
train_loss: 0.02790152
test_loss: 0.03326402
train_loss: 0.0290386
test_loss: 0.033194777
train_loss: 0.0263832
test_loss: 0.033028062
train_loss: 0.026846074
test_loss: 0.032592244
train_loss: 0.025926312
test_loss: 0.032306563
train_loss: 0.027878541
test_loss: 0.03230754
train_loss: 0.025960714
test_loss: 0.03210178
train_loss: 0.02692814
test_loss: 0.032058515
train_loss: 0.02757895
test_loss: 0.031733997
train_loss: 0.025667865
test_loss: 0.03163564
train_loss: 0.025543496
test_loss: 0.031184612
train_loss: 0.024776446
test_loss: 0.031217888
train_loss: 0.025162278
test_loss: 0.03106243
train_loss: 0.024910368
test_loss: 0.03047238
train_loss: 0.023830183
test_loss: 0.030727971
train_loss: 0.024886426
test_loss: 0.029497614
train_loss: 0.02373318
test_loss: 0.02977104
train_loss: 0.023741418
test_loss: 0.029575527
train_loss: 0.024642834
test_loss: 0.029591465
train_loss: 0.025390776
test_loss: 0.029352248
train_loss: 0.024100484
test_loss: 0.029329818
train_loss: 0.02216927
test_loss: 0.02876279
train_loss: 0.023780156
test_loss: 0.029604549
train_loss: 0.02296748
test_loss: 0.02831462
train_loss: 0.02349931
test_loss: 0.028162098
train_loss: 0.02218735
test_loss: 0.028075963
train_loss: 0.023029957
test_loss: 0.027687812
train_loss: 0.022985302
test_loss: 0.028084762
train_loss: 0.022762055
test_loss: 0.027935335
train_loss: 0.0234125
test_loss: 0.027368398
train_loss: 0.023013698
test_loss: 0.02675379
train_loss: 0.024444895
test_loss: 0.027711129
train_loss: 0.024023853
test_loss: 0.027323725
train_loss: 0.02271446
test_loss: 0.026716018
train_loss: 0.022807129
test_loss: 0.027071936
train_loss: 0.02280601
test_loss: 0.026958538
train_loss: 0.023045322
test_loss: 0.025930982
train_loss: 0.022255687
test_loss: 0.025752239
train_loss: 0.02165683
test_loss: 0.026169354
train_loss: 0.021870669
test_loss: 0.026029082
train_loss: 0.02275145
test_loss: 0.026082689
train_loss: 0.021579375
test_loss: 0.025403176
train_loss: 0.020645075
test_loss: 0.02554073
train_loss: 0.020306084
test_loss: 0.025278663
train_loss: 0.022557227
test_loss: 0.025202746
train_loss: 0.021272339
test_loss: 0.025293255
train_loss: 0.020377424
test_loss: 0.024561882
train_loss: 0.020100199
test_loss: 0.024691999
train_loss: 0.021133857
test_loss: 0.02474633
train_loss: 0.019864097
test_loss: 0.024763608
train_loss: 0.01994041
test_loss: 0.024483874
train_loss: 0.019846585
test_loss: 0.023415796
train_loss: 0.019717325
test_loss: 0.024266742
train_loss: 0.020285914
test_loss: 0.024261849
train_loss: 0.020393167
test_loss: 0.023625588
train_loss: 0.01924384
test_loss: 0.0237791
train_loss: 0.019449215
test_loss: 0.023671687
train_loss: 0.019873142
test_loss: 0.022997558
train_loss: 0.02044115
test_loss: 0.023053082
train_loss: 0.019372169
test_loss: 0.023645984
train_loss: 0.020113023
test_loss: 0.023149084
train_loss: 0.018210316
test_loss: 0.02337798
train_loss: 0.018413529
test_loss: 0.02277421
train_loss: 0.018559532
test_loss: 0.022683565
train_loss: 0.02060799
test_loss: 0.022981705
train_loss: 0.018580463
test_loss: 0.022133233
train_loss: 0.01834875
test_loss: 0.022462204
train_loss: 0.018745132
test_loss: 0.02219708
train_loss: 0.019624034
test_loss: 0.022368027
train_loss: 0.018492451
test_loss: 0.022204243
train_loss: 0.018643364
test_loss: 0.021779994
train_loss: 0.01838993
test_loss: 0.021960644
train_loss: 0.01812394
test_loss: 0.021562837
train_loss: 0.01811398
test_loss: 0.021269707
train_loss: 0.01832654
test_loss: 0.021875603
train_loss: 0.018433975
test_loss: 0.021173194
train_loss: 0.017877823
test_loss: 0.02113329
train_loss: 0.018456992
test_loss: 0.021675639
train_loss: 0.01805792
test_loss: 0.021312883
train_loss: 0.017135015
test_loss: 0.020906929
train_loss: 0.017802175
test_loss: 0.020693704
train_loss: 0.017286304
test_loss: 0.020657275
train_loss: 0.018129177
test_loss: 0.020596592
train_loss: 0.019023169
test_loss: 0.020415569
train_loss: 0.017316697
test_loss: 0.021005107
train_loss: 0.016682768
test_loss: 0.020815102
train_loss: 0.017103568
test_loss: 0.019796608
train_loss: 0.017262727
test_loss: 0.019781543
train_loss: 0.016381212
test_loss: 0.01946422
train_loss: 0.017644288
test_loss: 0.019865952
train_loss: 0.01706223
test_loss: 0.020105593
train_loss: 0.016968131
test_loss: 0.019987334
train_loss: 0.016920093
test_loss: 0.020277184
train_loss: 0.01681232
test_loss: 0.020258907
train_loss: 0.016634205
test_loss: 0.019839246/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

train_loss: 0.016922416
test_loss: 0.019655721
train_loss: 0.017048579
test_loss: 0.020698044
train_loss: 0.01701155
test_loss: 0.019671561
train_loss: 0.016814021
test_loss: 0.019773113
train_loss: 0.017412152
test_loss: 0.019655118
train_loss: 0.016870566
test_loss: 0.019093439
train_loss: 0.016516605
test_loss: 0.019575847
train_loss: 0.015788767
test_loss: 0.019077264
train_loss: 0.016433697
test_loss: 0.018721147
train_loss: 0.016733015
test_loss: 0.019726826
train_loss: 0.015618404
test_loss: 0.018742625
train_loss: 0.015544923
test_loss: 0.018677926
train_loss: 0.015875492
test_loss: 0.019201165
train_loss: 0.016708694
test_loss: 0.018671853
train_loss: 0.0149918115
test_loss: 0.01863123
train_loss: 0.015643088
test_loss: 0.019148797
train_loss: 0.015827851
test_loss: 0.018344784
train_loss: 0.015716841
test_loss: 0.01850972
train_loss: 0.015313441
test_loss: 0.018271325
train_loss: 0.015469257
test_loss: 0.01826802
train_loss: 0.016459757
test_loss: 0.019170402
train_loss: 0.0155284945
test_loss: 0.018810937
train_loss: 0.016172288
test_loss: 0.018223861
train_loss: 0.016663041
test_loss: 0.018256914
train_loss: 0.016428921
test_loss: 0.018302903
train_loss: 0.016535783
test_loss: 0.018336795
train_loss: 0.015305523
test_loss: 0.018504808
train_loss: 0.015691113
test_loss: 0.018086575
train_loss: 0.015138598
test_loss: 0.017496893
train_loss: 0.015230769
test_loss: 0.018284144
train_loss: 0.015552506
test_loss: 0.017381685
train_loss: 0.014878757
test_loss: 0.017952995
train_loss: 0.0156143615
test_loss: 0.017893422
train_loss: 0.013776382
test_loss: 0.017672725
train_loss: 0.016060729
test_loss: 0.017818898
train_loss: 0.014915561
test_loss: 0.017555874
train_loss: 0.016208591
test_loss: 0.01752501
train_loss: 0.014299512
test_loss: 0.017570164
train_loss: 0.01544424
test_loss: 0.01666256
train_loss: 0.014839907
test_loss: 0.017444754
train_loss: 0.015940746
test_loss: 0.017254954
train_loss: 0.014562655
test_loss: 0.0173726
train_loss: 0.01426778
test_loss: 0.01738911
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output67/f1_psi-2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output66/f1_psi-2_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output67/f1_psi-2_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c11b69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c10f16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c11b6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c11b6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c1139268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c1139598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c1076ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c1029598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c1023048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0fc2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c1029378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0fa2c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0fa5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0f66598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0f66620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0f087b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0ed52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0ed9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0eb28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0eb5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0e54158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0e71d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0e71840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0dec598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0dd1488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85c0da0e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85a93a48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85a93b12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85a93b1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85a93b1378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85a932a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85a92cf510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85a92cf0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85a92cf9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85a9293a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85a92937b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000605500303
Iter: 2 loss: 0.000951754337
Iter: 3 loss: 0.000550098193
Iter: 4 loss: 0.000503577641
Iter: 5 loss: 0.000620999606
Iter: 6 loss: 0.000486474
Iter: 7 loss: 0.000459922041
Iter: 8 loss: 0.000436317292
Iter: 9 loss: 0.000429596112
Iter: 10 loss: 0.000393315102
Iter: 11 loss: 0.000609860232
Iter: 12 loss: 0.000388299493
Iter: 13 loss: 0.000370377733
Iter: 14 loss: 0.000398994714
Iter: 15 loss: 0.000362151361
Iter: 16 loss: 0.000344303378
Iter: 17 loss: 0.000397141324
Iter: 18 loss: 0.000338836981
Iter: 19 loss: 0.000323000073
Iter: 20 loss: 0.000338471611
Iter: 21 loss: 0.00031388583
Iter: 22 loss: 0.000299874868
Iter: 23 loss: 0.00037162716
Iter: 24 loss: 0.00029759953
Iter: 25 loss: 0.000288711715
Iter: 26 loss: 0.000292373268
Iter: 27 loss: 0.000282600551
Iter: 28 loss: 0.000273528422
Iter: 29 loss: 0.000322073669
Iter: 30 loss: 0.00027214532
Iter: 31 loss: 0.000265937182
Iter: 32 loss: 0.000269946468
Iter: 33 loss: 0.000262004498
Iter: 34 loss: 0.000255720108
Iter: 35 loss: 0.000286218856
Iter: 36 loss: 0.00025461425
Iter: 37 loss: 0.000257814885
Iter: 38 loss: 0.000252411352
Iter: 39 loss: 0.000250205485
Iter: 40 loss: 0.000245255127
Iter: 41 loss: 0.000314117642
Iter: 42 loss: 0.000244980678
Iter: 43 loss: 0.000240446301
Iter: 44 loss: 0.000276349543
Iter: 45 loss: 0.000240144902
Iter: 46 loss: 0.000237330445
Iter: 47 loss: 0.000235638
Iter: 48 loss: 0.000234473846
Iter: 49 loss: 0.000230711608
Iter: 50 loss: 0.000252879341
Iter: 51 loss: 0.000230227626
Iter: 52 loss: 0.000227580182
Iter: 53 loss: 0.000227686847
Iter: 54 loss: 0.000225492171
Iter: 55 loss: 0.000221738723
Iter: 56 loss: 0.000247503689
Iter: 57 loss: 0.000221376904
Iter: 58 loss: 0.000219334033
Iter: 59 loss: 0.000222364557
Iter: 60 loss: 0.000218350644
Iter: 61 loss: 0.000215456646
Iter: 62 loss: 0.000218071407
Iter: 63 loss: 0.000213773965
Iter: 64 loss: 0.000211304519
Iter: 65 loss: 0.000220205344
Iter: 66 loss: 0.000210684389
Iter: 67 loss: 0.000208301019
Iter: 68 loss: 0.000217255132
Iter: 69 loss: 0.000207725156
Iter: 70 loss: 0.000205672826
Iter: 71 loss: 0.000220185626
Iter: 72 loss: 0.000205488876
Iter: 73 loss: 0.000203975098
Iter: 74 loss: 0.00020395068
Iter: 75 loss: 0.000203343283
Iter: 76 loss: 0.000201760413
Iter: 77 loss: 0.00021410585
Iter: 78 loss: 0.000201458068
Iter: 79 loss: 0.000199790549
Iter: 80 loss: 0.000211805091
Iter: 81 loss: 0.000199644564
Iter: 82 loss: 0.000198161579
Iter: 83 loss: 0.000199327449
Iter: 84 loss: 0.000197267596
Iter: 85 loss: 0.000195602741
Iter: 86 loss: 0.000196192355
Iter: 87 loss: 0.000194431763
Iter: 88 loss: 0.000192588661
Iter: 89 loss: 0.00020717131
Iter: 90 loss: 0.00019246267
Iter: 91 loss: 0.000191282597
Iter: 92 loss: 0.000192107749
Iter: 93 loss: 0.00019054873
Iter: 94 loss: 0.0001889634
Iter: 95 loss: 0.000195716479
Iter: 96 loss: 0.000188633217
Iter: 97 loss: 0.000187250858
Iter: 98 loss: 0.000190404447
Iter: 99 loss: 0.000186732912
Iter: 100 loss: 0.000185317171
Iter: 101 loss: 0.000190629216
Iter: 102 loss: 0.000184977282
Iter: 103 loss: 0.000183877099
Iter: 104 loss: 0.000188992271
Iter: 105 loss: 0.000183671626
Iter: 106 loss: 0.000183472279
Iter: 107 loss: 0.000183167576
Iter: 108 loss: 0.000182811709
Iter: 109 loss: 0.000181805663
Iter: 110 loss: 0.000186951453
Iter: 111 loss: 0.00018147938
Iter: 112 loss: 0.00018045
Iter: 113 loss: 0.000184545919
Iter: 114 loss: 0.000180217132
Iter: 115 loss: 0.00017929
Iter: 116 loss: 0.000185273529
Iter: 117 loss: 0.000179190392
Iter: 118 loss: 0.000178442453
Iter: 119 loss: 0.000179084804
Iter: 120 loss: 0.000178001384
Iter: 121 loss: 0.000177207592
Iter: 122 loss: 0.000177548718
Iter: 123 loss: 0.000176664238
Iter: 124 loss: 0.000175594512
Iter: 125 loss: 0.000178421527
Iter: 126 loss: 0.000175234265
Iter: 127 loss: 0.000174288492
Iter: 128 loss: 0.000179449315
Iter: 129 loss: 0.000174149944
Iter: 130 loss: 0.000173321649
Iter: 131 loss: 0.000173814769
Iter: 132 loss: 0.000172786924
Iter: 133 loss: 0.000171755892
Iter: 134 loss: 0.000177954178
Iter: 135 loss: 0.000171627296
Iter: 136 loss: 0.000170781539
Iter: 137 loss: 0.000172600034
Iter: 138 loss: 0.000170452418
Iter: 139 loss: 0.000170755171
Iter: 140 loss: 0.000170187
Iter: 141 loss: 0.000169933177
Iter: 142 loss: 0.000169327774
Iter: 143 loss: 0.000176142756
Iter: 144 loss: 0.000169270279
Iter: 145 loss: 0.000168668746
Iter: 146 loss: 0.000168468192
Iter: 147 loss: 0.000168122671
Iter: 148 loss: 0.000167421706
Iter: 149 loss: 0.000175161331
Iter: 150 loss: 0.000167406542
Iter: 151 loss: 0.000166824873
Iter: 152 loss: 0.000168506376
Iter: 153 loss: 0.000166642873
Iter: 154 loss: 0.000166076483
Iter: 155 loss: 0.000165626494
Iter: 156 loss: 0.000165453981
Iter: 157 loss: 0.000164683297
Iter: 158 loss: 0.000168831626
Iter: 159 loss: 0.000164568075
Iter: 160 loss: 0.000163924662
Iter: 161 loss: 0.000164679426
Iter: 162 loss: 0.000163581659
Iter: 163 loss: 0.000162798737
Iter: 164 loss: 0.000164538447
Iter: 165 loss: 0.000162499113
Iter: 166 loss: 0.00016174656
Iter: 167 loss: 0.000165635807
Iter: 168 loss: 0.000161626958
Iter: 169 loss: 0.000161033939
Iter: 170 loss: 0.000163165096
Iter: 171 loss: 0.00016088385
Iter: 172 loss: 0.00016068868
Iter: 173 loss: 0.000160614989
Iter: 174 loss: 0.000160302516
Iter: 175 loss: 0.000160141208
Iter: 176 loss: 0.000159995587
Iter: 177 loss: 0.000159650212
Iter: 178 loss: 0.000159055082
Iter: 179 loss: 0.000159055518
Iter: 180 loss: 0.000158440176
Iter: 181 loss: 0.000161622884
Iter: 182 loss: 0.000158341543
Iter: 183 loss: 0.000157833012
Iter: 184 loss: 0.000161316595
Iter: 185 loss: 0.000157782692
Iter: 186 loss: 0.000157354298
Iter: 187 loss: 0.000157797826
Iter: 188 loss: 0.000157116127
Iter: 189 loss: 0.000156654278
Iter: 190 loss: 0.00015647721
Iter: 191 loss: 0.000156224778
Iter: 192 loss: 0.000155560905
Iter: 193 loss: 0.000158685696
Iter: 194 loss: 0.00015543832
Iter: 195 loss: 0.000154887108
Iter: 196 loss: 0.000156207941
Iter: 197 loss: 0.000154687048
Iter: 198 loss: 0.000154095134
Iter: 199 loss: 0.000155447502
Iter: 200 loss: 0.000153873465
Iter: 201 loss: 0.000153264933
Iter: 202 loss: 0.000154801339
Iter: 203 loss: 0.000153053741
Iter: 204 loss: 0.000152632143
Iter: 205 loss: 0.00015262574
Iter: 206 loss: 0.000152221735
Iter: 207 loss: 0.000155229573
Iter: 208 loss: 0.000152190361
Iter: 209 loss: 0.000152018794
Iter: 210 loss: 0.000151585569
Iter: 211 loss: 0.000155396774
Iter: 212 loss: 0.0001515162
Iter: 213 loss: 0.000151050102
Iter: 214 loss: 0.000153122193
Iter: 215 loss: 0.000150957014
Iter: 216 loss: 0.000150570646
Iter: 217 loss: 0.000152587163
Iter: 218 loss: 0.000150509833
Iter: 219 loss: 0.000150158099
Iter: 220 loss: 0.000151219036
Iter: 221 loss: 0.0001500535
Iter: 222 loss: 0.000149695654
Iter: 223 loss: 0.000149509084
Iter: 224 loss: 0.000149344618
Iter: 225 loss: 0.000148886233
Iter: 226 loss: 0.000151191693
Iter: 227 loss: 0.000148809515
Iter: 228 loss: 0.000148390362
Iter: 229 loss: 0.000148766529
Iter: 230 loss: 0.000148145686
Iter: 231 loss: 0.000147676823
Iter: 232 loss: 0.000149535699
Iter: 233 loss: 0.000147569226
Iter: 234 loss: 0.000147082581
Iter: 235 loss: 0.000147495462
Iter: 236 loss: 0.000146794904
Iter: 237 loss: 0.000146339342
Iter: 238 loss: 0.000151369299
Iter: 239 loss: 0.000146329912
Iter: 240 loss: 0.000146270148
Iter: 241 loss: 0.00014614602
Iter: 242 loss: 0.00014605264
Iter: 243 loss: 0.000145760365
Iter: 244 loss: 0.000146252773
Iter: 245 loss: 0.000145562401
Iter: 246 loss: 0.000145116312
Iter: 247 loss: 0.000146592269
Iter: 248 loss: 0.000144994294
Iter: 249 loss: 0.000144611433
Iter: 250 loss: 0.00014711445
Iter: 251 loss: 0.000144570295
Iter: 252 loss: 0.000144226578
Iter: 253 loss: 0.000145286525
Iter: 254 loss: 0.000144126
Iter: 255 loss: 0.000143798563
Iter: 256 loss: 0.000144146
Iter: 257 loss: 0.000143617304
Iter: 258 loss: 0.000143231475
Iter: 259 loss: 0.000143613681
Iter: 260 loss: 0.000143013196
Iter: 261 loss: 0.000142572972
Iter: 262 loss: 0.000143877187
Iter: 263 loss: 0.000142437566
Iter: 264 loss: 0.000142052886
Iter: 265 loss: 0.00014327254
Iter: 266 loss: 0.000141942728
Iter: 267 loss: 0.000141541095
Iter: 268 loss: 0.000142132034
Iter: 269 loss: 0.000141347075
Iter: 270 loss: 0.000140979973
Iter: 271 loss: 0.00014247674
Iter: 272 loss: 0.000140900942
Iter: 273 loss: 0.000140994904
Iter: 274 loss: 0.000140757853
Iter: 275 loss: 0.000140636039
Iter: 276 loss: 0.000140320481
Iter: 277 loss: 0.000142773395
Iter: 278 loss: 0.000140258358
Iter: 279 loss: 0.000139971729
Iter: 280 loss: 0.000140109667
Iter: 281 loss: 0.000139779149
Iter: 282 loss: 0.000139422453
Iter: 283 loss: 0.000140845324
Iter: 284 loss: 0.000139342053
Iter: 285 loss: 0.000139010168
Iter: 286 loss: 0.00014099323
Iter: 287 loss: 0.00013896884
Iter: 288 loss: 0.000138700765
Iter: 289 loss: 0.000139072872
Iter: 290 loss: 0.000138567906
Iter: 291 loss: 0.000138269577
Iter: 292 loss: 0.000138595336
Iter: 293 loss: 0.000138107178
Iter: 294 loss: 0.000137788957
Iter: 295 loss: 0.000138531759
Iter: 296 loss: 0.000137671901
Iter: 297 loss: 0.000137308627
Iter: 298 loss: 0.000137794021
Iter: 299 loss: 0.000137125593
Iter: 300 loss: 0.000136725255
Iter: 301 loss: 0.000138279545
Iter: 302 loss: 0.000136630842
Iter: 303 loss: 0.000136296891
Iter: 304 loss: 0.000136969436
Iter: 305 loss: 0.000136159157
Iter: 306 loss: 0.000136129398
Iter: 307 loss: 0.000136016373
Iter: 308 loss: 0.000135843511
Iter: 309 loss: 0.00013574271
Iter: 310 loss: 0.000135670271
Iter: 311 loss: 0.000135514856
Iter: 312 loss: 0.000135222508
Iter: 313 loss: 0.000141643293
Iter: 314 loss: 0.000135221431
Iter: 315 loss: 0.000134900503
Iter: 316 loss: 0.000136615476
Iter: 317 loss: 0.000134851638
Iter: 318 loss: 0.000134606424
Iter: 319 loss: 0.000135941867
Iter: 320 loss: 0.000134571077
Iter: 321 loss: 0.000134315342
Iter: 322 loss: 0.00013478272
Iter: 323 loss: 0.000134202593
Iter: 324 loss: 0.000133962778
Iter: 325 loss: 0.000134268965
Iter: 326 loss: 0.000133839087
Iter: 327 loss: 0.000133552
Iter: 328 loss: 0.000133930065
Iter: 329 loss: 0.000133408
Iter: 330 loss: 0.000133067137
Iter: 331 loss: 0.000133960275
Iter: 332 loss: 0.000132952133
Iter: 333 loss: 0.000132634203
Iter: 334 loss: 0.000133783964
Iter: 335 loss: 0.00013255376
Iter: 336 loss: 0.000132269372
Iter: 337 loss: 0.00013270929
Iter: 338 loss: 0.000132136091
Iter: 339 loss: 0.000131866429
Iter: 340 loss: 0.000133925583
Iter: 341 loss: 0.000131846726
Iter: 342 loss: 0.000131626875
Iter: 343 loss: 0.000131625842
Iter: 344 loss: 0.000131539142
Iter: 345 loss: 0.000131304638
Iter: 346 loss: 0.000132891917
Iter: 347 loss: 0.000131250024
Iter: 348 loss: 0.000130996428
Iter: 349 loss: 0.000131341963
Iter: 350 loss: 0.00013086907
Iter: 351 loss: 0.00013055405
Iter: 352 loss: 0.000131509209
Iter: 353 loss: 0.000130459433
Iter: 354 loss: 0.000130184519
Iter: 355 loss: 0.00013354962
Iter: 356 loss: 0.000130180677
Iter: 357 loss: 0.000129997265
Iter: 358 loss: 0.000129895066
Iter: 359 loss: 0.00012981586
Iter: 360 loss: 0.000129553431
Iter: 361 loss: 0.00013027899
Iter: 362 loss: 0.000129467691
Iter: 363 loss: 0.000129199441
Iter: 364 loss: 0.00012955151
Iter: 365 loss: 0.000129063992
Iter: 366 loss: 0.000128757703
Iter: 367 loss: 0.000130185828
Iter: 368 loss: 0.000128701009
Iter: 369 loss: 0.0001284725
Iter: 370 loss: 0.000128927277
Iter: 371 loss: 0.000128377695
Iter: 372 loss: 0.000128105792
Iter: 373 loss: 0.000128478321
Iter: 374 loss: 0.000127970125
Iter: 375 loss: 0.000128228421
Iter: 376 loss: 0.000127873107
Iter: 377 loss: 0.000127804829
Iter: 378 loss: 0.000127610576
Iter: 379 loss: 0.000128583575
Iter: 380 loss: 0.000127545223
Iter: 381 loss: 0.000127334017
Iter: 382 loss: 0.00012756612
Iter: 383 loss: 0.000127217761
Iter: 384 loss: 0.000126987041
Iter: 385 loss: 0.000127792489
Iter: 386 loss: 0.000126926592
Iter: 387 loss: 0.000126733605
Iter: 388 loss: 0.000127974155
Iter: 389 loss: 0.000126712257
Iter: 390 loss: 0.000126509898
Iter: 391 loss: 0.000126909377
Iter: 392 loss: 0.000126426676
Iter: 393 loss: 0.000126242056
Iter: 394 loss: 0.000126440864
Iter: 395 loss: 0.00012614124
Iter: 396 loss: 0.000125916398
Iter: 397 loss: 0.000126132829
Iter: 398 loss: 0.000125788531
Iter: 399 loss: 0.000125550781
Iter: 400 loss: 0.000126645376
Iter: 401 loss: 0.000125505874
Iter: 402 loss: 0.000125273466
Iter: 403 loss: 0.000125502789
Iter: 404 loss: 0.000125142935
Iter: 405 loss: 0.00012487371
Iter: 406 loss: 0.000126391911
Iter: 407 loss: 0.000124837286
Iter: 408 loss: 0.000124728103
Iter: 409 loss: 0.000124720711
Iter: 410 loss: 0.000124576763
Iter: 411 loss: 0.000124575949
Iter: 412 loss: 0.000124461076
Iter: 413 loss: 0.000124339364
Iter: 414 loss: 0.00012411995
Iter: 415 loss: 0.000129434615
Iter: 416 loss: 0.00012412027
Iter: 417 loss: 0.000123871738
Iter: 418 loss: 0.000124375263
Iter: 419 loss: 0.000123771068
Iter: 420 loss: 0.000123510108
Iter: 421 loss: 0.000124456012
Iter: 422 loss: 0.000123444464
Iter: 423 loss: 0.00012324733
Iter: 424 loss: 0.000125730512
Iter: 425 loss: 0.000123245569
Iter: 426 loss: 0.000123081161
Iter: 427 loss: 0.00012303784
Iter: 428 loss: 0.000122935307
Iter: 429 loss: 0.000122715428
Iter: 430 loss: 0.000122989732
Iter: 431 loss: 0.000122601385
Iter: 432 loss: 0.000122351485
Iter: 433 loss: 0.000122942525
Iter: 434 loss: 0.000122260171
Iter: 435 loss: 0.000121986079
Iter: 436 loss: 0.00012266211
Iter: 437 loss: 0.000121889592
Iter: 438 loss: 0.000121656733
Iter: 439 loss: 0.000122703612
Iter: 440 loss: 0.000121611753
Iter: 441 loss: 0.00012139781
Iter: 442 loss: 0.000122768804
Iter: 443 loss: 0.000121374891
Iter: 444 loss: 0.000121197125
Iter: 445 loss: 0.000121197445
Iter: 446 loss: 0.000121129029
Iter: 447 loss: 0.00012093813
Iter: 448 loss: 0.000121962599
Iter: 449 loss: 0.000120879471
Iter: 450 loss: 0.000120639917
Iter: 451 loss: 0.000121161764
Iter: 452 loss: 0.000120548422
Iter: 453 loss: 0.000120327059
Iter: 454 loss: 0.00012092937
Iter: 455 loss: 0.000120254386
Iter: 456 loss: 0.000120058918
Iter: 457 loss: 0.000121550358
Iter: 458 loss: 0.00012004489
Iter: 459 loss: 0.000119859615
Iter: 460 loss: 0.000120412464
Iter: 461 loss: 0.000119804055
Iter: 462 loss: 0.00011964314
Iter: 463 loss: 0.000119632277
Iter: 464 loss: 0.000119512035
Iter: 465 loss: 0.000119285149
Iter: 466 loss: 0.000119741198
Iter: 467 loss: 0.000119192278
Iter: 468 loss: 0.000118974072
Iter: 469 loss: 0.000119751487
Iter: 470 loss: 0.000118918644
Iter: 471 loss: 0.00011870145
Iter: 472 loss: 0.00011911316
Iter: 473 loss: 0.000118610122
Iter: 474 loss: 0.000118409429
Iter: 475 loss: 0.000120110308
Iter: 476 loss: 0.000118398064
Iter: 477 loss: 0.00011834444
Iter: 478 loss: 0.000118307711
Iter: 479 loss: 0.00011825141
Iter: 480 loss: 0.000118089323
Iter: 481 loss: 0.000118767246
Iter: 482 loss: 0.000118025404
Iter: 483 loss: 0.00011781638
Iter: 484 loss: 0.000118201569
Iter: 485 loss: 0.000117726522
Iter: 486 loss: 0.000117510797
Iter: 487 loss: 0.000117704825
Iter: 488 loss: 0.000117385782
Iter: 489 loss: 0.000117142423
Iter: 490 loss: 0.000118982411
Iter: 491 loss: 0.000117124742
Iter: 492 loss: 0.000116948242
Iter: 493 loss: 0.000117931151
Iter: 494 loss: 0.000116923402
Iter: 495 loss: 0.000116741372
Iter: 496 loss: 0.000116832802
Iter: 497 loss: 0.000116621
Iter: 498 loss: 0.000116421419
Iter: 499 loss: 0.000116664138
Iter: 500 loss: 0.00011631642
Iter: 501 loss: 0.000116098468
Iter: 502 loss: 0.000116727679
Iter: 503 loss: 0.000116029536
Iter: 504 loss: 0.000115803166
Iter: 505 loss: 0.000116113035
Iter: 506 loss: 0.000115689872
Iter: 507 loss: 0.000115449962
Iter: 508 loss: 0.000116424133
Iter: 509 loss: 0.000115396906
Iter: 510 loss: 0.000115484901
Iter: 511 loss: 0.000115319388
Iter: 512 loss: 0.000115247851
Iter: 513 loss: 0.000115084069
Iter: 514 loss: 0.000117234966
Iter: 515 loss: 0.000115073322
Iter: 516 loss: 0.000114924871
Iter: 517 loss: 0.000114809438
Iter: 518 loss: 0.000114763294
Iter: 519 loss: 0.000114502545
Iter: 520 loss: 0.00011531022
Iter: 521 loss: 0.0001144259
Iter: 522 loss: 0.000114207724
Iter: 523 loss: 0.000115232993
Iter: 524 loss: 0.000114166767
Iter: 525 loss: 0.000113995222
Iter: 526 loss: 0.000114764087
Iter: 527 loss: 0.000113961425
Iter: 528 loss: 0.000113812435
Iter: 529 loss: 0.000114787777
Iter: 530 loss: 0.000113796894
Iter: 531 loss: 0.000113676433
Iter: 532 loss: 0.000113579743
Iter: 533 loss: 0.000113543007
Iter: 534 loss: 0.000113333124
Iter: 535 loss: 0.00011370163
Iter: 536 loss: 0.000113239876
Iter: 537 loss: 0.000113036782
Iter: 538 loss: 0.000113593516
Iter: 539 loss: 0.000112970236
Iter: 540 loss: 0.000112748377
Iter: 541 loss: 0.000113335183
Iter: 542 loss: 0.00011267377
Iter: 543 loss: 0.000112567097
Iter: 544 loss: 0.000112559137
Iter: 545 loss: 0.000112423186
Iter: 546 loss: 0.000112691712
Iter: 547 loss: 0.000112366717
Iter: 548 loss: 0.000112283029
Iter: 549 loss: 0.000112080954
Iter: 550 loss: 0.000114247872
Iter: 551 loss: 0.000112060174
Iter: 552 loss: 0.000111831338
Iter: 553 loss: 0.000112695678
Iter: 554 loss: 0.000111776419
Iter: 555 loss: 0.000111583242
Iter: 556 loss: 0.000111894464
Iter: 557 loss: 0.000111493893
Iter: 558 loss: 0.000111300978
Iter: 559 loss: 0.000112965688
Iter: 560 loss: 0.000111289672
Iter: 561 loss: 0.000111144152
Iter: 562 loss: 0.000111712834
Iter: 563 loss: 0.000111110421
Iter: 564 loss: 0.000110950161
Iter: 565 loss: 0.00011111088
Iter: 566 loss: 0.000110859015
Iter: 567 loss: 0.000110699853
Iter: 568 loss: 0.00011094501
Iter: 569 loss: 0.000110624198
Iter: 570 loss: 0.000110440298
Iter: 571 loss: 0.000110778703
Iter: 572 loss: 0.000110361012
Iter: 573 loss: 0.000110170957
Iter: 574 loss: 0.000110847061
Iter: 575 loss: 0.000110122353
Iter: 576 loss: 0.000109970046
Iter: 577 loss: 0.00011046994
Iter: 578 loss: 0.000109927896
Iter: 579 loss: 0.000109877474
Iter: 580 loss: 0.000109836117
Iter: 581 loss: 0.000109787848
Iter: 582 loss: 0.00010964612
Iter: 583 loss: 0.000110213849
Iter: 584 loss: 0.000109588102
Iter: 585 loss: 0.000109420493
Iter: 586 loss: 0.000109594759
Iter: 587 loss: 0.00010932739
Iter: 588 loss: 0.000109125671
Iter: 589 loss: 0.000109688248
Iter: 590 loss: 0.000109060406
Iter: 591 loss: 0.000108866334
Iter: 592 loss: 0.000109659093
Iter: 593 loss: 0.000108823791
Iter: 594 loss: 0.000108649787
Iter: 595 loss: 0.000109547516
Iter: 596 loss: 0.000108622
Iter: 597 loss: 0.000108467299
Iter: 598 loss: 0.000109349021
Iter: 599 loss: 0.000108446526
Iter: 600 loss: 0.000108333683
Iter: 601 loss: 0.000108286113
Iter: 602 loss: 0.000108227701
Iter: 603 loss: 0.000108055625
Iter: 604 loss: 0.000108511231
Iter: 605 loss: 0.00010799821
Iter: 606 loss: 0.000107847503
Iter: 607 loss: 0.000108236869
Iter: 608 loss: 0.000107795284
Iter: 609 loss: 0.000107619613
Iter: 610 loss: 0.000108032509
Iter: 611 loss: 0.000107555956
Iter: 612 loss: 0.00010764091
Iter: 613 loss: 0.00010750517
Iter: 614 loss: 0.000107454311
Iter: 615 loss: 0.000107323664
Iter: 616 loss: 0.000108405344
Iter: 617 loss: 0.000107300926
Iter: 618 loss: 0.000107172651
Iter: 619 loss: 0.000107177446
Iter: 620 loss: 0.000107071093
Iter: 621 loss: 0.0001068938
Iter: 622 loss: 0.000107268897
Iter: 623 loss: 0.00010682366
Iter: 624 loss: 0.000106627718
Iter: 625 loss: 0.000106896689
Iter: 626 loss: 0.000106530671
Iter: 627 loss: 0.000106320396
Iter: 628 loss: 0.000107603024
Iter: 629 loss: 0.000106295425
Iter: 630 loss: 0.000106142565
Iter: 631 loss: 0.000107013118
Iter: 632 loss: 0.000106121792
Iter: 633 loss: 0.000105972773
Iter: 634 loss: 0.000106398511
Iter: 635 loss: 0.000105925035
Iter: 636 loss: 0.000105795451
Iter: 637 loss: 0.000105786145
Iter: 638 loss: 0.000105689207
Iter: 639 loss: 0.000105515748
Iter: 640 loss: 0.000105884828
Iter: 641 loss: 0.000105447296
Iter: 642 loss: 0.000105284038
Iter: 643 loss: 0.000106072519
Iter: 644 loss: 0.000105254556
Iter: 645 loss: 0.000105152205
Iter: 646 loss: 0.000106656866
Iter: 647 loss: 0.000105152794
Iter: 648 loss: 0.000105017854
Iter: 649 loss: 0.0001051168
Iter: 650 loss: 0.000104935
Iter: 651 loss: 0.000104857259
Iter: 652 loss: 0.000104724735
Iter: 653 loss: 0.000104724895
Iter: 654 loss: 0.000104562547
Iter: 655 loss: 0.00010482363
Iter: 656 loss: 0.000104488645
Iter: 657 loss: 0.000104313956
Iter: 658 loss: 0.000104861327
Iter: 659 loss: 0.000104263745
Iter: 660 loss: 0.000104092614
Iter: 661 loss: 0.000104479564
Iter: 662 loss: 0.000104028302
Iter: 663 loss: 0.000103858256
Iter: 664 loss: 0.000104620987
Iter: 665 loss: 0.000103825478
Iter: 666 loss: 0.000103689468
Iter: 667 loss: 0.000104697778
Iter: 668 loss: 0.000103678365
Iter: 669 loss: 0.000103557453
Iter: 670 loss: 0.000103696904
Iter: 671 loss: 0.000103493236
Iter: 672 loss: 0.000103364146
Iter: 673 loss: 0.000103460981
Iter: 674 loss: 0.000103285725
Iter: 675 loss: 0.000103136525
Iter: 676 loss: 0.000103504441
Iter: 677 loss: 0.000103083905
Iter: 678 loss: 0.000102931
Iter: 679 loss: 0.00010349763
Iter: 680 loss: 0.000102893093
Iter: 681 loss: 0.000102937207
Iter: 682 loss: 0.000102835445
Iter: 683 loss: 0.00010279991
Iter: 684 loss: 0.000102690232
Iter: 685 loss: 0.000102887607
Iter: 686 loss: 0.000102618098
Iter: 687 loss: 0.000102452621
Iter: 688 loss: 0.000102766848
Iter: 689 loss: 0.000102382655
Iter: 690 loss: 0.000102220933
Iter: 691 loss: 0.000102604681
Iter: 692 loss: 0.000102162332
Iter: 693 loss: 0.000101977821
Iter: 694 loss: 0.000102424783
Iter: 695 loss: 0.000101910773
Iter: 696 loss: 0.000101742218
Iter: 697 loss: 0.000102268576
Iter: 698 loss: 0.000101691599
Iter: 699 loss: 0.00010152602
Iter: 700 loss: 0.000102702572
Iter: 701 loss: 0.000101511658
Iter: 702 loss: 0.000101382284
Iter: 703 loss: 0.000102032849
Iter: 704 loss: 0.000101360274
Iter: 705 loss: 0.000101254569
Iter: 706 loss: 0.000101233192
Iter: 707 loss: 0.000101162921
Iter: 708 loss: 0.000101012134
Iter: 709 loss: 0.000101265279
Iter: 710 loss: 0.00010094481
Iter: 711 loss: 0.00010078436
Iter: 712 loss: 0.000101321646
Iter: 713 loss: 0.000100740275
Iter: 714 loss: 0.000100721874
Iter: 715 loss: 0.000100675694
Iter: 716 loss: 0.000100601465
Iter: 717 loss: 0.000100494057
Iter: 718 loss: 0.000100491561
Iter: 719 loss: 0.000100411547
Iter: 720 loss: 0.000100328209
Iter: 721 loss: 0.000100314355
Iter: 722 loss: 0.000100165642
Iter: 723 loss: 0.000100319448
Iter: 724 loss: 0.000100083431
Iter: 725 loss: 9.99016265e-05
Iter: 726 loss: 0.000100643338
Iter: 727 loss: 9.98618198e-05
Iter: 728 loss: 9.97123498e-05
Iter: 729 loss: 9.99655895e-05
Iter: 730 loss: 9.96447925e-05
Iter: 731 loss: 9.9486e-05
Iter: 732 loss: 0.000100267673
Iter: 733 loss: 9.9459241e-05
Iter: 734 loss: 9.93288922e-05
Iter: 735 loss: 9.97360185e-05
Iter: 736 loss: 9.92916466e-05
Iter: 737 loss: 9.91356792e-05
Iter: 738 loss: 9.98362812e-05
Iter: 739 loss: 9.91055858e-05
Iter: 740 loss: 9.89897308e-05
Iter: 741 loss: 9.89821929e-05
Iter: 742 loss: 9.88944375e-05
Iter: 743 loss: 9.87305539e-05
Iter: 744 loss: 9.90114859e-05
Iter: 745 loss: 9.86586165e-05
Iter: 746 loss: 9.85374063e-05
Iter: 747 loss: 0.000100092366
Iter: 748 loss: 9.85374209e-05
Iter: 749 loss: 9.84516519e-05
Iter: 750 loss: 9.84502403e-05
Iter: 751 loss: 9.84164653e-05
Iter: 752 loss: 9.83139325e-05
Iter: 753 loss: 9.8560442e-05
Iter: 754 loss: 9.82539e-05
Iter: 755 loss: 9.80915211e-05
Iter: 756 loss: 9.84730723e-05
Iter: 757 loss: 9.80316545e-05
Iter: 758 loss: 9.78710304e-05
Iter: 759 loss: 9.82629281e-05
Iter: 760 loss: 9.78129683e-05
Iter: 761 loss: 9.76480587e-05
Iter: 762 loss: 9.82332276e-05
Iter: 763 loss: 9.76044757e-05
Iter: 764 loss: 9.74478608e-05
Iter: 765 loss: 9.76546289e-05
Iter: 766 loss: 9.73685746e-05
Iter: 767 loss: 9.71940899e-05
Iter: 768 loss: 9.81863122e-05
Iter: 769 loss: 9.71705449e-05
Iter: 770 loss: 9.70488254e-05
Iter: 771 loss: 9.80450641e-05
Iter: 772 loss: 9.70415567e-05
Iter: 773 loss: 9.69287139e-05
Iter: 774 loss: 9.69760586e-05
Iter: 775 loss: 9.68512613e-05
Iter: 776 loss: 9.67066e-05
Iter: 777 loss: 9.69190442e-05
Iter: 778 loss: 9.6635551e-05
Iter: 779 loss: 9.64961655e-05
Iter: 780 loss: 9.68234308e-05
Iter: 781 loss: 9.64449282e-05
Iter: 782 loss: 9.64288774e-05
Iter: 783 loss: 9.63812636e-05
Iter: 784 loss: 9.63113416e-05
Iter: 785 loss: 9.62196791e-05
Iter: 786 loss: 9.62135382e-05
Iter: 787 loss: 9.61383193e-05
Iter: 788 loss: 9.60087782e-05
Iter: 789 loss: 9.60083635e-05
Iter: 790 loss: 9.58419259e-05
Iter: 791 loss: 9.63602652e-05
Iter: 792 loss: 9.57936863e-05
Iter: 793 loss: 9.56446602e-05
Iter: 794 loss: 9.59945755e-05
Iter: 795 loss: 9.55900687e-05
Iter: 796 loss: 9.54285206e-05
Iter: 797 loss: 9.57317825e-05
Iter: 798 loss: 9.53599301e-05
Iter: 799 loss: 9.51847178e-05
Iter: 800 loss: 9.597803e-05
Iter: 801 loss: 9.51510883e-05
Iter: 802 loss: 9.50060785e-05
Iter: 803 loss: 9.54344141e-05
Iter: 804 loss: 9.49620953e-05
Iter: 805 loss: 9.48343077e-05
Iter: 806 loss: 9.60101825e-05
Iter: 807 loss: 9.482936e-05
Iter: 808 loss: 9.4722127e-05
Iter: 809 loss: 9.48512752e-05
Iter: 810 loss: 9.46663349e-05
Iter: 811 loss: 9.45381616e-05
Iter: 812 loss: 9.4578616e-05
Iter: 813 loss: 9.44471103e-05
Iter: 814 loss: 9.43380437e-05
Iter: 815 loss: 9.54445422e-05
Iter: 816 loss: 9.43348e-05
Iter: 817 loss: 9.42519e-05
Iter: 818 loss: 9.42514089e-05
Iter: 819 loss: 9.42150145e-05
Iter: 820 loss: 9.41038306e-05
Iter: 821 loss: 9.44491258e-05
Iter: 822 loss: 9.40498649e-05
Iter: 823 loss: 9.39079328e-05
Iter: 824 loss: 9.43945633e-05
Iter: 825 loss: 9.3870447e-05
Iter: 826 loss: 9.37339937e-05
Iter: 827 loss: 9.38843295e-05
Iter: 828 loss: 9.36590222e-05
Iter: 829 loss: 9.34943091e-05
Iter: 830 loss: 9.43094201e-05
Iter: 831 loss: 9.34663112e-05
Iter: 832 loss: 9.33167e-05
Iter: 833 loss: 9.35081916e-05
Iter: 834 loss: 9.32392359e-05
Iter: 835 loss: 9.30821e-05
Iter: 836 loss: 9.37839213e-05
Iter: 837 loss: 9.30513634e-05
Iter: 838 loss: 9.29055823e-05
Iter: 839 loss: 9.32067051e-05
Iter: 840 loss: 9.28478112e-05
Iter: 841 loss: 9.27239307e-05
Iter: 842 loss: 9.40386817e-05
Iter: 843 loss: 9.27205037e-05
Iter: 844 loss: 9.2613991e-05
Iter: 845 loss: 9.27802757e-05
Iter: 846 loss: 9.25638451e-05
Iter: 847 loss: 9.24562046e-05
Iter: 848 loss: 9.24203487e-05
Iter: 849 loss: 9.23567495e-05
Iter: 850 loss: 9.24345368e-05
Iter: 851 loss: 9.23001644e-05
Iter: 852 loss: 9.22518375e-05
Iter: 853 loss: 9.21328756e-05
Iter: 854 loss: 9.33669653e-05
Iter: 855 loss: 9.21188039e-05
Iter: 856 loss: 9.20214807e-05
Iter: 857 loss: 9.19526137e-05
Iter: 858 loss: 9.19184386e-05
Iter: 859 loss: 9.17599245e-05
Iter: 860 loss: 9.23838961e-05
Iter: 861 loss: 9.17234793e-05
Iter: 862 loss: 9.15811252e-05
Iter: 863 loss: 9.18160222e-05
Iter: 864 loss: 9.15168639e-05
Iter: 865 loss: 9.13408439e-05
Iter: 866 loss: 9.18720398e-05
Iter: 867 loss: 9.12883406e-05
Iter: 868 loss: 9.11407842e-05
Iter: 869 loss: 9.14807169e-05
Iter: 870 loss: 9.10858653e-05
Iter: 871 loss: 9.09332084e-05
Iter: 872 loss: 9.15867e-05
Iter: 873 loss: 9.09012088e-05
Iter: 874 loss: 9.07718932e-05
Iter: 875 loss: 9.13263284e-05
Iter: 876 loss: 9.07452777e-05
Iter: 877 loss: 9.0618516e-05
Iter: 878 loss: 9.12756659e-05
Iter: 879 loss: 9.05975903e-05
Iter: 880 loss: 9.04899207e-05
Iter: 881 loss: 9.05445195e-05
Iter: 882 loss: 9.04186527e-05
Iter: 883 loss: 9.03482432e-05
Iter: 884 loss: 9.03478358e-05
Iter: 885 loss: 9.02554748e-05
Iter: 886 loss: 9.02721222e-05
Iter: 887 loss: 9.01857711e-05
Iter: 888 loss: 9.01102758e-05
Iter: 889 loss: 8.99638835e-05
Iter: 890 loss: 9.29259113e-05
Iter: 891 loss: 8.99629231e-05
Iter: 892 loss: 8.98165454e-05
Iter: 893 loss: 9.04923581e-05
Iter: 894 loss: 8.97900609e-05
Iter: 895 loss: 8.96548881e-05
Iter: 896 loss: 8.97775899e-05
Iter: 897 loss: 8.9575944e-05
Iter: 898 loss: 8.94266e-05
Iter: 899 loss: 9.01453895e-05
Iter: 900 loss: 8.94006153e-05
Iter: 901 loss: 8.9268e-05
Iter: 902 loss: 8.93438482e-05
Iter: 903 loss: 8.91819727e-05
Iter: 904 loss: 8.90221418e-05
Iter: 905 loss: 8.99467777e-05
Iter: 906 loss: 8.90000374e-05
Iter: 907 loss: 8.88696959e-05
Iter: 908 loss: 8.91281e-05
Iter: 909 loss: 8.88149953e-05
Iter: 910 loss: 8.86875787e-05
Iter: 911 loss: 8.96686e-05
Iter: 912 loss: 8.86773341e-05
Iter: 913 loss: 8.85745249e-05
Iter: 914 loss: 8.90372758e-05
Iter: 915 loss: 8.8554596e-05
Iter: 916 loss: 8.84609253e-05
Iter: 917 loss: 8.84575347e-05
Iter: 918 loss: 8.83846224e-05
Iter: 919 loss: 8.84486e-05
Iter: 920 loss: 8.83399189e-05
Iter: 921 loss: 8.83152388e-05
Iter: 922 loss: 8.82417517e-05
Iter: 923 loss: 8.85652698e-05
Iter: 924 loss: 8.82149e-05
Iter: 925 loss: 8.81106098e-05
Iter: 926 loss: 8.80612497e-05
Iter: 927 loss: 8.8010529e-05
Iter: 928 loss: 8.78565043e-05
Iter: 929 loss: 8.84397523e-05
Iter: 930 loss: 8.78197e-05
Iter: 931 loss: 8.76872073e-05
Iter: 932 loss: 8.80078514e-05
Iter: 933 loss: 8.76390404e-05
Iter: 934 loss: 8.74993129e-05
Iter: 935 loss: 8.79327126e-05
Iter: 936 loss: 8.74570906e-05
Iter: 937 loss: 8.73228e-05
Iter: 938 loss: 8.75505284e-05
Iter: 939 loss: 8.72628734e-05
Iter: 940 loss: 8.71062584e-05
Iter: 941 loss: 8.7879147e-05
Iter: 942 loss: 8.70789954e-05
Iter: 943 loss: 8.69578071e-05
Iter: 944 loss: 8.72836754e-05
Iter: 945 loss: 8.69174473e-05
Iter: 946 loss: 8.67924e-05
Iter: 947 loss: 8.76600243e-05
Iter: 948 loss: 8.67800554e-05
Iter: 949 loss: 8.66800547e-05
Iter: 950 loss: 8.69506694e-05
Iter: 951 loss: 8.66476475e-05
Iter: 952 loss: 8.66053306e-05
Iter: 953 loss: 8.6601307e-05
Iter: 954 loss: 8.65462789e-05
Iter: 955 loss: 8.6450018e-05
Iter: 956 loss: 8.64491158e-05
Iter: 957 loss: 8.63672103e-05
Iter: 958 loss: 8.63322857e-05
Iter: 959 loss: 8.62898e-05
Iter: 960 loss: 8.61771405e-05
Iter: 961 loss: 8.63058813e-05
Iter: 962 loss: 8.61176086e-05
Iter: 963 loss: 8.59762658e-05
Iter: 964 loss: 8.62804227e-05
Iter: 965 loss: 8.59213396e-05
Iter: 966 loss: 8.57744e-05
Iter: 967 loss: 8.61186127e-05
Iter: 968 loss: 8.57203122e-05
Iter: 969 loss: 8.5567277e-05
Iter: 970 loss: 8.64406757e-05
Iter: 971 loss: 8.55464e-05
Iter: 972 loss: 8.5433654e-05
Iter: 973 loss: 8.5630083e-05
Iter: 974 loss: 8.53836755e-05
Iter: 975 loss: 8.52553785e-05
Iter: 976 loss: 8.54829123e-05
Iter: 977 loss: 8.51991645e-05
Iter: 978 loss: 8.50585711e-05
Iter: 979 loss: 8.58015264e-05
Iter: 980 loss: 8.5037e-05
Iter: 981 loss: 8.49465869e-05
Iter: 982 loss: 8.60522196e-05
Iter: 983 loss: 8.49452335e-05
Iter: 984 loss: 8.48762211e-05
Iter: 985 loss: 8.50159413e-05
Iter: 986 loss: 8.4848085e-05
Iter: 987 loss: 8.47684278e-05
Iter: 988 loss: 8.57699342e-05
Iter: 989 loss: 8.47675183e-05
Iter: 990 loss: 8.47347e-05
Iter: 991 loss: 8.46361654e-05
Iter: 992 loss: 8.49319185e-05
Iter: 993 loss: 8.45859104e-05
Iter: 994 loss: 8.44240712e-05
Iter: 995 loss: 8.47563861e-05
Iter: 996 loss: 8.43586749e-05
Iter: 997 loss: 8.42275767e-05
Iter: 998 loss: 8.45990871e-05
Iter: 999 loss: 8.4185609e-05
Iter: 1000 loss: 8.40459688e-05
Iter: 1001 loss: 8.45759569e-05
Iter: 1002 loss: 8.40130961e-05
Iter: 1003 loss: 8.38940105e-05
Iter: 1004 loss: 8.42499e-05
Iter: 1005 loss: 8.38587148e-05
Iter: 1006 loss: 8.37402331e-05
Iter: 1007 loss: 8.40529538e-05
Iter: 1008 loss: 8.37006519e-05
Iter: 1009 loss: 8.35898682e-05
Iter: 1010 loss: 8.38769192e-05
Iter: 1011 loss: 8.35528117e-05
Iter: 1012 loss: 8.34315724e-05
Iter: 1013 loss: 8.36394756e-05
Iter: 1014 loss: 8.33778176e-05
Iter: 1015 loss: 8.32611404e-05
Iter: 1016 loss: 8.40992288e-05
Iter: 1017 loss: 8.32506776e-05
Iter: 1018 loss: 8.3158855e-05
Iter: 1019 loss: 8.39235363e-05
Iter: 1020 loss: 8.31527068e-05
Iter: 1021 loss: 8.31018115e-05
Iter: 1022 loss: 8.31013167e-05
Iter: 1023 loss: 8.30546705e-05
Iter: 1024 loss: 8.29403652e-05
Iter: 1025 loss: 8.40726134e-05
Iter: 1026 loss: 8.29261e-05
Iter: 1027 loss: 8.28346092e-05
Iter: 1028 loss: 8.29760465e-05
Iter: 1029 loss: 8.2791179e-05
Iter: 1030 loss: 8.26932228e-05
Iter: 1031 loss: 8.28386837e-05
Iter: 1032 loss: 8.26453615e-05
Iter: 1033 loss: 8.25208e-05
Iter: 1034 loss: 8.28303e-05
Iter: 1035 loss: 8.24766321e-05
Iter: 1036 loss: 8.23592563e-05
Iter: 1037 loss: 8.26150354e-05
Iter: 1038 loss: 8.23141e-05
Iter: 1039 loss: 8.21984868e-05
Iter: 1040 loss: 8.28305492e-05
Iter: 1041 loss: 8.21815338e-05
Iter: 1042 loss: 8.20907881e-05
Iter: 1043 loss: 8.2082639e-05
Iter: 1044 loss: 8.20149289e-05
Iter: 1045 loss: 8.18733388e-05
Iter: 1046 loss: 8.26765317e-05
Iter: 1047 loss: 8.18534318e-05
Iter: 1048 loss: 8.17393084e-05
Iter: 1049 loss: 8.18585831e-05
Iter: 1050 loss: 8.16757602e-05
Iter: 1051 loss: 8.1590566e-05
Iter: 1052 loss: 8.15879612e-05
Iter: 1053 loss: 8.15347303e-05
Iter: 1054 loss: 8.21543363e-05
Iter: 1055 loss: 8.15339445e-05
Iter: 1056 loss: 8.14790255e-05
Iter: 1057 loss: 8.14503146e-05
Iter: 1058 loss: 8.14242376e-05
Iter: 1059 loss: 8.13597435e-05
Iter: 1060 loss: 8.1272141e-05
Iter: 1061 loss: 8.12677681e-05
Iter: 1062 loss: 8.115542e-05
Iter: 1063 loss: 8.14434316e-05
Iter: 1064 loss: 8.11166828e-05
Iter: 1065 loss: 8.10105703e-05
Iter: 1066 loss: 8.11944628e-05
Iter: 1067 loss: 8.09635167e-05
Iter: 1068 loss: 8.08366676e-05
Iter: 1069 loss: 8.10413549e-05
Iter: 1070 loss: 8.07778197e-05
Iter: 1071 loss: 8.06602766e-05
Iter: 1072 loss: 8.12687285e-05
Iter: 1073 loss: 8.06410244e-05
Iter: 1074 loss: 8.05382515e-05
Iter: 1075 loss: 8.08256227e-05
Iter: 1076 loss: 8.05051604e-05
Iter: 1077 loss: 8.04052397e-05
Iter: 1078 loss: 8.05196178e-05
Iter: 1079 loss: 8.0351143e-05
Iter: 1080 loss: 8.02184441e-05
Iter: 1081 loss: 8.06524913e-05
Iter: 1082 loss: 8.01813439e-05
Iter: 1083 loss: 8.00647686e-05
Iter: 1084 loss: 8.05397576e-05
Iter: 1085 loss: 8.00392154e-05
Iter: 1086 loss: 7.99652917e-05
Iter: 1087 loss: 7.99635891e-05
Iter: 1088 loss: 7.98899127e-05
Iter: 1089 loss: 8.01609131e-05
Iter: 1090 loss: 7.98711335e-05
Iter: 1091 loss: 7.98168621e-05
Iter: 1092 loss: 7.97863468e-05
Iter: 1093 loss: 7.97627436e-05
Iter: 1094 loss: 7.96954409e-05
Iter: 1095 loss: 7.96113745e-05
Iter: 1096 loss: 7.96051754e-05
Iter: 1097 loss: 7.94718362e-05
Iter: 1098 loss: 7.9794263e-05
Iter: 1099 loss: 7.94234365e-05
Iter: 1100 loss: 7.93055515e-05
Iter: 1101 loss: 7.95353e-05
Iter: 1102 loss: 7.92565e-05
Iter: 1103 loss: 7.91363564e-05
Iter: 1104 loss: 7.97911343e-05
Iter: 1105 loss: 7.91186612e-05
Iter: 1106 loss: 7.9009e-05
Iter: 1107 loss: 7.90930935e-05
Iter: 1108 loss: 7.89427213e-05
Iter: 1109 loss: 7.88225443e-05
Iter: 1110 loss: 7.93681902e-05
Iter: 1111 loss: 7.87995523e-05
Iter: 1112 loss: 7.8693658e-05
Iter: 1113 loss: 7.90881531e-05
Iter: 1114 loss: 7.86670862e-05
Iter: 1115 loss: 7.85638e-05
Iter: 1116 loss: 7.87944882e-05
Iter: 1117 loss: 7.85238371e-05
Iter: 1118 loss: 7.84300646e-05
Iter: 1119 loss: 7.8908306e-05
Iter: 1120 loss: 7.84142758e-05
Iter: 1121 loss: 7.83966461e-05
Iter: 1122 loss: 7.83703581e-05
Iter: 1123 loss: 7.83374417e-05
Iter: 1124 loss: 7.8260462e-05
Iter: 1125 loss: 7.91995844e-05
Iter: 1126 loss: 7.82539282e-05
Iter: 1127 loss: 7.81653944e-05
Iter: 1128 loss: 7.84256117e-05
Iter: 1129 loss: 7.81368071e-05
Iter: 1130 loss: 7.80658957e-05
Iter: 1131 loss: 7.80018e-05
Iter: 1132 loss: 7.79843831e-05
Iter: 1133 loss: 7.78615649e-05
Iter: 1134 loss: 7.83977302e-05
Iter: 1135 loss: 7.78370522e-05
Iter: 1136 loss: 7.77378955e-05
Iter: 1137 loss: 7.78480899e-05
Iter: 1138 loss: 7.76840461e-05
Iter: 1139 loss: 7.75529916e-05
Iter: 1140 loss: 7.79598922e-05
Iter: 1141 loss: 7.75151493e-05
Iter: 1142 loss: 7.74140935e-05
Iter: 1143 loss: 7.77706591e-05
Iter: 1144 loss: 7.7387449e-05
Iter: 1145 loss: 7.72806234e-05
Iter: 1146 loss: 7.74570071e-05
Iter: 1147 loss: 7.7231889e-05
Iter: 1148 loss: 7.7120174e-05
Iter: 1149 loss: 7.74327054e-05
Iter: 1150 loss: 7.70835177e-05
Iter: 1151 loss: 7.69687176e-05
Iter: 1152 loss: 7.72089625e-05
Iter: 1153 loss: 7.69231774e-05
Iter: 1154 loss: 7.69036342e-05
Iter: 1155 loss: 7.68757673e-05
Iter: 1156 loss: 7.68167229e-05
Iter: 1157 loss: 7.68053869e-05
Iter: 1158 loss: 7.67659221e-05
Iter: 1159 loss: 7.6707176e-05
Iter: 1160 loss: 7.67291058e-05
Iter: 1161 loss: 7.6666649e-05
Iter: 1162 loss: 7.65879231e-05
Iter: 1163 loss: 7.66252633e-05
Iter: 1164 loss: 7.6535056e-05
Iter: 1165 loss: 7.64435899e-05
Iter: 1166 loss: 7.65439618e-05
Iter: 1167 loss: 7.63947e-05
Iter: 1168 loss: 7.628811e-05
Iter: 1169 loss: 7.64996512e-05
Iter: 1170 loss: 7.62441123e-05
Iter: 1171 loss: 7.61372139e-05
Iter: 1172 loss: 7.64828437e-05
Iter: 1173 loss: 7.61078118e-05
Iter: 1174 loss: 7.5986085e-05
Iter: 1175 loss: 7.61549963e-05
Iter: 1176 loss: 7.59258e-05
Iter: 1177 loss: 7.58198585e-05
Iter: 1178 loss: 7.65556106e-05
Iter: 1179 loss: 7.58098613e-05
Iter: 1180 loss: 7.57168309e-05
Iter: 1181 loss: 7.56854133e-05
Iter: 1182 loss: 7.56329246e-05
Iter: 1183 loss: 7.54961075e-05
Iter: 1184 loss: 7.62540876e-05
Iter: 1185 loss: 7.54771245e-05
Iter: 1186 loss: 7.53591739e-05
Iter: 1187 loss: 7.56800073e-05
Iter: 1188 loss: 7.53209533e-05
Iter: 1189 loss: 7.53611603e-05
Iter: 1190 loss: 7.52852939e-05
Iter: 1191 loss: 7.52497e-05
Iter: 1192 loss: 7.517841e-05
Iter: 1193 loss: 7.65942677e-05
Iter: 1194 loss: 7.51780171e-05
Iter: 1195 loss: 7.51161715e-05
Iter: 1196 loss: 7.50818872e-05
Iter: 1197 loss: 7.50546242e-05
Iter: 1198 loss: 7.4957934e-05
Iter: 1199 loss: 7.57728412e-05
Iter: 1200 loss: 7.49520332e-05
Iter: 1201 loss: 7.48886814e-05
Iter: 1202 loss: 7.48423117e-05
Iter: 1203 loss: 7.48199309e-05
Iter: 1204 loss: 7.47094819e-05
Iter: 1205 loss: 7.49461833e-05
Iter: 1206 loss: 7.46659643e-05
Iter: 1207 loss: 7.45704601e-05
Iter: 1208 loss: 7.48249658e-05
Iter: 1209 loss: 7.45388097e-05
Iter: 1210 loss: 7.44237477e-05
Iter: 1211 loss: 7.45691359e-05
Iter: 1212 loss: 7.43650453e-05
Iter: 1213 loss: 7.4245283e-05
Iter: 1214 loss: 7.46595251e-05
Iter: 1215 loss: 7.42136e-05
Iter: 1216 loss: 7.41081894e-05
Iter: 1217 loss: 7.44924037e-05
Iter: 1218 loss: 7.40812829e-05
Iter: 1219 loss: 7.39802344e-05
Iter: 1220 loss: 7.44998906e-05
Iter: 1221 loss: 7.39637e-05
Iter: 1222 loss: 7.38771705e-05
Iter: 1223 loss: 7.39848692e-05
Iter: 1224 loss: 7.38314702e-05
Iter: 1225 loss: 7.38298404e-05
Iter: 1226 loss: 7.37792e-05
Iter: 1227 loss: 7.37558148e-05
Iter: 1228 loss: 7.36954535e-05
Iter: 1229 loss: 7.41578915e-05
Iter: 1230 loss: 7.36836155e-05
Iter: 1231 loss: 7.36217116e-05
Iter: 1232 loss: 7.37131777e-05
Iter: 1233 loss: 7.35916256e-05
Iter: 1234 loss: 7.34936548e-05
Iter: 1235 loss: 7.36613074e-05
Iter: 1236 loss: 7.34505302e-05
Iter: 1237 loss: 7.33663692e-05
Iter: 1238 loss: 7.35195063e-05
Iter: 1239 loss: 7.33287816e-05
Iter: 1240 loss: 7.32330518e-05
Iter: 1241 loss: 7.3303323e-05
Iter: 1242 loss: 7.31739419e-05
Iter: 1243 loss: 7.30669562e-05
Iter: 1244 loss: 7.33725319e-05
Iter: 1245 loss: 7.30324391e-05
Iter: 1246 loss: 7.29218227e-05
Iter: 1247 loss: 7.31517212e-05
Iter: 1248 loss: 7.28774903e-05
Iter: 1249 loss: 7.27814768e-05
Iter: 1250 loss: 7.34165587e-05
Iter: 1251 loss: 7.27708903e-05
Iter: 1252 loss: 7.26756e-05
Iter: 1253 loss: 7.26848375e-05
Iter: 1254 loss: 7.26020517e-05
Iter: 1255 loss: 7.24810379e-05
Iter: 1256 loss: 7.32596163e-05
Iter: 1257 loss: 7.24677957e-05
Iter: 1258 loss: 7.24101847e-05
Iter: 1259 loss: 7.24089332e-05
Iter: 1260 loss: 7.23338308e-05
Iter: 1261 loss: 7.23702833e-05
Iter: 1262 loss: 7.22832847e-05
Iter: 1263 loss: 7.22401746e-05
Iter: 1264 loss: 7.21841861e-05
Iter: 1265 loss: 7.2180861e-05
Iter: 1266 loss: 7.21105753e-05
Iter: 1267 loss: 7.26056533e-05
Iter: 1268 loss: 7.21032557e-05
Iter: 1269 loss: 7.20400785e-05
Iter: 1270 loss: 7.20867101e-05
Iter: 1271 loss: 7.20005773e-05
Iter: 1272 loss: 7.19188e-05
Iter: 1273 loss: 7.20158205e-05
Iter: 1274 loss: 7.18759256e-05
Iter: 1275 loss: 7.17817602e-05
Iter: 1276 loss: 7.19021336e-05
Iter: 1277 loss: 7.17339863e-05
Iter: 1278 loss: 7.16264913e-05
Iter: 1279 loss: 7.21767137e-05
Iter: 1280 loss: 7.16094219e-05
Iter: 1281 loss: 7.1534334e-05
Iter: 1282 loss: 7.15614733e-05
Iter: 1283 loss: 7.14818379e-05
Iter: 1284 loss: 7.13779737e-05
Iter: 1285 loss: 7.18817319e-05
Iter: 1286 loss: 7.13596e-05
Iter: 1287 loss: 7.12696637e-05
Iter: 1288 loss: 7.13109475e-05
Iter: 1289 loss: 7.12086694e-05
Iter: 1290 loss: 7.11118701e-05
Iter: 1291 loss: 7.21057877e-05
Iter: 1292 loss: 7.11082321e-05
Iter: 1293 loss: 7.10938184e-05
Iter: 1294 loss: 7.10789318e-05
Iter: 1295 loss: 7.10414824e-05
Iter: 1296 loss: 7.09490851e-05
Iter: 1297 loss: 7.18576848e-05
Iter: 1298 loss: 7.09365486e-05
Iter: 1299 loss: 7.08567823e-05
Iter: 1300 loss: 7.09019077e-05
Iter: 1301 loss: 7.08045118e-05
Iter: 1302 loss: 7.07214203e-05
Iter: 1303 loss: 7.123877e-05
Iter: 1304 loss: 7.07119907e-05
Iter: 1305 loss: 7.06361316e-05
Iter: 1306 loss: 7.09223314e-05
Iter: 1307 loss: 7.06180799e-05
Iter: 1308 loss: 7.05495913e-05
Iter: 1309 loss: 7.06324e-05
Iter: 1310 loss: 7.05134371e-05
Iter: 1311 loss: 7.04311e-05
Iter: 1312 loss: 7.04097038e-05
Iter: 1313 loss: 7.03586775e-05
Iter: 1314 loss: 7.02615653e-05
Iter: 1315 loss: 7.09648157e-05
Iter: 1316 loss: 7.02533289e-05
Iter: 1317 loss: 7.01785102e-05
Iter: 1318 loss: 7.02005418e-05
Iter: 1319 loss: 7.01251265e-05
Iter: 1320 loss: 7.00195815e-05
Iter: 1321 loss: 7.05951097e-05
Iter: 1322 loss: 7.00036107e-05
Iter: 1323 loss: 6.99150478e-05
Iter: 1324 loss: 7.01701792e-05
Iter: 1325 loss: 6.98871736e-05
Iter: 1326 loss: 6.98016302e-05
Iter: 1327 loss: 6.98595104e-05
Iter: 1328 loss: 6.97482828e-05
Iter: 1329 loss: 6.965801e-05
Iter: 1330 loss: 7.03605474e-05
Iter: 1331 loss: 6.96523493e-05
Iter: 1332 loss: 6.95935596e-05
Iter: 1333 loss: 6.95897907e-05
Iter: 1334 loss: 6.95595081e-05
Iter: 1335 loss: 6.94787159e-05
Iter: 1336 loss: 6.99954253e-05
Iter: 1337 loss: 6.94579503e-05
Iter: 1338 loss: 6.93775801e-05
Iter: 1339 loss: 6.94672e-05
Iter: 1340 loss: 6.93347611e-05
Iter: 1341 loss: 6.92343601e-05
Iter: 1342 loss: 6.95633498e-05
Iter: 1343 loss: 6.92054e-05
Iter: 1344 loss: 6.91294e-05
Iter: 1345 loss: 6.99093507e-05
Iter: 1346 loss: 6.91275709e-05
Iter: 1347 loss: 6.90521192e-05
Iter: 1348 loss: 6.90574379e-05
Iter: 1349 loss: 6.89928e-05
Iter: 1350 loss: 6.89150329e-05
Iter: 1351 loss: 6.91616e-05
Iter: 1352 loss: 6.88925793e-05
Iter: 1353 loss: 6.88150176e-05
Iter: 1354 loss: 6.88625587e-05
Iter: 1355 loss: 6.8764457e-05
Iter: 1356 loss: 6.86590211e-05
Iter: 1357 loss: 6.90450397e-05
Iter: 1358 loss: 6.86325366e-05
Iter: 1359 loss: 6.85478e-05
Iter: 1360 loss: 6.86515123e-05
Iter: 1361 loss: 6.85033956e-05
Iter: 1362 loss: 6.84011757e-05
Iter: 1363 loss: 6.90305678e-05
Iter: 1364 loss: 6.83892722e-05
Iter: 1365 loss: 6.83045218e-05
Iter: 1366 loss: 6.83733815e-05
Iter: 1367 loss: 6.82544487e-05
Iter: 1368 loss: 6.82801328e-05
Iter: 1369 loss: 6.82207537e-05
Iter: 1370 loss: 6.81823221e-05
Iter: 1371 loss: 6.81132078e-05
Iter: 1372 loss: 6.97594296e-05
Iter: 1373 loss: 6.81136298e-05
Iter: 1374 loss: 6.80583325e-05
Iter: 1375 loss: 6.79886871e-05
Iter: 1376 loss: 6.79833174e-05
Iter: 1377 loss: 6.78932047e-05
Iter: 1378 loss: 6.82892132e-05
Iter: 1379 loss: 6.78761e-05
Iter: 1380 loss: 6.77992939e-05
Iter: 1381 loss: 6.85136038e-05
Iter: 1382 loss: 6.77962744e-05
Iter: 1383 loss: 6.77371572e-05
Iter: 1384 loss: 6.79671939e-05
Iter: 1385 loss: 6.77230491e-05
Iter: 1386 loss: 6.76657801e-05
Iter: 1387 loss: 6.76031268e-05
Iter: 1388 loss: 6.75933115e-05
Iter: 1389 loss: 6.74848707e-05
Iter: 1390 loss: 6.77759526e-05
Iter: 1391 loss: 6.74493058e-05
Iter: 1392 loss: 6.7346693e-05
Iter: 1393 loss: 6.77662465e-05
Iter: 1394 loss: 6.73242757e-05
Iter: 1395 loss: 6.72275128e-05
Iter: 1396 loss: 6.73340837e-05
Iter: 1397 loss: 6.71744056e-05
Iter: 1398 loss: 6.70765512e-05
Iter: 1399 loss: 6.76684285e-05
Iter: 1400 loss: 6.70644222e-05
Iter: 1401 loss: 6.69881119e-05
Iter: 1402 loss: 6.70949812e-05
Iter: 1403 loss: 6.69501e-05
Iter: 1404 loss: 6.69551955e-05
Iter: 1405 loss: 6.69069123e-05
Iter: 1406 loss: 6.68748326e-05
Iter: 1407 loss: 6.67991262e-05
Iter: 1408 loss: 6.76229101e-05
Iter: 1409 loss: 6.67903078e-05
Iter: 1410 loss: 6.6722816e-05
Iter: 1411 loss: 6.66600463e-05
Iter: 1412 loss: 6.66444903e-05
Iter: 1413 loss: 6.65454645e-05
Iter: 1414 loss: 6.73551403e-05
Iter: 1415 loss: 6.65388798e-05
Iter: 1416 loss: 6.646369e-05
Iter: 1417 loss: 6.65717671e-05
Iter: 1418 loss: 6.64271647e-05
Iter: 1419 loss: 6.63399187e-05
Iter: 1420 loss: 6.70384e-05
Iter: 1421 loss: 6.63341198e-05
Iter: 1422 loss: 6.62667153e-05
Iter: 1423 loss: 6.63503597e-05
Iter: 1424 loss: 6.62323364e-05
Iter: 1425 loss: 6.61641607e-05
Iter: 1426 loss: 6.61050944e-05
Iter: 1427 loss: 6.60864534e-05
Iter: 1428 loss: 6.59964571e-05
Iter: 1429 loss: 6.70301743e-05
Iter: 1430 loss: 6.59953948e-05
Iter: 1431 loss: 6.59257421e-05
Iter: 1432 loss: 6.59595389e-05
Iter: 1433 loss: 6.5879125e-05
Iter: 1434 loss: 6.57942146e-05
Iter: 1435 loss: 6.62086168e-05
Iter: 1436 loss: 6.57797282e-05
Iter: 1437 loss: 6.57080818e-05
Iter: 1438 loss: 6.59144353e-05
Iter: 1439 loss: 6.56854536e-05
Iter: 1440 loss: 6.56476e-05
Iter: 1441 loss: 6.56350749e-05
Iter: 1442 loss: 6.56158227e-05
Iter: 1443 loss: 6.55632612e-05
Iter: 1444 loss: 6.58968347e-05
Iter: 1445 loss: 6.55502081e-05
Iter: 1446 loss: 6.54807809e-05
Iter: 1447 loss: 6.54926116e-05
Iter: 1448 loss: 6.5429027e-05
Iter: 1449 loss: 6.53399038e-05
Iter: 1450 loss: 6.5747452e-05
Iter: 1451 loss: 6.53233e-05
Iter: 1452 loss: 6.52370072e-05
Iter: 1453 loss: 6.54724863e-05
Iter: 1454 loss: 6.52098752e-05
Iter: 1455 loss: 6.51291848e-05
Iter: 1456 loss: 6.59740399e-05
Iter: 1457 loss: 6.51270238e-05
Iter: 1458 loss: 6.50754446e-05
Iter: 1459 loss: 6.50929214e-05
Iter: 1460 loss: 6.50393267e-05
Iter: 1461 loss: 6.49626963e-05
Iter: 1462 loss: 6.49734e-05
Iter: 1463 loss: 6.49044523e-05
Iter: 1464 loss: 6.48183632e-05
Iter: 1465 loss: 6.51024457e-05
Iter: 1466 loss: 6.47946945e-05
Iter: 1467 loss: 6.47112174e-05
Iter: 1468 loss: 6.48846471e-05
Iter: 1469 loss: 6.46769477e-05
Iter: 1470 loss: 6.45846303e-05
Iter: 1471 loss: 6.50185466e-05
Iter: 1472 loss: 6.45672117e-05
Iter: 1473 loss: 6.45238615e-05
Iter: 1474 loss: 6.45203691e-05
Iter: 1475 loss: 6.44616375e-05
Iter: 1476 loss: 6.44465472e-05
Iter: 1477 loss: 6.44097308e-05
Iter: 1478 loss: 6.43641688e-05
Iter: 1479 loss: 6.42762461e-05
Iter: 1480 loss: 6.61484737e-05
Iter: 1481 loss: 6.4276639e-05
Iter: 1482 loss: 6.41844599e-05
Iter: 1483 loss: 6.4540247e-05
Iter: 1484 loss: 6.41622319e-05
Iter: 1485 loss: 6.40798e-05
Iter: 1486 loss: 6.43517924e-05
Iter: 1487 loss: 6.40568323e-05
Iter: 1488 loss: 6.39751088e-05
Iter: 1489 loss: 6.41483057e-05
Iter: 1490 loss: 6.39429345e-05
Iter: 1491 loss: 6.38473139e-05
Iter: 1492 loss: 6.45037799e-05
Iter: 1493 loss: 6.38379861e-05
Iter: 1494 loss: 6.37666089e-05
Iter: 1495 loss: 6.38628335e-05
Iter: 1496 loss: 6.37300327e-05
Iter: 1497 loss: 6.36588375e-05
Iter: 1498 loss: 6.36574914e-05
Iter: 1499 loss: 6.36014e-05
Iter: 1500 loss: 6.35037723e-05
Iter: 1501 loss: 6.39153732e-05
Iter: 1502 loss: 6.34832541e-05
Iter: 1503 loss: 6.33908785e-05
Iter: 1504 loss: 6.3580359e-05
Iter: 1505 loss: 6.33541276e-05
Iter: 1506 loss: 6.32758602e-05
Iter: 1507 loss: 6.36206605e-05
Iter: 1508 loss: 6.3260195e-05
Iter: 1509 loss: 6.32536903e-05
Iter: 1510 loss: 6.32219817e-05
Iter: 1511 loss: 6.31986768e-05
Iter: 1512 loss: 6.31331932e-05
Iter: 1513 loss: 6.34757453e-05
Iter: 1514 loss: 6.31118455e-05
Iter: 1515 loss: 6.30361174e-05
Iter: 1516 loss: 6.31431831e-05
Iter: 1517 loss: 6.29979477e-05
Iter: 1518 loss: 6.29099668e-05
Iter: 1519 loss: 6.30068316e-05
Iter: 1520 loss: 6.28617272e-05
Iter: 1521 loss: 6.27745176e-05
Iter: 1522 loss: 6.32894807e-05
Iter: 1523 loss: 6.27634e-05
Iter: 1524 loss: 6.26849505e-05
Iter: 1525 loss: 6.30105e-05
Iter: 1526 loss: 6.26684341e-05
Iter: 1527 loss: 6.26008914e-05
Iter: 1528 loss: 6.30225e-05
Iter: 1529 loss: 6.25920875e-05
Iter: 1530 loss: 6.25343528e-05
Iter: 1531 loss: 6.24922322e-05
Iter: 1532 loss: 6.24717868e-05
Iter: 1533 loss: 6.23748274e-05
Iter: 1534 loss: 6.26591354e-05
Iter: 1535 loss: 6.23445812e-05
Iter: 1536 loss: 6.22573498e-05
Iter: 1537 loss: 6.22549e-05
Iter: 1538 loss: 6.21868312e-05
Iter: 1539 loss: 6.207361e-05
Iter: 1540 loss: 6.32185693e-05
Iter: 1541 loss: 6.20692299e-05
Iter: 1542 loss: 6.20286e-05
Iter: 1543 loss: 6.20280189e-05
Iter: 1544 loss: 6.19717612e-05
Iter: 1545 loss: 6.19305792e-05
Iter: 1546 loss: 6.1911036e-05
Iter: 1547 loss: 6.18607e-05
Iter: 1548 loss: 6.18113409e-05
Iter: 1549 loss: 6.17998303e-05
Iter: 1550 loss: 6.17178957e-05
Iter: 1551 loss: 6.1819941e-05
Iter: 1552 loss: 6.16748657e-05
Iter: 1553 loss: 6.15787867e-05
Iter: 1554 loss: 6.20092251e-05
Iter: 1555 loss: 6.15600366e-05
Iter: 1556 loss: 6.14903765e-05
Iter: 1557 loss: 6.15426106e-05
Iter: 1558 loss: 6.14472447e-05
Iter: 1559 loss: 6.13456359e-05
Iter: 1560 loss: 6.21036161e-05
Iter: 1561 loss: 6.13376833e-05
Iter: 1562 loss: 6.1271814e-05
Iter: 1563 loss: 6.16555e-05
Iter: 1564 loss: 6.12624863e-05
Iter: 1565 loss: 6.12106087e-05
Iter: 1566 loss: 6.11362775e-05
Iter: 1567 loss: 6.11337e-05
Iter: 1568 loss: 6.10443094e-05
Iter: 1569 loss: 6.16762263e-05
Iter: 1570 loss: 6.10358111e-05
Iter: 1571 loss: 6.09687741e-05
Iter: 1572 loss: 6.10442366e-05
Iter: 1573 loss: 6.09321396e-05
Iter: 1574 loss: 6.0848437e-05
Iter: 1575 loss: 6.11767173e-05
Iter: 1576 loss: 6.08290502e-05
Iter: 1577 loss: 6.08567643e-05
Iter: 1578 loss: 6.07996299e-05
Iter: 1579 loss: 6.07813345e-05
Iter: 1580 loss: 6.07232141e-05
Iter: 1581 loss: 6.08197588e-05
Iter: 1582 loss: 6.06842914e-05
Iter: 1583 loss: 6.06078975e-05
Iter: 1584 loss: 6.079374e-05
Iter: 1585 loss: 6.05811729e-05
Iter: 1586 loss: 6.05003552e-05
Iter: 1587 loss: 6.07035399e-05
Iter: 1588 loss: 6.04727429e-05
Iter: 1589 loss: 6.03888111e-05
Iter: 1590 loss: 6.07446491e-05
Iter: 1591 loss: 6.03714543e-05
Iter: 1592 loss: 6.02989385e-05
Iter: 1593 loss: 6.04037305e-05
Iter: 1594 loss: 6.02645523e-05
Iter: 1595 loss: 6.01910797e-05
Iter: 1596 loss: 6.10379138e-05
Iter: 1597 loss: 6.01899374e-05
Iter: 1598 loss: 6.01322063e-05
Iter: 1599 loss: 6.01140018e-05
Iter: 1600 loss: 6.0080125e-05
Iter: 1601 loss: 5.99961277e-05
Iter: 1602 loss: 6.01551e-05
Iter: 1603 loss: 5.99608247e-05
Iter: 1604 loss: 5.98839797e-05
Iter: 1605 loss: 6.01219836e-05
Iter: 1606 loss: 5.98613333e-05
Iter: 1607 loss: 5.97753897e-05
Iter: 1608 loss: 5.98624028e-05
Iter: 1609 loss: 5.9727965e-05
Iter: 1610 loss: 5.96978171e-05
Iter: 1611 loss: 5.96842729e-05
Iter: 1612 loss: 5.96279351e-05
Iter: 1613 loss: 5.95991551e-05
Iter: 1614 loss: 5.95720339e-05
Iter: 1615 loss: 5.95238234e-05
Iter: 1616 loss: 5.94897356e-05
Iter: 1617 loss: 5.94727499e-05
Iter: 1618 loss: 5.94067096e-05
Iter: 1619 loss: 5.94141056e-05
Iter: 1620 loss: 5.93550358e-05
Iter: 1621 loss: 5.92671749e-05
Iter: 1622 loss: 5.96303034e-05
Iter: 1623 loss: 5.92484103e-05
Iter: 1624 loss: 5.91747048e-05
Iter: 1625 loss: 5.95291203e-05
Iter: 1626 loss: 5.91614371e-05
Iter: 1627 loss: 5.9093687e-05
Iter: 1628 loss: 5.93776e-05
Iter: 1629 loss: 5.90795389e-05
Iter: 1630 loss: 5.90222553e-05
Iter: 1631 loss: 5.93526711e-05
Iter: 1632 loss: 5.90148338e-05
Iter: 1633 loss: 5.89575211e-05
Iter: 1634 loss: 5.8895359e-05
Iter: 1635 loss: 5.88865e-05
Iter: 1636 loss: 5.87989234e-05
Iter: 1637 loss: 5.90938944e-05
Iter: 1638 loss: 5.87751383e-05
Iter: 1639 loss: 5.86888127e-05
Iter: 1640 loss: 5.88879266e-05
Iter: 1641 loss: 5.86565075e-05
Iter: 1642 loss: 5.85709677e-05
Iter: 1643 loss: 5.90503951e-05
Iter: 1644 loss: 5.85589842e-05
Iter: 1645 loss: 5.85690213e-05
Iter: 1646 loss: 5.85277448e-05
Iter: 1647 loss: 5.85122943e-05
Iter: 1648 loss: 5.84631707e-05
Iter: 1649 loss: 5.85049347e-05
Iter: 1650 loss: 5.84213703e-05
Iter: 1651 loss: 5.83305409e-05
Iter: 1652 loss: 5.85779162e-05
Iter: 1653 loss: 5.83011206e-05
Iter: 1654 loss: 5.82233843e-05
Iter: 1655 loss: 5.84264744e-05
Iter: 1656 loss: 5.81980094e-05
Iter: 1657 loss: 5.81134736e-05
Iter: 1658 loss: 5.83423098e-05
Iter: 1659 loss: 5.80857122e-05
Iter: 1660 loss: 5.80089145e-05
Iter: 1661 loss: 5.83283945e-05
Iter: 1662 loss: 5.79921616e-05
Iter: 1663 loss: 5.79158e-05
Iter: 1664 loss: 5.8258e-05
Iter: 1665 loss: 5.79012e-05
Iter: 1666 loss: 5.78322288e-05
Iter: 1667 loss: 5.81222e-05
Iter: 1668 loss: 5.78176114e-05
Iter: 1669 loss: 5.77716419e-05
Iter: 1670 loss: 5.77799728e-05
Iter: 1671 loss: 5.77370156e-05
Iter: 1672 loss: 5.76634302e-05
Iter: 1673 loss: 5.77338e-05
Iter: 1674 loss: 5.76220154e-05
Iter: 1675 loss: 5.75492559e-05
Iter: 1676 loss: 5.77835417e-05
Iter: 1677 loss: 5.75287813e-05
Iter: 1678 loss: 5.75343365e-05
Iter: 1679 loss: 5.75004306e-05
Iter: 1680 loss: 5.74710357e-05
Iter: 1681 loss: 5.74251717e-05
Iter: 1682 loss: 5.74248115e-05
Iter: 1683 loss: 5.73864527e-05
Iter: 1684 loss: 5.73327125e-05
Iter: 1685 loss: 5.7330366e-05
Iter: 1686 loss: 5.72497884e-05
Iter: 1687 loss: 5.73849757e-05
Iter: 1688 loss: 5.72145691e-05
Iter: 1689 loss: 5.71413402e-05
Iter: 1690 loss: 5.75798367e-05
Iter: 1691 loss: 5.71331111e-05
Iter: 1692 loss: 5.70666925e-05
Iter: 1693 loss: 5.71891433e-05
Iter: 1694 loss: 5.70393458e-05
Iter: 1695 loss: 5.69749973e-05
Iter: 1696 loss: 5.7280693e-05
Iter: 1697 loss: 5.69627337e-05
Iter: 1698 loss: 5.68973919e-05
Iter: 1699 loss: 5.72228564e-05
Iter: 1700 loss: 5.6886387e-05
Iter: 1701 loss: 5.68365358e-05
Iter: 1702 loss: 5.68836804e-05
Iter: 1703 loss: 5.6807181e-05
Iter: 1704 loss: 5.67433162e-05
Iter: 1705 loss: 5.67964307e-05
Iter: 1706 loss: 5.67050192e-05
Iter: 1707 loss: 5.66359849e-05
Iter: 1708 loss: 5.68223768e-05
Iter: 1709 loss: 5.66128765e-05
Iter: 1710 loss: 5.65471273e-05
Iter: 1711 loss: 5.69695803e-05
Iter: 1712 loss: 5.65397349e-05
Iter: 1713 loss: 5.6479068e-05
Iter: 1714 loss: 5.73607467e-05
Iter: 1715 loss: 5.64785441e-05
Iter: 1716 loss: 5.64577276e-05
Iter: 1717 loss: 5.63979556e-05
Iter: 1718 loss: 5.66884046e-05
Iter: 1719 loss: 5.637711e-05
Iter: 1720 loss: 5.62887399e-05
Iter: 1721 loss: 5.65018745e-05
Iter: 1722 loss: 5.62572241e-05
Iter: 1723 loss: 5.6189263e-05
Iter: 1724 loss: 5.63226422e-05
Iter: 1725 loss: 5.6161698e-05
Iter: 1726 loss: 5.60797198e-05
Iter: 1727 loss: 5.63193607e-05
Iter: 1728 loss: 5.60546105e-05
Iter: 1729 loss: 5.59825785e-05
Iter: 1730 loss: 5.63868525e-05
Iter: 1731 loss: 5.5972e-05
Iter: 1732 loss: 5.59150212e-05
Iter: 1733 loss: 5.62470304e-05
Iter: 1734 loss: 5.59076e-05
Iter: 1735 loss: 5.58564934e-05
Iter: 1736 loss: 5.59902874e-05
Iter: 1737 loss: 5.58390202e-05
Iter: 1738 loss: 5.57936764e-05
Iter: 1739 loss: 5.57787716e-05
Iter: 1740 loss: 5.57520834e-05
Iter: 1741 loss: 5.56747036e-05
Iter: 1742 loss: 5.58736065e-05
Iter: 1743 loss: 5.56485902e-05
Iter: 1744 loss: 5.55856568e-05
Iter: 1745 loss: 5.5864155e-05
Iter: 1746 loss: 5.55735242e-05
Iter: 1747 loss: 5.55706029e-05
Iter: 1748 loss: 5.55457882e-05
Iter: 1749 loss: 5.55251245e-05
Iter: 1750 loss: 5.54643884e-05
Iter: 1751 loss: 5.57105523e-05
Iter: 1752 loss: 5.54391336e-05
Iter: 1753 loss: 5.53778373e-05
Iter: 1754 loss: 5.55390652e-05
Iter: 1755 loss: 5.5357068e-05
Iter: 1756 loss: 5.52865713e-05
Iter: 1757 loss: 5.536557e-05
Iter: 1758 loss: 5.52488127e-05
Iter: 1759 loss: 5.51742378e-05
Iter: 1760 loss: 5.54968574e-05
Iter: 1761 loss: 5.5158911e-05
Iter: 1762 loss: 5.50957775e-05
Iter: 1763 loss: 5.51889825e-05
Iter: 1764 loss: 5.50657569e-05
Iter: 1765 loss: 5.49899705e-05
Iter: 1766 loss: 5.54954531e-05
Iter: 1767 loss: 5.49821416e-05
Iter: 1768 loss: 5.49185206e-05
Iter: 1769 loss: 5.51765042e-05
Iter: 1770 loss: 5.4903714e-05
Iter: 1771 loss: 5.48496537e-05
Iter: 1772 loss: 5.4911121e-05
Iter: 1773 loss: 5.4820739e-05
Iter: 1774 loss: 5.47603522e-05
Iter: 1775 loss: 5.48395255e-05
Iter: 1776 loss: 5.47302952e-05
Iter: 1777 loss: 5.46634765e-05
Iter: 1778 loss: 5.48811731e-05
Iter: 1779 loss: 5.46438e-05
Iter: 1780 loss: 5.46394258e-05
Iter: 1781 loss: 5.46195151e-05
Iter: 1782 loss: 5.45921648e-05
Iter: 1783 loss: 5.45486764e-05
Iter: 1784 loss: 5.45482289e-05
Iter: 1785 loss: 5.45100775e-05
Iter: 1786 loss: 5.44546019e-05
Iter: 1787 loss: 5.44529757e-05
Iter: 1788 loss: 5.43774731e-05
Iter: 1789 loss: 5.45697658e-05
Iter: 1790 loss: 5.4351498e-05
Iter: 1791 loss: 5.42735761e-05
Iter: 1792 loss: 5.45436196e-05
Iter: 1793 loss: 5.42531852e-05
Iter: 1794 loss: 5.41837217e-05
Iter: 1795 loss: 5.43747228e-05
Iter: 1796 loss: 5.41621303e-05
Iter: 1797 loss: 5.40927067e-05
Iter: 1798 loss: 5.4284e-05
Iter: 1799 loss: 5.40702385e-05
Iter: 1800 loss: 5.4013206e-05
Iter: 1801 loss: 5.47132804e-05
Iter: 1802 loss: 5.40124238e-05
Iter: 1803 loss: 5.39645771e-05
Iter: 1804 loss: 5.39564608e-05
Iter: 1805 loss: 5.39243847e-05
Iter: 1806 loss: 5.38656677e-05
Iter: 1807 loss: 5.40733126e-05
Iter: 1808 loss: 5.3850843e-05
Iter: 1809 loss: 5.37948363e-05
Iter: 1810 loss: 5.38191489e-05
Iter: 1811 loss: 5.37566666e-05
Iter: 1812 loss: 5.37119049e-05
Iter: 1813 loss: 5.37106935e-05
Iter: 1814 loss: 5.36634143e-05
Iter: 1815 loss: 5.3890959e-05
Iter: 1816 loss: 5.36557563e-05
Iter: 1817 loss: 5.36322514e-05
Iter: 1818 loss: 5.35728905e-05
Iter: 1819 loss: 5.41018817e-05
Iter: 1820 loss: 5.35638683e-05
Iter: 1821 loss: 5.34981118e-05
Iter: 1822 loss: 5.37020678e-05
Iter: 1823 loss: 5.34782557e-05
Iter: 1824 loss: 5.34117571e-05
Iter: 1825 loss: 5.34489445e-05
Iter: 1826 loss: 5.33682796e-05
Iter: 1827 loss: 5.32960403e-05
Iter: 1828 loss: 5.39941466e-05
Iter: 1829 loss: 5.32928e-05
Iter: 1830 loss: 5.32344238e-05
Iter: 1831 loss: 5.32437916e-05
Iter: 1832 loss: 5.31899714e-05
Iter: 1833 loss: 5.3124455e-05
Iter: 1834 loss: 5.38742024e-05
Iter: 1835 loss: 5.31231926e-05
Iter: 1836 loss: 5.30699799e-05
Iter: 1837 loss: 5.32109152e-05
Iter: 1838 loss: 5.30525e-05
Iter: 1839 loss: 5.30010402e-05
Iter: 1840 loss: 5.29973331e-05
Iter: 1841 loss: 5.29579629e-05
Iter: 1842 loss: 5.28929777e-05
Iter: 1843 loss: 5.32212616e-05
Iter: 1844 loss: 5.28821e-05
Iter: 1845 loss: 5.28334676e-05
Iter: 1846 loss: 5.29951467e-05
Iter: 1847 loss: 5.28197525e-05
Iter: 1848 loss: 5.27822849e-05
Iter: 1849 loss: 5.27805023e-05
Iter: 1850 loss: 5.27584561e-05
Iter: 1851 loss: 5.26992917e-05
Iter: 1852 loss: 5.31030892e-05
Iter: 1853 loss: 5.26857475e-05
Iter: 1854 loss: 5.2629759e-05
Iter: 1855 loss: 5.27432458e-05
Iter: 1856 loss: 5.26070289e-05
Iter: 1857 loss: 5.25462965e-05
Iter: 1858 loss: 5.27211268e-05
Iter: 1859 loss: 5.2526666e-05
Iter: 1860 loss: 5.2472602e-05
Iter: 1861 loss: 5.25428113e-05
Iter: 1862 loss: 5.24445604e-05
Iter: 1863 loss: 5.23743038e-05
Iter: 1864 loss: 5.26934e-05
Iter: 1865 loss: 5.23610506e-05
Iter: 1866 loss: 5.23094714e-05
Iter: 1867 loss: 5.26320146e-05
Iter: 1868 loss: 5.2303265e-05
Iter: 1869 loss: 5.22597693e-05
Iter: 1870 loss: 5.24794305e-05
Iter: 1871 loss: 5.22526316e-05
Iter: 1872 loss: 5.22083465e-05
Iter: 1873 loss: 5.21907787e-05
Iter: 1874 loss: 5.21673937e-05
Iter: 1875 loss: 5.21081238e-05
Iter: 1876 loss: 5.2239142e-05
Iter: 1877 loss: 5.20855756e-05
Iter: 1878 loss: 5.20261856e-05
Iter: 1879 loss: 5.22908413e-05
Iter: 1880 loss: 5.20143039e-05
Iter: 1881 loss: 5.2018222e-05
Iter: 1882 loss: 5.19922978e-05
Iter: 1883 loss: 5.19758032e-05
Iter: 1884 loss: 5.19318601e-05
Iter: 1885 loss: 5.22821865e-05
Iter: 1886 loss: 5.19240566e-05
Iter: 1887 loss: 5.18795277e-05
Iter: 1888 loss: 5.18801025e-05
Iter: 1889 loss: 5.18442575e-05
Iter: 1890 loss: 5.17795706e-05
Iter: 1891 loss: 5.19026289e-05
Iter: 1892 loss: 5.17527369e-05
Iter: 1893 loss: 5.16820219e-05
Iter: 1894 loss: 5.19401037e-05
Iter: 1895 loss: 5.16639193e-05
Iter: 1896 loss: 5.16020373e-05
Iter: 1897 loss: 5.17352164e-05
Iter: 1898 loss: 5.157735e-05
Iter: 1899 loss: 5.15170359e-05
Iter: 1900 loss: 5.1923751e-05
Iter: 1901 loss: 5.15114152e-05
Iter: 1902 loss: 5.14663188e-05
Iter: 1903 loss: 5.16878e-05
Iter: 1904 loss: 5.14583044e-05
Iter: 1905 loss: 5.1412735e-05
Iter: 1906 loss: 5.14935564e-05
Iter: 1907 loss: 5.13928535e-05
Iter: 1908 loss: 5.13504201e-05
Iter: 1909 loss: 5.13861341e-05
Iter: 1910 loss: 5.13247287e-05
Iter: 1911 loss: 5.12743245e-05
Iter: 1912 loss: 5.14016938e-05
Iter: 1913 loss: 5.12567385e-05
Iter: 1914 loss: 5.12482875e-05
Iter: 1915 loss: 5.12341576e-05
Iter: 1916 loss: 5.12094703e-05
Iter: 1917 loss: 5.1172472e-05
Iter: 1918 loss: 5.11718317e-05
Iter: 1919 loss: 5.11338876e-05
Iter: 1920 loss: 5.10843311e-05
Iter: 1921 loss: 5.10810678e-05
Iter: 1922 loss: 5.10158716e-05
Iter: 1923 loss: 5.1194198e-05
Iter: 1924 loss: 5.09941055e-05
Iter: 1925 loss: 5.09243328e-05
Iter: 1926 loss: 5.10818281e-05
Iter: 1927 loss: 5.08986413e-05
Iter: 1928 loss: 5.08319208e-05
Iter: 1929 loss: 5.10927566e-05
Iter: 1930 loss: 5.08162448e-05
Iter: 1931 loss: 5.07558252e-05
Iter: 1932 loss: 5.09269303e-05
Iter: 1933 loss: 5.07358345e-05
Iter: 1934 loss: 5.06824799e-05
Iter: 1935 loss: 5.10643513e-05
Iter: 1936 loss: 5.06779616e-05
Iter: 1937 loss: 5.06288125e-05
Iter: 1938 loss: 5.07830482e-05
Iter: 1939 loss: 5.06144279e-05
Iter: 1940 loss: 5.05687203e-05
Iter: 1941 loss: 5.05935277e-05
Iter: 1942 loss: 5.05394419e-05
Iter: 1943 loss: 5.04895288e-05
Iter: 1944 loss: 5.05880889e-05
Iter: 1945 loss: 5.04688578e-05
Iter: 1946 loss: 5.04289637e-05
Iter: 1947 loss: 5.10648e-05
Iter: 1948 loss: 5.04292693e-05
Iter: 1949 loss: 5.03830452e-05
Iter: 1950 loss: 5.05042517e-05
Iter: 1951 loss: 5.03675328e-05
Iter: 1952 loss: 5.03426054e-05
Iter: 1953 loss: 5.02976764e-05
Iter: 1954 loss: 5.13626073e-05
Iter: 1955 loss: 5.0297218e-05
Iter: 1956 loss: 5.02413241e-05
Iter: 1957 loss: 5.02391849e-05
Iter: 1958 loss: 5.01959876e-05
Iter: 1959 loss: 5.01257455e-05
Iter: 1960 loss: 5.06076503e-05
Iter: 1961 loss: 5.01196118e-05
Iter: 1962 loss: 5.0067305e-05
Iter: 1963 loss: 5.01035902e-05
Iter: 1964 loss: 5.00353781e-05
Iter: 1965 loss: 4.99716152e-05
Iter: 1966 loss: 5.03937481e-05
Iter: 1967 loss: 4.9964794e-05
Iter: 1968 loss: 4.9915885e-05
Iter: 1969 loss: 5.00106944e-05
Iter: 1970 loss: 4.98958252e-05
Iter: 1971 loss: 4.98414302e-05
Iter: 1972 loss: 5.03062074e-05
Iter: 1973 loss: 4.98382396e-05
Iter: 1974 loss: 4.98013724e-05
Iter: 1975 loss: 4.98293884e-05
Iter: 1976 loss: 4.97794244e-05
Iter: 1977 loss: 4.97348155e-05
Iter: 1978 loss: 4.97503643e-05
Iter: 1979 loss: 4.9704e-05
Iter: 1980 loss: 4.96593857e-05
Iter: 1981 loss: 5.01481554e-05
Iter: 1982 loss: 4.96583962e-05
Iter: 1983 loss: 4.96236535e-05
Iter: 1984 loss: 5.01670911e-05
Iter: 1985 loss: 4.96233843e-05
Iter: 1986 loss: 4.96079047e-05
Iter: 1987 loss: 4.95656677e-05
Iter: 1988 loss: 4.98923473e-05
Iter: 1989 loss: 4.9557013e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script66
+ '[' -r STOP.script66 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output66/f1_psi-2_phi2.8/300_300_300_1
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1
+ for phi in 2.8
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output66/f1_psi-2_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output67/f1_psi-2_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output66/f1_psi-2_phi2.8 /home/mrdouglas/Manifold/experiments.final/output67/f1_psi-2_phi2.8
+ date
Sun Oct 25 03:21:14 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output66/f1_psi-2_phi2.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 2345 --load_model experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1 --function f1 --psi -2 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output66/f1_psi-2_phi2.8/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c7ae730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c6ec598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c6e3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c6f8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c70d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c70d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c6108c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c70d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c70d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c5d9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c5899d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c577d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c5ac510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c5a2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c5009d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c5a26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c5020d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c49e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c4299d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c423ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c457620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f033c444d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f031aea3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f031aecd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f031aebb488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f031aebba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f031ae5aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f031ae3e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f031ae3e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f031af2b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f02f4574f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f031ae9a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f031ae9a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f031ae6b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f02f45602f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f02f44c3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 1.9978529
test_loss: 1.9915428
train_loss: 1.9961947
test_loss: 1.9934977
train_loss: 2.0114625
test_loss: 7.9504747
train_loss: 1.9985946
test_loss: 1.989564
train_loss: 1.9974539
test_loss: 1.9922982
train_loss: 1.9993821
test_loss: 1.9923351
train_loss: 1.9908087
test_loss: 1.9943578
train_loss: 1.9822327
test_loss: 1.9922165
train_loss: 1.9919773
test_loss: 1.9925505
train_loss: 1.9979725
test_loss: 1.9924679
train_loss: 1.9764625
test_loss: 1.9935102
train_loss: 1.9953947
test_loss: 1.9938818
train_loss: 1.989798
test_loss: 1.9935408
train_loss: 1.9990263
test_loss: 1.9903829
train_loss: 1.9905789
test_loss: 1.9920596
train_loss: 1.9900584
test_loss: 1.9931116
train_loss: 1.9927145
test_loss: 1.9941994
train_loss: 1.984789
test_loss: 1.9947219
train_loss: 1.9894929
test_loss: 1.9949387
train_loss: 1.9952177
test_loss: 1.9954966
train_loss: 1.9990443
test_loss: 1.9938354
train_loss: 1.9893224
test_loss: 1.9949898
train_loss: 1.9980061
test_loss: 1.9949675
train_loss: 1.9975207
test_loss: 1.9926571
train_loss: 1.9994311
test_loss: 1.9947662
train_loss: 1.9987067
test_loss: 1.9947715
train_loss: 1.9928595
test_loss: 1.9916272
train_loss: 1.9935533
test_loss: 1.9937788
train_loss: 1.9941412
test_loss: 1.994585
train_loss: 1.9856129
test_loss: 1.993844
train_loss: 1.9951802
test_loss: 1.9919264
train_loss: 1.9994195
test_loss: 1.9934824
train_loss: 1.9850292
test_loss: 1.9931849
train_loss: 1.9870089
test_loss: 1.9938139
train_loss: 1.9984754
test_loss: 1.9937664
train_loss: 1.9994714
test_loss: 1.9941
train_loss: 1.999479
test_loss: 1.9939559
train_loss: 1.9860845
test_loss: 1.9976001
train_loss: 1.9963826
test_loss: 1.9955845
train_loss: 1.9965721
test_loss: 1.9940766
train_loss: 1.9854455
test_loss: 1.9946226
train_loss: 1.9993546
test_loss: 1.9940526
train_loss: 1.9946177
test_loss: 1.9931738
train_loss: 1.99138
test_loss: 1.9937973
train_loss: 1.9962134
test_loss: 1.993457
train_loss: 1.9920058
test_loss: 1.9914066
train_loss: 1.9967595
test_loss: 1.9937719
train_loss: 1.9980519
test_loss: 1.9937339
train_loss: 1.9886878
test_loss: 1.9939637
train_loss: 1.9983974
test_loss: 1.9930142
train_loss: 1.9888128
test_loss: 1.9938357
train_loss: 1.9990032
test_loss: 1.9923048
train_loss: 2.1042202
test_loss: 1.9923537
train_loss: 1.9961654
test_loss: 1.9932691
train_loss: 1.9955027
test_loss: 1.9951792
train_loss: 1.9954
test_loss: 1.9924605
train_loss: 1.9933681
test_loss: 1.9926449
train_loss: 1.995413
test_loss: 1.9938935
train_loss: 1.9870276
test_loss: 1.9921377
train_loss: 1.9938731
test_loss: 1.995103
train_loss: 1.9956095
test_loss: 1.994038
train_loss: 1.9837866
test_loss: 1.99319
train_loss: 1.997166
test_loss: 1.9946284
train_loss: 1.9908136
test_loss: 1.9953141
train_loss: 1.9888576
test_loss: 1.9939591
train_loss: 1.9933164
test_loss: 1.9951421
train_loss: 1.99722
test_loss: 1.9945666
train_loss: 1.9988563
test_loss: 1.9929574
train_loss: 1.9952061
test_loss: 1.9934725
train_loss: 1.9837054
test_loss: 1.9954721
train_loss: 1.9789279
test_loss: 1.9921628
train_loss: 1.9917932
test_loss: 1.9965655
train_loss: 1.9812355
test_loss: 1.9936957
train_loss: 1.9991549
test_loss: 1.9927856
train_loss: 1.9895201
test_loss: 1.9934925
train_loss: 1.9803898
test_loss: 1.9930639
train_loss: 1.9884772
test_loss: 1.9937176
train_loss: 1.9946165
test_loss: 1.9951397
train_loss: 1.989623
test_loss: 1.9936283
train_loss: 1.9873743
test_loss: 1.9922509
train_loss: 1.9949782
test_loss: 1.9922897
train_loss: 1.9944441
test_loss: 1.9943106
train_loss: 1.9960887
test_loss: 1.9938159
train_loss: 1.9846318
test_loss: 1.9925947
train_loss: 1.9809783
test_loss: 1.9945229
train_loss: 1.9823048
test_loss: 1.9940863
train_loss: 1.9958295
test_loss: 1.99581
train_loss: 1.9628115
test_loss: 1.9942418
train_loss: 1.991987
test_loss: 1.9938283
train_loss: 1.9985037
test_loss: 1.9934354
train_loss: 1.9779042
test_loss: 1.9934722
train_loss: 1.9889685
test_loss: 1.9932839
train_loss: 1.9957545
test_loss: 1.9937923
train_loss: 1.9958663
test_loss: 1.9936972
train_loss: 1.9839454
test_loss: 1.9934893
train_loss: 1.9909163
test_loss: 1.9945861
train_loss: 2.2080114
test_loss: 1.9961488
train_loss: 1.9937203
test_loss: 1.9938394
train_loss: 1.9675603
test_loss: 1.9934968
train_loss: 1.9836729
test_loss: 1.993404
train_loss: 1.9894695
test_loss: 1.994078
train_loss: 1.9988587
test_loss: 1.9921265
train_loss: 1.9988861
test_loss: 1.9943334
train_loss: 1.9938307
test_loss: 1.9939249
train_loss: 1.9896489
test_loss: 1.9949049
train_loss: 1.9806488
test_loss: 1.993373
train_loss: 1.9989929
test_loss: 1.9946682
train_loss: 1.9940878
test_loss: 1.9955734
train_loss: 1.9994564
test_loss: 1.9932064
train_loss: 1.9886606
test_loss: 1.9959072
train_loss: 1.985356
test_loss: 1.9930296
train_loss: 1.9855494
test_loss: 1.9936287
train_loss: 1.9982034
test_loss: 1.9933757
train_loss: 1.9968481
test_loss: 1.9918101
train_loss: 1.9947904
test_loss: 1.995607
train_loss: 1.9957762
test_loss: 1.9926338
train_loss: 1.9947369
test_loss: 2.0051973
train_loss: 1.9936299
test_loss: 1.9944648
train_loss: 1.9865854
test_loss: 1.9940062
train_loss: 1.9980848
test_loss: 1.994389
train_loss: 1.9754419
test_loss: 1.9941006
train_loss: 1.9965254
test_loss: 1.9945165
train_loss: 1.9994395
test_loss: 1.9959213
train_loss: 1.9885011
test_loss: 1.9953789
train_loss: 1.9981805
test_loss: 1.9939437
train_loss: 1.9753153
test_loss: 1.9943371
train_loss: 1.9932485
test_loss: 1.9945574
train_loss: 1.9916229
test_loss: 1.9920937
train_loss: 1.9988716
test_loss: 1.9927757
train_loss: 1.9987999
test_loss: 1.9955425
train_loss: 1.990065
test_loss: 1.9920579
train_loss: 1.9964795
test_loss: 2.0000973
train_loss: 1.994927
test_loss: 1.9897029
train_loss: 1.992627
test_loss: 1.9932101
train_loss: 1.9985003
test_loss: 1.9974241
train_loss: 1.981974
test_loss: 1.992638
train_loss: 1.9954264
test_loss: 1.994201
train_loss: 1.9881442
test_loss: 1.9932064
train_loss: 1.9948242
test_loss: 1.9949652
train_loss: 1.9893504
test_loss: 1.9928062
train_loss: 1.9942554
test_loss: 1.9945365
train_loss: 1.983772
test_loss: 1.9942838
train_loss: 1.9913123
test_loss: 1.9936982
train_loss: 1.9901121
test_loss: 1.9943205
train_loss: 1.9911808
test_loss: 1.9958713
train_loss: 1.981492
test_loss: 1.9940071
train_loss: 1.9916439
test_loss: 1.9928553
train_loss: 1.9983964
test_loss: 1.9972371
train_loss: 1.9924903
test_loss: 1.9937376
train_loss: 1.9954789
test_loss: 1.9933027
train_loss: 1.9983053
test_loss: 1.9937618
train_loss: 1.9912701
test_loss: 1.9942179
train_loss: 1.9929472
test_loss: 1.9938872
train_loss: 1.9971169
test_loss: 1.9934897
train_loss: 1.996497
test_loss: 1.9978846
train_loss: 1.9941853
test_loss: 1.9930712
train_loss: 1.9960325
test_loss: 1.9944831
train_loss: 1.9913762
test_loss: 1.9934387
train_loss: 1.987162
test_loss: 1.9949675
train_loss: 1.9863493
test_loss: 1.9937985
train_loss: 1.9962434
test_loss: 1.9928648
train_loss: 1.993949
test_loss: 1.9951255
train_loss: 1.9888296
test_loss: 1.9936779
train_loss: 1.9925718
test_loss: 1.9938624
train_loss: 1.9868217
test_loss: 1.9937506
train_loss: 1.9889703
test_loss: 1.9943537
train_loss: 1.9964267
test_loss: 1.9932721
train_loss: 1.9982378
test_loss: 1.9921715
train_loss: 1.9989259
test_loss: 1.9923465
train_loss: 1.9719993
test_loss: 1.9915302
train_loss: 1.9910767
test_loss: 1.9935977
train_loss: 1.9938895
test_loss: 1.9944015
train_loss: 1.9935566
test_loss: 1.9935042
train_loss: 1.9986523
test_loss: 1.9949605
train_loss: 1.9921612
test_loss: 1.994065
train_loss: 1.9954363
test_loss: 1.9950794
train_loss: 1.995481
test_loss: 1.9930134
train_loss: 1.9942392
test_loss: 1.9938246
train_loss: 1.9930111
test_loss: 1.9934053
train_loss: 1.9909167
test_loss: 1.993416
train_loss: 1.990546
test_loss: 1.9937327
train_loss: 1.9884851
test_loss: 1.9943646
train_loss: 1.9840012
test_loss: 1.9937015
train_loss: 1.9931351
test_loss: 1.9927168
train_loss: 1.9895157
test_loss: 1.9943856
train_loss: 1.9835973
test_loss: 1.9957513
train_loss: 1.9977864
test_loss: 1.9930241
train_loss: 1.9832107
test_loss: 1.9926618
train_loss: 1.9867411
test_loss: 1.995024
train_loss: 1.9964243
test_loss: 1.9951247
train_loss: 1.9960512
test_loss: 1.9919405
train_loss: 1.9919944
test_loss: 1.9918218
train_loss: 1.9949147
test_loss: 1.9943149
train_loss: 1.9868367
test_loss: 1.9930589
train_loss: 1.995737
test_loss: 1.9940761
train_loss: 1.9833308
test_loss: 1.9939114
train_loss: 1.9989429
test_loss: 1.9951103
train_loss: 1.9895537
test_loss: 1.9936377
train_loss: 1.9830289
test_loss: 1.9942195
train_loss: 1.9926422
test_loss: 1.9933618
train_loss: 1.9947679
test_loss: 1.9952025
train_loss: 1.9970298
test_loss: 1.9935117
train_loss: 1.9965665
test_loss: 1.9922484
train_loss: 1.9835813
test_loss: 1.9940836
train_loss: 1.9740651
test_loss: 1.9962612
train_loss: 1.9982281
test_loss: 1.9950612
train_loss: 1.983423
test_loss: 1.9938146
train_loss: 1.9813585
test_loss: 1.9953196
train_loss: 1.9905146
test_loss: 1.991979
train_loss: 1.9994124
test_loss: 1.9918981
train_loss: 1.979882
test_loss: 1.9913577
train_loss: 1.9987096
test_loss: 1.9929287
train_loss: 1.9979906
test_loss: 1.9926565
train_loss: 1.9881601
test_loss: 1.9938587
train_loss: 1.9829636
test_loss: 1.9945894
train_loss: 1.9912622
test_loss: 1.9925181
train_loss: 1.9963114
test_loss: 1.9931376
train_loss: 1.9942179
test_loss: 1.9935825
train_loss: 1.9968904
test_loss: 1.9940153
train_loss: 1.9994287
test_loss: 1.9939066
train_loss: 1.9891927
test_loss: 1.9951227
train_loss: 1.998749
test_loss: 1.9931453
train_loss: 1.9845991
test_loss: 1.9938674
train_loss: 1.9941428
test_loss: 1.9924693
train_loss: 1.9965584
test_loss: 1.9927899
train_loss: 1.9933999
test_loss: 1.9948071
train_loss: 1.991339
test_loss: 1.9921567
train_loss: 1.9905245
test_loss: 1.992697
train_loss: 1.9892689
test_loss: 1.9959375
train_loss: 1.9994849
test_loss: 1.9933646
train_loss: 1.9946722
test_loss: 1.9937274
train_loss: 1.9964447
test_loss: 1.9928179
train_loss: 1.9978979
test_loss: 1.9931366
train_loss: 1.9762639
test_loss: 1.9940375
train_loss: 1.9994891
test_loss: 1.9950398
train_loss: 1.990166
test_loss: 1.9922924
train_loss: 1.9952142
test_loss: 1.993371
train_loss: 1.9881322
test_loss: 1.9950874
train_loss: 1.995801
test_loss: 1.9923551
train_loss: 1.9939489
test_loss: 1.9934355
train_loss: 1.9864777
test_loss: 1.993503
train_loss: 1.9970304
test_loss: 1.9936477
train_loss: 1.986488
test_loss: 1.9951057
train_loss: 1.9850668
test_loss: 2.0050213
train_loss: 1.9987713
test_loss: 1.9952718
train_loss: 1.9919442
test_loss: 1.9916426
train_loss: 1.9883311
test_loss: 1.9946737
train_loss: 1.9881943
test_loss: 1.9935423
train_loss: 1.9857771
test_loss: 1.9936489
train_loss: 1.9967926
test_loss: 1.9917176
train_loss: 1.999493
test_loss: 1.9910997
train_loss: 2.0215364
test_loss: 1.9940133
train_loss: 1.989116
test_loss: 1.9928508
train_loss: 1.9822371
test_loss: 1.9973615
train_loss: 1.9954884
test_loss: 1.9952738
train_loss: 1.9901187
test_loss: 1.9965616
train_loss: 1.9850814
test_loss: 1.9939057
train_loss: 1.9705197
test_loss: 1.9942793
train_loss: 1.9991848
test_loss: 1.994682
train_loss: 1.9983666
test_loss: 1.9942129
train_loss: 1.9941607
test_loss: 1.9931645
train_loss: 1.9763355
test_loss: 1.9948443
train_loss: 1.999058
test_loss: 1.9968101
train_loss: 1.9857028
test_loss: 1.9919932
train_loss: 1.9951763
test_loss: 1.9929854
train_loss: 1.9955755
test_loss: 1.9960318
train_loss: 1.994893
test_loss: 1.9948362
train_loss: 1.9983034
test_loss: 1.9936811
train_loss: 1.9938287
test_loss: 1.9946204
train_loss: 1.9961221
test_loss: 1.9944124
train_loss: 1.99137
test_loss: 1.9926809
train_loss: 1.9988357
test_loss: 1.9939733
train_loss: 1.9821973
test_loss: 1.992294
train_loss: 1.9897572
test_loss: 1.9916185
train_loss: 1.9929312
test_loss: 1.9954557
train_loss: 1.996405
test_loss: 1.9930918
train_loss: 1.9851009
test_loss: 1.9924701
train_loss: 1.980573
test_loss: 1.9934341
train_loss: 1.9870943
test_loss: 1.9936612
train_loss: 1.991157
test_loss: 1.9939874
train_loss: 1.9902611
test_loss: 1.9946208
train_loss: 1.9804605
test_loss: 1.9934572
train_loss: 1.9817388
test_loss: 1.9933923
train_loss: 1.9981678
test_loss: 1.9928637
train_loss: 1.9966605
test_loss: 1.9956806
train_loss: 2.0255837
test_loss: 1.9928375
train_loss: 1.9984822
test_loss: 1.9933143
train_loss: 1.9960347
test_loss: 1.9932053
train_loss: 1.9962411
test_loss: 1.9933169
train_loss: 1.9960203
test_loss: 1.9916192
train_loss: 1.9907403
test_loss: 1.994919
train_loss: 1.9887292
test_loss: 1.9968172
train_loss: 1.9831488
test_loss: 1.9929523
train_loss: 1.9966663
test_loss: 1.99537
train_loss: 1.9892702
test_loss: 1.9931762
train_loss: 1.993391
test_loss: 1.9952246
train_loss: 1.9994321
test_loss: 1.9925455
train_loss: 1.994248
test_loss: 1.9941717
train_loss: 1.9981105
test_loss: 1.9935238
train_loss: 1.9938043
test_loss: 1.9929705
train_loss: 1.9976015
test_loss: 1.9943001
train_loss: 1.9978973
test_loss: 1.9975595
train_loss: 1.981846
test_loss: 1.9948149
train_loss: 2.0226288
test_loss: 1.9912535
train_loss: 1.9907693
test_loss: 1.9921162
train_loss: 1.9990125
test_loss: 1.9951078
train_loss: 1.9965868
test_loss: 1.9936435
train_loss: 1.9819489
test_loss: 1.9934729
train_loss: 1.9957316
test_loss: 1.9945804
train_loss: 1.9878131
test_loss: 1.9940076
train_loss: 1.985601
test_loss: 1.9943407
train_loss: 1.9866357
test_loss: 1.9979293
train_loss: 1.9931524
test_loss: 1.9937427
train_loss: 1.9733462
test_loss: 1.9939463
train_loss: 1.992313
test_loss: 1.9938362
train_loss: 1.9946299
test_loss: 1.993728
train_loss: 1.9809029
test_loss: 1.9933786
train_loss: 1.9990392
test_loss: 1.9948869
train_loss: 1.9919436
test_loss: 1.9945695
train_loss: 1.9963058
test_loss: 1.9939997
train_loss: 1.9852796
test_loss: 1.9928844
train_loss: 1.9889266
test_loss: 1.9940767
train_loss: 1.9991082
test_loss: 1.9990212
train_loss: 1.9875267
test_loss: 1.9914669
train_loss: 1.9751945
test_loss: 1.9943651
train_loss: 1.9882746
test_loss: 1.994994
train_loss: 1.9981593
test_loss: 1.9917017
train_loss: 1.9932494
test_loss: 1.9940028
train_loss: 1.9987283
test_loss: 1.9913191
train_loss: 1.9967465
test_loss: 1.9942718
train_loss: 1.9801561
test_loss: 1.9929613
train_loss: 1.9994345
test_loss: 1.9942412
train_loss: 1.993578
test_loss: 1.9956751
train_loss: 1.9959251
test_loss: 1.9930897
train_loss: 1.9994572
test_loss: 1.9939816
train_loss: 1.9881945
test_loss: 1.9939591
train_loss: 1.9853312
test_loss: 1.9948457
train_loss: 1.9781529
test_loss: 1.9926476
train_loss: 1.9891056
test_loss: 1.9941783
train_loss: 1.9911427
test_loss: 1.9937106
train_loss: 1.9889121
test_loss: 1.9926692
train_loss: 1.9984536
test_loss: 1.9932867
train_loss: 1.9935321
test_loss: 1.9920967
train_loss: 1.9861526
test_loss: 1.9935987
train_loss: 1.995985
test_loss: 1.9932806
train_loss: 1.9886545
test_loss: 1.9941068
train_loss: 1.9991252
test_loss: 1.9930211
train_loss: 1.9994366
test_loss: 1.9920355
train_loss: 1.9965521
test_loss: 2.010274
train_loss: 1.9987181
test_loss: 1.9938085
train_loss: 1.9912939
test_loss: 1.9927481
train_loss: 1.9917938
test_loss: 1.9932706
train_loss: 1.9874794
test_loss: 1.9940318
train_loss: 1.9937934
test_loss: 1.9941345
train_loss: 1.9900672
test_loss: 1.9951913
train_loss: 1.9889704
test_loss: 1.9912996
train_loss: 1.999372
test_loss: 1.9944248
train_loss: 1.9979827
test_loss: 1.9922358
train_loss: 1.994226
test_loss: 1.9930935
train_loss: 1.9935541
test_loss: 1.9951739
train_loss: 1.9861112
test_loss: 1.9983103
train_loss: 1.9957662
test_loss: 1.9949876
train_loss: 2.0130634
test_loss: 1.9965307
train_loss: 1.9893421
test_loss: 1.994001
train_loss: 1.98221
test_loss: 2.0008266
train_loss: 1.9943316
test_loss: 1.9913751
train_loss: 1.9899013
test_loss: 1.9942899
train_loss: 1.9967446
test_loss: 1.9923916
train_loss: 1.9892354
test_loss: 1.9923759
train_loss: 1.9897304
test_loss: 1.9925965
train_loss: 1.996636
test_loss: 1.9933355
train_loss: 1.9922721
test_loss: 1.9942887
train_loss: 1.9944974
test_loss: 1.9987422
train_loss: 1.9994649
test_loss: 1.9938316
train_loss: 1.9983134
test_loss: 1.9919753
train_loss: 1.9858674
test_loss: 1.9948223
train_loss: 1.9947201
test_loss: 1.9943438
train_loss: 1.9981402
test_loss: 1.9915743
train_loss: 1.9994123
test_loss: 1.9933246
train_loss: 1.9920659
test_loss: 1.9928191
train_loss: 1.9949831
test_loss: 1.9939749
train_loss: 1.9866673
test_loss: 1.9940554
train_loss: 1.9911427
test_loss: 1.993384
train_loss: /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
1.9909748
test_loss: 1.993182
train_loss: 1.9824989
test_loss: 1.9939269
train_loss: 1.9946995
test_loss: 1.993572
train_loss: 1.9929355
test_loss: 1.9939749
train_loss: 1.9890089
test_loss: 1.996154
train_loss: 1.9910612
test_loss: 1.9951323
train_loss: 1.9927883
test_loss: 1.9944283
train_loss: 1.9937359
test_loss: 1.9939585
train_loss: 1.9747652
test_loss: 1.9942478
train_loss: 1.9943889
test_loss: 1.9941579
train_loss: 1.9877484
test_loss: 1.9938085
train_loss: 1.9936383
test_loss: 2.0014946
train_loss: 1.9940038
test_loss: 1.9952216
train_loss: 1.9952161
test_loss: 1.9944946
train_loss: 1.9731281
test_loss: 1.9932897
train_loss: 1.9957967
test_loss: 1.9933399
train_loss: 2.0108645
test_loss: 1.9949322
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output67/f1_psi-2_phi2.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output66/f1_psi-2_phi2.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output67/f1_psi-2_phi2.8/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8849ac7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb884912598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb884916048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb884912b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8848be2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8848be400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8847d8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8847adc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8847ad488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb884743730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb884743ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb884732d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb884725ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8846ec620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8846919d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb884691488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb88464c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb88466bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb88466bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8845dbae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8845db620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb884593598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8845b3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8845586a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb884558400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb884528f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8845132f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb845844ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb845844ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb84581a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8457c3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8457641e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8457642f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb84578e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb845729488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb800733378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5230.58789
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Traceback (most recent call last):
  File "biholoNN_train.py", line 169, in <module>
    results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=train_func, initial_position=init_params, max_iterations=max_epochs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 287, in minimize
    parallel_iterations=parallel_iterations)[0]
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 574, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2499, in while_loop_v2
    return_same_structure=True)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2735, in while_loop
    loop_vars = body(*loop_vars)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 260, in _body
    max_line_search_iterations)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 156, in line_search_step
    max_iterations=max_iterations)  # No search needed for these.
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 259, in hager_zhang
    threshold_use_approximate_wolfe_condition)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 609, in _prepare_args
    val_initial = value_and_gradients_function(initial_step_size)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 251, in _restricted_func
    objective_value, gradient = value_and_gradients_function(pt)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 814, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
	 [[Sum_1/_34]]
  (1) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_f_16876]

Function call stack:
f -> f

++ basename experiments.final/script66
+ '[' -r STOP.script66 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output66/f1_psi-2_phi2.8/500_500_500_500_1
