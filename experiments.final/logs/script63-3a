+ RUN=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='2 3'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1 --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc3bc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc3bc1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc2f6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc25bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc255950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc166510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc1bcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc189ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc189730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc118b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0d3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0dd8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0a1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efbffea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc003b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc02f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5e3c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d8f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d8ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d53268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d0ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5cc5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5cd36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5cd37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5dfc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5dfc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5c83ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5c09510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5c29f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc03f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5b8c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5b8c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5b8c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5bf5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5bd7d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.019547626
test_loss: 0.019671382
train_loss: 0.0090725655
test_loss: 0.00898119
train_loss: 0.006269957
test_loss: 0.0066933804
train_loss: 0.005477133
test_loss: 0.0056914627
train_loss: 0.0053307153
test_loss: 0.005301539
train_loss: 0.005177132
test_loss: 0.0051679346
train_loss: 0.004584152
test_loss: 0.0050352486
train_loss: 0.004709363
test_loss: 0.0047426084
train_loss: 0.0045332955
test_loss: 0.004968934
train_loss: 0.004450911
test_loss: 0.004846005
train_loss: 0.0042535053
test_loss: 0.004670961
train_loss: 0.004462671
test_loss: 0.004772661
train_loss: 0.0041610645
test_loss: 0.0046513537
train_loss: 0.004110224
test_loss: 0.004519015
train_loss: 0.0041226554
test_loss: 0.0048108515
train_loss: 0.0041026324
test_loss: 0.0044956687
train_loss: 0.0039682505
test_loss: 0.004372526
train_loss: 0.004067778
test_loss: 0.004321856
train_loss: 0.0039699683
test_loss: 0.0043749553
train_loss: 0.003813988
test_loss: 0.0041737743
train_loss: 0.0040311324
test_loss: 0.0042153453
train_loss: 0.0039775805
test_loss: 0.0042263716
train_loss: 0.0038991314
test_loss: 0.004353083
train_loss: 0.004083889
test_loss: 0.0041781124
train_loss: 0.0038717086
test_loss: 0.004348058
train_loss: 0.004047476
test_loss: 0.0041527497
train_loss: 0.004040085
test_loss: 0.0044506085
train_loss: 0.0037939711
test_loss: 0.0041256505
train_loss: 0.0038523804
test_loss: 0.0042982325
train_loss: 0.003646867
test_loss: 0.004183979
train_loss: 0.003758031
test_loss: 0.004175936
train_loss: 0.0034912326
test_loss: 0.0041251816
train_loss: 0.0035836175
test_loss: 0.0042388123
train_loss: 0.003477535
test_loss: 0.003987771
train_loss: 0.0038018895
test_loss: 0.003973167
train_loss: 0.0035137008
test_loss: 0.003928891
train_loss: 0.00344377
test_loss: 0.0039027405
train_loss: 0.0034626015
test_loss: 0.0038776102
train_loss: 0.0036062435
test_loss: 0.0039891796
train_loss: 0.0034179867
test_loss: 0.0040519987
train_loss: 0.0034938576
test_loss: 0.003945753
train_loss: 0.0034357607
test_loss: 0.003971588
train_loss: 0.0036368994
test_loss: 0.0040100412
train_loss: 0.0035285421
test_loss: 0.0038912948
train_loss: 0.0034754837
test_loss: 0.004097174
train_loss: 0.0035042984
test_loss: 0.0039125034
train_loss: 0.0034002087
test_loss: 0.0037513967
train_loss: 0.0033820462
test_loss: 0.0039441707
train_loss: 0.0032757826
test_loss: 0.003755735
train_loss: 0.0035601624
test_loss: 0.0037517531
train_loss: 0.003427818
test_loss: 0.0037525482
train_loss: 0.0033806544
test_loss: 0.0038233057
train_loss: 0.003265006
test_loss: 0.0036814178
train_loss: 0.003451409
test_loss: 0.0037147857
train_loss: 0.0033702694
test_loss: 0.0038042273
train_loss: 0.0034005356
test_loss: 0.0040830285
train_loss: 0.0035511795
test_loss: 0.004015981
train_loss: 0.0032240718
test_loss: 0.0035995601
train_loss: 0.0033367996
test_loss: 0.0036661767
train_loss: 0.0032932581
test_loss: 0.0037487417
train_loss: 0.003391197
test_loss: 0.0038559055
train_loss: 0.0032253219
test_loss: 0.0037323972
train_loss: 0.0031579952
test_loss: 0.0035365745
train_loss: 0.0032422296
test_loss: 0.0035806636
train_loss: 0.0032111001
test_loss: 0.0038482659
train_loss: 0.003291798
test_loss: 0.0036947862
train_loss: 0.003109728
test_loss: 0.0036800758
train_loss: 0.0032820161
test_loss: 0.0036755698
train_loss: 0.003481783
test_loss: 0.0036724657
train_loss: 0.0029738753
test_loss: 0.0034515145
train_loss: 0.003355905
test_loss: 0.0037474388
train_loss: 0.003100234
test_loss: 0.0036929282
train_loss: 0.0031402444
test_loss: 0.0036745954
train_loss: 0.003534715
test_loss: 0.0035946153
train_loss: 0.003177234
test_loss: 0.0037799387
train_loss: 0.00330197
test_loss: 0.0037961465
train_loss: 0.002825023
test_loss: 0.0034027852
train_loss: 0.003039434
test_loss: 0.0035345107
train_loss: 0.0030412555
test_loss: 0.0036148957
train_loss: 0.0031319219
test_loss: 0.003370568
train_loss: 0.0031488682
test_loss: 0.003540186
train_loss: 0.0028720784
test_loss: 0.0035075166
train_loss: 0.0030095554
test_loss: 0.0036241552
train_loss: 0.0030054187
test_loss: 0.0036150832
train_loss: 0.0031567193
test_loss: 0.0035198943
train_loss: 0.0031195716
test_loss: 0.0035026195
train_loss: 0.003205
test_loss: 0.0035274802
train_loss: 0.0029213028
test_loss: 0.0035053417
train_loss: 0.0029996864
test_loss: 0.0035015382
train_loss: 0.0031255074
test_loss: 0.0037172693
train_loss: 0.0030857595
test_loss: 0.0036228434
train_loss: 0.0030892007
test_loss: 0.0035819702
train_loss: 0.0030290368
test_loss: 0.0035269032
train_loss: 0.0030759373
test_loss: 0.0035402868
train_loss: 0.0033104937
test_loss: 0.0037094196
train_loss: 0.0030366126
test_loss: 0.0037057057
train_loss: 0.0030122912
test_loss: 0.0036834236
train_loss: 0.0029721374
test_loss: 0.0035096689
train_loss: 0.0032497342
test_loss: 0.0034526417
train_loss: 0.0029798911
test_loss: 0.00347377
train_loss: 0.0027542568
test_loss: 0.003471824
train_loss: 0.0029923492
test_loss: 0.0035209518
train_loss: 0.0031998272
test_loss: 0.00355104
train_loss: 0.0032948111
test_loss: 0.003453693
train_loss: 0.0031483178
test_loss: 0.003475432
train_loss: 0.0033348862
test_loss: 0.0036273901
train_loss: 0.0030804984
test_loss: 0.0036315022
train_loss: 0.0032577193
test_loss: 0.0036114228
train_loss: 0.0031659636
test_loss: 0.003802141
train_loss: 0.0030901646
test_loss: 0.0035083375
train_loss: 0.0029157498
test_loss: 0.0034951034
train_loss: 0.0028182624
test_loss: 0.0033984596
train_loss: 0.0028133206
test_loss: 0.003326088
train_loss: 0.0029177275
test_loss: 0.0032958766
train_loss: 0.0029230635
test_loss: 0.0034473774
train_loss: 0.0030684215
test_loss: 0.0035568478
train_loss: 0.0033718667
test_loss: 0.0035185178
train_loss: 0.0029587739
test_loss: 0.0035073983
train_loss: 0.002879326
test_loss: 0.0034426556
train_loss: 0.002886285
test_loss: 0.003563428
train_loss: 0.0028952968
test_loss: 0.0034910107
train_loss: 0.0027492498
test_loss: 0.0035135318
train_loss: 0.0028979182
test_loss: 0.003454611
train_loss: 0.0030358643
test_loss: 0.0035228077
train_loss: 0.002753323
test_loss: 0.0033736704
train_loss: 0.0030144933
test_loss: 0.0034357475
train_loss: 0.0027485294
test_loss: 0.0035915799
train_loss: 0.0029275483
test_loss: 0.003461708
train_loss: 0.0026932012
test_loss: 0.0035387974
train_loss: 0.0029065844
test_loss: 0.003393748
train_loss: 0.002970633
test_loss: 0.0034050585
train_loss: 0.0031778896
test_loss: 0.0035654195
train_loss: 0.003054358
test_loss: 0.0034266133
train_loss: 0.0028854788
test_loss: 0.003346553
train_loss: 0.0028133374
test_loss: 0.0033250407
train_loss: 0.002825835
test_loss: 0.0033496371
train_loss: 0.002874393
test_loss: 0.0033193908
train_loss: 0.002926884
test_loss: 0.003607652
train_loss: 0.003126409
test_loss: 0.003431352
train_loss: 0.0030203417
test_loss: 0.0033926833
train_loss: 0.0028834217
test_loss: 0.003385762
train_loss: 0.0028411853
test_loss: 0.0033201354
train_loss: 0.0031297058
test_loss: 0.003518323
train_loss: 0.0028077648
test_loss: 0.0034006243
train_loss: 0.003007865
test_loss: 0.0033842642
train_loss: 0.0028573652
test_loss: 0.0034196775
train_loss: 0.002902547
test_loss: 0.0034724858
train_loss: 0.002824613
test_loss: 0.0032689595
train_loss: 0.002866327
test_loss: 0.003336344
train_loss: 0.0026882575
test_loss: 0.0032234036
train_loss: 0.0029545038
test_loss: 0.0033085416
train_loss: 0.0029573021
test_loss: 0.00350564
train_loss: 0.002799062
test_loss: 0.0034960934
train_loss: 0.0027994132
test_loss: 0.003284992
train_loss: 0.002702122
test_loss: 0.0033569399
train_loss: 0.0028348344
test_loss: 0.0033491221
train_loss: 0.0029710871
test_loss: 0.003295566
train_loss: 0.003029543
test_loss: 0.0035612963
train_loss: 0.0029697614
test_loss: 0.0034142302
train_loss: 0.002884876
test_loss: 0.003381833
train_loss: 0.0028450328
test_loss: 0.0033320785
train_loss: 0.0027937444
test_loss: 0.003335728
train_loss: 0.002887777
test_loss: 0.0032338463
train_loss: 0.002654045
test_loss: 0.0032966551
train_loss: 0.0029149055
test_loss: 0.0033503133
train_loss: 0.0026438679
test_loss: 0.003198512
train_loss: 0.0026022391
test_loss: 0.0032339573
train_loss: 0.0029360927
test_loss: 0.0031914106
train_loss: 0.0027526254
test_loss: 0.0033213145
train_loss: 0.0026618934
test_loss: 0.0032308914
train_loss: 0.0027489283
test_loss: 0.0033388557
train_loss: 0.003222722
test_loss: 0.0034210582
train_loss: 0.0034299411
test_loss: 0.0035947682
train_loss: 0.002843319
test_loss: 0.0035041643
train_loss: 0.0027061112
test_loss: 0.0033784953
train_loss: 0.003024421
test_loss: 0.0031818687
train_loss: 0.0030919171
test_loss: 0.0035386032
train_loss: 0.0028586753
test_loss: 0.0032383043
train_loss: 0.0027233376
test_loss: 0.0032418056
train_loss: 0.003011302
test_loss: 0.0033342505
train_loss: 0.0027760505
test_loss: 0.003261263
train_loss: 0.0030223336
test_loss: 0.0032019895
train_loss: 0.0026941323
test_loss: 0.0033408583
train_loss: 0.0028003599
test_loss: 0.0031526724
train_loss: 0.0029007941
test_loss: 0.0034161375
train_loss: 0.002626876
test_loss: 0.0033419884
train_loss: 0.002694023
test_loss: 0.003144593
train_loss: 0.00271307
test_loss: 0.003214994
train_loss: 0.0027537274
test_loss: 0.0031517926
train_loss: 0.0028093643
test_loss: 0.0031971983
train_loss: 0.0028385757
test_loss: 0.0032446086
train_loss: 0.0027733813
test_loss: 0.0034759967
train_loss: 0.0028015967
test_loss: 0.0034257513
train_loss: 0.002635348
test_loss: 0.0031476042
train_loss: 0.0023957689
test_loss: 0.0030443403
train_loss: 0.0026956652
test_loss: 0.0031948178
train_loss: 0.0027950779
test_loss: 0.0031661435
train_loss: 0.002696092
test_loss: 0.0032547778
train_loss: 0.0027798621
test_loss: 0.0032376836
train_loss: 0.002688308
test_loss: 0.003332539
train_loss: 0.0028173276
test_loss: 0.0031972623
train_loss: 0.0028132463
test_loss: 0.0033499089
train_loss: 0.0031623738
test_loss: 0.0033269152
train_loss: 0.002786164
test_loss: 0.0033834744
train_loss: 0.0031135888
test_loss: 0.0033840556
train_loss: 0.0026651553
test_loss: 0.0032382691
train_loss: 0.0026890999
test_loss: 0.003319003
train_loss: 0.002792087
test_loss: 0.0032163612
train_loss: 0.0026116234
test_loss: 0.0033806683
train_loss: 0.0026158623
test_loss: 0.0031239614
train_loss: 0.0026314107
test_loss: 0.0032441325
train_loss: 0.0026469356
test_loss: 0.0030629989
train_loss: 0.0025724557
test_loss: 0.00327408
train_loss: 0.0027236014
test_loss: 0.0032804494
train_loss: 0.0026818425
test_loss: 0.0032046246
train_loss: 0.0027574685
test_loss: 0.0032881144
train_loss: 0.0027022338
test_loss: 0.003108547
train_loss: 0.002743667
test_loss: 0.0033077
train_loss: 0.002777342
test_loss: 0.0033761356
train_loss: 0.002554835
test_loss: 0.0032199097
train_loss: 0.0026825909
test_loss: 0.0032697592
train_loss: 0.0026999621
test_loss: 0.0031754095
train_loss: 0.0024217698
test_loss: 0.0033690229
train_loss: 0.0025917653
test_loss: 0.0031233663
train_loss: 0.0028114829
test_loss: 0.0032382866
train_loss: 0.0027476624
test_loss: 0.003409831
train_loss: 0.0028319359
test_loss: 0.003225152
train_loss: 0.0025178567
test_loss: 0.003197765
train_loss: 0.0027305246
test_loss: 0.0032894956
train_loss: 0.002803234
test_loss: 0.0034056862
train_loss: 0.0028627482
test_loss: 0.0032638193
train_loss: 0.0029985865
test_loss: 0.0033122015
train_loss: 0.0028155248
test_loss: 0.0033387826
train_loss: 0.0027036667
test_loss: 0.0032714938
train_loss: 0.0028630737
test_loss: 0.0032813158
train_loss: 0.0026126604
test_loss: 0.0031988195
train_loss: 0.0026209902
test_loss: 0.0032417884
train_loss: 0.0027234002
test_loss: 0.0033010526
train_loss: 0.0026880282
test_loss: 0.0032584174
train_loss: 0.002762279
test_loss: 0.003310852
train_loss: 0.002726587
test_loss: 0.0032861647
train_loss: 0.0026257313
test_loss: 0.003142028
train_loss: 0.0025778664
test_loss: 0.0030626336
train_loss: 0.0025943327
test_loss: 0.0031601323
train_loss: 0.0026659602
test_loss: 0.0032112047
train_loss: 0.0025574851
test_loss: 0.0032067134
train_loss: 0.00271732
test_loss: 0.0032462399
train_loss: 0.0025556935
test_loss: 0.003166267
train_loss: 0.0025719004
test_loss: 0.0032335941
train_loss: 0.0027727224
test_loss: 0.0033520472
train_loss: 0.0026562877
test_loss: 0.0031408158
train_loss: 0.0026923446
test_loss: 0.003292029
train_loss: 0.0025160247
test_loss: 0.0031453564
train_loss: 0.0024189523
test_loss: 0.003115839
train_loss: 0.0026687554
test_loss: 0.0031823325
train_loss: 0.002682491
test_loss: 0.0031937123
train_loss: 0.0025997362
test_loss: 0.003151958
train_loss: 0.0027012008
test_loss: 0.0031147015
train_loss: 0.0025992217
test_loss: 0.003533864
train_loss: 0.0025204343
test_loss: 0.0032185288
train_loss: 0.0027215588
test_loss: 0.0032751097
train_loss: 0.0025348132
test_loss: 0.0031375815
train_loss: 0.0028095096
test_loss: 0.0032115935
train_loss: 0.0025713623
test_loss: 0.0032993476
train_loss: 0.002621413
test_loss: 0.0031954846
train_loss: 0.0024814971
test_loss: 0.0030397547
train_loss: 0.0026260803
test_loss: 0.0031995226
train_loss: 0.0025979038
test_loss: 0.0031914676
train_loss: 0.0025971734
test_loss: 0.003248692
train_loss: 0.0030938676
test_loss: 0.0034732234
train_loss: 0.0026483994
test_loss: 0.0032684952
train_loss: 0.0027586455
test_loss: 0.0033304596
train_loss: 0.0028772866
test_loss: 0.003260725
train_loss: 0.0027161753
test_loss: 0.0031898678
train_loss: 0.002536872
test_loss: 0.0031462116
train_loss: 0.002630613
test_loss: 0.0031388162
train_loss: 0.0025586563
test_loss: 0.003173081
train_loss: 0.0028531898
test_loss: 0.0031996535
train_loss: 0.0027549951
test_loss: 0.0033838912
train_loss: 0.0025846928
test_loss: 0.003104193
train_loss: 0.0024986614
test_loss: 0.0033008936
train_loss: 0.002775758
test_loss: 0.0032895943
train_loss: 0.002772307
test_loss: 0.0033783403
train_loss: 0.002718167
test_loss: 0.0032204606
train_loss: 0.0027224112
test_loss: 0.0030071212
train_loss: 0.0024644127
test_loss: 0.0031549106
train_loss: 0.002532889
test_loss: 0.0030257497
train_loss: 0.0024496533
test_loss: 0.0031925093
train_loss: 0.0023963158
test_loss: 0.0029779105
train_loss: 0.0024233332
test_loss: 0.0030250254
train_loss: 0.002536714
test_loss: 0.003176634
train_loss: 0.0027666967
test_loss: 0.0032432538
train_loss: 0.0024197388
test_loss: 0.003253465
train_loss: 0.0029169878
test_loss: 0.0034578792
train_loss: 0.0027120921
test_loss: 0.0031953193
train_loss: 0.0025132964
test_loss: 0.0030944238
train_loss: 0.002676434
test_loss: 0.0030667614
train_loss: 0.0023834049
test_loss: 0.00330171
train_loss: 0.0024319836
test_loss: 0.003156773
train_loss: 0.0024584215
test_loss: 0.0030539038
train_loss: 0.0027637458
test_loss: 0.0031473078
train_loss: 0.002597585
test_loss: 0.0032146797
train_loss: 0.0026314098
test_loss: 0.0032535386
train_loss: 0.0028311112
test_loss: 0.0031532736
train_loss: 0.0027307014
test_loss: 0.003292303
train_loss: 0.00267726
test_loss: 0.003286467
train_loss: 0.0026124779
test_loss: 0.0033061751
train_loss: 0.0025667772
test_loss: 0.0031463292
train_loss: 0.0025587752
test_loss: 0.0031405352
train_loss: 0.0024749776
test_loss: 0.0030493222
train_loss: 0.0025178455
test_loss: 0.0031446076
train_loss: 0.0024154168
test_loss: 0.0031210517
train_loss: 0.002609696
test_loss: 0.003080377
train_loss: 0.0026693458
test_loss: 0.0031875062
train_loss: 0.0025300256
test_loss: 0.0031683906
train_loss: 0.0024807556
test_loss: 0.003110037
train_loss: 0.0024542338
test_loss: 0.0030096434
train_loss: 0.002450978
test_loss: 0.0029865035
train_loss: 0.0026240128
test_loss: 0.0030784276
train_loss: 0.0025440054
test_loss: 0.0032461074
train_loss: 0.002582653
test_loss: 0.0030958874
train_loss: 0.002752099
test_loss: 0.0031691738
train_loss: 0.00269428
test_loss: 0.0030995032
train_loss: 0.0025932903
test_loss: 0.003092225
train_loss: 0.002448188
test_loss: 0.00313418
train_loss: 0.0025063078
test_loss: 0.0029509466
train_loss: 0.0024693797
test_loss: 0.0029468816
train_loss: 0.00262089
test_loss: 0.003243929
train_loss: 0.002646273
test_loss: 0.003057043
train_loss: 0.0025041103
test_loss: 0.0030433333
train_loss: 0.0029375395
test_loss: 0.0035068288
train_loss: 0.0024414447
test_loss: 0.0029233298
train_loss: 0.002566438
test_loss: 0.0032200178
train_loss: 0.0025737358
test_loss: 0.0033183282
train_loss: 0.0024729944
test_loss: 0.0032712352
train_loss: 0.0027658953
test_loss: 0.003351998
train_loss: 0.0025987923
test_loss: 0.0031846971
train_loss: 0.0026269965
test_loss: 0.0032000847
train_loss: 0.0024411494/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0032126354
train_loss: 0.002560078
test_loss: 0.003026019
train_loss: 0.002644809
test_loss: 0.003232324
train_loss: 0.0023800246
test_loss: 0.0029141563
train_loss: 0.0025498027
test_loss: 0.0029841291
train_loss: 0.0026895276
test_loss: 0.0031211667
train_loss: 0.0027047307
test_loss: 0.0032030554
train_loss: 0.0024833623
test_loss: 0.0031813611
train_loss: 0.0025495898
test_loss: 0.0030673563
train_loss: 0.0027053724
test_loss: 0.0033673234
train_loss: 0.002633778
test_loss: 0.0030812847
train_loss: 0.0025631562
test_loss: 0.0031251484
train_loss: 0.0024574527
test_loss: 0.003230523
train_loss: 0.002821933
test_loss: 0.003120549
train_loss: 0.0025756587
test_loss: 0.0031189083
train_loss: 0.00249699
test_loss: 0.0031774729
train_loss: 0.0026065325
test_loss: 0.0030327952
train_loss: 0.0025449756
test_loss: 0.0031637473
train_loss: 0.0026330429
test_loss: 0.003210316
train_loss: 0.0025113681
test_loss: 0.0030163305
train_loss: 0.002590535
test_loss: 0.003071573
train_loss: 0.0024417685
test_loss: 0.0031393494
train_loss: 0.0023713158
test_loss: 0.0029934049
train_loss: 0.0025528274
test_loss: 0.0029964452
train_loss: 0.0026006075
test_loss: 0.0032278167
train_loss: 0.0024897803
test_loss: 0.0030757496
train_loss: 0.002413476
test_loss: 0.0031063438
train_loss: 0.0025376377
test_loss: 0.0030348764
train_loss: 0.0024660714
test_loss: 0.0030175198
train_loss: 0.002412226
test_loss: 0.0030282724
train_loss: 0.00232203
test_loss: 0.002991953
train_loss: 0.0024197064
test_loss: 0.0031470598
train_loss: 0.0025783249
test_loss: 0.0031458843
train_loss: 0.0024773595
test_loss: 0.0030919118
train_loss: 0.0024919591
test_loss: 0.003150029
train_loss: 0.0025356773
test_loss: 0.0031160566
train_loss: 0.0025923483
test_loss: 0.0031655896
train_loss: 0.0024821889
test_loss: 0.0031022725
train_loss: 0.002557021
test_loss: 0.0029873028
train_loss: 0.002798224
test_loss: 0.0032589764
train_loss: 0.0027342234
test_loss: 0.003073332
train_loss: 0.0024486268
test_loss: 0.0030973046
train_loss: 0.002436045
test_loss: 0.0030808318
train_loss: 0.0029918118
test_loss: 0.0031587463
train_loss: 0.0025038186
test_loss: 0.00307083
train_loss: 0.0024309482
test_loss: 0.002891278
train_loss: 0.0023055123
test_loss: 0.0028691937
train_loss: 0.0022931718
test_loss: 0.0029486648
train_loss: 0.0024666006
test_loss: 0.0031697948
train_loss: 0.002434828
test_loss: 0.0031721594
train_loss: 0.002684794
test_loss: 0.0031193423
train_loss: 0.0025489626
test_loss: 0.0031713264
train_loss: 0.0026219536
test_loss: 0.0031988425
train_loss: 0.0027065915
test_loss: 0.003382226
train_loss: 0.0026149082
test_loss: 0.0031611882
train_loss: 0.002571876
test_loss: 0.003026249
train_loss: 0.0025392056
test_loss: 0.0030589036
train_loss: 0.0024663433
test_loss: 0.0030268452
train_loss: 0.0025131316
test_loss: 0.0030573246
train_loss: 0.0024927997
test_loss: 0.0030460132
train_loss: 0.0024919226
test_loss: 0.0030590286
train_loss: 0.002466178
test_loss: 0.0032317145
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f301d82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f301d8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f302ad048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f301e0e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30215378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f3016ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30157840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300fd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300fdd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300fd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300c4048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30098620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300c46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30056840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300560d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10094488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10094158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10050400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10012378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10039f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e647cf950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e647cfc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e647a8158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64751268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64751ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646fe6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646cc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646ccd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646f6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646cce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e6464d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64672950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646727b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64635378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e645d32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64579268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.20475843e-05
Iter: 2 loss: 1.07282967e-05
Iter: 3 loss: 9.60075886e-06
Iter: 4 loss: 8.76355e-06
Iter: 5 loss: 8.44306851e-06
Iter: 6 loss: 7.98728706e-06
Iter: 7 loss: 7.4197319e-06
Iter: 8 loss: 9.58292458e-06
Iter: 9 loss: 7.28441273e-06
Iter: 10 loss: 6.83092367e-06
Iter: 11 loss: 1.06186853e-05
Iter: 12 loss: 6.80403264e-06
Iter: 13 loss: 6.48051855e-06
Iter: 14 loss: 6.70939426e-06
Iter: 15 loss: 6.27997815e-06
Iter: 16 loss: 5.95777192e-06
Iter: 17 loss: 6.47154593e-06
Iter: 18 loss: 5.80829874e-06
Iter: 19 loss: 5.47589116e-06
Iter: 20 loss: 8.82174572e-06
Iter: 21 loss: 5.46545198e-06
Iter: 22 loss: 5.24184543e-06
Iter: 23 loss: 4.84430711e-06
Iter: 24 loss: 1.47764376e-05
Iter: 25 loss: 4.84428438e-06
Iter: 26 loss: 4.74110402e-06
Iter: 27 loss: 4.66936945e-06
Iter: 28 loss: 4.50187054e-06
Iter: 29 loss: 4.385377e-06
Iter: 30 loss: 4.32525258e-06
Iter: 31 loss: 4.14454962e-06
Iter: 32 loss: 4.28057683e-06
Iter: 33 loss: 4.03414197e-06
Iter: 34 loss: 3.81890459e-06
Iter: 35 loss: 3.86415877e-06
Iter: 36 loss: 3.65972119e-06
Iter: 37 loss: 3.63894878e-06
Iter: 38 loss: 3.55120392e-06
Iter: 39 loss: 3.43443344e-06
Iter: 40 loss: 3.33964408e-06
Iter: 41 loss: 3.30505873e-06
Iter: 42 loss: 3.15753778e-06
Iter: 43 loss: 3.27017437e-06
Iter: 44 loss: 3.06776656e-06
Iter: 45 loss: 3.03495631e-06
Iter: 46 loss: 2.99806379e-06
Iter: 47 loss: 2.95365726e-06
Iter: 48 loss: 2.87242142e-06
Iter: 49 loss: 4.77785488e-06
Iter: 50 loss: 2.87232206e-06
Iter: 51 loss: 2.78791458e-06
Iter: 52 loss: 3.51804e-06
Iter: 53 loss: 2.78344305e-06
Iter: 54 loss: 2.72244324e-06
Iter: 55 loss: 2.91235574e-06
Iter: 56 loss: 2.70460851e-06
Iter: 57 loss: 2.63651282e-06
Iter: 58 loss: 2.62994104e-06
Iter: 59 loss: 2.57990655e-06
Iter: 60 loss: 2.51113829e-06
Iter: 61 loss: 2.82366591e-06
Iter: 62 loss: 2.49793084e-06
Iter: 63 loss: 2.4140179e-06
Iter: 64 loss: 2.75533148e-06
Iter: 65 loss: 2.39547717e-06
Iter: 66 loss: 2.35556877e-06
Iter: 67 loss: 2.31339232e-06
Iter: 68 loss: 2.30621163e-06
Iter: 69 loss: 2.24544442e-06
Iter: 70 loss: 2.40341865e-06
Iter: 71 loss: 2.22477183e-06
Iter: 72 loss: 2.2070526e-06
Iter: 73 loss: 2.19802564e-06
Iter: 74 loss: 2.16815056e-06
Iter: 75 loss: 2.11784618e-06
Iter: 76 loss: 2.1176902e-06
Iter: 77 loss: 2.07874973e-06
Iter: 78 loss: 2.15191017e-06
Iter: 79 loss: 2.06220489e-06
Iter: 80 loss: 2.02366618e-06
Iter: 81 loss: 2.6221287e-06
Iter: 82 loss: 2.02366255e-06
Iter: 83 loss: 1.99591295e-06
Iter: 84 loss: 1.97023201e-06
Iter: 85 loss: 1.96364158e-06
Iter: 86 loss: 1.93588767e-06
Iter: 87 loss: 2.24007727e-06
Iter: 88 loss: 1.93526239e-06
Iter: 89 loss: 1.91058234e-06
Iter: 90 loss: 1.93829646e-06
Iter: 91 loss: 1.89719231e-06
Iter: 92 loss: 1.87156309e-06
Iter: 93 loss: 1.92643438e-06
Iter: 94 loss: 1.86156149e-06
Iter: 95 loss: 1.83688212e-06
Iter: 96 loss: 1.9255026e-06
Iter: 97 loss: 1.83061388e-06
Iter: 98 loss: 1.80538109e-06
Iter: 99 loss: 1.96951805e-06
Iter: 100 loss: 1.80270717e-06
Iter: 101 loss: 1.78597838e-06
Iter: 102 loss: 1.75176069e-06
Iter: 103 loss: 2.35989933e-06
Iter: 104 loss: 1.75104299e-06
Iter: 105 loss: 1.71490774e-06
Iter: 106 loss: 1.82133545e-06
Iter: 107 loss: 1.70372925e-06
Iter: 108 loss: 1.69595592e-06
Iter: 109 loss: 1.68801103e-06
Iter: 110 loss: 1.67285816e-06
Iter: 111 loss: 1.66232689e-06
Iter: 112 loss: 1.65688994e-06
Iter: 113 loss: 1.64021253e-06
Iter: 114 loss: 1.6363706e-06
Iter: 115 loss: 1.62562787e-06
Iter: 116 loss: 1.61658863e-06
Iter: 117 loss: 1.61360481e-06
Iter: 118 loss: 1.60305103e-06
Iter: 119 loss: 1.58066302e-06
Iter: 120 loss: 1.94111522e-06
Iter: 121 loss: 1.57991894e-06
Iter: 122 loss: 1.56510225e-06
Iter: 123 loss: 1.753883e-06
Iter: 124 loss: 1.56503688e-06
Iter: 125 loss: 1.54793599e-06
Iter: 126 loss: 1.53968915e-06
Iter: 127 loss: 1.53146607e-06
Iter: 128 loss: 1.51436882e-06
Iter: 129 loss: 1.63492257e-06
Iter: 130 loss: 1.5128785e-06
Iter: 131 loss: 1.50066421e-06
Iter: 132 loss: 1.55946077e-06
Iter: 133 loss: 1.49848347e-06
Iter: 134 loss: 1.48522565e-06
Iter: 135 loss: 1.50683718e-06
Iter: 136 loss: 1.47913545e-06
Iter: 137 loss: 1.46716707e-06
Iter: 138 loss: 1.4712864e-06
Iter: 139 loss: 1.45866852e-06
Iter: 140 loss: 1.44668047e-06
Iter: 141 loss: 1.43987006e-06
Iter: 142 loss: 1.43469867e-06
Iter: 143 loss: 1.43766715e-06
Iter: 144 loss: 1.42668432e-06
Iter: 145 loss: 1.42073657e-06
Iter: 146 loss: 1.40547809e-06
Iter: 147 loss: 1.53088683e-06
Iter: 148 loss: 1.40277632e-06
Iter: 149 loss: 1.38673431e-06
Iter: 150 loss: 1.4671441e-06
Iter: 151 loss: 1.38406631e-06
Iter: 152 loss: 1.37535835e-06
Iter: 153 loss: 1.37499774e-06
Iter: 154 loss: 1.36811445e-06
Iter: 155 loss: 1.35532036e-06
Iter: 156 loss: 1.63853383e-06
Iter: 157 loss: 1.3552816e-06
Iter: 158 loss: 1.34745403e-06
Iter: 159 loss: 1.44916498e-06
Iter: 160 loss: 1.34741845e-06
Iter: 161 loss: 1.33922572e-06
Iter: 162 loss: 1.3501882e-06
Iter: 163 loss: 1.33510173e-06
Iter: 164 loss: 1.32906678e-06
Iter: 165 loss: 1.32855871e-06
Iter: 166 loss: 1.32411787e-06
Iter: 167 loss: 1.31238835e-06
Iter: 168 loss: 1.36544054e-06
Iter: 169 loss: 1.31012712e-06
Iter: 170 loss: 1.2997973e-06
Iter: 171 loss: 1.32306889e-06
Iter: 172 loss: 1.2958867e-06
Iter: 173 loss: 1.28872193e-06
Iter: 174 loss: 1.29362309e-06
Iter: 175 loss: 1.28430418e-06
Iter: 176 loss: 1.27556814e-06
Iter: 177 loss: 1.28879969e-06
Iter: 178 loss: 1.27144017e-06
Iter: 179 loss: 1.27102862e-06
Iter: 180 loss: 1.26737245e-06
Iter: 181 loss: 1.26496093e-06
Iter: 182 loss: 1.25816371e-06
Iter: 183 loss: 1.2896171e-06
Iter: 184 loss: 1.25569e-06
Iter: 185 loss: 1.2480632e-06
Iter: 186 loss: 1.28360261e-06
Iter: 187 loss: 1.24657504e-06
Iter: 188 loss: 1.2391564e-06
Iter: 189 loss: 1.33258254e-06
Iter: 190 loss: 1.23910354e-06
Iter: 191 loss: 1.23403618e-06
Iter: 192 loss: 1.22424376e-06
Iter: 193 loss: 1.43131433e-06
Iter: 194 loss: 1.22418305e-06
Iter: 195 loss: 1.21736889e-06
Iter: 196 loss: 1.21738663e-06
Iter: 197 loss: 1.2118885e-06
Iter: 198 loss: 1.2372343e-06
Iter: 199 loss: 1.21080677e-06
Iter: 200 loss: 1.20751577e-06
Iter: 201 loss: 1.20309801e-06
Iter: 202 loss: 1.20285802e-06
Iter: 203 loss: 1.19759238e-06
Iter: 204 loss: 1.19753395e-06
Iter: 205 loss: 1.19457241e-06
Iter: 206 loss: 1.19063975e-06
Iter: 207 loss: 1.19039487e-06
Iter: 208 loss: 1.18433707e-06
Iter: 209 loss: 1.19288939e-06
Iter: 210 loss: 1.18139167e-06
Iter: 211 loss: 1.17410252e-06
Iter: 212 loss: 1.19245806e-06
Iter: 213 loss: 1.17158902e-06
Iter: 214 loss: 1.16682452e-06
Iter: 215 loss: 1.16640706e-06
Iter: 216 loss: 1.16383706e-06
Iter: 217 loss: 1.15817068e-06
Iter: 218 loss: 1.24125063e-06
Iter: 219 loss: 1.15792045e-06
Iter: 220 loss: 1.153021e-06
Iter: 221 loss: 1.18459025e-06
Iter: 222 loss: 1.15247281e-06
Iter: 223 loss: 1.14899831e-06
Iter: 224 loss: 1.14901673e-06
Iter: 225 loss: 1.14680677e-06
Iter: 226 loss: 1.14119121e-06
Iter: 227 loss: 1.18980392e-06
Iter: 228 loss: 1.14029308e-06
Iter: 229 loss: 1.13488841e-06
Iter: 230 loss: 1.20012896e-06
Iter: 231 loss: 1.13481724e-06
Iter: 232 loss: 1.12963914e-06
Iter: 233 loss: 1.14993168e-06
Iter: 234 loss: 1.12850535e-06
Iter: 235 loss: 1.12510497e-06
Iter: 236 loss: 1.12259909e-06
Iter: 237 loss: 1.12143118e-06
Iter: 238 loss: 1.11742122e-06
Iter: 239 loss: 1.11732288e-06
Iter: 240 loss: 1.11504505e-06
Iter: 241 loss: 1.11217935e-06
Iter: 242 loss: 1.11196937e-06
Iter: 243 loss: 1.10903034e-06
Iter: 244 loss: 1.11743475e-06
Iter: 245 loss: 1.10817564e-06
Iter: 246 loss: 1.10476765e-06
Iter: 247 loss: 1.11935583e-06
Iter: 248 loss: 1.10402993e-06
Iter: 249 loss: 1.10120106e-06
Iter: 250 loss: 1.13610395e-06
Iter: 251 loss: 1.10118481e-06
Iter: 252 loss: 1.09937264e-06
Iter: 253 loss: 1.09437758e-06
Iter: 254 loss: 1.12036093e-06
Iter: 255 loss: 1.09275402e-06
Iter: 256 loss: 1.08749214e-06
Iter: 257 loss: 1.15216324e-06
Iter: 258 loss: 1.08741472e-06
Iter: 259 loss: 1.08416725e-06
Iter: 260 loss: 1.13504871e-06
Iter: 261 loss: 1.0841718e-06
Iter: 262 loss: 1.08199515e-06
Iter: 263 loss: 1.07757057e-06
Iter: 264 loss: 1.15942134e-06
Iter: 265 loss: 1.07748133e-06
Iter: 266 loss: 1.07489404e-06
Iter: 267 loss: 1.10307724e-06
Iter: 268 loss: 1.07484777e-06
Iter: 269 loss: 1.07211781e-06
Iter: 270 loss: 1.08216e-06
Iter: 271 loss: 1.07144217e-06
Iter: 272 loss: 1.06949744e-06
Iter: 273 loss: 1.0674446e-06
Iter: 274 loss: 1.06706091e-06
Iter: 275 loss: 1.0640648e-06
Iter: 276 loss: 1.06407674e-06
Iter: 277 loss: 1.06195876e-06
Iter: 278 loss: 1.05840888e-06
Iter: 279 loss: 1.058409e-06
Iter: 280 loss: 1.05409549e-06
Iter: 281 loss: 1.05436379e-06
Iter: 282 loss: 1.05064498e-06
Iter: 283 loss: 1.05076776e-06
Iter: 284 loss: 1.04845958e-06
Iter: 285 loss: 1.04695312e-06
Iter: 286 loss: 1.05032268e-06
Iter: 287 loss: 1.04634546e-06
Iter: 288 loss: 1.04475464e-06
Iter: 289 loss: 1.04200194e-06
Iter: 290 loss: 1.04201672e-06
Iter: 291 loss: 1.03918035e-06
Iter: 292 loss: 1.05110621e-06
Iter: 293 loss: 1.03858952e-06
Iter: 294 loss: 1.036409e-06
Iter: 295 loss: 1.03641401e-06
Iter: 296 loss: 1.03492653e-06
Iter: 297 loss: 1.03164211e-06
Iter: 298 loss: 1.08116103e-06
Iter: 299 loss: 1.03153377e-06
Iter: 300 loss: 1.02828812e-06
Iter: 301 loss: 1.0423056e-06
Iter: 302 loss: 1.02760168e-06
Iter: 303 loss: 1.02464878e-06
Iter: 304 loss: 1.06251821e-06
Iter: 305 loss: 1.024624e-06
Iter: 306 loss: 1.02299077e-06
Iter: 307 loss: 1.02118599e-06
Iter: 308 loss: 1.0209335e-06
Iter: 309 loss: 1.01933176e-06
Iter: 310 loss: 1.01923308e-06
Iter: 311 loss: 1.01785849e-06
Iter: 312 loss: 1.01501041e-06
Iter: 313 loss: 1.06120854e-06
Iter: 314 loss: 1.01494095e-06
Iter: 315 loss: 1.01266767e-06
Iter: 316 loss: 1.02043202e-06
Iter: 317 loss: 1.01208389e-06
Iter: 318 loss: 1.01018236e-06
Iter: 319 loss: 1.03604202e-06
Iter: 320 loss: 1.01018134e-06
Iter: 321 loss: 1.00811917e-06
Iter: 322 loss: 1.00815328e-06
Iter: 323 loss: 1.00646434e-06
Iter: 324 loss: 1.00431521e-06
Iter: 325 loss: 1.00688862e-06
Iter: 326 loss: 1.00315833e-06
Iter: 327 loss: 1.00069826e-06
Iter: 328 loss: 1.0058211e-06
Iter: 329 loss: 9.99724193e-07
Iter: 330 loss: 9.98282644e-07
Iter: 331 loss: 9.98156338e-07
Iter: 332 loss: 9.97059374e-07
Iter: 333 loss: 9.94639322e-07
Iter: 334 loss: 1.02983267e-06
Iter: 335 loss: 9.94525e-07
Iter: 336 loss: 9.9239935e-07
Iter: 337 loss: 1.0041515e-06
Iter: 338 loss: 9.92098e-07
Iter: 339 loss: 9.90394255e-07
Iter: 340 loss: 1.01170406e-06
Iter: 341 loss: 9.9039562e-07
Iter: 342 loss: 9.88985903e-07
Iter: 343 loss: 9.86529585e-07
Iter: 344 loss: 9.86527311e-07
Iter: 345 loss: 9.84743224e-07
Iter: 346 loss: 9.84674671e-07
Iter: 347 loss: 9.82872621e-07
Iter: 348 loss: 9.79462698e-07
Iter: 349 loss: 1.05569461e-06
Iter: 350 loss: 9.79477818e-07
Iter: 351 loss: 9.77143e-07
Iter: 352 loss: 9.84179451e-07
Iter: 353 loss: 9.76425554e-07
Iter: 354 loss: 9.75107e-07
Iter: 355 loss: 9.74920795e-07
Iter: 356 loss: 9.73625674e-07
Iter: 357 loss: 9.72241423e-07
Iter: 358 loss: 9.72034741e-07
Iter: 359 loss: 9.70503379e-07
Iter: 360 loss: 9.72481757e-07
Iter: 361 loss: 9.69742587e-07
Iter: 362 loss: 9.67872438e-07
Iter: 363 loss: 9.76287e-07
Iter: 364 loss: 9.67468509e-07
Iter: 365 loss: 9.66048333e-07
Iter: 366 loss: 9.83036216e-07
Iter: 367 loss: 9.66036851e-07
Iter: 368 loss: 9.65064146e-07
Iter: 369 loss: 9.62922172e-07
Iter: 370 loss: 9.96670678e-07
Iter: 371 loss: 9.62870331e-07
Iter: 372 loss: 9.60935381e-07
Iter: 373 loss: 9.73162e-07
Iter: 374 loss: 9.60731541e-07
Iter: 375 loss: 9.59026806e-07
Iter: 376 loss: 9.76691581e-07
Iter: 377 loss: 9.58983492e-07
Iter: 378 loss: 9.58104238e-07
Iter: 379 loss: 9.57691555e-07
Iter: 380 loss: 9.57237262e-07
Iter: 381 loss: 9.56138138e-07
Iter: 382 loss: 9.69541702e-07
Iter: 383 loss: 9.56112444e-07
Iter: 384 loss: 9.5504015e-07
Iter: 385 loss: 9.53076494e-07
Iter: 386 loss: 9.98296e-07
Iter: 387 loss: 9.5307314e-07
Iter: 388 loss: 9.51441962e-07
Iter: 389 loss: 9.54883717e-07
Iter: 390 loss: 9.50838626e-07
Iter: 391 loss: 9.4944761e-07
Iter: 392 loss: 9.49391733e-07
Iter: 393 loss: 9.48482352e-07
Iter: 394 loss: 9.46378236e-07
Iter: 395 loss: 9.72584075e-07
Iter: 396 loss: 9.46225e-07
Iter: 397 loss: 9.44400767e-07
Iter: 398 loss: 9.58626742e-07
Iter: 399 loss: 9.44276053e-07
Iter: 400 loss: 9.42875886e-07
Iter: 401 loss: 9.5145225e-07
Iter: 402 loss: 9.42692964e-07
Iter: 403 loss: 9.41530857e-07
Iter: 404 loss: 9.45919851e-07
Iter: 405 loss: 9.41229644e-07
Iter: 406 loss: 9.40205382e-07
Iter: 407 loss: 9.3888076e-07
Iter: 408 loss: 9.3883034e-07
Iter: 409 loss: 9.36788922e-07
Iter: 410 loss: 9.40335e-07
Iter: 411 loss: 9.35840831e-07
Iter: 412 loss: 9.34671903e-07
Iter: 413 loss: 9.34489947e-07
Iter: 414 loss: 9.33620413e-07
Iter: 415 loss: 9.31887143e-07
Iter: 416 loss: 9.64431592e-07
Iter: 417 loss: 9.31864065e-07
Iter: 418 loss: 9.30779834e-07
Iter: 419 loss: 9.30615101e-07
Iter: 420 loss: 9.29722262e-07
Iter: 421 loss: 9.28192321e-07
Iter: 422 loss: 9.28185614e-07
Iter: 423 loss: 9.27033113e-07
Iter: 424 loss: 9.3496908e-07
Iter: 425 loss: 9.26901691e-07
Iter: 426 loss: 9.25723612e-07
Iter: 427 loss: 9.34999889e-07
Iter: 428 loss: 9.25642837e-07
Iter: 429 loss: 9.24928713e-07
Iter: 430 loss: 9.23368589e-07
Iter: 431 loss: 9.47222532e-07
Iter: 432 loss: 9.23295602e-07
Iter: 433 loss: 9.21843309e-07
Iter: 434 loss: 9.29808095e-07
Iter: 435 loss: 9.21674427e-07
Iter: 436 loss: 9.20089576e-07
Iter: 437 loss: 9.28043903e-07
Iter: 438 loss: 9.19841398e-07
Iter: 439 loss: 9.18676733e-07
Iter: 440 loss: 9.21139531e-07
Iter: 441 loss: 9.18234264e-07
Iter: 442 loss: 9.17117632e-07
Iter: 443 loss: 9.16168e-07
Iter: 444 loss: 9.15793635e-07
Iter: 445 loss: 9.14305588e-07
Iter: 446 loss: 9.27926578e-07
Iter: 447 loss: 9.1427404e-07
Iter: 448 loss: 9.13285589e-07
Iter: 449 loss: 9.24435085e-07
Iter: 450 loss: 9.13268e-07
Iter: 451 loss: 9.1255805e-07
Iter: 452 loss: 9.10904362e-07
Iter: 453 loss: 9.32707394e-07
Iter: 454 loss: 9.10812503e-07
Iter: 455 loss: 9.09750838e-07
Iter: 456 loss: 9.0956928e-07
Iter: 457 loss: 9.08849131e-07
Iter: 458 loss: 9.07273659e-07
Iter: 459 loss: 9.33617e-07
Iter: 460 loss: 9.07263029e-07
Iter: 461 loss: 9.06135767e-07
Iter: 462 loss: 9.06131334e-07
Iter: 463 loss: 9.04861622e-07
Iter: 464 loss: 9.06246e-07
Iter: 465 loss: 9.04200363e-07
Iter: 466 loss: 9.03314799e-07
Iter: 467 loss: 9.02596867e-07
Iter: 468 loss: 9.02321744e-07
Iter: 469 loss: 9.01238081e-07
Iter: 470 loss: 9.06204605e-07
Iter: 471 loss: 9.01024634e-07
Iter: 472 loss: 8.99725364e-07
Iter: 473 loss: 9.08971799e-07
Iter: 474 loss: 8.99615941e-07
Iter: 475 loss: 8.98804387e-07
Iter: 476 loss: 8.98108397e-07
Iter: 477 loss: 8.97889095e-07
Iter: 478 loss: 8.96440781e-07
Iter: 479 loss: 8.98965141e-07
Iter: 480 loss: 8.95753033e-07
Iter: 481 loss: 8.94562163e-07
Iter: 482 loss: 9.02938041e-07
Iter: 483 loss: 8.94429036e-07
Iter: 484 loss: 8.93242941e-07
Iter: 485 loss: 8.98788244e-07
Iter: 486 loss: 8.93023298e-07
Iter: 487 loss: 8.92223113e-07
Iter: 488 loss: 8.92562923e-07
Iter: 489 loss: 8.91691286e-07
Iter: 490 loss: 8.90903e-07
Iter: 491 loss: 9.02205215e-07
Iter: 492 loss: 8.90884621e-07
Iter: 493 loss: 8.90337105e-07
Iter: 494 loss: 8.89031696e-07
Iter: 495 loss: 9.07025878e-07
Iter: 496 loss: 8.88950694e-07
Iter: 497 loss: 8.88086504e-07
Iter: 498 loss: 8.88044042e-07
Iter: 499 loss: 8.87113515e-07
Iter: 500 loss: 8.88767204e-07
Iter: 501 loss: 8.86705834e-07
Iter: 502 loss: 8.85926966e-07
Iter: 503 loss: 8.83931421e-07
Iter: 504 loss: 8.96882625e-07
Iter: 505 loss: 8.83434723e-07
Iter: 506 loss: 8.82917e-07
Iter: 507 loss: 8.82254824e-07
Iter: 508 loss: 8.81350331e-07
Iter: 509 loss: 8.85073518e-07
Iter: 510 loss: 8.81151664e-07
Iter: 511 loss: 8.80575612e-07
Iter: 512 loss: 8.79671347e-07
Iter: 513 loss: 8.79650543e-07
Iter: 514 loss: 8.78607295e-07
Iter: 515 loss: 8.85073518e-07
Iter: 516 loss: 8.78478772e-07
Iter: 517 loss: 8.77481739e-07
Iter: 518 loss: 8.80662242e-07
Iter: 519 loss: 8.77203547e-07
Iter: 520 loss: 8.76102035e-07
Iter: 521 loss: 8.8191166e-07
Iter: 522 loss: 8.7592025e-07
Iter: 523 loss: 8.75141836e-07
Iter: 524 loss: 8.74489217e-07
Iter: 525 loss: 8.74277305e-07
Iter: 526 loss: 8.72937562e-07
Iter: 527 loss: 8.86430371e-07
Iter: 528 loss: 8.72884812e-07
Iter: 529 loss: 8.72220085e-07
Iter: 530 loss: 8.70939118e-07
Iter: 531 loss: 8.98156259e-07
Iter: 532 loss: 8.70945e-07
Iter: 533 loss: 8.70626309e-07
Iter: 534 loss: 8.70313215e-07
Iter: 535 loss: 8.69672249e-07
Iter: 536 loss: 8.69330904e-07
Iter: 537 loss: 8.69121266e-07
Iter: 538 loss: 8.68396569e-07
Iter: 539 loss: 8.67656581e-07
Iter: 540 loss: 8.67529138e-07
Iter: 541 loss: 8.66751066e-07
Iter: 542 loss: 8.66729863e-07
Iter: 543 loss: 8.66200594e-07
Iter: 544 loss: 8.65175593e-07
Iter: 545 loss: 8.86459418e-07
Iter: 546 loss: 8.65172296e-07
Iter: 547 loss: 8.63836362e-07
Iter: 548 loss: 8.65780407e-07
Iter: 549 loss: 8.63198e-07
Iter: 550 loss: 8.62027548e-07
Iter: 551 loss: 8.65568438e-07
Iter: 552 loss: 8.61632486e-07
Iter: 553 loss: 8.60752607e-07
Iter: 554 loss: 8.73235251e-07
Iter: 555 loss: 8.6075022e-07
Iter: 556 loss: 8.59844704e-07
Iter: 557 loss: 8.61714454e-07
Iter: 558 loss: 8.59445038e-07
Iter: 559 loss: 8.58873591e-07
Iter: 560 loss: 8.60174623e-07
Iter: 561 loss: 8.58654346e-07
Iter: 562 loss: 8.57818634e-07
Iter: 563 loss: 8.59308784e-07
Iter: 564 loss: 8.57440114e-07
Iter: 565 loss: 8.56547103e-07
Iter: 566 loss: 8.56162956e-07
Iter: 567 loss: 8.55708322e-07
Iter: 568 loss: 8.54969471e-07
Iter: 569 loss: 8.54952248e-07
Iter: 570 loss: 8.54389043e-07
Iter: 571 loss: 8.56728093e-07
Iter: 572 loss: 8.54253415e-07
Iter: 573 loss: 8.53773713e-07
Iter: 574 loss: 8.53023323e-07
Iter: 575 loss: 8.53001e-07
Iter: 576 loss: 8.52147082e-07
Iter: 577 loss: 8.53725339e-07
Iter: 578 loss: 8.51801133e-07
Iter: 579 loss: 8.51137088e-07
Iter: 580 loss: 8.51098946e-07
Iter: 581 loss: 8.5058889e-07
Iter: 582 loss: 8.49456512e-07
Iter: 583 loss: 8.62537149e-07
Iter: 584 loss: 8.49349931e-07
Iter: 585 loss: 8.48234e-07
Iter: 586 loss: 8.52005542e-07
Iter: 587 loss: 8.47896672e-07
Iter: 588 loss: 8.46767193e-07
Iter: 589 loss: 8.5059412e-07
Iter: 590 loss: 8.4649696e-07
Iter: 591 loss: 8.45991053e-07
Iter: 592 loss: 8.45890952e-07
Iter: 593 loss: 8.45397608e-07
Iter: 594 loss: 8.44536032e-07
Iter: 595 loss: 8.44548538e-07
Iter: 596 loss: 8.43997611e-07
Iter: 597 loss: 8.43989255e-07
Iter: 598 loss: 8.43429063e-07
Iter: 599 loss: 8.42460736e-07
Iter: 600 loss: 8.42476425e-07
Iter: 601 loss: 8.41549877e-07
Iter: 602 loss: 8.45297564e-07
Iter: 603 loss: 8.41367864e-07
Iter: 604 loss: 8.40289886e-07
Iter: 605 loss: 8.47998422e-07
Iter: 606 loss: 8.40223493e-07
Iter: 607 loss: 8.39517497e-07
Iter: 608 loss: 8.38600158e-07
Iter: 609 loss: 8.38527626e-07
Iter: 610 loss: 8.37586754e-07
Iter: 611 loss: 8.44689851e-07
Iter: 612 loss: 8.37515756e-07
Iter: 613 loss: 8.36798563e-07
Iter: 614 loss: 8.40188534e-07
Iter: 615 loss: 8.36690901e-07
Iter: 616 loss: 8.35836886e-07
Iter: 617 loss: 8.36806862e-07
Iter: 618 loss: 8.35384071e-07
Iter: 619 loss: 8.34814841e-07
Iter: 620 loss: 8.33676381e-07
Iter: 621 loss: 8.55742087e-07
Iter: 622 loss: 8.33667286e-07
Iter: 623 loss: 8.32488809e-07
Iter: 624 loss: 8.39207246e-07
Iter: 625 loss: 8.32328055e-07
Iter: 626 loss: 8.31689647e-07
Iter: 627 loss: 8.31618195e-07
Iter: 628 loss: 8.31007185e-07
Iter: 629 loss: 8.30376166e-07
Iter: 630 loss: 8.30304657e-07
Iter: 631 loss: 8.29509645e-07
Iter: 632 loss: 8.34257833e-07
Iter: 633 loss: 8.2938061e-07
Iter: 634 loss: 8.28540919e-07
Iter: 635 loss: 8.30037436e-07
Iter: 636 loss: 8.28162456e-07
Iter: 637 loss: 8.27589929e-07
Iter: 638 loss: 8.27927352e-07
Iter: 639 loss: 8.27223346e-07
Iter: 640 loss: 8.26513e-07
Iter: 641 loss: 8.26507289e-07
Iter: 642 loss: 8.2614838e-07
Iter: 643 loss: 8.25328414e-07
Iter: 644 loss: 8.35152036e-07
Iter: 645 loss: 8.2526833e-07
Iter: 646 loss: 8.24248787e-07
Iter: 647 loss: 8.28283248e-07
Iter: 648 loss: 8.24051938e-07
Iter: 649 loss: 8.23384767e-07
Iter: 650 loss: 8.23383232e-07
Iter: 651 loss: 8.22875279e-07
Iter: 652 loss: 8.228958e-07
Iter: 653 loss: 8.22484935e-07
Iter: 654 loss: 8.21790593e-07
Iter: 655 loss: 8.21685376e-07
Iter: 656 loss: 8.21185267e-07
Iter: 657 loss: 8.20417483e-07
Iter: 658 loss: 8.22158199e-07
Iter: 659 loss: 8.20100126e-07
Iter: 660 loss: 8.19356273e-07
Iter: 661 loss: 8.21448396e-07
Iter: 662 loss: 8.19138165e-07
Iter: 663 loss: 8.18442743e-07
Iter: 664 loss: 8.18427225e-07
Iter: 665 loss: 8.18000558e-07
Iter: 666 loss: 8.17034106e-07
Iter: 667 loss: 8.29179385e-07
Iter: 668 loss: 8.16960664e-07
Iter: 669 loss: 8.16267743e-07
Iter: 670 loss: 8.16224428e-07
Iter: 671 loss: 8.15675094e-07
Iter: 672 loss: 8.15259398e-07
Iter: 673 loss: 8.15058115e-07
Iter: 674 loss: 8.14615419e-07
Iter: 675 loss: 8.14596945e-07
Iter: 676 loss: 8.14174427e-07
Iter: 677 loss: 8.13272038e-07
Iter: 678 loss: 8.25364054e-07
Iter: 679 loss: 8.13192457e-07
Iter: 680 loss: 8.12569965e-07
Iter: 681 loss: 8.19832394e-07
Iter: 682 loss: 8.12542623e-07
Iter: 683 loss: 8.11963787e-07
Iter: 684 loss: 8.17965656e-07
Iter: 685 loss: 8.11958557e-07
Iter: 686 loss: 8.11520067e-07
Iter: 687 loss: 8.10924121e-07
Iter: 688 loss: 8.10893312e-07
Iter: 689 loss: 8.10035772e-07
Iter: 690 loss: 8.10240408e-07
Iter: 691 loss: 8.09423625e-07
Iter: 692 loss: 8.0837026e-07
Iter: 693 loss: 8.12490043e-07
Iter: 694 loss: 8.08134246e-07
Iter: 695 loss: 8.0732724e-07
Iter: 696 loss: 8.14849727e-07
Iter: 697 loss: 8.07252e-07
Iter: 698 loss: 8.06422861e-07
Iter: 699 loss: 8.10100914e-07
Iter: 700 loss: 8.06249432e-07
Iter: 701 loss: 8.05854029e-07
Iter: 702 loss: 8.0537643e-07
Iter: 703 loss: 8.05324817e-07
Iter: 704 loss: 8.04598358e-07
Iter: 705 loss: 8.13325869e-07
Iter: 706 loss: 8.04587501e-07
Iter: 707 loss: 8.04177e-07
Iter: 708 loss: 8.04485808e-07
Iter: 709 loss: 8.03935166e-07
Iter: 710 loss: 8.03355192e-07
Iter: 711 loss: 8.06749028e-07
Iter: 712 loss: 8.03292835e-07
Iter: 713 loss: 8.02827e-07
Iter: 714 loss: 8.01715828e-07
Iter: 715 loss: 8.14698467e-07
Iter: 716 loss: 8.01623344e-07
Iter: 717 loss: 8.01170813e-07
Iter: 718 loss: 8.01011652e-07
Iter: 719 loss: 8.00429916e-07
Iter: 720 loss: 7.9998108e-07
Iter: 721 loss: 7.99803274e-07
Iter: 722 loss: 7.99060558e-07
Iter: 723 loss: 8.00663713e-07
Iter: 724 loss: 7.98796e-07
Iter: 725 loss: 7.98046187e-07
Iter: 726 loss: 7.99547934e-07
Iter: 727 loss: 7.97765324e-07
Iter: 728 loss: 7.97001064e-07
Iter: 729 loss: 7.9725794e-07
Iter: 730 loss: 7.96481e-07
Iter: 731 loss: 7.96095378e-07
Iter: 732 loss: 7.95972085e-07
Iter: 733 loss: 7.95521657e-07
Iter: 734 loss: 7.94826406e-07
Iter: 735 loss: 7.94827372e-07
Iter: 736 loss: 7.94162361e-07
Iter: 737 loss: 7.98191422e-07
Iter: 738 loss: 7.94069706e-07
Iter: 739 loss: 7.93349727e-07
Iter: 740 loss: 7.95502103e-07
Iter: 741 loss: 7.93126787e-07
Iter: 742 loss: 7.92737069e-07
Iter: 743 loss: 7.96075e-07
Iter: 744 loss: 7.92708533e-07
Iter: 745 loss: 7.92315745e-07
Iter: 746 loss: 7.92277547e-07
Iter: 747 loss: 7.9199981e-07
Iter: 748 loss: 7.91533807e-07
Iter: 749 loss: 7.90948775e-07
Iter: 750 loss: 7.90883291e-07
Iter: 751 loss: 7.90144441e-07
Iter: 752 loss: 7.90136482e-07
Iter: 753 loss: 7.89758076e-07
Iter: 754 loss: 7.89070214e-07
Iter: 755 loss: 7.89073681e-07
Iter: 756 loss: 7.88218586e-07
Iter: 757 loss: 7.89541218e-07
Iter: 758 loss: 7.87832448e-07
Iter: 759 loss: 7.86881515e-07
Iter: 760 loss: 7.9203312e-07
Iter: 761 loss: 7.86749752e-07
Iter: 762 loss: 7.86121745e-07
Iter: 763 loss: 7.87361955e-07
Iter: 764 loss: 7.85843895e-07
Iter: 765 loss: 7.85435816e-07
Iter: 766 loss: 7.85394207e-07
Iter: 767 loss: 7.85099246e-07
Iter: 768 loss: 7.84389044e-07
Iter: 769 loss: 7.93985805e-07
Iter: 770 loss: 7.84310373e-07
Iter: 771 loss: 7.83687e-07
Iter: 772 loss: 7.92321259e-07
Iter: 773 loss: 7.83680207e-07
Iter: 774 loss: 7.83063797e-07
Iter: 775 loss: 7.83704195e-07
Iter: 776 loss: 7.82719894e-07
Iter: 777 loss: 7.82212226e-07
Iter: 778 loss: 7.8576e-07
Iter: 779 loss: 7.82158963e-07
Iter: 780 loss: 7.81534084e-07
Iter: 781 loss: 7.81158064e-07
Iter: 782 loss: 7.80920857e-07
Iter: 783 loss: 7.8036112e-07
Iter: 784 loss: 7.80531366e-07
Iter: 785 loss: 7.7998925e-07
Iter: 786 loss: 7.79565653e-07
Iter: 787 loss: 7.79473794e-07
Iter: 788 loss: 7.79264496e-07
Iter: 789 loss: 7.7867918e-07
Iter: 790 loss: 7.84992e-07
Iter: 791 loss: 7.78621313e-07
Iter: 792 loss: 7.77943569e-07
Iter: 793 loss: 7.78918775e-07
Iter: 794 loss: 7.77616151e-07
Iter: 795 loss: 7.76870309e-07
Iter: 796 loss: 7.81926e-07
Iter: 797 loss: 7.76811703e-07
Iter: 798 loss: 7.76072852e-07
Iter: 799 loss: 7.78379444e-07
Iter: 800 loss: 7.75872195e-07
Iter: 801 loss: 7.75144e-07
Iter: 802 loss: 7.80307232e-07
Iter: 803 loss: 7.75088665e-07
Iter: 804 loss: 7.7464847e-07
Iter: 805 loss: 7.74045e-07
Iter: 806 loss: 7.74019099e-07
Iter: 807 loss: 7.73476074e-07
Iter: 808 loss: 7.73475961e-07
Iter: 809 loss: 7.72928388e-07
Iter: 810 loss: 7.72864041e-07
Iter: 811 loss: 7.72482736e-07
Iter: 812 loss: 7.72105636e-07
Iter: 813 loss: 7.72103249e-07
Iter: 814 loss: 7.71850409e-07
Iter: 815 loss: 7.71184318e-07
Iter: 816 loss: 7.76198306e-07
Iter: 817 loss: 7.71066e-07
Iter: 818 loss: 7.70295458e-07
Iter: 819 loss: 7.72281226e-07
Iter: 820 loss: 7.70042504e-07
Iter: 821 loss: 7.69490498e-07
Iter: 822 loss: 7.69467647e-07
Iter: 823 loss: 7.6904746e-07
Iter: 824 loss: 7.68531493e-07
Iter: 825 loss: 7.68484938e-07
Iter: 826 loss: 7.67812e-07
Iter: 827 loss: 7.67576807e-07
Iter: 828 loss: 7.6721426e-07
Iter: 829 loss: 7.66559083e-07
Iter: 830 loss: 7.75023523e-07
Iter: 831 loss: 7.66534299e-07
Iter: 832 loss: 7.6602953e-07
Iter: 833 loss: 7.70227246e-07
Iter: 834 loss: 7.65986215e-07
Iter: 835 loss: 7.65551704e-07
Iter: 836 loss: 7.66756045e-07
Iter: 837 loss: 7.654005e-07
Iter: 838 loss: 7.65022435e-07
Iter: 839 loss: 7.64584797e-07
Iter: 840 loss: 7.64525339e-07
Iter: 841 loss: 7.63949856e-07
Iter: 842 loss: 7.71859845e-07
Iter: 843 loss: 7.6395105e-07
Iter: 844 loss: 7.63461344e-07
Iter: 845 loss: 7.64461447e-07
Iter: 846 loss: 7.63277399e-07
Iter: 847 loss: 7.62867671e-07
Iter: 848 loss: 7.65323307e-07
Iter: 849 loss: 7.62843399e-07
Iter: 850 loss: 7.62451123e-07
Iter: 851 loss: 7.61768774e-07
Iter: 852 loss: 7.78507456e-07
Iter: 853 loss: 7.61766955e-07
Iter: 854 loss: 7.61249794e-07
Iter: 855 loss: 7.6202582e-07
Iter: 856 loss: 7.61002525e-07
Iter: 857 loss: 7.60291641e-07
Iter: 858 loss: 7.6143408e-07
Iter: 859 loss: 7.60008447e-07
Iter: 860 loss: 7.5966318e-07
Iter: 861 loss: 7.5958792e-07
Iter: 862 loss: 7.59147497e-07
Iter: 863 loss: 7.58480212e-07
Iter: 864 loss: 7.58486863e-07
Iter: 865 loss: 7.57837938e-07
Iter: 866 loss: 7.57466751e-07
Iter: 867 loss: 7.57183784e-07
Iter: 868 loss: 7.5667765e-07
Iter: 869 loss: 7.56629163e-07
Iter: 870 loss: 7.56164582e-07
Iter: 871 loss: 7.58860665e-07
Iter: 872 loss: 7.5609978e-07
Iter: 873 loss: 7.55761562e-07
Iter: 874 loss: 7.55472456e-07
Iter: 875 loss: 7.55368092e-07
Iter: 876 loss: 7.54963935e-07
Iter: 877 loss: 7.58856913e-07
Iter: 878 loss: 7.54916243e-07
Iter: 879 loss: 7.54536757e-07
Iter: 880 loss: 7.55785209e-07
Iter: 881 loss: 7.54407097e-07
Iter: 882 loss: 7.54006635e-07
Iter: 883 loss: 7.54449047e-07
Iter: 884 loss: 7.53811491e-07
Iter: 885 loss: 7.53377549e-07
Iter: 886 loss: 7.5612337e-07
Iter: 887 loss: 7.53354925e-07
Iter: 888 loss: 7.53020572e-07
Iter: 889 loss: 7.52241e-07
Iter: 890 loss: 7.62030709e-07
Iter: 891 loss: 7.52197479e-07
Iter: 892 loss: 7.51506718e-07
Iter: 893 loss: 7.56591248e-07
Iter: 894 loss: 7.51427706e-07
Iter: 895 loss: 7.50965796e-07
Iter: 896 loss: 7.51984317e-07
Iter: 897 loss: 7.50743e-07
Iter: 898 loss: 7.50461879e-07
Iter: 899 loss: 7.50445054e-07
Iter: 900 loss: 7.50158e-07
Iter: 901 loss: 7.49654248e-07
Iter: 902 loss: 7.49649871e-07
Iter: 903 loss: 7.49185915e-07
Iter: 904 loss: 7.48891921e-07
Iter: 905 loss: 7.48747652e-07
Iter: 906 loss: 7.48253115e-07
Iter: 907 loss: 7.48211619e-07
Iter: 908 loss: 7.47757213e-07
Iter: 909 loss: 7.47432068e-07
Iter: 910 loss: 7.47305648e-07
Iter: 911 loss: 7.46811907e-07
Iter: 912 loss: 7.46586693e-07
Iter: 913 loss: 7.46337321e-07
Iter: 914 loss: 7.46074306e-07
Iter: 915 loss: 7.459887e-07
Iter: 916 loss: 7.45643661e-07
Iter: 917 loss: 7.45969487e-07
Iter: 918 loss: 7.4544414e-07
Iter: 919 loss: 7.45081365e-07
Iter: 920 loss: 7.45795091e-07
Iter: 921 loss: 7.44918907e-07
Iter: 922 loss: 7.44442332e-07
Iter: 923 loss: 7.4541822e-07
Iter: 924 loss: 7.44266629e-07
Iter: 925 loss: 7.43760324e-07
Iter: 926 loss: 7.44115823e-07
Iter: 927 loss: 7.43457e-07
Iter: 928 loss: 7.42970315e-07
Iter: 929 loss: 7.42389034e-07
Iter: 930 loss: 7.42316786e-07
Iter: 931 loss: 7.41956569e-07
Iter: 932 loss: 7.41865847e-07
Iter: 933 loss: 7.41445376e-07
Iter: 934 loss: 7.418476e-07
Iter: 935 loss: 7.41178155e-07
Iter: 936 loss: 7.40777182e-07
Iter: 937 loss: 7.40576354e-07
Iter: 938 loss: 7.40391e-07
Iter: 939 loss: 7.39869392e-07
Iter: 940 loss: 7.4142423e-07
Iter: 941 loss: 7.39676182e-07
Iter: 942 loss: 7.39147e-07
Iter: 943 loss: 7.39153904e-07
Iter: 944 loss: 7.38829954e-07
Iter: 945 loss: 7.38036874e-07
Iter: 946 loss: 7.44008446e-07
Iter: 947 loss: 7.37879361e-07
Iter: 948 loss: 7.37254709e-07
Iter: 949 loss: 7.45745069e-07
Iter: 950 loss: 7.37247888e-07
Iter: 951 loss: 7.36894322e-07
Iter: 952 loss: 7.36891707e-07
Iter: 953 loss: 7.36585434e-07
Iter: 954 loss: 7.36085326e-07
Iter: 955 loss: 7.47949457e-07
Iter: 956 loss: 7.36080096e-07
Iter: 957 loss: 7.35736648e-07
Iter: 958 loss: 7.3574256e-07
Iter: 959 loss: 7.35449589e-07
Iter: 960 loss: 7.34837045e-07
Iter: 961 loss: 7.4385531e-07
Iter: 962 loss: 7.34825562e-07
Iter: 963 loss: 7.34183175e-07
Iter: 964 loss: 7.39350696e-07
Iter: 965 loss: 7.3412474e-07
Iter: 966 loss: 7.3362321e-07
Iter: 967 loss: 7.33514923e-07
Iter: 968 loss: 7.33199442e-07
Iter: 969 loss: 7.32732644e-07
Iter: 970 loss: 7.32712351e-07
Iter: 971 loss: 7.32307058e-07
Iter: 972 loss: 7.32154888e-07
Iter: 973 loss: 7.31939053e-07
Iter: 974 loss: 7.31517616e-07
Iter: 975 loss: 7.31394152e-07
Iter: 976 loss: 7.31143189e-07
Iter: 977 loss: 7.30972943e-07
Iter: 978 loss: 7.30862439e-07
Iter: 979 loss: 7.30588852e-07
Iter: 980 loss: 7.29974488e-07
Iter: 981 loss: 7.40363248e-07
Iter: 982 loss: 7.2996761e-07
Iter: 983 loss: 7.29337444e-07
Iter: 984 loss: 7.30294e-07
Iter: 985 loss: 7.29076e-07
Iter: 986 loss: 7.28722057e-07
Iter: 987 loss: 7.28663338e-07
Iter: 988 loss: 7.28269299e-07
Iter: 989 loss: 7.27666759e-07
Iter: 990 loss: 7.2762839e-07
Iter: 991 loss: 7.27215706e-07
Iter: 992 loss: 7.33098545e-07
Iter: 993 loss: 7.27203087e-07
Iter: 994 loss: 7.26755957e-07
Iter: 995 loss: 7.26584801e-07
Iter: 996 loss: 7.26368398e-07
Iter: 997 loss: 7.25988855e-07
Iter: 998 loss: 7.26936776e-07
Iter: 999 loss: 7.25832365e-07
Iter: 1000 loss: 7.25410189e-07
Iter: 1001 loss: 7.26664609e-07
Iter: 1002 loss: 7.25284735e-07
Iter: 1003 loss: 7.24882057e-07
Iter: 1004 loss: 7.27706492e-07
Iter: 1005 loss: 7.24843801e-07
Iter: 1006 loss: 7.2442225e-07
Iter: 1007 loss: 7.24467895e-07
Iter: 1008 loss: 7.24075619e-07
Iter: 1009 loss: 7.23553285e-07
Iter: 1010 loss: 7.23222968e-07
Iter: 1011 loss: 7.23025e-07
Iter: 1012 loss: 7.22614686e-07
Iter: 1013 loss: 7.22596212e-07
Iter: 1014 loss: 7.22134587e-07
Iter: 1015 loss: 7.22082518e-07
Iter: 1016 loss: 7.21727474e-07
Iter: 1017 loss: 7.21257948e-07
Iter: 1018 loss: 7.21025458e-07
Iter: 1019 loss: 7.20768469e-07
Iter: 1020 loss: 7.20203843e-07
Iter: 1021 loss: 7.22166419e-07
Iter: 1022 loss: 7.2007424e-07
Iter: 1023 loss: 7.19663944e-07
Iter: 1024 loss: 7.19637e-07
Iter: 1025 loss: 7.19300488e-07
Iter: 1026 loss: 7.18609272e-07
Iter: 1027 loss: 7.30278941e-07
Iter: 1028 loss: 7.18604156e-07
Iter: 1029 loss: 7.18059198e-07
Iter: 1030 loss: 7.22227071e-07
Iter: 1031 loss: 7.18004e-07
Iter: 1032 loss: 7.17421926e-07
Iter: 1033 loss: 7.18598358e-07
Iter: 1034 loss: 7.17169428e-07
Iter: 1035 loss: 7.16743671e-07
Iter: 1036 loss: 7.16289719e-07
Iter: 1037 loss: 7.16208774e-07
Iter: 1038 loss: 7.1577017e-07
Iter: 1039 loss: 7.20018704e-07
Iter: 1040 loss: 7.1575414e-07
Iter: 1041 loss: 7.15274837e-07
Iter: 1042 loss: 7.182731e-07
Iter: 1043 loss: 7.15235103e-07
Iter: 1044 loss: 7.14948101e-07
Iter: 1045 loss: 7.14699468e-07
Iter: 1046 loss: 7.14647854e-07
Iter: 1047 loss: 7.14061628e-07
Iter: 1048 loss: 7.14459475e-07
Iter: 1049 loss: 7.13710961e-07
Iter: 1050 loss: 7.13046916e-07
Iter: 1051 loss: 7.16053648e-07
Iter: 1052 loss: 7.12932888e-07
Iter: 1053 loss: 7.12480755e-07
Iter: 1054 loss: 7.12491e-07
Iter: 1055 loss: 7.12045392e-07
Iter: 1056 loss: 7.11158577e-07
Iter: 1057 loss: 7.26368967e-07
Iter: 1058 loss: 7.11141809e-07
Iter: 1059 loss: 7.10749305e-07
Iter: 1060 loss: 7.10733e-07
Iter: 1061 loss: 7.10321331e-07
Iter: 1062 loss: 7.11565e-07
Iter: 1063 loss: 7.10205313e-07
Iter: 1064 loss: 7.09934284e-07
Iter: 1065 loss: 7.093671e-07
Iter: 1066 loss: 7.19991931e-07
Iter: 1067 loss: 7.09365054e-07
Iter: 1068 loss: 7.08958169e-07
Iter: 1069 loss: 7.08946118e-07
Iter: 1070 loss: 7.08568223e-07
Iter: 1071 loss: 7.08473749e-07
Iter: 1072 loss: 7.08232051e-07
Iter: 1073 loss: 7.07791969e-07
Iter: 1074 loss: 7.0704084e-07
Iter: 1075 loss: 7.07039931e-07
Iter: 1076 loss: 7.06994342e-07
Iter: 1077 loss: 7.06724904e-07
Iter: 1078 loss: 7.0636861e-07
Iter: 1079 loss: 7.06356445e-07
Iter: 1080 loss: 7.06112701e-07
Iter: 1081 loss: 7.05733044e-07
Iter: 1082 loss: 7.05623506e-07
Iter: 1083 loss: 7.05390505e-07
Iter: 1084 loss: 7.0487414e-07
Iter: 1085 loss: 7.04725494e-07
Iter: 1086 loss: 7.04399156e-07
Iter: 1087 loss: 7.03906835e-07
Iter: 1088 loss: 7.0386136e-07
Iter: 1089 loss: 7.03427304e-07
Iter: 1090 loss: 7.04635909e-07
Iter: 1091 loss: 7.03279284e-07
Iter: 1092 loss: 7.02840168e-07
Iter: 1093 loss: 7.02823968e-07
Iter: 1094 loss: 7.02430043e-07
Iter: 1095 loss: 7.02030093e-07
Iter: 1096 loss: 7.04046386e-07
Iter: 1097 loss: 7.01922943e-07
Iter: 1098 loss: 7.01509748e-07
Iter: 1099 loss: 7.05451725e-07
Iter: 1100 loss: 7.01482463e-07
Iter: 1101 loss: 7.01195063e-07
Iter: 1102 loss: 7.00630778e-07
Iter: 1103 loss: 7.11176654e-07
Iter: 1104 loss: 7.0062e-07
Iter: 1105 loss: 7.00060923e-07
Iter: 1106 loss: 7.01721717e-07
Iter: 1107 loss: 6.99897839e-07
Iter: 1108 loss: 6.99489419e-07
Iter: 1109 loss: 6.99456507e-07
Iter: 1110 loss: 6.99189e-07
Iter: 1111 loss: 6.98514555e-07
Iter: 1112 loss: 7.06496166e-07
Iter: 1113 loss: 6.98473514e-07
Iter: 1114 loss: 6.97750579e-07
Iter: 1115 loss: 7.02357283e-07
Iter: 1116 loss: 6.97680889e-07
Iter: 1117 loss: 6.97377345e-07
Iter: 1118 loss: 6.97330279e-07
Iter: 1119 loss: 6.97086762e-07
Iter: 1120 loss: 6.96548909e-07
Iter: 1121 loss: 7.02655427e-07
Iter: 1122 loss: 6.96481038e-07
Iter: 1123 loss: 6.95881226e-07
Iter: 1124 loss: 6.97032647e-07
Iter: 1125 loss: 6.95619747e-07
Iter: 1126 loss: 6.95016297e-07
Iter: 1127 loss: 6.96834832e-07
Iter: 1128 loss: 6.94825815e-07
Iter: 1129 loss: 6.94472533e-07
Iter: 1130 loss: 6.9440506e-07
Iter: 1131 loss: 6.94162054e-07
Iter: 1132 loss: 6.93588845e-07
Iter: 1133 loss: 6.99761301e-07
Iter: 1134 loss: 6.93507673e-07
Iter: 1135 loss: 6.9317997e-07
Iter: 1136 loss: 6.93141146e-07
Iter: 1137 loss: 6.92848744e-07
Iter: 1138 loss: 6.93476181e-07
Iter: 1139 loss: 6.92697313e-07
Iter: 1140 loss: 6.92387232e-07
Iter: 1141 loss: 6.92138656e-07
Iter: 1142 loss: 6.9204566e-07
Iter: 1143 loss: 6.9159762e-07
Iter: 1144 loss: 6.93548259e-07
Iter: 1145 loss: 6.91497576e-07
Iter: 1146 loss: 6.91143839e-07
Iter: 1147 loss: 6.94212929e-07
Iter: 1148 loss: 6.9111951e-07
Iter: 1149 loss: 6.90767706e-07
Iter: 1150 loss: 6.90106731e-07
Iter: 1151 loss: 7.03649334e-07
Iter: 1152 loss: 6.90103207e-07
Iter: 1153 loss: 6.89617423e-07
Iter: 1154 loss: 6.96551638e-07
Iter: 1155 loss: 6.89611909e-07
Iter: 1156 loss: 6.89101569e-07
Iter: 1157 loss: 6.90541469e-07
Iter: 1158 loss: 6.88933198e-07
Iter: 1159 loss: 6.88631303e-07
Iter: 1160 loss: 6.88047066e-07
Iter: 1161 loss: 7.0153726e-07
Iter: 1162 loss: 6.88050875e-07
Iter: 1163 loss: 6.87382681e-07
Iter: 1164 loss: 6.90725415e-07
Iter: 1165 loss: 6.87247962e-07
Iter: 1166 loss: 6.87014563e-07
Iter: 1167 loss: 6.87006946e-07
Iter: 1168 loss: 6.86677083e-07
Iter: 1169 loss: 6.86144517e-07
Iter: 1170 loss: 6.992218e-07
Iter: 1171 loss: 6.86145427e-07
Iter: 1172 loss: 6.8559757e-07
Iter: 1173 loss: 6.87229715e-07
Iter: 1174 loss: 6.85434429e-07
Iter: 1175 loss: 6.85001851e-07
Iter: 1176 loss: 6.84996166e-07
Iter: 1177 loss: 6.84709221e-07
Iter: 1178 loss: 6.84121e-07
Iter: 1179 loss: 6.95935455e-07
Iter: 1180 loss: 6.84123677e-07
Iter: 1181 loss: 6.8358e-07
Iter: 1182 loss: 6.85133955e-07
Iter: 1183 loss: 6.83405915e-07
Iter: 1184 loss: 6.82838049e-07
Iter: 1185 loss: 6.87179408e-07
Iter: 1186 loss: 6.82794905e-07
Iter: 1187 loss: 6.82434461e-07
Iter: 1188 loss: 6.84801876e-07
Iter: 1189 loss: 6.82382961e-07
Iter: 1190 loss: 6.82109089e-07
Iter: 1191 loss: 6.81638426e-07
Iter: 1192 loss: 6.81623135e-07
Iter: 1193 loss: 6.81233871e-07
Iter: 1194 loss: 6.81238134e-07
Iter: 1195 loss: 6.80802486e-07
Iter: 1196 loss: 6.8035115e-07
Iter: 1197 loss: 6.80262644e-07
Iter: 1198 loss: 6.7972104e-07
Iter: 1199 loss: 6.80632184e-07
Iter: 1200 loss: 6.79461436e-07
Iter: 1201 loss: 6.7904773e-07
Iter: 1202 loss: 6.84436486e-07
Iter: 1203 loss: 6.79060236e-07
Iter: 1204 loss: 6.78616175e-07
Iter: 1205 loss: 6.79163406e-07
Iter: 1206 loss: 6.78394713e-07
Iter: 1207 loss: 6.78118283e-07
Iter: 1208 loss: 6.77931666e-07
Iter: 1209 loss: 6.77837647e-07
Iter: 1210 loss: 6.77442927e-07
Iter: 1211 loss: 6.77438038e-07
Iter: 1212 loss: 6.77163143e-07
Iter: 1213 loss: 6.76579873e-07
Iter: 1214 loss: 6.85532882e-07
Iter: 1215 loss: 6.76534626e-07
Iter: 1216 loss: 6.75922308e-07
Iter: 1217 loss: 6.77098228e-07
Iter: 1218 loss: 6.75671572e-07
Iter: 1219 loss: 6.75049591e-07
Iter: 1220 loss: 6.8063423e-07
Iter: 1221 loss: 6.75037427e-07
Iter: 1222 loss: 6.74537887e-07
Iter: 1223 loss: 6.77359594e-07
Iter: 1224 loss: 6.74499574e-07
Iter: 1225 loss: 6.74151238e-07
Iter: 1226 loss: 6.7493886e-07
Iter: 1227 loss: 6.74052671e-07
Iter: 1228 loss: 6.73740658e-07
Iter: 1229 loss: 6.74417549e-07
Iter: 1230 loss: 6.73637487e-07
Iter: 1231 loss: 6.73240152e-07
Iter: 1232 loss: 6.74383e-07
Iter: 1233 loss: 6.73127204e-07
Iter: 1234 loss: 6.72768465e-07
Iter: 1235 loss: 6.72313945e-07
Iter: 1236 loss: 6.72285523e-07
Iter: 1237 loss: 6.71651094e-07
Iter: 1238 loss: 6.72526141e-07
Iter: 1239 loss: 6.7135187e-07
Iter: 1240 loss: 6.71056341e-07
Iter: 1241 loss: 6.70928785e-07
Iter: 1242 loss: 6.70605573e-07
Iter: 1243 loss: 6.70206191e-07
Iter: 1244 loss: 6.7016208e-07
Iter: 1245 loss: 6.69850579e-07
Iter: 1246 loss: 6.73217869e-07
Iter: 1247 loss: 6.6984262e-07
Iter: 1248 loss: 6.69482574e-07
Iter: 1249 loss: 6.69968472e-07
Iter: 1250 loss: 6.69287e-07
Iter: 1251 loss: 6.6904056e-07
Iter: 1252 loss: 6.68504526e-07
Iter: 1253 loss: 6.75555839e-07
Iter: 1254 loss: 6.68451889e-07
Iter: 1255 loss: 6.67738846e-07
Iter: 1256 loss: 6.71401267e-07
Iter: 1257 loss: 6.67622487e-07
Iter: 1258 loss: 6.67220775e-07
Iter: 1259 loss: 6.67218046e-07
Iter: 1260 loss: 6.66827191e-07
Iter: 1261 loss: 6.66332426e-07
Iter: 1262 loss: 6.66278595e-07
Iter: 1263 loss: 6.6595851e-07
Iter: 1264 loss: 6.65949415e-07
Iter: 1265 loss: 6.65608e-07
Iter: 1266 loss: 6.6537757e-07
Iter: 1267 loss: 6.65256835e-07
Iter: 1268 loss: 6.64914921e-07
Iter: 1269 loss: 6.66377559e-07
Iter: 1270 loss: 6.64836421e-07
Iter: 1271 loss: 6.64467052e-07
Iter: 1272 loss: 6.64105301e-07
Iter: 1273 loss: 6.64048116e-07
Iter: 1274 loss: 6.63717969e-07
Iter: 1275 loss: 6.63693413e-07
Iter: 1276 loss: 6.6333223e-07
Iter: 1277 loss: 6.63025389e-07
Iter: 1278 loss: 6.62963771e-07
Iter: 1279 loss: 6.62483842e-07
Iter: 1280 loss: 6.62727871e-07
Iter: 1281 loss: 6.62180469e-07
Iter: 1282 loss: 6.61928311e-07
Iter: 1283 loss: 6.61870502e-07
Iter: 1284 loss: 6.61594243e-07
Iter: 1285 loss: 6.60987666e-07
Iter: 1286 loss: 6.72918645e-07
Iter: 1287 loss: 6.60999149e-07
Iter: 1288 loss: 6.60544288e-07
Iter: 1289 loss: 6.60914679e-07
Iter: 1290 loss: 6.603006e-07
Iter: 1291 loss: 6.59819193e-07
Iter: 1292 loss: 6.59833177e-07
Iter: 1293 loss: 6.5940327e-07
Iter: 1294 loss: 6.59720058e-07
Iter: 1295 loss: 6.59158786e-07
Iter: 1296 loss: 6.58751333e-07
Iter: 1297 loss: 6.58779754e-07
Iter: 1298 loss: 6.58452336e-07
Iter: 1299 loss: 6.58022827e-07
Iter: 1300 loss: 6.58026e-07
Iter: 1301 loss: 6.57724968e-07
Iter: 1302 loss: 6.5709321e-07
Iter: 1303 loss: 6.64956588e-07
Iter: 1304 loss: 6.57031705e-07
Iter: 1305 loss: 6.56543534e-07
Iter: 1306 loss: 6.62495097e-07
Iter: 1307 loss: 6.56547911e-07
Iter: 1308 loss: 6.56127327e-07
Iter: 1309 loss: 6.5674044e-07
Iter: 1310 loss: 6.55942e-07
Iter: 1311 loss: 6.55619885e-07
Iter: 1312 loss: 6.55616191e-07
Iter: 1313 loss: 6.55340557e-07
Iter: 1314 loss: 6.54904454e-07
Iter: 1315 loss: 6.54904738e-07
Iter: 1316 loss: 6.54428277e-07
Iter: 1317 loss: 6.542287e-07
Iter: 1318 loss: 6.53975235e-07
Iter: 1319 loss: 6.53416805e-07
Iter: 1320 loss: 6.5824355e-07
Iter: 1321 loss: 6.53412087e-07
Iter: 1322 loss: 6.52946255e-07
Iter: 1323 loss: 6.58701822e-07
Iter: 1324 loss: 6.52936365e-07
Iter: 1325 loss: 6.52637823e-07
Iter: 1326 loss: 6.52077858e-07
Iter: 1327 loss: 6.62670175e-07
Iter: 1328 loss: 6.52061317e-07
Iter: 1329 loss: 6.51654318e-07
Iter: 1330 loss: 6.54817086e-07
Iter: 1331 loss: 6.51618279e-07
Iter: 1332 loss: 6.51247092e-07
Iter: 1333 loss: 6.54261441e-07
Iter: 1334 loss: 6.51197e-07
Iter: 1335 loss: 6.50939569e-07
Iter: 1336 loss: 6.50623178e-07
Iter: 1337 loss: 6.50598508e-07
Iter: 1338 loss: 6.50284164e-07
Iter: 1339 loss: 6.50268362e-07
Iter: 1340 loss: 6.50012453e-07
Iter: 1341 loss: 6.49495746e-07
Iter: 1342 loss: 6.59935e-07
Iter: 1343 loss: 6.49459707e-07
Iter: 1344 loss: 6.48910145e-07
Iter: 1345 loss: 6.49251433e-07
Iter: 1346 loss: 6.48550099e-07
Iter: 1347 loss: 6.48363937e-07
Iter: 1348 loss: 6.48231378e-07
Iter: 1349 loss: 6.47972058e-07
Iter: 1350 loss: 6.48233e-07
Iter: 1351 loss: 6.47848765e-07
Iter: 1352 loss: 6.47494289e-07
Iter: 1353 loss: 6.47297384e-07
Iter: 1354 loss: 6.47159766e-07
Iter: 1355 loss: 6.46708941e-07
Iter: 1356 loss: 6.48605749e-07
Iter: 1357 loss: 6.46610715e-07
Iter: 1358 loss: 6.46331614e-07
Iter: 1359 loss: 6.46328203e-07
Iter: 1360 loss: 6.46000501e-07
Iter: 1361 loss: 6.45392561e-07
Iter: 1362 loss: 6.56550128e-07
Iter: 1363 loss: 6.45392504e-07
Iter: 1364 loss: 6.4488097e-07
Iter: 1365 loss: 6.4644e-07
Iter: 1366 loss: 6.44739032e-07
Iter: 1367 loss: 6.44209251e-07
Iter: 1368 loss: 6.49675599e-07
Iter: 1369 loss: 6.44213344e-07
Iter: 1370 loss: 6.43959083e-07
Iter: 1371 loss: 6.43854946e-07
Iter: 1372 loss: 6.4368578e-07
Iter: 1373 loss: 6.43350745e-07
Iter: 1374 loss: 6.47753211e-07
Iter: 1375 loss: 6.43333408e-07
Iter: 1376 loss: 6.43124736e-07
Iter: 1377 loss: 6.42653276e-07
Iter: 1378 loss: 6.51051096e-07
Iter: 1379 loss: 6.42669875e-07
Iter: 1380 loss: 6.42184602e-07
Iter: 1381 loss: 6.44725105e-07
Iter: 1382 loss: 6.42144471e-07
Iter: 1383 loss: 6.41757424e-07
Iter: 1384 loss: 6.4544065e-07
Iter: 1385 loss: 6.41741963e-07
Iter: 1386 loss: 6.41462577e-07
Iter: 1387 loss: 6.41140559e-07
Iter: 1388 loss: 6.41089855e-07
Iter: 1389 loss: 6.40516305e-07
Iter: 1390 loss: 6.41217355e-07
Iter: 1391 loss: 6.40225323e-07
Iter: 1392 loss: 6.39748805e-07
Iter: 1393 loss: 6.42771624e-07
Iter: 1394 loss: 6.39679342e-07
Iter: 1395 loss: 6.39359598e-07
Iter: 1396 loss: 6.39356472e-07
Iter: 1397 loss: 6.39193956e-07
Iter: 1398 loss: 6.38732047e-07
Iter: 1399 loss: 6.44236422e-07
Iter: 1400 loss: 6.38713232e-07
Iter: 1401 loss: 6.38374104e-07
Iter: 1402 loss: 6.42452505e-07
Iter: 1403 loss: 6.38348922e-07
Iter: 1404 loss: 6.3797296e-07
Iter: 1405 loss: 6.38652182e-07
Iter: 1406 loss: 6.37839321e-07
Iter: 1407 loss: 6.37434596e-07
Iter: 1408 loss: 6.37567155e-07
Iter: 1409 loss: 6.37180392e-07
Iter: 1410 loss: 6.36663572e-07
Iter: 1411 loss: 6.41458939e-07
Iter: 1412 loss: 6.36635832e-07
Iter: 1413 loss: 6.36394589e-07
Iter: 1414 loss: 6.35850085e-07
Iter: 1415 loss: 6.44526949e-07
Iter: 1416 loss: 6.35828314e-07
Iter: 1417 loss: 6.35484753e-07
Iter: 1418 loss: 6.35478727e-07
Iter: 1419 loss: 6.35110837e-07
Iter: 1420 loss: 6.35888284e-07
Iter: 1421 loss: 6.34991807e-07
Iter: 1422 loss: 6.3472271e-07
Iter: 1423 loss: 6.34715661e-07
Iter: 1424 loss: 6.34460662e-07
Iter: 1425 loss: 6.34085268e-07
Iter: 1426 loss: 6.35393121e-07
Iter: 1427 loss: 6.33970558e-07
Iter: 1428 loss: 6.33641434e-07
Iter: 1429 loss: 6.36148798e-07
Iter: 1430 loss: 6.33628304e-07
Iter: 1431 loss: 6.33207719e-07
Iter: 1432 loss: 6.33160425e-07
Iter: 1433 loss: 6.32871263e-07
Iter: 1434 loss: 6.32494789e-07
Iter: 1435 loss: 6.32467618e-07
Iter: 1436 loss: 6.32186811e-07
Iter: 1437 loss: 6.31912485e-07
Iter: 1438 loss: 6.31906175e-07
Iter: 1439 loss: 6.31650607e-07
Iter: 1440 loss: 6.31566252e-07
Iter: 1441 loss: 6.31443356e-07
Iter: 1442 loss: 6.31166472e-07
Iter: 1443 loss: 6.33643936e-07
Iter: 1444 loss: 6.31153114e-07
Iter: 1445 loss: 6.30867646e-07
Iter: 1446 loss: 6.30713259e-07
Iter: 1447 loss: 6.30577802e-07
Iter: 1448 loss: 6.3029529e-07
Iter: 1449 loss: 6.30188651e-07
Iter: 1450 loss: 6.30010902e-07
Iter: 1451 loss: 6.29707188e-07
Iter: 1452 loss: 6.29680471e-07
Iter: 1453 loss: 6.29429564e-07
Iter: 1454 loss: 6.2895856e-07
Iter: 1455 loss: 6.28965836e-07
Iter: 1456 loss: 6.28492e-07
Iter: 1457 loss: 6.30171144e-07
Iter: 1458 loss: 6.28380519e-07
Iter: 1459 loss: 6.27981535e-07
Iter: 1460 loss: 6.32183401e-07
Iter: 1461 loss: 6.27980057e-07
Iter: 1462 loss: 6.27664861e-07
Iter: 1463 loss: 6.29244482e-07
Iter: 1464 loss: 6.27595909e-07
Iter: 1465 loss: 6.27339773e-07
Iter: 1466 loss: 6.27008262e-07
Iter: 1467 loss: 6.26995416e-07
Iter: 1468 loss: 6.26693748e-07
Iter: 1469 loss: 6.29912506e-07
Iter: 1470 loss: 6.26675e-07
Iter: 1471 loss: 6.26359e-07
Iter: 1472 loss: 6.26504516e-07
Iter: 1473 loss: 6.26124574e-07
Iter: 1474 loss: 6.25749408e-07
Iter: 1475 loss: 6.28030818e-07
Iter: 1476 loss: 6.25703137e-07
Iter: 1477 loss: 6.25354176e-07
Iter: 1478 loss: 6.25743155e-07
Iter: 1479 loss: 6.25157668e-07
Iter: 1480 loss: 6.24812628e-07
Iter: 1481 loss: 6.24479412e-07
Iter: 1482 loss: 6.2440165e-07
Iter: 1483 loss: 6.24110726e-07
Iter: 1484 loss: 6.24109e-07
Iter: 1485 loss: 6.23798599e-07
Iter: 1486 loss: 6.24142899e-07
Iter: 1487 loss: 6.23616813e-07
Iter: 1488 loss: 6.23375797e-07
Iter: 1489 loss: 6.23012795e-07
Iter: 1490 loss: 6.22982725e-07
Iter: 1491 loss: 6.22505922e-07
Iter: 1492 loss: 6.2605136e-07
Iter: 1493 loss: 6.22462323e-07
Iter: 1494 loss: 6.22126208e-07
Iter: 1495 loss: 6.26258384e-07
Iter: 1496 loss: 6.22143375e-07
Iter: 1497 loss: 6.21770937e-07
Iter: 1498 loss: 6.21385539e-07
Iter: 1499 loss: 6.21355071e-07
Iter: 1500 loss: 6.20887192e-07
Iter: 1501 loss: 6.21139293e-07
Iter: 1502 loss: 6.20602236e-07
Iter: 1503 loss: 6.20175683e-07
Iter: 1504 loss: 6.20160904e-07
Iter: 1505 loss: 6.19893e-07
Iter: 1506 loss: 6.1994092e-07
Iter: 1507 loss: 6.19684556e-07
Iter: 1508 loss: 6.1941364e-07
Iter: 1509 loss: 6.22340053e-07
Iter: 1510 loss: 6.19405512e-07
Iter: 1511 loss: 6.19138177e-07
Iter: 1512 loss: 6.18814909e-07
Iter: 1513 loss: 6.18789272e-07
Iter: 1514 loss: 6.1841871e-07
Iter: 1515 loss: 6.18457e-07
Iter: 1516 loss: 6.18118747e-07
Iter: 1517 loss: 6.17831859e-07
Iter: 1518 loss: 6.17798264e-07
Iter: 1519 loss: 6.17561e-07
Iter: 1520 loss: 6.17150477e-07
Iter: 1521 loss: 6.27151621e-07
Iter: 1522 loss: 6.17153034e-07
Iter: 1523 loss: 6.16654518e-07
Iter: 1524 loss: 6.16588125e-07
Iter: 1525 loss: 6.16222792e-07
Iter: 1526 loss: 6.15869567e-07
Iter: 1527 loss: 6.15844215e-07
Iter: 1528 loss: 6.15504746e-07
Iter: 1529 loss: 6.17095e-07
Iter: 1530 loss: 6.15448812e-07
Iter: 1531 loss: 6.15169483e-07
Iter: 1532 loss: 6.15144074e-07
Iter: 1533 loss: 6.14941428e-07
Iter: 1534 loss: 6.14614123e-07
Iter: 1535 loss: 6.14246801e-07
Iter: 1536 loss: 6.1421639e-07
Iter: 1537 loss: 6.13968382e-07
Iter: 1538 loss: 6.13872146e-07
Iter: 1539 loss: 6.13638804e-07
Iter: 1540 loss: 6.13344241e-07
Iter: 1541 loss: 6.13334805e-07
Iter: 1542 loss: 6.12956967e-07
Iter: 1543 loss: 6.17764726e-07
Iter: 1544 loss: 6.12951567e-07
Iter: 1545 loss: 6.12692531e-07
Iter: 1546 loss: 6.12464191e-07
Iter: 1547 loss: 6.1239291e-07
Iter: 1548 loss: 6.12064071e-07
Iter: 1549 loss: 6.11947883e-07
Iter: 1550 loss: 6.11731934e-07
Iter: 1551 loss: 6.11677e-07
Iter: 1552 loss: 6.11541225e-07
Iter: 1553 loss: 6.11326755e-07
Iter: 1554 loss: 6.1104231e-07
Iter: 1555 loss: 6.1102088e-07
Iter: 1556 loss: 6.10645e-07
Iter: 1557 loss: 6.10853135e-07
Iter: 1558 loss: 6.10382244e-07
Iter: 1559 loss: 6.1017e-07
Iter: 1560 loss: 6.10139068e-07
Iter: 1561 loss: 6.09834558e-07
Iter: 1562 loss: 6.09578763e-07
Iter: 1563 loss: 6.0951777e-07
Iter: 1564 loss: 6.09192625e-07
Iter: 1565 loss: 6.09786582e-07
Iter: 1566 loss: 6.09083827e-07
Iter: 1567 loss: 6.08767664e-07
Iter: 1568 loss: 6.10590291e-07
Iter: 1569 loss: 6.08714117e-07
Iter: 1570 loss: 6.08483731e-07
Iter: 1571 loss: 6.10028735e-07
Iter: 1572 loss: 6.08461278e-07
Iter: 1573 loss: 6.0822191e-07
Iter: 1574 loss: 6.08062e-07
Iter: 1575 loss: 6.07987e-07
Iter: 1576 loss: 6.07655352e-07
Iter: 1577 loss: 6.11013661e-07
Iter: 1578 loss: 6.07672121e-07
Iter: 1579 loss: 6.07411494e-07
Iter: 1580 loss: 6.07219704e-07
Iter: 1581 loss: 6.07152288e-07
Iter: 1582 loss: 6.06800654e-07
Iter: 1583 loss: 6.06514845e-07
Iter: 1584 loss: 6.06401898e-07
Iter: 1585 loss: 6.0586774e-07
Iter: 1586 loss: 6.08041205e-07
Iter: 1587 loss: 6.05746152e-07
Iter: 1588 loss: 6.05505193e-07
Iter: 1589 loss: 6.05496552e-07
Iter: 1590 loss: 6.05227683e-07
Iter: 1591 loss: 6.05557887e-07
Iter: 1592 loss: 6.0508421e-07
Iter: 1593 loss: 6.0487946e-07
Iter: 1594 loss: 6.04786237e-07
Iter: 1595 loss: 6.04684942e-07
Iter: 1596 loss: 6.04278796e-07
Iter: 1597 loss: 6.07159336e-07
Iter: 1598 loss: 6.04228774e-07
Iter: 1599 loss: 6.03999752e-07
Iter: 1600 loss: 6.03660624e-07
Iter: 1601 loss: 6.03669093e-07
Iter: 1602 loss: 6.03218837e-07
Iter: 1603 loss: 6.05277251e-07
Iter: 1604 loss: 6.03136527e-07
Iter: 1605 loss: 6.02787111e-07
Iter: 1606 loss: 6.06294691e-07
Iter: 1607 loss: 6.02791602e-07
Iter: 1608 loss: 6.02564e-07
Iter: 1609 loss: 6.02928367e-07
Iter: 1610 loss: 6.02457703e-07
Iter: 1611 loss: 6.02205432e-07
Iter: 1612 loss: 6.02782848e-07
Iter: 1613 loss: 6.0211471e-07
Iter: 1614 loss: 6.01825263e-07
Iter: 1615 loss: 6.02712873e-07
Iter: 1616 loss: 6.01759e-07
Iter: 1617 loss: 6.01523766e-07
Iter: 1618 loss: 6.01093461e-07
Iter: 1619 loss: 6.10187499e-07
Iter: 1620 loss: 6.01101647e-07
Iter: 1621 loss: 6.00623878e-07
Iter: 1622 loss: 6.0178138e-07
Iter: 1623 loss: 6.00450676e-07
Iter: 1624 loss: 5.99935163e-07
Iter: 1625 loss: 6.02779892e-07
Iter: 1626 loss: 5.99874056e-07
Iter: 1627 loss: 5.9971137e-07
Iter: 1628 loss: 5.99644068e-07
Iter: 1629 loss: 5.99463192e-07
Iter: 1630 loss: 5.99110308e-07
Iter: 1631 loss: 6.06976528e-07
Iter: 1632 loss: 5.99094506e-07
Iter: 1633 loss: 5.98965585e-07
Iter: 1634 loss: 5.98946144e-07
Iter: 1635 loss: 5.98771294e-07
Iter: 1636 loss: 5.98481051e-07
Iter: 1637 loss: 6.04561365e-07
Iter: 1638 loss: 5.98481e-07
Iter: 1639 loss: 5.98177166e-07
Iter: 1640 loss: 5.98568306e-07
Iter: 1641 loss: 5.98031875e-07
Iter: 1642 loss: 5.97730377e-07
Iter: 1643 loss: 6.00379906e-07
Iter: 1644 loss: 5.97693884e-07
Iter: 1645 loss: 5.97404835e-07
Iter: 1646 loss: 5.97903806e-07
Iter: 1647 loss: 5.97257497e-07
Iter: 1648 loss: 5.96949803e-07
Iter: 1649 loss: 5.97534608e-07
Iter: 1650 loss: 5.96830489e-07
Iter: 1651 loss: 5.96542691e-07
Iter: 1652 loss: 5.98772772e-07
Iter: 1653 loss: 5.9652416e-07
Iter: 1654 loss: 5.96302755e-07
Iter: 1655 loss: 5.96348514e-07
Iter: 1656 loss: 5.9616923e-07
Iter: 1657 loss: 5.95929691e-07
Iter: 1658 loss: 5.95722724e-07
Iter: 1659 loss: 5.9563348e-07
Iter: 1660 loss: 5.95260872e-07
Iter: 1661 loss: 5.96413486e-07
Iter: 1662 loss: 5.95141501e-07
Iter: 1663 loss: 5.94763719e-07
Iter: 1664 loss: 5.96446114e-07
Iter: 1665 loss: 5.94714038e-07
Iter: 1666 loss: 5.94232461e-07
Iter: 1667 loss: 5.97326675e-07
Iter: 1668 loss: 5.94189373e-07
Iter: 1669 loss: 5.93988148e-07
Iter: 1670 loss: 5.93995082e-07
Iter: 1671 loss: 5.93829e-07
Iter: 1672 loss: 5.93484629e-07
Iter: 1673 loss: 5.96245854e-07
Iter: 1674 loss: 5.9349793e-07
Iter: 1675 loss: 5.93286813e-07
Iter: 1676 loss: 5.92798131e-07
Iter: 1677 loss: 5.98822567e-07
Iter: 1678 loss: 5.92726053e-07
Iter: 1679 loss: 5.92353e-07
Iter: 1680 loss: 5.94279754e-07
Iter: 1681 loss: 5.92286767e-07
Iter: 1682 loss: 5.91939397e-07
Iter: 1683 loss: 5.93170739e-07
Iter: 1684 loss: 5.91847652e-07
Iter: 1685 loss: 5.91588673e-07
Iter: 1686 loss: 5.95145195e-07
Iter: 1687 loss: 5.91578896e-07
Iter: 1688 loss: 5.91314461e-07
Iter: 1689 loss: 5.91323897e-07
Iter: 1690 loss: 5.91142225e-07
Iter: 1691 loss: 5.90815262e-07
Iter: 1692 loss: 5.92457582e-07
Iter: 1693 loss: 5.90781099e-07
Iter: 1694 loss: 5.90526156e-07
Iter: 1695 loss: 5.91794731e-07
Iter: 1696 loss: 5.90479772e-07
Iter: 1697 loss: 5.90246486e-07
Iter: 1698 loss: 5.90206923e-07
Iter: 1699 loss: 5.90019e-07
Iter: 1700 loss: 5.89792194e-07
Iter: 1701 loss: 5.89829369e-07
Iter: 1702 loss: 5.89572778e-07
Iter: 1703 loss: 5.89289584e-07
Iter: 1704 loss: 5.92712809e-07
Iter: 1705 loss: 5.89286628e-07
Iter: 1706 loss: 5.88966259e-07
Iter: 1707 loss: 5.89928845e-07
Iter: 1708 loss: 5.88852686e-07
Iter: 1709 loss: 5.88628779e-07
Iter: 1710 loss: 5.88448415e-07
Iter: 1711 loss: 5.88397e-07
Iter: 1712 loss: 5.87960699e-07
Iter: 1713 loss: 5.91444518e-07
Iter: 1714 loss: 5.87921193e-07
Iter: 1715 loss: 5.87730767e-07
Iter: 1716 loss: 5.87273519e-07
Iter: 1717 loss: 5.91382388e-07
Iter: 1718 loss: 5.87197178e-07
Iter: 1719 loss: 5.86751923e-07
Iter: 1720 loss: 5.92018523e-07
Iter: 1721 loss: 5.86741123e-07
Iter: 1722 loss: 5.86517046e-07
Iter: 1723 loss: 5.88509806e-07
Iter: 1724 loss: 5.8650852e-07
Iter: 1725 loss: 5.86243857e-07
Iter: 1726 loss: 5.86154158e-07
Iter: 1727 loss: 5.85995508e-07
Iter: 1728 loss: 5.85716464e-07
Iter: 1729 loss: 5.8826646e-07
Iter: 1730 loss: 5.85697592e-07
Iter: 1731 loss: 5.85448106e-07
Iter: 1732 loss: 5.85525754e-07
Iter: 1733 loss: 5.85286102e-07
Iter: 1734 loss: 5.84934128e-07
Iter: 1735 loss: 5.85382168e-07
Iter: 1736 loss: 5.84743589e-07
Iter: 1737 loss: 5.84374447e-07
Iter: 1738 loss: 5.84291968e-07
Iter: 1739 loss: 5.84070676e-07
Iter: 1740 loss: 5.8399803e-07
Iter: 1741 loss: 5.83872747e-07
Iter: 1742 loss: 5.83686187e-07
Iter: 1743 loss: 5.83579379e-07
Iter: 1744 loss: 5.83505312e-07
Iter: 1745 loss: 5.83292831e-07
Iter: 1746 loss: 5.84341e-07
Iter: 1747 loss: 5.83258156e-07
Iter: 1748 loss: 5.83011399e-07
Iter: 1749 loss: 5.83056078e-07
Iter: 1750 loss: 5.82800851e-07
Iter: 1751 loss: 5.82522944e-07
Iter: 1752 loss: 5.82140046e-07
Iter: 1753 loss: 5.82139933e-07
Iter: 1754 loss: 5.81646532e-07
Iter: 1755 loss: 5.83796407e-07
Iter: 1756 loss: 5.81553252e-07
Iter: 1757 loss: 5.81377662e-07
Iter: 1758 loss: 5.81317522e-07
Iter: 1759 loss: 5.81128688e-07
Iter: 1760 loss: 5.80860331e-07
Iter: 1761 loss: 5.80861297e-07
Iter: 1762 loss: 5.8061903e-07
Iter: 1763 loss: 5.84439476e-07
Iter: 1764 loss: 5.80617041e-07
Iter: 1765 loss: 5.80436051e-07
Iter: 1766 loss: 5.80518417e-07
Iter: 1767 loss: 5.80279732e-07
Iter: 1768 loss: 5.80066853e-07
Iter: 1769 loss: 5.79830839e-07
Iter: 1770 loss: 5.79795312e-07
Iter: 1771 loss: 5.79336074e-07
Iter: 1772 loss: 5.81818881e-07
Iter: 1773 loss: 5.79281959e-07
Iter: 1774 loss: 5.78976255e-07
Iter: 1775 loss: 5.80177471e-07
Iter: 1776 loss: 5.78902814e-07
Iter: 1777 loss: 5.78540039e-07
Iter: 1778 loss: 5.80073333e-07
Iter: 1779 loss: 5.78447271e-07
Iter: 1780 loss: 5.78258e-07
Iter: 1781 loss: 5.78004403e-07
Iter: 1782 loss: 5.77976436e-07
Iter: 1783 loss: 5.77724791e-07
Iter: 1784 loss: 5.7770535e-07
Iter: 1785 loss: 5.77592232e-07
Iter: 1786 loss: 5.77320179e-07
Iter: 1787 loss: 5.80191113e-07
Iter: 1788 loss: 5.77273681e-07
Iter: 1789 loss: 5.76929153e-07
Iter: 1790 loss: 5.79390701e-07
Iter: 1791 loss: 5.76896e-07
Iter: 1792 loss: 5.76637035e-07
Iter: 1793 loss: 5.7930913e-07
Iter: 1794 loss: 5.7662362e-07
Iter: 1795 loss: 5.76474577e-07
Iter: 1796 loss: 5.76071272e-07
Iter: 1797 loss: 5.82487e-07
Iter: 1798 loss: 5.76053537e-07
Iter: 1799 loss: 5.7565984e-07
Iter: 1800 loss: 5.75661034e-07
Iter: 1801 loss: 5.75460717e-07
Iter: 1802 loss: 5.75344245e-07
Iter: 1803 loss: 5.7524926e-07
Iter: 1804 loss: 5.74943556e-07
Iter: 1805 loss: 5.75441e-07
Iter: 1806 loss: 5.7482157e-07
Iter: 1807 loss: 5.74544401e-07
Iter: 1808 loss: 5.76221112e-07
Iter: 1809 loss: 5.74504384e-07
Iter: 1810 loss: 5.74342835e-07
Iter: 1811 loss: 5.76678076e-07
Iter: 1812 loss: 5.74335957e-07
Iter: 1813 loss: 5.74127512e-07
Iter: 1814 loss: 5.73797934e-07
Iter: 1815 loss: 5.73784575e-07
Iter: 1816 loss: 5.73500643e-07
Iter: 1817 loss: 5.74474598e-07
Iter: 1818 loss: 5.73429361e-07
Iter: 1819 loss: 5.73073805e-07
Iter: 1820 loss: 5.75139211e-07
Iter: 1821 loss: 5.73028274e-07
Iter: 1822 loss: 5.72803401e-07
Iter: 1823 loss: 5.72388899e-07
Iter: 1824 loss: 5.80952474e-07
Iter: 1825 loss: 5.72393901e-07
Iter: 1826 loss: 5.72062902e-07
Iter: 1827 loss: 5.72067279e-07
Iter: 1828 loss: 5.71806368e-07
Iter: 1829 loss: 5.72723764e-07
Iter: 1830 loss: 5.71730538e-07
Iter: 1831 loss: 5.71548355e-07
Iter: 1832 loss: 5.71591386e-07
Iter: 1833 loss: 5.71438761e-07
Iter: 1834 loss: 5.71170062e-07
Iter: 1835 loss: 5.72653562e-07
Iter: 1836 loss: 5.71127202e-07
Iter: 1837 loss: 5.70963948e-07
Iter: 1838 loss: 5.70637326e-07
Iter: 1839 loss: 5.77107e-07
Iter: 1840 loss: 5.7064824e-07
Iter: 1841 loss: 5.70264888e-07
Iter: 1842 loss: 5.7250088e-07
Iter: 1843 loss: 5.70201166e-07
Iter: 1844 loss: 5.69829922e-07
Iter: 1845 loss: 5.70311386e-07
Iter: 1846 loss: 5.69628355e-07
Iter: 1847 loss: 5.69393478e-07
Iter: 1848 loss: 5.69372673e-07
Iter: 1849 loss: 5.69159965e-07
Iter: 1850 loss: 5.69022e-07
Iter: 1851 loss: 5.68937708e-07
Iter: 1852 loss: 5.68729433e-07
Iter: 1853 loss: 5.69253189e-07
Iter: 1854 loss: 5.68636267e-07
Iter: 1855 loss: 5.68332723e-07
Iter: 1856 loss: 5.69887106e-07
Iter: 1857 loss: 5.68294809e-07
Iter: 1858 loss: 5.68131384e-07
Iter: 1859 loss: 5.67763777e-07
Iter: 1860 loss: 5.73778493e-07
Iter: 1861 loss: 5.67756445e-07
Iter: 1862 loss: 5.67529185e-07
Iter: 1863 loss: 5.6750406e-07
Iter: 1864 loss: 5.67259349e-07
Iter: 1865 loss: 5.67227175e-07
Iter: 1866 loss: 5.67044424e-07
Iter: 1867 loss: 5.66752874e-07
Iter: 1868 loss: 5.66548692e-07
Iter: 1869 loss: 5.66458198e-07
Iter: 1870 loss: 5.66268341e-07
Iter: 1871 loss: 5.66227811e-07
Iter: 1872 loss: 5.66015274e-07
Iter: 1873 loss: 5.65701271e-07
Iter: 1874 loss: 5.65687e-07
Iter: 1875 loss: 5.65382038e-07
Iter: 1876 loss: 5.6587362e-07
Iter: 1877 loss: 5.65215885e-07
Iter: 1878 loss: 5.649066e-07
Iter: 1879 loss: 5.67416578e-07
Iter: 1880 loss: 5.64912227e-07
Iter: 1881 loss: 5.64654101e-07
Iter: 1882 loss: 5.6606109e-07
Iter: 1883 loss: 5.64613913e-07
Iter: 1884 loss: 5.64384322e-07
Iter: 1885 loss: 5.64269271e-07
Iter: 1886 loss: 5.6412506e-07
Iter: 1887 loss: 5.63852268e-07
Iter: 1888 loss: 5.64221409e-07
Iter: 1889 loss: 5.63705271e-07
Iter: 1890 loss: 5.63470735e-07
Iter: 1891 loss: 5.63468461e-07
Iter: 1892 loss: 5.63292645e-07
Iter: 1893 loss: 5.63061803e-07
Iter: 1894 loss: 5.63043955e-07
Iter: 1895 loss: 5.62783612e-07
Iter: 1896 loss: 5.62652815e-07
Iter: 1897 loss: 5.62525e-07
Iter: 1898 loss: 5.6240583e-07
Iter: 1899 loss: 5.62303455e-07
Iter: 1900 loss: 5.62156856e-07
Iter: 1901 loss: 5.61775e-07
Iter: 1902 loss: 5.65298e-07
Iter: 1903 loss: 5.61732634e-07
Iter: 1904 loss: 5.61372417e-07
Iter: 1905 loss: 5.65507662e-07
Iter: 1906 loss: 5.61352806e-07
Iter: 1907 loss: 5.61023569e-07
Iter: 1908 loss: 5.62045898e-07
Iter: 1909 loss: 5.60899252e-07
Iter: 1910 loss: 5.60693763e-07
Iter: 1911 loss: 5.60366e-07
Iter: 1912 loss: 5.60351054e-07
Iter: 1913 loss: 5.6000863e-07
Iter: 1914 loss: 5.61775778e-07
Iter: 1915 loss: 5.59932801e-07
Iter: 1916 loss: 5.59733905e-07
Iter: 1917 loss: 5.59700482e-07
Iter: 1918 loss: 5.5960129e-07
Iter: 1919 loss: 5.593227e-07
Iter: 1920 loss: 5.61924821e-07
Iter: 1921 loss: 5.59285127e-07
Iter: 1922 loss: 5.58887848e-07
Iter: 1923 loss: 5.60824049e-07
Iter: 1924 loss: 5.58857664e-07
Iter: 1925 loss: 5.58558668e-07
Iter: 1926 loss: 5.61896854e-07
Iter: 1927 loss: 5.58550425e-07
Iter: 1928 loss: 5.58348461e-07
Iter: 1929 loss: 5.58134388e-07
Iter: 1930 loss: 5.58081865e-07
Iter: 1931 loss: 5.57696126e-07
Iter: 1932 loss: 5.57789861e-07
Iter: 1933 loss: 5.5741657e-07
Iter: 1934 loss: 5.57062e-07
Iter: 1935 loss: 5.58173156e-07
Iter: 1936 loss: 5.56948919e-07
Iter: 1937 loss: 5.56787882e-07
Iter: 1938 loss: 5.56747295e-07
Iter: 1939 loss: 5.56577561e-07
Iter: 1940 loss: 5.56271743e-07
Iter: 1941 loss: 5.56268787e-07
Iter: 1942 loss: 5.5598656e-07
Iter: 1943 loss: 5.57393207e-07
Iter: 1944 loss: 5.55951715e-07
Iter: 1945 loss: 5.55608153e-07
Iter: 1946 loss: 5.56406235e-07
Iter: 1947 loss: 5.5547514e-07
Iter: 1948 loss: 5.55249528e-07
Iter: 1949 loss: 5.55191491e-07
Iter: 1950 loss: 5.55047905e-07
Iter: 1951 loss: 5.54900794e-07
Iter: 1952 loss: 5.54851511e-07
Iter: 1953 loss: 5.54707242e-07
Iter: 1954 loss: 5.54353733e-07
Iter: 1955 loss: 5.57930434e-07
Iter: 1956 loss: 5.54293479e-07
Iter: 1957 loss: 5.53968334e-07
Iter: 1958 loss: 5.55653855e-07
Iter: 1959 loss: 5.53914617e-07
Iter: 1960 loss: 5.53730331e-07
Iter: 1961 loss: 5.53721236e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6
+ date
Wed Oct 21 11:12:28 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1 --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee813df158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81429f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81429268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812f4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812ef598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812ef840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812796a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81215598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81215ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81297598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee8129d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d7c1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d7c1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee811b9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee811b9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d77c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d77c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d6fd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d6c5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d6aed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee811d99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d626ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d656620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d761268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d761a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d5a7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d58a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d59b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d52a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d552ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d60a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4ed6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4ed730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4f0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4da8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d673268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.025194433
test_loss: 0.023110636
train_loss: 0.010310841
test_loss: 0.010788235
train_loss: 0.0063897427
test_loss: 0.0069764024
train_loss: 0.0051481547
test_loss: 0.0055416953
train_loss: 0.004544401
test_loss: 0.004927233
train_loss: 0.0041004224
test_loss: 0.0046658306
train_loss: 0.004025884
test_loss: 0.0046371454
train_loss: 0.0040539065
test_loss: 0.0048913504
train_loss: 0.0037005583
test_loss: 0.004320377
train_loss: 0.0038519832
test_loss: 0.004373812
train_loss: 0.0038969778
test_loss: 0.004351088
train_loss: 0.0034851735
test_loss: 0.0041042506
train_loss: 0.0036536297
test_loss: 0.003953703
train_loss: 0.0032386193
test_loss: 0.004076515
train_loss: 0.0036976095
test_loss: 0.0042083035
train_loss: 0.003200599
test_loss: 0.0038717333
train_loss: 0.003669896
test_loss: 0.0039634537
train_loss: 0.0036293578
test_loss: 0.0041042273
train_loss: 0.0035273824
test_loss: 0.00402706
train_loss: 0.0034000925
test_loss: 0.0039601573
train_loss: 0.0032257442
test_loss: 0.0039587985
train_loss: 0.0031617791
test_loss: 0.0041256854
train_loss: 0.0032776278
test_loss: 0.004033732
train_loss: 0.003489577
test_loss: 0.0039704284
train_loss: 0.0032581529
test_loss: 0.0038812568
train_loss: 0.003302399
test_loss: 0.0038640324
train_loss: 0.0031706307
test_loss: 0.0038777892
train_loss: 0.0032950114
test_loss: 0.003842349
train_loss: 0.0035696058
test_loss: 0.003902768
train_loss: 0.0030589092
test_loss: 0.003805045
train_loss: 0.0032939606
test_loss: 0.003905695
train_loss: 0.0032471388
test_loss: 0.0039141746
train_loss: 0.0032466738
test_loss: 0.003913614
train_loss: 0.0033660373
test_loss: 0.0039042698
train_loss: 0.0031094453
test_loss: 0.003895641
train_loss: 0.0033132443
test_loss: 0.0039754156
train_loss: 0.0030440064
test_loss: 0.0038598785
train_loss: 0.003345397
test_loss: 0.0037141573
train_loss: 0.0031742766
test_loss: 0.0037122394
train_loss: 0.002975363
test_loss: 0.0038592075
train_loss: 0.0033824132
test_loss: 0.0038594436
train_loss: 0.0031651778
test_loss: 0.003784422
train_loss: 0.0029288712
test_loss: 0.0037089777
train_loss: 0.0028629424
test_loss: 0.003744931
train_loss: 0.0030417724
test_loss: 0.0038431708
train_loss: 0.0032339122
test_loss: 0.00375414
train_loss: 0.0032499414
test_loss: 0.0038089429
train_loss: 0.0033793258
test_loss: 0.0040466976
train_loss: 0.0033395006
test_loss: 0.0042521097
train_loss: 0.0033296042
test_loss: 0.003989337
train_loss: 0.0030479324
test_loss: 0.0037267958
train_loss: 0.003040888
test_loss: 0.0038123918
train_loss: 0.002938806
test_loss: 0.00356653
train_loss: 0.0032026
test_loss: 0.0036538597
train_loss: 0.0030390634
test_loss: 0.003685207
train_loss: 0.0032275766
test_loss: 0.0037612417
train_loss: 0.0029279138
test_loss: 0.0037155706
train_loss: 0.0031224648
test_loss: 0.0037791994
train_loss: 0.0031286594
test_loss: 0.0038725052
train_loss: 0.0031681855
test_loss: 0.0037410152
train_loss: 0.0031848645
test_loss: 0.0038372234
train_loss: 0.00307099
test_loss: 0.0036107765
train_loss: 0.002901545
test_loss: 0.003613769
train_loss: 0.0029660857
test_loss: 0.0035885198
train_loss: 0.0028820797
test_loss: 0.0038036003
train_loss: 0.0030736523
test_loss: 0.0035439965
train_loss: 0.0030246992
test_loss: 0.0037498293
train_loss: 0.0032962984
test_loss: 0.004160189
train_loss: 0.003458164
test_loss: 0.0039314656
train_loss: 0.0030575683
test_loss: 0.0037646634
train_loss: 0.003350905
test_loss: 0.0037582116
train_loss: 0.0031277672
test_loss: 0.0036485742
train_loss: 0.002863029
test_loss: 0.0036082736
train_loss: 0.0029652487
test_loss: 0.0036596362
train_loss: 0.0030650645
test_loss: 0.0036598444
train_loss: 0.0029050077
test_loss: 0.0038145753
train_loss: 0.003032151
test_loss: 0.0036894337
train_loss: 0.002973441
test_loss: 0.0036701972
train_loss: 0.002881941
test_loss: 0.0036679185
train_loss: 0.003000666
test_loss: 0.003624764
train_loss: 0.0033266996
test_loss: 0.0036752517
train_loss: 0.0029354435
test_loss: 0.0035241474
train_loss: 0.0028345035
test_loss: 0.0036741
train_loss: 0.0030226128
test_loss: 0.0036759083
train_loss: 0.0028988435
test_loss: 0.0038330383
train_loss: 0.0031913763
test_loss: 0.0036865815
train_loss: 0.003189894
test_loss: 0.003715343
train_loss: 0.0027654222
test_loss: 0.0035954476
train_loss: 0.003150758
test_loss: 0.0036764215
train_loss: 0.0030954543
test_loss: 0.0037503575
train_loss: 0.003275795
test_loss: 0.003607842
train_loss: 0.0035222836
test_loss: 0.0039272304
train_loss: 0.0033486062
test_loss: 0.0039387015
train_loss: 0.0029770704
test_loss: 0.0035907377
train_loss: 0.0030225338
test_loss: 0.0036193426
train_loss: 0.0031252555
test_loss: 0.0037949462
train_loss: 0.002927986
test_loss: 0.003545851
train_loss: 0.0031897714
test_loss: 0.0036758287
train_loss: 0.00304558
test_loss: 0.0035952167
train_loss: 0.0028907005
test_loss: 0.0035703809
train_loss: 0.0028551607
test_loss: 0.003632332
train_loss: 0.0028625177
test_loss: 0.0037777887
train_loss: 0.003197608
test_loss: 0.0036930216
train_loss: 0.003063746
test_loss: 0.0036270597
train_loss: 0.0029527394
test_loss: 0.003598627
train_loss: 0.0030734306
test_loss: 0.0035080684
train_loss: 0.0031148943
test_loss: 0.0036569056
train_loss: 0.0031162426
test_loss: 0.0036965352
train_loss: 0.002793735
test_loss: 0.0035429797
train_loss: 0.0030805566
test_loss: 0.0035965508
train_loss: 0.0029374138
test_loss: 0.0035569181
train_loss: 0.0028767865
test_loss: 0.0035153716
train_loss: 0.0027027573
test_loss: 0.003510368
train_loss: 0.0029938358
test_loss: 0.0036478892
train_loss: 0.002877255
test_loss: 0.0035644085
train_loss: 0.0029315865
test_loss: 0.0035883836
train_loss: 0.003148341
test_loss: 0.0036027245
train_loss: 0.0028622858
test_loss: 0.0037333136
train_loss: 0.002933751
test_loss: 0.003489398
train_loss: 0.0028089767
test_loss: 0.003548242
train_loss: 0.003005623
test_loss: 0.003625554
train_loss: 0.0028554625
test_loss: 0.0035614474
train_loss: 0.0030609185
test_loss: 0.0037922242
train_loss: 0.002963445
test_loss: 0.0034892834
train_loss: 0.0030901483
test_loss: 0.0036154513
train_loss: 0.0031753043
test_loss: 0.0037633495
train_loss: 0.0033824865
test_loss: 0.0036542956
train_loss: 0.0033060913
test_loss: 0.003656802
train_loss: 0.0032626255
test_loss: 0.0036742769
train_loss: 0.002852629
test_loss: 0.0037284396
train_loss: 0.0032796473
test_loss: 0.0036807293
train_loss: 0.00271149
test_loss: 0.003553167
train_loss: 0.0026753661
test_loss: 0.00345967
train_loss: 0.0026685856
test_loss: 0.0035688998
train_loss: 0.0031835968
test_loss: 0.0036233529
train_loss: 0.0029525298
test_loss: 0.0036478597
train_loss: 0.0029213473
test_loss: 0.0037102895
train_loss: 0.002891866
test_loss: 0.003683831
train_loss: 0.0029333204
test_loss: 0.0035722926
train_loss: 0.0029431519
test_loss: 0.0038019489
train_loss: 0.003229297
test_loss: 0.003790273
train_loss: 0.0029963767
test_loss: 0.0035170175
train_loss: 0.0030789382
test_loss: 0.0035909554
train_loss: 0.00312832
test_loss: 0.0034677628
train_loss: 0.0031166554
test_loss: 0.0036411923
train_loss: 0.0029026808
test_loss: 0.0036306987
train_loss: 0.0029749214
test_loss: 0.0036327266
train_loss: 0.0028737045
test_loss: 0.0038808207
train_loss: 0.0028568497
test_loss: 0.0035378635
train_loss: 0.002744075
test_loss: 0.0035089094
train_loss: 0.0029269224
test_loss: 0.0035761988
train_loss: 0.0031937184
test_loss: 0.0036028004
train_loss: 0.003247986
test_loss: 0.0036943448
train_loss: 0.0027181203
test_loss: 0.0036904525
train_loss: 0.0027926848
test_loss: 0.003474862
train_loss: 0.0026403589
test_loss: 0.0035099678
train_loss: 0.0030719098
test_loss: 0.003603512
train_loss: 0.0027578715
test_loss: 0.0035831982
train_loss: 0.0029677262
test_loss: 0.0035835824
train_loss: 0.0028936062
test_loss: 0.0035495725
train_loss: 0.0028784804
test_loss: 0.0036186944
train_loss: 0.002779987
test_loss: 0.0035799758
train_loss: 0.0031392942
test_loss: 0.0036591557
train_loss: 0.0028588283
test_loss: 0.003376408
train_loss: 0.002852135
test_loss: 0.0035133874
train_loss: 0.0027411848
test_loss: 0.003571226
train_loss: 0.0028520087
test_loss: 0.0035053839
train_loss: 0.0029995476
test_loss: 0.0035636427
train_loss: 0.002719915
test_loss: 0.003424328
train_loss: 0.0029842432
test_loss: 0.0036014493
train_loss: 0.0026673824
test_loss: 0.0036272835
train_loss: 0.0029081032
test_loss: 0.0035063066
train_loss: 0.0027544403
test_loss: 0.0034714208
train_loss: 0.0026343542
test_loss: 0.0035003081
train_loss: 0.0028731618
test_loss: 0.0033882926
train_loss: 0.0027693326
test_loss: 0.0035169336
train_loss: 0.0028663522
test_loss: 0.0034112083
train_loss: 0.002747799
test_loss: 0.003552817
train_loss: 0.002837382
test_loss: 0.0034938997
train_loss: 0.0029429472
test_loss: 0.0035976171
train_loss: 0.0027898212
test_loss: 0.0034525385
train_loss: 0.0027208833
test_loss: 0.0034761978
train_loss: 0.002852776
test_loss: 0.003529151
train_loss: 0.002935764
test_loss: 0.0036161134
train_loss: 0.0027843132
test_loss: 0.0034322976
train_loss: 0.0027609617
test_loss: 0.0035088062
train_loss: 0.0026829098
test_loss: 0.0036042384
train_loss: 0.0032645655
test_loss: 0.0035259088
train_loss: 0.0029549436
test_loss: 0.0036564092
train_loss: 0.0029499857
test_loss: 0.003631931
train_loss: 0.002750425
test_loss: 0.003483084
train_loss: 0.0028624614
test_loss: 0.0036002004
train_loss: 0.0030468656
test_loss: 0.0036573964
train_loss: 0.0028833176
test_loss: 0.0034793476
train_loss: 0.0028541868
test_loss: 0.0034817238
train_loss: 0.0028768494
test_loss: 0.0036347879
train_loss: 0.0031572252
test_loss: 0.0037191287
train_loss: 0.002836349
test_loss: 0.0036435435
train_loss: 0.0029057432
test_loss: 0.003509931
train_loss: 0.0029617993
test_loss: 0.0035664958
train_loss: 0.0026869
test_loss: 0.0034349007
train_loss: 0.002738635
test_loss: 0.0035018462
train_loss: 0.002650505
test_loss: 0.003415637
train_loss: 0.002967069
test_loss: 0.0037233087
train_loss: 0.0029728513
test_loss: 0.0036340826
train_loss: 0.0029142955
test_loss: 0.003645205
train_loss: 0.0026007365
test_loss: 0.0035181392
train_loss: 0.0027762263
test_loss: 0.003365362
train_loss: 0.0025828886
test_loss: 0.0034622103
train_loss: 0.0026776171
test_loss: 0.003530883
train_loss: 0.0028911894
test_loss: 0.003568275
train_loss: 0.0031026264
test_loss: 0.0036909545
train_loss: 0.0033109118
test_loss: 0.0036241328
train_loss: 0.0029827282
test_loss: 0.0038153597
train_loss: 0.0027518698
test_loss: 0.0035452298
train_loss: 0.0026707705
test_loss: 0.0034615532
train_loss: 0.0027483148
test_loss: 0.0034717747
train_loss: 0.0029308568
test_loss: 0.003680856
train_loss: 0.003044969
test_loss: 0.0034636213
train_loss: 0.002953954
test_loss: 0.0037542987
train_loss: 0.003003291
test_loss: 0.003823113
train_loss: 0.0031732689
test_loss: 0.003720542
train_loss: 0.003035988
test_loss: 0.0036456601
train_loss: 0.0027841162
test_loss: 0.0038518365
train_loss: 0.0030715931
test_loss: 0.0036400694
train_loss: 0.0032187318
test_loss: 0.0037891697
train_loss: 0.0028598248
test_loss: 0.003537194
train_loss: 0.0027437173
test_loss: 0.0033845971
train_loss: 0.0027679687
test_loss: 0.0036416869
train_loss: 0.003135328
test_loss: 0.0040364508
train_loss: 0.0027697433
test_loss: 0.0035608374
train_loss: 0.0030193096
test_loss: 0.003545773
train_loss: 0.002925136
test_loss: 0.0035090137
train_loss: 0.0028583899
test_loss: 0.003519357
train_loss: 0.0029421297
test_loss: 0.0035916783
train_loss: 0.002747239
test_loss: 0.003560001
train_loss: 0.0027080122
test_loss: 0.0033873178
train_loss: 0.0027677165
test_loss: 0.0034754013
train_loss: 0.0031422921
test_loss: 0.0039772647
train_loss: 0.0028983592
test_loss: 0.0037232114
train_loss: 0.0028942048
test_loss: 0.003769643
train_loss: 0.0027787765
test_loss: 0.0035911496
train_loss: 0.0029230262
test_loss: 0.003793312
train_loss: 0.0027649752
test_loss: 0.0036232912
train_loss: 0.0028759588
test_loss: 0.0035029189
train_loss: 0.0026381866
test_loss: 0.0034675244
train_loss: 0.0026246926
test_loss: 0.0034703454
train_loss: 0.0027601318
test_loss: 0.00341854
train_loss: 0.0030260868
test_loss: 0.0035969706
train_loss: 0.0028003943
test_loss: 0.0035250217
train_loss: 0.0027795385
test_loss: 0.0035082148
train_loss: 0.0027664346
test_loss: 0.0034375447
train_loss: 0.0028067513
test_loss: 0.003605116
train_loss: 0.0029763903
test_loss: 0.003496069
train_loss: 0.0026995775
test_loss: 0.0033905134
train_loss: 0.002756687
test_loss: 0.0033996417
train_loss: 0.0026535066
test_loss: 0.0034755028
train_loss: 0.0029052568
test_loss: 0.0034978641
train_loss: 0.0028094107
test_loss: 0.003555657
train_loss: 0.0030502258
test_loss: 0.00364097
train_loss: 0.0028081965
test_loss: 0.0036562171
train_loss: 0.0028560408
test_loss: 0.0035784517
train_loss: 0.0029341998
test_loss: 0.0034889274
train_loss: 0.0027865516
test_loss: 0.0034714502
train_loss: 0.002664049
test_loss: 0.0033623676
train_loss: 0.0028058197
test_loss: 0.0034668718
train_loss: 0.0028034814
test_loss: 0.003758159
train_loss: 0.00271322
test_loss: 0.0035413338
train_loss: 0.002708406
test_loss: 0.0035605552
train_loss: 0.0028300916
test_loss: 0.0035655801
train_loss: 0.002685506
test_loss: 0.0035234015
train_loss: 0.0030162027
test_loss: 0.003438278
train_loss: 0.0029662475
test_loss: 0.003619555
train_loss: 0.0027148128
test_loss: 0.0035323696
train_loss: 0.0026829836
test_loss: 0.0034789871
train_loss: 0.002907959
test_loss: 0.0038307533
train_loss: 0.0027909388
test_loss: 0.0035472647
train_loss: 0.0028285459
test_loss: 0.0035651668
train_loss: 0.002766843
test_loss: 0.0036169433
train_loss: 0.0028579938
test_loss: 0.003465784
train_loss: 0.0028299459
test_loss: 0.0035696423
train_loss: 0.0030124635
test_loss: 0.0035184496
train_loss: 0.0027219562
test_loss: 0.0033826723
train_loss: 0.002661201
test_loss: 0.0033943895
train_loss: 0.0026849462
test_loss: 0.0036153747
train_loss: 0.002888586
test_loss: 0.0035569558
train_loss: 0.0030185392
test_loss: 0.003604468
train_loss: 0.0027685617
test_loss: 0.0036190783
train_loss: 0.002805593
test_loss: 0.00341782
train_loss: 0.0025422175
test_loss: 0.003394879
train_loss: 0.002932062
test_loss: 0.0036453574
train_loss: 0.0028611978
test_loss: 0.003536034
train_loss: 0.0028123066
test_loss: 0.0034506128
train_loss: 0.00291736
test_loss: 0.0035616788
train_loss: 0.00291675
test_loss: 0.0036218492
train_loss: 0.0028547787
test_loss: 0.0037921644
train_loss: 0.0027452265
test_loss: 0.0035464007
train_loss: 0.0027861726
test_loss: 0.0036519743
train_loss: 0.002749468
test_loss: 0.0034882275
train_loss: 0.0027963126
test_loss: 0.003562525
train_loss: 0.0028435031
test_loss: 0.003572203
train_loss: 0.002677355
test_loss: 0.0035496447
train_loss: 0.0027444197
test_loss: 0.0034572629
train_loss: 0.0027560666
test_loss: 0.003414849
train_loss: 0.0027662767
test_loss: 0.003385235
train_loss: 0.002556981
test_loss: 0.003335892
train_loss: 0.002625689
test_loss: 0.0033425698
train_loss: 0.002791469
test_loss: 0.0034735631
train_loss: 0.0028822634
test_loss: 0.0034684893
train_loss: 0.0026253588
test_loss: 0.0033473806
train_loss: 0.0025504394
test_loss: 0.003384721
train_loss: 0.0028322095
test_loss: 0.0034621053
train_loss: 0.0027670169
test_loss: 0.0034692176
train_loss: 0.0027754628
test_loss: 0.0034996413
train_loss: 0.002820869
test_loss: 0.0035934863
train_loss: 0.0030362855
test_loss: 0.003546401
train_loss: 0.0030920883
test_loss: 0.003591036
train_loss: 0.0027525814
test_loss: 0.0035640462
train_loss: 0.0027084043
test_loss: 0.0034442917
train_loss: 0.002838762
test_loss: 0.0035925875
train_loss: 0.0026209902
test_loss: 0.0035216324
train_loss: 0.0027898934
test_loss: 0.0034359181
train_loss: 0.0029634715
test_loss: 0.003556626
train_loss: 0.0027387897
test_loss: 0.0035732682
train_loss: 0.002908708
test_loss: 0.0036236658
train_loss: 0.0030117137
test_loss: 0.0035683836
train_loss: 0.0025604859
test_loss: 0.003514366
train_loss: 0.002510483
test_loss: 0.0033008154
train_loss: 0.002830075
test_loss: 0.003543705
train_loss: 0.0029278002
test_loss: 0.0036412177
train_loss: 0.0028854022
test_loss: 0.0035969247
train_loss: 0.002672387
test_loss: 0.0035165
train_loss: 0.0026935318
test_loss: 0.0034021817
train_loss: 0.0028068204
test_loss: 0.0034845236
train_loss: 0.0026685237
test_loss: 0.0034821932
train_loss: 0.0028340076
test_loss: 0.0034771326
train_loss: 0.0026680012
test_loss: 0.0033820367
train_loss: 0.0027026166
test_loss: 0.0033927374
train_loss: 0.002621938
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0033718268
train_loss: 0.0028831642
test_loss: 0.0036163565
train_loss: 0.0028631624
test_loss: 0.003777802
train_loss: 0.002583895
test_loss: 0.0033533284
train_loss: 0.0027490614
test_loss: 0.003468036
train_loss: 0.0027345512
test_loss: 0.003370081
train_loss: 0.002703107
test_loss: 0.0034602731
train_loss: 0.0026799769
test_loss: 0.0033333814
train_loss: 0.0025981641
test_loss: 0.0035516394
train_loss: 0.0029105826
test_loss: 0.0035868532
train_loss: 0.002624886
test_loss: 0.003348778
train_loss: 0.0027439469
test_loss: 0.0035794894
train_loss: 0.0029758476
test_loss: 0.0036566532
train_loss: 0.002848931
test_loss: 0.0035978635
train_loss: 0.0026759403
test_loss: 0.0034384218
train_loss: 0.0028853333
test_loss: 0.0035702263
train_loss: 0.0028339908
test_loss: 0.0034181934
train_loss: 0.0029688873
test_loss: 0.0035700086
train_loss: 0.0028755302
test_loss: 0.003391671
train_loss: 0.0026547275
test_loss: 0.0035549547
train_loss: 0.0026906854
test_loss: 0.0034214174
train_loss: 0.0026764479
test_loss: 0.0035095455
train_loss: 0.0027557183
test_loss: 0.0035445194
train_loss: 0.0025431612
test_loss: 0.003316021
train_loss: 0.0025826297
test_loss: 0.0035065247
train_loss: 0.0027827797
test_loss: 0.0034740416
train_loss: 0.0029307697
test_loss: 0.0034419857
train_loss: 0.0026256875
test_loss: 0.0033539638
train_loss: 0.0025865885
test_loss: 0.00334915
train_loss: 0.0026383854
test_loss: 0.0034623016
train_loss: 0.0026779487
test_loss: 0.0033359984
train_loss: 0.0027340276
test_loss: 0.003374681
train_loss: 0.0026981414
test_loss: 0.0034982287
train_loss: 0.0026017786
test_loss: 0.003395537
train_loss: 0.0026805154
test_loss: 0.0036029115
train_loss: 0.0029659495
test_loss: 0.0036738839
train_loss: 0.0029576651
test_loss: 0.0035396868
train_loss: 0.0027964418
test_loss: 0.0034693992
train_loss: 0.0030361924
test_loss: 0.003419796
train_loss: 0.002599189
test_loss: 0.0036259864
train_loss: 0.0026632852
test_loss: 0.003484903
train_loss: 0.0027521858
test_loss: 0.0035516552
train_loss: 0.0030925511
test_loss: 0.0034198042
train_loss: 0.0030966988
test_loss: 0.0037768583
train_loss: 0.0029135533
test_loss: 0.0035740172
train_loss: 0.0029283806
test_loss: 0.0036423337
train_loss: 0.0026666597
test_loss: 0.003600704
train_loss: 0.002763742
test_loss: 0.0034406537
train_loss: 0.0026708941
test_loss: 0.00335256
train_loss: 0.0027775895
test_loss: 0.0033719984
train_loss: 0.0027703005
test_loss: 0.003506965
train_loss: 0.002602586
test_loss: 0.0035044774
train_loss: 0.0025832744
test_loss: 0.003363342
train_loss: 0.0028076593
test_loss: 0.0036327387
train_loss: 0.0028053906
test_loss: 0.0034676034
train_loss: 0.0026996483
test_loss: 0.0035479185
train_loss: 0.0026676415
test_loss: 0.003548929
train_loss: 0.0026622093
test_loss: 0.0034754404
train_loss: 0.0027137618
test_loss: 0.003487335
train_loss: 0.0025696382
test_loss: 0.0034455708
train_loss: 0.00280922
test_loss: 0.003439209
train_loss: 0.0026961837
test_loss: 0.0035228934
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e95f32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e95f3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e965c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e9562b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e94ec1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e95017b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e94b29d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e947e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e947e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e947e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e9476950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e978d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e940c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93d4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93d4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93a1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93a0400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93359d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92e7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e9310f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92b4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92b4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d48906a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92ba158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92b4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d4859488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d4806598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d482a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d47b7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d47e0d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d477d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d477da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d477dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d47547b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d477d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d46c5048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.32208734e-05
Iter: 2 loss: 1.09611901e-05
Iter: 3 loss: 1.07958094e-05
Iter: 4 loss: 9.96090785e-06
Iter: 5 loss: 1.14697614e-05
Iter: 6 loss: 9.59993395e-06
Iter: 7 loss: 8.78153332e-06
Iter: 8 loss: 1.47096916e-05
Iter: 9 loss: 8.71147586e-06
Iter: 10 loss: 8.30887802e-06
Iter: 11 loss: 8.07907691e-06
Iter: 12 loss: 7.90594277e-06
Iter: 13 loss: 7.35863296e-06
Iter: 14 loss: 8.42571444e-06
Iter: 15 loss: 7.13198961e-06
Iter: 16 loss: 6.63300398e-06
Iter: 17 loss: 1.38318337e-05
Iter: 18 loss: 6.63202536e-06
Iter: 19 loss: 6.37626772e-06
Iter: 20 loss: 6.2019908e-06
Iter: 21 loss: 6.10775623e-06
Iter: 22 loss: 5.68936048e-06
Iter: 23 loss: 6.19517732e-06
Iter: 24 loss: 5.46993488e-06
Iter: 25 loss: 5.36160223e-06
Iter: 26 loss: 5.30999023e-06
Iter: 27 loss: 5.16716545e-06
Iter: 28 loss: 4.89863623e-06
Iter: 29 loss: 1.08347658e-05
Iter: 30 loss: 4.89791455e-06
Iter: 31 loss: 4.61728587e-06
Iter: 32 loss: 5.67663346e-06
Iter: 33 loss: 4.54993415e-06
Iter: 34 loss: 4.32851175e-06
Iter: 35 loss: 4.29438569e-06
Iter: 36 loss: 4.14077158e-06
Iter: 37 loss: 3.84357872e-06
Iter: 38 loss: 5.31209844e-06
Iter: 39 loss: 3.79297626e-06
Iter: 40 loss: 3.57782915e-06
Iter: 41 loss: 5.10667496e-06
Iter: 42 loss: 3.55861289e-06
Iter: 43 loss: 3.39978556e-06
Iter: 44 loss: 5.11470353e-06
Iter: 45 loss: 3.39612e-06
Iter: 46 loss: 3.28133092e-06
Iter: 47 loss: 3.41964369e-06
Iter: 48 loss: 3.22107576e-06
Iter: 49 loss: 3.12973043e-06
Iter: 50 loss: 3.31936553e-06
Iter: 51 loss: 3.093337e-06
Iter: 52 loss: 3.00165675e-06
Iter: 53 loss: 4.24167501e-06
Iter: 54 loss: 3.00118541e-06
Iter: 55 loss: 2.94764322e-06
Iter: 56 loss: 2.96327e-06
Iter: 57 loss: 2.90899561e-06
Iter: 58 loss: 2.82303699e-06
Iter: 59 loss: 3.22491223e-06
Iter: 60 loss: 2.80706445e-06
Iter: 61 loss: 2.75553703e-06
Iter: 62 loss: 2.74355398e-06
Iter: 63 loss: 2.71044064e-06
Iter: 64 loss: 2.6327757e-06
Iter: 65 loss: 2.6175926e-06
Iter: 66 loss: 2.56590556e-06
Iter: 67 loss: 2.51287156e-06
Iter: 68 loss: 2.5035838e-06
Iter: 69 loss: 2.46320838e-06
Iter: 70 loss: 2.4090964e-06
Iter: 71 loss: 2.40621898e-06
Iter: 72 loss: 2.3430855e-06
Iter: 73 loss: 2.47880325e-06
Iter: 74 loss: 2.31860759e-06
Iter: 75 loss: 2.25322765e-06
Iter: 76 loss: 2.26344832e-06
Iter: 77 loss: 2.20386642e-06
Iter: 78 loss: 2.15033378e-06
Iter: 79 loss: 2.70899773e-06
Iter: 80 loss: 2.14893566e-06
Iter: 81 loss: 2.10100779e-06
Iter: 82 loss: 2.27160353e-06
Iter: 83 loss: 2.08869596e-06
Iter: 84 loss: 2.05159427e-06
Iter: 85 loss: 2.55137593e-06
Iter: 86 loss: 2.05139213e-06
Iter: 87 loss: 2.02834235e-06
Iter: 88 loss: 1.99461761e-06
Iter: 89 loss: 1.99366195e-06
Iter: 90 loss: 1.97211716e-06
Iter: 91 loss: 1.9698773e-06
Iter: 92 loss: 1.94647714e-06
Iter: 93 loss: 1.9267577e-06
Iter: 94 loss: 1.9203394e-06
Iter: 95 loss: 1.89493971e-06
Iter: 96 loss: 1.89478521e-06
Iter: 97 loss: 1.8747462e-06
Iter: 98 loss: 1.85834074e-06
Iter: 99 loss: 1.85248359e-06
Iter: 100 loss: 1.82979193e-06
Iter: 101 loss: 1.81854557e-06
Iter: 102 loss: 1.80761344e-06
Iter: 103 loss: 1.77651145e-06
Iter: 104 loss: 2.12115197e-06
Iter: 105 loss: 1.77588072e-06
Iter: 106 loss: 1.76005983e-06
Iter: 107 loss: 1.92702055e-06
Iter: 108 loss: 1.75967295e-06
Iter: 109 loss: 1.74611569e-06
Iter: 110 loss: 1.71861336e-06
Iter: 111 loss: 2.22029303e-06
Iter: 112 loss: 1.71811735e-06
Iter: 113 loss: 1.68880024e-06
Iter: 114 loss: 1.84478517e-06
Iter: 115 loss: 1.68422889e-06
Iter: 116 loss: 1.66238624e-06
Iter: 117 loss: 1.65788867e-06
Iter: 118 loss: 1.64346829e-06
Iter: 119 loss: 1.62506285e-06
Iter: 120 loss: 1.62468314e-06
Iter: 121 loss: 1.606821e-06
Iter: 122 loss: 1.6522331e-06
Iter: 123 loss: 1.60067873e-06
Iter: 124 loss: 1.58914645e-06
Iter: 125 loss: 1.59261378e-06
Iter: 126 loss: 1.58087641e-06
Iter: 127 loss: 1.56063413e-06
Iter: 128 loss: 1.69223404e-06
Iter: 129 loss: 1.55847681e-06
Iter: 130 loss: 1.54799079e-06
Iter: 131 loss: 1.58904299e-06
Iter: 132 loss: 1.54559791e-06
Iter: 133 loss: 1.53250619e-06
Iter: 134 loss: 1.52899702e-06
Iter: 135 loss: 1.52086261e-06
Iter: 136 loss: 1.5078034e-06
Iter: 137 loss: 1.49593029e-06
Iter: 138 loss: 1.49262269e-06
Iter: 139 loss: 1.47207902e-06
Iter: 140 loss: 1.60128684e-06
Iter: 141 loss: 1.46975503e-06
Iter: 142 loss: 1.45778199e-06
Iter: 143 loss: 1.45775709e-06
Iter: 144 loss: 1.448376e-06
Iter: 145 loss: 1.43408602e-06
Iter: 146 loss: 1.43386239e-06
Iter: 147 loss: 1.41958162e-06
Iter: 148 loss: 1.51175141e-06
Iter: 149 loss: 1.41798296e-06
Iter: 150 loss: 1.40533689e-06
Iter: 151 loss: 1.40044506e-06
Iter: 152 loss: 1.39356291e-06
Iter: 153 loss: 1.38081089e-06
Iter: 154 loss: 1.46525917e-06
Iter: 155 loss: 1.37955419e-06
Iter: 156 loss: 1.36930669e-06
Iter: 157 loss: 1.47549326e-06
Iter: 158 loss: 1.36900735e-06
Iter: 159 loss: 1.36108338e-06
Iter: 160 loss: 1.37655479e-06
Iter: 161 loss: 1.35785149e-06
Iter: 162 loss: 1.35268738e-06
Iter: 163 loss: 1.41098349e-06
Iter: 164 loss: 1.35261234e-06
Iter: 165 loss: 1.34695551e-06
Iter: 166 loss: 1.33899789e-06
Iter: 167 loss: 1.33866945e-06
Iter: 168 loss: 1.33185404e-06
Iter: 169 loss: 1.33178696e-06
Iter: 170 loss: 1.32752552e-06
Iter: 171 loss: 1.31566446e-06
Iter: 172 loss: 1.3813858e-06
Iter: 173 loss: 1.31219213e-06
Iter: 174 loss: 1.29876821e-06
Iter: 175 loss: 1.42010686e-06
Iter: 176 loss: 1.29809223e-06
Iter: 177 loss: 1.29049909e-06
Iter: 178 loss: 1.34012078e-06
Iter: 179 loss: 1.28963268e-06
Iter: 180 loss: 1.28088936e-06
Iter: 181 loss: 1.30044839e-06
Iter: 182 loss: 1.27751605e-06
Iter: 183 loss: 1.27233579e-06
Iter: 184 loss: 1.27208978e-06
Iter: 185 loss: 1.26810028e-06
Iter: 186 loss: 1.25966949e-06
Iter: 187 loss: 1.2618566e-06
Iter: 188 loss: 1.2534781e-06
Iter: 189 loss: 1.24698977e-06
Iter: 190 loss: 1.24693111e-06
Iter: 191 loss: 1.24145913e-06
Iter: 192 loss: 1.23995483e-06
Iter: 193 loss: 1.23664631e-06
Iter: 194 loss: 1.22767869e-06
Iter: 195 loss: 1.30580315e-06
Iter: 196 loss: 1.22718484e-06
Iter: 197 loss: 1.22282e-06
Iter: 198 loss: 1.25000247e-06
Iter: 199 loss: 1.22232063e-06
Iter: 200 loss: 1.21747939e-06
Iter: 201 loss: 1.21666335e-06
Iter: 202 loss: 1.21340622e-06
Iter: 203 loss: 1.20841514e-06
Iter: 204 loss: 1.24588951e-06
Iter: 205 loss: 1.20801747e-06
Iter: 206 loss: 1.20335949e-06
Iter: 207 loss: 1.1990852e-06
Iter: 208 loss: 1.1979987e-06
Iter: 209 loss: 1.1923654e-06
Iter: 210 loss: 1.19620336e-06
Iter: 211 loss: 1.18884896e-06
Iter: 212 loss: 1.18207265e-06
Iter: 213 loss: 1.20570348e-06
Iter: 214 loss: 1.18034495e-06
Iter: 215 loss: 1.17336606e-06
Iter: 216 loss: 1.24799487e-06
Iter: 217 loss: 1.17322134e-06
Iter: 218 loss: 1.16907654e-06
Iter: 219 loss: 1.16276942e-06
Iter: 220 loss: 1.16264562e-06
Iter: 221 loss: 1.15549506e-06
Iter: 222 loss: 1.1650875e-06
Iter: 223 loss: 1.15194348e-06
Iter: 224 loss: 1.14316822e-06
Iter: 225 loss: 1.19189076e-06
Iter: 226 loss: 1.14193631e-06
Iter: 227 loss: 1.13706244e-06
Iter: 228 loss: 1.1895014e-06
Iter: 229 loss: 1.13694909e-06
Iter: 230 loss: 1.13208944e-06
Iter: 231 loss: 1.13935516e-06
Iter: 232 loss: 1.12970793e-06
Iter: 233 loss: 1.12590783e-06
Iter: 234 loss: 1.18215007e-06
Iter: 235 loss: 1.12592863e-06
Iter: 236 loss: 1.12344514e-06
Iter: 237 loss: 1.12526072e-06
Iter: 238 loss: 1.12196722e-06
Iter: 239 loss: 1.11884799e-06
Iter: 240 loss: 1.1220784e-06
Iter: 241 loss: 1.11716406e-06
Iter: 242 loss: 1.11322026e-06
Iter: 243 loss: 1.12585178e-06
Iter: 244 loss: 1.11209465e-06
Iter: 245 loss: 1.10858423e-06
Iter: 246 loss: 1.1018459e-06
Iter: 247 loss: 1.24692019e-06
Iter: 248 loss: 1.10187943e-06
Iter: 249 loss: 1.09669088e-06
Iter: 250 loss: 1.14539341e-06
Iter: 251 loss: 1.09647863e-06
Iter: 252 loss: 1.09168468e-06
Iter: 253 loss: 1.12823489e-06
Iter: 254 loss: 1.09135192e-06
Iter: 255 loss: 1.0876297e-06
Iter: 256 loss: 1.08930692e-06
Iter: 257 loss: 1.08505947e-06
Iter: 258 loss: 1.0816043e-06
Iter: 259 loss: 1.07742517e-06
Iter: 260 loss: 1.0769952e-06
Iter: 261 loss: 1.07095866e-06
Iter: 262 loss: 1.10840858e-06
Iter: 263 loss: 1.07027665e-06
Iter: 264 loss: 1.06507264e-06
Iter: 265 loss: 1.08440463e-06
Iter: 266 loss: 1.06381958e-06
Iter: 267 loss: 1.05965228e-06
Iter: 268 loss: 1.05961988e-06
Iter: 269 loss: 1.05742379e-06
Iter: 270 loss: 1.06357106e-06
Iter: 271 loss: 1.05670574e-06
Iter: 272 loss: 1.05389358e-06
Iter: 273 loss: 1.05229947e-06
Iter: 274 loss: 1.05100253e-06
Iter: 275 loss: 1.04803485e-06
Iter: 276 loss: 1.07597077e-06
Iter: 277 loss: 1.04790706e-06
Iter: 278 loss: 1.04565891e-06
Iter: 279 loss: 1.04750688e-06
Iter: 280 loss: 1.04435514e-06
Iter: 281 loss: 1.04153014e-06
Iter: 282 loss: 1.03846901e-06
Iter: 283 loss: 1.03799368e-06
Iter: 284 loss: 1.03440505e-06
Iter: 285 loss: 1.07766937e-06
Iter: 286 loss: 1.03437378e-06
Iter: 287 loss: 1.03182595e-06
Iter: 288 loss: 1.03805746e-06
Iter: 289 loss: 1.0309177e-06
Iter: 290 loss: 1.02739318e-06
Iter: 291 loss: 1.03633352e-06
Iter: 292 loss: 1.02611023e-06
Iter: 293 loss: 1.02344165e-06
Iter: 294 loss: 1.01890851e-06
Iter: 295 loss: 1.01890373e-06
Iter: 296 loss: 1.0143915e-06
Iter: 297 loss: 1.04039293e-06
Iter: 298 loss: 1.01379555e-06
Iter: 299 loss: 1.01028729e-06
Iter: 300 loss: 1.02865579e-06
Iter: 301 loss: 1.00972716e-06
Iter: 302 loss: 1.00741045e-06
Iter: 303 loss: 1.0073785e-06
Iter: 304 loss: 1.00598891e-06
Iter: 305 loss: 1.00757245e-06
Iter: 306 loss: 1.00525699e-06
Iter: 307 loss: 1.00355169e-06
Iter: 308 loss: 1.00707371e-06
Iter: 309 loss: 1.00283103e-06
Iter: 310 loss: 1.00113675e-06
Iter: 311 loss: 1.00027307e-06
Iter: 312 loss: 9.99496e-07
Iter: 313 loss: 9.96480821e-07
Iter: 314 loss: 1.01509022e-06
Iter: 315 loss: 9.96097469e-07
Iter: 316 loss: 9.94271545e-07
Iter: 317 loss: 9.90219633e-07
Iter: 318 loss: 1.04900244e-06
Iter: 319 loss: 9.90051149e-07
Iter: 320 loss: 9.86195118e-07
Iter: 321 loss: 9.86191935e-07
Iter: 322 loss: 9.84414214e-07
Iter: 323 loss: 1.0016912e-06
Iter: 324 loss: 9.84367148e-07
Iter: 325 loss: 9.82682309e-07
Iter: 326 loss: 9.82427764e-07
Iter: 327 loss: 9.81248604e-07
Iter: 328 loss: 9.79077186e-07
Iter: 329 loss: 9.8299131e-07
Iter: 330 loss: 9.78148364e-07
Iter: 331 loss: 9.75793e-07
Iter: 332 loss: 9.75068815e-07
Iter: 333 loss: 9.73688543e-07
Iter: 334 loss: 9.70673113e-07
Iter: 335 loss: 9.79123797e-07
Iter: 336 loss: 9.69719622e-07
Iter: 337 loss: 9.68532277e-07
Iter: 338 loss: 9.67832e-07
Iter: 339 loss: 9.66566176e-07
Iter: 340 loss: 9.65246159e-07
Iter: 341 loss: 9.65027084e-07
Iter: 342 loss: 9.63038929e-07
Iter: 343 loss: 9.76647584e-07
Iter: 344 loss: 9.62873287e-07
Iter: 345 loss: 9.61238243e-07
Iter: 346 loss: 9.5946541e-07
Iter: 347 loss: 9.59227918e-07
Iter: 348 loss: 9.57402e-07
Iter: 349 loss: 9.57396196e-07
Iter: 350 loss: 9.56174745e-07
Iter: 351 loss: 9.54263896e-07
Iter: 352 loss: 9.54200232e-07
Iter: 353 loss: 9.51857487e-07
Iter: 354 loss: 9.57783e-07
Iter: 355 loss: 9.51066227e-07
Iter: 356 loss: 9.48698926e-07
Iter: 357 loss: 9.57716225e-07
Iter: 358 loss: 9.48143452e-07
Iter: 359 loss: 9.45680313e-07
Iter: 360 loss: 9.67553092e-07
Iter: 361 loss: 9.45572708e-07
Iter: 362 loss: 9.44364956e-07
Iter: 363 loss: 9.41646e-07
Iter: 364 loss: 9.79780111e-07
Iter: 365 loss: 9.41510848e-07
Iter: 366 loss: 9.39199424e-07
Iter: 367 loss: 9.71714826e-07
Iter: 368 loss: 9.39192e-07
Iter: 369 loss: 9.3734559e-07
Iter: 370 loss: 9.38332164e-07
Iter: 371 loss: 9.36151594e-07
Iter: 372 loss: 9.35228115e-07
Iter: 373 loss: 9.34934292e-07
Iter: 374 loss: 9.33854494e-07
Iter: 375 loss: 9.33877402e-07
Iter: 376 loss: 9.33003435e-07
Iter: 377 loss: 9.31792215e-07
Iter: 378 loss: 9.32928856e-07
Iter: 379 loss: 9.31069053e-07
Iter: 380 loss: 9.29033e-07
Iter: 381 loss: 9.31335308e-07
Iter: 382 loss: 9.2789378e-07
Iter: 383 loss: 9.2665465e-07
Iter: 384 loss: 9.31342868e-07
Iter: 385 loss: 9.26346274e-07
Iter: 386 loss: 9.24540871e-07
Iter: 387 loss: 9.2320289e-07
Iter: 388 loss: 9.22627123e-07
Iter: 389 loss: 9.20734067e-07
Iter: 390 loss: 9.22189201e-07
Iter: 391 loss: 9.1953973e-07
Iter: 392 loss: 9.17030377e-07
Iter: 393 loss: 9.22659069e-07
Iter: 394 loss: 9.16088084e-07
Iter: 395 loss: 9.13667805e-07
Iter: 396 loss: 9.26709561e-07
Iter: 397 loss: 9.13311794e-07
Iter: 398 loss: 9.11972108e-07
Iter: 399 loss: 9.11949428e-07
Iter: 400 loss: 9.10675681e-07
Iter: 401 loss: 9.08113066e-07
Iter: 402 loss: 9.55786732e-07
Iter: 403 loss: 9.08065658e-07
Iter: 404 loss: 9.05644583e-07
Iter: 405 loss: 9.12351e-07
Iter: 406 loss: 9.04869864e-07
Iter: 407 loss: 9.0272539e-07
Iter: 408 loss: 9.13573e-07
Iter: 409 loss: 9.02372108e-07
Iter: 410 loss: 9.01149178e-07
Iter: 411 loss: 9.01130079e-07
Iter: 412 loss: 8.99770043e-07
Iter: 413 loss: 9.00573923e-07
Iter: 414 loss: 8.98939561e-07
Iter: 415 loss: 8.97969699e-07
Iter: 416 loss: 8.98073949e-07
Iter: 417 loss: 8.97249379e-07
Iter: 418 loss: 8.95679534e-07
Iter: 419 loss: 9.03133127e-07
Iter: 420 loss: 8.95375251e-07
Iter: 421 loss: 8.93920514e-07
Iter: 422 loss: 8.9763995e-07
Iter: 423 loss: 8.93395395e-07
Iter: 424 loss: 8.9248681e-07
Iter: 425 loss: 8.9269696e-07
Iter: 426 loss: 8.91789512e-07
Iter: 427 loss: 8.90278102e-07
Iter: 428 loss: 8.96776385e-07
Iter: 429 loss: 8.89958301e-07
Iter: 430 loss: 8.88402099e-07
Iter: 431 loss: 8.88121122e-07
Iter: 432 loss: 8.87059059e-07
Iter: 433 loss: 8.8533136e-07
Iter: 434 loss: 8.8770787e-07
Iter: 435 loss: 8.84520489e-07
Iter: 436 loss: 8.83e-07
Iter: 437 loss: 8.91826403e-07
Iter: 438 loss: 8.82825418e-07
Iter: 439 loss: 8.81263702e-07
Iter: 440 loss: 8.93621348e-07
Iter: 441 loss: 8.81150697e-07
Iter: 442 loss: 8.80062657e-07
Iter: 443 loss: 8.80345567e-07
Iter: 444 loss: 8.79307606e-07
Iter: 445 loss: 8.78158346e-07
Iter: 446 loss: 8.76202819e-07
Iter: 447 loss: 8.76200033e-07
Iter: 448 loss: 8.76848276e-07
Iter: 449 loss: 8.75380351e-07
Iter: 450 loss: 8.74533725e-07
Iter: 451 loss: 8.74095406e-07
Iter: 452 loss: 8.73672889e-07
Iter: 453 loss: 8.72518513e-07
Iter: 454 loss: 8.70944291e-07
Iter: 455 loss: 8.70850272e-07
Iter: 456 loss: 8.69944301e-07
Iter: 457 loss: 8.69872281e-07
Iter: 458 loss: 8.68772759e-07
Iter: 459 loss: 8.68363202e-07
Iter: 460 loss: 8.67767369e-07
Iter: 461 loss: 8.66674156e-07
Iter: 462 loss: 8.7013791e-07
Iter: 463 loss: 8.66318373e-07
Iter: 464 loss: 8.64949925e-07
Iter: 465 loss: 8.65885852e-07
Iter: 466 loss: 8.64117055e-07
Iter: 467 loss: 8.6259729e-07
Iter: 468 loss: 8.78257424e-07
Iter: 469 loss: 8.62572904e-07
Iter: 470 loss: 8.61732e-07
Iter: 471 loss: 8.60188322e-07
Iter: 472 loss: 8.95568746e-07
Iter: 473 loss: 8.60173429e-07
Iter: 474 loss: 8.58461647e-07
Iter: 475 loss: 8.72134535e-07
Iter: 476 loss: 8.58337103e-07
Iter: 477 loss: 8.56826773e-07
Iter: 478 loss: 8.6970249e-07
Iter: 479 loss: 8.56758675e-07
Iter: 480 loss: 8.55935411e-07
Iter: 481 loss: 8.54855898e-07
Iter: 482 loss: 8.54788254e-07
Iter: 483 loss: 8.53388e-07
Iter: 484 loss: 8.5905657e-07
Iter: 485 loss: 8.53071469e-07
Iter: 486 loss: 8.52603193e-07
Iter: 487 loss: 8.52401911e-07
Iter: 488 loss: 8.51894697e-07
Iter: 489 loss: 8.50503966e-07
Iter: 490 loss: 8.59166789e-07
Iter: 491 loss: 8.50152446e-07
Iter: 492 loss: 8.48729599e-07
Iter: 493 loss: 8.5312729e-07
Iter: 494 loss: 8.48298e-07
Iter: 495 loss: 8.47107117e-07
Iter: 496 loss: 8.47113711e-07
Iter: 497 loss: 8.46293347e-07
Iter: 498 loss: 8.45683928e-07
Iter: 499 loss: 8.45394197e-07
Iter: 500 loss: 8.4437437e-07
Iter: 501 loss: 8.44571787e-07
Iter: 502 loss: 8.43661383e-07
Iter: 503 loss: 8.4215128e-07
Iter: 504 loss: 8.56788915e-07
Iter: 505 loss: 8.42095176e-07
Iter: 506 loss: 8.41205349e-07
Iter: 507 loss: 8.4246318e-07
Iter: 508 loss: 8.40791756e-07
Iter: 509 loss: 8.39801601e-07
Iter: 510 loss: 8.39143866e-07
Iter: 511 loss: 8.38742153e-07
Iter: 512 loss: 8.3727457e-07
Iter: 513 loss: 8.52333301e-07
Iter: 514 loss: 8.37216419e-07
Iter: 515 loss: 8.36211711e-07
Iter: 516 loss: 8.43435828e-07
Iter: 517 loss: 8.36141567e-07
Iter: 518 loss: 8.35307219e-07
Iter: 519 loss: 8.33936e-07
Iter: 520 loss: 8.33962758e-07
Iter: 521 loss: 8.32939577e-07
Iter: 522 loss: 8.47222111e-07
Iter: 523 loss: 8.32934177e-07
Iter: 524 loss: 8.31843863e-07
Iter: 525 loss: 8.36435106e-07
Iter: 526 loss: 8.31616148e-07
Iter: 527 loss: 8.31091029e-07
Iter: 528 loss: 8.30063925e-07
Iter: 529 loss: 8.52870471e-07
Iter: 530 loss: 8.30053295e-07
Iter: 531 loss: 8.28677344e-07
Iter: 532 loss: 8.30738827e-07
Iter: 533 loss: 8.28002044e-07
Iter: 534 loss: 8.27462372e-07
Iter: 535 loss: 8.27179633e-07
Iter: 536 loss: 8.26752967e-07
Iter: 537 loss: 8.2548388e-07
Iter: 538 loss: 8.30683121e-07
Iter: 539 loss: 8.24998892e-07
Iter: 540 loss: 8.23770051e-07
Iter: 541 loss: 8.43333453e-07
Iter: 542 loss: 8.23780567e-07
Iter: 543 loss: 8.22836171e-07
Iter: 544 loss: 8.27873123e-07
Iter: 545 loss: 8.22681784e-07
Iter: 546 loss: 8.21704646e-07
Iter: 547 loss: 8.20933906e-07
Iter: 548 loss: 8.20618425e-07
Iter: 549 loss: 8.19471097e-07
Iter: 550 loss: 8.22944855e-07
Iter: 551 loss: 8.19163233e-07
Iter: 552 loss: 8.1801079e-07
Iter: 553 loss: 8.32037699e-07
Iter: 554 loss: 8.17977707e-07
Iter: 555 loss: 8.17254545e-07
Iter: 556 loss: 8.1733458e-07
Iter: 557 loss: 8.16696456e-07
Iter: 558 loss: 8.15623423e-07
Iter: 559 loss: 8.16272404e-07
Iter: 560 loss: 8.14923112e-07
Iter: 561 loss: 8.14397822e-07
Iter: 562 loss: 8.14187558e-07
Iter: 563 loss: 8.13683471e-07
Iter: 564 loss: 8.12540293e-07
Iter: 565 loss: 8.29744351e-07
Iter: 566 loss: 8.12461963e-07
Iter: 567 loss: 8.11373752e-07
Iter: 568 loss: 8.12024155e-07
Iter: 569 loss: 8.1064286e-07
Iter: 570 loss: 8.09379912e-07
Iter: 571 loss: 8.12744e-07
Iter: 572 loss: 8.08948812e-07
Iter: 573 loss: 8.08348659e-07
Iter: 574 loss: 8.08196546e-07
Iter: 575 loss: 8.07568824e-07
Iter: 576 loss: 8.06934963e-07
Iter: 577 loss: 8.06838784e-07
Iter: 578 loss: 8.05885861e-07
Iter: 579 loss: 8.05634102e-07
Iter: 580 loss: 8.05053901e-07
Iter: 581 loss: 8.04081424e-07
Iter: 582 loss: 8.04064086e-07
Iter: 583 loss: 8.03297667e-07
Iter: 584 loss: 8.04052888e-07
Iter: 585 loss: 8.02844397e-07
Iter: 586 loss: 8.02020168e-07
Iter: 587 loss: 8.01240105e-07
Iter: 588 loss: 8.01085321e-07
Iter: 589 loss: 8.00187e-07
Iter: 590 loss: 8.00073281e-07
Iter: 591 loss: 7.99449e-07
Iter: 592 loss: 7.9871711e-07
Iter: 593 loss: 7.98600126e-07
Iter: 594 loss: 7.97634357e-07
Iter: 595 loss: 8.04420779e-07
Iter: 596 loss: 7.97546e-07
Iter: 597 loss: 7.96567747e-07
Iter: 598 loss: 8.02610202e-07
Iter: 599 loss: 7.96456561e-07
Iter: 600 loss: 7.95903702e-07
Iter: 601 loss: 7.94923494e-07
Iter: 602 loss: 7.94917e-07
Iter: 603 loss: 7.93652646e-07
Iter: 604 loss: 7.96728045e-07
Iter: 605 loss: 7.93155891e-07
Iter: 606 loss: 7.91978948e-07
Iter: 607 loss: 7.92164542e-07
Iter: 608 loss: 7.91035745e-07
Iter: 609 loss: 7.90932745e-07
Iter: 610 loss: 7.90455886e-07
Iter: 611 loss: 7.89866704e-07
Iter: 612 loss: 7.88559532e-07
Iter: 613 loss: 8.08437903e-07
Iter: 614 loss: 7.88508544e-07
Iter: 615 loss: 7.8736349e-07
Iter: 616 loss: 7.94200503e-07
Iter: 617 loss: 7.87206147e-07
Iter: 618 loss: 7.8644922e-07
Iter: 619 loss: 7.91778689e-07
Iter: 620 loss: 7.86334851e-07
Iter: 621 loss: 7.85515567e-07
Iter: 622 loss: 7.85682403e-07
Iter: 623 loss: 7.8489029e-07
Iter: 624 loss: 7.84045369e-07
Iter: 625 loss: 7.8405418e-07
Iter: 626 loss: 7.83365465e-07
Iter: 627 loss: 7.82253437e-07
Iter: 628 loss: 7.822523e-07
Iter: 629 loss: 7.81765152e-07
Iter: 630 loss: 7.8078051e-07
Iter: 631 loss: 8.00715725e-07
Iter: 632 loss: 7.80773462e-07
Iter: 633 loss: 7.8008128e-07
Iter: 634 loss: 7.80026653e-07
Iter: 635 loss: 7.79352376e-07
Iter: 636 loss: 7.79564e-07
Iter: 637 loss: 7.78904791e-07
Iter: 638 loss: 7.78365916e-07
Iter: 639 loss: 7.77271907e-07
Iter: 640 loss: 7.97065e-07
Iter: 641 loss: 7.77258265e-07
Iter: 642 loss: 7.76186482e-07
Iter: 643 loss: 7.91953312e-07
Iter: 644 loss: 7.76195293e-07
Iter: 645 loss: 7.75403805e-07
Iter: 646 loss: 7.78865058e-07
Iter: 647 loss: 7.75255899e-07
Iter: 648 loss: 7.74272735e-07
Iter: 649 loss: 7.74216232e-07
Iter: 650 loss: 7.73503473e-07
Iter: 651 loss: 7.7259233e-07
Iter: 652 loss: 7.73409454e-07
Iter: 653 loss: 7.72047e-07
Iter: 654 loss: 7.71088366e-07
Iter: 655 loss: 7.7732841e-07
Iter: 656 loss: 7.70964959e-07
Iter: 657 loss: 7.7007445e-07
Iter: 658 loss: 7.74981913e-07
Iter: 659 loss: 7.69936662e-07
Iter: 660 loss: 7.69278813e-07
Iter: 661 loss: 7.68757104e-07
Iter: 662 loss: 7.6855514e-07
Iter: 663 loss: 7.67754045e-07
Iter: 664 loss: 7.79093284e-07
Iter: 665 loss: 7.6774711e-07
Iter: 666 loss: 7.6697313e-07
Iter: 667 loss: 7.67938332e-07
Iter: 668 loss: 7.66584776e-07
Iter: 669 loss: 7.6616459e-07
Iter: 670 loss: 7.71202622e-07
Iter: 671 loss: 7.66160952e-07
Iter: 672 loss: 7.65669142e-07
Iter: 673 loss: 7.64588e-07
Iter: 674 loss: 7.8125646e-07
Iter: 675 loss: 7.64550293e-07
Iter: 676 loss: 7.63460093e-07
Iter: 677 loss: 7.6677577e-07
Iter: 678 loss: 7.63190428e-07
Iter: 679 loss: 7.62342779e-07
Iter: 680 loss: 7.63573951e-07
Iter: 681 loss: 7.61943681e-07
Iter: 682 loss: 7.61051183e-07
Iter: 683 loss: 7.75217302e-07
Iter: 684 loss: 7.61058743e-07
Iter: 685 loss: 7.60582566e-07
Iter: 686 loss: 7.6030426e-07
Iter: 687 loss: 7.60091439e-07
Iter: 688 loss: 7.59315583e-07
Iter: 689 loss: 7.59816089e-07
Iter: 690 loss: 7.58846454e-07
Iter: 691 loss: 7.57917178e-07
Iter: 692 loss: 7.6121745e-07
Iter: 693 loss: 7.57703901e-07
Iter: 694 loss: 7.56829706e-07
Iter: 695 loss: 7.63845435e-07
Iter: 696 loss: 7.56749671e-07
Iter: 697 loss: 7.56068857e-07
Iter: 698 loss: 7.56254508e-07
Iter: 699 loss: 7.55590804e-07
Iter: 700 loss: 7.54887196e-07
Iter: 701 loss: 7.56885242e-07
Iter: 702 loss: 7.54667212e-07
Iter: 703 loss: 7.53678478e-07
Iter: 704 loss: 7.56984775e-07
Iter: 705 loss: 7.53421205e-07
Iter: 706 loss: 7.52957703e-07
Iter: 707 loss: 7.59902264e-07
Iter: 708 loss: 7.52958613e-07
Iter: 709 loss: 7.52540473e-07
Iter: 710 loss: 7.51786956e-07
Iter: 711 loss: 7.51782522e-07
Iter: 712 loss: 7.5088667e-07
Iter: 713 loss: 7.50739e-07
Iter: 714 loss: 7.50134859e-07
Iter: 715 loss: 7.4925606e-07
Iter: 716 loss: 7.52716062e-07
Iter: 717 loss: 7.49067908e-07
Iter: 718 loss: 7.48267e-07
Iter: 719 loss: 7.48279945e-07
Iter: 720 loss: 7.4778211e-07
Iter: 721 loss: 7.46902856e-07
Iter: 722 loss: 7.67893255e-07
Iter: 723 loss: 7.46901094e-07
Iter: 724 loss: 7.4595431e-07
Iter: 725 loss: 7.51266214e-07
Iter: 726 loss: 7.45817317e-07
Iter: 727 loss: 7.45077955e-07
Iter: 728 loss: 7.47281206e-07
Iter: 729 loss: 7.44890258e-07
Iter: 730 loss: 7.44135491e-07
Iter: 731 loss: 7.48586899e-07
Iter: 732 loss: 7.44051e-07
Iter: 733 loss: 7.43425858e-07
Iter: 734 loss: 7.44607689e-07
Iter: 735 loss: 7.43210535e-07
Iter: 736 loss: 7.42653128e-07
Iter: 737 loss: 7.44146746e-07
Iter: 738 loss: 7.42467876e-07
Iter: 739 loss: 7.41872668e-07
Iter: 740 loss: 7.46273884e-07
Iter: 741 loss: 7.41804115e-07
Iter: 742 loss: 7.41425424e-07
Iter: 743 loss: 7.42715315e-07
Iter: 744 loss: 7.41323902e-07
Iter: 745 loss: 7.40825953e-07
Iter: 746 loss: 7.40433563e-07
Iter: 747 loss: 7.40257803e-07
Iter: 748 loss: 7.39647419e-07
Iter: 749 loss: 7.3890476e-07
Iter: 750 loss: 7.38827453e-07
Iter: 751 loss: 7.37666653e-07
Iter: 752 loss: 7.42724865e-07
Iter: 753 loss: 7.37456389e-07
Iter: 754 loss: 7.36782169e-07
Iter: 755 loss: 7.36757499e-07
Iter: 756 loss: 7.36202651e-07
Iter: 757 loss: 7.3574239e-07
Iter: 758 loss: 7.35571234e-07
Iter: 759 loss: 7.34949e-07
Iter: 760 loss: 7.3447444e-07
Iter: 761 loss: 7.34254968e-07
Iter: 762 loss: 7.33469051e-07
Iter: 763 loss: 7.46246656e-07
Iter: 764 loss: 7.3347411e-07
Iter: 765 loss: 7.32871626e-07
Iter: 766 loss: 7.34744901e-07
Iter: 767 loss: 7.32687454e-07
Iter: 768 loss: 7.31940531e-07
Iter: 769 loss: 7.32963827e-07
Iter: 770 loss: 7.3158418e-07
Iter: 771 loss: 7.31002331e-07
Iter: 772 loss: 7.3429203e-07
Iter: 773 loss: 7.30941565e-07
Iter: 774 loss: 7.30286615e-07
Iter: 775 loss: 7.32294779e-07
Iter: 776 loss: 7.3012535e-07
Iter: 777 loss: 7.2948751e-07
Iter: 778 loss: 7.31447813e-07
Iter: 779 loss: 7.29304e-07
Iter: 780 loss: 7.28735131e-07
Iter: 781 loss: 7.30021952e-07
Iter: 782 loss: 7.28545501e-07
Iter: 783 loss: 7.28043744e-07
Iter: 784 loss: 7.27054555e-07
Iter: 785 loss: 7.44983822e-07
Iter: 786 loss: 7.27038355e-07
Iter: 787 loss: 7.26161716e-07
Iter: 788 loss: 7.335085e-07
Iter: 789 loss: 7.261159e-07
Iter: 790 loss: 7.25380175e-07
Iter: 791 loss: 7.2867374e-07
Iter: 792 loss: 7.25262169e-07
Iter: 793 loss: 7.24336246e-07
Iter: 794 loss: 7.26569e-07
Iter: 795 loss: 7.23999506e-07
Iter: 796 loss: 7.23479388e-07
Iter: 797 loss: 7.22792663e-07
Iter: 798 loss: 7.22756909e-07
Iter: 799 loss: 7.21910283e-07
Iter: 800 loss: 7.27325528e-07
Iter: 801 loss: 7.21807226e-07
Iter: 802 loss: 7.21257607e-07
Iter: 803 loss: 7.23864844e-07
Iter: 804 loss: 7.21163701e-07
Iter: 805 loss: 7.20402056e-07
Iter: 806 loss: 7.22139134e-07
Iter: 807 loss: 7.201063e-07
Iter: 808 loss: 7.19547245e-07
Iter: 809 loss: 7.20399328e-07
Iter: 810 loss: 7.19305433e-07
Iter: 811 loss: 7.18500928e-07
Iter: 812 loss: 7.22408117e-07
Iter: 813 loss: 7.18359843e-07
Iter: 814 loss: 7.17758269e-07
Iter: 815 loss: 7.19295826e-07
Iter: 816 loss: 7.17536068e-07
Iter: 817 loss: 7.16956663e-07
Iter: 818 loss: 7.1846e-07
Iter: 819 loss: 7.16765612e-07
Iter: 820 loss: 7.1623424e-07
Iter: 821 loss: 7.1558793e-07
Iter: 822 loss: 7.15519377e-07
Iter: 823 loss: 7.1468304e-07
Iter: 824 loss: 7.19302079e-07
Iter: 825 loss: 7.14520183e-07
Iter: 826 loss: 7.13880922e-07
Iter: 827 loss: 7.15679732e-07
Iter: 828 loss: 7.13677082e-07
Iter: 829 loss: 7.13152701e-07
Iter: 830 loss: 7.13140366e-07
Iter: 831 loss: 7.12731946e-07
Iter: 832 loss: 7.1184968e-07
Iter: 833 loss: 7.26830535e-07
Iter: 834 loss: 7.11813868e-07
Iter: 835 loss: 7.10981169e-07
Iter: 836 loss: 7.12761903e-07
Iter: 837 loss: 7.10657673e-07
Iter: 838 loss: 7.0982e-07
Iter: 839 loss: 7.14968394e-07
Iter: 840 loss: 7.09676669e-07
Iter: 841 loss: 7.08921561e-07
Iter: 842 loss: 7.15065255e-07
Iter: 843 loss: 7.08874381e-07
Iter: 844 loss: 7.08308221e-07
Iter: 845 loss: 7.08941116e-07
Iter: 846 loss: 7.08056803e-07
Iter: 847 loss: 7.07674644e-07
Iter: 848 loss: 7.07638492e-07
Iter: 849 loss: 7.07418167e-07
Iter: 850 loss: 7.06902142e-07
Iter: 851 loss: 7.16909312e-07
Iter: 852 loss: 7.06909077e-07
Iter: 853 loss: 7.06222636e-07
Iter: 854 loss: 7.0892429e-07
Iter: 855 loss: 7.06041192e-07
Iter: 856 loss: 7.05606055e-07
Iter: 857 loss: 7.05182401e-07
Iter: 858 loss: 7.05093498e-07
Iter: 859 loss: 7.04276488e-07
Iter: 860 loss: 7.07192839e-07
Iter: 861 loss: 7.04082595e-07
Iter: 862 loss: 7.03378532e-07
Iter: 863 loss: 7.05329512e-07
Iter: 864 loss: 7.03134333e-07
Iter: 865 loss: 7.02379452e-07
Iter: 866 loss: 7.09492269e-07
Iter: 867 loss: 7.02363366e-07
Iter: 868 loss: 7.01900888e-07
Iter: 869 loss: 7.01768101e-07
Iter: 870 loss: 7.01482918e-07
Iter: 871 loss: 7.00803412e-07
Iter: 872 loss: 6.99981115e-07
Iter: 873 loss: 6.99914949e-07
Iter: 874 loss: 6.99439056e-07
Iter: 875 loss: 6.99358452e-07
Iter: 876 loss: 6.98910071e-07
Iter: 877 loss: 7.00948704e-07
Iter: 878 loss: 6.98799056e-07
Iter: 879 loss: 6.98344877e-07
Iter: 880 loss: 6.98572705e-07
Iter: 881 loss: 6.98056567e-07
Iter: 882 loss: 6.97406335e-07
Iter: 883 loss: 7.0213008e-07
Iter: 884 loss: 6.97367341e-07
Iter: 885 loss: 6.97108874e-07
Iter: 886 loss: 6.96991e-07
Iter: 887 loss: 6.96822667e-07
Iter: 888 loss: 6.9619216e-07
Iter: 889 loss: 6.96249344e-07
Iter: 890 loss: 6.95697963e-07
Iter: 891 loss: 6.9492927e-07
Iter: 892 loss: 6.95120093e-07
Iter: 893 loss: 6.9441694e-07
Iter: 894 loss: 6.9356804e-07
Iter: 895 loss: 6.99338045e-07
Iter: 896 loss: 6.93490904e-07
Iter: 897 loss: 6.93022798e-07
Iter: 898 loss: 6.98804456e-07
Iter: 899 loss: 6.93020525e-07
Iter: 900 loss: 6.92647177e-07
Iter: 901 loss: 6.92985395e-07
Iter: 902 loss: 6.92411334e-07
Iter: 903 loss: 6.91893604e-07
Iter: 904 loss: 6.91535263e-07
Iter: 905 loss: 6.91362857e-07
Iter: 906 loss: 6.90599165e-07
Iter: 907 loss: 6.90988941e-07
Iter: 908 loss: 6.90159254e-07
Iter: 909 loss: 6.89496e-07
Iter: 910 loss: 6.99784209e-07
Iter: 911 loss: 6.89495835e-07
Iter: 912 loss: 6.8889949e-07
Iter: 913 loss: 6.91769628e-07
Iter: 914 loss: 6.88788532e-07
Iter: 915 loss: 6.88304397e-07
Iter: 916 loss: 6.90205809e-07
Iter: 917 loss: 6.88197304e-07
Iter: 918 loss: 6.87631029e-07
Iter: 919 loss: 6.88156717e-07
Iter: 920 loss: 6.87303213e-07
Iter: 921 loss: 6.86853e-07
Iter: 922 loss: 6.87546446e-07
Iter: 923 loss: 6.86629164e-07
Iter: 924 loss: 6.85990926e-07
Iter: 925 loss: 6.87594365e-07
Iter: 926 loss: 6.85772591e-07
Iter: 927 loss: 6.8528152e-07
Iter: 928 loss: 6.84614236e-07
Iter: 929 loss: 6.84578538e-07
Iter: 930 loss: 6.83835935e-07
Iter: 931 loss: 6.90713705e-07
Iter: 932 loss: 6.83802455e-07
Iter: 933 loss: 6.83176e-07
Iter: 934 loss: 6.88519e-07
Iter: 935 loss: 6.83143867e-07
Iter: 936 loss: 6.82694917e-07
Iter: 937 loss: 6.82857547e-07
Iter: 938 loss: 6.82388759e-07
Iter: 939 loss: 6.81812253e-07
Iter: 940 loss: 6.816515e-07
Iter: 941 loss: 6.81308e-07
Iter: 942 loss: 6.80507924e-07
Iter: 943 loss: 6.83578833e-07
Iter: 944 loss: 6.8030721e-07
Iter: 945 loss: 6.79705295e-07
Iter: 946 loss: 6.83205201e-07
Iter: 947 loss: 6.79641403e-07
Iter: 948 loss: 6.79020332e-07
Iter: 949 loss: 6.82998291e-07
Iter: 950 loss: 6.78958713e-07
Iter: 951 loss: 6.78597189e-07
Iter: 952 loss: 6.8060524e-07
Iter: 953 loss: 6.78542278e-07
Iter: 954 loss: 6.78128686e-07
Iter: 955 loss: 6.77226637e-07
Iter: 956 loss: 6.89718149e-07
Iter: 957 loss: 6.77185e-07
Iter: 958 loss: 6.76750687e-07
Iter: 959 loss: 6.76711352e-07
Iter: 960 loss: 6.76337663e-07
Iter: 961 loss: 6.75995352e-07
Iter: 962 loss: 6.75910258e-07
Iter: 963 loss: 6.75292029e-07
Iter: 964 loss: 6.75647129e-07
Iter: 965 loss: 6.74874855e-07
Iter: 966 loss: 6.74299883e-07
Iter: 967 loss: 6.78282731e-07
Iter: 968 loss: 6.74213652e-07
Iter: 969 loss: 6.73542161e-07
Iter: 970 loss: 6.76495404e-07
Iter: 971 loss: 6.73415173e-07
Iter: 972 loss: 6.72948772e-07
Iter: 973 loss: 6.730628e-07
Iter: 974 loss: 6.72573492e-07
Iter: 975 loss: 6.72008468e-07
Iter: 976 loss: 6.72658643e-07
Iter: 977 loss: 6.71743805e-07
Iter: 978 loss: 6.71045655e-07
Iter: 979 loss: 6.73518798e-07
Iter: 980 loss: 6.70856195e-07
Iter: 981 loss: 6.70339375e-07
Iter: 982 loss: 6.73894874e-07
Iter: 983 loss: 6.70304075e-07
Iter: 984 loss: 6.6965913e-07
Iter: 985 loss: 6.71068051e-07
Iter: 986 loss: 6.69446763e-07
Iter: 987 loss: 6.69026747e-07
Iter: 988 loss: 6.73890099e-07
Iter: 989 loss: 6.69042208e-07
Iter: 990 loss: 6.68809264e-07
Iter: 991 loss: 6.68395842e-07
Iter: 992 loss: 6.68393909e-07
Iter: 993 loss: 6.67888e-07
Iter: 994 loss: 6.70340967e-07
Iter: 995 loss: 6.67779091e-07
Iter: 996 loss: 6.67183826e-07
Iter: 997 loss: 6.67183144e-07
Iter: 998 loss: 6.66721e-07
Iter: 999 loss: 6.6610346e-07
Iter: 1000 loss: 6.67098561e-07
Iter: 1001 loss: 6.65797643e-07
Iter: 1002 loss: 6.65284347e-07
Iter: 1003 loss: 6.6912844e-07
Iter: 1004 loss: 6.65248876e-07
Iter: 1005 loss: 6.64647473e-07
Iter: 1006 loss: 6.66090955e-07
Iter: 1007 loss: 6.64450226e-07
Iter: 1008 loss: 6.64039646e-07
Iter: 1009 loss: 6.63579954e-07
Iter: 1010 loss: 6.63507763e-07
Iter: 1011 loss: 6.62776756e-07
Iter: 1012 loss: 6.67116922e-07
Iter: 1013 loss: 6.62672619e-07
Iter: 1014 loss: 6.62108732e-07
Iter: 1015 loss: 6.62587e-07
Iter: 1016 loss: 6.61720605e-07
Iter: 1017 loss: 6.61317245e-07
Iter: 1018 loss: 6.61260515e-07
Iter: 1019 loss: 6.60910189e-07
Iter: 1020 loss: 6.61508807e-07
Iter: 1021 loss: 6.60729938e-07
Iter: 1022 loss: 6.60362502e-07
Iter: 1023 loss: 6.61396484e-07
Iter: 1024 loss: 6.60263936e-07
Iter: 1025 loss: 6.59900138e-07
Iter: 1026 loss: 6.59543389e-07
Iter: 1027 loss: 6.59454145e-07
Iter: 1028 loss: 6.59042541e-07
Iter: 1029 loss: 6.63097353e-07
Iter: 1030 loss: 6.59009629e-07
Iter: 1031 loss: 6.58571707e-07
Iter: 1032 loss: 6.58028398e-07
Iter: 1033 loss: 6.57954672e-07
Iter: 1034 loss: 6.57327575e-07
Iter: 1035 loss: 6.59866373e-07
Iter: 1036 loss: 6.57203486e-07
Iter: 1037 loss: 6.56754196e-07
Iter: 1038 loss: 6.62976561e-07
Iter: 1039 loss: 6.56759084e-07
Iter: 1040 loss: 6.56321617e-07
Iter: 1041 loss: 6.56223165e-07
Iter: 1042 loss: 6.55963e-07
Iter: 1043 loss: 6.55476072e-07
Iter: 1044 loss: 6.54805604e-07
Iter: 1045 loss: 6.54742053e-07
Iter: 1046 loss: 6.54197265e-07
Iter: 1047 loss: 6.54217274e-07
Iter: 1048 loss: 6.53730808e-07
Iter: 1049 loss: 6.54011899e-07
Iter: 1050 loss: 6.53410439e-07
Iter: 1051 loss: 6.52868096e-07
Iter: 1052 loss: 6.52881454e-07
Iter: 1053 loss: 6.52549488e-07
Iter: 1054 loss: 6.52698418e-07
Iter: 1055 loss: 6.52304152e-07
Iter: 1056 loss: 6.51883454e-07
Iter: 1057 loss: 6.5274935e-07
Iter: 1058 loss: 6.51675919e-07
Iter: 1059 loss: 6.51223e-07
Iter: 1060 loss: 6.50936101e-07
Iter: 1061 loss: 6.50733114e-07
Iter: 1062 loss: 6.50326115e-07
Iter: 1063 loss: 6.50334187e-07
Iter: 1064 loss: 6.49978801e-07
Iter: 1065 loss: 6.49612275e-07
Iter: 1066 loss: 6.49566971e-07
Iter: 1067 loss: 6.49011895e-07
Iter: 1068 loss: 6.49885578e-07
Iter: 1069 loss: 6.48766161e-07
Iter: 1070 loss: 6.48239279e-07
Iter: 1071 loss: 6.48233936e-07
Iter: 1072 loss: 6.47817103e-07
Iter: 1073 loss: 6.4725441e-07
Iter: 1074 loss: 6.47204388e-07
Iter: 1075 loss: 6.46637e-07
Iter: 1076 loss: 6.46966782e-07
Iter: 1077 loss: 6.46272156e-07
Iter: 1078 loss: 6.4557338e-07
Iter: 1079 loss: 6.50054744e-07
Iter: 1080 loss: 6.45528473e-07
Iter: 1081 loss: 6.45213106e-07
Iter: 1082 loss: 6.45225043e-07
Iter: 1083 loss: 6.44912802e-07
Iter: 1084 loss: 6.45602199e-07
Iter: 1085 loss: 6.44787576e-07
Iter: 1086 loss: 6.44495117e-07
Iter: 1087 loss: 6.44758302e-07
Iter: 1088 loss: 6.44294232e-07
Iter: 1089 loss: 6.43893259e-07
Iter: 1090 loss: 6.44460499e-07
Iter: 1091 loss: 6.43689532e-07
Iter: 1092 loss: 6.43258772e-07
Iter: 1093 loss: 6.43854321e-07
Iter: 1094 loss: 6.43059252e-07
Iter: 1095 loss: 6.42631448e-07
Iter: 1096 loss: 6.44932527e-07
Iter: 1097 loss: 6.42559144e-07
Iter: 1098 loss: 6.42107523e-07
Iter: 1099 loss: 6.42236046e-07
Iter: 1100 loss: 6.41768167e-07
Iter: 1101 loss: 6.4125453e-07
Iter: 1102 loss: 6.40937e-07
Iter: 1103 loss: 6.4074e-07
Iter: 1104 loss: 6.40402391e-07
Iter: 1105 loss: 6.4026608e-07
Iter: 1106 loss: 6.39997779e-07
Iter: 1107 loss: 6.39456971e-07
Iter: 1108 loss: 6.48684534e-07
Iter: 1109 loss: 6.39429174e-07
Iter: 1110 loss: 6.3882834e-07
Iter: 1111 loss: 6.41241513e-07
Iter: 1112 loss: 6.38694758e-07
Iter: 1113 loss: 6.38183508e-07
Iter: 1114 loss: 6.38217557e-07
Iter: 1115 loss: 6.37760536e-07
Iter: 1116 loss: 6.37576136e-07
Iter: 1117 loss: 6.37412e-07
Iter: 1118 loss: 6.37066e-07
Iter: 1119 loss: 6.37178687e-07
Iter: 1120 loss: 6.36841321e-07
Iter: 1121 loss: 6.36410391e-07
Iter: 1122 loss: 6.36574555e-07
Iter: 1123 loss: 6.36121911e-07
Iter: 1124 loss: 6.35572633e-07
Iter: 1125 loss: 6.37630308e-07
Iter: 1126 loss: 6.35436663e-07
Iter: 1127 loss: 6.34994649e-07
Iter: 1128 loss: 6.34852e-07
Iter: 1129 loss: 6.3459089e-07
Iter: 1130 loss: 6.33929631e-07
Iter: 1131 loss: 6.41697511e-07
Iter: 1132 loss: 6.33930426e-07
Iter: 1133 loss: 6.33570153e-07
Iter: 1134 loss: 6.33792922e-07
Iter: 1135 loss: 6.33326067e-07
Iter: 1136 loss: 6.329592e-07
Iter: 1137 loss: 6.33248931e-07
Iter: 1138 loss: 6.32708634e-07
Iter: 1139 loss: 6.32325566e-07
Iter: 1140 loss: 6.32319825e-07
Iter: 1141 loss: 6.32106776e-07
Iter: 1142 loss: 6.31548346e-07
Iter: 1143 loss: 6.35664946e-07
Iter: 1144 loss: 6.3144671e-07
Iter: 1145 loss: 6.30663408e-07
Iter: 1146 loss: 6.32962326e-07
Iter: 1147 loss: 6.30411591e-07
Iter: 1148 loss: 6.29688429e-07
Iter: 1149 loss: 6.32177944e-07
Iter: 1150 loss: 6.29515284e-07
Iter: 1151 loss: 6.29256e-07
Iter: 1152 loss: 6.29126362e-07
Iter: 1153 loss: 6.28908936e-07
Iter: 1154 loss: 6.28571797e-07
Iter: 1155 loss: 6.28582143e-07
Iter: 1156 loss: 6.28128305e-07
Iter: 1157 loss: 6.30034e-07
Iter: 1158 loss: 6.28059411e-07
Iter: 1159 loss: 6.27607278e-07
Iter: 1160 loss: 6.27464772e-07
Iter: 1161 loss: 6.27181407e-07
Iter: 1162 loss: 6.2662042e-07
Iter: 1163 loss: 6.28084194e-07
Iter: 1164 loss: 6.26408337e-07
Iter: 1165 loss: 6.2582933e-07
Iter: 1166 loss: 6.29668136e-07
Iter: 1167 loss: 6.25771747e-07
Iter: 1168 loss: 6.25298071e-07
Iter: 1169 loss: 6.26424e-07
Iter: 1170 loss: 6.25133225e-07
Iter: 1171 loss: 6.24718837e-07
Iter: 1172 loss: 6.24849577e-07
Iter: 1173 loss: 6.24420863e-07
Iter: 1174 loss: 6.23974756e-07
Iter: 1175 loss: 6.3077573e-07
Iter: 1176 loss: 6.23968731e-07
Iter: 1177 loss: 6.23569463e-07
Iter: 1178 loss: 6.23182302e-07
Iter: 1179 loss: 6.23078222e-07
Iter: 1180 loss: 6.22605683e-07
Iter: 1181 loss: 6.23176959e-07
Iter: 1182 loss: 6.22358073e-07
Iter: 1183 loss: 6.21907e-07
Iter: 1184 loss: 6.24765164e-07
Iter: 1185 loss: 6.21839035e-07
Iter: 1186 loss: 6.21554364e-07
Iter: 1187 loss: 6.21539357e-07
Iter: 1188 loss: 6.21316872e-07
Iter: 1189 loss: 6.20819151e-07
Iter: 1190 loss: 6.24223674e-07
Iter: 1191 loss: 6.20695857e-07
Iter: 1192 loss: 6.20302501e-07
Iter: 1193 loss: 6.20260039e-07
Iter: 1194 loss: 6.19987645e-07
Iter: 1195 loss: 6.19926482e-07
Iter: 1196 loss: 6.19729917e-07
Iter: 1197 loss: 6.19288926e-07
Iter: 1198 loss: 6.19151933e-07
Iter: 1199 loss: 6.18899151e-07
Iter: 1200 loss: 6.18450485e-07
Iter: 1201 loss: 6.18437355e-07
Iter: 1202 loss: 6.1815274e-07
Iter: 1203 loss: 6.17889896e-07
Iter: 1204 loss: 6.17826686e-07
Iter: 1205 loss: 6.17278715e-07
Iter: 1206 loss: 6.19232196e-07
Iter: 1207 loss: 6.1718913e-07
Iter: 1208 loss: 6.16734326e-07
Iter: 1209 loss: 6.20890717e-07
Iter: 1210 loss: 6.16724037e-07
Iter: 1211 loss: 6.16320676e-07
Iter: 1212 loss: 6.1594011e-07
Iter: 1213 loss: 6.15894919e-07
Iter: 1214 loss: 6.15372585e-07
Iter: 1215 loss: 6.15137367e-07
Iter: 1216 loss: 6.14858209e-07
Iter: 1217 loss: 6.14409885e-07
Iter: 1218 loss: 6.14360772e-07
Iter: 1219 loss: 6.139623e-07
Iter: 1220 loss: 6.16229727e-07
Iter: 1221 loss: 6.13879081e-07
Iter: 1222 loss: 6.13666884e-07
Iter: 1223 loss: 6.13166321e-07
Iter: 1224 loss: 6.1939329e-07
Iter: 1225 loss: 6.13113457e-07
Iter: 1226 loss: 6.12778194e-07
Iter: 1227 loss: 6.12724e-07
Iter: 1228 loss: 6.12519443e-07
Iter: 1229 loss: 6.12231474e-07
Iter: 1230 loss: 6.12210329e-07
Iter: 1231 loss: 6.11848236e-07
Iter: 1232 loss: 6.14035571e-07
Iter: 1233 loss: 6.11800715e-07
Iter: 1234 loss: 6.11453515e-07
Iter: 1235 loss: 6.12597887e-07
Iter: 1236 loss: 6.11375526e-07
Iter: 1237 loss: 6.11018436e-07
Iter: 1238 loss: 6.10623829e-07
Iter: 1239 loss: 6.10536233e-07
Iter: 1240 loss: 6.10076427e-07
Iter: 1241 loss: 6.15427e-07
Iter: 1242 loss: 6.10071766e-07
Iter: 1243 loss: 6.09675567e-07
Iter: 1244 loss: 6.11361429e-07
Iter: 1245 loss: 6.09607241e-07
Iter: 1246 loss: 6.09237077e-07
Iter: 1247 loss: 6.0880086e-07
Iter: 1248 loss: 6.08750611e-07
Iter: 1249 loss: 6.08267783e-07
Iter: 1250 loss: 6.10069e-07
Iter: 1251 loss: 6.08139544e-07
Iter: 1252 loss: 6.07937864e-07
Iter: 1253 loss: 6.07900688e-07
Iter: 1254 loss: 6.07653e-07
Iter: 1255 loss: 6.07198274e-07
Iter: 1256 loss: 6.16222223e-07
Iter: 1257 loss: 6.07169227e-07
Iter: 1258 loss: 6.06730168e-07
Iter: 1259 loss: 6.08234927e-07
Iter: 1260 loss: 6.06586468e-07
Iter: 1261 loss: 6.06155709e-07
Iter: 1262 loss: 6.10163511e-07
Iter: 1263 loss: 6.06160825e-07
Iter: 1264 loss: 6.05858759e-07
Iter: 1265 loss: 6.05337732e-07
Iter: 1266 loss: 6.05334662e-07
Iter: 1267 loss: 6.04859e-07
Iter: 1268 loss: 6.10540098e-07
Iter: 1269 loss: 6.04867637e-07
Iter: 1270 loss: 6.04433637e-07
Iter: 1271 loss: 6.04858e-07
Iter: 1272 loss: 6.04213142e-07
Iter: 1273 loss: 6.0374856e-07
Iter: 1274 loss: 6.04593481e-07
Iter: 1275 loss: 6.03536307e-07
Iter: 1276 loss: 6.03130616e-07
Iter: 1277 loss: 6.06499043e-07
Iter: 1278 loss: 6.0310316e-07
Iter: 1279 loss: 6.02762725e-07
Iter: 1280 loss: 6.03675687e-07
Iter: 1281 loss: 6.02631076e-07
Iter: 1282 loss: 6.02310195e-07
Iter: 1283 loss: 6.02134264e-07
Iter: 1284 loss: 6.01979878e-07
Iter: 1285 loss: 6.0156367e-07
Iter: 1286 loss: 6.031064e-07
Iter: 1287 loss: 6.01453962e-07
Iter: 1288 loss: 6.01081126e-07
Iter: 1289 loss: 6.01087095e-07
Iter: 1290 loss: 6.0086893e-07
Iter: 1291 loss: 6.00361545e-07
Iter: 1292 loss: 6.05802256e-07
Iter: 1293 loss: 6.00282874e-07
Iter: 1294 loss: 5.99713e-07
Iter: 1295 loss: 6.02746809e-07
Iter: 1296 loss: 5.99644522e-07
Iter: 1297 loss: 5.99212228e-07
Iter: 1298 loss: 6.04732e-07
Iter: 1299 loss: 5.992311e-07
Iter: 1300 loss: 5.98976726e-07
Iter: 1301 loss: 5.98463316e-07
Iter: 1302 loss: 6.06399567e-07
Iter: 1303 loss: 5.98423753e-07
Iter: 1304 loss: 5.98100087e-07
Iter: 1305 loss: 5.98073484e-07
Iter: 1306 loss: 5.97799044e-07
Iter: 1307 loss: 5.97622261e-07
Iter: 1308 loss: 5.97502321e-07
Iter: 1309 loss: 5.97092594e-07
Iter: 1310 loss: 5.98254246e-07
Iter: 1311 loss: 5.96945711e-07
Iter: 1312 loss: 5.96610789e-07
Iter: 1313 loss: 6.00426347e-07
Iter: 1314 loss: 5.9661636e-07
Iter: 1315 loss: 5.963376e-07
Iter: 1316 loss: 5.96184179e-07
Iter: 1317 loss: 5.96047244e-07
Iter: 1318 loss: 5.9564718e-07
Iter: 1319 loss: 5.96280756e-07
Iter: 1320 loss: 5.9544027e-07
Iter: 1321 loss: 5.95058054e-07
Iter: 1322 loss: 5.98325073e-07
Iter: 1323 loss: 5.95052938e-07
Iter: 1324 loss: 5.94661174e-07
Iter: 1325 loss: 5.95810093e-07
Iter: 1326 loss: 5.94556e-07
Iter: 1327 loss: 5.94354e-07
Iter: 1328 loss: 5.93938239e-07
Iter: 1329 loss: 6.01127169e-07
Iter: 1330 loss: 5.93930736e-07
Iter: 1331 loss: 5.93345874e-07
Iter: 1332 loss: 5.96804114e-07
Iter: 1333 loss: 5.9329119e-07
Iter: 1334 loss: 5.92810579e-07
Iter: 1335 loss: 5.96411724e-07
Iter: 1336 loss: 5.92766071e-07
Iter: 1337 loss: 5.92531933e-07
Iter: 1338 loss: 5.92029323e-07
Iter: 1339 loss: 6.02746638e-07
Iter: 1340 loss: 5.92025344e-07
Iter: 1341 loss: 5.91625e-07
Iter: 1342 loss: 5.91604476e-07
Iter: 1343 loss: 5.91260687e-07
Iter: 1344 loss: 5.91020694e-07
Iter: 1345 loss: 5.90910076e-07
Iter: 1346 loss: 5.90505579e-07
Iter: 1347 loss: 5.9386997e-07
Iter: 1348 loss: 5.90494e-07
Iter: 1349 loss: 5.90177592e-07
Iter: 1350 loss: 5.9120913e-07
Iter: 1351 loss: 5.90062314e-07
Iter: 1352 loss: 5.89708463e-07
Iter: 1353 loss: 5.89792194e-07
Iter: 1354 loss: 5.89424189e-07
Iter: 1355 loss: 5.89099841e-07
Iter: 1356 loss: 5.89823287e-07
Iter: 1357 loss: 5.889666e-07
Iter: 1358 loss: 5.88719331e-07
Iter: 1359 loss: 5.88729222e-07
Iter: 1360 loss: 5.88488774e-07
Iter: 1361 loss: 5.88101614e-07
Iter: 1362 loss: 5.88104967e-07
Iter: 1363 loss: 5.8773287e-07
Iter: 1364 loss: 5.87997192e-07
Iter: 1365 loss: 5.87490604e-07
Iter: 1366 loss: 5.87189334e-07
Iter: 1367 loss: 5.8719047e-07
Iter: 1368 loss: 5.86893e-07
Iter: 1369 loss: 5.8660936e-07
Iter: 1370 loss: 5.86562919e-07
Iter: 1371 loss: 5.86080091e-07
Iter: 1372 loss: 5.86815759e-07
Iter: 1373 loss: 5.85859e-07
Iter: 1374 loss: 5.85453677e-07
Iter: 1375 loss: 5.90081868e-07
Iter: 1376 loss: 5.85444582e-07
Iter: 1377 loss: 5.85126827e-07
Iter: 1378 loss: 5.8525211e-07
Iter: 1379 loss: 5.8489826e-07
Iter: 1380 loss: 5.84615464e-07
Iter: 1381 loss: 5.86271426e-07
Iter: 1382 loss: 5.84570728e-07
Iter: 1383 loss: 5.84202894e-07
Iter: 1384 loss: 5.84453801e-07
Iter: 1385 loss: 5.83984558e-07
Iter: 1386 loss: 5.83609449e-07
Iter: 1387 loss: 5.84418e-07
Iter: 1388 loss: 5.83502072e-07
Iter: 1389 loss: 5.83102519e-07
Iter: 1390 loss: 5.82792381e-07
Iter: 1391 loss: 5.82683867e-07
Iter: 1392 loss: 5.82777602e-07
Iter: 1393 loss: 5.8241551e-07
Iter: 1394 loss: 5.82253506e-07
Iter: 1395 loss: 5.81894824e-07
Iter: 1396 loss: 5.87813759e-07
Iter: 1397 loss: 5.81846791e-07
Iter: 1398 loss: 5.814112e-07
Iter: 1399 loss: 5.81478844e-07
Iter: 1400 loss: 5.81108168e-07
Iter: 1401 loss: 5.80703954e-07
Iter: 1402 loss: 5.84189081e-07
Iter: 1403 loss: 5.80668484e-07
Iter: 1404 loss: 5.8020521e-07
Iter: 1405 loss: 5.80997778e-07
Iter: 1406 loss: 5.79981247e-07
Iter: 1407 loss: 5.7971755e-07
Iter: 1408 loss: 5.7977428e-07
Iter: 1409 loss: 5.79531843e-07
Iter: 1410 loss: 5.79180437e-07
Iter: 1411 loss: 5.82317796e-07
Iter: 1412 loss: 5.79152e-07
Iter: 1413 loss: 5.78829599e-07
Iter: 1414 loss: 5.79151845e-07
Iter: 1415 loss: 5.786315e-07
Iter: 1416 loss: 5.78349272e-07
Iter: 1417 loss: 5.79268033e-07
Iter: 1418 loss: 5.78273671e-07
Iter: 1419 loss: 5.77866444e-07
Iter: 1420 loss: 5.78788445e-07
Iter: 1421 loss: 5.77717856e-07
Iter: 1422 loss: 5.77426874e-07
Iter: 1423 loss: 5.77113838e-07
Iter: 1424 loss: 5.77028516e-07
Iter: 1425 loss: 5.7652619e-07
Iter: 1426 loss: 5.79864263e-07
Iter: 1427 loss: 5.76501634e-07
Iter: 1428 loss: 5.76161483e-07
Iter: 1429 loss: 5.76167508e-07
Iter: 1430 loss: 5.75945592e-07
Iter: 1431 loss: 5.75449519e-07
Iter: 1432 loss: 5.82583368e-07
Iter: 1433 loss: 5.75439e-07
Iter: 1434 loss: 5.74868523e-07
Iter: 1435 loss: 5.76048706e-07
Iter: 1436 loss: 5.74611818e-07
Iter: 1437 loss: 5.74171395e-07
Iter: 1438 loss: 5.79095854e-07
Iter: 1439 loss: 5.74143826e-07
Iter: 1440 loss: 5.73749332e-07
Iter: 1441 loss: 5.75234651e-07
Iter: 1442 loss: 5.73648663e-07
Iter: 1443 loss: 5.73391958e-07
Iter: 1444 loss: 5.73122634e-07
Iter: 1445 loss: 5.73059708e-07
Iter: 1446 loss: 5.7262946e-07
Iter: 1447 loss: 5.75008585e-07
Iter: 1448 loss: 5.72556132e-07
Iter: 1449 loss: 5.72129295e-07
Iter: 1450 loss: 5.74622788e-07
Iter: 1451 loss: 5.72084e-07
Iter: 1452 loss: 5.71822397e-07
Iter: 1453 loss: 5.71846215e-07
Iter: 1454 loss: 5.71609576e-07
Iter: 1455 loss: 5.71185865e-07
Iter: 1456 loss: 5.73978753e-07
Iter: 1457 loss: 5.71151475e-07
Iter: 1458 loss: 5.70877887e-07
Iter: 1459 loss: 5.70358168e-07
Iter: 1460 loss: 5.82415055e-07
Iter: 1461 loss: 5.70371469e-07
Iter: 1462 loss: 5.70113968e-07
Iter: 1463 loss: 5.70079123e-07
Iter: 1464 loss: 5.69762562e-07
Iter: 1465 loss: 5.69778194e-07
Iter: 1466 loss: 5.69474537e-07
Iter: 1467 loss: 5.69195493e-07
Iter: 1468 loss: 5.69775466e-07
Iter: 1469 loss: 5.69083e-07
Iter: 1470 loss: 5.68799805e-07
Iter: 1471 loss: 5.68634334e-07
Iter: 1472 loss: 5.6851917e-07
Iter: 1473 loss: 5.68135533e-07
Iter: 1474 loss: 5.68156338e-07
Iter: 1475 loss: 5.67870416e-07
Iter: 1476 loss: 5.67707559e-07
Iter: 1477 loss: 5.67585914e-07
Iter: 1478 loss: 5.67156917e-07
Iter: 1479 loss: 5.67209781e-07
Iter: 1480 loss: 5.66837912e-07
Iter: 1481 loss: 5.66479457e-07
Iter: 1482 loss: 5.71177736e-07
Iter: 1483 loss: 5.66484232e-07
Iter: 1484 loss: 5.66103949e-07
Iter: 1485 loss: 5.67055622e-07
Iter: 1486 loss: 5.65970424e-07
Iter: 1487 loss: 5.65749815e-07
Iter: 1488 loss: 5.66857352e-07
Iter: 1489 loss: 5.65706387e-07
Iter: 1490 loss: 5.65451046e-07
Iter: 1491 loss: 5.65600544e-07
Iter: 1492 loss: 5.65276423e-07
Iter: 1493 loss: 5.64938091e-07
Iter: 1494 loss: 5.64553034e-07
Iter: 1495 loss: 5.64499942e-07
Iter: 1496 loss: 5.64481752e-07
Iter: 1497 loss: 5.64303491e-07
Iter: 1498 loss: 5.6408868e-07
Iter: 1499 loss: 5.63602157e-07
Iter: 1500 loss: 5.71240548e-07
Iter: 1501 loss: 5.635859e-07
Iter: 1502 loss: 5.63125809e-07
Iter: 1503 loss: 5.63913375e-07
Iter: 1504 loss: 5.62936634e-07
Iter: 1505 loss: 5.62487116e-07
Iter: 1506 loss: 5.65409721e-07
Iter: 1507 loss: 5.62448122e-07
Iter: 1508 loss: 5.62188291e-07
Iter: 1509 loss: 5.64394e-07
Iter: 1510 loss: 5.62153218e-07
Iter: 1511 loss: 5.61840864e-07
Iter: 1512 loss: 5.61736272e-07
Iter: 1513 loss: 5.61570971e-07
Iter: 1514 loss: 5.6119535e-07
Iter: 1515 loss: 5.61589104e-07
Iter: 1516 loss: 5.60977128e-07
Iter: 1517 loss: 5.60573881e-07
Iter: 1518 loss: 5.61557044e-07
Iter: 1519 loss: 5.60417561e-07
Iter: 1520 loss: 5.60037108e-07
Iter: 1521 loss: 5.65625271e-07
Iter: 1522 loss: 5.6003455e-07
Iter: 1523 loss: 5.59777391e-07
Iter: 1524 loss: 5.59649493e-07
Iter: 1525 loss: 5.5951341e-07
Iter: 1526 loss: 5.59131081e-07
Iter: 1527 loss: 5.62546234e-07
Iter: 1528 loss: 5.59117154e-07
Iter: 1529 loss: 5.58816964e-07
Iter: 1530 loss: 5.58310262e-07
Iter: 1531 loss: 5.58306283e-07
Iter: 1532 loss: 5.5800848e-07
Iter: 1533 loss: 5.58001489e-07
Iter: 1534 loss: 5.57703174e-07
Iter: 1535 loss: 5.59060652e-07
Iter: 1536 loss: 5.5763843e-07
Iter: 1537 loss: 5.57487965e-07
Iter: 1538 loss: 5.57101373e-07
Iter: 1539 loss: 5.59675698e-07
Iter: 1540 loss: 5.56965631e-07
Iter: 1541 loss: 5.56463306e-07
Iter: 1542 loss: 5.58012289e-07
Iter: 1543 loss: 5.56317104e-07
Iter: 1544 loss: 5.56105533e-07
Iter: 1545 loss: 5.560251e-07
Iter: 1546 loss: 5.55781639e-07
Iter: 1547 loss: 5.55712404e-07
Iter: 1548 loss: 5.5557939e-07
Iter: 1549 loss: 5.55203769e-07
Iter: 1550 loss: 5.55309498e-07
Iter: 1551 loss: 5.54906705e-07
Iter: 1552 loss: 5.54525286e-07
Iter: 1553 loss: 5.56854729e-07
Iter: 1554 loss: 5.54478e-07
Iter: 1555 loss: 5.541267e-07
Iter: 1556 loss: 5.55560291e-07
Iter: 1557 loss: 5.54039161e-07
Iter: 1558 loss: 5.53715722e-07
Iter: 1559 loss: 5.55173926e-07
Iter: 1560 loss: 5.53639e-07
Iter: 1561 loss: 5.53446853e-07
Iter: 1562 loss: 5.54107032e-07
Iter: 1563 loss: 5.53393875e-07
Iter: 1564 loss: 5.53128132e-07
Iter: 1565 loss: 5.52983352e-07
Iter: 1566 loss: 5.52873e-07
Iter: 1567 loss: 5.52697486e-07
Iter: 1568 loss: 5.52681627e-07
Iter: 1569 loss: 5.52477616e-07
Iter: 1570 loss: 5.52106485e-07
Iter: 1571 loss: 5.52117228e-07
Iter: 1572 loss: 5.51743938e-07
Iter: 1573 loss: 5.51710968e-07
Iter: 1574 loss: 5.51451478e-07
Iter: 1575 loss: 5.50894356e-07
Iter: 1576 loss: 5.52028439e-07
Iter: 1577 loss: 5.50719733e-07
Iter: 1578 loss: 5.5028454e-07
Iter: 1579 loss: 5.53048e-07
Iter: 1580 loss: 5.5027192e-07
Iter: 1581 loss: 5.49954166e-07
Iter: 1582 loss: 5.49941319e-07
Iter: 1583 loss: 5.49777042e-07
Iter: 1584 loss: 5.49423874e-07
Iter: 1585 loss: 5.55991051e-07
Iter: 1586 loss: 5.49412391e-07
Iter: 1587 loss: 5.4899e-07
Iter: 1588 loss: 5.50139134e-07
Iter: 1589 loss: 5.48856235e-07
Iter: 1590 loss: 5.48547177e-07
Iter: 1591 loss: 5.48543653e-07
Iter: 1592 loss: 5.48251705e-07
Iter: 1593 loss: 5.48497212e-07
Iter: 1594 loss: 5.4807623e-07
Iter: 1595 loss: 5.47818331e-07
Iter: 1596 loss: 5.4875818e-07
Iter: 1597 loss: 5.47742161e-07
Iter: 1598 loss: 5.47437e-07
Iter: 1599 loss: 5.4832924e-07
Iter: 1600 loss: 5.47329705e-07
Iter: 1601 loss: 5.47104207e-07
Iter: 1602 loss: 5.48894377e-07
Iter: 1603 loss: 5.47080901e-07
Iter: 1604 loss: 5.46845513e-07
Iter: 1605 loss: 5.47035199e-07
Iter: 1606 loss: 5.46726255e-07
Iter: 1607 loss: 5.46485239e-07
Iter: 1608 loss: 5.46244451e-07
Iter: 1609 loss: 5.46195906e-07
Iter: 1610 loss: 5.4577481e-07
Iter: 1611 loss: 5.46460342e-07
Iter: 1612 loss: 5.45600244e-07
Iter: 1613 loss: 5.45135379e-07
Iter: 1614 loss: 5.45299258e-07
Iter: 1615 loss: 5.44802845e-07
Iter: 1616 loss: 5.44213265e-07
Iter: 1617 loss: 5.458744e-07
Iter: 1618 loss: 5.44031309e-07
Iter: 1619 loss: 5.43650117e-07
Iter: 1620 loss: 5.43646e-07
Iter: 1621 loss: 5.43418196e-07
Iter: 1622 loss: 5.45393618e-07
Iter: 1623 loss: 5.43387841e-07
Iter: 1624 loss: 5.43166266e-07
Iter: 1625 loss: 5.42706402e-07
Iter: 1626 loss: 5.50681193e-07
Iter: 1627 loss: 5.42674343e-07
Iter: 1628 loss: 5.42300029e-07
Iter: 1629 loss: 5.44011527e-07
Iter: 1630 loss: 5.42230282e-07
Iter: 1631 loss: 5.41955615e-07
Iter: 1632 loss: 5.4195317e-07
Iter: 1633 loss: 5.41699876e-07
Iter: 1634 loss: 5.41506324e-07
Iter: 1635 loss: 5.41429074e-07
Iter: 1636 loss: 5.41183738e-07
Iter: 1637 loss: 5.41185784e-07
Iter: 1638 loss: 5.40960855e-07
Iter: 1639 loss: 5.40702217e-07
Iter: 1640 loss: 5.40684709e-07
Iter: 1641 loss: 5.40399355e-07
Iter: 1642 loss: 5.41884845e-07
Iter: 1643 loss: 5.40359e-07
Iter: 1644 loss: 5.40070914e-07
Iter: 1645 loss: 5.40166127e-07
Iter: 1646 loss: 5.39883445e-07
Iter: 1647 loss: 5.39598886e-07
Iter: 1648 loss: 5.40761675e-07
Iter: 1649 loss: 5.39527264e-07
Iter: 1650 loss: 5.3922372e-07
Iter: 1651 loss: 5.38999871e-07
Iter: 1652 loss: 5.38915174e-07
Iter: 1653 loss: 5.38530571e-07
Iter: 1654 loss: 5.41319537e-07
Iter: 1655 loss: 5.38511927e-07
Iter: 1656 loss: 5.38164045e-07
Iter: 1657 loss: 5.40689371e-07
Iter: 1658 loss: 5.38146537e-07
Iter: 1659 loss: 5.37933715e-07
Iter: 1660 loss: 5.37744256e-07
Iter: 1661 loss: 5.37669052e-07
Iter: 1662 loss: 5.37327196e-07
Iter: 1663 loss: 5.37517622e-07
Iter: 1664 loss: 5.37124095e-07
Iter: 1665 loss: 5.36852212e-07
Iter: 1666 loss: 5.36847892e-07
Iter: 1667 loss: 5.3656936e-07
Iter: 1668 loss: 5.36648031e-07
Iter: 1669 loss: 5.36377797e-07
Iter: 1670 loss: 5.36199252e-07
Iter: 1671 loss: 5.36190612e-07
Iter: 1672 loss: 5.36017296e-07
Iter: 1673 loss: 5.3567544e-07
Iter: 1674 loss: 5.42974362e-07
Iter: 1675 loss: 5.35644119e-07
Iter: 1676 loss: 5.35366212e-07
Iter: 1677 loss: 5.37419623e-07
Iter: 1678 loss: 5.35338415e-07
Iter: 1679 loss: 5.35083132e-07
Iter: 1680 loss: 5.35835397e-07
Iter: 1681 loss: 5.35004119e-07
Iter: 1682 loss: 5.34659762e-07
Iter: 1683 loss: 5.34315461e-07
Iter: 1684 loss: 5.34248898e-07
Iter: 1685 loss: 5.33884531e-07
Iter: 1686 loss: 5.3593817e-07
Iter: 1687 loss: 5.33833941e-07
Iter: 1688 loss: 5.33466789e-07
Iter: 1689 loss: 5.35669471e-07
Iter: 1690 loss: 5.33424782e-07
Iter: 1691 loss: 5.33174898e-07
Iter: 1692 loss: 5.35199945e-07
Iter: 1693 loss: 5.33150342e-07
Iter: 1694 loss: 5.32953322e-07
Iter: 1695 loss: 5.32644208e-07
Iter: 1696 loss: 5.3263193e-07
Iter: 1697 loss: 5.32301954e-07
Iter: 1698 loss: 5.33601337e-07
Iter: 1699 loss: 5.3222584e-07
Iter: 1700 loss: 5.31919341e-07
Iter: 1701 loss: 5.33476282e-07
Iter: 1702 loss: 5.31868864e-07
Iter: 1703 loss: 5.31479373e-07
Iter: 1704 loss: 5.32323838e-07
Iter: 1705 loss: 5.31340788e-07
Iter: 1706 loss: 5.31151954e-07
Iter: 1707 loss: 5.34001344e-07
Iter: 1708 loss: 5.31151e-07
Iter: 1709 loss: 5.30991485e-07
Iter: 1710 loss: 5.30619502e-07
Iter: 1711 loss: 5.35526453e-07
Iter: 1712 loss: 5.30601369e-07
Iter: 1713 loss: 5.30228e-07
Iter: 1714 loss: 5.31127284e-07
Iter: 1715 loss: 5.30087505e-07
Iter: 1716 loss: 5.29745137e-07
Iter: 1717 loss: 5.33658294e-07
Iter: 1718 loss: 5.29732347e-07
Iter: 1719 loss: 5.29458475e-07
Iter: 1720 loss: 5.2960462e-07
Iter: 1721 loss: 5.29274303e-07
Iter: 1722 loss: 5.28972294e-07
Iter: 1723 loss: 5.28585929e-07
Iter: 1724 loss: 5.28546593e-07
Iter: 1725 loss: 5.28337182e-07
Iter: 1726 loss: 5.28304838e-07
Iter: 1727 loss: 5.28023634e-07
Iter: 1728 loss: 5.28186661e-07
Iter: 1729 loss: 5.27837358e-07
Iter: 1730 loss: 5.27471968e-07
Iter: 1731 loss: 5.27378688e-07
Iter: 1732 loss: 5.27160694e-07
Iter: 1733 loss: 5.26773e-07
Iter: 1734 loss: 5.2776727e-07
Iter: 1735 loss: 5.2662125e-07
Iter: 1736 loss: 5.26388249e-07
Iter: 1737 loss: 5.26350163e-07
Iter: 1738 loss: 5.26184408e-07
Iter: 1739 loss: 5.26307417e-07
Iter: 1740 loss: 5.26077883e-07
Iter: 1741 loss: 5.25812538e-07
Iter: 1742 loss: 5.2662773e-07
Iter: 1743 loss: 5.25735e-07
Iter: 1744 loss: 5.25481596e-07
Iter: 1745 loss: 5.25479436e-07
Iter: 1746 loss: 5.2526957e-07
Iter: 1747 loss: 5.24988195e-07
Iter: 1748 loss: 5.25022074e-07
Iter: 1749 loss: 5.24779807e-07
Iter: 1750 loss: 5.24508096e-07
Iter: 1751 loss: 5.27625673e-07
Iter: 1752 loss: 5.24493146e-07
Iter: 1753 loss: 5.24225868e-07
Iter: 1754 loss: 5.24614052e-07
Iter: 1755 loss: 5.24086e-07
Iter: 1756 loss: 5.2381472e-07
Iter: 1757 loss: 5.23674316e-07
Iter: 1758 loss: 5.23552615e-07
Iter: 1759 loss: 5.23230085e-07
Iter: 1760 loss: 5.25724886e-07
Iter: 1761 loss: 5.23168467e-07
Iter: 1762 loss: 5.22886353e-07
Iter: 1763 loss: 5.25650535e-07
Iter: 1764 loss: 5.22875553e-07
Iter: 1765 loss: 5.22699224e-07
Iter: 1766 loss: 5.22412961e-07
Iter: 1767 loss: 5.22399091e-07
Iter: 1768 loss: 5.22043251e-07
Iter: 1769 loss: 5.22796199e-07
Iter: 1770 loss: 5.21920697e-07
Iter: 1771 loss: 5.21729362e-07
Iter: 1772 loss: 5.21689458e-07
Iter: 1773 loss: 5.21559173e-07
Iter: 1774 loss: 5.2137716e-07
Iter: 1775 loss: 5.21359425e-07
Iter: 1776 loss: 5.21020866e-07
Iter: 1777 loss: 5.22413416e-07
Iter: 1778 loss: 5.20955155e-07
Iter: 1779 loss: 5.20722892e-07
Iter: 1780 loss: 5.2035449e-07
Iter: 1781 loss: 5.29818294e-07
Iter: 1782 loss: 5.20342269e-07
Iter: 1783 loss: 5.20001777e-07
Iter: 1784 loss: 5.24035613e-07
Iter: 1785 loss: 5.19994e-07
Iter: 1786 loss: 5.19753542e-07
Iter: 1787 loss: 5.20445042e-07
Iter: 1788 loss: 5.19664297e-07
Iter: 1789 loss: 5.19393552e-07
Iter: 1790 loss: 5.20202491e-07
Iter: 1791 loss: 5.19332048e-07
Iter: 1792 loss: 5.19072671e-07
Iter: 1793 loss: 5.18950799e-07
Iter: 1794 loss: 5.1880113e-07
Iter: 1795 loss: 5.18451543e-07
Iter: 1796 loss: 5.19784351e-07
Iter: 1797 loss: 5.18369632e-07
Iter: 1798 loss: 5.18125205e-07
Iter: 1799 loss: 5.18125148e-07
Iter: 1800 loss: 5.17947228e-07
Iter: 1801 loss: 5.17499416e-07
Iter: 1802 loss: 5.23594167e-07
Iter: 1803 loss: 5.17453657e-07
Iter: 1804 loss: 5.17104752e-07
Iter: 1805 loss: 5.20553442e-07
Iter: 1806 loss: 5.17096964e-07
Iter: 1807 loss: 5.16892669e-07
Iter: 1808 loss: 5.16867487e-07
Iter: 1809 loss: 5.16707701e-07
Iter: 1810 loss: 5.16507953e-07
Iter: 1811 loss: 5.16491355e-07
Iter: 1812 loss: 5.16278362e-07
Iter: 1813 loss: 5.19214268e-07
Iter: 1814 loss: 5.16260627e-07
Iter: 1815 loss: 5.16109708e-07
Iter: 1816 loss: 5.15717147e-07
Iter: 1817 loss: 5.19587331e-07
Iter: 1818 loss: 5.15659792e-07
Iter: 1819 loss: 5.15221359e-07
Iter: 1820 loss: 5.15563158e-07
Iter: 1821 loss: 5.1497e-07
Iter: 1822 loss: 5.14713804e-07
Iter: 1823 loss: 5.14662815e-07
Iter: 1824 loss: 5.14396561e-07
Iter: 1825 loss: 5.14454314e-07
Iter: 1826 loss: 5.14188514e-07
Iter: 1827 loss: 5.13877239e-07
Iter: 1828 loss: 5.14199769e-07
Iter: 1829 loss: 5.13714781e-07
Iter: 1830 loss: 5.13388102e-07
Iter: 1831 loss: 5.15243073e-07
Iter: 1832 loss: 5.1333825e-07
Iter: 1833 loss: 5.13019e-07
Iter: 1834 loss: 5.14816293e-07
Iter: 1835 loss: 5.12964618e-07
Iter: 1836 loss: 5.12763336e-07
Iter: 1837 loss: 5.12892655e-07
Iter: 1838 loss: 5.1263396e-07
Iter: 1839 loss: 5.12353381e-07
Iter: 1840 loss: 5.12329734e-07
Iter: 1841 loss: 5.12111797e-07
Iter: 1842 loss: 5.12005101e-07
Iter: 1843 loss: 5.11906251e-07
Iter: 1844 loss: 5.11773e-07
Iter: 1845 loss: 5.1149118e-07
Iter: 1846 loss: 5.17353726e-07
Iter: 1847 loss: 5.11493681e-07
Iter: 1848 loss: 5.11225721e-07
Iter: 1849 loss: 5.14122064e-07
Iter: 1850 loss: 5.11215717e-07
Iter: 1851 loss: 5.11001815e-07
Iter: 1852 loss: 5.10994e-07
Iter: 1853 loss: 5.10804625e-07
Iter: 1854 loss: 5.10568839e-07
Iter: 1855 loss: 5.1027763e-07
Iter: 1856 loss: 5.10249833e-07
Iter: 1857 loss: 5.09853692e-07
Iter: 1858 loss: 5.11447809e-07
Iter: 1859 loss: 5.09768142e-07
Iter: 1860 loss: 5.09391e-07
Iter: 1861 loss: 5.11027338e-07
Iter: 1862 loss: 5.09317715e-07
Iter: 1863 loss: 5.09056292e-07
Iter: 1864 loss: 5.12682e-07
Iter: 1865 loss: 5.0904714e-07
Iter: 1866 loss: 5.08811e-07
Iter: 1867 loss: 5.08598305e-07
Iter: 1868 loss: 5.08533788e-07
Iter: 1869 loss: 5.08196877e-07
Iter: 1870 loss: 5.09189135e-07
Iter: 1871 loss: 5.08069e-07
Iter: 1872 loss: 5.07745483e-07
Iter: 1873 loss: 5.11767439e-07
Iter: 1874 loss: 5.07723712e-07
Iter: 1875 loss: 5.07505206e-07
Iter: 1876 loss: 5.07273e-07
Iter: 1877 loss: 5.07234745e-07
Iter: 1878 loss: 5.06951721e-07
Iter: 1879 loss: 5.06946378e-07
Iter: 1880 loss: 5.066629e-07
Iter: 1881 loss: 5.07114123e-07
Iter: 1882 loss: 5.06576896e-07
Iter: 1883 loss: 5.06351682e-07
Iter: 1884 loss: 5.06368906e-07
Iter: 1885 loss: 5.06198774e-07
Iter: 1886 loss: 5.05972878e-07
Iter: 1887 loss: 5.07136122e-07
Iter: 1888 loss: 5.05932917e-07
Iter: 1889 loss: 5.05661035e-07
Iter: 1890 loss: 5.05993455e-07
Iter: 1891 loss: 5.05544449e-07
Iter: 1892 loss: 5.05312187e-07
Iter: 1893 loss: 5.0499284e-07
Iter: 1894 loss: 5.04986588e-07
Iter: 1895 loss: 5.04550485e-07
Iter: 1896 loss: 5.05107153e-07
Iter: 1897 loss: 5.04313164e-07
Iter: 1898 loss: 5.0393794e-07
Iter: 1899 loss: 5.09197946e-07
Iter: 1900 loss: 5.03909632e-07
Iter: 1901 loss: 5.03630417e-07
Iter: 1902 loss: 5.04040145e-07
Iter: 1903 loss: 5.03505248e-07
Iter: 1904 loss: 5.03197327e-07
Iter: 1905 loss: 5.06782214e-07
Iter: 1906 loss: 5.0320125e-07
Iter: 1907 loss: 5.02970806e-07
Iter: 1908 loss: 5.02549e-07
Iter: 1909 loss: 5.11893575e-07
Iter: 1910 loss: 5.02554826e-07
Iter: 1911 loss: 5.02501848e-07
Iter: 1912 loss: 5.02352918e-07
Iter: 1913 loss: 5.02218e-07
Iter: 1914 loss: 5.0219154e-07
Iter: 1915 loss: 5.02088e-07
Iter: 1916 loss: 5.01867078e-07
Iter: 1917 loss: 5.02946136e-07
Iter: 1918 loss: 5.01835757e-07
Iter: 1919 loss: 5.016459e-07
Iter: 1920 loss: 5.01510954e-07
Iter: 1921 loss: 5.01431373e-07
Iter: 1922 loss: 5.01157558e-07
Iter: 1923 loss: 5.01232876e-07
Iter: 1924 loss: 5.00979695e-07
Iter: 1925 loss: 5.00580086e-07
Iter: 1926 loss: 5.02008788e-07
Iter: 1927 loss: 5.00478563e-07
Iter: 1928 loss: 5.00337364e-07
Iter: 1929 loss: 5.00309e-07
Iter: 1930 loss: 5.00178e-07
Iter: 1931 loss: 4.99844873e-07
Iter: 1932 loss: 5.03605861e-07
Iter: 1933 loss: 4.99831458e-07
Iter: 1934 loss: 4.99468911e-07
Iter: 1935 loss: 4.99968451e-07
Iter: 1936 loss: 4.99290195e-07
Iter: 1937 loss: 4.99023031e-07
Iter: 1938 loss: 4.99028602e-07
Iter: 1939 loss: 4.98785084e-07
Iter: 1940 loss: 4.99575322e-07
Iter: 1941 loss: 4.98735687e-07
Iter: 1942 loss: 4.98491488e-07
Iter: 1943 loss: 4.98629106e-07
Iter: 1944 loss: 4.98355121e-07
Iter: 1945 loss: 4.98216536e-07
Iter: 1946 loss: 4.98222334e-07
Iter: 1947 loss: 4.98061127e-07
Iter: 1948 loss: 4.9786604e-07
Iter: 1949 loss: 4.97864562e-07
Iter: 1950 loss: 4.97630083e-07
Iter: 1951 loss: 5.00328213e-07
Iter: 1952 loss: 4.97618e-07
Iter: 1953 loss: 4.97481153e-07
Iter: 1954 loss: 4.9712105e-07
Iter: 1955 loss: 4.99620796e-07
Iter: 1956 loss: 4.97053065e-07
Iter: 1957 loss: 4.96673124e-07
Iter: 1958 loss: 5.00051101e-07
Iter: 1959 loss: 4.96645271e-07
Iter: 1960 loss: 4.96387258e-07
Iter: 1961 loss: 4.99196517e-07
Iter: 1962 loss: 4.96387145e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2
+ date
Wed Oct 21 12:50:13 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/300_300_300_1 --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c1c9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c0d6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c1fe9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c1fec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c11b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c07e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bff0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bfc2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bfc2488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c03be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c051840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf8f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf8fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf1e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c0a3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c0a3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf8fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e169fdbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3beed950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf04f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e16a2f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e16a2f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e169578c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e169b1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e16986488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e16986d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd00a87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd00b0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd00b0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df0055950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd0017ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d487bc510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d487bc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d487e8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd0080a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e169180d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.04727656
test_loss: 0.04593319
train_loss: 0.020359008
test_loss: 0.020845871
train_loss: 0.012422589
test_loss: 0.012919019
train_loss: 0.008681728
test_loss: 0.010014267
train_loss: 0.007508901
test_loss: 0.008326084
train_loss: 0.0070775202
test_loss: 0.007591439
train_loss: 0.0068660257
test_loss: 0.007374335
train_loss: 0.006080945
test_loss: 0.007047101
train_loss: 0.0059288573
test_loss: 0.006896944
train_loss: 0.0056730565
test_loss: 0.0066436795
train_loss: 0.0054848674
test_loss: 0.0066064075
train_loss: 0.0055770627
test_loss: 0.006447815
train_loss: 0.005030862
test_loss: 0.0062142513
train_loss: 0.0053278534
test_loss: 0.006308455
train_loss: 0.005508627
test_loss: 0.006254092
train_loss: 0.0052335486
test_loss: 0.0059769065
train_loss: 0.0052238847
test_loss: 0.0058520995
train_loss: 0.0052503035
test_loss: 0.0060572126
train_loss: 0.0049848347
test_loss: 0.0058310544
train_loss: 0.0050358595
test_loss: 0.005838017
train_loss: 0.0048251357
test_loss: 0.0057813325
train_loss: 0.0045970464
test_loss: 0.005680734
train_loss: 0.005254079
test_loss: 0.005992147
train_loss: 0.0051401136
test_loss: 0.005638298
train_loss: 0.0049516526
test_loss: 0.006040196
train_loss: 0.0049243188
test_loss: 0.0058531957
train_loss: 0.0049594105
test_loss: 0.0056235217
train_loss: 0.0045231264
test_loss: 0.00546072
train_loss: 0.0043745926
test_loss: 0.005543978
train_loss: 0.004616551
test_loss: 0.005645988
train_loss: 0.004455081
test_loss: 0.0053735687
train_loss: 0.004432856
test_loss: 0.005245917
train_loss: 0.0042045265
test_loss: 0.005195771
train_loss: 0.00458317
test_loss: 0.005483595
train_loss: 0.0048983945
test_loss: 0.0055129845
train_loss: 0.004615143
test_loss: 0.0053911014
train_loss: 0.004250705
test_loss: 0.005238079
train_loss: 0.0045403917
test_loss: 0.0054373527
train_loss: 0.004365779
test_loss: 0.0054899617
train_loss: 0.0039407113
test_loss: 0.0051720697
train_loss: 0.0043285526
test_loss: 0.0054733674
train_loss: 0.0043695047
test_loss: 0.00541137
train_loss: 0.0043933936
test_loss: 0.005496816
train_loss: 0.0043144957
test_loss: 0.0054034204
train_loss: 0.004173503
test_loss: 0.0053329626
train_loss: 0.0044706417
test_loss: 0.005421031
train_loss: 0.004827565
test_loss: 0.005529195
train_loss: 0.0044879937
test_loss: 0.0054503772
train_loss: 0.004344031
test_loss: 0.0052826703
train_loss: 0.003995328
test_loss: 0.005146161
train_loss: 0.0039687185
test_loss: 0.0050902455
train_loss: 0.004396828
test_loss: 0.005498315
train_loss: 0.0043527707
test_loss: 0.005422782
train_loss: 0.004341125
test_loss: 0.005199777
train_loss: 0.0044846903
test_loss: 0.005458369
train_loss: 0.004154704
test_loss: 0.0050623333
train_loss: 0.004050733
test_loss: 0.0050403574
train_loss: 0.004188638
test_loss: 0.0052478765
train_loss: 0.004090078
test_loss: 0.0050378595
train_loss: 0.0040287785
test_loss: 0.005103703
train_loss: 0.004470502
test_loss: 0.005316942
train_loss: 0.004007516
test_loss: 0.0049864785
train_loss: 0.0040696487
test_loss: 0.005383338
train_loss: 0.0042900415
test_loss: 0.0051232157
train_loss: 0.004276801
test_loss: 0.005227621
train_loss: 0.004370775
test_loss: 0.005151008
train_loss: 0.004278271
test_loss: 0.0049110292
train_loss: 0.0041910363
test_loss: 0.0052007353
train_loss: 0.0040061213
test_loss: 0.0050654765
train_loss: 0.004230417
test_loss: 0.0052400604
train_loss: 0.0040801372
test_loss: 0.005054953
train_loss: 0.0041322256
test_loss: 0.0054430296
train_loss: 0.0039695287
test_loss: 0.0054028886
train_loss: 0.004188831
test_loss: 0.005048675
train_loss: 0.004211356
test_loss: 0.005226396
train_loss: 0.0040747295
test_loss: 0.0050390055
train_loss: 0.0040901573
test_loss: 0.0050086533
train_loss: 0.004527921
test_loss: 0.0051036985
train_loss: 0.0042344187
test_loss: 0.0053009433
train_loss: 0.0041879914
test_loss: 0.005729495
train_loss: 0.003962545
test_loss: 0.0051319753
train_loss: 0.00396231
test_loss: 0.0053949202
train_loss: 0.00406551
test_loss: 0.0050234813
train_loss: 0.004281554
test_loss: 0.0049675265
train_loss: 0.003840463
test_loss: 0.005004796
train_loss: 0.003861918
test_loss: 0.0048124064
train_loss: 0.004077878
test_loss: 0.0050758366
train_loss: 0.004348949
test_loss: 0.005189467
train_loss: 0.0042964127
test_loss: 0.005293627
train_loss: 0.004622271
test_loss: 0.0050062817
train_loss: 0.0036814963
test_loss: 0.004964522
train_loss: 0.0040515554
test_loss: 0.0048893
train_loss: 0.004003311
test_loss: 0.0050535263
train_loss: 0.0038656262
test_loss: 0.0052211843
train_loss: 0.004233581
test_loss: 0.0052128984
train_loss: 0.004244972
test_loss: 0.0050733113
train_loss: 0.003734096
test_loss: 0.0049749585
train_loss: 0.0039005298
test_loss: 0.0049818265
train_loss: 0.003852158
test_loss: 0.004963888
train_loss: 0.003809529
test_loss: 0.00485149
train_loss: 0.0038083717
test_loss: 0.0047299247
train_loss: 0.0038670884
test_loss: 0.004858309
train_loss: 0.004141531
test_loss: 0.0051075784
train_loss: 0.003854332
test_loss: 0.0051533887
train_loss: 0.003903283
test_loss: 0.0049368357
train_loss: 0.0035809367
test_loss: 0.004748202
train_loss: 0.0038458654
test_loss: 0.0049080527
train_loss: 0.0038158007
test_loss: 0.00491799
train_loss: 0.004049514
test_loss: 0.0050756894
train_loss: 0.003860538
test_loss: 0.0051745637
train_loss: 0.003849937
test_loss: 0.00490927
train_loss: 0.004208629
test_loss: 0.0052628107
train_loss: 0.004120264
test_loss: 0.005456044
train_loss: 0.003965639
test_loss: 0.004940358
train_loss: 0.004514213
test_loss: 0.0059202267
train_loss: 0.004546942
test_loss: 0.0053944564
train_loss: 0.0037996555
test_loss: 0.0050111846
train_loss: 0.0038276783
test_loss: 0.0052018655
train_loss: 0.0039272117
test_loss: 0.005350185
train_loss: 0.0044833925
test_loss: 0.0051778895
train_loss: 0.0037072478
test_loss: 0.004636128
train_loss: 0.004448153
test_loss: 0.0050402265
train_loss: 0.003690899
test_loss: 0.0049431333
train_loss: 0.0039519984
test_loss: 0.0050689736
train_loss: 0.0038204936
test_loss: 0.0049312045
train_loss: 0.003975883
test_loss: 0.0050026695
train_loss: 0.0039585894
test_loss: 0.0048604137
train_loss: 0.0039747055
test_loss: 0.0050108917
train_loss: 0.004116197
test_loss: 0.004873778
train_loss: 0.003840178
test_loss: 0.0050073643
train_loss: 0.0041151163
test_loss: 0.0051851054
train_loss: 0.0040529515
test_loss: 0.005340728
train_loss: 0.0042377613
test_loss: 0.0048807976
train_loss: 0.003853942
test_loss: 0.0047380463
train_loss: 0.0038662502
test_loss: 0.0050563496
train_loss: 0.003977609
test_loss: 0.0052877567
train_loss: 0.004229832
test_loss: 0.0052416623
train_loss: 0.004124572
test_loss: 0.004931567
train_loss: 0.0038881025
test_loss: 0.005167265
train_loss: 0.0041002464
test_loss: 0.005011109
train_loss: 0.003926753
test_loss: 0.0049422313
train_loss: 0.0038399552
test_loss: 0.004860274
train_loss: 0.0039294674
test_loss: 0.004936772
train_loss: 0.0043076114
test_loss: 0.005151269
train_loss: 0.0038964455
test_loss: 0.0051749563
train_loss: 0.0042674905
test_loss: 0.0055120415
train_loss: 0.004371381
test_loss: 0.0052845865
train_loss: 0.003917507
test_loss: 0.0053507416
train_loss: 0.0040365397
test_loss: 0.004937497
train_loss: 0.004112854
test_loss: 0.0051582004
train_loss: 0.00378283
test_loss: 0.005273264
train_loss: 0.0040811463
test_loss: 0.004974869
train_loss: 0.0037222137
test_loss: 0.0050213193
train_loss: 0.0037600438
test_loss: 0.0051019955
train_loss: 0.0040761176
test_loss: 0.005103604
train_loss: 0.0039141383
test_loss: 0.0049815676
train_loss: 0.003943599
test_loss: 0.004933741
train_loss: 0.0036051688
test_loss: 0.004821793
train_loss: 0.003996802
test_loss: 0.0049254415
train_loss: 0.0041669896
test_loss: 0.0051585496
train_loss: 0.003814748
test_loss: 0.0047874707
train_loss: 0.0036339108
test_loss: 0.004842961
train_loss: 0.0032806636
test_loss: 0.004777748
train_loss: 0.0036772594
test_loss: 0.0049174116
train_loss: 0.0035595447
test_loss: 0.0049722474
train_loss: 0.0039821295
test_loss: 0.0055728722
train_loss: 0.003976546
test_loss: 0.005032206
train_loss: 0.0037200756
test_loss: 0.005048874
train_loss: 0.003965466
test_loss: 0.00508684
train_loss: 0.0041211266
test_loss: 0.004866873
train_loss: 0.0040455298
test_loss: 0.0049285213
train_loss: 0.0037072406
test_loss: 0.0050595365
train_loss: 0.0037576396
test_loss: 0.0051021506
train_loss: 0.00394328
test_loss: 0.0050316104
train_loss: 0.004056524
test_loss: 0.0046314932
train_loss: 0.003911481
test_loss: 0.0051122336
train_loss: 0.00390288
test_loss: 0.00489292
train_loss: 0.0037824865
test_loss: 0.004989925
train_loss: 0.0035473034
test_loss: 0.004863419
train_loss: 0.0037664764
test_loss: 0.004689192
train_loss: 0.0037540945
test_loss: 0.0047975522
train_loss: 0.003950988
test_loss: 0.004949582
train_loss: 0.003888343
test_loss: 0.0051081227
train_loss: 0.0039047392
test_loss: 0.0051911385
train_loss: 0.00377404
test_loss: 0.004968389
train_loss: 0.0039344695
test_loss: 0.005116324
train_loss: 0.0035391035
test_loss: 0.0048437733
train_loss: 0.0037415815
test_loss: 0.004794464
train_loss: 0.0039209407
test_loss: 0.004901974
train_loss: 0.0037607695
test_loss: 0.0050636707
train_loss: 0.0038862037
test_loss: 0.0052259755
train_loss: 0.0042873253
test_loss: 0.0049492144
train_loss: 0.0038079913
test_loss: 0.004918185
train_loss: 0.0037005213
test_loss: 0.005089249
train_loss: 0.003431982
test_loss: 0.004877839
train_loss: 0.0036086128
test_loss: 0.0047208937
train_loss: 0.0037006603
test_loss: 0.004921338
train_loss: 0.0036298716
test_loss: 0.0049652276
train_loss: 0.0037296498
test_loss: 0.0048173307
train_loss: 0.0036027643
test_loss: 0.004906307
train_loss: 0.0035160019
test_loss: 0.0047667264
train_loss: 0.0038320138
test_loss: 0.0047839954
train_loss: 0.0037321148
test_loss: 0.004803929
train_loss: 0.0036232378
test_loss: 0.0049608443
train_loss: 0.0036300302
test_loss: 0.0047470443
train_loss: 0.0037102774
test_loss: 0.0049362793
train_loss: 0.0040042573
test_loss: 0.0052024634
train_loss: 0.0041158707
test_loss: 0.0049546733
train_loss: 0.0041469866
test_loss: 0.004946996
train_loss: 0.0035444396
test_loss: 0.0049022315
train_loss: 0.0037661796
test_loss: 0.004641427
train_loss: 0.0038128847
test_loss: 0.004902811
train_loss: 0.0038745464
test_loss: 0.0049332287
train_loss: 0.0041110376
test_loss: 0.0050099217
train_loss: 0.0039720386
test_loss: 0.0051286533
train_loss: 0.0037678874
test_loss: 0.004886622
train_loss: 0.004087669
test_loss: 0.0051859133
train_loss: 0.0038763434
test_loss: 0.005125674
train_loss: 0.0035955287
test_loss: 0.004824592
train_loss: 0.0034081298
test_loss: 0.0047683967
train_loss: 0.0037911674
test_loss: 0.0048964852
train_loss: 0.0037684948
test_loss: 0.0047038835
train_loss: 0.0041774055
test_loss: 0.005015044
train_loss: 0.004058008
test_loss: 0.0048378007
train_loss: 0.0044563645
test_loss: 0.0054318244
train_loss: 0.0038768742
test_loss: 0.004977419
train_loss: 0.00397171
test_loss: 0.0048505454
train_loss: 0.0035615708
test_loss: 0.004926521
train_loss: 0.0037269536
test_loss: 0.0049402616
train_loss: 0.0039808885
test_loss: 0.0048241336
train_loss: 0.0036824988
test_loss: 0.005066928
train_loss: 0.0040922295
test_loss: 0.004890088
train_loss: 0.0037516146
test_loss: 0.004964741
train_loss: 0.00399173
test_loss: 0.0049610077
train_loss: 0.0038581018
test_loss: 0.0048372685
train_loss: 0.0038447117
test_loss: 0.0046807523
train_loss: 0.003529645
test_loss: 0.0048054433
train_loss: 0.003696106
test_loss: 0.0048628133
train_loss: 0.0037786337
test_loss: 0.0048463847
train_loss: 0.00389648
test_loss: 0.0049441196
train_loss: 0.003771523
test_loss: 0.004878116
train_loss: 0.003561622
test_loss: 0.0049103065
train_loss: 0.003283627
test_loss: 0.0046903347
train_loss: 0.0038562922
test_loss: 0.004814605
train_loss: 0.0037946096
test_loss: 0.004786944
train_loss: 0.003511081
test_loss: 0.0047318027
train_loss: 0.0039523984
test_loss: 0.004821574
train_loss: 0.0034490344
test_loss: 0.00475605
train_loss: 0.0037097821
test_loss: 0.0049733724
train_loss: 0.0038622105
test_loss: 0.004852749
train_loss: 0.0033625509
test_loss: 0.0047495333
train_loss: 0.0039992076
test_loss: 0.004892646
train_loss: 0.0037324708
test_loss: 0.0051407684
train_loss: 0.003599293
test_loss: 0.0049426183
train_loss: 0.0036131798
test_loss: 0.0048286193
train_loss: 0.0035765797
test_loss: 0.004919613
train_loss: 0.003663085
test_loss: 0.004808716
train_loss: 0.0039158044
test_loss: 0.005048465
train_loss: 0.003322386
test_loss: 0.0046977303
train_loss: 0.0035637286
test_loss: 0.0048091314
train_loss: 0.003702079
test_loss: 0.0050267107
train_loss: 0.003567373
test_loss: 0.0048376545
train_loss: 0.0037503275
test_loss: 0.0049219183
train_loss: 0.0035771704
test_loss: 0.0048175137
train_loss: 0.0038435033
test_loss: 0.0047257356
train_loss: 0.0036536953
test_loss: 0.0049859425
train_loss: 0.0039665895
test_loss: 0.004996082
train_loss: 0.0035752114
test_loss: 0.0050314125
train_loss: 0.003708268
test_loss: 0.004878426
train_loss: 0.003476856
test_loss: 0.0046565323
train_loss: 0.0038073263
test_loss: 0.0052709184
train_loss: 0.0037491038
test_loss: 0.005157239
train_loss: 0.003892885
test_loss: 0.004988088
train_loss: 0.003675954
test_loss: 0.0048428318
train_loss: 0.0036040666
test_loss: 0.0047763838
train_loss: 0.0035724523
test_loss: 0.004797819
train_loss: 0.003851192
test_loss: 0.004779588
train_loss: 0.0036379024
test_loss: 0.0051172874
train_loss: 0.0036419386
test_loss: 0.0048529683
train_loss: 0.0036641068
test_loss: 0.0050730407
train_loss: 0.0033207324
test_loss: 0.004816549
train_loss: 0.0036593867
test_loss: 0.004765007
train_loss: 0.003601563
test_loss: 0.0048208255
train_loss: 0.0040498935
test_loss: 0.0052548354
train_loss: 0.004216535
test_loss: 0.005115986
train_loss: 0.0034445403
test_loss: 0.004891754
train_loss: 0.0039063683
test_loss: 0.0050516496
train_loss: 0.003740353
test_loss: 0.0048036575
train_loss: 0.00455743
test_loss: 0.0049278084
train_loss: 0.003604249
test_loss: 0.00480437
train_loss: 0.0036901748
test_loss: 0.004742387
train_loss: 0.0038011344
test_loss: 0.0048483755
train_loss: 0.0039541656
test_loss: 0.004903208
train_loss: 0.0037479568
test_loss: 0.004778453
train_loss: 0.0037362783
test_loss: 0.00487643
train_loss: 0.003569607
test_loss: 0.0048808493
train_loss: 0.0035378523
test_loss: 0.004611264
train_loss: 0.0032953059
test_loss: 0.0048598917
train_loss: 0.0034989503
test_loss: 0.0048073838
train_loss: 0.0038382118
test_loss: 0.004934508
train_loss: 0.0036582276
test_loss: 0.0048824293
train_loss: 0.00396282
test_loss: 0.004711601
train_loss: 0.0037907823
test_loss: 0.004908465
train_loss: 0.0037003588
test_loss: 0.004931736
train_loss: 0.003879197
test_loss: 0.0048509496
train_loss: 0.003970066
test_loss: 0.005178622
train_loss: 0.0040620547
test_loss: 0.004807851
train_loss: 0.0034926604
test_loss: 0.0045962287
train_loss: 0.003913221
test_loss: 0.00489393
train_loss: 0.0036599962
test_loss: 0.004917236
train_loss: 0.0039206184
test_loss: 0.0048973374
train_loss: 0.0037983404
test_loss: 0.0048753195
train_loss: 0.0037766013
test_loss: 0.004811995
train_loss: 0.0035362511
test_loss: 0.004782578
train_loss: 0.004312228
test_loss: 0.0049737548
train_loss: 0.0037195615
test_loss: 0.0048246905
train_loss: 0.0035572296
test_loss: 0.00490188
train_loss: 0.0036038803
test_loss: 0.0049216617
train_loss: 0.0037860763
test_loss: 0.0051057674
train_loss: 0.0038883863
test_loss: 0.0049812035
train_loss: 0.0036427223
test_loss: 0.0047516758
train_loss: 0.003776729
test_loss: 0.004851938
train_loss: 0.0039195246
test_loss: 0.0048897346
train_loss: 0.0038902499
test_loss: 0.004900598
train_loss: 0.0037145508
test_loss: 0.004710713
train_loss: 0.003312184
test_loss: 0.0046347575
train_loss: 0.0033938938
test_loss: 0.0048259455
train_loss: 0.003648201
test_loss: 0.0047019273
train_loss: 0.0037612522
test_loss: 0.00520391
train_loss: 0.003672793
test_loss: 0.004749397
train_loss: 0.0038904839
test_loss: 0.004914551
train_loss: 0.0035522443
test_loss: 0.005242437
train_loss: 0.0043468107
test_loss: 0.0049796416
train_loss: 0.0038637673
test_loss: 0.0049981177
train_loss: 0.004089058
test_loss: 0.004828428
train_loss: 0.0033134588
test_loss: 0.004659542
train_loss: 0.0036833468
test_loss: 0.004897387
train_loss: 0.003526947
test_loss: 0.0046550254
train_loss: 0.0036846504
test_loss: 0.0048910007
train_loss: 0.0032947916
test_loss: 0.004656339
train_loss: 0.0037518921
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.00491748
train_loss: 0.0037773207
test_loss: 0.0047932677
train_loss: 0.0036939448
test_loss: 0.0049175336
train_loss: 0.003745668
test_loss: 0.004673335
train_loss: 0.0037015053
test_loss: 0.004611061
train_loss: 0.003472522
test_loss: 0.004939607
train_loss: 0.0033002102
test_loss: 0.0046812017
train_loss: 0.003697203
test_loss: 0.0046533695
train_loss: 0.004150767
test_loss: 0.0050054826
train_loss: 0.0038694139
test_loss: 0.0049577495
train_loss: 0.0034809562
test_loss: 0.0048969663
train_loss: 0.0031691806
test_loss: 0.0048483075
train_loss: 0.003703667
test_loss: 0.0046880506
train_loss: 0.0034918059
test_loss: 0.0049812114
train_loss: 0.0038345787
test_loss: 0.0048520383
train_loss: 0.0038087736
test_loss: 0.0050681983
train_loss: 0.0036893548
test_loss: 0.004852346
train_loss: 0.0034846244
test_loss: 0.004621957
train_loss: 0.0034227418
test_loss: 0.004827002
train_loss: 0.0035287263
test_loss: 0.0049856976
train_loss: 0.0032755858
test_loss: 0.0047136103
train_loss: 0.0033541848
test_loss: 0.00471906
train_loss: 0.0038389787
test_loss: 0.0046294057
train_loss: 0.0036608658
test_loss: 0.0048499075
train_loss: 0.0037107347
test_loss: 0.0049177846
train_loss: 0.0037006314
test_loss: 0.004704923
train_loss: 0.0035241367
test_loss: 0.005058254
train_loss: 0.0033925944
test_loss: 0.004629879
train_loss: 0.003570295
test_loss: 0.004769428
train_loss: 0.003619008
test_loss: 0.0047128852
train_loss: 0.0033712944
test_loss: 0.004765036
train_loss: 0.0033661881
test_loss: 0.0046601538
train_loss: 0.0036110342
test_loss: 0.0049228705
train_loss: 0.0036711479
test_loss: 0.0048413556
train_loss: 0.0039656498
test_loss: 0.0050903563
train_loss: 0.0037513014
test_loss: 0.004807814
train_loss: 0.00345937
test_loss: 0.0047422014
train_loss: 0.003302209
test_loss: 0.004665612
train_loss: 0.00325434
test_loss: 0.0046447716
train_loss: 0.003593314
test_loss: 0.0048221
train_loss: 0.003548547
test_loss: 0.004798765
train_loss: 0.0036046212
test_loss: 0.0048847585
train_loss: 0.0037756732
test_loss: 0.0049653156
train_loss: 0.003723559
test_loss: 0.0048192535
train_loss: 0.0036715092
test_loss: 0.004856458
train_loss: 0.0034368187
test_loss: 0.005138211
train_loss: 0.003440099
test_loss: 0.0049758987
train_loss: 0.003595437
test_loss: 0.004681973
train_loss: 0.0033854526
test_loss: 0.0046673454
train_loss: 0.003344398
test_loss: 0.004659061
train_loss: 0.0035955035
test_loss: 0.004625402
train_loss: 0.003473172
test_loss: 0.0048095877
train_loss: 0.0036737993
test_loss: 0.004776191
train_loss: 0.0033062717
test_loss: 0.0047915187
train_loss: 0.0036162413
test_loss: 0.0048447866
train_loss: 0.0037277567
test_loss: 0.0046075084
train_loss: 0.003240052
test_loss: 0.004502551
train_loss: 0.003366332
test_loss: 0.00475155
train_loss: 0.0031512477
test_loss: 0.0047788117
train_loss: 0.0033541385
test_loss: 0.0046235244
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d9f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7deec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d9ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d29bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d43510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d43950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7c90bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d430d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7c3e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7c3ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7c10b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7bbfd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7bb5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b56840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b27840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b27bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b452f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b0ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7ac89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a66950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a668c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a20598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a55730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a00620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d79e4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1c15ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1be2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1bfc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1bfc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1bc5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1b66950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1b8a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1b8ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1b34620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1af2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1af28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.37979984e-05
Iter: 2 loss: 2.00734066e-05
Iter: 3 loss: 1.98604357e-05
Iter: 4 loss: 1.85861591e-05
Iter: 5 loss: 2.95694463e-05
Iter: 6 loss: 1.85166373e-05
Iter: 7 loss: 1.75345758e-05
Iter: 8 loss: 1.85106328e-05
Iter: 9 loss: 1.69821069e-05
Iter: 10 loss: 1.56597744e-05
Iter: 11 loss: 1.50086435e-05
Iter: 12 loss: 1.43753141e-05
Iter: 13 loss: 1.30340341e-05
Iter: 14 loss: 2.10407288e-05
Iter: 15 loss: 1.2864487e-05
Iter: 16 loss: 1.18328735e-05
Iter: 17 loss: 1.29643668e-05
Iter: 18 loss: 1.12712914e-05
Iter: 19 loss: 1.04086903e-05
Iter: 20 loss: 1.47030551e-05
Iter: 21 loss: 1.02621125e-05
Iter: 22 loss: 9.6544145e-06
Iter: 23 loss: 1.1926596e-05
Iter: 24 loss: 9.50703179e-06
Iter: 25 loss: 9.01402382e-06
Iter: 26 loss: 1.34773563e-05
Iter: 27 loss: 8.99142e-06
Iter: 28 loss: 8.68823099e-06
Iter: 29 loss: 8.29024793e-06
Iter: 30 loss: 8.26540054e-06
Iter: 31 loss: 7.66986159e-06
Iter: 32 loss: 1.02300583e-05
Iter: 33 loss: 7.5471562e-06
Iter: 34 loss: 7.0924707e-06
Iter: 35 loss: 7.60932335e-06
Iter: 36 loss: 6.84757651e-06
Iter: 37 loss: 6.44236843e-06
Iter: 38 loss: 9.5400892e-06
Iter: 39 loss: 6.41196266e-06
Iter: 40 loss: 6.33998707e-06
Iter: 41 loss: 6.27226427e-06
Iter: 42 loss: 6.13927523e-06
Iter: 43 loss: 5.97515555e-06
Iter: 44 loss: 5.96080281e-06
Iter: 45 loss: 5.7410607e-06
Iter: 46 loss: 7.1040331e-06
Iter: 47 loss: 5.71521332e-06
Iter: 48 loss: 5.58852753e-06
Iter: 49 loss: 5.52369602e-06
Iter: 50 loss: 5.46462661e-06
Iter: 51 loss: 5.26378744e-06
Iter: 52 loss: 5.86902752e-06
Iter: 53 loss: 5.203417e-06
Iter: 54 loss: 5.00308306e-06
Iter: 55 loss: 5.54604503e-06
Iter: 56 loss: 4.93731795e-06
Iter: 57 loss: 4.79714708e-06
Iter: 58 loss: 5.2442515e-06
Iter: 59 loss: 4.75701108e-06
Iter: 60 loss: 4.61417949e-06
Iter: 61 loss: 5.0772087e-06
Iter: 62 loss: 4.57410624e-06
Iter: 63 loss: 4.41454631e-06
Iter: 64 loss: 4.86618546e-06
Iter: 65 loss: 4.36421942e-06
Iter: 66 loss: 4.24622476e-06
Iter: 67 loss: 4.30293858e-06
Iter: 68 loss: 4.16674447e-06
Iter: 69 loss: 4.01927673e-06
Iter: 70 loss: 4.68120379e-06
Iter: 71 loss: 3.99051305e-06
Iter: 72 loss: 3.88383842e-06
Iter: 73 loss: 3.89412025e-06
Iter: 74 loss: 3.80156825e-06
Iter: 75 loss: 3.7089e-06
Iter: 76 loss: 3.70554471e-06
Iter: 77 loss: 3.63468962e-06
Iter: 78 loss: 4.45414207e-06
Iter: 79 loss: 3.63375761e-06
Iter: 80 loss: 3.59953106e-06
Iter: 81 loss: 3.52800407e-06
Iter: 82 loss: 4.741054e-06
Iter: 83 loss: 3.5263015e-06
Iter: 84 loss: 3.44726163e-06
Iter: 85 loss: 4.33360492e-06
Iter: 86 loss: 3.44572982e-06
Iter: 87 loss: 3.40523752e-06
Iter: 88 loss: 3.3436902e-06
Iter: 89 loss: 3.34276092e-06
Iter: 90 loss: 3.26602776e-06
Iter: 91 loss: 3.83028191e-06
Iter: 92 loss: 3.25964311e-06
Iter: 93 loss: 3.20700497e-06
Iter: 94 loss: 3.3958936e-06
Iter: 95 loss: 3.19345327e-06
Iter: 96 loss: 3.1326415e-06
Iter: 97 loss: 3.07977461e-06
Iter: 98 loss: 3.06346465e-06
Iter: 99 loss: 3.02397029e-06
Iter: 100 loss: 3.01965883e-06
Iter: 101 loss: 2.97814518e-06
Iter: 102 loss: 2.91228457e-06
Iter: 103 loss: 2.91168817e-06
Iter: 104 loss: 2.85222086e-06
Iter: 105 loss: 3.38087625e-06
Iter: 106 loss: 2.84926182e-06
Iter: 107 loss: 2.79919504e-06
Iter: 108 loss: 2.75318621e-06
Iter: 109 loss: 2.74100967e-06
Iter: 110 loss: 2.70164674e-06
Iter: 111 loss: 2.69908833e-06
Iter: 112 loss: 2.67061796e-06
Iter: 113 loss: 3.10302494e-06
Iter: 114 loss: 2.67066707e-06
Iter: 115 loss: 2.65094627e-06
Iter: 116 loss: 2.61486434e-06
Iter: 117 loss: 3.4523764e-06
Iter: 118 loss: 2.61485911e-06
Iter: 119 loss: 2.58815498e-06
Iter: 120 loss: 2.82357928e-06
Iter: 121 loss: 2.58684349e-06
Iter: 122 loss: 2.5590125e-06
Iter: 123 loss: 2.55460759e-06
Iter: 124 loss: 2.53534449e-06
Iter: 125 loss: 2.49855611e-06
Iter: 126 loss: 2.48258652e-06
Iter: 127 loss: 2.46367949e-06
Iter: 128 loss: 2.42752435e-06
Iter: 129 loss: 2.97072484e-06
Iter: 130 loss: 2.42749297e-06
Iter: 131 loss: 2.39622295e-06
Iter: 132 loss: 2.42907163e-06
Iter: 133 loss: 2.37885888e-06
Iter: 134 loss: 2.34845902e-06
Iter: 135 loss: 2.4816004e-06
Iter: 136 loss: 2.34227582e-06
Iter: 137 loss: 2.32174693e-06
Iter: 138 loss: 2.4337437e-06
Iter: 139 loss: 2.31865624e-06
Iter: 140 loss: 2.2934621e-06
Iter: 141 loss: 2.26942188e-06
Iter: 142 loss: 2.26368775e-06
Iter: 143 loss: 2.23628217e-06
Iter: 144 loss: 2.29141e-06
Iter: 145 loss: 2.22513063e-06
Iter: 146 loss: 2.1907922e-06
Iter: 147 loss: 2.35683819e-06
Iter: 148 loss: 2.1847186e-06
Iter: 149 loss: 2.18257856e-06
Iter: 150 loss: 2.17609886e-06
Iter: 151 loss: 2.16651961e-06
Iter: 152 loss: 2.14154784e-06
Iter: 153 loss: 2.33164383e-06
Iter: 154 loss: 2.13662179e-06
Iter: 155 loss: 2.10879034e-06
Iter: 156 loss: 2.24620635e-06
Iter: 157 loss: 2.10406051e-06
Iter: 158 loss: 2.08938536e-06
Iter: 159 loss: 2.31699664e-06
Iter: 160 loss: 2.08941287e-06
Iter: 161 loss: 2.07863582e-06
Iter: 162 loss: 2.06056757e-06
Iter: 163 loss: 2.06059849e-06
Iter: 164 loss: 2.03603736e-06
Iter: 165 loss: 2.09811878e-06
Iter: 166 loss: 2.02742012e-06
Iter: 167 loss: 2.00914815e-06
Iter: 168 loss: 2.09004611e-06
Iter: 169 loss: 2.00552404e-06
Iter: 170 loss: 1.98710677e-06
Iter: 171 loss: 2.08887445e-06
Iter: 172 loss: 1.98444332e-06
Iter: 173 loss: 1.97053214e-06
Iter: 174 loss: 1.95895427e-06
Iter: 175 loss: 1.95498546e-06
Iter: 176 loss: 1.93637743e-06
Iter: 177 loss: 1.93645792e-06
Iter: 178 loss: 1.92511197e-06
Iter: 179 loss: 1.93043229e-06
Iter: 180 loss: 1.91750314e-06
Iter: 181 loss: 1.90388835e-06
Iter: 182 loss: 1.90020887e-06
Iter: 183 loss: 1.89184493e-06
Iter: 184 loss: 1.87406852e-06
Iter: 185 loss: 1.95358939e-06
Iter: 186 loss: 1.87069236e-06
Iter: 187 loss: 1.86319278e-06
Iter: 188 loss: 1.86018929e-06
Iter: 189 loss: 1.85395243e-06
Iter: 190 loss: 1.8435785e-06
Iter: 191 loss: 1.84345822e-06
Iter: 192 loss: 1.83271322e-06
Iter: 193 loss: 1.83118118e-06
Iter: 194 loss: 1.82361555e-06
Iter: 195 loss: 1.8096398e-06
Iter: 196 loss: 2.00871864e-06
Iter: 197 loss: 1.80962866e-06
Iter: 198 loss: 1.8005876e-06
Iter: 199 loss: 1.8011101e-06
Iter: 200 loss: 1.79351434e-06
Iter: 201 loss: 1.78378195e-06
Iter: 202 loss: 1.79597635e-06
Iter: 203 loss: 1.77872653e-06
Iter: 204 loss: 1.76509434e-06
Iter: 205 loss: 1.7826992e-06
Iter: 206 loss: 1.75813454e-06
Iter: 207 loss: 1.7487223e-06
Iter: 208 loss: 1.74858008e-06
Iter: 209 loss: 1.74140541e-06
Iter: 210 loss: 1.72845773e-06
Iter: 211 loss: 2.04459593e-06
Iter: 212 loss: 1.72850173e-06
Iter: 213 loss: 1.71826639e-06
Iter: 214 loss: 1.71798251e-06
Iter: 215 loss: 1.71107081e-06
Iter: 216 loss: 1.71465945e-06
Iter: 217 loss: 1.70647138e-06
Iter: 218 loss: 1.69792474e-06
Iter: 219 loss: 1.71066858e-06
Iter: 220 loss: 1.69376767e-06
Iter: 221 loss: 1.6895068e-06
Iter: 222 loss: 1.68883832e-06
Iter: 223 loss: 1.68290399e-06
Iter: 224 loss: 1.66937707e-06
Iter: 225 loss: 1.85794727e-06
Iter: 226 loss: 1.66875816e-06
Iter: 227 loss: 1.66072869e-06
Iter: 228 loss: 1.69329735e-06
Iter: 229 loss: 1.65892482e-06
Iter: 230 loss: 1.65091024e-06
Iter: 231 loss: 1.67881558e-06
Iter: 232 loss: 1.64888684e-06
Iter: 233 loss: 1.63940422e-06
Iter: 234 loss: 1.66836628e-06
Iter: 235 loss: 1.636663e-06
Iter: 236 loss: 1.63025743e-06
Iter: 237 loss: 1.62546689e-06
Iter: 238 loss: 1.62344736e-06
Iter: 239 loss: 1.61341563e-06
Iter: 240 loss: 1.67690018e-06
Iter: 241 loss: 1.61229536e-06
Iter: 242 loss: 1.60530772e-06
Iter: 243 loss: 1.61361925e-06
Iter: 244 loss: 1.60169589e-06
Iter: 245 loss: 1.59155525e-06
Iter: 246 loss: 1.64066728e-06
Iter: 247 loss: 1.58982857e-06
Iter: 248 loss: 1.5840377e-06
Iter: 249 loss: 1.6030159e-06
Iter: 250 loss: 1.58239584e-06
Iter: 251 loss: 1.57606894e-06
Iter: 252 loss: 1.58158173e-06
Iter: 253 loss: 1.5724205e-06
Iter: 254 loss: 1.56270698e-06
Iter: 255 loss: 1.58252851e-06
Iter: 256 loss: 1.55883686e-06
Iter: 257 loss: 1.55463908e-06
Iter: 258 loss: 1.55466864e-06
Iter: 259 loss: 1.55040948e-06
Iter: 260 loss: 1.5612203e-06
Iter: 261 loss: 1.54900727e-06
Iter: 262 loss: 1.54548025e-06
Iter: 263 loss: 1.53664837e-06
Iter: 264 loss: 1.61109733e-06
Iter: 265 loss: 1.53514486e-06
Iter: 266 loss: 1.52678092e-06
Iter: 267 loss: 1.58492662e-06
Iter: 268 loss: 1.52602945e-06
Iter: 269 loss: 1.52030645e-06
Iter: 270 loss: 1.60345974e-06
Iter: 271 loss: 1.52026871e-06
Iter: 272 loss: 1.51529105e-06
Iter: 273 loss: 1.51127915e-06
Iter: 274 loss: 1.50975939e-06
Iter: 275 loss: 1.50332e-06
Iter: 276 loss: 1.51848542e-06
Iter: 277 loss: 1.50093229e-06
Iter: 278 loss: 1.49428524e-06
Iter: 279 loss: 1.51142603e-06
Iter: 280 loss: 1.49198559e-06
Iter: 281 loss: 1.48635331e-06
Iter: 282 loss: 1.51792358e-06
Iter: 283 loss: 1.48556819e-06
Iter: 284 loss: 1.47928449e-06
Iter: 285 loss: 1.48634024e-06
Iter: 286 loss: 1.47589321e-06
Iter: 287 loss: 1.47008802e-06
Iter: 288 loss: 1.50990843e-06
Iter: 289 loss: 1.46948696e-06
Iter: 290 loss: 1.46543312e-06
Iter: 291 loss: 1.4695097e-06
Iter: 292 loss: 1.4631014e-06
Iter: 293 loss: 1.45664899e-06
Iter: 294 loss: 1.46882041e-06
Iter: 295 loss: 1.45387116e-06
Iter: 296 loss: 1.45324225e-06
Iter: 297 loss: 1.45134504e-06
Iter: 298 loss: 1.44987166e-06
Iter: 299 loss: 1.44670912e-06
Iter: 300 loss: 1.50218898e-06
Iter: 301 loss: 1.44663829e-06
Iter: 302 loss: 1.44265482e-06
Iter: 303 loss: 1.43730563e-06
Iter: 304 loss: 1.43700447e-06
Iter: 305 loss: 1.43150351e-06
Iter: 306 loss: 1.46281263e-06
Iter: 307 loss: 1.43076704e-06
Iter: 308 loss: 1.42451393e-06
Iter: 309 loss: 1.46732214e-06
Iter: 310 loss: 1.42384738e-06
Iter: 311 loss: 1.42092824e-06
Iter: 312 loss: 1.41545797e-06
Iter: 313 loss: 1.5415759e-06
Iter: 314 loss: 1.41544501e-06
Iter: 315 loss: 1.40801046e-06
Iter: 316 loss: 1.45114655e-06
Iter: 317 loss: 1.40713e-06
Iter: 318 loss: 1.40235943e-06
Iter: 319 loss: 1.41501482e-06
Iter: 320 loss: 1.40079362e-06
Iter: 321 loss: 1.39560711e-06
Iter: 322 loss: 1.42624901e-06
Iter: 323 loss: 1.39497502e-06
Iter: 324 loss: 1.39055987e-06
Iter: 325 loss: 1.39394456e-06
Iter: 326 loss: 1.38780706e-06
Iter: 327 loss: 1.38298162e-06
Iter: 328 loss: 1.4095981e-06
Iter: 329 loss: 1.3821857e-06
Iter: 330 loss: 1.37860707e-06
Iter: 331 loss: 1.39425015e-06
Iter: 332 loss: 1.37786083e-06
Iter: 333 loss: 1.3747765e-06
Iter: 334 loss: 1.4036259e-06
Iter: 335 loss: 1.37461598e-06
Iter: 336 loss: 1.37141251e-06
Iter: 337 loss: 1.3679462e-06
Iter: 338 loss: 1.36745018e-06
Iter: 339 loss: 1.36339315e-06
Iter: 340 loss: 1.3647666e-06
Iter: 341 loss: 1.36047424e-06
Iter: 342 loss: 1.3565591e-06
Iter: 343 loss: 1.3685285e-06
Iter: 344 loss: 1.35538505e-06
Iter: 345 loss: 1.35118921e-06
Iter: 346 loss: 1.37292079e-06
Iter: 347 loss: 1.35047537e-06
Iter: 348 loss: 1.34579068e-06
Iter: 349 loss: 1.35928951e-06
Iter: 350 loss: 1.34423203e-06
Iter: 351 loss: 1.34189213e-06
Iter: 352 loss: 1.33714718e-06
Iter: 353 loss: 1.4304793e-06
Iter: 354 loss: 1.33707715e-06
Iter: 355 loss: 1.33158551e-06
Iter: 356 loss: 1.3925503e-06
Iter: 357 loss: 1.33147046e-06
Iter: 358 loss: 1.32819264e-06
Iter: 359 loss: 1.33952676e-06
Iter: 360 loss: 1.32728951e-06
Iter: 361 loss: 1.32336447e-06
Iter: 362 loss: 1.33437106e-06
Iter: 363 loss: 1.32214268e-06
Iter: 364 loss: 1.31889215e-06
Iter: 365 loss: 1.32351443e-06
Iter: 366 loss: 1.31732202e-06
Iter: 367 loss: 1.31349043e-06
Iter: 368 loss: 1.33715218e-06
Iter: 369 loss: 1.31311742e-06
Iter: 370 loss: 1.31070124e-06
Iter: 371 loss: 1.34851211e-06
Iter: 372 loss: 1.31074341e-06
Iter: 373 loss: 1.30888884e-06
Iter: 374 loss: 1.30648186e-06
Iter: 375 loss: 1.30639194e-06
Iter: 376 loss: 1.30261219e-06
Iter: 377 loss: 1.30057833e-06
Iter: 378 loss: 1.29888497e-06
Iter: 379 loss: 1.2952836e-06
Iter: 380 loss: 1.31140109e-06
Iter: 381 loss: 1.29460216e-06
Iter: 382 loss: 1.29106684e-06
Iter: 383 loss: 1.3053849e-06
Iter: 384 loss: 1.29027444e-06
Iter: 385 loss: 1.28683371e-06
Iter: 386 loss: 1.31085415e-06
Iter: 387 loss: 1.28647912e-06
Iter: 388 loss: 1.28427678e-06
Iter: 389 loss: 1.2808199e-06
Iter: 390 loss: 1.28084832e-06
Iter: 391 loss: 1.27706778e-06
Iter: 392 loss: 1.30461444e-06
Iter: 393 loss: 1.27672911e-06
Iter: 394 loss: 1.27374699e-06
Iter: 395 loss: 1.27363217e-06
Iter: 396 loss: 1.27140697e-06
Iter: 397 loss: 1.26777741e-06
Iter: 398 loss: 1.26775808e-06
Iter: 399 loss: 1.2657373e-06
Iter: 400 loss: 1.26895861e-06
Iter: 401 loss: 1.26480063e-06
Iter: 402 loss: 1.26256987e-06
Iter: 403 loss: 1.26851205e-06
Iter: 404 loss: 1.2618309e-06
Iter: 405 loss: 1.25880717e-06
Iter: 406 loss: 1.27452176e-06
Iter: 407 loss: 1.25840199e-06
Iter: 408 loss: 1.25633392e-06
Iter: 409 loss: 1.25894621e-06
Iter: 410 loss: 1.2552473e-06
Iter: 411 loss: 1.25350732e-06
Iter: 412 loss: 1.2517653e-06
Iter: 413 loss: 1.25140537e-06
Iter: 414 loss: 1.24800897e-06
Iter: 415 loss: 1.25483916e-06
Iter: 416 loss: 1.2466603e-06
Iter: 417 loss: 1.24352482e-06
Iter: 418 loss: 1.26050134e-06
Iter: 419 loss: 1.24301221e-06
Iter: 420 loss: 1.24084659e-06
Iter: 421 loss: 1.26413556e-06
Iter: 422 loss: 1.2407861e-06
Iter: 423 loss: 1.23870552e-06
Iter: 424 loss: 1.23473819e-06
Iter: 425 loss: 1.31865772e-06
Iter: 426 loss: 1.23473205e-06
Iter: 427 loss: 1.23122038e-06
Iter: 428 loss: 1.24270809e-06
Iter: 429 loss: 1.23030895e-06
Iter: 430 loss: 1.22652887e-06
Iter: 431 loss: 1.24442477e-06
Iter: 432 loss: 1.2258896e-06
Iter: 433 loss: 1.22390202e-06
Iter: 434 loss: 1.24162693e-06
Iter: 435 loss: 1.2238072e-06
Iter: 436 loss: 1.22157826e-06
Iter: 437 loss: 1.22062795e-06
Iter: 438 loss: 1.21945993e-06
Iter: 439 loss: 1.21849621e-06
Iter: 440 loss: 1.21817891e-06
Iter: 441 loss: 1.21693608e-06
Iter: 442 loss: 1.21672872e-06
Iter: 443 loss: 1.2158647e-06
Iter: 444 loss: 1.21401456e-06
Iter: 445 loss: 1.21284245e-06
Iter: 446 loss: 1.21207938e-06
Iter: 447 loss: 1.20928405e-06
Iter: 448 loss: 1.21848575e-06
Iter: 449 loss: 1.20847551e-06
Iter: 450 loss: 1.20601248e-06
Iter: 451 loss: 1.20475238e-06
Iter: 452 loss: 1.20364052e-06
Iter: 453 loss: 1.20092932e-06
Iter: 454 loss: 1.24107942e-06
Iter: 455 loss: 1.20094955e-06
Iter: 456 loss: 1.19902347e-06
Iter: 457 loss: 1.2065866e-06
Iter: 458 loss: 1.19863989e-06
Iter: 459 loss: 1.19638821e-06
Iter: 460 loss: 1.19720812e-06
Iter: 461 loss: 1.19476965e-06
Iter: 462 loss: 1.19305514e-06
Iter: 463 loss: 1.19207391e-06
Iter: 464 loss: 1.19127196e-06
Iter: 465 loss: 1.18831258e-06
Iter: 466 loss: 1.20560821e-06
Iter: 467 loss: 1.18790126e-06
Iter: 468 loss: 1.18557216e-06
Iter: 469 loss: 1.19123638e-06
Iter: 470 loss: 1.1847319e-06
Iter: 471 loss: 1.18202206e-06
Iter: 472 loss: 1.20247648e-06
Iter: 473 loss: 1.18182663e-06
Iter: 474 loss: 1.18070807e-06
Iter: 475 loss: 1.19422282e-06
Iter: 476 loss: 1.18064747e-06
Iter: 477 loss: 1.17940476e-06
Iter: 478 loss: 1.17825425e-06
Iter: 479 loss: 1.17795173e-06
Iter: 480 loss: 1.17619561e-06
Iter: 481 loss: 1.17741331e-06
Iter: 482 loss: 1.17514548e-06
Iter: 483 loss: 1.17296133e-06
Iter: 484 loss: 1.17949503e-06
Iter: 485 loss: 1.17225363e-06
Iter: 486 loss: 1.17015793e-06
Iter: 487 loss: 1.16984438e-06
Iter: 488 loss: 1.16834974e-06
Iter: 489 loss: 1.16554327e-06
Iter: 490 loss: 1.17953857e-06
Iter: 491 loss: 1.16508954e-06
Iter: 492 loss: 1.16350395e-06
Iter: 493 loss: 1.1634919e-06
Iter: 494 loss: 1.16225976e-06
Iter: 495 loss: 1.16117099e-06
Iter: 496 loss: 1.16080651e-06
Iter: 497 loss: 1.15872115e-06
Iter: 498 loss: 1.16198044e-06
Iter: 499 loss: 1.15769183e-06
Iter: 500 loss: 1.15525256e-06
Iter: 501 loss: 1.15527337e-06
Iter: 502 loss: 1.1532934e-06
Iter: 503 loss: 1.15121679e-06
Iter: 504 loss: 1.15119246e-06
Iter: 505 loss: 1.14993111e-06
Iter: 506 loss: 1.15845887e-06
Iter: 507 loss: 1.14979707e-06
Iter: 508 loss: 1.14860609e-06
Iter: 509 loss: 1.15192984e-06
Iter: 510 loss: 1.14816362e-06
Iter: 511 loss: 1.14635316e-06
Iter: 512 loss: 1.14786235e-06
Iter: 513 loss: 1.14527575e-06
Iter: 514 loss: 1.14375212e-06
Iter: 515 loss: 1.14237616e-06
Iter: 516 loss: 1.14198383e-06
Iter: 517 loss: 1.13968986e-06
Iter: 518 loss: 1.15437842e-06
Iter: 519 loss: 1.13945816e-06
Iter: 520 loss: 1.13750866e-06
Iter: 521 loss: 1.14106945e-06
Iter: 522 loss: 1.13669864e-06
Iter: 523 loss: 1.13477086e-06
Iter: 524 loss: 1.13523311e-06
Iter: 525 loss: 1.13336694e-06
Iter: 526 loss: 1.13174917e-06
Iter: 527 loss: 1.13176225e-06
Iter: 528 loss: 1.13013209e-06
Iter: 529 loss: 1.13046565e-06
Iter: 530 loss: 1.12893986e-06
Iter: 531 loss: 1.12670591e-06
Iter: 532 loss: 1.12982491e-06
Iter: 533 loss: 1.12558348e-06
Iter: 534 loss: 1.12373766e-06
Iter: 535 loss: 1.12344105e-06
Iter: 536 loss: 1.12223029e-06
Iter: 537 loss: 1.11992631e-06
Iter: 538 loss: 1.14793772e-06
Iter: 539 loss: 1.11983422e-06
Iter: 540 loss: 1.11837255e-06
Iter: 541 loss: 1.12038811e-06
Iter: 542 loss: 1.11764757e-06
Iter: 543 loss: 1.11576753e-06
Iter: 544 loss: 1.13891906e-06
Iter: 545 loss: 1.11574468e-06
Iter: 546 loss: 1.11472036e-06
Iter: 547 loss: 1.11845839e-06
Iter: 548 loss: 1.11445752e-06
Iter: 549 loss: 1.11376812e-06
Iter: 550 loss: 1.11221652e-06
Iter: 551 loss: 1.13543672e-06
Iter: 552 loss: 1.11216582e-06
Iter: 553 loss: 1.11004772e-06
Iter: 554 loss: 1.11583086e-06
Iter: 555 loss: 1.1094063e-06
Iter: 556 loss: 1.10772737e-06
Iter: 557 loss: 1.12446241e-06
Iter: 558 loss: 1.10766121e-06
Iter: 559 loss: 1.10633709e-06
Iter: 560 loss: 1.10403e-06
Iter: 561 loss: 1.1040031e-06
Iter: 562 loss: 1.10149745e-06
Iter: 563 loss: 1.12389762e-06
Iter: 564 loss: 1.10135966e-06
Iter: 565 loss: 1.09981374e-06
Iter: 566 loss: 1.1231366e-06
Iter: 567 loss: 1.09978077e-06
Iter: 568 loss: 1.09875987e-06
Iter: 569 loss: 1.09679331e-06
Iter: 570 loss: 1.14012403e-06
Iter: 571 loss: 1.09681548e-06
Iter: 572 loss: 1.09451378e-06
Iter: 573 loss: 1.10607425e-06
Iter: 574 loss: 1.09413384e-06
Iter: 575 loss: 1.09259872e-06
Iter: 576 loss: 1.09321456e-06
Iter: 577 loss: 1.09150767e-06
Iter: 578 loss: 1.08979839e-06
Iter: 579 loss: 1.11294219e-06
Iter: 580 loss: 1.0897752e-06
Iter: 581 loss: 1.08875702e-06
Iter: 582 loss: 1.08878157e-06
Iter: 583 loss: 1.08791687e-06
Iter: 584 loss: 1.08647464e-06
Iter: 585 loss: 1.08645656e-06
Iter: 586 loss: 1.08464758e-06
Iter: 587 loss: 1.09172925e-06
Iter: 588 loss: 1.08416646e-06
Iter: 589 loss: 1.08302538e-06
Iter: 590 loss: 1.08187328e-06
Iter: 591 loss: 1.081645e-06
Iter: 592 loss: 1.07943583e-06
Iter: 593 loss: 1.09265955e-06
Iter: 594 loss: 1.07911865e-06
Iter: 595 loss: 1.07752544e-06
Iter: 596 loss: 1.08401457e-06
Iter: 597 loss: 1.07716357e-06
Iter: 598 loss: 1.07583162e-06
Iter: 599 loss: 1.07665073e-06
Iter: 600 loss: 1.07500023e-06
Iter: 601 loss: 1.07395181e-06
Iter: 602 loss: 1.07395272e-06
Iter: 603 loss: 1.07297012e-06
Iter: 604 loss: 1.07101243e-06
Iter: 605 loss: 1.10794826e-06
Iter: 606 loss: 1.07101391e-06
Iter: 607 loss: 1.06915434e-06
Iter: 608 loss: 1.08391691e-06
Iter: 609 loss: 1.0689663e-06
Iter: 610 loss: 1.06770904e-06
Iter: 611 loss: 1.06651589e-06
Iter: 612 loss: 1.06620882e-06
Iter: 613 loss: 1.06405878e-06
Iter: 614 loss: 1.08557151e-06
Iter: 615 loss: 1.06397977e-06
Iter: 616 loss: 1.06353991e-06
Iter: 617 loss: 1.06318851e-06
Iter: 618 loss: 1.06269613e-06
Iter: 619 loss: 1.06162247e-06
Iter: 620 loss: 1.08025358e-06
Iter: 621 loss: 1.06156835e-06
Iter: 622 loss: 1.0601201e-06
Iter: 623 loss: 1.0623038e-06
Iter: 624 loss: 1.05942649e-06
Iter: 625 loss: 1.05797744e-06
Iter: 626 loss: 1.06335858e-06
Iter: 627 loss: 1.05761512e-06
Iter: 628 loss: 1.0564288e-06
Iter: 629 loss: 1.05701235e-06
Iter: 630 loss: 1.05568529e-06
Iter: 631 loss: 1.05417416e-06
Iter: 632 loss: 1.06560788e-06
Iter: 633 loss: 1.05406116e-06
Iter: 634 loss: 1.05291076e-06
Iter: 635 loss: 1.05307527e-06
Iter: 636 loss: 1.05202139e-06
Iter: 637 loss: 1.05079437e-06
Iter: 638 loss: 1.06537107e-06
Iter: 639 loss: 1.05078323e-06
Iter: 640 loss: 1.04965102e-06
Iter: 641 loss: 1.04929404e-06
Iter: 642 loss: 1.04865717e-06
Iter: 643 loss: 1.04729077e-06
Iter: 644 loss: 1.04887681e-06
Iter: 645 loss: 1.04656488e-06
Iter: 646 loss: 1.04498019e-06
Iter: 647 loss: 1.04707965e-06
Iter: 648 loss: 1.04405319e-06
Iter: 649 loss: 1.04264086e-06
Iter: 650 loss: 1.05535605e-06
Iter: 651 loss: 1.04259789e-06
Iter: 652 loss: 1.04122807e-06
Iter: 653 loss: 1.0550234e-06
Iter: 654 loss: 1.04120068e-06
Iter: 655 loss: 1.04041828e-06
Iter: 656 loss: 1.03956791e-06
Iter: 657 loss: 1.03946434e-06
Iter: 658 loss: 1.03836862e-06
Iter: 659 loss: 1.04329047e-06
Iter: 660 loss: 1.03811556e-06
Iter: 661 loss: 1.03713091e-06
Iter: 662 loss: 1.03831076e-06
Iter: 663 loss: 1.03656907e-06
Iter: 664 loss: 1.0355252e-06
Iter: 665 loss: 1.03557545e-06
Iter: 666 loss: 1.03469233e-06
Iter: 667 loss: 1.03266302e-06
Iter: 668 loss: 1.03755076e-06
Iter: 669 loss: 1.03194657e-06
Iter: 670 loss: 1.03049081e-06
Iter: 671 loss: 1.0463699e-06
Iter: 672 loss: 1.03046602e-06
Iter: 673 loss: 1.0296053e-06
Iter: 674 loss: 1.02976651e-06
Iter: 675 loss: 1.02890988e-06
Iter: 676 loss: 1.02724425e-06
Iter: 677 loss: 1.03112313e-06
Iter: 678 loss: 1.02661818e-06
Iter: 679 loss: 1.02566332e-06
Iter: 680 loss: 1.02619856e-06
Iter: 681 loss: 1.0250119e-06
Iter: 682 loss: 1.02342744e-06
Iter: 683 loss: 1.02178967e-06
Iter: 684 loss: 1.02149352e-06
Iter: 685 loss: 1.02110312e-06
Iter: 686 loss: 1.02059403e-06
Iter: 687 loss: 1.01961348e-06
Iter: 688 loss: 1.02158674e-06
Iter: 689 loss: 1.01921466e-06
Iter: 690 loss: 1.01851083e-06
Iter: 691 loss: 1.01812248e-06
Iter: 692 loss: 1.01783871e-06
Iter: 693 loss: 1.01676733e-06
Iter: 694 loss: 1.02040417e-06
Iter: 695 loss: 1.01646719e-06
Iter: 696 loss: 1.01538285e-06
Iter: 697 loss: 1.01679097e-06
Iter: 698 loss: 1.01476962e-06
Iter: 699 loss: 1.01370631e-06
Iter: 700 loss: 1.01604985e-06
Iter: 701 loss: 1.01326827e-06
Iter: 702 loss: 1.01199066e-06
Iter: 703 loss: 1.01431e-06
Iter: 704 loss: 1.01139381e-06
Iter: 705 loss: 1.00983357e-06
Iter: 706 loss: 1.01657372e-06
Iter: 707 loss: 1.00947523e-06
Iter: 708 loss: 1.00832244e-06
Iter: 709 loss: 1.01304829e-06
Iter: 710 loss: 1.00810621e-06
Iter: 711 loss: 1.00711657e-06
Iter: 712 loss: 1.01233297e-06
Iter: 713 loss: 1.00691159e-06
Iter: 714 loss: 1.00610339e-06
Iter: 715 loss: 1.00513796e-06
Iter: 716 loss: 1.00502473e-06
Iter: 717 loss: 1.00350485e-06
Iter: 718 loss: 1.00474188e-06
Iter: 719 loss: 1.00253931e-06
Iter: 720 loss: 1.00103136e-06
Iter: 721 loss: 1.02280444e-06
Iter: 722 loss: 1.00103432e-06
Iter: 723 loss: 1.00005104e-06
Iter: 724 loss: 1.00005161e-06
Iter: 725 loss: 9.99652229e-07
Iter: 726 loss: 9.98604492e-07
Iter: 727 loss: 1.00524812e-06
Iter: 728 loss: 9.9831891e-07
Iter: 729 loss: 9.97117354e-07
Iter: 730 loss: 1.00572242e-06
Iter: 731 loss: 9.96984681e-07
Iter: 732 loss: 9.96027325e-07
Iter: 733 loss: 1.0020583e-06
Iter: 734 loss: 9.95846449e-07
Iter: 735 loss: 9.95064852e-07
Iter: 736 loss: 9.93863523e-07
Iter: 737 loss: 9.93840558e-07
Iter: 738 loss: 9.9254e-07
Iter: 739 loss: 1.00677573e-06
Iter: 740 loss: 9.92536343e-07
Iter: 741 loss: 9.91444495e-07
Iter: 742 loss: 9.92467676e-07
Iter: 743 loss: 9.90787385e-07
Iter: 744 loss: 9.89893579e-07
Iter: 745 loss: 1.00299417e-06
Iter: 746 loss: 9.89896193e-07
Iter: 747 loss: 9.89378918e-07
Iter: 748 loss: 9.89797286e-07
Iter: 749 loss: 9.89060482e-07
Iter: 750 loss: 9.88051397e-07
Iter: 751 loss: 9.86086661e-07
Iter: 752 loss: 1.02772526e-06
Iter: 753 loss: 9.8608848e-07
Iter: 754 loss: 9.84190365e-07
Iter: 755 loss: 9.99396207e-07
Iter: 756 loss: 9.84078156e-07
Iter: 757 loss: 9.82987217e-07
Iter: 758 loss: 9.92278274e-07
Iter: 759 loss: 9.8290036e-07
Iter: 760 loss: 9.81946528e-07
Iter: 761 loss: 9.90124818e-07
Iter: 762 loss: 9.81903895e-07
Iter: 763 loss: 9.81522589e-07
Iter: 764 loss: 9.80479854e-07
Iter: 765 loss: 9.85855081e-07
Iter: 766 loss: 9.80161076e-07
Iter: 767 loss: 9.78585831e-07
Iter: 768 loss: 9.87139856e-07
Iter: 769 loss: 9.78336629e-07
Iter: 770 loss: 9.77212e-07
Iter: 771 loss: 9.88053785e-07
Iter: 772 loss: 9.77225909e-07
Iter: 773 loss: 9.76321417e-07
Iter: 774 loss: 9.75418288e-07
Iter: 775 loss: 9.75200237e-07
Iter: 776 loss: 9.74172e-07
Iter: 777 loss: 9.81425728e-07
Iter: 778 loss: 9.74065415e-07
Iter: 779 loss: 9.73057e-07
Iter: 780 loss: 9.75733428e-07
Iter: 781 loss: 9.72703674e-07
Iter: 782 loss: 9.71798841e-07
Iter: 783 loss: 9.76345746e-07
Iter: 784 loss: 9.71597842e-07
Iter: 785 loss: 9.70675501e-07
Iter: 786 loss: 9.71179702e-07
Iter: 787 loss: 9.70089104e-07
Iter: 788 loss: 9.68800123e-07
Iter: 789 loss: 9.7358668e-07
Iter: 790 loss: 9.68524e-07
Iter: 791 loss: 9.67770234e-07
Iter: 792 loss: 9.68671429e-07
Iter: 793 loss: 9.67357664e-07
Iter: 794 loss: 9.66739321e-07
Iter: 795 loss: 9.66761e-07
Iter: 796 loss: 9.66062089e-07
Iter: 797 loss: 9.65333356e-07
Iter: 798 loss: 9.65177378e-07
Iter: 799 loss: 9.64350306e-07
Iter: 800 loss: 9.63729576e-07
Iter: 801 loss: 9.63454454e-07
Iter: 802 loss: 9.61962428e-07
Iter: 803 loss: 9.6591566e-07
Iter: 804 loss: 9.61439582e-07
Iter: 805 loss: 9.60087846e-07
Iter: 806 loss: 9.76364618e-07
Iter: 807 loss: 9.60027137e-07
Iter: 808 loss: 9.59294539e-07
Iter: 809 loss: 9.59544195e-07
Iter: 810 loss: 9.58772716e-07
Iter: 811 loss: 9.5778887e-07
Iter: 812 loss: 9.58527835e-07
Iter: 813 loss: 9.57167572e-07
Iter: 814 loss: 9.56175427e-07
Iter: 815 loss: 9.70472684e-07
Iter: 816 loss: 9.56205099e-07
Iter: 817 loss: 9.55456e-07
Iter: 818 loss: 9.55749442e-07
Iter: 819 loss: 9.54946131e-07
Iter: 820 loss: 9.53925564e-07
Iter: 821 loss: 9.5861742e-07
Iter: 822 loss: 9.53783456e-07
Iter: 823 loss: 9.53028291e-07
Iter: 824 loss: 9.54321536e-07
Iter: 825 loss: 9.52748792e-07
Iter: 826 loss: 9.51911431e-07
Iter: 827 loss: 9.51543313e-07
Iter: 828 loss: 9.51167522e-07
Iter: 829 loss: 9.50883759e-07
Iter: 830 loss: 9.50509616e-07
Iter: 831 loss: 9.49992341e-07
Iter: 832 loss: 9.49495757e-07
Iter: 833 loss: 9.49391733e-07
Iter: 834 loss: 9.48704098e-07
Iter: 835 loss: 9.46843556e-07
Iter: 836 loss: 9.59018507e-07
Iter: 837 loss: 9.46423256e-07
Iter: 838 loss: 9.46211e-07
Iter: 839 loss: 9.45571514e-07
Iter: 840 loss: 9.44876547e-07
Iter: 841 loss: 9.4545328e-07
Iter: 842 loss: 9.44477392e-07
Iter: 843 loss: 9.43412488e-07
Iter: 844 loss: 9.43901114e-07
Iter: 845 loss: 9.42711836e-07
Iter: 846 loss: 9.41465e-07
Iter: 847 loss: 9.42471672e-07
Iter: 848 loss: 9.40744656e-07
Iter: 849 loss: 9.39845904e-07
Iter: 850 loss: 9.39808842e-07
Iter: 851 loss: 9.39204199e-07
Iter: 852 loss: 9.41525343e-07
Iter: 853 loss: 9.38975859e-07
Iter: 854 loss: 9.38263781e-07
Iter: 855 loss: 9.38092228e-07
Iter: 856 loss: 9.37574896e-07
Iter: 857 loss: 9.3653432e-07
Iter: 858 loss: 9.42004476e-07
Iter: 859 loss: 9.36353e-07
Iter: 860 loss: 9.35564231e-07
Iter: 861 loss: 9.35028197e-07
Iter: 862 loss: 9.34733123e-07
Iter: 863 loss: 9.34211073e-07
Iter: 864 loss: 9.34058221e-07
Iter: 865 loss: 9.33456477e-07
Iter: 866 loss: 9.33768433e-07
Iter: 867 loss: 9.33034073e-07
Iter: 868 loss: 9.32418402e-07
Iter: 869 loss: 9.31246177e-07
Iter: 870 loss: 9.52927451e-07
Iter: 871 loss: 9.3122037e-07
Iter: 872 loss: 9.29961118e-07
Iter: 873 loss: 9.35605158e-07
Iter: 874 loss: 9.29715213e-07
Iter: 875 loss: 9.2870107e-07
Iter: 876 loss: 9.32773617e-07
Iter: 877 loss: 9.28497172e-07
Iter: 878 loss: 9.27456256e-07
Iter: 879 loss: 9.34409172e-07
Iter: 880 loss: 9.27347912e-07
Iter: 881 loss: 9.26655503e-07
Iter: 882 loss: 9.27406063e-07
Iter: 883 loss: 9.26286532e-07
Iter: 884 loss: 9.25531822e-07
Iter: 885 loss: 9.25670633e-07
Iter: 886 loss: 9.24982601e-07
Iter: 887 loss: 9.24086351e-07
Iter: 888 loss: 9.35194521e-07
Iter: 889 loss: 9.24053e-07
Iter: 890 loss: 9.23389e-07
Iter: 891 loss: 9.25462132e-07
Iter: 892 loss: 9.23227844e-07
Iter: 893 loss: 9.22593927e-07
Iter: 894 loss: 9.2308062e-07
Iter: 895 loss: 9.22213e-07
Iter: 896 loss: 9.21304547e-07
Iter: 897 loss: 9.22134404e-07
Iter: 898 loss: 9.20759419e-07
Iter: 899 loss: 9.19937236e-07
Iter: 900 loss: 9.2945254e-07
Iter: 901 loss: 9.19917738e-07
Iter: 902 loss: 9.19049967e-07
Iter: 903 loss: 9.21144363e-07
Iter: 904 loss: 9.1874756e-07
Iter: 905 loss: 9.18211811e-07
Iter: 906 loss: 9.17568e-07
Iter: 907 loss: 9.17450052e-07
Iter: 908 loss: 9.16558e-07
Iter: 909 loss: 9.16548515e-07
Iter: 910 loss: 9.15857299e-07
Iter: 911 loss: 9.14607313e-07
Iter: 912 loss: 9.22222966e-07
Iter: 913 loss: 9.1445e-07
Iter: 914 loss: 9.13411839e-07
Iter: 915 loss: 9.1556609e-07
Iter: 916 loss: 9.12947e-07
Iter: 917 loss: 9.11788788e-07
Iter: 918 loss: 9.21969615e-07
Iter: 919 loss: 9.11708185e-07
Iter: 920 loss: 9.1111076e-07
Iter: 921 loss: 9.10478832e-07
Iter: 922 loss: 9.10420567e-07
Iter: 923 loss: 9.09451273e-07
Iter: 924 loss: 9.16924137e-07
Iter: 925 loss: 9.09370556e-07
Iter: 926 loss: 9.08853394e-07
Iter: 927 loss: 9.15114924e-07
Iter: 928 loss: 9.08849643e-07
Iter: 929 loss: 9.08392735e-07
Iter: 930 loss: 9.07526e-07
Iter: 931 loss: 9.26125608e-07
Iter: 932 loss: 9.07545882e-07
Iter: 933 loss: 9.06583864e-07
Iter: 934 loss: 9.18086357e-07
Iter: 935 loss: 9.0657943e-07
Iter: 936 loss: 9.06008495e-07
Iter: 937 loss: 9.07474885e-07
Iter: 938 loss: 9.05827676e-07
Iter: 939 loss: 9.05083425e-07
Iter: 940 loss: 9.07471872e-07
Iter: 941 loss: 9.04868443e-07
Iter: 942 loss: 9.04355602e-07
Iter: 943 loss: 9.03810587e-07
Iter: 944 loss: 9.03727653e-07
Iter: 945 loss: 9.02970555e-07
Iter: 946 loss: 9.03747434e-07
Iter: 947 loss: 9.02505576e-07
Iter: 948 loss: 9.01483304e-07
Iter: 949 loss: 9.02316572e-07
Iter: 950 loss: 9.00935675e-07
Iter: 951 loss: 8.99919257e-07
Iter: 952 loss: 9.11051643e-07
Iter: 953 loss: 8.99894189e-07
Iter: 954 loss: 8.9909787e-07
Iter: 955 loss: 9.01695898e-07
Iter: 956 loss: 8.98893518e-07
Iter: 957 loss: 8.98065423e-07
Iter: 958 loss: 8.99098666e-07
Iter: 959 loss: 8.97593452e-07
Iter: 960 loss: 8.96935717e-07
Iter: 961 loss: 8.97405812e-07
Iter: 962 loss: 8.96525933e-07
Iter: 963 loss: 8.95435278e-07
Iter: 964 loss: 9.01752799e-07
Iter: 965 loss: 8.95336029e-07
Iter: 966 loss: 8.94509469e-07
Iter: 967 loss: 8.96854203e-07
Iter: 968 loss: 8.94227469e-07
Iter: 969 loss: 8.93670403e-07
Iter: 970 loss: 8.94842628e-07
Iter: 971 loss: 8.93439619e-07
Iter: 972 loss: 8.92719186e-07
Iter: 973 loss: 8.96838912e-07
Iter: 974 loss: 8.92631931e-07
Iter: 975 loss: 8.91952595e-07
Iter: 976 loss: 8.95450967e-07
Iter: 977 loss: 8.9180827e-07
Iter: 978 loss: 8.9143623e-07
Iter: 979 loss: 8.90444539e-07
Iter: 980 loss: 8.97908365e-07
Iter: 981 loss: 8.90203523e-07
Iter: 982 loss: 8.89092291e-07
Iter: 983 loss: 9.00409304e-07
Iter: 984 loss: 8.89071259e-07
Iter: 985 loss: 8.88319221e-07
Iter: 986 loss: 8.88256693e-07
Iter: 987 loss: 8.87746296e-07
Iter: 988 loss: 8.86893758e-07
Iter: 989 loss: 8.94321886e-07
Iter: 990 loss: 8.86830946e-07
Iter: 991 loss: 8.8616531e-07
Iter: 992 loss: 8.89634464e-07
Iter: 993 loss: 8.86003e-07
Iter: 994 loss: 8.85236432e-07
Iter: 995 loss: 8.85319821e-07
Iter: 996 loss: 8.84702672e-07
Iter: 997 loss: 8.83718315e-07
Iter: 998 loss: 8.84605e-07
Iter: 999 loss: 8.8312504e-07
Iter: 1000 loss: 8.8234475e-07
Iter: 1001 loss: 8.82333552e-07
Iter: 1002 loss: 8.81720666e-07
Iter: 1003 loss: 8.83665166e-07
Iter: 1004 loss: 8.81494771e-07
Iter: 1005 loss: 8.81040705e-07
Iter: 1006 loss: 8.80565551e-07
Iter: 1007 loss: 8.8041287e-07
Iter: 1008 loss: 8.79821641e-07
Iter: 1009 loss: 8.79773722e-07
Iter: 1010 loss: 8.79289132e-07
Iter: 1011 loss: 8.80311291e-07
Iter: 1012 loss: 8.79096604e-07
Iter: 1013 loss: 8.78687842e-07
Iter: 1014 loss: 8.7804483e-07
Iter: 1015 loss: 8.78032324e-07
Iter: 1016 loss: 8.77127491e-07
Iter: 1017 loss: 8.77483615e-07
Iter: 1018 loss: 8.7649687e-07
Iter: 1019 loss: 8.75476e-07
Iter: 1020 loss: 8.82067752e-07
Iter: 1021 loss: 8.75350622e-07
Iter: 1022 loss: 8.74536624e-07
Iter: 1023 loss: 8.75670764e-07
Iter: 1024 loss: 8.74157081e-07
Iter: 1025 loss: 8.73262593e-07
Iter: 1026 loss: 8.78424032e-07
Iter: 1027 loss: 8.73109457e-07
Iter: 1028 loss: 8.72349915e-07
Iter: 1029 loss: 8.76735896e-07
Iter: 1030 loss: 8.72271357e-07
Iter: 1031 loss: 8.71595375e-07
Iter: 1032 loss: 8.71397447e-07
Iter: 1033 loss: 8.71034217e-07
Iter: 1034 loss: 8.70421331e-07
Iter: 1035 loss: 8.74783609e-07
Iter: 1036 loss: 8.70383246e-07
Iter: 1037 loss: 8.69654968e-07
Iter: 1038 loss: 8.70863573e-07
Iter: 1039 loss: 8.69337327e-07
Iter: 1040 loss: 8.68571078e-07
Iter: 1041 loss: 8.70260692e-07
Iter: 1042 loss: 8.68266909e-07
Iter: 1043 loss: 8.67939036e-07
Iter: 1044 loss: 8.67958249e-07
Iter: 1045 loss: 8.67574272e-07
Iter: 1046 loss: 8.66825758e-07
Iter: 1047 loss: 8.81243068e-07
Iter: 1048 loss: 8.66782671e-07
Iter: 1049 loss: 8.65928e-07
Iter: 1050 loss: 8.70291444e-07
Iter: 1051 loss: 8.65838274e-07
Iter: 1052 loss: 8.65274615e-07
Iter: 1053 loss: 8.64616482e-07
Iter: 1054 loss: 8.6454e-07
Iter: 1055 loss: 8.63713694e-07
Iter: 1056 loss: 8.68736038e-07
Iter: 1057 loss: 8.63634114e-07
Iter: 1058 loss: 8.6279249e-07
Iter: 1059 loss: 8.62635318e-07
Iter: 1060 loss: 8.62087745e-07
Iter: 1061 loss: 8.61221793e-07
Iter: 1062 loss: 8.72409544e-07
Iter: 1063 loss: 8.61193769e-07
Iter: 1064 loss: 8.60586738e-07
Iter: 1065 loss: 8.61840476e-07
Iter: 1066 loss: 8.60338616e-07
Iter: 1067 loss: 8.59456861e-07
Iter: 1068 loss: 8.61301601e-07
Iter: 1069 loss: 8.59082661e-07
Iter: 1070 loss: 8.58392241e-07
Iter: 1071 loss: 8.58936403e-07
Iter: 1072 loss: 8.57979899e-07
Iter: 1073 loss: 8.57285e-07
Iter: 1074 loss: 8.64072604e-07
Iter: 1075 loss: 8.57274642e-07
Iter: 1076 loss: 8.56696715e-07
Iter: 1077 loss: 8.58718522e-07
Iter: 1078 loss: 8.56511e-07
Iter: 1079 loss: 8.56082352e-07
Iter: 1080 loss: 8.57442046e-07
Iter: 1081 loss: 8.55935241e-07
Iter: 1082 loss: 8.55432859e-07
Iter: 1083 loss: 8.58673275e-07
Iter: 1084 loss: 8.55367603e-07
Iter: 1085 loss: 8.55027338e-07
Iter: 1086 loss: 8.54377618e-07
Iter: 1087 loss: 8.69920598e-07
Iter: 1088 loss: 8.54395466e-07
Iter: 1089 loss: 8.53747736e-07
Iter: 1090 loss: 8.58255476e-07
Iter: 1091 loss: 8.53654342e-07
Iter: 1092 loss: 8.53148606e-07
Iter: 1093 loss: 8.52229846e-07
Iter: 1094 loss: 8.5221609e-07
Iter: 1095 loss: 8.51225252e-07
Iter: 1096 loss: 8.58102567e-07
Iter: 1097 loss: 8.51148343e-07
Iter: 1098 loss: 8.5039e-07
Iter: 1099 loss: 8.50879928e-07
Iter: 1100 loss: 8.49853564e-07
Iter: 1101 loss: 8.48933e-07
Iter: 1102 loss: 8.59969532e-07
Iter: 1103 loss: 8.48956347e-07
Iter: 1104 loss: 8.48412583e-07
Iter: 1105 loss: 8.50755384e-07
Iter: 1106 loss: 8.48330671e-07
Iter: 1107 loss: 8.4779839e-07
Iter: 1108 loss: 8.47231888e-07
Iter: 1109 loss: 8.471294e-07
Iter: 1110 loss: 8.4638657e-07
Iter: 1111 loss: 8.54515e-07
Iter: 1112 loss: 8.46360479e-07
Iter: 1113 loss: 8.45805687e-07
Iter: 1114 loss: 8.48724824e-07
Iter: 1115 loss: 8.45692625e-07
Iter: 1116 loss: 8.4516887e-07
Iter: 1117 loss: 8.4642727e-07
Iter: 1118 loss: 8.44992371e-07
Iter: 1119 loss: 8.44450426e-07
Iter: 1120 loss: 8.48331183e-07
Iter: 1121 loss: 8.44407623e-07
Iter: 1122 loss: 8.44105841e-07
Iter: 1123 loss: 8.43428722e-07
Iter: 1124 loss: 8.52023618e-07
Iter: 1125 loss: 8.43405815e-07
Iter: 1126 loss: 8.42675377e-07
Iter: 1127 loss: 8.4918463e-07
Iter: 1128 loss: 8.42633483e-07
Iter: 1129 loss: 8.42041914e-07
Iter: 1130 loss: 8.41892586e-07
Iter: 1131 loss: 8.41559654e-07
Iter: 1132 loss: 8.40748612e-07
Iter: 1133 loss: 8.43601299e-07
Iter: 1134 loss: 8.40532607e-07
Iter: 1135 loss: 8.39845086e-07
Iter: 1136 loss: 8.39635391e-07
Iter: 1137 loss: 8.39246354e-07
Iter: 1138 loss: 8.38446226e-07
Iter: 1139 loss: 8.38444635e-07
Iter: 1140 loss: 8.37889161e-07
Iter: 1141 loss: 8.40203938e-07
Iter: 1142 loss: 8.3780526e-07
Iter: 1143 loss: 8.37243078e-07
Iter: 1144 loss: 8.36913159e-07
Iter: 1145 loss: 8.36702725e-07
Iter: 1146 loss: 8.35907144e-07
Iter: 1147 loss: 8.38955259e-07
Iter: 1148 loss: 8.35763103e-07
Iter: 1149 loss: 8.3522383e-07
Iter: 1150 loss: 8.35222522e-07
Iter: 1151 loss: 8.34852244e-07
Iter: 1152 loss: 8.35679145e-07
Iter: 1153 loss: 8.34718776e-07
Iter: 1154 loss: 8.34244872e-07
Iter: 1155 loss: 8.3507507e-07
Iter: 1156 loss: 8.34007039e-07
Iter: 1157 loss: 8.33537797e-07
Iter: 1158 loss: 8.33062927e-07
Iter: 1159 loss: 8.32945148e-07
Iter: 1160 loss: 8.32217438e-07
Iter: 1161 loss: 8.33127672e-07
Iter: 1162 loss: 8.31861655e-07
Iter: 1163 loss: 8.30988142e-07
Iter: 1164 loss: 8.38678204e-07
Iter: 1165 loss: 8.30969213e-07
Iter: 1166 loss: 8.30501563e-07
Iter: 1167 loss: 8.29892258e-07
Iter: 1168 loss: 8.29858209e-07
Iter: 1169 loss: 8.28935299e-07
Iter: 1170 loss: 8.33655236e-07
Iter: 1171 loss: 8.28738848e-07
Iter: 1172 loss: 8.28071336e-07
Iter: 1173 loss: 8.29499129e-07
Iter: 1174 loss: 8.2782509e-07
Iter: 1175 loss: 8.26953283e-07
Iter: 1176 loss: 8.32878186e-07
Iter: 1177 loss: 8.2690525e-07
Iter: 1178 loss: 8.26441124e-07
Iter: 1179 loss: 8.27380632e-07
Iter: 1180 loss: 8.26197379e-07
Iter: 1181 loss: 8.25741211e-07
Iter: 1182 loss: 8.26199539e-07
Iter: 1183 loss: 8.25439201e-07
Iter: 1184 loss: 8.249865e-07
Iter: 1185 loss: 8.24967458e-07
Iter: 1186 loss: 8.24628273e-07
Iter: 1187 loss: 8.25519066e-07
Iter: 1188 loss: 8.24503957e-07
Iter: 1189 loss: 8.24041308e-07
Iter: 1190 loss: 8.24435176e-07
Iter: 1191 loss: 8.23751634e-07
Iter: 1192 loss: 8.23363962e-07
Iter: 1193 loss: 8.23087703e-07
Iter: 1194 loss: 8.22928087e-07
Iter: 1195 loss: 8.22248523e-07
Iter: 1196 loss: 8.22575146e-07
Iter: 1197 loss: 8.21782578e-07
Iter: 1198 loss: 8.20895821e-07
Iter: 1199 loss: 8.2857639e-07
Iter: 1200 loss: 8.20860407e-07
Iter: 1201 loss: 8.20232913e-07
Iter: 1202 loss: 8.20954313e-07
Iter: 1203 loss: 8.19867182e-07
Iter: 1204 loss: 8.19319268e-07
Iter: 1205 loss: 8.1986e-07
Iter: 1206 loss: 8.18951776e-07
Iter: 1207 loss: 8.1817393e-07
Iter: 1208 loss: 8.19178581e-07
Iter: 1209 loss: 8.17789328e-07
Iter: 1210 loss: 8.16992042e-07
Iter: 1211 loss: 8.26021505e-07
Iter: 1212 loss: 8.16984141e-07
Iter: 1213 loss: 8.1637711e-07
Iter: 1214 loss: 8.18538865e-07
Iter: 1215 loss: 8.1620226e-07
Iter: 1216 loss: 8.15729436e-07
Iter: 1217 loss: 8.15686235e-07
Iter: 1218 loss: 8.15231033e-07
Iter: 1219 loss: 8.14637701e-07
Iter: 1220 loss: 8.21607728e-07
Iter: 1221 loss: 8.14587338e-07
Iter: 1222 loss: 8.14062673e-07
Iter: 1223 loss: 8.17092939e-07
Iter: 1224 loss: 8.13979739e-07
Iter: 1225 loss: 8.13607471e-07
Iter: 1226 loss: 8.14955683e-07
Iter: 1227 loss: 8.13503e-07
Iter: 1228 loss: 8.13134704e-07
Iter: 1229 loss: 8.12497547e-07
Iter: 1230 loss: 8.25655491e-07
Iter: 1231 loss: 8.12503345e-07
Iter: 1232 loss: 8.11718905e-07
Iter: 1233 loss: 8.14447105e-07
Iter: 1234 loss: 8.11476752e-07
Iter: 1235 loss: 8.10842778e-07
Iter: 1236 loss: 8.1188665e-07
Iter: 1237 loss: 8.10522408e-07
Iter: 1238 loss: 8.09831363e-07
Iter: 1239 loss: 8.157308e-07
Iter: 1240 loss: 8.09817607e-07
Iter: 1241 loss: 8.09355925e-07
Iter: 1242 loss: 8.09374114e-07
Iter: 1243 loss: 8.09078074e-07
Iter: 1244 loss: 8.08335e-07
Iter: 1245 loss: 8.08933e-07
Iter: 1246 loss: 8.07890387e-07
Iter: 1247 loss: 8.07296772e-07
Iter: 1248 loss: 8.11403879e-07
Iter: 1249 loss: 8.07220886e-07
Iter: 1250 loss: 8.06543767e-07
Iter: 1251 loss: 8.08372306e-07
Iter: 1252 loss: 8.06300818e-07
Iter: 1253 loss: 8.05724483e-07
Iter: 1254 loss: 8.06674336e-07
Iter: 1255 loss: 8.05462946e-07
Iter: 1256 loss: 8.04962099e-07
Iter: 1257 loss: 8.11520465e-07
Iter: 1258 loss: 8.04947e-07
Iter: 1259 loss: 8.04507351e-07
Iter: 1260 loss: 8.05708964e-07
Iter: 1261 loss: 8.04407e-07
Iter: 1262 loss: 8.04010142e-07
Iter: 1263 loss: 8.04683737e-07
Iter: 1264 loss: 8.0385712e-07
Iter: 1265 loss: 8.03400269e-07
Iter: 1266 loss: 8.02915167e-07
Iter: 1267 loss: 8.02807449e-07
Iter: 1268 loss: 8.02217698e-07
Iter: 1269 loss: 8.03423291e-07
Iter: 1270 loss: 8.01983219e-07
Iter: 1271 loss: 8.01280237e-07
Iter: 1272 loss: 8.03210241e-07
Iter: 1273 loss: 8.01033252e-07
Iter: 1274 loss: 8.00482894e-07
Iter: 1275 loss: 8.04053855e-07
Iter: 1276 loss: 8.00441285e-07
Iter: 1277 loss: 7.99889676e-07
Iter: 1278 loss: 8.00854764e-07
Iter: 1279 loss: 7.99624161e-07
Iter: 1280 loss: 7.99103532e-07
Iter: 1281 loss: 7.98702274e-07
Iter: 1282 loss: 7.98504743e-07
Iter: 1283 loss: 7.97609289e-07
Iter: 1284 loss: 8.0215375e-07
Iter: 1285 loss: 7.97467578e-07
Iter: 1286 loss: 7.9681331e-07
Iter: 1287 loss: 8.01009321e-07
Iter: 1288 loss: 7.96748054e-07
Iter: 1289 loss: 7.96193149e-07
Iter: 1290 loss: 7.99827e-07
Iter: 1291 loss: 7.96140512e-07
Iter: 1292 loss: 7.95761309e-07
Iter: 1293 loss: 7.96006134e-07
Iter: 1294 loss: 7.9560823e-07
Iter: 1295 loss: 7.95145695e-07
Iter: 1296 loss: 8.00713963e-07
Iter: 1297 loss: 7.95151664e-07
Iter: 1298 loss: 7.94746938e-07
Iter: 1299 loss: 7.94254447e-07
Iter: 1300 loss: 7.94199764e-07
Iter: 1301 loss: 7.93510935e-07
Iter: 1302 loss: 7.98085807e-07
Iter: 1303 loss: 7.93462107e-07
Iter: 1304 loss: 7.93051754e-07
Iter: 1305 loss: 7.9259064e-07
Iter: 1306 loss: 7.92518222e-07
Iter: 1307 loss: 7.91845764e-07
Iter: 1308 loss: 7.94605171e-07
Iter: 1309 loss: 7.91739808e-07
Iter: 1310 loss: 7.91152956e-07
Iter: 1311 loss: 7.92137257e-07
Iter: 1312 loss: 7.90897843e-07
Iter: 1313 loss: 7.90253353e-07
Iter: 1314 loss: 7.93460345e-07
Iter: 1315 loss: 7.90162858e-07
Iter: 1316 loss: 7.8960079e-07
Iter: 1317 loss: 7.91113337e-07
Iter: 1318 loss: 7.8941423e-07
Iter: 1319 loss: 7.88813736e-07
Iter: 1320 loss: 7.89202659e-07
Iter: 1321 loss: 7.88438342e-07
Iter: 1322 loss: 7.87834097e-07
Iter: 1323 loss: 7.88959937e-07
Iter: 1324 loss: 7.87520321e-07
Iter: 1325 loss: 7.87077738e-07
Iter: 1326 loss: 7.870334e-07
Iter: 1327 loss: 7.86535452e-07
Iter: 1328 loss: 7.86462408e-07
Iter: 1329 loss: 7.86150736e-07
Iter: 1330 loss: 7.85825591e-07
Iter: 1331 loss: 7.85780571e-07
Iter: 1332 loss: 7.85470434e-07
Iter: 1333 loss: 7.85113116e-07
Iter: 1334 loss: 7.85107488e-07
Iter: 1335 loss: 7.84641713e-07
Iter: 1336 loss: 7.86704561e-07
Iter: 1337 loss: 7.84579e-07
Iter: 1338 loss: 7.8418077e-07
Iter: 1339 loss: 7.840938e-07
Iter: 1340 loss: 7.83850453e-07
Iter: 1341 loss: 7.83306291e-07
Iter: 1342 loss: 7.8284927e-07
Iter: 1343 loss: 7.82708071e-07
Iter: 1344 loss: 7.82105417e-07
Iter: 1345 loss: 7.91235607e-07
Iter: 1346 loss: 7.82062784e-07
Iter: 1347 loss: 7.81585868e-07
Iter: 1348 loss: 7.81446261e-07
Iter: 1349 loss: 7.81145673e-07
Iter: 1350 loss: 7.80403e-07
Iter: 1351 loss: 7.86978148e-07
Iter: 1352 loss: 7.80386245e-07
Iter: 1353 loss: 7.79883521e-07
Iter: 1354 loss: 7.80534037e-07
Iter: 1355 loss: 7.79637503e-07
Iter: 1356 loss: 7.7902024e-07
Iter: 1357 loss: 7.79048833e-07
Iter: 1358 loss: 7.78542e-07
Iter: 1359 loss: 7.77796515e-07
Iter: 1360 loss: 7.81716608e-07
Iter: 1361 loss: 7.77633829e-07
Iter: 1362 loss: 7.77095806e-07
Iter: 1363 loss: 7.84275699e-07
Iter: 1364 loss: 7.77086314e-07
Iter: 1365 loss: 7.76729507e-07
Iter: 1366 loss: 7.78452886e-07
Iter: 1367 loss: 7.76662603e-07
Iter: 1368 loss: 7.76342404e-07
Iter: 1369 loss: 7.77790092e-07
Iter: 1370 loss: 7.76276693e-07
Iter: 1371 loss: 7.76052843e-07
Iter: 1372 loss: 7.75715307e-07
Iter: 1373 loss: 7.75680405e-07
Iter: 1374 loss: 7.75216e-07
Iter: 1375 loss: 7.78651781e-07
Iter: 1376 loss: 7.75190301e-07
Iter: 1377 loss: 7.74838554e-07
Iter: 1378 loss: 7.74288651e-07
Iter: 1379 loss: 7.74286775e-07
Iter: 1380 loss: 7.73571855e-07
Iter: 1381 loss: 7.74871921e-07
Iter: 1382 loss: 7.73272291e-07
Iter: 1383 loss: 7.72462897e-07
Iter: 1384 loss: 7.75460649e-07
Iter: 1385 loss: 7.72274859e-07
Iter: 1386 loss: 7.71689884e-07
Iter: 1387 loss: 7.77772811e-07
Iter: 1388 loss: 7.71672831e-07
Iter: 1389 loss: 7.71238206e-07
Iter: 1390 loss: 7.71525322e-07
Iter: 1391 loss: 7.70968654e-07
Iter: 1392 loss: 7.70383963e-07
Iter: 1393 loss: 7.71820623e-07
Iter: 1394 loss: 7.70226848e-07
Iter: 1395 loss: 7.69655628e-07
Iter: 1396 loss: 7.70572342e-07
Iter: 1397 loss: 7.69424275e-07
Iter: 1398 loss: 7.68921382e-07
Iter: 1399 loss: 7.71586201e-07
Iter: 1400 loss: 7.6884146e-07
Iter: 1401 loss: 7.68294399e-07
Iter: 1402 loss: 7.71391569e-07
Iter: 1403 loss: 7.68246537e-07
Iter: 1404 loss: 7.67828055e-07
Iter: 1405 loss: 7.71469445e-07
Iter: 1406 loss: 7.67840049e-07
Iter: 1407 loss: 7.67603069e-07
Iter: 1408 loss: 7.67277356e-07
Iter: 1409 loss: 7.67258143e-07
Iter: 1410 loss: 7.66812605e-07
Iter: 1411 loss: 7.67668212e-07
Iter: 1412 loss: 7.66639801e-07
Iter: 1413 loss: 7.66060339e-07
Iter: 1414 loss: 7.68384439e-07
Iter: 1415 loss: 7.65932043e-07
Iter: 1416 loss: 7.65542609e-07
Iter: 1417 loss: 7.64962124e-07
Iter: 1418 loss: 7.64925289e-07
Iter: 1419 loss: 7.64258857e-07
Iter: 1420 loss: 7.68460097e-07
Iter: 1421 loss: 7.64171e-07
Iter: 1422 loss: 7.63617209e-07
Iter: 1423 loss: 7.64917218e-07
Iter: 1424 loss: 7.63452192e-07
Iter: 1425 loss: 7.628459e-07
Iter: 1426 loss: 7.67626886e-07
Iter: 1427 loss: 7.62848686e-07
Iter: 1428 loss: 7.62431114e-07
Iter: 1429 loss: 7.6287472e-07
Iter: 1430 loss: 7.62240745e-07
Iter: 1431 loss: 7.61722845e-07
Iter: 1432 loss: 7.62143145e-07
Iter: 1433 loss: 7.61417084e-07
Iter: 1434 loss: 7.60704665e-07
Iter: 1435 loss: 7.61959029e-07
Iter: 1436 loss: 7.60365424e-07
Iter: 1437 loss: 7.60034254e-07
Iter: 1438 loss: 7.5995e-07
Iter: 1439 loss: 7.59609634e-07
Iter: 1440 loss: 7.6011986e-07
Iter: 1441 loss: 7.59428701e-07
Iter: 1442 loss: 7.58958095e-07
Iter: 1443 loss: 7.59115665e-07
Iter: 1444 loss: 7.58673252e-07
Iter: 1445 loss: 7.58233739e-07
Iter: 1446 loss: 7.59124191e-07
Iter: 1447 loss: 7.58054171e-07
Iter: 1448 loss: 7.57752389e-07
Iter: 1449 loss: 7.59784257e-07
Iter: 1450 loss: 7.57691396e-07
Iter: 1451 loss: 7.57336e-07
Iter: 1452 loss: 7.57250689e-07
Iter: 1453 loss: 7.57013368e-07
Iter: 1454 loss: 7.56500242e-07
Iter: 1455 loss: 7.56236204e-07
Iter: 1456 loss: 7.5600849e-07
Iter: 1457 loss: 7.55338363e-07
Iter: 1458 loss: 7.59327747e-07
Iter: 1459 loss: 7.55242e-07
Iter: 1460 loss: 7.54675398e-07
Iter: 1461 loss: 7.56114503e-07
Iter: 1462 loss: 7.54468374e-07
Iter: 1463 loss: 7.53994073e-07
Iter: 1464 loss: 7.58961278e-07
Iter: 1465 loss: 7.53981681e-07
Iter: 1466 loss: 7.53544498e-07
Iter: 1467 loss: 7.53007157e-07
Iter: 1468 loss: 7.52982146e-07
Iter: 1469 loss: 7.5218793e-07
Iter: 1470 loss: 7.57336124e-07
Iter: 1471 loss: 7.52117217e-07
Iter: 1472 loss: 7.51737957e-07
Iter: 1473 loss: 7.52436847e-07
Iter: 1474 loss: 7.51550829e-07
Iter: 1475 loss: 7.5130896e-07
Iter: 1476 loss: 7.51260586e-07
Iter: 1477 loss: 7.50997742e-07
Iter: 1478 loss: 7.51075959e-07
Iter: 1479 loss: 7.50813456e-07
Iter: 1480 loss: 7.50528841e-07
Iter: 1481 loss: 7.50837216e-07
Iter: 1482 loss: 7.50358822e-07
Iter: 1483 loss: 7.49932042e-07
Iter: 1484 loss: 7.49816195e-07
Iter: 1485 loss: 7.49576316e-07
Iter: 1486 loss: 7.49028e-07
Iter: 1487 loss: 7.52072424e-07
Iter: 1488 loss: 7.48977072e-07
Iter: 1489 loss: 7.48448088e-07
Iter: 1490 loss: 7.49468711e-07
Iter: 1491 loss: 7.48251e-07
Iter: 1492 loss: 7.47820707e-07
Iter: 1493 loss: 7.47717934e-07
Iter: 1494 loss: 7.47416e-07
Iter: 1495 loss: 7.46868579e-07
Iter: 1496 loss: 7.49227809e-07
Iter: 1497 loss: 7.46724481e-07
Iter: 1498 loss: 7.46221872e-07
Iter: 1499 loss: 7.46516889e-07
Iter: 1500 loss: 7.45839941e-07
Iter: 1501 loss: 7.45214095e-07
Iter: 1502 loss: 7.5145806e-07
Iter: 1503 loss: 7.45201e-07
Iter: 1504 loss: 7.44763e-07
Iter: 1505 loss: 7.4584068e-07
Iter: 1506 loss: 7.44559713e-07
Iter: 1507 loss: 7.44056763e-07
Iter: 1508 loss: 7.44171473e-07
Iter: 1509 loss: 7.43674264e-07
Iter: 1510 loss: 7.43208375e-07
Iter: 1511 loss: 7.46041e-07
Iter: 1512 loss: 7.43106057e-07
Iter: 1513 loss: 7.42742543e-07
Iter: 1514 loss: 7.46054411e-07
Iter: 1515 loss: 7.42728218e-07
Iter: 1516 loss: 7.4232878e-07
Iter: 1517 loss: 7.44320289e-07
Iter: 1518 loss: 7.42267105e-07
Iter: 1519 loss: 7.4195583e-07
Iter: 1520 loss: 7.41714871e-07
Iter: 1521 loss: 7.41575832e-07
Iter: 1522 loss: 7.40986138e-07
Iter: 1523 loss: 7.42011878e-07
Iter: 1524 loss: 7.4069203e-07
Iter: 1525 loss: 7.40331586e-07
Iter: 1526 loss: 7.42168652e-07
Iter: 1527 loss: 7.40286907e-07
Iter: 1528 loss: 7.39885763e-07
Iter: 1529 loss: 7.39946756e-07
Iter: 1530 loss: 7.39550728e-07
Iter: 1531 loss: 7.39008726e-07
Iter: 1532 loss: 7.3970773e-07
Iter: 1533 loss: 7.38731728e-07
Iter: 1534 loss: 7.38144706e-07
Iter: 1535 loss: 7.38446886e-07
Iter: 1536 loss: 7.37776077e-07
Iter: 1537 loss: 7.37113737e-07
Iter: 1538 loss: 7.40912697e-07
Iter: 1539 loss: 7.36998231e-07
Iter: 1540 loss: 7.36468678e-07
Iter: 1541 loss: 7.39496841e-07
Iter: 1542 loss: 7.36392792e-07
Iter: 1543 loss: 7.35978801e-07
Iter: 1544 loss: 7.38032554e-07
Iter: 1545 loss: 7.35904e-07
Iter: 1546 loss: 7.3559147e-07
Iter: 1547 loss: 7.35423214e-07
Iter: 1548 loss: 7.35229662e-07
Iter: 1549 loss: 7.34642867e-07
Iter: 1550 loss: 7.35721528e-07
Iter: 1551 loss: 7.34432888e-07
Iter: 1552 loss: 7.34176069e-07
Iter: 1553 loss: 7.34065907e-07
Iter: 1554 loss: 7.33815625e-07
Iter: 1555 loss: 7.33798572e-07
Iter: 1556 loss: 7.33610818e-07
Iter: 1557 loss: 7.33287e-07
Iter: 1558 loss: 7.338042e-07
Iter: 1559 loss: 7.33141633e-07
Iter: 1560 loss: 7.327871e-07
Iter: 1561 loss: 7.33022e-07
Iter: 1562 loss: 7.32548756e-07
Iter: 1563 loss: 7.32189392e-07
Iter: 1564 loss: 7.34083926e-07
Iter: 1565 loss: 7.32109e-07
Iter: 1566 loss: 7.31697583e-07
Iter: 1567 loss: 7.32139597e-07
Iter: 1568 loss: 7.31459238e-07
Iter: 1569 loss: 7.3099136e-07
Iter: 1570 loss: 7.31062187e-07
Iter: 1571 loss: 7.30654961e-07
Iter: 1572 loss: 7.30070781e-07
Iter: 1573 loss: 7.30843908e-07
Iter: 1574 loss: 7.29744897e-07
Iter: 1575 loss: 7.29166e-07
Iter: 1576 loss: 7.33337401e-07
Iter: 1577 loss: 7.2908864e-07
Iter: 1578 loss: 7.28625309e-07
Iter: 1579 loss: 7.29832323e-07
Iter: 1580 loss: 7.28484906e-07
Iter: 1581 loss: 7.27983718e-07
Iter: 1582 loss: 7.30834699e-07
Iter: 1583 loss: 7.27953534e-07
Iter: 1584 loss: 7.27570125e-07
Iter: 1585 loss: 7.27186148e-07
Iter: 1586 loss: 7.27117936e-07
Iter: 1587 loss: 7.27173244e-07
Iter: 1588 loss: 7.26844178e-07
Iter: 1589 loss: 7.26645e-07
Iter: 1590 loss: 7.26609358e-07
Iter: 1591 loss: 7.26486e-07
Iter: 1592 loss: 7.26251699e-07
Iter: 1593 loss: 7.26440703e-07
Iter: 1594 loss: 7.26097142e-07
Iter: 1595 loss: 7.25700602e-07
Iter: 1596 loss: 7.25842142e-07
Iter: 1597 loss: 7.25432699e-07
Iter: 1598 loss: 7.24993129e-07
Iter: 1599 loss: 7.26181668e-07
Iter: 1600 loss: 7.24813049e-07
Iter: 1601 loss: 7.24336871e-07
Iter: 1602 loss: 7.26857e-07
Iter: 1603 loss: 7.24251208e-07
Iter: 1604 loss: 7.23838752e-07
Iter: 1605 loss: 7.23520145e-07
Iter: 1606 loss: 7.23380595e-07
Iter: 1607 loss: 7.22757079e-07
Iter: 1608 loss: 7.25213795e-07
Iter: 1609 loss: 7.22617642e-07
Iter: 1610 loss: 7.22163293e-07
Iter: 1611 loss: 7.22805339e-07
Iter: 1612 loss: 7.21941433e-07
Iter: 1613 loss: 7.2134867e-07
Iter: 1614 loss: 7.22828304e-07
Iter: 1615 loss: 7.21149092e-07
Iter: 1616 loss: 7.20601918e-07
Iter: 1617 loss: 7.23760195e-07
Iter: 1618 loss: 7.20522905e-07
Iter: 1619 loss: 7.20036326e-07
Iter: 1620 loss: 7.22516518e-07
Iter: 1621 loss: 7.19924458e-07
Iter: 1622 loss: 7.1962279e-07
Iter: 1623 loss: 7.21505444e-07
Iter: 1624 loss: 7.19596926e-07
Iter: 1625 loss: 7.19178047e-07
Iter: 1626 loss: 7.19988293e-07
Iter: 1627 loss: 7.19039917e-07
Iter: 1628 loss: 7.18741035e-07
Iter: 1629 loss: 7.18598926e-07
Iter: 1630 loss: 7.18449655e-07
Iter: 1631 loss: 7.17942726e-07
Iter: 1632 loss: 7.19415084e-07
Iter: 1633 loss: 7.17735588e-07
Iter: 1634 loss: 7.17224566e-07
Iter: 1635 loss: 7.18000138e-07
Iter: 1636 loss: 7.1697923e-07
Iter: 1637 loss: 7.16577802e-07
Iter: 1638 loss: 7.20528192e-07
Iter: 1639 loss: 7.16549e-07
Iter: 1640 loss: 7.16178533e-07
Iter: 1641 loss: 7.15868168e-07
Iter: 1642 loss: 7.15784267e-07
Iter: 1643 loss: 7.15276883e-07
Iter: 1644 loss: 7.17541411e-07
Iter: 1645 loss: 7.15169676e-07
Iter: 1646 loss: 7.1469367e-07
Iter: 1647 loss: 7.14488692e-07
Iter: 1648 loss: 7.1427e-07
Iter: 1649 loss: 7.13614e-07
Iter: 1650 loss: 7.1901087e-07
Iter: 1651 loss: 7.1358852e-07
Iter: 1652 loss: 7.13156282e-07
Iter: 1653 loss: 7.13448344e-07
Iter: 1654 loss: 7.12947553e-07
Iter: 1655 loss: 7.1244915e-07
Iter: 1656 loss: 7.16758734e-07
Iter: 1657 loss: 7.12415044e-07
Iter: 1658 loss: 7.12030442e-07
Iter: 1659 loss: 7.13850454e-07
Iter: 1660 loss: 7.1193665e-07
Iter: 1661 loss: 7.1161071e-07
Iter: 1662 loss: 7.15535407e-07
Iter: 1663 loss: 7.11611619e-07
Iter: 1664 loss: 7.11432222e-07
Iter: 1665 loss: 7.1097395e-07
Iter: 1666 loss: 7.15148076e-07
Iter: 1667 loss: 7.10916368e-07
Iter: 1668 loss: 7.10414611e-07
Iter: 1669 loss: 7.13566692e-07
Iter: 1670 loss: 7.10379879e-07
Iter: 1671 loss: 7.09926098e-07
Iter: 1672 loss: 7.12190683e-07
Iter: 1673 loss: 7.09851747e-07
Iter: 1674 loss: 7.09513188e-07
Iter: 1675 loss: 7.09334813e-07
Iter: 1676 loss: 7.09136543e-07
Iter: 1677 loss: 7.08741368e-07
Iter: 1678 loss: 7.1330112e-07
Iter: 1679 loss: 7.08755806e-07
Iter: 1680 loss: 7.08404457e-07
Iter: 1681 loss: 7.08375183e-07
Iter: 1682 loss: 7.08112225e-07
Iter: 1683 loss: 7.07587276e-07
Iter: 1684 loss: 7.08143375e-07
Iter: 1685 loss: 7.07310164e-07
Iter: 1686 loss: 7.06882133e-07
Iter: 1687 loss: 7.08457151e-07
Iter: 1688 loss: 7.06767082e-07
Iter: 1689 loss: 7.06370088e-07
Iter: 1690 loss: 7.07504228e-07
Iter: 1691 loss: 7.06242872e-07
Iter: 1692 loss: 7.05759362e-07
Iter: 1693 loss: 7.05617e-07
Iter: 1694 loss: 7.05324851e-07
Iter: 1695 loss: 7.05019033e-07
Iter: 1696 loss: 7.04954e-07
Iter: 1697 loss: 7.04687295e-07
Iter: 1698 loss: 7.07085348e-07
Iter: 1699 loss: 7.04685704e-07
Iter: 1700 loss: 7.04485672e-07
Iter: 1701 loss: 7.04217712e-07
Iter: 1702 loss: 7.04189461e-07
Iter: 1703 loss: 7.03850787e-07
Iter: 1704 loss: 7.04370109e-07
Iter: 1705 loss: 7.03650812e-07
Iter: 1706 loss: 7.03245746e-07
Iter: 1707 loss: 7.02858301e-07
Iter: 1708 loss: 7.02753e-07
Iter: 1709 loss: 7.02097282e-07
Iter: 1710 loss: 7.07181471e-07
Iter: 1711 loss: 7.02018269e-07
Iter: 1712 loss: 7.01582621e-07
Iter: 1713 loss: 7.05891921e-07
Iter: 1714 loss: 7.01549652e-07
Iter: 1715 loss: 7.01174713e-07
Iter: 1716 loss: 7.01557383e-07
Iter: 1717 loss: 7.0099e-07
Iter: 1718 loss: 7.00598e-07
Iter: 1719 loss: 7.00752651e-07
Iter: 1720 loss: 7.00370379e-07
Iter: 1721 loss: 6.99968382e-07
Iter: 1722 loss: 7.04487e-07
Iter: 1723 loss: 6.99942575e-07
Iter: 1724 loss: 6.99530801e-07
Iter: 1725 loss: 6.99553709e-07
Iter: 1726 loss: 6.99243742e-07
Iter: 1727 loss: 6.98773249e-07
Iter: 1728 loss: 7.00226678e-07
Iter: 1729 loss: 6.98670362e-07
Iter: 1730 loss: 6.98265922e-07
Iter: 1731 loss: 6.98307304e-07
Iter: 1732 loss: 6.97958114e-07
Iter: 1733 loss: 6.98104714e-07
Iter: 1734 loss: 6.97744895e-07
Iter: 1735 loss: 6.97600285e-07
Iter: 1736 loss: 6.97349321e-07
Iter: 1737 loss: 7.02991656e-07
Iter: 1738 loss: 6.97335508e-07
Iter: 1739 loss: 6.9697478e-07
Iter: 1740 loss: 6.96840743e-07
Iter: 1741 loss: 6.9665964e-07
Iter: 1742 loss: 6.96092229e-07
Iter: 1743 loss: 6.99150291e-07
Iter: 1744 loss: 6.96017594e-07
Iter: 1745 loss: 6.95560573e-07
Iter: 1746 loss: 6.96130485e-07
Iter: 1747 loss: 6.95285166e-07
Iter: 1748 loss: 6.9489397e-07
Iter: 1749 loss: 6.9695534e-07
Iter: 1750 loss: 6.9481024e-07
Iter: 1751 loss: 6.94449e-07
Iter: 1752 loss: 6.96491497e-07
Iter: 1753 loss: 6.94405344e-07
Iter: 1754 loss: 6.94057121e-07
Iter: 1755 loss: 6.94021821e-07
Iter: 1756 loss: 6.93823267e-07
Iter: 1757 loss: 6.9338904e-07
Iter: 1758 loss: 6.93906372e-07
Iter: 1759 loss: 6.93195773e-07
Iter: 1760 loss: 6.92797e-07
Iter: 1761 loss: 6.92801109e-07
Iter: 1762 loss: 6.92517801e-07
Iter: 1763 loss: 6.92564868e-07
Iter: 1764 loss: 6.92256606e-07
Iter: 1765 loss: 6.91871264e-07
Iter: 1766 loss: 6.92001208e-07
Iter: 1767 loss: 6.91614787e-07
Iter: 1768 loss: 6.91104049e-07
Iter: 1769 loss: 6.9268242e-07
Iter: 1770 loss: 6.90965066e-07
Iter: 1771 loss: 6.90872525e-07
Iter: 1772 loss: 6.90706315e-07
Iter: 1773 loss: 6.9053408e-07
Iter: 1774 loss: 6.90238721e-07
Iter: 1775 loss: 6.97560324e-07
Iter: 1776 loss: 6.90230252e-07
Iter: 1777 loss: 6.89881176e-07
Iter: 1778 loss: 6.89929834e-07
Iter: 1779 loss: 6.89637091e-07
Iter: 1780 loss: 6.89190301e-07
Iter: 1781 loss: 6.92743811e-07
Iter: 1782 loss: 6.89171031e-07
Iter: 1783 loss: 6.88836735e-07
Iter: 1784 loss: 6.88748742e-07
Iter: 1785 loss: 6.88589353e-07
Iter: 1786 loss: 6.88078103e-07
Iter: 1787 loss: 6.9002823e-07
Iter: 1788 loss: 6.87979423e-07
Iter: 1789 loss: 6.87596582e-07
Iter: 1790 loss: 6.91350692e-07
Iter: 1791 loss: 6.87564579e-07
Iter: 1792 loss: 6.87267516e-07
Iter: 1793 loss: 6.87042302e-07
Iter: 1794 loss: 6.86952148e-07
Iter: 1795 loss: 6.86550493e-07
Iter: 1796 loss: 6.87208967e-07
Iter: 1797 loss: 6.86389967e-07
Iter: 1798 loss: 6.85962902e-07
Iter: 1799 loss: 6.9118289e-07
Iter: 1800 loss: 6.85951761e-07
Iter: 1801 loss: 6.85595296e-07
Iter: 1802 loss: 6.85809198e-07
Iter: 1803 loss: 6.85359794e-07
Iter: 1804 loss: 6.85028e-07
Iter: 1805 loss: 6.84993495e-07
Iter: 1806 loss: 6.84713427e-07
Iter: 1807 loss: 6.84301938e-07
Iter: 1808 loss: 6.86706869e-07
Iter: 1809 loss: 6.84251688e-07
Iter: 1810 loss: 6.83914664e-07
Iter: 1811 loss: 6.8713689e-07
Iter: 1812 loss: 6.83916483e-07
Iter: 1813 loss: 6.83623341e-07
Iter: 1814 loss: 6.84255554e-07
Iter: 1815 loss: 6.83451219e-07
Iter: 1816 loss: 6.83253688e-07
Iter: 1817 loss: 6.83033477e-07
Iter: 1818 loss: 6.82998461e-07
Iter: 1819 loss: 6.82611869e-07
Iter: 1820 loss: 6.83622034e-07
Iter: 1821 loss: 6.82512166e-07
Iter: 1822 loss: 6.82104314e-07
Iter: 1823 loss: 6.82875452e-07
Iter: 1824 loss: 6.81933273e-07
Iter: 1825 loss: 6.81557538e-07
Iter: 1826 loss: 6.82058328e-07
Iter: 1827 loss: 6.81350969e-07
Iter: 1828 loss: 6.80878202e-07
Iter: 1829 loss: 6.80901621e-07
Iter: 1830 loss: 6.80495646e-07
Iter: 1831 loss: 6.80260769e-07
Iter: 1832 loss: 6.80154e-07
Iter: 1833 loss: 6.79902087e-07
Iter: 1834 loss: 6.79781465e-07
Iter: 1835 loss: 6.79650725e-07
Iter: 1836 loss: 6.79287155e-07
Iter: 1837 loss: 6.79918571e-07
Iter: 1838 loss: 6.79150276e-07
Iter: 1839 loss: 6.78788126e-07
Iter: 1840 loss: 6.8120687e-07
Iter: 1841 loss: 6.78754816e-07
Iter: 1842 loss: 6.78426204e-07
Iter: 1843 loss: 6.79091215e-07
Iter: 1844 loss: 6.78260051e-07
Iter: 1845 loss: 6.77955427e-07
Iter: 1846 loss: 6.78107085e-07
Iter: 1847 loss: 6.77738342e-07
Iter: 1848 loss: 6.77561388e-07
Iter: 1849 loss: 6.77528703e-07
Iter: 1850 loss: 6.77284447e-07
Iter: 1851 loss: 6.76812761e-07
Iter: 1852 loss: 6.81352162e-07
Iter: 1853 loss: 6.76743866e-07
Iter: 1854 loss: 6.76289e-07
Iter: 1855 loss: 6.78761126e-07
Iter: 1856 loss: 6.76230343e-07
Iter: 1857 loss: 6.758616e-07
Iter: 1858 loss: 6.76458342e-07
Iter: 1859 loss: 6.75691297e-07
Iter: 1860 loss: 6.75257638e-07
Iter: 1861 loss: 6.76916557e-07
Iter: 1862 loss: 6.75162767e-07
Iter: 1863 loss: 6.74753039e-07
Iter: 1864 loss: 6.74690341e-07
Iter: 1865 loss: 6.74407033e-07
Iter: 1866 loss: 6.73965872e-07
Iter: 1867 loss: 6.75983358e-07
Iter: 1868 loss: 6.73904424e-07
Iter: 1869 loss: 6.73453826e-07
Iter: 1870 loss: 6.75994215e-07
Iter: 1871 loss: 6.73382317e-07
Iter: 1872 loss: 6.73015336e-07
Iter: 1873 loss: 6.73528632e-07
Iter: 1874 loss: 6.72848557e-07
Iter: 1875 loss: 6.72546207e-07
Iter: 1876 loss: 6.72857368e-07
Iter: 1877 loss: 6.72356691e-07
Iter: 1878 loss: 6.71954695e-07
Iter: 1879 loss: 6.7474889e-07
Iter: 1880 loss: 6.71900352e-07
Iter: 1881 loss: 6.71477778e-07
Iter: 1882 loss: 6.71422526e-07
Iter: 1883 loss: 6.71123e-07
Iter: 1884 loss: 6.71156158e-07
Iter: 1885 loss: 6.70957377e-07
Iter: 1886 loss: 6.70818054e-07
Iter: 1887 loss: 6.70549241e-07
Iter: 1888 loss: 6.75013098e-07
Iter: 1889 loss: 6.70544921e-07
Iter: 1890 loss: 6.70240752e-07
Iter: 1891 loss: 6.7003981e-07
Iter: 1892 loss: 6.69934e-07
Iter: 1893 loss: 6.69519522e-07
Iter: 1894 loss: 6.73777777e-07
Iter: 1895 loss: 6.69519295e-07
Iter: 1896 loss: 6.69209385e-07
Iter: 1897 loss: 6.69267592e-07
Iter: 1898 loss: 6.68949156e-07
Iter: 1899 loss: 6.68522318e-07
Iter: 1900 loss: 6.71283601e-07
Iter: 1901 loss: 6.68434495e-07
Iter: 1902 loss: 6.68128848e-07
Iter: 1903 loss: 6.68065866e-07
Iter: 1904 loss: 6.67852873e-07
Iter: 1905 loss: 6.67540519e-07
Iter: 1906 loss: 6.72802912e-07
Iter: 1907 loss: 6.67539098e-07
Iter: 1908 loss: 6.67290863e-07
Iter: 1909 loss: 6.67344636e-07
Iter: 1910 loss: 6.67119707e-07
Iter: 1911 loss: 6.66736639e-07
Iter: 1912 loss: 6.66666779e-07
Iter: 1913 loss: 6.66453843e-07
Iter: 1914 loss: 6.66122787e-07
Iter: 1915 loss: 6.66096184e-07
Iter: 1916 loss: 6.6585136e-07
Iter: 1917 loss: 6.65831635e-07
Iter: 1918 loss: 6.65650873e-07
Iter: 1919 loss: 6.65438847e-07
Iter: 1920 loss: 6.65441405e-07
Iter: 1921 loss: 6.65205789e-07
Iter: 1922 loss: 6.65086304e-07
Iter: 1923 loss: 6.64975516e-07
Iter: 1924 loss: 6.64711251e-07
Iter: 1925 loss: 6.64197159e-07
Iter: 1926 loss: 6.76713398e-07
Iter: 1927 loss: 6.64203526e-07
Iter: 1928 loss: 6.63800165e-07
Iter: 1929 loss: 6.67926e-07
Iter: 1930 loss: 6.63803405e-07
Iter: 1931 loss: 6.63416358e-07
Iter: 1932 loss: 6.64204833e-07
Iter: 1933 loss: 6.63251399e-07
Iter: 1934 loss: 6.62877369e-07
Iter: 1935 loss: 6.63499918e-07
Iter: 1936 loss: 6.62734124e-07
Iter: 1937 loss: 6.62280456e-07
Iter: 1938 loss: 6.63816593e-07
Iter: 1939 loss: 6.62153298e-07
Iter: 1940 loss: 6.61762556e-07
Iter: 1941 loss: 6.62376294e-07
Iter: 1942 loss: 6.61597937e-07
Iter: 1943 loss: 6.61267222e-07
Iter: 1944 loss: 6.65465905e-07
Iter: 1945 loss: 6.61256e-07
Iter: 1946 loss: 6.60991361e-07
Iter: 1947 loss: 6.60885178e-07
Iter: 1948 loss: 6.60728688e-07
Iter: 1949 loss: 6.60354829e-07
Iter: 1950 loss: 6.61852937e-07
Iter: 1951 loss: 6.60252e-07
Iter: 1952 loss: 6.59990803e-07
Iter: 1953 loss: 6.63954324e-07
Iter: 1954 loss: 6.59984607e-07
Iter: 1955 loss: 6.59828459e-07
Iter: 1956 loss: 6.59848183e-07
Iter: 1957 loss: 6.59692e-07
Iter: 1958 loss: 6.59361831e-07
Iter: 1959 loss: 6.60058106e-07
Iter: 1960 loss: 6.59220063e-07
Iter: 1961 loss: 6.59004741e-07
Iter: 1962 loss: 6.58736155e-07
Iter: 1963 loss: 6.58688e-07
Iter: 1964 loss: 6.58307727e-07
Iter: 1965 loss: 6.58488489e-07
Iter: 1966 loss: 6.58014528e-07
Iter: 1967 loss: 6.57581722e-07
Iter: 1968 loss: 6.59608872e-07
Iter: 1969 loss: 6.57508053e-07
Iter: 1970 loss: 6.57063481e-07
Iter: 1971 loss: 6.6039388e-07
Iter: 1972 loss: 6.5701704e-07
Iter: 1973 loss: 6.56788245e-07
Iter: 1974 loss: 6.56982365e-07
Iter: 1975 loss: 6.56636757e-07
Iter: 1976 loss: 6.56229929e-07
Iter: 1977 loss: 6.56291661e-07
Iter: 1978 loss: 6.55928261e-07
Iter: 1979 loss: 6.55529675e-07
Iter: 1980 loss: 6.59278328e-07
Iter: 1981 loss: 6.55484314e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4
+ date
Wed Oct 21 14:28:00 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/300_300_300_1 --function f1 --psi 2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444be4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444be4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444ce3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444c24bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444c2dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444bac400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444b66e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444baf840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444b3e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a51d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a7f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a21f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a0d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444af5e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a44d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444ac5ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444abf730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04214b9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042148eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444abf6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042155a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042155ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4b7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042155a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4b2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4b20d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc5248c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc524840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4fb488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4fbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4799d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc3c38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc3c3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc3c32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc3a39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc428620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
